__FILENAME__ = core
class AbstractBackend(object):
    def register_command_contexts(self, context):
        pass

    def register_options_contexts(self, context):
        pass

########NEW FILE########
__FILENAME__ = distutils_backend
import os.path as op

from bento.backends.core \
    import \
        AbstractBackend
from bento.commands.command_contexts \
    import \
        ConfigureContext, BuildContext

class DistutilsConfigureContext(ConfigureContext):
    pass

class DistutilsBuildContext(BuildContext):
    def __init__(self, global_context, cmd_argv, options_context, pkg, run_node):
        from bento.commands.build_distutils import DistutilsBuilder
        super(DistutilsBuildContext, self).__init__(global_context, cmd_argv, options_context, pkg, run_node)

        o, a = options_context.parser.parse_args(cmd_argv)
        if o.jobs:
            jobs = int(o.jobs)
        else:
            jobs = 1
        if o.verbose:
            verbose = int(o.verbose)
        else:
            verbose = 0
        self.verbose = verbose
        self.jobs = jobs

        build_path = self.build_node.path_from(self.run_node)
        self._distutils_builder = DistutilsBuilder(verbosity=self.verbose, build_base=build_path)

        def _builder_factory(category, builder):
            def _build(extension, **kw):
                outputs = builder(extension)
                nodes = [self.build_node.find_node(o) for o in outputs]
                from_node = self.build_node

                pkg_dir = op.dirname(extension.name.replace('.', op.sep))
                target_dir = op.join('$sitedir', pkg_dir)
                self.outputs_registry.register_outputs(category, extension.name, nodes,
                                                       from_node, target_dir)
            return _build

        self.builder_registry.register_category("extensions", 
            _builder_factory("extensions", self._distutils_builder.build_extension))
        self.builder_registry.register_category("compiled_libraries",
            _builder_factory("compiled_libraries", self._distutils_builder.build_compiled_library))

    def compile(self):
        super(DistutilsBuildContext, self).compile()
        reg = self.builder_registry
        for category in ("extensions", "compiled_libraries"):
            for name, extension in self._node_pkg.iter_category(category):
                builder = reg.builder(category, name)
                extension = extension.extension_from(extension.ref_node)
                builder(extension)

class DistutilsBackend(AbstractBackend):
    def register_command_contexts(self, context):
        context.register_command_context("configure", DistutilsConfigureContext)
        context.register_command_context("build", DistutilsBuildContext)

########NEW FILE########
__FILENAME__ = test_distutils
import os
import tempfile

from bento.commands.build_distutils \
    import \
        DistutilsBuilder
from bento.compat.api.moves \
    import \
        unittest
from bento.core.node \
    import \
        create_base_nodes
from bento.core.testing \
    import \
        require_c_compiler, DUMMY_C, DUMMY_CLIB
from bento.core.pkg_objects \
    import \
        Extension, CompiledLibrary

class TestDistutilsBuilder(unittest.TestCase):
    def setUp(self):
        self.old_cwd = os.getcwd()
        new_cwd = tempfile.mkdtemp()
        os.chdir(new_cwd)
        try:
            self.top_node, self.build_node, self.run_node = create_base_nodes()
        except:
            os.chdir(self.old_cwd)
            raise

    def tearDown(self):
        os.chdir(self.old_cwd)

    #@require_c_compiler("distutils")
    def test_extension_simple(self):
        self.top_node.make_node("foo.c").write(DUMMY_C % {"name": "foo"})

        builder = DistutilsBuilder()
        extension = Extension("foo", ["foo.c"])
        builder.build_extension(extension)

    @require_c_compiler("distutils")
    def test_multiple_extensions(self):
        self.top_node.make_node("foo.c").write(DUMMY_C % {"name": "foo"})
        self.top_node.make_node("bar.c").write(DUMMY_C % {"name": "bar"})

        builder = DistutilsBuilder()

        extensions = [Extension("bar", ["bar.c"]), Extension("foo", ["foo.c"])]
        for extension in extensions:
            builder.build_extension(extension)

    @require_c_compiler("distutils")
    def test_clib_simple(self):
        self.top_node.make_node("foo.c").write(DUMMY_CLIB % {"name": "foo"})

        builder = DistutilsBuilder()
        extension = CompiledLibrary("foo", ["foo.c"])
        builder.build_compiled_library(extension)

    @require_c_compiler("distutils")
    def test_multiple_clibs(self):
        self.top_node.make_node("foo.c").write(DUMMY_CLIB % {"name": "foo"})
        self.top_node.make_node("bar.c").write(DUMMY_CLIB % {"name": "bar"})

        builder = DistutilsBuilder()

        extensions = [CompiledLibrary("bar", ["bar.c"]), CompiledLibrary("foo", ["foo.c"])]
        for extension in extensions:
            builder.build_compiled_library(extension)

########NEW FILE########
__FILENAME__ = test_utils
from bento.compat.api.moves \
    import \
        unittest

from bento.backends.utils \
    import \
        load_backend

class TestLoadBackend(unittest.TestCase):
    def test_simple(self):
        for backend in ("Distutils", "Yaku"):
            load_backend(backend)

########NEW FILE########
__FILENAME__ = utils
import sys

import os.path as op

_BACKENDS = {
        "Waf": ("WafBackend", "waf_backend"),
        "Yaku": ("YakuBackend", "yaku_backend"),
        "Distutils": ("DistutilsBackend", "distutils_backend")
}

def load_backend(backend_name, backend_dirs=None):
    if backend_dirs is None:
        backend_dirs = [op.dirname(__file__)]
    else:
        backend_dirs = [op.dirname(__file__)] + backend_dirs

    if backend_name in _BACKENDS:
        class_name, module_name = _BACKENDS[backend_name]
        sys.path = backend_dirs + sys.path
        try:
            __import__(module_name)
            mod = sys.modules[module_name]
            backend = getattr(mod, class_name, None)
            if backend:
                return backend
            else:
                return ValueError("Backend class %r not found in module %r" % (class_name, mod))
        finally:
            for d in backend_dirs:
                sys.path.remove(d)
    else:
        raise ValueError("Unrecognized backend: %r" % (backend_name,))

########NEW FILE########
__FILENAME__ = waf_backend
import os
import sys
import logging

import os.path as op

import bento

from bento.commands.utils \
    import \
        has_compiled_code, has_cython_code
from bento.backends.core \
    import \
        AbstractBackend
from bento.errors \
    import \
        UsageException
from bento._config \
     import \
        SITEDIR
from bento.utils.utils \
    import \
        extract_exception

if "WAFDIR" in os.environ:
    WAFDIR = os.path.join(os.environ["WAFDIR"], "waflib")
else:
    WAFDIR = op.abspath(op.join(op.dirname(bento.__file__), os.pardir, "waflib"))
    if not op.exists(WAFDIR):
        WAFDIR = os.path.join(os.getcwd(), "waflib")
if not os.path.exists(WAFDIR):
    raise UsageException("""\
%r not found: required when using waf extras !
    You can set waf location using the WAFDIR environment variable, such as
    $WAFDIR contains the 'waflib' directory""" % WAFDIR)
sys.path.insert(0, os.path.dirname(WAFDIR))

WAF_TOOLDIR = op.join(SITEDIR, "bento", "backends", "waf_tools")

from waflib.Context \
    import \
        create_context
from waflib.Options \
    import \
        OptionsContext
from waflib import Options
from waflib import Context
from waflib import Logs
from waflib import Build
from waflib import Utils
import waflib

from bento.core.node_package \
    import \
        translate_name
from bento.commands.options \
    import \
        Option
from bento.commands.command_contexts \
    import \
        ConfigureContext, BuildContext

WAF_TOP = os.path.join(WAFDIR, os.pardir)

WAF_CONFIG_LOG = 'config.log'

__USE_NO_OUTPUT_LOGGING = False
def disable_output():
    # Make Betty proud...
    global __USE_NO_OUTPUT_LOGGING
    __USE_NO_OUTPUT_LOGGING = True

def make_stream_logger(name, stream):
    # stream should be a file-like object supporting write/read/?
    logger = logging.getLogger(name)
    hdlr = logging.StreamHandler(stream)
    formatter = logging.Formatter('%(message)s')
    hdlr.setFormatter(formatter)
    logger.addHandler(hdlr)
    logger.setLevel(logging.DEBUG)
    return logger

def _init_log_no_output():
    # XXX: all this heavily plays with waf internals - only use for unit
    # testing bento where waf output screws up nose own output/logging magic
    from six.moves import cStringIO
    Logs.got_tty = False
    Logs.get_term_cols = lambda: 80
    Logs.get_color = lambda cl: ''
    Logs.colors = Logs.color_dict()

    fake_output = cStringIO()
    def fake_pprint(col, str, label='', sep='\n'):
        fake_output.write("%s%s%s %s%s" % (Logs.colors(col), str, Logs.colors.NORMAL, label, sep))

    Logs.pprint = fake_pprint

    log = logging.getLogger('waflib')
    log.handlers = []
    log.filters = []
    hdlr = logging.StreamHandler(cStringIO())
    hdlr.setFormatter(Logs.formatter())
    log.addHandler(hdlr)
    log.addFilter(Logs.log_filter())
    log.setLevel(logging.DEBUG)
    Logs.log = log

def _init(run_path, source_path, build_path):
    if not (os.path.isabs(run_path) and os.path.isabs(source_path) and os.path.isabs(build_path)):
        raise ValueError("All paths must be absolute !")
    tooldir = os.path.join(WAFDIR, "Tools")

    sys.path.insert(0, tooldir)
    cwd = os.getcwd()

    if __USE_NO_OUTPUT_LOGGING is True:
        _init_log_no_output()
    else:
        Logs.init_log()

    class FakeModule(object):
        pass
    Context.g_module = FakeModule
    Context.g_module.root_path = os.path.abspath(__file__)
    Context.g_module.top = source_path
    Context.g_module.out = build_path

    Context.top_dir = source_path
    Context.run_dir = run_path
    Context.out_dir = build_path
    Context.waf_dir = WAF_TOP
    Context.launch_dir = run_path

def register_options(global_context):
    opt = Option("--use-distutils-flags", help="Use distutils compilation flags",
                 action="store_true", default=False)
    global_context.add_option("configure", opt)

    opt = Option("-p", "--progress", help="Use progress bar", action="store_true", dest="progress_bar")
    global_context.add_option("build", opt)

class ConfigureWafContext(ConfigureContext):
    def __init__(self, global_context, cmd_argv, options_context, pkg, run_node):
        super(ConfigureWafContext, self).__init__(global_context, cmd_argv, options_context, pkg, run_node)

        run_path = self.run_node.abspath()
        source_path = self.top_node.abspath()
        build_path = self.build_node.abspath()
        _init(run_path=run_path, source_path=source_path, build_path=build_path)

        opts = OptionsContext()
        opts.load("compiler_c")
        opts.load("custom_python", tooldir=[WAF_TOOLDIR])
        opts.parse_args([])
        self.waf_options_context = opts

        waf_context = create_context("configure", run_dir=source_path)
        waf_context.options = Options.options
        waf_context.init_dirs()
        waf_context.cachedir = waf_context.bldnode.make_node(Build.CACHE_DIR)
        waf_context.cachedir.mkdir()

        path = os.path.join(waf_context.bldnode.abspath(), WAF_CONFIG_LOG)
        waf_context.logger = Logs.make_logger(path, 'cfg')
        self.waf_context = waf_context

        self._old_path = None

    def configure(self):
        o, a = self.get_parsed_arguments()
        if o.use_distutils_flags:
            Options.options.use_distutils_flags = True
        else:
            Options.options.use_distutils_flags = False

        pkg = self.pkg
        # FIXME: this is wrong (not taking into account sub packages)
        needs_compiler = has_compiled_code(pkg)
        needs_cython = has_cython_code(pkg)

        conf = self.waf_context
        if needs_compiler:
            conf.load("compiler_c")
            conf.env["PYTHON"] = [sys.executable]
            conf.load("custom_python", tooldir=[WAF_TOOLDIR])
            conf.check_python_version((2,4,2))
            conf.check_python_headers()

        if needs_cython:
            # FIXME: how to make sure the tool loaded successfully ?
            conf.load("cython", tooldir=[WAF_TOOLDIR])

    def pre_recurse(self, local_node):
        ConfigureContext.pre_recurse(self, local_node)
        self._old_path = self.waf_context.path
        # Gymnastic to make a *waf* node from a *bento* node
        self.waf_context.path = self.waf_context.path.make_node(self.local_node.srcpath())

    def post_recurse(self):
        self.waf_context.path = self._old_path
        ConfigureContext.post_recurse(self)

    def finish(self):
        super(ConfigureWafContext, self).finish()
        self.waf_context.store()

def ext_name_to_path(name):
    """Convert extension name to path - the path does not include the
    file extension

    Example: foo.bar -> foo/bar
    """
    return name.replace('.', os.path.sep)

class BentoBuildContext(Build.BuildContext):
    """Waf build context with additional support to register builder output to
    bento build context."""
    def __init__(self, *a, **kw):
        Build.BuildContext.__init__(self, *a, **kw)
        # XXX: set into BuildWafContext
        self.bento_context = None

    def register_outputs(self, category, name, outputs, target_dir=None):
        outputs_registry = self.bento_context.outputs_registry

        # waf -> bento nodes translations
        nodes = [self.bento_context.build_node.make_node(n.bldpath()) for n in outputs]
        from_node = self.bento_context.build_node.find_node(outputs[0].parent.bldpath())

        pkg_dir = os.path.dirname(name.replace('.', os.path.sep))
        if target_dir is None:
            target_dir = os.path.join('$sitedir', pkg_dir)
        outputs_registry.register_outputs(category, name, nodes, from_node, target_dir)

@waflib.TaskGen.feature("bento")
@waflib.TaskGen.before_method("apply_link")
def apply_pyext_target_renaming(self):
    if not self.target:
        self.target = self.name
    self.target = self.target.replace(".", os.sep)

@waflib.TaskGen.feature("bento")
@waflib.TaskGen.before_method("apply_link")
def apply_pyext_cstlib_fpic(self):
    if "cstlib" in self.features and self.env["CC_NAME"] == "gcc":
        if "-fPIC" in self.env.CFLAGS_cshlib:
                self.env.CFLAGS_cstlib = ["-fPIC"]

@waflib.TaskGen.feature("bento")
@waflib.TaskGen.after_method("apply_link")
def apply_register_outputs(self):
    for x in self.features:
        if x == "cprogram" and "cxx" in self.features:
            x = "cxxprogram"
        if x == "cshlib" and "cxx" in self.features:
            x = "cxxshlib"

        if x in waflib.Task.classes:
            if issubclass(waflib.Task.classes[x], waflib.Tools.ccroot.link_task):
                link = x
                break
    else:
        return

    if "pyext" in self.features and "cshlib" in self.features:
        category = "extensions"
    else:
        category = "compiled_libraries"
    bento_context = self.bld.bento_context
    ref_node = bento_context.top_node.make_node(self.path.path_from(self.path.ctx.srcnode))
    name = translate_name(self.target, ref_node, bento_context.top_node)
    self.bld.register_outputs(category, name, self.link_task.outputs)

class BuildWafContext(BuildContext):
    def pre_recurse(self, local_node):
        super(BuildWafContext, self).pre_recurse(local_node)
        self._old_path = self.waf_context.path
        # Gymnastic to make a *waf* node from a *bento* node
        self.waf_context.path = self.waf_context.path.make_node(self.local_node.srcpath())

    def post_recurse(self):
        self.waf_context.path = self._old_path
        super(BuildWafContext, self).post_recurse()

    def __init__(self, global_context, cmd_argv, options_context, pkg, run_node):
        super(BuildWafContext, self).__init__(global_context, cmd_argv, options_context, pkg, run_node)

        o, a = options_context.parser.parse_args(cmd_argv)
        if o.jobs:
            jobs = int(o.jobs)
        else:
            jobs = 1
        if o.verbose:
            verbose = int(o.verbose)
            zones = ["runner"]
        else:
            verbose = 0
            zones = []
        if o.inplace:
            self.inplace = 1
        else:
            self.inplace = 0
        if o.progress_bar:
            self.progress_bar = True
        else:
            self.progress_bar = False

        Logs.verbose = verbose
        Logs.init_log()
        if zones is None:
            Logs.zones = []
        else:
            Logs.zones = zones

        run_path = self.run_node.abspath()
        source_path = self.top_node.abspath()
        build_path = self.build_node.abspath()
        _init(run_path=run_path, source_path=source_path, build_path=build_path)

        create_context("options").parse_args([])

        waf_context = create_context("build")
        waf_context.restore()
        if not waf_context.all_envs:
            waf_context.load_envs()
        waf_context.jobs = jobs
        waf_context.timer = Utils.Timer()
        if self.progress_bar:
            waf_context.progress_bar = 1
        waf_context.bento_context = self
        self.waf_context = waf_context

        def _default_extension_builder(extension, **kw):
            if not "features" in kw:
                kw["features"] = "c cshlib pyext bento"
            if not "source" in kw:
                kw["source"] = extension.sources[:]
            if not "name" in kw:
                kw["name"] = extension.name
            return self.waf_context(**kw)

        def _default_library_builder(library, **kw):
            if not "features" in kw:
                kw["features"] = "c cstlib pyext bento"
            if not "source" in kw:
                kw["source"] = library.sources[:]
            if not "name" in kw:
                kw["name"] = library.base_name
            if not "target" in kw:
                kw["target"] = library.name
            return self.waf_context(**kw)

        self.builder_registry.register_category("extensions", _default_extension_builder)
        self.builder_registry.register_category("compiled_libraries", _default_library_builder)

    def compile(self):
        super(BuildWafContext, self).compile()
        reg = self.builder_registry

        for category in ("extensions", "compiled_libraries"):
            for name, extension in self._node_pkg.iter_category(category):
                builder = reg.builder(category, name)
                self.pre_recurse(extension.ref_node)
                try:
                    extension = extension.extension_from(extension.ref_node)
                    task_gen = builder(extension)
                finally:
                    self.post_recurse()

        if self.progress_bar:
            sys.stderr.write(Logs.colors.cursor_off)
        try:
            self.waf_context.compile()
        except waflib.Errors.BuildError:
            e = extract_exception()
            if len(e.tasks) > 0:
                task0 = e.tasks[0]
                raise bento.errors.BuildError("Error while compiling %r"  % task0.inputs[0].abspath())
            else:
                raise
        finally:
            if self.progress_bar:
                c = len(self.waf_context.returned_tasks) or 1
                self.waf_context.to_log(self.waf_context.progress_line(c, c, Logs.colors.BLUE, Logs.colors.NORMAL))
                print('')
                sys.stdout.flush()
                sys.stderr.write(Logs.colors.cursor_on)

    def finish(self):
        super(BuildWafContext, self).finish()
        self.waf_context.store()

class WafBackend(AbstractBackend):
    def register_command_contexts(self, context):
        context.register_command_context("configure", ConfigureWafContext)
        context.register_command_context("build", BuildWafContext)

    def register_options_contexts(self, context):
        register_options(context)

########NEW FILE########
__FILENAME__ = arch
import re

from waflib.Tools.c_config import SNIP_EMPTY_PROGRAM
from waflib.Configure import conf

ARCHS = ["i386", "x86_64", "ppc", "ppc64"]

FILE_MACHO_RE = re.compile("Mach-O.*object ([a-zA-Z_0-9]+)")

@conf
def check_cc_arch(conf):
    env = conf.env
    archs = []

    for arch in ARCHS:
        env.stash()
        try:
            env.append_value('CFLAGS', ['-arch', arch])
            env.append_value('LINKFLAGS', ['-arch', arch])
            try:
                conf.check_cc(fragment=SNIP_EMPTY_PROGRAM, msg="Checking for %r suport" % arch)
                archs.append(arch)
            except conf.errors.ConfigurationError:
                pass
        finally:
            env.revert()

    env["ARCH_CC"] = archs

#def detect_arch(filename):

@conf
def check_cc_default_arch(conf):
    start_msg = "Checking for default CC arch"
    fragment = SNIP_EMPTY_PROGRAM
    output_var = "DEFAULT_CC_ARCH"

    return _check_default_arch(conf, start_msg, fragment, output_var)

@conf
def check_cxx_default_arch(conf):
    start_msg = "Checking for default CXX arch"
    fragment = SNIP_EMPTY_PROGRAM
    output_var = "DEFAULT_CXX_ARCH"

    return _check_default_arch(conf, start_msg, fragment, output_var)

@conf
def check_fc_default_arch(conf):
    start_msg = "Checking for default FC arch"
    fragment = """\
      program main
      end
"""
    output_var = "DEFAULT_FC_ARCH"
    compile_filename = 'test.f'
    features = "fc fcprogram"

    return _check_default_arch(conf, start_msg, fragment, output_var, compile_filename, features)

@conf
def _check_default_arch(conf, start_msg, fragment, output_var, compile_filename="test.c", features="c cprogram"):
    env = conf.env

    if not "FILE_BIN" in conf.env:
        file_bin = conf.find_program(["file"], var="FILE_BIN")
    else:
        file_bin = conf.env.FILE_BIN

    conf.start_msg(start_msg)
    ret = conf.check_cc(fragment=fragment, compile_filename=compile_filename, features=features)
    task_gen = conf.test_bld.groups[0][0]
    obj_filename = task_gen.tasks[0].outputs[0].abspath()
    out = conf.cmd_and_log([file_bin, obj_filename])
    m = FILE_MACHO_RE.search(out)
    if m is None:
        conf.fatal("Could not determine arch from output %r" % out)
    else:
        default_arch = m.group(1)
        conf.env[output_var] = default_arch
        conf.end_msg(default_arch)

########NEW FILE########
__FILENAME__ = blas_lapack
"""Experimental ! This will very likely change"""
import collections
import sys

from bento.commands.options \
    import \
        Option
from bento.backends.waf_backend \
    import \
        WAF_TOOLDIR

import waflib

from waflib import Options

_PLATFORM_DEFAULT_ORDER = collections.defaultdict(
    lambda: ["atlas", "mkl", "openblas", "generic"])
_PLATFORM_DEFAULT_ORDER.update({
        "win32": ["mkl", "generic"],
        "darwin": ["accelerate", "generic"],
})

_CBLAS_TO_KWARGS = {
        "mkl": {"lib": "mkl_intel_c,mkl_intel_thread,mkl_core,libiomp5md".split(",")},
        "atlas": {"lib": ["cblas", "atlas"]},
        "accelerate": {"framework": ["Accelerate"]},
        "openblas": {"lib": ["openblas"]},
        "generic": {"lib": ["cblas"]},
}

_LAPACK_TO_KWARGS = {
        "mkl": {"lib": "mkl_lapack95,mkl_blas95,mkl_intel_c,mkl_intel_thread,mkl_core,libiomp5md".split(",")},
        "atlas": {"lib": ["lapack", "f77blas", "cblas", "atlas"]},
        "accelerate": {"framework": ["Accelerate"]},
        "openblas": {"lib": ["openblas"]},
        "generic": {"lib": ["lapack", "blas"]},
}

_BLAS_TO_KWARGS = {
        "mkl": {"lib": "mkl_blas95,mkl_intel_c,mkl_intel_thread,mkl_core,libiomp5md".split(",")},
        "atlas": {"lib": ["f77blas", "cblas", "atlas"]},
        "accelerate": {"framework": ["Accelerate"]},
        "openblas": {"lib": ["openblas"]},
        "generic": {"lib": ["blas"]},
}

def get_blas_lapack_order(context):
    o, a = context.options_context.parser.parse_args(context.command_argv)
    if o.blas_lapack_type == "default" or o.blas_lapack_type is None:
        order = _PLATFORM_DEFAULT_ORDER[sys.platform]
    else:
        order = [o.blas_lapack_type]

    return order

def check_cblas(context, optimized):
    conf = context.waf_context

    msg = "Checking for %s (CBLAS)" % optimized.upper()

    kwargs = dict(_CBLAS_TO_KWARGS[optimized])
    kwargs.update({"msg": msg, "uselib_store": "CBLAS"})

    try:
        conf.check_cc(**kwargs)
        conf.env.HAS_CBLAS = True
    except waflib.Errors.ConfigurationError:
        conf.env.HAS_CBLAS = False

def check_blas(context, optimized):
    conf = context.waf_context

    msg = "Checking for %s (BLAS)" % optimized.upper()

    kwargs = _BLAS_TO_KWARGS[optimized]
    kwargs.update({"msg": msg, "uselib_store": "BLAS"})

    try:
        conf.check_cc(**kwargs)
        conf.env.HAS_BLAS = True
    except waflib.Errors.ConfigurationError:
        conf.env.HAS_BLAS = False

def check_lapack(context, optimized):
    conf = context.waf_context

    msg = "Checking for %s (LAPACK)" % optimized.upper()
    if optimized in ["openblas", "atlas"]:
        check_fortran(context)

    kwargs = dict(_LAPACK_TO_KWARGS[optimized])
    kwargs.update({"msg": msg, "uselib_store": "LAPACK"})

    try:
        conf.check_cc(**kwargs)
        conf.env.HAS_LAPACK = True
    except waflib.Errors.ConfigurationError:
        conf.env.HAS_LAPACK = False

def check_blas_lapack(context):
    conf = context.waf_context

    order = list(get_blas_lapack_order(context))

    o, a = context.options_context.parser.parse_args(context.command_argv)
    if o.blas_lapack_libdir:
        context.waf_context.env.append_value("LIBPATH", o.blas_lapack_libdir)

    for blas_lapack_name in order:
        check_cblas(context, blas_lapack_name)
        if conf.env.HAS_CBLAS:
            # prefer this source also for LAPACK
            order.remove(blas_lapack_name)
            order.insert(0, blas_lapack_name)
            break

    for blas_lapack_name in order:
        check_blas(context, blas_lapack_name)
        if conf.env.HAS_BLAS:
            break

    for blas_lapack_name in order:
        check_lapack(context, blas_lapack_name)
        if conf.env.HAS_LAPACK:
            break

    # You can manually set up blas/lapack as follows:
    #conf.env.HAS_CBLAS = True
    #conf.env.LIB_CBLAS = ["cblas", "atlas"]
    #conf.env.HAS_LAPACK = True
    #conf.env.LIB_LAPACK = ["lapack", "f77blas", "cblas", "atlas"]

def check_fortran(context):
    opts = context.waf_options_context
    conf = context.waf_context

    opts.load("compiler_fc")
    Options.options.check_fc = "gfortran"

    conf.load("compiler_fc")
    conf.load("ordered_c", tooldir=[WAF_TOOLDIR])

    conf.check_fortran_verbose_flag()
    conf.check_fortran_clib()

def add_options(global_context):
    global_context.add_option_group("configure", "blas_lapack", "blas/lapack")

    available_optimized = ",".join(_LAPACK_TO_KWARGS.keys())
    global_context.add_option("configure",
            Option("--blas-lapack-type", help="Which blas lapack to use (%s)" % available_optimized),
            "blas_lapack")

    global_context.add_option("configure",
            Option("--with-blas-lapack-libdir", dest="blas_lapack_libdir",
                   help="Where to look for BLAS/LAPACK dir"),
            "blas_lapack")

########NEW FILE########
__FILENAME__ = custom_python
import os
import sys

from waflib import Utils, Options, Errors
from waflib.Configure import conf
from waflib.TaskGen import before_method, after_method, feature
from waflib.Logs import debug, warn

DISTUTILS_IMP = ['from distutils.sysconfig import get_config_var, get_python_lib']

@feature('pyext')
@before_method('apply_link', 'apply_bundle')
def set_bundle(self):
	if Utils.unversioned_sys_platform() == 'darwin':
		self.mac_bundle = True

@conf
def check_python_version(conf, minver=None):
	"""
	Check if the python interpreter is found matching a given minimum version.
	minver should be a tuple, eg. to check for python >= 2.4.2 pass (2,4,2) as minver.

	If successful, PYTHON_VERSION is defined as 'MAJOR.MINOR'
	(eg. '2.4') of the actual python version found, and PYTHONDIR is
	defined, pointing to the site-packages directory appropriate for
	this python version, where modules/packages/extensions should be
	installed.

	:param minver: minimum version
	:type minver: tuple of int
	"""
	assert minver is None or isinstance(minver, tuple)
	pybin = conf.env['PYTHON']
	if not pybin:
		conf.fatal('could not find the python executable')

	# Get python version string
	cmd = pybin + ['-c', 'import sys\nfor x in sys.version_info: print(str(x))']
	debug('python: Running python command %r' % cmd)
	lines = conf.cmd_and_log(cmd).split()
	assert len(lines) == 5, "found %i lines, expected 5: %r" % (len(lines), lines)
	pyver_tuple = (int(lines[0]), int(lines[1]), int(lines[2]), lines[3], int(lines[4]))

	# compare python version with the minimum required
	result = (minver is None) or (pyver_tuple >= minver)

	if result:
		# define useful environment variables
		pyver = '.'.join([str(x) for x in pyver_tuple[:2]])
		conf.env['PYTHON_VERSION'] = pyver

	# Feedback
	pyver_full = '.'.join(map(str, pyver_tuple[:3]))
	if minver is None:
		conf.msg('Checking for python version', pyver_full)
	else:
		minver_str = '.'.join(map(str, minver))
		conf.msg('Checking for python version', pyver_tuple, ">= %s" % (minver_str,) and 'GREEN' or 'YELLOW')

	if not result:
		conf.fatal('The python version is too old, expecting %r' % (minver,))

@conf
def get_python_variables(self, variables, imports=None):
	"""
	Spawn a new python process to dump configuration variables

	:param variables: variables to print
	:type variables: list of string
	:param imports: one import by element
	:type imports: list of string
	:return: the variable values
	:rtype: list of string
	"""
	if not imports:
		try:
			imports = self.python_imports
		except AttributeError:
			imports = DISTUTILS_IMP

	program = list(imports) # copy
	program.append('')
	for v in variables:
		program.append("print(repr(%s))" % v)
	os_env = dict(os.environ)
	try:
		del os_env['MACOSX_DEPLOYMENT_TARGET'] # see comments in the OSX tool
	except KeyError:
		pass

	try:
		out = self.cmd_and_log(self.env.PYTHON + ['-c', '\n'.join(program)], env=os_env)
	except Errors.WafError:
		self.fatal('The distutils module is unusable: install "python-devel"?')
	return_values = []
	for s in out.split('\n'):
		s = s.strip()
		if not s:
			continue
		if s == 'None':
			return_values.append(None)
		elif s[0] == "'" and s[-1] == "'":
			return_values.append(s[1:-1])
		elif s[0].isdigit():
			return_values.append(int(s))
		else: break
	return return_values

@conf
def check_python_headers(conf):
	"""
	Check for headers and libraries necessary to extend or embed python by using the module *distutils*.
	On success the environment variables xxx_PYEXT and xxx_PYEMBED are added:

	* PYEXT: for compiling python extensions
	* PYEMBED: for embedding a python interpreter
	"""

	# FIXME rewrite

	if not conf.env['CC_NAME'] and not conf.env['CXX_NAME']:
		conf.fatal('load a compiler first (gcc, g++, ..)')

	if not conf.env['PYTHON_VERSION']:
		conf.check_python_version()

	env = conf.env
	pybin = env.PYTHON
	if not pybin:
		conf.fatal('could not find the python executable')

	v = 'INCLUDEPY SO LDFLAGS MACOSX_DEPLOYMENT_TARGET LDSHARED CFLAGS'.split()
	try:
		lst = conf.get_python_variables(["get_config_var('%s') or ''" % x for x in v])
	except RuntimeError:
		conf.fatal("Python development headers not found (-v for details).")

	vals = ['%s = %r' % (x, y) for (x, y) in zip(v, lst)]
	conf.to_log("Configuration returned from %r:\n%r\n" % (pybin, '\n'.join(vals)))

	dct = dict(zip(v, lst))
	x = 'MACOSX_DEPLOYMENT_TARGET'
	if dct[x]:
		conf.env[x] = conf.environ[x] = dct[x]

	env['pyext_PATTERN'] = '%s' + dct['SO'] # not a mistake

	if Options.options.use_distutils_flags:
		all_flags = dct['LDFLAGS'] + ' ' + dct['LDSHARED'] + ' ' + dct['CFLAGS']
		conf.parse_flags(all_flags, 'PYEXT')

	env['INCLUDES_PYEXT'] = [dct['INCLUDEPY']]

@feature('pyext')
@before_method('propagate_uselib_vars', 'apply_link')
@after_method('apply_bundle')
def init_pyext(self):
	"""
	Change the values of *cshlib_PATTERN* and *cxxshlib_PATTERN* to remove the
	*lib* prefix from library names.
	"""
	self.uselib = self.to_list(getattr(self, 'uselib', []))
	if not 'PYEXT' in self.uselib:
		self.uselib.append('PYEXT')
	# override shlib_PATTERN set by the osx module
	self.env['cshlib_PATTERN'] = self.env['cxxshlib_PATTERN'] = self.env['macbundle_PATTERN'] = self.env['pyext_PATTERN']

	try:
		if not self.install_path:
			return
	except AttributeError:
		self.install_path = '${PYTHONARCHDIR}'

def configure(conf):
	"""
	Detect the python interpreter
	"""
	default = [sys.executable]
	try:
		conf.find_program('python', var='PYTHON')
	except conf.errors.ConfigurationError:
		warn("could not find a python executable, setting to sys.executable '%s'" % sys.executable)
		conf.env.PYTHON = default

	if conf.env.PYTHON != default:
		warn("python executable '%s' different from sys.executable '%s'" % (conf.env.PYTHON, default))
	conf.env.PYTHON = conf.cmd_to_list(default)

	v = conf.env
	v['PYCMD'] = '"import sys, py_compile;py_compile.compile(sys.argv[1], sys.argv[2])"'
	v['PYFLAGS'] = ''
	v['PYFLAGS_OPT'] = '-O'

def options(opt):
	opt.add_option('--use-distutils-flags',
			action='store_true',
			default=False,
			help='Use flags as defined by distutils [Default:no]',
			dest='pyc')

########NEW FILE########
__FILENAME__ = cython
#! /usr/bin/env python
# encoding: utf-8
# Thomas Nagy, 2010

import re

import waflib.Logs as _msg
from waflib import Task
from waflib.TaskGen import extension, feature, after_method

cy_api_pat = re.compile(r'\s*?cdef\s*?(public|api)\w*')
re_cyt = re.compile('import\\s(\\w+)\\s*$', re.M)

@extension('.pyx')
def add_cython_file(self, node):
	"""
	Process a *.pyx* file given in the list of source files. No additional
	feature is required::

		def build(bld):
			bld(features='c cshlib pyext', source='main.c foo.pyx', target='app')
	"""
	ext = '.c'
	if 'cxx' in self.features:
		self.env.append_unique('CYTHONFLAGS', '--cplus')
		ext = '.cc'
	tsk = self.create_task('cython', node, node.change_ext(ext))
	self.source += tsk.outputs

@feature('c', 'cxx')
@after_method('propagate_uselib_vars', 'process_source')
def apply_cython_incpaths(self):
	lst = self.to_incnodes(self.env['CYTHON_INCLUDES'])
	self.cyhon_includes_nodes = lst
	self.env['CYTHON_INCPATHS'] = ["-I%s" % x.abspath() for x in lst]

class cython(Task.Task):
	run_str = '${CYTHON} ${CYTHON_INCPATHS} ${CYTHONFLAGS} -o ${TGT[0].abspath()} ${SRC}'
	color   = 'GREEN'

	vars    = ['INCLUDES']
	"""
	Rebuild whenever the INCLUDES change. The variables such as CYTHONFLAGS will be appended
	by the metaclass.
	"""

	ext_out = ['.h']
	"""
	The creation of a .h file is known only after the build has begun, so it is not
	possible to compute a build order just by looking at the task inputs/outputs.
	"""

	def runnable_status(self):
		"""
		Perform a double-check to add the headers created by cython
		to the output nodes. The scanner is executed only when the cython task
		must be executed (optimization).
		"""
		ret = super(cython, self).runnable_status()
		if ret == Task.ASK_LATER:
			return ret
		for x in self.generator.bld.raw_deps[self.uid()]:
			if x.startswith('header:'):
				self.outputs.append(self.inputs[0].parent.find_or_declare(x.replace('header:', '')))
		return super(cython, self).runnable_status()

	def scan(self):
		"""
		Return the dependent files (.pxd) by looking in the include folders.
		Put the headers to generate in the custom list "bld.raw_deps".
		To inspect the scanne results use::

			$ waf clean build --zones=deps
		"""
		txt = self.inputs[0].read()

		mods = []
		for m in re_cyt.finditer(txt):
			mods.append(m.group(1))

		_msg.debug("cython: mods %r" % mods)
		incs = getattr(self.generator, 'cython_includes', [])
		incs = [self.generator.path.find_dir(x) for x in incs]
		incs.append(self.inputs[0].parent)

		found = []
		missing = []
		for x in mods:
			for y in incs:
				k = y.find_resource(x + '.pxd')
				if k:
					found.append(k)
					break
			else:
				missing.append(x)
		_msg.debug("cython: found %r" % found)

		# Now the .h created - store them in bld.raw_deps for later use
		has_api = False
		has_public = False
		for l in txt.splitlines():
			if cy_api_pat.match(l):
				if ' api ' in l:
					has_api = True
				if ' public ' in l:
					has_public = True
		name = self.inputs[0].name.replace('.pyx', '')
		if has_api:
			missing.append('header:%s_api.h' % name)
		if has_public:
			missing.append('header:%s.h' % name)

		return (found, missing)

def options(ctx):
	ctx.add_option('--cython-flags', action='store', default='', help='space separated list of flags to pass to cython')

def configure(ctx):
	if not ctx.env.CC and not ctx.env.CXX:
		ctx.fatal('Load a C/C++ compiler first')
	if not ctx.env.PYTHON:
		ctx.fatal('Load the python tool first!')
	ctx.find_program('cython', var='CYTHON')
	if getattr(ctx.options, "cython_flags", None):
		ctx.env.CYTHONFLAGS = ctx.options.cython_flags
	ctx.env.CYTHON_INCLUDES = []

########NEW FILE########
__FILENAME__ = f2py
import os.path as op
import re

import numpy.f2py
import numpy.distutils.misc_util

from waflib \
    import \
        Task
from waflib.TaskGen \
    import \
        extension, feature, before_method, after_method

from interface_gen \
    import \
        generate_interface


CGEN_TEMPLATE   = '%smodule'
FOBJECT_FILE    = 'fortranobject.c'
FHEADER_FILE    = 'fortranobject.h'
FWRAP_TEMPLATE  = '%s-f2pywrappers.f'

# This path is relative to build directory
F2PY_TEMP_DIR = '.f2py'

# Those regex are copied from build_src in numpy.distutils.command
F2PY_MODNAME_MATCH = re.compile(r'\s*python\s*module\s*(?P<name>[\w_]+)',
                                re.I).match
F2PY_UMODNAME_MATCH = re.compile(r'\s*python\s*module\s*(?P<name>[\w_]*?'\
                                     '__user__[\w_]*)',re.I).match
# End of copy
#F2PY_INCLUDE_MATCH = re.compile(r"^\s+\<include_file=(\S+)\>")
# Copied from crackfortran.py in f2py
F2PY_INCLUDE_MATCH = re.compile(r'\s*include\s*(\'|")(?P<name>[^\'"]*)(\'|")',re.I)

def is_pyf(node):
    return node.name.endswith(".pyf")

def include_pyf(node):
    includes = []
    for line in node.read().splitlines():
        m = F2PY_INCLUDE_MATCH.match(line)
        if m:
            includes.append(m.groupdict()["name"])
    return includes

def f2py_modulename(node):
    """This returns the name of the module from the pyf source file.

    source is expected to be one string, containing the whole source file
    code."""
    name = None
    for line in node.read().splitlines():
        m = F2PY_MODNAME_MATCH(line)
        if m:
            if F2PY_UMODNAME_MATCH(line): # skip *__user__* names
                continue
            name = m.group('name')
            break
    return name

def generate_fake_interface(name, node):
    """Generate a (fake) .pyf file from another pyf file (!)."""
    content = """\
python module %(name)s
    usercode void empty_module(void) {}
    interface
    subroutine empty_module()
    intent(c) empty_module
    end subroutine empty_module
    end interface
end python module%(name)s
"""
    node.write(content % {"name": name})

@feature('f2py')
@before_method('apply_incpaths')
def apply_f2py_includes(task_gen):
    includes = task_gen.env["INCLUDES"]
    d = op.dirname(numpy.f2py.__file__)
    includes.append(op.join(d, 'src'))
    includes.extend(numpy.distutils.misc_util.get_numpy_include_dirs())
    task_gen.env["INCLUDES"] = includes

@feature('f2py_fortran')
@before_method('process_source')
def add_fortran_tasks(task_gen):
    assert "f2py" in task_gen.features
    
    if hasattr(task_gen, "name"):
        module_name = task_gen.name
    else:
        raise ValueError("Please define a name for task_gen %r" % task_gen)
    if len(task_gen.source) < 1:
        raise ValueError("Gne ?")
    sources = task_gen.to_nodes(task_gen.source)
    module_node = sources[0].parent.find_or_declare("%smodule.c" % module_name)

    f2py_task = task_gen.create_task('f2py_fortran', sources, module_node)
    add_f2py_tasks(task_gen, f2py_task, module_name, module_node)
    f2py_task.env.F2PYFLAGS.extend(["--lower", "-m", module_name])

def fake_interface_gen_callback(task_gen, node):
    return _interface_gen_callback(task_gen, node, "f2py_fake_interface")

def interface_gen_callback(task_gen, node):
    return _interface_gen_callback(task_gen, node, "f2py_interface")

def _interface_gen_callback(task_gen, node, interface_task_name):
    if not hasattr(task_gen, "name"):
        module_name = f2py_modulename(node)
    else:
        module_name = task_gen.name

    intermediate_output = node.parent.find_or_declare("%s.pyf" % module_name)
    module_node = node.parent.find_or_declare("%smodule.c" % module_name)
    # Guard against overwriting existing source code by accident. Did I say I
    # hate find_or_declare ?
    assert intermediate_output.is_bld()
    assert module_node.is_bld()

    interface_task = task_gen.create_task(interface_task_name, node, intermediate_output)

    f2py_task = task_gen.create_task('f2py', intermediate_output, module_node)
    add_f2py_tasks(task_gen, f2py_task, module_name, module_node)

def add_f2py_tasks(task_gen, f2py_task, module_name, module_node):
    build_dir = module_node.parent.bldpath()
    # FIXME: ask waf ML how flags sharing and co is supposed to work
    f2py_task.env.F2PYFLAGS = task_gen.env.F2PYFLAGS[:]
    f2py_task.env.F2PYFLAGS.extend(["--build-dir", build_dir])
    task_gen.source += f2py_task.outputs

    fobject_node = module_node.parent.find_or_declare("%s-fortranobject.c" % module_name)
    fobject_task = task_gen.create_task("f2py_fortran_object", [], fobject_node)
    task_gen.source += fobject_task.outputs

    fwrapper_node = module_node.parent.find_or_declare(FWRAP_TEMPLATE % module_name)
    fwrapper_task = task_gen.create_task("f2py_fwrapper", [], fwrapper_node)
    task_gen.source += fwrapper_task.outputs

@extension('.pyf')
def add_f2py_files(task_gen, node):
    ext = '.c'

    if not is_pyf(node):
        raise ValueError("Gne ?")

    if not hasattr(task_gen, "name"):
        module_name = f2py_modulename(node)
    else:
        module_name = task_gen.name
    module_node = node.parent.find_or_declare("%smodule.c" % module_name)

    f2py_task = task_gen.create_task('f2py', node, module_node)
    add_f2py_tasks(task_gen, f2py_task, module_name, module_node)

@extension('.ipyf')
def add_f2py_files(task_gen, node):
    ext = '.pyf'

    if not hasattr(task_gen, "name"):
        module_name = f2py_modulename(node)
    else:
        module_name = task_gen.name
    intermediate_output = node.parent.find_or_declare("%s.pyf" % module_name)
    assert intermediate_output.is_bld()

    if "f2py_interface" in task_gen.features:
        interface_task = task_gen.create_task("f2py_interface", node, intermediate_output)
    elif "f2py_fake_interface" in task_gen.features:
        interface_task = task_gen.create_task("f2py_fake_interface", node, intermediate_output)
    else:
        raise ValueError("You need to use f2py_interface or f2py_fake_interface for .ipyf !")
    task_gen.source += interface_task.outputs
    #module_node = node.parent.find_or_declare("%smodule.c" % module_name)

    #f2py_task = task_gen.create_task('f2py', node, module_node)
    #add_f2py_tasks(task_gen, f2py_task, module_name, module_node)

class _f2py_interface(Task.Task):
    pass

class f2py_fake_interface(_f2py_interface):
    def run(self):
        node = self.inputs[0]
        output = self.outputs[0]
        module_name = f2py_modulename(node)
        generate_fake_interface(module_name, output)
        return 0

class f2py_interface(_f2py_interface):
    def run(self):
        node = self.inputs[0]
        output = self.outputs[0]
        module_name = f2py_modulename(node)
        generate_interface(module_name, node.abspath(), output.abspath())
        return 0

    def scan(self):
        found = []
        missing = []

        node = self.inputs[0]
        if not hasattr(self, "name"):
            module_name = f2py_modulename(node)
        else:
            module_name = self.name

        includes = include_pyf(node)
        if includes:
            real_pyf = node.parent.find_or_declare("%s.pyf" % module_name)
            # Guard against overwriting existing source code by accident. Did I
            # say I hate find_or_declare ?
            assert real_pyf.is_bld()

            for inc in includes:
                x = node.parent.find_resource(inc)
                if x:
                    found.append(x)
                else:
                    missing.append(x)
        user_routines = node.parent.find_resource("%s_user_routines.pyf" % module_name)
        if user_routines:
            found.append(user_routines)
        return (found, missing)

class f2py_fortran_object(Task.Task):
    def run(self):
        node = self.generator.bld.bldnode.find_node(op.join(F2PY_TEMP_DIR, FOBJECT_FILE))
        output = self.outputs[0]
        output.write(node.read())
        return 0

class f2py_fwrapper(Task.Task):
    after = ["f2py"]
    def run(self):
        output = self.outputs[0]
        if not op.exists(output.abspath()):
            output.write("")
        return 0

# TODO: f2py from .pyf or from .f should be different tasks.
class _f2py_task(Task.Task):
    run_str = '${F2PY} ${F2PYFLAGS} ${SRC}'
    color   = 'CYAN'

class f2py(_f2py_task):
    ext_in = [".pyf"]
    ext_out = [".h", ".f", ".c"]

class f2py_fortran(_f2py_task):
    ext_in = [".pyf"]
    ext_out = [".h", ".f", ".c"]

def configure(conf):
    if not conf.env.CC and not conf.env.CXX:
        conf.fatal('Load a C/C++ compiler first')
    if not conf.env.PYTHON:
        conf.fatal('Load the python tool first!')
    conf.find_program('f2py', var='F2PY')
    # FIXME: this has nothing to do here
    conf.env.F2PYFLAGS = ["--quiet"]

    f2py_tempdir = conf.bldnode.make_node(F2PY_TEMP_DIR)
    f2py_tempdir.mkdir()

    fobject = f2py_tempdir.make_node(FOBJECT_FILE)

    d = op.dirname(numpy.f2py.__file__)
    source_c = op.join(d, 'src', FOBJECT_FILE)
    fobject.write(open(source_c).read())

########NEW FILE########
__FILENAME__ = interface_gen
#!/usr/bin/env python

import os
import re
from distutils.dir_util import mkpath

def all_subroutines(interface_in):
    # remove comments
    comment_block_exp = re.compile(r'/\*(?:\s|.)*?\*/')
    subroutine_exp = re.compile(r'subroutine (?:\s|.)*?end subroutine.*')
    function_exp = re.compile(r'function (?:\s|.)*?end function.*')

    interface = comment_block_exp.sub('',interface_in)
    subroutine_list = subroutine_exp.findall(interface)
    function_list = function_exp.findall(interface)
    subroutine_list = subroutine_list + function_list
    subroutine_list = map(lambda x: x.strip(),subroutine_list)
    return subroutine_list

def real_convert(val_string):
    return val_string

def complex_convert(val_string):
    return '(' + val_string + ',0.)'

def convert_types(interface_in,converter):
    regexp = re.compile(r'<type_convert=(.*?)>')
    interface = interface_in[:]
    while 1:
        sub = regexp.search(interface)
        if sub is None: break
        converted = converter(sub.group(1))
        interface = interface.replace(sub.group(),converted)
    return interface

def generic_expand(generic_interface,skip_names=[]):
    generic_types ={'s' :('real',            'real', real_convert,
                          'real'),
                    'd' :('double precision','double precision',real_convert,
                          'double precision'),
                    'c' :('complex',         'complex',complex_convert,
                          'real'),
                    'z' :('double complex',  'double complex',complex_convert,
                          'double precision'),
                    'cs':('complex',         'real',complex_convert,
                          'real'),
                    'zd':('double complex',  'double precision',complex_convert,
                          'double precision'),
                    'sc':('real',            'complex',real_convert,
                          'real'),
                    'dz':('double precision','double complex', real_convert,
                          'double precision')}
    generic_c_types = {'real':'float',
                       'double precision':'double',
                       'complex':'complex_float',
                       'double complex':'complex_double'}
    # cc_types is specific in ATLAS C BLAS, in particular, for complex arguments
    generic_cc_types = {'real':'float',
                       'double precision':'double',
                       'complex':'void',
                       'double complex':'void'}
    #2. get all subroutines
    subs = all_subroutines(generic_interface)
    #print len(subs)
    #loop through the subs
    type_exp = re.compile(r'<tchar=(.*?)>')
    TYPE_EXP = re.compile(r'<TCHAR=(.*?)>')
    routine_name = re.compile(r'(subroutine|function)\s*(?P<name>\w+)\s*\(')
    interface = ''
    for sub in subs:
        #3. Find the typecodes to use:
        m = type_exp.search(sub)
        if m is None:
            interface = interface + '\n\n' + sub
            continue
        type_chars = m.group(1)
        # get rid of spaces
        type_chars = type_chars.replace(' ','')
        # get a list of the characters (or character pairs)
        type_chars = type_chars.split(',')
        # Now get rid of the special tag that contained the types
        sub = re.sub(type_exp,'<tchar>',sub)
        m = TYPE_EXP.search(sub)
        if m is not None:
            sub = re.sub(TYPE_EXP,'<TCHAR>',sub)
        sub_generic = sub.strip()
        for char in type_chars:
            type_in,type_out,converter, rtype_in = generic_types[char]
            sub = convert_types(sub_generic,converter)
            function_def = sub.replace('<tchar>',char)
            function_def = function_def.replace('<TCHAR>',char.upper())
            function_def = function_def.replace('<type_in>',type_in)
            function_def = function_def.replace('<type_in_c>',
                                          generic_c_types[type_in])
            function_def = function_def.replace('<type_in_cc>',
                                          generic_cc_types[type_in])
            function_def = function_def.replace('<rtype_in>',rtype_in)
            function_def = function_def.replace('<rtype_in_c>',
                                          generic_c_types[rtype_in])
            function_def = function_def.replace('<type_out>',type_out)
            function_def = function_def.replace('<type_out_c>',
                                          generic_c_types[type_out])
            m = routine_name.match(function_def)
            if m:
                if m.group('name') in skip_names:
                    print 'Skipping',m.group('name')
                    continue
            else:
                print 'Possible bug: Failed to determine routines name'
            interface = interface + '\n\n' + function_def

    return interface

#def interface_to_module(interface_in,module_name,include_list,sdir='.'):
def interface_to_module(interface_in,module_name):
    pre_prefix = "!%f90 -*- f90 -*-\n"
    # heading and tail of the module definition.
    file_prefix = "\npython module " + module_name +" ! in\n" \
                  "!usercode '''#include \"cblas.h\"\n"\
                  "!'''\n"\
                  "    interface  \n"
    file_suffix = "\n    end interface\n" \
             "end module %s" % module_name
    return  pre_prefix + file_prefix + interface_in + file_suffix

def process_includes(interface_in,sdir='.'):
    include_exp = re.compile(r'\n\s*[^!]\s*<include_file=(.*?)>')
    include_files = include_exp.findall(interface_in)
    for filename in include_files:
        f = open(os.path.join(sdir,filename))
        interface_in = interface_in.replace('<include_file=%s>'%filename,
                                      f.read())
        f.close()
    return interface_in

def generate_interface(module_name,src_file,target_file,skip_names=[]):
    #print "generating",module_name,"interface"
    f = open(src_file)
    generic_interface = f.read()
    f.close()
    sdir = os.path.dirname(src_file)
    generic_interface = process_includes(generic_interface,sdir)
    generic_interface = generic_expand(generic_interface,skip_names)
    module_def = interface_to_module(generic_interface,module_name)
    mkpath(os.path.dirname(target_file))
    f = open(target_file,'w')
    user_routines = os.path.join(sdir,module_name+"_user_routines.pyf")
    if os.path.exists(user_routines):
        f2 = open(user_routines)
        f.write(f2.read())
        f2.close()
    f.write(module_def)
    f.close()

def process_all():
    # process the standard files.
    for name in ['fblas','cblas','clapack','flapack']:
        generate_interface(name,'generic_%s.pyf'%(name),name+'.pyf')


if __name__ == "__main__":
    process_all()

########NEW FILE########
__FILENAME__ = ordered_c
from waflib import Task
from waflib.TaskGen import before_method, feature, taskgen_method
from waflib.Tools import ccroot

import waflib.Tools.c

def colon(env, var1, var2):
	tmp = env[var1]
	if isinstance(var2, str):
		it = env[var2]
	else:
		it = var2
	if isinstance(tmp, str):
		return [tmp % x for x in it]
	else:
		lst = []
		for y in it:
			lst.extend(tmp)
			lst.append(y)
		return lst

@taskgen_method
def interpolate_flags(task_gen, line):
	env = task_gen.env
	reg_act = Task.reg_act

	extr = []
	def repl(match):
		g = match.group
		if g('dollar'): return "$"
		elif g('backslash'): return '\\\\'
		elif g('subst'): extr.append((g('var'), g('code'))); return "%s"
		return None

	line = reg_act.sub(repl, line) or line

	ret = []
	for (var, meth) in extr:
		if meth:
			if meth.startswith(':'):
				m = meth[1:]
				ret.extend(colon(env, var, m))
			else:
				raise NotImplementedError()
				ret.append("%s%s" % (var, meth))
		else:
			ret.extend(env[var])
	return ret

class c(waflib.Tools.c.c):
	run_str = '${CC} ${ARCH_ST:ARCH} ${CFLAGS} ${CPPFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${CC_SRC_F}${SRC} ${CC_TGT_F}${TGT} ${ORDERED_CFLAGS}'

class cprogram(waflib.Tools.c.cprogram):
	run_str = '${LINK_CC} ${LINKFLAGS} ${CCLNK_SRC_F}${SRC} ${CCLNK_TGT_F}${TGT[0].abspath()} ${RPATH_ST:RPATH} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${FRAMEWORK_ST:FRAMEWORK} ${ARCH_ST:ARCH} ${STLIB_MARKER} ${STLIBPATH_ST:STLIBPATH} ${STLIB_ST:STLIB} ${SHLIB_MARKER} ${LIBPATH_ST:LIBPATH} ${LIB_ST:LIB} ${ORDERED_LINKFLAGS}'

class cshlib(cprogram, waflib.Tools.c.cprogram):
    pass

@feature("cprogram", "cshlib", "cstlib", "python")
@before_method("propagate_uselib_vars")
def apply_ordered_flags(self):
	link_flag_name_to_template = {
		"FRAMEWORK": "${FRAMEWORK_ST:%s}",
		"LIB": "${LIB_ST:%s}",
		"LIBPATH": "${LIBPATH_ST:%s}",
		"LINKFLAGS": "${%s}",
		"STLIB": "${STLIB_ST:%s}"
    }

	c_flag_name_to_template = {
		"ARCH": "${ARCH_ST:%s}",
		"CFLAGS": "${%s}",
		"CPPFLAGS": "${%s}",
		"FRAMEWORKPATH": "${FRAMEWORKPATH_ST:%s}",
		"CPPPATH": "${CPPPATH_ST:%s}",
		"DEFINES": "${DEFINES_ST:%s}",
		"INCLUDES": "${CPPPATH_ST:%s}",
	}

	# FIXME: use correct uselib_vars
	uselib_vars = set()
	if "cprogram" in self.features:
		uselib_vars |= ccroot.USELIB_VARS["cprogram"]
	if "cshlib" in self.features:
		uselib_vars |= ccroot.USELIB_VARS["cshlib"]
	#if "c" in self.features:
	#	uselib_vars |= ccroot.USELIB_VARS["c"]
	#if "pyext" in self.features:
	#	uselib_vars |= ccroot.USELIB_VARS["pyext"]
	env = self.env

	uselibs = self.to_list(getattr(self, 'uselib', []))
	ordered_link = []
	ordered_c = []

	to_remove = []

	for uselib in uselibs:
		for uselib_var in uselib_vars:
			key = uselib_var + '_' + uselib
			value = env[key]
			if value:
				to_remove.append(key)
				if uselib_var in link_flag_name_to_template:
					flag_template = link_flag_name_to_template[uselib_var]
					ordered_link.append(flag_template % key)
				elif uselib_var in c_flag_name_to_template:
					flag_template = c_flag_name_to_template[uselib_var]
					ordered_c.append(flag_template % key)
				else:
					raise NotImplementedError(uselib_var)
	ordered_linkflags = self.interpolate_flags(" ".join(ordered_link))
	env["ORDERED_LINKFLAGS"] = ordered_linkflags
	#ordered_cflags = self.interpolate_flags(" ".join(ordered_c))
	#env["ORDERED_CFLAGS"] = ordered_cflags

	for key in to_remove:
		del env[key]

########NEW FILE########
__FILENAME__ = yaku_backend
import os.path as op

from bento.utils.utils \
    import \
        extract_exception
from bento.core.node_package \
    import \
        translate_name
from bento.core.subpackage \
    import \
        get_extensions, get_compiled_libraries
from bento.commands.command_contexts \
    import \
        ConfigureContext, BuildContext
from bento.errors \
    import \
        ConfigurationError
from bento.backends.core \
    import \
        AbstractBackend

import yaku.context
import yaku.errors

class ConfigureYakuContext(ConfigureContext):
    def __init__(self, global_context, cmd_argv, options_context, pkg, run_node):
        super(ConfigureYakuContext, self).__init__(global_context, cmd_argv, options_context, pkg, run_node)
        build_path = run_node._ctx.bldnode.path_from(run_node)
        source_path = run_node._ctx.srcnode.path_from(run_node)
        self.yaku_context = yaku.context.get_cfg(src_path=source_path, build_path=build_path)

    def configure(self):
        extensions = get_extensions(self.pkg, self.run_node)
        libraries = get_compiled_libraries(self.pkg, self.run_node)

        yaku_ctx = self.yaku_context
        if extensions or libraries:
            for t in ["ctasks", "pyext"]:
                try:
                    yaku_ctx.use_tools([t])
                except yaku.errors.ConfigurationFailure:
                    e = extract_exception()
                    raise ConfigurationError(str(e))

    def finish(self):
        super(ConfigureYakuContext, self).finish()
        self.yaku_context.store()

    def pre_recurse(self, local_node):
        super(ConfigureYakuContext, self).pre_recurse(local_node)
        self._old_path = self.yaku_context.path
        self.yaku_context.path = self.yaku_context.path.make_node(self.local_node.path_from(self.top_node))

    def post_recurse(self):
        self.yaku_context.path = self._old_path
        super(ConfigureYakuContext, self).post_recurse()

class BuildYakuContext(BuildContext):
    def __init__(self, global_context, cmd_argv, options_context, pkg, run_node):
        from bento.commands.build_yaku import build_extension, build_compiled_library

        super(BuildYakuContext, self).__init__(global_context, cmd_argv, options_context, pkg, run_node)
        build_path = run_node._ctx.bldnode.path_from(run_node)
        source_path = run_node._ctx.srcnode.path_from(run_node)
        self.yaku_context = yaku.context.get_bld(src_path=source_path, build_path=build_path)

        o, a = options_context.parser.parse_args(cmd_argv)
        if o.jobs:
            jobs = int(o.jobs)
        else:
            jobs = 1
        self.verbose = o.verbose
        self.jobs = jobs

        def _builder_factory(category, builder):
            def _build(extension, include_dirs=None, **kw):
                env = kw.get("env", {})
                if include_dirs:
                    env["include_dirs"] = include_dirs

                outputs = builder(self.yaku_context, extension, env=env)
                nodes = [self.build_node.make_node(o) for o in outputs]

                p = self.build_node.find_node(nodes[0].parent.bldpath())
                relative_name = extension.name.rsplit(".")[-1]
                full_name = translate_name(relative_name, p, self.build_node)

                from_node = p
                pkg_dir = op.dirname(full_name.replace('.', op.sep))
                target_dir = op.join('$sitedir', pkg_dir)
                self.outputs_registry.register_outputs(category, full_name, nodes,
                                                       from_node, target_dir)
            return _build

        self.builder_registry.register_category("extensions",
            _builder_factory("extensions", build_extension))
        self.builder_registry.register_category("compiled_libraries",
            _builder_factory("compiled_libraries", build_compiled_library))

    def finish(self):
        super(BuildYakuContext, self).finish()
        self.yaku_context.store()

    def compile(self):
        super(BuildYakuContext, self).compile()

        import yaku.task_manager
        bld = self.yaku_context
        bld.env["VERBOSE"] = self.verbose

        reg = self.builder_registry

        for category in ["extensions", "compiled_libraries"]:
            for name, item in self._node_pkg.iter_category(category):
                builder = reg.builder(category, name)
                self.pre_recurse(item.ref_node)
                try:
                    item = item.extension_from(item.ref_node)
                    builder(item)
                finally:
                    self.post_recurse()

        task_manager = yaku.task_manager.TaskManager(bld.tasks)
        if self.jobs < 2:
            runner = yaku.scheduler.SerialRunner(bld, task_manager)
        else:
            runner = yaku.scheduler.ParallelRunner(bld, task_manager, self.jobs)
        runner.start()
        runner.run()

        # TODO: inplace support

    def pre_recurse(self, local_node):
        super(BuildYakuContext, self).pre_recurse(local_node)
        self._old_path = self.yaku_context.path
        # FIXME: we should not modify yaku context, but add current node +
        # recurse support to yaku instead
        # Gymnastic to make a *yaku* node from a *bento* node
        self.yaku_context.path = self.yaku_context.path.make_node(self.local_node.path_from(self.top_node))

    def post_recurse(self):
        self.yaku_context.path = self._old_path
        super(BuildYakuContext, self).post_recurse()

class YakuBackend(AbstractBackend):
    def register_command_contexts(self, context):
        context.register_command_context("configure", ConfigureYakuContext)
        context.register_command_context("build", BuildYakuContext)

########NEW FILE########
__FILENAME__ = build
import os

import os.path as op

from bento.utils.utils \
    import \
        subst_vars
from bento.installed_package_description \
    import \
        BuildManifest, build_manifest_meta_from_pkg
from bento._config \
    import \
        BUILD_MANIFEST_PATH

from bento.commands.core \
    import \
        Option
from bento.commands.core \
    import \
        Command
from bento.utils \
    import \
        cpu_count

class SectionWriter(object):
    def __init__(self):
        self.sections = {}

    def store(self, filename, pkg):
        meta = build_manifest_meta_from_pkg(pkg)
        p = BuildManifest(self.sections, meta, pkg.executables)
        if not op.exists(op.dirname(filename)):
            os.makedirs(op.dirname(filename))
        p.write(filename)


def jobs_callback(option, opt, value, parser):
    setattr(parser.values, option.dest, cpu_count())

class BuildCommand(Command):
    long_descr = """\
Purpose: build the project
Usage:   bentomaker build [OPTIONS]."""
    short_descr = "build the project."
    common_options = Command.common_options \
                        + [Option("-i", "--inplace",
                                  help="Build extensions in place", action="store_true"),
                           Option("-j", "--jobs",
                                  help="Parallel builds (yaku build only - EXPERIMENTAL)",
                                  dest="jobs", action="callback", callback=jobs_callback),
                           Option("-v", "--verbose",
                                  help="Verbose output (yaku build only)",
                                  action="store_true")]

    def run(self, ctx):
        p = ctx.options_context.parser
        o, a = p.parse_args(ctx.command_argv)
        if o.help:
            p.print_help()
            return

        ctx.compile()
        ctx.post_compile()

    def finish(self, ctx):
        super(BuildCommand, self).finish(ctx)
        n = ctx.build_node.make_node(BUILD_MANIFEST_PATH)
        ctx.section_writer.store(n.abspath(), ctx.pkg)

def _config_content(paths):
    keys = sorted(paths.keys())
    n = max([len(k) for k in keys]) + 2
    content = []
    for name, value in sorted(paths.items()):
        content.append('%s = %r' % (name.upper().ljust(n), subst_vars(value, paths)))
    return "\n".join(content)


########NEW FILE########
__FILENAME__ = build_distutils
import os
import errno

from bento.utils.utils \
    import \
        extract_exception
from bento.compat.api \
    import \
        relpath
from bento.installed_package_description \
    import \
        InstalledSection
from bento.errors \
    import \
        CommandExecutionFailure

import bento.errors

def toyext_to_distext(e):
    """Convert a bento Extension instance to a distutils
    Extension."""
    # FIXME: this is temporary, will be removed once we do not depend
    # on distutils to build extensions anymore. That's why this is not
    # a method of the bento Extension class.
    from distutils.extension import Extension as DistExtension
    return DistExtension(e.name, sources=[s for s in e.sources],
                         include_dirs=[inc for inc in e.include_dirs])

def to_dist_compiled_library(library):
    name = module_to_path(library.name)
    return (os.path.basename(name), dict(sources=library.sources))

def module_to_path(module):
    return module.replace(".", os.path.sep)

class DistutilsBuilder(object):
    def __init__(self, verbosity=1, build_base=None):
        from distutils.dist import Distribution
        from distutils import log

        log.set_verbosity(verbosity)

        self._dist = Distribution()
        self._compilers = {}
        self._cmds = {}

        if build_base:
            opt_dict = self._dist.get_option_dict("build")
            opt_dict["build_base"] = ("bento", build_base)
        build = self._dist.get_command_obj("build")
        self._build_base = build.build_base

        self.ext_bld_cmd = self._setup_build_ext()
        self.clib_bld_cmd = self._setup_build_clib()

    def _setup_build_ext(self):
        # Horrible hack to initialize build_ext: build_ext initialization is
        # partially done within the run function (!), and is bypassed if no
        # extensions is available. We fake it just enough so that run does all
        # the initialization without trying to actually build anything.
        build = self._dist.get_command_obj("build")
        bld_cmd = build.get_finalized_command("build_ext")
        bld_cmd.initialize_options()
        bld_cmd.finalize_options()
        old_build_extensions = bld_cmd.build_extensions
        try:
            bld_cmd.build_extensions = lambda: None
            bld_cmd.extensions = [None]
            bld_cmd.run()
        finally:
            bld_cmd.build_extensions = old_build_extensions

        return bld_cmd

    def _setup_build_clib(self):
        # Horrible hack to initialize build_ext: build_ext initialization is
        # partially done within the run function (!), and is bypassed if no
        # extensions is available. We fake it just enough so that run does all
        # the initialization without trying to actually build anything.
        build = self._dist.get_command_obj("build")
        bld_cmd = build.get_finalized_command("build_clib")
        bld_cmd.initialize_options()
        bld_cmd.finalize_options()
        old_build_libraries = bld_cmd.build_libraries
        try:
            bld_cmd.build_libraries = lambda ignored: None
            bld_cmd.libraries = [None]
            bld_cmd.run()
        finally:
            bld_cmd.build_libraries = old_build_libraries

        return bld_cmd

    def _extension_filename(self, name, cmd):
        m = module_to_path(name)
        d, b = os.path.split(m)
        return os.path.join(d, cmd.get_ext_filename(b))

    def _compiled_library_filename(self, name, compiler):
        m = module_to_path(name)
        d, b = os.path.split(m)
        return os.path.join(d, compiler.library_filename(b))

    def build_extension(self, extension):
        import distutils.errors

        dist_extension = toyext_to_distext(extension)

        bld_cmd = self.ext_bld_cmd
        try:
            bld_cmd.build_extension(dist_extension)

            base, filename = os.path.split(self._extension_filename(dist_extension.name, bld_cmd))
            fullname = os.path.join(bld_cmd.build_lib, base, filename)
            return [relpath(fullname, self._build_base)]
        except distutils.errors.DistutilsError:
            e = extract_exception()
            raise BuildError(str(e))

    def build_compiled_library(self, library):
        import distutils.errors

        bld_cmd = self.clib_bld_cmd
        compiler = bld_cmd.compiler
        base, filename = os.path.split(self._compiled_library_filename(library.name, compiler))
        old_build_clib = bld_cmd.build_clib
        if base:
            # workaround for a distutils issue: distutils put all C libraries
            # in the same directory, and we cannot control the output directory
            # from the name - we need to hack build_clib directory
            bld_cmd.build_clib = os.path.join(old_build_clib, base)
        try:
            try:
                # workaround for yet another bug in distutils: distutils fucks up when
                # building a static library if the target alread exists on at least mac
                # os x.
                target = os.path.join(old_build_clib, base, filename)
                try:
                    os.remove(target)
                except OSError:
                    e = extract_exception()
                    if e.errno != errno.ENOENT:
                        raise
                build_info = {"sources": library.sources,
                        "include_dirs": library.include_dirs}
                bld_cmd.build_libraries([(library.name, build_info)])

                return [relpath(target, self._build_base)]
            except distutils.errors.DistutilsError:
                e = extract_exception()
                raise bento.errors.BuildError(str(e))
        finally:
            bld_cmd.build_clib = old_build_clib

########NEW FILE########
__FILENAME__ = build_egg
import os
import warnings

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.commands.core \
    import \
        Command, Option
from bento.commands.egg_utils \
    import \
        EggInfo, egg_filename
from bento.utils.utils import pprint, extract_exception
from bento.core \
    import \
        PackageMetadata
from bento.private.bytecode \
    import \
        bcompile, PyCompileError
from bento.installed_package_description \
    import \
        BuildManifest, iter_files

import bento.compat.api as compat
import bento.utils.path

class BuildEggCommand(Command):
    long_descr = """\
Purpose: build egg
Usage:   bentomaker build_egg [OPTIONS]"""
    short_descr = "build egg."
    common_options = Command.common_options \
                        + [Option("--output-dir",
                                  help="Output directory", default="dist"),
                           Option("--output-file",
                                  help="Output filename")]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return
        output_dir = o.output_dir
        output_file = o.output_file

        n = ctx.build_node.make_node(BUILD_MANIFEST_PATH)
        build_manifest = BuildManifest.from_file(n.abspath())
        build_egg(build_manifest, ctx.build_node, ctx.build_node, output_dir, output_file)

def build_egg(build_manifest, build_node, source_root, output_dir=None, output_file=None):
    meta = PackageMetadata.from_build_manifest(build_manifest)
    egg_info = EggInfo.from_build_manifest(build_manifest, build_node)

    # FIXME: fix egg name
    if output_dir is None:
        if output_file is None:
            egg = egg_filename(os.path.join("dist", meta.fullname))
        else:
            egg = os.path.join("dist", output_file)
    else:
        if output_file is None:
            egg = egg_filename(os.path.join(output_dir, meta.fullname))
        else:
            egg = os.path.join(output_dir, output_file)
    bento.utils.path.ensure_dir(egg)

    egg_scheme = {"prefix": source_root.abspath(),
                  "eprefix": source_root.abspath(),
                  "sitedir": source_root.abspath()}

    zid = compat.ZipFile(egg, "w", compat.ZIP_DEFLATED)
    try:
        for filename, cnt in egg_info.iter_meta(build_node):
            zid.writestr(os.path.join("EGG-INFO", filename), cnt)

        for kind, source, target in build_manifest.iter_built_files(source_root, egg_scheme):
            if not kind in ["executables"]:
                zid.write(source.abspath(), target.path_from(source_root))
            if kind == "pythonfiles":
                try:
                    bytecode = bcompile(source.abspath())
                except PyCompileError:
                    e = extract_exception()
                    warnings.warn("Error byte-compiling %r" % source.abspath())
                else:
                    zid.writestr("%sc" % target.path_from(source_root), bcompile(source.abspath()))
    finally:
        zid.close()

    return

########NEW FILE########
__FILENAME__ = build_mpkg
import os
import sys
import tempfile
import shutil

from bento.core.platforms.sysconfig \
    import \
        get_scheme
from bento.utils.utils \
    import \
        subst_vars
from bento.installed_package_description \
    import \
        BuildManifest, iter_files
from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.commands.core \
    import \
        Command, Option
from bento.commands.mpkg_utils \
    import \
        build_pkg, PackageInfo, MetaPackageInfo, make_mpkg_plist, make_mpkg_description
from bento.utils.utils \
    import \
        MODE_755

def get_default_scheme(pkg_name, py_version_short=None, prefix=None):
    if py_version_short is None:
        py_version_short = ".".join([str(i) for i in sys.version_info[:2]])
    if prefix is None:
        prefix = sys.exec_prefix
    scheme, _ = get_scheme(sys.platform)
    scheme["prefix"] = scheme["eprefix"] = prefix
    scheme["pkgname"] = pkg_name
    scheme["py_version_short"] = py_version_short
    ret = {}
    for k in scheme:
        ret[k] = subst_vars(scheme[k], scheme)
    return ret

class BuildMpkgCommand(Command):
    long_descr = """\
Purpose: build Mac OS X mpkg
Usage:   bentomaker build_mpkg [OPTIONS]"""
    short_descr = "build mpkg."
    common_options = Command.common_options \
                        + [Option("--output-dir",
                                  help="Output directory", default="dist"),
                           Option("--output-file",
                                  help="Output filename")]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return

        root = ctx.top_node
        while root.height() > 0:
            root = root.parent

        default_scheme = get_default_scheme(ctx.pkg.name)
        default_prefix = default_scheme["prefix"]
        default_sitedir = default_scheme["sitedir"]

        n = ctx.build_node.make_node(BUILD_MANIFEST_PATH)
        build_manifest = BuildManifest.from_file(n.abspath())
        name = build_manifest.meta["name"]
        version = build_manifest.meta["version"]
        py_short = ".".join([str(i) for i in sys.version_info[:2]])
        if o.output_file is None:
            mpkg_name = "%s-%s-py%s.mpkg" % (name, version, py_short)
        else:
            mpkg_name = o.output_file

        categories = set()
        file_sections = build_manifest.resolve_paths(ctx.build_node)
        for kind, source, target in iter_files(file_sections):
            categories.add(kind)

        # Mpkg metadata
        mpkg_root = os.path.join(os.getcwd(), o.output_dir, mpkg_name)
        mpkg_cdir = os.path.join(mpkg_root, "Contents")
        if os.path.exists(mpkg_root):
            shutil.rmtree(mpkg_root)
        os.makedirs(mpkg_cdir)
        f = open(os.path.join(mpkg_cdir, "PkgInfo"), "w")
        try:
            f.write("pmkrpkg1")
        finally:
            f.close()
        mpkg_info = MetaPackageInfo.from_build_manifest(build_manifest)

        purelib_pkg = "%s-purelib-%s-py%s.pkg" % (name, version, py_short)
        scripts_pkg = "%s-scripts-%s-py%s.pkg" % (name, version, py_short)
        datafiles_pkg = "%s-datafiles-%s-py%s.pkg" % (name, version, py_short)
        mpkg_info.packages = [purelib_pkg, scripts_pkg, datafiles_pkg]
        make_mpkg_plist(mpkg_info, os.path.join(mpkg_cdir, "Info.plist"))

        mpkg_rdir = os.path.join(mpkg_root, "Contents", "Resources")
        os.makedirs(mpkg_rdir)
        make_mpkg_description(mpkg_info, os.path.join(mpkg_rdir, "Description.plist"))

        # Package the stuff which ends up into site-packages
        pkg_root = os.path.join(mpkg_root, "Contents", "Packages", purelib_pkg)
        build_pkg_from_temp(ctx, build_manifest, pkg_root, root, "/", ["pythonfiles"], "Pure Python modules and packages")

        pkg_root = os.path.join(mpkg_root, "Contents", "Packages", scripts_pkg)
        build_pkg_from_temp(ctx, build_manifest, pkg_root, root, "/", ["executables"], "Scripts and binaries")

        pkg_root = os.path.join(mpkg_root, "Contents", "Packages", datafiles_pkg)
        build_pkg_from_temp(ctx, build_manifest, pkg_root, root, "/", ["bentofiles", "datafiles"], "Data files")

def build_pkg_from_temp(ctx, build_manifest, pkg_root, root_node, install_root, categories, description=None):
    d = tempfile.mkdtemp()
    try:
        tmp_root = root_node.make_node(d)
        prefix_node = tmp_root.make_node(root_node.make_node(sys.exec_prefix).path_from(root_node))
        prefix = eprefix = prefix_node.abspath()
        build_manifest.update_paths({"prefix": prefix, "eprefix": eprefix})
        file_sections = build_manifest.resolve_paths(ctx.build_node)
        for kind, source, target in iter_files(file_sections):
            if kind in categories:
                #if not os.path.exists(target.parent.abspath()):
                #    os.makedirs(os.path.dirname(target))
                target.parent.mkdir()
                shutil.copy(source.abspath(), target.abspath())
                if kind == "executables":
                    os.chmod(target.abspath(), MODE_755)

        pkg_name = os.path.splitext(os.path.basename(pkg_root))[0]
        pkg_info = PackageInfo(pkg_name=pkg_name,
            prefix=install_root, source_root=d, pkg_root=pkg_root, description=description)
        build_pkg(pkg_info)
    finally:
        shutil.rmtree(d)

########NEW FILE########
__FILENAME__ = build_msi
import shutil

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.commands.core \
    import \
        Command, Option
from bento.commands.msi_utils \
    import \
        create_msi_installer
from bento.installed_package_description \
    import \
        BuildManifest, iter_files
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def build_msi_tree(build_manifest, src_root_node, msi_tree_root):
    msi_scheme = {"prefix": msi_tree_root.abspath(),
                  "eprefix": msi_tree_root.abspath()}

    for kind, source, target in build_manifest.iter_built_files(src_root_node, msi_scheme):
        if kind == "pythonfiles":
            compiled = target.change_ext(".pyc")
            compiled.safe_write(bcompile(source.abspath()))
        target.parent.mkdir()
        shutil.copy(source.abspath(), target.abspath())

class BuildMsiCommand(Command):
    long_descr = """\
Purpose: build MSI
Usage:   bentomaker build_msi [OPTIONS]"""
    short_descr = "build msi."
    common_options = Command.common_options \
                        + [Option("--output-dir",
                                  help="Output directory", default="dist"),
                           Option("--output-file",
                                  help="Output filename")]

    def run(self, context):
        o, a = context.get_parsed_arguments()
        if o.help:
            p.print_help()
            return
        output_dir = o.output_dir
        output_file = o.output_file

        n = context.build_node.find_node(BUILD_MANIFEST_PATH)
        manifest = BuildManifest.from_file(n.abspath())
        msi_root = context.build_node.make_node("msi")

        build_msi_tree(manifest, context.build_node, msi_root)

        create_msi_installer(context.pkg, context.run_node, msi_root, o.output_file, o.output_dir)

########NEW FILE########
__FILENAME__ = build_pkg_info
import optparse

from bento.core.package import \
        PackageDescription
from bento.conv import \
        write_pkg_info

from bento._config \
    import \
        BENTO_SCRIPT
from bento.errors \
    import \
        UsageException
from bento.commands.core import \
        Command

class BuildPkgInfoCommand(Command):
    long_descr = """\
Purpose: generate PKG-INFO file
Usage:   bentomaker build_pkg_info [OPTIONS]"""
    short_descr = "generate PKG-INFO file."
    common_options = Command.common_options + [
        optparse.Option("-o", "--output", dest="output", help="Output file for PKG-INFO"),
    ]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return
        bento_script = ctx.top_node.find_node(BENTO_SCRIPT)

        if not o.output:
            pkg_info = "PKG-INFO"
        else:
            pkg_info = o.output

        pkg = PackageDescription.from_file(bento_script.abspath())

        fid = open(pkg_info, "w")
        try:
            write_pkg_info(pkg, fid)
        finally:
            fid.close()

########NEW FILE########
__FILENAME__ = build_wininst
import os
import tempfile

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.commands.core \
    import \
        Command, Option
from bento.commands.egg_utils \
    import \
        EggInfo, egg_info_dirname
from bento.commands.wininst_utils \
    import \
        wininst_filename, create_exe
from bento.core \
    import \
        PackageMetadata
from bento.installed_package_description \
    import \
        BuildManifest, iter_files

import bento.compat.api as compat
import bento.utils.path

class BuildWininstCommand(Command):
    long_descr = """\
Purpose: build wininst
Usage:   bentomaker build_wininst [OPTIONS]"""
    short_descr = "build wininst."
    common_options = Command.common_options \
                        + [Option("--output-dir",
                                  help="Output directory", default="dist"),
                           Option("--output-file",
                                  help="Output filename")]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return

        n = ctx.build_node.make_node(BUILD_MANIFEST_PATH)
        build_manifest = BuildManifest.from_file(n.abspath())
        create_wininst(build_manifest, src_root_node=ctx.build_node, build_node=ctx.build_node,
                       wininst=o.output_file,
                       output_dir=o.output_dir)

def create_wininst(build_manifest, src_root_node, build_node, egg_info=None, wininst=None, output_dir=None):
    meta = PackageMetadata.from_build_manifest(build_manifest)
    if egg_info is None:
        egg_info = EggInfo.from_build_manifest(build_manifest, build_node)

    # XXX: do this correctly, maybe use same as distutils ?
    if wininst is None:
        wininst = os.path.join(output_dir, wininst_filename(meta))
    else:
        wininst = os.path.join(output_dir, wininst)
    bento.utils.path.ensure_dir(wininst)

    egg_info_dir = os.path.join("PURELIB", egg_info_dirname(meta.fullname))

    fid, arcname = tempfile.mkstemp(prefix="zip")
    zid = compat.ZipFile(arcname, "w", compat.ZIP_DEFLATED)
    try:
        for filename, cnt in egg_info.iter_meta(build_node):
            zid.writestr(os.path.join(egg_info_dir, filename), cnt)

        wininst_paths = compat.defaultdict(lambda: r"DATA\share\$pkgname")
        wininst_paths.update({"bindir": "SCRIPTS", "sitedir": "PURELIB",
                              "gendatadir": "$sitedir"})
        d = {}
        for k in build_manifest._path_variables:
            d[k] = wininst_paths[k]
        build_manifest.update_paths(d)
        file_sections = build_manifest.resolve_paths(src_root_node)

        def write_content(source, target, kind):
            zid.write(source.abspath(), target.abspath())

        for kind, source, target in iter_files(file_sections):
            write_content(source, target, kind)

    finally:
        zid.close()
        os.close(fid)

    create_exe(build_manifest, arcname, wininst)

########NEW FILE########
__FILENAME__ = build_yaku
import sys
import os
import shutil

from bento.installed_package_description \
    import \
        InstalledSection
from bento.errors \
    import \
        CommandExecutionFailure
from bento.utils.utils \
    import \
        cpu_count, extract_exception
import bento.errors

import yaku.task_manager
import yaku.context
import yaku.scheduler
import yaku.errors

def build_extension(bld, extension, env=None):
    builder = bld.builders["pyext"]
    try:
        if env is None:
            env = {"PYEXT_CPPPATH": extension.include_dirs}
        else:
            val = env.get("PYEXT_CPPPATH", [])
            val.extend(extension.include_dirs)
        tasks = builder.extension(extension.name, extension.sources, env)
        if len(tasks) > 1:
            outputs = tasks[0].gen.outputs
        else:
            outputs = []
        return [n.bldpath() for n in outputs]
    except RuntimeError:
        e = extract_exception()
        msg = "Building extension %s failed: %s" % \
              (extension.name, str(e))
        raise CommandExecutionFailure(msg)

def build_compiled_library(bld, clib, env=None):
    builder = bld.builders["ctasks"]
    try:
        for p in clib.include_dirs:
            builder.env["CPPPATH"].insert(0, p)
        outputs = builder.static_library(clib.name, clib.sources, env)
        return [n.bldpath() for n in outputs]
    except RuntimeError:
        e = extract_exception()
        msg = "Building library %s failed: %s" % (clib.name, str(e))
        raise CommandExecutionFailure(msg)

########NEW FILE########
__FILENAME__ = command_contexts
import os
import sys
import string

import os.path as op

from bento.errors \
    import \
        InvalidPackage
from bento.utils.utils \
    import \
        is_string, subst_vars
from bento.core.node_package \
    import \
        NodeRepresentation
from bento.commands.build \
    import \
        _config_content
from bento.core.meta \
    import \
        PackageMetadata
from bento.commands.script_utils \
    import \
        create_posix_script, create_win32_script
from bento.commands.build \
    import \
        SectionWriter
from bento.commands.registries \
    import \
        OutputRegistry, ISectionRegistry, BuilderRegistry
from bento.commands.install \
    import \
        copy_installer
from bento.installed_package_description \
    import \
        InstalledSection

class DummyContextManager(object):
    def __init__(self, pre, post):
        self.pre = pre
        self.post = post

    def __enter__(self):
        self.pre()

    def __exit__(self, *a, **kw):
        self.post()

class CmdContext(object):
    def __init__(self, global_context, command_argv, options_context, pkg, run_node):
        self._global_context = global_context
        self.pkg = pkg

        self.options_context = options_context
        self.command_argv = command_argv

        # All of run/top/cur nodes are set to the same value for the base
        # context: without a bento.info available, neither build directory, nor
        # out-of-tree concepts make much sense.

        # CWD node
        self.run_node = run_node
        # Top source node (the one containing the top bento.info)
        self.top_node = run_node
        # cur_node refers to the current path when recursing into sub directories
        self.cur_node = run_node

        # Recursive related members
        self.local_node = None
        self.local_pkg = None

    def recurse_manager(self, local_node):
        """
        Return a dummy object to use for recurse if one wants to use context
        manager. Example::

            with context.recurse_manager(local_node):
                func(context)
        """
        return DummyContextManager(lambda: self.pre_recurse(local_node),
                                   lambda: self.post_recurse())

    def pre_recurse(self, local_node):
        """
        Note
        ----
        Every call to pre_recurse should be followed by a call to post_recurse.

        Calling pre_recurse for the top hook node must work as well (but could
        do nothing)
        """
        if local_node == self.run_node:
            self.local_node = self.run_node
            return
        else:
            if not local_node.is_src():
                raise IOError("node %r is not in source tree !" % local_node.abspath())
            self.local_node = local_node

            def _get_sub_package():
                relative_pos = local_node.path_from(self.top_or_sub_directory_node)
                local_bento = self.top_node.find_node(op.join(relative_pos, "bento.info"))
                k = local_bento.path_from(self.run_node)
                if k is None:
                    raise IOError("%r not found" % os.path.join(local_node.abspath(), "bento.info"))
                else:
                    return self.pkg.subpackages.get(k, None)
            self.local_pkg = _get_sub_package()

    def post_recurse(self):
        # Setting those to None is not strictly necessary, but this makes
        # things more consistent for debugging (context state exactly same
        # before pre_recurse and after post_recurse
        self.local_node = None
        self.local_pkg = None

    def get_parsed_arguments(self):
        return self.options_context.parser.parse_args(self.command_argv)

    def retrieve_scheme(self):
        return self._global_context.retrieve_scheme()

    def retrieve_configured_scheme(self):
        configure_argv = self._global_context.retrieve_command_argv("configure")
        return self._global_context.retrieve_configured_scheme(configure_argv)

    # This is run before the associated command pre-hooks
    def init(self):
        pass

    # This is run after the associated command pre-hooks, but before the command run function
    def configure(self):
        pass

    # This is run after the associated command post_hooks
    def finish(self):
        pass

class ContextWithBuildDirectory(CmdContext):
    def __init__(self, *a, **kw):
        super(ContextWithBuildDirectory, self).__init__(*a, **kw)
        self.build_root = self.run_node.make_node("build")

        # TODO: kept for compatibility. Remove it ?
        assert self.run_node is not None
        self.top_node = self.run_node._ctx.srcnode
        self.build_node = self.run_node._ctx.bldnode

        if self.pkg.sub_directory:
            self.top_or_sub_directory_node = self.top_node.make_node(self.pkg.sub_directory)
            self.build_or_sub_directory_node = self.build_node.make_node(self.pkg.sub_directory)
        else:
            self.top_or_sub_directory_node = self.top_node
            self.build_or_sub_directory_node = self.build_node

    def make_source_node(self, path):
        n = self.top_node.find_node(path)
        if n is None:
            raise IOError("file %s not found" % (op.join(self.top_node.abspath(), path)))
        else:
            return n

    def make_build_node(self, path):
        n = self.build_node.make_node(path)
        n.parent.mkdir()
        return n

class ConfigureContext(ContextWithBuildDirectory):
    pass

def _generic_iregistrer(category, name, nodes, from_node, target_dir):
    source_dir = os.path.join("$_srcrootdir", from_node.bldpath())
    files = [n.path_from(from_node) for n in nodes]
    return InstalledSection.from_source_target_directories(
        category, name, source_dir, target_dir, files)

def fill_metadata_template(content, metadata):
    tpl = string.Template(content)

    def _safe_repr(val):
        # FIXME: actually not safe at all. Needs to escape and all.
        if is_string(val):
            if len(val.splitlines()) > 1:
                return '"""%s"""' % (val,)
            else:
                return '"%s"' % (val,)
        else:
            return repr(val)

    meta_dict = dict((k.upper(), _safe_repr(v)) for k, v in metadata.items())

    return tpl.substitute(meta_dict)

def _write_template(template_node, metadata):
    source_content = template_node.read()
    output_content = fill_metadata_template(source_content, metadata)

    output = template_node.change_ext("")
    output.safe_write(output_content)
    return output

def write_template(top_node, template_file, package, additional_metadata=None):
    if additional_metadata is None:
        additional_metadata = {}

    source = top_node.find_node(template_file)
    if source is None:
        raise InvalidPackage("File %r not found (defined in 'MetaTemplateFile' field)" \
                             % (package.meta_template_file,))
    package_metadata = PackageMetadata.from_package(package)
    meta = dict((k, getattr(package_metadata, k)) for k in package_metadata.metadata_attributes)
    meta.update(additional_metadata)
    return _write_template(source, meta)

class BuildContext(ContextWithBuildDirectory):
    def __init__(self, global_context, command_argv, options_context, pkg, run_node):
        super(BuildContext, self).__init__(global_context, command_argv, options_context, pkg, run_node)
        self.builder_registry = BuilderRegistry()
        self.section_writer = SectionWriter()

        o, a = self.options_context.parser.parse_args(command_argv)
        if o.inplace:
            self.inplace = True
        else:
            self.inplace = False
        # Builders signature:
        #   - first argument: name, str. Name of the entity to be built
        #   - second argument: object. Value returned by
        #   NodePackage.iter_category for this category

        # TODO: # Refactor builders so that they directoy register outputs
        # instead of returning stuff to be registered (to allow for "delayed"
        # registration)
        def data_section_builder(name, section):
            return name, section.nodes, section.ref_node, section.target_dir

        def package_builder(name, node_py_package):
            return name, node_py_package.nodes, self.top_or_sub_directory_node, "$sitedir"

        def module_builder(name, node):
            return name, [node], self.top_or_sub_directory_node, "$sitedir"

        def script_builder(name, executable):
            scripts_node = self.build_node.make_node("scripts-%s" % sys.version[:3])
            scripts_node.mkdir()
            if sys.platform == "win32":
                nodes = create_win32_script(name, executable, scripts_node)
            else:
                nodes = create_posix_script(name, executable, scripts_node)
            return name, nodes, scripts_node, "$bindir"

        self.builder_registry.register_category("datafiles", data_section_builder)
        self.builder_registry.register_category("packages", package_builder)
        self.builder_registry.register_category("modules", module_builder)
        self.builder_registry.register_category("scripts", script_builder)

        if self.pkg.sub_directory is not None:
            sub_directory_node = self.top_node.find_node(self.pkg.sub_directory)
        else:
            sub_directory_node = None
        self._node_pkg = NodeRepresentation(run_node, self.top_node, sub_directory_node)
        self._node_pkg.update_package(pkg)

        categories = (("packages", "pythonfiles"), ("modules", "pythonfiles"), ("datafiles", "datafiles"),
                      ("scripts", "executables"), ("extensions", "extensions"),
                      ("compiled_libraries", "compiled_libraries"))
        self.outputs_registry = OutputRegistry(categories)

        self.isection_registry = ISectionRegistry()
        self.isection_registry.register_category("extensions", _generic_iregistrer)
        self.isection_registry.register_category("compiled_libraries", _generic_iregistrer)
        self.isection_registry.register_category("packages", _generic_iregistrer)
        self.isection_registry.register_category("modules", _generic_iregistrer)
        self.isection_registry.register_category("datafiles", _generic_iregistrer)
        self.isection_registry.register_category("scripts", _generic_iregistrer)

        self._current_default_section = 0

        self._meta = {}

    def register_metadata(self, name, value):
        self._meta[name] = value

    def register_category(self, category_name, category_type="pythonfiles"):
        self.outputs_registry.register_category(category_name, category_type)
        self.isection_registry.register_category(category_name, _generic_iregistrer)

    def register_outputs_simple(self, nodes, from_node=None, target_dir='$sitedir'):
        category_name = "hook_registered"
        section_name = "hook_registered%d" % self._current_default_section

        if not category_name in self.outputs_registry.categories:
            self.register_category(category_name)
        self.register_outputs(category_name, section_name, nodes, from_node, target_dir)

        self._current_default_section += 1

    def register_outputs(self, category_name, section_name, nodes, from_node=None, target_dir="$sitedir"):
        if from_node is None:
            from_node = self.build_node
        self.outputs_registry.register_outputs(category_name, section_name, nodes, from_node, target_dir)

    def _compute_extension_name(self, extension_name):
        if self.local_node is None:
            raise ValueError("Forgot to call pre_recurse ?")
        if self.local_node != self.top_node:
            parent = self.local_node.srcpath().split(os.path.sep)
            return ".".join(parent + [extension_name])
        else:
            return extension_name

    def register_builder(self, extension_name, builder):
        full_name = self._compute_extension_name(extension_name)
        self.builder_registry.register_callback("extensions", full_name, builder)

    def default_builder(self, extension, **kw):
        return self.builder_registry.default_callback(
                                        "extensions",
                                        extension,
                                        **kw)

    def tweak_extension(self, extension_name, **kw):
        def _builder(extension):
            return self.default_builder(extension, **kw)
        full_name = self._compute_extension_name(extension_name)
        return self.builder_registry.register_callback("extensions", full_name, _builder)

    def tweak_library(self, lib_name, **kw):
        def _builder(lib_name):
            return self.default_library_builder(lib_name, **kw)
        full_name = self._compute_extension_name(lib_name)
        return self.builder_registry.register_callback("compiled_libraries", full_name, _builder)

    def default_library_builder(self, library, **kw):
        return self.builder_registry.default_callback(
                                        "compiled_libraries",
                                        library,
                                        **kw)

    def disable_extension(self, extension_name):
        def nobuild(extension):
            pass
        self.register_builder(extension_name, nobuild)

    def register_compiled_library_builder(self, clib_name, builder):
        full_name = self._compute_extension_name(clib_name)
        self.builder_registry.register_callback("compiled_libraries", full_name, builder)

    def compile(self):
        for category in ("packages", "modules", "datafiles"):
            for name, value in self._node_pkg.iter_category(category):
                builder = self.builder_registry.builder(category, name)
                name, nodes, from_node, target_dir = builder(name, value)
                self.outputs_registry.register_outputs(category, name, nodes, from_node, target_dir)

        category = "scripts"
        for name, executable in self.pkg.executables.items():
            builder = self.builder_registry.builder(category, name)
            name, nodes, from_node, target_dir = builder(name, executable)
            self.outputs_registry.register_outputs(category, name, nodes, from_node, target_dir)

        if self.pkg.config_py:
            content = _config_content(self.retrieve_configured_scheme())
            target_node = self.build_node.make_node(self.pkg.config_py)
            target_node.parent.mkdir()
            target_node.safe_write(content)
            self.outputs_registry.register_outputs("modules", "bento_config", [target_node],
                                                   self.build_node, "$sitedir")

        if self.pkg.meta_template_files:
            target_nodes = []
            for template in self.pkg.meta_template_files:
                target_node = write_template(self.top_node, template, self.pkg, self._meta)
                target_nodes.append(target_node)
            self.outputs_registry.register_outputs("modules", "meta_from_template", target_nodes,
                                               self.build_node, "$sitedir")
    def post_compile(self):
        # Do the output_registry -> installed sections registry convertion
        section_writer = self.section_writer

        for category, name, nodes, from_node, target_dir in self.outputs_registry.iter_over_category():
            installed_category = self.outputs_registry.installed_categories[category]
            if installed_category in section_writer.sections:
                sections = section_writer.sections[installed_category]
            else:
                sections = section_writer.sections[installed_category] = {}
            registrer = self.isection_registry.registrer(category, name)
            sections[name] = registrer(installed_category, name, nodes, from_node, target_dir)

        # FIXME: this is quite stupid.
        if self.inplace:
            scheme = self.retrieve_scheme()
            scheme["prefix"] = scheme["eprefix"] = self.run_node.abspath()
            scheme["sitedir"] = self.run_node.abspath()

            if self.pkg.config_py:
                target_node = self.build_node.find_node(self.pkg.config_py)
            else:
                target_node = None

            def _install_node(category, node, from_node, target_dir):
                installed_path = subst_vars(target_dir, scheme)
                target = os.path.join(installed_path, node.path_from(from_node))
                copy_installer(node.path_from(self.run_node), target, category)

            intree = (self.top_node == self.run_node)
            if intree:
                for category, name, nodes, from_node, target_dir in self.outputs_registry.iter_over_category():
                    for node in nodes:
                        if node != target_node and node.is_bld():
                            _install_node(category, node, from_node, target_dir)
            else:
                for category, name, nodes, from_node, target_dir in self.outputs_registry.iter_over_category():
                    for node in nodes:
                        if node != target_node:
                            _install_node(category, node, from_node, target_dir)

class SdistContext(ContextWithBuildDirectory):
    def __init__(self, global_context, cmd_args, option_context, pkg, run_node):
        super(SdistContext, self).__init__(global_context, cmd_args, option_context, pkg, run_node)
        self._meta = {}

        self._node_pkg = NodeRepresentation(run_node, self.top_node)
        self._node_pkg.update_package(pkg)

    def register_metadata(self, name, value):
        self._meta[name] = value

    def register_source_node(self, node, archive_name=None):
        """Register a node into the source distribution.

        archive_name is an optional string which will be used for the file name
        in the archive."""
        self._node_pkg._extra_source_nodes.append(node)
        if archive_name:
            self._node_pkg._aliased_source_nodes[node] = archive_name

    def configure(self):
        if self.pkg.meta_template_files:
            for template in self.pkg.meta_template_files:
                output = write_template(self.top_node, template, self.pkg, self._meta)
                self.register_source_node(output, output.bldpath())

class HelpContext(CmdContext):
    def __init__(self, *a, **kw):
        super(HelpContext, self).__init__(*a, **kw)
        self.short_descriptions = {}
        for cmd_name in self._global_context.command_names(public_only=False):
            cmd = self._global_context.retrieve_command(cmd_name)
            self.short_descriptions[cmd_name] = cmd.short_descr

    def retrieve_options_context(self, cmd_name):
        return self._global_context.retrieve_options_context(cmd_name)

    def is_options_context_registered(self, cmd_name):
        return self._global_context.is_options_context_registered(cmd_name)

########NEW FILE########
__FILENAME__ = configure
import sys
import os
import os.path as op

from six import moves

from bento.utils.utils \
    import \
        subst_vars, virtualenv_prefix
from bento.core.platforms import \
        get_scheme
from bento._config \
    import \
        CONFIGURED_STATE_DUMP, BENTO_SCRIPT

from bento.commands.core \
    import \
        Command, Option
from bento.errors \
    import \
        UsageException

def set_scheme_unix(scheme, options, package):
    # This mess is the simplest solution I can think of to support:
    #   - /usr/local as default for prefix while using debian/ubuntu changes
    #   (dist-packages instead of site-packages)
    #   - virtualenv support
    #   - arbitrary python install
    #
    # Cases to consider:
    #   - if prefix is given:
    #       - prefix and exec_prefix are customized following their options
    #   - if prefix is not given:
    #       - if under virtualenv: prefix and exec_prefix are set to their sys.* values
    #       - else on unix != darwin: using /usr/local
    #       - else on darwin: using sys values
    if options.prefix is not None:
        scheme["prefix"] = options.prefix
        if options.eprefix is None:
            scheme["eprefix"] = scheme["prefix"]
        else:
            scheme["eprefix"] = options.eprefix
    elif options.prefix is None and options.eprefix is not None:
        raise NotImplementedError("Customizing exec_prefix without " \
                                  "customizing prefix is not implemented yet")
    elif options.prefix is None and options.eprefix is None:
        venv_prefix = virtualenv_prefix()
        if venv_prefix is not None:
            scheme["prefix"] = scheme["eprefix"] = venv_prefix
        elif sys.platform == "darwin":
            scheme["prefix"] = op.normpath(sys.prefix)
            scheme["eprefix"] = op.normpath(sys.exec_prefix)
        else:
            # XXX: unix_local is an ubuntu/debian thing only ?
            from distutils.command.install import INSTALL_SCHEMES
            if "unix_local" in INSTALL_SCHEMES:
                dist_scheme = INSTALL_SCHEMES["unix_local"]
                # This madness is used to support ubuntu/debian customization
                prefix = "/usr/local"
                base = "/usr"
                py_version = sys.version.split()[0]
                py_version_short = py_version[0:3]
                dist_name = package.name
                v = {"base": base, "py_version_short": py_version_short, "dist_name": dist_name}

                scheme["prefix"] = scheme["eprefix"] = prefix
                scheme["sitedir"] = subst_vars(dist_scheme["purelib"], v)
                scheme["includedir"] = subst_vars(dist_scheme["headers"], v)
            else:
                scheme["prefix"] = scheme["eprefix"] = "/usr/local"

def set_scheme_win32(scheme, options, package):
    if options.prefix is not None:
        scheme["prefix"] = options.prefix
        if options.eprefix is None:
            scheme["eprefix"] = scheme["prefix"]
        else:
            scheme["eprefix"] = options.eprefix
    elif options.prefix is None and options.eprefix is not None:
        raise NotImplementedError("Customizing exec_prefix without " \
                                  "customizing prefix is not implemented yet")
    elif options.prefix is None and options.eprefix is None:
        venv_prefix = virtualenv_prefix()
        if venv_prefix is not None:
            scheme["prefix"] = scheme["eprefix"] = venv_prefix
        else:
            scheme["prefix"] = op.normpath(sys.prefix)
            scheme["eprefix"] = op.normpath(sys.exec_prefix)

def set_scheme_options(scheme, options, package):
    """Set path variables given in options in scheme dictionary."""
    for k in scheme:
        if hasattr(options, k):
            val = getattr(options, k)
            if val:
                if not os.path.isabs(val):
                    msg = "value given for path option '%s' " \
                          "should be an absolute path (value " \
                          "given was '%s')" % (k, val)
                    raise UsageException(msg)
                scheme[k] = val

    if os.name == "posix":
        set_scheme_unix(scheme, options, package)
    elif os.name == "nt":
        set_scheme_win32(scheme, options, package)
    else:
        raise NotImplementedError("OS %s not supported" % os.name)

    # XXX: define default somewhere and stick with it
    if options.prefix is not None and options.eprefix is None:
        scheme["eprefix"] = scheme["prefix"]

def get_flag_values(cmd, cmd_argv):
    """Return flag values from the command instance and its arguments.

    Assumes cmd is an instance of Configure."""
    o, a = cmd.options_context.parser.parse_args(cmd_argv)
    flag_values = _get_flag_values(cmd.flags, o)
    return flag_values

def _get_flag_values(flag_names, options):
    """Return flag values as defined by the options."""
    flag_values = {}
    for option_name in flag_names:
        flag_value = getattr(options, option_name, None)
        if flag_value is not None:
            if flag_value.lower() in ["true", "yes"]:
                flag_values[option_name] = True
            elif flag_value.lower() in ["false", "no"]:
                flag_values[option_name] = False
            else:
                msg = """Option %s expects a true or false argument"""
                raise UsageException(msg % "--%s" % option_name)
    return flag_values

def _setup_options_parser(options_context, package_options):
    """Setup the command options parser, merging standard options as well
    as custom options defined in the bento.info file, if any.
    """
    scheme, scheme_opts_d = get_scheme(sys.platform)

    p = options_context

    # Create default path options
    scheme_opts = {}
    for name, opt_d in scheme_opts_d.items():
        o = opt_d.pop("opts")
        opt = Option(*o, **opt_d)
        scheme_opts[name] = opt

    # Add custom path options (as defined in bento.info) to the path scheme
    for name, f in package_options.path_options.items():
        scheme_opts[name] = \
            Option('--%s' % f.name,
                   help='%s [%s]' % (f.description, f.default_value))

    p.add_group("installation_options", "Installation fine tuning")
    for opt in scheme_opts.values():
        p.add_option(opt, "installation_options")

    flag_opts = {}
    if package_options.flag_options:
        p.add_group("optional_features", "Optional features")
        for name, v in package_options.flag_options.items():
            flag_opts[name] = Option(
                    "--%s" % v.name,
                    help="%s [default=%s]" % (v.description, v.default_value))
            p.add_option(flag_opts[name], "optional_features")

class ConfigureCommand(Command):
    long_descr = """\
Purpose: configure the project
Usage: bentomaker configure [OPTIONS]"""
    short_descr = "configure the project."

    def __init__(self, *a, **kw):
        super(ConfigureCommand, self).__init__(*a, **kw)
        self.scheme = {}
        self.flags = []

    def register_options(self, options_context, package_options=None):
        _setup_options_parser(options_context, package_options)

        self.scheme.update(_compute_scheme(package_options))
        self.flags.extend(package_options.flag_options.keys())

    def run(self, ctx):
        bento_script = ctx.top_node.find_node(BENTO_SCRIPT)
        if bento_script is None:
            raise IOError("%s not found ?" % BENTO_SCRIPT)

        args = ctx.command_argv
        o, a = ctx.options_context.parser.parse_args(args)
        if o.help:
            ctx.options_context.parser.print_help()
            return

        set_scheme_options(self.scheme, o, ctx.pkg)

def _compute_scheme(package_options):
    """Compute path and flags-related options as defined in the script file(s)

    Parameters
    ----------
    package_options: PackageOptions
    """
    scheme, scheme_opts_d = get_scheme(sys.platform)

    # XXX: abstract away those, as it is copied from distutils
    scheme['py_version_short'] = ".".join(str(part) for part in sys.version_info[:2])
    scheme['pkgname'] = package_options.name

    for name, f in package_options.path_options.items():
        scheme[name] = f.default_value
    return scheme

########NEW FILE########
__FILENAME__ = contexts
from bento.commands.configure \
    import \
        _compute_scheme, set_scheme_options
from bento.commands.registries \
    import \
        CommandRegistry, ContextRegistry, OptionsRegistry
from bento.commands.dependency \
    import \
        CommandScheduler
from bento.commands.hooks \
    import \
        HookRegistry
from bento.commands.wrapper_utils \
    import \
        resolve_and_run_command
from bento.utils.utils \
    import \
        read_or_create_dict

from six.moves \
    import \
        cPickle

class GlobalContext(object):
    def __init__(self, command_data_db, commands_registry=None, contexts_registry=None,
            options_registry=None, commands_scheduler=None):
        self._commands_registry = commands_registry or CommandRegistry()
        self._contexts_registry = contexts_registry or ContextRegistry()
        self._options_registry = options_registry or OptionsRegistry()
        self._scheduler = commands_scheduler or CommandScheduler()
        self._hooks_registry = HookRegistry()

        self.backend = None
        self._package_options = None

        self._command_data_db = command_data_db
        if command_data_db is None:
            self._command_data_store = {}
        else:
            self._command_data_store = read_or_create_dict(command_data_db.abspath())

    def store(self):
        if self._command_data_db:
            self._command_data_db.safe_write(cPickle.dumps(self._command_data_store), "wb")

    def run_command(self, command_name, command_argv, package, run_node):
        resolve_and_run_command(self, command_name, command_argv, run_node, package)

    #------------
    # Command API
    #------------
    def register_command(self, cmd_name, cmd, public=True):
        """Register a command name to a command instance.

        Parameters
        ----------
        cmd_name: str
            name of the command
        cmd: object
            instance from a subclass of Command
        """
        self._commands_registry.register(cmd_name, cmd, public)

    def retrieve_command(self, cmd_name):
        """Return the command instance registered for the given command name."""
        return self._commands_registry.retrieve(cmd_name)

    def is_command_registered(self, cmd_name):
        """Return True if the command is registered."""
        return self._commands_registry.is_registered(cmd_name)

    def command_names(self, public_only=True):
        if public_only:
            return self._commands_registry.public_command_names()
        else:
            return self._commands_registry.command_names()

    #--------------------
    # Command Context API
    #--------------------
    def register_command_context(self, cmd_name, klass):
        self._contexts_registry.register(cmd_name, klass)

    def retrieve_command_context(self, cmd_name):
        return self._contexts_registry.retrieve(cmd_name)

    def is_command_context_registered(self, cmd_name):
        """Return True if the command context is registered."""
        return self._contexts_registry.is_registered(cmd_name)

    #--------------------
    # Command Options API
    #--------------------
    def register_options_context_without_command(self, name, context):
        """Register options_context for special 'commands' that has no command
        context attached to them

        This is typically used for global help, and other help-only commands
        such as 'globals'."""
        return self._options_registry.register(name, context)

    def register_options_context(self, cmd_name, context):
        cmd = self.retrieve_command(cmd_name)
        if self._package_options is not None and hasattr(cmd, "register_options"):
            cmd.register_options(context, self._package_options)

        return self._options_registry.register(cmd_name, context)

    def retrieve_options_context(self, cmd_name):
        return self._options_registry.retrieve(cmd_name)

    def is_options_context_registered(self, cmd_name):
        return self._options_registry.is_registered(cmd_name)

    def add_option_group(self, cmd_name, name, title):
        """Add a new option group for the given command.
        
        Parameters
        ----------
        cmd_name: str
            name of the command
        name: str
            name of the group option
        title: str
            title of the group
        """
        ctx = self._options_registry.retrieve(cmd_name)
        ctx.add_group(name, title)

    def add_option(self, cmd_name, option, group=None):
        """Add a new option for the given command.

        Parameters
        ----------
        cmd_name: str
            name of the command
        option: str
            name of the option
        group: str, None
            group to associated with
        """
        ctx = self._options_registry.retrieve(cmd_name)
        ctx.add_option(option, group)

    def register_package_options(self, package_options):
        self._package_options = package_options

    # FIXME: rename
    def retrieve_scheme(self):
        """Return the path scheme, including any custom path defined in the
        bento.info script (Path sections)."""
        return _compute_scheme(self._package_options)

    def retrieve_configured_scheme(self, command_argv=None):
        """Return the configured path scheme with the given command argv.

        Note
        ----
        This can only be safely called once regiser_options_context has been
        called for the configure command"""
        assert self._package_options is not None
        assert self.is_options_context_registered("configure")

        if command_argv is None:
            command_argv = []
        scheme = _compute_scheme(self._package_options)
        options_context = self.retrieve_options_context("configure")
        o, a = options_context.parser.parse_args(command_argv)
        set_scheme_options(scheme, o, self._package_options)
        return scheme

    #-----------------------
    # Command dependency API
    #-----------------------
    def set_before(self, cmd_name, cmd_name_before):
        """Specify that command cmd_name_before should run before cmd_name."""
        self._scheduler.set_before(cmd_name, cmd_name_before)

    def set_after(self, cmd_name, cmd_name_after):
        """Specify that command cmd_name_before should run after cmd_name."""
        self._scheduler.set_after(cmd_name, cmd_name_after)

    def retrieve_dependencies(self, cmd_name):
        """Return the ordered list of command names to run before the given
        command name."""
        return self._scheduler.order(cmd_name)

    #---------
    # Hook API
    #---------
    def add_pre_hook(self, hook, cmd_name):
        self._hooks_registry.add_pre_hook(hook, cmd_name)

    def add_post_hook(self, hook, cmd_name):
        self._hooks_registry.add_post_hook(hook, cmd_name)

    def retrieve_pre_hooks(self, cmd_name):
        return self._hooks_registry.retrieve_pre_hooks(cmd_name)

    def retrieve_post_hooks(self, cmd_name):
        return self._hooks_registry.retrieve_post_hooks(cmd_name)

    #------------
    # Backend API
    #------------
    def register_backend(self, backend):
        self.backend = backend

    def retrieve_command_argv(self, command_name):
        # FIXME: returning empty list if nothing is available hides bugs.
        return self._command_data_store.get(command_name, [])

    def save_command_argv(self, command_name, command_argv):
        self._command_data_store[command_name] = command_argv

########NEW FILE########
__FILENAME__ = core
from optparse \
    import \
        Option

import bento

from bento.errors \
    import \
        UsageException

USAGE = """\
bentomaker %(version)s -- an alternative to distutils-based systems
Usage: %(name)s [command [options]]
"""

class Command(object):
    long_descr = """\
Purpose: command's purposed (default description)
Usage: command's usage (default description)
"""
    short_descr = None
    # XXX: decide how to deal with subcommands options
    common_options = [Option('-h', '--help',
                             help="Show this message and exits.",
                             action="store_true")]

    def run(self, ctx):
        raise NotImplementedError("run method should be implemented by command classes.")

    def init(self, ctx):
        pass

    def register_options(self, options_context, package_options=None):
        pass

    def finish(self, ctx):
        pass

class HelpCommand(Command):
    long_descr = """\
Purpose: Show help on a command or other topic.
Usage:   bentomaker help [TOPIC] or bentomaker help [COMMAND]."""
    short_descr = "gives help on a given topic or command."
    def run(self, ctx):
        cmd_args = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(cmd_args)
        if o.help:
            p.print_help()
            return
        if len(a) < 1:
            print(get_simple_usage(ctx))
            return

        # Parse the options for help command itself
        cmd_name = None
        help_args = []
        if not cmd_args[0].startswith('-'):
            cmd_name = cmd_args[0]
            cmd_args.pop(0)
        else:
            # eat all the help options
            _args = [cmd_args.pop(0)]
            while not _args[0].startswith('-') and len(cmd_args) > 0:
                _args.append(cmd_args.pop(0))
            help_args = _args
            cmd_name = cmd_args[0]

        if ctx.is_options_context_registered(cmd_name):
            options_context = ctx.retrieve_options_context(cmd_name)
            p = options_context.parser
            p.print_help()
        else:
            raise UsageException("error: command %s not recognized" % cmd_name)

def fill_string(s, minlen):
    if len(s) < minlen:
        s += " " * (minlen - len(s))
    return s

def get_simple_usage(context):
    """Return simple usage as a string.

    Expects an HelpContext instance.
    """
    ret = [USAGE % {"name": "bentomaker",
                    "version": bento.__version__}]
    ret.append("Basic Commands:")

    commands = []

    def add_group(cmd_names):
        for name in cmd_names:
            doc = context.short_descriptions[name]
            if doc is None:
                doc = "undocumented"
            header = "  %(script)s %(cmd_name)s" % \
                        {"script": "bentomaker",
                         "cmd_name": name}
            commands.append((header, doc))
    add_group(["configure", "build", "install"])
    commands.append(("", ""))
    add_group(["sdist", "build_wininst", "build_egg"])
    commands.append(("", ""))

    commands.append(("  %s help configure" % "bentomaker",
                     "more help on e.g. configure command"))
    commands.append(("  %s help commands" % "bentomaker",
                     "list all commands"))
    commands.append(("  %s help globals" % "bentomaker",
                     "show global options"))

    minlen = max([len(header) for header, hlp in commands]) + 2
    for header, hlp in commands:
        ret.append(fill_string(header, minlen) + hlp)
    return "\n".join(ret)

########NEW FILE########
__FILENAME__ = dependency
import os
import sys

from six.moves import cPickle

from bento.compat.api \
    import \
        defaultdict
import bento.utils.io2

def _invert_dependencies(deps):
    """Given a dictionary of edge -> dependencies representing a DAG, "invert"
    all the dependencies."""
    ideps = {}
    for k, v in deps.items():
        for d in v:
            l = ideps.get(d, None)
            if l:
                l.append(k)
            else:
                l = [k]
            ideps[d] = l

    return ideps

class CommandScheduler(object):
    def __init__(self):
        self.before = defaultdict(list)
        self.command_names = {}

    def _register(self, command_name):
        if not command_name in self.command_names:
            self.command_names[command_name] = command_name

    def set_before(self, command_name, command_name_prev):
        """Set command_name_prev to be run before command_name"""
        self._register(command_name)
        self._register(command_name_prev)
        if not command_name_prev in self.before[command_name]:
            self.before[command_name].append(command_name_prev)

    def set_after(self, command_name, command_name_next):
        """Set command_name_next to be run after command_name"""
        return self.set_before(command_name_next, command_name)

    def order(self, target):
        after = _invert_dependencies(self.before)

        visited = {}
        out = []

        # DFS-based topological sort: this is better to only get the
        # dependencies of a given target command instead of sorting the whole
        # dag
        def _visit(n, stack_visited):
            if n in stack_visited:
                raise ValueError("Cycle detected: %r" % after)
            else:
                stack_visited[n] = None
            if not n in visited:
                visited[n] = None
                for m, v in after.items():
                    if n in v:
                        _visit(m, stack_visited)
                out.append(n)
            stack_visited.pop(n)
        _visit(target, {})
        return [self.command_names[o] for o in out[:-1]]

class CommandDataProvider(object):
    @classmethod
    def from_file(cls, filename):
        """Create a new instance from a pickled file.

        If the file does not exist, creates an instance with no data.
        """
        if os.path.exists(filename):
            fid = open(filename, "rb")
            try:
                return cls(cPickle.load(fid))
            finally:
                fid.close()
        else:
            return cls()

    def store(self, filename):
        bento.utils.io2.safe_write(filename, lambda fid: cPickle.dump(self._data, fid))

    def __init__(self, data=None):
        self._data = defaultdict(list)
        if data:
            self._data.update(data)

    def __getitem__(self, command_name):
        return self._data[command_name]

    def __setitem__(self, command_name, command_argv):
        self._data[command_name] = command_argv

########NEW FILE########
__FILENAME__ = egg_utils
import os
import sys
import zipfile

from six.moves import cStringIO

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.conv \
    import \
        to_distutils_meta
from bento.core \
    import \
        PackageMetadata
from bento.installed_package_description \
    import \
        iter_source_files, BuildManifest

def egg_filename(fullname, pyver=None):
    if not pyver:
        pyver = ".".join([str(i) for i in sys.version_info[:2]])
    return "%s-py%s.egg" % (fullname, pyver)

def egg_info_dirname(fullname, pyver=None):
    if not pyver:
        pyver = ".".join([str(i) for i in sys.version_info[:2]])
    return "%s-py%s.egg-info" % (fullname, pyver)

class EggInfo(object):
    @classmethod
    def from_build_manifest(cls, build_manifest, src_node):
        meta = PackageMetadata.from_build_manifest(build_manifest)
        executables = build_manifest.executables

        file_sections = build_manifest.resolve_paths(src_node)
        sources = list([n.abspath() for n in iter_source_files(file_sections)])

        ret = cls(meta, executables, sources)
        ret.build_manifest = build_manifest
        return ret

    def __init__(self, meta, executables, sources):
        self._dist_meta = to_distutils_meta(meta)

        self.sources = sources
        self.meta = meta
        self.executables = executables
        self.build_manifest = None

    def get_pkg_info(self):
        tmp = cStringIO()
        self._dist_meta.write_pkg_file(tmp)
        ret = tmp.getvalue()
        tmp.close()
        return ret

    def get_sources(self):
        return "\n".join([os.path.normpath(f) for f in self.sources])

    def get_install_requires(self):
        return "\n".join(self.meta.install_requires)

    def get_top_levels(self):
        # Last newline added for compatibility with setuptools
        return "\n".join(self.meta.top_levels + [''])

    def get_not_zip_safe(self):
        return "\n"

    def get_dependency_links(self):
        return "\n"

    def get_entry_points(self):
        ret = []
        ret.append("[console_scripts]")
        ret.extend([exe.full_representation() for exe in \
                        self.executables.values()])
        ret.append('')
        return "\n".join(ret)

    def get_build_manifest_info(self, build_manifest_node):
        # FIXME: this is wrong. Rethink the EggInfo interface and its
        # relationship with build_manifest
        if self.build_manifest is None:
            return build_manifest_node.read()
        else:
            tmp = cStringIO()
            self.build_manifest._write(tmp)
            ret = tmp.getvalue()
            tmp.close()
            return ret

    def iter_meta(self, build_node):
        build_manifest_node = build_node.make_node(BUILD_MANIFEST_PATH)
        func_table = {
                "pkg_info": self.get_pkg_info,
                "sources": self.get_sources,
                "install_requires": self.get_install_requires,
                "top_levels": self.get_top_levels,
                "not_zip_safe": self.get_not_zip_safe,
                "dependency_links": self.get_dependency_links,
                "entry_points": self.get_entry_points,
                "build_manifest_info": lambda: self.get_build_manifest_info(build_manifest_node),
            }
        file_table = {
                "pkg_info": "PKG-INFO",
                "sources": "SOURCES.txt",
                "install_requires": "requires.txt",
                "top_levels": "top_level.txt",
                "not_zip_safe": "not-zip-safe",
                "dependency_links": "dependency_links.txt",
                "entry_points": "entry_points.txt",
                "build_manifest_info": "build_manifest.info",
            }

        for k in func_table:
            yield file_table[k], func_table[k]()

def extract_egg(egg, extract_dir):
    # Given a bento-produced egg, extract its content in the given directory,
    # and returned the corresponding build_manifest info instance
    build_manifest = BuildManifest.from_egg(egg)
    # egg scheme
    build_manifest.update_paths({"prefix": ".", "eprefix": ".", "sitedir": "."})

    zid = zipfile.ZipFile(egg)
    try:
        for type, sections in build_manifest.files.items():
            for name, section in sections.items():
                target_dir = build_manifest.resolve_path(section.target_dir)
                section.source_dir = os.path.join(extract_dir, target_dir)
                for source, target in section.files:
                    g = os.path.join(target_dir, target)
                    g = os.path.normpath(g)
                    zid.extract(g, extract_dir)
    finally:
        zid.close()

    return build_manifest

########NEW FILE########
__FILENAME__ = hooks
import imp
import os
import re
import sys
import traceback

from six.moves \
    import \
        StringIO

from bento.commands.core \
    import \
        Command
from bento.compat \
    import \
        inspect as compat_inspect
from bento.utils.utils \
    import \
        extract_exception
from bento.errors \
    import \
        InvalidHook

SAFE_MODULE_NAME = re.compile("[^a-zA-Z_]")

class HookRegistry(object):
    def __init__(self):
        self._pre_hooks = {}
        self._post_hooks = {}

    def add_pre_hook(self, hook, cmd_name):
        if cmd_name in self._pre_hooks:
            self._pre_hooks[cmd_name].append(hook)
        else:
            self._pre_hooks[cmd_name] = [hook]

    def add_post_hook(self, hook, cmd_name):
        if cmd_name in self._post_hooks:
            self._post_hooks[cmd_name].append(hook)
        else:
            self._post_hooks[cmd_name] = [hook]

    def retrieve_pre_hooks(self, cmd_name):
        return self._pre_hooks.get(cmd_name, [])

    def retrieve_post_hooks(self, cmd_name):
        return self._post_hooks.get(cmd_name, [])

def find_pre_hooks(modules, cmd_name):
    """Retrieve all pre hooks instances defined in given modules list.

    This should be used to find prehooks defined through the hook.pre_*. This
    works by looking for all WrappedCommand instances in the modules.

    Parameters
    ----------
    modules: seq
        list of modules to look into
    cmd_name: str
        command name
    """
    pre_hooks = []
    for module in modules:
        pre_hooks.extend([f for f in vars(module).values() if isinstance(f,
            PreHookWrapper) and f.cmd_name == cmd_name])
    return pre_hooks

def find_post_hooks(modules, cmd_name):
    """Retrieve all post hooks instances defined in given modules list.

    This should be used to find prehooks defined through the hook.pre_*. This
    works by looking for all WrappedCommand instances in the modules.

    Parameters
    ----------
    modules: seq
        list of modules to look into
    cmd_name: str
        command name
    """
    post_hooks = []
    for module in modules:
        post_hooks.extend([f for f in vars(module).values() if isinstance(f,
            PostHookWrapper) and f.cmd_name == cmd_name])
    return post_hooks

def find_startup_hooks(modules):
    """Retrieve all startup hook instances defined in given modules list.

    This should be used to find hooks defined through the hook.startup.

    Parameters
    ----------
    modules: seq
        list of modules to look into
    """
    hooks = []
    for module in modules:
        hooks.extend([f for f in vars(module).values() if isinstance(f, StartupHook)])
    return hooks

def find_shutdown_hooks(modules):
    """Retrieve all shutdown hook instances defined in given modules list.

    This should be used to find hooks defined through the hook.shutdown.

    Parameters
    ----------
    modules: seq
        list of modules to look into
    """
    hooks = []
    for module in modules:
        hooks.extend([f for f in vars(module).values() if isinstance(f, ShutdownHook)])
    return hooks

def find_options_hooks(modules):
    """Retrieve all options hook instances defined in given modules list.

    This should be used to find hooks defined through the hook.options.

    Parameters
    ----------
    modules: seq
        list of modules to look into
    """
    hooks = []
    for module in modules:
        hooks.extend([f for f in vars(module).values() if isinstance(f, OptionsHook)])
    return hooks

class _HookWrapperBase(object):
    def __init__(self, func, cmd_name, local_dir):
        self._func = func
        self.local_dir = local_dir
        self.cmd_name = cmd_name
        self.name = func.__name__

    def __call__(self, ctx):
        return self._func(ctx)

    def __getattr__(self, k):
        return getattr(self._func, k)

class PreHookWrapper(_HookWrapperBase):
    pass

class PostHookWrapper(_HookWrapperBase):
    pass

class _GlobalHookBase(object):
    def __init__(self, func):
        self._func = func

    def __call__(self, ctx):
        return self._func(ctx)

    def __getattr__(self, k):
        return getattr(self._func, k)

class StartupHook(_GlobalHookBase):
    pass

class ShutdownHook(_GlobalHookBase):
    pass

class OptionsHook(_GlobalHookBase):
    pass

def _make_hook_decorator(command_name, kind):
    def decorator(f):
        local_dir = os.path.dirname(compat_inspect.stack()[1][1])
        if kind == "post":
            hook = PostHookWrapper(f, command_name, local_dir)
        elif kind == "pre":
            hook = PreHookWrapper(f, command_name, local_dir)
        else:
            raise ValueError("invalid hook kind %s" % kind)
        return hook
    return decorator

post_configure = _make_hook_decorator("configure", "post")
pre_configure = _make_hook_decorator("configure", "pre")
post_build = _make_hook_decorator("build", "post")
pre_build = _make_hook_decorator("build", "pre")
post_sdist = _make_hook_decorator("sdist", "post")
pre_sdist = _make_hook_decorator("sdist", "pre")

def options(f):
    return OptionsHook(f)

def startup(f):
    return StartupHook(f)

def shutdown(f):
    return ShutdownHook(f)

def create_hook_module(target):
    safe_name = SAFE_MODULE_NAME.sub("_", target, len(target))
    module_name = "bento_hook_%s" % safe_name
    main_file = os.path.abspath(target)
    module = imp.new_module(module_name)
    module.__file__ = main_file
    code = open(main_file).read()

    sys.path.insert(0, os.path.dirname(main_file))
    try:
        exec(compile(code, main_file, 'exec'), module.__dict__)
        sys.modules[module_name] = module
    except Exception:
        sys.path.pop(0)
        e = extract_exception()
        tb = sys.exc_info()[2]
        s = StringIO()
        traceback.print_tb(tb, file=s)
        msg = """\
Could not import hook file %r: caught exception %r
Original traceback (most recent call last)
%s\
""" % (main_file, e, s.getvalue())
        raise InvalidHook(msg)

    module.root_path = main_file
    return module

class WrappedCommand(Command):
    def __init__(self, func):
        super(WrappedCommand, self).__init__()
        self._func = func
        self.name = func.__name__

    def __call__(self, ctx):
        return self.run(ctx)

    def run(self, ctx):
        return self._func(ctx)

    def __getattr__(self, k):
        return getattr(self._func, k)

def command(f):
    """Decorator to create a new command from a simple function

    The function should take one CommandContext instance

    Example
    -------
 
    A simple command may be defined as follows::

        @command
        def hello(context):
            print "hello"
    """
    return WrappedCommand(f)

def find_command_hooks(modules):
    """Retrieve all command instances defined in given modules list.

    This should be used to find commands defined through the hook.command. This
    works by looking for all WrappedCommand instances in the modules.

    Parameters
    ----------
    modules: seq
        list of modules to look into
    """
    commands = []
    for module in modules:
        commands.extend([f for f in vars(module).values() if isinstance(f,
            WrappedCommand)])
    return commands

########NEW FILE########
__FILENAME__ = install
import os
import shutil
import subprocess
import errno

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.installed_package_description import \
    BuildManifest, iter_files

from bento.commands.core import \
    Command, Option
from bento.utils.utils import \
    pprint, extract_exception, MODE_755, MODE_777

def _rollback_operation(line):
    operation, arg = line.split()
    if operation == "MKDIR":
        try:
            os.rmdir(arg)
        except OSError:
            e = extract_exception()
            # FIXME: hardcoded errno !!
            if e.errno != 66:
                raise
    elif operation == "COPY":
        os.remove(arg)
    else:
        raise ValueError("Unknown operation: %s" % operation)

def rollback_transaction(f):
    fid = open(f)
    try:
        lines = fid.readlines()
        for i in range(len(lines)-1, -1, -1):
            _rollback_operation(lines[i])
            lines.pop(i)
    finally:
        fid.close()
        if len(lines) < 1:
            os.remove(f)
        else:
            fid = open(f, "w")
            try:
                fid.writelines(lines)
            finally:
                fid.close()

class TransactionLog(object):
    """Naive version of a journal to rollback interrupted install."""
    def __init__(self, journal_filename):
        if os.path.exists(journal_filename):
            raise IOError("file %s already exists" % journal_filename)
        open(journal_filename, "w").close()
        self.f = open(journal_filename, "w")
        self.journal_filename = journal_filename

    def copy(self, source, target, category):
        if os.path.exists(target):
            self.rollback()
            raise ValueError("File %s already exists, rolled back installation" % target)
        d = os.path.dirname(target)
        if not os.path.exists(d):
            self.makedirs(d)
        self.f.write("COPY %s\n" % target)
        shutil.copy(source, target)
        if category == "executables":
            os.chmod(target, MODE_755)

    def makedirs(self, name, mode=MODE_777):
        head, tail = os.path.split(name)
        if not tail:
            head, tail = os.path.split(head)
        if head and tail and not os.path.exists(head):
            try:
                self.makedirs(head, mode)
            except OSError:
                e = extract_exception()
                if e.errno != errno.EEXIST:
                    raise
            if tail == os.curdir:
                return
        self.mkdir(name, mode)

    def mkdir(self, name, mode=MODE_777):
        self.f.write("MKDIR %s\n" % name) 
        self.f.flush()
        os.mkdir(name, mode)

    def close(self):
        if self.f is not None:
            self.f.close()

    def rollback(self):
        self.f.close()
        self.f = open(self.journal_filename, "r")
        lines = self.f.readlines()
        for line in lines[::-1]:
            _rollback_operation(line.strip())
        self.f = None

def copy_installer(source, target, kind):
    dtarget = os.path.dirname(target)
    if not os.path.exists(dtarget):
        os.makedirs(dtarget)
    shutil.copy(source, target)
    if kind == "executables":
        os.chmod(target, MODE_755)

def unix_installer(source, target, kind):
    if kind in ["executables"]:
        mode = "755"
    else:
        mode = "644"
    cmd = ["install", "-m", mode, source, target]
    strcmd = "INSTALL %s -> %s" % (source, target)
    pprint('GREEN', strcmd)
    if not os.path.exists(os.path.dirname(target)):
        os.makedirs(os.path.dirname(target))
    subprocess.check_call(cmd)

class InstallCommand(Command):
    long_descr = """\
Purpose: install the project
Usage:   bentomaker install [OPTIONS]."""
    short_descr = "install the project."
    common_options = Command.common_options + \
                        [Option("-t", "--transaction",
                                help="Do a transaction-based install", action="store_true"),
                         Option("-n", "--dry-run", "--list-files",
                                help="List installed files (do not install anything)",
                                action="store_true", dest="list_files")]
    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return

        n = ctx.build_node.make_node(BUILD_MANIFEST_PATH)
        build_manifest = BuildManifest.from_file(n.abspath())
        scheme = ctx.retrieve_configured_scheme()
        build_manifest.update_paths(scheme)
        node_sections = build_manifest.resolve_paths_with_destdir(ctx.build_node)

        if o.list_files:
            # XXX: this won't take into account action in post install scripts.
            # A better way would be to log install steps and display those, but
            # this will do for now.
            for kind, source, target in iter_files(node_sections):
                print(target.abspath())
            return

        if o.transaction:
            trans = TransactionLog("transaction.log")
            try:
                for kind, source, target in iter_files(node_sections):
                    trans.copy(source.abspath(), target.abspath(), kind)
            finally:
                trans.close()
        else:
            for kind, source, target in iter_files(node_sections):
                copy_installer(source.abspath(), target.abspath(), kind)

########NEW FILE########
__FILENAME__ = mpkg_utils
import os
import sys
import subprocess

import six

import plistlib

from bento.compat.api \
    import \
        check_call

BENTO_INFO = "0.0.5"

MKBOM = "/usr/bin/mkbom"

def _unicode(*args, **kw):
    if six.PY3:
        return str(*args, **kw)
    else:
        return unicode(*args, **kw)

def path_requirement(SpecArgument, Level=six.u('requires'), **kw):
    return dict(
        Level=Level,
        SpecType=six.u('file'),
        SpecArgument=unicode_path(SpecArgument),
        SpecProperty=six.u('NSFileType'),
        TestOperator=six.u('eq'),
        TestObject=six.u('NSFileTypeDirectory'),
        **kw
    )

def common_info(pkg_info):
    # Keys that can appear in any package
    name = _unicode(pkg_info.name)
    major, minor = pkg_info.version_info[0], pkg_info.version_info[1]
    version = pkg_info.version
    defaults = dict(
        CFBundleGetInfoString='%s %s' % (name, version),
        CFBundleIdentifier='org.pythonmac.%s' % (name,),
        CFBundleName=name,
        CFBundleShortVersionString=_unicode(version),
        IFMajorVersion=major,
        IFMinorRevision=minor,
        IFPkgFormatVersion=0.10000000149011612,
        IFRequirementDicts=[path_requirement(six.u('/'))],
        PythonInfoDict=dict(
            PythonLongVersion=_unicode(sys.version),
            PythonShortVersion=_unicode(sys.version[:3]),
            PythonExecutable=_unicode(sys.executable),
            bento_version=dict(
                version=BENTO_INFO
            ),
        ),
    )
    return defaults

def common_description(pkg_info):
    return dict(
        IFPkgDescriptionTitle=_unicode(pkg_info.name),
        IFPkgDescriptionVersion=_unicode(pkg_info.version),
    )

def unicode_path(path, encoding=sys.getfilesystemencoding()):
    if isinstance(path, six.text_type):
        return path
    return _unicode(path, encoding)

def write(dct, path):
    p = plistlib.Plist()
    p.update(dct)
    p.write(path)

def ensure_directories(pkg_info):
    for d in [pkg_info.contents, pkg_info.resources, pkg_info.en_lproj]:
        if not os.path.exists(d):
            os.makedirs(d)

def build_bom(pkg_info):
    check_call([MKBOM, pkg_info.source_root, pkg_info.bom])

def build_archive(pkg_info):
    check_call(["pax", "-w", "-f", pkg_info.archive, "-x", "cpio", "-z", "."],
                          cwd=pkg_info.source_root)

def build_info_plist(pkg_info):
    d = common_info(pkg_info)

    # Keys that can only appear in single packages
    d.update(dict(
        IFPkgFlagAllowBackRev=False,
        IFPkgFlagAuthorizationAction=six.u('AdminAuthorization'),
        IFPkgFlagFollowLinks=True,
        IFPkgFlagInstallFat=False,
        IFPkgFlagIsRequired=False,
        IFPkgFlagOverwritePermissions=False,
        IFPkgFlagRelocatable=False,
        IFPkgFlagRestartAction=six.u('NoRestart'),
        IFPkgFlagRootVolumeOnly=True,
        IFPkgFlagUpdateInstalledLangauges=False,
    ))

    d.update(dict(
        IFPkgFlagAuthorizationAction=pkg_info.auth,
        IFPkgFlagDefaultLocation=unicode_path(pkg_info.prefix),
    ))
    write(d, pkg_info.info_plist)

def build_pkg_info(pkg_info):
    fid = open(pkg_info.pkg_info, "w")
    try:
        fid.write("pmkrpkg1")
    finally:
        fid.close()

def build_description_plist(pkg_info):
    desc = common_description(pkg_info)
    desc['IFPkgDescriptionDescription'] = pkg_info.description
    write(desc, pkg_info.description_plist)

def build_pkg(pkg_info):
    ensure_directories(pkg_info)

    build_bom(pkg_info)
    build_archive(pkg_info)

    build_info_plist(pkg_info)

    build_pkg_info(pkg_info)

    build_description_plist(pkg_info)

class PackageInfo(object):
    def __init__(self, pkg_name, prefix, source_root, pkg_root, admin=True, description=None, version=None):
        if admin:
            self.auth = six.u("AdminAuthorization")
        else:
            self.auth = six.u("RootAuthorization")

        # Where things will be installed by Mac OS X installer
        self.prefix = prefix
        # Root directory for files to be packaged
        self.source_root = source_root
        # Root directory for produced .pkg directory/file
        self.pkg_root = pkg_root

        self.name = pkg_name
        # FIXME: version handling -> use distutils2 version module
        self.version_info = (0, 0, 5, None)
        if version is None:
            self.version = ""
        else:
            self.version = version

        if description:
            self.description = description
        else:
            self.description = ""

        self.contents = os.path.join(self.pkg_root, "Contents")
        self.resources = os.path.join(self.contents, "Resources")
        self.en_lproj = os.path.join(self.resources, "en.lproj")

        self.bom = os.path.join(self.contents, "Archive.bom")
        self.archive = os.path.join(self.contents, "Archive.pax.gz")
        self.info_plist = os.path.join(self.contents, "Info.plist")
        self.pkg_info = os.path.join(self.contents, "PkgInfo")

        self.description_plist = os.path.join(self.en_lproj, "Description.plist")

class MetaPackageInfo(object):
    @classmethod
    def from_build_manifest(cls, build_manifest):
        m = build_manifest.meta
        info_string = "%s %s" % (m["name"], m["version"])
        identifier = "com.github.cournape.bento"
        version_info = (0, 0, 5)
        return cls(m["name"], info_string, version_info, identifier, m["summary"])

    def __init__(self, name, info_string, version_info, identifier, summary):
        self.major, self.minor, self.micro = version_info[0], version_info[1], version_info[2]

        self.info_string = info_string
        self.name = name
        self.identifier = identifier
        self.description = summary

        self.short_version = ".".join([str(i) for i in [self.major, self.minor, self.micro]])

def make_mpkg_plist(mpkg_info, path):
    pl = dict(
            CFBundleGetInfoString=mpkg_info.info_string,
            CFBundleIdentifier=mpkg_info.identifier,
            CFBundleName=mpkg_info.name,
            CFBundleShortVersionString=mpkg_info.short_version,
            IFMajorVersion=mpkg_info.major,
            IFMinorVersion=mpkg_info.minor,
            IFPkgFlagComponentDirectory="Contents/Packages",
            IFPkgFlagPackageList=[
                dict(
                    IFPkgFlagPackageLocation=pkg,
                    IFPkgFlagPackageSelection='selected'
                )
                for pkg in mpkg_info.packages
            ],
            IFPkgFormatVersion=0.10000000149011612,
            IFPkgFlagBackgroundScaling="proportional",
            IFPkgFlagBackgroundAlignment="left",
            IFPkgFlagAuthorizationAction="RootAuthorization",
        )

    write(pl, path)
    return pl

def make_mpkg_description(mpkg_info, path):
    d = dict(IFPkgDescriptionTitle=mpkg_info.name,
             IFPkgDescriptionDescription=mpkg_info.description,
             IFPkgDescriptionVersion=mpkg_info.short_version)
    write(d, path)

########NEW FILE########
__FILENAME__ = msi_utils
import os
import msilib

from distutils import sysconfig
from msilib import schema, sequence, add_data, text

from msilib \
    import \
        Dialog, Directory, Feature

from bento.core.meta \
    import \
        PackageMetadata

ALL_VERSIONS = ['2.0', '2.1', '2.2', '2.3', '2.4',
                '2.5', '2.6', '2.7', '2.8', '2.9',
                '3.0', '3.1', '3.2', '3.3', '3.4',
                '3.5', '3.6', '3.7', '3.8', '3.9']
OTHER_VERSION = 'X'

class PyDialog(Dialog):
    """Dialog class with a fixed layout: controls at the top, then a ruler,
    then a list of buttons: back, next, cancel. Optionally a bitmap at the
    left."""
    def __init__(self, *args, **kw):
        """Dialog(database, name, x, y, w, h, attributes, title, first,
        default, cancel, bitmap=true)"""
        Dialog.__init__(self, *args)
        ruler = self.h - 36
        #if kw.get("bitmap", True):
        #    self.bitmap("Bitmap", 0, 0, bmwidth, ruler, "PythonWin")
        self.line("BottomLine", 0, ruler, self.w, 0)

    def title(self, title):
        "Set the title text of the dialog at the top."
        # name, x, y, w, h, flags=Visible|Enabled|Transparent|NoPrefix,
        # text, in VerdanaBold10
        self.text("Title", 15, 10, 320, 60, 0x30003,
                  r"{\VerdanaBold10}%s" % title)

    def back(self, title, next, name = "Back", active = 1):
        """Add a back button with a given title, the tab-next button,
        its name in the Control table, possibly initially disabled.

        Return the button, so that events can be associated"""
        if active:
            flags = 3 # Visible|Enabled
        else:
            flags = 1 # Visible
        return self.pushbutton(name, 180, self.h-27 , 56, 17, flags, title, next)

    def cancel(self, title, next, name = "Cancel", active = 1):
        """Add a cancel button with a given title, the tab-next button,
        its name in the Control table, possibly initially disabled.

        Return the button, so that events can be associated"""
        if active:
            flags = 3 # Visible|Enabled
        else:
            flags = 1 # Visible
        return self.pushbutton(name, 304, self.h-27, 56, 17, flags, title, next)

    def next(self, title, next, name = "Next", active = 1):
        """Add a Next button with a given title, the tab-next button,
        its name in the Control table, possibly initially disabled.

        Return the button, so that events can be associated"""
        if active:
            flags = 3 # Visible|Enabled
        else:
            flags = 1 # Visible
        return self.pushbutton(name, 236, self.h-27, 56, 17, flags, title, next)

    def xbutton(self, name, title, next, xpos):
        """Add a button with a given title, the tab-next button,
        its name in the Control table, giving its x position; the
        y-position is aligned with the other buttons.

        Return the button, so that events can be associated"""
        return self.pushbutton(name, int(self.w*xpos - 28), self.h-27, 56, 17, 3, title, next)

def add_find_python(db, versions):
    """Adds code to the installer to compute the location of Python.

    Properties PYTHON.MACHINE.X.Y and PYTHON.USER.X.Y will be set from the
    registry for each version of Python.

    Properties TARGETDIRX.Y will be set from PYTHON.USER.X.Y if defined,
    else from PYTHON.MACHINE.X.Y.

    Properties PYTHONX.Y will be set to TARGETDIRX.Y\\python.exe"""

    start = 402
    for ver in versions:
        install_path = r"SOFTWARE\Python\PythonCore\%s\InstallPath" % ver
        machine_reg = "python.machine." + ver
        user_reg = "python.user." + ver
        machine_prop = "PYTHON.MACHINE." + ver
        user_prop = "PYTHON.USER." + ver
        machine_action = "PythonFromMachine" + ver
        user_action = "PythonFromUser" + ver
        exe_action = "PythonExe" + ver
        target_dir_prop = "TARGETDIR" + ver
        exe_prop = "PYTHON" + ver
        if msilib.Win64:
            # type: msidbLocatorTypeRawValue + msidbLocatorType64bit
            Type = 2+16
        else:
            Type = 2
        add_data(db, "RegLocator",
                [(machine_reg, 2, install_path, None, Type),
                 (user_reg, 1, install_path, None, Type)])
        add_data(db, "AppSearch",
                [(machine_prop, machine_reg),
                 (user_prop, user_reg)])
        add_data(db, "CustomAction",
                [(machine_action, 51+256, target_dir_prop, "[" + machine_prop + "]"),
                 (user_action, 51+256, target_dir_prop, "[" + user_prop + "]"),
                 (exe_action, 51+256, exe_prop, "[" + target_dir_prop + "]\\python.exe"),
                ])
        add_data(db, "InstallExecuteSequence",
                [(machine_action, machine_prop, start),
                 (user_action, user_prop, start + 1),
                 (exe_action, None, start + 2),
                ])
        add_data(db, "InstallUISequence",
                [(machine_action, machine_prop, start),
                 (user_action, user_prop, start + 1),
                 (exe_action, None, start + 2),
                ])
        add_data(db, "Condition",
                [("Python" + ver, 0, "NOT TARGETDIR" + ver)])
        start += 4
        assert start < 500

def add_files(db, msi_node, versions, other_version, install_script=None):
    if install_script is not None:
        raise NotImplementedError("Support for msi install script not yet implemented")
    cab = msilib.CAB("distfiles")
    rootdir = msi_node.abspath()

    root = Directory(db, cab, None, rootdir, "TARGETDIR", "SourceDir")
    f = Feature(db, "Python", "Python", "Everything",
                0, 1, directory="TARGETDIR")

    items = [(f, root, '')]
    for version in versions + [other_version]:
        target = "TARGETDIR" + version
        name = default = "Python" + version
        desc = "Everything"
        if version is other_version:
            title = "Python from another location"
            level = 2
        else:
            title = "Python %s from registry" % version
            level = 1
        f = Feature(db, name, title, desc, 1, level, directory=target)
        dir = Directory(db, cab, root, rootdir, target, default)
        items.append((f, dir, version))
    db.Commit()

    seen = {}
    for feature, dir, version in items:
        todo = [dir]
        while todo:
            dir = todo.pop()
            for file in os.listdir(dir.absolute):
                afile = os.path.join(dir.absolute, file)
                if os.path.isdir(afile):
                    short = "%s|%s" % (dir.make_short(file), file)
                    default = file + version
                    newdir = Directory(db, cab, dir, file, default, short)
                    todo.append(newdir)
                else:
                    if not dir.component:
                        dir.start_component(dir.logical, feature, 0)
                    if afile not in seen:
                        key = seen[afile] = dir.add_file(file)
                        if file==install_script:
                            if install_script_key:
                                raise DistutilsOptionError(
                                      "Multiple files with name %s" % file)
                            install_script_key = '[#%s]' % key
                    else:
                        key = seen[afile]
                        add_data(db, "DuplicateFile",
                            [(key + version, dir.component, key, None, dir.logical)])
        db.Commit()
    cab.commit(db)

def add_scripts(db, install_scripts=None, pre_install_scripts=None):
    if install_scripts:
        raise NotImplementedError("install scripts not yet supported")

    if pre_install_scripts:
        raise NotImplementedError("pre install scripts not yet supported")

def add_ui(db, fullname, versions, other_version):
    x = y = 50
    w = 370
    h = 300
    title = "[ProductName] Setup"

    # see "Dialog Style Bits"
    modal = 3      # visible | modal
    modeless = 1   # visible

    # UI customization properties
    add_data(db, "Property",
             # See "DefaultUIFont Property"
             [("DefaultUIFont", "DlgFont8"),
              # See "ErrorDialog Style Bit"
              ("ErrorDialog", "ErrorDlg"),
              ("Progress1", "Install"),   # modified in maintenance type dlg
              ("Progress2", "installs"),
              ("MaintenanceForm_Action", "Repair"),
              # possible values: ALL, JUSTME
              ("WhichUsers", "ALL")
             ])

    # Fonts, see "TextStyle Table"
    add_data(db, "TextStyle",
             [("DlgFont8", "Tahoma", 9, None, 0),
              ("DlgFontBold8", "Tahoma", 8, None, 1), #bold
              ("VerdanaBold10", "Verdana", 10, None, 1),
              ("VerdanaRed9", "Verdana", 9, 255, 0),
             ])

    # UI Sequences, see "InstallUISequence Table", "Using a Sequence Table"
    # Numbers indicate sequence; see sequence.py for how these action integrate
    add_data(db, "InstallUISequence",
             [("PrepareDlg", "Not Privileged or Windows9x or Installed", 140),
              ("WhichUsersDlg", "Privileged and not Windows9x and not Installed", 141),
              # In the user interface, assume all-users installation if privileged.
              ("SelectFeaturesDlg", "Not Installed", 1230),
              # XXX no support for resume installations yet
              #("ResumeDlg", "Installed AND (RESUME OR Preselected)", 1240),
              ("MaintenanceTypeDlg", "Installed AND NOT RESUME AND NOT Preselected", 1250),
              ("ProgressDlg", None, 1280)])

    add_data(db, 'ActionText', text.ActionText)
    add_data(db, 'UIText', text.UIText)
    #####################################################################
    # Standard dialogs: FatalError, UserExit, ExitDialog
    fatal=PyDialog(db, "FatalError", x, y, w, h, modal, title,
                 "Finish", "Finish", "Finish")
    fatal.title("[ProductName] Installer ended prematurely")
    fatal.back("< Back", "Finish", active = 0)
    fatal.cancel("Cancel", "Back", active = 0)
    fatal.text("Description1", 15, 70, 320, 80, 0x30003,
               "[ProductName] setup ended prematurely because of an error.  Your system has not been modified.  To install this program at a later time, please run the installation again.")
    fatal.text("Description2", 15, 155, 320, 20, 0x30003,
               "Click the Finish button to exit the Installer.")
    c=fatal.next("Finish", "Cancel", name="Finish")
    c.event("EndDialog", "Exit")

    user_exit = PyDialog(db, "UserExit", x, y, w, h, modal, title,
                 "Finish", "Finish", "Finish")
    user_exit.title("[ProductName] Installer was interrupted")
    user_exit.back("< Back", "Finish", active = 0)
    user_exit.cancel("Cancel", "Back", active = 0)
    user_exit.text("Description1", 15, 70, 320, 80, 0x30003,
               "[ProductName] setup was interrupted.  Your system has not been modified.  "
               "To install this program at a later time, please run the installation again.")
    user_exit.text("Description2", 15, 155, 320, 20, 0x30003,
               "Click the Finish button to exit the Installer.")
    c = user_exit.next("Finish", "Cancel", name="Finish")
    c.event("EndDialog", "Exit")

    exit_dialog = PyDialog(db, "ExitDialog", x, y, w, h, modal, title,
                         "Finish", "Finish", "Finish")
    exit_dialog.title("Completing the [ProductName] Installer")
    exit_dialog.back("< Back", "Finish", active = 0)
    exit_dialog.cancel("Cancel", "Back", active = 0)
    exit_dialog.text("Description", 15, 235, 320, 20, 0x30003,
               "Click the Finish button to exit the Installer.")
    c = exit_dialog.next("Finish", "Cancel", name="Finish")
    c.event("EndDialog", "Return")

    #####################################################################
    # Required dialog: FilesInUse, ErrorDlg
    inuse = PyDialog(db, "FilesInUse",
                     x, y, w, h,
                     19,                # KeepModeless|Modal|Visible
                     title,
                     "Retry", "Retry", "Retry", bitmap=False)
    inuse.text("Title", 15, 6, 200, 15, 0x30003,
               r"{\DlgFontBold8}Files in Use")
    inuse.text("Description", 20, 23, 280, 20, 0x30003,
           "Some files that need to be updated are currently in use.")
    inuse.text("Text", 20, 55, 330, 50, 3,
               "The following applications are using files that need to be updated by this setup. Close these applications and then click Retry to continue the installation or Cancel to exit it.")
    inuse.control("List", "ListBox", 20, 107, 330, 130, 7, "FileInUseProcess",
                  None, None, None)
    c=inuse.back("Exit", "Ignore", name="Exit")
    c.event("EndDialog", "Exit")
    c=inuse.next("Ignore", "Retry", name="Ignore")
    c.event("EndDialog", "Ignore")
    c=inuse.cancel("Retry", "Exit", name="Retry")
    c.event("EndDialog","Retry")

    # See "Error Dialog". See "ICE20" for the required names of the controls.
    error = Dialog(db, "ErrorDlg",
                   50, 10, 330, 101,
                   65543,       # Error|Minimize|Modal|Visible
                   title,
                   "ErrorText", None, None)
    error.text("ErrorText", 50,9,280,48,3, "")
    #error.control("ErrorIcon", "Icon", 15, 9, 24, 24, 5242881, None, "py.ico", None, None)
    error.pushbutton("N",120,72,81,21,3,"No",None).event("EndDialog","ErrorNo")
    error.pushbutton("Y",240,72,81,21,3,"Yes",None).event("EndDialog","ErrorYes")
    error.pushbutton("A",0,72,81,21,3,"Abort",None).event("EndDialog","ErrorAbort")
    error.pushbutton("C",42,72,81,21,3,"Cancel",None).event("EndDialog","ErrorCancel")
    error.pushbutton("I",81,72,81,21,3,"Ignore",None).event("EndDialog","ErrorIgnore")
    error.pushbutton("O",159,72,81,21,3,"Ok",None).event("EndDialog","ErrorOk")
    error.pushbutton("R",198,72,81,21,3,"Retry",None).event("EndDialog","ErrorRetry")

    #####################################################################
    # Global "Query Cancel" dialog
    cancel = Dialog(db, "CancelDlg", 50, 10, 260, 85, 3, title,
                    "No", "No", "No")
    cancel.text("Text", 48, 15, 194, 30, 3,
                "Are you sure you want to cancel [ProductName] installation?")
    #cancel.control("Icon", "Icon", 15, 15, 24, 24, 5242881, None,
    #               "py.ico", None, None)
    c=cancel.pushbutton("Yes", 72, 57, 56, 17, 3, "Yes", "No")
    c.event("EndDialog", "Exit")

    c=cancel.pushbutton("No", 132, 57, 56, 17, 3, "No", "Yes")
    c.event("EndDialog", "Return")

    #####################################################################
    # Global "Wait for costing" dialog
    costing = Dialog(db, "WaitForCostingDlg", 50, 10, 260, 85, modal, title,
                     "Return", "Return", "Return")
    costing.text("Text", 48, 15, 194, 30, 3,
                 "Please wait while the installer finishes determining your disk space requirements.")
    c = costing.pushbutton("Return", 102, 57, 56, 17, 3, "Return", None)
    c.event("EndDialog", "Exit")

    #####################################################################
    # Preparation dialog: no user input except cancellation
    prep = PyDialog(db, "PrepareDlg", x, y, w, h, modeless, title,
                    "Cancel", "Cancel", "Cancel")
    prep.text("Description", 15, 70, 320, 40, 0x30003,
              "Please wait while the Installer prepares to guide you through the installation.")
    prep.title("Welcome to the [ProductName] Installer")
    c=prep.text("ActionText", 15, 110, 320, 20, 0x30003, "Pondering...")
    c.mapping("ActionText", "Text")
    c=prep.text("ActionData", 15, 135, 320, 30, 0x30003, None)
    c.mapping("ActionData", "Text")
    prep.back("Back", None, active=0)
    prep.next("Next", None, active=0)
    c=prep.cancel("Cancel", None)
    c.event("SpawnDialog", "CancelDlg")

    #####################################################################
    # Feature (Python directory) selection
    seldlg = PyDialog(db, "SelectFeaturesDlg", x, y, w, h, modal, title,
                    "Next", "Next", "Cancel")
    seldlg.title("Select Python Installations")

    seldlg.text("Hint", 15, 30, 300, 20, 3,
                "Select the Python locations where %s should be installed."
                % fullname)

    seldlg.back("< Back", None, active=0)
    c = seldlg.next("Next >", "Cancel")
    order = 1
    c.event("[TARGETDIR]", "[SourceDir]", ordering=order)
    for version in versions + [other_version]:
        order += 1
        c.event("[TARGETDIR]", "[TARGETDIR%s]" % version,
                "FEATURE_SELECTED AND &Python%s=3" % version,
                ordering=order)
    c.event("SpawnWaitDialog", "WaitForCostingDlg", ordering=order + 1)
    c.event("EndDialog", "Return", ordering=order + 2)
    c = seldlg.cancel("Cancel", "Features")
    c.event("SpawnDialog", "CancelDlg")

    c = seldlg.control("Features", "SelectionTree", 15, 60, 300, 120, 3,
                       "FEATURE", None, "PathEdit", None)
    c.event("[FEATURE_SELECTED]", "1")
    ver = other_version
    install_other_cond = "FEATURE_SELECTED AND &Python%s=3" % ver
    dont_install_other_cond = "FEATURE_SELECTED AND &Python%s<>3" % ver

    c = seldlg.text("Other", 15, 200, 300, 15, 3,
                    "Provide an alternate Python location")
    c.condition("Enable", install_other_cond)
    c.condition("Show", install_other_cond)
    c.condition("Disable", dont_install_other_cond)
    c.condition("Hide", dont_install_other_cond)

    c = seldlg.control("PathEdit", "PathEdit", 15, 215, 300, 16, 1,
                       "TARGETDIR" + ver, None, "Next", None)
    c.condition("Enable", install_other_cond)
    c.condition("Show", install_other_cond)
    c.condition("Disable", dont_install_other_cond)
    c.condition("Hide", dont_install_other_cond)

    #####################################################################
    # Disk cost
    cost = PyDialog(db, "DiskCostDlg", x, y, w, h, modal, title,
                    "OK", "OK", "OK", bitmap=False)
    cost.text("Title", 15, 6, 200, 15, 0x30003,
              "{\DlgFontBold8}Disk Space Requirements")
    cost.text("Description", 20, 20, 280, 20, 0x30003,
              "The disk space required for the installation of the selected features.")
    cost.text("Text", 20, 53, 330, 60, 3,
              "The highlighted volumes (if any) do not have enough disk space "
          "available for the currently selected features.  You can either "
          "remove some files from the highlighted volumes, or choose to "
          "install less features onto local drive(s), or select different "
          "destination drive(s).")
    cost.control("VolumeList", "VolumeCostList", 20, 100, 330, 150, 393223,
                 None, "{120}{70}{70}{70}{70}", None, None)
    cost.xbutton("OK", "Ok", None, 0.5).event("EndDialog", "Return")

    #####################################################################
    # WhichUsers Dialog. Only available on NT, and for privileged users.
    # This must be run before FindRelatedProducts, because that will
    # take into account whether the previous installation was per-user
    # or per-machine. We currently don't support going back to this
    # dialog after "Next" was selected; to support this, we would need to
    # find how to reset the ALLUSERS property, and how to re-run
    # FindRelatedProducts.
    # On Windows9x, the ALLUSERS property is ignored on the command line
    # and in the Property table, but installer fails according to the documentation
    # if a dialog attempts to set ALLUSERS.
    whichusers = PyDialog(db, "WhichUsersDlg", x, y, w, h, modal, title,
                        "AdminInstall", "Next", "Cancel")
    whichusers.title("Select whether to install [ProductName] for all users of this computer.")
    # A radio group with two options: allusers, justme
    g = whichusers.radiogroup("AdminInstall", 15, 60, 260, 50, 3,
                              "WhichUsers", "", "Next")
    g.add("ALL", 0, 5, 150, 20, "Install for all users")
    g.add("JUSTME", 0, 25, 150, 20, "Install just for me")

    whichusers.back("Back", None, active=0)

    c = whichusers.next("Next >", "Cancel")
    c.event("[ALLUSERS]", "1", 'WhichUsers="ALL"', 1)
    c.event("EndDialog", "Return", ordering = 2)

    c = whichusers.cancel("Cancel", "AdminInstall")
    c.event("SpawnDialog", "CancelDlg")

    #####################################################################
    # Installation Progress dialog (modeless)
    progress = PyDialog(db, "ProgressDlg", x, y, w, h, modeless, title,
                        "Cancel", "Cancel", "Cancel", bitmap=False)
    progress.text("Title", 20, 15, 200, 15, 0x30003,
                  "{\DlgFontBold8}[Progress1] [ProductName]")
    progress.text("Text", 35, 65, 300, 30, 3,
                  "Please wait while the Installer [Progress2] [ProductName]. "
                  "This may take several minutes.")
    progress.text("StatusLabel", 35, 100, 35, 20, 3, "Status:")

    c=progress.text("ActionText", 70, 100, w-70, 20, 3, "Pondering...")
    c.mapping("ActionText", "Text")

    #c=progress.text("ActionData", 35, 140, 300, 20, 3, None)
    #c.mapping("ActionData", "Text")

    c=progress.control("ProgressBar", "ProgressBar", 35, 120, 300, 10, 65537,
                       None, "Progress done", None, None)
    c.mapping("SetProgress", "Progress")

    progress.back("< Back", "Next", active=False)
    progress.next("Next >", "Cancel", active=False)
    progress.cancel("Cancel", "Back").event("SpawnDialog", "CancelDlg")

    ###################################################################
    # Maintenance type: repair/uninstall
    maint = PyDialog(db, "MaintenanceTypeDlg", x, y, w, h, modal, title,
                     "Next", "Next", "Cancel")
    maint.title("Welcome to the [ProductName] Setup Wizard")
    maint.text("BodyText", 15, 63, 330, 42, 3,
               "Select whether you want to repair or remove [ProductName].")
    g=maint.radiogroup("RepairRadioGroup", 15, 108, 330, 60, 3,
                        "MaintenanceForm_Action", "", "Next")
    #g.add("Change", 0, 0, 200, 17, "&Change [ProductName]")
    g.add("Repair", 0, 18, 200, 17, "&Repair [ProductName]")
    g.add("Remove", 0, 36, 200, 17, "Re&move [ProductName]")

    maint.back("< Back", None, active=False)
    c=maint.next("Finish", "Cancel")
    # Change installation: Change progress dialog to "Change", then ask
    # for feature selection
    #c.event("[Progress1]", "Change", 'MaintenanceForm_Action="Change"', 1)
    #c.event("[Progress2]", "changes", 'MaintenanceForm_Action="Change"', 2)

    # Reinstall: Change progress dialog to "Repair", then invoke reinstall
    # Also set list of reinstalled features to "ALL"
    c.event("[REINSTALL]", "ALL", 'MaintenanceForm_Action="Repair"', 5)
    c.event("[Progress1]", "Repairing", 'MaintenanceForm_Action="Repair"', 6)
    c.event("[Progress2]", "repairs", 'MaintenanceForm_Action="Repair"', 7)
    c.event("Reinstall", "ALL", 'MaintenanceForm_Action="Repair"', 8)

    # Uninstall: Change progress to "Remove", then invoke uninstall
    # Also set list of removed features to "ALL"
    c.event("[REMOVE]", "ALL", 'MaintenanceForm_Action="Remove"', 11)
    c.event("[Progress1]", "Removing", 'MaintenanceForm_Action="Remove"', 12)
    c.event("[Progress2]", "removes", 'MaintenanceForm_Action="Remove"', 13)
    c.event("Remove", "ALL", 'MaintenanceForm_Action="Remove"', 14)

    # Close dialog when maintenance action scheduled
    c.event("EndDialog", "Return", 'MaintenanceForm_Action<>"Change"', 20)
    #c.event("NewDialog", "SelectFeaturesDlg", 'MaintenanceForm_Action="Change"', 21)

    maint.cancel("Cancel", "RepairRadioGroup").event("SpawnDialog", "CancelDlg")

def create_msi_installer(package, run_node, msi_root_node, installer_name=None, output_dir="dist"):
    meta = PackageMetadata.from_package(package)

    string_version = "%d.%d.%d" % (meta.version_major, meta.version_minor, meta.version_micro)

    fullname = "%s-%s" % (package.name, string_version)
    if installer_name is None:
        installer_name = "%s-%s.msi" % (package.name, string_version)
    parent_node = run_node.make_node(output_dir)
    if parent_node is None:
        raise IOError()
    installer_node = parent_node.make_node(installer_name)
    installer_name = installer_node.abspath()
    installer_node.parent.mkdir()

    author = meta.author

    short_version = sysconfig.get_python_version()

    has_ext_modules = True
    if has_ext_modules:
        target_version = short_version
    else:
        target_version = None

    if target_version:
        product_name = "Python %s %s" % (target_version, meta.fullname)
    else:
        product_name = "Python %s" % meta.fullname

    if target_version:
        versions = [target_version]
    else:
        versions = list(ALL_VERSIONS)

    db = msilib.init_database(installer_name, schema, product_name, msilib.gen_uuid(), string_version, author)
    msilib.add_tables(db, sequence)

    props = [('DistVersion', meta.version)]
    email = meta.author_email or meta.maintainer_email
    if email:
        props.append(("ARPCONTACT", email))
    if meta.url:
        props.append(("ARPURLINFOABOUT", meta.url))
    if props:
        add_data(db, 'Property', props)

    add_find_python(db, versions)
    add_files(db, msi_root_node, versions, OTHER_VERSION)
    add_scripts(db)
    add_ui(db, fullname, versions, OTHER_VERSION)
    db.Commit()

########NEW FILE########
__FILENAME__ = options
"""Option handling module.

This module is concerned with options/argument parsing from both
bento and bento subcommands, as well as help message
formatting."""
import optparse

from optparse \
    import \
        Option

class IndentedHelpFormatter(optparse.IndentedHelpFormatter):
    def format_usage(self, usage):
        return optparse._("%s\n" % usage)

# Goal of separate options context:
#   - separate options handling from commands themselves (simplify commands)
#   - easier to add options from hook files
#   - should help hiding command implementation detail from hooks and high
#   level tools such as bentomaker (close coupling at the moment)
class OptionsContext(object):
    @classmethod
    def from_command(cls, cmd):
        usage = cmd.long_descr
        ret = cls(usage=usage)
        for o in cmd.common_options:
            ret.add_option(o)
        return ret

    def __init__(self, usage=None):
        kw = {"add_help_option": False, "formatter": IndentedHelpFormatter()}
        if usage is not None:
            kw["usage"] = usage
        self.parser = optparse.OptionParser(**kw)
        self._groups = {}
        self._is_setup = False

    def setup(self, package_options):
        self.add_group("build_customization", "Build customization")
        opt = optparse.Option("--use-distutils", help="Build extensions with distutils",
                              action="store_true")
        self.add_option(opt, "build_customization")

        self._is_setup = True

    def add_option(self, option, group=None):
        if group is None:
            self.parser.add_option(option)
        else:
            if group in self._groups:
                self._groups[group].add_option(option)
            else:
                raise ValueError("Unknown option group %r" % group)

    def has_group(self, group):
        return group in self._groups

    def add_group(self, name, title):
        grp = optparse.OptionGroup(self.parser, title)
        self._groups[name] = grp
        self.parser.add_option_group(grp)

########NEW FILE########
__FILENAME__ = parse
import os

from pprint import \
        pprint

from bento.errors \
    import \
        UsageException, ParseError
from bento.commands.core import \
        Command, Option
from bento.parser.misc \
    import \
        build_ast_from_data
from bento.utils.utils \
    import \
        extract_exception

class ParseCommand(Command):
    long_descr = """\
Purpose: query the given package description file (debugging tool)
Usage:   bentomaker parse [OPTIONS]"""
    short_descr = "parse the package description file."
    common_options = Command.common_options + [
        Option("-f", "--flags", action="store_true",
               help="print flags variables"),
        Option("-p", "--path", action="store_true",
               help="print paths variables"),
        Option("-m", "--meta-field", dest="meta_field",
               help="print given meta field")]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return

        if len(a) < 1:
            filename = "bento.info"
        else:
            filename = a[0]

        if not os.path.exists(filename):
            raise UsageException("%s: error: file %s does not exist" % "bentomaker")

        f = open(filename, "r")
        try:
            data = f.read()
            try:
                parsed = build_ast_from_data(data)
            except ParseError:
                e = extract_exception()
                msg = "Error while parsing file %s\n" % filename
                e.args = (msg,) +  e.args
                raise e
            if o.flags:
                try:
                    flags = parsed["flags"]
                    for flag in flags:
                        print(flags[flag])
                except KeyError:
                    pass
            elif o.path:
                try:
                    paths = parsed["paths"]
                    for path in paths:
                        print(paths[path])
                except KeyError:
                    pass
            elif o.meta_field:
                try:
                    print(parsed[o.meta_field])
                except KeyError:
                    raise ValueError("Field %s not found in metadata" % o.meta_field)
            else:
                pprint(parsed)
        finally:
            f.close()


########NEW FILE########
__FILENAME__ = register
import os.path as op

import bento.errors

from bento.commands.core \
    import \
        Command, Option
from bento.pypi.register_utils \
    import \
        build_post_data, post_to_server, DEFAULT_REPOSITORY, read_pypirc, PyPIConfig

from six \
    import \
        PY3

if PY3:
    from urllib.request \
        import \
            HTTPPasswordMgr, urlparse
else:
    from urllib2 \
        import \
            HTTPPasswordMgr
    from urlparse \
        import \
            urlparse

def _read_pypirc(repository):
    filename = op.join(op.expanduser("~"), ".pypirc")
    if not op.exists(filename):
        raise bento.errors.UsageException(
                "file %r not found (automatic creation of .pypirc not supported yet" % filename)
    try:
        return read_pypirc(repository)
    except bento.errors.InvalidPyPIConfig:
        raise bento.errors.UsageException("repository %r not found in %r" % (repository, filename))

class RegisterPyPI(Command):
    long_descr = """\
Purpose: register the package to pypi
Usage: bentomaker register [OPTIONS]"""
    short_descr = "register packages to pypi."
    common_options = Command.common_options \
                        + [Option("-r", "--repository",
                                  help="Repository to use in .pypirc"),
                           Option("-u", "--username",
                                  help="Username to use for registration"),
                           Option("-p", "--password",
                                  help="Password to use for registration"),
                           Option( "--repository-url",
                                  help="Repository URL to use for registration"),
                                  ]
    def run(self, context):
        o, a = context.get_parsed_arguments()
        if o.repository and (o.username or o.password or o.repository_url):
            raise bento.errors.UsageException(
                    "Cannot specify repository and username/password/url at the same time")
        if not (o.repository or (o.username or o.password or o.repository_url)):
            # FIXME: why does distutils use DEFAULT_REPOSITORY (i.e. an url)
            # here ?
            config = _read_pypirc(DEFAULT_REPOSITORY)
        elif o.repository:
            config = _read_pypirc(o.repository)
        else:
            config = PyPIConfig(o.username, o.password, o.repository_url)

        auth = HTTPPasswordMgr()
        host = urlparse(config.repository)[1]
        auth.add_password(config.realm, host, config.username, config.password)

        post_data = build_post_data(context.pkg, "submit")
        code, msg = post_to_server(post_data, config, auth)
        if code != 200:
            raise bento.errors.BentoError("Error while submitting package metadata to server: %r" % msg)

########NEW FILE########
__FILENAME__ = registries
from bento.compat.api \
    import \
        defaultdict

class CommandRegistry(object):
    def __init__(self):
        # command line name -> command class
        self._klasses = {}
        # command line name -> None for private commands
        self._privates = {}

    def register(self, name, cmd_klass, public=True):
        if name in self._klasses:
            raise ValueError("context for command %r already registered !" % name)
        else:
            self._klasses[name] = cmd_klass
            if not public:
                self._privates[name] = None

    def retrieve(self, name):
        cmd_klass = self._klasses.get(name, None)
        if cmd_klass is None:
            raise ValueError("No command class registered for name %r" % name)
        else:
            return cmd_klass

    def is_registered(self, name):
        return name in self._klasses

    def command_names(self):
        return self._klasses.keys()

    def public_command_names(self):
        return [k for k in self._klasses.keys() if not k in self._privates]

class ContextRegistry(object):
    def __init__(self, default=None):
        self._contexts = {}
        self.set_default(default)

    def set_default(self, default):
        self._default = default

    def is_registered(self, cmd_name):
        return cmd_name in self._contexts

    def register(self, cmd_name, context):
        if cmd_name in self._contexts:
            raise ValueError("context for command %r already registered !" % cmd_name)
        else:
            self._contexts[cmd_name] = context

    def retrieve(self, cmd_name):
        context = self._contexts.get(cmd_name, None)
        if context is None:
            if self._default is None:
                raise ValueError("No context registered for command %r" % cmd_name)
            else:
                return self._default
        else:
            return context

class OptionsRegistry(object):
    """Registry for command -> option context"""
    def __init__(self):
        # command line name -> context *instance*
        self._contexts = {}

    def register(self, cmd_name, options_context):
        if cmd_name in self._contexts:
            raise ValueError("options context for command %r already registered !" % cmd_name)
        else:
            self._contexts[cmd_name] = options_context

    def is_registered(self, cmd_name):
        return cmd_name in self._contexts

    def retrieve(self, cmd_name):
        options_context = self._contexts.get(cmd_name, None)
        if options_context is None:
            raise ValueError("No options context registered for cmd_name %r" % cmd_name)
        else:
            return options_context

class _Dummy(object):
    pass

class _RegistryBase(object):
    """A simple registry of sets of callbacks, one set per category."""
    def __init__(self):
        self._callbacks = {}
        self.categories = _Dummy()

    def register_category(self, category, default_builder):
        if category in self._callbacks:
            raise ValueError("Category %r already registered" % category)
        else:
            self._callbacks[category] = defaultdict(lambda: default_builder)
            setattr(self.categories, category, _Dummy())

    def register_callback(self, category, name, builder):
        c = self._callbacks.get(category, None)
        if c is not None:
            c[name] = builder
            cat = getattr(self.categories, category)
            setattr(cat, name, builder)
        else:
            raise ValueError("category %s is not registered yet" % category)

    def callback(self, category, name):
        if not category in self._callbacks:
            raise ValueError("Unregistered category %r" % category)
        else:
            return self._callbacks[category][name]

    def default_callback(self, category, *a, **kw):
        if not category in self._callbacks:
            raise ValueError("Unregistered category %r" % category)
        else:
            return self._callbacks[category].default_factory()(*a, **kw)

class BuilderRegistry(_RegistryBase):
    builder = _RegistryBase.callback

class ISectionRegistry(_RegistryBase):
    registrer = _RegistryBase.callback

class OutputRegistry(object):
    def __init__(self, categories=None):
        self.categories = {}
        self.installed_categories = {}
        if categories:
            for category, installed_category in categories:
                self.register_category(category, installed_category)

    def register_category(self, category, installed_category):
        if category in self.categories:
            raise ValueError("Category %r already registered")
        else:
            self.categories[category] = {}
            self.installed_categories[category] = installed_category

    def register_outputs(self, category, name, nodes, from_node, target_dir):
        if not category in self.categories:
            raise ValueError("Unknown category %r" % category)
        else:
            cat = self.categories[category]
            if name in cat:
                raise ValueError("Outputs for categoryr=%r and name=%r already registered" % (category, name))
            else:
                cat[name] = (nodes, from_node, target_dir)

    def iter_category(self, category):
        if not category in self.categories:
            raise ValueError("Unknown category %r" % category)
        else:
            for k, v in self.categories[category].items():
                yield k, v[0], v[1], v[2]

    def iter_over_category(self):
        for category in self.categories:
            for name, nodes, from_node, target_dir in self.iter_category(category):
                yield category, name, nodes, from_node, target_dir


########NEW FILE########
__FILENAME__ = script_utils
import os
import sys
import re

from bento._config \
    import \
        _CLI

import six

SYS_EXECUTABLE = os.path.normpath(sys.executable)

SCRIPT_TEXT = """\
# BENTO AUTOGENERATED-CONSOLE SCRIPT
if __name__ == '__main__':
    import sys
    from %(module)s import %(function)s
    sys.exit(%(function)s())
"""

_LAUNCHER_MANIFEST = """
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<assembly xmlns="urn:schemas-microsoft-com:asm.v1" manifestVersion="1.0">
 <assemblyIdentity version="1.0.0.0"
 processorArchitecture="X86"
 name="%s.exe"
 type="win32"/>

 <!-- Identify the application security requirements. -->
 <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3">
 <security>
 <requestedPrivileges>
 <requestedExecutionLevel level="asInvoker" uiAccess="false"/>
 </requestedPrivileges>
 </security>
 </trustInfo>
</assembly>"""

# XXX: taken from setuptools, audit this
def nt_quote_arg(arg):
    """Quote a command line argument according to Windows parsing
    rules"""

    result = []
    needquote = False
    nb = 0

    needquote = (" " in arg) or ("\t" in arg)
    if needquote:
        result.append('"')

    for c in arg:
        if c == '\\':
            nb += 1
        elif c == '"':
            # double preceding backslashes, then add a \"
            result.append('\\' * (nb*2) + '\\"')
            nb = 0
        else:
            if nb:
                result.append('\\' * nb)
                nb = 0
            result.append(c)

    if nb:
        result.append('\\' * nb)

    if needquote:
        result.append('\\' * nb)    # double the trailing backslashes
        result.append('"')

    return ''.join(result)

# XXX: taken verbatim from setuptools, rewrite this crap
def get_script_header(executable=SYS_EXECUTABLE, wininst=False):
    from distutils.command.build_scripts import first_line_re

    match = first_line_re.match(six.b(""))

    options = ''
    if match:
        options = match.group(1) or ''
        if options:
            options = ' ' + options
    if wininst:
        executable = "python.exe"
    else:
        executable = nt_quote_arg(executable)

    hdr = "#!%(executable)s%(options)s\n" % locals()
    if six.u(hdr).encode('ascii') != hdr:
        # Non-ascii path to sys.executable, use -x to prevent warnings
        if options:
            if options.strip().startswith('-'):
                options = ' -x' + options.strip()[1:]
            # else: punt, we can't do it, let the warning happen anyway
        else:
            options = ' -x'
    #executable = fix_jython_executable(executable, options)
    hdr = "#!%(executable)s%(options)s\n" % locals()
    return hdr

def create_scripts(executables, bdir):
    ret = {}

    for name, executable in executables.items():
        if sys.platform == "win32":
            ret[name] = create_win32_script(name, executable, bdir)
        else:
            ret[name] = create_posix_script(name, executable, bdir)
    return ret

def create_win32_script(name, executable, scripts_node):
    script_text = SCRIPT_TEXT % {"python_exec": SYS_EXECUTABLE,
            "module": executable.module,
            "function": executable.function}

    wininst = False
    header = get_script_header(SYS_EXECUTABLE, wininst)

    ext = '-script.py'
    launcher = _CLI

    new_header = re.sub('(?i)pythonw.exe', 'python.exe', header)

    if os.path.exists(new_header[2:-1]) or sys.platform != 'win32':
        hdr = new_header
    else:
        hdr = header

    fid = open(launcher, "rb")
    try:
        cnt = fid.read()
    finally:
        fid.close()

    def _write(name, cnt, mode):
        target = scripts_node.make_node(name)
        target.safe_write(cnt, "w%s" % mode)
        return target

    nodes = []
    nodes.append(_write(name + ext, hdr + script_text, 't'))
    nodes.append(_write(name + ".exe", cnt, 'b'))
    nodes.append(_write(name + ".exe.manifest", _LAUNCHER_MANIFEST % (name,), 't'))

    return nodes

def create_posix_script(name, executable, scripts_node):
    header = "#!%(python_exec)s\n" % {"python_exec": SYS_EXECUTABLE}
    cnt = SCRIPT_TEXT % {"python_exec": SYS_EXECUTABLE,
            "module": executable.module,
            "function": executable.function}

    n = scripts_node.make_node(name)
    n.safe_write(header + cnt)
    return [n]

########NEW FILE########
__FILENAME__ = sdist
import tarfile

import os.path as op

import bento.compat.api as compat
import bento.errors

from bento.commands.core \
    import \
        Command, Option
from bento.conv \
    import \
        write_pkg_info

from six.moves \
    import \
        StringIO

def archive_basename(pkg):
    if pkg.version:
        return "%s-%s" % (pkg.name, pkg.version)
    else:
        return pkg.name

def create_tarball(node_pkg, archive_root, archive_node):
    tf = tarfile.open(archive_node.abspath(), "w:gz")
    try:
        for filename, alias in node_pkg.iter_source_files():
            tf.add(filename, op.join(archive_root, alias))
    finally:
        tf.close()

def create_zarchive(node_pkg, archive_root, archive_node):
    zid = compat.ZipFile(archive_node.abspath(), "w", compat.ZIP_DEFLATED)
    try:
        for filename, alias in node_pkg.iter_source_files():
            zid.write(filename, op.join(archive_root, alias))
    finally:
        zid.close()

_FORMATS = {"gztar": {"ext": ".tar.gz", "func": create_tarball},
            "zip": {"ext": ".zip", "func": create_zarchive}}

def create_archive(archive_name, archive_root, node_pkg, top_node, run_node, format="tgz", output_directory="dist"):
    if not format in _FORMATS:
        raise ValueError("Unknown format: %r" % (format,))

    archive_node = top_node.make_node(op.join(output_directory, archive_name))
    archive_node.parent.mkdir()

    _FORMATS[format]["func"](node_pkg, archive_root, archive_node) 
    return archive_root, archive_node

class SdistCommand(Command):
    long_descr = """\
Purpose: create a tarball for the project
Usage:   bentomaker sdist [OPTIONS]."""
    short_descr = "create a tarball."
    common_options = Command.common_options \
                        + [Option("--output-dir",
                                  help="Output directory", default="dist"),
                           Option("--format",
                                  help="Archive format (supported: 'gztar', 'zip')", default="gztar"),
                           Option("--output-file",
                                  help="Archive filename (default: $pkgname-$version.$archive_extension)")]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a =  p.parse_args(argv)
        if o.help:
            p.print_help()
            return

        pkg = ctx.pkg
        format = o.format

        archive_root = "%s-%s" % (pkg.name, pkg.version)
        if not o.output_file:
            archive_name = archive_basename(pkg) + _FORMATS[format]["ext"]
        else:
            output = op.basename(o.output_file)
            if output != o.output_file:
                raise bento.errors.BentoError("Invalid output file: should not contain any directory")
            archive_name = output

        s = StringIO()
        write_pkg_info(ctx.pkg, s)
        n = ctx.build_node.make_node("PKG_INFO")
        n.parent.mkdir()
        n.write(s.getvalue())
        ctx.register_source_node(n, "PKG_INFO")

        # XXX: find a better way to pass archive name from other commands (used
        # by distcheck ATM)
        self.archive_root, self.archive_node = create_archive(archive_name, archive_root, ctx._node_pkg,
                ctx.top_node, ctx.run_node, o.format, o.output_dir)

########NEW FILE########
__FILENAME__ = sphinx_command
import copy
import os
import subprocess
import sys

import os.path as op

import bento.errors

from bento.commands.core \
    import \
        Command
from bento.commands.options \
    import \
        Option
from bento.utils.utils \
    import \
        extract_exception

_OUTPUT_DEFAULT = "html"

# FIXME: where to allow to customize this ?
_DOC_ROOT = "doc"
# relative to _DOC_ROOT
_SOURCE_ROOT = "source"

def guess_source_dir():
    for guess in ('doc', 'docs'):
        if not op.isdir(guess):
            continue
        for root, dirnames, filenames in os.walk(guess):
            if 'conf.py' in filenames:
                return root
    return None

class SphinxCommand(Command):
    long_descr = """\
Purpose: build sphinx documentation
Usage:   bentomaker sphinx [OPTIONS]."""
    short_descr = "build the project sphinx documentation."
    common_options = Command.common_options + \
                        [Option("--output-format",
                                help="Doc output format (default: %r)" % _OUTPUT_DEFAULT,
                                default=_OUTPUT_DEFAULT),
                         Option("--source-dir",
                             help="Doc source directory (guessed if not specified)"),
                         Option("--config-dir",
                             help="Config directory (guessed if not specified)"),
                         ]

    def can_run(self):
        try:
            import sphinx.application
            return True
        except ImportError:
            return False
        except SyntaxError:
            return False

    def run(self, context):
        if not self.can_run():
            return bento.errors.CommandExecutionFailure("sphinx not available")
        import sphinx.application

        p = context.options_context.parser
        o, a = p.parse_args(context.command_argv)
        if o.output_format != "html":
            raise ValueError("Only html output supported for now")
        else:
            builder = "html"

        if o.source_dir is None:
            source_dir = guess_source_dir()
        else:
            source_dir = o.source_dir
        if source_dir is None:
            raise bento.errors.UsageException("""\
Doc source dir could not be found: please specify the directory where your
sphinx conf.py is located with the --source-dir option""")
        if not op.isabs(source_dir):
            source_dir = op.join(context.top_node.abspath(), source_dir)

        sphinx_build = context.build_node.make_node("sphinx")
        html_build = sphinx_build.make_node(o.output_format)
        doctrees_build = sphinx_build.make_node("doctrees")

        doc_html_build = html_build.abspath()
        doc_doctrees_build = doctrees_build.abspath()

        confoverrides = {}
        status_stream = sys.stdout
        fresh_env = False
        force_all = False

        app = sphinx.application.Sphinx(
                source_dir, source_dir,
                doc_html_build, doc_doctrees_build,
                builder, confoverrides, status_stream,
                freshenv=fresh_env)
        try:
            app.build(force_all=force_all)
        except Exception:
            err = extract_exception()
            raise bento.errors.CommandExecutionFailure("error while building doc: %r" % str(err))

########NEW FILE########
__FILENAME__ = test_build
import os
import sys
import shutil
import tempfile

import os.path as op

from six.moves \
    import \
        cStringIO

from bento.compat.api.moves \
    import \
        unittest

from bento.core.options \
    import \
        PackageOptions
from bento.core.package \
    import \
        PackageDescription
from bento.core.testing\
    import \
        expected_failure, skip_if
from bento.core.node \
    import \
        create_base_nodes
from bento.utils.utils \
    import \
        subst_vars
from bento.backends.distutils_backend \
    import \
        DistutilsBuildContext, DistutilsConfigureContext
from bento.backends.yaku_backend \
    import \
        BuildYakuContext, ConfigureYakuContext, YakuBackend
from bento.commands.hooks \
    import \
        PreHookWrapper
from bento.commands.wrapper_utils \
    import \
        run_command_in_context
from bento.commands.options \
    import \
        OptionsContext
from bento.commands.build \
    import \
        BuildCommand
from bento.core.testing \
    import \
        create_fake_package_from_bento_infos, create_fake_package_from_bento_info, \
        require_c_compiler
from bento.commands.tests.utils \
    import \
        prepare_configure, prepare_build, EncodedStringIO, \
        prepare_command, create_global_context, prepare_package
from bento.errors \
    import \
        UsageException


BENTO_INFO_WITH_EXT = """\
Name: foo

Library:
    Extension: foo
        Sources: foo.c
"""

BENTO_INFO_WITH_CLIB = """\
Name: foo

Library:
    CompiledLibrary: foo
        Sources: foo.c
"""

BENTO_INFO = """\
Name: foo

Library:
    Packages: foo, foo.bar
    Modules: fubar
"""

class _TestBuildSimpleExtension(unittest.TestCase):
    # Those should be set by subclasses
    _configure_context = None
    _build_context = None
    def setUp(self):
        self.save = None
        self.d = None

        self.save = os.getcwd()
        self.d = tempfile.mkdtemp()
        os.chdir(self.d)

        try:
            self.top_node, self.build_node, self.run_node = \
                    create_base_nodes(self.d, os.path.join(self.d, "build"))

            def builder_factory(build_context):
                def _dummy(extension):
                    from_node = self.build_node

                    pkg_dir = op.dirname(extension.name.replace('.', op.sep))
                    target_dir = op.join('$sitedir', pkg_dir)
                    build_context.outputs_registry.register_outputs("extensions",
                        extension.name, [], from_node, target_dir)
                return _dummy

            self._builder_factory = builder_factory
        except:
            os.chdir(self.save)
            raise

    def tearDown(self):
        if self.save:
            os.chdir(self.save)
        if self.d:
            shutil.rmtree(self.d)

    def _run_configure_and_build(self, bentos, bscripts=None, configure_argv=None, build_argv=None):
        conf, configure, bld, build = self._run_configure(bentos, bscripts,
                configure_argv, build_argv)
        run_command_in_context(bld, build)

        return conf, configure, bld, build

    def _run_configure(self, bentos, bscripts=None, configure_argv=None, build_argv=None):
        top_node = self.top_node

        create_fake_package_from_bento_infos(top_node, bentos, bscripts)

        conf, configure = prepare_configure(top_node, bentos["bento.info"], self._configure_context)
        run_command_in_context(conf, configure)

        bld, build = prepare_build(top_node, bentos["bento.info"], self._build_context, build_argv)

        return conf, configure, bld, build

    def _resolve_isection(self, node, isection):
        source_dir = subst_vars(isection.source_dir, {"_srcrootdir": self.build_node.abspath()})
        isection.source_dir = source_dir
        return isection

    def test_no_extension(self):
        self._run_configure_and_build({"bento.info": BENTO_INFO})

    def test_simple_extension(self):
        conf, configure, bld, build = self._run_configure_and_build({"bento.info": BENTO_INFO_WITH_EXT})

        sections = bld.section_writer.sections["extensions"]
        for extension in conf.pkg.extensions.values():
            isection = self._resolve_isection(bld.run_node, sections[extension.name])
            self.assertTrue(os.path.exists(os.path.join(isection.source_dir, isection.files[0][0])))

    def test_disable_extension(self):
        conf, configure, bld, build = self._run_configure({"bento.info": BENTO_INFO_WITH_EXT})
        pre_hook = PreHookWrapper(lambda context: context.disable_extension("foo"),
                                  "build", self.d)
        run_command_in_context(bld, build, pre_hooks=[pre_hook])

        assert "extensions" not in bld.section_writer.sections

    @expected_failure
    def test_disable_nonexisting_extension(self):
        conf, configure, bld, build = self._run_configure({"bento.info": BENTO_INFO_WITH_EXT})

        pre_hook = PreHookWrapper(lambda context: context.disable_extension("foo2"),
                                  "build", self.d)
        run_command_in_context(bld, build, pre_hooks=[pre_hook])

        assert "extensions" not in bld.section_writer.sections

    def test_extension_registration(self):
        top_node = self.top_node

        bento_info = """\
Name: foo

Library:
    Extension: _foo
        Sources: src/foo.c
    Extension: _bar
        Sources: src/bar.c
"""
        bentos = {"bento.info": bento_info}

        conf, configure, bld, build = self._run_configure(bentos)

        def pre_build(context):
            builder = self._builder_factory(context)
            context.register_builder("_bar", builder)
        pre_hook = PreHookWrapper(pre_build, "build", self.d)
        run_command_in_context(bld, build, pre_hooks=[pre_hook])

        sections = bld.section_writer.sections["extensions"]
        self.failUnless(len(sections) == 2)
        self.failUnless(len(sections["_bar"].files) == 0)

    def test_extension_tweak(self):
        bento_info = """\
Name: foo

Library:
    Extension: _foo
        Sources: src/foo.c
    Extension: _bar
        Sources: src/bar.c
"""
        bentos = {"bento.info": bento_info}

        conf, configure, bld, build = self._run_configure(bentos)

        # To check that include_dirs is passed along the slightly
        # over-engineered callchain, we wrap the default build with a function
        # that check the include_dirs is step up correctly.
        def pre_build(context):
            context.tweak_extension("_bar", include_dirs=["fubar"])
        pre_hook = PreHookWrapper(pre_build, "build", self.d)
        old_default_builder = bld.default_builder
        try:
            def _builder_wrapper(extension, **kw):
                if extension.name == "_bar":
                    self.assertTrue(kw.get("include_dirs", []) == ["fubar"])
                return old_default_builder(extension, **kw)
            bld.default_builder = _builder_wrapper
            run_command_in_context(bld, build, pre_hooks=[pre_hook])
        finally:
            bld.default_builder = old_default_builder

        sections = bld.section_writer.sections["extensions"]
        self.failUnless(len(sections) == 2)

    def test_simple_library(self):
        conf, configure, bld, build = self._run_configure_and_build({"bento.info": BENTO_INFO_WITH_CLIB})
        sections = bld.section_writer.sections["compiled_libraries"]
        for library in conf.pkg.compiled_libraries.values():
            isection = self._resolve_isection(bld.run_node, sections[library.name])
            self.assertTrue(os.path.exists(os.path.join(isection.source_dir, isection.files[0][0])))

    def test_simple_inplace(self):
        _, _, bld, build = self._run_configure_and_build({"bento.info": BENTO_INFO_WITH_EXT}, build_argv=["-i"])
        sections = bld.section_writer.sections["extensions"]
        source_dir = bld.top_node.abspath()
        for section in sections.values():
            isection = self._resolve_isection(bld.run_node, section)
            for f in isection.files[0]:
                self.assertTrue(op.exists(op.join(source_dir, f)))

class TestBuildDistutils(_TestBuildSimpleExtension):
    def setUp(self):
        from bento.commands.build_distutils import DistutilsBuilder
        _TestBuildSimpleExtension.setUp(self)
        self._distutils_builder = DistutilsBuilder()

        self._configure_context = DistutilsConfigureContext
        self._build_context = DistutilsBuildContext

    @require_c_compiler("distutils")
    def test_simple_inplace(self):
        super(TestBuildDistutils, self).test_simple_inplace()

    @require_c_compiler("distutils")
    def test_extension_registration(self):
        super(TestBuildDistutils, self).test_extension_registration()

    @require_c_compiler("distutils")
    def test_simple_library(self):
        super(TestBuildDistutils, self).test_simple_library()

    @require_c_compiler("distutils")
    def test_simple_extension(self):
        super(TestBuildDistutils, self).test_simple_extension()

class TestBuildYaku(_TestBuildSimpleExtension):
    def setUp(self):
        super(TestBuildYaku, self).setUp()

        self._configure_context = ConfigureYakuContext
        self._build_context = BuildYakuContext

    @require_c_compiler("yaku")
    def test_simple_inplace(self):
        super(TestBuildYaku, self).test_simple_inplace()

    @require_c_compiler("yaku")
    def test_extension_registration(self):
        super(TestBuildYaku, self).test_extension_registration()

    @require_c_compiler("yaku")
    def test_simple_library(self):
        super(TestBuildYaku, self).test_simple_library()

    @require_c_compiler("yaku")
    def test_simple_extension(self):
        super(TestBuildYaku, self).test_simple_extension()

    @require_c_compiler("yaku")
    def test_disable_extension(self):
        super(TestBuildYaku, self).test_disable_extension()

    @require_c_compiler("yaku")
    def test_disable_nonexisting_extension(self):
        super(TestBuildYaku, self).test_disable_nonexisting_extension()

def _not_has_waf():
    try:
        import bento.backends.waf_backend
        bento.backends.waf_backend.disable_output()
        return False
    except SyntaxError:
        return True
    except UsageException:
        return True

def skip_no_waf(f):
    return skip_if(_not_has_waf(), "waf not found")(f)

class TestBuildWaf(_TestBuildSimpleExtension):
    #def __init__(self, *a, **kw):
    #    super(TestBuildWaf, self).__init__(*a, **kw)

    #    # Add skip_no_waf decorator to any test function
    #    for m in dir(self):
    #        a = getattr(self, m)
    #        if isinstance(a, types.MethodType) and _NOSE_CONFIG.testMatch.match(m):
    #            setattr(self, m, skip_no_waf(a))

    def _run_configure(self, bentos, bscripts=None, configure_argv=None, build_argv=None):
        from bento.backends.waf_backend import make_stream_logger
        from bento.backends.waf_backend import WafBackend

        top_node = self.top_node

        bento_info = bentos["bento.info"]
        package = PackageDescription.from_string(bento_info)
        package_options = PackageOptions.from_string(bento_info)

        create_fake_package_from_bento_info(top_node, bento_info)
        top_node.make_node("bento.info").safe_write(bento_info)

        global_context = create_global_context(package, package_options, WafBackend())
        conf, configure = prepare_command(global_context, "configure",
                configure_argv, package, top_node)
        run_command_in_context(conf, configure)

        bld, build = prepare_command(global_context, "build", build_argv, package, top_node)
        bld.waf_context.logger = make_stream_logger("build", cStringIO())
        return conf, configure, bld, build

    @skip_no_waf
    def test_simple_extension(self):
        super(TestBuildWaf, self).test_simple_extension()

    @skip_no_waf
    def test_simple_library(self):
        super(TestBuildWaf, self).test_simple_library()

    @skip_no_waf
    def test_no_extension(self):
        super(TestBuildWaf, self).test_no_extension()

    @skip_no_waf
    def test_extension_registration(self):
        super(TestBuildWaf, self).test_extension_registration()

    @skip_no_waf
    def test_disable_extension(self):
        super(TestBuildWaf, self).test_disable_extension()

    @skip_no_waf
    def test_disable_nonexisting_extension(self):
        super(TestBuildWaf, self).test_disable_extension()

    @skip_no_waf
    def test_simple_inplace(self):
        super(TestBuildWaf, self).test_simple_inplace()

    @skip_no_waf
    def test_extension_tweak(self):
        super(TestBuildWaf, self).test_extension_tweak()

    def setUp(self):
        self._fake_output = None
        self._stderr = sys.stderr
        self._stdout = sys.stdout
        super(TestBuildWaf, self).setUp()
        # XXX: ugly stuff to make waf and nose happy together
        sys.stdout = EncodedStringIO()
        sys.stderr = EncodedStringIO()

        if _not_has_waf():
            return
        else:
            from bento.backends.waf_backend import ConfigureWafContext, BuildWafContext

            self._configure_context = ConfigureWafContext
            self._build_context = BuildWafContext

    def tearDown(self):
        super(TestBuildWaf, self).tearDown()
        sys.stderr = self._stderr
        sys.stdout = self._stdout

class TestBuildCommand(unittest.TestCase):
    def setUp(self):
        self.d = tempfile.mkdtemp()
        self.top_node, self.build_node, self.run_node = create_base_nodes(self.d, os.path.join(self.d, "build"))

        self.old_dir = os.getcwd()
        os.chdir(self.d)

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def _execute_build(self, bento_info):
        create_fake_package_from_bento_info(self.top_node, bento_info)
        # FIXME: this should be done automatically in create_fake_package_from_bento_info
        self.top_node.make_node("bento.info").safe_write(bento_info)

        package = PackageDescription.from_string(bento_info)
        package_options = PackageOptions.from_string(bento_info)

        global_context = create_global_context(package, package_options, YakuBackend())
        conf, configure = prepare_command(global_context, "configure", [], package, self.run_node)
        run_command_in_context(conf, configure)

        bld, build = prepare_command(global_context, "build", [], package, self.run_node)
        run_command_in_context(bld, build)

        return bld

    def test_simple(self):
        self._execute_build(BENTO_INFO)

    def test_executables(self):
        bento_info = """\
Name: foo

Library:
    Packages: foo

Executable: foomaker
    Module: foomain
    Function: main
"""
        self._execute_build(bento_info)

    def test_config_py(self):
        bento_info = """\
Name: foo

ConfigPy: foo/__config.py

Library:
    Packages: foo
"""
        self._execute_build(bento_info)

    def test_meta_template_file(self):
        bento_info = """\
Name: foo

MetaTemplateFiles: foo/__package_info.py.in

Library:
    Packages: foo
"""
        n = self.top_node.make_node(op.join("foo", "__package_info.py.in"))
        n.parent.mkdir()
        n.write("""\
NAME = $NAME
""")
        self._execute_build(bento_info)
        package_info = self.build_node.find_node(op.join("foo", "__package_info.py"))

        r_package_info = """\
NAME = "foo"
"""
        self.assertEqual(package_info.read(), r_package_info)

class TestBuildDirectoryBase(unittest.TestCase):
    def setUp(self):
        self.d = tempfile.mkdtemp()

        try:
            self.top_node, self.build_node, self.run_node = create_base_nodes(self.d,
                    os.path.join(self.d, "yoyobuild"))

            self.old_dir = os.getcwd()
            os.chdir(self.d)
        except:
            shutil.rmtree(self.d)
            raise

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

class TestBuildDirectory(TestBuildDirectoryBase):
    def test_simple_yaku(self):
        top_node = self.top_node

        create_fake_package_from_bento_info(top_node, BENTO_INFO_WITH_EXT)
        conf, configure = prepare_configure(top_node, BENTO_INFO_WITH_EXT, ConfigureYakuContext)
        run_command_in_context(conf, configure)

        build = BuildCommand()
        opts = OptionsContext.from_command(build)

        bld = BuildYakuContext(None, [], opts, conf.pkg, top_node)
        run_command_in_context(bld, build)

    def test_simple_distutils(self):
        top_node = self.top_node

        create_fake_package_from_bento_info(top_node, BENTO_INFO_WITH_EXT)
        conf, configure = prepare_configure(top_node, BENTO_INFO_WITH_EXT, DistutilsConfigureContext)
        run_command_in_context(conf, configure)

        build = BuildCommand()
        opts = OptionsContext.from_command(build)

        bld = DistutilsBuildContext(None, [], opts, conf.pkg, top_node)
        run_command_in_context(bld, build)

class TestBuildDirectoryWaf(TestBuildDirectoryBase):
    def setUp(self):
        TestBuildDirectoryBase.setUp(self)

        self._stderr = sys.stderr
        self._stdout = sys.stdout

        sys.stdout = EncodedStringIO()
        sys.stderr = EncodedStringIO()

    def tearDown(self):
        sys.stderr = self._stderr
        sys.stdout = self._stdout

        TestBuildDirectoryBase.tearDown(self)

    @skip_no_waf
    def test_simple_waf(self):
        from bento.backends.waf_backend import make_stream_logger
        from bento.backends.waf_backend import WafBackend

        top_node = self.top_node

        package = PackageDescription.from_string(BENTO_INFO_WITH_EXT)
        package_options = PackageOptions.from_string(BENTO_INFO_WITH_EXT)

        create_fake_package_from_bento_info(top_node, BENTO_INFO_WITH_EXT)
        top_node.make_node("bento.info").safe_write(BENTO_INFO_WITH_EXT)

        global_context = create_global_context(package, package_options, WafBackend())
        conf, configure = prepare_command(global_context, "configure", [], package, top_node)
        run_command_in_context(conf, configure)

        bld, build = prepare_command(global_context, "build", [], package, top_node)
        bld.waf_context.logger = make_stream_logger("build", cStringIO())
        run_command_in_context(bld, build)

class _SandboxMixin(unittest.TestCase):
    def setUp(self):
        self.new_cwd = tempfile.mkdtemp()
        try:
            self.old_cwd = os.getcwd()
            os.chdir(self.new_cwd)
        except:
            shutil.rmtree(self.new_cwd)
            raise

    def tearDown(self):
        os.chdir(self.old_cwd)
        shutil.rmtree(self.new_cwd)

class TestOutputRegistration(_SandboxMixin):
    def setUp(self):
        super(TestOutputRegistration, self).setUp()
        self.top_node, self.build_node, self.run_node = create_base_nodes()

    def _run_build_with_pre_hook(self, hook_func):
        package = PackageDescription.from_string(BENTO_INFO)
        global_context = prepare_package(self.top_node, BENTO_INFO)

        conf, configure = prepare_command(global_context, "configure", [], package, self.top_node)
        run_command_in_context(conf, configure)

        pre_hook = PreHookWrapper(hook_func, self.build_node.path_from(self.top_node), self.top_node.abspath())
        bld, build = prepare_command(global_context, "build", [], package, self.top_node)
        run_command_in_context(bld, build, pre_hooks=[pre_hook])

        return bld

    def test_simple(self):
        def hook(context):
            context.register_category("dummy")
            n = context.make_build_node("foo.txt")
            context.register_outputs("dummy", "dummy1", [n])
        bld = self._run_build_with_pre_hook(hook)

        self.assertTrue("dummy" in bld.outputs_registry.categories)
        nodes = []
        for name, _nodes, source_dir, target_dir, in bld.outputs_registry.iter_category("dummy"):
            nodes.extend(_nodes)
        self.assertTrue(nodes, [self.build_node.find_node("foo.txt")])

    def test_simple_registration(self):
        def hook(context):
            n = context.make_build_node("foo.txt")
            context.register_outputs_simple([n])
            n = context.make_build_node("bar.txt")
            context.register_outputs_simple([n])
        bld = self._run_build_with_pre_hook(hook)

        self.assertTrue("hook_registered" in bld.outputs_registry.categories)
        nodes = []
        for name, _nodes, source_dir, target_dir, in bld.outputs_registry.iter_category("hook_registered"):
            nodes.extend(_nodes)
        self.assertTrue(nodes, [self.build_node.find_node("foo.txt")])

class TestNameTranslation(_SandboxMixin):
    def setUp(self):
        super(TestNameTranslation, self).setUp()
        self.top_node, self.build_node, self.run_node = create_base_nodes()

    def test_compiled_library(self):
        from bento.backends.distutils_backend import DistutilsBuildContext

        r_full_name = "lib/_foo"

        bento_info = """\
Name: foo

Library:
    CompiledLibrary: lib/_foo
        Sources: foo.c
"""
        package = PackageDescription.from_string(bento_info)
        create_fake_package_from_bento_info(self.top_node, bento_info)

        options = OptionsContext.from_command(BuildCommand())
        context = DistutilsBuildContext(None, [], options, package, self.run_node)
        context.pre_recurse(self.top_node)
        try:
            def builder(a):
                self.assertEqual(a.name, r_full_name)
                builder.is_called = True
            builder.is_called = False
            context.register_compiled_library_builder("lib/_foo", builder)
        finally:
            context.post_recurse()
        context.compile()

        self.assertTrue(builder.is_called, "registered builder not called")

########NEW FILE########
__FILENAME__ = test_command_contexts
from bento.commands.registries \
    import \
        _RegistryBase
from bento.compat.api import moves

class Test_RegistryBase(moves.unittest.TestCase):
    def test_simple(self):
        registry = _RegistryBase()
        registry.register_category("dummy", lambda: 1)
        registry.register_callback("dummy", "dummy_func", lambda: 2)

        self.assertEqual(registry.callback("dummy", "dummy_func")(), 2)
        self.assertEqual(registry.callback("dummy", "non_existing_dummy_func")(), 1)

    def test_double_registration(self):
        registry = _RegistryBase()
        registry.register_category("dummy", lambda: 1)
        self.assertRaises(ValueError, lambda: registry.register_category("dummy", lambda: 2))

        self.assertEqual(registry.callback("dummy", "non_existing_dummy_func")(), 1)

    def test_missing_category(self):
        registry = _RegistryBase()
        self.assertRaises(ValueError, lambda: registry.register_callback("dummy", "dummy_func", lambda: 2))
        self.assertRaises(ValueError, lambda: registry.callback("dummy", "dummy_func"))

    def test_default_callback(self):
        registry = _RegistryBase()

        registry.register_category("dummy", lambda: 1)
        self.assertEqual(registry.default_callback("dummy"), 1)
        self.assertRaises(ValueError, lambda: registry.default_callback("non_existing_category"))

########NEW FILE########
__FILENAME__ = test_configure
import os
import sys
import shutil
import tempfile

import posixpath

import mock

from bento.compat.api.moves \
    import \
        unittest

from bento.core.options \
    import \
        PackageOptions
from bento.core \
    import \
        PackageDescription
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.commands.wrapper_utils \
    import \
        run_command_in_context
from bento.commands.tests.utils \
    import \
        prepare_configure
from bento.backends.yaku_backend \
    import \
        ConfigureYakuContext
from bento.commands.configure \
    import \
        _compute_scheme, set_scheme_unix, set_scheme_win32

BENTO_INFO = """\
Name: Sphinx
Version: 0.6.3
Summary: Python documentation generator
Url: http://sphinx.pocoo.org/
DownloadUrl: http://pypi.python.org/pypi/Sphinx
Description: Some long description.
Author: Georg Brandl
AuthorEmail: georg@python.org
Maintainer: Georg Brandl
MaintainerEmail: georg@python.org
License: BSD
"""

PY_VERSION_SHORT = ".".join(str(_) for _ in sys.version_info[:2])
PY_VERSION_SHORT_NO_DOT = "".join(str(_) for _ in sys.version_info[:2])

class TestConfigureCommand(unittest.TestCase):
    def setUp(self):
        self.d = tempfile.mkdtemp()
        self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))

        self.old_dir = os.getcwd()
        os.chdir(self.d)

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def test_simple(self):
        root = self.root
        run_node = root.find_node(self.d)

        conf, configure = prepare_configure(run_node, BENTO_INFO, ConfigureYakuContext)
        run_command_in_context(conf, configure)

    def test_flags(self):
        bento_info = """\
Name: foo

Flag: floupi
    Description: some floupi flag
    Default: true
"""
        run_node = self.root.find_node(self.d)

        conf, configure = prepare_configure(run_node, bento_info, ConfigureYakuContext, ["--floupi=false"])
        run_command_in_context(conf, configure)

UNIX_REFERENCE = {
        'destdir': "/",
        'prefix': None,
        'eprefix': None,
        'bindir': '$eprefix/bin',
        'sbindir': '$eprefix/sbin',
        'libexecdir': '$eprefix/libexec',
        'sysconfdir': '$prefix/etc',
        'sharedstatedir': '$prefix/com',
        'localstatedir': '$prefix/var',
        'libdir': '$eprefix/lib',
        'includedir': '$prefix/include',
        'datarootdir': '$prefix/share',
        'datadir': '$datarootdir',
        'mandir': '$datarootdir/man',
        'infodir': '$datarootdir/info',
        'localedir': '$datarootdir/locale',
        'docdir': '$datarootdir/doc/$pkgname',
        'htmldir': '$docdir',
        'dvidir': '$docdir',
        'psdir': '$docdir',
        'pdfdir': '$docdir',
        'sitedir': '$libdir/python$py_version_short/site-packages',
        'pkgdatadir': '$datadir/$pkgname'
}

WIN32_REFERENCE = {
        'destdir': "C:\\",
        'prefix': None,
        'eprefix': r'$prefix',
        'bindir': r'$eprefix\Scripts',
        'sbindir': r'$eprefix\Scripts',
        'libexecdir': r'$eprefix\Scripts',
        'sysconfdir': r'$prefix\etc',
        'sharedstatedir': r'$prefix\com',
        'localstatedir': r'$prefix\var',
        'libdir': r'$eprefix\lib',
        'includedir': r'$prefix\include',
        'datarootdir': r'$prefix\share',
        'datadir': r'$datarootdir',
        'mandir': r'$datarootdir\man',
        'infodir': r'$datarootdir\info',
        'localedir': r'$datarootdir\locale',
        'docdir': r'$datarootdir\doc\$pkgname',
        'htmldir': r'$docdir',
        'dvidir': r'$docdir',
        'psdir': r'$docdir',
        'pdfdir': r'$docdir',
        'sitedir': r'$prefix\Lib\site-packages',
        'pkgdatadir': r'$datadir\$pkgname'
    }
MOCK_DEBIAN_SCHEME = {
        'purelib': '$base/local/lib/python$py_version_short/dist-packages',
        'headers': '$base/local/include/python$py_version_short/$dist_name',
        'scripts': '$base/local/bin',
}


class TestUnixScheme(unittest.TestCase):
    def setUp(self):
        super(TestUnixScheme, self).setUp()
        options = mock.Mock()
        options.eprefix = None
        options.prefix = None

        self.options = options

    def _compute_scheme(self, bento_info, options):
        package_options = PackageOptions.from_string(bento_info)
        pkg = PackageDescription.from_string(bento_info)
        scheme = _compute_scheme(package_options)
        set_scheme_unix(scheme, options, pkg)

        return scheme


    @mock.patch("sys.platform", "linux2")
    @mock.patch("bento.commands.configure.virtualenv_prefix", lambda: None)
    @mock.patch("bento.core.platforms.sysconfig.bento.utils.path.find_root", lambda ignored: r"/")
    @mock.patch("distutils.command.install.INSTALL_SCHEMES", {"unix_prefix": UNIX_REFERENCE})
    def test_scheme_default(self):
        bento_info = """\
Name: foo
"""
        self.options.prefix = self.eprefix = None

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")
        py_version_short = scheme.pop("py_version_short")
        pkgname = scheme.pop("pkgname")

        self.assertEqual(prefix, "/usr/local")
        self.assertEqual(eprefix, "/usr/local")
        self.assertEqual(pkgname, "foo")
        self.assertEqual(py_version_short, PY_VERSION_SHORT)

        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(UNIX_REFERENCE[k], v, "discrepency for path %s: %s vs %s" % (k, UNIX_REFERENCE[k], v))

    @mock.patch("sys.platform", "darwin")
    @mock.patch("bento.core.platforms.sysconfig.bento.utils.path.find_root", lambda ignored: r"/")
    @mock.patch("bento.commands.configure.op.normpath", posixpath.normpath)
    @mock.patch("bento.commands.configure.virtualenv_prefix", lambda: None)
    @mock.patch("sys.prefix", "/Library/Frameworks/Python.framework/Versions/2.8")
    @mock.patch("sys.exec_prefix", "/Exec/Library/Frameworks/Python.framework/Versions/2.8")
    def test_scheme_default_darwin(self):
        bento_info = """\
Name: foo
"""
        self.options.prefix = self.eprefix = None

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")
        py_version_short = scheme.pop("py_version_short")
        pkgname = scheme.pop("pkgname")

        self.assertEqual(prefix, sys.prefix)
        self.assertEqual(eprefix, sys.exec_prefix)
        self.assertEqual(pkgname, "foo")
        self.assertEqual(py_version_short, PY_VERSION_SHORT)

        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(UNIX_REFERENCE[k], v)

    @mock.patch("sys.platform", "linux2")
    @mock.patch("bento.core.platforms.sysconfig.bento.utils.path.find_root", lambda ignored: r"/")
    def test_scheme_with_prefix(self):
        bento_info = """\
Name: foo
"""
        self.options.prefix = "/home/guido"

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")
        py_version_short = scheme.pop("py_version_short")
        pkgname = scheme.pop("pkgname")

        self.assertEqual(prefix, "/home/guido")
        self.assertEqual(eprefix, "/home/guido")
        self.assertEqual(pkgname, "foo")
        self.assertEqual(py_version_short, PY_VERSION_SHORT)

        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(UNIX_REFERENCE[k], v)

        self.options.eprefix = "/home/exec/guido"

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")
        py_version_short = scheme.pop("py_version_short")
        pkgname = scheme.pop("pkgname")

        self.assertEqual(prefix, "/home/guido")
        self.assertEqual(eprefix, "/home/exec/guido")
        self.assertEqual(pkgname, "foo")
        self.assertEqual(py_version_short, PY_VERSION_SHORT)

        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(UNIX_REFERENCE[k], v)

    @mock.patch("sys.platform", "linux2")
    def test_scheme_with_eprefix_fail(self):
        bento_info = """\
Name: foo
"""
        self.options.eprefix = "/home/guido"

        self.assertRaises(NotImplementedError, lambda: self._compute_scheme(bento_info, self.options))

    @mock.patch("sys.platform", "linux2")
    @mock.patch("bento.commands.configure.virtualenv_prefix", lambda: None)
    @mock.patch("bento.core.platforms.sysconfig.bento.utils.path.find_root", lambda ignored: r"/")
    @mock.patch("distutils.command.install.INSTALL_SCHEMES", {"unix_local": MOCK_DEBIAN_SCHEME}, create=True)
    def test_scheme_debian(self):
        bento_info = """\
Name: foo
"""

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")
        sitedir = scheme.pop("sitedir")
        includedir = scheme.pop("includedir")

        self.assertEqual(prefix, "/usr/local")
        self.assertEqual(eprefix, "/usr/local")
        self.assertEqual(sitedir, "/usr/local/lib/python%s/dist-packages" % PY_VERSION_SHORT)
        self.assertEqual(includedir, "/usr/local/include/python%s/foo" % PY_VERSION_SHORT)

        scheme.pop("py_version_short")
        scheme.pop("pkgname")
        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(UNIX_REFERENCE[k], v)

    @mock.patch("sys.platform", "linux2")
    @mock.patch("bento.core.platforms.sysconfig.bento.utils.path.find_root", lambda ignored: r"/")
    @mock.patch("bento.commands.configure.virtualenv_prefix", lambda: "/home/guido/.env")
    def test_scheme_venv(self):
        bento_info = """\
Name: foo
"""

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")

        self.assertEqual(prefix, "/home/guido/.env")
        self.assertEqual(eprefix, "/home/guido/.env")

        scheme.pop("py_version_short")
        scheme.pop("pkgname")
        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(UNIX_REFERENCE[k], v)

class TestWin32Scheme(unittest.TestCase):
    def setUp(self):
        super(TestWin32Scheme, self).setUp()
        options = mock.Mock()
        options.eprefix = None
        options.prefix = None

        self.options = options

    def _compute_scheme(self, bento_info, options):
        package_options = PackageOptions.from_string(bento_info)
        pkg = PackageDescription.from_string(bento_info)
        scheme = _compute_scheme(package_options)
        set_scheme_win32(scheme, options, pkg)

        return scheme

    @mock.patch("sys.platform", "win32")
    @mock.patch("bento.core.platforms.sysconfig.bento.utils.path.find_root", lambda ignored: "C:\\")
    @mock.patch("sys.prefix", r"C:\Python%s" % PY_VERSION_SHORT_NO_DOT)
    @mock.patch("sys.exec_prefix", r"C:\Python%s" % PY_VERSION_SHORT_NO_DOT)
    def test_scheme_default(self):
        bento_info = """\
Name: foo
"""
        self.options.prefix = self.eprefix = None

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")
        py_version_short = scheme.pop("py_version_short")
        pkgname = scheme.pop("pkgname")

        self.assertEqual(prefix, sys.prefix)
        self.assertEqual(eprefix, sys.exec_prefix)
        self.assertEqual(pkgname, "foo")
        self.assertEqual(py_version_short, PY_VERSION_SHORT)

        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(WIN32_REFERENCE[k], v, "discrepency for path %s: %s vs %s" % (k, WIN32_REFERENCE[k], v))

    @mock.patch("sys.platform", "win32")
    @mock.patch("bento.core.platforms.sysconfig.bento.utils.path.find_root", lambda ignored: "C:\\")
    def test_scheme_prefix(self):
        bento_info = """\
Name: foo
"""
        self.options.prefix = r"C:\foo"
        self.eprefix = None

        scheme = self._compute_scheme(bento_info, self.options)
        prefix = scheme.pop("prefix")
        eprefix = scheme.pop("eprefix")
        scheme.pop("py_version_short")
        scheme.pop("pkgname")

        self.assertEqual(prefix, r"C:\foo")
        self.assertEqual(eprefix, r"C:\foo")

        # Check that other values in scheme have not been modified
        for k, v in scheme.items():
            self.assertEqual(WIN32_REFERENCE[k], v, "discrepency for path %s: %s vs %s" % (k, WIN32_REFERENCE[k], v))

########NEW FILE########
__FILENAME__ = test_contexts
import sys

from bento.commands.contexts \
    import \
        GlobalContext
from bento.compat.api.moves \
    import \
        unittest
from bento.core.options \
    import \
        PackageOptions
from bento.core.platforms \
    import \
        get_scheme

class TestGlobalContextCustomizedOptions(unittest.TestCase):
    def setUp(self):
        self.context = GlobalContext(None)
        self.maxDiff = None

    def _test(self, package_options, r_scheme):
        self.context.register_package_options(package_options)
        scheme = self.context.retrieve_scheme()

        default_scheme, _ = get_scheme(sys.platform)
        r_scheme.update(default_scheme)
        r_scheme['py_version_short'] = ".".join(str(part) for part in sys.version_info[:2])
        r_scheme['pkgname'] = package_options.name

        self.assertEqual(scheme, r_scheme)

    def test_no_options(self):
        package_options = PackageOptions.from_string("""\
Name: foo
""")
        self._test(package_options, {})

    def test_path_option(self):
        package_options = PackageOptions.from_string("""\
Name: foo

Path: floupi
    Description: yoyo
    Default: /yeah
""")
        self._test(package_options, {"floupi": "/yeah"})

########NEW FILE########
__FILENAME__ = test_core
import os

from bento.compat.api.moves \
    import \
        unittest
from bento.core \
    import \
        PackageDescription
from bento.core.node \
    import \
        create_first_node
from bento.commands.core \
    import \
        HelpCommand, Command
from bento.commands.command_contexts \
    import \
        HelpContext
from bento.commands.wrapper_utils \
    import \
        run_command_in_context
from bento.commands.contexts \
    import \
        GlobalContext
from bento.commands.options \
    import \
        OptionsContext
import bento.commands.registries

class TestHelpCommand(unittest.TestCase):
    def setUp(self):
        self.run_node = create_first_node(os.getcwd())

        registry = bento.commands.registries.CommandRegistry()

        # help command assumes those always exist
        registry.register("configure", Command)
        registry.register("build", Command)
        registry.register("install", Command)
        registry.register("sdist", Command)
        registry.register("build_wininst", Command)
        registry.register("build_egg", Command)
        self.registry = registry

        self.options_registry = bento.commands.registries.OptionsRegistry()
        self.options_registry.register("configure", OptionsContext())

    def test_simple(self):
        help = HelpCommand()
        options = OptionsContext()
        for option in HelpCommand.common_options:
            options.add_option(option)
        global_context = GlobalContext(None,
                commands_registry=self.registry,
                options_registry=self.options_registry)
        pkg = PackageDescription()
        context = HelpContext(global_context, [], options, pkg, self.run_node)

        run_command_in_context(context, help)

    def test_command(self):
        help = HelpCommand()
        options = OptionsContext()
        for option in HelpCommand.common_options:
            options.add_option(option)
        pkg = PackageDescription()

        global_context = GlobalContext(None,
                commands_registry=self.registry,
                options_registry=self.options_registry)
        context = HelpContext(global_context, ["configure"], options, pkg, self.run_node)

        run_command_in_context(context, help)

########NEW FILE########
__FILENAME__ = test_dependency
from bento.compat.api.moves \
    import \
        unittest

from bento.commands.dependency \
    import \
        CommandScheduler

class TestCommandScheduler(unittest.TestCase):
    def test_simple(self):
        scheduler = CommandScheduler()
        scheduler.set_before("task2", "task1")
        scheduler.set_after("task2", "task3")
        scheduler.set_after("task2", "task4")
        scheduler.set_before("task4", "task3")

        tasks = scheduler.order("task4")
        self.assertEqual(tasks, ["task1", "task2", "task3"])

    def test_cycle(self):
        scheduler = CommandScheduler()
        scheduler.set_before("task2", "task1")
        scheduler.set_before("task1", "task2")

        self.assertRaises(ValueError, lambda: scheduler.order("task1"))

########NEW FILE########
__FILENAME__ = test_egg
import os
import shutil
import tempfile

from bento.compat.api.moves \
    import \
        unittest

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.core.node \
    import \
        create_base_nodes
from bento.core.package \
    import \
        PackageDescription
from bento.core \
    import \
        PackageMetadata
from bento.core.pkg_objects \
    import \
        Extension

from bento.commands.egg_utils \
    import \
        EggInfo

DESCR = """\
Name: Sphinx
Version: 0.6.3
Summary: Python documentation generator
Url: http://sphinx.pocoo.org/
DownloadUrl: http://pypi.python.org/pypi/Sphinx
Description: Some long description.
Author: Georg Brandl
AuthorEmail: georg@python.org
Maintainer: Georg Brandl
MaintainerEmail: georg@python.org
License: BSD

Library:
    Packages:
        sphinx,
        sphinx.builders
    Modules:
        cat.py
    Extension: _dog
        Sources: src/dog.c

Executable: sphinx-build
    Module: sphinx
    Function: main
"""

DUMMY_C = r"""\
#include <Python.h>
#include <stdio.h>

static PyObject*
hello(PyObject *self, PyObject *args)
{
    printf("Hello from C\n");
    Py_INCREF(Py_None);
    return Py_None;
}

static PyMethodDef HelloMethods[] = {
    {"hello",  hello, METH_VARARGS, "Print a hello world."},
    {NULL, NULL, 0, NULL}        /* Sentinel */
};

PyMODINIT_FUNC
init%(name)s(void)
{
    (void) Py_InitModule("%(name)s", HelloMethods);
}
"""

def create_fake_package(top_node, packages=None, modules=None, extensions=[]):
    if packages is None:
        packages = []
    if modules is None:
        modules = []
    if extensions is None:
        extensions = []

    for p in packages:
        d = p.replace(".", os.sep)
        n = top_node.make_node(d)
        n.mkdir()
        init = n.make_node("__init__.py")
        init.write("")
    for m in modules:
        d = m.replace(".", os.sep)
        n = top_node.make_node("%s.py" % d)
    for extension in extensions:
        main = extension.sources[0]
        n = top_node.make_node(main)
        n.parent.mkdir()
        n.write(DUMMY_C % {"name": extension.name})
        for s in extension.sources[1:]:
            n = top_node.make_node(s)
            n.write("")

class TestEggInfo(unittest.TestCase):
    def setUp(self):
        self.old_dir = None
        self.tmpdir = None

        self.old_dir = os.getcwd()
        self.tmpdir = tempfile.mkdtemp()

        os.chdir(self.tmpdir)
        self.top_node, self.build_node, self.run_node = \
                create_base_nodes(self.tmpdir, os.path.join(self.tmpdir, "build"))

    def tearDown(self):
        if self.old_dir:
            os.chdir(self.old_dir)
        if os.path.exists(self.tmpdir):
            shutil.rmtree(self.tmpdir)

    def _prepare_egg_info(self):
        create_fake_package(self.top_node, ["sphinx", "sphinx.builders"],
                            ["cat.py"], [Extension("_dog", [os.path.join("src", "dog.c")])])
        build_manifest_file = self.build_node.make_node(BUILD_MANIFEST_PATH)
        build_manifest_file.parent.mkdir()
        build_manifest_file.write("")

        files = [os.path.join("sphinx", "builders", "__init__.py"),
                 os.path.join("sphinx", "__init__.py"),
                 os.path.join("src", "dog.c"),
                 os.path.join("cat.py")]

        pkg = PackageDescription.from_string(DESCR)
        meta = PackageMetadata.from_package(pkg)
        executables = pkg.executables

        return EggInfo(meta, executables, files)

    def test_pkg_info(self):
        egg_info = self._prepare_egg_info()
        res = egg_info.get_pkg_info()
        ref = """\
Metadata-Version: 1.1
Name: Sphinx
Version: 0.6.3
Summary: Python documentation generator
Home-page: http://sphinx.pocoo.org/
Author: Georg Brandl
Author-email: georg@python.org
License: BSD
Download-URL: http://pypi.python.org/pypi/Sphinx
Description: Some long description.
Platform: UNKNOWN
"""
        self.assertEqual(res, ref)

    def test_iter_meta(self):
        egg_info = self._prepare_egg_info()
        for name, content in egg_info.iter_meta(self.build_node):
            pass

########NEW FILE########
__FILENAME__ = test_hooks
import os
import os.path as op
import shutil
import tempfile

from bento.compat.api.moves \
    import \
        unittest

from bento.core.node \
    import \
        create_root_with_source_tree
from bento.core.testing \
    import \
        create_fake_package_from_bento_infos
from bento.commands.command_contexts \
    import \
        ConfigureContext
from bento.commands.hooks \
    import \
        create_hook_module, find_pre_hooks, find_post_hooks

from bento.commands.tests.utils \
    import \
        prepare_configure

class TestHooks(unittest.TestCase):
    def setUp(self):
        self.old_cwd = os.getcwd()
        self.new_cwd = tempfile.mkdtemp()
        try:
            os.chdir(self.new_cwd)
            try:
                root = create_root_with_source_tree(self.new_cwd,
                        op.join(self.new_cwd, "build"))
                self.top_node = root._ctx.srcnode
                self.build_node = root._ctx.bldnode
            except:
                os.chdir(self.old_cwd)
                raise
        except:
            shutil.rmtree(self.new_cwd)
            raise

    def tearDown(self):
        os.chdir(self.old_cwd)
        shutil.rmtree(self.new_cwd)

    def test_simple(self):
        bscript = """\
from bento.commands import hooks

PRE = False
POST = False

@hooks.pre_configure
def pre_configure(ctx):
    global PRE
    PRE = True

@hooks.post_configure
def post_configure(ctx):
    global POST
    POST = True
"""

        bento = """\
Name: foo
Version: 1.0

HookFile: bscript
"""

        create_fake_package_from_bento_infos(self.top_node, {"bento.info": bento},
                {"bscript": bscript})

        conf, configure = prepare_configure(self.top_node, bento,
                ConfigureContext)
        bscript = self.top_node.search("bscript")
        m = create_hook_module(bscript.abspath())
        self.assertEqual(len(find_pre_hooks([m], "configure")), 1)
        self.assertEqual(len(find_post_hooks([m], "configure")), 1)

########NEW FILE########
__FILENAME__ = test_install
import os
import shutil
import tempfile
import os.path as op

from bento.compat.api.moves \
    import \
        unittest

from bento.core.node \
    import \
        create_root_with_source_tree

from bento.backends.yaku_backend \
    import \
        ConfigureYakuContext
from bento.commands.command_contexts \
    import \
        ContextWithBuildDirectory
from bento.commands.contexts \
    import \
        GlobalContext
from bento.commands.wrapper_utils \
    import \
        run_command_in_context
from bento.commands.tests.utils \
    import \
        prepare_configure, prepare_build
from bento.commands.install \
    import \
        InstallCommand, TransactionLog, rollback_transaction
from bento.commands.options \
    import \
        OptionsContext

from bento.core.options \
    import \
        PackageOptions
from bento.core.testing \
    import \
        create_fake_package_from_bento_info, require_c_compiler

class TestBuildCommand(unittest.TestCase):
    def setUp(self):
        self.d = tempfile.mkdtemp()
        self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))
        self.top_node = self.root.find_node(self.d)

        self.old_dir = os.getcwd()
        os.chdir(self.d)

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def _run_configure_and_build(self, bento_info, install_prefix):
        top_node = self.top_node

        create_fake_package_from_bento_info(top_node, bento_info)

        context = GlobalContext(None)
        options = PackageOptions.from_string(bento_info)
        context.register_package_options(options)

        cmd_argv = ["--prefix=%s" % install_prefix, "--exec-prefix=%s" % install_prefix]

        conf, configure = prepare_configure(top_node, bento_info, ConfigureYakuContext, cmd_argv)

        context.register_command("configure", configure)
        options_context = OptionsContext.from_command(configure)
        if not context.is_options_context_registered("configure"):
            context.register_options_context("configure", options_context)
        context.save_command_argv("configure", cmd_argv)

        run_command_in_context(conf, configure)

        bld, build = prepare_build(top_node, bento_info)
        run_command_in_context(bld, build)

        return context, conf, configure, bld, build

    def _test_dry_run(self, bento_info):
        install_prefix = tempfile.mkdtemp()
        try:
            context, conf, configure, bld, build = self._run_configure_and_build(bento_info, install_prefix)

            install = InstallCommand()
            opts = OptionsContext.from_command(install)

            inst = ContextWithBuildDirectory(context, ["--list-files"], opts, conf.pkg, self.top_node)
            run_command_in_context(inst, install)
        finally:
            shutil.rmtree(install_prefix)

    def _test_run(self, bento_info):
        install_prefix = tempfile.mkdtemp()
        try:
            context, conf, configure, bld, build = self._run_configure_and_build(bento_info, install_prefix)

            install = InstallCommand()
            opts = OptionsContext.from_command(install)

            inst = ContextWithBuildDirectory(context, [], opts, conf.pkg, self.top_node)
            run_command_in_context(inst, install)
        finally:
            shutil.rmtree(install_prefix)

    def test_simple(self):
        """Test whether install runs at all for a trivial package."""
        bento_info = """\
Name: foo

Library:
    Packages: foo, foo.bar
    Modules: fubar
"""
        self._test_run(bento_info)

    def test_simple_list_only(self):
        """Test whether install runs at all for a trivial package."""
        bento_info = """\
Name: foo

Library:
    Packages: foo, foo.bar
    Modules: fubar
"""
        self._test_dry_run(bento_info)

    @require_c_compiler()
    def test_simple_extension_list_only(self):
        """Test whether install runs at all for a trivial package."""
        bento_info = """\
Name: foo

Library:
    Packages: foo, foo.bar
    Modules: fubar
    Extension: foo
        Sources: src/foo.c
"""
        self._test_dry_run(bento_info)

def write_simple_tree(base_dir):
    """Write some files and subdirectories in base_dir."""
    files = []
    cur_dir = base_dir
    for i in range(25):
        if i % 3 == 0:
            cur_dir = op.join(cur_dir, "dir%d" % i)
            os.makedirs(cur_dir)
        filename = op.join(cur_dir, "foo%s.txt" % i)
        fid = open(filename, "wt")
        try:
            fid.write("file %d" % i)
        finally:
            fid.close()
        files.append(filename)
    return files

class TestTransactionLog(unittest.TestCase):
    def setUp(self):
        self.base_dir = tempfile.mkdtemp()
        self.files = write_simple_tree(self.base_dir)

    def tearDown(self):
        shutil.rmtree(self.base_dir)

    def test_simple(self):
        """Test a simple, uninterrupted run."""
        target_prefix = op.join(self.base_dir, "foo")
        trans_file = op.join(self.base_dir, "trans.log")
        self._test_simple(target_prefix, trans_file)

    def _test_simple(self, target_prefix, trans_file):
        targets = []
        log = TransactionLog(trans_file)
        try:
            for source in self.files:
                target = op.join(target_prefix, op.basename(source))
                targets.append(target)
                log.copy(source, target, None)
        finally:
            log.close()

        for target in targets:
            self.assertTrue(op.exists(target))

    def test_rollback_transaction(self):
        target_prefix = op.join(self.base_dir, "foo")
        trans_file = op.join(self.base_dir, "trans.log")

        self._test_simple(target_prefix, trans_file)

        rollback_transaction(trans_file)
        self.assertFalse(op.exists(target_prefix))

    def test_simple_interrupted(self):
        """Test a simple, interrupted run."""
        class _InterruptException(Exception):
            pass

        target_prefix = op.join(self.base_dir, "foo")

        trans_file = op.join(self.base_dir, "trans.log")
        targets = []
        log = TransactionLog(trans_file)
        try:
            try:
                for i, source in enumerate(self.files):
                    if i > len(self.files) / 3:
                        raise _InterruptException()
                    target = op.join(target_prefix, op.basename(source))
                    targets.append(target)
                    log.copy(source, target, None)
            except _InterruptException:
                log.rollback()
                for target in targets:
                    self.assertFalse(op.exists(target))
                return
            self.fail("Expected failure at this point !")
        finally:
            log.close()

########NEW FILE########
__FILENAME__ = test_misc
from bento.commands.build_wininst \
    import \
        BuildWininstCommand
from bento.commands.build_egg \
    import \
        BuildEggCommand

########NEW FILE########
__FILENAME__ = test_recursive
import os
import shutil
import tempfile

import os.path as op

from bento.compat.api.moves \
    import \
        unittest
from bento.core \
    import \
        PackageDescription, PackageOptions
from bento.errors \
    import \
        InvalidPackage
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.core.node_package \
    import \
        NodeRepresentation
from bento.commands.hooks \
    import \
        create_hook_module, find_pre_hooks
from bento.commands.wrapper_utils\
    import \
        run_command_in_context
from bento.backends.yaku_backend \
    import \
        ConfigureYakuContext
from bento.installed_package_description \
    import \
        InstalledSection

from bento.core.testing \
    import \
        create_fake_package_from_bento_infos
from bento.commands.tests.utils \
    import \
        prepare_configure, prepare_build, comparable_installed_sections

def comparable_representation(top_node, node_pkg):
    """Return a dictionary representing the node_pkg to be used for
    comparison."""
    d = {"packages": {}, "extensions": {}}
    for k, v in node_pkg.iter_category("extensions"):
        d["extensions"][k] = v.extension_from(top_node)
    for k, v in node_pkg.iter_category("packages"):
        d["packages"][k] = (v.full_name, v.nodes, v.top_node, v.top_or_lib_node)
    return d

class TestRecurseBase(unittest.TestCase):
    def setUp(self):
        self.old_dir = os.getcwd()

        self.d = tempfile.mkdtemp()
        self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))
        self.run_node = self.root.find_node(self.d)
        self.top_node = self.run_node

        os.chdir(self.d)

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def _create_package_and_reference(self, bento_info, r_bento_info):
        pkg = PackageDescription.from_string(bento_info)
        node_pkg = NodeRepresentation(self.run_node, self.top_node)
        node_pkg.update_package(pkg)

        r_pkg = PackageDescription.from_string(r_bento_info)
        r_node_pkg = NodeRepresentation(self.run_node, self.top_node)
        r_node_pkg.update_package(r_pkg)

        return node_pkg, r_node_pkg

    def test_py_packages(self):
        run_node = self.run_node

        bento_info = """\
Name: foo

Recurse: bar

Library:
    Packages: bar
"""
        sub_bento_info = """\
Library:
    Packages: foo
"""

        r_bento_info = """\
Name: foo

Library:
    Packages: bar, bar.foo
"""

        bentos = {"bento.info": bento_info,
                  op.join("bar", "bento.info"): sub_bento_info}
        create_fake_package_from_bento_infos(run_node, bentos)

        node_pkg, r_node_pkg = self._create_package_and_reference(bento_info, r_bento_info)
        self.assertEqual(comparable_representation(self.top_node, node_pkg),
                         comparable_representation(self.top_node, r_node_pkg))


    def test_extension(self):
        run_node = self.run_node

        bento_info = """\
Name: foo

Recurse: bar

Library:
    Extension: foo
        Sources: src/foo.c
"""
        sub_bento_info = """\
Library:
    Extension: foo
        Sources: src/foo.c
"""

        r_bento_info = """\
Name: foo

Library:
    Extension: foo
        Sources: src/foo.c
    Extension: bar.foo
        Sources: bar/src/foo.c
"""

        bentos = {"bento.info": bento_info,
                  op.join("bar", "bento.info"): sub_bento_info}
        create_fake_package_from_bento_infos(run_node, bentos)

        node_pkg, r_node_pkg = self._create_package_and_reference(bento_info, r_bento_info)
        self.assertEqual(comparable_representation(self.top_node, node_pkg),
                         comparable_representation(self.top_node, r_node_pkg))

    def test_basics(self):
        run_node = self.run_node

        bento_info = """\
Name: foo

Recurse:
    bar
"""
        bento_info2 = """\
Recurse:
    foo

Library:
    Extension: _foo
        Sources: foo.c
    CompiledLibrary: _bar
        Sources: foo.c
"""

        bento_info3 = """\
Library:
    Packages: sub2
"""
        bentos = {"bento.info": bento_info, os.path.join("bar", "bento.info"): bento_info2,
                  os.path.join("bar", "foo", "bento.info"): bento_info3}
        create_fake_package_from_bento_infos(run_node, bentos)

        r_bento_info = """\
Name: foo

Library:
    Packages:
        bar.foo.sub2
    Extension: bar._foo
        Sources: bar/foo.c
    CompiledLibrary: bar._bar
        Sources: bar/foo.c
"""

        node_pkg, r_node_pkg = self._create_package_and_reference(bento_info, r_bento_info)

        self.assertEqual(comparable_representation(self.top_node, node_pkg),
                         comparable_representation(self.top_node, r_node_pkg))

    def test_py_module_invalid(self):
        """Ensure we get a package error when defining py modules in recursed
        bento.info."""
        bento_info = """\
Name: foo

Recurse: bar
"""
        sub_bento_info = """\
Library:
    Modules: foo
"""
        bentos = {"bento.info": bento_info,
                  os.path.join("bar", "bento.info"): sub_bento_info}
        self.assertRaises(InvalidPackage,
                          lambda: create_fake_package_from_bento_infos(self.run_node, bentos))

    def test_hook(self):
        root = self.root
        top_node = self.top_node

        bento_info = """\
Name: foo

HookFile:
    bar/bscript

Recurse:
    bar
"""
        bento_info2 = """\
Library:
    Packages: fubar
"""

        bscript = """\
from bento.commands import hooks
@hooks.pre_configure
def configure(ctx):
    packages = ctx.local_pkg.packages
    ctx.local_node.make_node("test").write(str(packages))
"""
        bentos = {"bento.info": bento_info, os.path.join("bar", "bento.info"): bento_info2}
        bscripts = {os.path.join("bar", "bscript"): bscript}
        create_fake_package_from_bento_infos(top_node, bentos, bscripts)

        conf, configure = prepare_configure(self.run_node, bento_info, ConfigureYakuContext)
        try:
            hook = top_node.search("bar/bscript")
            m = create_hook_module(hook.abspath())
            for hook in find_pre_hooks([m], "configure"):
                conf.pre_recurse(root.find_dir(hook.local_dir))
                try:
                    hook(conf)
                finally:
                    conf.post_recurse()

            test = top_node.search("bar/test")
            if test:
                self.failUnlessEqual(test.read(), "['fubar']")
            else:
                self.fail("test dummy not found")
        finally:
            configure.finish(conf)
            conf.finish()

class TestInstalledSections(unittest.TestCase):
    """Test registered installed sections are the expected ones when using
    recursive support."""
    def setUp(self):
        self.old_dir = os.getcwd()

        self.d = tempfile.mkdtemp()
        self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))
        self.run_node = self.root.find_node(self.d)
        self.top_node = self.run_node

        os.chdir(self.d)

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def _test_installed_sections(self, bento_infos, r_sections):
        create_fake_package_from_bento_infos(self.top_node, bento_infos)

        conf, configure = prepare_configure(self.run_node, bento_infos["bento.info"])
        run_command_in_context(conf, configure)

        bld, build = prepare_build(self.top_node, bento_infos["bento.info"])
        run_command_in_context(bld, build)

        sections = bld.section_writer.sections

        self.assertEqual(comparable_installed_sections(sections),
                         comparable_installed_sections(r_sections))

    def test_packages(self):
        bento_info = """\
Name: foo

Recurse:
    foo

Library:
    Packages: foo
"""
        sub_bento_info = """\
Library:
    Packages: bar
"""

        r_bar_section = InstalledSection.from_source_target_directories("pythonfiles",
                                     "foo",
                                     "$_srcrootdir/..",
                                     "$sitedir",
                                     ["foo/__init__.py"])
        r_foo_bar_section = InstalledSection.from_source_target_directories("pythonfiles",
                                     "foo.bar",
                                     "$_srcrootdir/..",
                                     "$sitedir",
                                     ["foo/bar/__init__.py"])
        r_sections = {"pythonfiles":
                         {"foo": r_bar_section,
                          "foo.bar": r_foo_bar_section}}
        self._test_installed_sections({"bento.info": bento_info, op.join("foo", "bento.info"): sub_bento_info},
                                      r_sections)

    def test_extension(self):
        bento_info = """\
Name: foo

Recurse: bar
"""

        sub_bento_info = """\
Library:
    Extension: foo
        Sources: src/foo.c
"""

        r_section = InstalledSection.from_source_target_directories("extensions",
                                     "bar.foo",
                                     "$_srcrootdir/bar",
                                     "$sitedir/bar",
                                     ["foo.so"])
        r_sections = {"extensions":
                        {"bar.foo":
                            r_section}}
        self._test_installed_sections({"bento.info": bento_info, op.join("bar", "bento.info"): sub_bento_info},
                                      r_sections)

########NEW FILE########
__FILENAME__ = test_script_utils
import os
import tempfile

import os.path as op

import mock

from bento.core.node \
    import \
        create_base_nodes
from bento.core.pkg_objects \
    import \
        Executable
from bento.compat.api.moves \
    import \
        unittest
from bento.commands.script_utils \
    import \
        nt_quote_arg, create_win32_script, create_posix_script, create_scripts

class TestMisc(unittest.TestCase):
    def test_nt_quote_arg(self):
        s = nt_quote_arg("foo")
        self.assertEqual(s, "foo")

        s = nt_quote_arg(" foo")
        self.assertEqual(s, '" foo"')

        s = nt_quote_arg("\ foo")
        self.assertEqual(s, '"\\ foo"')

class TestCreateScript(unittest.TestCase):
    def setUp(self):
        super(TestCreateScript, self).setUp()

        self.old_cwd = os.getcwd()
        self.d = tempfile.mkdtemp()

        try:
            os.chdir(self.d)
            self.top_node, self.build_node, self.run_node = create_base_nodes(self.d, op.join(self.d, "build"))
            self.build_node.mkdir()
        except:
            os.chdir(self.old_cwd)

    def tearDown(self):
        os.chdir(self.old_cwd)
        super(TestCreateScript, self).tearDown()

    def _test_create_script(self, func, executable, r_list):
        nodes = func("foo", executable, self.build_node)
        self.assertEqual(set(self.build_node.listdir()), set(r_list))
        self.assertEqual(set([node.path_from(self.build_node) for node in nodes]), set(r_list))

    def _test_create_scripts(self, executable, r_list):
        res = create_scripts({"foo": executable}, self.build_node)
        nodes = res["foo"]
        self.assertEqual(set(self.build_node.listdir()), set(r_list))
        self.assertEqual(set([node.path_from(self.build_node) for node in nodes]), set(r_list))

    @mock.patch("sys.platform", "win32")
    def test_create_script_on_win32(self):
        r_list = set(["foo-script.py", "foo.exe", "foo.exe.manifest"])
        executable = Executable("foo", "foo.bar", "fubar")
        self._test_create_scripts(executable, r_list)

    @mock.patch("sys.platform", "darwin")
    def test_create_script_on_posix(self):
        r_list = set(["foo"])
        executable = Executable("foo", "foo.bar", "fubar")
        self._test_create_scripts(executable, r_list)

    def test_create_posix_script(self):
        r_list = ["foo"]
        executable = Executable("foo", "foo.bar", "fubar")
        self._test_create_script(create_posix_script, executable, r_list)

    def test_create_win32_script(self):
        executable = Executable("foo", "foo.bar", "fubar")
        r_list = set(["foo-script.py", "foo.exe", "foo.exe.manifest"])
        self._test_create_script(create_win32_script, executable, r_list)

########NEW FILE########
__FILENAME__ = test_sdist
import os
import os.path as op
import tempfile
import shutil
import zipfile

from bento.compat.api.moves \
    import \
        unittest
from bento.core.package \
    import \
        PackageDescription
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.commands.options \
    import \
        OptionsContext
from bento.commands.command_contexts \
    import \
        SdistContext
from bento.commands.sdist \
    import \
        SdistCommand
from bento.commands.wrapper_utils \
    import \
        run_command_in_context
from bento.core.testing \
    import \
        create_fake_package_from_bento_infos, create_fake_package_from_bento_info
from bento.convert.utils \
    import \
        canonalize_path

class TestBaseSdist(unittest.TestCase):
    def setUp(self):
        self.save = os.getcwd()
        self.d = tempfile.mkdtemp()
        os.chdir(self.d)
        try:
            self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))
            self.top_node = self.root._ctx.srcnode
            self.build_node = self.root._ctx.bldnode
            self.run_node = self.root.find_node(self.d)
        except Exception:
            os.chdir(self.save)
            raise

    def tearDown(self):
        os.chdir(self.save)
        shutil.rmtree(self.d)

    def _assert_archive_equality(self, archive, r_archive_list):
        r_archive_list = set(canonalize_path(f) for f in r_archive_list)
        archive = self.run_node.find_node(archive)
        z = zipfile.ZipFile(archive.abspath(), "r")
        try:
            archive_list = set(z.namelist())
            self.assertEqual(archive_list, r_archive_list)
        finally:
            z.close()

    def test_simple_package(self):
        bento_info = """\
Name: foo
Version: 1.0

ExtraSourceFiles: yeah.info

Library:
    Packages: foo, foo.bar
    Modules: fubar
"""
        archive_list = [op.join("foo-1.0", f) for f in ["yeah.info",
                                                        "PKG_INFO",
                                                        op.join("foo", "__init__.py"),
                                                        op.join("foo", "bar", "__init__.py"),
                                                        "fubar.py"]]

        create_fake_package_from_bento_info(self.top_node, bento_info)
        package = PackageDescription.from_string(bento_info)

        sdist = SdistCommand()
        opts = OptionsContext.from_command(sdist)
        cmd_argv = ["--output-file=foo.zip", "--format=zip"]

        context = SdistContext(None, cmd_argv, opts, package, self.run_node)
        run_command_in_context(context, sdist)

        self._assert_archive_equality(op.join("dist", "foo.zip"), archive_list)

    def test_extra_source_registration(self):
        bento_info = """\
Name: foo
Version: 1.0

Library:
    Modules: fubar
"""
        archive_list = [op.join("foo-1.0", f) for f in ["fubar.py", "yeah.info", "PKG_INFO"]]

        extra_node = self.top_node.make_node("yeah.info")
        extra_node.write("")

        create_fake_package_from_bento_info(self.top_node, bento_info)
        package = PackageDescription.from_string(bento_info)

        sdist = SdistCommand()
        opts = OptionsContext.from_command(sdist)
        cmd_argv = ["--output-file=foo.zip", "--format=zip"]

        context = SdistContext(None, cmd_argv, opts, package, self.run_node)
        context.register_source_node(self.top_node.find_node("yeah.info"))
        run_command_in_context(context, sdist)

        self._assert_archive_equality(op.join("dist", "foo.zip"), archive_list)

    def test_extra_source_with_alias(self):
        bento_info = """\
Name: foo
Version: 1.0

Library:
    Modules: fubar
"""
        archive_list = [op.join("foo-1.0", f) for f in ["fubar.py", "bohou.info", "PKG_INFO"]]

        extra_node = self.top_node.make_node("yeah.info")
        extra_node.write("")

        create_fake_package_from_bento_info(self.top_node, bento_info)
        package = PackageDescription.from_string(bento_info)

        sdist = SdistCommand()
        opts = OptionsContext.from_command(sdist)
        cmd_argv = ["--output-file=foo.zip", "--format=zip"]

        context = SdistContext(None, cmd_argv, opts, package, self.run_node)
        context.register_source_node(self.top_node.find_node("yeah.info"), archive_name="bohou.info")
        run_command_in_context(context, sdist)

        self._assert_archive_equality(op.join("dist", "foo.zip"), archive_list)

    def test_template_filling(self):
        bento_info = """\
Name: foo
Version: 1.0

MetaTemplateFiles: release.py.in

Library:
    Modules: fubar
"""
        archive_list = [op.join("foo-1.0", f) for f in ["fubar.py", "release.py.in", "release.py", "PKG_INFO"]]

        template = self.top_node.make_node("release.py.in")
        template.write("""\
NAME = $NAME
VERSION = $VERSION
""")

        create_fake_package_from_bento_info(self.top_node, bento_info)
        package = PackageDescription.from_string(bento_info)

        sdist = SdistCommand()
        opts = OptionsContext.from_command(sdist)
        cmd_argv = ["--output-file=foo.zip", "--format=zip"]

        context = SdistContext(None, cmd_argv, opts, package, self.run_node)
        run_command_in_context(context, sdist)

        self._assert_archive_equality(op.join("dist", "foo.zip"), archive_list)

########NEW FILE########
__FILENAME__ = test_sphinx
import os
import shutil
import tempfile

import mock

import bento.core.testing

from bento.commands.command_contexts \
    import \
        ContextWithBuildDirectory
from bento.commands.options \
    import \
        OptionsContext
from bento.commands.sphinx_command \
    import \
        SphinxCommand
from bento.commands.wrapper_utils \
    import \
        run_command_in_context
from bento.compat.api.moves \
    import \
        unittest
from bento.core.package \
    import \
        PackageDescription
from bento.core.node \
    import \
        create_base_nodes

def has_sphinx():
    try:
        import sphinx
        return True
    except ImportError:
        return False

class TestSphinx(unittest.TestCase):
    def setUp(self):
        self.save = os.getcwd()
        self.d = tempfile.mkdtemp()
        os.chdir(self.d)
        try:
            self.top_node, self.build_node, self.run_node = create_base_nodes()
        except Exception:
            os.chdir(self.save)
            raise

    def tearDown(self):
        os.chdir(self.save)
        shutil.rmtree(self.d)

    @bento.core.testing.skip_if(not has_sphinx(), "sphinx not available, skipping sphinx command test(s)")
    def test_simple(self):
        n = self.top_node.make_node("doc/conf.py")
        n.parent.mkdir()
        n.write("")

        n = self.top_node.make_node("doc/contents.rst")
        n.write("")

        bento_info = "Name: foo"
        package = PackageDescription.from_string(bento_info)

        sphinx = SphinxCommand()
        opts = OptionsContext.from_command(sphinx)
        context = ContextWithBuildDirectory(None, [], opts, package, self.run_node)

        run_command_in_context(context, sphinx)

########NEW FILE########
__FILENAME__ = test_sub_directory
import os
import os.path as op
import tempfile
import shutil
import unittest

from bento.errors \
    import \
        InvalidPackage
from bento.core \
    import \
        PackageDescription
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.core.node_package \
    import \
        NodeRepresentation
from bento.core.testing \
    import \
        create_fake_package_from_bento_info
from bento.commands.tests.utils \
    import \
        prepare_configure, prepare_build
from bento.commands.wrapper_utils\
    import \
        run_command_in_context
from bento.installed_package_description \
    import \
        InstalledSection

from bento.commands.tests.utils \
    import \
        comparable_installed_sections
from bento.core.testing \
    import \
        require_c_compiler

class TestSubDirectory(unittest.TestCase):
    def setUp(self):
        self.old_dir = os.getcwd()

        self.d = tempfile.mkdtemp()
        self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))
        self.run_node = self.root.find_node(self.d)
        self.top_node = self.run_node

        os.chdir(self.d)

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def test_recurse(self):
        """Check that we raise a proper exception when mixing recurse and
        sub_directory features."""
        bento_info = """\
Name: foo

Recurse: bar

Library:
    SubDirectory: lib
"""
        self.assertRaises(InvalidPackage, lambda: PackageDescription.from_string(bento_info))

    def _test_installed_sections(self, bento_info, r_sections):
        create_fake_package_from_bento_info(self.top_node, bento_info)

        conf, configure = prepare_configure(self.run_node, bento_info)
        run_command_in_context(conf, configure)

        bld, build = prepare_build(self.top_node, bento_info)
        run_command_in_context(bld, build)

        sections = bld.section_writer.sections

        self.assertEqual(comparable_installed_sections(sections),
                         comparable_installed_sections(r_sections))

    def test_packages(self):
        """Test sub_directory support for packages."""
        self.maxDiff = 1024

        bento_info = """\
Name: foo

Library:
    SubDirectory: lib
    Packages: foo
"""

        r_section = InstalledSection.from_source_target_directories("pythonfiles",
                                     "foo",
                                     "$_srcrootdir/../lib",
                                     "$sitedir",
                                     ["foo/__init__.py"])
        r_sections = {"pythonfiles":
                         {"foo": r_section}}
        self._test_installed_sections(bento_info, r_sections)

    @require_c_compiler("yaku")
    def test_extension(self):
        """Test sub_directory support for C extensions."""
        bento_info = """\
Name: foo

Library:
    SubDirectory: lib
    Extension: foo
        Sources: src/foo.c
"""

        r_section = InstalledSection.from_source_target_directories("extensions",
                                     "foo",
                                     "$_srcrootdir/.",
                                     "$sitedir/",
                                     ["foo.so"])
        r_sections = {"extensions":
                        {"foo":
                            r_section}}
        self._test_installed_sections(bento_info, r_sections)

    @require_c_compiler("yaku")
    def test_compiled_library(self):
        """Test sub_directory support for C compiled libraries."""
        bento_info = """\
Name: foo

Library:
    SubDirectory: lib
    CompiledLibrary: foo
        Sources: src/foo.c
"""

        r_section = InstalledSection.from_source_target_directories("compiled_libraries",
                                     "foo",
                                     "$_srcrootdir/.",
                                     "$sitedir/",
                                     ["libfoo.a"])
        r_sections = {"compiled_libraries":
                         {"foo": r_section}}
        self._test_installed_sections(bento_info, r_sections)

    def test_module(self):
        """Test sub_directory support for python module."""
        bento_info = """\
Name: foo

Library:
    SubDirectory: lib
    Modules: foo
"""

        r_section = InstalledSection.from_source_target_directories("pythonfiles",
                                     "foo",
                                     "$_srcrootdir/../lib",
                                     "$sitedir",
                                     ["foo.py"])
        r_sections = {"pythonfiles":
                         {"foo": r_section}}
        self._test_installed_sections(bento_info, r_sections)

########NEW FILE########
__FILENAME__ = test_utils
import os
import tempfile
import shutil

import os.path as op

from bento.core \
    import \
        PackageDescription
from bento.compat.api.moves \
    import \
        unittest
from bento.core.node \
    import \
        create_base_nodes
from bento.core.testing \
    import \
        create_fake_package_from_bento_infos

from bento.commands.utils \
    import \
        has_cython_code

class TestIsUingCython(unittest.TestCase):
    def setUp(self):
        self.old_dir = os.getcwd()

        self.d = tempfile.mkdtemp()
        os.chdir(self.d)

        self.top_node, self.build_node, self.run_node = create_base_nodes()

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def test_simple_cython(self):
        bento_info = """\
Library:
    Extension: foo
        Sources: foo1.c, foo2.pyx
"""
        bentos = {"bento.info": bento_info}
        create_fake_package_from_bento_infos(self.run_node, bentos)

        pkg = PackageDescription.from_string(bento_info)
        self.assertTrue(has_cython_code(pkg))

    def test_simple_no_cython(self):
        bento_info = """\
Library:
    Extension: foo
        Sources: foo1.c, foo2.c
"""
        bentos = {"bento.info": bento_info}
        create_fake_package_from_bento_infos(self.run_node, bentos)

        pkg = PackageDescription.from_string(bento_info)
        self.assertFalse(has_cython_code(pkg))

    def test_sub_package(self):
        bento_info = """\
Recurse: foo

Library:
    Extension: foo
        Sources: foo1.c
"""
        bento_sub1_info = """
Library:
    Extension: bar
        Sources: bar.pyx
"""
        bentos = {"bento.info": bento_info, op.join("foo", "bento.info"): bento_sub1_info}
        create_fake_package_from_bento_infos(self.run_node, bentos)

        pkg = PackageDescription.from_string(bento_info)
        self.assertTrue(has_cython_code(pkg))

########NEW FILE########
__FILENAME__ = test_wininst
import os
import shutil
import tempfile
import zipfile

import os.path as op

import mock

import bento.commands.build_wininst

from bento.commands.build_wininst \
    import \
        create_wininst
from bento.compat.api.moves \
    import \
        unittest
from bento.core.node \
    import \
        create_base_nodes
from bento.installed_package_description \
    import \
        BuildManifest

class TestWininstInfo(unittest.TestCase):
    def setUp(self):
        self.old_dir = None
        self.tmpdir = None

        self.old_dir = os.getcwd()
        self.tmpdir = tempfile.mkdtemp()

        try:
            self.top_node, self.build_node, self.run_node = \
                    create_base_nodes(self.tmpdir, op.join(self.tmpdir, "build"))
            os.chdir(self.tmpdir)
        except:
            shutil.rmtree(self.tmpdir)
            raise

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.tmpdir)

    @mock.patch("bento.commands.build_wininst.create_exe", mock.MagicMock())
    def test_simple(self):
        """This just tests whether create_wininst runs at all and produces a zip-file."""
        ipackage = BuildManifest({}, {"name": "foo", "version": "1.0"}, {})
        create_wininst(ipackage, self.build_node, self.build_node, wininst="foo.exe", output_dir="dist")
        arcname = bento.commands.build_wininst.create_exe.call_args[0][1]
        fp = zipfile.ZipFile(arcname)
        try:
            fp.namelist()
        finally:
            fp.close()

########NEW FILE########
__FILENAME__ = test_wininst_utils
import encodings
import os
import shutil
import tempfile
import mock

try:
    from hashlib import md5
except ImportError:
    from md5 import md5

import os.path as op
import distutils.msvccompiler

from bento._config \
    import \
        WININST_DIR
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.testing.misc \
    import \
        create_simple_build_manifest_args
from bento.commands.wininst_utils \
    import \
        get_inidata, create_exe, get_exe_bytes
from bento.installed_package_description \
    import \
        BuildManifest

from bento.compat.api import moves

class TestWininstUtils(moves.unittest.TestCase):
    def setUp(self):
        self.src_root = tempfile.mkdtemp()
        self.bld_root = op.join(self.src_root, "build")

        root = create_root_with_source_tree(self.src_root, self.bld_root)
        self.top_node = root.find_node(self.src_root)

    def tearDown(self):
        shutil.rmtree(self.top_node.abspath())

    def test_get_inidata_run(self):
        """Simply execute get_inidata."""
        # FIXME: do a real test here
        meta, sections, nodes = create_simple_build_manifest_args(self.top_node)
        build_manifest = BuildManifest(sections, meta, {})
        get_inidata(build_manifest)

    @mock.patch('distutils.msvccompiler.get_build_version', lambda: 9.0)
    @mock.patch('encodings._cache', {"mbcs": encodings.search_function("ascii")})
    def test_create_exe(self):
        # FIXME: do a real test here
        meta, sections, nodes = create_simple_build_manifest_args(self.top_node)
        build_manifest = BuildManifest(sections, meta, {})

        fid, arcname = tempfile.mkstemp(prefix="zip")
        try:
            f = tempfile.NamedTemporaryFile(suffix=".exe")
            try:
                create_exe(build_manifest, arcname, f.name)
            finally:
                f.close()
        finally:
            os.close(fid)

class TestGetExeBytes(moves.unittest.TestCase):
    def _test_get_exe_bytes(self, version, exe_name):
        exe_name = op.join(WININST_DIR, exe_name)
        distutils.msvccompiler.get_build_version = lambda: version
        binary_header = get_exe_bytes()
        fid = open(exe_name, "rb")
        try:
            r_md5 = md5(fid.read())
            self.assertEqual(md5(binary_header).hexdigest(), r_md5.hexdigest())
        finally:
            fid.close()

    @mock.patch('distutils.msvccompiler.get_build_version', lambda: 9.0)
    def test_get_exe_bytes_9(self):
        self._test_get_exe_bytes(9.0, "wininst-9.0.exe")

    @mock.patch('distutils.msvccompiler.get_build_version', lambda: 9.0)
    def test_get_exe_bytes_8(self):
        self._test_get_exe_bytes(8.0, "wininst-8.0.exe")

    @mock.patch('distutils.msvccompiler.get_build_version', lambda: 7.1)
    def test_get_exe_bytes_7_1(self):
        self._test_get_exe_bytes(7.1, "wininst-7.1.exe")

########NEW FILE########
__FILENAME__ = utils
import os.path as op

from six.moves \
    import \
        StringIO

from bento.backends.yaku_backend \
    import \
        ConfigureYakuContext, BuildYakuContext, YakuBackend
from bento.commands.build \
    import \
        BuildCommand
from bento.commands.configure \
    import \
        ConfigureCommand
from bento.commands.contexts \
    import \
        GlobalContext
from bento.commands.options \
    import \
        OptionsContext
from bento.core \
    import \
        PackageDescription, PackageOptions
from bento.core.testing \
    import \
        create_fake_package_from_bento_info

def prepare_configure(run_node, bento_info, context_klass=ConfigureYakuContext, cmd_argv=None):
    if cmd_argv is None:
        cmd_argv = []
    return _prepare_command(run_node, bento_info, ConfigureCommand, context_klass, cmd_argv)

def prepare_build(run_node, bento_info, context_klass=BuildYakuContext, cmd_argv=None):
    if cmd_argv is None:
        cmd_argv = []
    return _prepare_command(run_node, bento_info, BuildCommand, context_klass, cmd_argv)

def _prepare_command(run_node, bento_info, cmd_klass, context_klass, cmd_argv):
    top_node = run_node._ctx.srcnode
    top_node.make_node("bento.info").safe_write(bento_info)

    package = PackageDescription.from_string(bento_info)
    package_options = PackageOptions.from_string(bento_info)

    cmd = cmd_klass()
    options_context = OptionsContext.from_command(cmd)

    cmd.register_options(options_context, package_options)

    global_context = GlobalContext(None)
    global_context.register_package_options(package_options)

    context = context_klass(global_context, cmd_argv, options_context, package, run_node)
    return context, cmd

def create_global_context(package, package_options, backend=None):
    if backend is None:
        backend = YakuBackend()

    global_context = GlobalContext(None)
    global_context.register_package_options(package_options)
    if backend:
        global_context.backend = backend

    build = BuildCommand()
    configure = ConfigureCommand()

    commands = (("configure", configure), ("build", build))

    for cmd_name, cmd in commands:
        global_context.register_command(cmd_name, cmd)
        options_context = OptionsContext.from_command(cmd)
        global_context.register_options_context(cmd_name, options_context)

    global_context.backend.register_command_contexts(global_context)
    global_context.backend.register_options_contexts(global_context)

    return global_context

def prepare_package(top_node, bento_info, backend=None):
    package = PackageDescription.from_string(bento_info)
    package_options = PackageOptions.from_string(bento_info)

    create_fake_package_from_bento_info(top_node, bento_info)
    top_node.make_node("bento.info").safe_write(bento_info)

    return create_global_context(package, package_options, backend)

def prepare_command(global_context, cmd_name, cmd_argv, package, run_node):
    if cmd_argv is None:
        cmd_argv = []

    def _create_context(cmd_name):
        context_klass = global_context.retrieve_command_context(cmd_name)
        options_context = global_context.retrieve_options_context(cmd_name)
        context = context_klass(global_context, cmd_argv, options_context, package, run_node)
        return context

    return _create_context(cmd_name), global_context.retrieve_command(cmd_name)

# Super ugly stuff to make waf and nose happy: nose happily override
# sys.stdout/sys.stderr, and waf expects real files (with encoding and co). We
# fake it until waf is happy
class EncodedStringIO(object):
    def __init__(self):
        self._data = StringIO()
        self.encoding = "ascii"

    def read(self):
        return self._data.read()

    def write(self, data):
        return self._data.write(data)

    def flush(self):
        self._data.flush()

def comparable_installed_sections(sections):
    # Hack to compare compiled sections without having to hardcode the exact,
    # platform-dependent name
    class _InstalledSectionProxy(object):
        def __init__(self, installed_section):
            self.installed_section = installed_section

        def __eq__(self, other):
            _self = self.installed_section
            _other = other.installed_section
            def are_files_comparables():
                if len(_self.files) != len(_other.files):
                    return False
                else:
                    for i in range(len(_self.files)):
                        for j in range(2):
                            if op.dirname(_self.files[i][j]) != \
                                    op.dirname(_other.files[i][j]):
                                return False
                    return True

            return _self.name == _other.name and \
                _self.source_dir == _other.source_dir and \
                _self.target_dir == _other.target_dir and \
                are_files_comparables()

        def __repr__(self):
            return self.installed_section.__repr__()

    def proxy_categories(sections):
        for category in sections:
            for section_name, section in sections[category].items():
                sections[category ][section_name] = _InstalledSectionProxy(section)
        return sections

    return proxy_categories(sections)

########NEW FILE########
__FILENAME__ = upload
from bento.commands.core \
    import \
        Command, Option
from bento.commands.register \
    import \
        _read_pypirc
from bento.pypi.register_utils \
    import \
        DEFAULT_REPOSITORY, PyPIConfig
from bento.pypi.upload_utils \
    import \
        upload

import bento.errors

_SUPPORTED_DISTRIBUTIONS = {"source": "sdist", "egg": "bdist_egg"}

class UploadPyPI(Command):
    long_descr = """\
Purpose: register the package to pypi
Usage: bentomaker register [OPTIONS] distribution_file"""
    short_descr = "register packages to pypi."
    common_options = Command.common_options \
                        + [Option("-r", "--repository",
                                  help="Repository to use in .pypirc"),
                           Option("-u", "--username",
                                  help="Username to use for registration"),
                           Option("-p", "--password",
                                  help="Password to use for registration"),
                           Option("--repository-url",
                                  help="Repository URL to use for registration"),
                           Option("-t", "--distribution-type",
                                  help="Force distribution type (sdist, bdist_wininst, etc...)"),
                                  ]
    def run(self, context):
        o, a = context.get_parsed_arguments()
        if len(a) < 1:
            context.options_context.parser.print_usage()
            # FIXME
            raise NotImplementedError("expected file argument")
        else:
            filename = a[0]

        if o.repository and (o.username or o.password or o.repository_url):
            raise bento.errors.UsageException(
                    "Cannot specify repository and username/password/url at the same time")
        if not (o.repository or (o.username or o.password or o.repository_url)):
            # FIXME: why does distutils use DEFAULT_REPOSITORY (i.e. an url)
            # here ?
            config = _read_pypirc(DEFAULT_REPOSITORY)
        elif o.repository:
            config = _read_pypirc(o.repository)
        else:
            config = PyPIConfig(o.username, o.password, o.repository_url)

        if o.distribution_type is None:
            # FIXME
            raise NotImplementedError("automatic distribution type not yet implemented")
        if not o.distribution_type in _SUPPORTED_DISTRIBUTIONS:
            raise bento.errors.BentoError(
                "Unsupported distribution type %r (supported types: %s)" % \
                (o.distribution_type,
                 ", ".join(repr(i) for i in _SUPPORTED_DISTRIBUTIONS)))

        upload_type = _SUPPORTED_DISTRIBUTIONS[o.distribution_type]
        upload(filename, upload_type, context.pkg, config=config)

########NEW FILE########
__FILENAME__ = utils
def has_cython_code(pkg):
    for extension in pkg.extensions.values():
        for source in extension.sources:
            if source.endswith("pyx"):
                return True

    if pkg.subpackages:
        for spkg in pkg.subpackages.values():
            for extension in spkg.extensions.values():
                for source in extension.sources:
                    if source.endswith("pyx"):
                        return True
    return False

def has_compiled_code(pkg):
    has_compiled_code = len(pkg.extensions) > 0 or len(pkg.compiled_libraries) > 0
    if not has_compiled_code:
        if pkg.subpackages:
            for v in pkg.subpackages.values():
                if len(v.extensions) > 0 or len(v.compiled_libraries) > 0:
                    has_compiled_code = True
                    break
    return has_compiled_code

########NEW FILE########
__FILENAME__ = wininst_utils
import sys
import os
import time

from distutils.util \
    import \
        get_platform
from distutils.sysconfig \
    import \
        get_python_version

from bento._config \
    import \
        WININST_DIR
from bento.core \
    import \
        PackageMetadata
import bento

import six

def wininst_filename(meta, pyver=None):
    if not pyver:
        pyver = ".".join([str(i) for i in sys.version_info[:2]])
    return "%s-py%s.win32.exe" % (meta.fullname, pyver)

# Stolen from distutils.commands.bdist_wininst
# FIXME: improve this code, in particular integration with
# BuildManifest

# FIXME: deal with this correctly, in particular MSVC - most likely we will
# need to hardcode things depending on python versions
def get_exe_bytes (target_version=None, plat_name=None):
    if target_version is None:
        target_version = ""
    if plat_name is None:
        plat_name = get_platform()
    from distutils.msvccompiler import get_build_version
    # If a target-version other than the current version has been
    # specified, then using the MSVC version from *this* build is no good.
    # Without actually finding and executing the target version and parsing
    # its sys.version, we just hard-code our knowledge of old versions.
    # NOTE: Possible alternative is to allow "--target-version" to
    # specify a Python executable rather than a simple version string.
    # We can then execute this program to obtain any info we need, such
    # as the real sys.version string for the build.
    cur_version = get_python_version()
    if target_version and target_version != cur_version:
        raise NotImplementedError("target_version handling not implemented yet.")
        # If the target version is *later* than us, then we assume they
        # use what we use
        # string compares seem wrong, but are what sysconfig.py itself uses
        if self.target_version > cur_version:
            bv = get_build_version()
        else:
            if self.target_version < "2.4":
                bv = 6.0
            else:
                bv = 7.1
    else:
        # for current version - use authoritative check.
        bv = get_build_version()

    # wininst-x.y.exe directory
    # XXX: put those somewhere else, and per-python version preferably
    directory = WININST_DIR
    # we must use a wininst-x.y.exe built with the same C compiler
    # used for python.  XXX What about mingw, borland, and so on?

    # if plat_name starts with "win" but is not "win32"
    # we want to strip "win" and leave the rest (e.g. -amd64)
    # for all other cases, we don't want any suffix
    if plat_name != 'win32' and plat_name[:3] == 'win':
        sfix = plat_name[3:]
    else:
        sfix = ''

    filename = os.path.join(directory, "wininst-%.1f%s.exe" % (bv, sfix))
    return open(filename, "rb").read()

def create_exe(build_manifest, arcname, installer_name, bitmap=None, dist_dir="bento"):
    import struct

    if not os.path.exists(dist_dir):
        os.makedirs(dist_dir)

    cfgdata = get_inidata(build_manifest)

    if bitmap:
        bitmapdata = open(bitmap, "rb").read()
        bitmaplen = len(bitmapdata)
    else:
        bitmaplen = 0

    fid = open(installer_name, "wb")
    fid.write(get_exe_bytes())
    if bitmap:
        fid.write(bitmapdata)

    # Convert cfgdata from unicode to ascii, mbcs encoded
    cfgdata = cfgdata.encode("mbcs") + six.b("\0")
    fid.write(cfgdata)

    # The 'magic number' 0x1234567B is used to make sure that the
    # binary layout of 'cfgdata' is what the wininst.exe binary
    # expects.  If the layout changes, increment that number, make
    # the corresponding changes to the wininst.exe sources, and
    # recompile them.
    header = struct.pack("<iii",
                         0x1234567B,       # tag
                         len(cfgdata),     # length
                         bitmaplen,        # number of bytes in bitmap
                         )
    fid.write(header)
    fid.write(open(arcname, "rb").read())

def get_inidata(build_manifest):
    # Return data describing the installation.
    meta = PackageMetadata.from_build_manifest(build_manifest)

    # Write the [metadata] section.
    lines = []
    lines.append("[metadata]")

    # 'info' will be displayed in the installer's dialog box,
    # describing the items to be installed.
    info = meta.description + '\n'

    # Escape newline characters
    def escape(s):
        return s.replace("\n", "\\n")

    for name in ["author", "author_email", "summary", "maintainer",
                 "maintainer_email", "name", "url", "version"]:
        data = getattr(meta, name)
        if name == "summary":
            name = "description"
        if data:
            info = info + ("\n    %s: %s" % \
                           (name.capitalize(), escape(data)))
            lines.append("%s=%s" % (name, escape(data)))

    # The [setup] section contains entries controlling
    # the installer runtime.
    lines.append("\n[Setup]")
    # FIXME: handle install scripts
    #if self.install_script:
    #    lines.append("install_script=%s" % self.install_script)
    lines.append("info=%s" % escape(info))
    # FIXME: handle this correctly
    lines.append("target_compile=1")
    lines.append("target_optimize=1")
    #if self.target_version:
    #    lines.append("target_version=%s" % self.target_version)
    #if self.user_access_control:
    #    lines.append("user_access_control=%s" % self.user_access_control)

    title = meta.fullname
    lines.append("title=%s" % escape(title))
    build_info = "Built %s with bento-%s" % \
                 (time.ctime(time.time()), bento.__version__)
    lines.append("build_info=%s" % build_info)
    return "\n".join(lines)

########NEW FILE########
__FILENAME__ = wrapper_utils
import os

from bento.compat.api \
    import \
        relpath

from bento.commands.hooks \
    import \
        create_hook_module

def run_with_dependencies(global_context, cmd_name, cmd_argv, run_node, top_node, package):
    """Run the given command, including its dependencies as defined in the
    global_context."""
    deps = global_context.retrieve_dependencies(cmd_name)
    for dep_cmd_name in deps:
        dep_cmd_argv = global_context.retrieve_command_argv(dep_cmd_name)
        resolve_and_run_command(global_context, dep_cmd_name, dep_cmd_argv, run_node, package)
    resolve_and_run_command(global_context, cmd_name, cmd_argv, run_node, package)

def resolve_and_run_command(global_context, cmd_name, cmd_argv, run_node, package):
    """Run the given Command instance inside its context, including any hook
    and/or override."""
    cmd = global_context.retrieve_command(cmd_name)
    context_klass = global_context.retrieve_command_context(cmd_name)
    options_context = global_context.retrieve_options_context(cmd_name)

    context = context_klass(global_context, cmd_argv, options_context, package, run_node)

    pre_hooks = global_context.retrieve_pre_hooks(cmd_name)
    post_hooks = global_context.retrieve_post_hooks(cmd_name)

    run_command_in_context(context, cmd, pre_hooks, post_hooks)

    return cmd, context

def run_command_in_context(context, cmd, pre_hooks=None, post_hooks=None):
    """Run the given command instance with the hooks within its context. """
    if pre_hooks is None:
        pre_hooks = []
    if post_hooks is None:
        post_hooks = []

    top_node = context.top_node
    cmd_funcs = [(cmd.run, top_node.abspath())]

    def _run_hooks(hooks):
        for hook in hooks:
            local_node = top_node.find_dir(relpath(hook.local_dir, top_node.abspath()))
            context.pre_recurse(local_node)
            try:
                hook(context)
            finally:
                context.post_recurse()

    context.init()
    try:
        cmd.init(context)

        _run_hooks(pre_hooks)

        context.configure()

        while cmd_funcs:
            cmd_func, local_dir = cmd_funcs.pop(0)
            local_node = top_node.find_dir(relpath(local_dir, top_node.abspath()))
            context.pre_recurse(local_node)
            try:
                cmd_func(context)
            finally:
                context.post_recurse()

        _run_hooks(post_hooks)

        cmd.finish(context)
    finally:
        context.finish()

    return cmd, context

def set_main(pkg, top_node, build_node):
    modules = []
    hook_files = pkg.hook_files
    for name, spkg in pkg.subpackages.items():
        hook_files.extend([os.path.join(spkg.rdir, h) for h in spkg.hook_files])

    # TODO: find doublons
    for f in hook_files:
        hook_node = top_node.make_node(f)
        if hook_node is None or not os.path.exists(hook_node.abspath()):
            raise ValueError("Hook file %s not found" % f)
        modules.append(create_hook_module(hook_node.abspath()))
    return modules

########NEW FILE########
__FILENAME__ = api
import os
import sys

if os.name == "posix":
    from bento.compat.posix_path \
        import \
            relpath
    rename = os.rename
elif os.name == "nt":
    from bento.compat.nt_path \
        import \
            relpath
    from bento.compat.rename \
        import \
            rename
else:
    raise ImportError("relpath implementation for os %s not included" \
                      % os.name)

try:
    from subprocess \
        import \
            check_call, CalledProcessError
except ImportError:
    from bento.compat._subprocess \
        import \
            check_call, CalledProcessError

if sys.version_info < (2, 6, 0):
    # zipfile for python < 2.6 has some issues with filename encoding, use or
    # own copy
    from bento.compat._zipfile \
        import \
            ZipFile, ZIP_DEFLATED
else:
    from zipfile \
        import \
            ZipFile, ZIP_DEFLATED

if sys.version_info < (2, 6, 0):
    import simplejson as json
else:
    import json

try:
    from collections \
        import \
            defaultdict
except ImportError:
    from bento.compat._collections \
        import \
            defaultdict

if sys.version_info < (2, 6, 0):
    from bento.compat._tempfile \
        import \
            NamedTemporaryFile
else:
    from tempfile \
        import \
            NamedTemporaryFile

if sys.version_info < (2, 5, 0):
    from bento.compat.__tarfile_c \
        import \
            TarFile
else:
    from tarfile \
        import \
            TarFile

if sys.version_info < (3, 0, 0):
    input = raw_input
else:
    input = input

try:
    from functools import partial, wraps
except ImportError:
    from bento.compat._functools import partial, wraps

from bento.compat.misc \
    import \
        MovedModule, _MovedItems

_moved_attributes = []

if sys.version_info < (2, 7, 0):
    _moved_attributes.append(MovedModule("unittest", "unittest2"))
else:
    _moved_attributes.append(MovedModule("unittest", "unittest"))

for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
del attr

moves = sys.modules["bento.compat.api.moves"] = _MovedItems("moves")

########NEW FILE########
__FILENAME__ = dist
import sys
import distutils.dist 

from distutils.util \
    import \
        check_environ, strtobool, rfc822_escape

# Taken from python 2.6.5
PKG_INFO_ENCODING = 'utf-8'
class _DistributionMetadata:
    """Dummy class to hold the distribution meta-data: name, version,
    author, and so forth.
    """

    _METHOD_BASENAMES = ("name", "version", "author", "author_email",
                         "maintainer", "maintainer_email", "url",
                         "license", "description", "long_description",
                         "keywords", "platforms", "fullname", "contact",
                         "contact_email", "license", "classifiers",
                         "download_url",
                         # PEP 314
                         "provides", "requires", "obsoletes",
                         )

    def __init__ (self):
        self.name = None
        self.version = None
        self.author = None
        self.author_email = None
        self.maintainer = None
        self.maintainer_email = None
        self.url = None
        self.license = None
        self.description = None
        self.long_description = None
        self.keywords = None
        self.platforms = None
        self.classifiers = None
        self.download_url = None
        # PEP 314
        self.provides = None
        self.requires = None
        self.obsoletes = None

    def write_pkg_info (self, base_dir):
        """Write the PKG-INFO file into the release tree.
        """
        pkg_info = open( os.path.join(base_dir, 'PKG-INFO'), 'w')

        self.write_pkg_file(pkg_info)

        pkg_info.close()

    # write_pkg_info ()

    def write_pkg_file (self, file):
        """Write the PKG-INFO format data to a file object.
        """
        version = '1.0'
        if (self.provides or self.requires or self.obsoletes or
            self.classifiers or self.download_url):
            version = '1.1'

        self._write_field(file, 'Metadata-Version', version)
        self._write_field(file, 'Name', self.get_name())
        self._write_field(file, 'Version', self.get_version())
        self._write_field(file, 'Summary', self.get_description())
        self._write_field(file, 'Home-page', self.get_url())
        self._write_field(file, 'Author', self.get_contact())
        self._write_field(file, 'Author-email', self.get_contact_email())
        self._write_field(file, 'License', self.get_license())
        if self.download_url:
            self._write_field(file, 'Download-URL', self.download_url)

        long_desc = rfc822_escape( self.get_long_description())
        self._write_field(file, 'Description', long_desc)

        keywords = ",".join(self.get_keywords())
        if keywords:
            self._write_field(file, 'Keywords', keywords)

        self._write_list(file, 'Platform', self.get_platforms())
        self._write_list(file, 'Classifier', self.get_classifiers())

        # PEP 314
        self._write_list(file, 'Requires', self.get_requires())
        self._write_list(file, 'Provides', self.get_provides())
        self._write_list(file, 'Obsoletes', self.get_obsoletes())

    def _write_field(self, file, name, value):
        file.write('%s: %s\n' % (name, self._encode_field(value)))

    def _write_list (self, file, name, values):

        for value in values:
            self._write_field(file, name, value)

    def _encode_field(self, value):
        if value is None:
            return None
        if isinstance(value, unicode):
            return value.encode(PKG_INFO_ENCODING)
        return str(value)

    # -- Metadata query methods ----------------------------------------

    def get_name (self):
        return self.name or "UNKNOWN"

    def get_version(self):
        return self.version or "0.0.0"

    def get_fullname (self):
        return "%s-%s" % (self.get_name(), self.get_version())

    def get_author(self):
        return self._encode_field(self.author) or "UNKNOWN"

    def get_author_email(self):
        return self.author_email or "UNKNOWN"

    def get_maintainer(self):
        return self._encode_field(self.maintainer) or "UNKNOWN"

    def get_maintainer_email(self):
        return self.maintainer_email or "UNKNOWN"

    def get_contact(self):
        return (self._encode_field(self.maintainer) or
                self._encode_field(self.author) or "UNKNOWN")

    def get_contact_email(self):
        return (self.maintainer_email or
                self.author_email or
                "UNKNOWN")

    def get_url(self):
        return self.url or "UNKNOWN"

    def get_license(self):
        return self.license or "UNKNOWN"
    get_licence = get_license

    def get_description(self):
        return self._encode_field(self.description) or "UNKNOWN"

    def get_long_description(self):
        return self._encode_field(self.long_description) or "UNKNOWN"

    def get_keywords(self):
        return self.keywords or []

    def get_platforms(self):
        return self.platforms or ["UNKNOWN"]

    def get_classifiers(self):
        return self.classifiers or []

    def get_download_url(self):
        return self.download_url or "UNKNOWN"

    # PEP 314

    def get_requires(self):
        return self.requires or []

    def set_requires(self, value):
        import distutils.versionpredicate
        for v in value:
            distutils.versionpredicate.VersionPredicate(v)
        self.requires = value

    def get_provides(self):
        return self.provides or []

    def set_provides(self, value):
        value = [v.strip() for v in value]
        for v in value:
            import distutils.versionpredicate
            distutils.versionpredicate.split_provision(v)
        self.provides = value

    def get_obsoletes(self):
        return self.obsoletes or []

    def set_obsoletes(self, value):
        import distutils.versionpredicate
        for v in value:
            distutils.versionpredicate.VersionPredicate(v)
        self.obsoletes = value

if sys.version_info[0] < 3:
    DistributionMetadata = _DistributionMetadata
else:
    DistributionMetadata = distutils.dist.DistributionMetadata

########NEW FILE########
__FILENAME__ = inspect
"""Subset of inspect module from upstream python

We use this instead of upstream because upstream inspect is slow to import, and
significanly contributes to numpy import times. Importing this copy has almost
no overhead.
"""

import os
import sys
import types
import string
import imp
import linecache
import re

__all__ = ['getargspec', 'formatargspec']

# ----------------------------------------------------------- type-checking
def ismodule(object):
    """Return true if the object is a module.

    Module objects provide these attributes:
        __doc__         documentation string
        __file__        filename (missing for built-in modules)"""
    return isinstance(object, types.ModuleType)

def isclass(object):
    """Return true if the object is a class.

    Class objects provide these attributes:
        __doc__         documentation string
        __module__      name of module in which this class was defined"""
    if sys.version_info[0] < 3:
        return isinstance(object, types.ClassType) or hasattr(object, '__bases__')
    else:
        return isinstance(object, type) or hasattr(object, '__bases__')

def ismethod(object):
    """Return true if the object is an instance method.

    Instance method objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this method was defined
        im_class        class object in which this method belongs
        im_func         function object containing implementation of method
        im_self         instance to which this method is bound, or None"""
    return isinstance(object, types.MethodType)

def isfunction(object):
    """Return true if the object is a user-defined function.

    Function objects provide these attributes:
        __doc__         documentation string
        __name__        name with which this function was defined
        func_code       code object containing compiled function bytecode
        func_defaults   tuple of any default values for arguments
        func_doc        (same as __doc__)
        func_globals    global namespace in which this function was defined
        func_name       (same as __name__)"""
    return isinstance(object, types.FunctionType)

def istraceback(object):
    """Return true if the object is a traceback.

    Traceback objects provide these attributes:
        tb_frame        frame object at this level
        tb_lasti        index of last attempted instruction in bytecode
        tb_lineno       current line number in Python source code
        tb_next         next inner traceback object (called by this level)"""
    return isinstance(object, types.TracebackType)

def isframe(object):
    """Return true if the object is a frame object.

    Frame objects provide these attributes:
        f_back          next outer frame object (this frame's caller)
        f_builtins      built-in namespace seen by this frame
        f_code          code object being executed in this frame
        f_exc_traceback traceback if raised in this frame, or None
        f_exc_type      exception type if raised in this frame, or None
        f_exc_value     exception value if raised in this frame, or None
        f_globals       global namespace seen by this frame
        f_lasti         index of last attempted instruction in bytecode
        f_lineno        current line number in Python source code
        f_locals        local namespace seen by this frame
        f_restricted    0 or 1 if frame is in restricted execution mode
        f_trace         tracing function for this frame, or None"""
    return isinstance(object, types.FrameType)

def iscode(object):
    """Return true if the object is a code object.

    Code objects provide these attributes:
        co_argcount     number of arguments (not including * or ** args)
        co_code         string of raw compiled bytecode
        co_consts       tuple of constants used in the bytecode
        co_filename     name of file in which this code object was created
        co_firstlineno  number of first line in Python source code
        co_flags        bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg
        co_lnotab       encoded mapping of line numbers to bytecode indices
        co_name         name with which this code object was defined
        co_names        tuple of names of local variables
        co_nlocals      number of local variables
        co_stacksize    virtual machine stack space required
        co_varnames     tuple of names of arguments and local variables"""
    return isinstance(object, types.CodeType)

# ------------------------------------------------ argument list extraction
# These constants are from Python's compile.h.
CO_OPTIMIZED, CO_NEWLOCALS, CO_VARARGS, CO_VARKEYWORDS = 1, 2, 4, 8

def getargs(co):
    """Get information about the arguments accepted by a code object.

    Three things are returned: (args, varargs, varkw), where 'args' is
    a list of argument names (possibly containing nested lists), and
    'varargs' and 'varkw' are the names of the * and ** arguments or None."""

    if not iscode(co):
        raise TypeError('arg is not a code object')

    code = co.co_code
    nargs = co.co_argcount
    names = co.co_varnames
    args = list(names[:nargs])
    step = 0

    # The following acrobatics are for anonymous (tuple) arguments.
    for i in range(nargs):
        if args[i][:1] in ['', '.']:
            stack, remain, count = [], [], []
            while step < len(code):
                op = ord(code[step])
                step = step + 1
                if op >= dis.HAVE_ARGUMENT:
                    opname = dis.opname[op]
                    value = ord(code[step]) + ord(code[step+1])*256
                    step = step + 2
                    if opname in ['UNPACK_TUPLE', 'UNPACK_SEQUENCE']:
                        remain.append(value)
                        count.append(value)
                    elif opname == 'STORE_FAST':
                        stack.append(names[value])

                        # Special case for sublists of length 1: def foo((bar))
                        # doesn't generate the UNPACK_TUPLE bytecode, so if
                        # `remain` is empty here, we have such a sublist.
                        if not remain:
                            stack[0] = [stack[0]]
                            break
                        else:
                            remain[-1] = remain[-1] - 1
                            while remain[-1] == 0:
                                remain.pop()
                                size = count.pop()
                                stack[-size:] = [stack[-size:]]
                                if not remain: break
                                remain[-1] = remain[-1] - 1
                            if not remain: break
            args[i] = stack[0]

    varargs = None
    if co.co_flags & CO_VARARGS:
        varargs = co.co_varnames[nargs]
        nargs = nargs + 1
    varkw = None
    if co.co_flags & CO_VARKEYWORDS:
        varkw = co.co_varnames[nargs]
    return args, varargs, varkw

def getargspec(func):
    """Get the names and default values of a function's arguments.

    A tuple of four things is returned: (args, varargs, varkw, defaults).
    'args' is a list of the argument names (it may contain nested lists).
    'varargs' and 'varkw' are the names of the * and ** arguments or None.
    'defaults' is an n-tuple of the default values of the last n arguments.
    """

    if ismethod(func):
        func = func.im_func
    if not isfunction(func):
        raise TypeError('arg is not a Python function')
    args, varargs, varkw = getargs(func.func_code)
    return args, varargs, varkw, func.func_defaults

def getargvalues(frame):
    """Get information about arguments passed into a particular frame.

    A tuple of four things is returned: (args, varargs, varkw, locals).
    'args' is a list of the argument names (it may contain nested lists).
    'varargs' and 'varkw' are the names of the * and ** arguments or None.
    'locals' is the locals dictionary of the given frame."""
    args, varargs, varkw = getargs(frame.f_code)
    return args, varargs, varkw, frame.f_locals

def joinseq(seq):
    if len(seq) == 1:
        return '(' + seq[0] + ',)'
    else:
        return '(' + ', '.join(seq) + ')'

def strseq(object, convert, join=joinseq):
    """Recursively walk a sequence, stringifying each element."""
    if type(object) in [types.ListType, types.TupleType]:
        return join(map(lambda o, c=convert, j=join: strseq(o, c, j), object))
    else:
        return convert(object)

def formatargspec(args, varargs=None, varkw=None, defaults=None,
                  formatarg=str,
                  formatvarargs=lambda name: '*' + name,
                  formatvarkw=lambda name: '**' + name,
                  formatvalue=lambda value: '=' + repr(value),
                  join=joinseq):
    """Format an argument spec from the 4 values returned by getargspec.

    The first four arguments are (args, varargs, varkw, defaults).  The
    other four arguments are the corresponding optional formatting functions
    that are called to turn names and values into strings.  The ninth
    argument is an optional function to format the sequence of arguments."""
    specs = []
    if defaults:
        firstdefault = len(args) - len(defaults)
    for i in range(len(args)):
        spec = strseq(args[i], formatarg, join)
        if defaults and i >= firstdefault:
            spec = spec + formatvalue(defaults[i - firstdefault])
        specs.append(spec)
    if varargs is not None:
        specs.append(formatvarargs(varargs))
    if varkw is not None:
        specs.append(formatvarkw(varkw))
    return '(' + ', '.join(specs) + ')'

def formatargvalues(args, varargs, varkw, locals,
                    formatarg=str,
                    formatvarargs=lambda name: '*' + name,
                    formatvarkw=lambda name: '**' + name,
                    formatvalue=lambda value: '=' + repr(value),
                    join=joinseq):
    """Format an argument spec from the 4 values returned by getargvalues.

    The first four arguments are (args, varargs, varkw, locals).  The
    next four arguments are the corresponding optional formatting functions
    that are called to turn names and values into strings.  The ninth
    argument is an optional function to format the sequence of arguments."""
    def convert(name, locals=locals,
                formatarg=formatarg, formatvalue=formatvalue):
        return formatarg(name) + formatvalue(locals[name])
    specs = []
    for i in range(len(args)):
        specs.append(strseq(args[i], convert, join))
    if varargs:
        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))
    if varkw:
        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))
    return '(' + string.join(specs, ', ') + ')'

# -------------------------------------------------- source code extraction
def indentsize(line):
    """Return the indent size, in spaces, at the start of a line of text."""
    expline = string.expandtabs(line)
    return len(expline) - len(string.lstrip(expline))

def getdoc(object):
    """Get the documentation string for an object.

    All tabs are expanded to spaces.  To clean up docstrings that are
    indented to line up with blocks of code, any whitespace than can be
    uniformly removed from the second line onwards is removed."""
    try:
        doc = object.__doc__
    except AttributeError:
        return None
    if not isinstance(doc, types.StringTypes):
        return None
    try:
        lines = string.split(string.expandtabs(doc), '\n')
    except UnicodeError:
        return None
    else:
        # Find minimum indentation of any non-blank lines after first line.
        margin = sys.maxint
        for line in lines[1:]:
            content = len(string.lstrip(line))
            if content:
                indent = len(line) - content
                margin = min(margin, indent)
        # Remove indentation.
        if lines:
            lines[0] = lines[0].lstrip()
        if margin < sys.maxint:
            for i in range(1, len(lines)): lines[i] = lines[i][margin:]
        # Remove any trailing or leading blank lines.
        while lines and not lines[-1]:
            lines.pop()
        while lines and not lines[0]:
            lines.pop(0)
        return string.join(lines, '\n')

def getfile(object):
    """Work out which source or compiled file an object was defined in."""
    if ismodule(object):
        if hasattr(object, '__file__'):
            return object.__file__
        raise TypeError('arg is a built-in module')
    if isclass(object):
        object = sys.modules.get(object.__module__)
        if hasattr(object, '__file__'):
            return object.__file__
        raise TypeError('arg is a built-in class')
    if ismethod(object):
        object = object.im_func
    if isfunction(object):
        object = object.func_code
    if istraceback(object):
        object = object.tb_frame
    if isframe(object):
        object = object.f_code
    if iscode(object):
        return object.co_filename
    raise TypeError('arg is not a module, class, method, '
                    'function, traceback, frame, or code object')

def findsource(object):
    """Return the entire source file and starting line number for an object.

    The argument may be a module, class, method, function, traceback, frame,
    or code object.  The source code is returned as a list of all the lines
    in the file and the line number indexes a line in that list.  An IOError
    is raised if the source code cannot be retrieved."""
    file = getsourcefile(object) or getfile(object)
    lines = linecache.getlines(file)
    if not lines:
        raise IOError('could not get source code')

    if ismodule(object):
        return lines, 0

    if isclass(object):
        name = object.__name__
        pat = re.compile(r'^\s*class\s*' + name + r'\b')
        for i in range(len(lines)):
            if pat.match(lines[i]): return lines, i
        else:
            raise IOError('could not find class definition')

    if ismethod(object):
        object = object.im_func
    if isfunction(object):
        object = object.func_code
    if istraceback(object):
        object = object.tb_frame
    if isframe(object):
        object = object.f_code
    if iscode(object):
        if not hasattr(object, 'co_firstlineno'):
            raise IOError('could not find function definition')
        lnum = object.co_firstlineno - 1
        pat = re.compile(r'^(\s*def\s)|(.*(?<!\w)lambda(:|\s))|^(\s*@)')
        while lnum > 0:
            if pat.match(lines[lnum]): break
            lnum = lnum - 1
        return lines, lnum
    raise IOError('could not find code object')

def getmoduleinfo(path):
    """Get the module name, suffix, mode, and module type for a given file."""
    filename = os.path.basename(path)
    def _f(suffix, mode, mtype):
       return (-len(suffix), suffix, mode, mtype)
    suffixes = map(_f, imp.get_suffixes())
    suffixes.sort() # try longest suffixes first, in case they overlap
    for neglen, suffix, mode, mtype in suffixes:
        if filename[neglen:] == suffix:
            return filename[:neglen], suffix, mode, mtype

def getmodulename(path):
    """Return the module name for a given file, or None."""
    info = getmoduleinfo(path)
    if info: return info[0]

def getsourcefile(object):
    """Return the Python source file an object was defined in, if it exists."""
    filename = getfile(object)
    if filename[-4:].lower() in ['.pyc', '.pyo']:
        filename = filename[:-4] + '.py'
    for suffix, mode, kind in imp.get_suffixes():
        if 'b' in mode and filename[-len(suffix):].lower() == suffix:
            # Looks like a binary file.  We want to only return a text file.
            return None
    if os.path.exists(filename):
        return filename

# -------------------------------------------------- stack frame extraction
def getframeinfo(frame, context=1):
    """Get information about a frame or traceback object.

    A tuple of five things is returned: the filename, the line number of
    the current line, the function name, a list of lines of context from
    the source code, and the index of the current line within that list.
    The optional second argument specifies the number of lines of context
    to return, which are centered around the current line."""
    if istraceback(frame):
        lineno = frame.tb_lineno
        frame = frame.tb_frame
    else:
        lineno = frame.f_lineno
    if not isframe(frame):
        raise TypeError('arg is not a frame or traceback object')

    filename = getsourcefile(frame) or getfile(frame)
    if context > 0:
        start = lineno - 1 - context//2
        try:
            lines, lnum = findsource(frame)
        except IOError:
            lines = index = None
        else:
            start = max(start, 1)
            start = max(0, min(start, len(lines) - context))
            lines = lines[start:start+context]
            index = lineno - 1 - start
    else:
        lines = index = None

    return (filename, lineno, frame.f_code.co_name, lines, index)

def getlineno(frame):
    """Get the line number from a frame object, allowing for optimization."""
    # FrameType.f_lineno is now a descriptor that grovels co_lnotab
    return frame.f_lineno

def getouterframes(frame, context=1):
    """Get a list of records for a frame and all higher (calling) frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context."""
    framelist = []
    while frame:
        framelist.append((frame,) + getframeinfo(frame, context))
        frame = frame.f_back
    return framelist

def getinnerframes(tb, context=1):
    """Get a list of records for a traceback's frame and all lower frames.

    Each record contains a frame object, filename, line number, function
    name, a list of lines of context, and index within the context."""
    framelist = []
    while tb:
        framelist.append((tb.tb_frame,) + getframeinfo(tb, context))
        tb = tb.tb_next
    return framelist

currentframe = sys._getframe

def stack(context=1):
    """Return a list of records for the stack above the caller's frame."""
    return getouterframes(sys._getframe(1), context)

if __name__ == '__main__':
    import inspect
    def foo(x, y, z=None):
        return None

    print(inspect.getargs(foo.func_code))
    print(getargs(foo.func_code))

    print(inspect.getargspec(foo))
    print(getargspec(foo))

    print(inspect.formatargspec(*inspect.getargspec(foo)))
    print(formatargspec(*getargspec(foo)))

########NEW FILE########
__FILENAME__ = misc
# Most of the code below is shamelessly copied from the six library from
# Benjamin Peterson
import sys
import types

PY3 = sys.version_info[0] == 3

def _import_module(name):
    """Import module, returning the module after the last dot."""
    __import__(name)
    return sys.modules[name]

class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)
        # This is a bit ugly, but it avoids running this again.
        delattr(tp, self.name)
        return result

class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)

class _MovedItems(types.ModuleType):
    """Lazy loading of moved objects"""

########NEW FILE########
__FILENAME__ = nt_path
from os.path \
    import \
        abspath, splitunc, curdir, sep, pardir, join

def relpath(path, start=curdir):
    """Return a relative version of a path"""

    if not path:
        raise ValueError("no path specified")
    start_list = abspath(start).split(sep)
    path_list = abspath(path).split(sep)
    if start_list[0].lower() != path_list[0].lower():
        unc_path, rest = splitunc(path)
        unc_start, rest = splitunc(start)
        if bool(unc_path) ^ bool(unc_start):
            raise ValueError("Cannot mix UNC and non-UNC paths (%s and %s)"
                                                                % (path, start))
        else:
            raise ValueError("path is on drive %s, start on drive %s"
                                                % (path_list[0], start_list[0]))
    # Work out how much of the filepath is shared by start and path.
    for i in range(min(len(start_list), len(path_list))):
        if start_list[i].lower() != path_list[i].lower():
            break
    else:
        i += 1

    rel_list = [pardir] * (len(start_list)-i) + path_list[i:]
    if not rel_list:
        return curdir
    return join(*rel_list)

########NEW FILE########
__FILENAME__ = posix_path
from os.path \
    import \
        abspath, commonprefix, join

# strings representing various path-related bits and pieces
curdir = '.'
pardir = '..'
sep = '/'

def relpath(path, start=curdir):
    """Return a relative version of a path"""

    if not path:
        raise ValueError("no path specified")

    start_list = abspath(start).split(sep)
    path_list = abspath(path).split(sep)

    # Work out how much of the filepath is shared by start and path.
    i = len(commonprefix([start_list, path_list]))

    rel_list = [pardir] * (len(start_list)-i) + path_list[i:]
    if not rel_list:
        return curdir
    return join(*rel_list)

########NEW FILE########
__FILENAME__ = rename
import os.path
import os
import random
import errno

def rename(src, dst):
    "Atomic rename on windows."
    # This is taken from mercurial
    try:
        os.rename(src, dst)
    except OSError:
        # If dst exists, rename will fail on windows, and we cannot
        # unlink an opened file. Instead, the destination is moved to
        # a temporary location if it already exists.

        def tempname(prefix):
            for i in range(5):
                fn = '%s-%08x' % (prefix, random.randint(0, 0xffffffff))
                if not os.path.exists(fn):
                    return fn
            raise IOError((errno.EEXIST, "No usable temporary filename found"))

        temp = tempname(dst)
        os.rename(dst, temp)
        try:
            os.unlink(temp)
        except:
            # Some rude AV-scanners on Windows may cause the unlink to
            # fail. Not aborting here just leaks the temp file, whereas
            # aborting at this point may leave serious inconsistencies.
            # Ideally, we would notify the user here.
            pass
        os.rename(src, dst)

########NEW FILE########
__FILENAME__ = test_tempfile
import sys
import os
import re
import tempfile

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves import unittest

# Finicky: to make sure we test our NamedTemporaryFile and not something else...
tempfile.NamedTemporaryFile = NamedTemporaryFile

def _bytes(s):
    if sys.version_info[0] < 3:
        return s
    else:
        return bytes(s.encode("ascii"))

# Code below copied verbatim from python 2.6.4 test_tempfile.py file
class TC(unittest.TestCase):

    str_check = re.compile(r"[a-zA-Z0-9_-]{6}$")

    def failOnException(self, what, ei=None):
        if ei is None:
            ei = sys.exc_info()
        self.fail("%s raised %s: %s" % (what, ei[0], ei[1]))

    def nameCheck(self, name, dir, pre, suf):
        (ndir, nbase) = os.path.split(name)
        npre  = nbase[:len(pre)]
        nsuf  = nbase[len(nbase)-len(suf):]

        # check for equality of the absolute paths!
        self.assertEqual(os.path.abspath(ndir), os.path.abspath(dir),
                         "file '%s' not in directory '%s'" % (name, dir))
        self.assertEqual(npre, pre,
                         "file '%s' does not begin with '%s'" % (nbase, pre))
        self.assertEqual(nsuf, suf,
                         "file '%s' does not end with '%s'" % (nbase, suf))

        nbase = nbase[len(pre):len(nbase)-len(suf)]
        self.assert_(self.str_check.match(nbase),
                     "random string '%s' does not match /^[a-zA-Z0-9_-]{6}$/"
                     % nbase)

class test_NamedTemporaryFile(TC):
    """Test NamedTemporaryFile()."""

    def do_create(self, dir=None, pre="", suf="", delete=True):
        if dir is None:
            dir = tempfile.gettempdir()
        try:
            file = tempfile.NamedTemporaryFile(dir=dir, prefix=pre, suffix=suf,
                                               delete=delete)
        except:
            self.failOnException("NamedTemporaryFile")

        self.nameCheck(file.name, dir, pre, suf)
        return file


    def test_basic(self):
        # NamedTemporaryFile can create files
        self.do_create()
        self.do_create(pre="a")
        self.do_create(suf="b")
        self.do_create(pre="a", suf="b")
        self.do_create(pre="aa", suf=".txt")

    def test_creates_named(self):
        # NamedTemporaryFile creates files with names
        f = tempfile.NamedTemporaryFile()
        self.failUnless(os.path.exists(f.name),
                        "NamedTemporaryFile %s does not exist" % f.name)

    def test_del_on_close(self):
        # A NamedTemporaryFile is deleted when closed
        dir = tempfile.mkdtemp()
        try:
            f = tempfile.NamedTemporaryFile(dir=dir)
            f.write(_bytes('blat'))
            f.close()
            self.failIf(os.path.exists(f.name),
                        "NamedTemporaryFile %s exists after close" % f.name)
        finally:
            os.rmdir(dir)

    def test_dis_del_on_close(self):
        # Tests that delete-on-close can be disabled
        dir = tempfile.mkdtemp()
        tmp = None
        try:
            f = tempfile.NamedTemporaryFile(dir=dir, delete=False)
            tmp = f.name
            f.write(_bytes('blat'))
            f.close()
            self.failUnless(os.path.exists(f.name),
                        "NamedTemporaryFile %s missing after close" % f.name)
        finally:
            if tmp is not None:
                os.unlink(tmp)
            os.rmdir(dir)

    def test_multiple_close(self):
        # A NamedTemporaryFile can be closed many times without error
        f = tempfile.NamedTemporaryFile()
        f.write(_bytes('abc\n'))
        f.close()
        try:
            f.close()
            f.close()
        except:
            self.failOnException("close")

########NEW FILE########
__FILENAME__ = _collections
# Pure python implementation of defaultdict, taken from Python Recipe 523034 by
# Jason Kirtland
class defaultdict(dict):
    def __init__(self, default_factory=None, *a, **kw):
        if (default_factory is not None and
            not hasattr(default_factory, '__call__')):
            raise TypeError('first argument must be callable')
        dict.__init__(self, *a, **kw)
        self.default_factory = default_factory

    def __getitem__(self, key):
        try:
            return dict.__getitem__(self, key)
        except KeyError:
            return self.__missing__(key)

    def __missing__(self, key):
        if self.default_factory is None:
            raise KeyError(key)
        self[key] = value = self.default_factory()
        return value

    def __reduce__(self):
        if self.default_factory is None:
            args = tuple()
        else:
            args = self.default_factory,
        return type(self), args, None, None, self.items()

    def copy(self):
        return self.__copy__()

    def __copy__(self):
        return type(self)(self.default_factory, self)

    def __deepcopy__(self, memo):
        import copy
        return type(self)(self.default_factory,
                          copy.deepcopy(self.items()))

    def __repr__(self):
        return 'defaultdict(%s, %s)' % (self.default_factory,
                                        dict.__repr__(self))

########NEW FILE########
__FILENAME__ = _functools
WRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__doc__')
WRAPPER_UPDATES = ('__dict__',)
def update_wrapper(wrapper,
                   wrapped,
                   assigned = WRAPPER_ASSIGNMENTS,
                   updated = WRAPPER_UPDATES):
    """Update a wrapper function to look like the wrapped function

       wrapper is the function to be updated
       wrapped is the original function
       assigned is a tuple naming the attributes assigned directly
       from the wrapped function to the wrapper function (defaults to
       functools.WRAPPER_ASSIGNMENTS)
       updated is a tuple naming the attributes of the wrapper that
       are updated with the corresponding attribute from the wrapped
       function (defaults to functools.WRAPPER_UPDATES)
    """
    for attr in assigned:
        setattr(wrapper, attr, getattr(wrapped, attr))
    for attr in updated:
        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
    # Return the wrapper so this can be used as a decorator via partial()
    return wrapper

def wraps(wrapped,
          assigned = WRAPPER_ASSIGNMENTS,
          updated = WRAPPER_UPDATES):
    """Decorator factory to apply update_wrapper() to a wrapper function

       Returns a decorator that invokes update_wrapper() with the decorated
       function as the wrapper argument and the arguments to wraps() as the
       remaining arguments. Default arguments are as for update_wrapper().
       This is a convenience function to simplify applying partial() to
       update_wrapper().
    """
    return partial(update_wrapper, wrapped=wrapped,
                   assigned=assigned, updated=updated)

def partial(func, *a, **kw):
    def f(*_a, **_kw):
        new_a = list(a) + list(_a)
        new_kw = kw
        new_kw.update(_kw)
        return func(*new_a, **new_kw)
    f = update_wrapper(f, func)
    return f


########NEW FILE########
__FILENAME__ = _subprocess
from subprocess \
    import \
        call

# Code taken from python 2.6.5
class CalledProcessError(Exception):
    """This exception is raised when a process run by check_call() returns
    a non-zero exit status.  The exit status will be stored in the
    returncode attribute."""
    def __init__(self, returncode, cmd):
        self.returncode = returncode
        self.cmd = cmd
    def __str__(self):
        return "Command '%s' returned non-zero exit status %d" % (self.cmd, self.returncode)

def check_call(*popenargs, **kwargs):
    """Run command with arguments.  Wait for command to complete.  If
    the exit code was zero then return, otherwise raise
    CalledProcessError.  The CalledProcessError object will have the
    return code in the returncode attribute.

    The arguments are the same as for the Popen constructor.  Example:

    check_call(["ls", "-l"])
    """
    retcode = call(*popenargs, **kwargs)
    cmd = kwargs.get("args")
    if cmd is None:
        cmd = popenargs[0]
    if retcode:
        raise CalledProcessError(retcode, cmd)
    return retcode



########NEW FILE########
__FILENAME__ = _tempfile
"""
NamedTemporaryFile with delete option for python 2.4/2.5
"""
import os as _os
from tempfile \
    import \
        gettempdir, template

# Importing private stuff is ok here as this is for compat, and won't change
# (checked that python 2.4.4 has those values, has not checked for 2.5.*, but
# 2.6.4 define those to same values...
from tempfile \
    import \
        _text_openflags, _bin_openflags, _mkstemp_inner

# Code below taken verbatim from python 2.6.4
class _TemporaryFileWrapper:
    """Temporary file wrapper

    This class provides a wrapper around files opened for
    temporary use.  In particular, it seeks to automatically
    remove the file when it is no longer needed.
    """

    def __init__(self, file, name, delete=True):
        self.file = file
        self.name = name
        self.close_called = False
        self.delete = delete

    def __getattr__(self, name):
        # Attribute lookups are delegated to the underlying file
        # and cached for non-numeric results
        # (i.e. methods are cached, closed and friends are not)
        file = self.__dict__['file']
        a = getattr(file, name)
        if not issubclass(type(a), type(0)):
            setattr(self, name, a)
        return a

    # The underlying __enter__ method returns the wrong object
    # (self.file) so override it to return the wrapper
    def __enter__(self):
        self.file.__enter__()
        return self

    # NT provides delete-on-close as a primitive, so we don't need
    # the wrapper to do anything special.  We still use it so that
    # file.name is useful (i.e. not "(fdopen)") with NamedTemporaryFile.
    if _os.name != 'nt':
        # Cache the unlinker so we don't get spurious errors at
        # shutdown when the module-level "os" is None'd out.  Note
        # that this must be referenced as self.unlink, because the
        # name TemporaryFileWrapper may also get None'd out before
        # __del__ is called.
        unlink = _os.unlink

        def close(self):
            if not self.close_called:
                self.close_called = True
                self.file.close()
                if self.delete:
                    self.unlink(self.name)

        def __del__(self):
            self.close()

        # Need to trap __exit__ as well to ensure the file gets
        # deleted when used in a with statement
        def __exit__(self, exc, value, tb):
            result = self.file.__exit__(exc, value, tb)
            self.close()
            return result


def NamedTemporaryFile(mode='w+b', bufsize=-1, suffix="",
                       prefix=template, dir=None, delete=True):
    """Create and return a temporary file.
    Arguments:
    'prefix', 'suffix', 'dir' -- as for mkstemp.
    'mode' -- the mode argument to os.fdopen (default "w+b").
    'bufsize' -- the buffer size argument to os.fdopen (default -1).
    'delete' -- whether the file is deleted on close (default True).
    The file is created as mkstemp() would do it.

    Returns an object with a file-like interface; the name of the file
    is accessible as file.name.  The file will be automatically deleted
    when it is closed unless the 'delete' argument is set to False.
    """

    if dir is None:
        dir = gettempdir()

    if 'b' in mode:
        flags = _bin_openflags
    else:
        flags = _text_openflags

    # Setting O_TEMPORARY in the flags causes the OS to delete
    # the file when it is closed.  This is only supported by Windows.
    if _os.name == 'nt' and delete:
        flags |= _os.O_TEMPORARY

    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)
    file = _os.fdopen(fd, mode, bufsize)
    return _TemporaryFileWrapper(file, name, delete)


########NEW FILE########
__FILENAME__ = _zipfile
"""
Read and write ZIP files.
"""
import struct, os, time, sys, shutil
import binascii, cStringIO, stat

try:
    import zlib # We may need its compression method
    crc32 = zlib.crc32
except ImportError:
    zlib = None
    crc32 = binascii.crc32

__all__ = ["BadZipfile", "error", "ZIP_STORED", "ZIP_DEFLATED", "is_zipfile",
           "ZipInfo", "ZipFile", "PyZipFile", "LargeZipFile" ]

class BadZipfile(Exception):
    pass


class LargeZipFile(Exception):
    """
    Raised when writing a zipfile, the zipfile requires ZIP64 extensions
    and those extensions are disabled.
    """

error = BadZipfile      # The exception raised by this module

ZIP64_LIMIT = (1 << 31) - 1
ZIP_FILECOUNT_LIMIT = 1 << 16
ZIP_MAX_COMMENT = (1 << 16) - 1

# constants for Zip file compression methods
ZIP_STORED = 0
ZIP_DEFLATED = 8
# Other ZIP compression methods not supported

# Below are some formats and associated data for reading/writing headers using
# the struct module.  The names and structures of headers/records are those used
# in the PKWARE description of the ZIP file format:
#     http://www.pkware.com/documents/casestudies/APPNOTE.TXT
# (URL valid as of January 2008)

# The "end of central directory" structure, magic number, size, and indices
# (section V.I in the format document)
structEndArchive = "<4s4H2LH"
stringEndArchive = "PK\005\006"
sizeEndCentDir = struct.calcsize(structEndArchive)

_ECD_SIGNATURE = 0
_ECD_DISK_NUMBER = 1
_ECD_DISK_START = 2
_ECD_ENTRIES_THIS_DISK = 3
_ECD_ENTRIES_TOTAL = 4
_ECD_SIZE = 5
_ECD_OFFSET = 6
_ECD_COMMENT_SIZE = 7
# These last two indices are not part of the structure as defined in the
# spec, but they are used internally by this module as a convenience
_ECD_COMMENT = 8
_ECD_LOCATION = 9

# The "central directory" structure, magic number, size, and indices
# of entries in the structure (section V.F in the format document)
structCentralDir = "<4s4B4HL2L5H2L"
stringCentralDir = "PK\001\002"
sizeCentralDir = struct.calcsize(structCentralDir)

# indexes of entries in the central directory structure
_CD_SIGNATURE = 0
_CD_CREATE_VERSION = 1
_CD_CREATE_SYSTEM = 2
_CD_EXTRACT_VERSION = 3
_CD_EXTRACT_SYSTEM = 4
_CD_FLAG_BITS = 5
_CD_COMPRESS_TYPE = 6
_CD_TIME = 7
_CD_DATE = 8
_CD_CRC = 9
_CD_COMPRESSED_SIZE = 10
_CD_UNCOMPRESSED_SIZE = 11
_CD_FILENAME_LENGTH = 12
_CD_EXTRA_FIELD_LENGTH = 13
_CD_COMMENT_LENGTH = 14
_CD_DISK_NUMBER_START = 15
_CD_INTERNAL_FILE_ATTRIBUTES = 16
_CD_EXTERNAL_FILE_ATTRIBUTES = 17
_CD_LOCAL_HEADER_OFFSET = 18

# The "local file header" structure, magic number, size, and indices
# (section V.A in the format document)
structFileHeader = "<4s2B4HL2L2H"
stringFileHeader = "PK\003\004"
sizeFileHeader = struct.calcsize(structFileHeader)

_FH_SIGNATURE = 0
_FH_EXTRACT_VERSION = 1
_FH_EXTRACT_SYSTEM = 2
_FH_GENERAL_PURPOSE_FLAG_BITS = 3
_FH_COMPRESSION_METHOD = 4
_FH_LAST_MOD_TIME = 5
_FH_LAST_MOD_DATE = 6
_FH_CRC = 7
_FH_COMPRESSED_SIZE = 8
_FH_UNCOMPRESSED_SIZE = 9
_FH_FILENAME_LENGTH = 10
_FH_EXTRA_FIELD_LENGTH = 11

# The "Zip64 end of central directory locator" structure, magic number, and size
structEndArchive64Locator = "<4sLQL"
stringEndArchive64Locator = "PK\x06\x07"
sizeEndCentDir64Locator = struct.calcsize(structEndArchive64Locator)

# The "Zip64 end of central directory" record, magic number, size, and indices
# (section V.G in the format document)
structEndArchive64 = "<4sQ2H2L4Q"
stringEndArchive64 = "PK\x06\x06"
sizeEndCentDir64 = struct.calcsize(structEndArchive64)

_CD64_SIGNATURE = 0
_CD64_DIRECTORY_RECSIZE = 1
_CD64_CREATE_VERSION = 2
_CD64_EXTRACT_VERSION = 3
_CD64_DISK_NUMBER = 4
_CD64_DISK_NUMBER_START = 5
_CD64_NUMBER_ENTRIES_THIS_DISK = 6
_CD64_NUMBER_ENTRIES_TOTAL = 7
_CD64_DIRECTORY_SIZE = 8
_CD64_OFFSET_START_CENTDIR = 9

def is_zipfile(filename):
    """Quickly see if file is a ZIP file by checking the magic number."""
    try:
        fpin = open(filename, "rb")
        endrec = _EndRecData(fpin)
        fpin.close()
        if endrec:
            return True                 # file has correct magic number
    except IOError:
        pass
    return False

def _EndRecData64(fpin, offset, endrec):
    """
    Read the ZIP64 end-of-archive records and use that to update endrec
    """
    fpin.seek(offset - sizeEndCentDir64Locator, 2)
    data = fpin.read(sizeEndCentDir64Locator)
    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)
    if sig != stringEndArchive64Locator:
        return endrec

    if diskno != 0 or disks != 1:
        raise BadZipfile("zipfiles that span multiple disks are not supported")

    # Assume no 'zip64 extensible data'
    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)
    data = fpin.read(sizeEndCentDir64)
    sig, sz, create_version, read_version, disk_num, disk_dir, \
            dircount, dircount2, dirsize, diroffset = \
            struct.unpack(structEndArchive64, data)
    if sig != stringEndArchive64:
        return endrec

    # Update the original endrec using data from the ZIP64 record
    endrec[_ECD_SIGNATURE] = sig
    endrec[_ECD_DISK_NUMBER] = disk_num
    endrec[_ECD_DISK_START] = disk_dir
    endrec[_ECD_ENTRIES_THIS_DISK] = dircount
    endrec[_ECD_ENTRIES_TOTAL] = dircount2
    endrec[_ECD_SIZE] = dirsize
    endrec[_ECD_OFFSET] = diroffset
    return endrec


def _EndRecData(fpin):
    """Return data from the "End of Central Directory" record, or None.

    The data is a list of the nine items in the ZIP "End of central dir"
    record followed by a tenth item, the file seek offset of this record."""

    # Determine file size
    fpin.seek(0, 2)
    filesize = fpin.tell()

    # Check to see if this is ZIP file with no archive comment (the
    # "end of central directory" structure should be the last item in the
    # file if this is the case).
    try:
        fpin.seek(-sizeEndCentDir, 2)
    except IOError:
        return None
    data = fpin.read()
    if data[0:4] == stringEndArchive and data[-2:] == "\000\000":
        # the signature is correct and there's no comment, unpack structure
        endrec = struct.unpack(structEndArchive, data)
        endrec=list(endrec)

        # Append a blank comment and record start offset
        endrec.append("")
        endrec.append(filesize - sizeEndCentDir)

        # Try to read the "Zip64 end of central directory" structure
        return _EndRecData64(fpin, -sizeEndCentDir, endrec)

    # Either this is not a ZIP file, or it is a ZIP file with an archive
    # comment.  Search the end of the file for the "end of central directory"
    # record signature. The comment is the last item in the ZIP file and may be
    # up to 64K long.  It is assumed that the "end of central directory" magic
    # number does not appear in the comment.
    maxCommentStart = max(filesize - (1 << 16) - sizeEndCentDir, 0)
    fpin.seek(maxCommentStart, 0)
    data = fpin.read()
    start = data.rfind(stringEndArchive)
    if start >= 0:
        # found the magic number; attempt to unpack and interpret
        recData = data[start:start+sizeEndCentDir]
        endrec = list(struct.unpack(structEndArchive, recData))
        comment = data[start+sizeEndCentDir:]
        # check that comment length is correct
        if endrec[_ECD_COMMENT_SIZE] == len(comment):
            # Append the archive comment and start offset
            endrec.append(comment)
            endrec.append(maxCommentStart + start)

            # Try to read the "Zip64 end of central directory" structure
            return _EndRecData64(fpin, maxCommentStart + start - filesize,
                                 endrec)

    # Unable to find a valid end of central directory structure
    return


class ZipInfo (object):
    """Class with attributes describing each file in the ZIP archive."""

    __slots__ = (
            'orig_filename',
            'filename',
            'date_time',
            'compress_type',
            'comment',
            'extra',
            'create_system',
            'create_version',
            'extract_version',
            'reserved',
            'flag_bits',
            'volume',
            'internal_attr',
            'external_attr',
            'header_offset',
            'CRC',
            'compress_size',
            'file_size',
            '_raw_time',
        )

    def __init__(self, filename="NoName", date_time=(1980,1,1,0,0,0)):
        self.orig_filename = filename   # Original file name in archive

        # Terminate the file name at the first null byte.  Null bytes in file
        # names are used as tricks by viruses in archives.
        null_byte = filename.find(chr(0))
        if null_byte >= 0:
            filename = filename[0:null_byte]
        # This is used to ensure paths in generated ZIP files always use
        # forward slashes as the directory separator, as required by the
        # ZIP format specification.
        if os.sep != "/" and os.sep in filename:
            filename = filename.replace(os.sep, "/")

        self.filename = filename        # Normalized file name
        self.date_time = date_time      # year, month, day, hour, min, sec
        # Standard values:
        self.compress_type = ZIP_STORED # Type of compression for the file
        self.comment = ""               # Comment for each file
        self.extra = ""                 # ZIP extra data
        if sys.platform == 'win32':
            self.create_system = 0          # System which created ZIP archive
        else:
            # Assume everything else is unix-y
            self.create_system = 3          # System which created ZIP archive
        self.create_version = 20        # Version which created ZIP archive
        self.extract_version = 20       # Version needed to extract archive
        self.reserved = 0               # Must be zero
        self.flag_bits = 0              # ZIP flag bits
        self.volume = 0                 # Volume number of file header
        self.internal_attr = 0          # Internal attributes
        self.external_attr = 0          # External file attributes
        # Other attributes are set by class ZipFile:
        # header_offset         Byte offset to the file header
        # CRC                   CRC-32 of the uncompressed file
        # compress_size         Size of the compressed file
        # file_size             Size of the uncompressed file

    def FileHeader(self):
        """Return the per-file header as a string."""
        dt = self.date_time
        dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]
        dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)
        if self.flag_bits & 0x08:
            # Set these to zero because we write them after the file data
            CRC = compress_size = file_size = 0
        else:
            CRC = self.CRC
            compress_size = self.compress_size
            file_size = self.file_size

        extra = self.extra

        if file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT:
            # File is larger than what fits into a 4 byte integer,
            # fall back to the ZIP64 extension
            fmt = '<HHQQ'
            extra = extra + struct.pack(fmt,
                    1, struct.calcsize(fmt)-4, file_size, compress_size)
            file_size = 0xffffffff
            compress_size = 0xffffffff
            self.extract_version = max(45, self.extract_version)
            self.create_version = max(45, self.extract_version)

        filename, flag_bits = self._encodeFilenameFlags()
        header = struct.pack(structFileHeader, stringFileHeader,
                 self.extract_version, self.reserved, flag_bits,
                 self.compress_type, dostime, dosdate, CRC,
                 compress_size, file_size,
                 len(filename), len(extra))
        return header + filename + extra

    def _encodeFilenameFlags(self):
        if isinstance(self.filename, unicode):
            try:
                return self.filename.encode('ascii'), self.flag_bits
            except UnicodeEncodeError:
                return self.filename.encode('utf-8'), self.flag_bits | 0x800
        else:
            return self.filename, self.flag_bits

    def _decodeFilename(self):
        if self.flag_bits & 0x800:
            return self.filename.decode('utf-8')
        else:
            return self.filename

    def _decodeExtra(self):
        # Try to decode the extra field.
        extra = self.extra
        unpack = struct.unpack
        while extra:
            tp, ln = unpack('<HH', extra[:4])
            if tp == 1:
                if ln >= 24:
                    counts = unpack('<QQQ', extra[4:28])
                elif ln == 16:
                    counts = unpack('<QQ', extra[4:20])
                elif ln == 8:
                    counts = unpack('<Q', extra[4:12])
                elif ln == 0:
                    counts = ()
                else:
                    raise RuntimeError, "Corrupt extra field %s"%(ln,)

                idx = 0

                # ZIP64 extension (large files and/or large archives)
                if self.file_size in (0xffffffffffffffffL, 0xffffffffL):
                    self.file_size = counts[idx]
                    idx += 1

                if self.compress_size == 0xFFFFFFFFL:
                    self.compress_size = counts[idx]
                    idx += 1

                if self.header_offset == 0xffffffffL:
                    old = self.header_offset
                    self.header_offset = counts[idx]
                    idx+=1

            extra = extra[ln+4:]


class _ZipDecrypter:
    """Class to handle decryption of files stored within a ZIP archive.

    ZIP supports a password-based form of encryption. Even though known
    plaintext attacks have been found against it, it is still useful
    to be able to get data out of such a file.

    Usage:
        zd = _ZipDecrypter(mypwd)
        plain_char = zd(cypher_char)
        plain_text = map(zd, cypher_text)
    """

    def _GenerateCRCTable():
        """Generate a CRC-32 table.

        ZIP encryption uses the CRC32 one-byte primitive for scrambling some
        internal keys. We noticed that a direct implementation is faster than
        relying on binascii.crc32().
        """
        poly = 0xedb88320
        table = [0] * 256
        for i in range(256):
            crc = i
            for j in range(8):
                if crc & 1:
                    crc = ((crc >> 1) & 0x7FFFFFFF) ^ poly
                else:
                    crc = ((crc >> 1) & 0x7FFFFFFF)
            table[i] = crc
        return table
    crctable = _GenerateCRCTable()

    def _crc32(self, ch, crc):
        """Compute the CRC32 primitive on one byte."""
        return ((crc >> 8) & 0xffffff) ^ self.crctable[(crc ^ ord(ch)) & 0xff]

    def __init__(self, pwd):
        self.key0 = 305419896
        self.key1 = 591751049
        self.key2 = 878082192
        for p in pwd:
            self._UpdateKeys(p)

    def _UpdateKeys(self, c):
        self.key0 = self._crc32(c, self.key0)
        self.key1 = (self.key1 + (self.key0 & 255)) & 4294967295
        self.key1 = (self.key1 * 134775813 + 1) & 4294967295
        self.key2 = self._crc32(chr((self.key1 >> 24) & 255), self.key2)

    def __call__(self, c):
        """Decrypt a single character."""
        c = ord(c)
        k = self.key2 | 2
        c = c ^ (((k * (k^1)) >> 8) & 255)
        c = chr(c)
        self._UpdateKeys(c)
        return c

class ZipExtFile:
    """File-like object for reading an archive member.
       Is returned by ZipFile.open().
    """

    def __init__(self, fileobj, zipinfo, decrypt=None):
        self.fileobj = fileobj
        self.decrypter = decrypt
        self.bytes_read = 0L
        self.rawbuffer = ''
        self.readbuffer = ''
        self.linebuffer = ''
        self.eof = False
        self.univ_newlines = False
        self.nlSeps = ("\n", )
        self.lastdiscard = ''

        self.compress_type = zipinfo.compress_type
        self.compress_size = zipinfo.compress_size

        self.closed  = False
        self.mode    = "r"
        self.name = zipinfo.filename

        # read from compressed files in 64k blocks
        self.compreadsize = 64*1024
        if self.compress_type == ZIP_DEFLATED:
            self.dc = zlib.decompressobj(-15)

    def set_univ_newlines(self, univ_newlines):
        self.univ_newlines = univ_newlines

        # pick line separator char(s) based on universal newlines flag
        self.nlSeps = ("\n", )
        if self.univ_newlines:
            self.nlSeps = ("\r\n", "\r", "\n")

    def __iter__(self):
        return self

    def next(self):
        nextline = self.readline()
        if not nextline:
            raise StopIteration()

        return nextline

    def close(self):
        self.closed = True

    def _checkfornewline(self):
        nl, nllen = -1, -1
        if self.linebuffer:
            # ugly check for cases where half of an \r\n pair was
            # read on the last pass, and the \r was discarded.  In this
            # case we just throw away the \n at the start of the buffer.
            if (self.lastdiscard, self.linebuffer[0]) == ('\r','\n'):
                self.linebuffer = self.linebuffer[1:]

            for sep in self.nlSeps:
                nl = self.linebuffer.find(sep)
                if nl >= 0:
                    nllen = len(sep)
                    return nl, nllen

        return nl, nllen

    def readline(self, size = -1):
        """Read a line with approx. size. If size is negative,
           read a whole line.
        """
        if size < 0:
            size = sys.maxint
        elif size == 0:
            return ''

        # check for a newline already in buffer
        nl, nllen = self._checkfornewline()

        if nl >= 0:
            # the next line was already in the buffer
            nl = min(nl, size)
        else:
            # no line break in buffer - try to read more
            size -= len(self.linebuffer)
            while nl < 0 and size > 0:
                buf = self.read(min(size, 100))
                if not buf:
                    break
                self.linebuffer += buf
                size -= len(buf)

                # check for a newline in buffer
                nl, nllen = self._checkfornewline()

            # we either ran out of bytes in the file, or
            # met the specified size limit without finding a newline,
            # so return current buffer
            if nl < 0:
                s = self.linebuffer
                self.linebuffer = ''
                return s

        buf = self.linebuffer[:nl]
        self.lastdiscard = self.linebuffer[nl:nl + nllen]
        self.linebuffer = self.linebuffer[nl + nllen:]

        # line is always returned with \n as newline char (except possibly
        # for a final incomplete line in the file, which is handled above).
        return buf + "\n"

    def readlines(self, sizehint = -1):
        """Return a list with all (following) lines. The sizehint parameter
        is ignored in this implementation.
        """
        result = []
        while True:
            line = self.readline()
            if not line: break
            result.append(line)
        return result

    def read(self, size = None):
        # act like file() obj and return empty string if size is 0
        if size == 0:
            return ''

        # determine read size
        bytesToRead = self.compress_size - self.bytes_read

        # adjust read size for encrypted files since the first 12 bytes
        # are for the encryption/password information
        if self.decrypter is not None:
            bytesToRead -= 12

        if size is not None and size >= 0:
            if self.compress_type == ZIP_STORED:
                lr = len(self.readbuffer)
                bytesToRead = min(bytesToRead, size - lr)
            elif self.compress_type == ZIP_DEFLATED:
                if len(self.readbuffer) > size:
                    # the user has requested fewer bytes than we've already
                    # pulled through the decompressor; don't read any more
                    bytesToRead = 0
                else:
                    # user will use up the buffer, so read some more
                    lr = len(self.rawbuffer)
                    bytesToRead = min(bytesToRead, self.compreadsize - lr)

        # avoid reading past end of file contents
        if bytesToRead + self.bytes_read > self.compress_size:
            bytesToRead = self.compress_size - self.bytes_read

        # try to read from file (if necessary)
        if bytesToRead > 0:
            bytes = self.fileobj.read(bytesToRead)
            self.bytes_read += len(bytes)
            self.rawbuffer += bytes

            # handle contents of raw buffer
            if self.rawbuffer:
                newdata = self.rawbuffer
                self.rawbuffer = ''

                # decrypt new data if we were given an object to handle that
                if newdata and self.decrypter is not None:
                    newdata = ''.join(map(self.decrypter, newdata))

                # decompress newly read data if necessary
                if newdata and self.compress_type == ZIP_DEFLATED:
                    newdata = self.dc.decompress(newdata)
                    self.rawbuffer = self.dc.unconsumed_tail
                    if self.eof and len(self.rawbuffer) == 0:
                        # we're out of raw bytes (both from the file and
                        # the local buffer); flush just to make sure the
                        # decompressor is done
                        newdata += self.dc.flush()
                        # prevent decompressor from being used again
                        self.dc = None

                self.readbuffer += newdata


        # return what the user asked for
        if size is None or len(self.readbuffer) <= size:
            bytes = self.readbuffer
            self.readbuffer = ''
        else:
            bytes = self.readbuffer[:size]
            self.readbuffer = self.readbuffer[size:]

        return bytes


class ZipFile:
    """ Class with methods to open, read, write, close, list zip files.

    z = ZipFile(file, mode="r", compression=ZIP_STORED, allowZip64=False)

    file: Either the path to the file, or a file-like object.
          If it is a path, the file will be opened and closed by ZipFile.
    mode: The mode can be either read "r", write "w" or append "a".
    compression: ZIP_STORED (no compression) or ZIP_DEFLATED (requires zlib).
    allowZip64: if True ZipFile will create files with ZIP64 extensions when
                needed, otherwise it will raise an exception when this would
                be necessary.

    """

    fp = None                   # Set here since __del__ checks it

    def __init__(self, file, mode="r", compression=ZIP_STORED, allowZip64=False):
        """Open the ZIP file with mode read "r", write "w" or append "a"."""
        if mode not in ("r", "w", "a"):
            raise RuntimeError('ZipFile() requires mode "r", "w", or "a"')

        if compression == ZIP_STORED:
            pass
        elif compression == ZIP_DEFLATED:
            if not zlib:
                raise RuntimeError,\
                      "Compression requires the (missing) zlib module"
        else:
            raise RuntimeError, "That compression method is not supported"

        self._allowZip64 = allowZip64
        self._didModify = False
        self.debug = 0  # Level of printing: 0 through 3
        self.NameToInfo = {}    # Find file info given name
        self.filelist = []      # List of ZipInfo instances for archive
        self.compression = compression  # Method of compression
        self.mode = key = mode.replace('b', '')[0]
        self.pwd = None
        self.comment = ''

        # Check if we were passed a file-like object
        if isinstance(file, basestring):
            self._filePassed = 0
            self.filename = file
            modeDict = {'r' : 'rb', 'w': 'wb', 'a' : 'r+b'}
            try:
                self.fp = open(file, modeDict[mode])
            except IOError:
                if mode == 'a':
                    mode = key = 'w'
                    self.fp = open(file, modeDict[mode])
                else:
                    raise
        else:
            self._filePassed = 1
            self.fp = file
            self.filename = getattr(file, 'name', None)

        if key == 'r':
            self._GetContents()
        elif key == 'w':
            pass
        elif key == 'a':
            try:                        # See if file is a zip file
                self._RealGetContents()
                # seek to start of directory and overwrite
                self.fp.seek(self.start_dir, 0)
            except BadZipfile:          # file is not a zip file, just append
                self.fp.seek(0, 2)
        else:
            if not self._filePassed:
                self.fp.close()
                self.fp = None
            raise RuntimeError, 'Mode must be "r", "w" or "a"'

    def _GetContents(self):
        """Read the directory, making sure we close the file if the format
        is bad."""
        try:
            self._RealGetContents()
        except BadZipfile:
            if not self._filePassed:
                self.fp.close()
                self.fp = None
            raise

    def _RealGetContents(self):
        """Read in the table of contents for the ZIP file."""
        fp = self.fp
        endrec = _EndRecData(fp)
        if not endrec:
            raise BadZipfile, "File is not a zip file"
        if self.debug > 1:
            print endrec
        size_cd = endrec[_ECD_SIZE]             # bytes in central directory
        offset_cd = endrec[_ECD_OFFSET]         # offset of central directory
        self.comment = endrec[_ECD_COMMENT]     # archive comment

        # "concat" is zero, unless zip was concatenated to another file
        concat = endrec[_ECD_LOCATION] - size_cd - offset_cd
        if endrec[_ECD_SIGNATURE] == stringEndArchive64:
            # If Zip64 extension structures are present, account for them
            concat -= (sizeEndCentDir64 + sizeEndCentDir64Locator)

        if self.debug > 2:
            inferred = concat + offset_cd
            print "given, inferred, offset", offset_cd, inferred, concat
        # self.start_dir:  Position of start of central directory
        self.start_dir = offset_cd + concat
        fp.seek(self.start_dir, 0)
        data = fp.read(size_cd)
        fp = cStringIO.StringIO(data)
        total = 0
        while total < size_cd:
            centdir = fp.read(sizeCentralDir)
            if centdir[0:4] != stringCentralDir:
                raise BadZipfile, "Bad magic number for central directory"
            centdir = struct.unpack(structCentralDir, centdir)
            if self.debug > 2:
                print centdir
            filename = fp.read(centdir[_CD_FILENAME_LENGTH])
            # Create ZipInfo instance to store file information
            x = ZipInfo(filename)
            x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH])
            x.comment = fp.read(centdir[_CD_COMMENT_LENGTH])
            x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET]
            (x.create_version, x.create_system, x.extract_version, x.reserved,
                x.flag_bits, x.compress_type, t, d,
                x.CRC, x.compress_size, x.file_size) = centdir[1:12]
            x.volume, x.internal_attr, x.external_attr = centdir[15:18]
            # Convert date/time code to (year, month, day, hour, min, sec)
            x._raw_time = t
            x.date_time = ( (d>>9)+1980, (d>>5)&0xF, d&0x1F,
                                     t>>11, (t>>5)&0x3F, (t&0x1F) * 2 )

            x._decodeExtra()
            x.header_offset = x.header_offset + concat
            x.filename = x._decodeFilename()
            self.filelist.append(x)
            self.NameToInfo[x.filename] = x

            # update total bytes read from central directory
            total = (total + sizeCentralDir + centdir[_CD_FILENAME_LENGTH]
                     + centdir[_CD_EXTRA_FIELD_LENGTH]
                     + centdir[_CD_COMMENT_LENGTH])

            if self.debug > 2:
                print "total", total


    def namelist(self):
        """Return a list of file names in the archive."""
        l = []
        for data in self.filelist:
            l.append(data.filename)
        return l

    def infolist(self):
        """Return a list of class ZipInfo instances for files in the
        archive."""
        return self.filelist

    def printdir(self):
        """Print a table of contents for the zip file."""
        print "%-46s %19s %12s" % ("File Name", "Modified    ", "Size")
        for zinfo in self.filelist:
            date = "%d-%02d-%02d %02d:%02d:%02d" % zinfo.date_time[:6]
            print "%-46s %s %12d" % (zinfo.filename, date, zinfo.file_size)

    def testzip(self):
        """Read all the files and check the CRC."""
        chunk_size = 2 ** 20
        for zinfo in self.filelist:
            try:
                # Read by chunks, to avoid an OverflowError or a
                # MemoryError with very large embedded files.
                f = self.open(zinfo.filename, "r")
                while f.read(chunk_size):     # Check CRC-32
                    pass
            except BadZipfile:
                return zinfo.filename

    def getinfo(self, name):
        """Return the instance of ZipInfo given 'name'."""
        info = self.NameToInfo.get(name)
        if info is None:
            raise KeyError(
                'There is no item named %r in the archive' % name)

        return info

    def setpassword(self, pwd):
        """Set default password for encrypted files."""
        self.pwd = pwd

    def read(self, name, pwd=None):
        """Return file bytes (as a string) for name."""
        return self.open(name, "r", pwd).read()

    def open(self, name, mode="r", pwd=None):
        """Return file-like object for 'name'."""
        if mode not in ("r", "U", "rU"):
            raise RuntimeError, 'open() requires mode "r", "U", or "rU"'
        if not self.fp:
            raise RuntimeError, \
                  "Attempt to read ZIP archive that was already closed"

        # Only open a new file for instances where we were not
        # given a file object in the constructor
        if self._filePassed:
            zef_file = self.fp
        else:
            zef_file = open(self.filename, 'rb')

        # Make sure we have an info object
        if isinstance(name, ZipInfo):
            # 'name' is already an info object
            zinfo = name
        else:
            # Get info object for name
            zinfo = self.getinfo(name)

        zef_file.seek(zinfo.header_offset, 0)

        # Skip the file header:
        fheader = zef_file.read(sizeFileHeader)
        if fheader[0:4] != stringFileHeader:
            raise BadZipfile, "Bad magic number for file header"

        fheader = struct.unpack(structFileHeader, fheader)
        fname = zef_file.read(fheader[_FH_FILENAME_LENGTH])
        if fheader[_FH_EXTRA_FIELD_LENGTH]:
            zef_file.read(fheader[_FH_EXTRA_FIELD_LENGTH])

        if fname != zinfo.orig_filename:
            raise BadZipfile, \
                      'File name in directory "%s" and header "%s" differ.' % (
                          zinfo.orig_filename, fname)

        # check for encrypted flag & handle password
        is_encrypted = zinfo.flag_bits & 0x1
        zd = None
        if is_encrypted:
            if not pwd:
                pwd = self.pwd
            if not pwd:
                raise RuntimeError, "File %s is encrypted, " \
                      "password required for extraction" % name

            zd = _ZipDecrypter(pwd)
            # The first 12 bytes in the cypher stream is an encryption header
            #  used to strengthen the algorithm. The first 11 bytes are
            #  completely random, while the 12th contains the MSB of the CRC,
            #  or the MSB of the file time depending on the header type
            #  and is used to check the correctness of the password.
            bytes = zef_file.read(12)
            h = map(zd, bytes[0:12])
            if zinfo.flag_bits & 0x8:
                # compare against the file type from extended local headers
                check_byte = (zinfo._raw_time >> 8) & 0xff
            else:
                # compare against the CRC otherwise
                check_byte = (zinfo.CRC >> 24) & 0xff
            if ord(h[11]) != check_byte:
                raise RuntimeError("Bad password for file", name)

        # build and return a ZipExtFile
        if zd is None:
            zef = ZipExtFile(zef_file, zinfo)
        else:
            zef = ZipExtFile(zef_file, zinfo, zd)

        # set universal newlines on ZipExtFile if necessary
        if "U" in mode:
            zef.set_univ_newlines(True)
        return zef

    def extract(self, member, path=None, pwd=None):
        """Extract a member from the archive to the current working directory,
           using its full name. Its file information is extracted as accurately
           as possible. `member' may be a filename or a ZipInfo object. You can
           specify a different directory using `path'.
        """
        if not isinstance(member, ZipInfo):
            member = self.getinfo(member)

        if path is None:
            path = os.getcwd()

        return self._extract_member(member, path, pwd)

    def extractall(self, path=None, members=None, pwd=None):
        """Extract all members from the archive to the current working
           directory. `path' specifies a different directory to extract to.
           `members' is optional and must be a subset of the list returned
           by namelist().
        """
        if members is None:
            members = self.namelist()

        for zipinfo in members:
            self.extract(zipinfo, path, pwd)

    def _extract_member(self, member, targetpath, pwd):
        """Extract the ZipInfo object 'member' to a physical
           file on the path targetpath.
        """
        # build the destination pathname, replacing
        # forward slashes to platform specific separators.
        # Strip trailing path separator, unless it represents the root.
        if (targetpath[-1:] in (os.path.sep, os.path.altsep)
            and len(os.path.splitdrive(targetpath)[1]) > 1):
            targetpath = targetpath[:-1]

        # don't include leading "/" from file name if present
        if member.filename[0] == '/':
            targetpath = os.path.join(targetpath, member.filename[1:])
        else:
            targetpath = os.path.join(targetpath, member.filename)

        targetpath = os.path.normpath(targetpath)

        # Create all upper directories if necessary.
        upperdirs = os.path.dirname(targetpath)
        if upperdirs and not os.path.exists(upperdirs):
            os.makedirs(upperdirs)

        if member.filename[-1] == '/':
            if not os.path.isdir(targetpath):
                os.mkdir(targetpath)
            return targetpath

        source = self.open(member, pwd=pwd)
        target = file(targetpath, "wb")
        shutil.copyfileobj(source, target)
        source.close()
        target.close()

        return targetpath

    def _writecheck(self, zinfo):
        """Check for errors before writing a file to the archive."""
        if zinfo.filename in self.NameToInfo:
            if self.debug:      # Warning for duplicate names
                print "Duplicate name:", zinfo.filename
        if self.mode not in ("w", "a"):
            raise RuntimeError, 'write() requires mode "w" or "a"'
        if not self.fp:
            raise RuntimeError, \
                  "Attempt to write ZIP archive that was already closed"
        if zinfo.compress_type == ZIP_DEFLATED and not zlib:
            raise RuntimeError, \
                  "Compression requires the (missing) zlib module"
        if zinfo.compress_type not in (ZIP_STORED, ZIP_DEFLATED):
            raise RuntimeError, \
                  "That compression method is not supported"
        if zinfo.file_size > ZIP64_LIMIT:
            if not self._allowZip64:
                raise LargeZipFile("Filesize would require ZIP64 extensions")
        if zinfo.header_offset > ZIP64_LIMIT:
            if not self._allowZip64:
                raise LargeZipFile("Zipfile size would require ZIP64 extensions")

    def write(self, filename, arcname=None, compress_type=None):
        """Put the bytes from filename into the archive under the name
        arcname."""
        if not self.fp:
            raise RuntimeError(
                  "Attempt to write to ZIP archive that was already closed")

        st = os.stat(filename)
        isdir = stat.S_ISDIR(st.st_mode)
        mtime = time.localtime(st.st_mtime)
        date_time = mtime[0:6]
        # Create ZipInfo instance to store file information
        if arcname is None:
            arcname = filename
        arcname = os.path.normpath(os.path.splitdrive(arcname)[1])
        while arcname[0] in (os.sep, os.altsep):
            arcname = arcname[1:]
        if isdir:
            arcname += '/'
        zinfo = ZipInfo(arcname, date_time)
        zinfo.external_attr = (st[0] & 0xFFFF) << 16L      # Unix attributes
        if compress_type is None:
            zinfo.compress_type = self.compression
        else:
            zinfo.compress_type = compress_type

        zinfo.file_size = st.st_size
        zinfo.flag_bits = 0x00
        zinfo.header_offset = self.fp.tell()    # Start of header bytes

        self._writecheck(zinfo)
        self._didModify = True

        if isdir:
            zinfo.file_size = 0
            zinfo.compress_size = 0
            zinfo.CRC = 0
            self.filelist.append(zinfo)
            self.NameToInfo[zinfo.filename] = zinfo
            self.fp.write(zinfo.FileHeader())
            return

        fp = open(filename, "rb")
        # Must overwrite CRC and sizes with correct data later
        zinfo.CRC = CRC = 0
        zinfo.compress_size = compress_size = 0
        zinfo.file_size = file_size = 0
        self.fp.write(zinfo.FileHeader())
        if zinfo.compress_type == ZIP_DEFLATED:
            cmpr = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,
                 zlib.DEFLATED, -15)
        else:
            cmpr = None
        while 1:
            buf = fp.read(1024 * 8)
            if not buf:
                break
            file_size = file_size + len(buf)
            CRC = crc32(buf, CRC) & 0xffffffff
            if cmpr:
                buf = cmpr.compress(buf)
                compress_size = compress_size + len(buf)
            self.fp.write(buf)
        fp.close()
        if cmpr:
            buf = cmpr.flush()
            compress_size = compress_size + len(buf)
            self.fp.write(buf)
            zinfo.compress_size = compress_size
        else:
            zinfo.compress_size = file_size
        zinfo.CRC = CRC
        zinfo.file_size = file_size
        # Seek backwards and write CRC and file sizes
        position = self.fp.tell()       # Preserve current position in file
        self.fp.seek(zinfo.header_offset + 14, 0)
        self.fp.write(struct.pack("<LLL", zinfo.CRC, zinfo.compress_size,
              zinfo.file_size))
        self.fp.seek(position, 0)
        self.filelist.append(zinfo)
        self.NameToInfo[zinfo.filename] = zinfo

    def writestr(self, zinfo_or_arcname, bytes):
        """Write a file into the archive.  The contents is the string
        'bytes'.  'zinfo_or_arcname' is either a ZipInfo instance or
        the name of the file in the archive."""
        if not isinstance(zinfo_or_arcname, ZipInfo):
            zinfo = ZipInfo(filename=zinfo_or_arcname,
                            date_time=time.localtime(time.time())[:6])
            zinfo.compress_type = self.compression
            zinfo.external_attr = 0600 << 16
        else:
            zinfo = zinfo_or_arcname

        if not self.fp:
            raise RuntimeError(
                  "Attempt to write to ZIP archive that was already closed")

        zinfo.file_size = len(bytes)            # Uncompressed size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self._writecheck(zinfo)
        self._didModify = True
        zinfo.CRC = crc32(bytes) & 0xffffffff       # CRC-32 checksum
        if zinfo.compress_type == ZIP_DEFLATED:
            co = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,
                 zlib.DEFLATED, -15)
            bytes = co.compress(bytes) + co.flush()
            zinfo.compress_size = len(bytes)    # Compressed size
        else:
            zinfo.compress_size = zinfo.file_size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self.fp.write(zinfo.FileHeader())
        self.fp.write(bytes)
        self.fp.flush()
        if zinfo.flag_bits & 0x08:
            # Write CRC and file sizes after the file data
            self.fp.write(struct.pack("<LLL", zinfo.CRC, zinfo.compress_size,
                  zinfo.file_size))
        self.filelist.append(zinfo)
        self.NameToInfo[zinfo.filename] = zinfo

    def __del__(self):
        """Call the "close()" method in case the user forgot."""
        self.close()

    def close(self):
        """Close the file, and for mode "w" and "a" write the ending
        records."""
        if self.fp is None:
            return

        if self.mode in ("w", "a") and self._didModify: # write ending records
            count = 0
            pos1 = self.fp.tell()
            for zinfo in self.filelist:         # write central directory
                count = count + 1
                dt = zinfo.date_time
                dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]
                dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)
                extra = []
                if zinfo.file_size > ZIP64_LIMIT \
                        or zinfo.compress_size > ZIP64_LIMIT:
                    extra.append(zinfo.file_size)
                    extra.append(zinfo.compress_size)
                    file_size = 0xffffffff
                    compress_size = 0xffffffff
                else:
                    file_size = zinfo.file_size
                    compress_size = zinfo.compress_size

                if zinfo.header_offset > ZIP64_LIMIT:
                    extra.append(zinfo.header_offset)
                    header_offset = 0xffffffffL
                else:
                    header_offset = zinfo.header_offset

                extra_data = zinfo.extra
                if extra:
                    # Append a ZIP64 field to the extra's
                    extra_data = struct.pack(
                            '<HH' + 'Q'*len(extra),
                            1, 8*len(extra), *extra) + extra_data

                    extract_version = max(45, zinfo.extract_version)
                    create_version = max(45, zinfo.create_version)
                else:
                    extract_version = zinfo.extract_version
                    create_version = zinfo.create_version

                try:
                    filename, flag_bits = zinfo._encodeFilenameFlags()
                    centdir = struct.pack(structCentralDir,
                     stringCentralDir, create_version,
                     zinfo.create_system, extract_version, zinfo.reserved,
                     flag_bits, zinfo.compress_type, dostime, dosdate,
                     zinfo.CRC, compress_size, file_size,
                     len(filename), len(extra_data), len(zinfo.comment),
                     0, zinfo.internal_attr, zinfo.external_attr,
                     header_offset)
                except DeprecationWarning:
                    print >>sys.stderr, (structCentralDir,
                     stringCentralDir, create_version,
                     zinfo.create_system, extract_version, zinfo.reserved,
                     zinfo.flag_bits, zinfo.compress_type, dostime, dosdate,
                     zinfo.CRC, compress_size, file_size,
                     len(zinfo.filename), len(extra_data), len(zinfo.comment),
                     0, zinfo.internal_attr, zinfo.external_attr,
                     header_offset)
                    raise
                self.fp.write(centdir)
                self.fp.write(filename)
                self.fp.write(extra_data)
                self.fp.write(zinfo.comment)

            pos2 = self.fp.tell()
            # Write end-of-zip-archive record
            centDirCount = count
            centDirSize = pos2 - pos1
            centDirOffset = pos1
            if (centDirCount >= ZIP_FILECOUNT_LIMIT or
                centDirOffset > ZIP64_LIMIT or
                centDirSize > ZIP64_LIMIT):
                # Need to write the ZIP64 end-of-archive records
                zip64endrec = struct.pack(
                        structEndArchive64, stringEndArchive64,
                        44, 45, 45, 0, 0, centDirCount, centDirCount,
                        centDirSize, centDirOffset)
                self.fp.write(zip64endrec)

                zip64locrec = struct.pack(
                        structEndArchive64Locator,
                        stringEndArchive64Locator, 0, pos2, 1)
                self.fp.write(zip64locrec)
                centDirCount = min(centDirCount, 0xFFFF)
                centDirSize = min(centDirSize, 0xFFFFFFFF)
                centDirOffset = min(centDirOffset, 0xFFFFFFFF)

            # check for valid comment length
            if len(self.comment) >= ZIP_MAX_COMMENT:
                if self.debug > 0:
                    msg = 'Archive comment is too long; truncating to %d bytes' \
                          % ZIP_MAX_COMMENT
                self.comment = self.comment[:ZIP_MAX_COMMENT]

            endrec = struct.pack(structEndArchive, stringEndArchive,
                                 0, 0, centDirCount, centDirCount,
                                 centDirSize, centDirOffset, len(self.comment))
            self.fp.write(endrec)
            self.fp.write(self.comment)
            self.fp.flush()

        if not self._filePassed:
            self.fp.close()
        self.fp = None


class PyZipFile(ZipFile):
    """Class to create ZIP archives with Python library files and packages."""

    def writepy(self, pathname, basename = ""):
        """Add all files from "pathname" to the ZIP archive.

        If pathname is a package directory, search the directory and
        all package subdirectories recursively for all *.py and enter
        the modules into the archive.  If pathname is a plain
        directory, listdir *.py and enter all modules.  Else, pathname
        must be a Python *.py file and the module will be put into the
        archive.  Added modules are always module.pyo or module.pyc.
        This method will compile the module.py into module.pyc if
        necessary.
        """
        dir, name = os.path.split(pathname)
        if os.path.isdir(pathname):
            initname = os.path.join(pathname, "__init__.py")
            if os.path.isfile(initname):
                # This is a package directory, add it
                if basename:
                    basename = "%s/%s" % (basename, name)
                else:
                    basename = name
                if self.debug:
                    print "Adding package in", pathname, "as", basename
                fname, arcname = self._get_codename(initname[0:-3], basename)
                if self.debug:
                    print "Adding", arcname
                self.write(fname, arcname)
                dirlist = os.listdir(pathname)
                dirlist.remove("__init__.py")
                # Add all *.py files and package subdirectories
                for filename in dirlist:
                    path = os.path.join(pathname, filename)
                    root, ext = os.path.splitext(filename)
                    if os.path.isdir(path):
                        if os.path.isfile(os.path.join(path, "__init__.py")):
                            # This is a package directory, add it
                            self.writepy(path, basename)  # Recursive call
                    elif ext == ".py":
                        fname, arcname = self._get_codename(path[0:-3],
                                         basename)
                        if self.debug:
                            print "Adding", arcname
                        self.write(fname, arcname)
            else:
                # This is NOT a package directory, add its files at top level
                if self.debug:
                    print "Adding files from directory", pathname
                for filename in os.listdir(pathname):
                    path = os.path.join(pathname, filename)
                    root, ext = os.path.splitext(filename)
                    if ext == ".py":
                        fname, arcname = self._get_codename(path[0:-3],
                                         basename)
                        if self.debug:
                            print "Adding", arcname
                        self.write(fname, arcname)
        else:
            if pathname[-3:] != ".py":
                raise RuntimeError, \
                      'Files added with writepy() must end with ".py"'
            fname, arcname = self._get_codename(pathname[0:-3], basename)
            if self.debug:
                print "Adding file", arcname
            self.write(fname, arcname)

    def _get_codename(self, pathname, basename):
        """Return (filename, archivename) for the path.

        Given a module name path, return the correct file path and
        archive name, compiling if necessary.  For example, given
        /python/lib/string, return (/python/lib/string.pyc, string).
        """
        file_py  = pathname + ".py"
        file_pyc = pathname + ".pyc"
        file_pyo = pathname + ".pyo"
        if os.path.isfile(file_pyo) and \
                            os.stat(file_pyo).st_mtime >= os.stat(file_py).st_mtime:
            fname = file_pyo    # Use .pyo file
        elif not os.path.isfile(file_pyc) or \
             os.stat(file_pyc).st_mtime < os.stat(file_py).st_mtime:
            import py_compile
            if self.debug:
                print "Compiling", file_py
            try:
                py_compile.compile(file_py, file_pyc, None, True)
            except py_compile.PyCompileError,err:
                print err.msg
            fname = file_pyc
        else:
            fname = file_pyc
        archivename = os.path.split(fname)[1]
        if basename:
            archivename = "%s/%s" % (basename, archivename)
        return (fname, archivename)


def main(args = None):
    import textwrap
    USAGE=textwrap.dedent("""\
        Usage:
            zipfile.py -l zipfile.zip        # Show listing of a zipfile
            zipfile.py -t zipfile.zip        # Test if a zipfile is valid
            zipfile.py -e zipfile.zip target # Extract zipfile into target dir
            zipfile.py -c zipfile.zip src ... # Create zipfile from sources
        """)
    if args is None:
        args = sys.argv[1:]

    if not args or args[0] not in ('-l', '-c', '-e', '-t'):
        print USAGE
        sys.exit(1)

    if args[0] == '-l':
        if len(args) != 2:
            print USAGE
            sys.exit(1)
        zf = ZipFile(args[1], 'r')
        zf.printdir()
        zf.close()

    elif args[0] == '-t':
        if len(args) != 2:
            print USAGE
            sys.exit(1)
        zf = ZipFile(args[1], 'r')
        zf.testzip()
        print "Done testing"

    elif args[0] == '-e':
        if len(args) != 3:
            print USAGE
            sys.exit(1)

        zf = ZipFile(args[1], 'r')
        out = args[2]
        for path in zf.namelist():
            if path.startswith('./'):
                tgt = os.path.join(out, path[2:])
            else:
                tgt = os.path.join(out, path)

            tgtdir = os.path.dirname(tgt)
            if not os.path.exists(tgtdir):
                os.makedirs(tgtdir)
            fp = open(tgt, 'wb')
            fp.write(zf.read(path))
            fp.close()
        zf.close()

    elif args[0] == '-c':
        if len(args) < 3:
            print USAGE
            sys.exit(1)

        def addToZip(zf, path, zippath):
            if os.path.isfile(path):
                zf.write(path, zippath, ZIP_DEFLATED)
            elif os.path.isdir(path):
                for nm in os.listdir(path):
                    addToZip(zf,
                            os.path.join(path, nm), os.path.join(zippath, nm))
            # else: ignore

        zf = ZipFile(args[1], 'w', allowZip64=True)
        for src in args[2:]:
            addToZip(zf, src, os.path.basename(src))

        zf.close()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = __tarfile_c
import operator
import tarfile

class TarFile(tarfile.TarFile):
    # Add extractall method to tarfile.TarFile (python 2.4*). Copied
    # from upstream v2.6.5
    def extractall(self, path=".", members=None):
        """Extract all members from the archive to the current working
           directory and set owner, modification time and permissions on
           directories afterwards. `path' specifies a different directory
           to extract to. `members' is optional and must be a subset of the
           list returned by getmembers().
        """
        directories = []

        if members is None:
            members = self

        for tarinfo in members:
            if tarinfo.isdir():
                # Extract directories with a safe mode.
                directories.append(tarinfo)
                tarinfo = copy.copy(tarinfo)
                tarinfo.mode = 0700
            self.extract(tarinfo, path)

        # Reverse sort directories.
        directories.sort(key=operator.attrgetter('name'))
        directories.reverse()

        # Set correct owner, mtime and filemode on directories.
        for tarinfo in directories:
            dirpath = os.path.join(path, tarinfo.name)
            try:
                self.chown(tarinfo, dirpath)
                self.utime(tarinfo, dirpath)
                self.chmod(tarinfo, dirpath)
            except ExtractError, e:
                if self.errorlevel > 1:
                    raise
                else:
                    self._dbg(1, "tarfile: %s" % e)

########NEW FILE########
__FILENAME__ = conv
import os

import bento.core.node

from bento.errors import \
        InvalidPackage
from bento.core import \
        PackageDescription
from bento.core.pkg_objects import \
        Executable
from bento.utils.utils import \
        is_string

_PKG_TO_DIST = {
        "ext_modules": lambda pkg: [v for v  in \
                                    pkg.extensions.values()],
        "platforms": lambda pkg: [v for v  in pkg.platforms],
        "packages": lambda pkg: [v for v  in pkg.packages],
        "py_modules": lambda pkg: [v for v  in pkg.py_modules],
}

_META_PKG_TO_DIST = {}
def _setup():
    for k in ["name", "url", "author", "author_email", "maintainer",
              "maintainer_email", "license", "download_url"]:
        def _f(attr):
            return lambda pkg: getattr(pkg, attr)
        _META_PKG_TO_DIST[k] = _f(k)
    _META_PKG_TO_DIST["long_description"] = lambda pkg: pkg.description
    _META_PKG_TO_DIST["description"] = lambda pkg: pkg.summary
    def _version_(pkg):
        if pkg.version is None:
            return "UNKNOWN"
        else:
            return pkg.version
    _META_PKG_TO_DIST["version"] = _version_
_setup()
_PKG_TO_DIST.update(_META_PKG_TO_DIST)

def pkg_to_distutils_meta(pkg):
    """Obtain meta data information from pkg into a dictionary which may be
    used directly as an argument for setup function in distutils."""
    d = {}
    for k, v in _META_PKG_TO_DIST.items():
        d[k] = v(pkg)
    return d

def pkg_to_distutils_meta_pkg_info(pkg):
    meta = pkg_to_distutils_meta(pkg)
    meta["summary"] = meta.pop("description")
    meta["description"] = meta.pop("long_description")

    return meta

def pkg_to_distutils(pkg):
    """Convert PackageDescription instance to a dict which may be used
    as argument to distutils/setuptools setup function."""
    d = {}

    for k, v in _PKG_TO_DIST.items():
        d[k] = v(pkg)

    return d

def validate_package(pkg_name, base_node):
    """Given a python package name, check whether it is indeed an existing
    package.

    Package is looked relatively to base_node."""
    # XXX: this function is wrong - use the code from setuptools
    pkg_dir = pkg_name.replace(".", os.path.sep)
    pkg_node = base_node.find_node(pkg_dir)
    if pkg_node is None:
        raise InvalidPackage("directory %s in %s does not exist" % (pkg_dir, base_node.abspath()))
    init = pkg_node.find_node('__init__.py')
    if init is None:
        raise InvalidPackage(
                "Missing __init__.py in package %s (in directory %s)"
                % (pkg_name, base_node.abspath()))
    return pkg_node

def find_package(pkg_name, base_node):
    """Given a python package name, find all its modules relatively to
    base_node."""
    pkg_node = validate_package(pkg_name, base_node)
    ret = []
    for f in pkg_node.listdir():
        if f.endswith(".py"):
            node = pkg_node.find_node(f)
            ret.append(node.path_from(base_node))
    return ret

def validate_packages(pkgs, top):
    ret_pkgs = []
    for pkg in pkgs:
        try:
            validate_package(pkg, top)
        except InvalidPackage:
            # FIXME: add the package as data here
            pass
        else:
            ret_pkgs.append(pkg)
    return ret_pkgs

def distutils_to_package_description(dist):
    root = bento.core.node.Node("", None)
    top = root.find_dir(os.getcwd())

    data = {}

    data['name'] = dist.get_name()
    data['version'] = dist.get_version()
    data['author'] = dist.get_author()
    data['author_email'] = dist.get_author_email()
    data['maintainer'] = dist.get_contact()
    data['maintainer_email'] = dist.get_contact_email()
    data['summary'] = dist.get_description()
    data['description'] = dist.get_long_description().replace("#", "\#")
    data['license'] = dist.get_license()
    data['platforms'] = dist.get_platforms()

    data['download_url'] = dist.get_download_url()
    data['url'] = dist.get_url()

    # XXX: reliable way to detect whether Distribution was monkey-patched by
    # setuptools
    try:
        reqs = getattr(dist, "install_requires")
        # FIXME: how to detect this correctly
        if is_string(reqs):
            reqs = [reqs]
        data['install_requires'] = reqs
    except AttributeError:
        pass

    if dist.py_modules is None:
        data['py_modules'] = []
    else:
        data['py_modules'] = dist.py_modules
    if dist.packages is None:
        packages = []
    else:
        packages = dist.packages
    data['packages'] = validate_packages(packages, top)
    if dist.ext_modules:
        data['extensions'] = dict([(e.name, e) for e in dist.ext_modules])
    else:
        data['extensions'] = {}
    data['classifiers'] = dist.get_classifiers()

    data["executables"] = {}

    entry_points = entry_points_from_dist(dist)
    if entry_points:
        console_scripts = entry_points.get("console_scripts", [])
        for entry in console_scripts:
            exe = Executable.from_representation(entry)
            data["executables"][exe.name] = exe

    return PackageDescription(**data)

_DIST_CONV_DICT = {
    "long_description": lambda meta: meta.description,
    "description": lambda meta: meta.summary,
    # TODO: keywords not implemented yet
    "keywords": lambda meta: [],
    "fullname": lambda meta: "%s-%s" % (meta.name, meta.version),
    "contact": lambda meta: (meta.maintainer or
                             meta.author or
                             "UNKNOWN"),
    "contact_email": lambda meta: (meta.maintainer_email or
                                   meta.author_email or
                                   "UNKNOWN"),
    "requires": lambda meta: meta.install_requires,
    "provides": lambda meta: [],
    "obsoletes": lambda meta: []
}

def to_distutils_meta(meta):
    from bento.compat.dist \
        import \
            DistributionMetadata
    ret = DistributionMetadata()
    for m in ret._METHOD_BASENAMES:
        try:
            val = _DIST_CONV_DICT[m](meta)
        except KeyError:
            val = getattr(meta, m)
        setattr(ret, m, val)

    return ret

def write_pkg_info(pkg, file):
    dist_meta = to_distutils_meta(pkg)
    dist_meta.write_pkg_file(file)

def entry_points_from_dist(dist):
    if hasattr(dist, "entry_points"):
        from pkg_resources import split_sections
        if is_string(dist.entry_points):
            entry_points = {}
            sections = split_sections(dist.entry_points)
            for group, lines in sections:
                group = group.strip()
                entry_points[group] = lines
        else:
            entry_points = dist.entry_points
    else:
        entry_points = {}
    return entry_points

########NEW FILE########
__FILENAME__ = commands
import sys
import traceback
import optparse

import os.path as op

from six.moves \
    import \
        cStringIO

from bento.utils.utils \
    import \
        pprint, extract_exception, comma_list_split
from bento.core.package \
    import \
        static_representation
from bento.commands.core \
    import \
        Command
from bento.errors \
    import \
        UsageException, ConvertionError
from bento.convert.core \
    import \
        detect_monkeys, monkey_patch, analyse_setup_py, build_pkg
from bento.convert.utils \
    import \
        whole_test

class ConvertCommand(Command):
    long_descr = """\
Purpose: convert a setup.py to an .info file
Usage:   bentomaker convert [OPTIONS] setup.py"""
    short_descr = "convert distutils/setuptools project to bento."
    common_options = Command.common_options + [
        optparse.Option("-t", help="TODO", default="automatic",
               dest="type"),
        optparse.Option("-o", "--output", help="output file",
               default="bento.info",
               dest="output_filename"),
        optparse.Option("-v", "--verbose", help="verbose run",
               action="store_true"),
        optparse.Option("--setup-arguments",
               help="arguments to give to setup" \
                    "For example, --setup-arguments=-q,-n,--with-speedup will " \
                    "call python setup.py -q -n --with-speedup",
               dest="setup_args")]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return
        if len(a) < 1:
            filename = "setup.py"
        else:
            filename = a[0]
        if not op.exists(filename):
            raise ValueError("file %s not found" % filename)

        output = o.output_filename
        if op.exists(output):
            raise UsageException("file %s exists, not overwritten" % output)

        if o.verbose:
            show_output = True
        else:
            show_output = False

        if o.setup_args:
            setup_args = comma_list_split(o.setup_args)
        else:
            setup_args = ["-q", "-n"]

        monkey_patch_mode = o.type

        convert_log = "convert.log"
        log = open(convert_log, "w")
        try:
            try:
                convert(ctx, filename, setup_args, monkey_patch_mode, o.verbose, output, log, show_output)
            except ConvertionError:
                raise
            except Exception:
                e = extract_exception()
                log.write("Error while converting - traceback:\n")
                tb = sys.exc_info()[2]
                traceback.print_tb(tb, file=log)
                msg = "Error while converting %s - you may look at %s for " \
                      "details (Original exception: %s %s)" 
                raise ConvertionError(msg % (filename, convert_log, type(e), str(e)))
        finally:
            log.flush()
            log.close()

def convert(ctx, filename, setup_args, monkey_patch_mode, verbose, output, log, show_output=True):
    if monkey_patch_mode == "automatic":
        try:
            if verbose:
                pprint("PINK",
                       "Catching monkey (this may take a while) ...")
            monkey_patch_mode = detect_monkeys(filename, show_output, log)
            if verbose:
                pprint("PINK", "Detected mode: %s" % monkey_patch_mode)
        except ValueError:
            e = extract_exception()
            raise UsageException("Error while detecting setup.py type " \
                                 "(original error: %s)" % str(e))

    monkey_patch(ctx.top_node, monkey_patch_mode, filename)
    dist, package_objects = analyse_setup_py(filename, setup_args)
    pkg, options = build_pkg(dist, package_objects, ctx.top_node)

    out = static_representation(pkg, options)
    if output == '-':
        for line in out.splitlines():
            pprint("YELLOW", line)
    else:
        fid = open(output, "w")
        try:
            fid.write(out)
        finally:
            fid.close()

class DetectTypeCommand(Command):
    long_descr = """\
Purpose: detect type of distutils extension used by given setup.py
Usage:   bentomaker detect_type [OPTIONS]."""
    short_descr = "detect extension type."
    common_options = Command.common_options + [
        optparse.Option("-i", "--input", help="TODO", default="setup.py", dest="setup_file"),
        optparse.Option("-v", "--verbose", help="verbose run", action="store_true")]

    def run(self, ctx):
        argv = ctx.command_argv
        p = ctx.options_context.parser
        o, a = p.parse_args(argv)
        if o.help:
            p.print_help()
            return
        verbose = o.verbose

        log = cStringIO()

        if verbose:
            print("=================================================================")
            print("Detecting used distutils extension(s) ... (This may take a while)")
        monkey_patch_mode = whole_test(o.setup_file, o.verbose, log)
        if verbose:
            print("Done !")
            print("=================================================================")
        print("Detected type: %r" % monkey_patch_mode)

########NEW FILE########
__FILENAME__ = core
import os
import sys
import warnings
import posixpath

import os.path as op

from bento.compat.api \
    import \
        relpath
from bento.conv \
    import \
        find_package, distutils_to_package_description
from bento.utils.utils \
    import \
        pprint, extract_exception
from bento.core.pkg_objects \
    import \
        DataFiles

from bento.errors \
    import \
        UsageException, ConvertionError
from bento.convert.utils \
    import \
        canonalize_path

import bento.errors

# ====================================================
# Code to convert existing setup.py to bento.info
# ====================================================
DIST_GLOBAL = None
PACKAGE_OBJECTS = None

def canonalized_path_to_package(path):
    return path.replace(posixpath.sep, ".")

def _convert_numpy_data_files(top_node, source_dir, files):
    """Convert data_files pairs to the common format we use.

    numpy.distutils internally keeps data as a pair (package_path, files_list),
    where files_list is relative to the top source path. We convert this

    Parameters
    ----------
    top_node: node
        top directory of the source tree (as a node).
    source_dir: str
        the source directory (as a path string, relative to top node).
    files: seq
        list of files (relative to top source)

    Returns
    -------
    pkg_name: str
        name of the package
    source_dir: str
        source directory
    target_dir: str
        target directory
    files: seq
        list of files (relative to source directory)
    """
    source_node = top_node.find_node(source_dir)
    if source_node is None:
        raise ConvertionError("directory %r not found" % source_dir)
    nodes = []
    for f in files:
        node = top_node.find_node(f)
        if node is None:
            raise ConvertionError("file %s refered in data_files not found" % f)
        nodes.append(node)
    pkg_name = canonalized_path_to_package(source_dir)
    target_dir = canonalize_path(op.join("$sitedir", source_dir))
    return pkg_name, source_dir, target_dir, [node.path_from(source_node) for node in nodes]

class _PackageObjects(object):
    """This private class is used to record the distribution data in our
    instrumented setup."""
    def __init__(self, monkey_patch_mode="distutils"):
        self.package_data = {}
        self.extra_source_files = {}
        self.dist_data_files = []
        self.data_files = []

        self.monkey_patch_mode = monkey_patch_mode
        self.build_lib = None

    def iter_data_files(self, top_node):
        for pkg_name, source_dir, build_dir, files in self.data_files:
            if files:
                target_dir = relpath(build_dir, self.build_lib)
                yield pkg_name, source_dir, op.join("$sitedir", target_dir), files
        if self.monkey_patch_mode == "setuptools_numpy":
            if self.dist_data_files:
                assert len(self.dist_data_files[0]) == 2, "Unhandled data files representation"
                for source_dir, files in self.dist_data_files:
                    yield _convert_numpy_data_files(top_node, source_dir, files)
        else:
            yield "", ".", "$sitedir", self.dist_data_files

# XXX: this is where the magic happens. This is highly dependent on the
# setup.py, whether it uses distutils, numpy.distutils, setuptools and whatnot.
def monkey_patch(top_node, type, filename):
    supported = ["distutils", "numpy_distutils", "setuptools", "setuptools_numpy"]

    if type == "distutils":
        from distutils.core import setup as old_setup
        from distutils.command.build_py import build_py as old_build_py
        from distutils.command.sdist import sdist as old_sdist
        from distutils.dist import Distribution as _Distribution
        from distutils.filelist import FileList
    elif type == "setuptools":
        from setuptools import setup as old_setup
        from setuptools.command.build_py import build_py as old_build_py
        from setuptools.command.sdist import sdist as old_sdist
        from distutils.dist import Distribution as _Distribution
        from distutils.filelist import FileList
    elif type == "numpy_distutils":
        import numpy.distutils
        import distutils.core
        from numpy.distutils.core import setup as old_setup
        from numpy.distutils.command.build_py import build_py as old_build_py
        from numpy.distutils.command.sdist import sdist as old_sdist
        from numpy.distutils.numpy_distribution import NumpyDistribution as _Distribution
        from distutils.filelist import FileList
    elif type == "setuptools_numpy":
        import setuptools
        import numpy.distutils
        import distutils.core
        from numpy.distutils.core import setup as old_setup
        from numpy.distutils.command.build_py import build_py as old_build_py
        from numpy.distutils.command.sdist import sdist as old_sdist
        from numpy.distutils.numpy_distribution import NumpyDistribution as _Distribution
        from distutils.filelist import FileList
    else:
        raise UsageException("Unknown converter: %s (known converters are %s)" % 
                         (type, ", ".join(supported)))

    def get_extra_source_files():
        """Return the list of files included in the tarball."""
        # FIXME: handle redundancies between data files, package data files
        # (i.e. installed data files) and files included as part of modules,
        # packages, extensions, .... Given the giant mess that distutils makes
        # of things here, it may not be possible to get everything right,
        # though.
        dist = _Distribution()
        sdist = old_sdist(dist)
        sdist.initialize_options()
        sdist.finalize_options()
        sdist.manifest_only = True
        sdist.filelist = FileList()
        sdist.distribution.script_name = filename
        sdist.get_file_list()
        return sdist.filelist.files


    def new_setup(**kw):
        global DIST_GLOBAL, PACKAGE_OBJECTS
        package_objects = _PackageObjects(monkey_patch_mode=type)

        package_dir = kw.get("package_dir", None)
        if package_dir:
            keys = list(package_dir.keys())
            if len(keys) > 1:
                raise ConvertionError("setup call with package_dir=%r argument is not supported !" \
                                      % package_dir)
            elif len(keys) == 1:
                if package_dir.values()[0] != '':
                    raise ConvertionError("setup call with package_dir=%r argument is not supported !" \
                                          % package_dir)

        cmdclass = kw.get("cmdclass", {})
        try:
            _build_py = cmdclass["build_py"]
        except KeyError:
            _build_py = old_build_py

        class build_py_recorder(_build_py):
            def run(self):
                _build_py.run(self)

                package_objects.build_lib = self.build_lib
                package_objects.extra_source_files = get_extra_source_files()

                # This is simply the data_files argument passed to setup
                if self.distribution.data_files is not None:
                    package_objects.dist_data_files.extend(self.distribution.data_files)
                # those are created from package_data stuff (the stuff included
                # if include_package_data=True as well)
                package_objects.data_files = self.data_files

        cmdclass["build_py"] = build_py_recorder
        kw["cmdclass"] = cmdclass

        dist = old_setup(**kw)
        DIST_GLOBAL = dist
        PACKAGE_OBJECTS = package_objects
        return dist

    if type == "distutils":
        import distutils.core
        distutils.core.setup = new_setup
    elif type == "setuptools":
        import distutils.core
        import setuptools
        distutils.core.setup = new_setup
        setuptools.setup = new_setup
    elif type == "setuptools_numpy":
        numpy.distutils.core.setup = new_setup
        setuptools.setup = new_setup
        distutils.core.setup = new_setup
    elif type == "numpy_distutils":
        numpy.distutils.core.setup = new_setup
        distutils.core.setup = new_setup
    else:
        raise UsageException("Unknown converter: %s (known converters are %s)" % 
                         (type, ", ".join(supported)))

def analyse_setup_py(filename, setup_args, verbose=False):
    # This is the dirty part: we run setup.py inside this process, and pass
    # data back through global variables. Not sure if there is a better way to
    # do this
    if verbose:
        pprint('PINK', "======================================================")
        pprint('PINK', " Analysing %s (running %s) .... " % (filename, filename))

    # exec_globals contains the globals used to execute the setup.py
    exec_globals = {}
    exec_globals.update(globals())
    # Some setup.py files call setup from their main, so execute them as if
    # they were the main script
    exec_globals["__name__"] = "__main__"
    exec_globals["__file__"] = op.abspath(filename)

    _saved_argv = sys.argv[:]
    _saved_sys_path = sys.path
    try:
        try:
            sys.argv = [filename] + setup_args + ["build_py"]
            # XXX: many packages import themselves to get version at build
            # time, and setuptools screw this up by inserting stuff first. Is
            # there a better way ?
            sys.path.insert(0, op.dirname(filename))
            fid = open(filename, "r")
            try:
                exec(fid.read(), exec_globals)
                if type == "distutils" and "setuptools" in sys.modules and verbose:
                    pprint("YELLOW", "Setuptools detected in distutils mode !!!")
            finally:
                fid.close()
        except ConvertionError:
            raise
        except Exception:
            e = extract_exception()
            pprint('RED', "Got exception: %s" % e)
            raise
    finally:
        sys.argv = _saved_argv
        sys.path = _saved_sys_path

    live_objects = PACKAGE_OBJECTS
    dist = DIST_GLOBAL
    if dist is None:
        raise ValueError("setup monkey-patching failed")
    else:
        if verbose:
            pprint('PINK', " %s analyse done " % filename)
            pprint('PINK', "======================================================")
        return dist, live_objects

def build_pkg(dist, package_objects, top_node):
    pkg = distutils_to_package_description(dist)
    modules = []
    for m in pkg.py_modules:
        if isinstance(m, basestring):
            modules.append(m)
        else:
            warnings.warn("The module %s it not understood" % str(m))
    pkg.py_modules = modules

    path_options = []
    data_sections = {}

    extra_source_files = []
    if package_objects.extra_source_files:
        extra_source_files.extend([canonalize_path(f) 
                                  for f in package_objects.extra_source_files])

    for pkg_name, source_dir, target_dir, files in package_objects.iter_data_files(top_node):
        if len(files) > 0:
            if len(pkg_name) > 0:
                name = "%s_data" % pkg_name.replace(".", "_")
            else:
                name = "dist_data"
            source_dir = canonalize_path(source_dir)
            target_dir = canonalize_path(target_dir)
            files = [canonalize_path(f) for f in files]
            data_sections[name] = DataFiles(name, files, target_dir, source_dir)
    pkg.data_files.update(data_sections)

    if dist.scripts:
        name = "%s_scripts" % pkg.name
        target_dir = "$bindir"
        pkg.data_files[name] = DataFiles(name, dist.scripts, target_dir, ".")

    # numpy.distutils bug: packages are appended twice to the Distribution
    # instance, so we prune the list here
    pkg.packages = sorted(list(set(pkg.packages)))
    options = {"path_options": path_options}

    pkg.extra_source_files = sorted(prune_extra_files(extra_source_files, pkg, top_node))

    return pkg, options

def prune_extra_files(files, pkg, top_node):
    package_files = []
    for p in pkg.packages:
        package_files.extend(find_package(p, top_node))
    package_files = [canonalize_path(f) for f in package_files]

    data_files = []
    for data_section in pkg.data_files.values():
        data_files.extend([posixpath.join(data_section.source_dir, f) for f in data_section.files])

    redundant = package_files + data_files + pkg.py_modules

    return prune_file_list(files, redundant)

def detect_monkeys(setup_py, show_output, log):
    from bento.convert.utils import \
        test_distutils, test_setuptools, test_numpy, test_setuptools_numpy, \
        test_can_run

    if not test_can_run(setup_py, show_output, log):
        raise bento.errors.SetupCannotRun()

    def print_delim(string):
        if show_output:
            pprint("YELLOW", string)

    print_delim("----------------- Testing distutils ------------------")
    use_distutils = test_distutils(setup_py, show_output, log)
    print_delim("----------------- Testing setuptools -----------------")
    use_setuptools = test_setuptools(setup_py, show_output, log)
    print_delim("------------ Testing numpy.distutils -----------------")
    use_numpy = test_numpy(setup_py, show_output, log)
    print_delim("--- Testing numpy.distutils patched by setuptools ----")
    use_setuptools_numpy = test_setuptools_numpy(setup_py, show_output, log)
    print_delim("Is distutils ? %s" % use_distutils)
    print_delim("Is setuptools ? %s" % use_setuptools)
    print_delim("Is numpy distutils ? %s" % use_numpy)
    print_delim("Is setuptools numpy ? %s" % use_setuptools_numpy)

    if use_distutils and not (use_setuptools or use_numpy or use_setuptools_numpy):
        return "distutils"
    elif use_setuptools  and not (use_numpy or use_setuptools_numpy):
        return "setuptools"
    elif use_numpy  and not use_setuptools_numpy:
        return "numpy"
    elif use_setuptools_numpy:
        return "setuptools_numpy"
    else:
        raise ValueError("Unsupported converter")

def prune_file_list(files, redundant):
    """Prune a list of files relatively to a second list.

    Return a subsequence of `files' which contains only files not in
    `redundant'

    Parameters
    ----------
    files: seq
        list of files to prune.
    redundant: seq
        list of candidate files to prune.
    """
    files_set = set([posixpath.normpath(f) for f in files])
    redundant_set = set([posixpath.normpath(f) for f in redundant])

    return list(files_set.difference(redundant_set))

########NEW FILE########
__FILENAME__ = test_convert
import os
import sys
import tempfile
import shutil

import os.path as op

from bento.compat.api.moves \
    import \
        unittest
from bento.core.package \
    import \
        PackageDescription
from bento.core.pkg_objects \
    import \
        DataFiles
from bento.testing.sub_test_case \
    import \
        SubprocessTestCase
from bento.core.node \
    import \
        create_first_node
from bento.convert.core \
    import \
        monkey_patch, analyse_setup_py, build_pkg, _convert_numpy_data_files, prune_extra_files

class CommonTest(SubprocessTestCase):
    def setUp(self):
        self.save = os.getcwd()
        self.d = tempfile.mkdtemp()
        os.chdir(self.d)
        try:
            self.top_node = create_first_node(self.d)
        except Exception:
            os.chdir(self.save)
            raise

    def tearDown(self):
        os.chdir(self.save)
        shutil.rmtree(self.d)

class TestMonkeyPatch(CommonTest):
    def test_distutils(self):
        monkey_patch(self.top_node, "distutils", "setup.py")
        self.assertTrue("setuptools" not in sys.modules)

    def test_setuptools(self):
        monkey_patch(self.top_node, "setuptools", "setup.py")
        self.assertTrue("setuptools" in sys.modules)

class TestBuildPackage(CommonTest):
    def test_setuptools_include_package(self):
        top = self.top_node

        top.make_node("yeah.txt").write("")
        top.make_node("foo").mkdir()
        top.find_node("foo").make_node("__init__.py").write("")
        top.find_node("foo").make_node("foo.info").write("")
        top.make_node("MANIFEST.in").write("""\
include yeah.txt
include foo/foo.info
""")

        top.make_node("setup.py").write("""\
import setuptools
from distutils.core import setup

setup(name="foo", include_package_data=True, packages=["foo"])
""")

        monkey_patch(top, "setuptools", "setup.py")
        dist, package_objects = analyse_setup_py("setup.py", ["-n", "-q"])
        pkg, options = build_pkg(dist, package_objects, top)

        self.assertEqual(pkg.data_files, {"foo_data": DataFiles("foo_data", ["foo.info"], "$sitedir/foo", "foo")})
        self.assertEqual(pkg.extra_source_files, ["setup.py", "yeah.txt"])

    def test_scripts(self):
        top = self.top_node

        top.make_node("foo").write("")

        top.make_node("setup.py").write("""\
from distutils.core import setup

setup(name="foo", scripts=["foo"])
""")

        monkey_patch(top, "distutils", "setup.py")
        dist, package_objects = analyse_setup_py("setup.py", ["-n", "-q"])
        pkg, options = build_pkg(dist, package_objects, top)

        self.assertEqual(pkg.data_files, {"foo_scripts": DataFiles("foo_scripts", ["foo"], "$bindir", ".")})

class TestMisc(unittest.TestCase):
    def setUp(self):
        super(TestMisc, self).setUp()
        self.save = os.getcwd()
        self.d = tempfile.mkdtemp()
        os.chdir(self.d)
        try:
            self.top_node = create_first_node(self.d)
        except Exception:
            os.chdir(self.save)
            raise

    def tearDown(self):
        os.chdir(self.save)
        shutil.rmtree(self.d)

        super(TestMisc, self).tearDown()

    def test_convert_numpy_data_files(self):
        source_dir = "foo/bar"
        files = [op.join(source_dir, f) for f in ["foufou", "fubar"]]
        for f in files:
            node = self.top_node.make_node(f)
            node.parent.mkdir()
            node.write("")

        pkg_name, source_dir, target_dir, files = _convert_numpy_data_files(self.top_node, source_dir, files)

        self.assertEqual(pkg_name, "foo.bar")

    def test_prune_extra_files(self):
        files = ["doc/foo.info", "yeah.txt", "foo/__init__.py"]
        for f in files:
            n = self.top_node.make_node(f)
            n.parent.mkdir()
            n.write("")

        bento_info = """\
Name: foo

ExtraSourceFiles: yeah.txt

DataFiles: doc
    SourceDir: .
    TargetDir: $sitedir
    Files: doc/foo.info

Library:
    Packages: foo
"""
        pkg = PackageDescription.from_string(bento_info)
        files = prune_extra_files(files, pkg, self.top_node)
        self.assertEqual(files, ["yeah.txt"])

########NEW FILE########
__FILENAME__ = test_convert_command
import os
import copy
import shutil
import tempfile

import os.path as op

import bento.convert.commands

from bento.errors \
    import \
        UsageException
from bento.compat.api.moves \
    import \
        unittest
from bento.core.package \
    import \
        PackageDescription
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.core.testing \
    import \
        create_fake_package_from_bento_info
from bento.commands.command_contexts \
    import \
        CmdContext
from bento.commands.options \
    import \
        OptionsContext
from bento.convert.commands \
    import \
        ConvertCommand, DetectTypeCommand
from bento.testing.sub_test_case \
    import \
        SubprocessTestCase

dummy_meta_data = dict(
        name="foo",
        version="1.0",
        description="a few words",
        long_description="some more words",
        url="http://example.com",
        download_url="http://example.com/download",
        author="John Doe",
        maintainer="John Doe",
        author_email="john@example.com",
        maintainer_email="john@example.com",
        license="BSD",
        platforms=["UNIX"],
)

bento_dummy_meta_data = copy.copy(dummy_meta_data)
bento_dummy_meta_data["platforms"] = ",".join(bento_dummy_meta_data["platforms"])

bento_meta_data_template = """\
Name: %(name)s
Version: %(version)s
Summary: %(description)s
Url: %(url)s
DownloadUrl: %(download_url)s
Description: %(long_description)s
Author: %(author)s
AuthorEmail: %(author_email)s
Maintainer: %(maintainer)s
MaintainerEmail: %(maintainer_email)s
License: %(license)s
Platforms: %(platforms)s"""

def _run_convert_command(top_node, run_node, setup_py, bento_info, cmd_argv):
    setup_node = top_node.make_node("setup.py")
    setup_node.safe_write(setup_py)

    create_fake_package_from_bento_info(top_node, bento_info)
    package = PackageDescription.from_string(bento_info)

    cmd = ConvertCommand()
    opts = OptionsContext.from_command(cmd)

    context = CmdContext(None, cmd_argv, opts, package, run_node)
    cmd.run(context)
    cmd.finish(context)
    context.finish()

class CommonTestCase(unittest.TestCase):
    def setUp(self):
        super(CommonTestCase, self).setUp()
        self.save = os.getcwd()
        self.d = tempfile.mkdtemp()
        os.chdir(self.d)
        try:
            self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))
            self.top_node = self.root._ctx.srcnode
            self.build_node = self.root._ctx.bldnode
            self.run_node = self.root.find_node(self.d)
        except Exception:
            os.chdir(self.save)
            raise

    def tearDown(self):
        os.chdir(self.save)
        shutil.rmtree(self.d)

        super(CommonTestCase, self).tearDown()

class TestConvertCommand(SubprocessTestCase, CommonTestCase):
    def test_simple_package(self):
        bento_meta_data = bento_meta_data_template % bento_dummy_meta_data
        bento_info = """\
%s

ExtraSourceFiles:
    setup.py

Library:
    Packages:
        foo
""" % bento_meta_data

        setup_py = """\
from distutils.core import setup
setup(packages=["foo"], **%s)
""" % dummy_meta_data

        output = "foo.info"
        cmd_argv = ["--output=%s" % output, "-t", "distutils"]

        _run_convert_command(self.top_node, self.run_node, setup_py, bento_info, cmd_argv=cmd_argv)
        gen_bento = self.top_node.find_node(output)
        self.assertEqual(gen_bento.read(), bento_info)

    def test_package_data_distutils(self):
        bento_meta_data = bento_meta_data_template % bento_dummy_meta_data
        bento_info = """\
%s

ExtraSourceFiles:
    setup.py

DataFiles: foo_data
    SourceDir: foo
    TargetDir: $sitedir/foo
    Files:
        info.txt

Library:
    Packages:
        foo
""" % bento_meta_data

        setup_py = """\
from distutils.core import setup
setup(packages=["foo"], package_data={"foo": ["*txt"]}, **%s)
""" % dummy_meta_data

        data_node = self.top_node.make_node(op.join("foo", "info.txt"))
        data_node.parent.mkdir()
        data_node.write("")

        output = "foo.info"
        cmd_argv = ["--output=%s" % output, "-t", "distutils"]

        _run_convert_command(self.top_node, self.run_node, setup_py, bento_info, cmd_argv=cmd_argv)
        gen_bento = self.top_node.find_node(output)
        self.assertEqual(gen_bento.read(), bento_info)

class TestMockedConvertCommand(CommonTestCase):
    """Test the convert command UI."""
    def setUp(self):
        super(TestMockedConvertCommand, self).setUp()

        def dummy_convert(ctx, filename, setup_args, monkey_patch_mode, verbose, output, log, show_output):
            pass
        self.old_convert = bento.convert.commands.convert
        try:
            bento.convert.commands.convert = lambda *a: None
        except:
            bento.convert.commands.convert = self.old_convert

    def tearDown(self):
        bento.convert.commands.convert.convert = self.old_convert

        super(TestMockedConvertCommand, self).tearDown()

    def test_simple(self):
        bento_info = """\
Name: foo

Library:
    Packages:
        foo
"""

        setup_py = """\
from distutils.core import setup
setup(packages=["foo"], name="foo")
"""
        _run_convert_command(self.top_node, self.run_node, setup_py, bento_info, [])

    def test_help(self):
        bento_info = """\
Name: foo

Library:
    Packages:
        foo
"""

        setup_py = """\
from distutils.core import setup
setup(packages=["foo"], name="foo")
"""
        _run_convert_command(self.top_node, self.run_node, setup_py, bento_info, ["-h"])

    def test_not_overwritten(self):
        bento_info = """\
Name: foo

Library:
    Packages:
        foo
"""

        setup_py = """\
from distutils.core import setup
setup(packages=["foo"], name="foo")
"""
        self.top_node.make_node("bento.info").write("")
        self.assertRaises(UsageException,
                          lambda: _run_convert_command(self.top_node,
                              self.run_node, setup_py, bento_info,
                              ["--output=bento.info"]))

class TestMockedDetectTypeCommand(CommonTestCase):
    """Test the detect_type command UI."""
    def setUp(self):
        super(TestMockedDetectTypeCommand, self).setUp()

        self.old_whole_test = bento.convert.commands.whole_test
        try:
            bento.convert.commands.whole_test = lambda *a: None
        except:
            bento.convert.commands.whole_test = self.old_whole_test

    def tearDown(self):
        bento.convert.commands.convert.whole_test = self.old_whole_test

        super(TestMockedDetectTypeCommand, self).tearDown()

    def _run_command(self):
        setup_node = self.top_node.make_node("setup.py")
        setup_node.safe_write("")

        create_fake_package_from_bento_info(self.top_node, "")
        package = PackageDescription.from_string("")

        cmd = DetectTypeCommand()
        opts = OptionsContext.from_command(cmd)

        context = CmdContext(None, [], opts, package, self.run_node)
        cmd.run(context)
        cmd.finish(context)
        context.finish()

    def test_simple(self):
        self._run_command()

########NEW FILE########
__FILENAME__ = test_utils
from bento.compat.api.moves import unittest

from bento.convert.utils \
    import \
        canonalize_path

class TestCanonalizePath(unittest.TestCase):
    def test_simple(self):
        self.assertEqual(canonalize_path(r"foo\bar"), "foo/bar")

########NEW FILE########
__FILENAME__ = utils
import os
import shutil
import sys
import tempfile
import ntpath
import posixpath

import os.path as op

from six \
    import \
        BytesIO

from subprocess \
    import \
        PIPE, STDOUT, Popen

from bento.utils.utils \
    import \
        pprint

distutils_code = """\
import sys
import distutils.core

from distutils.core import setup as _setup

DIST = None

def new_setup(**kw):
    global DIST
    dist = _setup(**kw)
    DIST = dist
    return dist

filename = '%(filename)s'

globals = {}
globals["__name__"] = "__main__"
globals["__file__"] = filename

sys.argv = [filename, "config"]
distutils.core.setup = new_setup
sys.path.insert(0, '%(odir)s')

fp = open(filename, "rt")
try:
    exec(fp.read(), globals)
finally:
    fp.close()

if DIST is None:
    sys.exit(-2)

if DIST.__module__ == "distutils.dist":
    sys.exit(0)
else:
    sys.exit(-1)
"""

setuptools_code = """\
import sys
import setuptools
import distutils.core

from setuptools import setup as _setup

DIST = None

def new_setup(**kw):
    global DIST
    dist = _setup(**kw)
    DIST = dist
    return dist

filename = '%(filename)s'

globals = {}
globals["__name__"] = "__main__"
globals["__file__"] = filename

sys.argv = [filename, "-q", "--help"]
setuptools.setup = new_setup
distutils.core.setup = new_setup
sys.path.insert(0, '%(odir)s')
fp = open(filename, "rt")
try:
    exec(fp.read(), globals)
finally:
    fp.close()


if DIST is None:
    sys.exit(-2)

if DIST.__module__ == "setuptools.dist":
    sys.exit(0)
else:
    sys.exit(-1)
"""

numpy_code = """\
import sys
try:
    import numpy.distutils.core
    from numpy.distutils.core import setup as _setup

    DIST = None

    def new_setup(**kw):
        global DIST
        dist = _setup(**kw)
        DIST = dist
        return dist

    filename = '%(filename)s'

    globals = {}
    globals["__name__"] = "__main__"
    globals["__file__"] = filename

    sys.argv = [filename, "-q", "--name"]
    numpy.distutils.core.setup = new_setup
    sys.path.insert(0, '%(odir)s')

    fp = open(filename, "rt")
    try:
        exec(fp.read(), globals)
    finally:
        fp.close()

    if DIST is None:
        sys.exit(-2)

    if DIST.__module__ == "numpy.distutils.numpy_distribution":
        sys.exit(0)
    else:
        sys.exit(-1)
except ImportError:
    sys.exit(-1)
"""

setuptools_numpy_code = """\
import sys
try:
    import setuptools
    import numpy.distutils.core
    from numpy.distutils.core import setup as _setup

    DIST = None

    def new_setup(**kw):
        global DIST
        dist = _setup(**kw)
        DIST = dist
        return dist

    filename = '%(filename)s'

    globals = {}
    globals["__name__"] = "__main__"
    globals["__file__"] = filename

    sys.argv = [filename, "-q", "--name"]
    numpy.distutils.core.setup = new_setup
    sys.path.insert(0, '%(odir)s')
    fp = open(filename, "rt")
    try:
        exec(fp, globals)
    finally:
        fp.close()


    if DIST is None:
        sys.exit(-2)

    if DIST.__module__ == "numpy.distutils.numpy_distribution" and \\
        "setuptools" in sys.modules:
        sys.exit(0)
    else:
        sys.exit(-1)
except ImportError:
    sys.exit(-1)
"""

can_run_code = """\
import sys

filename = '%(filename)s'

globals = {}
globals["__name__"] = "__main__"
globals["__file__"] = filename

sys.argv = [filename, "-q", "--name"]
sys.path.insert(0, '%(odir)s')

fp = open(filename, "rt")
try:
    exec(fp.read(), globals)
finally:
    fp.close()
"""

def logged_run(cmd, buffer):
    """Return exit code."""
    pid = Popen(cmd, stdout=PIPE, stderr=STDOUT)
    pid.wait()

    buffer.write(pid.stdout.read())
    return pid.returncode
    
def _test(code, setup_py, show_output, log):
    d = tempfile.mkdtemp()
    try:
        filename = op.join(d, "setup.py")

        cmd = [sys.executable, filename]
        log.write(" | Running %s, content below\n" % " ".join(cmd))
        log.writelines([" | | %s\n" % line for line in code.splitlines()])

        fp = open(filename, "wt")
        try:
            fp.write(code)
        finally:
            fp.close()

        buf = BytesIO()
        st = logged_run(cmd, buf)

        # FIXME: handle this correctly
        log.write(" | return of the command is %d and output is\n" % st)
        val = buf.getvalue()
        if val:
            log.writelines([" | | %s\n" % line for line in val.splitlines()])
            log.write("\n")
        else:
            log.write(" | (None)\n")
        log.write("\n")
        return st == 0
    finally:
        shutil.rmtree(d)

def test_distutils(setup_py, show_output, log):
    odir = os.path.dirname(os.path.abspath(setup_py))
    log.write("bentomaker: convert\n")
    log.write(" -> testing straight distutils\n")
    return _test(distutils_code % {"filename": setup_py, "odir": odir}, setup_py,
                 show_output, log)

def test_setuptools(setup_py, show_output, log):
    odir = os.path.dirname(os.path.abspath(setup_py))
    log.write("bentomaker: convert\n")
    log.write(" -> testing setuptools\n")
    return _test(setuptools_code % {"filename": setup_py, "odir": odir}, setup_py,
                 show_output, log)

def test_numpy(setup_py, show_output, log):
    odir = os.path.dirname(os.path.abspath(setup_py))
    log.write("bentomaker: convert\n")
    log.write(" -> testing straight numpy.distutils\n")
    return _test(numpy_code % {"filename": setup_py, "odir": odir}, setup_py,
                 show_output, log)

def test_setuptools_numpy(setup_py, show_output, log):
    odir = os.path.dirname(os.path.abspath(setup_py))
    log.write("bentomaker: convert\n")
    log.write(" -> testing numpy.distutils monkey-patched by setuptools\n")
    return _test(setuptools_numpy_code % {"filename": setup_py, "odir": odir}, setup_py,
                 show_output, log)

def test_can_run(setup_py, show_output, log):
    odir = os.path.dirname(os.path.abspath(setup_py))
    log.write("bentomaker: convert\n")
    log.write(" -> testing whether setup.py can be executed without errors\n")
    return _test(can_run_code % {"filename": setup_py, "odir": odir}, setup_py,
                 show_output, log)

def whole_test(setup_py, verbose, log):
    if verbose:
        show_output = True
    else:
        show_output = False

    if not test_can_run(setup_py, show_output, log):
        pass
    if verbose:
        pprint("YELLOW", "----------------- Testing distutils ------------------")
    use_distutils = test_distutils(setup_py, show_output, log)
    if verbose:
        pprint("YELLOW", "----------------- Testing setuptools -----------------")
    use_setuptools = test_setuptools(setup_py, show_output, log)
    if verbose:
        pprint("YELLOW", "------------ Testing numpy.distutils -----------------")
    use_numpy = test_numpy(setup_py, show_output, log)
    if verbose:
        pprint("YELLOW", "--- Testing numpy.distutils patched by setuptools ----")
    use_setuptools_numpy = test_setuptools_numpy(setup_py, show_output, log)
    if verbose:
        print("Is distutils ? %d" % use_distutils)
        print("Is setuptools ? %d" % use_setuptools)
        print("Is numpy distutils ? %d" % use_numpy)
        print("Is setuptools numpy ? %d" % use_setuptools_numpy)

    if use_distutils and not (use_setuptools or use_numpy or use_setuptools_numpy):
        return "distutils"
    elif use_setuptools  and not (use_numpy or use_setuptools_numpy):
        return "setuptools"
    elif use_numpy  and not use_setuptools_numpy:
        return "numpy.distutils"
    elif use_setuptools_numpy:
        return "setuptools + numpy.distutils converter"
    else:
        return "Unsupported converter"

def canonalize_path(path):
    """Convert a win32 path to unix path."""
    head, tail = ntpath.split(path)
    lst = [tail]
    while head and tail:
        head, tail = ntpath.split(head)
        lst.insert(0, tail)
    lst.insert(0, head)

    return posixpath.join(*lst)

########NEW FILE########
__FILENAME__ = meta
from bento.private.version \
    import \
        NormalizedVersion, is_valid_version, suggest_normalized_version
from bento.errors \
    import \
        InvalidPackage

_METADATA_EXPLICIT = ["name", "version", "summary", "url", "author",
        "author_email", "maintainer", "maintainer_email", "license", "description",
        "platforms", "install_requires", "build_requires", "download_url",
        "classifiers", "top_levels", "description_from_file", "keywords"]

_METADATA_FIELDS = _METADATA_EXPLICIT + ["version_major", "version_minor",
                                         "version_micro", "version_postdev"]

def _set_metadata(obj, name, version=None, summary=None, url=None,
        author=None, author_email=None, maintainer=None,
        maintainer_email=None, license=None, description=None,
        platforms=None, install_requires=None, build_requires=None,
        download_url=None, classifiers=None, top_levels=None,
        description_from_file=None, keywords=None):
    obj.name = name

    obj.version = version

    if version is None:
        obj.version_major = 0
        obj.version_minor = 0
        obj.version_micro = 0
        obj.version_postdev = ""
    else:
        if not is_valid_version(version):
            raise InvalidPackage("Invalid version: %r (suggested version: %s)" \
                                 % (version, suggest_normalized_version(version)))
        v = NormalizedVersion(version)
        obj.version_major = v.parts[0][0]
        obj.version_minor = v.parts[0][1]
        if len(v.parts[0]) > 2:
            obj.version_micro = v.parts[0][2]
        else:
            obj.version_micro = 0
        # FIXME: look at distutils version stuff more carefully
        obj.version_postdev = ""
        #if v.parts[1] != ('f',):
        #    raise InvalidPackage("Unsupported version: %r (prerelease part)" % (version,))
        #if v.parts[2] == ('f',):
        #    obj.version_postdev = ""
        #else:
        #    obj.version_postdev = "".join([str(i) for i in v.parts[2]])

    # FIXME: one should set metadata default elsewhere, and suggest good values
    # for developers
    obj.summary = summary or ""
    obj.url = url or ""
    obj.download_url = download_url or ""
    obj.author = author or ""
    obj.author_email = author_email or ""
    obj.maintainer = maintainer or ""
    obj.maintainer_email = maintainer_email or ""
    obj.license = license or ""
    obj.description = description or ""
    obj.description_from_file = description_from_file

    if not install_requires:
        obj.install_requires = []
    else:
        obj.install_requires = install_requires

    if not build_requires:
        obj.build_requires = []
    else:
        obj.build_requires = build_requires

    if not platforms:
        obj.platforms = []
    else:
        obj.platforms = platforms

    if not classifiers:
        obj.classifiers = []
    else:
        obj.classifiers = classifiers

    if not top_levels:
        obj.top_levels = []
    else:
        obj.top_levels = top_levels

    if not keywords:
        obj.keywords = []
    else:
        obj.keywords = keywords

    return obj

class PackageMetadata(object):
    metadata_attributes = _METADATA_FIELDS + ["fullname", "contact", "contact_email"]

    @classmethod
    def from_build_manifest(cls, build_manifest):
        return cls(**build_manifest.meta)

    @classmethod
    def from_package(cls, pkg):
        kw = {}
        for k in _METADATA_EXPLICIT:
            if hasattr(pkg, k):
                kw[k] = getattr(pkg, k)
        return cls(**kw)

    def __init__(self, name, version=None, summary=None, url=None,
            author=None, author_email=None, maintainer=None,
            maintainer_email=None, license=None, description=None,
            platforms=None, install_requires=None, build_requires=None,
            download_url=None, classifiers=None, top_levels=None,
            description_from_file=None, keywords=None):
        # Package metadata
        _args = locals()
        kw = dict([(k, _args[k]) for k in _METADATA_FIELDS if k in _args])
        _set_metadata(self, **kw)

        # FIXME: not implemented yet
        self.provides = []
        self.obsoletes = []

    @property
    def fullname(self):
        return "%s-%s" % (self.name, self.version)

    @property
    def contact(self):
        return (self.maintainer or
                self.author or
                "UNKNOWN")

    @property
    def contact_email(self):
        return (self.maintainer_email or
                self.author_email or
                "UNKNOWN")

########NEW FILE########
__FILENAME__ = node
"""
Node class: this is used to build a in-memory representation of the filesystem
in python (as a tree of Nodes). This is mainly used to compute relative
position of files in the filesystem without having to explicitly rely on
absolute paths. This is also more reliable than samepath and relpath, and quite
efficient.

Ripped off from waf (v 1.6), by Thomas Nagy. The cool design is his, bugs most
certainly mine :) We removed a few things which are not useful for bento.
"""
import os, shutil, re, sys, errno

import os.path as op

from bento.compat.api \
    import \
        rename, NamedTemporaryFile
from bento.utils.utils \
    import \
        is_string, extract_exception

def to_list(sth):
    if isinstance(sth, str):
        return sth.split()
    else:
        return sth

exclude_regs = '''
**/*~
**/#*#
**/.#*
**/%*%
**/._*
**/CVS
**/CVS/**
**/.cvsignore
**/SCCS
**/SCCS/**
**/vssver.scc
**/.svn
**/.svn/**
**/BitKeeper
**/.git
**/.git/**
**/.gitignore
**/.bzr
**/.bzrignore
**/.bzr/**
**/.hg
**/.hg/**
**/_MTN
**/_MTN/**
**/.arch-ids
**/{arch}
**/_darcs
**/_darcs/**
**/.DS_Store'''
"""
Ant patterns for files and folders to exclude while doing the
recursive traversal in :py:meth:`waflib.Node.Node.ant_glob`
"""

def split_path(path):
    return path.split('/')

def split_path_cygwin(path):
    if path.startswith('//'):
        ret = path.split('/')[2:]
        ret[0] = '/' + ret[0]
        return ret
    return path.split('/')

re_sp = re.compile('[/\\\\]')
def split_path_win32(path):
    if path.startswith('\\\\'):
        ret = re.split(re_sp, path)[2:]
        ret[0] = '\\' + ret[0]
        return ret
    return re.split(re_sp, path)

if sys.platform == 'cygwin':
    split_path = split_path_cygwin
elif sys.platform == 'win32':
    split_path = split_path_win32

class Node(object):
    __slots__ = ('name', 'sig', 'children', 'parent', 'cache_abspath', 'cache_isdir')
    def __init__(self, name, parent):
        self.name = name
        self.parent = parent

        if parent:
            if name in parent.children:
                raise ValueError('node %s exists in the parent files %r already' % (name, parent))
            parent.children[name] = self

    def __setstate__(self, data):
        self.name = data[0]
        self.parent = data[1]
        if data[2] is not None:
            self.children = data[2]
        if data[3] is not None:
            self.sig = data[3]

    def __getstate__(self):
        return (self.name, self.parent, getattr(self, 'children', None), getattr(self, 'sig', None))

    def __str__(self):
        return self.name

    def __repr__(self):
        return self.abspath()

    def __hash__(self):
        return id(self) # TODO see if it is still the case
        #raise Errors.WafError('do not hash nodes (too expensive)')

    def __eq__(self, node):
        return id(self) == id(node)

    def __copy__(self):
        "nodes are not supposed to be copied"
        raise NotImplementedError('nodes are not supposed to be copied')

    def read(self, flags='r'):
        "get the contents, assuming the node is a file"
        fid = open(self.abspath(), flags)
        try:
            return fid.read()
        finally:
            fid.close()

    def write(self, data, flags='w'):
        "write some text to the physical file, assuming the node is a file"
        f = None
        try:
            f = open(self.abspath(), flags)
            f.write(data)
        finally:
            if f:
                f.close()

    def safe_write(self, data, flags='w'):
        tmp = self.parent.make_node([self.name + ".tmp"])
        tmp.write(data, flags)
        rename(tmp.abspath(), self.abspath())

    def chmod(self, val):
        "change file/dir permissions"
        os.chmod(self.abspath(), val)

    def delete(self):
        """Delete the file/folder physically (but not the node)"""
        if getattr(self, 'children', None):
            shutil.rmtree(self.abspath())
            delattr(self, 'children')
        else:
            os.unlink(self.abspath())

    def suffix(self):
        "scons-like - hot zone so do not touch"
        k = max(0, self.name.rfind('.'))
        return self.name[k:]

    def height(self):
        "amount of parents"
        d = self
        val = -1
        while d:
            d = d.parent
            val += 1
        return val

    def listdir(self):
        "list the directory contents"
        return os.listdir(self.abspath())

    def mkdir(self):
        "write a directory for the node"
        if getattr(self, 'cache_isdir', None):
            return

        self.parent.mkdir()

        if self.name:
            try:
                os.mkdir(self.abspath())
            except OSError:
                e = extract_exception()
                if e.errno != errno.EEXIST:
                    raise

            if not os.path.isdir(self.abspath()):
                raise IOError('%s is not a directory' % self)

            try:
                self.children
            except:
                self.children = {}

        self.cache_isdir = True

    def find_node(self, lst):
        "read the file system, make the nodes as needed"
        if is_string(lst):
            lst = [x for x in split_path(lst) if x and x != '.']

        cur = self
        for x in lst:
            if x == '..':
                cur = cur.parent
                continue

            try:
                if x in cur.children:
                    cur = cur.children[x]
                    continue
            except:
                cur.children = {}

            # optimistic: create the node first then look if it was correct to do so
            cur = self.__class__(x, cur)
            try:
                os.stat(cur.abspath())
            except:
                del cur.parent.children[x]
                return None

        ret = cur

        try:
            while not getattr(cur.parent, 'cache_isdir', None):
                cur = cur.parent
                cur.cache_isdir = True
        except AttributeError:
            pass

        return ret

    def make_node(self, lst):
        "make a branch of nodes"
        if is_string(lst):
            lst = [x for x in split_path(lst) if x and x != '.']

        cur = self
        for x in lst:
            if x == '..':
                cur = cur.parent
                continue

            if getattr(cur, 'children', {}):
                if x in cur.children:
                    cur = cur.children[x]
                    continue
            else:
                cur.children = {}
            cur = self.__class__(x, cur)
        return cur

    def search(self, lst):
        "dumb search for existing nodes"
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        cur = self
        try:
            for x in lst:
                if x == '..':
                    cur = cur.parent
                else:
                    cur = cur.children[x]
            return cur
        except:
            pass

    def path_from(self, node):
        """path of this node seen from the other
            self = foo/bar/xyz.txt
            node = foo/stuff/
            -> ../bar/xyz.txt
        """
        c1 = self
        c2 = node

        c1h = c1.height()
        c2h = c2.height()

        lst = []
        up = 0

        while c1h > c2h:
            lst.append(c1.name)
            c1 = c1.parent
            c1h -= 1

        while c2h > c1h:
            up += 1
            c2 = c2.parent
            c2h -= 1

        while id(c1) != id(c2):
            lst.append(c1.name)
            up += 1

            c1 = c1.parent
            c2 = c2.parent

        for i in range(up):
            lst.append('..')
        lst.reverse()
        return os.sep.join(lst) or '.'

    def abspath(self):
        """
        absolute path
        cache into the build context, cache_node_abspath
        """
        try:
            return self.cache_abspath
        except:
            pass
        # think twice before touching this (performance + complexity + correctness)
        if not self.parent:
            val = os.sep == '/' and os.sep or ''
        elif not self.parent.name:
            # drive letter for win32
            val = (os.sep == '/' and os.sep or '') + self.name
        else:
            val = self.parent.abspath() + os.sep + self.name

        self.cache_abspath = val
        return val

    def is_child_of(self, node):
        "does this node belong to the subtree node"
        p = self
        diff = self.height() - node.height()
        while diff > 0:
            diff -= 1
            p = p.parent
        return id(p) == id(node)

    def _ant_iter(self, accept=None, maxdepth=25, pats=[], dir=False, src=True, remove=True):
        """
        Semi-private and recursive method used by ant_glob.

        :param accept: function used for accepting/rejecting a node, returns the patterns that can be still accepted in recursion
        :type accept: function
        :param maxdepth: maximum depth in the filesystem (25)
        :type maxdepth: int
        :param pats: list of patterns to accept and list of patterns to exclude
        :type pats: tuple
        :param dir: return folders too (False by default)
        :type dir: bool
        :param src: return files (True by default)
        :type src: bool
        :param remove: remove files/folders that do not exist (True by default)
        :type remove: bool
        """
        dircont = self.listdir()
        dircont.sort()

        try:
            lst = set(self.children.keys())
            if remove:
                for x in lst - set(dircont):
                    del self.children[x]
        except:
            self.children = {}

        for name in dircont:
            npats = accept(name, pats)
            if npats and npats[0]:
                accepted = [] in npats[0]

                node = self.make_node([name])

                isdir = os.path.isdir(node.abspath())
                if accepted:
                    if isdir:
                        if dir:
                            yield node
                    else:
                        if src:
                            yield node

                if getattr(node, 'cache_isdir', None) or isdir:
                    node.cache_isdir = True
                    if maxdepth:
                        for k in node._ant_iter(accept=accept, maxdepth=maxdepth - 1, pats=npats, dir=dir, src=src):
                            yield k
        raise StopIteration

    def ant_glob(self, *k, **kw):
        """
        This method is used for finding files across folders. It behaves like ant patterns:

        * ``**/*`` find all files recursively
        * ``**/*.class`` find all files ending by .class
        * ``..`` find files having two dot characters

        For example::

            def configure(cfg):
                cfg.path.ant_glob('**/*.cpp') # find all .cpp files
                cfg.root.ant_glob('etc/*.txt') # using the filesystem root can be slow
                cfg.path.ant_glob('*.cpp', excl=['*.c'], src=True, dir=False)

        For more information see http://ant.apache.org/manual/dirtasks.html

        The nodes that correspond to files and folders that do not exist will be removed

        :param incl: ant patterns or list of patterns to include
        :type incl: string or list of strings
        :param excl: ant patterns or list of patterns to exclude
        :type excl: string or list of strings
        :param dir: return folders too (False by default)
        :type dir: bool
        :param src: return files (True by default)
        :type src: bool
        :param remove: remove files/folders that do not exist (True by default)
        :type remove: bool
        :param maxdepth: maximum depth of recursion
        :type maxdepth: int
        """

        src = kw.get('src', True)
        dir = kw.get('dir', False)

        excl = kw.get('excl', exclude_regs)
        incl = k and k[0] or kw.get('incl', '**')

        def to_pat(s):
            lst = to_list(s)
            ret = []
            for x in lst:
                x = x.replace('\\', '/').replace('//', '/')
                if x.endswith('/'):
                    x += '**'
                lst2 = x.split('/')
                accu = []
                for k in lst2:
                    if k == '**':
                        accu.append(k)
                    else:
                        k = k.replace('.', '[.]').replace('*','.*').replace('?', '.').replace('+', '\\+')
                        k = '^%s$' % k
                        accu.append(re.compile(k))
                ret.append(accu)
            return ret

        def filtre(name, nn):
            ret = []
            for lst in nn:
                if not lst:
                    pass
                elif lst[0] == '**':
                    ret.append(lst)
                    if len(lst) > 1:
                        if lst[1].match(name):
                            ret.append(lst[2:])
                    else:
                        ret.append([])
                elif lst[0].match(name):
                    ret.append(lst[1:])
            return ret

        def accept(name, pats):
            nacc = filtre(name, pats[0])
            nrej = filtre(name, pats[1])
            if [] in nrej:
                nacc = []
            return [nacc, nrej]

        ret = [x for x in self._ant_iter(accept=accept, pats=[to_pat(incl), to_pat(excl)], maxdepth=25, dir=dir, src=src, remove=kw.get('remove', True))]
        if kw.get('flat', False):
            return ' '.join([x.path_from(self) for x in ret])

        return ret

    def find_dir(self, lst):
        """
        search a folder in the filesystem
        create the corresponding mappings source <-> build directories
        """
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        node = self.find_node(lst)
        try:
            os.path.isdir(node.abspath())
        except OSError:
            return None
        return node

class NodeWithBuild(Node):
    """
    Never create directly, use create_root_with_source_tree function.

    Every instance of this class must have srcnode/bldnode attributes attached
    to it *outside* __init__ (we need to create nodes before being able to
    refer to them in the instances...)
    """
    _ctx = None
    def is_src(self):
        """
        True if the node is below the source directory
        note: !is_src does not imply is_bld()

        :rtype: bool
        """
        cur = self
        x = id(self._ctx.srcnode)
        y = id(self._ctx.bldnode)
        while cur.parent:
            if id(cur) == y:
                return False
            if id(cur) == x:
                return True
            cur = cur.parent
        return False

    def is_bld(self):
        """
        True if the node is below the build directory
        note: !is_bld does not imply is_src

        :rtype: bool
        """
        cur = self
        y = id(self._ctx.bldnode)
        while cur.parent:
            if id(cur) == y:
                return True
            cur = cur.parent
        return False

    def get_bld(self):
        """for a src node, will return the equivalent bld node (or self if not possible)"""
        cur = self
        x = id(self._ctx.srcnode)
        y = id(self._ctx.bldnode)
        lst = []
        while cur.parent:
            if id(cur) == y:
                return self
            if id(cur) == x:
                lst.reverse()
                return self._ctx.bldnode.make_node(lst)
            lst.append(cur.name)
            cur = cur.parent
        return self

    def bldpath(self):
        "Path seen from the build directory default/src/foo.cpp"
        return self.path_from(self._ctx.bldnode)

    def srcpath(self):
        "Path seen from the source directory ../src/foo.cpp"
        return self.path_from(self._ctx.srcnode)

    def declare(self, lst):
        """
        if 'self' is in build directory, try to return an existing node
        if no node is found, create it in the build directory
        """
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        node = self.get_bld().search(lst)
        if node:
            if not os.path.isfile(node.abspath()):
                node.sig = None
                try:
                    node.parent.mkdir()
                except:
                    pass
            return node
        node = self.get_bld().make_node(lst)
        node.parent.mkdir()
        return node

    def change_ext(self, ext):
        "node of the same path, but with a different extension."
        name = self.name
        # XXX: is using name.find(".") as done in waf a bug ?
        k = name.rfind('.')
        if k >= 0:
            name = name[:k] + ext
        else:
            name = name + ext

        return self.parent.declare([name])

class _NodeContext(object):
    __slot__ = ("srcnode", "bldnode")

def create_first_node(source_path):
    """
    source_path be an absolute path
    """
    root = NodeWithBuild("", None)
    top = root.find_node(source_path)
    if top is None:
        raise IOError("Invalid source_path: %r" % source_path)
    return top

def create_root_with_source_tree(source_path, build_path):
    """
    Both source_path and build_path should be absolute paths
    """
    root = NodeWithBuild("", None)
    top = root.find_node(source_path)
    if top is None:
        raise IOError("Invalid source_path: %r" % source_path)
    build = root.make_node(build_path)

    node_context = _NodeContext()
    node_context.srcnode = top
    node_context.bldnode = build
    NodeWithBuild._ctx = node_context

    return root

def create_base_nodes(source_path=None, build_path=None, run_path=None):
    if source_path is None:
        source_path = os.getcwd()
    if build_path is None:
        build_path = op.join(source_path, "build")
    if run_path is None:
        run_path = os.getcwd()
    root = create_root_with_source_tree(source_path, build_path)
    top_node  = root.find_node(source_path)
    build_node  = root.find_node(build_path)
    run_node = root.find_node(run_path)
    return top_node, build_node, run_node

def find_root(n):
    while n.parent:
        n = n.parent
    return n

########NEW FILE########
__FILENAME__ = node_package
import os

import warnings

from bento.core.pkg_objects \
    import \
        Extension
from bento.core.node \
    import \
        split_path

def translate_name(name, ref_node, from_node):
    if from_node != ref_node:
        parent_pkg = ref_node.path_from(from_node).replace(os.sep, ".")
        return ".".join([parent_pkg, name])
    else:
        return name

class NodeDataFiles(object):
    def __init__(self, name, nodes, ref_node, target_dir):
        self.name = name
        self.nodes = nodes
        self.ref_node = ref_node
        self.target_dir = target_dir

class NodeExtension(object):
    def __init__(self, name, nodes, top_node, ref_node, sub_directory_node=None, include_dirs=None):
        self.name = name
        self.top_node = top_node
        self.ref_node = ref_node
        self.nodes = nodes

        if sub_directory_node is None:
            self.top_or_lib_node = top_node
        else:
            self.top_or_lib_node = sub_directory_node

        if not ref_node.is_child_of(self.top_or_lib_node):
            self.full_name = name
        else:
            self.full_name = translate_name(name, ref_node, self.top_or_lib_node)

        if include_dirs is None:
            self.include_dirs = []
        else:
            self.include_dirs = include_dirs

    def extension_from(self, from_node=None):
        if len(self.nodes) < 1:
            return Extension(self.name, [])
        else:
            if from_node is None:
                from_node = self.nodes[0].srcnode
            if not from_node.is_src():
                raise ValueError("node %s is not a source directory !" % from_node.abspath())
            if not self.ref_node.is_child_of(from_node):
                raise ValueError("from_node should be a parent of %s, but is %s" % \
                                 (self.ref_node.abspath(), from_node.abspath()))
            else:
                def translate_full_name(full_name):
                    parent_pkg = from_node.path_from(self.top_node)
                    if parent_pkg == ".":
                        parent_components = []
                    else:
                        parent_components = split_path(parent_pkg)
                    full_name_components = self.full_name.split(".")
                    if not full_name_components[:len(parent_components)] == parent_components:
                        raise ValueError("Internal bug: unexpected parent/name components: %s %s" % \
                                         (parent_components, full_name_components))
                    else:
                        return ".".join(full_name_components[len(parent_components):])
                relative_name = translate_full_name(self.full_name)
                return Extension(relative_name, sources=[n.path_from(from_node) for n in self.nodes])

class NodePythonPackage(object):
    def __init__(self, name, nodes, top_node, ref_node, sub_directory_node=None):
        self.nodes = nodes
        self.top_node = top_node
        self.ref_node = ref_node

        if sub_directory_node is None:
            self.top_or_lib_node = top_node
        else:
            self.top_or_lib_node = sub_directory_node

        if not ref_node.is_child_of(self.top_or_lib_node):
            raise IOError()

        self.full_name = translate_name(name, ref_node, self.top_or_lib_node)

class NodeRepresentation(object):
    """Node-based representation of a Package content."""
    def __init__(self, run_node, top_node, sub_directory_node=None):
        self.top_node = top_node
        self.run_node = run_node
        self.sub_directory_node = sub_directory_node

        if sub_directory_node is None:
            self.top_or_sub_directory_node = top_node
        else:
            if not sub_directory_node.is_child_of(top_node):
                raise IOError("sub_directory_node %r is not a subdirectory of %s" % \
                              (sub_directory_node, top_node))
            self.top_or_sub_directory_node = sub_directory_node

        self._registry = {}
        for category in ("modules", "packages", "extensions",
                         "compiled_libraries", "datafiles"):
            self._registry[category] = {}

        self._extra_source_nodes = []
        self._aliased_source_nodes = {}

    def to_node_extension(self, extension, source_node, ref_node):
        nodes = []
        for s in extension.sources:
            _nodes = source_node.ant_glob(s)
            if len(_nodes) < 1:
                #name = translate_name(extension.name, ref_node, self.top_or_sub_directory_node)
                raise IOError("Sources glob entry %r for extension %r did not return any result" \
                              % (s, extension.name))
            else:
                nodes.extend(_nodes)
        if extension.include_dirs:
            if self.sub_directory_node:
                raise NotImplementedError("include dirs translation not implemented yet")
            else:
                include_dirs = []
                for include_dir in extension.include_dirs:
                    n = source_node.find_dir(include_dir)
                    if n is None:
                        raise IOError("include dir %s is invalid" % include_dir)
                    else:
                        include_dirs.append(n)
        return NodeExtension(extension.name, nodes, self.top_node, ref_node, self.sub_directory_node)

    def _run_in_subpackage(self, pkg, func):
        for name, sub_pkg in pkg.subpackages.items():
            ref_node = self.top_node.find_node(sub_pkg.rdir)
            if ref_node is None:
                raise IOError("directory %s relative to %s not found !" % (sub_pkg.rdir,
                              self.top_node.abspath()))
            func(sub_pkg, ref_node)

    def _update_extensions(self, pkg):
        for name, extension in pkg.extensions.items():
            ref_node = self.top_node
            extension = self.to_node_extension(extension, self.top_node, ref_node)
            self._registry["extensions"][extension.full_name] = extension

        def _subpackage_extension(sub_package, ref_node):
            for name, extension in sub_package.extensions.items():
                extension = self.to_node_extension(extension, ref_node, ref_node)
                full_name = translate_name(name, ref_node, self.top_node)
                self._registry["extensions"][full_name] = extension
        self._run_in_subpackage(pkg, _subpackage_extension)

    def _update_libraries(self, pkg):
        for name, compiled_library in pkg.compiled_libraries.items():
            ref_node = self.top_node
            compiled_library = self.to_node_extension(compiled_library, self.top_node, ref_node)
            self._registry["compiled_libraries"][name] = compiled_library

        def _subpackage_compiled_libraries(sub_package, ref_node):
            for name, compiled_library in sub_package.compiled_libraries.items():
                compiled_library = self.to_node_extension(compiled_library, ref_node, ref_node)
                name = translate_name(name, ref_node, self.top_node)
                self._registry["compiled_libraries"][name] = compiled_library
        self._run_in_subpackage(pkg, _subpackage_compiled_libraries)

    def _update_py_packages(self, pkg):
        def _resolve_package(package_name, ref_node):
            init = os.path.join(*(package_name.split(".") + ["__init__.py"]))
            n = ref_node.find_node(init)
            if n is None:
                raise IOError("init file for package %s not found (looked for %r)!" \
                              % (package_name, init))
            else:
                p = n.parent
                nodes = [p.find_node(f) for f in p.listdir() if f.endswith(".py")]
                node_package = NodePythonPackage(package_name, nodes, self.top_node,
                                                 ref_node, self.sub_directory_node)
                self._registry["packages"][node_package.full_name] = node_package

        def _subpackage_resolve_package(sub_package, ref_node):
            for package in sub_package.packages:
                _resolve_package(package, ref_node)

        for package in pkg.packages:
            _resolve_package(package, self.top_or_sub_directory_node)
        self._run_in_subpackage(pkg, _subpackage_resolve_package)

    def _update_data_files(self, pkg):
        for name, data_section in pkg.data_files.items():
            ref_node = self.top_node.find_node(data_section.source_dir)
            nodes = []
            for f in data_section.files:
                ns = ref_node.ant_glob(f)
                if len(ns) < 1:
                    raise IOError("File/glob %s could not be resolved (data file section %s)" % (f, name))
                else:
                    nodes.extend(ns)
            self._registry["datafiles"][name] = NodeDataFiles(name, nodes, ref_node, data_section.target_dir)

    def _update_py_modules(self, pkg):
        for m in pkg.py_modules:
            n = self.top_or_sub_directory_node.find_node("%s.py" % m)
            if n is None:
                raise IOError("file for module %s not found" % m)
            else:
                self._registry["modules"][m] = n

    def _update_extra_sources(self, pkg):
        for s in pkg.extra_source_files:
            nodes = self.top_node.ant_glob(s)
            if len(nodes) < 1:
                warnings.warn("extra source files glob entry %r did not return any result" % (s,))
            self._extra_source_nodes.extend(nodes)

    def update_package(self, pkg):
        self._update_py_packages(pkg)
        self._update_py_modules(pkg)

        self._update_extensions(pkg)
        self._update_libraries(pkg)

        self._update_data_files(pkg)
        self._update_extra_sources(pkg)

    def iter_category(self, category):
        if category in self._registry:
            return self._registry[category].items()
        else:
            raise ValueError("Unknown category %s" % category)

    def register_entity(self, category, name, entity):
        if category in self._registry:
            self._registry[category][name] = entity
        else:
            raise ValueError("Category %r not registered" % category)

    def iter_source_nodes(self):
        for n in self._extra_source_nodes:
            yield n

        for d in self._registry["datafiles"].values():
            for n in d.nodes:
                yield n

        for m in self._registry["modules"].values():
            yield m
        for package in self._registry["packages"].values():
            for n in package.nodes:
                yield n

        for extension in self._registry["extensions"].values():
            for n in extension.nodes:
                yield n
        for compiled_library in self._registry["compiled_libraries"].values():
            for n in compiled_library.nodes:
                yield n

    def iter_source_files(self):
        for n in self.iter_source_nodes():
            filename = n.path_from(self.run_node)
            alias = self._aliased_source_nodes.get(n, filename)
            yield filename, alias

########NEW FILE########
__FILENAME__ = options
from bento.parser.misc \
    import \
        raw_parse, build_ast_from_raw_dict
from bento.core.pkg_objects \
    import \
        PathOption, FlagOption

def raw_to_options_kw(raw):
    d = build_ast_from_raw_dict(raw)

    kw = {}
    if not "name" in d:
        raise ValueError("No name field found")
    kw["name"] = d["name"]

    kw["path_options"] = {}
    path_options = d.get("path_options", {})
    for name, path in path_options.items():
        kw["path_options"][name] = PathOption(path["name"],
                                              path["default"],
                                              path["description"])

    kw["flag_options"] = {}
    flag_options = d.get("flag_options", {})
    for name, flag in flag_options.items():
        kw["flag_options"][name] = FlagOption(flag["name"],
                                              flag["default"],
                                              flag["description"])

    return kw

class PackageOptions(object):
    @classmethod
    def __from_data(cls, data):
        raw = raw_parse(data)
        kw = raw_to_options_kw(raw)
        return cls(**kw)

    @classmethod
    def from_string(cls, str):
        """Create a PackageOptions instance from a bento.info content."""
        return cls.__from_data(str)

    @classmethod
    def from_file(cls, filename):
        """Create a PackageOptions instance from a bento.info file."""
        fid = open(filename, 'r')
        try:
            data = fid.read()
            return cls.__from_data(data)
        finally:
            fid.close()

    def __init__(self, name, path_options=None, flag_options=None):
        """Create a PackageOptions instance

        Parameters
        ----------
        name: str
            name of the package
        path_options: dict
            dict of path options
        flag_options: dict
            dict of flag options
        """
        self.name = name

        if not path_options:
            self.path_options = {}
        else:
            self.path_options = path_options

        if not flag_options:
            self.flag_options = {}
        else:
            self.flag_options = flag_options

########NEW FILE########
__FILENAME__ = package
import os

from copy \
    import \
        deepcopy

from bento.core.pkg_objects \
    import \
        Extension, DataFiles, Executable, CompiledLibrary
from bento.core.meta \
    import \
        _set_metadata, _METADATA_FIELDS
from bento.parser.misc \
    import \
        build_ast_from_raw_dict, raw_parse
from bento.compat.api \
    import \
        relpath
from bento.core.subpackage \
    import \
        SubPackageDescription
from bento.core.parse_helpers \
    import \
        extract_top_dicts, extract_top_dicts_subento
from bento.errors \
    import \
        InvalidPackage, InternalBentoError
import bento.utils.path

def _parse_libraries(libraries):
    ret = {}
    if len(libraries) > 0:
        if not list(libraries.keys()) == ["default"]:
            raise NotImplementedError(
                    "Non default library not yet supported")

        default = libraries["default"]
        for k in ["packages", "py_modules", "install_requires", "sub_directory"]:
            if k in default:
                ret[k] = default[k]

        ret["extensions"] = {}
        for k, v in default.get("extensions", {}).items():
            ret["extensions"][k] = Extension.from_parse_dict(v)

        ret["compiled_libraries"] = {}
        for k, v in default.get("compiled_libraries", {}).items():
            ret["compiled_libraries"][k] = \
                    CompiledLibrary.from_parse_dict(v)
    return ret

def recurse_subentos(subentos, source_dir):
    filenames = []
    subpackages = {}

    # FIXME: this is damn ugly - using nodes would be good here
    def _recurse(subento, cwd):
        f = os.path.normpath(os.path.join(cwd, subento, "bento.info"))
        if not os.path.exists(f):
            raise ValueError("%s not found !" % f)
        filenames.append(relpath(f, source_dir))

        fid = open(f)
        try:
            key = relpath(f, source_dir)
            rdir = relpath(os.path.join(cwd, subento), source_dir)

            d = raw_parse(fid.read(), f)
            kw, subentos = raw_to_subpkg_kw(d)
            subpackages[key] = SubPackageDescription(rdir, **kw)
            hooks_as_abspaths = [os.path.normpath(os.path.join(cwd, subento, h)) \
                                 for h in subpackages[key].hook_files]
            filenames.extend([relpath(f, source_dir) for f in hooks_as_abspaths])
            for s in subentos:
                _recurse(s, os.path.join(cwd, subento))
        finally:
            fid.close()

    for s in subentos:
        _recurse(s, source_dir)
    return subpackages, filenames

def build_libs_from_dict(libraries_d):
    return _parse_libraries(libraries_d)

def build_executables_from_dict(executables_d):
    executables = {}
    for name, executable in executables_d.items():
        executables[name] = Executable.from_parse_dict(executable)
    return executables

def build_data_files_from_dict(data_files_d):
    data_files = {}
    for name, data_file_d in data_files_d.items():
        data_files[name] = DataFiles.from_parse_dict(data_file_d)
    return data_files

def raw_to_subpkg_kw(raw_dict):
    d = build_ast_from_raw_dict(raw_dict)

    libraries_d, misc_d = extract_top_dicts_subento(deepcopy(d))

    kw = {}
    libraries = build_libs_from_dict(libraries_d)
    kw.update(libraries)
    kw["hook_files"] = misc_d["hook_files"]
    if libraries_d:
        sub_directory = kw.pop("sub_directory")
        if sub_directory is not None:
            raise InternalBentoError("Unexpected sub_directory while parsing recursed bendo")

    return kw, misc_d["subento"]

def raw_to_pkg_kw(raw_dict, user_flags, bento_info=None):
    if bento_info is None:
        source_dir = os.getcwd()
    else:
        # bento_info may be a string or a node
        try:
            source_dir = bento_info.parent.abspath()
            bento_info_path = bento_info.srcpath()
        except AttributeError:
            if os.path.isabs(bento_info):
                source_dir = os.path.dirname(bento_info)
                bento_info_path = os.path.basename(bento_info)
            else:
                source_dir = os.getcwd()
                bento_info_path = os.path.basename(bento_info)
                assert bento_info_path == bento_info

    d = build_ast_from_raw_dict(raw_dict, user_flags)

    meta_d, libraries_d, options_d, misc_d = extract_top_dicts(deepcopy(d))
    libraries = build_libs_from_dict(libraries_d)
    executables = build_executables_from_dict(misc_d.pop("executables"))
    data_files = build_data_files_from_dict(misc_d.pop("data_files"))

    kw = {}
    kw.update(meta_d)
    for k in libraries:
        kw[k] = libraries[k]
    kw["executables"] = executables
    kw["data_files"] = data_files

    misc_d.pop("path_options")
    misc_d.pop("flag_options")

    if "subento" in misc_d:
        subentos = misc_d.pop("subento")
        if len(subentos) > 0 and libraries and libraries["sub_directory"] is not None:
            raise InvalidPackage("You cannot use both Recurse and Library:SubDirectory features !")
        else:
            subpackages, files = recurse_subentos(subentos, source_dir=source_dir)
            kw["subpackages"] = subpackages
    else:
        files = []

    kw.update(misc_d)
    if bento_info is not None:
        files.append(bento_info_path)
    files.extend(misc_d["hook_files"])
    # XXX: Do we want to automatically add the hook and bento files in extra
    # source files at the PackageDescription level ?
    kw["extra_source_files"].extend(files)

    if "description_from_file" in kw:
        if bento_info:
            description_file = os.path.join(source_dir, kw["description_from_file"])
        else:
            description_file = kw["description_from_file"]
        if not os.path.exists(description_file):
            raise IOError("Description file %r not found" % (description_file,))
        else:
            f = open(description_file)
            try:
                kw["description"] = f.read()
            finally:
                f.close()
    return kw, files

class PackageDescription:
    @classmethod
    def __from_data(cls, data, user_flags, filename=None):
        if not user_flags:
            user_flags = {}

        d = raw_parse(data, filename)
        kw, files = raw_to_pkg_kw(d, user_flags, filename)
        return cls(**kw)

    @classmethod
    def from_string(cls, s, user_flags=None):
        """Create a PackageDescription from a string containing the package
        description."""
        return cls.__from_data(s, user_flags)

    @classmethod
    def from_file(cls, filename, user_flags=None):
        """Create a PackageDescription from a bento.info file."""
        info_file = open(filename, 'r')
        try:
            data = info_file.read()
            ret = cls.__from_data(data, user_flags, filename)
            return ret
        finally:
            info_file.close()

    # FIXME: this stuff has passed the uglyness threshold quite some time
    # ago...
    def __init__(self, name=None, version=None, summary=None, url=None,
            author=None, author_email=None, maintainer=None,
            maintainer_email=None, license=None, description=None,
            platforms=None, packages=None, py_modules=None, extensions=None,
            install_requires=None, build_requires=None,
            download_url=None, extra_source_files=None, data_files=None,
            classifiers=None, provides=None, obsoletes=None, executables=None,
            hook_files=None, config_py=None, compiled_libraries=None,
            subpackages=None, description_from_file=None, meta_template_files=None,
            keywords=None, sub_directory=None, use_backends=None):
        # XXX: should we check that we have sequences when required
        # (py_modules, etc...) ?

        # Package content
        if not packages:
            self.packages = []
        else:
            self.packages = packages

        # Package content
        if not subpackages:
            self.subpackages = {}
        else:
            self.subpackages = subpackages

        if not py_modules:
            self.py_modules = []
        else:
            self.py_modules = py_modules

        if extensions:
            self.extensions = extensions
        else:
            self.extensions = {}
        if compiled_libraries:
            self.compiled_libraries = compiled_libraries
        else:
            self.compiled_libraries = {}

        if not extra_source_files:
            self.extra_source_files = []
        else:
            self.extra_source_files = extra_source_files

        if not data_files:
            self.data_files = {}
        else:
            self.data_files = data_files

        if not executables:
            self.executables = {}
        else:
            self.executables = executables

        self.use_backends = use_backends or []

        pkgs = []
        for p in self.packages:
            pkgs.append(p)
        for p in self.py_modules:
            pkgs.append(p)
        for p in self.extensions.values():
            pkgs.append(p.name)
        top_levels = [i for i in pkgs if not "." in i]

        # Package metadata
        _args = locals()
        kw = dict([(k, _args[k]) for k in _METADATA_FIELDS if k in _args])
        _set_metadata(self, **kw)

        if hook_files is not None:
            self.hook_files = hook_files
        else:
            self.hook_files = []

        if config_py is not None and os.sep != "/":
            self.config_py = bento.utils.path.unnormalize_path(config_py)
        else:
            self.config_py = config_py

        if meta_template_files is None:
            meta_template_files = []
        self.meta_template_files = [bento.utils.path.unnormalize_path(f) for f in meta_template_files]

        self.extra_source_files.extend(self.meta_template_files)

        self.sub_directory = sub_directory

def static_representation(pkg, options={}):
    """Return the static representation of the given PackageDescription
    instance as a string."""
    indent_level = 4
    r = []

    def indented_list(head, seq, ind):
        r.append("%s%s:" % (' ' * (ind - 1) * indent_level, head))
        r.append(',\n'.join([' ' * ind * indent_level + i for i in seq]))

    if pkg.name:
        r.append("Name: %s" % pkg.name)
    if pkg.version:
        r.append("Version: %s" % pkg.version)
    if pkg.summary:
        r.append("Summary: %s" % pkg.summary)
    if pkg.url:
        r.append("Url: %s" % pkg.url)
    if pkg.download_url:
        r.append("DownloadUrl: %s" % pkg.download_url)
    if pkg.description:
        lines = pkg.description.splitlines()
        description = [lines[0]]
        if len(lines) > 1:
            description.extend([' ' * indent_level  + line for line in lines[1:]])
        r.append("Description: %s" % "\n".join(description))
    if pkg.author:
        r.append("Author: %s" % pkg.author)
    if pkg.author_email:
        r.append("AuthorEmail: %s" % pkg.author_email)
    if pkg.maintainer:
        r.append("Maintainer: %s" % pkg.maintainer)
    if pkg.maintainer_email:
        r.append("MaintainerEmail: %s" % pkg.maintainer_email)
    if pkg.license:
        r.append("License: %s" % pkg.license)
    if pkg.platforms:
        r.append("Platforms: %s" % ",".join(pkg.platforms))
    if pkg.classifiers:
        indented_list("Classifiers", pkg.classifiers, 1)

    if options:
        for k in options:
            if k == "path_options":
                for p in options["path_options"]:
                    r.append('')
                    r.append("Path: %s" % p.name)
                    r.append(' ' * indent_level + "Description: %s" % p.description)
                    r.append(' ' * indent_level + "Default: %s" % p.default_value)
            else:
                raise ValueError("Gne ? %s" % k)
        r.append('')

    if pkg.extra_source_files:
        indented_list("ExtraSourceFiles", pkg.extra_source_files, 1)
        r.append('')

    if pkg.data_files:
        for section in pkg.data_files.values():
            r.append("DataFiles: %s" % section.name)
            r.append(' ' * indent_level + "SourceDir: %s" % section.source_dir)
            r.append(' ' * indent_level + "TargetDir: %s" % section.target_dir)
            indented_list("Files", section.files, 2)
            r.append('')

    # Fix indentation handling instead of hardcoding it
    if pkg.py_modules or pkg.packages or pkg.extensions:
        r.append("Library:")

        if pkg.install_requires:
            indented_list("InstallRequires", pkg.install_requires, 2)
        if pkg.py_modules:
            indented_list("Modules", pkg.py_modules, 2)
        if pkg.packages:
            indented_list("Packages", pkg.packages, 2)

        if pkg.extensions:
            for name, ext in pkg.extensions.items():
                r.append(' ' * indent_level + "Extension: %s" % name)
                indented_list("Sources", ext.sources, 3)
                if ext.include_dirs:
                    indented_list("IncludeDirs", ext.include_dirs, 3)
        r.append("")

    for name, value in pkg.executables.items():
        r.append("Executable: %s" % name)
        r.append(' ' * indent_level + "Module: %s" % value.module)
        r.append(' ' * indent_level + "Function: %s" % value.function)
        r.append("")
    return "\n".join(r)

def static_representation_yaml(package, options={}):
    """Return the static representation of the given PackageDescription
    instance as a string."""
    indent_level = 4

########NEW FILE########
__FILENAME__ = parse_helpers
from bento.errors \
    import \
        InvalidPackage
from bento.core.meta \
    import \
        _METADATA_FIELDS

def extract_top_dicts(d):
    """Given an "abstract" dictionary returned by parse_to_dict, build
    meta, library, options and misc dictionaries."""
    meta = {}
    misc = {"extra_source_files": [],
            "executables": {},
            "data_files": {},
            "hook_files": [],
            "config_py": None,
            "meta_template_files": [],
            "flag_options": [],
            "path_options": [],
            "subento": [],
            "use_backends": []}
    options = {}

    for k in _METADATA_FIELDS:
        if k in d:
            meta[k] = d.pop(k)
    if "libraries" in d:
        libraries = d.pop("libraries")
    else:
        libraries = {}
    for k in misc.keys():
        if k in d:
            misc[k] = d.pop(k)

    if len(d) > 0:
        raise ValueError("Unknown entry(ies) %s" % d.keys())

    return meta, libraries, options, misc

def extract_top_dicts_subento(d):
    """Given an "abstract" dictionary returned by parse_to_dict, build
    library, and misc dictionaries.
    
    This function should be used for subentos"""
    misc = {"subento": [], "hook_files": []}

    if "libraries" in d:
        libraries = d.pop("libraries")
    else:
        libraries = {}
    # FIXME: bento vs subento visitor. Those should not be defined in the first
    # place for subento.
    for library in libraries.values():
        for k, field_name in [("install_requires", "InstallRequires"), ("py_modules", "Modules")]:
            v = library.pop(k)
            if len(v) > 0:
                raise InvalidPackage("Invalid entry %r in recursed bento file(s)" % field_name)
    for k in misc.keys():
        if k in d:
            misc[k] = d.pop(k)

    # FIXME: bento vs subento visitor. Those should not be defined in the first
    # place for subento.
    for k in ["path_options", "flag_options", "data_files", "extra_source_files", "executables"]:
        v = d.pop(k)
        if len(v) > 0:
            raise ValueError("Invalid non empty entry %s" % k)
    if len(d) > 0:
        raise ValueError("Unknown entry(ies) %s" % d.keys())

    return libraries, misc

########NEW FILE########
__FILENAME__ = pkg_objects
import os.path as op

import bento.utils.path

class FlagOption(object):
    def __init__(self, name, default_value, description=None):
        self.name = name
        self.default_value = default_value
        self.description = description

    def __str__(self):
        r = """\
Flag %s
    default value: %s
    description: %s"""
        return r % (self.name, self.default_value, self.description)

class PathOption(object):
    def __init__(self, name, default_value, description=None):
        self.name = name
        self.default_value = default_value
        self.description = description

    def __str__(self):
        r = """\
Customizable path: %s
    default value: %s
    description: %s"""
        return r % (self.name, self.default_value, self.description)

class DataFiles(object):
    @classmethod
    def from_parse_dict(cls, d):
        return cls(**d)

    def __init__(self, name, files=None, target_dir=None, source_dir=None):
        self.name = name

        if files is not None:
            self.files = files
        else:
            self.files = []

        if target_dir is not None:
            self.target_dir = target_dir
        else:
            self.target_dir = "$sitedir"

        if source_dir is not None:
            self.source_dir = source_dir
        else:
            self.source_dir = "."

    def __repr__(self):
        return "DataSection(files=%r, target_dir=%r, source_dir=%r)" % \
               (self.files, self.target_dir, self.source_dir)

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

class Executable(object):
    @classmethod
    def from_parse_dict(cls, d):
        return cls(**d)

    @classmethod
    def from_representation(cls, s):
        if not "=" in s:
            raise ValueError("s should be of the form name=module:function")
        name, value = [j.strip() for j in s.split("=")]
        if not ":" in value:
            raise ValueError(
                "string representation should be of the form module:function, not %s"
                % value)
        module, function = value.split(":", 1)
        return cls(name, module, function)

    def __init__(self, name, module, function):
        # FIXME: check that module is a module name ?
        self.name = name
        self.module = module
        self.function = function

    # FIXME: this function should not really be here...
    def representation(self):
        return ":".join([self.module, self.function])

    # FIXME: this function should not really be here...
    def full_representation(self):
        return "%s = %s" % (self.name, self.representation())

    def __repr__(self):
        return repr({"name": self.name, "module": self.module, "function": self.function})

class Compiled(object):
    @classmethod
    def from_parse_dict(cls, d):
        return cls(**d)

    def __init__(self, name, sources, include_dirs=None):
        self.name = name
        self.base_name = op.basename(name)
        self.sources = [bento.utils.path.normalize_path(p) for p in sources]
        if include_dirs is None:
            self.include_dirs = []
        else:
            self.include_dirs = include_dirs

    def __str__(self):
        return "%s(%s, %s, %s)" % (self.__class__.__name__, self.name,
                                   self.sources, self.include_dirs)

    def __eq__(self, other):
        return self.name == other.name \
                and self.sources == other.sources \
                and self.include_dirs == other.include_dirs

    #def __repr__(self):
    #    return self.__str__()

class Extension(Compiled):
    pass

class CompiledLibrary(Compiled):
    pass

########NEW FILE########
__FILENAME__ = sysconfig
import os
import sys

import bento.utils.path

def get_scheme(platform):
    # Whenever you add a variable in schemes, you should add one in
    # scheme_opts as well, otherwise the it will not be customizable from
    # configure.
    schemes = {
        'unix': {
            'destdir': bento.utils.path.find_root(sys.prefix),
            'prefix': sys.prefix,
            'eprefix': sys.exec_prefix,
            'bindir': '$eprefix/bin',
            'sbindir': '$eprefix/sbin',
            'libexecdir': '$eprefix/libexec',
            'sysconfdir': '$prefix/etc',
            'sharedstatedir': '$prefix/com',
            'localstatedir': '$prefix/var',
            'libdir': '$eprefix/lib',
            'includedir': '$prefix/include',
            'datarootdir': '$prefix/share',
            'datadir': '$datarootdir',
            'mandir': '$datarootdir/man',
            'infodir': '$datarootdir/info',
            'localedir': '$datarootdir/locale',
            'docdir': '$datarootdir/doc/$pkgname',
            'htmldir': '$docdir',
            'dvidir': '$docdir',
            'psdir': '$docdir',
            'pdfdir': '$docdir',
            'sitedir': '$libdir/python$py_version_short/site-packages',
            'pkgdatadir': '$datadir/$pkgname'
        },
        'win32': {
            'destdir': bento.utils.path.find_root(sys.prefix),
            'prefix': sys.prefix,
            'eprefix': r'$prefix',
            'bindir': r'$eprefix\Scripts',
            'sbindir': r'$eprefix\Scripts',
            'libexecdir': r'$eprefix\Scripts',
            'sysconfdir': r'$prefix\etc',
            'sharedstatedir': r'$prefix\com',
            'localstatedir': r'$prefix\var',
            'libdir': r'$eprefix\lib',
            'includedir': r'$prefix\include',
            'datarootdir': r'$prefix\share',
            'datadir': r'$datarootdir',
            'mandir': r'$datarootdir\man',
            'infodir': r'$datarootdir\info',
            'localedir': r'$datarootdir\locale',
            'docdir': r'$datarootdir\doc\$pkgname',
            'htmldir': r'$docdir',
            'dvidir': r'$docdir',
            'psdir': r'$docdir',
            'pdfdir': r'$docdir',
            'sitedir': r'$prefix\Lib\site-packages',
            'pkgdatadir': r'$datadir\$pkgname'
        }
    }

    schemes_opts = {
        'prefix': {'opts': ['--prefix'],
                   'help': 'install architecture-independent files '
                           'in PREFIX [%s]'},
        'eprefix': {'opts': ['--exec-prefix'],
                   'help': 'install architecture-dependent files '
                           'in EPREFIX [%s]',
                    'dest': 'eprefix'},
        'bindir': {'opts': ['--bindir'],
                   'help': 'user executables [%s]'},
        'sbindir': {'opts': ['--sbindir'],
                   'help': 'system admin executables [%s]'},
        'libexecdir': {'opts': ['--libexecdir'],
                       'help': 'program executables [%s]'},
        'sysconfdir': {'opts': ['--sysconfdir'],
                       'help': 'read-only single-machine data [%s]'},
        'sharedstatedir': {'opts': ['--sharedstatedir'],
                           'help': 'modifiable architecture-independent data [%s]'},
        'localstatedir': {'opts': ['--localstatedir'],
                          'help': 'modifiable single-machine data [%s]'},
        'libdir': {'opts': ['--libdir'],
                   'help': 'object code library [%s]'},
        'includedir': {'opts': ['--includedir'],
                   'help': 'C header files [%s]'},
        'datarootdir': {'opts': ['--datarootdir'],
                   'help': 'read-only arch.-independent data root [%s]'},
        'datadir': {'opts': ['--datadir'],
                   'help': 'read-only arch.-independent data [%s]'},
        'infodir': {'opts': ['--infodir'],
                   'help': 'info documentation [%s]'},
        'localedir': {'opts': ['--localedir'],
                   'help': 'locale-dependent files [%s]'},
        'mandir': {'opts': ['--mandir'],
                   'help': 'man documentation [%s]'},
        'docdir': {'opts': ['--docdir'],
                   'help': 'documentation root [%s]'},
        'htmldir': {'opts': ['--htmldir'],
                   'help': 'html documentation [%s]'},
        'dvidir': {'opts': ['--dvidir'],
                   'help': 'dvi documentation [%s]'},
        'psdir': {'opts': ['--psdir'],
                   'help': 'ps documentation [%s]'},
        'pdfdir': {'opts': ['--pdfdir'],
                   'help': 'pdf documentation [%s]'},
        'sitedir': {'opts': ['--sitedir'],
                    'help': 'python site-packages [%s]'},
        'pkgdatadir': {'opts': ['--pkgdatadir'],
                    'help': 'package-specific data dir [%s]'},
        'destdir': {'opts': ['--destdir'],
                    'help': 'alternate root to install to [%s]'}
    }
    if platform.startswith('win32'):
        pkg_platform = platform
    else:
        pkg_platform = 'unix'

    try:
        scheme = schemes[pkg_platform]
    except KeyError:
        raise ValueError("Platform %s not yet supported" % platform)

    scheme_opts = {}
    for k, v in schemes_opts.items():
        val = schemes_opts[k].copy()
        val['help'] = val['help'] % scheme[k]
        scheme_opts[k] = val

    return scheme, scheme_opts

########NEW FILE########
__FILENAME__ = subpackage
import os

from bento.core.pkg_objects \
    import \
        Extension, CompiledLibrary

class SubPackageDescription:
    def __init__(self, rdir, packages=None, extensions=None,
                 compiled_libraries=None, py_modules=None, hook_files=None):
        self.rdir = rdir
        if packages is None:
            self.packages = []
        else:
            self.packages = packages
        if extensions is None:
            self.extensions = {}
        else:
            self.extensions = extensions
        if compiled_libraries is None:
            self.compiled_libraries = {}
        else:
            self.compiled_libraries = compiled_libraries
        if py_modules is None:
            self.py_modules = []
        else:
            self.py_modules = py_modules

        if hook_files is None:
            self.hook_files = []
        else:
            self.hook_files = hook_files

    def __repr__(self):
        return repr({"packages": self.packages,
                     "py_modules": self.py_modules,
                     "clibs": self.compiled_libraries,
                     "extensions": self.extensions})

def flatten_subpackage_packages(spkg, top_node):
    """Translate the (python) packages from a subpackage relatively to
    the given top node.
    """
    local_node = top_node.find_dir(spkg.rdir)
    parent_pkg = local_node.path_from(top_node).replace(os.pathsep, ".")
    ret = ["%s.%s" % (parent_pkg, p) for p in spkg.packages]
    return ret

def flatten_subpackage_extensions(spkg, top_node):
    """Translate the extensions from a subpackage relatively to the
    given top node.

    Extension name, source files and include directories paths are all
    translated relatively to the top node.

    Returns
    -------
    d : dict
        {ext_name: ext} dictionary

    Example
    -------
    Defining in /foo/bar the extension::

        Extension("_hello", sources=["src/hellomodule.c"])

    and top_node corresponding to /foo, the
    extension would be translated as::

        Extension("bar._hello", sources=["bar/src/hellomodule.c"])
    """
    local_node = top_node.find_dir(spkg.rdir)
    if local_node is None:
        raise IOError("Path %s not found" % \
                      os.path.join(top_node.abspath(), spkg.rdir))
    elif local_node == top_node:
        raise ValueError("Subpackage in top directory ??")

    ret = {}
    for name, extension in spkg.extensions.items():
        parent_pkg = spkg.rdir.replace(os.sep, ".")
        full_name = parent_pkg + ".%s" % name
        sources = []
        for s in extension.sources:
            node = local_node.find_node(s)
            if node is None:
                raise IOError("File %s not found" % s)
            sources.append(node.path_from(top_node))
        include_dirs = [
                local_node.find_node(d).path_from(top_node) \
                for d in extension.include_dirs]
        ret[full_name] = Extension(full_name, sources, include_dirs)
    return ret

def flatten_subpackage_compiled_libraries(spkg, top_node):
    """Translate the compiled libraries from a subpackage relatively
    to the given top node.

    Source files and include directories paths are all
    translated relatively to the top node.

    Returns
    -------
    d : dict
        {name: clib} dictionary

    Example
    -------
    Defining in /foo/bar the compiled library::

        CompiledLibrary("fubar", sources=["src/fubar.c"])

    and top_node corresponding to /foo, the
    extension would be translated as::

        CompiledLibrary("fubar", sources=["bar/src/fubar.c"])
    """
    local_node = top_node.find_dir(spkg.rdir)
    if local_node is None:
        raise IOError("Path %s not found" % \
                      os.path.join(top_node.abspath(), spkg.rdir))
    elif local_node == top_node:
        raise ValueError("Subpackage in top directory ??")

    ret = {}
    for name, clib in spkg.compiled_libraries.items():
        nodes = []
        for source in clib.sources:
            _nodes = local_node.ant_glob(source)
            if len(_nodes) < 1:
                raise IOError("Pattern %r did not resolve to anything !" % source)
            else:
                nodes.extend(_nodes)
        sources = [node.path_from(top_node) for node in nodes]
        include_dirs = [
                local_node.find_node(d).path_from(top_node) \
                for d in clib.include_dirs]
        parent_pkg = spkg.rdir.replace(os.sep, ".")
        full_name = ".".join([parent_pkg, name])
        ret[full_name] = CompiledLibrary(full_name, sources, include_dirs)
    return ret

def get_extensions(pkg, top_node):
    """Return the dictionary {name: extension} of all every extension
    in pkg, including the one defined in subpackages (if any).

    Note
    ----
    Extensions defined in subpackages are translated relatively to
    top_dir
    """
    extensions = {}
    for name, ext in pkg.extensions.items():
        extensions[name] = ext
    for spkg in pkg.subpackages.values():
        extensions.update(
                flatten_subpackage_extensions(spkg, top_node))
    return extensions

def get_compiled_libraries(pkg, top_node):
    """Return the dictionary {name: extension} of every compiled library in
    pkg, including the one defined in subpackages (if any).

    Note
    ----
    Extensions defined in subpackages are translated relatively to
    top_dir
    """
    libraries = {}
    for name, ext in pkg.compiled_libraries.items():
        libraries[name] = ext
    for spkg in pkg.subpackages.values():
        local_libs = flatten_subpackage_compiled_libraries(spkg,
                                                           top_node)
        libraries.update(local_libs)
    return libraries

def get_packages(pkg, top_node):
    """Return the dictionary {name: package} of every (python) package
    in pkg, including the one defined in subpackages (if any).
    """
    packages = [p for p in pkg.packages]
    for spkg in pkg.subpackages.values():
        local_pkgs = flatten_subpackage_packages(spkg, top_node)
        packages.extend(local_pkgs)
    return packages

########NEW FILE########
__FILENAME__ = testing
import os
import sys
import tempfile

from bento.compat.api.moves \
    import \
        unittest
from bento.core.package \
    import \
        raw_parse, raw_to_pkg_kw
from bento.core.pkg_objects \
    import \
        Extension, CompiledLibrary
from bento.utils.utils \
    import \
        memoized

if "nose" in sys.modules:
    from bento.core._nose_compat import install_proxy, install_result
    install_proxy()
    install_result()

def skip_if(condition, msg=""):
    return unittest.skipIf(condition, msg)

def expected_failure(f):
    return unittest.expectedFailure(f)

def require_c_compiler(builder="yaku"):
    if builder == "yaku":
        return _require_c_compiler_yaku()
    elif builder == "distutils":
        return _require_c_compiler_distutils()
    else:
        raise ValueError("Unrecognized builder: %r" % builder)

@memoized
def _require_c_compiler_distutils():
    from bento.commands.build_distutils import DistutilsBuilder
    builder = DistutilsBuilder()
    try:
        if len(builder.ext_bld_cmd.compiler.executables) < 1:
            return unittest.skipIf(True, "No C compiler available")
        return unittest.skipIf(False, "")
    except Exception:
        return unittest.skipIf(True, "No C compiler available")

@memoized
def _require_c_compiler_yaku():
    import yaku.context
    source_path = tempfile.mkdtemp()
    build_path = os.path.join(source_path, "build")
    context = yaku.context.get_cfg(src_path=source_path, build_path=build_path)
    try:
        context.use_tools(["pyext", "ctasks"])
        return unittest.skipIf(False, "")
    except ValueError:
        return unittest.skipIf(True, "No C compiler available")

DUMMY_C = r"""\
#include <Python.h>
#include <stdio.h>

static PyObject*
hello(PyObject *self, PyObject *args)
{
    printf("Hello from C\n");
    Py_INCREF(Py_None);
    return Py_None;
}

static PyMethodDef HelloMethods[] = {
    {"hello",  hello, METH_VARARGS, "Print a hello world."},
    {NULL, NULL, 0, NULL}        /* Sentinel */
};

PyMODINIT_FUNC
init%(name)s(void)
{
    (void) Py_InitModule("%(name)s", HelloMethods);
}
"""

DUMMY_CLIB = r"""\
int hello(void)
{
    return 0;
}
"""
def create_fake_package_from_bento_info(top_node, bento_info):
    d = raw_parse(bento_info)
    _kw, files = raw_to_pkg_kw(d, {}, None)
    kw = {}
    if "extensions" in _kw:
        kw["extensions"] = _kw["extensions"].values()
    if "py_modules" in _kw:
        kw["modules"] = _kw["py_modules"]
    if "packages" in _kw:
        kw["packages"] = _kw["packages"]
    if "compiled_libraries" in _kw:
        kw["compiled_libraries"] = _kw["compiled_libraries"].values()
    if "extra_source_files" in _kw:
        kw["extra_source_files"] = _kw["extra_source_files"]
    if "sub_directory" in _kw:
        kw["sub_directory"] = _kw["sub_directory"]
    if "data_files" in _kw:
        kw["data_files"] = _kw["data_files"]
    return create_fake_package(top_node, **kw)

def create_fake_package_from_bento_infos(top_node, bento_infos, bscripts=None):
    if bscripts is None:
        bscripts = {}
    for loc, content in bento_infos.items():
        n = top_node.make_node(loc)
        n.parent.mkdir()
        n.write(content)
    for loc, content in bscripts.items():
        n = top_node.make_node(loc)
        n.parent.mkdir()
        n.write(content)

    d = raw_parse(bento_infos["bento.info"])
    _kw, files = raw_to_pkg_kw(d, {}, None)
    subpackages = _kw.get("subpackages", {})

    py_modules = _kw.get("py_modules", [])
    if "extensions" in _kw:
        extensions = list(_kw["extensions"].values())
    else:
        extensions = []
    if "compiled_libraries" in _kw:
        compiled_libraries = list(_kw["compiled_libraries"].values())
    else:
        compiled_libraries = []
    if "extra_source_files" in _kw:
        extra_source_files = list(_kw["extra_source_files"])
    else:
        extra_source_files = []

    packages = _kw.get("packages", [])
    for name, spkg in subpackages.items():
        n = top_node.search(name)
        n.write(bento_infos[name])
        d = n.parent
        for py_module in spkg.py_modules:
            m = d.make_node(py_module)
            py_modules.append(m.path_from(top_node))

        extensions.extend(flatten_extensions(top_node, spkg))
        compiled_libraries.extend(flatten_compiled_libraries(top_node, spkg))
        packages.extend(flatten_packages(top_node, spkg))

    return create_fake_package(top_node, packages, py_modules, extensions, compiled_libraries,
                               extra_source_files)

def create_fake_package(top_node, packages=None, modules=None, extensions=None, compiled_libraries=None,
                        extra_source_files=None, sub_directory=None, data_files=None):
    if sub_directory is not None:
        top_or_lib_node = top_node.make_node(sub_directory)
        top_or_lib_node.mkdir()
    else:
        top_or_lib_node = top_node

    if packages is None:
        packages = []
    if modules is None:
        modules = []
    if extensions is None:
        extensions = []
    if compiled_libraries is None:
        compiled_libraries = []
    if extra_source_files is None:
        extra_source_files = []
    if data_files is None:
        data_files= {}

    for p in packages:
        d = p.replace(".", os.sep)
        n = top_or_lib_node.make_node(d)
        n.mkdir()
        init = n.make_node("__init__.py")
        init.write("")
    for m in modules:
        d = m.replace(".", os.sep)
        n = top_or_lib_node.make_node("%s.py" % d)
        n.parent.mkdir()
        n.write("")
    for extension in extensions:
        main = extension.sources[0]
        n = top_node.make_node(main)
        n.parent.mkdir()
        n.write(DUMMY_C % {"name": extension.name.split(".")[-1]})
        for s in extension.sources[1:]:
            n = top_or_lib_node.make_node(s)
            n.write("")
    for library in compiled_libraries:
        main = library.sources[0]
        n = top_node.make_node(main)
        n.parent.mkdir()
        n.write(DUMMY_CLIB % {"name": library.name.split(".")[-1]})
        for s in library.sources[1:]:
            n = top_or_lib_node.make_node(s)
            n.write("")
    for f in extra_source_files:
        n = top_or_lib_node.find_node(f)
        # FIXME: we don't distinguish between extra_source_files as specified
        # in the bento files and the final extra source file list which contain
        # extra files (including the bento files themselves). We need to create
        # fake files in the former case, but not in the latter.
        if n is None:
            n = top_or_lib_node.make_node(f)
            n.write("")

    for section in data_files.values():
        source_dir_node = top_or_lib_node.make_node(section.source_dir)
        source_dir_node.mkdir()
        for f in section.files:
            n = source_dir_node.make_node(f)
            n.write("")

# FIXME: Those flatten extensions are almost redundant with the ones in
# bento.core.subpackages. Here, we do not ensure that the nodes actually exist
# on the fs (make_node vs find_node). But maybe we do not need to check file
# existence in bento.core.subpackages either (do it at another layer)
def flatten_extensions(top_node, subpackage):
    ret = []

    d = top_node.find_dir(subpackage.rdir)
    root_name = ".".join(subpackage.rdir.split("/"))
    for extension in subpackage.extensions.values():
        sources = [d.make_node(s).path_from(top_node) for s in extension.sources]
        full_name = root_name + ".%s" % extension.name
        ret.append(Extension(full_name, sources))
    return ret

def flatten_compiled_libraries(top_node, subpackage):
    ret = []

    d = top_node.find_dir(subpackage.rdir)
    root_name = ".".join(subpackage.rdir.split("/"))
    for library in subpackage.compiled_libraries.values():
        sources = [d.make_node(s).path_from(top_node) for s in library.sources]
        full_name = root_name + ".%s" % library.name
        ret.append(CompiledLibrary(full_name, sources))
    return ret

def flatten_packages(top_node, subpackage):
    ret = {}

    d = top_node.find_dir(subpackage.rdir)
    parent_pkg = ".".join(subpackage.rdir.split("/"))
    return ["%s.%s" % (parent_pkg, p) for p in subpackage.packages]


########NEW FILE########
__FILENAME__ = test_node
import os
import sys
import pickle
import tempfile
import shutil
import copy

import os.path as op

from bento.compat.api.moves \
    import \
        unittest
from bento.core.node \
    import \
        Node, create_root_with_source_tree, find_root, split_path_win32, split_path_cygwin

class TestNode(unittest.TestCase):
    def setUp(self):
        self.root = Node("", None)

        if sys.version_info[0] < 3:
            self._string_classes = [str, unicode]
        else:
            self._string_classes = [str]

    def test_scratch_creation(self):
        root = Node("", None)
        self.assertEqual(root, root)

    def test_find_node(self):
        r_n = os.path.abspath(os.getcwd())

        for f in self._string_classes:
            n = self.root.find_node(f(os.getcwd()))
            self.assertTrue(n)
            assert n.abspath() == r_n

    def test_make_node(self):
        r_n = os.path.abspath(os.getcwd())

        for f in self._string_classes:
            n = self.root.make_node(f(os.getcwd()))
            self.assertTrue(n)
            assert n.abspath() == r_n, "%s vs %s" % (n.abspath(), r_n)

    def test_serialization(self):
        """Test pickled/unpickle round trip."""
        n = self.root.find_node(os.getcwd())
        r_n = pickle.loads(pickle.dumps(n))
        self.assertEqual(n.name, r_n.name)
        self.assertEqual(n.abspath(), r_n.abspath())
        self.assertEqual(n.parent.abspath(), r_n.parent.abspath())
        self.assertEqual([child.abspath() for child in getattr(n, "children", [])],
                         [child.abspath() for child in getattr(n, "children", [])])

    def test_str_repr(self):
        d = tempfile.mkdtemp()
        try:
            r_n = os.path.abspath(os.path.join(d, "foo.txt"))
            node = self.root.make_node(r_n)
            self.assertEqual(str(node), "foo.txt")
            self.assertEqual(repr(node), r_n)
        finally:
            shutil.rmtree(d)

    def test_invalid_copy(self):
        self.assertRaises(NotImplementedError, lambda: copy.copy(self.root))

class TestNodeInsideTemp(unittest.TestCase):
    def setUp(self):
        root = Node("", None)
        self.d_node = root.find_node(tempfile.mkdtemp())
        assert self.d_node is not None

    def tearDown(self):
        shutil.rmtree(self.d_node.abspath())

    def test_delete(self):
        n = self.d_node.make_node("foo.txt")
        n.write("foo")
        self.assertEqual(n.read(), "foo")
        n.delete()
        self.assertEqual(self.d_node.listdir(), [])

    def test_delete_dir(self):
        foo_node = self.d_node.make_node("foo")
        n = foo_node.make_node("bar.txt")
        n.parent.mkdir()
        n.write("foo")
        self.assertEqual(n.read(), "foo")
        foo_node.delete()
        self.assertEqual(self.d_node.listdir(), [])

    def test_mkdir(self):
        bar_node = self.d_node.make_node(op.join("foo", "bar"))
        os.makedirs(bar_node.parent.abspath())
        os.makedirs(bar_node.abspath())
        bar_node.mkdir()

    def test_suffix(self):
        foo = self.d_node.make_node("foo.txt")
        self.assertEqual(foo.suffix(), ".txt")

        foo = self.d_node.make_node("foo.txt.swp")
        self.assertEqual(foo.suffix(), ".swp")

    def test_make_node(self):
        foo = self.d_node.make_node("foo")
        foo.make_node("../bar")
        self.assertTrue(self.d_node.find_node("bar") is not None)

    def test_ant(self):
        for filename in ["bar.txt", "foo.bar", "fubar.txt"]:
            n = self.d_node.make_node(filename)
            n.write("")
        nodes = self.d_node.ant_glob("*.txt")
        bar = self.d_node.find_node("bar.txt")
        fubar = self.d_node.find_node("fubar.txt")
        self.assertEqual(set([fubar.abspath(), bar.abspath()]), set([node.abspath() for node in nodes]))

    def test_ant_excl(self):
        for filename in ["bar.txt", "foo.bar", "fubar.txt"]:
            n = self.d_node.make_node(filename)
            n.write("")
        nodes = self.d_node.ant_glob("*", excl=["*.txt"])
        foobar = self.d_node.find_node("foo.bar")
        self.assertEqual(set(node.abspath() for node in nodes), set([foobar.abspath()]))

class TestNodeWithBuild(unittest.TestCase):
    def setUp(self):
        top = os.getcwd()
        build = os.path.join(os.getcwd(), "_tmp_build")

        self.root = create_root_with_source_tree(top, build)

    def test_root(self):
        self.assertEqual(self.root, find_root(self.root))

    def test_cwd_node(self):
        cur_node = self.root.make_node(os.getcwd())
        self.assertEqual(os.getcwd(), cur_node.abspath())

class TestUtils(unittest.TestCase):
    def test_split_path_win32(self):
        self.assertEqual(split_path_win32(r"C:\foo\bar"), ["C:", "foo", "bar"])
        self.assertEqual(split_path_win32(r"\\"), ["\\"])
        self.assertEqual(split_path_win32(""), [''])

    def test_split_path_cygwin(self):
        self.assertEqual(split_path_cygwin("/foo/bar"), ["", "foo", "bar"])
        self.assertEqual(split_path_cygwin("//"), ["/"])
        self.assertEqual(split_path_cygwin(""), [''])

########NEW FILE########
__FILENAME__ = test_node_representation
import os
import tempfile
import shutil

import os.path as op

from bento.compat.api.moves \
    import \
        unittest

from bento.core.package \
    import \
        PackageDescription
from bento.core.node_package \
    import \
        NodeRepresentation
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.core.testing \
    import \
        create_fake_package_from_bento_info

class TestNodeRepresentation(unittest.TestCase):
    def setUp(self):
        self.d = tempfile.mkdtemp()
        self.root = create_root_with_source_tree(self.d, os.path.join(self.d, "build"))
        self.top_node = self.root.find_node(self.d)

        self.old_dir = os.getcwd()
        os.chdir(self.d)

    def tearDown(self):
        os.chdir(self.old_dir)
        shutil.rmtree(self.d)

    def test_simple(self):
        bento_info = """\
Name: foo

Library:
    Extension: _foo
        Sources: src/foo.c, src/bar.c
"""
        create_fake_package_from_bento_info(self.top_node, bento_info)

        bento_info = """\
Name: foo

Library:
    Extension: _foo
        Sources: src/foo.c, src/bar.c
"""
        pkg = PackageDescription.from_string(bento_info)
        node_pkg = NodeRepresentation(self.top_node, self.top_node)
        node_pkg.update_package(pkg)

        extensions = dict(node_pkg.iter_category("extensions"))
        self.assertEqual(len(extensions), 1)
        self.assertEqual(len(extensions["_foo"].nodes), 2)

    def test_sources_glob(self):
        bento_info = """\
Name: foo

Library:
    Extension: _foo
        Sources: src/foo.c, src/bar.c
"""
        create_fake_package_from_bento_info(self.top_node, bento_info)

        bento_info = """\
Name: foo

Library:
    Extension: _foo
        Sources: src/*.c
"""
        pkg = PackageDescription.from_string(bento_info)
        node_pkg = NodeRepresentation(self.top_node, self.top_node)
        node_pkg.update_package(pkg)

        extensions = dict(node_pkg.iter_category("extensions"))
        self.assertEqual(len(extensions), 1)
        self.assertEqual(len(extensions["_foo"].nodes), 2)

    def test_iter_source_nodes(self):
        r_files = set([op.join("src", "foo.c"),
            op.join("src", "bar.c"),
            op.join("src", "fubar.c"),
            op.join("foo", "__init__.py"),
            op.join("foo", "bar", "__init__.py"),
            "fu.py", "foo.1"])

        bento_info = """\
Name: foo

DataFiles: foo
    TargetDir: $sharedir
    Files: foo.1

Library:
    Extension: _foo
        Sources: src/foo.c, src/bar.c
    CompiledLibrary: fubar
        Sources: src/fubar.c
    Packages: foo, foo.bar
    Modules: fu
"""
        create_fake_package_from_bento_info(self.top_node, bento_info)

        package = PackageDescription.from_string(bento_info)
        node_package = NodeRepresentation(self.top_node, self.top_node)
        node_package.update_package(package)

        files = set(n.path_from(self.top_node) for n in node_package.iter_source_nodes())
        self.assertEqual(files, r_files)

########NEW FILE########
__FILENAME__ = test_package
import os
import tempfile

from bento.compat.api.moves \
    import \
        unittest
from bento.core.package \
    import \
        PackageDescription
from bento.core.package import static_representation
from bento.core.meta import PackageMetadata
from bento.core.pkg_objects import DataFiles
from bento.core.node import Node

def create_file(file, makedirs=True):
    if makedirs:
        dirname = os.path.dirname(file)
        if not os.path.exists(dirname):
            os.makedirs(dirname)
    fid = open(file, "w").close()

def clean_tree(files):
    dirs = []
    for f in files:
        dirs.append(os.path.dirname(f))
        if os.path.exists(f):
            os.remove(f)

    for d in sorted(set(dirs))[::-1]:
        os.rmdir(d)

class TestStaticRepresentation(unittest.TestCase):
    def test_metadata(self):
        bento_info = """\
Name: Sphinx
Version: 0.6.3
Summary: Python documentation generator
Url: http://sphinx.pocoo.org/
DownloadUrl: http://pypi.python.org/pypi/Sphinx
Description: Some long description.
Author: Georg Brandl
AuthorEmail: georg@python.org
Maintainer: Georg Brandl
MaintainerEmail: georg@python.org
License: BSD
Platforms: any
Classifiers:
    Development Status :: 4 - Beta,
    Environment :: Console,
    Environment :: Web Environment,
    Intended Audience :: Developers,
    License :: OSI Approved :: BSD License,
    Operating System :: OS Independent,
    Programming Language :: Python,
    Topic :: Documentation,
    Topic :: Utilities
"""
        self._static_representation(bento_info)

    def test_simple_library(self):
        bento_info = """\
Name: foo

Library:
    Packages: foo
"""
        self._static_representation(bento_info)

    def _static_representation(self, bento_info):
        r_pkg = PackageDescription.from_string(bento_info)
        # We recompute pkg to avoid dealing with stylistic difference between
        # original and static_representation
        pkg = PackageDescription.from_string(static_representation(r_pkg))

        self.assertEqual(static_representation(pkg), static_representation(r_pkg))

class TestPackageMetadata(unittest.TestCase):
    def test_ctor(self):
        meta = PackageMetadata(name="foo", version="1.0", author="John Doe",
                               author_email="john@doe.com")
        self.assertEqual(meta.fullname, "foo-1.0")
        self.assertEqual(meta.contact, "John Doe")
        self.assertEqual(meta.contact_email, "john@doe.com")

########NEW FILE########
__FILENAME__ = test_pkg_objects
import os
import tempfile
import sys

from bento.compat.api.moves \
    import \
        unittest
from bento.core.pkg_objects \
    import \
        DataFiles, Executable

class TestDataFiles(unittest.TestCase):
    def test_simple(self):
        data = DataFiles("data", files=["foo.c"])
        self.assertEqual(data.name, "data")
        self.assertEqual(data.files, ["foo.c"])
        self.assertEqual(data.source_dir, ".")
        self.assertEqual(data.target_dir, "$sitedir")

    def test_from_dict(self):
        parsed_dict = {"name": "data",
                       "files": ["foo.c", "yo.c"], "target_dir": "foo"}
        data = DataFiles.from_parse_dict(parsed_dict)
        self.assertEqual(data.name, "data")
        self.assertEqual(data.files, ["foo.c", "yo.c"])
        self.assertEqual(data.target_dir, "foo")
    # TODO: test with a populated temp dir

class TestExecutable(unittest.TestCase):
    def test_basic(self):
        exe = Executable.from_representation("foo = core:main")
        self.assertEqual(exe.name, "foo")
        self.assertEqual(exe.module, "core")
        self.assertEqual(exe.function, "main")

        self.assertEqual(exe.full_representation(), "foo = core:main")

########NEW FILE########
__FILENAME__ = test_subpackage
import shutil
import tempfile

from bento.compat.api.moves \
    import \
        unittest
from bento.core.node \
    import \
        create_first_node, find_root
import bento.core.pkg_objects as pkg_objects
import bento.core.package as package
import bento.core.subpackage as subpackage

def create_fake_tree(top_node, tree):
    for f in tree:
        n = top_node.make_node(f)
        n.parent.mkdir()
        n.write("")

class TestTranslation(unittest.TestCase):
    """Those tests check 'translation' from local definition (as
    defined in the subento file) to the top directory."""
    def test_extension(self):
        tree = [
            "foo/src/hellomodule.c",
            "foo/bento.info",
            "bento.info"]
        extension = pkg_objects.Extension("_hello",
                        sources=["src/hellomodule.c"],
                        include_dirs=["."])
        spkg = package.SubPackageDescription(
                        rdir="foo",
                        extensions={"_hello": extension})

        d = tempfile.mkdtemp()
        try:
            top = create_first_node(d)
            create_fake_tree(top, tree)
            extensions = subpackage.flatten_subpackage_extensions(
                    spkg, top)
            self.failUnless("foo._hello" in extensions)
            extension = extensions["foo._hello"]
            self.assertEqual(extension.sources, ["foo/src/hellomodule.c"])
            self.assertEqual(extension.include_dirs, ["foo"])
        finally:
            shutil.rmtree(d)

    def _test_compiled_library(self, tree, clib, spkg, sources, include_dirs):
        d = tempfile.mkdtemp()
        try:
            top = create_first_node(d)
            create_fake_tree(top, tree)
            clibs = subpackage.flatten_subpackage_compiled_libraries(
                    spkg, top)
            self.failUnless("bar.clib" in clibs)
            clib = clibs["bar.clib"]
            self.assertEqual(clib.sources, sources)
            self.assertEqual(clib.include_dirs, include_dirs)
        finally:
            shutil.rmtree(d)


    def test_compiled_library(self):
        tree = [
            "bar/src/clib.c",
            "bar/bento.info",
            "bento.info"]
        clib = pkg_objects.CompiledLibrary("clib",
                        sources=["src/clib.c"],
                        include_dirs=["."])
        spkg = package.SubPackageDescription(
                        rdir="bar",
                        compiled_libraries={"clib": clib})

        self._test_compiled_library(tree, clib, spkg, ["bar/src/clib.c"], ["bar"])

    def test_compiled_library_glob(self):
        tree = [
            "bar/src/clib.c",
            "bar/src/clib2.c",
            "bar/src/clib3.f",
            "bar/bento.info",
            "bento.info"]
        clib = pkg_objects.CompiledLibrary("clib",
                        sources=["src/*.c", "src/*.f"],
                        include_dirs=["."])
        spkg = package.SubPackageDescription(
                        rdir="bar",
                        compiled_libraries={"clib": clib})

        self._test_compiled_library(tree, clib, spkg, ["bar/src/clib.c", "bar/src/clib2.c", "bar/src/clib3.f"],
                                    ["bar"])

########NEW FILE########
__FILENAME__ = _nose_compat
import warnings

try:
    from unittest.case import _ExpectedFailure as ExpectedFailure, _UnexpectedSuccess as UnexpectedSuccess
except ImportError:
    from unittest2.case import _ExpectedFailure as ExpectedFailure, _UnexpectedSuccess as UnexpectedSuccess

def install_proxy():
    import nose.proxy

    class MyResultProxy(nose.proxy.ResultProxy):
        def addExpectedFailure(self, test, err):
            #from nose.plugins.expected import ExpectedFailure
            self.assertMyTest(test)
            plugins = self.plugins
            plugins.addError(self.test, (ExpectedFailure, err, None))
            addExpectedFailure = getattr(self.result, "addExpectedFailure", None)
            if addExpectedFailure:
                self.result.addExpectedFailure(self.test, self._prepareErr(err))
            else:
                warnings.warn("TestResult has no addExpectedFailure method, reporting as passes",
                              RuntimeWarning)
                self.result.addSuccess(self)

        def addUnexpectedSuccess(self, test):
            #from nose.plugins.expected import UnexpectedSuccess
            self.assertMyTest(test)
            plugins = self.plugins
            plugins.addError(self.test, (UnexpectedSuccess, None, None))
            self.result.addUnexpectedSuccess(self.test)
            if self.config.stopOnError:
                self.shouldStop = True
    nose.proxy.ResultProxy = MyResultProxy

def install_result():
    import nose.result

    class MyTextTestResult(nose.result.TextTestResult):
        def addExpectedFailure(self, test, err):
            # 2.7 expected failure compat
            if ExpectedFailure in self.errorClasses:
                storage, label, isfail = self.errorClasses[ExpectedFailure]
                storage.append((test, self._exc_info_to_string(err, test)))
                self.printLabel(label, (ExpectedFailure, '', None))

        def addUnexpectedSuccess(self, test):
            # 2.7 unexpected success compat
            if UnexpectedSuccess in self.errorClasses:
                storage, label, isfail = self.errorClasses[UnexpectedSuccess]
                storage.append((test, 'This test was marked as an expected '
                    'failure, but it succeeded.'))
                self.printLabel(label, (UnexpectedSuccess, '', None))
    nose.result.TextTestResult = MyTextTestResult

########NEW FILE########
__FILENAME__ = bdist_egg
import os

from bento.distutils.utils \
    import \
        _is_setuptools_activated
if _is_setuptools_activated():
    from setuptools.command.bdist_egg \
        import \
            bdist_egg as old_bdist_egg
else:
    raise ValueError("You cannot use bdist_egg without setuptools enabled first")

class bdist_egg(old_bdist_egg):
    cmd_name = "build_egg"
    def run(self):
        self.run_command("build")

        cmd_argv = ["--output-dir=%s" % self.dist_dir]
        self.distribution.run_command_in_context(self.cmd_name, cmd_argv)

########NEW FILE########
__FILENAME__ = build
from distutils.command.build \
    import \
        build as old_build

class build(old_build):
    cmd_name = "build"
    def run(self):
        self.run_command("config")
        self.distribution.run_command_in_context(self.cmd_name, [])

########NEW FILE########
__FILENAME__ = config
import os
import sys

import os.path as op

from distutils.command.config \
    import \
        config as old_config

from bento.core.platforms \
    import \
        get_scheme

class config(old_config):
    cmd_name = "configure"
    def __init__(self, *a, **kw):
        old_config.__init__(self, *a, **kw)

    def initialize_options(self):
        old_config.initialize_options(self)

    def finalize_options(self):
        old_config.finalize_options(self)

    def _get_install_scheme(self):
        pkg = self.distribution.pkg

        install = self.get_finalized_command("install")
        return install.scheme

    def run(self):
        dist = self.distribution

        scheme = self._get_install_scheme()
        argv = []
        for k, v in scheme.items():
            if k == "exec_prefix":
                k = "exec-prefix"
            argv.append("--%s=%s" % (k, v))

        dist.run_command_in_context(self.cmd_name, argv)
        dist.global_context.save_command_argv("configure", argv)

########NEW FILE########
__FILENAME__ = egg_info
import os
import os.path as op

from bento.distutils.utils \
    import \
        _is_setuptools_activated
if _is_setuptools_activated():
    from setuptools.command.egg_info \
        import \
            egg_info as old_egg_info
else:
    raise ValueError("You cannot use egg_info without setuptools enabled first")

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.installed_package_description \
    import \
        BuildManifest
from bento.commands.egg_utils \
    import \
        EggInfo

class egg_info(old_egg_info):
    def run(self):
        self.run_command("build")
        dist = self.distribution

        n = dist.build_node.make_node(BUILD_MANIFEST_PATH)
        build_manifest = BuildManifest.from_file(n.abspath())

        egg_info = EggInfo.from_build_manifest(build_manifest, dist.build_node)

        egg_info_dir = op.join(self.egg_base, "%s.egg-info" % dist.pkg.name)
        try:
            os.makedirs(egg_info_dir)
        except OSError:
            e = extract_exception()
            if e.errno != 17:
                raise
        for filename, cnt in egg_info.iter_meta(dist.build_node):
            filename = op.join(egg_info_dir, filename)
            fid = open(filename, "w")
            try:
                fid.write(cnt)
            finally:
                fid.close()

########NEW FILE########
__FILENAME__ = install
import sys
import os
import os.path as op
import warnings

from distutils.cmd \
    import \
        Command
from distutils.command.install \
    import \
        INSTALL_SCHEMES

from bento._config \
    import \
        BUILD_MANIFEST_PATH
from bento.installed_package_description \
    import \
        BuildManifest, iter_files
from bento.utils.utils import subst_vars
import bento.utils.io2

class install(Command):
    cmd_name = "install"
    description = "Install wrapper to install bento package"

    user_options = [
        ('prefix=', None, "installation prefix"),
        ('exec-prefix=', None, "prefix for platform-specific files"),

        ('record=', None, "Record file containing every path created by install command"),
        ('root=', None, "(Unix-only) Alternative root (equivalent to destdir option in bento)"),

        ('dry-run', 'n', "(Unix-only) Alternative root (equivalent to destdir option in bento)"),
        ('single-version-externally-managed', None, "Do nothing. For compatibility with pip only."),
        ('install-headers=', None, "Do nothing. For compatibility with pip only."),
        ('force', None, "Do nothing. For compatibility with distutils."),
    ]

    def initialize_options(self):
        self.prefix = None
        self.exec_prefix = None
        self.record = None
        self.root = None
        self.dry_run = None
        self.install_headers = None
        self.single_version_externally_managed = None

        self.scheme = {}

    def finalize_options(self):
        if self.install_headers is not None:
            warnings.warn("--install-headers option is ignored.")

        if self.prefix is None:
            prefix_customized = False
        else:
            prefix_customized = True

        if self.prefix is None:
            if self.exec_prefix is not None:
                raise DistutilsOptionError("must not supply exec-prefix without prefix")

            self.prefix = os.path.normpath(sys.prefix)
            self.exec_prefix = os.path.normpath(sys.exec_prefix)
        else:
            if self.exec_prefix is None:
                self.exec_prefix = self.prefix

        if os.name == "posix":
            self._finalize_unix(prefix_customized)
        else:
            self._finalize_other(prefix_customized)

    def _finalize_other(self, is_prefix_customized):
        scheme = self.scheme

        if self.root:
            raise ValueError("Option root is meaningless on non-posix platforms !")

        if is_prefix_customized:
            prefix = self.prefix
            exec_prefix = self.exec_prefix
        else:
            prefix = sys.prefix
            exec_prefix = sys.exec_prefix
        scheme['prefix'] = prefix
        scheme['exec_prefix'] = exec_prefix

    def _finalize_unix(self, is_prefix_customized):
        scheme = self.scheme

        if self.root:
            scheme["destdir"] = self.root

        # TODO: user and home schemes

        py_version_short = ".".join(map(str, sys.version_info[:2]))
        dist_name = self.distribution.pkg.name

        if is_prefix_customized:
            if op.normpath(self.prefix) != '/usr/local':
                # unix prefix
                scheme['prefix'] = self.prefix
                scheme['exec_prefix'] = self.exec_prefix
            else:
                scheme['prefix'] = self.prefix
                scheme['exec_prefix'] = self.exec_prefix
                # use deb_system on debian-like systems
                if 'deb_system' in INSTALL_SCHEMES:
                    v = {'py_version_short': py_version_short, 'dist_name': dist_name, 'base': self.prefix}
                    scheme['includedir'] = subst_vars(INSTALL_SCHEMES['deb_system']['headers'], v)
                    scheme['sitedir'] = subst_vars(INSTALL_SCHEMES['deb_system']['purelib'], v)
        else:
            # If no prefix is specified, we'd like to avoid installing anything
            # in /usr by default (i.e. like autotools, everything in
            # /usr/local).  If prefix is not /usr, then we can't really guess
            # the best default location.
            if hasattr(sys, 'real_prefix'): # run under virtualenv
                prefix = sys.prefix
                exec_prefix = sys.exec_prefix
            elif sys.prefix == '/usr':
                prefix = exec_prefix = '/usr/local'
            else:
                prefix = sys.prefix
                exec_prefix = sys.exec_prefix
            scheme['prefix'] = prefix
            scheme['exec_prefix'] = exec_prefix
            # use unix_local on debian-like systems
            if 'unix_local' in INSTALL_SCHEMES:
                v = {'py_version_short': py_version_short, 'dist_name': dist_name, 'base': prefix}
                scheme['includedir'] = subst_vars(INSTALL_SCHEMES['unix_local']['headers'], v)
                scheme['sitedir'] = subst_vars(INSTALL_SCHEMES['unix_local']['purelib'], v)

    def run(self):
        self.run_command("build")
        args = []

        if self.dry_run == 1:
            args.append("--dry-run")
        self.distribution.run_command_in_context(self.cmd_name, args)
        if self.record:
            self.write_record()

    def write_record(self):
        dist = self.distribution

        options_context = dist.global_context.retrieve_options_context(self.cmd_name)
        cmd_context_klass = dist.global_context.retrieve_command_context(self.cmd_name)
        context = cmd_context_klass(dist.global_context, [], options_context, dist.pkg, dist.run_node)

        n = context.build_node.make_node(BUILD_MANIFEST_PATH)
        build_manifest = BuildManifest.from_file(n.abspath())
        scheme = context.retrieve_configured_scheme()
        build_manifest.update_paths(scheme)
        file_sections = build_manifest.resolve_paths_with_destdir(src_root_node=context.build_node)

        def writer(fid):
            for kind, source, target in iter_files(file_sections):
                fid.write("%s\n" % target.abspath())
        bento.utils.io2.safe_write(self.record, writer, "w")

########NEW FILE########
__FILENAME__ = sdist
import sys

from distutils.command.sdist \
    import \
        sdist as old_sdist

class sdist(old_sdist):
    def run(self):
        cmd_name = "sdist"
        cmd_argv = ["--output-dir=%s" % self.dist_dir]
        self.distribution.run_command_in_context(cmd_name, cmd_argv)

########NEW FILE########
__FILENAME__ = dist
import os

from bento.distutils.utils \
    import \
        _is_setuptools_activated
if _is_setuptools_activated():
    from setuptools \
        import \
            Distribution
else:
    from distutils.dist \
        import \
            Distribution

from bento.commands.configure \
    import \
        _setup_options_parser
from bento.commands.core \
    import \
        HelpCommand
from bento.commands.hooks \
    import \
        find_pre_hooks, find_post_hooks, find_startup_hooks, \
        find_shutdown_hooks, find_options_hooks, find_command_hooks
from bento.commands.wrapper_utils \
    import \
        set_main
from bento.compat.api \
    import \
        defaultdict
from bento.conv \
    import \
        pkg_to_distutils_meta
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.core.package \
    import \
        PackageDescription
from bento.core.options \
    import \
        PackageOptions
from bento.commands.contexts \
    import \
        GlobalContext
from bento.commands.command_contexts \
    import \
        CmdContext, SdistContext, ContextWithBuildDirectory
from bento.commands.registries \
    import \
        CommandRegistry, ContextRegistry, OptionsRegistry
from bento.backends.yaku_backend \
    import \
        ConfigureYakuContext, BuildYakuContext
from bento.commands.build_egg \
    import \
        BuildEggCommand
import bento.commands.wrapper_utils

from bento.commands.dependency \
    import \
        CommandScheduler
from bento.commands.build \
    import \
        BuildCommand
from bento.commands.configure \
    import \
        ConfigureCommand
from bento.commands.install \
    import \
        InstallCommand
from bento.commands.sdist \
    import \
        SdistCommand
from bento.commands.options \
    import \
        OptionsContext
from bento.distutils.commands.config \
    import \
        config
from bento.distutils.commands.build \
    import \
        build
from bento.distutils.commands.install \
    import \
        install
from bento.distutils.commands.sdist \
    import \
        sdist

_BENTO_MONKEYED_CLASSES = {"build": build, "config": config, "install": install, "sdist": sdist}

if _is_setuptools_activated():
    from bento.distutils.commands.bdist_egg \
        import \
            bdist_egg
    from bento.distutils.commands.egg_info \
        import \
            egg_info
    _BENTO_MONKEYED_CLASSES["bdist_egg"] = bdist_egg
    _BENTO_MONKEYED_CLASSES["egg_info"] = egg_info

def _setup_cmd_classes(attrs):
    cmdclass = attrs.get("cmdclass", {})
    for klass in _BENTO_MONKEYED_CLASSES:
        if not klass in cmdclass:
            cmdclass[klass] = _BENTO_MONKEYED_CLASSES[klass]
    attrs["cmdclass"] = cmdclass
    return attrs

def global_context_factory(package_options):
    # FIXME: factor this out with the similar code in bentomakerlib
    global_context = GlobalContext(None)
    global_context.register_package_options(package_options)

    register_commands(global_context)
    register_command_contexts(global_context)
    for cmd_name in global_context.command_names():
        cmd = global_context.retrieve_command(cmd_name)
        options_context = OptionsContext.from_command(cmd)

        if not global_context.is_options_context_registered(cmd_name):
            global_context.register_options_context(cmd_name, options_context)

    return global_context

def register_command_contexts(global_context):
    default_mapping = defaultdict(lambda: ContextWithBuildDirectory)
    default_mapping.update(dict([
            ("configure", ConfigureYakuContext),
            ("build", BuildYakuContext),
            ("install", ContextWithBuildDirectory),
            ("sdist", SdistContext)]))

    for cmd_name in global_context.command_names(public_only=False):
        if not global_context.is_command_context_registered(cmd_name):
            global_context.register_command_context(cmd_name, default_mapping[cmd_name])

def register_commands(global_context):
    global_context.register_command("configure", ConfigureCommand())
    global_context.register_command("build", BuildCommand())
    global_context.register_command("install", InstallCommand())
    global_context.register_command("sdist", SdistCommand())
    global_context.register_command("build_egg", BuildEggCommand())

class BentoDistribution(Distribution):
    def get_command_class(self, command):
        # Better raising an error than having some weird behavior for a command
        # we don't support
        if self.script_args is not None \
           and command in self.script_args \
           and command not in _BENTO_MONKEYED_CLASSES:
            raise ValueError("Command %s is not supported by bento.distutils compat layer" % command)
        return Distribution.get_command_class(self, command)

    def __init__(self, attrs=None):
        if attrs is None:
            attrs = {}

        if not "bento_info" in attrs:
            bento_info = "bento.info"
        else:
            bento_info = attrs["bento.info"]
        self.pkg = PackageDescription.from_file(bento_info)
        package_options = PackageOptions.from_file(bento_info)

        attrs = _setup_cmd_classes(attrs)

        d = pkg_to_distutils_meta(self.pkg)
        attrs.update(d)

        Distribution.__init__(self, attrs)

        self.packages = self.pkg.packages
        self.py_modules = self.pkg.py_modules
        if hasattr(self, "entry_points"):
            if self.entry_points is None:
                self.entry_points = {}
            console_scripts = [e.full_representation() for e in self.pkg.executables.values()]
            if "console_scripts" in self.entry_points:
                self.entry_points["console_scripts"].extend(console_scripts)
            else:
                self.entry_points["console_scripts"] = console_scripts

        source_root = os.getcwd()
        build_root = os.path.join(source_root, "build")
        root = create_root_with_source_tree(source_root, build_root)
        self.top_node = root._ctx.srcnode
        self.build_node = root._ctx.bldnode
        self.run_node = root._ctx.srcnode

        self.global_context = global_context_factory(package_options)
        mods = set_main(self.pkg, self.top_node, self.build_node)
        self._setup_hooks(self.pkg, self.global_context, mods)

    def _setup_hooks(self, package, global_context, mods):
        if package.use_backends:
            if len(package.use_backends) > 1:
                raise ValueError("Only up to one backend supported for now")
            else:
                assert global_context.backend is None
                global_context.backend = load_backend(package.use_backends[0])()

        startup_hooks = find_startup_hooks(mods)
        option_hooks = find_options_hooks(mods)
        shutdown_hooks = find_shutdown_hooks(mods)

        if startup_hooks:
            # FIXME: there should be an error or a warning if startup defined in
            # mods beyond the first one
            startup_hooks[0](global_context)

        if global_context.backend:
            global_context.backend.register_command_contexts(global_context)
        for command in find_command_hooks(mods):
            global_context.register_command(command.name, command)

        if global_context.backend:
            global_context.backend.register_options_contexts(global_context)

        if option_hooks:
            # FIXME: there should be an error or a warning if shutdown defined in
            # mods beyond the first one
            option_hooks[0](global_context)

        # FIXME: this registered options for new commands registered in hook. It
        # should be made all in one place (hook and non-hook)
        for cmd_name in global_context.command_names(public_only=False):
            if not global_context.is_options_context_registered(cmd_name):
                # FIXME: this should be supported in global context directly
                # (redundant with bentomakerlib)
                cmd = global_context.retrieve_command(cmd_name)
                context = OptionsContext.from_command(cmd)

                if not global_context.is_options_context_registered(cmd_name):
                    global_context.register_options_context(cmd_name, context)

        for cmd_name in global_context.command_names():
            for hook in find_pre_hooks(mods, cmd_name):
                global_context.add_pre_hook(hook, cmd_name)
            for hook in find_post_hooks(mods, cmd_name):
                global_context.add_post_hook(hook, cmd_name)

    def run_command_in_context(self, cmd_name, cmd_argv):
        return bento.commands.wrapper_utils.resolve_and_run_command(self.global_context,
            cmd_name, cmd_argv, self.run_node, self.pkg)

    def has_data_files(self):
        return len(self.pkg.data_files) > 0        

########NEW FILE########
__FILENAME__ = monkey_patch
import sys

def monkey_patch():
    # XXX: keep the import here to avoid any side-effects from mere import of
    # bento.distutils
    from bento.distutils.utils \
        import \
            _is_setuptools_activated
    import bento.distutils.dist

    # Install it throughout the distutils
    _MODULES = []
    if _is_setuptools_activated():
        import setuptools.dist
        _MODULES.append(setuptools.dist)
    import distutils.dist, distutils.core, distutils.cmd
    _MODULES.extend([distutils.dist, distutils.core, distutils.cmd])
    for module in _MODULES:
        module.Distribution = bento.distutils.dist.BentoDistribution

def setup(mode=None):
    """Call setup after monkey-patching with the given mode.

    Parameters
    ----------
    mode: None/str
        'distutils' or 'setuptools' for now
    """
    if mode is None:
        if "setuptools" in sys.modules:
            mode = "setuptools"
        else:
            mode = "distutils"
    if not mode in ("distutils", "setuptools"):
        raise ValueError("Only 'setuptools' and 'distutils' are supported modes")
    __import__(mode)
    monkey_patch()
    from distutils.core import setup as _setup
    return _setup()

########NEW FILE########
__FILENAME__ = common
import os
import os.path as op
import tempfile
import shutil

from bento.compat.api.moves \
    import \
        unittest
from bento.distutils.dist \
    import \
        BentoDistribution
from bento.distutils.commands.install \
    import \
        install
        
class DistutilsCommandTestBase(unittest.TestCase):
    def setUp(self):
        self.old_cwd = os.getcwd()
        self.new_cwd = tempfile.mkdtemp()
        try:
            os.chdir(self.new_cwd)
        except:
            os.chdir(self.old_cwd)
            shutil.rmtree(self.new_cwd)
            raise

    def tearDown(self):
        os.chdir(self.old_cwd)
        shutil.rmtree(self.new_cwd)

########NEW FILE########
__FILENAME__ = test_build
import os.path as op

from bento.distutils.tests.common \
    import \
        DistutilsCommandTestBase
from bento.distutils.dist \
    import \
        BentoDistribution
from bento.distutils.commands.build \
    import \
        build
        
class TestBuildCommand(DistutilsCommandTestBase):
    def test_hook(self):
        "Test hook is executed at all in distutils compat mode."
        fid = open(op.join(self.new_cwd, "bento.info"), "wt")
        try:
            fid.write("""\
Name: foo
HookFile: bscript
""")
        finally:
            fid.close()

        fid = open(op.join(self.new_cwd, "bscript"), "wt")
        try:
            fid.write("""\
from bento.commands import hooks

@hooks.pre_build
def pre_build(context):
    raise ValueError()
""")
        finally:
            fid.close()

        dist = BentoDistribution()
        cmd = build(dist)
        cmd.ensure_finalized()
        self.assertRaises(ValueError, cmd.run)

########NEW FILE########
__FILENAME__ = test_egg_info
import os.path as op

from bento.distutils.tests.common \
    import \
        DistutilsCommandTestBase

import bento.core.testing

class TestEggInfoCommand(DistutilsCommandTestBase):
    @bento.core.testing.skip_if(True)
    def test_simple(self):
        from bento.distutils.dist \
            import \
                BentoDistribution
        from bento.distutils.commands.egg_info \
            import \
                egg_info

        fid = open(op.join(self.new_cwd, "bento.info"), "wt")
        try:
            fid.write("""\
Name: foo
""")
        finally:
            fid.close()

        dist = BentoDistribution()
        cmd = egg_info(dist)

        cmd.ensure_finalized()

        cmd.run()

########NEW FILE########
__FILENAME__ = test_install
import os.path as op

from bento.distutils.tests.common \
    import \
        DistutilsCommandTestBase
from bento.distutils.dist \
    import \
        BentoDistribution
from bento.distutils.commands.install \
    import \
        install
        
class TestInstallCommand(DistutilsCommandTestBase):
    def test_simple(self):
        fid = open(op.join(self.new_cwd, "bento.info"), "wt")
        try:
            fid.write("""\
Name: foo
""")
        finally:
            fid.close()

        dist = BentoDistribution()
        cmd = install(dist)

        install_dir = op.join(self.new_cwd, "install")
        cmd.prefix = install_dir
        cmd.dry_run = True
        cmd.record = op.join(self.new_cwd, "record.txt")
        cmd.ensure_finalized()

        cmd.run()

########NEW FILE########
__FILENAME__ = test_sdist
import os.path as op

from bento.distutils.tests.common \
    import \
        DistutilsCommandTestBase
from bento.distutils.dist \
    import \
        BentoDistribution
from bento.distutils.commands.sdist \
    import \
        sdist

class TestSdistCommand(DistutilsCommandTestBase):
    def test_simple(self):
        fid = open(op.join(self.new_cwd, "bento.info"), "wt")
        try:
            fid.write("""\
Name: foo
""")
        finally:
            fid.close()

        dist = BentoDistribution()
        cmd = sdist(dist)

        cmd.ensure_finalized()

        cmd.run()

########NEW FILE########
__FILENAME__ = utils
import sys

def _is_setuptools_activated():
    return "setuptools" in sys.modules

########NEW FILE########
__FILENAME__ = errors
class BentoError(Exception):
    pass

class InternalBentoError(BentoError):
    def __str__(self):
        return "unexpected error: %s (most likely a bento bug)"

class InvalidPackage(BentoError):
    pass

class UsageException(BentoError):
    pass

class ParseError(BentoError):
    def __init__(self, msg="", token=None):
        self.msg = msg
        self.token = token
        if token is not None:
            self.lineno = token.lineno
            self.tp = token.type
            self.value = token.value
        else:
            self.lineno = self.tp = self.value = None
        self.filename = None
        # Don't use super here because Exception are not new-class object on 2.4
        BentoError.__init__(self, msg)

    def __str__(self):
        if self.filename is None or self.token is None:
            return Exception.__str__(self)
        else:
            msg = '  File "%(file)s", line %(lineno)d\n' \
                  % {"file": self.filename, "lineno": self.lineno}
            f = open(self.filename)
            try:
                # This is expensive, there must be a better way
                cnt = f.read()
                lines = cnt.splitlines()
                linepos = sum([(len(l)+1) for l in lines[:self.lineno-1]])
                msg += "%s\n" % lines[self.lineno-1]
                pos = self.token.lexpos
                msg += " " * (pos - linepos) + "^\n"
                msg += "Syntax error"
                return msg
            finally:
                f.close()

    def __repr__(self):
        return self.__str__()

class InvalidHook(BentoError):
    pass

class CommandExecutionFailure(BentoError):
    pass

class ConfigurationError(CommandExecutionFailure):
    pass

class BuildError(CommandExecutionFailure):
    pass

class ConvertionError(BentoError):
    pass

class SetupCannotRun(ConvertionError):
    pass

class UnsupportedFeature(ConvertionError):
    pass

class InvalidPyPIConfig(BentoError):
    pass

class PyPIError(BentoError):
    pass

class InvalidRepository(PyPIError):
    pass

########NEW FILE########
__FILENAME__ = installed_package_description
import os
import sys
import copy
import warnings

from bento.compat.api import json
import bento.compat.api as compat

from bento.core.node \
    import \
        find_root
from bento.core.platforms \
    import \
        get_scheme
from bento.utils.utils import subst_vars, same_content, fix_kw, explode_path
from bento.core.pkg_objects \
    import \
        Executable
import bento.utils.path

def build_manifest_meta_from_pkg(pkg):
    """Return meta dict for Installed pkg from a PackageDescription
    instance."""
    meta = {}
    for m in ["name", "version", "summary", "url", "author",
              "author_email", "license", "download_url", "description",
              "platforms", "classifiers", "install_requires", 
              "top_levels"]:
        meta[m] = getattr(pkg, m)
    return meta

class InstalledSection(object):
    @classmethod
    def from_source_target_directories(cls, category, name, source_dir, target_dir, files):
        files = [(f, f) for f in files]
        return cls(category, name, source_dir, target_dir, files)

    def __init__(self, category, name, srcdir, target, files):
        self.category = category
        self.name = name
        if os.sep != "/":
            self.source_dir = bento.utils.path.normalize_path(srcdir)
            self.target_dir = bento.utils.path.normalize_path(target)
            self.files = [(bento.utils.path.normalize_path(f), bento.utils.path.normalize_path(g)) for f, g in files]
        else:
            self.source_dir = srcdir
            self.target_dir = target
            self.files = files

    def __repr__(self):
        ret = """\
InstalledSection(%(category)s,
    %(name)s,
    %(source_dir)s,
    %(target_dir)s,
    %(files)s)""" % {"name": self.name, "category": self.category,
            "source_dir": self.source_dir, "target_dir": self.target_dir,
            "files": self.files}
        return ret

    def __eq__(self, other):
        return self.category == other.category and \
                self.name == other.name and \
                self.source_dir == other.source_dir and \
                self.target_dir == other.target_dir and \
                self.files == other.files

def iter_source_files(file_sections):
    for kind in file_sections:
        if not kind in ["executables"]:
            for name, section in file_sections[kind].items():
                for f in section:
                    yield f[0]

def iter_files(file_sections):
    # XXX: what to do with multiple source for a same target ? It is not always
    # easy to avoid this situation, especially for python files and files
    # installed from wildcards. For now, we raise an exception unless the
    # sources have the exact same content, but this may not be enough (what if
    # category changes ? This may cause different target permissions and other
    # category-specific post-processing during install)
    installed_files = {}
    def _is_redundant(source, target):
        source_path = source.abspath()
        target_path = target.abspath()
        # If this install already installs something @ target, we raise an
        # error unless the content is exactly the same
        if not target in installed_files:
            installed_files[target] = source
            return False
        else:
            if not same_content(source_path, installed_files[target].abspath()):
                # See top comment: not sure there is any good solution to
                # select which one should be selected when a target has
                # multiple sources
                if source.is_src() and installed_files[target].is_bld():
                    return False
                if source.is_bld() and installed_files[target].is_src():
                    return True
                else:
                    raise IOError("Multiple source_path for same target_path %r ! %s and %s" % \
                                  (target_path, source_path, installed_files[target].abspath()))
            else:
                return True

    for kind in file_sections:
        for name, section in file_sections[kind].items():
            for source, target in section:
                if not _is_redundant(source, target):
                    yield kind, source, target

class BuildManifest(object):
    @classmethod
    def from_egg(cls, egg_path):
        zid = compat.ZipFile(egg_path)
        try:
            data = json.loads(zid.read("EGG-INFO/build_manifest.info"))
            return cls.from_json_dict(data)
        finally:
            zid.close()

    @classmethod
    def from_json_dict(cls, d):
        return cls.__from_data(d)

    @classmethod
    def from_string(cls, s):
        return cls.__from_data(json.loads(s))

    @classmethod
    def from_file(cls, filename):
        fid = open(filename)
        try:
            return cls.__from_data(json.load(fid))
        finally:
            fid.close()

    @classmethod
    def __from_data(cls, data):
        meta_vars = fix_kw(data["meta"])
        #variables = data["variables"]
        install_paths = data.get("install_paths", None)

        executables = {}
        for name, executable in data["executables"].items():
            executables[name] = Executable.from_parse_dict(fix_kw(executable))

        file_sections = {}

        def json_to_file_section(data):
            category = data["category"]
            name = data["name"]
            section = InstalledSection(category, name, data["source_dir"],
                                       data["target_dir"], data["files"])
            return category, name, section

        for section in data["file_sections"]:
            category, name, files = json_to_file_section(section)
            if category in file_sections:
                if name in file_sections[category]:
                    raise ValueError("section %s of type %s already exists !" % (name, category))
                file_sections[category][name] = files
            else:
                file_sections[category] = {name: files}
        return cls(file_sections, meta_vars, executables, install_paths)

    def __init__(self, file_sections, meta, executables, path_variables=None):
        self.file_sections = file_sections
        self.meta = meta
        if path_variables is None:
            self._path_variables = get_scheme(sys.platform)[0]
        else:
            self._path_variables = path_variables
        self.executables = executables

        self._variables = {"pkgname": self.meta["name"],
                           "py_version_short": ".".join([str(i) for i in sys.version_info[:2]])}

    def write(self, filename):
        fid = open(filename, "w")
        try:
            return self._write(fid)
        finally:
            fid.close()

    def _write(self, fid):
        def executable_to_json(executable):
            return {"name": executable.name,
                    "module": executable.module,
                    "function": executable.function}

        def section_to_json(section):
            return {"name": section.name,
                    "category": section.category,
                    "source_dir": section.source_dir,
                    "target_dir": section.target_dir,
                    "files": section.files}

        data = {}
        data["meta"] = self.meta

        executables = dict([(k, executable_to_json(v)) \
                            for k, v in self.executables.items()])
        data["executables"] = executables
        data["install_paths"] = self._path_variables

        file_sections = []
        for category, value in self.file_sections.items():
            if category in ["pythonfiles", "bentofiles"]:
                for i in value.values():
                    i.srcdir = "$_srcrootdir"
                    file_sections.append(section_to_json(i))
            elif category in ["datafiles", "extensions", "executables",
                        "compiled_libraries"]:
                for i in value.values():
                    file_sections.append(section_to_json(i))
            else:
                warnings.warn("Unknown category %r" % category)
                for i in value.values():
                    file_sections.append(section_to_json(i))
        data["file_sections"] = file_sections
        if "BENTOMAKER_PRETTY" in os.environ:
            json.dump(data, fid, sort_keys=True, indent=4)
        else:
            json.dump(data, fid, separators=(',', ':'))

    def update_paths(self, paths):
        for k, v in paths.items():
            self._path_variables[k] = v

    def iter_built_files(self, src_root_node, scheme=None):
        if scheme is None:
            scheme = {}
        self.update_paths(scheme)
        return iter_files(self.resolve_paths(src_root_node))

    def resolve_path(self, path):
        variables = copy.copy(self._path_variables)
        variables.update(self._variables)
        return subst_vars(path, variables)

    def resolve_paths_with_destdir(self, src_root_node):
        """Same as resolve_paths, but prefix every path with $destdir."""
        return self._resolve_paths(src_root_node, use_destdir=True)

    def resolve_paths(self, src_root_node):
        return self._resolve_paths(src_root_node, use_destdir=False)

    def _resolve_paths(self, src_root_node, use_destdir):
        variables = copy.copy(self._path_variables)
        variables.update(self._variables)
        variables['_srcrootdir'] = src_root_node.abspath()

        root = find_root(src_root_node)

        def _prefix_destdir(path):
            destdir = subst_vars("$destdir", variables)
            if path:
                tail = explode_path(path)[1:]
                if not tail:
                    raise ValueError("Invalid target directory in section %r "
                                     "(not absolute: %r)" % (name, path))
                return os.path.join(destdir, os.path.join(*tail))
            else:
                raise ValueError("Invalid target directory in section "
                                 "%r: %r" % (name, path))

        node_sections = {}
        for category in self.file_sections:
            node_sections[category] = {}
            for name, section in self.file_sections[category].items():
                srcdir = subst_vars(section.source_dir, variables)
                target = subst_vars(section.target_dir, variables)

                if use_destdir:
                    target = _prefix_destdir(target)

                srcdir_node = root.find_node(srcdir)
                if srcdir_node is None:
                    raise IOError("directory %r not found !" % (srcdir,))
                target_node = root.make_node(target)
                node_sections[category][name] = \
                        [(srcdir_node.find_node(f), target_node.make_node(g))
                         for f, g in section.files]

        return node_sections

########NEW FILE########
__FILENAME__ = lexer
import re

from ply.lex \
    import \
        lex, LexToken

from bento.errors \
    import \
        ParseError, InternalBentoError
from bento.parser.utils \
    import \
        Peeker, BackwardGenerator, count_lines

import six

word_fields = [
    ("AUTHOR_EMAIL_ID", r"AuthorEmail"),
    ("COMPILED_LIBRARY_ID", r"CompiledLibrary"),
    ("CONFIG_PY_ID", r"ConfigPy"),
    ("DATAFILES_ID", r"DataFiles"),
    ("DEFAULT_ID", r"Default"),
    ("DESCRIPTION_FROM_FILE_ID", r"DescriptionFromFile"),
    ("DOWNLOAD_URL_ID", r"DownloadUrl"),
    ("EXECUTABLE_ID", r"Executable"),
    ("FLAG_ID", r"Flag"),
    ("PATH_ID", r"Path"),
    ("FUNCTION_ID", r"Function"),
    ("LIBRARY_ID", r"Library"),
    ("MAINTAINER_EMAIL_ID", r"MaintainerEmail"),
    ("MODULE_ID", r"Module"),
    ("NAME_ID", r"Name"),
    ("SRCDIR_ID", r"SourceDir"),
    ("SUB_DIRECTORY_ID", r"SubDirectory"),
    ("TARGET_ID", r"TargetDir"),
    ("URL_ID", r"Url"),
    ("VERSION_ID", r"Version"),
]

line_fields = [
    ("AUTHOR_ID", r"Author"),
    ("LICENSE_ID", r"License"),
    ("MAINTAINER_ID", r"Maintainer"),
    ("SUMMARY_ID", r"Summary"),
]

comma_line_fields = [
    ("BUILD_REQUIRES_ID", r"BuildRequires"),
    ("CLASSIFIERS_ID", r"Classifiers"),
    ("INSTALL_REQUIRES_ID", r"InstallRequires"),
    ("PLATFORMS_ID", r"Platforms"),
]

comma_word_fields = [
    ("EXTENSION_ID", r"Extension"),
    ("EXTRA_SOURCE_FILES_ID", r"ExtraSourceFiles"),
    ("FILES_ID", r"Files"),
    ("HOOK_FILE_ID", r"HookFile"),
    ("INCLUDE_DIRS_ID", r"IncludeDirs"),
    ("KEYWORDS_ID", r"Keywords"),
    ("META_TEMPLATE_FILE_ID", r"MetaTemplateFile"),
    ("META_TEMPLATE_FILES_ID", r"MetaTemplateFiles"),
    ("MODULES_ID", r"Modules"),
    ("PACKAGES_ID", r"Packages"),
    ("RECURSE_ID", r"Recurse"),
    ("SOURCES_ID", r"Sources"),
    ("USE_BACKENDS_ID", r"UseBackends"),
]

multilines_fields = [
    ("DESCRIPTION_ID", r"Description"),
]

keyword_fields = word_fields + line_fields + comma_line_fields + comma_word_fields + multilines_fields
keyword_tokens = [k[0] for k in keyword_fields]

keyword_misc = dict([
        ('if', 'IF'),
        ('else', 'ELSE'),
        ('true', 'TRUE'),
        ('false', 'FALSE'),
        ('not', 'NOT_OP'),
        ('flag', 'FLAG_OP'),
        ('os', 'OS_OP'),
])
keyword_misc_tokens = keyword_misc.values()

tokens = ["COLON", "WS", "WORD", "NEWLINE", "STRING", "MULTILINES_STRING",
          "BLOCK_MULTILINES_STRING", "COMMA", "INDENT", "DEDENT", "LPAR",
          "RPAR", "BACKSLASH"] \
        + list(keyword_tokens) + list(keyword_misc_tokens)

states = [
        ("insidestring", "exclusive"),
        ("insideword", "exclusive"),
        # To be used for keywords that accept multiline values (description,
        # etc...)
        ("insidemstring", "exclusive"),
        ("insidemstringnotcontinued", "exclusive"),

        ("insidewcommalistfirstline", "inclusive"),
        ("insidewcommalist", "inclusive"),

        ("insidescommalistfirstline", "exclusive"),
        ("insidescommalist", "exclusive"),
]

ESCAPING_CHAR = {"BACKSLASH": True}

def t_NEWLINE(t):
    r"(\n|\r\n)"
    t.lexer.lineno += len(t.value)
    return t
NEWLINE_PATTERN = t_NEWLINE.__doc__

def t_BACKSLASH(t):
    r"\\"
    return t

def t_TAB(t):
    r"\t"
    raise SyntaxError("Tab not supported")

R_NEWLINE = re.compile(t_NEWLINE.__doc__)

t_WS = r" [ ]+"

def t_COLON(t):
    r":"
    return t

keywords_dict = dict((v, k) for k, v in keyword_fields)
keywords_dict.update(keyword_misc)
word_keywords = dict((k, v) for k, v in word_fields)
line_keywords = dict((k, v) for k, v in line_fields)
multilines_keywords = dict((k, v) for k, v in multilines_fields)
comma_line_keywords = dict((k, v) for k, v in comma_line_fields)
comma_word_keywords = dict((k, v) for k, v in comma_word_fields)

def t_FIELD(t):
    r"\w+(?=\s*:)"
    try:
        type = keywords_dict[t.value]
    except KeyError:
        raise ParseError("Unrecognized keyword: %r" % (t.value,), t)
    else:
        t.type = type
        if type in line_keywords:
            t_begin_insidestring(t)
        elif type in multilines_keywords:
            # FIXME: we differentiate between top Description (accepting multiline)
            # and description within flag block (where we cannot currently accept
            # multilines).
            if t.lexpos >= 1:
                if R_NEWLINE.match(t.lexer.lexdata[t.lexpos-1]):
                    t_begin_insidemstring(t)
                else:
                     t_begin_insidestring(t)
            else:
                t_begin_insidemstring(t)
        elif type in comma_line_keywords:
            t_begin_inside_scommalistfirstline(t)
        elif type in comma_word_keywords:
            t_begin_inside_wcommalistfirstline(t)
        else:
            t_begin_inside_word(t)
        return t

def t_COMMENT(t):
    r'[ ]*\#[^\r\n]*'
    pass

def t_WORD(t):
    r'[^\#^\s\\\(\)]+'
    if t.value in keyword_misc:
        t.type = keyword_misc[t.value]
    return t

t_LPAR = r"\("
t_RPAR = r"\)"

def t_begin_inside_word(t):
    r'start_insideword'
    t.lexer.begin('insideword')

def t_end_inside_word(t):
    r'start_insideword'
    t.lexer.begin('INITIAL')

def t_begin_insidestring(t):
    r'start_insidestring'
    t.lexer.begin('insidestring')

def t_end_insidestring(t):
    r'end_insidestring'
    t.lexer.begin('INITIAL')

def t_begin_insidemstring(t):
    r'start_insidemstring'
    t.lexer.begin('insidemstring')

def t_end_insidemstring(t):
    r'end_insidemstring'
    t.lexer.begin('INITIAL')

def t_begin_inside_mstringnotcontinued(t):
    r'start_inside_mstringnotcontinued'
    t.lexer.begin("insidemstringnotcontinued")

def t_end_inside_mstringnotcontinued(t):
    r'end_inside_mstringnotcontinued'
    t.lexer.begin("INITIAL")

# Word comma list
def t_begin_inside_wcommalist(t):
    r'start_inside_wcommalist'
    t.lexer.begin('insidewcommalist')

def t_end_inside_wcommalist(t):
    r'end_inside_wcommalist'
    t.lexer.begin('INITIAL')

def t_begin_inside_wcommalistfirstline(t):
    r'start_inside_wcommalistfirstline'
    t.lexer.begin('insidewcommalistfirstline')

def t_end_inside_wcommalistfirstline(t):
    r'end_inside_wcommalistfirstline'
    t.lexer.begin('INITIAL')

# string comma list
def t_begin_inside_scommalist(t):
    r'start_inside_commalistw'
    t.lexer.begin('insidescommalist')

def t_end_inside_scommalist(t):
    r'end_inside_commalistw'
    t.lexer.begin('INITIAL')

def t_begin_inside_scommalistfirstline(t):
    r'start_inside_scommalistfirstline'
    t.lexer.begin('insidescommalistfirstline')

def t_end_inside_scommalistfirstline(t):
    r'end_inside_scommalistfirstline'
    t.lexer.begin('INITIAL')

#------------
# Inside word
#------------
def t_insideword_NEWLINE(t):
    t.lexer.lineno += len(t.value)
    t_end_inside_word(t)
    return t
t_insideword_NEWLINE.__doc__ = NEWLINE_PATTERN

def t_insideword_COLON(t):
    return t
t_insideword_COLON.__doc__ = t_COLON.__doc__

def t_insideword_WS(t):
    return t
t_insideword_WS.__doc__ = t_WS

def t_insideword_WORD(t):
    return t
t_insideword_WORD.__doc__ = t_WORD.__doc__

t_insideword_COMMENT = t_COMMENT

#-------------------------------
# Inside single line string rule
#-------------------------------
def t_insidestring_newline(t):
    t.lexer.lineno += len(t.value)
    t.type = "NEWLINE"
    t_end_insidestring(t)
    return t
t_insidestring_newline.__doc__ = NEWLINE_PATTERN

def t_insidestring_COLON(t):
    r':'
    return t

def t_insidestring_WS(t):
    r' [ ]+'
    return t

def t_insidestring_STRING(t):
    r'[^\\\r\n]+'
    t_end_insidestring(t)
    return t

#-----------------------
# multiline string rules
#-----------------------
def t_insidemstring_COLON(t):
    r':(?=.*\S+.*)'
    return t

def t_insidemstring_COLON_NO_CONTINUED(t):
    r':(?!.*\S.*)'
    t.type = "COLON"
    t_begin_inside_mstringnotcontinued(t)
    return t

def t_insidemstring_WS(t):
    r'[ ]+'
    return t

def t_insidemstring_NEWLINE(t):
    t.lexer.lineno += len(t.value)
    return t
t_insidemstring_NEWLINE.__doc__ = t_NEWLINE.__doc__

def t_insidemstring_MULTILINES_STRING(t):
    r'.+((\n[ ]+.+$)|(\n^[ ]*$))*'
    t.lexer.lineno += count_lines(t.value) - 1
    t_end_inside_mstringnotcontinued(t)
    return t

# Multiline string that starts on same line as Description
def t_insidemstringnotcontinued_NEWLINE(t):
    return t
t_insidemstringnotcontinued_NEWLINE.__doc__ = t_NEWLINE.__doc__

def t_insidemstringnotcontinued_WS(t):
    return t
t_insidemstringnotcontinued_WS.__doc__ = t_WS

def t_insidemstringnotcontinued_BLOCK_MULTILINES_STRING(t):
    r'.+((\n[ ]+.+$)|(\n^[ ]*$))*'
    t_end_inside_mstringnotcontinued(t)
    return t

#------------------------
# inside comma word list rules
#------------------------
def t_insidewcommalistfirstline_COLON(t):
    r":"
    return t

def t_insidewcommalistfirstline_WS(t):
    r' [ ]+'
    return t

def t_insidewcommalistfirstline_WORD(t):
    r'[^,\#^\s\\\(\)]+(?=,)'
    t_begin_inside_wcommalist(t)
    return t

def t_insidewcommalistfirstline_WORD_STOP(t):
    r'[^\#,\s\\\(\)]+(?!,)'
    t.type = "WORD"
    t_end_inside_wcommalistfirstline(t)
    return t

def t_insidewcommalistfirstline_NEWLINE(t):
    t.lexer.lineno += len(t.value)
    return t
t_insidewcommalistfirstline_NEWLINE.__doc__ = NEWLINE_PATTERN

def t_insidewcommalistfirstline_COMMA(t):
    r','
    return t

def t_insidewcommalist_WORD(t):
    r'[^,\s]+(?=,)'
    return t

def t_insidewcommalist_WORD_STOP(t):
    r'[^,\s]+(?!,)'
    t.type = "WORD"
    t_end_inside_wcommalist(t)
    return t

def t_insidewcommalist_NEWLINE(t):
    t.lexer.lineno += len(t.value)
    return t
t_insidewcommalist_NEWLINE.__doc__ = NEWLINE_PATTERN

def t_insidewcommalist_WS(t):
    r' [ ]+'
    return t

def t_insidewcommalist_COMMA(t):
    r','
    return t

#------------------
# comma list string
#------------------
def t_insidescommalistfirstline_COLON(t):
    r":"
    return t

def t_insidescommalistfirstline_WS(t):
    r' [ ]+'
    return t

def t_insidescommalistfirstline_STRING(t):
    r'[^,\n(\r\n)]+(?=,)'
    t_begin_inside_scommalist(t)
    return t

def t_insidescommalistfirstline_STRING_STOP(t):
    r'[^,\n(\r\n)]+(?!,)'
    t.type = "STRING"
    t_end_inside_scommalistfirstline(t)
    return t

def t_insidescommalistfirstline_NEWLINE(t):
    t.lexer.lineno += len(t.value)
    return t
t_insidescommalistfirstline_NEWLINE.__doc__ = NEWLINE_PATTERN

def t_insidescommalistfirstline_COMMA(t):
    r','
    return t

def t_insidescommalist_WS(t):
    r' [ ]+'
    return t

def t_insidescommalist_STRING(t):
    r'[^,\n(\r\n)]+(?=,)'
    return t

def t_insidescommalist_STRING_STOP(t):
    r'[^,\n(\r\n)]+(?!,)'
    t.type = "STRING"
    t_end_inside_scommalist(t)
    return t

def t_insidescommalist_NEWLINE(t):
    t.lexer.lineno += len(t.value)
    return t
t_insidescommalist_NEWLINE.__doc__ = NEWLINE_PATTERN

def t_insidescommalist_COMMA(t):
    r','
    return t

# Error handling rule
def t_error(t):
    raise ParseError("Illegal character '%s'" % t.value[0], t)

def t_insideword_error(t):
    raise ParseError("Illegal character (inside word state) '%s'" % t.value[0], t)

def t_insidestring_error(t):
    raise ParseError("Illegal character (insidestring state) '%s'" % t.value[0], t)

def t_insidemstring_error(t):
    raise ParseError("Illegal character (insidemstring state) '%s'" % t.value[0], t)

def t_insidemstringnotcontinued_error(t):
    raise ParseError("Illegal character (inside_mstringnotcontinued state) '%s'" % t.value[0], t)

def t_insidewcommalist_error(t):
    raise ParseError("Illegal character (inside wcommalist state) '%s'" % t.value[0], t)

def t_insidewcommalistfirstline_error(t):
    raise ParseError("Illegal character (inside wcommalistfirstline state) '%s'" % t.value[0], t)

def t_insidescommalist_error(t):
    raise ParseError("Illegal character (inside scommalist state) '%s'" % t.value[0], t)

def t_insidescommalistfirstline_error(t):
    raise ParseError("Illegal character (inside scommalistfirstline state) '%s'" % t.value[0], t)

#--------
# Filters
#--------
def detect_escaped(stream):
    """Post process the given stream (token iterator) to generate escaped
    character for characters preceded by the escaping token."""
    for t in stream:
        is_escaping_char = ESCAPING_CHAR.get(t.type, False)
        if is_escaping_char:
            try:
                t = six.advance_iterator(stream)
            except StopIteration:
                raise SyntaxError("EOF while escaping token %r (line %d)" %
                                  (t.value, t.lineno-1))
            t.escaped = True
        else:
            t.escaped = False
        yield t

def merge_escaped(stream):
    """Merge tokens whose escaped attribute is True together.

    Must be run after detect_escaped.

    Parameters
    ----------
    stream: iterator
    """
    stream = Peeker(stream, EOF)
    queue = []

    t = six.advance_iterator(stream)
    while t:
        if t.escaped:
            queue.append(t)
        else:
            if t.type == "WORD":
                if queue:
                    queue.append(t)
                    n = stream.peek()
                    if not n.escaped:
                        t.value = "".join([c.value for c in queue])
                        yield t
                        queue = []
                else:
                    n = stream.peek()
                    if n.escaped:
                        queue.append(t)
                    else:
                        yield t
            else:
                if queue:
                    queue[-1].value = "".join([c.value for c in queue])
                    queue[-1].type = "WORD"
                    yield queue[-1]
                    queue = []
                yield t
        try:
            t = six.advance_iterator(stream)
        except StopIteration:
            if queue:
                t.value = "".join([c.value for c in queue])
                t.type = "WORD"
                yield t
            return

def filter_ws_and_newline(stream):
    """Remove NEWLINE and WS tokens from the stream.

    Parameters
    ----------
    stream: iterator
        iterator of tokens."""
    for item in stream:
        if item.type not in ["NEWLINE", "WS"]:
            yield item

def remove_lines_indent(s, indent=None):
    lines = s.splitlines()
    if len(lines) > 1:
        if indent is None:
            indent = len(lines[1]) - len(lines[1].lstrip())
        for i in range(1, len(lines)):
            if len(lines[i]) >= indent:
                lines[i] = lines[i][indent:]
            else:
                lines[i] = ""
        return "\n".join(lines)
    else:
        return s

def post_process_string(it):
    """Remove spurious spacing from *MULTILINE_STRING tokens."""
    it = BackwardGenerator(it)
    for token in it:
        if token.type == "BLOCK_MULTILINES_STRING":
            previous = it.previous()
            if not previous.type == "INDENT":
                raise InternalBentoError(
                        "Error while post processing block line: %s -> %s" \
                        % (previous, token))
            else:
                indent = previous.value
                token.value = remove_lines_indent(token.value, indent)
                token.type = "MULTILINES_STRING"
        elif token.type == "MULTILINES_STRING":
            token.value = remove_lines_indent(token.value)
        yield token

def indent_generator(toks):
    """Post process the given stream of tokens to generate INDENT/DEDENT
    tokens.
    
    Note
    ----
    Each generated token's value is the total amount of spaces from the
    beginning of the line.
    
    The way indentation tokens are generated is similar to how it works in
    python."""
    stack = [0]

    # Dummy token to track the token just before the current one
    former = LexToken()
    former.type = "NEWLINE"
    former.value = "dummy"
    former.lineno = 0
    former.lexpos = -1

    def generate_dedent(stck, tok):
        amount = stck.pop(0)
        return new_dedent(amount, tok)

    for token in toks:
        if former.type == "NEWLINE":
            if token.type == "WS":
                indent = len(token.value)
            else:
                indent = 0

            if indent == stack[0]:
                former = token
                if indent > 0:
                    token = six.advance_iterator(toks)
                    former = token
                    yield token
                else:
                    yield former
            elif indent > stack[0]:
                stack.insert(0, indent)
                ind = new_indent(indent, token)
                former = ind
                yield ind
            elif indent < stack[0]:
                if not indent in stack:
                    raise ValueError("Wrong indent at line %d" % token.lineno)
                while stack[0] > indent:
                    former = generate_dedent(stack, token)
                    yield former
                if stack[0] > 0:
                    former = six.advance_iterator(toks)
                    yield former
                else:
                    former = token
                    yield token
        else:
            former = token
            yield token

    # Generate additional DEDENT so that the number of INDENT/DEDENT always
    # match
    while len(stack) > 1:
        former = generate_dedent(stack, token)
        yield former

def new_indent(amount, token):
    tok = LexToken()
    tok.type = "INDENT"
    tok.value = amount
    tok.lineno = token.lineno
    tok.lexpos = token.lexpos
    return tok

def new_dedent(amount, token):
    tok = LexToken()
    tok.type = "DEDENT"
    tok.value = amount
    tok.lineno = token.lineno
    tok.lexpos = token.lexpos
    return tok

class _Dummy(object):
    def __init__(self):
        self.type = "EOF"
        self.escaped = False
        self.value = None

    def __repr__(self):
        return "DummyToken(EOF)"

EOF = _Dummy()

class BentoLexer(object):
    def __init__(self, optimize=False):
        self.lexer = lex(reflags=re.UNICODE|re.MULTILINE, debug=0, optimize=optimize, nowarn=0, lextab='lextab')

    def input(self, data):
        self.lexer.input(data)
        stream = iter(self.lexer.token, None)
        stream = detect_escaped(stream)
        stream = merge_escaped(stream)
        stream = indent_generator(stream)
        stream = filter_ws_and_newline(stream)
        stream = post_process_string(stream)
        self.stream = stream

    def __iter__(self):
        return iter(self.token, None)

    def token(self):
        try:
            return six.advance_iterator(self.stream)
        except StopIteration:
            pass

########NEW FILE########
__FILENAME__ = misc
from bento.utils.utils \
    import \
        extract_exception
from bento.errors \
    import \
        ParseError
from bento.parser.nodes \
    import \
        ast_walk
from bento.parser.parser \
    import \
        parse as _parse
from bento.parser.visitor \
    import \
        Dispatcher

def raw_parse(data, filename=None):
    try:
        ret = _parse(data)
        return ret
    except ParseError:
        e = extract_exception()
        e.filename = filename
        raise

def build_ast_from_raw_dict(raw_dict, user_flags=None):
    dispatcher = Dispatcher(user_flags)
    res = ast_walk(raw_dict, dispatcher)
    return res

def build_ast_from_data(data, user_flags=None, filename=None):
    """Parse the given data to a dictionary which is easy to exploit
    at later stages."""
    d = raw_parse(data, filename)
    return build_ast_from_raw_dict(d, user_flags)

########NEW FILE########
__FILENAME__ = nodes
import sys

from six.moves import cPickle

def __copy(d):
    # Faster than deepcopy - ideally remove the need for deepcopy altogether
    return cPickle.loads(cPickle.dumps(d, protocol=2))

class Node(object):
    def __init__(self, tp, children=None, value=None):
        self.type = tp
        if children:
            self.children = children
        else:
            self.children = []
        self.value = value

    def __str__(self):
        return "Node(%r)" % self.type

    def __repr__(self):
        return "Node(%r)" % self.type

def ast_pprint(root, cur_ind=0, ind_val=4, string=None):
    """Pretty printer for the yacc-based parser."""
    _buf = []

    def _ast_pprint(_root, _cur_ind):
        if not hasattr(_root, "children"):
            _buf.append(str(_root))
        else:
            if _root.children:
                _buf.append("%sNode(type='%s'):" % (' ' * _cur_ind * ind_val,
                                                    _root.type))
                for c in _root.children:
                    _ast_pprint(c, _cur_ind + 1)
            else:
                msg = "%sNode(type='%s'" % (' ' * _cur_ind * ind_val,
                                            _root.type)
                if _root.value is not None:
                    msg += ", value=%r)" % _root.value
                else:
                    msg += ")"
                _buf.append(msg)

    _ast_pprint(root, cur_ind)
    if string is None:
        print("\n".join(_buf))
    else:
        string.write("\n".join(_buf))

def ast_walk(root, dispatcher, debug=False):
    """Walk the given tree and for each node apply the corresponding function
    as defined by the dispatcher.
    
    If one node type does not have any function defined for it, it simply
    returns the node unchanged.
    
    Parameters
    ----------
    root : Node
        top of the tree to walk into.
    dispatcher : Dispatcher
        defines the action for each node type.
    """
    def _walker(par):
        children = []
        for c in par.children:
            children.append(_walker(c))

        par.children = [c for c in children if c is not None]
        try:
            func = dispatcher.action_dict[par.type]
            return func(par)
        except KeyError:
            if debug:
                print("no action for type %s" % par.type)
            return par

    # FIXME: we need to copy the dict because the dispatcher modify the dict in
    # place ATM, and we call this often. This is very expensive, and should be
    # removed as it has a huge cost in starting time (~30 % in hot case)
    return _walker(__copy(root))

########NEW FILE########
__FILENAME__ = parser
import sys
import errno

import os.path as op

import ply.yacc

import bento.parser.rules

from bento._config \
    import \
        _PICKLED_PARSETAB, _OPTIMIZE_LEX, _DEBUG_YACC
from bento.utils.utils \
    import \
        extract_exception
from bento.errors \
    import \
        InternalBentoError, BentoError, ParseError
from bento.parser.lexer \
    import \
        BentoLexer, tokens as _tokens

# XXX: is there a less ugly way to do this ?
__GLOBALS = globals()
for k in dir(bento.parser.rules):
    if k.startswith("p_"):
        __GLOBALS[k] = getattr(bento.parser.rules, k)

# Do not remove: this is used by PLY. We filter out tokens which used only as
# intermediataries inside the lexer to avoid ply.lex warns about unused tokens
# in the grammar.
tokens = [t for t in _tokens if not t in ["WS", "NEWLINE", "BACKSLASH", "BLOCK_MULTILINES_STRING"]]

def _has_parser_changed(picklefile):
    # FIXME: private function to determine whether ply will need to write to
    # the pickled grammar file. Highly implementation dependent.
    from ply.yacc import PlyLogger, get_caller_module_dict, ParserReflect, YaccError, LRTable
    errorlog = PlyLogger(sys.stderr)

    pdict = get_caller_module_dict(2)

    # Collect parser information from the dictionary
    pinfo = ParserReflect(pdict, log=errorlog)
    pinfo.get_all()

    if pinfo.error:
        raise YaccError("Unable to build parser")

    # Check signature against table files (if any)
    signature = pinfo.signature()

    # Read the tables
    try:
        lr = LRTable()
        read_signature = lr.read_pickle(picklefile)
        return read_signature != signature
    except Exception:
        return True

class Parser(object):
    def __init__(self, lexer=None):
        if lexer is None:
            self.lexer = BentoLexer(optimize=_OPTIMIZE_LEX)
        else:
            self.lexer = lexer

        picklefile = _PICKLED_PARSETAB
        if not op.exists(picklefile):
            try:
                fid = open(picklefile, "wb")
                fid.close()
            except IOError:
                e = extract_exception()
                raise BentoError("Could not write pickle file %r (original error was %r)" % (picklefile, e))
        else:
            try:
                fid = open(_PICKLED_PARSETAB, "wb")
                fid.close()
            except IOError:
                # In case of read-only support (no write access, zip import,
                # etc...)
                e = extract_exception()
                if e.errno == errno.EACCES:
                    if _has_parser_changed(_PICKLED_PARSETAB):
                        raise BentoError("Cannot write new updated grammar to file %r" % _PICKLED_PARSETAB)
                else:
                    raise
        self.parser = ply.yacc.yacc(start="stmt_list",
                                picklefile=picklefile,
                                debug=_DEBUG_YACC)

    def parse(self, data):
        res = self.parser.parse(data, lexer=self.lexer)
        ## FIXME: this is stupid, deal correctly with empty ast in the grammar proper
        #if res is None:
        #    res = Node("empty")
        return res

    def reset(self):
        # XXX: implements reset for lexer
        self.lexer = BentoLexer(optimize=_OPTIMIZE_LEX)
        # XXX: ply parser.reset method expects those attributes to
        # exist
        self.parser.statestack = []
        self.parser.symstack = []
        return self.parser.restart()

__PARSER = None
def parse(data):
    global __PARSER
    if __PARSER is None:
        __PARSER = Parser()
    else:
        __PARSER.reset()
    return __PARSER.parse(data)
    #return Parser().parse(data)

def p_error(p):
    if p is None:
        raise InternalBentoError("Unknown parsing error (parser/lexer bug ? Please report this with your bento.info)")
    else:
        msg = "yacc: Syntax error at line %d, Token(%s, %r)" % \
                (p.lineno, p.type, p.value)
        if hasattr(p.lexer, "lexdata"):
            raw_data = p.lexer.lexdata
            data = raw_data.splitlines()
            line_lexpos = sum(len(line) + 1 for line in data[:p.lineno-1])
            inline_lexpos = p.lexpos - line_lexpos
            msg += "\n%s" % (data[p.lineno-1],)
            msg += "\n%s" % (" " * inline_lexpos + "^",)
        raise ParseError(msg, p)

########NEW FILE########
__FILENAME__ = rules
import warnings

from bento.parser.nodes import Node

#-------------
#   Grammar
#-------------
def p_stmt_list(p):
    """stmt_list : stmt_list stmt"""
    p[0] = p[1]
    if p[2].type not in ("newline",):
        p[0].children.append(p[2])

def p_stmt_list_term(p):
    """stmt_list : stmt"""
    p[0] = Node("stmt_list", children=[p[1]])

def p_stmt_list_empty(p):
    """stmt_list : empty"""
    p[0] = Node("stmt_list", [])

def p_stmt(p):
    """stmt : meta_stmt
            | data_files
            | exec
            | extra_source_files
            | flag
            | library
            | path
    """
    p[0] = p[1]

def p_empty(p):
    "empty :"
    pass

#----------------
#   Meta data
#----------------
def p_meta_stmt(p):
    """meta_stmt : meta_author_stmt
                 | meta_author_email_stmt
                 | meta_classifiers_stmt
                 | meta_config_py_stmt
                 | meta_description_stmt
                 | meta_description_from_file_stmt
                 | meta_download_url_stmt
                 | meta_hook_file_stmt
                 | meta_keywords_stmt
                 | meta_license_stmt
                 | meta_maintainer_stmt
                 | meta_maintainer_email_stmt
                 | meta_name_stmt
                 | meta_platforms_stmt
                 | meta_recurse_stmt
                 | meta_summary_stmt
                 | meta_meta_template_files_stmt
                 | meta_use_backends_stmt
                 | meta_url_stmt
                 | meta_version_stmt
    """
    p[0] = p[1]

def p_meta_description(p):
    """meta_description_stmt : DESCRIPTION_ID COLON MULTILINES_STRING
    """
    p[0] = Node("description", value=p[3])

def p_meta_description_indented(p):
    """meta_description_stmt : DESCRIPTION_ID COLON INDENT MULTILINES_STRING DEDENT
    """
    p[0] = Node("description", value=p[4])

def p_meta_name_stmt(p):
    """meta_name_stmt : NAME_ID COLON WORD
    """
    p[0] = Node("name", value=p[3])

def p_meta_summary_stmt(p):
    """meta_summary_stmt : SUMMARY_ID COLON STRING
    """
    p[0] = Node("summary", value=p[3])

def p_meta_url_stmt(p):
    """meta_url_stmt : URL_ID COLON WORD
    """
    p[0] = Node("url", value=p[3])

def p_meta_download_url_stmt(p):
    """meta_download_url_stmt : DOWNLOAD_URL_ID COLON WORD
    """
    p[0] = Node("download_url", value=p[3])

def p_meta_author_stmt(p):
    """meta_author_stmt : AUTHOR_ID COLON STRING
    """
    p[0] = Node("author", value=p[3])

def p_meta_author_email_stmt(p):
    """meta_author_email_stmt : AUTHOR_EMAIL_ID COLON WORD
    """
    p[0] = Node("author_email", value=p[3])

def p_meta_maintainer_stmt(p):
    """meta_maintainer_stmt : MAINTAINER_ID COLON STRING
    """
    p[0] = Node("maintainer", value=p[3])

def p_meta_maintainer_email_stmt(p):
    """meta_maintainer_email_stmt : MAINTAINER_EMAIL_ID COLON WORD
    """
    p[0] = Node("maintainer_email", value=p[3])

def p_meta_license_stmt(p):
    """meta_license_stmt : LICENSE_ID COLON STRING
    """
    p[0] = Node("license", value=p[3])

def p_meta_description_from_file_stmt(p):
    """meta_description_from_file_stmt : DESCRIPTION_FROM_FILE_ID COLON WORD"""
    p[0] = Node("description_from_file", value=p[3])

def p_meta_platforms_stmt(p):
    """meta_platforms_stmt : PLATFORMS_ID COLON scomma_list
    """
    p[0] = Node("platforms", value=p[3].value)

def p_meta_keywords_stmt(p):
    """meta_keywords_stmt : KEYWORDS_ID COLON wcomma_list
    """
    p[0] = Node("keywords", value=p[3].value)

def p_meta_version_stmt(p):
    """meta_version_stmt : VERSION_ID COLON version
    """
    p[0] = Node("version", value=p[3].value)

def p_meta_config_py_stmt(p):
    """meta_config_py_stmt : CONFIG_PY_ID COLON WORD
    """
    p[0] = Node("config_py", value=p[3])

def p_meta_meta_template_file_stmt(p):
    """meta_meta_template_files_stmt : META_TEMPLATE_FILE_ID COLON WORD
    """
    warnings.warn("MetaTemplateFile field is obsolete - use MetaTemplateFiles itself")
    p[0] = Node("meta_template_files", value=[p[3]])

def p_meta_meta_template_files_stmt(p):
    """meta_meta_template_files_stmt : META_TEMPLATE_FILES_ID COLON wcomma_list
    """
    p[0] = Node("meta_template_files", value=p[3].value)

def p_meta_classifiers_stmt(p):
    """meta_classifiers_stmt : CLASSIFIERS_ID COLON classifiers_list"""
    p[0] = Node("classifiers", value=p[3].value)

def p_classifiers_list(p):
    """classifiers_list : indented_classifiers_list
    """
    p[0] = Node("classifiers", value=p[1].value)

def p_classifiers_list_term(p):
    """classifiers_list : classifiers
    """
    p[0] = Node("classifiers", value=p[1].value)

def p_indented_comma_list1(p):
    """indented_classifiers_list : classifiers COMMA INDENT classifiers DEDENT
    """
    p[0] = p[1]
    p[0].value.extend(p[4].value)

def p_indented_comma_list2(p):
    """indented_classifiers_list : INDENT classifiers DEDENT
    """
    p[0] = p[2]

def p_classifiers(p):
    """classifiers : classifiers COMMA classifier"""
    p[0] = p[1]
    p[0].value.append(p[3].value)

def p_classifiers_term(p):
    """classifiers : classifier"""
    p[0] = Node("classifiers", value=[p[1].value])

def p_classifier(p):
    """classifier : STRING"""
    p[0] = Node("classifier", value=p[1])

def p_meta_hook_file_stmt(p):
    """meta_hook_file_stmt : HOOK_FILE_ID COLON wcomma_list
    """
    p[0] = Node("hook_files", value=p[3].value)

def p_meta_subento_stmt(p):
    """meta_recurse_stmt : RECURSE_ID COLON wcomma_list"""
    p[0] = Node("subento", value=p[3].value)

def p_meta_use_backends_stmt(p):
    """meta_use_backends_stmt : USE_BACKENDS_ID COLON wcomma_list"""
    p[0] = Node("use_backends", value=p[3].value)

#---------------------------------------
# Data files and extra sources handling
#---------------------------------------
def p_extra_source_files(p):
    """extra_source_files : EXTRA_SOURCE_FILES_ID COLON wcomma_list"""
    p[0] = Node("extra_source_files", value=p[3].value)
 
def p_data_files(p):
    """data_files : data_files_declaration INDENT data_files_stmts DEDENT
    """
    p[0] = Node("data_files", children=[p[1]])
    p[0].children.append(p[3])

def p_data_files_declaration(p):
    """data_files_declaration : DATAFILES_ID COLON WORD"""
    p[0] = Node("data_files_declaration", value=p[3])

def p_data_files_stmts(p):
    """data_files_stmts : data_files_stmts data_files_stmt"""
    p[0] = Node("data_files_stmts", children=(p[1].children + [p[2]]))

def p_data_files_stmts_term(p):
    """data_files_stmts : data_files_stmt"""
    p[0] = Node("data_files_stmts", children=[p[1]])

def p_data_files_stmt(p):
    """data_files_stmt : data_files_target
                       | data_files_files
                       | data_files_srcdir
    """
    p[0] = p[1]

def p_data_files_target(p):
    """data_files_target : TARGET_ID COLON WORD"""
    p[0] = Node("target_dir", value=p[3])

def p_data_files_srcdir(p):
    """data_files_srcdir : SRCDIR_ID COLON WORD"""
    p[0] = Node("source_dir", value=p[3])

def p_data_files_files(p):
    """data_files_files : FILES_ID COLON wcomma_list"""
    p[0] = Node("files", value=p[3].value)

#------------------
#   Flag section
#------------------
def p_flag(p):
    """flag : flag_declaration INDENT flag_stmts DEDENT"""
    p[0] = Node("flag", children=[p[1], p[3]])

def p_flag_declaration(p):
    """flag_declaration : FLAG_ID COLON WORD"""
    p[0] = Node("flag_declaration", value=p[3])

def p_flag_stmts(p):
    """flag_stmts : flag_stmts flag_stmt"""
    p[0] = Node("flag_stmts", children=(p[1].children + [p[2]]))

def p_flag_stmts_term(p):
    """flag_stmts : flag_stmt"""
    p[0] = Node("flag_stmts", children=[p[1]])

def p_flag_stmt(p):
    """flag_stmt : flag_description
                 | flag_default"""
    p[0] = p[1]

def p_flag_description(p):
    """flag_description : DESCRIPTION_ID COLON STRING"""
    p[0] = Node("flag_description", value=p[3])

def p_flag_default(p):
    """flag_default : DEFAULT_ID COLON WORD"""
    p[0] = Node("flag_default", value=p[3])

#---------------------
#   Path section
#---------------------
def p_path(p):
    """path : path_declaration INDENT path_stmts DEDENT"""
    p[0] = Node("path", children=[p[1], p[3]])

def p_path_declaration(p):
    """path_declaration : PATH_ID COLON WORD"""
    p[0] = Node("path_declaration", value=p[3])

def p_path_stmts(p):
    """path_stmts : path_stmts path_stmt"""
    p[0] = Node("path_stmts", children=(p[1].children + [p[2]]))

def p_path_stmts_term(p):
    """path_stmts : path_stmt"""
    p[0] = Node("path_stmts", children=[p[1]])

def p_path_stmt(p):
    """path_stmt : path_description
                 | path_default
                 | conditional_stmt"""
    p[0] = p[1]

def p_path_description(p):
    """path_description : DESCRIPTION_ID COLON STRING"""
    #"""path_description : meta_description_stmt"""
    p[0] = Node("path_description", value=p[3])

def p_path_default(p):
    """path_default : DEFAULT_ID COLON WORD"""
    p[0] = Node("path_default", value=p[3])

#---------------------
#   Library section
#---------------------
def p_library(p):
    """library : library_declaration INDENT library_stmts DEDENT
    """
    p[0] = Node("library", children=[p[1]])
    p[0].children.append(p[3])

def p_library_decl_only(p):
    """library : library_declaration
    """
    p[0] = Node("library", children=[p[1]])

def p_library_declaration(p):
    """library_declaration : LIBRARY_ID COLON library_name"""
    p[0] = p[3]

def p_library_name(p):
    """library_name : WORD
                    |"""
    if len(p) == 1:
        name = "default"
    else:
        name = p[1]
    p[0] = Node("library_name", value=name)

def p_library_stmts(p):
    """library_stmts : library_stmts library_stmt
    """
    children = p[1].children
    children.append(p[2])
    p[0] = Node("library_stmts", children=children)

def p_library_stmts_term(p):
    """library_stmts : library_stmt
    """
    p[0] = Node("library_stmts", children=[p[1]])

def p_library_stmt(p):
    """library_stmt : build_requires_stmt
                    | compiled_library_stmt
                    | conditional_stmt
                    | extension_stmt
                    | modules_stmt
                    | packages_stmt
                    | sub_directory_stmt
    """
    p[0] = p[1]

def p_packages_stmt(p):
    """packages_stmt : PACKAGES_ID COLON wcomma_list"""
    p[0] = Node("packages", value=p[3].value)

def p_modules_stmt(p):
    """modules_stmt : MODULES_ID COLON wcomma_list"""
    p[0] = Node("modules", value=p[3].value)

def p_sub_directory_stmt(p):
    """sub_directory_stmt : SUB_DIRECTORY_ID COLON WORD"""
    p[0] = Node("sub_directory", value=p[3])

def p_extension_stmt_content(p):
    """extension_stmt : extension_decl INDENT extension_field_stmts DEDENT"""
    p[0] = Node("extension", children=[p[1]])
    p[0].children.append(p[3])

def p_extension_field_stmts(p):
    """extension_field_stmts : extension_field_stmts extension_field_stmt"""
    children = p[1].children
    children.append(p[2])
    p[0] = Node("extension_field_stmts", children=children)

def p_extension_field_stmts_term(p):
    """extension_field_stmts : extension_field_stmt"""
    p[0] = Node("extension_field_stmts", children=[p[1]])

def p_extension_decl(p):
    """extension_decl : EXTENSION_ID COLON WORD"""
    p[0] = Node("extension_declaration", value=p[3])

def p_extension_sources(p):
    """extension_field_stmt : SOURCES_ID COLON wcomma_list"""
    p[0] = Node("sources", value=p[3].value)

def p_extension_include_dirs(p):
    """extension_field_stmt : INCLUDE_DIRS_ID COLON wcomma_list"""
    p[0] = Node("include_dirs", value=p[3].value)

def p_compiled_library_stmt_content(p):
    """compiled_library_stmt : compiled_library_decl INDENT compiled_library_field_stmts DEDENT"""
    p[0] = Node("compiled_library", children=[p[1]])
    p[0].children.append(p[3])

def p_compiled_library_field_stmts(p):
    """compiled_library_field_stmts : compiled_library_field_stmts compiled_library_field_stmt"""
    children = p[1].children
    children.append(p[2])
    p[0] = Node("compiled_library_field_stmts", children=children)

def p_compiled_library_field_stmts_term(p):
    """compiled_library_field_stmts : compiled_library_field_stmt"""
    p[0] = Node("compiled_library_field_stmts", children=[p[1]])

def p_compiled_library_decl(p):
    """compiled_library_decl : COMPILED_LIBRARY_ID COLON WORD"""
    p[0] = Node("compiled_library_declaration", value=p[3])

def p_compiled_library_sources(p):
    """compiled_library_field_stmt : SOURCES_ID COLON wcomma_list"""
    p[0] = Node("sources", value=p[3].value)

def p_compiled_library_include_dirs(p):
    """compiled_library_field_stmt : INCLUDE_DIRS_ID COLON wcomma_list"""
    p[0] = Node("include_dirs", value=p[3].value)

def p_build_requires_stmt(p):
    """build_requires_stmt : BUILD_REQUIRES_ID COLON scomma_list"""
    p[0] = Node("build_requires", value=p[3].value)

def p_install_requires_stmt(p):
    """build_requires_stmt : INSTALL_REQUIRES_ID COLON scomma_list"""
    p[0] = Node("install_requires", value=p[3].value)

#---------------------
# Conditional handling
#---------------------
def p_in_conditional_stmts(p):
    """in_conditional_stmts : library_stmts
                            | path_stmts
    """
    p[0] = p[1]

def p_conditional_if_only(p):
    """conditional_stmt : IF test COLON INDENT in_conditional_stmts DEDENT"""
    p[0] = Node("conditional", value=p[2], children=[p[5]])

def p_conditional_if_else(p):
    """conditional_stmt : IF test COLON INDENT in_conditional_stmts DEDENT \
                          ELSE COLON INDENT in_conditional_stmts DEDENT
    """
    p[0] = Node("conditional", value=p[2], children=[p[5], p[10]])

def p_test(p):
    """test : bool
            | os_var
            | flag_var"""
    p[0] = p[1]

def p_os_var(p):
    """os_var : OS_OP LPAR WORD RPAR"""
    p[0] = Node("osvar", value=p[3])

def p_flag_var(p):
    """flag_var : FLAG_OP LPAR WORD RPAR"""
    p[0] = Node("flagvar", value=p[3])

def p_not_flag_var(p):
    """flag_var : NOT_OP FLAG_OP LPAR WORD RPAR"""
    p[0] = Node("not_flagvar", value=p[4])

def p_cond_expr_true(p):
    """bool : TRUE"""
    p[0] = Node("bool", value=True)

def p_cond_expr_true_not(p):
    """bool : NOT_OP FALSE"""
    p[0] = Node("bool", value=True)

def p_cond_expr_false(p):
    """bool : FALSE"""
    p[0] = Node("bool", value=False)

def p_cond_expr_false_not(p):
    """bool : NOT_OP TRUE"""
    p[0] = Node("bool", value=False)

#----------------------
#  Executable section
#----------------------
def p_executable(p):
    """exec : exec_decl INDENT exec_stmts DEDENT"""
    p[0] = Node("executable", children=[p[1], p[3]])

def p_exec_declaration(p):
    """exec_decl : EXECUTABLE_ID COLON WORD"""
    p[0] = Node("exec_name", value=p[3])

def p_exec_stmts(p):
    """exec_stmts : exec_stmts exec_stmt"""
    p[0] = Node("exec_stmts", children=(p[1].children + [p[2]]))

def p_exec_stmts_term(p):
    """exec_stmts : exec_stmt"""
    p[0] = Node("exec_stmts", children=[p[1]])

def p_exec_stmt(p):
    """exec_stmt : function
                 | module"""
    p[0] = p[1]

def p_exec_module(p):
    """module : MODULE_ID COLON WORD"""
    p[0] = Node("module", value=p[3])

def p_exec_function(p):
    """function : FUNCTION_ID COLON WORD"""
    p[0] = Node("function", value=p[3])

# List of words handling
def p_wcomma_list_indented(p):
    """wcomma_list : comma_words COMMA INDENT comma_words DEDENT
    """
    p[0] = Node("wcomma_list", value=(p[1].value + p[4].value))

def p_wcomma_list_indented2(p):
    """wcomma_list : INDENT comma_words DEDENT
    """
    p[0] = Node("wcomma_list", value=p[2].value)

def p_wcomma_list(p):
    """wcomma_list : comma_words
    """
    p[0] = Node("wcomma_list", value=p[1].value)

#def p_indented_wcomma_list(p):
#    """indented_wcomma_list : WORD COMMA INDENT comma_words DEDENT
#    """
#    p[0] = p[1]
#    p[0].value.extend(p[4].value)
#
#def p_indented_wcomma_list_term(p):
#    """indented_comma_list : INDENT comma_words DEDENT
#    """
#    p[0] = p[2]

def p_comma_words(p):
    """comma_words : comma_words COMMA WORD
    """
    p[0] = p[1]
    p[0].value.append(p[3])

def p_comma_words_term(p):
    """comma_words : WORD
    """
    p[0] = Node("comma_words", value=[p[1]])

# List of strings handling
def p_scomma_list_indented(p):
    """scomma_list : indented_scomma_list 
    """
    p[0] = Node("scomma_list", value=p[1].value)

def p_scomma_list(p):
    """scomma_list : comma_strings
    """
    p[0] = Node("scomma_list", value=p[1].value)

def p_indented_scomma_list(p):
    """indented_scomma_list : comma_strings COMMA INDENT comma_strings DEDENT
    """
    p[0] = p[1]
    p[0].value.extend(p[4].value)

def p_indented_scomma_list_term(p):
    """indented_scomma_list : INDENT comma_strings DEDENT
    """
    p[0] = p[2]

def p_comma_strings(p):
    """comma_strings : comma_strings COMMA STRING
    """
    p[0] = p[1]
    p[0].value.append(p[3])

def p_comma_strings_term(p):
    """comma_strings : STRING
    """
    p[0] = Node("comma_strings", value=[p[1]])

# FIXME: proper literal for version
def p_version(p):
    """version : WORD"""
    p[0] = Node("version", value=p[1])

########NEW FILE########
__FILENAME__ = data_distribute
ref = {'author': 'The fellowship of the packaging',
 'author_email': 'distutils-sig@python.org',
 'data_files': {},
 'description': '===============================\nInstalling and Using Distribute\n===============================\n\n.. contents:: **Table of Contents**\n\n-----------\nDisclaimers\n-----------\n\nAbout the fork\n==============\n\n`Distribute` is a fork of the `Setuptools` project.\n\nDistribute is intended to replace Setuptools as the standard method\nfor working with Python module distributions.\n\nThe fork has two goals:\n\n- Providing a backward compatible version to replace Setuptools\n  and make all distributions that depend on Setuptools work as\n  before, but with less bugs and behaviorial issues.\n\n  This work is done in the 0.6.x series.\n\n  Starting with version 0.6.2, Distribute supports Python 3.\n  Installing and using distribute for Python 3 code works exactly\n  the same as for Python 2 code, but Distribute also helps you to support\n  Python 2 and Python 3 from the same source code by letting you run 2to3\n  on the code as a part of the build process, by setting the keyword parameter\n  ``use_2to3`` to True. See http://packages.python.org/distribute for more\n  information.\n\n- Refactoring the code, and releasing it in several distributions.\n  This work is being done in the 0.7.x series but not yet released.\n\nThe roadmap is still evolving, and the page that is up-to-date is\nlocated at : `http://packages.python.org/distribute/roadmap`.\n\nIf you install `Distribute` and want to switch back for any reason to\n`Setuptools`, get to the `Uninstallation instructions`_ section.\n\nMore documentation\n==================\n\nYou can get more information in the Sphinx-based documentation, located\nat http://packages.python.org/distribute. This documentation includes the old\nSetuptools documentation that is slowly replaced, and brand new content.\n\nAbout the installation process\n==============================\n\nThe `Distribute` installer modifies your installation by de-activating an\nexisting installation of `Setuptools` in a bootstrap process. This process\nhas been tested in various installation schemes and contexts but in case of a\nbug during this process your Python installation might be left in a broken\nstate. Since all modified files and directories are copied before the\ninstallation starts, you will be able to get back to a normal state by reading\nthe instructions in the `Uninstallation instructions`_ section.\n\nIn any case, it is recommended to save you `site-packages` directory before\nyou start the installation of `Distribute`.\n\n-------------------------\nInstallation Instructions\n-------------------------\n\nDistribute is only released as a source distribution.\n\nIt can be installed using pip, and can be done so with the source tarball,\nor by using the ``distribute_setup.py`` script provided online.\n\n``distribute_setup.py`` is the simplest and preferred way on all systems.\n\ndistribute_setup.py\n===================\n\nDownload\n`distribute_setup.py <http://python-distribute.org/distribute_setup.py>`_\nand execute it, using the Python interpreter of your choice.\n\nIf your shell has the ``curl`` program you can do::\n\n    $ curl -O http://python-distribute.org/distribute_setup.py\n    $ python distribute_setup.py\n\nNotice this file is also provided in the source release.\n\npip\n===\n\nRun easy_install or pip::\n\n    $ pip install distribute\n\nSource installation\n===================\n\nDownload the source tarball, uncompress it, then run the install command::\n\n    $ curl -O http://pypi.python.org/packages/source/d/distribute/distribute-0.6.27.tar.gz\n    $ tar -xzvf distribute-0.6.27.tar.gz\n    $ cd distribute-0.6.27\n    $ python setup.py install\n\n---------------------------\nUninstallation Instructions\n---------------------------\n\nLike other distutils-based distributions, Distribute doesn\'t provide an\nuninstaller yet. It\'s all done manually! We are all waiting for PEP 376\nsupport in Python.\n\nDistribute is installed in three steps:\n\n1. it gets out of the way an existing installation of Setuptools\n2. it installs a `fake` setuptools installation\n3. it installs distribute\n\nDistribute can be removed like this:\n\n- remove the ``distribute*.egg`` file located in your site-packages directory\n- remove the ``setuptools.pth`` file located in you site-packages directory\n- remove the easy_install script located in you ``sys.prefix/bin`` directory\n- remove the ``setuptools*.egg`` directory located in your site-packages directory,\n  if any.\n\nIf you want to get back to setuptools:\n\n- reinstall setuptools using its instruction.\n\nLastly:\n\n- remove the *.OLD.* directory located in your site-packages directory if any,\n  **once you have checked everything was working correctly again**.\n\n-------------------------\nQuick help for developers\n-------------------------\n\nTo create an egg which is compatible with Distribute, use the same\npractice as with Setuptools, e.g.::\n\n    from setuptools import setup\n\n    setup(...\n    )\n\nTo use `pkg_resources` to access data files in the egg, you should\nrequire the Setuptools distribution explicitly::\n\n    from setuptools import setup\n\n    setup(...\n        install_requires=[\'setuptools\']\n    )\n\nOnly if you need Distribute-specific functionality should you depend\non it explicitly. In this case, replace the Setuptools dependency::\n\n    from setuptools import setup\n\n    setup(...\n        install_requires=[\'distribute\']\n    )\n\n-----------\nInstall FAQ\n-----------\n\n- **Why is Distribute wrapping my Setuptools installation?**\n\n   Since Distribute is a fork, and since it provides the same package\n   and modules, it renames the existing Setuptools egg and inserts a\n   new one which merely wraps the Distribute code. This way, full\n   backwards compatibility is kept for packages which rely on the\n   Setuptools modules.\n\n   At the same time, packages can meet their dependency on Setuptools\n   without actually installing it (which would disable Distribute).\n\n- **How does Distribute interact with virtualenv?**\n\n  Everytime you create a virtualenv it will install setuptools by default.\n  You either need to re-install Distribute in it right after or pass the\n  ``--distribute`` option when creating it.\n\n  Once installed, your virtualenv will use Distribute transparently.\n\n  Although, if you have Setuptools installed in your system-wide Python,\n  and if the virtualenv you are in was generated without the `--no-site-packages`\n  option, the Distribute installation will stop.\n\n  You need in this case to build a virtualenv with the `--no-site-packages`\n  option or to install `Distribute` globally.\n\n- **How does Distribute interacts with zc.buildout?**\n\n  You can use Distribute in your zc.buildout, with the --distribute option,\n  starting at zc.buildout 1.4.2::\n\n  $ python bootstrap.py --distribute\n\n  For previous zc.buildout versions, *the only thing* you need to do\n  is use the bootstrap at `http://python-distribute.org/bootstrap.py`.  Run\n  that bootstrap and ``bin/buildout`` (and all other buildout-generated\n  scripts) will transparently use distribute instead of setuptools.  You do\n  not need a specific buildout release.\n\n  A shared eggs directory is no problem (since 0.6.6): the setuptools egg is\n  left in place unmodified.  So other buildouts that do not yet use the new\n  bootstrap continue to work just fine.  And there is no need to list\n  ``distribute`` somewhere in your eggs: using the bootstrap is enough.\n\n  The source code for the bootstrap script is located at\n  `http://bitbucket.org/tarek/buildout-distribute`.\n\n\n\n-----------------------------\nFeedback and getting involved\n-----------------------------\n\n- Mailing list: http://mail.python.org/mailman/listinfo/distutils-sig\n- Issue tracker: http://bitbucket.org/tarek/distribute/issues/\n- Code Repository: http://bitbucket.org/tarek/distribute\n\n=======\nCHANGES\n=======\n\n------\n0.6.27\n------\n\n* Support current snapshots of CPython 3.3.\n* Distribute now recognizes README.rst as a standard, default readme file.\n* Exclude \'encodings\' modules when removing modules from sys.modules.\n  Workaround for \\#285.\n* Issue \\#231: Don\'t fiddle with system python when used with buildout\n  (bootstrap.py)\n\n------\n0.6.26\n------\n\n* Issue \\#183: Symlinked files are now extracted from source distributions.\n* Issue \\#227: Easy_install fetch parameters are now passed during the\n  installation of a source distribution; now fulfillment of setup_requires\n  dependencies will honor the parameters passed to easy_install.\n\n------\n0.6.25\n------\n\n* Issue \\#258: Workaround a cache issue\n* Issue \\#260: distribute_setup.py now accepts the --user parameter for\n  Python 2.6 and later.\n* Issue \\#262: package_index.open_with_auth no longer throws LookupError\n  on Python 3.\n* Issue \\#269: AttributeError when an exception occurs reading Manifest.in\n  on late releases of Python.\n* Issue \\#272: Prevent TypeError when namespace package names are unicode\n  and single-install-externally-managed is used. Also fixes PIP issue\n  449.\n* Issue \\#273: Legacy script launchers now install with Python2/3 support.\n\n------\n0.6.24\n------\n\n* Issue \\#249: Added options to exclude 2to3 fixers\n\n------\n0.6.23\n------\n\n* Issue \\#244: Fixed a test\n* Issue \\#243: Fixed a test\n* Issue \\#239: Fixed a test\n* Issue \\#240: Fixed a test\n* Issue \\#241: Fixed a test\n* Issue \\#237: Fixed a test\n* Issue \\#238: easy_install now uses 64bit executable wrappers on 64bit Python\n* Issue \\#208: Fixed parsed_versions, it now honors post-releases as noted in the documentation\n* Issue \\#207: Windows cli and gui wrappers pass CTRL-C to child python process\n* Issue \\#227: easy_install now passes its arguments to setup.py bdist_egg\n* Issue \\#225: Fixed a NameError on Python 2.5, 2.4\n\n------\n0.6.21\n------\n\n* Issue \\#225: FIxed a regression on py2.4\n\n------\n0.6.20\n------\n\n* Issue \\#135: Include url in warning when processing URLs in package_index.\n* Issue \\#212: Fix issue where easy_instal fails on Python 3 on windows installer.\n* Issue \\#213: Fix typo in documentation.\n\n------\n0.6.19\n------\n\n* Issue 206: AttributeError: \'HTTPMessage\' object has no attribute \'getheaders\'\n\n------\n0.6.18\n------\n\n* Issue 210: Fixed a regression introduced by Issue 204 fix.\n\n------\n0.6.17\n------\n\n* Support \'DISTRIBUTE_DISABLE_VERSIONED_EASY_INSTALL_SCRIPT\' environment\n  variable to allow to disable installation of easy_install-${version} script.\n* Support Python >=3.1.4 and >=3.2.1.\n* Issue 204: Don\'t try to import the parent of a namespace package in\n  declare_namespace\n* Issue 196: Tolerate responses with multiple Content-Length headers\n* Issue 205: Sandboxing doesn\'t preserve working_set. Leads to setup_requires\n  problems.\n\n------\n0.6.16\n------\n\n* Builds sdist gztar even on Windows (avoiding Issue 193).\n* Issue 192: Fixed metadata omitted on Windows when package_dir\n  specified with forward-slash.\n* Issue 195: Cython build support.\n* Issue 200: Issues with recognizing 64-bit packages on Windows.\n\n------\n0.6.15\n------\n\n* Fixed typo in bdist_egg\n* Several issues under Python 3 has been solved.\n* Issue 146: Fixed missing DLL files after easy_install of windows exe package.\n\n------\n0.6.14\n------\n\n* Issue 170: Fixed unittest failure. Thanks to Toshio.\n* Issue 171: Fixed race condition in unittests cause deadlocks in test suite.\n* Issue 143: Fixed a lookup issue with easy_install.\n  Thanks to David and Zooko.\n* Issue 174: Fixed the edit mode when its used with setuptools itself\n\n------\n0.6.13\n------\n\n* Issue 160: 2.7 gives ValueError("Invalid IPv6 URL")\n* Issue 150: Fixed using ~/.local even in a --no-site-packages virtualenv\n* Issue 163: scan index links before external links, and don\'t use the md5 when\n  comparing two distributions\n\n------\n0.6.12\n------\n\n* Issue 149: Fixed various failures on 2.3/2.4\n\n------\n0.6.11\n------\n\n* Found another case of SandboxViolation - fixed\n* Issue 15 and 48: Introduced a socket timeout of 15 seconds on url openings\n* Added indexsidebar.html into MANIFEST.in\n* Issue 108: Fixed TypeError with Python3.1\n* Issue 121: Fixed --help install command trying to actually install.\n* Issue 112: Added an os.makedirs so that Tarek\'s solution will work.\n* Issue 133: Added --no-find-links to easy_install\n* Added easy_install --user\n* Issue 100: Fixed develop --user not taking \'.\' in PYTHONPATH into account\n* Issue 134: removed spurious UserWarnings. Patch by VanLindberg\n* Issue 138: cant_write_to_target error when setup_requires is used.\n* Issue 147: respect the sys.dont_write_bytecode flag\n\n------\n0.6.10\n------\n\n* Reverted change made for the DistributionNotFound exception because\n  zc.buildout uses the exception message to get the name of the\n  distribution.\n\n-----\n0.6.9\n-----\n\n* Issue 90: unknown setuptools version can be added in the working set\n* Issue 87: setupt.py doesn\'t try to convert distribute_setup.py anymore\n  Initial Patch by arfrever.\n* Issue 89: added a side bar with a download link to the doc.\n* Issue 86: fixed missing sentence in pkg_resources doc.\n* Added a nicer error message when a DistributionNotFound is raised.\n* Issue 80: test_develop now works with Python 3.1\n* Issue 93: upload_docs now works if there is an empty sub-directory.\n* Issue 70: exec bit on non-exec files\n* Issue 99: now the standalone easy_install command doesn\'t uses a\n  "setup.cfg" if any exists in the working directory. It will use it\n  only if triggered by ``install_requires`` from a setup.py call\n  (install, develop, etc).\n* Issue 101: Allowing ``os.devnull`` in Sandbox\n* Issue 92: Fixed the "no eggs" found error with MacPort\n  (platform.mac_ver() fails)\n* Issue 103: test_get_script_header_jython_workaround not run\n  anymore under py3 with C or POSIX local. Contributed by Arfrever.\n* Issue 104: remvoved the assertion when the installation fails,\n  with a nicer message for the end user.\n* Issue 100: making sure there\'s no SandboxViolation when\n  the setup script patches setuptools.\n\n-----\n0.6.8\n-----\n\n* Added "check_packages" in dist. (added in Setuptools 0.6c11)\n* Fixed the DONT_PATCH_SETUPTOOLS state.\n\n-----\n0.6.7\n-----\n\n* Issue 58: Added --user support to the develop command\n* Issue 11: Generated scripts now wrap their call to the script entry point\n  in the standard "if name == \'main\'"\n* Added the \'DONT_PATCH_SETUPTOOLS\' environment variable, so virtualenv\n  can drive an installation that doesn\'t patch a global setuptools.\n* Reviewed unladen-swallow specific change from\n  http://code.google.com/p/unladen-swallow/source/detail?spec=svn875&r=719\n  and determined that it no longer applies. Distribute should work fine with\n  Unladen Swallow 2009Q3.\n* Issue 21: Allow PackageIndex.open_url to gracefully handle all cases of a\n  httplib.HTTPException instead of just InvalidURL and BadStatusLine.\n* Removed virtual-python.py from this distribution and updated documentation\n  to point to the actively maintained virtualenv instead.\n* Issue 64: use_setuptools no longer rebuilds the distribute egg every\n  time it is run\n* use_setuptools now properly respects the requested version\n* use_setuptools will no longer try to import a distribute egg for the\n  wrong Python version\n* Issue 74: no_fake should be True by default.\n* Issue 72: avoid a bootstrapping issue with easy_install -U\n\n-----\n0.6.6\n-----\n\n* Unified the bootstrap file so it works on both py2.x and py3k without 2to3\n  (patch by Holger Krekel)\n\n-----\n0.6.5\n-----\n\n* Issue 65: cli.exe and gui.exe are now generated at build time,\n  depending on the platform in use.\n\n* Issue 67: Fixed doc typo (PEP 381/382)\n\n* Distribute no longer shadows setuptools if we require a 0.7-series\n  setuptools.  And an error is raised when installing a 0.7 setuptools with\n  distribute.\n\n* When run from within buildout, no attempt is made to modify an existing\n  setuptools egg, whether in a shared egg directory or a system setuptools.\n\n* Fixed a hole in sandboxing allowing builtin file to write outside of\n  the sandbox.\n\n-----\n0.6.4\n-----\n\n* Added the generation of `distribute_setup_3k.py` during the release.\n  This close http://bitbucket.org/tarek/distribute/issue/52.\n\n* Added an upload_docs command to easily upload project documentation to\n  PyPI\'s http://packages.python.org.\n  This close http://bitbucket.org/tarek/distribute/issue/56.\n\n* Fixed a bootstrap bug on the use_setuptools() API.\n\n-----\n0.6.3\n-----\n\nsetuptools\n==========\n\n* Fixed a bunch of calls to file() that caused crashes on Python 3.\n\nbootstrapping\n=============\n\n* Fixed a bug in sorting that caused bootstrap to fail on Python 3.\n\n-----\n0.6.2\n-----\n\nsetuptools\n==========\n\n* Added Python 3 support; see docs/python3.txt.\n  This closes http://bugs.python.org/setuptools/issue39.\n\n* Added option to run 2to3 automatically when installing on Python 3.\n  This closes http://bitbucket.org/tarek/distribute/issue/31.\n\n* Fixed invalid usage of requirement.parse, that broke develop -d.\n  This closes http://bugs.python.org/setuptools/issue44.\n\n* Fixed script launcher for 64-bit Windows.\n  This closes http://bugs.python.org/setuptools/issue2.\n\n* KeyError when compiling extensions.\n  This closes http://bugs.python.org/setuptools/issue41.\n\nbootstrapping\n=============\n\n* Fixed bootstrap not working on Windows.\n  This closes http://bitbucket.org/tarek/distribute/issue/49.\n\n* Fixed 2.6 dependencies.\n  This closes http://bitbucket.org/tarek/distribute/issue/50.\n\n* Make sure setuptools is patched when running through easy_install\n  This closes http://bugs.python.org/setuptools/issue40.\n\n-----\n0.6.1\n-----\n\nsetuptools\n==========\n\n* package_index.urlopen now catches BadStatusLine and malformed url errors.\n  This closes http://bitbucket.org/tarek/distribute/issue/16 and\n  http://bitbucket.org/tarek/distribute/issue/18.\n\n* zip_ok is now False by default. This closes\n  http://bugs.python.org/setuptools/issue33.\n\n* Fixed invalid URL error catching. http://bugs.python.org/setuptools/issue20.\n\n* Fixed invalid bootstraping with easy_install installation\n  http://bitbucket.org/tarek/distribute/issue/40.\n  Thanks to Florian Schulze for the help.\n\n* Removed buildout/bootstrap.py. A new repository will create a specific\n  bootstrap.py script.\n\n\nbootstrapping\n=============\n\n* The boostrap process leave setuptools alone if detected in the system\n  and --root or --prefix is provided, but is not in the same location.\n  This closes http://bitbucket.org/tarek/distribute/issue/10.\n\n---\n0.6\n---\n\nsetuptools\n==========\n\n* Packages required at build time where not fully present at install time.\n  This closes http://bitbucket.org/tarek/distribute/issue/12.\n\n* Protected against failures in tarfile extraction. This closes\n  http://bitbucket.org/tarek/distribute/issue/10.\n\n* Made Jython api_tests.txt doctest compatible. This closes\n  http://bitbucket.org/tarek/distribute/issue/7.\n\n* sandbox.py replaced builtin type file with builtin function open. This\n  closes http://bitbucket.org/tarek/distribute/issue/6.\n\n* Immediately close all file handles. This closes\n  http://bitbucket.org/tarek/distribute/issue/3.\n\n* Added compatibility with Subversion 1.6. This references\n  http://bitbucket.org/tarek/distribute/issue/1.\n\npkg_resources\n=============\n\n* Avoid a call to /usr/bin/sw_vers on OSX and use the official platform API\n  instead. Based on a patch from ronaldoussoren. This closes\n  http://bitbucket.org/tarek/distribute/issue/5.\n\n* Fixed a SandboxViolation for mkdir that could occur in certain cases.\n  This closes http://bitbucket.org/tarek/distribute/issue/13.\n\n* Allow to find_on_path on systems with tight permissions to fail gracefully.\n  This closes http://bitbucket.org/tarek/distribute/issue/9.\n\n* Corrected inconsistency between documentation and code of add_entry.\n  This closes http://bitbucket.org/tarek/distribute/issue/8.\n\n* Immediately close all file handles. This closes\n  http://bitbucket.org/tarek/distribute/issue/3.\n\neasy_install\n============\n\n* Immediately close all file handles. This closes\n  http://bitbucket.org/tarek/distribute/issue/3.\n',
 'download_url': 'UNKNOWN',
 'executables': {},
 'extra_source_files': [],
 'flag_options': {},
 'hook_files': [],
 'libraries': {},
 'license': 'PSF or ZPL',
 'maintainer': 'The fellowship of the packaging',
 'maintainer_email': 'distutils-sig@python.org',
 'name': 'distribute',
 'path_options': {},
 'summary': 'Easily download, build, install, upgrade, and uninstall Python packages',
 'url': 'http://packages.python.org/distribute',
 'version': '0.6.27'}

########NEW FILE########
__FILENAME__ = data_jinja2
ref = {'author': 'Armin Ronacher',
 'author_email': 'armin.ronacher@active-4.com',
 'classifiers': ['Development Status :: 5 - Production/Stable',
                 'Environment :: Web Environment',
                 'Intended Audience :: Developers',
                 'License :: OSI Approved :: BSD License',
                 'Operating System :: OS Independent',
                 'Programming Language :: Python',
                 'Topic :: Internet :: WWW/HTTP :: Dynamic Content',
                 'Topic :: Software Development :: Libraries :: Python Modules',
                 'Topic :: Text Processing :: Markup :: HTML'],
 'data_files': {},
 'description': 'Jinja2\n~~~~~~\n\nJinja2 is a template engine written in pure Python.  It provides a\n`Django`_ inspired non-XML syntax but supports inline expressions and\nan optional `sandboxed`_ environment.\n\nNutshell\n--------\n\nHere a small example of a Jinja template::\n\n    {% extends \'base.html\' %}\n    {% block title %}Memberlist{% endblock %}\n    {% block content %}\n      <ul>\n      {% for user in users %}\n        <li><a href="{{ user.url }}">{{ user.username }}</a></li>\n      {% endfor %}\n      </ul>\n    {% endblock %}\n\nPhilosophy\n----------\n\nApplication logic is for the controller but don\'t try to make the life\nfor the template designer too hard by giving him too few functionality.\n\nFor more informations visit the new `Jinja2 webpage`_ and `documentation`_.\n\nThe `Jinja2 tip`_ is installable via `easy_install` with ``easy_install\nJinja2==dev``.\n\n.. _sandboxed: http://en.wikipedia.org/wiki/Sandbox_(computer_security)\n.. _Django: http://www.djangoproject.com/\n.. _Jinja2 webpage: http://jinja.pocoo.org/\n.. _documentation: http://jinja.pocoo.org/2/documentation/\n.. _Jinja2 tip: http://dev.pocoo.org/hg/jinja2-main/archive/tip.tar.gz\\#egg=Jinja2-dev',
 'download_url': 'UNKNOWN',
 'executables': {},
 'extra_source_files': ['AUTHORS',
                        'CHANGES',
                        'LICENSE',
                        'MANIFEST.in',
                        'Makefile',
                        'TODO',
                        'artwork/jinjalogo.svg',
                        'docs/Makefile',
                        'docs/_build/.ignore',
                        'docs/_build/html/.buildinfo',
                        'docs/_build/html/_sources/api.txt',
                        'docs/_build/html/_sources/changelog.txt',
                        'docs/_build/html/_sources/extensions.txt',
                        'docs/_build/html/_sources/faq.txt',
                        'docs/_build/html/_sources/index.txt',
                        'docs/_build/html/_sources/integration.txt',
                        'docs/_build/html/_sources/intro.txt',
                        'docs/_build/html/_sources/sandbox.txt',
                        'docs/_build/html/_sources/switching.txt',
                        'docs/_build/html/_sources/templates.txt',
                        'docs/_build/html/_sources/tricks.txt',
                        'docs/_build/html/_static/basic.css',
                        'docs/_build/html/_static/darkmetal.png',
                        'docs/_build/html/_static/default.css',
                        'docs/_build/html/_static/doctools.js',
                        'docs/_build/html/_static/file.png',
                        'docs/_build/html/_static/headerbg.png',
                        'docs/_build/html/_static/implementation.png',
                        'docs/_build/html/_static/jinja.js',
                        'docs/_build/html/_static/jinjabanner.png',
                        'docs/_build/html/_static/jquery.js',
                        'docs/_build/html/_static/metal.png',
                        'docs/_build/html/_static/minus.png',
                        'docs/_build/html/_static/navigation.png',
                        'docs/_build/html/_static/note.png',
                        'docs/_build/html/_static/plus.png',
                        'docs/_build/html/_static/print.css',
                        'docs/_build/html/_static/pygments.css',
                        'docs/_build/html/_static/searchtools.js',
                        'docs/_build/html/_static/style.css',
                        'docs/_build/html/_static/watermark.png',
                        'docs/_build/html/_static/watermark_blur.png',
                        'docs/_build/html/api.html',
                        'docs/_build/html/changelog.html',
                        'docs/_build/html/extensions.html',
                        'docs/_build/html/faq.html',
                        'docs/_build/html/genindex.html',
                        'docs/_build/html/index.html',
                        'docs/_build/html/integration.html',
                        'docs/_build/html/intro.html',
                        'docs/_build/html/objects.inv',
                        'docs/_build/html/sandbox.html',
                        'docs/_build/html/search.html',
                        'docs/_build/html/searchindex.js',
                        'docs/_build/html/switching.html',
                        'docs/_build/html/templates.html',
                        'docs/_build/html/tricks.html',
                        'docs/_static/.ignore',
                        'docs/_static/darkmetal.png',
                        'docs/_static/headerbg.png',
                        'docs/_static/implementation.png',
                        'docs/_static/jinja.js',
                        'docs/_static/jinjabanner.png',
                        'docs/_static/metal.png',
                        'docs/_static/navigation.png',
                        'docs/_static/note.png',
                        'docs/_static/print.css',
                        'docs/_static/style.css',
                        'docs/_static/watermark.png',
                        'docs/_static/watermark_blur.png',
                        'docs/_templates/.ignore',
                        'docs/_templates/genindex.html',
                        'docs/_templates/layout.html',
                        'docs/_templates/opensearch.xml',
                        'docs/_templates/page.html',
                        'docs/_templates/search.html',
                        'docs/api.rst',
                        'docs/cache_extension.py',
                        'docs/changelog.rst',
                        'docs/conf.py',
                        'docs/extensions.rst',
                        'docs/faq.rst',
                        'docs/index.rst',
                        'docs/integration.rst',
                        'docs/intro.rst',
                        'docs/jinjaext.py',
                        'docs/jinjaext.pyc',
                        'docs/sandbox.rst',
                        'docs/switching.rst',
                        'docs/templates.rst',
                        'docs/tricks.rst',
                        'examples/basic/cycle.py',
                        'examples/basic/debugger.py',
                        'examples/basic/inheritance.py',
                        'examples/basic/templates/broken.html',
                        'examples/basic/templates/subbroken.html',
                        'examples/basic/test.py',
                        'examples/basic/test_filter_and_linestatements.py',
                        'examples/basic/test_loop_filter.py',
                        'examples/basic/translate.py',
                        'examples/bench.py',
                        'examples/profile.py',
                        'examples/rwbench/django/_form.html',
                        'examples/rwbench/django/_input_field.html',
                        'examples/rwbench/django/_textarea.html',
                        'examples/rwbench/django/index.html',
                        'examples/rwbench/django/layout.html',
                        'examples/rwbench/djangoext.py',
                        'examples/rwbench/djangoext.pyc',
                        'examples/rwbench/genshi/helpers.html',
                        'examples/rwbench/genshi/index.html',
                        'examples/rwbench/genshi/layout.html',
                        'examples/rwbench/jinja/helpers.html',
                        'examples/rwbench/jinja/index.html',
                        'examples/rwbench/jinja/layout.html',
                        'examples/rwbench/mako/helpers.html',
                        'examples/rwbench/mako/index.html',
                        'examples/rwbench/mako/layout.html',
                        'examples/rwbench/rwbench.py',
                        'examples/rwbench/rwbench.pyc',
                        'ext/JinjaTemplates.tmbundle.tar.gz',
                        'ext/Vim/htmljinja.vim',
                        'ext/Vim/jinja.vim',
                        'ext/django2jinja/django2jinja.py',
                        'ext/django2jinja/example.py',
                        'ext/django2jinja/templates/index.html',
                        'ext/django2jinja/templates/layout.html',
                        'ext/django2jinja/templates/subtemplate.html',
                        'ext/djangojinja2.py',
                        'ext/inlinegettext.py',
                        'ext/jinja.el',
                        'setup.cfg',
                        'setup.py',
                        'tests/loaderres/__init__$py.class',
                        'tests/loaderres/__init__.py',
                        'tests/loaderres/__init__.pyc',
                        'tests/loaderres/templates/broken.html',
                        'tests/loaderres/templates/foo/test.html',
                        'tests/loaderres/templates/syntaxerror.html',
                        'tests/loaderres/templates/test.html',
                        'tests/test_debug$py.class',
                        'tests/test_debug.py',
                        'tests/test_debug.pyc',
                        'tests/test_ext$py.class',
                        'tests/test_ext.py',
                        'tests/test_ext.pyc',
                        'tests/test_filters$py.class',
                        'tests/test_filters.py',
                        'tests/test_filters.pyc',
                        'tests/test_forloop$py.class',
                        'tests/test_forloop.py',
                        'tests/test_forloop.pyc',
                        'tests/test_heavy$py.class',
                        'tests/test_heavy.py',
                        'tests/test_heavy.pyc',
                        'tests/test_i18n$py.class',
                        'tests/test_i18n.py',
                        'tests/test_i18n.pyc',
                        'tests/test_ifcondition$py.class',
                        'tests/test_ifcondition.py',
                        'tests/test_ifcondition.pyc',
                        'tests/test_imports$py.class',
                        'tests/test_imports.py',
                        'tests/test_imports.pyc',
                        'tests/test_inheritance$py.class',
                        'tests/test_inheritance.py',
                        'tests/test_inheritance.pyc',
                        'tests/test_lexer$py.class',
                        'tests/test_lexer.py',
                        'tests/test_lexer.pyc',
                        'tests/test_loaders$py.class',
                        'tests/test_loaders.py',
                        'tests/test_loaders.pyc',
                        'tests/test_lrucache$py.class',
                        'tests/test_lrucache.py',
                        'tests/test_lrucache.pyc',
                        'tests/test_macros$py.class',
                        'tests/test_macros.py',
                        'tests/test_macros.pyc',
                        'tests/test_meta$py.class',
                        'tests/test_meta.py',
                        'tests/test_meta.pyc',
                        'tests/test_old_bugs$py.class',
                        'tests/test_old_bugs.py',
                        'tests/test_old_bugs.pyc',
                        'tests/test_parser$py.class',
                        'tests/test_parser.py',
                        'tests/test_parser.pyc',
                        'tests/test_security$py.class',
                        'tests/test_security.py',
                        'tests/test_security.pyc',
                        'tests/test_streaming$py.class',
                        'tests/test_streaming.py',
                        'tests/test_streaming.pyc',
                        'tests/test_syntax$py.class',
                        'tests/test_syntax.py',
                        'tests/test_syntax.pyc',
                        'tests/test_tests$py.class',
                        'tests/test_tests.py',
                        'tests/test_tests.pyc',
                        'tests/test_undefined$py.class',
                        'tests/test_undefined.py',
                        'tests/test_undefined.pyc',
                        'tests/test_various$py.class',
                        'tests/test_various.py',
                        'tests/test_various.pyc'],
 'flag_options': {},
 'hook_files': [],
 'libraries': {'default': {'build_requires': [],
                           'compiled_libraries': {},
                           'extensions': {},
                           'install_requires': [],
                           'name': 'default',
                           'packages': ['jinja2'],
                           'py_modules': [],
                           'sub_directory': None}},
 'license': 'BSD',
 'maintainer': 'Armin Ronacher',
 'maintainer_email': 'armin.ronacher@active-4.com',
 'name': 'Jinja2',
 'path_options': {'gendatadir': {'default': '$sitedir',
                                 'description': 'Directory for datafiles obtained from distutils conversion',
                                 'name': 'gendatadir'}},
 'platforms': ['UNKNOWN'],
 'summary': 'A small but fast and easy to use stand-alone template engine written in pure python.',
 'url': 'http://jinja.pocoo.org/',
 'version': '2.2.1'}

########NEW FILE########
__FILENAME__ = data_sphinx
ref = {'author': 'Georg Brandl',
 'author_email': 'georg@python.org',
 'classifiers': ['Development Status :: 4 - Beta',
                 'Environment :: Console',
                 'Environment :: Web Environment',
                 'Intended Audience :: Developers',
                 'License :: OSI Approved :: BSD License',
                 'Operating System :: OS Independent',
                 'Programming Language :: Python',
                 'Topic :: Documentation',
                 'Topic :: Utilities'],
 'data_files': {'gendata_sphinx': {'files': ['texinputs/Makefile',
                                             'texinputs/fncychap.sty',
                                             'texinputs/howto.cls',
                                             'texinputs/manual.cls',
                                             'texinputs/python.ist',
                                             'texinputs/sphinx.sty',
                                             'texinputs/tabulary.sty',
                                             'themes/basic/defindex.html',
                                             'themes/basic/genindex-single.html',
                                             'themes/basic/genindex-split.html',
                                             'themes/basic/genindex.html',
                                             'themes/basic/layout.html',
                                             'themes/basic/modindex.html',
                                             'themes/basic/opensearch.xml',
                                             'themes/basic/page.html',
                                             'themes/basic/search.html',
                                             'themes/basic/theme.conf',
                                             'themes/basic/changes/frameset.html',
                                             'themes/basic/changes/rstsource.html',
                                             'themes/basic/changes/versionchanges.html',
                                             'themes/basic/static/basic.css',
                                             'themes/basic/static/doctools.js',
                                             'themes/basic/static/file.png',
                                             'themes/basic/static/jquery.js',
                                             'themes/basic/static/minus.png',
                                             'themes/basic/static/plus.png',
                                             'themes/basic/static/searchtools.js',
                                             'themes/default/theme.conf',
                                             'themes/default/static/default.css_t',
                                             'themes/sphinxdoc/layout.html',
                                             'themes/sphinxdoc/theme.conf',
                                             'themes/sphinxdoc/static/contents.png',
                                             'themes/sphinxdoc/static/navigation.png',
                                             'themes/sphinxdoc/static/sphinxdoc.css',
                                             'themes/traditional/theme.conf',
                                             'themes/traditional/static/traditional.css'],
                                   'name': 'gendata_sphinx',
                                   'source_dir': 'sphinx',
                                   'target_dir': '$gendatadir/sphinx'},
                'gendata_sphinx_ext_autosummary': {'files': ['templates/module'],
                                                   'name': 'gendata_sphinx_ext_autosummary',
                                                   'source_dir': 'sphinx/ext/autosummary',
                                                   'target_dir': '$gendatadir/sphinx/ext/autosummary'},
                'gendata_sphinx_locale': {'files': ['sphinx.pot',
                                                    'cs/LC_MESSAGES/sphinx.js',
                                                    'cs/LC_MESSAGES/sphinx.mo',
                                                    'cs/LC_MESSAGES/sphinx.po',
                                                    'de/LC_MESSAGES/sphinx.js',
                                                    'de/LC_MESSAGES/sphinx.mo',
                                                    'de/LC_MESSAGES/sphinx.po',
                                                    'es/LC_MESSAGES/sphinx.js',
                                                    'es/LC_MESSAGES/sphinx.mo',
                                                    'es/LC_MESSAGES/sphinx.po',
                                                    'fi/LC_MESSAGES/sphinx.js',
                                                    'fi/LC_MESSAGES/sphinx.mo',
                                                    'fi/LC_MESSAGES/sphinx.po',
                                                    'fr/LC_MESSAGES/sphinx.js',
                                                    'fr/LC_MESSAGES/sphinx.mo',
                                                    'fr/LC_MESSAGES/sphinx.po',
                                                    'it/LC_MESSAGES/sphinx.js',
                                                    'it/LC_MESSAGES/sphinx.mo',
                                                    'it/LC_MESSAGES/sphinx.po',
                                                    'ja/LC_MESSAGES/sphinx.js',
                                                    'ja/LC_MESSAGES/sphinx.mo',
                                                    'ja/LC_MESSAGES/sphinx.po',
                                                    'nl/LC_MESSAGES/sphinx.js',
                                                    'nl/LC_MESSAGES/sphinx.mo',
                                                    'nl/LC_MESSAGES/sphinx.po',
                                                    'pl/LC_MESSAGES/sphinx.js',
                                                    'pl/LC_MESSAGES/sphinx.mo',
                                                    'pl/LC_MESSAGES/sphinx.po',
                                                    'pt_BR/LC_MESSAGES/sphinx.js',
                                                    'pt_BR/LC_MESSAGES/sphinx.mo',
                                                    'pt_BR/LC_MESSAGES/sphinx.po',
                                                    'ru/LC_MESSAGES/sphinx.js',
                                                    'ru/LC_MESSAGES/sphinx.mo',
                                                    'ru/LC_MESSAGES/sphinx.po',
                                                    'sl/LC_MESSAGES/sphinx.js',
                                                    'sl/LC_MESSAGES/sphinx.mo',
                                                    'sl/LC_MESSAGES/sphinx.po',
                                                    'uk_UA/LC_MESSAGES/sphinx.js',
                                                    'uk_UA/LC_MESSAGES/sphinx.mo',
                                                    'uk_UA/LC_MESSAGES/sphinx.po',
                                                    'zh_TW/LC_MESSAGES/sphinx.js',
                                                    'zh_TW/LC_MESSAGES/sphinx.mo',
                                                    'zh_TW/LC_MESSAGES/sphinx.po'],
                                          'name': 'gendata_sphinx_locale',
                                          'source_dir': 'sphinx/locale',
                                          'target_dir': '$gendatadir/sphinx/locale'},
                'gendata_sphinx_pycode': {'files': ['Grammar.txt'],
                                          'name': 'gendata_sphinx_pycode',
                                          'source_dir': 'sphinx/pycode',
                                          'target_dir': '$gendatadir/sphinx/pycode'}},
 'description': 'Sphinx is a tool that makes it easy to create intelligent and beautiful\ndocumentation for Python projects (or other documents consisting of\nmultiple reStructuredText sources), written by Georg Brandl.\nIt was originally created to translate the new Python documentation,\nbut has now been cleaned up in the hope that it will be useful to many\nother projects.\n\nSphinx uses reStructuredText as its markup language, and many of its strengths\ncome from the power and straightforwardness of reStructuredText and its\nparsing and translating suite, the Docutils.\n\nAlthough it is still under constant development, the following features\nare already present, work fine and can be seen "in action" in the Python docs:\n\n* Output formats: HTML (including Windows HTML Help), plain text and LaTeX,\n  for printable PDF versions\n* Extensive cross-references: semantic markup and automatic links\n  for functions, classes, glossary terms and similar pieces of information\n* Hierarchical structure: easy definition of a document tree, with automatic\n  links to siblings, parents and children\n* Automatic indices: general index as well as a module index\n* Code handling: automatic highlighting using the Pygments highlighter\n* Various extensions are available, e.g. for automatic testing of snippets\n  and inclusion of appropriately formatted docstrings.\n\nA development egg can be found `here\n<http://bitbucket.org/birkenfeld/sphinx/get/tip.gz\\#egg=Sphinx-dev>`_.',
 'download_url': 'http://pypi.python.org/pypi/Sphinx',
 'executables': {'sphinx-autogen': {'function': 'main',
                                    'module': 'sphinx.ext.autosummary.generate',
                                    'name': 'sphinx-autogen'},
                 'sphinx-build': {'function': 'main',
                                  'module': 'sphinx',
                                  'name': 'sphinx-build'},
                 'sphinx-quickstart': {'function': 'main',
                                       'module': 'sphinx.quickstart',
                                       'name': 'sphinx-quickstart'}},
 'extra_source_files': ['AUTHORS',
                        'CHANGES',
                        'EXAMPLES',
                        'LICENSE',
                        'Makefile',
                        'README',
                        'TODO',
                        'babel.cfg',
                        'doc/Makefile',
                        'doc/_static/sphinx.png',
                        'doc/_templates/index.html',
                        'doc/_templates/indexsidebar.html',
                        'doc/_templates/layout.html',
                        'doc/builders.rst',
                        'doc/changes.rst',
                        'doc/concepts.rst',
                        'doc/conf.py',
                        'doc/config.rst',
                        'doc/contents.rst',
                        'doc/examples.rst',
                        'doc/ext/appapi.rst',
                        'doc/ext/autodoc.rst',
                        'doc/ext/autosummary.rst',
                        'doc/ext/builderapi.rst',
                        'doc/ext/coverage.rst',
                        'doc/ext/doctest.rst',
                        'doc/ext/graphviz.rst',
                        'doc/ext/ifconfig.rst',
                        'doc/ext/inheritance.rst',
                        'doc/ext/intersphinx.rst',
                        'doc/ext/math.rst',
                        'doc/ext/refcounting.rst',
                        'doc/ext/todo.rst',
                        'doc/ext/tutorial.rst',
                        'doc/extensions.rst',
                        'doc/faq.rst',
                        'doc/glossary.rst',
                        'doc/intro.rst',
                        'doc/markup/code.rst',
                        'doc/markup/desc.rst',
                        'doc/markup/index.rst',
                        'doc/markup/inline.rst',
                        'doc/markup/misc.rst',
                        'doc/markup/para.rst',
                        'doc/rest.rst',
                        'doc/sphinx-build.1',
                        'doc/sphinx-quickstart.1',
                        'doc/templating.rst',
                        'doc/theming.rst',
                        'ez_setup.py',
                        'setup.cfg',
                        'setup.py',
                        'sphinx-autogen.py',
                        'sphinx-build.py',
                        'sphinx-quickstart.py',
                        'tests/coverage.py',
                        'tests/etree13/ElementPath.py',
                        'tests/etree13/ElementTree.py',
                        'tests/etree13/HTMLTreeBuilder.py',
                        'tests/etree13/__init__.py',
                        'tests/path.py',
                        'tests/root/Makefile',
                        'tests/root/_static/README',
                        'tests/root/_templates/layout.html',
                        'tests/root/autodoc.txt',
                        'tests/root/autosummary.txt',
                        'tests/root/bom.txt',
                        'tests/root/conf.py',
                        'tests/root/contents.txt',
                        'tests/root/desc.txt',
                        'tests/root/ext.py',
                        'tests/root/images.txt',
                        'tests/root/img.gif',
                        'tests/root/img.pdf',
                        'tests/root/img.png',
                        'tests/root/includes.txt',
                        'tests/root/literal.inc',
                        'tests/root/markup.txt',
                        'tests/root/math.txt',
                        'tests/root/rimg.png',
                        'tests/root/special/api.h',
                        'tests/root/special/code.py',
                        'tests/root/subdir/images.txt',
                        'tests/root/subdir/img.png',
                        'tests/root/subdir/include.inc',
                        'tests/root/subdir/includes.txt',
                        'tests/root/subdir/simg.png',
                        'tests/root/svgimg.pdf',
                        'tests/root/svgimg.svg',
                        'tests/root/testtheme/layout.html',
                        'tests/root/testtheme/static/staticimg.png',
                        'tests/root/testtheme/static/statictmpl.html_t',
                        'tests/root/testtheme/theme.conf',
                        'tests/root/wrongenc.inc',
                        'tests/root/ziptheme.zip',
                        'tests/run.py',
                        'tests/test_application.py',
                        'tests/test_autodoc.py',
                        'tests/test_build.py',
                        'tests/test_config.py',
                        'tests/test_coverage.py',
                        'tests/test_env.py',
                        'tests/test_highlighting.py',
                        'tests/test_i18n.py',
                        'tests/test_markup.py',
                        'tests/test_quickstart.py',
                        'tests/test_search.py',
                        'tests/test_theming.py',
                        'tests/util.py',
                        'utils/check_sources.py',
                        'utils/pylintrc',
                        'utils/reindent.py'],
 'flag_options': {},
 'hook_files': [],
 'libraries': {'default': {'build_requires': [],
                           'compiled_libraries': {},
                           'extensions': {},
                           'install_requires': ['Pygments>=0.8',
                                                'Jinja2>=2.1',
                                                'docutils>=0.4'],
                           'name': 'default',
                           'packages': ['sphinx',
                                        'sphinx.builders',
                                        'sphinx.directives',
                                        'sphinx.ext',
                                        'sphinx.ext.autosummary',
                                        'sphinx.locale',
                                        'sphinx.pycode',
                                        'sphinx.pycode.pgen2',
                                        'sphinx.util',
                                        'sphinx.writers'],
                           'py_modules': [],
                           'sub_directory': None}},
 'license': 'BSD',
 'maintainer': 'Georg Brandl',
 'maintainer_email': 'georg@python.org',
 'name': 'Sphinx',
 'path_options': {'gendatadir': {'default': '$sitedir',
                                 'description': 'Directory for dataFiles obtained from distutils conversion',
                                 'name': 'gendatadir'}},
 'platforms': ['any'],
 'summary': 'Python documentation generator',
 'url': 'http://sphinx.pocoo.org/',
 'version': '0.6.3'}

########NEW FILE########
__FILENAME__ = generate
import sys

import os.path as op

from pprint \
    import \
        pprint

from bento.parser.nodes import ast_walk
from bento.parser.visitor import Dispatcher
from bento.parser.parser import parse

if __name__ == "__main__":
    if len(sys.argv) > 1:
        arg = sys.argv[1]
        data = open(arg).read()
    else:
        raise ValueError("Usage: generate foo.info")

    base, _ = op.splitext(op.basename(arg))
    base = "data_" + base
    py_module  = op.join(op.dirname(arg), base + ".py")
    p = parse(data)
    dispatcher = Dispatcher()
    res = ast_walk(p, dispatcher)
    with open(py_module, "w") as fid:
        fid.write("ref = ")
        pprint(res, fid)

########NEW FILE########
__FILENAME__ = test
import os
import sys

from bento.compat.api.moves \
    import \
        unittest
from bento.parser.nodes \
    import \
        ast_walk
from bento.parser.visitor \
    import \
        Dispatcher
from bento.parser.parser \
    import \
        parse as _parse

def parse(data):
    p = _parse(data)
    dispatcher = Dispatcher()
    return ast_walk(p, dispatcher)

class TestPackages(unittest.TestCase):
    def _test_functional(self, root):
        info = os.path.join(os.path.dirname(__file__), root + ".info")

        saved_path = sys.path[:]
        try:
            sys.path.insert(0, os.path.dirname(__file__))
            m = __import__("data_%s" % root)

            tested = parse(open(info).read())
            self.assertEqual(tested, m.ref, "divergence for %s" % info)
        finally:
            sys.path = saved_path

    def test_sphinx(self):
        self._test_functional("sphinx")

    def test_jinja2(self):
        self._test_functional("jinja2")

    def test_distribute(self):
        self._test_functional("distribute")

########NEW FILE########
__FILENAME__ = simple_package
from bento.core import \
        PackageDescription
from bento.core.pkg_objects import \
        Extension

DESCR = """\
Name: Sphinx
Version: 0.6.3
Summary: Python documentation generator
Url: http://sphinx.pocoo.org/
DownloadUrl: http://pypi.python.org/pypi/Sphinx
Description: Some long description.
Author: Georg Brandl
AuthorEmail: georg@python.org
Maintainer: Georg Brandl
MaintainerEmail: georg@python.org
License: BSD
Platforms: any
Classifiers:
    Development Status :: 4 - Beta,
    Environment :: Console,
    Environment :: Web Environment,
    Intended Audience :: Developers,
    License :: OSI Approved :: BSD License,
    Operating System :: OS Independent,
    Programming Language :: Python,
    Topic :: Documentation,
    Topic :: Utilities

Library:
    Packages:
        sphinx,
        sphinx.builders
    Modules:
        cat.py
    Extension: _dog
        sources: src/dog.c
"""

PKG = PackageDescription(
    name="Sphinx",
    version="0.6.3",
    summary="Python documentation generator",
    url="http://sphinx.pocoo.org/",
    download_url="http://pypi.python.org/pypi/Sphinx",
    description="Some long description.",
    author="Georg Brandl",
    author_email="georg@python.org",
    maintainer="Georg Brandl",
    maintainer_email="georg@python.org",
    license="BSD",
    platforms=["any"],
    classifiers=[
        "Development Status :: 4 - Beta",
        "Environment :: Console",
        "Environment :: Web Environment",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: BSD License",
        "Operating System :: OS Independent",
        "Programming Language :: Python",
        "Topic :: Documentation",
        "Topic :: Utilities",],
    py_modules=["cat.py"],
    packages=["sphinx", "sphinx.builders"],
    extensions={"_dog": Extension(name="_dog", sources=["src/dog.c"])},
)

########NEW FILE########
__FILENAME__ = test_lexer
from bento.errors \
    import \
        ParseError
from bento.utils.utils \
    import \
        extract_exception, is_string
from bento.parser.lexer \
    import \
        BentoLexer

from bento.compat.api.moves import unittest

TestCase = unittest.TestCase

def split(s):
    ret = []
    for i in s.split(" "):
        ret.extend(i.splitlines())
    return ret

class TestLexer(TestCase):
    def setUp(self):
        self.lexer = BentoLexer()

    def _test(self, data, ref):
        self.lexer.input(data)
        self._test_impl(data, ref)

    def _test_impl(self, data, ref):
        res = []
        while True:
            tok = self.lexer.token()
            if not tok:
                break
            res.append(tok.type)

        if is_string(ref):
            ref = split(ref)
        try:
            self.assertEqual(res, ref)
        except AssertionError:
            e = extract_exception()
            cnt = 0
            for i, j in zip(res, ref):
                if not i == j:
                    break
                cnt += 1
            print("Break at index %d" % cnt)
            raise e

    def _get_tokens(self, data):
        self.lexer.input(data)
        return list(self.lexer)

# Test tokenizer stage before indentation generation
class TestLexerStageOne(TestLexer):
    def setUp(self):
        self.lexer = BentoLexer()

    def test_single_line(self):
        data = """\
Name: yo
"""
        ref = ["NAME_ID", "COLON", "WORD"]
        self._test(data, ref)

    def test_two_lines(self):
        data = """\
Name: yo
Summary: a brief summary
"""
        ref = ["NAME_ID", "COLON", "WORD",
               "SUMMARY_ID", "COLON", "STRING"]
        self._test(data, ref)

    def test_tab(self):
        data = """\
Library:
\tpackages
"""
        self.assertRaises(SyntaxError, lambda: self._test(data, []))

class TestLexerStageTwo(TestLexer):
    def setUp(self):
        self.lexer = BentoLexer()

    def test_simple(self):
        data = "yoyo"
        ref = "WORD"
        self._test(data, ref)

    def test_empty(self):
        data = ""
        ref = []
        self._test(data, ref)

    def test_simple_escape(self):
        data = "yoyo\ "
        ref = "WORD"
        self._test(data, ref)

        tokens = self._get_tokens(data)
        self.assertEqual(tokens[0].value, "yoyo ")

    def test_double_escape(self):
        data = "yoyo\\\\ "
        ref = "WORD"
        self._test(data, ref)

        tokens = self._get_tokens(data)
        self.assertEqual(tokens[0].value, "yoyo\\")

    def test_wrong_escape(self):
        data = "yoyo\\"
        def f():
            self._get_tokens(data)
        self.assertRaises(SyntaxError, f)

class TestLexerStageThree(TestLexer):
    def setUp(self):
        self.lexer = BentoLexer()

    def test_simple(self):
        data = "yoyo"
        ref = "WORD"
        self._test(data, ref)

    def test_empty(self):
        data = ""
        ref = []
        self._test(data, ref)

    def test_simple_escape(self):
        data = "yoyo\ "
        ref = "WORD"
        self._test(data, ref)

        tokens = self._get_tokens(data)
        self.assertEqual(tokens[0].value, "yoyo ")

    def test_simple_escape2(self):
        data = "\ yoyo\ "
        ref = "WORD"
        self._test(data, ref)

        tokens = self._get_tokens(data)
        self.assertEqual(tokens[0].value, " yoyo ")

    def test_double_escape(self):
        data = "yoyo\ \ yaya"
        ref = "WORD"
        self._test(data, ref)

        tokens = self._get_tokens(data)
        self.assertEqual(tokens[0].value, "yoyo  yaya")

    def test_literal_escape(self):
        data = "yoyo\\\\"
        ref = "WORD"
        self._test(data, ref)

        tokens = self._get_tokens(data)
        self.assertEqual(tokens[0].value, "yoyo\\")

class TestLexerStageFour(TestLexer):
    def setUp(self):
        self.lexer = BentoLexer()

    def test_single_line(self):
        data = """\
Name: yo
"""
        ref = ["NAME_ID", "COLON", "WORD"]
        self._test(data, ref)

    def test_two_lines(self):
        data = """\
Name: yo
Summary: a brief summary
"""
        ref = ["NAME_ID", "COLON", "WORD",
               "SUMMARY_ID", "COLON", "STRING"]
        self._test(data, ref)

    def test_simple_indent(self):
        data = """\
Packages:
    yo
"""
        ref = """\
PACKAGES_ID COLON INDENT WORD DEDENT
"""
        self._test(data, ref)

    def test_simple_indent2(self):
        data = """\
Packages:
    yo,
    yeah
"""
        ref = """\
PACKAGES_ID COLON
INDENT
WORD COMMA
WORD
DEDENT"""
        self._test(data, ref)

    def test_indent_newlines(self):
        data = """\
Description:
    some
    words

    and then more
"""
        ref = """\
DESCRIPTION_ID COLON INDENT MULTILINES_STRING DEDENT
"""
        self._test(data, ref)

    def test_double_indentation(self):
        data = """\
Description:
    some
     words
"""
        ref = """\
DESCRIPTION_ID COLON INDENT MULTILINES_STRING DEDENT
"""
        self._test(data, ref)

    def test_simple_dedent(self):
        data = """\
Packages:
    some
Name: words
"""
        ref = """\
PACKAGES_ID COLON INDENT WORD DEDENT
NAME_ID COLON WORD
"""
        self._test(data, ref)

    def test_simple_indent_dedent(self):
        data = """\
Library:
    Packages:
        yo
        yo.foo
    Modules:
        foo.py
    Extension: _bar
"""
        ref = """\
LIBRARY_ID COLON
INDENT
PACKAGES_ID COLON
INDENT
WORD
WORD
DEDENT
MODULES_ID COLON
INDENT
WORD
DEDENT
EXTENSION_ID COLON WORD
DEDENT
"""
        self._test(data, ref)

    def test_complex_indent(self):
        data = """\
Library:
    Packages:
        yo
        yo.foo
    Modules:
        foo.py
        bar.py
Extension: yeah
Extension: yeah2
"""
        ref = """\
LIBRARY_ID COLON
INDENT
PACKAGES_ID COLON
INDENT
WORD WORD
DEDENT
MODULES_ID COLON
INDENT
WORD
WORD
DEDENT DEDENT
EXTENSION_ID COLON WORD
EXTENSION_ID COLON WORD
"""
        self._test(data, ref)

    def test_indent_value(self):
        data = """\
Description: some
    words and whatnot
      .
"""
        ref = """\
DESCRIPTION_ID COLON
MULTILINES_STRING
"""
        self._test(data, ref)
        tokens = self._get_tokens(data)

        string = tokens[-1].value
        self.assertEqual(string, """\
some
words and whatnot
  .\
""")

    def test_indent_value2(self):
        data = """\
Description: some
    words and whatnot
      .
Name: yo
"""
        ref = """\
DESCRIPTION_ID COLON MULTILINES_STRING
NAME_ID COLON WORD
"""
        self._test(data, ref)

class TestMultilineString(TestLexer):
    def test_description_in_flag(self):
        data = """\
Flag: foo
    Default: true
    Description: yo mama

Library:
    if not flag(foo):
        Modules: foo.py
"""
        ref = """\
FLAG_ID COLON WORD
INDENT
DEFAULT_ID COLON WORD
DESCRIPTION_ID COLON STRING
DEDENT
LIBRARY_ID COLON
INDENT
IF NOT_OP FLAG_OP LPAR WORD RPAR COLON
INDENT
MODULES_ID COLON WORD
DEDENT
DEDENT
"""
        self._test(data, ref)

class TestLexerStageFive(TestLexer):
    def setUp(self):
        self.lexer = BentoLexer()

    def test_single_line(self):
        data = """\
Name: yo
"""
        ref = ["NAME_ID", "COLON", "WORD"]
        self._test(data, ref)

    def test_two_lines(self):
        data = """\
Name: yo
Summary: a brief summary
"""
        ref = ["NAME_ID", "COLON", "WORD",
               "SUMMARY_ID", "COLON", "STRING"]
        self._test(data, ref)
        tokens = self._get_tokens(data)
        self.assertEqual(tokens[-1].value, "a brief summary")

    def test_simple_indent(self):
        data = """\
Packages:
    yo
"""
        ref = ["PACKAGES_ID", "COLON",
               "INDENT", "WORD", "DEDENT"]
        self._test(data, ref)

    def test_simple_indent2(self):
        data = """\
Packages:
    yo,
    yeah
"""
        ref = ["PACKAGES_ID", "COLON",
               "INDENT",
               "WORD", "COMMA",
               "WORD",
               "DEDENT"]
        self._test(data, ref)

    def test_indent_newlines(self):
        data = """\
Description:
    some
    words

    and then more
"""
        ref = ["DESCRIPTION_ID", "COLON", "INDENT", "MULTILINES_STRING", "DEDENT"]
        self._test(data, ref)

    def test_double_indentation(self):
        data = """\
Description:
    some
     words
"""
        ref = ["DESCRIPTION_ID", "COLON",
               "INDENT", "MULTILINES_STRING",
               "DEDENT"]
        self._test(data, ref)

        tokens = self._get_tokens(data)
        string = tokens[-2].value
        self.assertEqual(string, """\
some
 words""")

    def test_simple_dedent(self):
        data = """\
Packages:
    some
Name: words
"""
        ref = ["PACKAGES_ID", "COLON",
               "INDENT", "WORD", "DEDENT",
               "NAME_ID", "COLON", "WORD"]
        self._test(data, ref)

    def test_simple_indent_dedent(self):
        data = """\
Library:
    Packages:
        yo
        yo.foo
    Modules:
        foo.py
    Extension: _bar
"""
        ref = ["LIBRARY_ID", "COLON",
               "INDENT",
               "PACKAGES_ID", "COLON",
               "INDENT",
               "WORD",
               "WORD",
               "DEDENT",
               "MODULES_ID", "COLON",
               "INDENT", "WORD",
               "DEDENT",
               "EXTENSION_ID", "COLON", "WORD",
               "DEDENT"]
        self._test(data, ref)

    def test_complex_indent(self):
        data = """\
Library:
    Packages:
        yo
        yo.foo
    Modules:
        foo.py
        bar.py
Extension: yeah
Extension: yeah2
"""
        ref = ["LIBRARY_ID", "COLON",
               "INDENT",
               "PACKAGES_ID", "COLON",
               "INDENT", "WORD",
               "WORD",
               "DEDENT",
               "MODULES_ID", "COLON",
               "INDENT", "WORD",
               "WORD",
               "DEDENT", "DEDENT",
               "EXTENSION_ID", "COLON", "WORD",
               "EXTENSION_ID", "COLON", "WORD"]
        self._test(data, ref)

    def test_indent_value(self):
        data = """\
Description: some
    words and whatnot
      .
"""
        ref = ["DESCRIPTION_ID", "COLON", "MULTILINES_STRING"]
        self._test(data, ref)

        tokens = self._get_tokens(data,)
        string = tokens[-1].value
        self.assertEqual(string, """\
some
words and whatnot
  .""")

    def test_indent_value2(self):
        data = """\
Description: some
    words and whatnot
      .
Name: yo
"""
        ref = ["DESCRIPTION_ID", "COLON", "MULTILINES_STRING",
               "NAME_ID", "COLON", "WORD"]
        self._test(data, ref)

        tokens = self._get_tokens(data,)
        string = tokens[-4].value
        self.assertEqual(string, """\
some
words and whatnot
  .""")


    def test_comma(self):
        data = """\
Library:
    Packages:
        yo,
        bar,
        yo
"""
        ref = ["LIBRARY_ID", "COLON",
               "INDENT", "PACKAGES_ID", "COLON",
               "INDENT", "WORD", "COMMA",
               "WORD", "COMMA",
               "WORD",
               "DEDENT",
               "DEDENT"
               ]
        self._test(data, ref)

    def test_comma2(self):
        data = """\
Library:
    Packages: foo, bar
    Packages:
        foo,
        bar
    Packages: foo,
        bar
"""
        ref = ["LIBRARY_ID", "COLON",
               "INDENT",
               "PACKAGES_ID", "COLON", "WORD", "COMMA", "WORD",
               "PACKAGES_ID", "COLON", "INDENT", "WORD", "COMMA", "WORD", "DEDENT",
               "PACKAGES_ID", "COLON", "WORD", "COMMA", "INDENT", "WORD",
               "DEDENT",
               "DEDENT"]
        self._test(data, ref)

    def test_tab(self):
        data = """\
Library:
\tpackages
"""
        self.assertRaises(SyntaxError, lambda: self._test(data, []))

    def test_rest_literal1(self):
        data = '''\
Description:
    Sphinx is a tool that makes it easy to create intelligent and beautiful
    documentation for Python projects (or other documents consisting of
    multiple reStructuredText sources), written by Georg Brandl.
    It was originally created to translate the new Python documentation,
    but has now been cleaned up in the hope that it will be useful to many
    other projects.
'''
        ref_str = """\
DESCRIPTION_ID COLON
INDENT
MULTILINES_STRING
DEDENT
"""

        self._test(data, split(ref_str))

        tokens = self._get_tokens(data,)
        string = tokens[-2].value
        self.assertEqual(string, """\
Sphinx is a tool that makes it easy to create intelligent and beautiful
documentation for Python projects (or other documents consisting of
multiple reStructuredText sources), written by Georg Brandl.
It was originally created to translate the new Python documentation,
but has now been cleaned up in the hope that it will be useful to many
other projects.""")

        data = """
Description:
    Sphinx uses reStructuredText as its markup language, and many of its strengths
    come from the power and straightforwardness of reStructuredText and its
    parsing and translating suite, the Docutils.
"""
        ref_str = """\
DESCRIPTION_ID COLON
INDENT
MULTILINES_STRING
DEDENT
"""
        self._test(data, split(ref_str))
    
        data = """\
Description:
    Although it is still under constant development, the following features
    are already present, work fine and can be seen "in action" in the Python docs:
"""

        ref_str = """\
DESCRIPTION_ID COLON
INDENT
MULTILINES_STRING
DEDENT
"""

        self._test(data, split(ref_str))

        data = """\
Description:
    * Output formats: HTML (including Windows HTML Help), plain text and LaTeX,
      for printable PDF versions
"""

        ref_str = """\
DESCRIPTION_ID COLON
INDENT
MULTILINES_STRING
DEDENT
"""

        self._test(data, split(ref_str))

    def test_colon_in_word(self):
        data = "Url: http://foo.com"

        ref_str = "URL_ID COLON WORD"

        self._test(data, ref_str)

    def test_rest_literal2(self):
        self.maxDiff = None

        data = '''\
Description:
    Sphinx is a tool that makes it easy to create intelligent and beautiful
    documentation for Python projects (or other documents consisting of
    multiple reStructuredText sources), written by Georg Brandl.
    It was originally created to translate the new Python documentation,
    but has now been cleaned up in the hope that it will be useful to many
    other projects.

    Sphinx uses reStructuredText as its markup language, and many of its strengths
    come from the power and straightforwardness of reStructuredText and its
    parsing and translating suite, the Docutils.

    Although it is still under constant development, the following features
    are already present, work fine and can be seen "in action" in the Python docs:

    * Output formats: HTML (including Windows HTML Help), plain text and LaTeX,
      for printable PDF versions
    * Extensive cross-references: semantic markup and automatic links
      for functions, classes, glossary terms and similar pieces of information
    * Hierarchical structure: easy definition of a document tree, with automatic
      links to siblings, parents and children
    * Automatic indices: general index as well as a module index
    * Code handling: automatic highlighting using the Pygments highlighter
    * Various extensions are available, e.g. for automatic testing of snippets
      and inclusion of appropriately formatted docstrings.

    A development egg can be found `here
    <http://bitbucket.org/birkenfeld/sphinx/get/tip.gz#egg=Sphinx-dev>`_.
'''
        ref_str = """\
DESCRIPTION_ID COLON INDENT
MULTILINES_STRING
DEDENT
"""
        self._test(data, split(ref_str))

        tokens = self._get_tokens(data)
        string = tokens[-2].value
        self.assertMultiLineEqual(string, """\
Sphinx is a tool that makes it easy to create intelligent and beautiful
documentation for Python projects (or other documents consisting of
multiple reStructuredText sources), written by Georg Brandl.
It was originally created to translate the new Python documentation,
but has now been cleaned up in the hope that it will be useful to many
other projects.

Sphinx uses reStructuredText as its markup language, and many of its strengths
come from the power and straightforwardness of reStructuredText and its
parsing and translating suite, the Docutils.

Although it is still under constant development, the following features
are already present, work fine and can be seen "in action" in the Python docs:

* Output formats: HTML (including Windows HTML Help), plain text and LaTeX,
  for printable PDF versions
* Extensive cross-references: semantic markup and automatic links
  for functions, classes, glossary terms and similar pieces of information
* Hierarchical structure: easy definition of a document tree, with automatic
  links to siblings, parents and children
* Automatic indices: general index as well as a module index
* Code handling: automatic highlighting using the Pygments highlighter
* Various extensions are available, e.g. for automatic testing of snippets
  and inclusion of appropriately formatted docstrings.

A development egg can be found `here
<http://bitbucket.org/birkenfeld/sphinx/get/tip.gz#egg=Sphinx-dev>`_.""")

    def test_space_no_space(self):
        """Test whitespace-only lines are handled correctly."""
        ws = " " * 4
        data = """\
Description:
    a few words
%s
    and some more

    and still more.

""" % ws

        ref_str = "DESCRIPTION_ID COLON INDENT MULTILINES_STRING DEDENT"
        self._test(data, ref_str)

        tokens = self._get_tokens(data)
        string = tokens[-2].value

        ref_str = "a few words\n\nand some more\n\nand still more.\n"
        self.assertMultiLineEqual(string, ref_str)

    def test_ref_literal2(self):
        # Test transition from SCANING_MULTILINE_FIELD
        data = """\
Description: a summary
Name: yo
"""
        ref_str = """\
DESCRIPTION_ID COLON MULTILINES_STRING
NAME_ID COLON WORD
"""
        self._test(data, split(ref_str))

    def test_indented_multiline(self):
        data = """\
Path: path
    Description: descr
    Name: name
"""

        ref_str = """\
PATH_ID COLON WORD
INDENT
DESCRIPTION_ID COLON STRING
NAME_ID COLON WORD
DEDENT
"""
        self._test(data, split(ref_str))

    def test_indented_multiline2(self):
        data = """\
Classifiers:
    foo
Path: path
    Description: descr
    Name: name
"""

        ref_str = """\
CLASSIFIERS_ID COLON
INDENT
STRING
DEDENT
PATH_ID COLON WORD
INDENT
DESCRIPTION_ID COLON STRING
NAME_ID COLON WORD
DEDENT
"""
        self._test(data, split(ref_str))

class TestNewLines(TestLexer):
    def setUp(self):
        self.lexer = BentoLexer()

    # Test we throw away NEWLINES except in literals
    def test_lastnewline(self):
        data = """\
Name: yo
"""
        ref_str = """\
NAME_ID COLON WORD
"""
        self._test(data, split(ref_str))

    def test_start_with_newlines(self):
        data = """\

Name: yo
"""
        ref_str = """\
NAME_ID COLON WORD
"""
        self._test(data, split(ref_str))

    def test_start_with_newlines2(self):
        data = """\
Summary: a summary
"""
        ref_str = """\
SUMMARY_ID COLON STRING
"""
        self._test(data, split(ref_str))

    def test_dedent_newline(self):
        data = """\
Description: Sphinx
    is
        a
    tool
"""

        ref_str = """\
DESCRIPTION_ID COLON MULTILINES_STRING
"""
        self._test(data, split(ref_str))

        tokens = self._get_tokens(data)
        string = tokens[-1].value
        self.assertEqual(string, """\
Sphinx
is
    a
tool""")

    def test_single_line(self):
        data = "Name: word"

        ref_str = "NAME_ID COLON WORD"
        self._test(data, split(ref_str))

class TestComment(TestLexer):
    def setUp(self):
        self.lexer = BentoLexer()

    def test_simple(self):
        data = """\
# Simple comment
Name: foo
"""

        ref_str = "NAME_ID COLON WORD"
        self._test(data, split(ref_str))

    def test_simple_inline(self):
        data = """\
Name: foo # inline comment
"""

        ref_str = "NAME_ID COLON WORD"
        self._test(data, split(ref_str))

        tokens = self._get_tokens(data)
        name = tokens[-1].value
        self.assertEqual(name, "foo")

    def test_simple_inline2(self):
        data = """\
ExtraSourceFiles:
    # indented comment
    foo
"""

        ref_str = "EXTRA_SOURCE_FILES_ID COLON INDENT WORD DEDENT"
        self._test(data, split(ref_str))

class TestMeta(TestLexer):
    def test_license(self):
        data = """\
License: PSF or  ZPL
"""

        ref_str = "LICENSE_ID COLON STRING"
        self._test(data, split(ref_str))

class TestErrorHandling(TestLexer):
    def test_multiline_string_count(self):
        data = """\
Description: hey
    how are
        you
        doing ?
NName: foo
"""

        self.lexer.input(data)
        try:
            list(self.lexer)
            self.fail("lexer did not raise expected ParseError")
        except ParseError:
            e = extract_exception()
            self.assertEqual(e.token.lexer.lineno, 5, "Invalid line number: %d" % e.token.lexer.lineno)

########NEW FILE########
__FILENAME__ = test_parser
from six.moves import cStringIO

from bento.errors \
    import \
        ParseError
from bento.utils.utils \
    import \
        extract_exception
from bento.parser.nodes \
    import \
        ast_pprint
from bento.parser.parser \
    import \
        parse

from bento.compat.api.moves import unittest

TestCase = unittest.TestCase

class _TestGrammar(TestCase):
    def _test(self, data, expected):
        s = cStringIO()

        p = parse(data)
        ast_pprint(p, string=s)

        try:
            self.assertEqual(s.getvalue(), expected)
        except AssertionError:
            msg = s.getvalue()
            msg += "\n%s" % str(expected)
            raise AssertionError("assertion error:\n%s" % msg)

class TestMeta(_TestGrammar):
    def test_meta_name(self):
        data = "Name: yo"
        expected = """\
Node(type='stmt_list'):
    Node(type='name', value='yo')\
"""

        self._test(data, expected)

    def test_meta_url(self):
        data = "Url: http://example.com"
        expected = """\
Node(type='stmt_list'):
    Node(type='url', value='http://example.com')\
"""

        self._test(data, expected)

    def test_recurse(self):
        data = "Recurse: foo, bar"
        expected = """\
Node(type='stmt_list'):
    Node(type='subento', value=['foo', 'bar'])\
"""

        self._test(data, expected)

    def test_meta_summary(self):
        data = "Summary: a few words of description."
        expected = """\
Node(type='stmt_list'):
    Node(type='summary', value='a few words of description.')"""

        self._test(data, expected)

    def test_meta_author(self):
        data = "Author: John Doe"
        expected = """\
Node(type='stmt_list'):
    Node(type='author', value='John Doe')"""

        self._test(data, expected)

    def test_meta_author_email(self):
        data = "AuthorEmail: john@doe.com"
        expected = """\
Node(type='stmt_list'):
    Node(type='author_email', value='john@doe.com')"""

        self._test(data, expected)

    def test_meta_maintainer_email(self):
        data = "MaintainerEmail: john@doe.com"
        expected = """\
Node(type='stmt_list'):
    Node(type='maintainer_email', value='john@doe.com')"""

        self._test(data, expected)

    def test_meta_maintainer(self):
        data = "Maintainer: John Doe"
        expected = """\
Node(type='stmt_list'):
    Node(type='maintainer', value='John Doe')"""

        self._test(data, expected)

    def test_meta_license(self):
        data = "License: BSD"
        expected = """\
Node(type='stmt_list'):
    Node(type='license', value='BSD')"""

        self._test(data, expected)

    def test_meta_version(self):
        data = "Version: 1.0"
        expected = """\
Node(type='stmt_list'):
    Node(type='version', value='1.0')\
"""

        self._test(data, expected)

    def test_meta_platforms(self):
        data = "Platforms: any"
        expected = """\
Node(type='stmt_list'):
    Node(type='platforms', value=['any'])"""

        self._test(data, expected)

    def test_meta_classifiers_single_line(self):
        data = "Classifiers: yo"
        expected = """\
Node(type='stmt_list'):
    Node(type='classifiers', value=['yo'])\
"""

        self._test(data, expected)

    def test_meta_classifiers_multi_lines(self):
        data = """\
Classifiers: yo,
    yeah
"""
        expected = """\
Node(type='stmt_list'):
    Node(type='classifiers', value=['yo', 'yeah'])\
"""

        self._test(data, expected)

    def test_meta_classifiers_indent_only(self):
        data = """\
Classifiers:
    yo1,
    yo2\
"""
        expected = """\
Node(type='stmt_list'):
    Node(type='classifiers', value=['yo1', 'yo2'])\
"""

        self._test(data, expected)

    def test_meta_classifiers_full(self):
        data = """\
Classifiers: yo1,
    yo2\
"""
        expected = """\
Node(type='stmt_list'):
    Node(type='classifiers', value=['yo1', 'yo2'])\
"""

        self._test(data, expected)

    def test_meta_stmts(self):
        data = """\
Name: yo
Summary: yeah\
"""
        expected = """\
Node(type='stmt_list'):
    Node(type='name', value='yo')
    Node(type='summary', value='yeah')"""

        self._test(data, expected)

    def test_empty(self):
        data = ""
        expected = "Node(type='stmt_list')"

        self._test(data, expected)

    def test_newline(self):
        data = "\n"
        expected = "Node(type='stmt_list')"

        self._test(data, expected)

    def test_description_single_line(self):
        data = "Description: some words."
        expected = """\
Node(type='stmt_list'):
    Node(type='description', value='some words.')"""

        self._test(data, expected)

    def test_description_simple_indent(self):
        data = """\
Description:
    some words."""
        expected = """\
Node(type='stmt_list'):
    Node(type='description', value='some words.')"""

        self._test(data, expected)

    def test_description_simple_indent_wse(self):
        "Test indented block with ws error."
        data = """\
Description:   
    some words."""
        expected = """\
Node(type='stmt_list'):
    Node(type='description', value='some words.')"""

        self._test(data, expected)

    def test_description_complex_indent(self):
        data = """\
Description:
    some
        indented
            words
    ."""

        description = """\
some
    indented
        words
."""
        expected = """\
Node(type='stmt_list'):
    Node(type='description', value=%r)""" % description

        self._test(data, expected)

class TestConditional(_TestGrammar):
    def test_not_bool(self):
        data = """\
Library:
    if not true:
        Modules: foo.py
"""
        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='conditional'):
                Node(type='library_stmts'):
                    Node(type='modules', value=['foo.py'])"""

        self._test(data, expected)

    def test_not_flag(self):
        data = """\
Flag: foo
    Default: true
    Description: yo mama

Library:
    if not flag(foo):
        Modules: foo.py
"""
        expected = """\
Node(type='stmt_list'):
    Node(type='flag'):
        Node(type='flag_declaration', value='foo')
        Node(type='flag_stmts'):
            Node(type='flag_default', value='true')
            Node(type='flag_description', value='yo mama')
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='conditional'):
                Node(type='library_stmts'):
                    Node(type='modules', value=['foo.py'])"""

        self._test(data, expected)

class TestLibrary(_TestGrammar):
    def test_modules(self):
        data = """\
Library:
    Modules: foo.py
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='modules', value=['foo.py'])"""

        self._test(data, expected)

    def test_modules2(self):
        data = """\
Library:
    Modules: foo.py,
        bar.py,
        fubar.py
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='modules', value=['foo.py', 'bar.py', 'fubar.py'])"""

        self._test(data, expected)

    def test_modules3(self):
        data = """\
Library:
    Modules:
        bar.py,
        fubar.py
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='modules', value=['bar.py', 'fubar.py'])"""

        self._test(data, expected)

    def test_packages(self):
        data = """\
Library:
    Packages:
        foo, bar
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='packages', value=['foo', 'bar'])"""

        self._test(data, expected)

    def test_build_requires(self):
        data = """\
Library:
    BuildRequires:
        foo, bar
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='build_requires', value=['foo', 'bar'])"""

        self._test(data, expected)

    def test_build_requires2(self):
        data = """\
Library:
    BuildRequires:
        foo, bar
    BuildRequires: fubar
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='build_requires', value=['foo', 'bar'])
            Node(type='build_requires', value=['fubar'])"""

        self._test(data, expected)

    def test_subdir(self):
        data = """\
Library:
    SubDirectory: lib
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='library'):
        Node(type='library_name', value='default')
        Node(type='library_stmts'):
            Node(type='sub_directory', value='lib')"""

        self._test(data, expected)

class TestExecutable(_TestGrammar):
    def test_simple(self):
        data = """\
Executable: foo
    Module: foo.bar
    Function: main
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='executable'):
        Node(type='exec_name', value='foo')
        Node(type='exec_stmts'):
            Node(type='module', value='foo.bar')
            Node(type='function', value='main')"""

        self._test(data, expected)

class TestPath(_TestGrammar):
    def test_simple(self):
        data = """\
Path: foo
    Default: foo_default
    Description: foo_description
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='path'):
        Node(type='path_declaration', value='foo')
        Node(type='path_stmts'):
            Node(type='path_default', value='foo_default')
            Node(type='path_description', value='foo_description')"""

        self._test(data, expected)

    def test_conditional(self):
        data = """\
Path: foo
    if true:
        Default: foo_default
    Description: foo_description
"""

        expected = """\
Node(type='stmt_list'):
    Node(type='path'):
        Node(type='path_declaration', value='foo')
        Node(type='path_stmts'):
            Node(type='conditional'):
                Node(type='path_stmts'):
                    Node(type='path_default', value='foo_default')
            Node(type='path_description', value='foo_description')"""

        self._test(data, expected)

class TestErrorHandling(TestCase):
    def test_invalid_keyword(self):
        data = """\
Library:
    Name: foo
"""

        try:
            parse(data)
            self.fail("parser did not raise expected ParseError")
        except ParseError:
            e = extract_exception()
            self.assertMultiLineEqual(e.msg, """\
yacc: Syntax error at line 2, Token(NAME_ID, 'Name')
    Name: foo
    ^""")

    def test_invalid_keyword_comment(self):
        """Check comments don't screw up line counting."""
        data = """\
# Some useless
# comments
Library:
    Name: foo
"""

        try:
            parse(data)
            self.fail("parser did not raise expected ParseError")
        except ParseError:
            e = extract_exception()
            self.assertMultiLineEqual(e.msg, """\
yacc: Syntax error at line 4, Token(NAME_ID, 'Name')
    Name: foo
    ^""")

########NEW FILE########
__FILENAME__ = test_parsing
import os
import tempfile
import stat

import os.path as op

from bento.compat.api.moves \
    import \
        unittest
from bento.core.pkg_objects \
    import \
        PathOption, FlagOption, DataFiles
from bento.core.options \
    import \
        PackageOptions
from bento.core.package \
    import \
        PackageDescription
from bento.utils.utils \
    import \
        extract_exception
from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.errors \
    import \
        BentoError, ParseError

from bento.parser import parser as parser_module

#old = sys.path[:]
#try:
#    sys.path.insert(0, join(dirname(__file__), "pkgdescr"))
#    from simple_package import PKG, DESCR
#finally:
#    sys.path = old

class TestParseError(unittest.TestCase):
    def setUp(self):
        self.f = NamedTemporaryFile(mode="w", delete=False)

    def tearDown(self):
        self.f.close()
        os.remove(self.f.name)

    def test_simple(self):
        text = """\
NName: foo
"""
        error_msg = "Unrecognized keyword: 'NName'"
        self.assertRaisesRegexp(ParseError, error_msg, lambda : PackageDescription.from_string(text))

    def test_simple_filename(self):
        f = self.f
        f.write("NName: foo")
        f.flush()
        self.assertRaises(ParseError, lambda : PackageDescription.from_file(f.name))

    def test_error_string(self):
        f = self.f
        f.write("NName: foo")
        f.flush()
        try:
            PackageDescription.from_file(f.name)
            raise AssertionError("Should raise here !")
        except ParseError:
            e = extract_exception()
            self.assertEqual(str(e), """\
  File "%s", line 1
NName: foo
^
Syntax error""" % f.name)

class TestDataFiles(unittest.TestCase):
    def test_simple(self):
        text = """\
Name: foo

DataFiles: data
    TargetDir: $datadir
    Files:
        foo.data
"""
        r_data = DataFiles("data", files=["foo.data"], target_dir="$datadir")
        pkg = PackageDescription.from_string(text)
        self.failUnless("data" in pkg.data_files)
        self.assertEqual(pkg.data_files["data"].__dict__, r_data.__dict__)
    
class TestOptions(unittest.TestCase):
    simple_text = """\
Name: foo

Flag: flag1
    Description: flag1 description
    Default: false

Path: foo
    Description: foo description
    Default: /usr/lib
"""
    def _test_simple(self, opts):
        self.failUnless(opts.name, "foo")

        flag = FlagOption("flag1", "false", "flag1 description")
        self.failUnless(opts.flag_options.keys(), ["flags"])
        self.failUnless(opts.flag_options["flag1"], flag.__dict__)

        path = PathOption("foo", "/usr/lib", "foo description")
        self.failUnless(opts.path_options.keys(), ["foo"])
        self.failUnless(opts.path_options["foo"], path.__dict__)

    def test_simple_from_string(self):
        s = self.simple_text
        opts = PackageOptions.from_string(s)
        self._test_simple(opts)

    def test_simple_from_file(self):
        fid, filename = tempfile.mkstemp(suffix=".info", text=True)
        try:
            os.write(fid, self.simple_text.encode())
            opts = PackageOptions.from_file(filename)
            self._test_simple(opts)
        finally:
            os.close(fid)
            os.remove(filename)

class TestParserCaching(unittest.TestCase):
    def setUp(self):
        wdir = tempfile.mkdtemp()
        self.subwdir= "bar"
        self.old = os.getcwd()
        os.chdir(wdir)

    def tearDown(self):
        os.chdir(self.old)

    def test_no_cached_failure(self):
        """Ensure we raise an error when the cached parser file does not exists
        and we cannot create one."""
        os.makedirs(self.subwdir)
        os.chmod(self.subwdir, stat.S_IREAD | stat.S_IEXEC)
        parsetab = op.join(self.subwdir, "parsetab")

        old_parsetab = parser_module._PICKLED_PARSETAB
        try:
            parser_module._PICKLED_PARSETAB = parsetab
            try:
                parser_module.Parser()
                self.assertTrue(len(self._list_files()) == 0, "Ply created a cached file in CWD")
                self.fail("Expected an error when creating a parser in read-only dir !")
            except BentoError:
                pass
        finally:
            parser_module._PICKLED_PARSETAB = old_parsetab

    def _list_files(self):
        """Return the list of files in cwd (including subdirectories)."""
        created_files = []
        for root, dirs, files in os.walk(os.getcwd()):
            created_files.extend([op.join(root, f) for f in files])
        return created_files

    def test_read_only_cached(self):
        """Test that we can create a parser backed by a read-only parsetab
        file."""
        os.makedirs(self.subwdir)
        parsetab = op.join(self.subwdir, "parsetab")

        old_parsetab = parser_module._PICKLED_PARSETAB
        try:
            parser_module._PICKLED_PARSETAB = parsetab
            parser_module.Parser()
            self.assertEqual(self._list_files(), [op.abspath(parsetab)])

            os.chmod(self.subwdir, stat.S_IREAD | stat.S_IEXEC)
            os.chmod(parsetab, stat.S_IREAD)

            parser_module.Parser()
            # This ensures ply did not write another cached file behind our back
            self.assertEqual(self._list_files(), [op.abspath(parsetab)],
                             "Ply created another cached parsetab file !")
        finally:
            parser_module._PICKLED_PARSETAB = old_parsetab

########NEW FILE########
__FILENAME__ = test_visitor
import sys
import warnings

from bento.compat.api.moves \
    import \
        unittest
from bento.parser.parser \
    import \
        parse
from bento.parser.nodes \
    import \
        ast_walk
from bento.parser.visitor \
    import \
        Dispatcher

def parse_and_analyse(data):
    p = parse(data)
    dispatcher = Dispatcher()
    res = ast_walk(p, dispatcher)

    return res

def _empty_description():
    d = {"libraries": {}, "path_options": {}, "flag_options": {},
         "data_files": {}, "extra_source_files": [], "executables": {},
         "hook_files": []}
    return d

def _empty_library():
    d = {"name": "default", "py_modules": [], "packages": [], "extensions": {},
         "build_requires": [], "install_requires": [], "compiled_libraries": {},
         "sub_directory": None}
    return d

class TestSimpleMeta(unittest.TestCase):
    def setUp(self):
        self.ref = _empty_description()

    def test_empty(self):
        data = ""
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_name(self):
        data = """\
Name: foo
"""
        self.ref["name"] = "foo"
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_summary(self):
        data = """\
Summary: a few words.
"""
        self.ref.update({"summary": "a few words."})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_author(self):
        data = """\
Author: John Doe
"""
        self.ref.update({"author": "John Doe"})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_maintainer(self):
        data = """\
Maintainer: John Doe
"""
        self.assertEqual(parse_and_analyse(data)["maintainer"], "John Doe")

    def test_use_backends(self):
        data = """\
UseBackends: foo
"""
        self.assertEqual(parse_and_analyse(data)["use_backends"], ["foo"])

        data = """\
UseBackends: foo, bar
"""
        self.assertEqual(parse_and_analyse(data)["use_backends"], ["foo", "bar"])

    def test_hook(self):
        data = """\
HookFile: bscript
"""
        self.assertEqual(parse_and_analyse(data)["hook_files"], ["bscript"])

        data = """\
HookFile: bscript, bscript2
"""
        self.assertEqual(parse_and_analyse(data)["hook_files"], ["bscript", "bscript2"])

class TestDescription(unittest.TestCase):
    def setUp(self):
        self.ref = _empty_description()

    def test_simple_single_line(self):
        data = "Description: some simple description"

        self.ref.update({"description": "some simple description"})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_simple_indented_block(self):
        data = """\
Description:
    some simple description
    on multiple
    lines.
"""
        self.ref["description"] = "some simple description\non multiple\nlines."
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_simple_indented_block2(self):
        data = """\
Description: some simple description
    on multiple
    lines.
"""
        self.ref["description"] = "some simple description\non multiple\nlines."
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_nested_indented_block3(self):
        data = """\
Description: some
    simple
        description
            on
    
    multiple

    lines.
"""

        self.ref["description"] = """some
simple
    description
        on

multiple

lines.\
"""
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_nested_indented_block2(self):
        last_indent = " " * 4
        data = """\
Description: some
    simple
        description
            on
%s
Name: foo
""" % last_indent

        self.ref["name"] = "foo"
        self.ref["description"] = """some
simple
    description
        on
"""
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_description_from_file(self):
        data = """\
DescriptionFromFile: foo.rst
"""
        ret = parse_and_analyse(data)
        self.assertEqual(ret["description_from_file"], "foo.rst")

    def test_meta_template_file(self):
        s = warnings.filters[:]
        warnings.filterwarnings("ignore")
        try:
            data = """\
MetaTemplateFile: foo.py.in
"""
            ret = parse_and_analyse(data)
            self.assertEqual(ret["meta_template_files"], ["foo.py.in"])
        finally:
            warnings.filters = s

    def test_meta_template_files(self):
        data = """\
MetaTemplateFiles: foo.py.in
"""
        ret = parse_and_analyse(data)
        self.assertEqual(ret["meta_template_files"], ["foo.py.in"])

class TestLibrary(unittest.TestCase):
    def setUp(self):
        self.ref = _empty_description()
        self.ref["libraries"]["default"] = _empty_library()

    def test_empty(self):
        data = """\
Library:
"""

        #self.ref["libraries"] = {"default": {"name": "default"}}
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_sub_directory(self):
        self.maxDiff = None
        data = """\
Library:
    SubDirectory: lib
"""

        self.ref["libraries"]["default"]["sub_directory"] = "lib"
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_modules(self):
        data = """\
Library:
    Modules: foo.py
"""

        self.ref["libraries"]["default"].update({"py_modules": ["foo.py"]})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_simple_extension(self):
        data = """\
Library:
    Extension: _foo
        Sources: foo.c
"""
        extension = {"name": "_foo", "sources": ["foo.c"]}
        self.ref["libraries"]["default"].update({"extensions": {"_foo": extension}})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_extension_include_dirs(self):
        data = """\
Library:
    Extension: _foo
        Sources: foo.c
        IncludeDirs: foo
"""
        r_extension = {"name": "_foo", "sources": ["foo.c"], "include_dirs": ["foo"]}
        extension = parse_and_analyse(data)["libraries"]["default"]["extensions"]["_foo"]
        self.assertEqual(extension, r_extension)

    def test_double_sources_extension(self):
        data = """\
Library:
    Extension: _foo
        Sources: foo.c
        Sources: bar.c
"""
        self.assertRaises(ValueError, lambda: parse_and_analyse(data))

    def test_build_requires(self):
        data = """\
Library:
    BuildRequires: foo
"""
        self.ref["libraries"]["default"].update({"build_requires": ["foo"]})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_install_requires(self):
        data = """\
Library:
    InstallRequires: foo
"""
        self.ref["libraries"]["default"].update({"install_requires": ["foo"]})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_compiled_library_include_dirs(self):
        data = """\
Library:
    CompiledLibrary: _foo
        Sources: foo.c
        IncludeDirs: foo
"""
        r_compiled_library = {"name": "_foo", "sources": ["foo.c"], "include_dirs": ["foo"]}
        compiled_library = parse_and_analyse(data)["libraries"]["default"]["compiled_libraries"]["_foo"]
        self.assertEqual(compiled_library, r_compiled_library)

    def test_double_sources_compiled_library(self):
        data = """\
Library:
    CompiledLibrary: _foo
        Sources: foo.c
        Sources: bar.c
"""
        self.assertRaises(ValueError, lambda: parse_and_analyse(data))

class TestPath(unittest.TestCase):
    def test_simple(self):
        data = """\
Path: manpath
    Description: man path
    Default: /usr/share/man
"""

        descr = _empty_description()
        descr["path_options"] = {"manpath": {"name": "manpath",
                                             "default": "/usr/share/man",
                                             "description": "man path"}}
        self.assertEqual(parse_and_analyse(data), descr)

    def test_multiple_path_sections(self):
        data = """\
Path: manpath
    Description: man path
    Default: /usr/share/man

Path: varpath
    Description: var path
    Default: /var
"""

        data_sections = parse_and_analyse(data)["path_options"]
        r_data_sections = {
                "manpath": {"name": "manpath", "description": "man path", "default": "/usr/share/man"},
                "varpath": {"name": "varpath", "description": "var path", "default": "/var"},
        }
        self.assertEqual(data_sections, r_data_sections)

    def test_conditional_path_section(self):
        data_template = """\
Path: manpath
    Description: man path
    if %s:
        Default: /usr/share/woman
    else:
        Default: /usr/share/man
"""

        data_sections = parse_and_analyse(data_template % "true")["path_options"]
        r_data_sections = {
                "manpath": {"name": "manpath", "description": "man path", "default": "/usr/share/woman"},
        }
        self.assertEqual(data_sections, r_data_sections)

        data_sections = parse_and_analyse(data_template % "false")["path_options"]
        r_data_sections = {
                "manpath": {"name": "manpath", "description": "man path", "default": "/usr/share/man"},
        }
        self.assertEqual(data_sections, r_data_sections)

    def test_invalid_path_section1(self):
        data = """\
Path: manpath
    Description: description
"""
        self.assertRaises(ValueError, lambda: parse_and_analyse(data))

        data = """\
Path: manpath
    Default: /foo/bar
"""
        self.assertRaises(ValueError, lambda: parse_and_analyse(data))

class TestDataFiles(unittest.TestCase):
    def test_simple(self):
        data = """\
DataFiles: man1doc
    TargetDir: /usr/share/man/man1
    SourceDir: doc/man1
    Files: foo.1
"""

        ref = _empty_description()
        ref["data_files"] = {"man1doc": {"name": "man1doc",
                                         "target_dir": "/usr/share/man/man1",
                                         "source_dir": "doc/man1",
                                         "files": ["foo.1"]}}
        self.assertEqual(parse_and_analyse(data), ref)

class TestFlag(unittest.TestCase):
    def setUp(self):
        self.ref = _empty_description()

    def test_simple(self):
        data = """\
Flag: debug
    Description: debug flag
    Default: false
"""
        self.ref["flag_options"] = {"debug": {"name": "debug",
                                              "default": "false",
                                              "description": "debug flag"}}
        self.assertEqual(parse_and_analyse(data), self.ref)

class TestConditional(unittest.TestCase):
    def setUp(self):
        self.ref = _empty_description()
        self.ref["libraries"]["default"] = _empty_library()

    def test_literal(self):
        data = """\
Library:
    if true:
        Modules: foo.py, bar.py
    else:
        Modules:  fubar.py
"""

        self.ref["libraries"]["default"].update({"py_modules": ["foo.py", "bar.py"]})
        self.assertEqual(parse_and_analyse(data), self.ref)

        data = """\
Library:
    if false:
        Modules: foo.py, bar.py
    else:
        Modules:  fubar.py
"""
        self.ref["libraries"]["default"]["py_modules"] = ["fubar.py"]

        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_os(self):
        data = """\
Library:
    if os(%s):
        Modules: foo.py, bar.py
    else:
        Modules:  fubar.py
""" % sys.platform

        self.ref["libraries"]["default"].update({"py_modules": ["foo.py", "bar.py"]})
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_flag(self):
        data = """\
Flag: debug
    Default: true

Library:
    if flag(debug):
        Modules: foo.py, bar.py
    else:
        Modules:  fubar.py
"""

        self.ref["libraries"]["default"].update({"py_modules": ["foo.py", "bar.py"]})
        self.ref["flag_options"] = {"debug": {"default": "true",
                                              "name": "debug"}}
        self.assertEqual(parse_and_analyse(data), self.ref)

        data = """\
Flag: debug
    Default: false

Library:
    if flag(debug):
        Modules: foo.py, bar.py
    else:
        Modules:  fubar.py
"""

        self.ref["libraries"]["default"]["py_modules"] = ["fubar.py"]
        self.ref["flag_options"]["debug"] = {"default": "false", "name": "debug"}

        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_not_flag(self):
        data = """\
Flag: debug
    Default: true

Library:
    if not flag(debug):
        Modules: foo.py, bar.py
    else:
        Modules:  fubar.py
"""

        self.ref["libraries"]["default"].update({"py_modules": ["fubar.py"]})
        self.ref["flag_options"] = {"debug": {"default": "true",
                                              "name": "debug"}}
        self.assertEqual(parse_and_analyse(data), self.ref)

    def test_missing_flag(self):
        data = """\
Flag: debug
    Default: true

Library:
    if flag(ddebug):
        Modules: bar.py
"""
        self.assertRaises(ValueError, lambda: parse_and_analyse(data))

        data = """\
Flag: debug
    Default: true

Library:
    if not flag(ddebug):
        Modules: bar.py
"""
        self.assertRaises(ValueError, lambda: parse_and_analyse(data))

    def test_not(self):
        data = """\
Library:
    if not true:
        Modules: foo.py, bar.py
    else:
        Modules:  fubar.py
"""

        self.ref["libraries"]["default"].update({"py_modules": ["fubar.py"]})
        self.assertEqual(parse_and_analyse(data), self.ref)

class TestExecutable(unittest.TestCase):
    def setUp(self):
        self.ref = _empty_description()

    def test_modules(self):
        data = """\
Executable: foo
    Module: foo.bar
    Function: main
"""
        self.ref["executables"] = {"foo": {"module": "foo.bar",
                                           "function": "main",
                                           "name": "foo"}}
        self.assertEqual(parse_and_analyse(data), self.ref)

########NEW FILE########
__FILENAME__ = utils
import sys

import six

class Peeker(object):
    """Generator to enable "peeking" the next item

    Parameters
    ----------
    it : iterator
        iterator which we one to peek in
    dummy :
        if not None, will be returned by peek if the iterator is empty

    Example
    -------
    >>> a = [1, 2, 3, 4]
    >>> peeker = Peeker(a)
    >>> for i in peeker:
    >>>     try:
    >>>         next = peeker.peek()
    >>>         print "Next to %d is %d" % (i, next)
    >>>     except StopIteration:
    >>>         print "End of stream", i
    """
    def __init__(self, it, dummy=None):
        self._it = iter(it)
        self._cache = None
        if dummy is None:
            self.peek = self._peek_no_dummy
        else:
            self.peek = self._peek_dummy
        self._dummy = dummy

    def __next__(self):
        return self.next()

    def next(self):
        if self._cache:
            i = self._cache
            self._cache = None
            return i
        else:
            return six.advance_iterator(self._it)
        #self._cache = None
        #return i

    def _peek_dummy(self):
        if self._cache:
            return self._cache
        else:
            try:
                i = six.advance_iterator(self._it)
            except StopIteration:
                return self._dummy
            self._cache = i
            return i

    def _peek_no_dummy(self):
        if self._cache:
            return self._cache
        else:
            i = six.advance_iterator(self._it)
            self._cache = i
            return i

    def __iter__(self):
        return self

class BackwardGenerator(object):
    def __init__(self, gen):
        self._gen = gen
        self._cache = []
        self._previous = None

    def next(self):
        c = six.advance_iterator(self._gen)
        if len(self._cache) == 2:
            old, new = self._cache
            self._cache = [new]
        self._cache.append(c)
        return c

    def __next__(self):
        return self.next()

    def previous(self):
        if len(self._cache) < 2:
            raise ValueError()
        return self._cache[0]

    def __iter__(self):
        return self

def print_tokens_simple(lexer):
    while True:
        tok = lexer.token()
        if not tok:
            break
        print(tok)

def count_lines(s):
    return len(s.splitlines())

########NEW FILE########
__FILENAME__ = visitor
import sys
import copy

from bento.parser.nodes \
    import \
        Node

# XXX: fix the str vs bool issue with flag variables
_LIT_BOOL = {"true": True, "false": False, True: True, False: False}

class Dispatcher(object):
    def __init__(self, user_values=None):
        self._d = {
                "path_options": {},
                "flag_options": {},
                "libraries": {},
                "executables": {},
                "data_files": {},
                "extra_source_files": [],
                "hook_files": [],
        }
        self.action_dict = {
            "empty": self.empty,
            "stmt_list": self.stmt_list,
            "description": self.description,
            "description_from_file": self.description_from_file,
            "summary": self.summary,
            "author": self.author,
            "maintainer": self.maintainer,
            "hook_files": self.hook_files,
            "config_py": self.config_py,
            "meta_template_file": self.meta_template_file,
            "subento": self.subento,
            "use_backends": self.use_backends,
            # Library
            "library": self.library,
            "library_name": self.library_name,
            "library_stmts": self.library_stmts,
            # Path
            "path": self.path,
            "path_default": self.path_default,
            "path_stmts": self.path_stmts,
            "path_description": self.path_description,
            # Flag
            "flag": self.flag,
            "flag_default": self.flag_default,
            "flag_stmts": self.flag_stmts,
            "flag_description": self.flag_description,
            # Extension
            "extension": self.extension,
            "extension_declaration": self.extension_declaration,
            "extension_field_stmts": self.extension_field_stmts,
            # Pure C library
            "compiled_library": self.compiled_library,
            "compiled_library_declaration": self.compiled_library_declaration,
            "compiled_library_field_stmts": self.compiled_library_field_stmts,
            # Conditional
            "conditional": self.conditional,
            "osvar": self.osvar,
            "flagvar": self.flagvar,
            "not_flagvar": self.not_flagvar,
            "bool": self.bool_var,
            # Extra source files
            "extra_source_files": self.extra_source_files,
            # Data files handling
            "data_files": self.data_files,
            "data_files_stmts": self.data_files_stmts,
            # Executable
            "executable": self.executable,
            "exec_stmts": self.exec_stmts,
            "exec_name": self.exec_name,
            "function": self.function,
            "module": self.module,
            "sub_directory": self.sub_directory,
        }
        if user_values is not None:
            self._vars = copy.deepcopy(user_values)
        else:
            self._vars = {}

    def empty(self, node):
        return {}

    def stmt_list(self, node):
        for c in node.children:
            if c.type in ["name", "description", "version", "summary", "url",
                          "download_url", "author", "author_email",
                          "maintainer", "maintainer_email", "license",
                          "platforms", "classifiers", "hook_files",
                          "config_py", "description_from_file",
                          "meta_template_files", "keywords", "use_backends"]:
                self._d[c.type] = c.value
            elif c.type == "path":
                self._d["path_options"].update({c.value["name"]: c.value})
            elif c.type == "flag":
                self._d["flag_options"].update({c.value["name"]: c.value})
            elif c.type == "library":
                self._d["libraries"].update({c.value["name"]: c.value})
            elif c.type == "executable":
                self._d["executables"].update({c.value["name"]: c.value})
            elif c.type == "data_files":
                self._d["data_files"].update({c.value["name"]: c.value})
            else:
                raise ValueError("Unhandled top statement (%s)" % c)
        return self._d

    def summary(self, node):
        return node

    def author(self, node):
        return node

    def maintainer(self, node):
        return self.author(node)

    def hook_files(self, node):
        self._d["hook_files"].extend(node.value)

    def config_py(self, node):
        return node

    def meta_template_file(self, node):
        return node

    def description_from_file(self, node):
        return node

    def description(self, node):
        return node

    #--------------------------
    # Library section handlers
    #--------------------------
    def library(self, node):
        library = {"py_modules": [],
                   "install_requires": [],
                   "build_requires": [],
                   "packages": [],
                   "extensions": {},
                   "compiled_libraries": {},
                   "sub_directory": None,
                   }

        def update(library_dict, c):
            if type(c) == list:
                for i in c:
                    update(library_dict, i)
            elif c.type == "name":
                library_dict["name"] = c.value
            elif c.type == "modules":
                library_dict["py_modules"].extend(c.value)
            elif c.type == "packages":
                library_dict["packages"].extend(c.value)
            elif c.type in ("build_requires", "install_requires"):
                library_dict[c.type].extend(c.value)
            elif c.type == "extension":
                name = c.value["name"]
                library_dict["extensions"][name] = c.value
            elif c.type == "compiled_library":
                name = c.value["name"]
                library_dict["compiled_libraries"][name] = c.value
            elif c.type == "sub_directory":
                library_dict["sub_directory"] = c.value
            else:
                raise ValueError("Unhandled node type: %s" % c)

        if len(node.children) > 1:
            nodes = [node.children[0]] + node.children[1]
        else:
            nodes = [node.children[0]]
        for c in nodes:
            update(library, c)
        return Node("library", value=library)

    def library_name(self, node):
        return Node("name", value=node.value)

    def library_stmts(self, node):
        return node.children

    def extension(self, node):
        ret = {}
        seen = set()

        def _ensure_unique(field):
            if field in seen:
                raise ValueError("Field %r for extension %r is specified more than once !" % (field, ret["name"]))
            else:
                seen.add(field)

        def update(extension_dict, c):
            if type(c) == list:
                for i in c:
                    update(extension_dict, i)
            elif c.type == "name":
                ret["name"] = c.value
            elif c.type == "sources":
                _ensure_unique("sources")
                ret["sources"] = c.value
            elif c.type == "include_dirs":
                _ensure_unique("include_dirs")
                ret["include_dirs"] = c.value
            else:
                raise ValueError("Gne ?")
        for c in [node.children[0]] + node.children[1]:
            update(ret, c)
        return Node("extension", value=ret)

    def extension_field_stmts(self, node):
        return node.children

    def extension_declaration(self, node):
        return Node("name", value=node.value)

    def compiled_library(self, node):
        ret = {"sources": [], "include_dirs": []}
        seen = set()

        def _ensure_unique(field):
            if field in seen:
                raise ValueError("Field %r for compiled library %r is specified more than once !" % (field, ret["name"]))
            else:
                seen.add(field)

        def update(compiled_library_dict, c):
            if c.type == "name":
                ret["name"] = c.value
            elif c.type == "sources":
                _ensure_unique("sources")
                ret["sources"] = c.value
            elif c.type == "include_dirs":
                _ensure_unique("include_dirs")
                ret["include_dirs"] = c.value
            else:
                raise ValueError("Unknown node %s" % c)
        for c in [node.children[0]] + node.children[1]:
            update(ret, c)
        return Node("compiled_library", value=ret)

    def compiled_library_field_stmts(self, node):
        return node.children

    def compiled_library_declaration(self, node):
        return Node("name", value=node.value)

    def sub_directory(self, node):
        return Node("sub_directory", value=node.value)

    def use_backends(self, node):
        return Node("use_backends", value=node.value)

    #-----------------
    #   Path option
    #-----------------
    def path_stmts(self, node):
        return node.children

    def path(self, node):
        path = {}
        def update(c):
            if type(c) == list:
                for i in c:
                    update(i)
            elif c.type == "path_declaration":
                path["name"] = c.value
            elif c.type == "path_description":
                path["description"] = c.value
            elif c.type == "default":
                path["default"] = c.value
            else:
                raise SyntaxError("GNe ?")
        if len(node.children) > 1:
            nodes = [node.children[0]] + node.children[1]
        else:
            nodes = [node.children[0]]
        for node in nodes:
            update(node)

        if not "description" in path:
            raise ValueError("missing description in path section %r" %
                             (path["name"],))
        if not "default" in path:
            raise ValueError("missing default in path section %r" %
                             (path["name"],))
        return Node("path", value=path)

    def path_default(self, node):
        return Node("default", value=node.value)

    def path_description(self, node):
        return node

    #-----------------
    #   Flag option
    #-----------------
    # XXX: refactor path/flag handling, as they are almost identical
    def flag(self, node):
        flag = {}
        for i in [node.children[0]] + node.children[1]:
            if i.type == "flag_declaration":
                flag["name"] = i.value
            elif i.type == "flag_description":
                flag["description"] = i.value
            elif i.type == "default":
                flag["default"] = i.value
            else:
                raise SyntaxError("GNe ?")

        if not flag["default"] in ["true", "false"]:
            raise SyntaxError("invalid default value %s for flag %s" \
                              % (flag["default"], flag["name"])) 

        if not flag["name"] in self._vars:
            self._vars[flag["name"]] = flag["default"]

        return Node("flag", value=flag)

    def flag_default(self, node):
        return Node("default", value=node.value)

    def flag_stmts(self, node):
        return node.children

    def flag_description(self, node):
        return node

    #-------------------
    #   Conditionals
    #-------------------
    def conditional(self, node):
        test = node.value
        if self.action_dict[test.type](test):
            return node.children[:1]
        else:
            return node.children[1:]

    def osvar(self, node):
        os_name = node.value
        return os_name == sys.platform

    def bool_var(self, node):
        return node.value

    def not_flagvar(self, node):
        name = node.value
        try:
            value = self._vars[name]
        except KeyError:
            raise ValueError("Unknown flag variable %s" % name)
        else:
            return not _LIT_BOOL[value]

    def flagvar(self, node):
        name = node.value
        try:
            value = self._vars[name]
        except KeyError:
            raise ValueError("Unknown flag variable %s" % name)
        else:
            return _LIT_BOOL[value]

    def extra_source_files(self, node):
        self._d["extra_source_files"].extend(node.value)

    def subento(self, node):
        if "subento" in self._d:
            self._d["subento"].extend(node.value)
        else:
            self._d["subento"] = node.value

    # Data handling
    def data_files(self, node):
        d = {}

        def update(data_d, c):
            if type(c) == list:
                for  i in c:
                    update(data_d, i)
            elif c.type == "data_files_declaration":
                d["name"] = c.value
            elif c.type == "source_dir":
                d["source_dir"] = c.value
            elif c.type == "target_dir":
                d["target_dir"] = c.value
            elif c.type == "files":
                d["files"] = c.value
            else:
                raise ValueError("Unhandled node type: %s" % c)

        for c in node.children:
            update(d, c)

        return Node("data_files", value=d)

    def data_files_stmts(self, node):
        return node.children

    # Executable handling
    def executable(self, node):
        d = {}

        def update(exec_d, c):
            if type(c) == list:
                for  i in c:
                    update(exec_d, i)
            elif c.type == "name":
                exec_d["name"] = c.value
            elif c.type == "module":
                exec_d["module"] = c.value
            elif c.type == "function":
                exec_d["function"] = c.value
            else:
                raise ValueError("Unhandled node type: %s" % c)

        for c in node.children:
            update(d, c)

        return Node("executable", value=d)

    def exec_stmts(self, node):
        return node.children

    def exec_name(self, node):
        return Node("name", value=node.value)

    def function(self, node):
        return Node("function", value=node.value)

    def module(self, node):
        return Node("module", value=node.value)

########NEW FILE########
__FILENAME__ = bytecode
import sys

# Utils to compile .py to .pyc inside zip file
# XXX: this is implementation detail of .pyc, copied from py_compile.py.
# Unfortunately, there is no way that I know of to write the bytecode into a
# string to be used by ZipFile (using compiler is way too slow). Also, the
# py_compile code has not changed much for 10 years.
# XXX: the code has changed quite a few times in python 3.x timeline, we need
# to keep too many copies. Maybe it is not worth it to support this feature
# altogether ?
from py_compile \
    import \
        PyCompileError
if sys.version_info[0] < 3:
    from _bytecode_2 \
        import \
            bcompile
else:
    from bento.private._bytecode_3 \
        import \
            bcompile

########NEW FILE########
__FILENAME__ = version
import re

__all__ = ['NormalizedVersion', 'suggest_normalized_version',
           'VersionPredicate', 'is_valid_version', 'is_valid_versions',
           'is_valid_predicate']

# A marker used in the second and third parts of the `parts` tuple, for
# versions that don't have those segments, to sort properly. An example
# of versions in sort order ('highest' last):
#   1.0b1                 ((1,0), ('b',1), ('f',))
#   1.0.dev345            ((1,0), ('f',),  ('dev', 345))
#   1.0                   ((1,0), ('f',),  ('f',))
#   1.0.post256.dev345    ((1,0), ('f',),  ('f', 'post', 256, 'dev', 345))
#   1.0.post345           ((1,0), ('f',),  ('f', 'post', 345, 'f'))
#                                   ^        ^                 ^
#   'b' < 'f' ---------------------/         |                 |
#                                            |                 |
#   'dev' < 'f' < 'post' -------------------/                  |
#                                                              |
#   'dev' < 'f' ----------------------------------------------/
# Other letters would do, but 'f' for 'final' is kind of nice.
_FINAL_MARKER = ('f',)

_VERSION_RE = re.compile(r'''
    ^
    (?P<version>\d+\.\d+)          # minimum 'N.N'
    (?P<extraversion>(?:\.\d+)*)   # any number of extra '.N' segments
    (?:
        (?P<prerel>[abc]|rc)       # 'a'=alpha, 'b'=beta, 'c'=release candidate
                                   # 'rc'= alias for release candidate
        (?P<prerelversion>\d+(?:\.\d+)*)
    )?
    (?P<postdev>(\.post(?P<post>\d+))?(\.dev(?P<dev>\d+))?)?
    $''', re.VERBOSE)

class IrrationalVersionError(Exception):
    """This is an irrational version."""
    pass


class HugeMajorVersionNumError(IrrationalVersionError):
    """An irrational version because the major version number is huge
    (often because a year or date was used).

    See `error_on_huge_major_num` option in `NormalizedVersion` for details.
    This guard can be disabled by setting that option False.
    """
    pass

class NormalizedVersion(object):
    """A rational version.

    Good:
        1.2         # equivalent to "1.2.0"
        1.2.0
        1.2a1
        1.2.3a2
        1.2.3b1
        1.2.3c1
        1.2.3.4
        TODO: fill this out

    Bad:
        1           # mininum two numbers
        1.2a        # release level must have a release serial
        1.2.3b
    """
    def __init__(self, s, error_on_huge_major_num=True):
        """Create a NormalizedVersion instance from a version string.

        @param s {str} The version string.
        @param error_on_huge_major_num {bool} Whether to consider an
            apparent use of a year or full date as the major version number
            an error. Default True. One of the observed patterns on PyPI before
            the introduction of `NormalizedVersion` was version numbers like
            this:
                2009.01.03
                20040603
                2005.01
            This guard is here to strongly encourage the package author to
            use an alternate version, because a release deployed into PyPI
            and, e.g. downstream Linux package managers, will forever remove
            the possibility of using a version number like "1.0" (i.e.
            where the major number is less than that huge major number).
        """
        self.is_final = True  # by default, consider a version as final.
        self._parse(s, error_on_huge_major_num)

    @classmethod
    def from_parts(cls, version, prerelease=_FINAL_MARKER,
                   devpost=_FINAL_MARKER):
        return cls(cls.parts_to_str((version, prerelease, devpost)))

    def _parse(self, s, error_on_huge_major_num=True):
        """Parses a string version into parts."""
        match = _VERSION_RE.search(s)
        if not match:
            raise IrrationalVersionError(s)

        groups = match.groupdict()
        parts = []

        # main version
        block = self._parse_numdots(groups['version'], s, False, 2)
        extraversion = groups.get('extraversion')
        if extraversion not in ('', None):
            block += self._parse_numdots(extraversion[1:], s)
        parts.append(tuple(block))

        # prerelease
        prerel = groups.get('prerel')
        if prerel is not None:
            block = [prerel]
            block += self._parse_numdots(groups.get('prerelversion'), s,
                                         pad_zeros_length=1)
            parts.append(tuple(block))
            self.is_final = False
        else:
            parts.append(_FINAL_MARKER)

        # postdev
        if groups.get('postdev'):
            post = groups.get('post')
            dev = groups.get('dev')
            postdev = []
            if post is not None:
                postdev.extend([_FINAL_MARKER[0], 'post', int(post)])
                if dev is None:
                    postdev.append(_FINAL_MARKER[0])
            if dev is not None:
                postdev.extend(['dev', int(dev)])
                self.is_final = False
            parts.append(tuple(postdev))
        else:
            parts.append(_FINAL_MARKER)
        self.parts = tuple(parts)
        if error_on_huge_major_num and self.parts[0][0] > 1980:
            raise HugeMajorVersionNumError("huge major version number, %r, "
                "which might cause future problems: %r" % (self.parts[0][0], s))

    def _parse_numdots(self, s, full_ver_str, drop_trailing_zeros=True,
                       pad_zeros_length=0):
        """Parse 'N.N.N' sequences, return a list of ints.

        @param s {str} 'N.N.N...' sequence to be parsed
        @param full_ver_str {str} The full version string from which this
            comes. Used for error strings.
        @param drop_trailing_zeros {bool} Whether to drop trailing zeros
            from the returned list. Default True.
        @param pad_zeros_length {int} The length to which to pad the
            returned list with zeros, if necessary. Default 0.
        """
        nums = []
        for n in s.split("."):
            if len(n) > 1 and n[0] == '0':
                raise IrrationalVersionError("cannot have leading zero in "
                    "version number segment: '%s' in %r" % (n, full_ver_str))
            nums.append(int(n))
        if drop_trailing_zeros:
            while nums and nums[-1] == 0:
                nums.pop()
        while len(nums) < pad_zeros_length:
            nums.append(0)
        return nums

    def __str__(self):
        return self.parts_to_str(self.parts)

    @classmethod
    def parts_to_str(cls, parts):
        """Transforms a version expressed in tuple into its string
        representation."""
        # XXX This doesn't check for invalid tuples
        main, prerel, postdev = parts
        s = '.'.join(str(v) for v in main)
        if prerel is not _FINAL_MARKER:
            s += prerel[0]
            s += '.'.join(str(v) for v in prerel[1:])
        if postdev and postdev is not _FINAL_MARKER:
            if postdev[0] == 'f':
                postdev = postdev[1:]
            i = 0
            while i < len(postdev):
                if i % 2 == 0:
                    s += '.'
                s += str(postdev[i])
                i += 1
        return s

    def __repr__(self):
        return "%s('%s')" % (self.__class__.__name__, self)

    def _cannot_compare(self, other):
        raise TypeError("cannot compare %s and %s"
                % (type(self).__name__, type(other).__name__))

    def __eq__(self, other):
        if not isinstance(other, NormalizedVersion):
            self._cannot_compare(other)
        return self.parts == other.parts

    def __lt__(self, other):
        if not isinstance(other, NormalizedVersion):
            self._cannot_compare(other)
        return self.parts < other.parts

    def __ne__(self, other):
        return not self.__eq__(other)

    def __gt__(self, other):
        return not (self.__lt__(other) or self.__eq__(other))

    def __le__(self, other):
        return self.__eq__(other) or self.__lt__(other)

    def __ge__(self, other):
        return self.__eq__(other) or self.__gt__(other)

    # See http://docs.python.org/reference/datamodel#object.__hash__
    def __hash__(self):
        return hash(self.parts)


def suggest_normalized_version(s):
    """Suggest a normalized version close to the given version string.

    If you have a version string that isn't rational (i.e. NormalizedVersion
    doesn't like it) then you might be able to get an equivalent (or close)
    rational version from this function.

    This does a number of simple normalizations to the given string, based
    on observation of versions currently in use on PyPI. Given a dump of
    those version during PyCon 2009, 4287 of them:
    - 2312 (53.93%) match NormalizedVersion without change
      with the automatic suggestion
    - 3474 (81.04%) match when using this suggestion method

    @param s {str} An irrational version string.
    @returns A rational version string, or None, if couldn't determine one.
    """
    try:
        NormalizedVersion(s)
        return s   # already rational
    except IrrationalVersionError:
        pass

    rs = s.lower()

    # part of this could use maketrans
    for orig, repl in (('-alpha', 'a'), ('-beta', 'b'), ('alpha', 'a'),
                       ('beta', 'b'), ('rc', 'c'), ('-final', ''),
                       ('-pre', 'c'),
                       ('-release', ''), ('.release', ''), ('-stable', ''),
                       ('+', '.'), ('_', '.'), (' ', ''), ('.final', ''),
                       ('final', '')):
        rs = rs.replace(orig, repl)

    # if something ends with dev or pre, we add a 0
    rs = re.sub(r"pre$", r"pre0", rs)
    rs = re.sub(r"dev$", r"dev0", rs)

    # if we have something like "b-2" or "a.2" at the end of the
    # version, that is pobably beta, alpha, etc
    # let's remove the dash or dot
    rs = re.sub(r"([abc|rc])[\-\.](\d+)$", r"\1\2", rs)

    # 1.0-dev-r371 -> 1.0.dev371
    # 0.1-dev-r79 -> 0.1.dev79
    rs = re.sub(r"[\-\.](dev)[\-\.]?r?(\d+)$", r".\1\2", rs)

    # Clean: 2.0.a.3, 2.0.b1, 0.9.0~c1
    rs = re.sub(r"[.~]?([abc])\.?", r"\1", rs)

    # Clean: v0.3, v1.0
    if rs.startswith('v'):
        rs = rs[1:]

    # Clean leading '0's on numbers.
    #TODO: unintended side-effect on, e.g., "2003.05.09"
    # PyPI stats: 77 (~2%) better
    rs = re.sub(r"\b0+(\d+)(?!\d)", r"\1", rs)

    # Clean a/b/c with no version. E.g. "1.0a" -> "1.0a0". Setuptools infers
    # zero.
    # PyPI stats: 245 (7.56%) better
    rs = re.sub(r"(\d+[abc])$", r"\g<1>0", rs)

    # the 'dev-rNNN' tag is a dev tag
    rs = re.sub(r"\.?(dev-r|dev\.r)\.?(\d+)$", r".dev\2", rs)

    # clean the - when used as a pre delimiter
    rs = re.sub(r"-(a|b|c)(\d+)$", r"\1\2", rs)

    # a terminal "dev" or "devel" can be changed into ".dev0"
    rs = re.sub(r"[\.\-](dev|devel)$", r".dev0", rs)

    # a terminal "dev" can be changed into ".dev0"
    rs = re.sub(r"(?![\.\-])dev$", r".dev0", rs)

    # a terminal "final" or "stable" can be removed
    rs = re.sub(r"(final|stable)$", "", rs)

    # The 'r' and the '-' tags are post release tags
    #   0.4a1.r10       ->  0.4a1.post10
    #   0.9.33-17222    ->  0.9.3.post17222
    #   0.9.33-r17222   ->  0.9.3.post17222
    rs = re.sub(r"\.?(r|-|-r)\.?(\d+)$", r".post\2", rs)

    # Clean 'r' instead of 'dev' usage:
    #   0.9.33+r17222   ->  0.9.3.dev17222
    #   1.0dev123       ->  1.0.dev123
    #   1.0.git123      ->  1.0.dev123
    #   1.0.bzr123      ->  1.0.dev123
    #   0.1a0dev.123    ->  0.1a0.dev123
    # PyPI stats:  ~150 (~4%) better
    rs = re.sub(r"\.?(dev|git|bzr)\.?(\d+)$", r".dev\2", rs)

    # Clean '.pre' (normalized from '-pre' above) instead of 'c' usage:
    #   0.2.pre1        ->  0.2c1
    #   0.2-c1         ->  0.2c1
    #   1.0preview123   ->  1.0c123
    # PyPI stats: ~21 (0.62%) better
    rs = re.sub(r"\.?(pre|preview|-c)(\d+)$", r"c\g<2>", rs)

    # Tcl/Tk uses "px" for their post release markers
    rs = re.sub(r"p(\d+)$", r".post\1", rs)

    try:
        NormalizedVersion(rs)
        return rs   # already rational
    except IrrationalVersionError:
        pass
    return None


# A predicate is: "ProjectName (VERSION1, VERSION2, ..)
_PREDICATE = re.compile(r"(?i)^\s*(\w[\s\w-]*(?:\.\w*)*)(.*)")
_VERSIONS = re.compile(r"^\s*\((?P<versions>.*)\)\s*$|^\s*(?P<versions2>.*)\s*$")
_PLAIN_VERSIONS = re.compile(r"^\s*(.*)\s*$")
_SPLIT_CMP = re.compile(r"^\s*(<=|>=|<|>|!=|==)\s*([^\s,]+)\s*$")


def _split_predicate(predicate):
    match = _SPLIT_CMP.match(predicate)
    if match is None:
        # probably no op, we'll use "=="
        comp, version = '==', predicate
    else:
        comp, version = match.groups()
    return comp, NormalizedVersion(version)


class VersionPredicate(object):
    """Defines a predicate: ProjectName (>ver1,ver2, ..)"""

    _operators = {"<": lambda x, y: x < y,
                  ">": lambda x, y: x > y,
                  "<=": lambda x, y: str(x).startswith(str(y)) or x < y,
                  ">=": lambda x, y: str(x).startswith(str(y)) or x > y,
                  "==": lambda x, y: str(x).startswith(str(y)),
                  "!=": lambda x, y: not str(x).startswith(str(y)),
                  }

    def __init__(self, predicate):
        self._string = predicate
        predicate = predicate.strip()
        match = _PREDICATE.match(predicate)
        if match is None:
            raise ValueError('Bad predicate "%s"' % predicate)

        name, predicates = match.groups()
        self.name = name.strip()
        self.predicates = []
        if predicates is None:
            return

        predicates = _VERSIONS.match(predicates.strip())
        if predicates is None:
            return

        predicates = predicates.groupdict()
        if predicates['versions'] is not None:
            versions = predicates['versions']
        else:
            versions = predicates.get('versions2')

        if versions is not None:
            for version in versions.split(','):
                if version.strip() == '':
                    continue
                self.predicates.append(_split_predicate(version))

    def match(self, version):
        """Check if the provided version matches the predicates."""
        if isinstance(version, str):
            version = NormalizedVersion(version)
        for operator, predicate in self.predicates:
            if not self._operators[operator](version, predicate):
                return False
        return True

    def __repr__(self):
        return self._string


class _Versions(VersionPredicate):
    def __init__(self, predicate):
        predicate = predicate.strip()
        match = _PLAIN_VERSIONS.match(predicate)
        self.name = None
        predicates = match.groups()[0]
        self.predicates = [_split_predicate(pred.strip())
                           for pred in predicates.split(',')]


class _Version(VersionPredicate):
    def __init__(self, predicate):
        predicate = predicate.strip()
        match = _PLAIN_VERSIONS.match(predicate)
        self.name = None
        self.predicates = _split_predicate(match.groups()[0])


def is_valid_predicate(predicate):
    try:
        VersionPredicate(predicate)
    except (ValueError, IrrationalVersionError):
        return False
    else:
        return True


def is_valid_versions(predicate):
    try:
        _Versions(predicate)
    except (ValueError, IrrationalVersionError):
        return False
    else:
        return True


def is_valid_version(predicate):
    try:
        _Version(predicate)
    except (ValueError, IrrationalVersionError):
        return False
    else:
        return True


def get_version_predicate(requirements):
    """Return a VersionPredicate object, from a string or an already
    existing object.
    """
    if isinstance(requirements, str):
        requirements = VersionPredicate(requirements)
    return requirements

########NEW FILE########
__FILENAME__ = _bytecode_2
import os
import sys
import marshal

# XXX: implementation details from py_compile
from py_compile import \
    wr_long, MAGIC, PyCompileError
import __builtin__

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

def bcompile(source):
    """Return the compiled bytecode from the given filename as a string ."""
    f = open(source, 'U')
    try:
        try:
            timestamp = long(os.fstat(f.fileno()).st_mtime)
        except AttributeError:
            timestamp = long(os.stat(file).st_mtime)
        codestring = f.read()
        f.close()
        if codestring and codestring[-1] != '\n':
            codestring = codestring + '\n'
        try:
            codeobject = __builtin__.compile(codestring, source, 'exec')
        except Exception,err:
            raise PyCompileError(err.__class__, err.args, source)
        fc = StringIO()
        try:
            fc.write('\0\0\0\0')
            wr_long(fc, timestamp)
            fc.write(marshal.dumps(codeobject))
            fc.flush()
            fc.seek(0, 0)
            fc.write(MAGIC)
            return fc.getvalue()
        finally:
            fc.close()
    finally:
        f.close()

########NEW FILE########
__FILENAME__ = _bytecode_3
import os
import sys
import builtins
import io
import py_compile
import marshal

if sys.version_info[:2] < (3, 2):
    def _bcompile(file, cfile=None, dfile=None, doraise=False):
        encoding = py_compile.read_encoding(file, "utf-8")
        f = open(file, 'U', encoding=encoding)
        try:
            timestamp = int(os.fstat(f.fileno()).st_mtime)
        except AttributeError:
            timestamp = int(os.stat(file).st_mtime)
        codestring = f.read()
        f.close()
        if codestring and codestring[-1] != '\n':
            codestring = codestring + '\n'
        try:
            codeobject = builtins.compile(codestring, dfile or file,'exec')
        except Exception as err:
            py_exc = py_compile.PyCompileError(err.__class__, err, dfile or file)
            if doraise:
                raise py_exc
            else:
                sys.stderr.write(py_exc.msg + '\n')
                return
        fc = io.BytesIO()
        try:
            fc.write(b'\0\0\0\0')
            py_compile.wr_long(fc, timestamp)
            marshal.dump(codeobject, fc)
            fc.flush()
            fc.seek(0, 0)
            fc.write(py_compile.MAGIC)
            return fc.getvalue()
        finally:
            fc.close()
else:
    import tokenize
    import imp
    import errno
    def _bcompile(file, cfile=None, dfile=None, doraise=False, optimize=-1):
        with tokenize.open(file) as f:
            try:
                timestamp = int(os.fstat(f.fileno()).st_mtime)
            except AttributeError:
                timestamp = int(os.stat(file).st_mtime)
            codestring = f.read()
        try:
            codeobject = builtins.compile(codestring, dfile or file, 'exec',
                                          optimize=optimize)
        except Exception as err:
            py_exc = py_compile.PyCompileError(err.__class__, err, dfile or file)
            if doraise:
                raise py_exc
            else:
                sys.stderr.write(py_exc.msg + '\n')
                return
        if cfile is None:
            if optimize >= 0:
                cfile = imp.cache_from_source(file, debug_override=not optimize)
            else:
                cfile = imp.cache_from_source(file)
        try:
            os.makedirs(os.path.dirname(cfile))
        except OSError as error:
            if error.errno != errno.EEXIST:
                raise
        fc = io.BytesIO()
        try:
            fc.write(b'\0\0\0\0')
            py_compile.wr_long(fc, timestamp)
            marshal.dump(codeobject, fc)
            fc.flush()
            fc.seek(0, 0)
            fc.write(py_compile.MAGIC)
            return fc.getvalue()
        finally:
            fc.close()

def bcompile(file):
    return _bcompile(file, doraise=True)

########NEW FILE########
__FILENAME__ = lex
# -----------------------------------------------------------------------------
# ply: lex.py
#
# Copyright (C) 2001-2009,
# David M. Beazley (Dabeaz LLC)
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
# 
# * Redistributions of source code must retain the above copyright notice,
#   this list of conditions and the following disclaimer.  
# * Redistributions in binary form must reproduce the above copyright notice, 
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.  
# * Neither the name of the David Beazley or Dabeaz LLC may be used to
#   endorse or promote products derived from this software without
#  specific prior written permission. 
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
# -----------------------------------------------------------------------------

__version__    = "3.3"
__tabversion__ = "3.2"       # Version of table file used

import re, sys, types, copy, os

# This tuple contains known string types
try:
    # Python 2.6
    StringTypes = (types.StringType, types.UnicodeType)
except AttributeError:
    # Python 3.0
    StringTypes = (str, bytes)

# Extract the code attribute of a function. Different implementations
# are for Python 2/3 compatibility.

if sys.version_info[0] < 3:
    def func_code(f):
        return f.func_code
else:
    def func_code(f):
        return f.__code__

# This regular expression is used to match valid token names
_is_identifier = re.compile(r'^[a-zA-Z0-9_]+$')

# Exception thrown when invalid token encountered and no default error
# handler is defined.

class LexError(Exception):
    def __init__(self,message,s):
         self.args = (message,)
         self.text = s

# Token class.  This class is used to represent the tokens produced.
class LexToken(object):
    def __str__(self):
        return "LexToken(%s,%r,%d,%d)" % (self.type,self.value,self.lineno,self.lexpos)
    def __repr__(self):
        return str(self)

# This object is a stand-in for a logging object created by the 
# logging module.  

class PlyLogger(object):
    def __init__(self,f):
        self.f = f
    def critical(self,msg,*args,**kwargs):
        self.f.write((msg % args) + "\n")

    def warning(self,msg,*args,**kwargs):
        self.f.write("WARNING: "+ (msg % args) + "\n")

    def error(self,msg,*args,**kwargs):
        self.f.write("ERROR: " + (msg % args) + "\n")

    info = critical
    debug = critical

# Null logger is used when no output is generated. Does nothing.
class NullLogger(object):
    def __getattribute__(self,name):
        return self
    def __call__(self,*args,**kwargs):
        return self

# -----------------------------------------------------------------------------
#                        === Lexing Engine ===
#
# The following Lexer class implements the lexer runtime.   There are only
# a few public methods and attributes:
#
#    input()          -  Store a new string in the lexer
#    token()          -  Get the next token
#    clone()          -  Clone the lexer
#
#    lineno           -  Current line number
#    lexpos           -  Current position in the input string
# -----------------------------------------------------------------------------

class Lexer:
    def __init__(self):
        self.lexre = None             # Master regular expression. This is a list of
                                      # tuples (re,findex) where re is a compiled
                                      # regular expression and findex is a list
                                      # mapping regex group numbers to rules
        self.lexretext = None         # Current regular expression strings
        self.lexstatere = {}          # Dictionary mapping lexer states to master regexs
        self.lexstateretext = {}      # Dictionary mapping lexer states to regex strings
        self.lexstaterenames = {}     # Dictionary mapping lexer states to symbol names
        self.lexstate = "INITIAL"     # Current lexer state
        self.lexstatestack = []       # Stack of lexer states
        self.lexstateinfo = None      # State information
        self.lexstateignore = {}      # Dictionary of ignored characters for each state
        self.lexstateerrorf = {}      # Dictionary of error functions for each state
        self.lexreflags = 0           # Optional re compile flags
        self.lexdata = None           # Actual input data (as a string)
        self.lexpos = 0               # Current position in input text
        self.lexlen = 0               # Length of the input text
        self.lexerrorf = None         # Error rule (if any)
        self.lextokens = None         # List of valid tokens
        self.lexignore = ""           # Ignored characters
        self.lexliterals = ""         # Literal characters that can be passed through
        self.lexmodule = None         # Module
        self.lineno = 1               # Current line number
        self.lexoptimize = 0          # Optimized mode

    def clone(self,object=None):
        c = copy.copy(self)

        # If the object parameter has been supplied, it means we are attaching the
        # lexer to a new object.  In this case, we have to rebind all methods in
        # the lexstatere and lexstateerrorf tables.

        if object:
            newtab = { }
            for key, ritem in self.lexstatere.items():
                newre = []
                for cre, findex in ritem:
                     newfindex = []
                     for f in findex:
                         if not f or not f[0]:
                             newfindex.append(f)
                             continue
                         newfindex.append((getattr(object,f[0].__name__),f[1]))
                newre.append((cre,newfindex))
                newtab[key] = newre
            c.lexstatere = newtab
            c.lexstateerrorf = { }
            for key, ef in self.lexstateerrorf.items():
                c.lexstateerrorf[key] = getattr(object,ef.__name__)
            c.lexmodule = object
        return c

    # ------------------------------------------------------------
    # writetab() - Write lexer information to a table file
    # ------------------------------------------------------------
    def writetab(self,tabfile,outputdir=""):
        if isinstance(tabfile,types.ModuleType):
            return
        basetabfilename = tabfile.split(".")[-1]
        filename = os.path.join(outputdir,basetabfilename)+".py"
        tf = open(filename,"w")
        tf.write("# %s.py. This file automatically created by PLY (version %s). Don't edit!\n" % (tabfile,__version__))
        tf.write("_tabversion   = %s\n" % repr(__version__))
        tf.write("_lextokens    = %s\n" % repr(self.lextokens))
        tf.write("_lexreflags   = %s\n" % repr(self.lexreflags))
        tf.write("_lexliterals  = %s\n" % repr(self.lexliterals))
        tf.write("_lexstateinfo = %s\n" % repr(self.lexstateinfo))

        tabre = { }
        # Collect all functions in the initial state
        initial = self.lexstatere["INITIAL"]
        initialfuncs = []
        for part in initial:
            for f in part[1]:
                if f and f[0]:
                    initialfuncs.append(f)

        for key, lre in self.lexstatere.items():
             titem = []
             for i in range(len(lre)):
                  titem.append((self.lexstateretext[key][i],_funcs_to_names(lre[i][1],self.lexstaterenames[key][i])))
             tabre[key] = titem

        tf.write("_lexstatere   = %s\n" % repr(tabre))
        tf.write("_lexstateignore = %s\n" % repr(self.lexstateignore))

        taberr = { }
        for key, ef in self.lexstateerrorf.items():
             if ef:
                  taberr[key] = ef.__name__
             else:
                  taberr[key] = None
        tf.write("_lexstateerrorf = %s\n" % repr(taberr))
        tf.close()

    # ------------------------------------------------------------
    # readtab() - Read lexer information from a tab file
    # ------------------------------------------------------------
    def readtab(self,tabfile,fdict):
        if isinstance(tabfile,types.ModuleType):
            lextab = tabfile
        else:
            if sys.version_info[0] < 3:
                exec("import %s as lextab" % tabfile)
            else:
                env = { }
                exec("import %s as lextab" % tabfile, env,env)
                lextab = env['lextab']

        if getattr(lextab,"_tabversion","0.0") != __version__:
            raise ImportError("Inconsistent PLY version")

        self.lextokens      = lextab._lextokens
        self.lexreflags     = lextab._lexreflags
        self.lexliterals    = lextab._lexliterals
        self.lexstateinfo   = lextab._lexstateinfo
        self.lexstateignore = lextab._lexstateignore
        self.lexstatere     = { }
        self.lexstateretext = { }
        for key,lre in lextab._lexstatere.items():
             titem = []
             txtitem = []
             for i in range(len(lre)):
                  titem.append((re.compile(lre[i][0],lextab._lexreflags | re.VERBOSE),_names_to_funcs(lre[i][1],fdict)))
                  txtitem.append(lre[i][0])
             self.lexstatere[key] = titem
             self.lexstateretext[key] = txtitem
        self.lexstateerrorf = { }
        for key,ef in lextab._lexstateerrorf.items():
             self.lexstateerrorf[key] = fdict[ef]
        self.begin('INITIAL')

    # ------------------------------------------------------------
    # input() - Push a new string into the lexer
    # ------------------------------------------------------------
    def input(self,s):
        # Pull off the first character to see if s looks like a string
        c = s[:1]
        if not isinstance(c,StringTypes):
            raise ValueError("Expected a string")
        self.lexdata = s
        self.lexpos = 0
        self.lexlen = len(s)

    # ------------------------------------------------------------
    # begin() - Changes the lexing state
    # ------------------------------------------------------------
    def begin(self,state):
        if not state in self.lexstatere:
            raise ValueError("Undefined state")
        self.lexre = self.lexstatere[state]
        self.lexretext = self.lexstateretext[state]
        self.lexignore = self.lexstateignore.get(state,"")
        self.lexerrorf = self.lexstateerrorf.get(state,None)
        self.lexstate = state

    # ------------------------------------------------------------
    # push_state() - Changes the lexing state and saves old on stack
    # ------------------------------------------------------------
    def push_state(self,state):
        self.lexstatestack.append(self.lexstate)
        self.begin(state)

    # ------------------------------------------------------------
    # pop_state() - Restores the previous state
    # ------------------------------------------------------------
    def pop_state(self):
        self.begin(self.lexstatestack.pop())

    # ------------------------------------------------------------
    # current_state() - Returns the current lexing state
    # ------------------------------------------------------------
    def current_state(self):
        return self.lexstate

    # ------------------------------------------------------------
    # skip() - Skip ahead n characters
    # ------------------------------------------------------------
    def skip(self,n):
        self.lexpos += n

    # ------------------------------------------------------------
    # opttoken() - Return the next token from the Lexer
    #
    # Note: This function has been carefully implemented to be as fast
    # as possible.  Don't make changes unless you really know what
    # you are doing
    # ------------------------------------------------------------
    def token(self):
        # Make local copies of frequently referenced attributes
        lexpos    = self.lexpos
        lexlen    = self.lexlen
        lexignore = self.lexignore
        lexdata   = self.lexdata

        while lexpos < lexlen:
            # This code provides some short-circuit code for whitespace, tabs, and other ignored characters
            if lexdata[lexpos] in lexignore:
                lexpos += 1
                continue

            # Look for a regular expression match
            for lexre,lexindexfunc in self.lexre:
                m = lexre.match(lexdata,lexpos)
                if not m: continue

                # Create a token for return
                tok = LexToken()
                tok.value = m.group()
                tok.lineno = self.lineno
                tok.lexpos = lexpos

                i = m.lastindex
                func,tok.type = lexindexfunc[i]

                if not func:
                   # If no token type was set, it's an ignored token
                   if tok.type:
                      self.lexpos = m.end()
                      return tok
                   else:
                      lexpos = m.end()
                      break

                lexpos = m.end()

                # If token is processed by a function, call it

                tok.lexer = self      # Set additional attributes useful in token rules
                self.lexmatch = m
                self.lexpos = lexpos

                newtok = func(tok)

                # Every function must return a token, if nothing, we just move to next token
                if not newtok:
                    lexpos    = self.lexpos         # This is here in case user has updated lexpos.
                    lexignore = self.lexignore      # This is here in case there was a state change
                    break

                # Verify type of the token.  If not in the token map, raise an error
                if not self.lexoptimize:
                    if not newtok.type in self.lextokens:
                        raise LexError("%s:%d: Rule '%s' returned an unknown token type '%s'" % (
                            func_code(func).co_filename, func_code(func).co_firstlineno,
                            func.__name__, newtok.type),lexdata[lexpos:])

                return newtok
            else:
                # No match, see if in literals
                if lexdata[lexpos] in self.lexliterals:
                    tok = LexToken()
                    tok.value = lexdata[lexpos]
                    tok.lineno = self.lineno
                    tok.type = tok.value
                    tok.lexpos = lexpos
                    self.lexpos = lexpos + 1
                    return tok

                # No match. Call t_error() if defined.
                if self.lexerrorf:
                    tok = LexToken()
                    tok.value = self.lexdata[lexpos:]
                    tok.lineno = self.lineno
                    tok.type = "error"
                    tok.lexer = self
                    tok.lexpos = lexpos
                    self.lexpos = lexpos
                    newtok = self.lexerrorf(tok)
                    if lexpos == self.lexpos:
                        # Error method didn't change text position at all. This is an error.
                        raise LexError("Scanning error. Illegal character '%s'" % (lexdata[lexpos]), lexdata[lexpos:])
                    lexpos = self.lexpos
                    if not newtok: continue
                    return newtok

                self.lexpos = lexpos
                raise LexError("Illegal character '%s' at index %d" % (lexdata[lexpos],lexpos), lexdata[lexpos:])

        self.lexpos = lexpos + 1
        if self.lexdata is None:
             raise RuntimeError("No input string given with input()")
        return None

    # Iterator interface
    def __iter__(self):
        return self

    def next(self):
        t = self.token()
        if t is None:
            raise StopIteration
        return t

    __next__ = next

# -----------------------------------------------------------------------------
#                           ==== Lex Builder ===
#
# The functions and classes below are used to collect lexing information
# and build a Lexer object from it.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# get_caller_module_dict()
#
# This function returns a dictionary containing all of the symbols defined within
# a caller further down the call stack.  This is used to get the environment
# associated with the yacc() call if none was provided.
# -----------------------------------------------------------------------------

def get_caller_module_dict(levels):
    try:
        raise RuntimeError
    except RuntimeError:
        e,b,t = sys.exc_info()
        f = t.tb_frame
        while levels > 0:
            f = f.f_back                   
            levels -= 1
        ldict = f.f_globals.copy()
        if f.f_globals != f.f_locals:
            ldict.update(f.f_locals)

        return ldict

# -----------------------------------------------------------------------------
# _funcs_to_names()
#
# Given a list of regular expression functions, this converts it to a list
# suitable for output to a table file
# -----------------------------------------------------------------------------

def _funcs_to_names(funclist,namelist):
    result = []
    for f,name in zip(funclist,namelist):
         if f and f[0]:
             result.append((name, f[1]))
         else:
             result.append(f)
    return result

# -----------------------------------------------------------------------------
# _names_to_funcs()
#
# Given a list of regular expression function names, this converts it back to
# functions.
# -----------------------------------------------------------------------------

def _names_to_funcs(namelist,fdict):
     result = []
     for n in namelist:
          if n and n[0]:
              result.append((fdict[n[0]],n[1]))
          else:
              result.append(n)
     return result

# -----------------------------------------------------------------------------
# _form_master_re()
#
# This function takes a list of all of the regex components and attempts to
# form the master regular expression.  Given limitations in the Python re
# module, it may be necessary to break the master regex into separate expressions.
# -----------------------------------------------------------------------------

def _form_master_re(relist,reflags,ldict,toknames):
    if not relist: return []
    regex = "|".join(relist)
    try:
        lexre = re.compile(regex,re.VERBOSE | reflags)

        # Build the index to function map for the matching engine
        lexindexfunc = [ None ] * (max(lexre.groupindex.values())+1)
        lexindexnames = lexindexfunc[:]

        for f,i in lexre.groupindex.items():
            handle = ldict.get(f,None)
            if type(handle) in (types.FunctionType, types.MethodType):
                lexindexfunc[i] = (handle,toknames[f])
                lexindexnames[i] = f
            elif handle is not None:
                lexindexnames[i] = f
                if f.find("ignore_") > 0:
                    lexindexfunc[i] = (None,None)
                else:
                    lexindexfunc[i] = (None, toknames[f])
        
        return [(lexre,lexindexfunc)],[regex],[lexindexnames]
    except Exception:
        m = int(len(relist)/2)
        if m == 0: m = 1
        llist, lre, lnames = _form_master_re(relist[:m],reflags,ldict,toknames)
        rlist, rre, rnames = _form_master_re(relist[m:],reflags,ldict,toknames)
        return llist+rlist, lre+rre, lnames+rnames

# -----------------------------------------------------------------------------
# def _statetoken(s,names)
#
# Given a declaration name s of the form "t_" and a dictionary whose keys are
# state names, this function returns a tuple (states,tokenname) where states
# is a tuple of state names and tokenname is the name of the token.  For example,
# calling this with s = "t_foo_bar_SPAM" might return (('foo','bar'),'SPAM')
# -----------------------------------------------------------------------------

def _statetoken(s,names):
    nonstate = 1
    parts = s.split("_")
    for i in range(1,len(parts)):
         if not parts[i] in names and parts[i] != 'ANY': break
    if i > 1:
       states = tuple(parts[1:i])
    else:
       states = ('INITIAL',)

    if 'ANY' in states:
       states = tuple(names)

    tokenname = "_".join(parts[i:])
    return (states,tokenname)


# -----------------------------------------------------------------------------
# LexerReflect()
#
# This class represents information needed to build a lexer as extracted from a
# user's input file.
# -----------------------------------------------------------------------------
class LexerReflect(object):
    def __init__(self,ldict,log=None,reflags=0):
        self.ldict      = ldict
        self.error_func = None
        self.tokens     = []
        self.reflags    = reflags
        self.stateinfo  = { 'INITIAL' : 'inclusive'}
        self.files      = {}
        self.error      = 0

        if log is None:
            self.log = PlyLogger(sys.stderr)
        else:
            self.log = log

    # Get all of the basic information
    def get_all(self):
        self.get_tokens()
        self.get_literals()
        self.get_states()
        self.get_rules()
        
    # Validate all of the information
    def validate_all(self):
        self.validate_tokens()
        self.validate_literals()
        self.validate_rules()
        return self.error

    # Get the tokens map
    def get_tokens(self):
        tokens = self.ldict.get("tokens",None)
        if not tokens:
            self.log.error("No token list is defined")
            self.error = 1
            return

        if not isinstance(tokens,(list, tuple)):
            self.log.error("tokens must be a list or tuple")
            self.error = 1
            return
        
        if not tokens:
            self.log.error("tokens is empty")
            self.error = 1
            return

        self.tokens = tokens

    # Validate the tokens
    def validate_tokens(self):
        terminals = {}
        for n in self.tokens:
            if not _is_identifier.match(n):
                self.log.error("Bad token name '%s'",n)
                self.error = 1
            if n in terminals:
                self.log.warning("Token '%s' multiply defined", n)
            terminals[n] = 1

    # Get the literals specifier
    def get_literals(self):
        self.literals = self.ldict.get("literals","")

    # Validate literals
    def validate_literals(self):
        try:
            for c in self.literals:
                if not isinstance(c,StringTypes) or len(c) > 1:
                    self.log.error("Invalid literal %s. Must be a single character", repr(c))
                    self.error = 1
                    continue

        except TypeError:
            self.log.error("Invalid literals specification. literals must be a sequence of characters")
            self.error = 1

    def get_states(self):
        self.states = self.ldict.get("states",None)
        # Build statemap
        if self.states:
             if not isinstance(self.states,(tuple,list)):
                  self.log.error("states must be defined as a tuple or list")
                  self.error = 1
             else:
                  for s in self.states:
                        if not isinstance(s,tuple) or len(s) != 2:
                               self.log.error("Invalid state specifier %s. Must be a tuple (statename,'exclusive|inclusive')",repr(s))
                               self.error = 1
                               continue
                        name, statetype = s
                        if not isinstance(name,StringTypes):
                               self.log.error("State name %s must be a string", repr(name))
                               self.error = 1
                               continue
                        if not (statetype == 'inclusive' or statetype == 'exclusive'):
                               self.log.error("State type for state %s must be 'inclusive' or 'exclusive'",name)
                               self.error = 1
                               continue
                        if name in self.stateinfo:
                               self.log.error("State '%s' already defined",name)
                               self.error = 1
                               continue
                        self.stateinfo[name] = statetype

    # Get all of the symbols with a t_ prefix and sort them into various
    # categories (functions, strings, error functions, and ignore characters)

    def get_rules(self):
        tsymbols = [f for f in self.ldict if f[:2] == 't_' ]

        # Now build up a list of functions and a list of strings

        self.toknames = { }        # Mapping of symbols to token names
        self.funcsym =  { }        # Symbols defined as functions
        self.strsym =   { }        # Symbols defined as strings
        self.ignore   = { }        # Ignore strings by state
        self.errorf   = { }        # Error functions by state

        for s in self.stateinfo:
             self.funcsym[s] = []
             self.strsym[s] = []

        if len(tsymbols) == 0:
            self.log.error("No rules of the form t_rulename are defined")
            self.error = 1
            return

        for f in tsymbols:
            t = self.ldict[f]
            states, tokname = _statetoken(f,self.stateinfo)
            self.toknames[f] = tokname

            if hasattr(t,"__call__"):
                if tokname == 'error':
                    for s in states:
                        self.errorf[s] = t
                elif tokname == 'ignore':
                    line = func_code(t).co_firstlineno
                    file = func_code(t).co_filename
                    self.log.error("%s:%d: Rule '%s' must be defined as a string",file,line,t.__name__)
                    self.error = 1
                else:
                    for s in states: 
                        self.funcsym[s].append((f,t))
            elif isinstance(t, StringTypes):
                if tokname == 'ignore':
                    for s in states:
                        self.ignore[s] = t
                    if "\\" in t:
                        self.log.warning("%s contains a literal backslash '\\'",f)

                elif tokname == 'error':
                    self.log.error("Rule '%s' must be defined as a function", f)
                    self.error = 1
                else:
                    for s in states: 
                        self.strsym[s].append((f,t))
            else:
                self.log.error("%s not defined as a function or string", f)
                self.error = 1

        # Sort the functions by line number
        for f in self.funcsym.values():
            if sys.version_info[0] < 3:
                f.sort(lambda x,y: cmp(func_code(x[1]).co_firstlineno,func_code(y[1]).co_firstlineno))
            else:
                # Python 3.0
                f.sort(key=lambda x: func_code(x[1]).co_firstlineno)

        # Sort the strings by regular expression length
        for s in self.strsym.values():
            if sys.version_info[0] < 3:
                s.sort(lambda x,y: (len(x[1]) < len(y[1])) - (len(x[1]) > len(y[1])))
            else:
                # Python 3.0
                s.sort(key=lambda x: len(x[1]),reverse=True)

    # Validate all of the t_rules collected 
    def validate_rules(self):
        for state in self.stateinfo:
            # Validate all rules defined by functions

            

            for fname, f in self.funcsym[state]:
                line = func_code(f).co_firstlineno
                file = func_code(f).co_filename
                self.files[file] = 1

                tokname = self.toknames[fname]
                if isinstance(f, types.MethodType):
                    reqargs = 2
                else:
                    reqargs = 1
                nargs = func_code(f).co_argcount
                if nargs > reqargs:
                    self.log.error("%s:%d: Rule '%s' has too many arguments",file,line,f.__name__)
                    self.error = 1
                    continue

                if nargs < reqargs:
                    self.log.error("%s:%d: Rule '%s' requires an argument", file,line,f.__name__)
                    self.error = 1
                    continue

                if not f.__doc__:
                    self.log.error("%s:%d: No regular expression defined for rule '%s'",file,line,f.__name__)
                    self.error = 1
                    continue

                try:
                    c = re.compile("(?P<%s>%s)" % (fname,f.__doc__), re.VERBOSE | self.reflags)
                    if c.match(""):
                        self.log.error("%s:%d: Regular expression for rule '%s' matches empty string", file,line,f.__name__)
                        self.error = 1
                except re.error:
                    _etype, e, _etrace = sys.exc_info()
                    self.log.error("%s:%d: Invalid regular expression for rule '%s'. %s", file,line,f.__name__,e)
                    if '#' in f.__doc__:
                        self.log.error("%s:%d. Make sure '#' in rule '%s' is escaped with '\\#'",file,line, f.__name__)
                    self.error = 1

            # Validate all rules defined by strings
            for name,r in self.strsym[state]:
                tokname = self.toknames[name]
                if tokname == 'error':
                    self.log.error("Rule '%s' must be defined as a function", name)
                    self.error = 1
                    continue

                if not tokname in self.tokens and tokname.find("ignore_") < 0:
                    self.log.error("Rule '%s' defined for an unspecified token %s",name,tokname)
                    self.error = 1
                    continue

                try:
                    c = re.compile("(?P<%s>%s)" % (name,r),re.VERBOSE | self.reflags)
                    if (c.match("")):
                         self.log.error("Regular expression for rule '%s' matches empty string",name)
                         self.error = 1
                except re.error:
                    _etype, e, _etrace = sys.exc_info()
                    self.log.error("Invalid regular expression for rule '%s'. %s",name,e)
                    if '#' in r:
                         self.log.error("Make sure '#' in rule '%s' is escaped with '\\#'",name)
                    self.error = 1

            if not self.funcsym[state] and not self.strsym[state]:
                self.log.error("No rules defined for state '%s'",state)
                self.error = 1

            # Validate the error function
            efunc = self.errorf.get(state,None)
            if efunc:
                f = efunc
                line = func_code(f).co_firstlineno
                file = func_code(f).co_filename
                self.files[file] = 1

                if isinstance(f, types.MethodType):
                    reqargs = 2
                else:
                    reqargs = 1
                nargs = func_code(f).co_argcount
                if nargs > reqargs:
                    self.log.error("%s:%d: Rule '%s' has too many arguments",file,line,f.__name__)
                    self.error = 1

                if nargs < reqargs:
                    self.log.error("%s:%d: Rule '%s' requires an argument", file,line,f.__name__)
                    self.error = 1

        for f in self.files:
            self.validate_file(f)


    # -----------------------------------------------------------------------------
    # validate_file()
    #
    # This checks to see if there are duplicated t_rulename() functions or strings
    # in the parser input file.  This is done using a simple regular expression
    # match on each line in the given file.  
    # -----------------------------------------------------------------------------

    def validate_file(self,filename):
        import os.path
        base,ext = os.path.splitext(filename)
        if ext != '.py': return         # No idea what the file is. Return OK

        try:
            f = open(filename)
            lines = f.readlines()
            f.close()
        except IOError:
            return                      # Couldn't find the file.  Don't worry about it

        fre = re.compile(r'\s*def\s+(t_[a-zA-Z_0-9]*)\(')
        sre = re.compile(r'\s*(t_[a-zA-Z_0-9]*)\s*=')

        counthash = { }
        linen = 1
        for l in lines:
            m = fre.match(l)
            if not m:
                m = sre.match(l)
            if m:
                name = m.group(1)
                prev = counthash.get(name)
                if not prev:
                    counthash[name] = linen
                else:
                    self.log.error("%s:%d: Rule %s redefined. Previously defined on line %d",filename,linen,name,prev)
                    self.error = 1
            linen += 1
            
# -----------------------------------------------------------------------------
# lex(module)
#
# Build all of the regular expression rules from definitions in the supplied module
# -----------------------------------------------------------------------------
def lex(module=None,object=None,debug=0,optimize=0,lextab="lextab",reflags=0,nowarn=0,outputdir="", debuglog=None, errorlog=None):
    global lexer
    ldict = None
    stateinfo  = { 'INITIAL' : 'inclusive'}
    lexobj = Lexer()
    lexobj.lexoptimize = optimize
    global token,input

    if errorlog is None:
        errorlog = PlyLogger(sys.stderr)

    if debug:
        if debuglog is None:
            debuglog = PlyLogger(sys.stderr)

    # Get the module dictionary used for the lexer
    if object: module = object

    if module:
        _items = [(k,getattr(module,k)) for k in dir(module)]
        ldict = dict(_items)
    else:
        ldict = get_caller_module_dict(2)

    # Collect parser information from the dictionary
    linfo = LexerReflect(ldict,log=errorlog,reflags=reflags)
    linfo.get_all()
    if not optimize:
        if linfo.validate_all():
            raise SyntaxError("Can't build lexer")

    if optimize and lextab:
        try:
            lexobj.readtab(lextab,ldict)
            token = lexobj.token
            input = lexobj.input
            lexer = lexobj
            return lexobj

        except ImportError:
            pass

    # Dump some basic debugging information
    if debug:
        debuglog.info("lex: tokens   = %r", linfo.tokens)
        debuglog.info("lex: literals = %r", linfo.literals)
        debuglog.info("lex: states   = %r", linfo.stateinfo)

    # Build a dictionary of valid token names
    lexobj.lextokens = { }
    for n in linfo.tokens:
        lexobj.lextokens[n] = 1

    # Get literals specification
    if isinstance(linfo.literals,(list,tuple)):
        lexobj.lexliterals = type(linfo.literals[0])().join(linfo.literals)
    else:
        lexobj.lexliterals = linfo.literals

    # Get the stateinfo dictionary
    stateinfo = linfo.stateinfo

    regexs = { }
    # Build the master regular expressions
    for state in stateinfo:
        regex_list = []

        # Add rules defined by functions first
        for fname, f in linfo.funcsym[state]:
            line = func_code(f).co_firstlineno
            file = func_code(f).co_filename
            regex_list.append("(?P<%s>%s)" % (fname,f.__doc__))
            if debug:
                debuglog.info("lex: Adding rule %s -> '%s' (state '%s')",fname,f.__doc__, state)

        # Now add all of the simple rules
        for name,r in linfo.strsym[state]:
            regex_list.append("(?P<%s>%s)" % (name,r))
            if debug:
                debuglog.info("lex: Adding rule %s -> '%s' (state '%s')",name,r, state)

        regexs[state] = regex_list

    # Build the master regular expressions

    if debug:
        debuglog.info("lex: ==== MASTER REGEXS FOLLOW ====")

    for state in regexs:
        lexre, re_text, re_names = _form_master_re(regexs[state],reflags,ldict,linfo.toknames)
        lexobj.lexstatere[state] = lexre
        lexobj.lexstateretext[state] = re_text
        lexobj.lexstaterenames[state] = re_names
        if debug:
            for i in range(len(re_text)):
                debuglog.info("lex: state '%s' : regex[%d] = '%s'",state, i, re_text[i])

    # For inclusive states, we need to add the regular expressions from the INITIAL state
    for state,stype in stateinfo.items():
        if state != "INITIAL" and stype == 'inclusive':
             lexobj.lexstatere[state].extend(lexobj.lexstatere['INITIAL'])
             lexobj.lexstateretext[state].extend(lexobj.lexstateretext['INITIAL'])
             lexobj.lexstaterenames[state].extend(lexobj.lexstaterenames['INITIAL'])

    lexobj.lexstateinfo = stateinfo
    lexobj.lexre = lexobj.lexstatere["INITIAL"]
    lexobj.lexretext = lexobj.lexstateretext["INITIAL"]
    lexobj.lexreflags = reflags

    # Set up ignore variables
    lexobj.lexstateignore = linfo.ignore
    lexobj.lexignore = lexobj.lexstateignore.get("INITIAL","")

    # Set up error functions
    lexobj.lexstateerrorf = linfo.errorf
    lexobj.lexerrorf = linfo.errorf.get("INITIAL",None)
    if not lexobj.lexerrorf:
        errorlog.warning("No t_error rule is defined")

    # Check state information for ignore and error rules
    for s,stype in stateinfo.items():
        if stype == 'exclusive':
              if not s in linfo.errorf:
                   errorlog.warning("No error rule is defined for exclusive state '%s'", s)
              if not s in linfo.ignore and lexobj.lexignore:
                   errorlog.warning("No ignore rule is defined for exclusive state '%s'", s)
        elif stype == 'inclusive':
              if not s in linfo.errorf:
                   linfo.errorf[s] = linfo.errorf.get("INITIAL",None)
              if not s in linfo.ignore:
                   linfo.ignore[s] = linfo.ignore.get("INITIAL","")

    # Create global versions of the token() and input() functions
    token = lexobj.token
    input = lexobj.input
    lexer = lexobj

    # If in optimize mode, we write the lextab
    if lextab and optimize:
        lexobj.writetab(lextab,outputdir)

    return lexobj

# -----------------------------------------------------------------------------
# runmain()
#
# This runs the lexer as a main program
# -----------------------------------------------------------------------------

def runmain(lexer=None,data=None):
    if not data:
        try:
            filename = sys.argv[1]
            f = open(filename)
            data = f.read()
            f.close()
        except IndexError:
            sys.stdout.write("Reading from standard input (type EOF to end):\n")
            data = sys.stdin.read()

    if lexer:
        _input = lexer.input
    else:
        _input = input
    _input(data)
    if lexer:
        _token = lexer.token
    else:
        _token = token

    while 1:
        tok = _token()
        if not tok: break
        sys.stdout.write("(%s,%r,%d,%d)\n" % (tok.type, tok.value, tok.lineno,tok.lexpos))

# -----------------------------------------------------------------------------
# @TOKEN(regex)
#
# This decorator function can be used to set the regex expression on a function
# when its docstring might need to be set in an alternative way
# -----------------------------------------------------------------------------

def TOKEN(r):
    def set_doc(f):
        if hasattr(r,"__call__"):
            f.__doc__ = r.__doc__
        else:
            f.__doc__ = r
        return f
    return set_doc

# Alternative spelling of the TOKEN decorator
Token = TOKEN


########NEW FILE########
__FILENAME__ = yacc
# -----------------------------------------------------------------------------
# ply: yacc.py
#
# Copyright (C) 2001-2009,
# David M. Beazley (Dabeaz LLC)
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
# 
# * Redistributions of source code must retain the above copyright notice,
#   this list of conditions and the following disclaimer.  
# * Redistributions in binary form must reproduce the above copyright notice, 
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.  
# * Neither the name of the David Beazley or Dabeaz LLC may be used to
#   endorse or promote products derived from this software without
#  specific prior written permission. 
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
# -----------------------------------------------------------------------------
#
# This implements an LR parser that is constructed from grammar rules defined
# as Python functions. The grammer is specified by supplying the BNF inside
# Python documentation strings.  The inspiration for this technique was borrowed
# from John Aycock's Spark parsing system.  PLY might be viewed as cross between
# Spark and the GNU bison utility.
#
# The current implementation is only somewhat object-oriented. The
# LR parser itself is defined in terms of an object (which allows multiple
# parsers to co-exist).  However, most of the variables used during table
# construction are defined in terms of global variables.  Users shouldn't
# notice unless they are trying to define multiple parsers at the same
# time using threads (in which case they should have their head examined).
#
# This implementation supports both SLR and LALR(1) parsing.  LALR(1)
# support was originally implemented by Elias Ioup (ezioup@alumni.uchicago.edu),
# using the algorithm found in Aho, Sethi, and Ullman "Compilers: Principles,
# Techniques, and Tools" (The Dragon Book).  LALR(1) has since been replaced
# by the more efficient DeRemer and Pennello algorithm.
#
# :::::::: WARNING :::::::
#
# Construction of LR parsing tables is fairly complicated and expensive.
# To make this module run fast, a *LOT* of work has been put into
# optimization---often at the expensive of readability and what might
# consider to be good Python "coding style."   Modify the code at your
# own risk!
# ----------------------------------------------------------------------------

__version__    = "3.3"
__tabversion__ = "3.2"       # Table version

#-----------------------------------------------------------------------------
#                     === User configurable parameters ===
#
# Change these to modify the default behavior of yacc (if you wish)
#-----------------------------------------------------------------------------

yaccdebug   = 1                # Debugging mode.  If set, yacc generates a
                               # a 'parser.out' file in the current directory

debug_file  = 'parser.out'     # Default name of the debugging file
tab_module  = 'parsetab'       # Default name of the table module
default_lr  = 'LALR'           # Default LR table generation method

error_count = 3                # Number of symbols that must be shifted to leave recovery mode

yaccdevel   = 0                # Set to True if developing yacc.  This turns off optimized
                               # implementations of certain functions.

resultlimit = 40               # Size limit of results when running in debug mode.

pickle_protocol = 0            # Protocol to use when writing pickle files

import re, types, sys, os.path

# Compatibility function for python 2.6/3.0
if sys.version_info[0] < 3:
    def func_code(f):
        return f.func_code
else:
    def func_code(f):
        return f.__code__

# Compatibility
try:
    MAXINT = sys.maxint
except AttributeError:
    MAXINT = sys.maxsize

# Python 2.x/3.0 compatibility.
def load_ply_lex():
    if sys.version_info[0] < 3:
        import lex
    else:
        import ply.lex as lex
    return lex

# This object is a stand-in for a logging object created by the 
# logging module.   PLY will use this by default to create things
# such as the parser.out file.  If a user wants more detailed
# information, they can create their own logging object and pass
# it into PLY.

class PlyLogger(object):
    def __init__(self,f):
        self.f = f
    def debug(self,msg,*args,**kwargs):
        self.f.write((msg % args) + "\n")
    info     = debug

    def warning(self,msg,*args,**kwargs):
        self.f.write("WARNING: "+ (msg % args) + "\n")

    def error(self,msg,*args,**kwargs):
        self.f.write("ERROR: " + (msg % args) + "\n")

    critical = debug

# Null logger is used when no output is generated. Does nothing.
class NullLogger(object):
    def __getattribute__(self,name):
        return self
    def __call__(self,*args,**kwargs):
        return self
        
# Exception raised for yacc-related errors
class YaccError(Exception):   pass

# Format the result message that the parser produces when running in debug mode.
def format_result(r):
    repr_str = repr(r)
    if '\n' in repr_str: repr_str = repr(repr_str)
    if len(repr_str) > resultlimit:
        repr_str = repr_str[:resultlimit]+" ..."
    result = "<%s @ 0x%x> (%s)" % (type(r).__name__,id(r),repr_str)
    return result


# Format stack entries when the parser is running in debug mode
def format_stack_entry(r):
    repr_str = repr(r)
    if '\n' in repr_str: repr_str = repr(repr_str)
    if len(repr_str) < 16:
        return repr_str
    else:
        return "<%s @ 0x%x>" % (type(r).__name__,id(r))

#-----------------------------------------------------------------------------
#                        ===  LR Parsing Engine ===
#
# The following classes are used for the LR parser itself.  These are not
# used during table construction and are independent of the actual LR
# table generation algorithm
#-----------------------------------------------------------------------------

# This class is used to hold non-terminal grammar symbols during parsing.
# It normally has the following attributes set:
#        .type       = Grammar symbol type
#        .value      = Symbol value
#        .lineno     = Starting line number
#        .endlineno  = Ending line number (optional, set automatically)
#        .lexpos     = Starting lex position
#        .endlexpos  = Ending lex position (optional, set automatically)

class YaccSymbol:
    def __str__(self):    return self.type
    def __repr__(self):   return str(self)

# This class is a wrapper around the objects actually passed to each
# grammar rule.   Index lookup and assignment actually assign the
# .value attribute of the underlying YaccSymbol object.
# The lineno() method returns the line number of a given
# item (or 0 if not defined).   The linespan() method returns
# a tuple of (startline,endline) representing the range of lines
# for a symbol.  The lexspan() method returns a tuple (lexpos,endlexpos)
# representing the range of positional information for a symbol.

class YaccProduction:
    def __init__(self,s,stack=None):
        self.slice = s
        self.stack = stack
        self.lexer = None
        self.parser= None
    def __getitem__(self,n):
        if n >= 0: return self.slice[n].value
        else: return self.stack[n].value

    def __setitem__(self,n,v):
        self.slice[n].value = v

    def __getslice__(self,i,j):
        return [s.value for s in self.slice[i:j]]

    def __len__(self):
        return len(self.slice)

    def lineno(self,n):
        return getattr(self.slice[n],"lineno",0)

    def set_lineno(self,n,lineno):
        self.slice[n].lineno = lineno

    def linespan(self,n):
        startline = getattr(self.slice[n],"lineno",0)
        endline = getattr(self.slice[n],"endlineno",startline)
        return startline,endline

    def lexpos(self,n):
        return getattr(self.slice[n],"lexpos",0)

    def lexspan(self,n):
        startpos = getattr(self.slice[n],"lexpos",0)
        endpos = getattr(self.slice[n],"endlexpos",startpos)
        return startpos,endpos

    def error(self):
       raise SyntaxError


# -----------------------------------------------------------------------------
#                               == LRParser ==
#
# The LR Parsing engine.
# -----------------------------------------------------------------------------

class LRParser:
    def __init__(self,lrtab,errorf):
        self.productions = lrtab.lr_productions
        self.action      = lrtab.lr_action
        self.goto        = lrtab.lr_goto
        self.errorfunc   = errorf

    def errok(self):
        self.errorok     = 1

    def restart(self):
        del self.statestack[:]
        del self.symstack[:]
        sym = YaccSymbol()
        sym.type = '$end'
        self.symstack.append(sym)
        self.statestack.append(0)

    def parse(self,input=None,lexer=None,debug=0,tracking=0,tokenfunc=None):
        if debug or yaccdevel:
            if isinstance(debug,int):
                debug = PlyLogger(sys.stderr)
            return self.parsedebug(input,lexer,debug,tracking,tokenfunc)
        elif tracking:
            return self.parseopt(input,lexer,debug,tracking,tokenfunc)
        else:
            return self.parseopt_notrack(input,lexer,debug,tracking,tokenfunc)
        

    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    # parsedebug().
    #
    # This is the debugging enabled version of parse().  All changes made to the
    # parsing engine should be made here.   For the non-debugging version,
    # copy this code to a method parseopt() and delete all of the sections
    # enclosed in:
    #
    #      #--! DEBUG
    #      statements
    #      #--! DEBUG
    #
    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    def parsedebug(self,input=None,lexer=None,debug=None,tracking=0,tokenfunc=None):
        lookahead = None                 # Current lookahead symbol
        lookaheadstack = [ ]             # Stack of lookahead symbols
        actions = self.action            # Local reference to action table (to avoid lookup on self.)
        goto    = self.goto              # Local reference to goto table (to avoid lookup on self.)
        prod    = self.productions       # Local reference to production list (to avoid lookup on self.)
        pslice  = YaccProduction(None)   # Production object passed to grammar rules
        errorcount = 0                   # Used during error recovery 

        # --! DEBUG
        debug.info("PLY: PARSE DEBUG START")
        # --! DEBUG

        # If no lexer was given, we will try to use the lex module
        if not lexer:
            lex = load_ply_lex()
            lexer = lex.lexer

        # Set up the lexer and parser objects on pslice
        pslice.lexer = lexer
        pslice.parser = self

        # If input was supplied, pass to lexer
        if input is not None:
            lexer.input(input)

        if tokenfunc is None:
           # Tokenize function
           get_token = lexer.token
        else:
           get_token = tokenfunc

        # Set up the state and symbol stacks

        statestack = [ ]                # Stack of parsing states
        self.statestack = statestack
        symstack   = [ ]                # Stack of grammar symbols
        self.symstack = symstack

        pslice.stack = symstack         # Put in the production
        errtoken   = None               # Err token

        # The start state is assumed to be (0,$end)

        statestack.append(0)
        sym = YaccSymbol()
        sym.type = "$end"
        symstack.append(sym)
        state = 0
        while 1:
            # Get the next symbol on the input.  If a lookahead symbol
            # is already set, we just use that. Otherwise, we'll pull
            # the next token off of the lookaheadstack or from the lexer

            # --! DEBUG
            debug.debug('')
            debug.debug('State  : %s', state)
            # --! DEBUG

            if not lookahead:
                if not lookaheadstack:
                    lookahead = get_token()     # Get the next token
                else:
                    lookahead = lookaheadstack.pop()
                if not lookahead:
                    lookahead = YaccSymbol()
                    lookahead.type = "$end"

            # --! DEBUG
            debug.debug('Stack  : %s',
                        ("%s . %s" % (" ".join([xx.type for xx in symstack][1:]), str(lookahead))).lstrip())
            # --! DEBUG

            # Check the action table
            ltype = lookahead.type
            t = actions[state].get(ltype)

            if t is not None:
                if t > 0:
                    # shift a symbol on the stack
                    statestack.append(t)
                    state = t
                    
                    # --! DEBUG
                    debug.debug("Action : Shift and goto state %s", t)
                    # --! DEBUG

                    symstack.append(lookahead)
                    lookahead = None

                    # Decrease error count on successful shift
                    if errorcount: errorcount -=1
                    continue

                if t < 0:
                    # reduce a symbol on the stack, emit a production
                    p = prod[-t]
                    pname = p.name
                    plen  = p.len

                    # Get production function
                    sym = YaccSymbol()
                    sym.type = pname       # Production name
                    sym.value = None

                    # --! DEBUG
                    if plen:
                        debug.info("Action : Reduce rule [%s] with %s and goto state %d", p.str, "["+",".join([format_stack_entry(_v.value) for _v in symstack[-plen:]])+"]",-t)
                    else:
                        debug.info("Action : Reduce rule [%s] with %s and goto state %d", p.str, [],-t)
                        
                    # --! DEBUG

                    if plen:
                        targ = symstack[-plen-1:]
                        targ[0] = sym

                        # --! TRACKING
                        if tracking:
                           t1 = targ[1]
                           sym.lineno = t1.lineno
                           sym.lexpos = t1.lexpos
                           t1 = targ[-1]
                           sym.endlineno = getattr(t1,"endlineno",t1.lineno)
                           sym.endlexpos = getattr(t1,"endlexpos",t1.lexpos)

                        # --! TRACKING

                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                        # The code enclosed in this section is duplicated 
                        # below as a performance optimization.  Make sure
                        # changes get made in both locations.

                        pslice.slice = targ
                        
                        try:
                            # Call the grammar rule with our special slice object
                            del symstack[-plen:]
                            del statestack[-plen:]
                            p.callable(pslice)
                            # --! DEBUG
                            debug.info("Result : %s", format_result(pslice[0]))
                            # --! DEBUG
                            symstack.append(sym)
                            state = goto[statestack[-1]][pname]
                            statestack.append(state)
                        except SyntaxError:
                            # If an error was set. Enter error recovery state
                            lookaheadstack.append(lookahead)
                            symstack.pop()
                            statestack.pop()
                            state = statestack[-1]
                            sym.type = 'error'
                            lookahead = sym
                            errorcount = error_count
                            self.errorok = 0
                        continue
                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    
                    else:

                        # --! TRACKING
                        if tracking:
                           sym.lineno = lexer.lineno
                           sym.lexpos = lexer.lexpos
                        # --! TRACKING

                        targ = [ sym ]

                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                        # The code enclosed in this section is duplicated 
                        # above as a performance optimization.  Make sure
                        # changes get made in both locations.

                        pslice.slice = targ

                        try:
                            # Call the grammar rule with our special slice object
                            p.callable(pslice)
                            # --! DEBUG
                            debug.info("Result : %s", format_result(pslice[0]))
                            # --! DEBUG
                            symstack.append(sym)
                            state = goto[statestack[-1]][pname]
                            statestack.append(state)
                        except SyntaxError:
                            # If an error was set. Enter error recovery state
                            lookaheadstack.append(lookahead)
                            symstack.pop()
                            statestack.pop()
                            state = statestack[-1]
                            sym.type = 'error'
                            lookahead = sym
                            errorcount = error_count
                            self.errorok = 0
                        continue
                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                if t == 0:
                    n = symstack[-1]
                    result = getattr(n,"value",None)
                    # --! DEBUG
                    debug.info("Done   : Returning %s", format_result(result))
                    debug.info("PLY: PARSE DEBUG END")
                    # --! DEBUG
                    return result

            if t == None:

                # --! DEBUG
                debug.error('Error  : %s',
                            ("%s . %s" % (" ".join([xx.type for xx in symstack][1:]), str(lookahead))).lstrip())
                # --! DEBUG

                # We have some kind of parsing error here.  To handle
                # this, we are going to push the current token onto
                # the tokenstack and replace it with an 'error' token.
                # If there are any synchronization rules, they may
                # catch it.
                #
                # In addition to pushing the error token, we call call
                # the user defined p_error() function if this is the
                # first syntax error.  This function is only called if
                # errorcount == 0.
                if errorcount == 0 or self.errorok:
                    errorcount = error_count
                    self.errorok = 0
                    errtoken = lookahead
                    if errtoken.type == "$end":
                        errtoken = None               # End of file!
                    if self.errorfunc:
                        global errok,token,restart
                        errok = self.errok        # Set some special functions available in error recovery
                        token = get_token
                        restart = self.restart
                        if errtoken and not hasattr(errtoken,'lexer'):
                            errtoken.lexer = lexer
                        tok = self.errorfunc(errtoken)
                        del errok, token, restart   # Delete special functions

                        if self.errorok:
                            # User must have done some kind of panic
                            # mode recovery on their own.  The
                            # returned token is the next lookahead
                            lookahead = tok
                            errtoken = None
                            continue
                    else:
                        if errtoken:
                            if hasattr(errtoken,"lineno"): lineno = lookahead.lineno
                            else: lineno = 0
                            if lineno:
                                sys.stderr.write("yacc: Syntax error at line %d, token=%s\n" % (lineno, errtoken.type))
                            else:
                                sys.stderr.write("yacc: Syntax error, token=%s" % errtoken.type)
                        else:
                            sys.stderr.write("yacc: Parse error in input. EOF\n")
                            return

                else:
                    errorcount = error_count

                # case 1:  the statestack only has 1 entry on it.  If we're in this state, the
                # entire parse has been rolled back and we're completely hosed.   The token is
                # discarded and we just keep going.

                if len(statestack) <= 1 and lookahead.type != "$end":
                    lookahead = None
                    errtoken = None
                    state = 0
                    # Nuke the pushback stack
                    del lookaheadstack[:]
                    continue

                # case 2: the statestack has a couple of entries on it, but we're
                # at the end of the file. nuke the top entry and generate an error token

                # Start nuking entries on the stack
                if lookahead.type == "$end":
                    # Whoa. We're really hosed here. Bail out
                    return

                if lookahead.type != 'error':
                    sym = symstack[-1]
                    if sym.type == 'error':
                        # Hmmm. Error is on top of stack, we'll just nuke input
                        # symbol and continue
                        lookahead = None
                        continue
                    t = YaccSymbol()
                    t.type = 'error'
                    if hasattr(lookahead,"lineno"):
                        t.lineno = lookahead.lineno
                    t.value = lookahead
                    lookaheadstack.append(lookahead)
                    lookahead = t
                else:
                    symstack.pop()
                    statestack.pop()
                    state = statestack[-1]       # Potential bug fix

                continue

            # Call an error function here
            raise RuntimeError("yacc: internal parser error!!!\n")

    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    # parseopt().
    #
    # Optimized version of parse() method.  DO NOT EDIT THIS CODE DIRECTLY.
    # Edit the debug version above, then copy any modifications to the method
    # below while removing #--! DEBUG sections.
    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


    def parseopt(self,input=None,lexer=None,debug=0,tracking=0,tokenfunc=None):
        lookahead = None                 # Current lookahead symbol
        lookaheadstack = [ ]             # Stack of lookahead symbols
        actions = self.action            # Local reference to action table (to avoid lookup on self.)
        goto    = self.goto              # Local reference to goto table (to avoid lookup on self.)
        prod    = self.productions       # Local reference to production list (to avoid lookup on self.)
        pslice  = YaccProduction(None)   # Production object passed to grammar rules
        errorcount = 0                   # Used during error recovery 

        # If no lexer was given, we will try to use the lex module
        if not lexer:
            lex = load_ply_lex()
            lexer = lex.lexer
        
        # Set up the lexer and parser objects on pslice
        pslice.lexer = lexer
        pslice.parser = self

        # If input was supplied, pass to lexer
        if input is not None:
            lexer.input(input)

        if tokenfunc is None:
           # Tokenize function
           get_token = lexer.token
        else:
           get_token = tokenfunc

        # Set up the state and symbol stacks

        statestack = [ ]                # Stack of parsing states
        self.statestack = statestack
        symstack   = [ ]                # Stack of grammar symbols
        self.symstack = symstack

        pslice.stack = symstack         # Put in the production
        errtoken   = None               # Err token

        # The start state is assumed to be (0,$end)

        statestack.append(0)
        sym = YaccSymbol()
        sym.type = '$end'
        symstack.append(sym)
        state = 0
        while 1:
            # Get the next symbol on the input.  If a lookahead symbol
            # is already set, we just use that. Otherwise, we'll pull
            # the next token off of the lookaheadstack or from the lexer

            if not lookahead:
                if not lookaheadstack:
                    lookahead = get_token()     # Get the next token
                else:
                    lookahead = lookaheadstack.pop()
                if not lookahead:
                    lookahead = YaccSymbol()
                    lookahead.type = '$end'

            # Check the action table
            ltype = lookahead.type
            t = actions[state].get(ltype)

            if t is not None:
                if t > 0:
                    # shift a symbol on the stack
                    statestack.append(t)
                    state = t

                    symstack.append(lookahead)
                    lookahead = None

                    # Decrease error count on successful shift
                    if errorcount: errorcount -=1
                    continue

                if t < 0:
                    # reduce a symbol on the stack, emit a production
                    p = prod[-t]
                    pname = p.name
                    plen  = p.len

                    # Get production function
                    sym = YaccSymbol()
                    sym.type = pname       # Production name
                    sym.value = None

                    if plen:
                        targ = symstack[-plen-1:]
                        targ[0] = sym

                        # --! TRACKING
                        if tracking:
                           t1 = targ[1]
                           sym.lineno = t1.lineno
                           sym.lexpos = t1.lexpos
                           t1 = targ[-1]
                           sym.endlineno = getattr(t1,"endlineno",t1.lineno)
                           sym.endlexpos = getattr(t1,"endlexpos",t1.lexpos)

                        # --! TRACKING

                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                        # The code enclosed in this section is duplicated 
                        # below as a performance optimization.  Make sure
                        # changes get made in both locations.

                        pslice.slice = targ
                        
                        try:
                            # Call the grammar rule with our special slice object
                            del symstack[-plen:]
                            del statestack[-plen:]
                            p.callable(pslice)
                            symstack.append(sym)
                            state = goto[statestack[-1]][pname]
                            statestack.append(state)
                        except SyntaxError:
                            # If an error was set. Enter error recovery state
                            lookaheadstack.append(lookahead)
                            symstack.pop()
                            statestack.pop()
                            state = statestack[-1]
                            sym.type = 'error'
                            lookahead = sym
                            errorcount = error_count
                            self.errorok = 0
                        continue
                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    
                    else:

                        # --! TRACKING
                        if tracking:
                           sym.lineno = lexer.lineno
                           sym.lexpos = lexer.lexpos
                        # --! TRACKING

                        targ = [ sym ]

                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                        # The code enclosed in this section is duplicated 
                        # above as a performance optimization.  Make sure
                        # changes get made in both locations.

                        pslice.slice = targ

                        try:
                            # Call the grammar rule with our special slice object
                            p.callable(pslice)
                            symstack.append(sym)
                            state = goto[statestack[-1]][pname]
                            statestack.append(state)
                        except SyntaxError:
                            # If an error was set. Enter error recovery state
                            lookaheadstack.append(lookahead)
                            symstack.pop()
                            statestack.pop()
                            state = statestack[-1]
                            sym.type = 'error'
                            lookahead = sym
                            errorcount = error_count
                            self.errorok = 0
                        continue
                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                if t == 0:
                    n = symstack[-1]
                    return getattr(n,"value",None)

            if t == None:

                # We have some kind of parsing error here.  To handle
                # this, we are going to push the current token onto
                # the tokenstack and replace it with an 'error' token.
                # If there are any synchronization rules, they may
                # catch it.
                #
                # In addition to pushing the error token, we call call
                # the user defined p_error() function if this is the
                # first syntax error.  This function is only called if
                # errorcount == 0.
                if errorcount == 0 or self.errorok:
                    errorcount = error_count
                    self.errorok = 0
                    errtoken = lookahead
                    if errtoken.type == '$end':
                        errtoken = None               # End of file!
                    if self.errorfunc:
                        global errok,token,restart
                        errok = self.errok        # Set some special functions available in error recovery
                        token = get_token
                        restart = self.restart
                        if errtoken and not hasattr(errtoken,'lexer'):
                            errtoken.lexer = lexer
                        tok = self.errorfunc(errtoken)
                        del errok, token, restart   # Delete special functions

                        if self.errorok:
                            # User must have done some kind of panic
                            # mode recovery on their own.  The
                            # returned token is the next lookahead
                            lookahead = tok
                            errtoken = None
                            continue
                    else:
                        if errtoken:
                            if hasattr(errtoken,"lineno"): lineno = lookahead.lineno
                            else: lineno = 0
                            if lineno:
                                sys.stderr.write("yacc: Syntax error at line %d, token=%s\n" % (lineno, errtoken.type))
                            else:
                                sys.stderr.write("yacc: Syntax error, token=%s" % errtoken.type)
                        else:
                            sys.stderr.write("yacc: Parse error in input. EOF\n")
                            return

                else:
                    errorcount = error_count

                # case 1:  the statestack only has 1 entry on it.  If we're in this state, the
                # entire parse has been rolled back and we're completely hosed.   The token is
                # discarded and we just keep going.

                if len(statestack) <= 1 and lookahead.type != '$end':
                    lookahead = None
                    errtoken = None
                    state = 0
                    # Nuke the pushback stack
                    del lookaheadstack[:]
                    continue

                # case 2: the statestack has a couple of entries on it, but we're
                # at the end of the file. nuke the top entry and generate an error token

                # Start nuking entries on the stack
                if lookahead.type == '$end':
                    # Whoa. We're really hosed here. Bail out
                    return

                if lookahead.type != 'error':
                    sym = symstack[-1]
                    if sym.type == 'error':
                        # Hmmm. Error is on top of stack, we'll just nuke input
                        # symbol and continue
                        lookahead = None
                        continue
                    t = YaccSymbol()
                    t.type = 'error'
                    if hasattr(lookahead,"lineno"):
                        t.lineno = lookahead.lineno
                    t.value = lookahead
                    lookaheadstack.append(lookahead)
                    lookahead = t
                else:
                    symstack.pop()
                    statestack.pop()
                    state = statestack[-1]       # Potential bug fix

                continue

            # Call an error function here
            raise RuntimeError("yacc: internal parser error!!!\n")

    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    # parseopt_notrack().
    #
    # Optimized version of parseopt() with line number tracking removed. 
    # DO NOT EDIT THIS CODE DIRECTLY. Copy the optimized version and remove
    # code in the #--! TRACKING sections
    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    def parseopt_notrack(self,input=None,lexer=None,debug=0,tracking=0,tokenfunc=None):
        lookahead = None                 # Current lookahead symbol
        lookaheadstack = [ ]             # Stack of lookahead symbols
        actions = self.action            # Local reference to action table (to avoid lookup on self.)
        goto    = self.goto              # Local reference to goto table (to avoid lookup on self.)
        prod    = self.productions       # Local reference to production list (to avoid lookup on self.)
        pslice  = YaccProduction(None)   # Production object passed to grammar rules
        errorcount = 0                   # Used during error recovery 

        # If no lexer was given, we will try to use the lex module
        if not lexer:
            lex = load_ply_lex()
            lexer = lex.lexer
        
        # Set up the lexer and parser objects on pslice
        pslice.lexer = lexer
        pslice.parser = self

        # If input was supplied, pass to lexer
        if input is not None:
            lexer.input(input)

        if tokenfunc is None:
           # Tokenize function
           get_token = lexer.token
        else:
           get_token = tokenfunc

        # Set up the state and symbol stacks

        statestack = [ ]                # Stack of parsing states
        self.statestack = statestack
        symstack   = [ ]                # Stack of grammar symbols
        self.symstack = symstack

        pslice.stack = symstack         # Put in the production
        errtoken   = None               # Err token

        # The start state is assumed to be (0,$end)

        statestack.append(0)
        sym = YaccSymbol()
        sym.type = '$end'
        symstack.append(sym)
        state = 0
        while 1:
            # Get the next symbol on the input.  If a lookahead symbol
            # is already set, we just use that. Otherwise, we'll pull
            # the next token off of the lookaheadstack or from the lexer

            if not lookahead:
                if not lookaheadstack:
                    lookahead = get_token()     # Get the next token
                else:
                    lookahead = lookaheadstack.pop()
                if not lookahead:
                    lookahead = YaccSymbol()
                    lookahead.type = '$end'

            # Check the action table
            ltype = lookahead.type
            t = actions[state].get(ltype)

            if t is not None:
                if t > 0:
                    # shift a symbol on the stack
                    statestack.append(t)
                    state = t

                    symstack.append(lookahead)
                    lookahead = None

                    # Decrease error count on successful shift
                    if errorcount: errorcount -=1
                    continue

                if t < 0:
                    # reduce a symbol on the stack, emit a production
                    p = prod[-t]
                    pname = p.name
                    plen  = p.len

                    # Get production function
                    sym = YaccSymbol()
                    sym.type = pname       # Production name
                    sym.value = None

                    if plen:
                        targ = symstack[-plen-1:]
                        targ[0] = sym

                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                        # The code enclosed in this section is duplicated 
                        # below as a performance optimization.  Make sure
                        # changes get made in both locations.

                        pslice.slice = targ
                        
                        try:
                            # Call the grammar rule with our special slice object
                            del symstack[-plen:]
                            del statestack[-plen:]
                            p.callable(pslice)
                            symstack.append(sym)
                            state = goto[statestack[-1]][pname]
                            statestack.append(state)
                        except SyntaxError:
                            # If an error was set. Enter error recovery state
                            lookaheadstack.append(lookahead)
                            symstack.pop()
                            statestack.pop()
                            state = statestack[-1]
                            sym.type = 'error'
                            lookahead = sym
                            errorcount = error_count
                            self.errorok = 0
                        continue
                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    
                    else:

                        targ = [ sym ]

                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                        # The code enclosed in this section is duplicated 
                        # above as a performance optimization.  Make sure
                        # changes get made in both locations.

                        pslice.slice = targ

                        try:
                            # Call the grammar rule with our special slice object
                            p.callable(pslice)
                            symstack.append(sym)
                            state = goto[statestack[-1]][pname]
                            statestack.append(state)
                        except SyntaxError:
                            # If an error was set. Enter error recovery state
                            lookaheadstack.append(lookahead)
                            symstack.pop()
                            statestack.pop()
                            state = statestack[-1]
                            sym.type = 'error'
                            lookahead = sym
                            errorcount = error_count
                            self.errorok = 0
                        continue
                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                if t == 0:
                    n = symstack[-1]
                    return getattr(n,"value",None)

            if t == None:

                # We have some kind of parsing error here.  To handle
                # this, we are going to push the current token onto
                # the tokenstack and replace it with an 'error' token.
                # If there are any synchronization rules, they may
                # catch it.
                #
                # In addition to pushing the error token, we call call
                # the user defined p_error() function if this is the
                # first syntax error.  This function is only called if
                # errorcount == 0.
                if errorcount == 0 or self.errorok:
                    errorcount = error_count
                    self.errorok = 0
                    errtoken = lookahead
                    if errtoken.type == '$end':
                        errtoken = None               # End of file!
                    if self.errorfunc:
                        global errok,token,restart
                        errok = self.errok        # Set some special functions available in error recovery
                        token = get_token
                        restart = self.restart
                        if errtoken and not hasattr(errtoken,'lexer'):
                            errtoken.lexer = lexer
                        tok = self.errorfunc(errtoken)
                        del errok, token, restart   # Delete special functions

                        if self.errorok:
                            # User must have done some kind of panic
                            # mode recovery on their own.  The
                            # returned token is the next lookahead
                            lookahead = tok
                            errtoken = None
                            continue
                    else:
                        if errtoken:
                            if hasattr(errtoken,"lineno"): lineno = lookahead.lineno
                            else: lineno = 0
                            if lineno:
                                sys.stderr.write("yacc: Syntax error at line %d, token=%s\n" % (lineno, errtoken.type))
                            else:
                                sys.stderr.write("yacc: Syntax error, token=%s" % errtoken.type)
                        else:
                            sys.stderr.write("yacc: Parse error in input. EOF\n")
                            return

                else:
                    errorcount = error_count

                # case 1:  the statestack only has 1 entry on it.  If we're in this state, the
                # entire parse has been rolled back and we're completely hosed.   The token is
                # discarded and we just keep going.

                if len(statestack) <= 1 and lookahead.type != '$end':
                    lookahead = None
                    errtoken = None
                    state = 0
                    # Nuke the pushback stack
                    del lookaheadstack[:]
                    continue

                # case 2: the statestack has a couple of entries on it, but we're
                # at the end of the file. nuke the top entry and generate an error token

                # Start nuking entries on the stack
                if lookahead.type == '$end':
                    # Whoa. We're really hosed here. Bail out
                    return

                if lookahead.type != 'error':
                    sym = symstack[-1]
                    if sym.type == 'error':
                        # Hmmm. Error is on top of stack, we'll just nuke input
                        # symbol and continue
                        lookahead = None
                        continue
                    t = YaccSymbol()
                    t.type = 'error'
                    if hasattr(lookahead,"lineno"):
                        t.lineno = lookahead.lineno
                    t.value = lookahead
                    lookaheadstack.append(lookahead)
                    lookahead = t
                else:
                    symstack.pop()
                    statestack.pop()
                    state = statestack[-1]       # Potential bug fix

                continue

            # Call an error function here
            raise RuntimeError("yacc: internal parser error!!!\n")

# -----------------------------------------------------------------------------
#                          === Grammar Representation ===
#
# The following functions, classes, and variables are used to represent and
# manipulate the rules that make up a grammar. 
# -----------------------------------------------------------------------------

import re

# regex matching identifiers
_is_identifier = re.compile(r'^[a-zA-Z0-9_-]+$')

# -----------------------------------------------------------------------------
# class Production:
#
# This class stores the raw information about a single production or grammar rule.
# A grammar rule refers to a specification such as this:
#
#       expr : expr PLUS term 
#
# Here are the basic attributes defined on all productions
#
#       name     - Name of the production.  For example 'expr'
#       prod     - A list of symbols on the right side ['expr','PLUS','term']
#       prec     - Production precedence level
#       number   - Production number.
#       func     - Function that executes on reduce
#       file     - File where production function is defined
#       lineno   - Line number where production function is defined
#
# The following attributes are defined or optional.
#
#       len       - Length of the production (number of symbols on right hand side)
#       usyms     - Set of unique symbols found in the production
# -----------------------------------------------------------------------------

class Production(object):
    reduced = 0
    def __init__(self,number,name,prod,precedence=('right',0),func=None,file='',line=0):
        self.name     = name
        self.prod     = tuple(prod)
        self.number   = number
        self.func     = func
        self.callable = None
        self.file     = file
        self.line     = line
        self.prec     = precedence

        # Internal settings used during table construction
        
        self.len  = len(self.prod)   # Length of the production

        # Create a list of unique production symbols used in the production
        self.usyms = [ ]             
        for s in self.prod:
            if s not in self.usyms:
                self.usyms.append(s)

        # List of all LR items for the production
        self.lr_items = []
        self.lr_next = None

        # Create a string representation
        if self.prod:
            self.str = "%s -> %s" % (self.name," ".join(self.prod))
        else:
            self.str = "%s -> <empty>" % self.name

    def __str__(self):
        return self.str

    def __repr__(self):
        return "Production("+str(self)+")"

    def __len__(self):
        return len(self.prod)

    def __nonzero__(self):
        return 1

    def __getitem__(self,index):
        return self.prod[index]
            
    # Return the nth lr_item from the production (or None if at the end)
    def lr_item(self,n):
        if n > len(self.prod): return None
        p = LRItem(self,n)

        # Precompute the list of productions immediately following.  Hack. Remove later
        try:
            p.lr_after = Prodnames[p.prod[n+1]]
        except (IndexError,KeyError):
            p.lr_after = []
        try:
            p.lr_before = p.prod[n-1]
        except IndexError:
            p.lr_before = None

        return p
    
    # Bind the production function name to a callable
    def bind(self,pdict):
        if self.func:
            self.callable = pdict[self.func]

# This class serves as a minimal standin for Production objects when
# reading table data from files.   It only contains information
# actually used by the LR parsing engine, plus some additional
# debugging information.
class MiniProduction(object):
    def __init__(self,str,name,len,func,file,line):
        self.name     = name
        self.len      = len
        self.func     = func
        self.callable = None
        self.file     = file
        self.line     = line
        self.str      = str
    def __str__(self):
        return self.str
    def __repr__(self):
        return "MiniProduction(%s)" % self.str

    # Bind the production function name to a callable
    def bind(self,pdict):
        if self.func:
            self.callable = pdict[self.func]


# -----------------------------------------------------------------------------
# class LRItem
#
# This class represents a specific stage of parsing a production rule.  For
# example: 
#
#       expr : expr . PLUS term 
#
# In the above, the "." represents the current location of the parse.  Here
# basic attributes:
#
#       name       - Name of the production.  For example 'expr'
#       prod       - A list of symbols on the right side ['expr','.', 'PLUS','term']
#       number     - Production number.
#
#       lr_next      Next LR item. Example, if we are ' expr -> expr . PLUS term'
#                    then lr_next refers to 'expr -> expr PLUS . term'
#       lr_index   - LR item index (location of the ".") in the prod list.
#       lookaheads - LALR lookahead symbols for this item
#       len        - Length of the production (number of symbols on right hand side)
#       lr_after    - List of all productions that immediately follow
#       lr_before   - Grammar symbol immediately before
# -----------------------------------------------------------------------------

class LRItem(object):
    def __init__(self,p,n):
        self.name       = p.name
        self.prod       = list(p.prod)
        self.number     = p.number
        self.lr_index   = n
        self.lookaheads = { }
        self.prod.insert(n,".")
        self.prod       = tuple(self.prod)
        self.len        = len(self.prod)
        self.usyms      = p.usyms

    def __str__(self):
        if self.prod:
            s = "%s -> %s" % (self.name," ".join(self.prod))
        else:
            s = "%s -> <empty>" % self.name
        return s

    def __repr__(self):
        return "LRItem("+str(self)+")"

# -----------------------------------------------------------------------------
# rightmost_terminal()
#
# Return the rightmost terminal from a list of symbols.  Used in add_production()
# -----------------------------------------------------------------------------
def rightmost_terminal(symbols, terminals):
    i = len(symbols) - 1
    while i >= 0:
        if symbols[i] in terminals:
            return symbols[i]
        i -= 1
    return None

# -----------------------------------------------------------------------------
#                           === GRAMMAR CLASS ===
#
# The following class represents the contents of the specified grammar along
# with various computed properties such as first sets, follow sets, LR items, etc.
# This data is used for critical parts of the table generation process later.
# -----------------------------------------------------------------------------

class GrammarError(YaccError): pass

class Grammar(object):
    def __init__(self,terminals):
        self.Productions  = [None]  # A list of all of the productions.  The first
                                    # entry is always reserved for the purpose of
                                    # building an augmented grammar

        self.Prodnames    = { }     # A dictionary mapping the names of nonterminals to a list of all
                                    # productions of that nonterminal.

        self.Prodmap      = { }     # A dictionary that is only used to detect duplicate
                                    # productions.

        self.Terminals    = { }     # A dictionary mapping the names of terminal symbols to a
                                    # list of the rules where they are used.

        for term in terminals:
            self.Terminals[term] = []

        self.Terminals['error'] = []

        self.Nonterminals = { }     # A dictionary mapping names of nonterminals to a list
                                    # of rule numbers where they are used.

        self.First        = { }     # A dictionary of precomputed FIRST(x) symbols

        self.Follow       = { }     # A dictionary of precomputed FOLLOW(x) symbols

        self.Precedence   = { }     # Precedence rules for each terminal. Contains tuples of the
                                    # form ('right',level) or ('nonassoc', level) or ('left',level)

        self.UsedPrecedence = { }   # Precedence rules that were actually used by the grammer.
                                    # This is only used to provide error checking and to generate
                                    # a warning about unused precedence rules.

        self.Start = None           # Starting symbol for the grammar


    def __len__(self):
        return len(self.Productions)

    def __getitem__(self,index):
        return self.Productions[index]

    # -----------------------------------------------------------------------------
    # set_precedence()
    #
    # Sets the precedence for a given terminal. assoc is the associativity such as
    # 'left','right', or 'nonassoc'.  level is a numeric level.
    #
    # -----------------------------------------------------------------------------

    def set_precedence(self,term,assoc,level):
        assert self.Productions == [None],"Must call set_precedence() before add_production()"
        if term in self.Precedence:
            raise GrammarError("Precedence already specified for terminal '%s'" % term)
        if assoc not in ['left','right','nonassoc']:
            raise GrammarError("Associativity must be one of 'left','right', or 'nonassoc'")
        self.Precedence[term] = (assoc,level)
 
    # -----------------------------------------------------------------------------
    # add_production()
    #
    # Given an action function, this function assembles a production rule and
    # computes its precedence level.
    #
    # The production rule is supplied as a list of symbols.   For example,
    # a rule such as 'expr : expr PLUS term' has a production name of 'expr' and
    # symbols ['expr','PLUS','term'].
    #
    # Precedence is determined by the precedence of the right-most non-terminal
    # or the precedence of a terminal specified by %prec.
    #
    # A variety of error checks are performed to make sure production symbols
    # are valid and that %prec is used correctly.
    # -----------------------------------------------------------------------------

    def add_production(self,prodname,syms,func=None,file='',line=0):

        if prodname in self.Terminals:
            raise GrammarError("%s:%d: Illegal rule name '%s'. Already defined as a token" % (file,line,prodname))
        if prodname == 'error':
            raise GrammarError("%s:%d: Illegal rule name '%s'. error is a reserved word" % (file,line,prodname))
        if not _is_identifier.match(prodname):
            raise GrammarError("%s:%d: Illegal rule name '%s'" % (file,line,prodname))

        # Look for literal tokens 
        for n,s in enumerate(syms):
            if s[0] in "'\"":
                 try:
                     c = eval(s)
                     if (len(c) > 1):
                          raise GrammarError("%s:%d: Literal token %s in rule '%s' may only be a single character" % (file,line,s, prodname))
                     if not c in self.Terminals:
                          self.Terminals[c] = []
                     syms[n] = c
                     continue
                 except SyntaxError:
                     pass
            if not _is_identifier.match(s) and s != '%prec':
                raise GrammarError("%s:%d: Illegal name '%s' in rule '%s'" % (file,line,s, prodname))
        
        # Determine the precedence level
        if '%prec' in syms:
            if syms[-1] == '%prec':
                raise GrammarError("%s:%d: Syntax error. Nothing follows %%prec" % (file,line))
            if syms[-2] != '%prec':
                raise GrammarError("%s:%d: Syntax error. %%prec can only appear at the end of a grammar rule" % (file,line))
            precname = syms[-1]
            prodprec = self.Precedence.get(precname,None)
            if not prodprec:
                raise GrammarError("%s:%d: Nothing known about the precedence of '%s'" % (file,line,precname))
            else:
                self.UsedPrecedence[precname] = 1
            del syms[-2:]     # Drop %prec from the rule
        else:
            # If no %prec, precedence is determined by the rightmost terminal symbol
            precname = rightmost_terminal(syms,self.Terminals)
            prodprec = self.Precedence.get(precname,('right',0)) 
            
        # See if the rule is already in the rulemap
        map = "%s -> %s" % (prodname,syms)
        if map in self.Prodmap:
            m = self.Prodmap[map]
            raise GrammarError("%s:%d: Duplicate rule %s. " % (file,line, m) +
                               "Previous definition at %s:%d" % (m.file, m.line))

        # From this point on, everything is valid.  Create a new Production instance
        pnumber  = len(self.Productions)
        if not prodname in self.Nonterminals:
            self.Nonterminals[prodname] = [ ]

        # Add the production number to Terminals and Nonterminals
        for t in syms:
            if t in self.Terminals:
                self.Terminals[t].append(pnumber)
            else:
                if not t in self.Nonterminals:
                    self.Nonterminals[t] = [ ]
                self.Nonterminals[t].append(pnumber)

        # Create a production and add it to the list of productions
        p = Production(pnumber,prodname,syms,prodprec,func,file,line)
        self.Productions.append(p)
        self.Prodmap[map] = p

        # Add to the global productions list
        try:
            self.Prodnames[prodname].append(p)
        except KeyError:
            self.Prodnames[prodname] = [ p ]
        return 0

    # -----------------------------------------------------------------------------
    # set_start()
    #
    # Sets the starting symbol and creates the augmented grammar.  Production 
    # rule 0 is S' -> start where start is the start symbol.
    # -----------------------------------------------------------------------------

    def set_start(self,start=None):
        if not start:
            start = self.Productions[1].name
        if start not in self.Nonterminals:
            raise GrammarError("start symbol %s undefined" % start)
        self.Productions[0] = Production(0,"S'",[start])
        self.Nonterminals[start].append(0)
        self.Start = start

    # -----------------------------------------------------------------------------
    # find_unreachable()
    #
    # Find all of the nonterminal symbols that can't be reached from the starting
    # symbol.  Returns a list of nonterminals that can't be reached.
    # -----------------------------------------------------------------------------

    def find_unreachable(self):
        
        # Mark all symbols that are reachable from a symbol s
        def mark_reachable_from(s):
            if reachable[s]:
                # We've already reached symbol s.
                return
            reachable[s] = 1
            for p in self.Prodnames.get(s,[]):
                for r in p.prod:
                    mark_reachable_from(r)

        reachable   = { }
        for s in list(self.Terminals) + list(self.Nonterminals):
            reachable[s] = 0

        mark_reachable_from( self.Productions[0].prod[0] )

        return [s for s in list(self.Nonterminals)
                        if not reachable[s]]
    
    # -----------------------------------------------------------------------------
    # infinite_cycles()
    #
    # This function looks at the various parsing rules and tries to detect
    # infinite recursion cycles (grammar rules where there is no possible way
    # to derive a string of only terminals).
    # -----------------------------------------------------------------------------

    def infinite_cycles(self):
        terminates = {}

        # Terminals:
        for t in self.Terminals:
            terminates[t] = 1

        terminates['$end'] = 1

        # Nonterminals:

        # Initialize to false:
        for n in self.Nonterminals:
            terminates[n] = 0

        # Then propagate termination until no change:
        while 1:
            some_change = 0
            for (n,pl) in self.Prodnames.items():
                # Nonterminal n terminates iff any of its productions terminates.
                for p in pl:
                    # Production p terminates iff all of its rhs symbols terminate.
                    for s in p.prod:
                        if not terminates[s]:
                            # The symbol s does not terminate,
                            # so production p does not terminate.
                            p_terminates = 0
                            break
                    else:
                        # didn't break from the loop,
                        # so every symbol s terminates
                        # so production p terminates.
                        p_terminates = 1

                    if p_terminates:
                        # symbol n terminates!
                        if not terminates[n]:
                            terminates[n] = 1
                            some_change = 1
                        # Don't need to consider any more productions for this n.
                        break

            if not some_change:
                break

        infinite = []
        for (s,term) in terminates.items():
            if not term:
                if not s in self.Prodnames and not s in self.Terminals and s != 'error':
                    # s is used-but-not-defined, and we've already warned of that,
                    # so it would be overkill to say that it's also non-terminating.
                    pass
                else:
                    infinite.append(s)

        return infinite


    # -----------------------------------------------------------------------------
    # undefined_symbols()
    #
    # Find all symbols that were used the grammar, but not defined as tokens or
    # grammar rules.  Returns a list of tuples (sym, prod) where sym in the symbol
    # and prod is the production where the symbol was used. 
    # -----------------------------------------------------------------------------
    def undefined_symbols(self):
        result = []
        for p in self.Productions:
            if not p: continue

            for s in p.prod:
                if not s in self.Prodnames and not s in self.Terminals and s != 'error':
                    result.append((s,p))
        return result

    # -----------------------------------------------------------------------------
    # unused_terminals()
    #
    # Find all terminals that were defined, but not used by the grammar.  Returns
    # a list of all symbols.
    # -----------------------------------------------------------------------------
    def unused_terminals(self):
        unused_tok = []
        for s,v in self.Terminals.items():
            if s != 'error' and not v:
                unused_tok.append(s)

        return unused_tok

    # ------------------------------------------------------------------------------
    # unused_rules()
    #
    # Find all grammar rules that were defined,  but not used (maybe not reachable)
    # Returns a list of productions.
    # ------------------------------------------------------------------------------

    def unused_rules(self):
        unused_prod = []
        for s,v in self.Nonterminals.items():
            if not v:
                p = self.Prodnames[s][0]
                unused_prod.append(p)
        return unused_prod

    # -----------------------------------------------------------------------------
    # unused_precedence()
    #
    # Returns a list of tuples (term,precedence) corresponding to precedence
    # rules that were never used by the grammar.  term is the name of the terminal
    # on which precedence was applied and precedence is a string such as 'left' or
    # 'right' corresponding to the type of precedence. 
    # -----------------------------------------------------------------------------

    def unused_precedence(self):
        unused = []
        for termname in self.Precedence:
            if not (termname in self.Terminals or termname in self.UsedPrecedence):
                unused.append((termname,self.Precedence[termname][0]))
                
        return unused

    # -------------------------------------------------------------------------
    # _first()
    #
    # Compute the value of FIRST1(beta) where beta is a tuple of symbols.
    #
    # During execution of compute_first1, the result may be incomplete.
    # Afterward (e.g., when called from compute_follow()), it will be complete.
    # -------------------------------------------------------------------------
    def _first(self,beta):

        # We are computing First(x1,x2,x3,...,xn)
        result = [ ]
        for x in beta:
            x_produces_empty = 0

            # Add all the non-<empty> symbols of First[x] to the result.
            for f in self.First[x]:
                if f == '<empty>':
                    x_produces_empty = 1
                else:
                    if f not in result: result.append(f)

            if x_produces_empty:
                # We have to consider the next x in beta,
                # i.e. stay in the loop.
                pass
            else:
                # We don't have to consider any further symbols in beta.
                break
        else:
            # There was no 'break' from the loop,
            # so x_produces_empty was true for all x in beta,
            # so beta produces empty as well.
            result.append('<empty>')

        return result

    # -------------------------------------------------------------------------
    # compute_first()
    #
    # Compute the value of FIRST1(X) for all symbols
    # -------------------------------------------------------------------------
    def compute_first(self):
        if self.First:
            return self.First

        # Terminals:
        for t in self.Terminals:
            self.First[t] = [t]

        self.First['$end'] = ['$end']

        # Nonterminals:

        # Initialize to the empty set:
        for n in self.Nonterminals:
            self.First[n] = []

        # Then propagate symbols until no change:
        while 1:
            some_change = 0
            for n in self.Nonterminals:
                for p in self.Prodnames[n]:
                    for f in self._first(p.prod):
                        if f not in self.First[n]:
                            self.First[n].append( f )
                            some_change = 1
            if not some_change:
                break
        
        return self.First

    # ---------------------------------------------------------------------
    # compute_follow()
    #
    # Computes all of the follow sets for every non-terminal symbol.  The
    # follow set is the set of all symbols that might follow a given
    # non-terminal.  See the Dragon book, 2nd Ed. p. 189.
    # ---------------------------------------------------------------------
    def compute_follow(self,start=None):
        # If already computed, return the result
        if self.Follow:
            return self.Follow

        # If first sets not computed yet, do that first.
        if not self.First:
            self.compute_first()

        # Add '$end' to the follow list of the start symbol
        for k in self.Nonterminals:
            self.Follow[k] = [ ]

        if not start:
            start = self.Productions[1].name

        self.Follow[start] = [ '$end' ]

        while 1:
            didadd = 0
            for p in self.Productions[1:]:
                # Here is the production set
                for i in range(len(p.prod)):
                    B = p.prod[i]
                    if B in self.Nonterminals:
                        # Okay. We got a non-terminal in a production
                        fst = self._first(p.prod[i+1:])
                        hasempty = 0
                        for f in fst:
                            if f != '<empty>' and f not in self.Follow[B]:
                                self.Follow[B].append(f)
                                didadd = 1
                            if f == '<empty>':
                                hasempty = 1
                        if hasempty or i == (len(p.prod)-1):
                            # Add elements of follow(a) to follow(b)
                            for f in self.Follow[p.name]:
                                if f not in self.Follow[B]:
                                    self.Follow[B].append(f)
                                    didadd = 1
            if not didadd: break
        return self.Follow


    # -----------------------------------------------------------------------------
    # build_lritems()
    #
    # This function walks the list of productions and builds a complete set of the
    # LR items.  The LR items are stored in two ways:  First, they are uniquely
    # numbered and placed in the list _lritems.  Second, a linked list of LR items
    # is built for each production.  For example:
    #
    #   E -> E PLUS E
    #
    # Creates the list
    #
    #  [E -> . E PLUS E, E -> E . PLUS E, E -> E PLUS . E, E -> E PLUS E . ]
    # -----------------------------------------------------------------------------

    def build_lritems(self):
        for p in self.Productions:
            lastlri = p
            i = 0
            lr_items = []
            while 1:
                if i > len(p):
                    lri = None
                else:
                    lri = LRItem(p,i)
                    # Precompute the list of productions immediately following
                    try:
                        lri.lr_after = self.Prodnames[lri.prod[i+1]]
                    except (IndexError,KeyError):
                        lri.lr_after = []
                    try:
                        lri.lr_before = lri.prod[i-1]
                    except IndexError:
                        lri.lr_before = None

                lastlri.lr_next = lri
                if not lri: break
                lr_items.append(lri)
                lastlri = lri
                i += 1
            p.lr_items = lr_items

# -----------------------------------------------------------------------------
#                            == Class LRTable ==
#
# This basic class represents a basic table of LR parsing information.  
# Methods for generating the tables are not defined here.  They are defined
# in the derived class LRGeneratedTable.
# -----------------------------------------------------------------------------

class VersionError(YaccError): pass

class LRTable(object):
    def __init__(self):
        self.lr_action = None
        self.lr_goto = None
        self.lr_productions = None
        self.lr_method = None

    def read_table(self,module):
        if isinstance(module,types.ModuleType):
            parsetab = module
        else:
            if sys.version_info[0] < 3:
                exec("import %s as parsetab" % module)
            else:
                env = { }
                exec("import %s as parsetab" % module, env, env)
                parsetab = env['parsetab']

        if parsetab._tabversion != __tabversion__:
            raise VersionError("yacc table file version is out of date")

        self.lr_action = parsetab._lr_action
        self.lr_goto = parsetab._lr_goto

        self.lr_productions = []
        for p in parsetab._lr_productions:
            self.lr_productions.append(MiniProduction(*p))

        self.lr_method = parsetab._lr_method
        return parsetab._lr_signature

    def read_pickle(self,filename):
        try:
            import cPickle as pickle
        except ImportError:
            import pickle

        in_f = open(filename,"rb")

        tabversion = pickle.load(in_f)
        if tabversion != __tabversion__:
            raise VersionError("yacc table file version is out of date")
        self.lr_method = pickle.load(in_f)
        signature      = pickle.load(in_f)
        self.lr_action = pickle.load(in_f)
        self.lr_goto   = pickle.load(in_f)
        productions    = pickle.load(in_f)

        self.lr_productions = []
        for p in productions:
            self.lr_productions.append(MiniProduction(*p))

        in_f.close()
        return signature

    # Bind all production function names to callable objects in pdict
    def bind_callables(self,pdict):
        for p in self.lr_productions:
            p.bind(pdict)
    
# -----------------------------------------------------------------------------
#                           === LR Generator ===
#
# The following classes and functions are used to generate LR parsing tables on 
# a grammar.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# digraph()
# traverse()
#
# The following two functions are used to compute set valued functions
# of the form:
#
#     F(x) = F'(x) U U{F(y) | x R y}
#
# This is used to compute the values of Read() sets as well as FOLLOW sets
# in LALR(1) generation.
#
# Inputs:  X    - An input set
#          R    - A relation
#          FP   - Set-valued function
# ------------------------------------------------------------------------------

def digraph(X,R,FP):
    N = { }
    for x in X:
       N[x] = 0
    stack = []
    F = { }
    for x in X:
        if N[x] == 0: traverse(x,N,stack,F,X,R,FP)
    return F

def traverse(x,N,stack,F,X,R,FP):
    stack.append(x)
    d = len(stack)
    N[x] = d
    F[x] = FP(x)             # F(X) <- F'(x)

    rel = R(x)               # Get y's related to x
    for y in rel:
        if N[y] == 0:
             traverse(y,N,stack,F,X,R,FP)
        N[x] = min(N[x],N[y])
        for a in F.get(y,[]):
            if a not in F[x]: F[x].append(a)
    if N[x] == d:
       N[stack[-1]] = MAXINT
       F[stack[-1]] = F[x]
       element = stack.pop()
       while element != x:
           N[stack[-1]] = MAXINT
           F[stack[-1]] = F[x]
           element = stack.pop()

class LALRError(YaccError): pass

# -----------------------------------------------------------------------------
#                             == LRGeneratedTable ==
#
# This class implements the LR table generation algorithm.  There are no
# public methods except for write()
# -----------------------------------------------------------------------------

class LRGeneratedTable(LRTable):
    def __init__(self,grammar,method='LALR',log=None):
        if method not in ['SLR','LALR']:
            raise LALRError("Unsupported method %s" % method)

        self.grammar = grammar
        self.lr_method = method

        # Set up the logger
        if not log:
            log = NullLogger()
        self.log = log

        # Internal attributes
        self.lr_action     = {}        # Action table
        self.lr_goto       = {}        # Goto table
        self.lr_productions  = grammar.Productions    # Copy of grammar Production array
        self.lr_goto_cache = {}        # Cache of computed gotos
        self.lr0_cidhash   = {}        # Cache of closures

        self._add_count    = 0         # Internal counter used to detect cycles

        # Diagonistic information filled in by the table generator
        self.sr_conflict   = 0
        self.rr_conflict   = 0
        self.conflicts     = []        # List of conflicts

        self.sr_conflicts  = []
        self.rr_conflicts  = []

        # Build the tables
        self.grammar.build_lritems()
        self.grammar.compute_first()
        self.grammar.compute_follow()
        self.lr_parse_table()

    # Compute the LR(0) closure operation on I, where I is a set of LR(0) items.

    def lr0_closure(self,I):
        self._add_count += 1

        # Add everything in I to J
        J = I[:]
        didadd = 1
        while didadd:
            didadd = 0
            for j in J:
                for x in j.lr_after:
                    if getattr(x,"lr0_added",0) == self._add_count: continue
                    # Add B --> .G to J
                    J.append(x.lr_next)
                    x.lr0_added = self._add_count
                    didadd = 1

        return J

    # Compute the LR(0) goto function goto(I,X) where I is a set
    # of LR(0) items and X is a grammar symbol.   This function is written
    # in a way that guarantees uniqueness of the generated goto sets
    # (i.e. the same goto set will never be returned as two different Python
    # objects).  With uniqueness, we can later do fast set comparisons using
    # id(obj) instead of element-wise comparison.

    def lr0_goto(self,I,x):
        # First we look for a previously cached entry
        g = self.lr_goto_cache.get((id(I),x),None)
        if g: return g

        # Now we generate the goto set in a way that guarantees uniqueness
        # of the result

        s = self.lr_goto_cache.get(x,None)
        if not s:
            s = { }
            self.lr_goto_cache[x] = s

        gs = [ ]
        for p in I:
            n = p.lr_next
            if n and n.lr_before == x:
                s1 = s.get(id(n),None)
                if not s1:
                    s1 = { }
                    s[id(n)] = s1
                gs.append(n)
                s = s1
        g = s.get('$end',None)
        if not g:
            if gs:
                g = self.lr0_closure(gs)
                s['$end'] = g
            else:
                s['$end'] = gs
        self.lr_goto_cache[(id(I),x)] = g
        return g

    # Compute the LR(0) sets of item function
    def lr0_items(self):

        C = [ self.lr0_closure([self.grammar.Productions[0].lr_next]) ]
        i = 0
        for I in C:
            self.lr0_cidhash[id(I)] = i
            i += 1

        # Loop over the items in C and each grammar symbols
        i = 0
        while i < len(C):
            I = C[i]
            i += 1

            # Collect all of the symbols that could possibly be in the goto(I,X) sets
            asyms = { }
            for ii in I:
                for s in ii.usyms:
                    asyms[s] = None

            for x in asyms:
                g = self.lr0_goto(I,x)
                if not g:  continue
                if id(g) in self.lr0_cidhash: continue
                self.lr0_cidhash[id(g)] = len(C)
                C.append(g)

        return C

    # -----------------------------------------------------------------------------
    #                       ==== LALR(1) Parsing ====
    #
    # LALR(1) parsing is almost exactly the same as SLR except that instead of
    # relying upon Follow() sets when performing reductions, a more selective
    # lookahead set that incorporates the state of the LR(0) machine is utilized.
    # Thus, we mainly just have to focus on calculating the lookahead sets.
    #
    # The method used here is due to DeRemer and Pennelo (1982).
    #
    # DeRemer, F. L., and T. J. Pennelo: "Efficient Computation of LALR(1)
    #     Lookahead Sets", ACM Transactions on Programming Languages and Systems,
    #     Vol. 4, No. 4, Oct. 1982, pp. 615-649
    #
    # Further details can also be found in:
    #
    #  J. Tremblay and P. Sorenson, "The Theory and Practice of Compiler Writing",
    #      McGraw-Hill Book Company, (1985).
    #
    # -----------------------------------------------------------------------------

    # -----------------------------------------------------------------------------
    # compute_nullable_nonterminals()
    #
    # Creates a dictionary containing all of the non-terminals that might produce
    # an empty production.
    # -----------------------------------------------------------------------------

    def compute_nullable_nonterminals(self):
        nullable = {}
        num_nullable = 0
        while 1:
           for p in self.grammar.Productions[1:]:
               if p.len == 0:
                    nullable[p.name] = 1
                    continue
               for t in p.prod:
                    if not t in nullable: break
               else:
                    nullable[p.name] = 1
           if len(nullable) == num_nullable: break
           num_nullable = len(nullable)
        return nullable

    # -----------------------------------------------------------------------------
    # find_nonterminal_trans(C)
    #
    # Given a set of LR(0) items, this functions finds all of the non-terminal
    # transitions.    These are transitions in which a dot appears immediately before
    # a non-terminal.   Returns a list of tuples of the form (state,N) where state
    # is the state number and N is the nonterminal symbol.
    #
    # The input C is the set of LR(0) items.
    # -----------------------------------------------------------------------------

    def find_nonterminal_transitions(self,C):
         trans = []
         for state in range(len(C)):
             for p in C[state]:
                 if p.lr_index < p.len - 1:
                      t = (state,p.prod[p.lr_index+1])
                      if t[1] in self.grammar.Nonterminals:
                            if t not in trans: trans.append(t)
             state = state + 1
         return trans

    # -----------------------------------------------------------------------------
    # dr_relation()
    #
    # Computes the DR(p,A) relationships for non-terminal transitions.  The input
    # is a tuple (state,N) where state is a number and N is a nonterminal symbol.
    #
    # Returns a list of terminals.
    # -----------------------------------------------------------------------------

    def dr_relation(self,C,trans,nullable):
        dr_set = { }
        state,N = trans
        terms = []

        g = self.lr0_goto(C[state],N)
        for p in g:
           if p.lr_index < p.len - 1:
               a = p.prod[p.lr_index+1]
               if a in self.grammar.Terminals:
                   if a not in terms: terms.append(a)

        # This extra bit is to handle the start state
        if state == 0 and N == self.grammar.Productions[0].prod[0]:
           terms.append('$end')

        return terms

    # -----------------------------------------------------------------------------
    # reads_relation()
    #
    # Computes the READS() relation (p,A) READS (t,C).
    # -----------------------------------------------------------------------------

    def reads_relation(self,C, trans, empty):
        # Look for empty transitions
        rel = []
        state, N = trans

        g = self.lr0_goto(C[state],N)
        j = self.lr0_cidhash.get(id(g),-1)
        for p in g:
            if p.lr_index < p.len - 1:
                 a = p.prod[p.lr_index + 1]
                 if a in empty:
                      rel.append((j,a))

        return rel

    # -----------------------------------------------------------------------------
    # compute_lookback_includes()
    #
    # Determines the lookback and includes relations
    #
    # LOOKBACK:
    #
    # This relation is determined by running the LR(0) state machine forward.
    # For example, starting with a production "N : . A B C", we run it forward
    # to obtain "N : A B C ."   We then build a relationship between this final
    # state and the starting state.   These relationships are stored in a dictionary
    # lookdict.
    #
    # INCLUDES:
    #
    # Computes the INCLUDE() relation (p,A) INCLUDES (p',B).
    #
    # This relation is used to determine non-terminal transitions that occur
    # inside of other non-terminal transition states.   (p,A) INCLUDES (p', B)
    # if the following holds:
    #
    #       B -> LAT, where T -> epsilon and p' -L-> p
    #
    # L is essentially a prefix (which may be empty), T is a suffix that must be
    # able to derive an empty string.  State p' must lead to state p with the string L.
    #
    # -----------------------------------------------------------------------------

    def compute_lookback_includes(self,C,trans,nullable):

        lookdict = {}          # Dictionary of lookback relations
        includedict = {}       # Dictionary of include relations

        # Make a dictionary of non-terminal transitions
        dtrans = {}
        for t in trans:
            dtrans[t] = 1

        # Loop over all transitions and compute lookbacks and includes
        for state,N in trans:
            lookb = []
            includes = []
            for p in C[state]:
                if p.name != N: continue

                # Okay, we have a name match.  We now follow the production all the way
                # through the state machine until we get the . on the right hand side

                lr_index = p.lr_index
                j = state
                while lr_index < p.len - 1:
                     lr_index = lr_index + 1
                     t = p.prod[lr_index]

                     # Check to see if this symbol and state are a non-terminal transition
                     if (j,t) in dtrans:
                           # Yes.  Okay, there is some chance that this is an includes relation
                           # the only way to know for certain is whether the rest of the
                           # production derives empty

                           li = lr_index + 1
                           while li < p.len:
                                if p.prod[li] in self.grammar.Terminals: break      # No forget it
                                if not p.prod[li] in nullable: break
                                li = li + 1
                           else:
                                # Appears to be a relation between (j,t) and (state,N)
                                includes.append((j,t))

                     g = self.lr0_goto(C[j],t)               # Go to next set
                     j = self.lr0_cidhash.get(id(g),-1)     # Go to next state

                # When we get here, j is the final state, now we have to locate the production
                for r in C[j]:
                     if r.name != p.name: continue
                     if r.len != p.len:   continue
                     i = 0
                     # This look is comparing a production ". A B C" with "A B C ."
                     while i < r.lr_index:
                          if r.prod[i] != p.prod[i+1]: break
                          i = i + 1
                     else:
                          lookb.append((j,r))
            for i in includes:
                 if not i in includedict: includedict[i] = []
                 includedict[i].append((state,N))
            lookdict[(state,N)] = lookb

        return lookdict,includedict

    # -----------------------------------------------------------------------------
    # compute_read_sets()
    #
    # Given a set of LR(0) items, this function computes the read sets.
    #
    # Inputs:  C        =  Set of LR(0) items
    #          ntrans   = Set of nonterminal transitions
    #          nullable = Set of empty transitions
    #
    # Returns a set containing the read sets
    # -----------------------------------------------------------------------------

    def compute_read_sets(self,C, ntrans, nullable):
        FP = lambda x: self.dr_relation(C,x,nullable)
        R =  lambda x: self.reads_relation(C,x,nullable)
        F = digraph(ntrans,R,FP)
        return F

    # -----------------------------------------------------------------------------
    # compute_follow_sets()
    #
    # Given a set of LR(0) items, a set of non-terminal transitions, a readset,
    # and an include set, this function computes the follow sets
    #
    # Follow(p,A) = Read(p,A) U U {Follow(p',B) | (p,A) INCLUDES (p',B)}
    #
    # Inputs:
    #            ntrans     = Set of nonterminal transitions
    #            readsets   = Readset (previously computed)
    #            inclsets   = Include sets (previously computed)
    #
    # Returns a set containing the follow sets
    # -----------------------------------------------------------------------------

    def compute_follow_sets(self,ntrans,readsets,inclsets):
         FP = lambda x: readsets[x]
         R  = lambda x: inclsets.get(x,[])
         F = digraph(ntrans,R,FP)
         return F

    # -----------------------------------------------------------------------------
    # add_lookaheads()
    #
    # Attaches the lookahead symbols to grammar rules.
    #
    # Inputs:    lookbacks         -  Set of lookback relations
    #            followset         -  Computed follow set
    #
    # This function directly attaches the lookaheads to productions contained
    # in the lookbacks set
    # -----------------------------------------------------------------------------

    def add_lookaheads(self,lookbacks,followset):
        for trans,lb in lookbacks.items():
            # Loop over productions in lookback
            for state,p in lb:
                 if not state in p.lookaheads:
                      p.lookaheads[state] = []
                 f = followset.get(trans,[])
                 for a in f:
                      if a not in p.lookaheads[state]: p.lookaheads[state].append(a)

    # -----------------------------------------------------------------------------
    # add_lalr_lookaheads()
    #
    # This function does all of the work of adding lookahead information for use
    # with LALR parsing
    # -----------------------------------------------------------------------------

    def add_lalr_lookaheads(self,C):
        # Determine all of the nullable nonterminals
        nullable = self.compute_nullable_nonterminals()

        # Find all non-terminal transitions
        trans = self.find_nonterminal_transitions(C)

        # Compute read sets
        readsets = self.compute_read_sets(C,trans,nullable)

        # Compute lookback/includes relations
        lookd, included = self.compute_lookback_includes(C,trans,nullable)

        # Compute LALR FOLLOW sets
        followsets = self.compute_follow_sets(trans,readsets,included)

        # Add all of the lookaheads
        self.add_lookaheads(lookd,followsets)

    # -----------------------------------------------------------------------------
    # lr_parse_table()
    #
    # This function constructs the parse tables for SLR or LALR
    # -----------------------------------------------------------------------------
    def lr_parse_table(self):
        Productions = self.grammar.Productions
        Precedence  = self.grammar.Precedence
        goto   = self.lr_goto         # Goto array
        action = self.lr_action       # Action array
        log    = self.log             # Logger for output

        actionp = { }                 # Action production array (temporary)
        
        log.info("Parsing method: %s", self.lr_method)

        # Step 1: Construct C = { I0, I1, ... IN}, collection of LR(0) items
        # This determines the number of states

        C = self.lr0_items()

        if self.lr_method == 'LALR':
            self.add_lalr_lookaheads(C)

        # Build the parser table, state by state
        st = 0
        for I in C:
            # Loop over each production in I
            actlist = [ ]              # List of actions
            st_action  = { }
            st_actionp = { }
            st_goto    = { }
            log.info("")
            log.info("state %d", st)
            log.info("")
            for p in I:
                log.info("    (%d) %s", p.number, str(p))
            log.info("")

            for p in I:
                    if p.len == p.lr_index + 1:
                        if p.name == "S'":
                            # Start symbol. Accept!
                            st_action["$end"] = 0
                            st_actionp["$end"] = p
                        else:
                            # We are at the end of a production.  Reduce!
                            if self.lr_method == 'LALR':
                                laheads = p.lookaheads[st]
                            else:
                                laheads = self.grammar.Follow[p.name]
                            for a in laheads:
                                actlist.append((a,p,"reduce using rule %d (%s)" % (p.number,p)))
                                r = st_action.get(a,None)
                                if r is not None:
                                    # Whoa. Have a shift/reduce or reduce/reduce conflict
                                    if r > 0:
                                        # Need to decide on shift or reduce here
                                        # By default we favor shifting. Need to add
                                        # some precedence rules here.
                                        sprec,slevel = Productions[st_actionp[a].number].prec
                                        rprec,rlevel = Precedence.get(a,('right',0))
                                        if (slevel < rlevel) or ((slevel == rlevel) and (rprec == 'left')):
                                            # We really need to reduce here.
                                            st_action[a] = -p.number
                                            st_actionp[a] = p
                                            if not slevel and not rlevel:
                                                log.info("  ! shift/reduce conflict for %s resolved as reduce",a)
                                                self.sr_conflicts.append((st,a,'reduce'))
                                            Productions[p.number].reduced += 1
                                        elif (slevel == rlevel) and (rprec == 'nonassoc'):
                                            st_action[a] = None
                                        else:
                                            # Hmmm. Guess we'll keep the shift
                                            if not rlevel:
                                                log.info("  ! shift/reduce conflict for %s resolved as shift",a)
                                                self.sr_conflicts.append((st,a,'shift'))
                                    elif r < 0:
                                        # Reduce/reduce conflict.   In this case, we favor the rule
                                        # that was defined first in the grammar file
                                        oldp = Productions[-r]
                                        pp = Productions[p.number]
                                        if oldp.line > pp.line:
                                            st_action[a] = -p.number
                                            st_actionp[a] = p
                                            chosenp,rejectp = pp,oldp
                                            Productions[p.number].reduced += 1
                                            Productions[oldp.number].reduced -= 1
                                        else:
                                            chosenp,rejectp = oldp,pp
                                        self.rr_conflicts.append((st,chosenp,rejectp))
                                        log.info("  ! reduce/reduce conflict for %s resolved using rule %d (%s)", a,st_actionp[a].number, st_actionp[a])
                                    else:
                                        raise LALRError("Unknown conflict in state %d" % st)
                                else:
                                    st_action[a] = -p.number
                                    st_actionp[a] = p
                                    Productions[p.number].reduced += 1
                    else:
                        i = p.lr_index
                        a = p.prod[i+1]       # Get symbol right after the "."
                        if a in self.grammar.Terminals:
                            g = self.lr0_goto(I,a)
                            j = self.lr0_cidhash.get(id(g),-1)
                            if j >= 0:
                                # We are in a shift state
                                actlist.append((a,p,"shift and go to state %d" % j))
                                r = st_action.get(a,None)
                                if r is not None:
                                    # Whoa have a shift/reduce or shift/shift conflict
                                    if r > 0:
                                        if r != j:
                                            raise LALRError("Shift/shift conflict in state %d" % st)
                                    elif r < 0:
                                        # Do a precedence check.
                                        #   -  if precedence of reduce rule is higher, we reduce.
                                        #   -  if precedence of reduce is same and left assoc, we reduce.
                                        #   -  otherwise we shift
                                        rprec,rlevel = Productions[st_actionp[a].number].prec
                                        sprec,slevel = Precedence.get(a,('right',0))
                                        if (slevel > rlevel) or ((slevel == rlevel) and (rprec == 'right')):
                                            # We decide to shift here... highest precedence to shift
                                            Productions[st_actionp[a].number].reduced -= 1
                                            st_action[a] = j
                                            st_actionp[a] = p
                                            if not rlevel:
                                                log.info("  ! shift/reduce conflict for %s resolved as shift",a)
                                                self.sr_conflicts.append((st,a,'shift'))
                                        elif (slevel == rlevel) and (rprec == 'nonassoc'):
                                            st_action[a] = None
                                        else:
                                            # Hmmm. Guess we'll keep the reduce
                                            if not slevel and not rlevel:
                                                log.info("  ! shift/reduce conflict for %s resolved as reduce",a)
                                                self.sr_conflicts.append((st,a,'reduce'))

                                    else:
                                        raise LALRError("Unknown conflict in state %d" % st)
                                else:
                                    st_action[a] = j
                                    st_actionp[a] = p

            # Print the actions associated with each terminal
            _actprint = { }
            for a,p,m in actlist:
                if a in st_action:
                    if p is st_actionp[a]:
                        log.info("    %-15s %s",a,m)
                        _actprint[(a,m)] = 1
            log.info("")
            # Print the actions that were not used. (debugging)
            not_used = 0
            for a,p,m in actlist:
                if a in st_action:
                    if p is not st_actionp[a]:
                        if not (a,m) in _actprint:
                            log.debug("  ! %-15s [ %s ]",a,m)
                            not_used = 1
                            _actprint[(a,m)] = 1
            if not_used:
                log.debug("")

            # Construct the goto table for this state

            nkeys = { }
            for ii in I:
                for s in ii.usyms:
                    if s in self.grammar.Nonterminals:
                        nkeys[s] = None
            for n in nkeys:
                g = self.lr0_goto(I,n)
                j = self.lr0_cidhash.get(id(g),-1)
                if j >= 0:
                    st_goto[n] = j
                    log.info("    %-30s shift and go to state %d",n,j)

            action[st] = st_action
            actionp[st] = st_actionp
            goto[st] = st_goto
            st += 1


    # -----------------------------------------------------------------------------
    # write()
    #
    # This function writes the LR parsing tables to a file
    # -----------------------------------------------------------------------------

    def write_table(self,modulename,outputdir='',signature=""):
        basemodulename = modulename.split(".")[-1]
        filename = os.path.join(outputdir,basemodulename) + ".py"
        try:
            f = open(filename,"w")

            f.write("""
# %s
# This file is automatically generated. Do not edit.
_tabversion = %r

_lr_method = %r

_lr_signature = %r
    """ % (filename, __tabversion__, self.lr_method, signature))

            # Change smaller to 0 to go back to original tables
            smaller = 1

            # Factor out names to try and make smaller
            if smaller:
                items = { }

                for s,nd in self.lr_action.items():
                   for name,v in nd.items():
                      i = items.get(name)
                      if not i:
                         i = ([],[])
                         items[name] = i
                      i[0].append(s)
                      i[1].append(v)

                f.write("\n_lr_action_items = {")
                for k,v in items.items():
                    f.write("%r:([" % k)
                    for i in v[0]:
                        f.write("%r," % i)
                    f.write("],[")
                    for i in v[1]:
                        f.write("%r," % i)

                    f.write("]),")
                f.write("}\n")

                f.write("""
_lr_action = { }
for _k, _v in _lr_action_items.items():
   for _x,_y in zip(_v[0],_v[1]):
      if not _x in _lr_action:  _lr_action[_x] = { }
      _lr_action[_x][_k] = _y
del _lr_action_items
""")

            else:
                f.write("\n_lr_action = { ");
                for k,v in self.lr_action.items():
                    f.write("(%r,%r):%r," % (k[0],k[1],v))
                f.write("}\n");

            if smaller:
                # Factor out names to try and make smaller
                items = { }

                for s,nd in self.lr_goto.items():
                   for name,v in nd.items():
                      i = items.get(name)
                      if not i:
                         i = ([],[])
                         items[name] = i
                      i[0].append(s)
                      i[1].append(v)

                f.write("\n_lr_goto_items = {")
                for k,v in items.items():
                    f.write("%r:([" % k)
                    for i in v[0]:
                        f.write("%r," % i)
                    f.write("],[")
                    for i in v[1]:
                        f.write("%r," % i)

                    f.write("]),")
                f.write("}\n")

                f.write("""
_lr_goto = { }
for _k, _v in _lr_goto_items.items():
   for _x,_y in zip(_v[0],_v[1]):
       if not _x in _lr_goto: _lr_goto[_x] = { }
       _lr_goto[_x][_k] = _y
del _lr_goto_items
""")
            else:
                f.write("\n_lr_goto = { ");
                for k,v in self.lr_goto.items():
                    f.write("(%r,%r):%r," % (k[0],k[1],v))
                f.write("}\n");

            # Write production table
            f.write("_lr_productions = [\n")
            for p in self.lr_productions:
                if p.func:
                    f.write("  (%r,%r,%d,%r,%r,%d),\n" % (p.str,p.name, p.len, p.func,p.file,p.line))
                else:
                    f.write("  (%r,%r,%d,None,None,None),\n" % (str(p),p.name, p.len))
            f.write("]\n")
            f.close()

        except IOError:
            e = sys.exc_info()[1]
            sys.stderr.write("Unable to create '%s'\n" % filename)
            sys.stderr.write(str(e)+"\n")
            return


    # -----------------------------------------------------------------------------
    # pickle_table()
    #
    # This function pickles the LR parsing tables to a supplied file object
    # -----------------------------------------------------------------------------

    def pickle_table(self,filename,signature=""):
        try:
            import cPickle as pickle
        except ImportError:
            import pickle
        outf = open(filename,"wb")
        pickle.dump(__tabversion__,outf,pickle_protocol)
        pickle.dump(self.lr_method,outf,pickle_protocol)
        pickle.dump(signature,outf,pickle_protocol)
        pickle.dump(self.lr_action,outf,pickle_protocol)
        pickle.dump(self.lr_goto,outf,pickle_protocol)

        outp = []
        for p in self.lr_productions:
            if p.func:
                outp.append((p.str,p.name, p.len, p.func,p.file,p.line))
            else:
                outp.append((str(p),p.name,p.len,None,None,None))
        pickle.dump(outp,outf,pickle_protocol)
        outf.close()

# -----------------------------------------------------------------------------
#                            === INTROSPECTION ===
#
# The following functions and classes are used to implement the PLY
# introspection features followed by the yacc() function itself.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# get_caller_module_dict()
#
# This function returns a dictionary containing all of the symbols defined within
# a caller further down the call stack.  This is used to get the environment
# associated with the yacc() call if none was provided.
# -----------------------------------------------------------------------------

def get_caller_module_dict(levels):
    try:
        raise RuntimeError
    except RuntimeError:
        e,b,t = sys.exc_info()
        f = t.tb_frame
        while levels > 0:
            f = f.f_back                   
            levels -= 1
        ldict = f.f_globals.copy()
        if f.f_globals != f.f_locals:
            ldict.update(f.f_locals)

        return ldict

# -----------------------------------------------------------------------------
# parse_grammar()
#
# This takes a raw grammar rule string and parses it into production data
# -----------------------------------------------------------------------------
def parse_grammar(doc,file,line):
    grammar = []
    # Split the doc string into lines
    pstrings = doc.splitlines()
    lastp = None
    dline = line
    for ps in pstrings:
        dline += 1
        p = ps.split()
        if not p: continue
        try:
            if p[0] == '|':
                # This is a continuation of a previous rule
                if not lastp:
                    raise SyntaxError("%s:%d: Misplaced '|'" % (file,dline))
                prodname = lastp
                syms = p[1:]
            else:
                prodname = p[0]
                lastp = prodname
                syms   = p[2:]
                assign = p[1]
                if assign != ':' and assign != '::=':
                    raise SyntaxError("%s:%d: Syntax error. Expected ':'" % (file,dline))

            grammar.append((file,dline,prodname,syms))
        except SyntaxError:
            raise
        except Exception:
            raise SyntaxError("%s:%d: Syntax error in rule '%s'" % (file,dline,ps.strip()))

    return grammar

# -----------------------------------------------------------------------------
# ParserReflect()
#
# This class represents information extracted for building a parser including
# start symbol, error function, tokens, precedence list, action functions,
# etc.
# -----------------------------------------------------------------------------
class ParserReflect(object):
    def __init__(self,pdict,log=None):
        self.pdict      = pdict
        self.start      = None
        self.error_func = None
        self.tokens     = None
        self.files      = {}
        self.grammar    = []
        self.error      = 0

        if log is None:
            self.log = PlyLogger(sys.stderr)
        else:
            self.log = log

    # Get all of the basic information
    def get_all(self):
        self.get_start()
        self.get_error_func()
        self.get_tokens()
        self.get_precedence()
        self.get_pfunctions()
        
    # Validate all of the information
    def validate_all(self):
        self.validate_start()
        self.validate_error_func()
        self.validate_tokens()
        self.validate_precedence()
        self.validate_pfunctions()
        self.validate_files()
        return self.error

    # Compute a signature over the grammar
    def signature(self):
        try:
            from hashlib import md5
        except ImportError:
            from md5 import md5
        try:
            sig = md5()
            if self.start:
                sig.update(self.start.encode('latin-1'))
            if self.prec:
                sig.update("".join(["".join(p) for p in self.prec]).encode('latin-1'))
            if self.tokens:
                sig.update(" ".join(self.tokens).encode('latin-1'))
            for f in self.pfuncs:
                if f[3]:
                    sig.update(f[3].encode('latin-1'))
        except (TypeError,ValueError):
            pass
        return sig.digest()

    # -----------------------------------------------------------------------------
    # validate_file()
    #
    # This method checks to see if there are duplicated p_rulename() functions
    # in the parser module file.  Without this function, it is really easy for
    # users to make mistakes by cutting and pasting code fragments (and it's a real
    # bugger to try and figure out why the resulting parser doesn't work).  Therefore,
    # we just do a little regular expression pattern matching of def statements
    # to try and detect duplicates.
    # -----------------------------------------------------------------------------

    def validate_files(self):
        # Match def p_funcname(
        fre = re.compile(r'\s*def\s+(p_[a-zA-Z_0-9]*)\(')

        for filename in self.files.keys():
            base,ext = os.path.splitext(filename)
            if ext != '.py': return 1          # No idea. Assume it's okay.

            try:
                f = open(filename)
                lines = f.readlines()
                f.close()
            except IOError:
                continue

            counthash = { }
            for linen,l in enumerate(lines):
                linen += 1
                m = fre.match(l)
                if m:
                    name = m.group(1)
                    prev = counthash.get(name)
                    if not prev:
                        counthash[name] = linen
                    else:
                        self.log.warning("%s:%d: Function %s redefined. Previously defined on line %d", filename,linen,name,prev)

    # Get the start symbol
    def get_start(self):
        self.start = self.pdict.get('start')

    # Validate the start symbol
    def validate_start(self):
        if self.start is not None:
            if not isinstance(self.start,str):
                self.log.error("'start' must be a string")

    # Look for error handler
    def get_error_func(self):
        self.error_func = self.pdict.get('p_error')

    # Validate the error function
    def validate_error_func(self):
        if self.error_func:
            if isinstance(self.error_func,types.FunctionType):
                ismethod = 0
            elif isinstance(self.error_func, types.MethodType):
                ismethod = 1
            else:
                self.log.error("'p_error' defined, but is not a function or method")
                self.error = 1
                return

            eline = func_code(self.error_func).co_firstlineno
            efile = func_code(self.error_func).co_filename
            self.files[efile] = 1

            if (func_code(self.error_func).co_argcount != 1+ismethod):
                self.log.error("%s:%d: p_error() requires 1 argument",efile,eline)
                self.error = 1

    # Get the tokens map
    def get_tokens(self):
        tokens = self.pdict.get("tokens",None)
        if not tokens:
            self.log.error("No token list is defined")
            self.error = 1
            return

        if not isinstance(tokens,(list, tuple)):
            self.log.error("tokens must be a list or tuple")
            self.error = 1
            return
        
        if not tokens:
            self.log.error("tokens is empty")
            self.error = 1
            return

        self.tokens = tokens

    # Validate the tokens
    def validate_tokens(self):
        # Validate the tokens.
        if 'error' in self.tokens:
            self.log.error("Illegal token name 'error'. Is a reserved word")
            self.error = 1
            return

        terminals = {}
        for n in self.tokens:
            if n in terminals:
                self.log.warning("Token '%s' multiply defined", n)
            terminals[n] = 1

    # Get the precedence map (if any)
    def get_precedence(self):
        self.prec = self.pdict.get("precedence",None)

    # Validate and parse the precedence map
    def validate_precedence(self):
        preclist = []
        if self.prec:
            if not isinstance(self.prec,(list,tuple)):
                self.log.error("precedence must be a list or tuple")
                self.error = 1
                return
            for level,p in enumerate(self.prec):
                if not isinstance(p,(list,tuple)):
                    self.log.error("Bad precedence table")
                    self.error = 1
                    return

                if len(p) < 2:
                    self.log.error("Malformed precedence entry %s. Must be (assoc, term, ..., term)",p)
                    self.error = 1
                    return
                assoc = p[0]
                if not isinstance(assoc,str):
                    self.log.error("precedence associativity must be a string")
                    self.error = 1
                    return
                for term in p[1:]:
                    if not isinstance(term,str):
                        self.log.error("precedence items must be strings")
                        self.error = 1
                        return
                    preclist.append((term,assoc,level+1))
        self.preclist = preclist

    # Get all p_functions from the grammar
    def get_pfunctions(self):
        p_functions = []
        for name, item in self.pdict.items():
            if name[:2] != 'p_': continue
            if name == 'p_error': continue
            if isinstance(item,(types.FunctionType,types.MethodType)):
                line = func_code(item).co_firstlineno
                file = func_code(item).co_filename
                p_functions.append((line,file,name,item.__doc__))

        # Sort all of the actions by line number
        p_functions.sort()
        self.pfuncs = p_functions


    # Validate all of the p_functions
    def validate_pfunctions(self):
        grammar = []
        # Check for non-empty symbols
        if len(self.pfuncs) == 0:
            self.log.error("no rules of the form p_rulename are defined")
            self.error = 1
            return 
        
        for line, file, name, doc in self.pfuncs:
            func = self.pdict[name]
            if isinstance(func, types.MethodType):
                reqargs = 2
            else:
                reqargs = 1
            if func_code(func).co_argcount > reqargs:
                self.log.error("%s:%d: Rule '%s' has too many arguments",file,line,func.__name__)
                self.error = 1
            elif func_code(func).co_argcount < reqargs:
                self.log.error("%s:%d: Rule '%s' requires an argument",file,line,func.__name__)
                self.error = 1
            elif not func.__doc__:
                self.log.warning("%s:%d: No documentation string specified in function '%s' (ignored)",file,line,func.__name__)
            else:
                try:
                    parsed_g = parse_grammar(doc,file,line)
                    for g in parsed_g:
                        grammar.append((name, g))
                except SyntaxError:
                    e = sys.exc_info()[1]
                    self.log.error(str(e))
                    self.error = 1

                # Looks like a valid grammar rule
                # Mark the file in which defined.
                self.files[file] = 1

        # Secondary validation step that looks for p_ definitions that are not functions
        # or functions that look like they might be grammar rules.

        for n,v in self.pdict.items():
            if n[0:2] == 'p_' and isinstance(v, (types.FunctionType, types.MethodType)): continue
            if n[0:2] == 't_': continue
            if n[0:2] == 'p_' and n != 'p_error':
                self.log.warning("'%s' not defined as a function", n)
            if ((isinstance(v,types.FunctionType) and func_code(v).co_argcount == 1) or
                (isinstance(v,types.MethodType) and func_code(v).co_argcount == 2)):
                try:
                    doc = v.__doc__.split(" ")
                    if doc[1] == ':':
                        self.log.warning("%s:%d: Possible grammar rule '%s' defined without p_ prefix",
                                         func_code(v).co_filename, func_code(v).co_firstlineno,n)
                except Exception:
                    pass

        self.grammar = grammar

# -----------------------------------------------------------------------------
# yacc(module)
#
# Build a parser
# -----------------------------------------------------------------------------

def yacc(method='LALR', debug=yaccdebug, module=None, tabmodule=tab_module, start=None, 
         check_recursion=1, optimize=0, write_tables=1, debugfile=debug_file,outputdir='',
         debuglog=None, errorlog = None, picklefile=None):

    global parse                 # Reference to the parsing method of the last built parser

    # If pickling is enabled, table files are not created

    if picklefile:
        write_tables = 0

    if errorlog is None:
        errorlog = PlyLogger(sys.stderr)

    # Get the module dictionary used for the parser
    if module:
        _items = [(k,getattr(module,k)) for k in dir(module)]
        pdict = dict(_items)
    else:
        pdict = get_caller_module_dict(2)

    # Collect parser information from the dictionary
    pinfo = ParserReflect(pdict,log=errorlog)
    pinfo.get_all()

    if pinfo.error:
        raise YaccError("Unable to build parser")

    # Check signature against table files (if any)
    signature = pinfo.signature()

    # Read the tables
    try:
        lr = LRTable()
        if picklefile:
            read_signature = lr.read_pickle(picklefile)
        else:
            read_signature = lr.read_table(tabmodule)
        if optimize or (read_signature == signature):
            try:
                lr.bind_callables(pinfo.pdict)
                parser = LRParser(lr,pinfo.error_func)
                parse = parser.parse
                return parser
            except Exception:
                e = sys.exc_info()[1]
                errorlog.warning("There was a problem loading the table file: %s", repr(e))
    except VersionError:
        e = sys.exc_info()
        errorlog.warning(str(e))
    except Exception:
        pass

    if debuglog is None:
        if debug:
            debuglog = PlyLogger(open(debugfile,"w"))
        else:
            debuglog = NullLogger()

    debuglog.info("Created by PLY version %s (http://www.dabeaz.com/ply)", __version__)


    errors = 0

    # Validate the parser information
    if pinfo.validate_all():
        raise YaccError("Unable to build parser")
    
    if not pinfo.error_func:
        errorlog.warning("no p_error() function is defined")

    # Create a grammar object
    grammar = Grammar(pinfo.tokens)

    # Set precedence level for terminals
    for term, assoc, level in pinfo.preclist:
        try:
            grammar.set_precedence(term,assoc,level)
        except GrammarError:
            e = sys.exc_info()[1]
            errorlog.warning("%s",str(e))

    # Add productions to the grammar
    for funcname, gram in pinfo.grammar:
        file, line, prodname, syms = gram
        try:
            grammar.add_production(prodname,syms,funcname,file,line)
        except GrammarError:
            e = sys.exc_info()[1]
            errorlog.error("%s",str(e))
            errors = 1

    # Set the grammar start symbols
    try:
        if start is None:
            grammar.set_start(pinfo.start)
        else:
            grammar.set_start(start)
    except GrammarError:
        e = sys.exc_info()[1]
        errorlog.error(str(e))
        errors = 1

    if errors:
        raise YaccError("Unable to build parser")

    # Verify the grammar structure
    undefined_symbols = grammar.undefined_symbols()
    for sym, prod in undefined_symbols:
        errorlog.error("%s:%d: Symbol '%s' used, but not defined as a token or a rule",prod.file,prod.line,sym)
        errors = 1

    unused_terminals = grammar.unused_terminals()
    if unused_terminals:
        debuglog.info("")
        debuglog.info("Unused terminals:")
        debuglog.info("")
        for term in unused_terminals:
            errorlog.warning("Token '%s' defined, but not used", term)
            debuglog.info("    %s", term)

    # Print out all productions to the debug log
    if debug:
        debuglog.info("")
        debuglog.info("Grammar")
        debuglog.info("")
        for n,p in enumerate(grammar.Productions):
            debuglog.info("Rule %-5d %s", n, p)

    # Find unused non-terminals
    unused_rules = grammar.unused_rules()
    for prod in unused_rules:
        errorlog.warning("%s:%d: Rule '%s' defined, but not used", prod.file, prod.line, prod.name)

    if len(unused_terminals) == 1:
        errorlog.warning("There is 1 unused token")
    if len(unused_terminals) > 1:
        errorlog.warning("There are %d unused tokens", len(unused_terminals))

    if len(unused_rules) == 1:
        errorlog.warning("There is 1 unused rule")
    if len(unused_rules) > 1:
        errorlog.warning("There are %d unused rules", len(unused_rules))

    if debug:
        debuglog.info("")
        debuglog.info("Terminals, with rules where they appear")
        debuglog.info("")
        terms = list(grammar.Terminals)
        terms.sort()
        for term in terms:
            debuglog.info("%-20s : %s", term, " ".join([str(s) for s in grammar.Terminals[term]]))
        
        debuglog.info("")
        debuglog.info("Nonterminals, with rules where they appear")
        debuglog.info("")
        nonterms = list(grammar.Nonterminals)
        nonterms.sort()
        for nonterm in nonterms:
            debuglog.info("%-20s : %s", nonterm, " ".join([str(s) for s in grammar.Nonterminals[nonterm]]))
        debuglog.info("")

    if check_recursion:
        unreachable = grammar.find_unreachable()
        for u in unreachable:
            errorlog.warning("Symbol '%s' is unreachable",u)

        infinite = grammar.infinite_cycles()
        for inf in infinite:
            errorlog.error("Infinite recursion detected for symbol '%s'", inf)
            errors = 1
        
    unused_prec = grammar.unused_precedence()
    for term, assoc in unused_prec:
        errorlog.error("Precedence rule '%s' defined for unknown symbol '%s'", assoc, term)
        errors = 1

    if errors:
        raise YaccError("Unable to build parser")
    
    # Run the LRGeneratedTable on the grammar
    if debug:
        errorlog.debug("Generating %s tables", method)
            
    lr = LRGeneratedTable(grammar,method,debuglog)

    if debug:
        num_sr = len(lr.sr_conflicts)

        # Report shift/reduce and reduce/reduce conflicts
        if num_sr == 1:
            errorlog.warning("1 shift/reduce conflict")
        elif num_sr > 1:
            errorlog.warning("%d shift/reduce conflicts", num_sr)

        num_rr = len(lr.rr_conflicts)
        if num_rr == 1:
            errorlog.warning("1 reduce/reduce conflict")
        elif num_rr > 1:
            errorlog.warning("%d reduce/reduce conflicts", num_rr)

    # Write out conflicts to the output file
    if debug and (lr.sr_conflicts or lr.rr_conflicts):
        debuglog.warning("")
        debuglog.warning("Conflicts:")
        debuglog.warning("")

        for state, tok, resolution in lr.sr_conflicts:
            debuglog.warning("shift/reduce conflict for %s in state %d resolved as %s",  tok, state, resolution)
        
        already_reported = {}
        for state, rule, rejected in lr.rr_conflicts:
            if (state,id(rule),id(rejected)) in already_reported:
                continue
            debuglog.warning("reduce/reduce conflict in state %d resolved using rule (%s)", state, rule)
            debuglog.warning("rejected rule (%s) in state %d", rejected,state)
            errorlog.warning("reduce/reduce conflict in state %d resolved using rule (%s)", state, rule)
            errorlog.warning("rejected rule (%s) in state %d", rejected, state)
            already_reported[state,id(rule),id(rejected)] = 1
        
        warned_never = []
        for state, rule, rejected in lr.rr_conflicts:
            if not rejected.reduced and (rejected not in warned_never):
                debuglog.warning("Rule (%s) is never reduced", rejected)
                errorlog.warning("Rule (%s) is never reduced", rejected)
                warned_never.append(rejected)

    # Write the table file if requested
    if write_tables:
        lr.write_table(tabmodule,outputdir,signature)

    # Write a pickled version of the tables
    if picklefile:
        lr.pickle_table(picklefile,signature)

    # Build the parser
    lr.bind_callables(pinfo.pdict)
    parser = LRParser(lr,pinfo.error_func)

    parse = parser.parse
    return parser

########NEW FILE########
__FILENAME__ = decoder
"""Implementation of JSONDecoder
"""
import re
import sys
import struct

from simplejson.scanner import make_scanner
def _import_c_scanstring():
    try:
        from simplejson._speedups import scanstring
        return scanstring
    except ImportError:
        return None
c_scanstring = _import_c_scanstring()

__all__ = ['JSONDecoder']

FLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL

def _floatconstants():
    _BYTES = '7FF80000000000007FF0000000000000'.decode('hex')
    # The struct module in Python 2.4 would get frexp() out of range here
    # when an endian is specified in the format string. Fixed in Python 2.5+
    if sys.byteorder != 'big':
        _BYTES = _BYTES[:8][::-1] + _BYTES[8:][::-1]
    nan, inf = struct.unpack('dd', _BYTES)
    return nan, inf, -inf

NaN, PosInf, NegInf = _floatconstants()


class JSONDecodeError(ValueError):
    """Subclass of ValueError with the following additional properties:
    
    msg: The unformatted error message
    doc: The JSON document being parsed
    pos: The start index of doc where parsing failed
    end: The end index of doc where parsing failed (may be None)
    lineno: The line corresponding to pos
    colno: The column corresponding to pos
    endlineno: The line corresponding to end (may be None)
    endcolno: The column corresponding to end (may be None)
    
    """
    def __init__(self, msg, doc, pos, end=None):
        ValueError.__init__(self, errmsg(msg, doc, pos, end=end))
        self.msg = msg
        self.doc = doc
        self.pos = pos
        self.end = end
        self.lineno, self.colno = linecol(doc, pos)
        if end is not None:
            self.endlineno, self.endcolno = linecol(doc, pos)
        else:
            self.endlineno, self.endcolno = None, None


def linecol(doc, pos):
    lineno = doc.count('\n', 0, pos) + 1
    if lineno == 1:
        colno = pos
    else:
        colno = pos - doc.rindex('\n', 0, pos)
    return lineno, colno


def errmsg(msg, doc, pos, end=None):
    # Note that this function is called from _speedups
    lineno, colno = linecol(doc, pos)
    if end is None:
        #fmt = '{0}: line {1} column {2} (char {3})'
        #return fmt.format(msg, lineno, colno, pos)
        fmt = '%s: line %d column %d (char %d)'
        return fmt % (msg, lineno, colno, pos)
    endlineno, endcolno = linecol(doc, end)
    #fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'
    #return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)
    fmt = '%s: line %d column %d - line %d column %d (char %d - %d)'
    return fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)


_CONSTANTS = {
    '-Infinity': NegInf,
    'Infinity': PosInf,
    'NaN': NaN,
}

STRINGCHUNK = re.compile(r'(.*?)(["\\\x00-\x1f])', FLAGS)
BACKSLASH = {
    '"': u'"', '\\': u'\\', '/': u'/',
    'b': u'\b', 'f': u'\f', 'n': u'\n', 'r': u'\r', 't': u'\t',
}

DEFAULT_ENCODING = "utf-8"

def py_scanstring(s, end, encoding=None, strict=True,
        _b=BACKSLASH, _m=STRINGCHUNK.match):
    """Scan the string s for a JSON string. End is the index of the
    character in s after the quote that started the JSON string.
    Unescapes all valid JSON string escape sequences and raises ValueError
    on attempt to decode an invalid string. If strict is False then literal
    control characters are allowed in the string.

    Returns a tuple of the decoded string and the index of the character in s
    after the end quote."""
    if encoding is None:
        encoding = DEFAULT_ENCODING
    chunks = []
    _append = chunks.append
    begin = end - 1
    while 1:
        chunk = _m(s, end)
        if chunk is None:
            raise JSONDecodeError(
                "Unterminated string starting at", s, begin)
        end = chunk.end()
        content, terminator = chunk.groups()
        # Content is contains zero or more unescaped string characters
        if content:
            if not isinstance(content, unicode):
                content = unicode(content, encoding)
            _append(content)
        # Terminator is the end of string, a literal control character,
        # or a backslash denoting that an escape sequence follows
        if terminator == '"':
            break
        elif terminator != '\\':
            if strict:
                msg = "Invalid control character %r at" % (terminator,)
                #msg = "Invalid control character {0!r} at".format(terminator)
                raise JSONDecodeError(msg, s, end)
            else:
                _append(terminator)
                continue
        try:
            esc = s[end]
        except IndexError:
            raise JSONDecodeError(
                "Unterminated string starting at", s, begin)
        # If not a unicode escape sequence, must be in the lookup table
        if esc != 'u':
            try:
                char = _b[esc]
            except KeyError:
                msg = "Invalid \\escape: " + repr(esc)
                raise JSONDecodeError(msg, s, end)
            end += 1
        else:
            # Unicode escape sequence
            esc = s[end + 1:end + 5]
            next_end = end + 5
            if len(esc) != 4:
                msg = "Invalid \\uXXXX escape"
                raise JSONDecodeError(msg, s, end)
            uni = int(esc, 16)
            # Check for surrogate pair on UCS-4 systems
            if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:
                msg = "Invalid \\uXXXX\\uXXXX surrogate pair"
                if not s[end + 5:end + 7] == '\\u':
                    raise JSONDecodeError(msg, s, end)
                esc2 = s[end + 7:end + 11]
                if len(esc2) != 4:
                    raise JSONDecodeError(msg, s, end)
                uni2 = int(esc2, 16)
                uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))
                next_end += 6
            char = unichr(uni)
            end = next_end
        # Append the unescaped character
        _append(char)
    return u''.join(chunks), end


# Use speedup if available
scanstring = c_scanstring or py_scanstring

WHITESPACE = re.compile(r'[ \t\n\r]*', FLAGS)
WHITESPACE_STR = ' \t\n\r'

def JSONObject((s, end), encoding, strict, scan_once, object_hook,
        object_pairs_hook, memo=None,
        _w=WHITESPACE.match, _ws=WHITESPACE_STR):
    # Backwards compatibility
    if memo is None:
        memo = {}
    memo_get = memo.setdefault
    pairs = []
    # Use a slice to prevent IndexError from being raised, the following
    # check will raise a more specific ValueError if the string is empty
    nextchar = s[end:end + 1]
    # Normally we expect nextchar == '"'
    if nextchar != '"':
        if nextchar in _ws:
            end = _w(s, end).end()
            nextchar = s[end:end + 1]
        # Trivial empty object
        if nextchar == '}':
            if object_pairs_hook is not None:
                result = object_pairs_hook(pairs)
                return result, end
            pairs = {}
            if object_hook is not None:
                pairs = object_hook(pairs)
            return pairs, end + 1
        elif nextchar != '"':
            raise JSONDecodeError("Expecting property name", s, end)
    end += 1
    while True:
        key, end = scanstring(s, end, encoding, strict)
        key = memo_get(key, key)

        # To skip some function call overhead we optimize the fast paths where
        # the JSON key separator is ": " or just ":".
        if s[end:end + 1] != ':':
            end = _w(s, end).end()
            if s[end:end + 1] != ':':
                raise JSONDecodeError("Expecting : delimiter", s, end)

        end += 1

        try:
            if s[end] in _ws:
                end += 1
                if s[end] in _ws:
                    end = _w(s, end + 1).end()
        except IndexError:
            pass

        try:
            value, end = scan_once(s, end)
        except StopIteration:
            raise JSONDecodeError("Expecting object", s, end)
        pairs.append((key, value))

        try:
            nextchar = s[end]
            if nextchar in _ws:
                end = _w(s, end + 1).end()
                nextchar = s[end]
        except IndexError:
            nextchar = ''
        end += 1

        if nextchar == '}':
            break
        elif nextchar != ',':
            raise JSONDecodeError("Expecting , delimiter", s, end - 1)

        try:
            nextchar = s[end]
            if nextchar in _ws:
                end += 1
                nextchar = s[end]
                if nextchar in _ws:
                    end = _w(s, end + 1).end()
                    nextchar = s[end]
        except IndexError:
            nextchar = ''

        end += 1
        if nextchar != '"':
            raise JSONDecodeError("Expecting property name", s, end - 1)

    if object_pairs_hook is not None:
        result = object_pairs_hook(pairs)
        return result, end
    pairs = dict(pairs)
    if object_hook is not None:
        pairs = object_hook(pairs)
    return pairs, end

def JSONArray((s, end), scan_once, _w=WHITESPACE.match, _ws=WHITESPACE_STR):
    values = []
    nextchar = s[end:end + 1]
    if nextchar in _ws:
        end = _w(s, end + 1).end()
        nextchar = s[end:end + 1]
    # Look-ahead for trivial empty array
    if nextchar == ']':
        return values, end + 1
    _append = values.append
    while True:
        try:
            value, end = scan_once(s, end)
        except StopIteration:
            raise JSONDecodeError("Expecting object", s, end)
        _append(value)
        nextchar = s[end:end + 1]
        if nextchar in _ws:
            end = _w(s, end + 1).end()
            nextchar = s[end:end + 1]
        end += 1
        if nextchar == ']':
            break
        elif nextchar != ',':
            raise JSONDecodeError("Expecting , delimiter", s, end)

        try:
            if s[end] in _ws:
                end += 1
                if s[end] in _ws:
                    end = _w(s, end + 1).end()
        except IndexError:
            pass

    return values, end

class JSONDecoder(object):
    """Simple JSON <http://json.org> decoder

    Performs the following translations in decoding by default:

    +---------------+-------------------+
    | JSON          | Python            |
    +===============+===================+
    | object        | dict              |
    +---------------+-------------------+
    | array         | list              |
    +---------------+-------------------+
    | string        | unicode           |
    +---------------+-------------------+
    | number (int)  | int, long         |
    +---------------+-------------------+
    | number (real) | float             |
    +---------------+-------------------+
    | true          | True              |
    +---------------+-------------------+
    | false         | False             |
    +---------------+-------------------+
    | null          | None              |
    +---------------+-------------------+

    It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as
    their corresponding ``float`` values, which is outside the JSON spec.

    """

    def __init__(self, encoding=None, object_hook=None, parse_float=None,
            parse_int=None, parse_constant=None, strict=True,
            object_pairs_hook=None):
        """
        *encoding* determines the encoding used to interpret any
        :class:`str` objects decoded by this instance (``'utf-8'`` by
        default).  It has no effect when decoding :class:`unicode` objects.

        Note that currently only encodings that are a superset of ASCII work,
        strings of other encodings should be passed in as :class:`unicode`.

        *object_hook*, if specified, will be called with the result of every
        JSON object decoded and its return value will be used in place of the
        given :class:`dict`.  This can be used to provide custom
        deserializations (e.g. to support JSON-RPC class hinting).

        *object_pairs_hook* is an optional function that will be called with
        the result of any object literal decode with an ordered list of pairs.
        The return value of *object_pairs_hook* will be used instead of the
        :class:`dict`.  This feature can be used to implement custom decoders
        that rely on the order that the key and value pairs are decoded (for
        example, :func:`collections.OrderedDict` will remember the order of
        insertion). If *object_hook* is also defined, the *object_pairs_hook*
        takes priority.

        *parse_float*, if specified, will be called with the string of every
        JSON float to be decoded.  By default, this is equivalent to
        ``float(num_str)``. This can be used to use another datatype or parser
        for JSON floats (e.g. :class:`decimal.Decimal`).

        *parse_int*, if specified, will be called with the string of every
        JSON int to be decoded.  By default, this is equivalent to
        ``int(num_str)``.  This can be used to use another datatype or parser
        for JSON integers (e.g. :class:`float`).

        *parse_constant*, if specified, will be called with one of the
        following strings: ``'-Infinity'``, ``'Infinity'``, ``'NaN'``.  This
        can be used to raise an exception if invalid JSON numbers are
        encountered.

        *strict* controls the parser's behavior when it encounters an
        invalid control character in a string. The default setting of
        ``True`` means that unescaped control characters are parse errors, if
        ``False`` then control characters will be allowed in strings.

        """
        self.encoding = encoding
        self.object_hook = object_hook
        self.object_pairs_hook = object_pairs_hook
        self.parse_float = parse_float or float
        self.parse_int = parse_int or int
        self.parse_constant = parse_constant or _CONSTANTS.__getitem__
        self.strict = strict
        self.parse_object = JSONObject
        self.parse_array = JSONArray
        self.parse_string = scanstring
        self.memo = {}
        self.scan_once = make_scanner(self)

    def decode(self, s, _w=WHITESPACE.match):
        """Return the Python representation of ``s`` (a ``str`` or ``unicode``
        instance containing a JSON document)

        """
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
        end = _w(s, end).end()
        if end != len(s):
            raise JSONDecodeError("Extra data", s, end, len(s))
        return obj

    def raw_decode(self, s, idx=0):
        """Decode a JSON document from ``s`` (a ``str`` or ``unicode``
        beginning with a JSON document) and return a 2-tuple of the Python
        representation and the index in ``s`` where the document ended.

        This can be used to decode a JSON document from a string that may
        have extraneous data at the end.

        """
        try:
            obj, end = self.scan_once(s, idx)
        except StopIteration:
            raise JSONDecodeError("No JSON object could be decoded", s, idx)
        return obj, end

########NEW FILE########
__FILENAME__ = encoder
"""Implementation of JSONEncoder
"""
import re
from decimal import Decimal

def _import_speedups():
    try:
        from simplejson import _speedups
        return _speedups.encode_basestring_ascii, _speedups.make_encoder
    except ImportError:
        return None, None
c_encode_basestring_ascii, c_make_encoder = _import_speedups()

from simplejson.decoder import PosInf

ESCAPE = re.compile(r'[\x00-\x1f\\"\b\f\n\r\t]')
ESCAPE_ASCII = re.compile(r'([\\"]|[^\ -~])')
HAS_UTF8 = re.compile(r'[\x80-\xff]')
ESCAPE_DCT = {
    '\\': '\\\\',
    '"': '\\"',
    '\b': '\\b',
    '\f': '\\f',
    '\n': '\\n',
    '\r': '\\r',
    '\t': '\\t',
}
for i in range(0x20):
    #ESCAPE_DCT.setdefault(chr(i), '\\u{0:04x}'.format(i))
    ESCAPE_DCT.setdefault(chr(i), '\\u%04x' % (i,))

FLOAT_REPR = repr

def encode_basestring(s):
    """Return a JSON representation of a Python string

    """
    if isinstance(s, str) and HAS_UTF8.search(s) is not None:
        s = s.decode('utf-8')
    def replace(match):
        return ESCAPE_DCT[match.group(0)]
    return u'"' + ESCAPE.sub(replace, s) + u'"'


def py_encode_basestring_ascii(s):
    """Return an ASCII-only JSON representation of a Python string

    """
    if isinstance(s, str) and HAS_UTF8.search(s) is not None:
        s = s.decode('utf-8')
    def replace(match):
        s = match.group(0)
        try:
            return ESCAPE_DCT[s]
        except KeyError:
            n = ord(s)
            if n < 0x10000:
                #return '\\u{0:04x}'.format(n)
                return '\\u%04x' % (n,)
            else:
                # surrogate pair
                n -= 0x10000
                s1 = 0xd800 | ((n >> 10) & 0x3ff)
                s2 = 0xdc00 | (n & 0x3ff)
                #return '\\u{0:04x}\\u{1:04x}'.format(s1, s2)
                return '\\u%04x\\u%04x' % (s1, s2)
    return '"' + str(ESCAPE_ASCII.sub(replace, s)) + '"'


encode_basestring_ascii = (
    c_encode_basestring_ascii or py_encode_basestring_ascii)

class JSONEncoder(object):
    """Extensible JSON <http://json.org> encoder for Python data structures.

    Supports the following objects and types by default:

    +-------------------+---------------+
    | Python            | JSON          |
    +===================+===============+
    | dict              | object        |
    +-------------------+---------------+
    | list, tuple       | array         |
    +-------------------+---------------+
    | str, unicode      | string        |
    +-------------------+---------------+
    | int, long, float  | number        |
    +-------------------+---------------+
    | True              | true          |
    +-------------------+---------------+
    | False             | false         |
    +-------------------+---------------+
    | None              | null          |
    +-------------------+---------------+

    To extend this to recognize other objects, subclass and implement a
    ``.default()`` method with another method that returns a serializable
    object for ``o`` if possible, otherwise it should call the superclass
    implementation (to raise ``TypeError``).

    """
    item_separator = ', '
    key_separator = ': '
    def __init__(self, skipkeys=False, ensure_ascii=True,
            check_circular=True, allow_nan=True, sort_keys=False,
            indent=None, separators=None, encoding='utf-8', default=None,
            use_decimal=False):
        """Constructor for JSONEncoder, with sensible defaults.

        If skipkeys is false, then it is a TypeError to attempt
        encoding of keys that are not str, int, long, float or None.  If
        skipkeys is True, such items are simply skipped.

        If ensure_ascii is true, the output is guaranteed to be str
        objects with all incoming unicode characters escaped.  If
        ensure_ascii is false, the output will be unicode object.

        If check_circular is true, then lists, dicts, and custom encoded
        objects will be checked for circular references during encoding to
        prevent an infinite recursion (which would cause an OverflowError).
        Otherwise, no such check takes place.

        If allow_nan is true, then NaN, Infinity, and -Infinity will be
        encoded as such.  This behavior is not JSON specification compliant,
        but is consistent with most JavaScript based encoders and decoders.
        Otherwise, it will be a ValueError to encode such floats.

        If sort_keys is true, then the output of dictionaries will be
        sorted by key; this is useful for regression tests to ensure
        that JSON serializations can be compared on a day-to-day basis.

        If indent is a string, then JSON array elements and object members
        will be pretty-printed with a newline followed by that string repeated
        for each level of nesting. ``None`` (the default) selects the most compact
        representation without any newlines. For backwards compatibility with
        versions of simplejson earlier than 2.1.0, an integer is also accepted
        and is converted to a string with that many spaces.

        If specified, separators should be a (item_separator, key_separator)
        tuple.  The default is (', ', ': ').  To get the most compact JSON
        representation you should specify (',', ':') to eliminate whitespace.

        If specified, default is a function that gets called for objects
        that can't otherwise be serialized.  It should return a JSON encodable
        version of the object or raise a ``TypeError``.

        If encoding is not None, then all input strings will be
        transformed into unicode using that encoding prior to JSON-encoding.
        The default is UTF-8.
        
        If use_decimal is true (not the default), ``decimal.Decimal`` will
        be supported directly by the encoder. For the inverse, decode JSON
        with ``parse_float=decimal.Decimal``.

        """

        self.skipkeys = skipkeys
        self.ensure_ascii = ensure_ascii
        self.check_circular = check_circular
        self.allow_nan = allow_nan
        self.sort_keys = sort_keys
        self.use_decimal = use_decimal
        if isinstance(indent, (int, long)):
            indent = ' ' * indent
        self.indent = indent
        if separators is not None:
            self.item_separator, self.key_separator = separators
        if default is not None:
            self.default = default
        self.encoding = encoding

    def default(self, o):
        """Implement this method in a subclass such that it returns
        a serializable object for ``o``, or calls the base implementation
        (to raise a ``TypeError``).

        For example, to support arbitrary iterators, you could
        implement default like this::

            def default(self, o):
                try:
                    iterable = iter(o)
                except TypeError:
                    pass
                else:
                    return list(iterable)
                return JSONEncoder.default(self, o)

        """
        raise TypeError(repr(o) + " is not JSON serializable")

    def encode(self, o):
        """Return a JSON string representation of a Python data structure.

        >>> from simplejson import JSONEncoder
        >>> JSONEncoder().encode({"foo": ["bar", "baz"]})
        '{"foo": ["bar", "baz"]}'

        """
        # This is for extremely simple cases and benchmarks.
        if isinstance(o, basestring):
            if isinstance(o, str):
                _encoding = self.encoding
                if (_encoding is not None
                        and not (_encoding == 'utf-8')):
                    o = o.decode(_encoding)
            if self.ensure_ascii:
                return encode_basestring_ascii(o)
            else:
                return encode_basestring(o)
        # This doesn't pass the iterator directly to ''.join() because the
        # exceptions aren't as detailed.  The list call should be roughly
        # equivalent to the PySequence_Fast that ''.join() would do.
        chunks = self.iterencode(o, _one_shot=True)
        if not isinstance(chunks, (list, tuple)):
            chunks = list(chunks)
        if self.ensure_ascii:
            return ''.join(chunks)
        else:
            return u''.join(chunks)

    def iterencode(self, o, _one_shot=False):
        """Encode the given object and yield each string
        representation as available.

        For example::

            for chunk in JSONEncoder().iterencode(bigobject):
                mysocket.write(chunk)

        """
        if self.check_circular:
            markers = {}
        else:
            markers = None
        if self.ensure_ascii:
            _encoder = encode_basestring_ascii
        else:
            _encoder = encode_basestring
        if self.encoding != 'utf-8':
            def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):
                if isinstance(o, str):
                    o = o.decode(_encoding)
                return _orig_encoder(o)

        def floatstr(o, allow_nan=self.allow_nan,
                _repr=FLOAT_REPR, _inf=PosInf, _neginf=-PosInf):
            # Check for specials. Note that this type of test is processor
            # and/or platform-specific, so do tests which don't depend on
            # the internals.

            if o != o:
                text = 'NaN'
            elif o == _inf:
                text = 'Infinity'
            elif o == _neginf:
                text = '-Infinity'
            else:
                return _repr(o)

            if not allow_nan:
                raise ValueError(
                    "Out of range float values are not JSON compliant: " +
                    repr(o))

            return text


        key_memo = {}
        if (_one_shot and c_make_encoder is not None
                and not self.indent and not self.sort_keys):
            _iterencode = c_make_encoder(
                markers, self.default, _encoder, self.indent,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, self.allow_nan, key_memo, self.use_decimal)
        else:
            _iterencode = _make_iterencode(
                markers, self.default, _encoder, self.indent, floatstr,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, _one_shot, self.use_decimal)
        try:
            return _iterencode(o, 0)
        finally:
            key_memo.clear()


class JSONEncoderForHTML(JSONEncoder):
    """An encoder that produces JSON safe to embed in HTML.

    To embed JSON content in, say, a script tag on a web page, the
    characters &, < and > should be escaped. They cannot be escaped
    with the usual entities (e.g. &amp;) because they are not expanded
    within <script> tags.
    """

    def encode(self, o):
        # Override JSONEncoder.encode because it has hacks for
        # performance that make things more complicated.
        chunks = self.iterencode(o, True)
        if self.ensure_ascii:
            return ''.join(chunks)
        else:
            return u''.join(chunks)

    def iterencode(self, o, _one_shot=False):
        chunks = super(JSONEncoderForHTML, self).iterencode(o, _one_shot)
        for chunk in chunks:
            chunk = chunk.replace('&', '\\u0026')
            chunk = chunk.replace('<', '\\u003c')
            chunk = chunk.replace('>', '\\u003e')
            yield chunk


def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,
        _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,
        _use_decimal,
        ## HACK: hand-optimized bytecode; turn globals into locals
        False=False,
        True=True,
        ValueError=ValueError,
        basestring=basestring,
        Decimal=Decimal,
        dict=dict,
        float=float,
        id=id,
        int=int,
        isinstance=isinstance,
        list=list,
        long=long,
        str=str,
        tuple=tuple,
    ):

    def _iterencode_list(lst, _current_indent_level):
        if not lst:
            yield '[]'
            return
        if markers is not None:
            markerid = id(lst)
            if markerid in markers:
                raise ValueError("Circular reference detected")
            markers[markerid] = lst
        buf = '['
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + (_indent * _current_indent_level)
            separator = _item_separator + newline_indent
            buf += newline_indent
        else:
            newline_indent = None
            separator = _item_separator
        first = True
        for value in lst:
            if first:
                first = False
            else:
                buf = separator
            if isinstance(value, basestring):
                yield buf + _encoder(value)
            elif value is None:
                yield buf + 'null'
            elif value is True:
                yield buf + 'true'
            elif value is False:
                yield buf + 'false'
            elif isinstance(value, (int, long)):
                yield buf + str(value)
            elif isinstance(value, float):
                yield buf + _floatstr(value)
            elif _use_decimal and isinstance(value, Decimal):
                yield buf + str(value)
            else:
                yield buf
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                for chunk in chunks:
                    yield chunk
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + (_indent * _current_indent_level)
        yield ']'
        if markers is not None:
            del markers[markerid]

    def _iterencode_dict(dct, _current_indent_level):
        if not dct:
            yield '{}'
            return
        if markers is not None:
            markerid = id(dct)
            if markerid in markers:
                raise ValueError("Circular reference detected")
            markers[markerid] = dct
        yield '{'
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + (_indent * _current_indent_level)
            item_separator = _item_separator + newline_indent
            yield newline_indent
        else:
            newline_indent = None
            item_separator = _item_separator
        first = True
        if _sort_keys:
            items = dct.items()
            items.sort(key=lambda kv: kv[0])
        else:
            items = dct.iteritems()
        for key, value in items:
            if isinstance(key, basestring):
                pass
            # JavaScript is weakly typed for these, so it makes sense to
            # also allow them.  Many encoders seem to do something like this.
            elif isinstance(key, float):
                key = _floatstr(key)
            elif key is True:
                key = 'true'
            elif key is False:
                key = 'false'
            elif key is None:
                key = 'null'
            elif isinstance(key, (int, long)):
                key = str(key)
            elif _skipkeys:
                continue
            else:
                raise TypeError("key " + repr(key) + " is not a string")
            if first:
                first = False
            else:
                yield item_separator
            yield _encoder(key)
            yield _key_separator
            if isinstance(value, basestring):
                yield _encoder(value)
            elif value is None:
                yield 'null'
            elif value is True:
                yield 'true'
            elif value is False:
                yield 'false'
            elif isinstance(value, (int, long)):
                yield str(value)
            elif isinstance(value, float):
                yield _floatstr(value)
            elif _use_decimal and isinstance(value, Decimal):
                yield str(value)
            else:
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                for chunk in chunks:
                    yield chunk
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + (_indent * _current_indent_level)
        yield '}'
        if markers is not None:
            del markers[markerid]

    def _iterencode(o, _current_indent_level):
        if isinstance(o, basestring):
            yield _encoder(o)
        elif o is None:
            yield 'null'
        elif o is True:
            yield 'true'
        elif o is False:
            yield 'false'
        elif isinstance(o, (int, long)):
            yield str(o)
        elif isinstance(o, float):
            yield _floatstr(o)
        elif isinstance(o, (list, tuple)):
            for chunk in _iterencode_list(o, _current_indent_level):
                yield chunk
        elif isinstance(o, dict):
            for chunk in _iterencode_dict(o, _current_indent_level):
                yield chunk
        elif _use_decimal and isinstance(o, Decimal):
            yield str(o)
        else:
            if markers is not None:
                markerid = id(o)
                if markerid in markers:
                    raise ValueError("Circular reference detected")
                markers[markerid] = o
            o = _default(o)
            for chunk in _iterencode(o, _current_indent_level):
                yield chunk
            if markers is not None:
                del markers[markerid]

    return _iterencode

########NEW FILE########
__FILENAME__ = ordered_dict
"""Drop-in replacement for collections.OrderedDict by Raymond Hettinger

http://code.activestate.com/recipes/576693/

"""
from UserDict import DictMixin

# Modified from original to support Python 2.4, see
# http://code.google.com/p/simplejson/issues/detail?id=53
try:
    all
except NameError:
    def all(seq):
        for elem in seq:
            if not elem:
                return False
        return True

class OrderedDict(dict, DictMixin):

    def __init__(self, *args, **kwds):
        if len(args) > 1:
            raise TypeError('expected at most 1 arguments, got %d' % len(args))
        try:
            self.__end
        except AttributeError:
            self.clear()
        self.update(*args, **kwds)

    def clear(self):
        self.__end = end = []
        end += [None, end, end]         # sentinel node for doubly linked list
        self.__map = {}                 # key --> [key, prev, next]
        dict.clear(self)

    def __setitem__(self, key, value):
        if key not in self:
            end = self.__end
            curr = end[1]
            curr[2] = end[1] = self.__map[key] = [key, curr, end]
        dict.__setitem__(self, key, value)

    def __delitem__(self, key):
        dict.__delitem__(self, key)
        key, prev, next = self.__map.pop(key)
        prev[2] = next
        next[1] = prev

    def __iter__(self):
        end = self.__end
        curr = end[2]
        while curr is not end:
            yield curr[0]
            curr = curr[2]

    def __reversed__(self):
        end = self.__end
        curr = end[1]
        while curr is not end:
            yield curr[0]
            curr = curr[1]

    def popitem(self, last=True):
        if not self:
            raise KeyError('dictionary is empty')
        # Modified from original to support Python 2.4, see
        # http://code.google.com/p/simplejson/issues/detail?id=53
        if last:
            key = reversed(self).next()
        else:
            key = iter(self).next()
        value = self.pop(key)
        return key, value

    def __reduce__(self):
        items = [[k, self[k]] for k in self]
        tmp = self.__map, self.__end
        del self.__map, self.__end
        inst_dict = vars(self).copy()
        self.__map, self.__end = tmp
        if inst_dict:
            return (self.__class__, (items,), inst_dict)
        return self.__class__, (items,)

    def keys(self):
        return list(self)

    setdefault = DictMixin.setdefault
    update = DictMixin.update
    pop = DictMixin.pop
    values = DictMixin.values
    items = DictMixin.items
    iterkeys = DictMixin.iterkeys
    itervalues = DictMixin.itervalues
    iteritems = DictMixin.iteritems

    def __repr__(self):
        if not self:
            return '%s()' % (self.__class__.__name__,)
        return '%s(%r)' % (self.__class__.__name__, self.items())

    def copy(self):
        return self.__class__(self)

    @classmethod
    def fromkeys(cls, iterable, value=None):
        d = cls()
        for key in iterable:
            d[key] = value
        return d

    def __eq__(self, other):
        if isinstance(other, OrderedDict):
            return len(self)==len(other) and \
                   all(p==q for p, q in  zip(self.items(), other.items()))
        return dict.__eq__(self, other)

    def __ne__(self, other):
        return not self == other

########NEW FILE########
__FILENAME__ = scanner
"""JSON token scanner
"""
import re
def _import_c_make_scanner():
    try:
        from simplejson._speedups import make_scanner
        return make_scanner
    except ImportError:
        return None
c_make_scanner = _import_c_make_scanner()

__all__ = ['make_scanner']

NUMBER_RE = re.compile(
    r'(-?(?:0|[1-9]\d*))(\.\d+)?([eE][-+]?\d+)?',
    (re.VERBOSE | re.MULTILINE | re.DOTALL))

def py_make_scanner(context):
    parse_object = context.parse_object
    parse_array = context.parse_array
    parse_string = context.parse_string
    match_number = NUMBER_RE.match
    encoding = context.encoding
    strict = context.strict
    parse_float = context.parse_float
    parse_int = context.parse_int
    parse_constant = context.parse_constant
    object_hook = context.object_hook
    object_pairs_hook = context.object_pairs_hook
    memo = context.memo

    def _scan_once(string, idx):
        try:
            nextchar = string[idx]
        except IndexError:
            raise StopIteration

        if nextchar == '"':
            return parse_string(string, idx + 1, encoding, strict)
        elif nextchar == '{':
            return parse_object((string, idx + 1), encoding, strict,
                _scan_once, object_hook, object_pairs_hook, memo)
        elif nextchar == '[':
            return parse_array((string, idx + 1), _scan_once)
        elif nextchar == 'n' and string[idx:idx + 4] == 'null':
            return None, idx + 4
        elif nextchar == 't' and string[idx:idx + 4] == 'true':
            return True, idx + 4
        elif nextchar == 'f' and string[idx:idx + 5] == 'false':
            return False, idx + 5

        m = match_number(string, idx)
        if m is not None:
            integer, frac, exp = m.groups()
            if frac or exp:
                res = parse_float(integer + (frac or '') + (exp or ''))
            else:
                res = parse_int(integer)
            return res, m.end()
        elif nextchar == 'N' and string[idx:idx + 3] == 'NaN':
            return parse_constant('NaN'), idx + 3
        elif nextchar == 'I' and string[idx:idx + 8] == 'Infinity':
            return parse_constant('Infinity'), idx + 8
        elif nextchar == '-' and string[idx:idx + 9] == '-Infinity':
            return parse_constant('-Infinity'), idx + 9
        else:
            raise StopIteration

    def scan_once(string, idx):
        try:
            return _scan_once(string, idx)
        finally:
            memo.clear()

    return scan_once

make_scanner = c_make_scanner or py_make_scanner

########NEW FILE########
__FILENAME__ = tool
r"""Command-line tool to validate and pretty-print JSON

Usage::

    $ echo '{"json":"obj"}' | python -m simplejson.tool
    {
        "json": "obj"
    }
    $ echo '{ 1.2:3.4}' | python -m simplejson.tool
    Expecting property name: line 1 column 2 (char 2)

"""
import sys
import simplejson as json

def main():
    if len(sys.argv) == 1:
        infile = sys.stdin
        outfile = sys.stdout
    elif len(sys.argv) == 2:
        infile = open(sys.argv[1], 'rb')
        outfile = sys.stdout
    elif len(sys.argv) == 3:
        infile = open(sys.argv[1], 'rb')
        outfile = open(sys.argv[2], 'wb')
    else:
        raise SystemExit(sys.argv[0] + " [infile [outfile]]")
    try:
        obj = json.load(infile,
                        object_pairs_hook=json.OrderedDict,
                        use_decimal=True)
    except ValueError, e:
        raise SystemExit(e)
    json.dump(obj, outfile, sort_keys=True, indent='    ', use_decimal=True)
    outfile.write('\n')


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = six
"""Utilities for writing code that runs on Python 2 and 3"""

import sys
import types

__author__ = "Benjamin Peterson <benjamin@python.org>"
__version__ = "1.0.0"


# True if we are running on Python 3.
PY3 = sys.version_info[0] == 3

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    MAXSIZE = sys.maxint


def _add_doc(func, doc):
    """Add documentation to a function."""
    func.__doc__ = doc


def _import_module(name):
    """Import module, returning the module after the last dot."""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)
        # This is a bit ugly, but it avoids running this again.
        delattr(tp, self.name)
        return result


class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)



class _MovedItems(types.ModuleType):
    """Lazy loading of moved objects"""


_moved_attributes = [
    MovedAttribute("cStringIO", "cStringIO", "io", "StringIO"),
    MovedAttribute("reload_module", "__builtin__", "imp", "reload"),
    MovedAttribute("reduce", "__builtin__", "functools"),
    MovedAttribute("StringIO", "StringIO", "io"),
    MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),

    MovedModule("builtins", "__builtin__"),
    MovedModule("configparser", "ConfigParser"),
    MovedModule("copyreg", "copy_reg"),
    MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
    MovedModule("http_cookies", "Cookie", "http.cookies"),
    MovedModule("html_entities", "htmlentitydefs", "html.entities"),
    MovedModule("html_parser", "HTMLParser", "html.parser"),
    MovedModule("http_client", "httplib", "http.client"),
    MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
    MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
    MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
    MovedModule("cPickle", "cPickle", "pickle"),
    MovedModule("queue", "Queue"),
    MovedModule("reprlib", "repr"),
    MovedModule("socketserver", "SocketServer"),
    MovedModule("tkinter", "Tkinter"),
    MovedModule("tkinter_dialog", "Dialog", "tkinter.dialog"),
    MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
    MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
    MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
    MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
    MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
    MovedModule("tkinter_colorchooser", "tkColorChooser",
                "tkinter.colorchooser"),
    MovedModule("tkinter_commondialog", "tkCommonDialog",
                "tkinter.commondialog"),
    MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_font", "tkFont", "tkinter.font"),
    MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog",
                "tkinter.simpledialog"),
    MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
    MovedModule("winreg", "_winreg"),
]
for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
del attr

moves = sys.modules["six.moves"] = _MovedItems("moves")


def add_move(move):
    """Add an item to six.moves."""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """Remove item from six.moves."""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError("no such move, %r" % (name,))


if PY3:
    _meth_func = "__func__"
    _meth_self = "__self__"

    _func_code = "__code__"
    _func_defaults = "__defaults__"
else:
    _meth_func = "im_func"
    _meth_self = "im_self"

    _func_code = "func_code"
    _func_defaults = "func_defaults"


if PY3:
    def get_unbound_function(unbound):
        return unbound


    advance_iterator = next

    def callable(obj):
        return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)
else:
    def get_unbound_function(unbound):
        return unbound.im_func


    def advance_iterator(it):
        return it.next()

    callable = callable
_add_doc(get_unbound_function,
         """Get the function out of a possibly unbound function""")


def get_method_function(meth):
    """Get the underlying function of a bound method."""
    return getattr(meth, _meth_func)


def get_method_self(meth):
    """Get the self of a bound method."""
    return getattr(meth, _meth_self)


def get_function_code(func):
    """Get code object of a function."""
    return getattr(func, _func_code)


def get_function_defaults(func):
    """Get defaults of a function."""
    return getattr(func, _func_defaults)


if PY3:
    def b(s):
        return s.encode("latin-1")
    def u(s):
        return s
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
else:
    def b(s):
        return s
    def u(s):
        return unicode(s, "unicode_escape")
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
_add_doc(b, """Byte literal""")
_add_doc(u, """Text literal""")


if PY3:
    exec_ = eval("exec")


    def reraise(tp, value, tb=None):
        if value.__traceback__ is not tb:
            raise value.with_traceback(tb)
        raise value


    print_ = eval("print")


    def with_metaclass(meta, base=object):
        ns = dict(base=base, meta=meta)
        exec_("""class NewBase(base, metaclass=meta):
    pass""", ns)
        return ns["NewBase"]


else:
    def exec_(code, globs=None, locs=None):
        """Execute code in a namespace."""
        if globs is None:
            frame = sys._getframe(1)
            globs = frame.f_globals
            if locs is None:
                locs = frame.f_locals
            del frame
        elif locs is None:
            locs = globs
        exec("""exec code in globs, locs""")


    exec_("""def reraise(tp, value, tb=None):
    raise tp, value, tb
""")


    def print_(*args, **kwargs):
        """The new-style print function."""
        fp = kwargs.pop("file", sys.stdout)
        if fp is None:
            return
        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop("sep", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError("sep must be None or a string")
        end = kwargs.pop("end", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError("end must be None or a string")
        if kwargs:
            raise TypeError("invalid keyword arguments to print()")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode("\n")
            space = unicode(" ")
        else:
            newline = "\n"
            space = " "
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)


    def with_metaclass(meta, base=object):
        class NewBase(base):
            __metaclass__ = meta
        return NewBase


_add_doc(reraise, """Reraise an exception.""")
_add_doc(with_metaclass, """Create a base class with a metaclass""")

########NEW FILE########
__FILENAME__ = example
import os
import sys

from yaku.scheduler \
    import \
        run_tasks
from yaku.context \
    import \
        get_bld, get_cfg
import yaku.tools

def configure(conf):
    ctx.load_tool("python_2to3")

def build(ctx):
    builder = ctx.builders["python_2to3"]
    files = []
    for r, ds, fs in os.walk("foo"):
        files.extend([os.path.join(r, f) for f in fs])
    builder.convert("", files)

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.setup_tools()
    ctx.store()

    ctx = get_bld()
    build(ctx)
    try:
        run_tasks(ctx)
    finally:
        ctx.store()

########NEW FILE########
__FILENAME__ = example
from yaku.scheduler \
    import \
        run_tasks
from yaku.context \
    import \
        get_bld, get_cfg

def configure(ctx):
    ctx.use_tools(["ctasks", "cxxtasks"])
    ctx.env.append("DEFINES", "_FOO")

def build(ctx):
    builder = ctx.builders["cxxtasks"]
    builder.program("foo", ["src/main.cxx"])

    builder = ctx.builders["ctasks"]
    builder.static_library("bar", ["src/bar.c"])

if __name__ == "__main__":
    build_path = "BBUILD"
    ctx = get_cfg(build_path=build_path)
    configure(ctx)
    ctx.store()

    ctx = get_bld(build_path=build_path)
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = try_example
from yaku.scheduler \
    import \
        run_tasks
from yaku.context \
    import \
        get_bld, get_cfg
from yaku.conftests \
    import \
        check_func, check_header, check_compiler, check_cpp_symbol

def configure(ctx):
    ctx.use_tools(["ctasks", "cxxtasks"])
    ctx.env.append("DEFINES", "_FOO")

    cc = ctx.builders["ctasks"]
    assert cc.try_compile("foo", "int main() {}")
    assert not cc.try_compile("foo", "intt main() {}")

    assert cc.try_program("foo", "int main() {}")
    assert not cc.try_program("foo", "intt main() {}")

    assert cc.try_static_library("foo", "int foo() {}")
    assert not cc.try_static_library("foo", "intt foo() {}")

    assert check_func(ctx, "malloc")
    assert not check_func(ctx, "mmalloc")

    assert check_header(ctx, "stdio.h")
    assert not check_header(ctx, "stdioo.h")

    assert check_compiler(ctx)
    assert check_cpp_symbol(ctx, "NULL", ["stdio.h"])
    assert not check_cpp_symbol(ctx, "VERY_UNLIKELY_SYMOBOL_@aqwhn")

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = build
import sys
import os

from yaku.task_manager \
    import \
        set_extension_hook, get_extension_hook
from yaku.scheduler \
    import \
        run_tasks
from yaku.context \
    import \
        get_bld, get_cfg
import yaku.node
import yaku.errors

def configure(ctx):
    ctx.use_tools(["ctasks", "pyext", "template"])
    try:
        ctx.use_tools(["cython"])
    except yaku.errors.ToolNotFound:
        raise RuntimeError("Cython not found - please install Cython!")
    ctx.env.update({"SUBST_DICT": {"VERSION": "0.0.2"}})

def build(ctx):
    pyext = ctx.builders["pyext"]

    old_hook = get_extension_hook(".in")
    def foo(self, node):
        recursive_suffixes = [".c", ".cxx", ".pyx"]
        out = node.change_ext("")
        # XXX: is it really safe to items to a list one recurses on ?
        if out.suffix() in recursive_suffixes:
            self.sources.append(out)
        return old_hook(self, node)
    set_extension_hook(".in", foo)

    pyext.extension("hello", ["hello.pyx.in"])

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = conf_example
import os

from yaku.utils \
    import \
        ensure_dir
from yaku.context \
    import \
        get_cfg
from yaku.conftests \
    import \
        check_compiler, check_header, check_func, check_lib, check_type, generate_config_h

def configure(conf):
    conf.use_tools(["ctasks"])

    log_filename = os.path.join("build", "config.log")
    ensure_dir(log_filename)

    conf.log = open(log_filename, "w")
    try:
        # TODO
        #  - support for env update
        #  - config header support
        #  - confdefs header support
        check_compiler(conf)
        check_header(conf, "stdio.h")
        check_header(conf, "stdio")
        check_type(conf, "char")
        #check_type(conf, "complex")
        check_type(conf, "complex", headers=["complex.h"])
        if check_func(conf, "exp"):
            mlib = []
        else:
            if not check_lib(conf, "m", "exp"):
                raise ValueError("What is mlib ?")
            else:
                mlib = ["m"]
        check_lib(conf, mlib, "exp")
        #check_lib(conf, lib="mm")
        check_func(conf, "floor", libs=mlib)
        check_func(conf, "floor")

        generate_config_h(conf.conf_results, "build/conf/config.h")
    finally:
        conf.log.close()

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = example4
import os
import sys

from yaku.context \
    import \
        get_bld, get_cfg
from yaku.scheduler \
    import \
        run_tasks

def configure(ctx):
    # The tool ctask works as follows:
    # - When a tool is *loaded* (load_tool), get its configure function (dummy
    # is setup if no configure is found)
    # - When a tool is *used* (use_tool), run its configure function
    # - given a list of candidates, run the detect function for every candidate
    # until detect returns True
    # - if one candidate found, record its name and run its setup function
    # - if none found, fails
    tools = ctx.use_tools(["ctasks"])

def build(ctx):
    builder = ctx.builders["ctasks"]
    builder.program("main", [os.path.join("src", "main.c")])

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = example5
import sys
import os

from yaku.context \
    import \
        get_bld, get_cfg
import yaku.errors

from pprint import pprint

def get_clang_env(ctx):
    from distutils.sysconfig import get_config_var, get_python_inc
    env = {
            "CC": ["clang"],
            "CPPPATH": [get_python_inc()],
            "BASE_CFLAGS": ["-fno-strict-aliasing"],
            "OPT": ["-O2", "-Wall"],
            "SHARED": ["-fPIC"],
            "SHLINK": ["clang"],
            "LIBDIR": [],
            "LIBS": [],
            "SO": ".so",
            "LDFLAGS": []
            }

    if sys.platform == "darwin":
        env["LDFLAGS"].extend(["-bundle", "-undefined", "dynamic_lookup"])
    else:
        env["LDFLAGS"].append("-shared")
    return env

def pyext_configure(ctx, compiler_type="default"):
    # How we do it
    # - query distutils C compiler ($CC variable)
    # - try to determine yaku tool name from $CC
    # - get options from sysconfig
    # - apply necessary variables from yaku tool to $PYEXT_
    # "namespace"

    from yaku.sysconfig import detect_distutils_cc
    from yaku.tools.pyext import detect_cc_type, setup_pyext_env

    if True:
        dist_env = setup_pyext_env(ctx)
        ctx.env.update(dist_env)
    else:
        compiler_type = "clang"
        dist_env = get_clang_env(ctx)
        for name, value in dist_env.items():
            ctx.env["PYEXT_%s" % name] = value
        ctx.env["PYEXT_FMT"] = "%%s%s" % dist_env["SO"]
        ctx.env["PYEXT_CFLAGS"] = ctx.env["PYEXT_BASE_CFLAGS"] + \
                ctx.env["PYEXT_OPT"] + \
                ctx.env["PYEXT_SHARED"]
        ctx.env["PYEXT_SHLINKFLAGS"] = dist_env["LDFLAGS"]

    if compiler_type == "default":
        cc = detect_distutils_cc(ctx)
        cc_type = detect_cc_type(ctx, cc)
    else:
        cc_type = compiler_type

    old_env = ctx.env
    ctx.env = {}
    sys.path.insert(0, os.path.dirname(yaku.tools.__file__))
    try:
        try:
            mod = __import__(cc_type)
            mod.setup(ctx)
        except ImportError:
            raise RuntimeError("No tool %s is available (import failed)" \
                            % cc_type)

        # XXX: this is ugly - find a way to have tool-specific env...
        cc_env = ctx.env
        ctx.env = old_env
        TRANSFER = ["CPPPATH_FMT", "LIBDIR_FMT", "LIB_FMT", "CC_OBJECT_FMT", "CC_TGT_F", "CC_SRC_F", "LINK_TGT_F", "LINK_SRC_F"]
        for k in TRANSFER:
            ctx.env["PYEXT_%s" % k] = cc_env[k]
    finally:
        sys.path.pop(0)

def configure(ctx):
    ctx.load_tool("ctasks")
    ctx.load_tool("pyext")
    ctx._tool_modules["pyext"].configure = pyext_configure

def build(ctx):
    ctx.builders["pyext"].extension("_bar", 
            [os.path.join("src", "hellomodule.c")])
    from yaku.scheduler import SerialRunner
    from yaku.task_manager import TaskManager
    runner = SerialRunner(ctx, TaskManager(ctx.tasks))
    runner.start()
    runner.run()

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.setup_tools()
    ctx.store()

    ctx = get_bld()
    build(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = file_hook
import yaku.task_manager

from yaku.task_manager import get_extension_hook, set_file_hook
from yaku.context import get_cfg, get_bld
from yaku.scheduler import run_tasks

def file_hook(self, node):
    print("Yo mama", node.name)
    return get_extension_hook(".c")(self, node)

def configure(ctx):
    ctx.use_tools(["ctasks"])
    set_file_hook(ctx, "src/fubar.c", file_hook)

def build(ctx):
    builder = ctx.builders["ctasks"].program
    builder("src/main", sources=["src/main.c", "src/fubar.c"])

if __name__ == "__main__":
    cfg = get_cfg()
    configure(cfg)
    cfg.store()

    bld = get_bld()
    build(bld)
    run_tasks(bld)
    bld.store()

########NEW FILE########
__FILENAME__ = conf_fortran
import sys

from yaku.scheduler \
    import \
        run_tasks
from yaku.context \
    import \
        get_bld, get_cfg

from yaku.conftests.fconftests \
    import \
        check_fcompiler, check_fortran_verbose_flag, \
        check_fortran_runtime_flags, check_fortran_dummy_main, \
        check_fortran_mangling

def configure(ctx):
    ctx.use_tools(["ctasks", "cxxtasks"])
    ctx.load_tool("fortran")
    ctx.builders["fortran"].configure(candidates=["gfortran"])

    check_fcompiler(ctx)
    check_fortran_verbose_flag(ctx)
    check_fortran_runtime_flags(ctx)
    check_fortran_dummy_main(ctx)
    check_fortran_mangling(ctx)

def build(ctx):
    builder = ctx.builders["fortran"]
    builder.program("fbar", ["src/bar.f"])

    builder = ctx.builders["ctasks"]
    builder.program("cbar", ["src/bar.c"])

    builder = ctx.builders["cxxtasks"]
    builder.program("cxxbar", ["src/bar.cxx"])

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = fortran
from yaku.scheduler \
    import \
        run_tasks
from yaku.context \
    import \
        get_bld, get_cfg
import yaku.node
import yaku.errors

def configure(ctx):
    ctx.use_tools(["fortran", "ctasks"])
    assert ctx.builders["fortran"].try_compile("foo", """\
       program foo
       end
""")

    assert not ctx.builders["fortran"].try_compile("foo", """\
       pprogram foo
       end
""")

    assert ctx.builders["fortran"].try_program("foo", """\
       program foo
       end
""")

    assert not ctx.builders["fortran"].try_program("foo", """\
       pprogram foo
       end
""")

def build(ctx):
    builder = ctx.builders["fortran"]
    builder.program("fbar", ["src/bar.f"])

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = node_example
import os

from yaku.node import Node as _Node
from yaku.context import get_cfg, get_bld
from yaku.scheduler import run_tasks

class FakeContext(object): pass

if __name__ == "__main__":
    blddir = os.path.join("build")
    if not os.path.exists(blddir):
        os.makedirs(blddir)

    #ctx = FakeContext()
    class Node(_Node):
        ctx = FakeContext()
    root = Node("", None)

    start_dir = os.path.abspath(os.getcwd())
    start = root.find_dir(start_dir)
    bldnode = start.find_dir(blddir)
    Node.ctx.srcnode = start
    Node.ctx.bldnode = bldnode

    ctx = get_cfg()
    for t in ["cython", "pyext", "ctasks"]:
        ctx.load_tool(t)
    ctx.setup_tools()
    ctx.store()

    bld = get_bld()
    bld.src_root = start
    bld.bld_root = bldnode
    pyext = bld.builders["pyext"]
    pyext.extension("_foo", ["src/bar.pyx"])
    run_tasks(bld)
    bld.store()

########NEW FILE########
__FILENAME__ = numpytest
import os
import distutils.sysconfig

from yaku.utils \
    import \
        ensure_dir
from yaku.context \
    import \
        get_cfg
from yaku.conftests \
    import \
        check_compiler, check_header, check_func, check_lib, check_type, \
        generate_config_h, check_type_size, define, check_funcs_at_once

def configure(conf):
    conf.use_tools(["ctasks"])

    log_filename = os.path.join("build", "config.log")
    ensure_dir(log_filename)

    conf.log = open(log_filename, "w")
    try:
        check_compiler(conf)
        conf.env["CPPPATH"].append(distutils.sysconfig.get_python_inc())
        if not check_header(conf, "Python.h"):
            raise RuntimeError("Python header not found !")
        check_header(conf, "math.h")
        for mlibs in [[], ["m"]]:
            if check_func(conf, "floor", libs=mlibs):
                break
        for tp in ("short", "int", "long"):
            check_type_size(conf, tp)
        for tp in ("float", "double", "long double"):
            check_type_size(conf, tp)
        check_type(conf, "Py_intptr_t", headers=["Python.h"])
        define(conf, "NPY_NO_SMP")

        mfuncs = ('expl', 'expf', 'log1p', 'expm1', 'asinh', 'atanhf',
                'atanhl', 'rint', 'trunc')
        check_funcs_at_once(conf, mfuncs)
        generate_config_h(conf.conf_results, "build/conf/config.h")
    finally:
        conf.log.close()

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = override_ext
import sys
import os
import copy

from yaku.context \
    import \
        get_bld, get_cfg
from yaku.scheduler \
    import \
        run_tasks
from yaku.task_manager \
    import \
        get_extension_hook, set_file_hook, set_extension_hook, \
        wrap_extension_hook

def configure(ctx):
    ctx.use_tools(["ctasks"])

# A hook wrapper takes the existing hook as an argument, and returns
# the new hook (callable).
def custom_c_hooker(c_hook):
    # Dummy hook printing the node name
    def hook(ctx, node):
        print("Compiling file: %s, with flags %s" % \
                (node.name, " ".join(ctx.env["CFLAGS"])))
        return c_hook(ctx, node)
    return hook

def build(ctx):
    program = ctx.builders["ctasks"].program
    c_hook = wrap_extension_hook(".c", custom_c_hooker)
    try:
        program("main1", sources=["src/main.c"])
    finally:
        set_extension_hook(".c", c_hook)
    program("main2", sources=["src/main2.c"])

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = pyext_example
import os
import sys

from yaku.scheduler \
    import \
        run_tasks
from yaku.context \
    import \
        get_bld, get_cfg

from yaku.conftests \
    import \
        check_compiler, check_header
from yaku.conftests.pytests \
    import \
        check_module

def configure(ctx):
    ctx.use_tools(["pyext", "ctasks"], ["tools"])

    check_compiler(ctx)
    check_header(ctx, "stdio.h")
    check_module(ctx, "sys")
    check_module(ctx, "ssys")

def build(ctx):
    builder = ctx.builders["pyext"]
    builder.extension("_bar", ["src/hellomodule.c"])

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = example
import subprocess

from yaku.context \
    import \
        get_bld, get_cfg
from yaku.scheduler \
    import \
        run_tasks

def git_revision():
    p = subprocess.Popen("git rev-parse --short HEAD", stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE, shell=True)
    o, _ = p.communicate()
    if p.returncode:
        raise ValueError("Failed to execute git rev-parse")
    else:
        return o

def configure(ctx):
    tools = ctx.use_tools(["template"])

def build(ctx):
    builder = ctx.builders["template"]
    vars = {}
    vars["HEAD"] = git_revision()
    builder.render(["foo.ini.in"], vars)

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = task_precedence
import sys
import os

from yaku.context \
    import \
        get_bld, get_cfg
from yaku.scheduler \
    import \
        run_tasks
from yaku.task \
    import \
        task_factory

import yaku.tools
import yaku.errors

from yaku.pprint import pprint

# Hack to track we run all copy tasks before any convert one
__RUN_CONVERT = False

def copy_func(self):
    if __RUN_CONVERT:
        raise AssertionError("precendence test failed")
    source, target = self.inputs[0], self.outputs[0]
    pprint('BLUE', "%-16s%s" % (self.name.upper(), self.inputs[0].srcpath()))
    target.write(source.read())

def convert_func(self):
    global __RUN_CONVERT
    __RUN_CONVERT = True
    source, target = self.inputs[0], self.outputs[0]
    pprint('BLUE', "%-16s%s" % (self.name.upper(), self.inputs[0].srcpath()))
    target.write("")

class DummyBuilder(yaku.tools.Builder):
    def __init__(self, ctx):
        super(DummyBuilder, self).__init__(ctx)

    def build(self, sources, env=None):
        sources = [self.ctx.src_root.find_resource(s) for s in sources]
        #task_gen = TaskGen("dummy", self.ctx, sources, name)
        env = yaku.tools._merge_env(self.env, env)

        copy_tf = task_factory("convert_copy")
        convert_tf = task_factory("convert_do")
        convert_tf.before.append(copy_tf.__name__)

        py3k_tmp = self.ctx.bld_root.declare("_py3k")
        tasks = []
        for source in sources:
            target = py3k_tmp.declare(source.srcpath())
            task = copy_tf([target], [source])
            task.func = copy_func
            task.env_vars = {}
            tasks.append(task)

            source = target
            target = self.ctx.bld_root.declare(source.srcpath())
            task = convert_tf([target], [source])
            task.func = convert_func
            task.env_vars = {}
            tasks.append(task)

        for t in tasks:
            t.env = env
        self.ctx.tasks.extend(tasks)

        return []

def configure(ctx):
    pass
    #ctx.builders["dummy"] = DummyBuilder(ctx)

def build(ctx):
    blder = DummyBuilder(ctx)
    files = []
    for r, ds, fs in os.walk("."):
        for f in fs:
            if f.endswith(".py"):
                files.append(os.path.join(r, f))
    blder.build(files)

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.setup_tools()
    ctx.store()

    ctx = get_bld()
    build(ctx)
    tasks = ctx.tasks
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = test_env
import sys
import os
import copy

from yaku.context \
    import \
        get_bld, get_cfg
from yaku.scheduler \
    import \
        run_tasks

def configure(ctx):
    ctx.use_tools(["ctasks"])

def build(ctx):
    # To *override* options, you should clone a builder + replacing
    # options there
    builder = ctx.builders["ctasks"].clone()
    builder.env["CFLAGS"] = ["-g", "-DNDEBUG"]
    builder.program("main", sources=["src/main.c"], env={"CFLAGS": ["-O2"]})
    builder.program("main2", sources=["src/main2.c"])

    # env argument to methods *add* option - note that main3.c is not
    # built with -g nor -DNDEBUG
    program = ctx.builders["ctasks"].program
    program("main2", sources=["src/main3.c"], env={"CFLAGS": ["-Os"]})

if __name__ == "__main__":
    ctx = get_cfg()
    configure(ctx)
    ctx.store()

    ctx = get_bld()
    build(ctx)
    run_tasks(ctx)
    ctx.store()

########NEW FILE########
__FILENAME__ = py3tool
#!/usr/bin/env python3
# -*- python -*-
"""
%prog SUBMODULE...

Hack to pipe submodules of Numpy through 2to3 and build them in-place
one-by-one.

Example usage:

    python3 tools/py3tool.py testing distutils core

This will copy files to _py3k/numpy, add a dummy __init__.py and
version.py on the top level, and copy and 2to3 the files of the three
submodules.

When running py3tool again, only changed files are re-processed, which
makes the test-bugfix cycle faster.

"""
from optparse import OptionParser
import shutil
import os
import sys
import re
import subprocess
import fnmatch

if os.environ.get('USE_2TO3CACHE'):
    import lib2to3cache

BASE = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
TEMP = os.path.normpath(os.path.join(BASE, '_py3k'))

SCRIPT_2TO3 = os.path.join(BASE, 'tools', '2to3.py')

EXTRA_2TO3_FLAGS = {
    '*/setup.py': '-x import',
    'numpy/core/code_generators/generate_umath.py': '-x import',
    'numpy/core/code_generators/generate_numpy_api.py': '-x import',
    'numpy/core/code_generators/generate_ufunc_api.py': '-x import',
    'numpy/core/defchararray.py': '-x unicode',
    'numpy/compat/py3k.py': '-x unicode',
    'numpy/ma/timer_comparison.py': 'skip',
    'numpy/distutils/system_info.py': '-x reduce',
    'numpy/f2py/auxfuncs.py': '-x reduce',
    'numpy/lib/arrayterator.py': '-x reduce',
    'numpy/lib/tests/test_arrayterator.py': '-x reduce',
    'numpy/ma/core.py': '-x reduce',
    'numpy/ma/tests/test_core.py': '-x reduce',
    'numpy/ma/tests/test_old_ma.py': '-x reduce',
    'numpy/ma/timer_comparison.py': '-x reduce',
    'numpy/oldnumeric/ma.py': '-x reduce',
}

def main():
    p = OptionParser(usage=__doc__.strip())
    p.add_option("--clean", "-c", action="store_true",
                 help="clean source directory")
    options, args = p.parse_args()

    if not args:
        p.error('no submodules given')
    else:
        dirs = ['numpy/%s' % x for x in map(os.path.basename, args)]

    # Prepare
    if not os.path.isdir(TEMP):
        os.makedirs(TEMP)

    # Set up dummy files (for building only submodules)
    dummy_files = {
        '__init__.py': 'from numpy.version import version as __version__',
        'version.py': 'version = "1.4.0.dev"'
    }

    for fn, content in dummy_files.items():
        fn = os.path.join(TEMP, 'numpy', fn)
        if not os.path.isfile(fn):
            try:
                os.makedirs(os.path.dirname(fn))
            except OSError:
                pass
            f = open(fn, 'wb+')
            f.write(content.encode('ascii'))
            f.close()

    # Environment
    pp = [os.path.abspath(TEMP)]
    def getenv():
        env = dict(os.environ)
        env.update({'PYTHONPATH': ':'.join(pp)})
        return env

    # Copy
    for d in dirs:
        src = os.path.join(BASE, d)
        dst = os.path.join(TEMP, d)

        # Run 2to3
        sync_2to3(dst=dst,
                  src=src,
                  patchfile=os.path.join(TEMP, os.path.basename(d) + '.patch'),
                  clean=options.clean)

        # Run setup.py, falling back to Pdb post-mortem on exceptions
        setup_py = os.path.join(dst, 'setup.py')
        if os.path.isfile(setup_py):
            code = """\
import pdb, sys, traceback
p = pdb.Pdb()
try:
    import __main__
    __main__.__dict__.update({
        "__name__": "__main__", "__file__": "setup.py",
        "__builtins__": __builtins__})
    fp = open("setup.py", "rb")
    try:
        exec(compile(fp.read(), "setup.py", 'exec'))
    finally:
        fp.close()
except SystemExit:
    raise
except:
    traceback.print_exc()
    t = sys.exc_info()[2]
    p.interaction(None, t)
"""
            ret = subprocess.call([sys.executable, '-c', code,
                                   'build_ext', '-i'],
                                  cwd=dst,
                                  env=getenv())
            if ret != 0:
                raise RuntimeError("Build failed.")

        # Run nosetests
        subprocess.call(['nosetests3', '-v', d], cwd=TEMP)

def custom_mangling(filename):
    import_mangling = [
        os.path.join('core', '__init__.py'),
        os.path.join('core', 'numeric.py'),
        os.path.join('core', '_internal.py'),
        os.path.join('core', 'arrayprint.py'),
        os.path.join('core', 'fromnumeric.py'),
        os.path.join('numpy', '__init__.py'),
        os.path.join('lib', 'npyio.py'),
        os.path.join('lib', 'function_base.py'),
        os.path.join('fft', 'fftpack.py'),
        os.path.join('random', '__init__.py'),
    ]

    if any(filename.endswith(x) for x in import_mangling):
        f = open(filename, 'r')
        text = f.read()
        f.close()
        for mod in ['multiarray', 'scalarmath', 'umath', '_sort',
                    '_compiled_base', 'core', 'lib', 'testing', 'fft',
                    'polynomial', 'random', 'ma', 'linalg', 'compat',
                    'mtrand', '_dotblas']:
            text = re.sub(r'^(\s*)import %s' % mod,
                          r'\1from . import %s' % mod,
                          text, flags=re.M)
            text = re.sub(r'^(\s*)from %s import' % mod,
                          r'\1from .%s import' % mod,
                          text, flags=re.M)
        text = text.replace('from matrixlib', 'from .matrixlib')
        f = open(filename, 'w')
        f.write(text)
        f.close()

def walk_sync(dir1, dir2, _seen=None):
    if _seen is None:
        seen = {}
    else:
        seen = _seen

    if not dir1.endswith(os.path.sep):
        dir1 = dir1 + os.path.sep

    # Walk through stuff (which we haven't yet gone through) in dir1
    for root, dirs, files in os.walk(dir1):
        sub = root[len(dir1):]
        if sub in seen:
            dirs = [x for x in dirs if x not in seen[sub][0]]
            files = [x for x in files if x not in seen[sub][1]]
            seen[sub][0].extend(dirs)
            seen[sub][1].extend(files)
        else:
            seen[sub] = (dirs, files)
        if not dirs and not files:
            continue
        yield os.path.join(dir1, sub), os.path.join(dir2, sub), dirs, files

    if _seen is None:
        # Walk through stuff (which we haven't yet gone through) in dir2
        for root2, root1, dirs, files in walk_sync(dir2, dir1, _seen=seen):
            yield root1, root2, dirs, files

def sync_2to3(src, dst, patchfile=None, clean=False):
    import lib2to3.main
    from io import StringIO

    to_convert = []

    for src_dir, dst_dir, dirs, files in walk_sync(src, dst):
        for fn in dirs + files:
            src_fn = os.path.join(src_dir, fn)
            dst_fn = os.path.join(dst_dir, fn)

            # skip temporary etc. files
            if fn.startswith('.#') or fn.endswith('~'):
                continue

            # remove non-existing
            if os.path.exists(dst_fn) and not os.path.exists(src_fn):
                if clean:
                    if os.path.isdir(dst_fn):
                        shutil.rmtree(dst_fn)
                    else:
                        os.unlink(dst_fn)
                continue

            # make directories
            if os.path.isdir(src_fn):
                if not os.path.isdir(dst_fn):
                    os.makedirs(dst_fn)
                continue

            dst_dir = os.path.dirname(dst_fn)
            if os.path.isfile(dst_fn) and not os.path.isdir(dst_dir):
                os.makedirs(dst_dir)

            # don't replace up-to-date files
            try:
                if os.path.isfile(dst_fn) and \
                       os.stat(dst_fn).st_mtime >= os.stat(src_fn).st_mtime:
                    continue
            except OSError:
                pass

            # copy file
            shutil.copyfile(src_fn, dst_fn)

            # add .py files to 2to3 list
            if dst_fn.endswith('.py'):
                to_convert.append((src_fn, dst_fn))

    # run 2to3
    flag_sets = {}
    for fn, dst_fn in to_convert:
        flag = ''
        for pat, opt in EXTRA_2TO3_FLAGS.items():
            if fnmatch.fnmatch(fn, pat):
                flag = opt
                break
        flag_sets.setdefault(flag, []).append(dst_fn)

    if patchfile:
        p = open(patchfile, 'wb+')
    else:
        p = open(os.devnull, 'wb')

    for flags, filenames in flag_sets.items():
        if flags == 'skip':
            continue

        _old_stdout = sys.stdout
        try:
            sys.stdout = StringIO()
            lib2to3.main.main("lib2to3.fixes", ['-w'] + flags.split()+filenames)
        finally:
            sys.stdout = _old_stdout

    for fn, dst_fn in to_convert:
        # perform custom mangling
        custom_mangling(dst_fn)

    p.close()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = build_context
import os

from cPickle \
    import \
        load, dump

from yaku.utils \
    import \
        rename

CACHE_FILE = ".cache.lock"

class BuildContext(object):
    def __init__(self):
        self.object_tasks = []
        self.cache = {}
        self.env = {}

    def load(self):
        if os.path.exists(CACHE_FILE):
            fid = open(CACHE_FILE, "rb")
            try:
                self.cache = load(fid)
            finally:
                fid.close()
        else:
            self.cache = {}

    def save(self):
        # Use rename to avoid corrupting the cache if interrupted
        tmp_fid = open(CACHE_FILE + ".tmp", "w")
        try:
            dump(self.cache, tmp_fid)
        finally:
            tmp_fid.close()
        rename(CACHE_FILE + ".tmp", CACHE_FILE)

def get_bld():
    bld = BuildContext()
    bld.load()

    bld.env.update({
            "VERBOSE": False,
            "BLDDIR": "build"})

    return bld


########NEW FILE########
__FILENAME__ = py3k
import sys

def get_exception():
    return sys.exc_info()[1]

########NEW FILE########
__FILENAME__ = rename
import os
import os.path
import random

from yaku.compat.py3k \
    import \
        get_exception

def rename(src, dst):
    "Atomic rename on windows."
    # This is taken from mercurial
    try:
        os.rename(src, dst)
    except OSError:
        err = get_exception()
        # If dst exists, rename will fail on windows, and we cannot
        # unlink an opened file. Instead, the destination is moved to
        # a temporary location if it already exists.

        def tempname(prefix):
            for i in range(5):
                fn = '%s-%08x' % (prefix, random.randint(0, 0xffffffff))
                if not os.path.exists(fn):
                    return fn
            raise IOError(errno.EEXIST, "No usable temporary filename found")

        temp = tempname(dst)
        os.rename(dst, temp)
        try:
            os.unlink(temp)
        except:
            # Some rude AV-scanners on Windows may cause the unlink to
            # fail. Not aborting here just leaks the temp file, whereas
            # aborting at this point may leave serious inconsistencies.
            # Ideally, we would notify the user here.
            pass
        os.rename(src, dst)

########NEW FILE########
__FILENAME__ = compiled_fun
import os
import re
import sys

from yaku.environment \
    import \
        Environment

COMPILE_TEMPLATE_SHELL = '''
def f(task):
    env = task.env
    bld = task.bld
    wd = getattr(task, 'cwd', None)
    p = env.get_flat
    cmd = \'\'\' %s \'\'\' % s
    return task.exec_command(cmd, cwd=wd, env=env['ENV'])
'''

COMPILE_TEMPLATE_NOSHELL = '''
def f(task):
	env = task.env
	bld_root = task.gen.bld.bld_root
	wd = getattr(task, 'cwd', None)
	def to_list(xx):
		if isinstance(xx, str): return [xx]
		return xx
	lst = []
	%s
	lst = [x for x in lst if x]
	return task.exec_command(lst, cwd=wd, env=env['ENV'])
'''


def funex(c):
    dc = {}
    exec(c, dc)
    return dc['f']

reg_act = re.compile(r"(?P<backslash>\\)|(?P<dollar>\$\$)|(?P<subst>\$\{(?P<var>\w+)(?P<code>.*?)\})", re.M)
def compile_fun_shell(name, line):
    """Compiles a string (once) into a function, eg:
    simple_task_type('c++', '${CXX} -o ${TGT[0]} ${SRC} -I ${SRC[0].parent.bldpath()}')

    The env variables (CXX, ..) on the task must not hold dicts (order)
    The reserved keywords TGT and SRC represent the task input and output nodes

    quick test:
    bld(source='wscript', rule='echo "foo\\${SRC[0].name}\\bar"')
    """

    extr = []
    def repl(match):
        g = match.group
        if g('dollar'): return "$"
        elif g('backslash'): return '\\\\'
        elif g('subst'): extr.append((g('var'), g('code'))); return "%s"
        return None

    line = reg_act.sub(repl, line)

    parm = []
    dvars = []
    app = parm.append
    for (var, meth) in extr:
        if var == 'SRC':
            if meth:
                app('task.inputs%s' % meth)
            else:
                app('" ".join([i.path_from(bld.bldnode) for i in task.inputs)')
        elif var == 'TGT':
            if meth:
                app('task.outputs%s' % meth)
            else:
                app('" ".join([i.path_from(bld.bldnode) for i in task.outputs)')
        else:
            if not var in dvars:
                dvars.append(var)
            app("p('%s')" % var)
    if parm:
        parm = "%% (%s) " % (',\n\t\t'.join(parm))
    else:
        parm = ''

    c = COMPILE_TEMPLATE_SHELL % (line, parm)

    return (funex(c), dvars)

def compile_fun_noshell(name, line):

    extr = []
    def repl(match):
        g = match.group
        if g('dollar'): return "$"
        elif g('subst'): extr.append((g('var'), g('code'))); return "<<|@|>>"
        return None

    line2 = reg_act.sub(repl, line)
    params = line2.split('<<|@|>>')

    buf = []
    dvars = []
    app = buf.append
    for x in range(len(extr)):
        params[x] = params[x].strip()
        if params[x]:
            app("lst.extend(%r)" % params[x].split())
        (var, meth) = extr[x]
        if var == 'SRC':
            if meth: app('lst.append(task.inputs%s)' % meth)
            else:
                app('lst.extend([i.path_from(bld_root) for i in task.inputs])')
        elif var == 'TGT':
            if meth: app('lst.append(task.outputs%s)' % meth)
            else:
                app('lst.extend([i.path_from(bld_root) for i in task.outputs])')
        else:
            app('lst.extend(to_list(env.get(%r, [])))' % var)
            if not var in dvars: dvars.append(var)

    if params[-1]:
        app("lst.extend(%r)" % shlex.split(params[-1]))

    fun = COMPILE_TEMPLATE_NOSHELL % "\n\t".join(buf)
    return (funex(fun), dvars)

def compile_fun(name, line, shell=None):
    "commands can be launched by the shell or not"
    if line.find('<') > 0 or line.find('>') > 0 or line.find('&&') > 0:
        shell = True

    if shell is None:
        if sys.platform == 'win32':
            shell = False
        else:
            shell = True

    if shell:
        return compile_fun_shell(name, line)
    else:
        return compile_fun_noshell(name, line)

########NEW FILE########
__FILENAME__ = conf
import os
import sys
import re

try:
    from hashlib import md5
except ImportError:
    from md5 import md5

if sys.version_info[0] < 3:
    from cStringIO \
        import \
            StringIO
else:
    from io \
        import \
            StringIO

from yaku.errors \
    import \
        UnknownTask
from yaku.utils \
    import \
        ensure_dir

def create_file(conf, code, prefix="", suffix=""):
    filename = "%s%s%s" % (prefix, md5(code.encode()).hexdigest(), suffix)
    node = conf.bld_root.declare(filename)
    node.write(code)
    return node

def with_conf_blddir(conf, name, body, func):
    """'Context manager' to execute a series of tasks into code-specific build
    directory.
    
    func must be a callable taking no arguments
    """
    old_root, new_root = create_conf_blddir(conf, name, body)
    try:
        conf.bld_root = new_root
        conf.bld_root.ctx.bldnode = new_root
        return func()
    finally:
        conf.bld_root = old_root
        conf.bld_root.ctx.bldnode = old_root

def write_log(conf, log, tasks, code, succeed, explanation):
    for line in code.splitlines():
        log.write("  |%s\n" % line)

    if succeed:
        log.write("---> Succeeded !\n")
    else:
        log.write("---> Failure !\n")
        log.write("~~~~~~~~~~~~~~\n")
        log.write(explanation)
        log.write("~~~~~~~~~~~~~~\n")

    s = StringIO()
    s.write("Command sequence was:\n")
    for t in tasks:
        try:
            cmd = conf.get_cmd(t)
            s.write("%s\n" % " ".join(cmd))
            stdout = conf.get_stdout(t)
            if stdout:
                s.write("\n")
                for line in stdout.splitlines():
                    s.write("%s\n" % line)
                s.write("\n")
        except UnknownTask:
            break
    log.write(s.getvalue())
    log.write("\n")

def create_conf_blddir(conf, name, body):
    dirname = ".conf-%s-%s" % (name, hash(name+body))
    bld_root = os.path.join(conf.bld_root.abspath(), dirname)
    if not os.path.exists(bld_root):
        os.makedirs(bld_root)
    bld_root = conf.bld_root.make_node(dirname)
    old_root = conf.bld_root
    return old_root, bld_root

########NEW FILE########
__FILENAME__ = conftests
import copy

def check_compiler(conf, msg=None):
    code = """\
int main(void)
{
    return 0;
}
"""

    if msg is None:
        conf.start_message("Checking whether C compiler works")
    else:
        conf.start_message(msg)
    ret = conf.builders["ctasks"].try_program("check_cc", code, None)
    if ret:
        conf.end_message("yes")
    else:
        conf.end_message("no")
    return ret

def check_cpp_symbol(conf, symbol, headers=None):
    code = []
    if headers:
        for h in headers:
            code.append(r"#include <%s>" % h)

    code.append(r"""
int main()
{
#ifndef %s
    (void) %s;
#endif
    ;
    return 0;
}
""" % (symbol, symbol))
    src = "\n".join(code)

    conf.start_message("Checking for declaration %s" % symbol)
    ret = conf.builders["ctasks"].try_compile("check_cpp_symbol", src, headers)
    conf.conf_results.append({"type": "decl", "value": symbol,
                              "result": ret})
    if ret:
        conf.end_message("yes")
    else:
        conf.end_message("no")
    return ret

def check_type(conf, type_name, headers=None):
    code = r"""
int main() {
  if ((%(name)s *) 0)
    return 0;
  if (sizeof (%(name)s))
    return 0;
}
""" % {'name': type_name}

    conf.start_message("Checking for type %s" % type_name)
    ret = conf.builders["ctasks"].try_compile("check_type", code,
                        headers)
    conf.conf_results.append({"type": "type", "value": type_name,
                              "result": ret})
    if ret:
        conf.end_message("yes")
    else:
        conf.end_message("no")
    return ret

def check_type_size(conf, type_name, headers=None, expect=None):
    """\
    This check can be used to get the size of a given type, or to
    check whether the type is of expected size.

    Arguments
    ---------
    conf : object
        configure context instance
    type_name : str
        the type to check
    includes : sequence
        list of headers to include in the test code before testing the
        type
    expect : sequence
        if given, will test wether the type has the given number of
            bytes.  If not given, will automatically find the size.
    """

    conf.start_message("Checking for sizeof %s ..." % type_name)
    body = r"""
typedef %(type)s yaku_check_sizeof_type;
int main ()
{
    static int test_array [1 - 2 * !(((long) (sizeof (yaku_check_sizeof_type))) >= 0)];
    test_array [0] = 0

    ;
    return 0;
}
""" % {"type": type_name}

    ret = conf.builders["ctasks"].try_compile("check_type_size", body, headers)
    if not ret:
        conf.end_message("no (cannot compile type)")
        return False

    if expect is None:
        # this fails to *compile* if size > sizeof(type)
        body = r"""
typedef %(type)s npy_check_sizeof_type;
int main ()
{
    static int test_array [1 - 2 * !(((long) (sizeof (npy_check_sizeof_type))) <= %(size)s)];
    test_array [0] = 0

    ;
    return 0;
}
"""
        # The principle is simple: we first find low and high bounds
        # of size for the type, where low/high are looked up on a log
        # scale. Then, we do a binary search to find the exact size
        # between low and high
        low = 0
        mid = 0
        while True:
            code = body % {'type': type_name, 'size': mid}
            ret = conf.builders["ctasks"].try_compile("check_type_size",
                    code, headers)
            if ret:
                break
            #log.info("failure to test for bound %d" % mid)
            low = mid + 1
            mid = 2 * mid + 1

        high = mid
        # Binary search:
        while low != high:
            mid = (high - low) / 2 + low
            code = body % {'type': type_name, 'size': mid}
            if conf.builders["ctasks"].try_compile("check_type_size",
                    code, headers):
                high = mid
            else:
                low = mid + 1
        ret = low
        size = low
        conf.end_message("%d" % low)
    else:
        raise NotImplementedError("Expect arg not yet implemented")

    conf.conf_results.append({"type": "type_size", "value": type_name,
                              "result": ret})
    return ret

def define(conf, name, value=None, comment=None):
    """\
    Define a new preprocessing symbol in the current config header.

    If value is None. then #define name is written. If value is not
    none, then #define name value is written.

    Parameters
    ----------
    conf: object
        Instance of the current configure context
    name: str
        Name of the symbol to define
    value : (None)
        If given and not None, the symbol will have this value
    comment: str
        will be put as a C comment in the header, to explain the
        meaning of the value (appropriate C comments /* and */ will be
        put automatically).
    """
    lines = []
    if comment:
        comment_str = "\n/* %s */" % comment
        lines.append(comment_str)
    else:
        lines.append("\n")

    if value is not None:
        define_str = "#define %s %s" % (name, value)
    else:
        define_str = "#define %s" % name
    lines.append(define_str)
    lines.append('')
    content = "\n".join(lines)

    conf.conf_results.append({"type": "define", "value": content,
        "result": True})

def check_header(conf, header):
    code = r"""
#include <%s>
""" % header

    conf.start_message("Checking for header %s" % header)
    ret = conf.builders["ctasks"].try_compile("check_header", code, None)
    if ret:
        conf.end_message("yes")
    else:
        conf.end_message("no !")
    conf.conf_results.append({"type": "header", "value": header,
                              "result": ret})
    return ret

def check_func(conf, func, libs=None):
    if libs is None:
        libs = []
    # Handle MSVC intrinsics: force MS compiler to make a function
    # call. Useful to test for some functions when built with
    # optimization on, to avoid build error because the intrinsic and
    # our 'fake' test declaration do not match.
    code = r"""
char %(func)s (void);

#ifdef _MSC_VER
#pragma function(%(func)s)
#endif

int main (void)
{
    return %(func)s();
}
""" % {"func": func}

    if libs:
        msg = "Checking for function %s in %s" % (func, " ".join([conf.env["LIB_FMT"] % lib for lib in libs]))
    else:
        msg = "Checking for function %s" % func
    conf.start_message(msg)

    old_lib = copy.deepcopy(conf.env["LIBS"])
    try:
        for lib in libs[::-1]:
            conf.env["LIBS"].insert(0, lib)
        ret = conf.builders["ctasks"].try_program("check_func", code, None)
        if ret:
            conf.end_message("yes")
        else:
            conf.end_message("no !")
    finally:
        conf.env["LIBS"] = old_lib
    conf.conf_results.append({"type": "func", "value": func,
                              "result": ret})
    return ret

def check_lib(conf, libs, func):
    # XXX: refactor with check_func

    # Handle MSVC intrinsics: force MS compiler to make a function
    # call. Useful to test for some functions when built with
    # optimization on, to avoid build error because the intrinsic and
    # our 'fake' test declaration do not match.
    code = r"""
char %(func)s (void);

#ifdef _MSC_VER
#pragma function(%(func)s)
#endif

int main (void)
{
    return %(func)s();
}
    """ % {"func": func}

    conf.start_message("Checking for function %s in %s" % \
                       (func, " ".join([conf.env["LIB_FMT"] % lib for lib in libs])))

    old_lib = copy.deepcopy(conf.env["LIBS"])
    try:
        for i in range(len(libs)):
            conf.env["LIBS"].insert(i, libs[i])
        ret = conf.builders["ctasks"].try_program("check_lib", code, None)
        if ret:
            conf.end_message("yes")
        else:
            conf.end_message("no !")
    finally:
        conf.env["LIBS"] = old_lib
    for lib in libs:
        conf.conf_results.append({"type": "lib", "value": lib,
                                  "result": ret, "func": func})
    return ret

def check_funcs_at_once(conf, funcs, libs=None):
    if libs is None:
        libs = []

    header = []
    header = ['#ifdef __cplusplus']
    header.append('extern "C" {')
    header.append('#endif')
    for f in funcs:
        header.append("\tchar %s();" % f)
        # Handle MSVC intrinsics: force MS compiler to make a function
        # call. Useful to test for some functions when built with
        # optimization on, to avoid build error because the intrinsic
        # and our 'fake' test declaration do not match.
        header.append("#ifdef _MSC_VER")
        header.append("#pragma function(%s)" % f)
        header.append("#endif")
    header.append('#ifdef __cplusplus')
    header.append('};')
    header.append('#endif')
    header = "\n".join(header)

    tmp = []
    for f in funcs:
        tmp.append("\t%s();" % f)
    tmp = "\n".join(tmp)

    body = r"""
%(include)s
%(header)s

int main (void)
{
    %(tmp)s
        return 0;
}
""" % {"tmp": tmp, "include": "", "header": header}

    conf.start_message("Checking for functions %s" % ", ".join(funcs))
    old_lib = copy.deepcopy(conf.env["LIBS"])
    try:
        for lib in libs[::-1]:
            conf.env["LIBS"].insert(0, lib)
        ret = conf.builders["ctasks"].try_program("check_func", body, None)
        if ret:
            conf.end_message("yes")
        else:
            conf.end_message("no !")
    finally:
        conf.env["LIBS"] = old_lib

    for func in funcs:
        conf.conf_results.append({"type": "func", "value": func,
                                  "result": ret})
    return ret

########NEW FILE########
__FILENAME__ = fconftests
"""
Fortran-specific configuration tests
"""
import sys
import copy

from yaku.conftests.fconftests_imp \
    import \
        is_output_verbose, parse_flink

FC_VERBOSE_FLAG = "FC_VERBOSE_FLAG"
FC_RUNTIME_LDFLAGS = "FC_RUNTIME_LDFLAGS"
FC_DUMMY_MAIN = "FC_DUMMY_MAIN"

def check_fcompiler(conf, msg=None):
    code = """\
       program main
       end
"""
    if msg is None:
        conf.start_message("Checking whether Fortran compiler works")
    else:
        conf.start_message(msg)
    ret = conf.builders["fortran"].try_program("check_fcompiler", code)
    if ret:
        conf.end_message("yes")
    else:
        conf.end_message("no !")
        conf.fail_configuration("")
    return ret

def check_fortran_verbose_flag(conf):
    code = """\
       program main
       end
"""
    conf.start_message("Checking for verbose flag")
    if not conf.builders["ctasks"].configured:
        raise ValueError("'ctasks'r needs to be configured first!")
    if sys.platform == "win32":
        conf.end_message("none needed")
        conf.env[FC_VERBOSE_FLAG] = []
        return True
    for flag in ["-v", "--verbose", "-V", "-verbose"]:
        old = copy.deepcopy(conf.env["F77_LINKFLAGS"])
        try:
            conf.env["F77_LINKFLAGS"].append(flag)
            ret = conf.builders["fortran"].try_program("check_fc_verbose", code)
            if not ret:
                continue
            stdout = conf.get_stdout(conf.last_task)
            if ret and is_output_verbose(stdout):
                conf.end_message(flag)
                conf.env[FC_VERBOSE_FLAG] = flag
                return True
        finally:
            conf.env["F77_LINKFLAGS"] = old
    conf.end_message("failed !")
    conf.fail_configuration("")
    return False

def check_fortran_runtime_flags(conf):
    if not conf.builders["ctasks"].configured:
        raise ValueError("'ctasks'r needs to be configured first!")
    if sys.platform == "win32":
        return _check_fortran_runtime_flags_win32(conf)
    else:
        return _check_fortran_runtime_flags(conf)

def _check_fortran_runtime_flags_win32(conf):
    if conf.env["cc_type"] == "msvc":
        conf.start_message("Checking for fortran runtime flags")
        conf.end_message("none needed")
        conf.env[FC_RUNTIME_LDFLAGS] = []
    else:
        raise NotImplementedError("GNU support on win32 not ready")

def _check_fortran_runtime_flags(conf):
    if not FC_VERBOSE_FLAG in conf.env:
        raise ValueError("""\
You need to call check_fortran_verbose_flag before getting runtime
flags (or to define the %s variable)""" % FC_VERBOSE_FLAG)
    code = """\
       program main
       end
"""

    conf.start_message("Checking for fortran runtime flags")

    old = copy.deepcopy(conf.env["F77_LINKFLAGS"])
    try:
        conf.env["F77_LINKFLAGS"].append(conf.env["FC_VERBOSE_FLAG"])
        ret = conf.builders["fortran"].try_program("check_fc", code)
        if ret:
            stdout = conf.get_stdout(conf.last_task)
            flags = parse_flink(stdout)
            conf.end_message("%r" % " ".join(flags))
            conf.env[FC_RUNTIME_LDFLAGS] = flags
            return True
        else:
            conf.end_message("failed !")
            return False
    finally:
        conf.env["F77_LINKFLAGS"] = old
    return False

def check_fortran_dummy_main(conf):
    code_tpl = """\
#ifdef __cplusplus
        extern "C"
#endif
int %(main)s()
{
    return 1;
}

int main()
{
    return 0;
}
"""

    conf.start_message("Checking whether fortran needs dummy main")

    old = copy.deepcopy(conf.env["F77_LINKFLAGS"])
    try:
        conf.env["F77_LINKFLAGS"].extend(conf.env[FC_RUNTIME_LDFLAGS])
        ret = conf.builders["ctasks"].try_program("check_fc_dummy_main",
                code_tpl % {"main": "FC_DUMMY_MAIN"})
        if ret:
            conf.end_message("none")
            conf.env[FC_DUMMY_MAIN] = None
            return True
        else:
            conf.end_message("failed !")
            return False
    finally:
        conf.env["F77_LINKFLAGS"] = old

def check_fortran_mangling(conf):
    subr = """
      subroutine foobar()
      return
      end
      subroutine foo_bar()
      return
      end
"""
    main_tmpl = """
      int %s() { return 1; }
"""
    prog_tmpl = """
      void %(foobar)s(void);
      void %(foo_bar)s(void);
      int main() {
      %(foobar)s();
      %(foo_bar)s();
      return 0;
      }
"""

    conf.start_message("Checking fortran mangling scheme")
    old = {}
    for k in ["F77_LINKFLAGS", "LIBS", "LIBDIR"]:
        old[k] = copy.deepcopy(conf.env[k])
    try:
        mangling_lib = "check_fc_mangling_lib"
        ret = conf.builders["fortran"].try_static_library(mangling_lib, subr)
        if ret:
            if conf.env[FC_DUMMY_MAIN] is not None:
                main = main_tmpl % conf.env["FC_DUMMY_MAIN"]
            else:
                main = ""
            conf.env["LIBS"].insert(0, mangling_lib)
            libdir = conf.last_task.outputs[-1].parent.abspath()
            conf.env["LIBDIR"].insert(0, libdir)

            for u, du, case in mangling_generator():
                names = {"foobar": mangle_func("foobar", u, du, case),
                         "foo_bar": mangle_func("foo_bar", u, du, case)}
                prog = prog_tmpl % names
                name = "check_fc_mangling_main"
                def _name(u):
                    if u == "_":
                        return "u"
                    else:
                        return "nu"
                name += "_%s_%s_%s" % (_name(u), _name(du), case)
                ret = conf.builders["ctasks"].try_program(name, main + prog)
                if ret:
                    conf.env["FC_MANGLING"] = (u, du, case)
                    conf.end_message("%r %r %r" % (u, du, case))
                    return
            conf.end_message("failed !")
            conf.fail_configuration(None)
        else:
            conf.end_message("failed !")
            conf.fail_configuration(None)

    finally:
        for k in old:
            conf.env[k] = old[k]

def mangling_generator():
    for under in ['_', '']:
        for double_under in ['', '_']:
            for case in ["lower", "upper"]:
                yield under, double_under, case

def mangle_func(name, under, double_under, case):
    return getattr(name, case)() + under + (name.find("_") != -1 and double_under or '')

########NEW FILE########
__FILENAME__ = fconftests_imp
import os
import sys
import re
import shlex

GCC_DRIVER_LINE = re.compile('^Driving:')
POSIX_STATIC_EXT = re.compile('\S+\.a')
POSIX_LIB_FLAGS = re.compile('-l\S+')

def is_output_verbose(out):
    for line in out.splitlines():
        if not GCC_DRIVER_LINE.search(line):
            if POSIX_STATIC_EXT.search(line) or POSIX_LIB_FLAGS.search(line):
                return True
    return False

# linkflags which match those are ignored
LINKFLAGS_IGNORED = [r'-lang*', r'-lcrt[a-zA-Z0-9]*\.o', r'-lc$', r'-lSystem',
                     r'-libmil', r'-LIST:*', r'-LNO:*']
if os.name == 'nt':
    LINKFLAGS_IGNORED.extend([r'-lfrt*', r'-luser32',
            r'-lkernel32', r'-ladvapi32', r'-lmsvcrt',
            r'-lshell32', r'-lmingw', r'-lmoldname'])
else:
    LINKFLAGS_IGNORED.append(r'-lgcc*')

RLINKFLAGS_IGNORED = [re.compile(f) for f in LINKFLAGS_IGNORED]

def _match_ignore(line):
    """True if the line should be ignored."""
    if [i for i in RLINKFLAGS_IGNORED if i.match(line)]:
        return True
    else:
        return False

def parse_flink(output):
    """Given the output of verbose link of fortran compiler, this
    returns a list of flags necessary for linking using the standard
    linker."""
    # TODO: On windows ?
    final_flags = []
    for line in output.splitlines():
        if not GCC_DRIVER_LINE.match(line):
            _parse_f77link_line(line, final_flags)
    return final_flags

SPACE_OPTS = re.compile('^-[LRuYz]$')
NOSPACE_OPTS = re.compile('^-[RL]')

def _parse_f77link_line(line, final_flags):
    #line = line.encode("utf-8")
    lexer = shlex.shlex(line, posix = True)
    lexer.whitespace_split = True

    t = lexer.get_token()
    tmp_flags = []
    while t:
        def parse(token):
            # Here we go (convention for wildcard is shell, not regex !)
            #   1 TODO: we first get some root .a libraries
            #   2 TODO: take everything starting by -bI:*
            #   3 Ignore the following flags: -lang* | -lcrt*.o | -lc |
            #   -lgcc* | -lSystem | -libmil | -LANG:=* | -LIST:* | -LNO:*)
            #   4 take into account -lkernel32
            #   5 For options of the kind -[[LRuYz]], as they take one argument
            #   after, the actual option is the next token 
            #   6 For -YP,*: take and replace by -Larg where arg is the old
            #   argument
            #   7 For -[lLR]*: take

            # step 3
            if _match_ignore(token):
                pass
            # step 4
            elif token.startswith('-lkernel32') and sys.platform == 'cygwin':
                tmp_flags.append(token)
            # step 5
            elif SPACE_OPTS.match(token):
                t = lexer.get_token()
                # FIXME: this does not make any sense ... pull out
                # what we need from this section
                #if t.startswith('P,'):
                #    t = t[2:]
                #    for opt in t.split(os.pathsep):
                #        tmp_flags.append('-L%s' % opt)
            # step 6
            elif NOSPACE_OPTS.match(token):
                tmp_flags.append(token)
            # step 7
            elif POSIX_LIB_FLAGS.match(token):
                tmp_flags.append(token)
            else:
                # ignore anything not explicitely taken into account
                pass

            t = lexer.get_token()
            return t
        t = parse(t)

    final_flags.extend(tmp_flags)
    return final_flags

########NEW FILE########
__FILENAME__ = pytests
import sys

def check_module(conf, module_name):
    conf.start_message("Checking for module %r" % (module_name,))
    python = sys.executable
    cmd = [python, "-c", "'import %s'" % module_name]
    ret = conf.try_command(" ".join(cmd), kw={"shell": True})
    if ret:
        conf.end_message("yes")
    else:
        conf.end_message("no")
    return ret

########NEW FILE########
__FILENAME__ = context
import os
import sys
import subprocess

if sys.version_info[0] < 3:
    from cPickle \
        import \
            load, dump, dumps
else:
    from pickle \
        import \
            load, dump, dumps

from yaku._config \
    import \
        DEFAULT_ENV, BUILD_CONFIG, BUILD_CACHE, CONFIG_CACHE, HOOK_DUMP, \
        _OUTPUT
from yaku.environment \
    import \
        Environment
from yaku.tools \
    import \
        import_tools
from yaku.utils \
    import \
        ensure_dir, rename, join_bytes
from yaku.errors \
    import \
        UnknownTask, ConfigurationFailure, TaskRunFailure, WindowsError
import yaku.node
import yaku.task_manager

def create_top_nodes(start_dir, build_dir):
    root = yaku.node.Node("", None)
    if not os.path.exists(build_dir):
        os.makedirs(build_dir)
    if not os.path.exists(start_dir):
        raise ValueError("%s does not exist ???" % start_dir)
    srcnode = root.find_dir(start_dir)
    bldnode = root.find_dir(build_dir)

    # FIXME
    class _FakeContext(object):
        pass
    yaku.node.Node.ctx = _FakeContext()
    yaku.node.Node.ctx.srcnode = srcnode
    yaku.node.Node.ctx.bldnode = bldnode

    return srcnode, bldnode

# XXX: yet another hack. We need to guarantee that a same file is
# represented by exactly the same node instance (defined as the same
# id) everywhere. Pickling does not guarantee that, so for the files
# mapping, we store the nodes as paths and pickle that. We recreate
# the nodes from there when we need to reload it.
def _hook_id_to_hook_path(hook_dict):
    # convert a hook dict indexed by id to a hook dict indexed by
    # paths (for storage)
    return dict([(k.srcpath(), v) for k, v in hook_dict.items()])

def _hook_path_to_hook_id(src_root, hook_dict):
    # convert a hook dict indexed by paths to a hook dict indexed by
    # id
    return dict([(src_root.find_resource(k), v) for \
                 k, v in hook_dict.items()])

class ConfigureContext(object):
    def __init__(self):
        self.env = Environment()
        self.tools = []
        self._tool_modules = {}
        self.builders = {}
        self.cache = {}
        self.conf_results = []
        self._configured = {}
        self._stdout_cache = {}
        self._cmd_cache = {}

        self.src_root = None
        self.bld_root = None
        self.path = None

    def load_tool(self, tool, tooldir=None):
        _t = import_tools([tool], tooldir)
        self.tools.append({"tool": tool, "tooldir": tooldir})
        mod = _t[tool]
        self._tool_modules[tool] = mod
        if hasattr(mod, "get_builder"):
            self.builders[tool] = mod.get_builder(self)
        return mod

    def use_tools(self, tools, tooldir=None):
        ret = {}
        for t in tools:
            if t in self._tool_modules:
                _t = self._tool_modules[t]
            else:
                _t = self.load_tool(t, tooldir)
            ret[t] = _t
        self.setup_tools()
        return ret

    def setup_tools(self):
        for builder in self.builders.values():
            if not builder.configured:
               builder.configure()
               self._configured[builder] = True

    def store(self):
        default_env = self.bld_root.make_node(DEFAULT_ENV)
        self.env.store(default_env.abspath())

        self.log.close()

        config_cache = self.bld_root.make_node(CONFIG_CACHE)
        out = []
        out.append(dumps(self.cache))
        out.append(dumps(self._stdout_cache))
        out.append(dumps(self._cmd_cache))
        config_cache.write(join_bytes(out), flags="wb")

        build_config = self.bld_root.make_node(BUILD_CONFIG)
        build_config.write("%r\n" % self.tools)

        hook_dump = self.bld_root.make_node(HOOK_DUMP)
        s = dumps({"extensions": yaku.task_manager.RULES_REGISTRY,
                   "files": _hook_id_to_hook_path(yaku.task_manager.FILES_REGISTRY)})
        hook_dump.write(s, flags="wb")

    def start_message(self, msg):
        _OUTPUT.write(msg + "... ")
        self.log.write("=" * 79 + "\n")
        self.log.write("%s\n" % msg)

    def end_message(self, msg):
        _OUTPUT.write("%s\n" % msg)

    def fail_configuration(self, msg):
        msg = "%s\nPlease look at the configuration log %r" % (msg, self.log.name)
        self.log.flush()
        raise ConfigurationFailure(msg)

    def set_cmd_cache(self, task, cmd):
        self._cmd_cache[task.get_uid()] = cmd[:]

    def get_cmd(self, task):
        tid = task.get_uid()
        try:
            return self._cmd_cache[tid]
        except KeyError:
            raise UnknownTask

    def set_stdout_cache(self, task, stdout):
        self._stdout_cache[task.get_uid()] = stdout

    def get_stdout(self, task):
        tid = task.get_uid()
        try:
            return self._stdout_cache[tid]
        except KeyError:
            raise UnknownTask

    def try_command(self, cmd, cwd=None, env=None, kw=None):
        if cwd is None:
            cwd = self.bld_root.abspath()
        if kw is None:
            kw = {}
        if env is not None:
            kw["env"] = env

        def log_failure(succeed, explanation):
            if succeed:
                self.log.write("---> Succeeded !\n")
            else:
                self.log.write("---> Failure !\n")
                self.log.write("~~~~~~~~~~~~~~\n")
                self.log.write(explanation)
                self.log.write("~~~~~~~~~~~~~~\n")
            self.log.write("Command was:\n")
            self.log.write("%s\n" % cmd)
            self.log.write("\n")

        try:
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT, cwd=cwd, **kw)
            stdout = p.communicate()[0].decode("utf-8")
            log_failure(p.returncode == 0, stdout)
            if p.returncode:
                return False
            return True
        except OSError:
            e = get_exception()
            log_failure(False, str(e))
        except WindowsError:
            e = get_exception()
            log_failure(False, str(e))
        return False

def load_tools(self, fid):
    tools = eval(fid.read())
    for t in tools:
        _t = import_tools([t["tool"]], t["tooldir"])
        tool_name = t["tool"]
        tool_mod = _t[tool_name]
        if hasattr(tool_mod, "get_builder"):
            self.builders[tool_name] = tool_mod.get_builder(self)
        # XXX: this is ugly - need to rethinkg tool
        # initialization/configuration
        if hasattr(tool_mod, "init"):
            tool_mod.init()
    self.tools = tools

class BuildContext(object):
    def __init__(self):
        self.env = Environment()
        self.tools = []
        self.cache = {}
        self.builders = {}
        self.tasks = []

    def load(self, src_path=None, build_path="build"):
        if src_path is None:
            src_path = os.getcwd()
        src_path = os.path.abspath(src_path)
        build_path = os.path.abspath(os.path.join(os.getcwd(), build_path))
        if not os.path.exists(build_path):
            raise IOError("%s not found (did you use different build_path for configure and build contexts ?)" \
                          % build_path)

        srcnode, bldnode = create_top_nodes(src_path, build_path)
        self.src_root = srcnode
        self.bld_root = bldnode
        self.path = srcnode

        self.env = Environment()
        default_env = bldnode.find_node(DEFAULT_ENV)
        if default_env:
            self.env.load(default_env.abspath())
        if not os.path.abspath(self.env["BLDDIR"]) == bldnode.abspath():
            raise ValueError("Gne ?")

        build_config = bldnode.find_node(BUILD_CONFIG)
        if build_config is None:
            raise IOError("Did not find %r in %r" % (BUILD_CONFIG, bldnode.abspath()))
        else:
            f = open(build_config.abspath())
            try:
                load_tools(self, f)
            finally:
                f.close()

        build_cache = bldnode.find_node(BUILD_CACHE)
        if build_cache is not None:
            fid = open(build_cache.abspath(), "rb")
            try:
                self.cache = load(fid)
            finally:
                fid.close()
        else:
            self.cache = {}

        hook_dump = bldnode.find_node(HOOK_DUMP)
        fid = open(hook_dump.abspath(), "rb")
        try:
            data = load(fid)
            yaku.task_manager.RULES_REGISTRY = data["extensions"]
            yaku.task_manager.FILES_REGISTRY = _hook_path_to_hook_id(srcnode, data["files"])
        finally:
            fid.close()

    def store(self):
        build_cache = self.bld_root.make_node(BUILD_CACHE)
        tmp_fid = open(build_cache.abspath() + ".tmp", "wb")
        try:
            dump(self.cache, tmp_fid)
        finally:
            tmp_fid.close()
        rename(build_cache.abspath() + ".tmp", build_cache.abspath())

    def set_stdout_cache(self, task, stdout):
        pass

    def set_cmd_cache(self, task, stdout):
        pass

def myopen(filename, mode="r"):
    if "w" in mode:
        ensure_dir(filename)
    return open(filename, mode)

def get_cfg(src_path=None, build_path="build"):
    ctx = ConfigureContext()
    config_cache = os.path.join(build_path, CONFIG_CACHE)
    if os.path.exists(config_cache):
        fid = open(config_cache, "rb")
        try:
            ctx.cache = load(fid)
            ctx._stdout_cache = load(fid)
            ctx._cmd_cache = load(fid)
        finally:
            fid.close()

    # XXX: how to reload existing environment ?
    env = Environment()
    if not "BLDDIR" in env:
        env["BLDDIR"] = build_path
    # FIXME: nothing to do here
    env["VERBOSE"] = False
    if "-v" in sys.argv:
        env["VERBOSE"] = True
    # Keep this as is - we do want a dictionary for 'serialization', and python
    # 3 os.environ is an object instead of a dict
    env["ENV"] = dict([(k, v) for k, v in os.environ.items()])

    if src_path is None:
        src_path = os.getcwd()
    srcnode, bldnode = create_top_nodes(
            os.path.abspath(src_path),
            os.path.abspath(env["BLDDIR"]))
    ctx.src_root = srcnode
    ctx.bld_root = bldnode
    # src_root and bld_root never change, but path may. All source nodes are
    # created relatively to path (kinda 'virtual' cwd)
    ctx.path = srcnode

    ctx.env = env
    ctx.log = myopen(os.path.join(env["BLDDIR"], "config.log"), "w")
    return ctx

def get_bld(src_path=None, build_path="build"):
    ctx = BuildContext()
    ctx.load(src_path=src_path, build_path=build_path)

    return ctx

########NEW FILE########
__FILENAME__ = environment
import re
import os

from yaku.utils \
    import \
    ensure_dir, rename

re_imp = re.compile('^(#)*?([^#=]*?)\ =\ (.*?)$', re.M)

class Environment(dict):
    def get_flat(self, k):
        s = self[k]
        if isinstance(s, str):
            return s
        else:
            return " ".join(s)

    def store(self, filename):
        tmp = filename + ".tmp"
        ensure_dir(tmp)
        fid = open(tmp, "w")
        try:
            for k in sorted(self.keys()):
                fid.write("%s = %r\n" % (k, self[k]))
        finally:
            fid.close()
        rename(tmp, filename)

    def load(self, filename):
        f = open(filename)
        for m in re_imp.finditer(f.read()):
            self[m.group(2)] = eval(m.group(3))

    def append(self, var, value, create=False):
        """Append a single item to the variable var."""
        if create:
            cur = self.get(var, [])
            self[var] = cur
        else:
            cur = self[var]
        cur.append(value)

    def append_unique(self, var, value, create=False):
        """Append a single item to the variable var if not already there. Does
        nothing otherwise"""
        if create:
            cur = self.get(var, [])
            self[var] = cur
        else:
            cur = self[var]
        if not value in cur:
            cur.append(value)

    def extend(self, var, values, create=False):
        if create:
            cur = self.get(var, [])
            self[var] = cur
        else:
            cur = self[var]
        cur.extend(values)

    def prepend(self, var, value, create=False):
        """Prepend a single item to the list."""
        if create:
            cur = self.get(var, [])
        else:
            cur = self[var]
        self[var] = [value] + cur

    def prextend(self, var, values, create=False):
        """Prepend a list of values in front of self[var]."""
        if create:
            cur = self.get(var, [])
        else:
            cur = self[var]
        self[var] = values + cur

########NEW FILE########
__FILENAME__ = errors
import sys
if sys.version_info >= (3,):
    import builtins
    if not hasattr(builtins, "WindowsError"):
        class WindowsError(Exception):
            pass
    else:
        WindowsError = builtins.WindowsError
else:
    import __builtin__
    if not hasattr(__builtin__, "WindowsError"):
        class WindowsError(Exception):
            pass
    else:
        WindowsError = __builtin__.WindowsError

class YakuError(Exception):
    pass

class TaskRunFailure(YakuError):
    def __init__(self, cmd, explain=None):
        self.cmd = cmd
        self.explain = explain

    def __str__(self):
        ret = "Command failed. Error was: \n\t%s\ncommand line was\n\t%r" % (self.explain.rstrip(),
                                                                          " ".join(self.cmd))
        if sys.version_info < (3,):
            return ret.encode("utf-8")
        else:
            return ret

class ConfigurationFailure(YakuError):
    pass

class UnknownTask(YakuError):
    pass

class ToolNotFound(YakuError):
    pass

########NEW FILE########
__FILENAME__ = node
"""
Ripped off from waf (v 1.6), by Thomas Nagy

The cool design is his, bugs most certainly mine :)
"""

import os, shutil, re, sys

def split_path(path):
    return path.split('/')

def split_path_cygwin(path):
    if path.startswith('//'):
        ret = path.split('/')[2:]
        ret[0] = '/' + ret[0]
        return ret
    return path.split('/')

re_sp = re.compile('[/\\\\]')
def split_path_win32(path):
    if path.startswith('\\\\'):
        ret = re.split(re_sp, path)[2:]
        ret[0] = '\\' + ret[0]
        return ret
    return re.split(re_sp, path)

if sys.platform == 'cygwin':
    split_path = split_path_cygwin
elif sys.platform == 'win32':
    split_path = split_path_win32

class Node(object):
    """
    This class is divided into two parts, the basic methods meant for filesystem access, and
    the ones which are bound to a context (the build context). The imports do not reflect this.
    """

    __slots__ = ('name', 'sig', 'children', 'parent', 'cache_abspath', 'cache_isdir')
    def __init__(self, name, parent):
        self.name = name
        self.parent = parent

        if parent:
            if name in parent.children:
                raise Errors.WafError('node %s exists in the parent files %r already' % (name, parent))
            parent.children[name] = self

    def __setstate__(self, data):
        self.name = data[0]
        self.parent = data[1]
        if data[2] is not None:
            self.children = data[2]
        if data[3] is not None:
            self.sig = data[3]

    def __getstate__(self):
        return (self.name, self.parent, getattr(self, 'children', None), getattr(self, 'sig', None))

    def __str__(self):
        return self.name

    def __repr__(self):
        return self.abspath()

    def __hash__(self):
        return id(self) # TODO see if it is still the case
        #raise Errors.WafError('do not hash nodes (too expensive)')

    def __eq__(self, node):
        return id(self) == id(node)

    def __copy__(self):
        "nodes are not supposed to be copied"
        raise Errors.WafError('nodes are not supposed to be copied')

    def read(self, flags='r'):
        "get the contents, assuming the node is a file"
        fid = open(self.abspath(), flags)
        try:
            return fid.read()
        finally:
            fid.close()

    def write(self, data, flags='w'):
        "write some text to the physical file, assuming the node is a file"
        f = None
        try:
            f = open(self.abspath(), flags)
            f.write(data)
        finally:
            if f:
                f.close()

    def chmod(self, val):
        "change file/dir permissions"
        os.chmod(self.abspath(), val)

    def delete(self):
        "delete the file physically, do not destroy the nodes"
        try:
            shutil.rmtree(self.abspath())
        except:
            pass

        try:
            delattr(self, 'children')
        except:
            pass

    def suffix(self):
        "scons-like - hot zone so do not touch"
        k = max(0, self.name.rfind('.'))
        return self.name[k:]

    def height(self):
        "amount of parents"
        d = self
        val = -1
        while d:
            d = d.parent
            val += 1
        return val

    def listdir(self):
        "list the directory contents"
        return os.listdir(self.abspath())

    def mkdir(self):
        "write a directory for the node"
        if getattr(self, 'cache_isdir', None):
            return

        try:
            self.parent.mkdir()
        except:
            pass

        if self.name:
            try:
                os.mkdir(self.abspath())
            except OSError:
                pass

            if not os.path.isdir(self.abspath()):
                raise IOError('%s is not a directory' % self)

            try:
                self.children
            except:
                self.children = {}

        self.cache_isdir = True

    def find_node(self, lst):
        "read the file system, make the nodes as needed"

        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        cur = self
        for x in lst:
            if x == '..':
                cur = cur.parent
                continue

            try:
                if x in cur.children:
                    cur = cur.children[x]
                    continue
            except:
                cur.children = {}

            # optimistic: create the node first then look if it was correct to do so
            cur = self.__class__(x, cur)
            try:
                os.stat(cur.abspath())
            except:
                del cur.parent.children[x]
                return None

        ret = cur

        try:
            while not getattr(cur.parent, 'cache_isdir', None):
                cur = cur.parent
                cur.cache_isdir = True
        except AttributeError:
            pass

        return ret

    def make_node(self, lst):
        "make a branch of nodes"
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        cur = self
        for x in lst:
            if x == '..':
                cur = cur.parent
                continue

            if getattr(cur, 'children', {}):
                if x in cur.children:
                    cur = cur.children[x]
                    continue
            else:
                cur.children = {}
            cur = self.__class__(x, cur)
        return cur

    def search(self, lst):
        "dumb search for existing nodes"
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        cur = self
        try:
            for x in lst:
                if x == '..':
                    cur = cur.parent
                else:
                    cur = cur.children[x]
            return cur
        except:
            pass

    def path_from(self, node):
        """path of this node seen from the other
            self = foo/bar/xyz.txt
            node = foo/stuff/
            -> ../bar/xyz.txt
        """
        # common root in rev 7673

        c1 = self
        c2 = node

        c1h = c1.height()
        c2h = c2.height()

        lst = []
        up = 0

        while c1h > c2h:
            lst.append(c1.name)
            c1 = c1.parent
            c1h -= 1

        while c2h > c1h:
            up += 1
            c2 = c2.parent
            c2h -= 1

        while id(c1) != id(c2):
            lst.append(c1.name)
            up += 1

            c1 = c1.parent
            c2 = c2.parent

        for i in range(up):
            lst.append('..')
        lst.reverse()
        return os.sep.join(lst) or '.'

    def abspath(self):
        """
        absolute path
        cache into the build context, cache_node_abspath
        """
        try:
            return self.cache_abspath
        except:
            pass
        # think twice before touching this (performance + complexity + correctness)
        if not self.parent:
            val = os.sep == '/' and os.sep or ''
        elif not self.parent.name:
            # drive letter for win32
            val = (os.sep == '/' and os.sep or '') + self.name
        else:
            val = self.parent.abspath() + os.sep + self.name

        self.cache_abspath = val
        return val

    def is_child_of(self, node):
        "does this node belong to the subtree node"
        p = self
        diff = self.height() - node.height()
        while diff > 0:
            diff -= 1
            p = p.parent
        return id(p) == id(node)

    # --------------------------------------------------------------------------------
    # the following methods require the source/build folders (bld.srcnode/bld.bldnode)
    # using a subclass is a possibility, but is that really necessary?
    # --------------------------------------------------------------------------------

    def is_src(self):
        """note: !is_src does not imply is_bld()"""
        cur = self
        x = id(self.ctx.srcnode)
        y = id(self.ctx.bldnode)
        while cur.parent:
            if id(cur) == y:
                return False
            if id(cur) == x:
                return True
            cur = cur.parent
        return False

    def is_bld(self):
        """note: !is_bld does not imply is_src"""
        cur = self
        y = id(self.ctx.bldnode)
        while cur.parent:
            if id(cur) == y:
                return True
            cur = cur.parent
        return False

    def get_src(self):
        """for a build node, will return the equivalent src node (or self if not possible)"""
        cur = self
        x = id(self.ctx.srcnode)
        y = id(self.ctx.bldnode)
        lst = []
        while cur.parent:
            if id(cur) == y:
                lst.reverse()
                return self.ctx.srcnode.make_node(lst)
            if id(cur) == x:
                return self
            lst.append(cur.name)
            cur = cur.parent
        return self

    def get_bld(self):
        """for a src node, will return the equivalent bld node (or self if not possible)"""
        cur = self
        x = id(self.ctx.srcnode)
        y = id(self.ctx.bldnode)
        lst = []
        while cur.parent:
            if id(cur) == y:
                return self
            if id(cur) == x:
                lst.reverse()
                return self.ctx.bldnode.make_node(lst)
            lst.append(cur.name)
            cur = cur.parent
        return self

    def find_resource(self, lst):
        """
        try to find a declared build node or a source file
        """
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']
        elif issubclass(type(lst), type(self)):
            return lst

        node = self.get_bld().search(lst)
        if node:
            return node

        self = self.get_src()
        node = self.search(lst)
        if node:
            return node

        node = self.find_node(lst)
        if node:
            return node

        return node

    def find_or_declare(self, lst):
        """
        if 'self' is in build directory, try to return an existing node
        if no node is found, go to the source directory
        try to find an existing node in the source directory
        if no node is found, create it in the build directory
        """
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        node = self.get_bld().search(lst)
        if node:
            if not os.path.isfile(node.abspath()):
                node.sig = None
                try:
                    node.parent.mkdir()
                except:
                    pass
            return node
        self = self.get_src()
        node = self.find_node(lst)
        if node:
            if not os.path.isfile(node.abspath()):
                node.sig = None
                try:
                    node.parent.mkdir()
                except:
                    pass
            return node
        node = self.get_bld().make_node(lst)
        node.parent.mkdir()
        return node

    def declare(self, lst):
        """
        if 'self' is in build directory, try to return an existing node
        if no node is found, create it in the build directory
        """
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        node = self.get_bld().search(lst)
        if node:
            if not os.path.isfile(node.abspath()):
                node.sig = None
                try:
                    node.parent.mkdir()
                except:
                    pass
            return node
        node = self.get_bld().make_node(lst)
        node.parent.mkdir()
        return node

    def find_dir(self, lst):
        """
        search a folder in the filesystem
        create the corresponding mappings source <-> build directories
        """
        if isinstance(lst, str):
            lst = [x for x in split_path(lst) if x and x != '.']

        node = self.find_node(lst)
        try:
            os.path.isdir(node.abspath())
        except OSError:
            return None
        return node

    # helpers for building things
    def change_ext(self, ext):
        "node of the same path, but with a different extension - hot zone so do not touch"
        name = self.name
        # XXX: is using name.find(".") as done in waf a bug ?
        k = name.rfind('.')
        if k >= 0:
            name = name[:k] + ext
        else:
            name = name + ext

        return self.parent.find_or_declare([name])

    def nice_path(self, env=None):
        "printed in the console, open files easily from the launch directory"
        return self.path_from(self.ctx.launch_node())

    def bldpath(self):
        "path seen from the build directory default/src/foo.cpp"
        return self.path_from(self.ctx.bldnode)

    def srcpath(self):
        "path seen from the source directory ../src/foo.cpp"
        return self.path_from(self.ctx.srcnode)

    def relpath(self):
        "if a build node, bldpath, else srcpath"
        cur = self
        x = id(self.ctx.bldnode)
        while cur.parent:
            if id(cur) == x:
                return self.ctxpath()
            cur = cur.parent
        return self.srcpath()

    def bld_dir(self):
        "build path without the file name"
        return self.parent.bldpath()

    def bld_base(self):
        "build path without the extension: src/dir/foo(.cpp)"
        s = os.path.splitext(self.name)[0]
        return self.bld_dir() + os.sep + s

########NEW FILE########
__FILENAME__ = pprint
import os
import sys
import re
import glob

from os.path import \
    join, split, splitext, dirname

from yaku._config \
    import \
        _OUTPUT

# Color handling for terminals (taken from waf)
COLORS_LST = {
        'USE' : True,
        'BOLD'  :'\x1b[01;1m',
        'RED'   :'\x1b[01;31m',
        'GREEN' :'\x1b[32m',
        'YELLOW':'\x1b[33m',
        'PINK'  :'\x1b[35m',
        'BLUE'  :'\x1b[01;34m',
        'CYAN'  :'\x1b[36m',
        'NORMAL':'\x1b[0m',
        'cursor_on'  :'\x1b[?25h',
        'cursor_off' :'\x1b[?25l',
}

GOT_TTY = not os.environ.get('TERM', 'dumb') in ['dumb', 'emacs']
if GOT_TTY:
    try:
        GOT_TTY = _OUTPUT.isatty()
    except AttributeError:
        GOT_TTY = False
if not GOT_TTY or 'NOCOLOR' in os.environ:
    COLORS_LST['USE'] = False

def get_color(cl):
    if not COLORS_LST['USE']:
        return ''
    return COLORS_LST.get(cl, '')

class foo(object):
    def __getattr__(self, a):
        return get_color(a)
    def __call__(self, a):
        return get_color(a)
COLORS = foo()

def pprint(color, str):
    _OUTPUT.write('%s%s%s\n' % (COLORS(color), str, COLORS('NORMAL')))

########NEW FILE########
__FILENAME__ = scheduler
import sys
import traceback
if sys.version_info[0] < 3:
    import Queue as queue
else:
    import queue
import threading

from yaku.task_manager \
    import \
        run_task, order_tasks, TaskManager
from yaku.utils \
    import \
        get_exception
import yaku.errors

def run_tasks(ctx, tasks=None):
    if tasks is None:
        tasks = ctx.tasks
    task_manager = TaskManager(tasks)
    s = SerialRunner(ctx, task_manager)
    s.start()
    s.run()

def run_tasks_parallel(ctx, tasks=None, maxjobs=1):
    if tasks is None:
        tasks = ctx.tasks
    task_manager = TaskManager(tasks)
    r = ParallelRunner(ctx, task_manager, maxjobs)
    r.start()
    r.run()

class SerialRunner(object):
    def __init__(self, ctx, task_manager):
        self.ctx = ctx
        self.task_manager = task_manager

    def start(self):
        # Dummy to give same interface as ParallelRunner
        pass

    def run(self):
        grp = self.task_manager.next_set()
        while grp:
            for task in grp:
                run_task(self.ctx, task)
            grp = self.task_manager.next_set()

class ParallelRunner(object):
    def __init__(self, ctx, task_manager, maxjobs=1):
        self.njobs = maxjobs
        self.task_manager = task_manager
        self.ctx = ctx

        self.worker_queue = queue.Queue()
        self.error_out = queue.Queue()
        self.failure_lock = threading.Lock()
        self.stop = False

    def start(self):
        def _worker():
            # XXX: this whole thing is an hack - find a better way to
            # notify task execution failure to all worker threads
            while not self.stop:
                task = self.worker_queue.get()
                try:
                    run_task(self.ctx, task)
                except yaku.errors.TaskRunFailure:
                    e = get_exception()
                    self.failure_lock.acquire()
                    self.stop = True
                    self.failure_lock.release()
                    task.error_msg = e.explain
                    task.error_cmd = e.cmd
                    self.error_out.put(task)
                except Exception:
                    e = get_exception()
                    exc_type, exc_value, tb = sys.exc_info()
                    lines = traceback.format_exception(exc_type, exc_value, tb)
                    self.failure_lock.acquire()
                    self.stop = True
                    self.failure_lock.release()
                    task.error_msg = "".join(lines)
                    task.error_cmd = []
                    self.error_out.put(task)
                self.worker_queue.task_done()

        for i in range(self.njobs):
            t = threading.Thread(target=_worker)
            t.setDaemon(True)
            t.start()

    def run(self):
        grp = self.task_manager.next_set()
        while grp:
            for task in grp:
                self.worker_queue.put(task)
            # XXX: we only join once we detect the worker queue to be empty, to
            # avoid blocking for a long time. This is naive, and will break if
            # the worker_queue is filled after this point
            while not self.stop:
                if self.worker_queue.empty():
                    self.worker_queue.join()
                    break
            if not self.error_out.empty():
                task = self.error_out.get()
                msg = task.error_msg
                cmd = task.error_cmd
                raise yaku.errors.TaskRunFailure(cmd, msg)

            grp = self.task_manager.next_set()

########NEW FILE########
__FILENAME__ = sysconfig
import os
import sys
import distutils.unixccompiler
import distutils.ccompiler
import distutils.sysconfig
import distutils.command.build_ext as build_ext
import distutils.dist as dist

from distutils \
    import \
        errors
from distutils \
    import \
        sysconfig

DEFAULT_COMPILERS = {
        "win32": [None, "mingw32"],
        "default": [None]
}

def _mingw32_cc():
    compiler_type = "mingw32"
    compiler = distutils.ccompiler.new_compiler(compiler=compiler_type)
    return compiler.compiler_so

def detect_distutils_cc(ctx):
    if not sys.platform in DEFAULT_COMPILERS:
        plat = "default"
    else:
        plat = sys.platform

    sys.stderr.write("Detecting distutils C compiler... ")
    compiler_type = \
            distutils.ccompiler.get_default_compiler()
    if sys.platform == "win32":
        if compiler_type == "msvc":
            try:
                compiler = distutils.ccompiler.new_compiler(
                        compiler="msvc")
                compiler.initialize()
                cc = [compiler.cc]
            except errors.DistutilsPlatformError:
                cc = _mingw32_cc()
        else:
            cc = _mingw32_cc()
    else:
        cc = distutils.sysconfig.get_config_var("CC")
        # FIXME: use shlex for proper escaping handling
        cc = cc.split()
    sys.stderr.write("%s\n" % compiler_type)
    return cc

# XXX: unixccompiler instances are the only classes where we can hope
# to get semi-sensical data. Reusing them also makes transition easier
# for packagers, as their compilation options will be reused.
# OTOH, having a sane tooling system will make customization much
# easier.
def get_configuration(compiler_type=None):
    plat = os.name
    if compiler_type is None:
        compiler_type = distutils.ccompiler.get_default_compiler(plat)

    if not compiler_type in distutils.ccompiler.compiler_class:
        raise ValueError("compiler type %s is not recognized" %
                         compiler_type)

    env = {"CC": [],
        "CPPPATH": [],
        "BASE_CFLAGS": [],
        "OPT": [],
        "SHARED": [],
        "CFLAGS": [],
        "SHLINK": [],
        "LDFLAGS": [],
        "LIBDIR": [],
        "LIBS": [],
        "SO": ""}

    env["CPPPATH"].append(sysconfig.get_python_inc())
    if compiler_type == "unix":
        env["CC"].extend(sysconfig.get_config_var("CC").split(" "))
        env["BASE_CFLAGS"].extend(sysconfig.get_config_var("BASECFLAGS").split(" "))
        env["OPT"].extend(sysconfig.get_config_var("OPT").split(" "))
        env["SHARED"].extend(sysconfig.get_config_var("CCSHARED").split(" "))

        env["SHLINK"] = sysconfig.get_config_var("LDSHARED").split(" ")
        env["SO"] = sysconfig.get_config_var("SO")
        env["LDFLAGS"] = sysconfig.get_config_var("LDFLAGS").split()
        if "-pthread" in sysconfig.get_config_var("LDFLAGS"):
            env["LDFLAGS"].insert(0, "-pthread")
        env["CFLAGS"].extend(sysconfig.get_config_var("CFLAGS").split(" "))
        env["FRAMEWORKS"] = []
        setup_unix(env)
    elif compiler_type == "msvc":
        setup_msvc(env)
    elif compiler_type == "mingw32":
        setup_mingw32(env)
    else:
        raise ValueError("Gne ?")

    return env

def _get_ext_library_dirs():
    binst = build_ext.build_ext(dist.Distribution())
    binst.initialize_options()
    binst.finalize_options()
    return binst.library_dirs

def _get_ext_libraries(compiler):
    binst = build_ext.build_ext(dist.Distribution())
    binst.compiler = compiler
    binst.initialize_options()
    binst.finalize_options()
    class _FakeExt(object):
        def __init__(self):
            self.libraries = []
    return binst.get_libraries(_FakeExt())

def setup_unix(env):
    if sys.platform == "darwin":
        env["LDFLAGS"].extend(["-bundle", "-undefined", "dynamic_lookup"])

        def _strip_arch(flag):
            value = env[flag]
            while "-arch" in value:
                id = value.index("-arch")
                value.pop(id)
                value.pop(id)
            return value
        for flag in ["BASE_CFLAGS", "LDFLAGS"]:
            env[flag] = _strip_arch(flag)

def setup_msvc(env):
    compiler = distutils.ccompiler.new_compiler(
            compiler="msvc")
    compiler.initialize()

    env["CC"] = compiler.cc
    env["BASE_CFLAGS"].extend(compiler.compile_options)

    env["SHLINK"] = compiler.linker
    env["SO"] = ".pyd"
    env["LDFLAGS"] = compiler.ldflags_shared
    env["LIBDIR"].extend( _get_ext_library_dirs())

def setup_mingw32(env):
    compiler = distutils.ccompiler.new_compiler(
            compiler="mingw32")

    env["CC"] = ["gcc"]
    env["BASE_CFLAGS"].extend(["-mno-cygwin"])

    env["SHLINK"] = ["gcc", "-mno-cygwin", "-shared"]
    env["SO"] = ".pyd"
    #env["LDFLAGS"] = compiler.ldflags_shared
    env["LIBDIR"].extend( _get_ext_library_dirs())

    libs = _get_ext_libraries(compiler)
    libs += compiler.dll_libraries 
    env["LIBS"].extend(libs)

########NEW FILE########
__FILENAME__ = task
import os
import sys
try:
    from hashlib import md5
except ImportError:
    from md5 import md5
import subprocess

if sys.version_info[0] < 3:
    from cPickle \
        import \
            dumps
else:
    from pickle \
        import \
            dumps

from yaku.pprint \
    import \
        pprint
from yaku.utils \
    import \
        get_exception, is_string, function_code
from yaku.errors \
    import \
        TaskRunFailure, WindowsError

# TODO:
#   - factory for tasks, so that tasks can be created from strings
#   instead of import (import not extensible)

# Task factory - taken from waf, to avoid metaclasses magic
class _TaskFakeMetaclass(type):
    def __init__(cls, name, bases, d):
        super(_TaskFakeMetaclass, cls).__init__(name, bases, d)
        name = cls.__name__

_CLASSES = {}
def task_factory(name):
    global _CLASSES
    try:
        klass = _CLASSES[name]
    except KeyError:
        klass = _TaskFakeMetaclass('%sTask' % name, (_Task,), {})
        klass.name = name
        _CLASSES[name] = klass
    return klass

base = _TaskFakeMetaclass('__task_base', (object,), {})

class _Task(object):
    before = []
    after = []
    def __init__(self, outputs, inputs, func=None, deps=None, env=None, env_vars=None):
        if is_string(inputs):
            self.inputs = [inputs]
        else:
            self.inputs = inputs
        if is_string(outputs):
            self.outputs = [outputs]
        else:
            self.outputs = outputs
        self.uid = None
        self.func = func
        if deps is None:
            self.deps = []
        else:
            self.deps = deps
        self.cache = None
        self.env = env
        self.env_vars = env_vars
        self.scan = None
        self.disable_output = False
        self.log = None

    # UID and signature functionalities
    #----------------------------------
    def get_uid(self):
        if self.uid is None:
            m = md5()
            up = m.update
            up(self.__class__.__name__.encode())
            for x in self.inputs + self.outputs:
                up(x.abspath().encode())
            self.uid = m.digest()
        return self.uid

    def signature(self):
        if self.cache is None:
            sig = self._signature()
            self.cache = sig
            return sig
        else:
            return self.cache

    def _signature(self):
        m = md5()

        self._sig_explicit_deps(m)
        for k in self.env_vars:
            m.update(dumps(self.env[k]))
        if self.func:
            m.update(function_code(self.func).co_code)
        return m.digest()

    def _sig_explicit_deps(self, m):
        for s in self.inputs + self.deps:
            #if os.path.exists(s):
            #    m.update(open(s).read())
            m.update(s.read(flags="rb"))
        return m.digest()
        
    # execution
    #----------
    def run(self):
        self.func(self)

    def exec_command(self, cmd, cwd, env=None):
        if cwd is None:
            cwd = self.gen.bld.bld_root.abspath()
        kw = {}
        if env is not None:
            kw["env"] = env
        if not self.disable_output:
            if self.env["VERBOSE"]:
                pprint('GREEN', " ".join([str(c) for c in cmd]))
            else:
                pprint('GREEN', "%-16s%s" % (self.name.upper(), " ".join([i.bldpath() for i in self.inputs])))

        self.gen.bld.set_cmd_cache(self, cmd)
        try:
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT, cwd=cwd, **kw)
            stdout = p.communicate()[0].decode("utf-8")
            if p.returncode:
                raise TaskRunFailure(cmd, stdout)
            if sys.version_info >= (3,):
                stdout = stdout
            else:
                stdout = stdout.encode("utf-8")
            if self.disable_output:
                self.log.write(stdout)
            else:
                sys.stderr.write(stdout)
            self.gen.bld.set_stdout_cache(self, stdout)
        except OSError:
            e = get_exception()
            raise TaskRunFailure(cmd, str(e))
        except WindowsError:
            e = get_exception()
            raise TaskRunFailure(cmd, str(e))

    def __repr__(self):
        ins = ",".join([i.name for i in self.inputs])
        outs = ",".join([i.name for i in self.outputs])
        return "'%s: %s -> %s'" % (self.name, ins, outs)

########NEW FILE########
__FILENAME__ = task_manager
import os

from yaku.environment \
    import \
        Environment

RULES_REGISTRY = {}
FILES_REGISTRY = {}

def extension(ext):
    def _f(f):
        RULES_REGISTRY[ext] = f
        return f
    return _f

def set_extension_hook(ext, hook):
    old = RULES_REGISTRY.get(ext, None)
    RULES_REGISTRY[ext] = hook
    return old

def wrap_extension_hook(ext, hook_factory):
    old = RULES_REGISTRY.get(ext, None)
    RULES_REGISTRY[ext] = hook_factory(old)
    return old

def get_extension_hook(ext):
    try:
        return RULES_REGISTRY[ext]
    except KeyError:
        raise ValueError("No hook registered for extension %r" % ext)

def set_file_hook(ctx, source, hook):
    node = ctx.src_root.find_resource(source)
    if node is None:
        raise IOError("file %s not found" % source)
    FILES_REGISTRY[node] = hook

class NoHookException(Exception):
    pass

def hash_task(t):
    # FIXME: ext_in and ext_out should not be computed from the files
    ext_in = [os.path.splitext(s.name)[1] for s in t.inputs]
    ext_out = [os.path.splitext(s.name)[1] for s in t.outputs]
    tup = tuple(ext_in + ext_out + t.before + t.after)
    return hash((t.__class__.__name__, tup))

class TaskManager(object):
    def __init__(self, tasks):
        self.tasks = tasks

        self.groups = {}
        self.order = {}
        self.make_groups()
        self.make_order()

    def set_order(self, a, b):
        if not a in self.order:
            self.order[a] = set()
        self.order[a].add(b)

    def make_order(self):
        keys = list(self.groups.keys())
        max = len(keys)
        for i in range(max):
            t1 = self.groups[keys[i]][0]
            for j in range(i + 1, max):
                t2 = self.groups[keys[j]][0]

                if t2.__class__.__name__ in t1.before:
                    self.set_order(keys[j], keys[i])
                elif t1.__class__.__name__ in t2.before:
                    self.set_order(keys[i], keys[j])
                else:
                    # add the constraints based on the comparisons
                    val = self.compare_exts(t1, t2)
                    if val > 0:
                        self.set_order(keys[i], keys[j])
                    elif val < 0:
                        self.set_order(keys[j], keys[i])

    def make_groups(self):
        # XXX: we assume tasks with same input/output suffix can run
        # in // (naive emulation of csr-like scheduler in waf)
        groups = self.groups
        for t in self.tasks:
            h = hash_task(t)
            if h in groups:
                groups[h].append(t)
            else:
                groups[h] = [t]

    def next_set(self):
        keys = self.groups.keys()

        unconnected = []
        remainder = []

        for u in keys:
            for k in self.order.values():
                if u in k:
                    remainder.append(u)
                    break
            else:
                unconnected.append(u)

        toreturn = []
        for y in unconnected:
            toreturn.extend(self.groups[y])

        # remove stuff only after
        for y in unconnected:
                try:
                    self.order.__delitem__(y)
                except KeyError:
                    pass
                self.groups.__delitem__(y)

        if not toreturn and remainder:
            raise Exception("circular order constraint detected %r" % remainder)

        return toreturn


    def compare_exts(self, t1, t2):
        "extension production"
        def _get_in(t):
            return [os.path.splitext(s.name)[1] for s in t.inputs]
        def _get_out(t):
            return [os.path.splitext(s.name)[1] for s in t.outputs]

        in_ = _get_in(t1)
        out_ = _get_out(t2)
        for k in in_:
            if k in out_:
                return -1
        in_ = _get_in(t2)
        out_ = _get_out(t1)
        for k in in_:
            if k in out_:
                return 1
        return 0

def run_task(ctx, task):
    def _run(t):
        t.run()
        ctx.cache[tuid] = t.signature()

    tuid = task.get_uid()
    # XXX: there may be a better way to do this without stating output
    # (we want to know if the task has already been executed in a
    # previous run)
    for o in task.outputs:
        if not os.path.exists(o.abspath()):
            _run(task)
            break
    if not tuid in ctx.cache:
        _run(task)
    else:
        sig = task.signature()
        if sig != ctx.cache[tuid]:
            _run(task)

def build_dag(tasks):
    # Build dependency graph (DAG)
    # task_deps[target] = list_of_dependencies
    # At this point, task_deps is not guaranteed to be a DAG (may have
    # cycle) - will be detected during topological sort
    task_deps = {}
    output_to_tuid = {}
    for t in tasks:
        for o in t.outputs:
            try:
                task_deps[o].extend(t.inputs + t.deps)
            except KeyError:
                task_deps[o] = t.inputs[:] + t.deps[:]
            output_to_tuid[o] = t.get_uid()
    return task_deps, output_to_tuid

def topo_sort(task_deps):
    # Topological sort (depth-first search)
    # XXX: cycle detection is missing
    tmp = []
    nodes = []
    for dep in task_deps.values():
        nodes.extend(dep)
    nodes.extend(task_deps.keys())
    nodes = set(nodes)

    visited = set()
    def visit(node):
        if not node in visited:
           visited.add(node)
           deps = task_deps.get(node, None)
           if deps:
               for c in deps:
                   visit(c)
           tmp.append(node)

    for node in nodes:
        visit(node)

    return tmp

def _get_hook(source):
    if source in FILES_REGISTRY:
        return FILES_REGISTRY[source]
    for ext in RULES_REGISTRY:
        if source.name.endswith(ext):
            return RULES_REGISTRY[ext]
    raise NoHookException(
            "No rule defined for extension %r" % source.suffix())

class TaskGen(object):
    def __init__(self, name, bld, sources, target):
        self.bld = bld
        self.name = name
        self.sources = sources
        self.target = target

        self.env = Environment()

    def process(self):
        tasks = []
        for s in self.sources:
            f = _get_hook(s)
            tsks = f(self, s)
            tasks.extend(tsks)
        return tasks

class CompiledTaskGen(TaskGen):
    def __init__(self, name, bld, sources, target):
        TaskGen.__init__(self, name, bld, sources, target)
        self.object_tasks = []
        self.link_task = None
        self.has_cxx = False

    def add_objects(self, tasks):
        """Add new object tasks, assuming the link task has already
        been defined."""
        if self.link_task is None:
            raise ValueError("You cannot add object nodes "
                             "before setting the link task !")
        else:
            self.object_tasks += tasks

def order_tasks(tasks):
    tuid_to_task = dict([(t.get_uid(), t) for t in tasks])

    task_deps, output_to_tuid = build_dag(tasks)

    yo = topo_sort(task_deps)
    ordered_tasks = []
    for output in yo:
        if output in output_to_tuid:
            ordered_tasks.append(tuid_to_task[output_to_tuid[output]])

    return ordered_tasks


########NEW FILE########
__FILENAME__ = test_basic
from yaku.tests.test_helpers \
    import \
        TmpContextBase
from yaku.context \
    import \
        get_cfg, get_bld
from yaku.conftests \
    import \
        check_compiler

class ContextTest(TmpContextBase):
    def test_load_store_simple(self):
        """Test one can load and store the contexts."""
        ctx = get_cfg()
        ctx.store()

        ctx = get_bld()
        ctx.store()

class SimpleCCTest(TmpContextBase):
    def test_compiler(self):
        ctx = get_cfg()
        ctx.store()

        ctx = get_bld()
        ctx.store()


########NEW FILE########
__FILENAME__ = test_helpers
import os
import tempfile
import shutil
from unittest import TestCase

class TmpContextBase(TestCase):
    def setUp(self):
        self.cwd = os.getcwd()
        self.d = tempfile.mkdtemp()
        os.chdir(self.d)

    def tearDown(self):
        shutil.rmtree(self.d)
        os.chdir(self.cwd)

########NEW FILE########
__FILENAME__ = ar
import yaku.utils

def setup(ctx):
    env = ctx.env

    ctx.env["STLINK"] = ["ar"]
    ctx.env["STLINK_TGT_F"] = []
    ctx.env["STLINK_SRC_F"] = []
    ctx.env["STLINKFLAGS"] = ["rcs"]
    ctx.env["STATICLIB_FMT"] = "lib%s.a"

def detect(ctx):
    if yaku.utils.find_program("ar") is None:
        return False
    else:
        return True

########NEW FILE########
__FILENAME__ = cc
def detect(ctx):
    env = ctx.env

    ctx.env["CC"] = ["cc"]
    ctx.env["CC_TGT_F"] = ["-c", "-o"]
    ctx.env["CC_SRC_F"] = []
    ctx.env["CFLAGS"] = []
    ctx.env["CPPPATH"] = []
    ctx.env["CPPPATH_FMT"] = "-I%s"
    ctx.env["LINK"] = ["cc"]
    ctx.env["LINKFLAGS"] = []
    ctx.env["LIBS"] = []
    ctx.env["LIB_FMT"] = "-l%s"
    ctx.env["LIBDIR_FMT"] = "-L%s"

########NEW FILE########
__FILENAME__ = clang
import yaku.utils

def setup(ctx):
    env = ctx.env

    ctx.env["CC"] = ["clang"]
    ctx.env["CC_TGT_F"] = ["-c", "-o"]
    ctx.env["CC_SRC_F"] = []
    ctx.env["CFLAGS"] = []
    ctx.env["DEFINES"] = []
    ctx.env["LINK"] = ["clang"]
    ctx.env["LINKFLAGS"] = []
    ctx.env["LINK_TGT_F"] = ["-o"]
    ctx.env["LINK_SRC_F"] = []
    ctx.env["SHAREDLIB_FMT"] = "lib%s.so"
    ctx.env["SHLINK"] = ["clang"]
    ctx.env["SHLINKFLAGS"] = []
    ctx.env["SHLINK_TGT_F"] = ["-o"]
    ctx.env["SHLINK_SRC_F"] = []
    ctx.env["MODLINK"] = ["clang", "-bundle", "-undefined", "dynamic_lookup"]
    ctx.env["MODLINKFLAGS"] = []
    ctx.env["MODLINK_TGT_F"] = ["-o"]
    ctx.env["MODLINK_SRC_F"] = []
    ctx.env["CPPPATH"] = []
    ctx.env["CPPPATH_FMT"] = "-I%s"
    ctx.env["LIBDIR"] = []
    ctx.env["LIBS"] = []
    ctx.env["LIB_FMT"] = "-l%s"
    ctx.env["LIBDIR_FMT"] = "-L%s"

    ctx.env["CC_OBJECT_FMT"] = "%s.o"
    ctx.env["PROGRAM_FMT"] = "%s"

    ctx.env["CXX"] = ["clang"]
    ctx.env["CXX_TGT_F"] = ["-c", "-o"]
    ctx.env["CXX_SRC_F"] = []
    ctx.env["CXXFLAGS"] = []
    ctx.env["CXXLINK"] = ["clang"]
    ctx.env["CXXLINKFLAGS"] = []
    ctx.env["CXXLINK_TGT_F"] = ["-o"]
    ctx.env["CXXLINK_SRC_F"] = []
    ctx.env["CXXSHLINK"] = ["clang"]
    ctx.env["CXXSHLINKFLAGS"] = []
    ctx.env["CXXSHLINK_TGT_F"] = ["-o"]
    ctx.env["CXXSHLINK_SRC_F"] = []
    ctx.env["CPPPATH"] = []
    ctx.env["CPPPATH_FMT"] = "-I%s"
    ctx.env["LIBDIR"] = []
    ctx.env["LIBS"] = []
    ctx.env["FRAMEWORKS"] = []
    ctx.env["LIB_FMT"] = "-l%s"
    ctx.env["LIBDIR_FMT"] = "-L%s"

    ctx.env["CXX_OBJECT_FMT"] = "%s.o"
    ctx.env["PROGRAM_FMT"] = "%s"

def detect(ctx):
    if yaku.utils.find_program("clang") is None:
        return False
    else:
        return True

########NEW FILE########
__FILENAME__ = ctasks
import sys
import os
import copy

import yaku.tools

from yaku.task \
    import \
        task_factory
from yaku.task_manager \
    import \
        extension, CompiledTaskGen, set_extension_hook
from yaku.utils \
    import \
        find_deps, ensure_dir
from yaku.compiled_fun \
    import \
        compile_fun
from yaku.errors \
    import \
        TaskRunFailure
from yaku.scheduler \
    import \
        run_tasks
from yaku.conf \
    import \
        with_conf_blddir
from yaku._config \
    import \
        _OUTPUT
import yaku.tools

ccompile, cc_vars = compile_fun("cc", "${CC} ${CFLAGS} ${APP_DEFINES} ${INCPATH} ${CC_TGT_F}${TGT[0].abspath()} ${CC_SRC_F}${SRC}", False)

shccompile, sgcc_vars = compile_fun("cc", "${CC} ${CFLAGS} ${CFLAGS_SH} ${APP_DEFINES} ${INCPATH} ${CC_TGT_F}${TGT[0].abspath()} ${CC_SRC_F}${SRC}", False)

ccprogram, ccprogram_vars = compile_fun("ccprogram", "${LINK} ${LINK_TGT_F}${TGT[0].abspath()} ${LINK_SRC_F}${SRC} ${APP_LIBDIR} ${APP_LIBS} ${LINKFLAGS}", False)

cshlink, cshlink_vars = compile_fun("cshlib", "${SHLINK} ${APP_LIBDIR} ${APP_LIBS} ${SHLINK_TGT_F}${TGT[0].abspath()} ${SHLINK_SRC_F}${SRC} ${SHLINKFLAGS}", False)

clink, clink_vars = compile_fun("clib", "${STLINK} ${STLINKFLAGS} ${STLINK_TGT_F}${TGT[0].abspath()} ${STLINK_SRC_F}${SRC}", False)

@extension('.c')
def c_hook(self, node):
    tasks = ccompile_task(self, node)
    self.object_tasks.extend(tasks)
    return tasks

def ccompile_task(self, node):
    base = self.env["CC_OBJECT_FMT"] % node.name
    target = node.parent.declare(base)
    ensure_dir(target.abspath())

    task = task_factory("cc")(inputs=[node], outputs=[target], func=ccompile, env=self.env)
    task.gen = self
    task.env_vars = cc_vars
    return [task]

def shared_c_hook(self, node):
    tasks = shared_ccompile_task(self, node)
    self.object_tasks.extend(tasks)
    return tasks

def shared_ccompile_task(self, node):
    base = self.env["CC_OBJECT_FMT"] % node.name
    target = node.parent.declare(base)
    ensure_dir(target.abspath())

    task = task_factory("shcc")(inputs=[node], outputs=[target], func=shccompile, env=self.env)
    task.gen = self
    task.env_vars = cc_vars
    return [task]

def shlink_task(self, name):
    objects = [tsk.outputs[0] for tsk in self.object_tasks]

    folder, base = os.path.split(name)
    tmp = folder + os.path.sep + self.env["SHAREDLIB_FMT"] % base
    target = self.bld.path.declare(tmp)
    ensure_dir(target.abspath())

    task = task_factory("cc_shlink")(inputs=objects, outputs=[target], func=cshlink, env=self.env)
    task.gen = self
    task.env_vars = cshlink_vars
    return [task]

def static_link_task(self, name):
    objects = [tsk.outputs[0] for tsk in self.object_tasks]

    folder, base = os.path.split(name)
    tmp = folder + os.path.sep + self.env["STATICLIB_FMT"] % base
    target = self.bld.path.declare(tmp)
    ensure_dir(target.abspath())

    task = task_factory("cc_stlink")(inputs=objects, outputs=[target], func=clink, env=self.env)
    task.gen = self
    task.env_vars = clink_vars
    return [task]

def ccprogram_task(self, name):
    objects = [tsk.outputs[0] for tsk in self.object_tasks]
    def declare_target():
        folder, base = os.path.split(name)
        tmp = folder + os.path.sep + self.env["PROGRAM_FMT"] % base
        return self.bld.path.declare(tmp)
    target = declare_target()
    ensure_dir(target.abspath())

    task = task_factory("cc_program")(inputs=objects, outputs=[target], func=ccprogram, env=self.env)
    task.gen = self
    task.env_vars = ccprogram_vars
    return [task]

def apply_define(task_gen):
    defines = task_gen.env["DEFINES"]
    task_gen.env["APP_DEFINES"] = [task_gen.env["DEFINES_FMT"] % p for p in defines]

def apply_cpppath(task_gen):
    cpppaths = task_gen.env["CPPPATH"]
    implicit_paths = set([s.parent.srcpath() \
                          for s in task_gen.sources])
    srcnode = task_gen.sources[0].ctx.srcnode

    relcpppaths = []
    for p in cpppaths:
        if not os.path.isabs(p):
            node = srcnode.find_node(p)
            assert node is not None, "could not find %s" % p
            relcpppaths.append(node.bldpath())
        else:
            relcpppaths.append(p)
    cpppaths = list(implicit_paths) + relcpppaths
    task_gen.env["INCPATH"] = [
            task_gen.env["CPPPATH_FMT"] % p
            for p in cpppaths]

def apply_libs(task_gen):
    libs = task_gen.env["LIBS"]
    task_gen.env["APP_LIBS"] = [
            task_gen.env["LIB_FMT"] % lib for lib in libs]

def apply_libdir(task_gen):
    libdir = task_gen.env["LIBDIR"]
    task_gen.env["APP_LIBDIR"] = [
            task_gen.env["LIBDIR_FMT"] % d for d in libdir]

class CCBuilder(yaku.tools.Builder):
    def clone(self):
        return CCBuilder(self.ctx)

    def __init__(self, ctx):
        yaku.tools.Builder.__init__(self, ctx)

    def _compile(self, task_gen, name):
        apply_define(task_gen)
        apply_cpppath(task_gen)

        tasks = task_gen.process()
        for t in tasks:
            t.env = task_gen.env
        return tasks

    def compile(self, name, sources, env=None):
        sources = self.to_nodes(sources)
        task_gen = CompiledTaskGen("cccompile", self.ctx, sources, name)
        task_gen.env = yaku.tools._merge_env(self.env, env)
        tasks = self._compile(task_gen, name)
        self.ctx.tasks.extend(tasks)

        outputs = []
        for t in tasks:
            outputs.extend(t.outputs)
        return outputs

    def try_compile(self, name, body, headers=None):
        return with_conf_blddir(self.ctx, name, body,
                                lambda : yaku.tools.try_task_maker(self.ctx, self._compile, name, body, headers))

    def try_compile_no_blddir(self, name, body, headers=None, env=None):
        return yaku.tools.try_task_maker(self.ctx, self._compile, name, body, headers, env)

    def static_library(self, name, sources, env=None):
        sources = self.to_nodes(sources)
        task_gen = CompiledTaskGen("ccstaticlib", self.ctx, sources, name)
        task_gen.env = yaku.tools._merge_env(self.env, env)

        tasks = self._static_library(task_gen, name)
        self.ctx.tasks.extend(tasks)
        outputs = []
        for t in task_gen.link_task:
            outputs.extend(t.outputs)
        return outputs

    def _static_library(self, task_gen, name):
        apply_define(task_gen)
        apply_cpppath(task_gen)
        apply_libdir(task_gen)
        apply_libs(task_gen)

        tasks = task_gen.process()
        ltask = static_link_task(task_gen, name)
        tasks.extend(ltask)
        for t in tasks:
            t.env = task_gen.env
        task_gen.link_task = ltask
        return tasks

    def try_static_library(self, name, body, headers=None):
        return with_conf_blddir(self.ctx, name, body,
                                lambda : yaku.tools.try_task_maker(self.ctx, self._static_library, name, body, headers))

    def try_static_library_no_blddir(self, name, body, headers=None, env=None):
        return yaku.tools.try_task_maker(self.ctx, self._static_library, name, body, headers, env)

    def shared_library(self, name, sources, env=None):
        sources = self.to_nodes(sources)
        task_gen = CompiledTaskGen("ccsharedlib", self.ctx, sources, name)
        task_gen.env = yaku.tools._merge_env(self.env, env)

        tasks = self._shared_library(task_gen, name)
        self.ctx.tasks.extend(tasks)
        outputs = []
        for t in task_gen.link_task:
            outputs.extend(t.outputs)
        return outputs

    def _shared_library(self, task_gen, name):
        old_hook = set_extension_hook(".c", shared_c_hook)

        apply_define(task_gen)
        apply_cpppath(task_gen)
        apply_libdir(task_gen)
        apply_libs(task_gen)

        tasks = task_gen.process()
        ltask = shlink_task(task_gen, name)
        tasks.extend(ltask)
        for t in tasks:
            t.env = task_gen.env
        task_gen.link_task = ltask

        set_extension_hook(".c", old_hook)
        return tasks

    def try_shared_library(self, name, body, headers=None):
        return with_conf_blddir(self.ctx, name, body,
                                lambda : yaku.tools.try_task_maker(self.ctx, self._shared_library, name, body, headers))

    def try_shared_library_no_blddir(self, name, body, headers=None, env=None):
        return yaku.tools.try_task_maker(self.ctx, self._shared_library, name, body, headers, env)

    def program(self, name, sources, env=None):
        sources = self.to_nodes(sources)
        task_gen = CompiledTaskGen("ccprogram", self.ctx,
                                   sources, name)
        task_gen.env = yaku.tools._merge_env(self.env, env)
        tasks = self._program(task_gen, name)

        self.ctx.tasks.extend(tasks)
        outputs = []
        for t in task_gen.link_task:
            outputs.extend(t.outputs)
        return outputs

    def _program(self, task_gen, name):
        apply_define(task_gen)
        apply_cpppath(task_gen)
        apply_libdir(task_gen)
        apply_libs(task_gen)

        tasks = task_gen.process()
        ltask = ccprogram_task(task_gen, name)
        tasks.extend(ltask)
        for t in tasks:
            t.env = task_gen.env
        task_gen.link_task = ltask
        return tasks

    def try_program(self, name, body, headers=None, env=None):
        return with_conf_blddir(self.ctx, name, body,
                                lambda : yaku.tools.try_task_maker(self.ctx, self._program, name, body, headers, env))

    def try_program_no_blddir(self, name, body, headers=None, env=None):
        return yaku.tools.try_task_maker(self.ctx, self._program, name, body, headers, env)

    def configure(self, candidates=None):
        ctx = self.ctx
        if candidates is None:
            if sys.platform == "win32":
                candidates = ["msvc", "gcc"]
            else:
                candidates = ["gcc", "cc"]

        def _detect_cc():
            detected = None
            sys.path.insert(0, os.path.dirname(yaku.tools.__file__))
            try:
                for cc_type in candidates:
                    _OUTPUT.write("Looking for %s (c compiler) ... " % cc_type)
                    try:
                        mod = __import__(cc_type)
                        if mod.detect(ctx):
                            _OUTPUT.write("yes\n")
                            ctx.env["cc_type"] = cc_type
                            detected = cc_type
                            break
                    except:
                        pass
                    _OUTPUT.write("no!\n")
                return detected
            finally:
                sys.path.pop(0)

        cc_type = _detect_cc()
        if cc_type is None:
            raise ValueError("No C compiler found!")
        cc = ctx.load_tool(cc_type)
        cc.setup(ctx)

        if sys.platform != "win32":
            ar = ctx.load_tool("ar")
            ar.setup(ctx)

        ctx.start_message("Checking whether %s can build objects" % cc_type)
        if self.try_compile("foo", "int foo() {return 0;}"):
            ctx.end_message("yes")
        else:
            ctx.end_message("no")
            ctx.fail_configuration("")
        ctx.start_message("Checking whether %s can build programs" % cc_type)
        if self.try_program("foo", "int main() {return 0;}"):
            ctx.end_message("yes")
        else:
            ctx.end_message("no")
            ctx.fail_configuration("")
        ctx.start_message("Checking whether %s can build static libraries" % cc_type)
        if self.try_static_library("foo", "int foo() {return 0;}"):
            ctx.end_message("yes")
        else:
            ctx.end_message("no")
            ctx.fail_configuration("")
        ctx.start_message("Checking whether %s can link static libraries to exe" % cc_type)
        def f():
            assert self.try_static_library_no_blddir("foo", "int foo() { return 0;}")
            if self.try_program_no_blddir("exe", "int foo(); int main() { return foo();}",
                                          env={"LIBS": ["foo"]}):
                ctx.end_message("yes")
            else:
                ctx.end_message("no")
                ctx.fail_configuration("")
        with_conf_blddir(self.ctx, "exelib", "checking static link", f)

        shared_code = """\
#ifdef _MSC_VER
#define __YAKU_DLL_MARK __declspec(dllexport)
#else
#define __YAKU_DLL_MARK
#endif

__YAKU_DLL_MARK int foo()
{
    return 0;
}
"""
        ctx.start_message("Checking whether %s can build shared libraries" % cc_type)
        if self.try_shared_library("foo", shared_code):
            ctx.end_message("yes")
        else:
            ctx.end_message("no")
            ctx.fail_configuration("")

        ctx.start_message("Checking whether %s can link shared libraries to exe" % cc_type)
        def f():
            assert self.try_shared_library_no_blddir("foo", shared_code)
            if self.try_program_no_blddir("exe", "int foo(); int main() { return foo();}",
                                          env={"LIBS": ["foo"]}):
                ctx.end_message("yes")
            else:
                ctx.end_message("no")
                ctx.fail_configuration("")
        with_conf_blddir(self.ctx, "exeshlib", "checking shared link", f)
        self.configured = True

def get_builder(ctx):
    return CCBuilder(ctx)

########NEW FILE########
__FILENAME__ = cxxtasks
import sys
import os
import copy

import yaku.tools

from yaku.task \
    import \
        task_factory
from yaku.task_manager \
    import \
        extension, CompiledTaskGen
from yaku.utils \
    import \
        find_deps, ensure_dir, get_exception
from yaku.compiled_fun \
    import \
        compile_fun
from yaku.tools.ctasks \
    import \
        apply_cpppath, apply_libdir, apply_libs, apply_define
import yaku.tools

cxxcompile, cxx_vars = compile_fun("cxx", "${CXX} ${CXXFLAGS} ${INCPATH} ${APP_DEFINES} ${CXX_TGT_F}${TGT[0].abspath()} ${CXX_SRC_F}${SRC}", False)

cxxprogram, cxxprogram_vars = compile_fun("cxxprogram", "${CXXLINK} ${CXXLINK_TGT_F}${TGT[0].abspath()} ${CXXLINK_SRC_F}${SRC} ${APP_LIBDIR} ${APP_LIBS} ${CXXLINKFLAGS}", False)

@extension('.cxx')
def cxx_hook(self, node):
    tasks = cxxcompile_task(self, node)
    self.object_tasks.extend(tasks)
    return tasks

def cxxcompile_task(self, node):
    base = self.env["CXX_OBJECT_FMT"] % node.name
    target = node.parent.declare(base)
    ensure_dir(target.abspath())

    task = task_factory("cxx")(inputs=[node], outputs=[target])
    task.gen = self
    task.env_vars = cxx_vars
    #print find_deps("foo.c", ["."])
    #task.scan = lambda : find_deps(node, ["."])
    #task.deps.extend(task.scan())
    task.env = self.env
    task.func = cxxcompile
    return [task]

def cxxprogram_task(self, name):
    objects = [tsk.outputs[0] for tsk in self.object_tasks]
    def declare_target():
        folder, base = os.path.split(name)
        tmp = folder + os.path.sep + self.env["PROGRAM_FMT"] % base
        return self.bld.path.declare(tmp)
    target = declare_target()
    ensure_dir(target.abspath())

    task = task_factory("cxxprogram")(inputs=objects, outputs=[target])
    task.gen = self
    task.env = self.env
    task.func = cxxprogram
    task.env_vars = cxxprogram_vars
    return [task]

class CXXBuilder(yaku.tools.Builder):
    def clone(self):
        return CXXBuilder(self.ctx)

    def __init__(self, ctx):
        yaku.tools.Builder.__init__(self, ctx)

    def ccompile(self, name, sources, env=None):
        task_gen = CompiledTaskGen("cxccompile", self.ctx,
                                   sources, name)
        task_gen.env = yaku.tools._merge_env(self.env, env)
        apply_define(task_gen)
        apply_cpppath(task_gen)

        tasks = task_gen.process()
        for t in tasks:
            t.env = task_gen.env
        self.ctx.tasks.extend(tasks)

        outputs = []
        for t in tasks:
            outputs.extend(t.outputs)
        return outputs

    def program(self, name, sources, env=None):
        sources = [self.ctx.src_root.find_resource(s) for s in sources]
        task_gen = CompiledTaskGen("cxxprogram", self.ctx,
                                   sources, name)
        task_gen.env = yaku.tools._merge_env(self.env, env)
        apply_define(task_gen)
        apply_cpppath(task_gen)
        apply_libdir(task_gen)
        apply_libs(task_gen)

        tasks = task_gen.process()
        ltask = cxxprogram_task(task_gen, name)
        tasks.extend(ltask)
        for t in tasks:
            t.env = task_gen.env
        self.ctx.tasks.extend(tasks)
        self.link_task = ltask

        outputs = []
        for t in ltask:
            outputs.extend(t.outputs)
        return outputs

    def configure(self, candidates=None):
        ctx = self.ctx
        if candidates is None:
            if sys.platform == "win32":
                candidates = ["msvc", "gxx"]
            else:
                candidates = ["gxx", "cxx"]

        def _detect_cxx():
            detected = None
            sys.path.insert(0, os.path.dirname(yaku.tools.__file__))
            try:
                for cxx_type in candidates:
                    sys.stderr.write("Looking for %s (c++ compiler) ... " % cxx_type)
                    try:
                        mod = __import__(cxx_type)
                        if mod.detect(ctx):
                            sys.stderr.write("yes\n")
                            detected = cxx_type
                            break
                    except ImportError:
                        raise
                    except:
                        pass
                    sys.stderr.write("no!\n")
                return detected
            finally:
                sys.path.pop(0)

        cxx_type = _detect_cxx()
        if cxx_type is None:
            raise ValueError("No CXX compiler found!")
        cxx = ctx.load_tool(cxx_type)
        cxx.setup(ctx)

        if sys.platform != "win32":
            ar = ctx.load_tool("ar")
            ar.setup(ctx)
        self.configured = True

def get_builder(ctx):
    return CXXBuilder(ctx)

########NEW FILE########
__FILENAME__ = cython
import os
import sys

from yaku.task_manager \
    import \
        extension, get_extension_hook
from yaku.task \
    import \
        task_factory
from yaku.compiled_fun \
    import \
        compile_fun
from yaku.utils \
    import \
        ensure_dir, find_program
import yaku.errors

@extension(".pyx")
def cython_hook(self, node):
    self.sources.append(node.change_ext(".c"))
    return cython_task(self, node)

def cython_task(self, node):
    out = node.change_ext(".c")
    target = node.parent.declare(out.name)
    ensure_dir(target.name)

    task = task_factory("cython")(inputs=[node], outputs=[target])
    task.gen = self
    task.env_vars = []
    task.env = self.env

    self.env["CYTHON_INCPATH"] = ["-I%s" % p for p in
                self.env["CYTHON_CPPPATH"]]
    task.func = compile_fun("cython", "${CYTHON} ${SRC} -o ${TGT} ${CYTHON_INCPATH}",
                            False)[0]
    return [task]

class CythonBuilder(yaku.tools.Builder):
    def __init__(self, ctx):
        yaku.tools.Builder.__init__(self, ctx)

    def configure(self):
        ctx = self.ctx
        sys.stderr.write("Looking for cython... ")
        if detect(ctx):
            sys.stderr.write("yes\n")
        else:
            sys.stderr.write("no!\n")
            raise yaku.errors.ToolNotFound()
        ctx.env["CYTHON_CPPPATH"] = []
        ctx.env["CYTHON"] = [sys.executable, "-m", "cython"]
        ctx.env["ENV"]["PYTHONPATH"] = os.environ["PYTHONPATH"]

def detect(ctx):
    if find_program("cython") is None:
        return False
    else:
        return True

def get_builder(ctx):
    return CythonBuilder(ctx)

########NEW FILE########
__FILENAME__ = fortran
import os
import copy

from yaku.task_manager \
    import \
        extension, CompiledTaskGen
from yaku.task \
    import \
        task_factory
from yaku.compiled_fun \
    import \
        compile_fun
from yaku.utils \
    import \
        ensure_dir, get_exception
from yaku.tools.ctasks \
    import \
        apply_libdir
from yaku.errors \
    import \
        TaskRunFailure
from yaku.scheduler \
    import \
        run_tasks
from yaku.conf \
    import \
        with_conf_blddir, create_file, write_log
import yaku.tools

f77_compile, f77_vars = compile_fun("f77", "${F77} ${F77FLAGS} ${F77_TGT_F}${TGT[0].abspath()} ${F77_SRC_F}${SRC}", False)

fprogram, fprogram_vars = compile_fun("fprogram", "${F77_LINK} ${F77_LINK_SRC_F}${SRC} ${F77_LINK_TGT_F}${TGT[0].abspath()} ${APP_LIBDIR} ${F77_LINKFLAGS}", False)

@extension(".f")
def fortran_task(self, node):
    tasks = fcompile_task(self, node)
    return tasks

def fcompile_task(self, node):
    base = self.env["F77_OBJECT_FMT"] % node.name
    target = node.parent.declare(base)
    ensure_dir(target.abspath())

    task = task_factory("f77")(inputs=[node], outputs=[target])
    task.gen = self
    task.env_vars = f77_vars
    task.env = self.env
    task.func = f77_compile
    self.object_tasks.append(task)
    return [task]

class FortranBuilder(yaku.tools.Builder):
    def __init__(self, ctx):
        yaku.tools.Builder.__init__(self, ctx)

    def _try_task_maker(self, task_maker, name, body):
        conf = self.ctx
        code =  body
        sources = [create_file(conf, code, name, ".f")]

        task_gen = CompiledTaskGen("conf", conf, sources, name)
        task_gen.env.update(copy.deepcopy(conf.env))

        tasks = task_maker(task_gen, name)
        self.ctx.last_task = tasks[-1]

        for t in tasks:
            t.disable_output = True
            t.log = conf.log

        succeed = False
        explanation = None
        try:
            run_tasks(conf, tasks)
            succeed = True
        except TaskRunFailure:
            e = get_exception()
            explanation = str(e)

        write_log(conf, conf.log, tasks, code, succeed, explanation)
        return succeed

    def try_compile(self, name, body):
        # FIXME: temporary workaround for cyclic import between ctasks and conf
        #from yaku.conf import with_conf_blddir
        return with_conf_blddir(self.ctx, name, body,
                                lambda : self._try_task_maker(self._compile, name, body))

    def compile(self, name, sources, env=None):
        _env = copy.deepcopy(self.env)
        if env is not None:
            _env.update(env)

        task_gen = CompiledTaskGen("fcompile", self.ctx,
                                   sources, name)
        task_gen.env = _env
        tasks = self._compile(name, sources)
        self.ctx.tasks.extend(tasks)

        outputs = []
        for t in tasks:
            outputs.extend(t.outputs)
        return outputs

    def _compile(self, task_gen, name):
        tasks = task_gen.process()
        for t in tasks:
            t.env = task_gen.env
        return tasks

    def try_static_library(self, name, body):
        # FIXME: temporary workaround for cyclic import between ctasks and conf
        #from yaku.conf import with_conf_blddir
        cc_builder = self.ctx.builders["ctasks"]
        return with_conf_blddir(self.ctx, name, body,
                                lambda : self._try_task_maker(cc_builder._static_library, name, body))

    def try_program(self, name, body):
        # FIXME: temporary workaround for cyclic import between ctasks and conf
        #from yaku.conf import with_conf_blddir
        return with_conf_blddir(self.ctx, name, body,
                                lambda : self._try_task_maker(self._program, name, body))

    def program(self, name, sources, env=None):
        _env = copy.deepcopy(self.env)
        if env is not None:
            _env.update(env)

        sources = [self.ctx.src_root.find_resource(s) for s in sources]
        task_gen = CompiledTaskGen("fprogram", self.ctx,
                                   sources, name)
        task_gen.env = _env
        tasks = self._program(task_gen, name)

        self.ctx.tasks.extend(tasks)

        outputs = []
        for t in task_gen.link_task:
            outputs.extend(t.outputs)
        return outputs

    def _program(self, task_gen, name):
        apply_libdir(task_gen)

        tasks = task_gen.process()
        ltask = fprogram_task(task_gen, name)
        tasks.extend(ltask)
        for t in tasks:
            t.env = task_gen.env
        task_gen.link_task = ltask
        return tasks

    def configure(self, candidates=None):
        ctx = self.ctx
        if candidates is None:
            compiler_type = "default"
        else:
            compiler_type = candidates[0]

        if compiler_type == "default":
            fc_type = None
            for tool in ["gfortran", "g77"]:
                fc = ctx.load_tool(tool)
                if fc.detect(ctx):
                    fc_type = tool
            if fc_type is None:
                raise ValueError("No fortran compiler found")
        else:
            fc_type = compiler_type
        fc = ctx.load_tool(fc_type)
        fc.setup(ctx)
        self.configured = True

def fprogram_task(self, name):
    objects = [tsk.outputs[0] for tsk in self.object_tasks]
    def declare_target():
        folder, base = os.path.split(name)
        tmp = folder + os.path.sep + self.env["F77_PROGRAM_FMT"] % base
        return self.bld.path.declare(tmp)
    target = declare_target()
    ensure_dir(target.abspath())

    task = task_factory("fprogram")(inputs=objects, outputs=[target])
    task.gen = self
    task.env = self.env
    task.func = fprogram
    task.env_vars = fprogram_vars
    return [task]

def get_builder(ctx):
    return FortranBuilder(ctx)

def mangler(name, under, double_under, case):
    return getattr(name, case)() + under + (name.replace("_", double_under) and double_under)

########NEW FILE########
__FILENAME__ = g77
import yaku.utils

def setup(ctx):
    env = ctx.env

    ctx.env.update(
       {"F77": ["g77"],
        "F77_LINK": ["g77"],
        "F77FLAGS": ["-W", "-g"],
        "F77_TGT_F": ["-o"],
        "F77_SRC_F": ["-c"],
        "F77_LINK_TGT_F": ["-o"],
        "F77_LINK_SRC_F": [],
        "F77_PROGRAM_FMT": "%s"})

def detect(ctx):
    if yaku.utils.find_program("g77") is None:
        return False
    else:
        return True

########NEW FILE########
__FILENAME__ = gcc
import yaku.utils

def setup(ctx):
    env = ctx.env

    ctx.env["CC"] = ["gcc"]
    ctx.env["CC_TGT_F"] = ["-c", "-o"]
    ctx.env["CC_SRC_F"] = []
    ctx.env["CFLAGS"] = ["-Wall"]
    ctx.env["CFLAGS_SH"] = ["-fPIC"]
    ctx.env["DEFINES"] = []
    ctx.env["DEFINES_FMT"] = "-D%s"
    ctx.env["LINK"] = ["gcc"]
    ctx.env["LINKFLAGS"] = []
    ctx.env["LINK_TGT_F"] = ["-o"]
    ctx.env["LINK_SRC_F"] = []
    ctx.env["SHAREDLIB_FMT"] = "lib%s.so"
    ctx.env["SHLINK"] = ["gcc", "-shared"]
    ctx.env["SHLINKFLAGS"] = []
    ctx.env["SHLINK_TGT_F"] = ["-o"]
    ctx.env["SHLINK_SRC_F"] = []
    ctx.env["MODLINK"] = ["gcc", "-bundle", "-undefined", "dynamic_lookup"]
    ctx.env["MODLINKFLAGS"] = []
    ctx.env["MODLINK_TGT_F"] = ["-o"]
    ctx.env["MODLINK_SRC_F"] = []
    ctx.env["CPPPATH"] = []
    ctx.env["CPPPATH_FMT"] = "-I%s"
    ctx.env["LIBDIR"] = []
    ctx.env["LIBS"] = []
    ctx.env["FRAMEWORKS"] = []
    ctx.env["LIB_FMT"] = "-l%s"
    ctx.env["LIBDIR_FMT"] = "-L%s"

    ctx.env["CC_OBJECT_FMT"] = "%s.o"
    ctx.env["PROGRAM_FMT"] = "%s"

def detect(ctx):
    if yaku.utils.find_program("gcc") is None:
        return False
    else:
        return True

########NEW FILE########
__FILENAME__ = gfortran
import yaku.utils

def setup(ctx):
    env = ctx.env

    ctx.env.update(
       {"F77": ["gfortran"],
        "F77_LINK": ["gfortran"],
        "F77FLAGS": ["-W", "-g"],
        "F77_TGT_F": ["-o"],
        "F77_SRC_F": ["-c"],
        "F77_LINKFLAGS": [],
        "F77_LINK_TGT_F": ["-o"],
        "F77_LINK_SRC_F": [],
        "F77_OBJECT_FMT": "%s.o",
        "F77_PROGRAM_FMT": "%s"})

def detect(ctx):
    if yaku.utils.find_program("gfortran") is None:
        return False
    else:
        return True

########NEW FILE########
__FILENAME__ = gxx
import yaku.utils

def setup(ctx):
    env = ctx.env

    ctx.env["CXX"] = ["g++"]
    ctx.env["CXX_TGT_F"] = ["-c", "-o"]
    ctx.env["CXX_SRC_F"] = []
    ctx.env["CXXFLAGS"] = ["-Wall"]
    ctx.env["CXXLINK"] = ["g++"]
    ctx.env["CXXLINKFLAGS"] = []
    ctx.env["CXXLINK_TGT_F"] = ["-o"]
    ctx.env["CXXLINK_SRC_F"] = []
    ctx.env["CXXSHLINK"] = ["g++", "-shared"]
    ctx.env["CXXSHLINKFLAGS"] = []
    ctx.env["CXXSHLINK_TGT_F"] = ["-o"]
    ctx.env["CXXSHLINK_SRC_F"] = []
    ctx.env["CPPPATH"] = []
    ctx.env["CPPPATH_FMT"] = "-I%s"
    ctx.env["LIBDIR"] = []
    ctx.env["LIBS"] = []
    ctx.env["FRAMEWORKS"] = []
    ctx.env["LIB_FMT"] = "-l%s"
    ctx.env["LIBDIR_FMT"] = "-L%s"

    ctx.env["CXX_OBJECT_FMT"] = "%s.o"
    ctx.env["PROGRAM_FMT"] = "%s"

def detect(ctx):
    if yaku.utils.find_program("gcc") is None:
        return False
    else:
        return True

########NEW FILE########
__FILENAME__ = ifort
import sys
import os
if sys.platform == "win32":
    import _winreg

import yaku.utils
import yaku.task

if sys.platform == "win32":
    from yaku.tools.mscommon.common \
        import \
            read_keys, open_key, close_key, get_output
    from yaku.tools.msvc \
        import \
            _exec_command_factory

_ROOT = {"amd64": r"Software\Wow6432Node\Intel\Suites",
         "ia32": r"Software\Intel\Compilers"}

_FC_ROOT = {"amd64": r"Software\Wow6432Node\Intel\Compilers",
         "ia32": r"Software\Intel\Compilers"}

_ABI2BATABI = {"amd64": "intel64", "ia32": "ia32"}

def find_versions_fc(abi):
    base = _winreg.HKEY_LOCAL_MACHINE
    key = os.path.join(_FC_ROOT[abi], "Fortran")

    availables = {}
    versions = read_keys(base, key)
    if versions is None:
        return availables
    for v in versions:
        verk = os.path.join(key, v)
        key = open_key(verk)
        try:
            maj = _winreg.QueryValueEx(key, "Major Version")[0]
            min = _winreg.QueryValueEx(key, "Minor Version")[0]
            bld = _winreg.QueryValueEx(key, "Revision")[0]
            availables[(maj, min, bld)] = verk
        finally:
            close_key(key)
    return availables

def product_dir_fc(root):
    k = open_key(root)
    try:
        return _winreg.QueryValueEx(k, "ProductDir")[0]
    finally:
        close_key(k)

def setup(ctx):
    env = ctx.env

    env.update(
       {"F77": ["ifort"],
        "F77_LINK": ["ifort"],
        "F77_LINKFLAGS": [],
        "F77FLAGS": [],
        "F77_TGT_F": ["-o"],
        "F77_SRC_F": ["-c"],
        "F77_LINK_TGT_F": ["-o"],
        "F77_LINK_SRC_F": [],
        "F77_OBJECT_FMT": "%s.o",
        "F77_PROGRAM_FMT": "%s"})
    if sys.platform == "win32":
        env.update(
           {"F77FLAGS": ["/nologo"],
            "F77_TGT_F": ["/object:"],
            "F77_SRC_F": ["/c"],
            "F77_LINKFLAGS": ["/nologo"],
            "F77_LINK_TGT_F": ["/link", "/out:"],
            "F77_OBJECT_FMT": "%s.obj",
            "F77_PROGRAM_FMT": "%s.exe"})

        abi = "amd64"

        availables = find_versions_fc(abi)
        if len(availables) < 1:
            raise ValueError("No ifort version found for abi %s" % abi)

        versions = sorted(availables.keys())[::-1]
        pdir = product_dir_fc(availables[versions[0]])
        batfile = os.path.join(pdir, "bin", "ifortvars.bat")

        d = get_output(ctx, batfile, _ABI2BATABI[abi])
        for k, v in d.items():
            if k in ["LIB"]:
                ctx.env.extend("LIBDIR", v, create=True)
            elif k in ["INCLUDE"]:
                ctx.env.extend("CPPPATH", v, create=True)
        for p in d["PATH"]:
            exe = os.path.join(p, "ifort.exe")
            if os.path.exists(exe):
                env["F77"] = [exe]
                env["F77_LINK"] = [exe]
                break

if sys.platform == "win32":
    for task_class in ["f77", "fprogram"]:
       klass = yaku.task.task_factory(task_class)
       saved = klass.exec_command
       klass.exec_command = _exec_command_factory(saved)

def detect(ctx):
    if yaku.utils.find_program("ifort") is None:
        return False
    else:
        return True

########NEW FILE########
__FILENAME__ = common
import os
import re
import subprocess
import _winreg

def open_key(path):
    return _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE, path)

def close_key(k):
    return _winreg.CloseKey(k)

def get_output(conf, vcbat, args=None):
    """Parse the output of given bat file, with given args."""
    if args is None:
        args = ""
    cnt = """\
echo @off
set INCLUDE=
set LIB=
call "%(bat)s" %(args)s
echo PATH=%%PATH%%
echo LIB=%%LIB%%
echo INCLUDE=%%INCLUDE%%
""" % {"bat": vcbat, "args": args}
    batnode = conf.bld_root.declare("foo.bat")
    batnode.write(cnt)

    p = subprocess.Popen(["cmd", "/E:one", "/V:on", "/C", batnode.abspath()], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    (out, err) = p.communicate()
    for line in out.splitlines():
        if re.match("^Error", line):
            print "Error: %r" % line
            raise RuntimeError("Error while executing bat script: %r" % out)

    res = {"PATH": re.compile("^PATH=(.+)$"),
        "INCLUDE": re.compile("^INCLUDE=(.+)$"),
        "LIB": re.compile("^LIB=(.+)$")}
    ret = {}
    for name, r in res.items():
        for line in out.splitlines():
            m = r.match(line)
            if m:
                v = m.group(1).split(os.pathsep)
                # Is there a better way to avoid spurious empty paths ?
                ret[name] = [x for x in v if x != ""]

    return ret

def read_keys(base, key):
    """Return list of registry keys."""
    try:
        handle = _winreg.OpenKeyEx(base, key)
    except _winreg.error:
        return None
    L = []
    i = 0
    while True:
        try:
            k = _winreg.EnumKey(handle, i)
        except _winreg.error:
            break
        L.append(k)
        i += 1
    return L

def read_values(base, key):
    try:
        handle = _winreg.OpenKeyEx(base, key)
    except _winreg.error:
        return None

    d = {}
    i = 0
    while True:
        try:
            name, value, type = _winreg.EnumValue(handle, i)
        except _winreg.error, e:
            break
        d[convert_mbcs(name)] = convert_mbcs(value)
        i += 1
    return d

def read_value(key, root=_winreg.HKEY_LOCAL_MACHINE):
    base = os.path.dirname(key)
    val = os.path.basename(key)
    try:
        handle = _winreg.OpenKeyEx(root, base)
        try:
            value, type = _winreg.QueryValueEx(handle, val)
            return value
        finally:
            _winreg.CloseKey(handle)
    except _winreg.error:
        return None

def convert_mbcs(s):
    dec = getattr(s, "decode", None)
    if dec is not None:
        try:
            s = dec("mbcs")
        except UnicodeError:
            pass
    return s

########NEW FILE########
__FILENAME__ = msvc
import os
import re
import subprocess
import _winreg

import yaku.task
from yaku.tools.mscommon.common \
    import \
        read_values, read_value, get_output

def _exec_command_factory(saved):
    def msvc_exec_command(self, cmd, cwd, env=None):
        new_cmd = []
        carry = ""
        for c in cmd:
            if c in ["/Fo", "/out:", "/OUT:", "/object:"]:
                carry = c
            else:
                c = carry + c
                carry = ""
                new_cmd.append(c)

        env = dict(os.environ)
        env.update(PATH=os.pathsep.join(self.env["PATH"]))
        saved(self, new_cmd, cwd, env=env)
    return msvc_exec_command

# Dict to 'canonalize' the arch
_ARCH_TO_CANONICAL = {
    "amd64"     : "amd64",
    "emt64"     : "amd64",
    "i386"      : "x86",
    "i486"      : "x86",
    "i586"      : "x86",
    "i686"      : "x86",
    "ia64"      : "ia64",
    "itanium"   : "ia64",
    "x86"       : "x86",
    "x86_64"    : "amd64",
}

# Given a (host, target) tuple, return the argument for the bat file. Both host
# and targets should be canonalized.
_HOST_TARGET_ARCH_TO_BAT_ARCH = {
    ("x86", "x86"): "x86",
    ("x86", "amd64"): "x86_amd64",
    ("amd64", "amd64"): "amd64",
    ("amd64", "x86"): "x86",
    ("x86", "ia64"): "x86_ia64"
}

_VCVER = ["10.0", "9.0", "9.0Exp","8.0", "8.0Exp","7.1", "7.0", "6.0"]

_VCVER_TO_PRODUCT_DIR = {
        '10.0': [
            r'Microsoft\VisualStudio\10.0\Setup\VC\ProductDir'],
        '9.0': [
            r'Microsoft\VisualStudio\9.0\Setup\VC\ProductDir'],
        '9.0Exp' : [
            r'Microsoft\VCExpress\9.0\Setup\VC\ProductDir'],
        '8.0': [
            r'Microsoft\VisualStudio\8.0\Setup\VC\ProductDir'],
        '8.0Exp': [
            r'Microsoft\VCExpress\8.0\Setup\VC\ProductDir'],
        '7.1': [
            r'Microsoft\VisualStudio\7.1\Setup\VC\ProductDir'],
        '7.0': [
            r'Microsoft\VisualStudio\7.0\Setup\VC\ProductDir'],
        '6.0': [
            r'Microsoft\VisualStudio\6.0\Setup\Microsoft Visual C++\ProductDir']
}

_is_win64 = None

def is_win64():
    """Return true if running on windows 64 bits.
    
    Works whether python itself runs in 64 bits or 32 bits."""
    # Unfortunately, python does not provide a useful way to determine
    # if the underlying Windows OS is 32-bit or 64-bit.  Worse, whether
    # the Python itself is 32-bit or 64-bit affects what it returns,
    # so nothing in sys.* or os.* help.  

    # Apparently the best solution is to use env vars that Windows
    # sets.  If PROCESSOR_ARCHITECTURE is not x86, then the python
    # process is running in 64 bit mode (on a 64-bit OS, 64-bit
    # hardware, obviously).
    # If this python is 32-bit but the OS is 64, Windows will set
    # ProgramW6432 and PROCESSOR_ARCHITEW6432 to non-null.
    # (Checking for HKLM\Software\Wow6432Node in the registry doesn't
    # work, because some 32-bit installers create it.)
    global _is_win64
    if _is_win64 is None:
        # I structured these tests to make it easy to add new ones or
        # add exceptions in the future, because this is a bit fragile.
        _is_win64 = False
        if os.environ.get('PROCESSOR_ARCHITECTURE','x86') != 'x86':
            _is_win64 = True
        if os.environ.get('PROCESSOR_ARCHITEW6432'):
            _is_win64 = True
        if os.environ.get('ProgramW6432'):
            _is_win64 = True
    return _is_win64


def msvc_version_to_maj_min(msvc_version):
   msvc_version_numeric = ''.join([x for  x in msvc_version if x in string_digits + '.'])

   t = msvc_version_numeric.split(".")
   if not len(t) == 2:
       raise ValueError("Unrecognized version %s (%s)" % (msvc_version,msvc_version_numeric))
   try:
       maj = int(t[0])
       min = int(t[1])
       return maj, min
   except ValueError, e:
       raise ValueError("Unrecognized version %s (%s)" % (msvc_version,msvc_version_numeric))

def is_host_target_supported(host_target, msvc_version):
    """Return True if the given (host, target) tuple is supported given the
    msvc version.

    Parameters
    ----------
    host_target: tuple
        tuple of (canonalized) host-target, e.g. ("x86", "amd64") for cross
        compilation from 32 bits windows to 64 bits.
    msvc_version: str
        msvc version (major.minor, e.g. 10.0)

    Note
    ----
    This only check whether a given version *may* support the given (host,
    target), not that the toolchain is actually present on the machine.
    """
    # We assume that any Visual Studio version supports x86 as a target
    if host_target[1] != "x86":
        maj, min = msvc_version_to_maj_min(msvc_version)
        if maj < 8:
            return False

    return True

def find_vc_pdir(msvc_version):
    """Try to find the product directory for the given
    version.

    Note
    ----
    If for some reason the requested version could not be found, an
    exception which inherits from VisualCException will be raised."""
    base = _winreg.HKEY_LOCAL_MACHINE
    root = 'Software\\'
    if is_win64():
        root = root + 'Wow6432Node\\'
    try:
        hkeys = _VCVER_TO_PRODUCT_DIR[msvc_version]
    except KeyError:
        #debug("Unknown version of MSVC: %s" % msvc_version)
        raise ValueError("Unknown version %s" % msvc_version)

    for key in hkeys:
        key = root + key
        comps = read_value(key)
        if comps is not None:
            if os.path.exists(comps):
                return comps
            else:
                raise ValueError("registry dir %s not found on the filesystem" % comps)
    return None

def find_versions(abi):
    base = _winreg.HKEY_LOCAL_MACHINE
    key = os.path.join(_FC_ROOT[abi], "Fortran")

    availables = {}
    versions = read_keys(base, key)
    if versions is None:
        return availables
    for v in versions:
        verk = os.path.join(key, v)
        key = open_key(verk)
        try:
            maj = _winreg.QueryValueEx(key, "Major Version")[0]
            min = _winreg.QueryValueEx(key, "Minor Version")[0]
            bld = _winreg.QueryValueEx(key, "Revision")[0]
            availables[(maj, min, bld)] = verk
        finally:
            close_key(key)
    return availables

def _detect_msvc(ctx):
    from string import digits as string_digits

    msvc_version_info = (9, 0)
    msvc_version = "9.0"
    pdir = find_vc_pdir(msvc_version)
    if pdir is None:
        raise ValueError("VS 9.0 not found")

    # filter out e.g. "Exp" from the version name
    msvc_ver_numeric = ''.join([x for x in msvc_version if x in string_digits + "."])
    vernum = float(msvc_ver_numeric)
    if 7 <= vernum < 8:
        pdir = os.path.join(pdir, os.pardir, "Common7", "Tools")
        batfilename = os.path.join(pdir, "vsvars32.bat")
    elif vernum < 7:
        pdir = os.path.join(pdir, "Bin")
        batfilename = os.path.join(pdir, "vcvars32.bat")
    else: # >= 8
        batfilename = os.path.join(pdir, "vcvarsall.bat")

    vc_paths = get_output(ctx, batfilename, "x86")
    cc = None
    linker = None
    lib = None
    for p in vc_paths["PATH"]:
        _cc = os.path.join(p, "cl.exe")
        _linker = os.path.join(p, "link.exe")
        _lib = os.path.join(p, "lib.exe")
        if os.path.exists(_cc) and os.path.exists(_linker) and os.path.exists(_lib):
            cc = _cc
            linker = _linker
            lib = _lib
            break
    if cc is None or linker is None:
        raise RuntimeError("Could not find cl.exe/link.exe")

    return cc, linker, lib, vc_paths, msvc_version_info

def setup(ctx):
    env = ctx.env

    cc, linker, lib, vc_paths, msvc_version_info = _detect_msvc(ctx)
    ctx.env["PATH"] = vc_paths["PATH"][:]
    ctx.env.prextend("CPPPATH", vc_paths["INCLUDE"], create=True)
    ctx.env.prextend("LIBDIR", vc_paths["LIB"], create=True)

    ctx.env["CC"] = [cc]
    ctx.env["CC_TGT_F"] = ["/c", "/Fo"]
    ctx.env["CC_SRC_F"] = []
    ctx.env["CFLAGS"] = ["/nologo"]
    ctx.env["CPPPATH_FMT"] = "/I%s"
    ctx.env["DEFINES"] = []
    ctx.env["DEFINES_FMT"] = "/D%s"
    ctx.env["LINK"] = [linker]
    ctx.env["LINK_TGT_F"] = ["/out:"]
    ctx.env["LINK_SRC_F"] = []
    ctx.env["LINKFLAGS"] = ["/nologo"]
    ctx.env["SHLINK"] = [linker, "/DLL"]
    ctx.env["SHLINK_TGT_F"] = ["/out:"]
    ctx.env["SHLINK_SRC_F"] = []
    ctx.env["SHLINKFLAGS"] = []
    ctx.env["MODLINK"] = [linker, "/DLL"]
    ctx.env["MODLINK_TGT_F"] = ["/out:"]
    ctx.env["MODLINK_SRC_F"] = []
    ctx.env["MODLINKFLAGS"] = ["/nologo"]
    ctx.env["LIBS"] = []
    ctx.env["LIB_FMT"] = "%s.lib"
    ctx.env["LIBDIR"] = []
    ctx.env["LIBDIR_FMT"] = "/LIBPATH:%s"

    ctx.env["STLINK"] = [lib]
    ctx.env["STLINK_TGT_F"] = ["/OUT:"]
    ctx.env["STLINK_SRC_F"] = []
    ctx.env["STLINKFLAGS"] = ["/nologo"]
    ctx.env["STATICLIB_FMT"] = "%s.lib"

    ctx.env["CXX"] = [cc]
    ctx.env["CXX_TGT_F"] = ["/c", "/Fo"]
    ctx.env["CXX_SRC_F"] = []
    ctx.env["CXXFLAGS"] = ["/nologo"]
    if msvc_version_info >= (9, 0):
        ctx.env.append("CXXFLAGS", "/EHsc")
    ctx.env["CXXLINK"] = [linker]
    ctx.env["CXXLINKFLAGS"] = ["/nologo"]
    ctx.env["CXXLINK_TGT_F"] = ["/out:"]
    ctx.env["CXXLINK_SRC_F"] = []
    ctx.env["CXXSHLINK"] = [linker]
    ctx.env["CXXSHLINKFLAGS"] = []
    ctx.env["CXXSHLINK_TGT_F"] = ["/out:"]
    ctx.env["CXXSHLINK_SRC_F"] = []

    ctx.env["CC_OBJECT_FMT"] = "%s.obj"
    ctx.env["CXX_OBJECT_FMT"] = "%s.obj"
    ctx.env["SHAREDLIB_FMT"] = "%s.dll"
    ctx.env["PROGRAM_FMT"] = "%s.exe"

    for k, v in vc_paths.items():
        k = k.encode("ascii")
        if k in ["LIB"]:
            env.extend("LIBDIR", v, create=True)
        elif k in ["CPPPATH"]:
            env.extend(k, v, create=True)

for task_class in ["cc", "shcc", "cc_shlink", "cc_stlink", "cc_program", "cxx", "cxxprogram", "pycc", "pycxx", "pylink"]:
    klass = yaku.task.task_factory(task_class)
    saved = klass.exec_command
    klass.exec_command = _exec_command_factory(saved)

def detect(ctx):
    _detect_msvc(ctx)
    return True

########NEW FILE########
__FILENAME__ = pyext
import sys
import os
import copy
import distutils
import distutils.sysconfig
import re
import warnings
import errno

from subprocess \
    import \
        Popen, PIPE, STDOUT

from yaku.task_manager \
    import \
        topo_sort, build_dag, \
        CompiledTaskGen, set_extension_hook
from yaku.sysconfig \
    import \
        get_configuration, detect_distutils_cc
from yaku.compiled_fun \
    import \
        compile_fun
from yaku.task \
    import \
        task_factory
from yaku.utils \
    import \
        ensure_dir, get_exception
from yaku.environment \
    import \
        Environment
from yaku.conftests \
    import \
        check_compiler, check_header
from yaku.tools.ctasks \
    import \
        apply_define
from yaku.scheduler \
    import \
        run_tasks
from yaku.conf \
    import \
        with_conf_blddir, create_file, write_log
from yaku.errors \
    import \
        TaskRunFailure
from yaku._config \
    import \
        _OUTPUT
import yaku.tools

pylink, pylink_vars = compile_fun("pylink", "${PYEXT_SHLINK} ${PYEXT_LINK_TGT_F}${TGT[0].abspath()} ${PYEXT_LINK_SRC_F}${SRC} ${PYEXT_APP_LIBDIR} ${PYEXT_APP_LIBS} ${PYEXT_APP_FRAMEWORKS} ${PYEXT_SHLINKFLAGS}", False)

pycc, pycc_vars = compile_fun("pycc", "${PYEXT_CC} ${PYEXT_CFLAGS} ${PYEXT_INCPATH} ${PYEXT_CC_TGT_F}${TGT[0].abspath()} ${PYEXT_CC_SRC_F}${SRC}", False)

pycxx, pycxx_vars = compile_fun("pycxx", "${PYEXT_CXX} ${PYEXT_CXXFLAGS} ${PYEXT_INCPATH} ${PYEXT_CXX_TGT_F}${TGT[0].abspath()} ${PYEXT_CXX_SRC_F}${SRC}", False)

pycxxlink, pycxxlink_vars = compile_fun("pycxxlink", "${PYEXT_CXXSHLINK} ${PYEXT_LINK_TGT_F}${TGT[0].abspath()} ${PYEXT_LINK_SRC_F}${SRC} ${PYEXT_APP_LIBDIR} ${PYEXT_APP_LIBS} ${PYEXT_APP_FRAMEWORKS} ${PYEXT_SHLINKFLAGS}", False)

# pyext env <-> sysconfig env conversion

_SYS_TO_PYENV = {
        "PYEXT_SHCC": "CC",
        "PYEXT_CCSHARED": "CCSHARED",
        "PYEXT_SHLINK": "LDSHARED",
        "PYEXT_SUFFIX": "SO",
        "PYEXT_CFLAGS": "CFLAGS",
        "PYEXT_OPT": "OPT",
        "PYEXT_LIBDIR": "LIBDIR",
}

_PYENV_REQUIRED = [
        "LIBDIR_FMT",
        "LIBS",
        "LIB_FMT",
        "CPPPATH_FMT",
        "CC_TGT_F",
        "CC_SRC_F",
        "LINK_TGT_F",
        "LINK_SRC_F",
]

_SYS_TO_CCENV = {
        "CC": "CC",
        "SHCC": "CCSHARED",
        "SHLINK": "LDSHARED",
        "SO": "SO",
        "CFLAGS": "CFLAGS",
        "OPT": "OPT",
        "LIBDIR": "LIBDIR",
        "LIBDIR_FMT": "LIBDIR_FMT",
        "LIBS": "LIBS",
        "LIB_FMT": "LIB_FMT",
        "CPPPATH_FMT": "CPPPATH_FMT",
        "CC_TGT_F": "CC_TGT_F",
        "CC_SRC_F": "CC_SRC_F",
        "CXX": "CXX",
        "CXXSHLINK": "CXXSHLINK",
}

def setup_pyext_env(ctx, cc_type="default", use_distutils=True):
    pyenv = Environment()
    if use_distutils:
        if cc_type == "default":
            dist_env = get_configuration()
        else:
            dist_env = get_configuration(cc_type)
        for name, value in dist_env.items():
            pyenv["PYEXT_%s" % name] = value
        pyenv["PYEXT_FMT"] = "%%s%s" % dist_env["SO"]
        pyenv["PYEXT_SHLINKFLAGS"] = dist_env["LDFLAGS"]
    else:
        old_env = ctx.env
        ctx.env = Environment()
        cc_env = None
        sys.path.insert(0, os.path.dirname(yaku.tools.__file__))
        try:
            try:
                mod = __import__(cc_type)
                mod.setup(ctx)
            except ImportError:
                raise RuntimeError("No tool %s is available (import failed)" \
                                % cc_type)
            cc_env = ctx.env
        finally:
            sys.path.pop(0)
            ctx.env = old_env
        pyenv["PYEXT_CC"] = cc_env["CC"]
        pyenv["PYEXT_CFLAGS"] = cc_env["CFLAGS"]
        pyenv["PYEXT_LIBDIR"] = cc_env["LIBDIR"]
        pyenv["PYEXT_LIBS"] = cc_env["LIBS"]
        pyenv["PYEXT_FMT"] = "%s.so"
        pyenv["PYEXT_SHLINK"] = cc_env["MODLINK"]
        pyenv["PYEXT_SHLINKFLAGS"] = cc_env["MODLINKFLAGS"]
        pyenv["PYEXT_CPPPATH"] = cc_env["CPPPATH"]
        pyenv.append("PYEXT_CPPPATH", distutils.sysconfig.get_python_inc(), create=True)
        if sys.platform == "win32":
            pyenv.append("PYEXT_LIBDIR", os.path.join(sys.exec_prefix, "libs"))

    return pyenv

def pycc_hook(self, node):
    tasks = pycc_task(self, node)
    self.object_tasks.extend(tasks)
    return tasks

def pycc_task(self, node):
    base = self.env["CC_OBJECT_FMT"] % node.name
    target = node.parent.declare(base)
    ensure_dir(target.abspath())

    task = task_factory("pycc")(inputs=[node], outputs=[target])
    task.gen = self
    task.env_vars = pycc_vars
    task.env = self.env
    task.func = pycc
    return [task]

def pycxx_hook(self, node):
    tasks = pycxx_task(self, node)
    self.object_tasks.extend(tasks)
    self.has_cxx = True
    return tasks

def pycxx_task(self, node):
    base = self.env["CXX_OBJECT_FMT"] % node.name
    target = node.parent.declare(base)
    ensure_dir(target.abspath())

    task = task_factory("pycxx")(inputs=[node], outputs=[target])
    task.gen = self
    task.env_vars = pycxx_vars
    task.env = self.env
    task.func = pycxx
    return [task]

def pylink_task(self, name):
    objects = [tsk.outputs[0] for tsk in self.object_tasks]
    if len(objects) < 1:
        warnings.warn("task %s has no inputs !" % name)
    def declare_target():
        folder, base = os.path.split(name)
        tmp = folder + os.path.sep + self.env["PYEXT_FMT"] % base
        return self.bld.path.declare(tmp)
    target = declare_target()
    ensure_dir(target.abspath())

    task = task_factory("pylink")(inputs=objects, outputs=[target])
    task.gen = self
    task.func = pylink
    task.env_vars = pylink_vars
    self.link_task = task

    return [task]

# XXX: fix merge env location+api
class PythonBuilder(yaku.tools.Builder):
    def clone(self):
        return PythonBuilder(self.ctx)

    def __init__(self, ctx):
        yaku.tools.Builder.__init__(self, ctx)

    def _compile(self, task_gen, name):
        apply_define(task_gen)
        apply_cpppath(task_gen)

        tasks = task_gen.process()
        for t in tasks:
            t.env = task_gen.env
        return tasks

    def try_compile(self, name, body, headers=None):
        old_hook = set_extension_hook(".c", pycc_task)
        try:
            return with_conf_blddir(self.ctx, name, body,
                                    lambda : yaku.tools.try_task_maker(self.ctx, self._compile, name, body, headers))
        finally:
            set_extension_hook(".c", old_hook)

    def try_extension(self, name, body, headers=None):
        old_hook = set_extension_hook(".c", pycc_task)
        try:
            return with_conf_blddir(self.ctx, name, body,
                                    lambda : yaku.tools.try_task_maker(self.ctx, self._extension, name, body, headers))
        finally:
            set_extension_hook(".c", old_hook)

    def _extension(self, task_gen, name):
        bld = self.ctx
        base = name.replace(".", os.sep)

        tasks = []

        old_hook = set_extension_hook(".c", pycc_hook)
        old_hook_cxx = set_extension_hook(".cxx", pycxx_hook)

        apply_define(task_gen)
        apply_cpppath(task_gen)
        apply_libpath(task_gen)
        apply_libs(task_gen)
        apply_frameworks(task_gen)

        tasks = task_gen.process()

        ltask = pylink_task(task_gen, base)
        task_gen.link_task = ltask
        if task_gen.has_cxx:
            task_gen.link_task[-1].func = pycxxlink
            task_gen.link_task[-1].env_vars = pycxxlink_vars

        tasks.extend(ltask)
        for t in tasks:
            t.env = task_gen.env

        set_extension_hook(".c", old_hook)
        set_extension_hook(".cxx", old_hook_cxx)
        return tasks

    def extension(self, name, sources, env=None):
        sources = self.to_nodes(sources)
        task_gen = CompiledTaskGen("pyext", self.ctx, sources, name)
        task_gen.bld = self.ctx
        task_gen.env = yaku.tools._merge_env(self.env, env)
        tasks = self._extension(task_gen, name)
        self.ctx.tasks.extend(tasks)

        outputs = []
        for t in task_gen.link_task:
            outputs.extend(t.outputs)
        task_gen.outputs = outputs
        return tasks

    def try_extension(self, name, body, headers=None):
        return with_conf_blddir(self.ctx, name, body,
                                lambda : yaku.tools.try_task_maker(self.ctx, self._extension, name, body, headers))

    def configure(self, candidates=None, use_distutils=True):
        ctx = self.ctx
        # How we do it
        # 1: for distutils-based configuration
        #   - get compile/flags flags from sysconfig
        #   - detect yaku tool name from CC used by distutils:
        #       - get the compiler executable used by distutils ($CC
        #       variable)
        #       - try to determine yaku tool name from $CC
        #   - apply necessary variables from yaku tool to $PYEXT_
        #   "namespace"
        if candidates is None:
            compiler_type = "default"
        else:
            compiler_type = candidates[0]

        if use_distutils:
            dist_env = setup_pyext_env(ctx, compiler_type)
            ctx.env.update(dist_env)

            cc_exec = get_distutils_cc_exec(ctx, compiler_type)
            yaku_cc_type = detect_cc_type(ctx, cc_exec)
            if yaku_cc_type is None:
                raise ValueError("No adequate C compiler found (distutils mode)")

            _setup_compiler(ctx, yaku_cc_type)

            cxx_exec = get_distutils_cxx_exec(ctx, compiler_type)
            yaku_cxx_type = detect_cxx_type(ctx, cxx_exec)
            if yaku_cxx_type is None:
                raise ValueError("No adequate CXX compiler found (distutils mode)")

            _setup_cxxcompiler(ctx, yaku_cxx_type)
        else:
            dist_env = setup_pyext_env(ctx, compiler_type, False)
            ctx.env.update(dist_env)
            _setup_compiler(ctx, compiler_type)

        pycode = r"""\
#include <Python.h>
#include <stdio.h>

static PyObject*
hello(PyObject *self, PyObject *args)
{
    printf("Hello from C\n");
    Py_INCREF(Py_None);
    return Py_None;
}

static PyMethodDef HelloMethods[] = {
    {"hello",  hello, METH_VARARGS, "Print a hello world."},
    {NULL, NULL, 0, NULL}        /* Sentinel */
};

PyMODINIT_FUNC
init_bar(void)
{
    (void) Py_InitModule("_bar", HelloMethods);
}
"""
        ctx.start_message("Checking whether %s can build python object code" % compiler_type)
        try:
            self.try_compile("foo", pycode)
            ctx.end_message("yes")
        except TaskRunFailure:
            e = get_exception()
            ctx.end_message("no")
            ctx.fail_configuration(str(e))

        ctx.start_message("Checking whether %s can build python extension" % compiler_type)
        try:
            self.try_extension("foo", pycode)
            ctx.end_message("yes")
        except TaskRunFailure:
            e = get_exception()
            ctx.end_message("no")
            ctx.fail_configuration(str(e))
        self.configured = True

def get_builder(ctx):
    return PythonBuilder(ctx)

CC_SIGNATURE = {
        "clang": re.compile("clang version"),
        "gcc": re.compile("gcc version"),
        "msvc": re.compile("Microsoft \(R\) (32-bit |)C/C\+\+ Optimizing Compiler")
}

def detect_cc_type(ctx, cc_cmd):
    return _detect_cc_type(ctx, cc_cmd)

def detect_cxx_type(ctx, cxx_cmd):
    cxx_type = _detect_cc_type(ctx, cxx_cmd)
    if cxx_type == "gcc":
        return "gxx"
    else:
        return cxx_type

def _detect_cc_type(ctx, cc_cmd):
    cc_type = None

    def detect_type(vflag):
        cmd = cc_cmd + [vflag]
        env = os.environ
        env["LC_ALL"] = env["LANGUAGE"] = "C"
        p = Popen(cmd, stdout=PIPE, stderr=STDOUT, env=env)
        out = p.communicate()[0].decode()
        for k, v in CC_SIGNATURE.items():
            m = v.search(out)
            if m:
                return k
        return None

    _OUTPUT.write("Detecting CC type... ")
    if sys.platform == "win32":
        for v in ["", "-v"]:
            cc_type = detect_type(v)
    else:
        try:
            for v in ["-v", "-V", "-###"]:
                cc_type = detect_type(v)
                if cc_type:
                    break
            if cc_type is None:
                cc_type = "cc"
        except OSError:
            e = extract_exception()
            if e.errno == errno.ENOENT:
                raise ValueError("compiler %r not found" % " ".join(cc_cmd))
            else:
                raise ValueError("Unexpected error %r when testing compiler %r" % (e, cc_cmd))
    _OUTPUT.write("%s\n" % cc_type)
    return cc_type

def get_distutils_cc_exec(ctx, compiler_type="default"):
    from distutils import ccompiler
    from distutils.sysconfig import customize_compiler

    _OUTPUT.write("Detecting distutils CC exec ... ")
    if compiler_type == "default":
        compiler_type = \
                distutils.ccompiler.get_default_compiler()

    compiler = ccompiler.new_compiler(compiler=compiler_type)
    customize_compiler(compiler)
    if compiler_type == "msvc":
        compiler.initialize()
        cc = [compiler.cc]
    else:
        cc = compiler.compiler_so[:1]
    _OUTPUT.write("%s\n" % " ".join(cc))
    return cc

def get_distutils_cxx_exec(ctx, compiler_type="default"):
    from distutils import ccompiler
    from distutils.sysconfig import customize_compiler

    _OUTPUT.write("Detecting distutils CXX exec ... ")
    if compiler_type == "default":
        compiler_type = \
                distutils.ccompiler.get_default_compiler()

    compiler = ccompiler.new_compiler(compiler=compiler_type)
    if compiler_type == "msvc":
        compiler.initialize()
        cc = [compiler.cc]
    else:
        customize_compiler(compiler)
        cc = compiler.compiler_cxx
    _OUTPUT.write("%s\n" % " ".join(cc))
    return cc

def _setup_compiler(ctx, cc_type):
    old_env = ctx.env
    ctx.env = Environment()
    cc_env = None
    sys.path.insert(0, os.path.dirname(yaku.tools.__file__))
    try:
        try:
            mod = __import__(cc_type)
            mod.setup(ctx)
        except ImportError:
            raise RuntimeError("No tool %s is available (import failed)" \
                            % cc_type)

        # XXX: this is ugly - find a way to have tool-specific env...
        cc_env = ctx.env
    finally:
        sys.path.pop(0)
        ctx.env = old_env

    copied_values = ["CC", "CPPPATH_FMT", "LIBDIR_FMT", "LIB_FMT",
            "CC_OBJECT_FMT", "CC_TGT_F", "CC_SRC_F", "LINK_TGT_F",
            "LINK_SRC_F", "SHLINK"]
    for k in copied_values:
        ctx.env["PYEXT_%s" % k] = cc_env[k]
    ctx.env.prextend("PYEXT_CPPPATH", cc_env["CPPPATH"])
    ctx.env.prextend("PYEXT_LIBDIR", cc_env["LIBDIR"])

def _setup_cxxcompiler(ctx, cxx_type):
    old_env = ctx.env
    ctx.env = Environment()
    sys.path.insert(0, os.path.dirname(yaku.tools.__file__))
    try:
        mod = __import__(cxx_type)
        mod.setup(ctx)
        cxx_env = ctx.env
    finally:
        sys.path.pop(0)
        ctx.env = old_env

    for k in ["CXX", "CXXFLAGS", "CXX_TGT_F", "CXX_SRC_F",
              "CXXSHLINK"]:
        ctx.env["PYEXT_%s" % k] = cxx_env[k]

# FIXME: find a way to reuse this kind of code between tools
def apply_frameworks(task_gen):
    # XXX: do this correctly (platform specific tool config)
    if sys.platform == "darwin":
        frameworks = task_gen.env.get("PYEXT_FRAMEWORKS", [])
        task_gen.env["PYEXT_APP_FRAMEWORKS"] = []
        for framework in frameworks:
            task_gen.env["PYEXT_APP_FRAMEWORKS"].extend(["-framework", framework])
    else:
        task_gen.env["PYEXT_APP_FRAMEWORKS"] = []

def apply_libs(task_gen):
    libs = task_gen.env["PYEXT_LIBS"]
    task_gen.env["PYEXT_APP_LIBS"] = [
            task_gen.env["PYEXT_LIB_FMT"] % lib for lib in libs]

def apply_libpath(task_gen):
    libdir = task_gen.env["PYEXT_LIBDIR"]
    #implicit_paths = set([
    #    os.path.join(task_gen.env["BLDDIR"], os.path.dirname(s))
    #    for s in task_gen.sources])
    implicit_paths = []
    libdir = list(implicit_paths) + libdir
    task_gen.env["PYEXT_APP_LIBDIR"] = [
            task_gen.env["PYEXT_LIBDIR_FMT"] % d for d in libdir]

def apply_cpppath(task_gen):
    cpppaths = task_gen.env["PYEXT_CPPPATH"]
    implicit_paths = set([s.parent.srcpath() \
                          for s in task_gen.sources])
    srcnode = task_gen.sources[0].ctx.srcnode

    relcpppaths = []
    for p in cpppaths:
        if not os.path.isabs(p):
            node = srcnode.find_node(p)
            assert node is not None, "could not find %s" % p
            relcpppaths.append(node.bldpath())
        else:
            relcpppaths.append(p)
    cpppaths = list(implicit_paths) + relcpppaths
    task_gen.env["PYEXT_INCPATH"] = [
            task_gen.env["PYEXT_CPPPATH_FMT"] % p
            for p in cpppaths]

########NEW FILE########
__FILENAME__ = python_2to3
import os
import sys
import shutil
import subprocess

from cStringIO \
    import \
        StringIO

import lib2to3.main

from yaku.errors \
    import \
        TaskRunFailure
from yaku.task \
    import \
        task_factory
from yaku.task_manager \
    import \
        TaskGen, extension
from yaku.pprint \
    import \
        pprint

import yaku.tools

def convert_func(self):
    if not len(self.inputs) == 1:
        raise ValueError("convert_func needs exactly one input")
    source, target = self.inputs[0], self.outputs[0]

    pprint('GREEN', "%-16s%s" % (self.name.upper(),
           " ".join([s.srcpath() for s in self.inputs])))
    cmd = ["2to3", "-w", "--no-diffs", "-n", source.abspath()]
    st = subprocess.call(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if st != 0:
        pprint('RED', "FAILED %-16s%s" % (self.name.upper(),
               " ".join([s.srcpath() for s in self.inputs])))
        raise TaskRunFailure(cmd)
    target.write(source.read())

def copy_func(self):
    source, target = self.inputs[0], self.outputs[0]
    pprint('YELLOW', "%-16s%s" % (self.name.upper(),
           " ".join([s.srcpath() for s in self.inputs])))
    target.write(source.read())

class Py3kConverterBuilder(yaku.tools.Builder):
    def __init__(self, ctx):
        super(Py3kConverterBuilder, self).__init__(ctx)

    def _process_exclude(self, env):
        if "2TO3_EXCLUDE_LIST" in env:
            excludes = env["2TO3_EXCLUDE_LIST"]
        else:
            excludes = []
        dirs = []
        fs = []
        for e in excludes:
            n = self.ctx.src_root.find_node(e)
            if os.path.isdir(n.abspath()):
                dirs.append(n)
            else:
                fs.append(n)

        def _exclude(n):
            for d in dirs:
                if n.is_child_of(d):
                    return True
            if n in fs:
                return True
            return False
        return _exclude

    def convert(self, name, sources, env=None):
        # Basic principle: we first copy the whole tree (defined by the sources
        # list) into a temporary directory, which is then used for the
        # convertion. Notes:
        # - the whole tree needs to be copied before any 2to3 execution as 2to3
        # depends on the tree structure (e.g. for local vs absolute imports)
        # - because 2to3 can only modify files in place, we need to copy things
        # twice to avoid re-applying 2to3 several times (2to3 is not
        # idem-potent).
        # - the exclude process is particularly ugly...
        env = yaku.tools._merge_env(self.env, env)

        flter = self._process_exclude(env)
        self.env["__2TO3_FILTER"] = flter

        files = [self.ctx.src_root.find_resource(f) for f in sources]

        convert_tf = task_factory("2to3")
        copy_tf = task_factory("2to3_prepare")
        convert_tf.before.append(copy_tf.__name__)

        py3k_tmp = self.ctx.bld_root.declare("_py3k_tmp")
        py3k_top = self.ctx.bld_root.declare("py3k")
        tasks = []
        for f in files:
            target = py3k_tmp.declare(f.srcpath())
            task = copy_tf(inputs=[f], outputs=[target])
            task.func = copy_func
            task.env_vars = {}
            task.env = env
            tasks.append(task)

            if f.name.endswith(".py") and not flter(f):
                source = target
                target = py3k_top.declare(source.path_from(py3k_tmp))
                task = convert_tf(inputs=[source], outputs=[target])
                task.func = convert_func
                task.env_vars = {}
                task.env = env
                tasks.append(task)
            else:
                source = f
                target = py3k_top.declare(source.srcpath())
                task = copy_tf(inputs=[source], outputs=[target])
                task.func = copy_func
                task.env_vars = {}
                task.env = env
                tasks.append(task)

        self.ctx.tasks.extend(tasks)

        outputs = []
        for t in tasks:
            outputs.extend(t.outputs)
        return outputs

def get_builder(ctx):
    return Py3kConverterBuilder(ctx)

########NEW FILE########
__FILENAME__ = swig
import os

from yaku.task_manager \
    import \
        extension, get_extension_hook
from yaku.task \
    import \
        task_factory
from yaku.compiled_fun \
    import \
        compile_fun
from yaku.utils \
    import \
        ensure_dir

swig_func, swig_vars = compile_fun("swig", "${SWIG} ${SWIGFLAGS} -o ${TGT[0]} ${SRC}", False)

@extension(".i")
def swig_hook(self, name):
    # FIXME: only handle C extension (no C++)
    base = os.path.splitext(name)[0]
    target = os.path.join(self.env["BLDDIR"], base)
    targets = [target + "_wrap.c", target + ".py"]
    for t in targets:
        ensure_dir(t)
    task = task_factory("swig")(inputs=name, outputs=targets)
    task.func = swig_func
    task.env_vars = swig_vars
    task.env = self.env

    compile_task = get_extension_hook(".c")
    ctask = compile_task(self, targets[0])
    return [task] + ctask

########NEW FILE########
__FILENAME__ = template
import yaku.tools

from yaku.task \
    import \
        task_factory
from yaku.task_manager \
    import \
        extension, TaskGen

def render(task):
    import jinja2
    content = task.inputs[0].read()
    template = jinja2.Template(content)
    task.outputs[0].write(template.render(**task.env["SUBST_DICT"]))

@extension(".in")
def template_task(task_gen, node):
    out = node.change_ext("")
    target = node.parent.declare(out.name)
    task = task_factory("subst")(inputs=[node], outputs=[target], func=render)
    task.env_vars = ["SUBST_DICT"]
    task.env = task_gen.env
    return [task]

class TemplateBuilder(yaku.tools.Builder):
    def render(self, sources, vars=None):
        if vars is None:
            self.env["SUBST_DICT"]  = {}
        else:
            self.env["SUBST_DICT"]  = vars
        return self._task_gen_factory("subst", "noname", sources, None)

def get_builder(context):
    return TemplateBuilder(context)

########NEW FILE########
__FILENAME__ = utils
import sys
import re
import os

from yaku.compat.rename \
    import \
        rename
from yaku.compat.py3k \
    import \
        get_exception

def ensure_dir(path):
    dirname = os.path.dirname(path)
    if dirname and not os.path.exists(dirname):
        os.makedirs(dirname)

re_inc = re.compile(\
    '^[ \t]*(#|%:)[ \t]*(include)[ \t]*(.*)\r*$',
    re.IGNORECASE | re.MULTILINE)


RE_INCLUDE = re.compile('^\s*(<(?P<a>.*)>|"(?P<b>.*)")')

re_nl = re.compile('\\\\\r*\n', re.MULTILINE)
re_cpp = re.compile(\
    r"""(/\*[^*]*\*+([^/*][^*]*\*+)*/)|//[^\n]*|("(\\.|[^"\\])*"|'(\\.|[^'\\])*'|.[^/"'\\]*)""",
    re.MULTILINE)

def repl(m):
    s = m.group(1)
    if s is not None: return ' '
    s = m.group(3)
    if s is None: return ''
    return s

def extract_include(txt, defs):
    """process a line in the form "#include foo" to return a string representing the file"""
    m = RE_INCLUDE.search(txt)
    if m:
        if m.group('a'):
            return '<', m.group('a')
        if m.group('b'):
            return '"', m.group('b')

    return None, None

def lines_includes(filename):
    code = open(filename).read()
    #if use_trigraphs:
    #   for (a, b) in trig_def: code = code.split(a).join(b)
    code = re_nl.sub('', code)
    code = re_cpp.sub(repl, code)
    return [(m.group(2), m.group(3)) for m in re.finditer(re_inc, code)]

def find_deps(node, cpppaths=["/usr/include", "."]):
    nodes = []
    names = []

    def _find_deps(node):
        lst = lines_includes(node)

        for (_, line) in lst:
            t, filename = extract_include(line, None)
            if t is None:
                continue
            if filename in names:
                continue

            found = None
            for n in cpppaths:
                if found:
                    break
                if os.path.exists(os.path.join(n, filename)):
                    found = os.path.join(n, filename)
                #else:
                #    # XXX: most likely wrong
                #    found = os.path.join(n, filename)
                #    nodes.append(found)

            if not found:
                if not filename in names:
                    names.append(filename)
            elif not found in nodes:
                nodes.append(found)
                _find_deps(found)

    _find_deps(node)
    return nodes

def find_program(program, path_list=None):
    if path_list is None:
        path_list = os.environ["PATH"].split(os.pathsep)

    for p in path_list:
        ppath = os.path.join(p, program)
        if sys.platform == "win32":
            for ext in [".exe"]:
                epath = ppath + ext
                if os.path.exists(epath):
                    return epath
        else:
            if os.path.exists(ppath):
                return ppath

    return None

if sys.version_info[0] < 3:
    from yaku._utils_py2 import join_bytes, function_code
    def is_string(s):
        return isinstance(s, basestring)
else:
    from yaku._utils_py3 import join_bytes, function_code
    def is_string(s):
        return isinstance(s, str)

def extract_exception():
    """Extract the last exception.

    Used to avoid the except ExceptionType as e, which cannot be written the
    same across supported versions. I.e::

        try:
            ...
        except Exception, e:
            ...

    becomes:

        try:
            ...
        except Exception:
            e = extract_exception()
    """
    return sys.exc_info()[1]

########NEW FILE########
__FILENAME__ = _config
"""Global configuration

__file__, etc... are only allowed in this module.
"""
import sys

from os.path \
    import \
        join, abspath, dirname

TOOLDIRS = [abspath(join(dirname(__file__), "tools"))]

# relative to the build directory
DEFAULT_ENV = "default.env.py"
BUILD_CONFIG = "build.config.py"
HOOK_DUMP = ".hooks.pck"

CONFIG_CACHE = ".config.pck"
BUILD_CACHE = ".build.pck"

_OUTPUT = sys.stdout

########NEW FILE########
__FILENAME__ = _utils_py2
__all__ = ["join_bytes"]

def join_bytes(seq):
    return "".join(seq)

def function_code(f):
    return f.func_code

########NEW FILE########
__FILENAME__ = _utils_py3
__all__ = ["join_bytes"]

def join_bytes(seq):
    return b"".join(seq)

def function_code(f):
    return f.__code__

########NEW FILE########
__FILENAME__ = register_utils
import os.path as op

from bento.conv \
    import \
        pkg_to_distutils_meta
from bento.utils.utils \
    import \
        extract_exception
from bento.errors \
    import \
        InvalidPyPIConfig

from six.moves \
    import \
        configparser, StringIO
from six \
    import \
        PY3

import six

if PY3:
    from urllib.request \
        import \
            Request, HTTPBasicAuthHandler, HTTPError, URLError, build_opener
else:
    from urllib2 \
        import \
            Request, HTTPBasicAuthHandler, HTTPError, URLError, build_opener

DEFAULT_REPOSITORY = 'http://pypi.python.org/pypi'
DEFAULT_REALM = 'pypi'

REALM = DEFAULT_REALM
REPOSITORY = 'http://testpypi.python.org/pypi'

_BOUNDARY = six.b('--------------GHSKFJDLGDS7543FJKLFHRE75642756743254')

def _read_new_format(config, repository):
    sections = config.sections()

    # let's get the list of servers
    index_servers = config.get('distutils', 'index-servers')
    _servers = [server.strip() for server in
                index_servers.split('\n')
                if server.strip() != '']
    if _servers == []:
        # nothing set, let's try to get the default pypi
        if 'pypi' in sections:
            _servers = ['pypi']
        else:
            # the file is not properly defined
            raise InvalidPyPIConfig("No index-servers section or pypi section")
    for server in _servers:
        current = PyPIConfig()
        current.username = config.get(server, 'username')
        current.server = server

        # optional params
        def _get_default(key, default):
            if config.has_option(server, key):
                return config.get(server, key)
            else:
                return default

        current.repository = _get_default("repository", DEFAULT_REPOSITORY)
        current.realm = _get_default("realm", DEFAULT_REALM)
        current.password = _get_default("password", None)

        if (current.server == repository or current.repository == repository):
            return current
    raise InvalidPyPIConfig("No section for repository %r found" % repository)

def _read_old_format(config):
    server = 'server-login'
    if config.has_option(server, 'repository'):
        repository = config.get(server, 'repository')
    else:
        repository = DEFAULT_REPOSITORY

    return PyPIConfig(username=config.get(server, 'username'),
            password=config.get(server, 'password'),
            repository=repository,
            server=server,
            realm=DEFAULT_REALM)

def read_pypirc(repository=DEFAULT_REPOSITORY):
    """Read the default .pypirc file.

    Returns a PyPIConfig instance if the default .pypirc can be found. Raises
    an IOError otherwise

    Parameters
    ----------
    repository: str
        repository to use
    """
    rc = op.join(op.expanduser('~'), '.pypirc')
    if op.exists(rc):
        fp = open(rc, "rt")
        try:
            return parse_pypirc(fp, repository)
        finally:
            fp.close()
    else:
        return IOError("Default pypirc config file not found: %r" % rc)

def parse_pypirc(fp, repository=DEFAULT_REPOSITORY):
    """Parse the given pypi config file.

    Returns a PyPIConfig instance if the file can be parsed.

    Parameters
    ----------
    fp: file-like object
        contains the content of the config file
    repository: str
        repository to look for
    """
    config = configparser.RawConfigParser()
    config.readfp(fp)
    sections = config.sections()
    if 'distutils' in sections:
        return _read_new_format(config, repository)
    elif 'server-login' in sections:
        return _read_old_format(config)
    else:
        msg = "Unrecognized format"
        if hasattr(fp, "name"):
            msg += " (for file %r)" % fp.name
        raise ValueError(msg)

class PyPIConfig(object):
    @classmethod
    def from_file(cls, fp=None, repository=DEFAULT_REPOSITORY):
        """Create a PyPIConfig instance from the give file for the give repository.

        Parameters
        ----------
        fp: file-like object or None
            If None, attemps to read the .pypirc file. Otherwise, must be a
            file-like object
        repository: str
            Repository to consider in the .pypirc file.
        """
        if fp is None:
            return read_pypirc(repository)
        else:
            return parse_pypirc(fp, repository)

    @classmethod
    def from_string(cls, s, repository=DEFAULT_REPOSITORY):
        return cls.from_file(StringIO(s), repository)

    def __init__(self, username=None, password=None, repository=None,
            server=None, realm=None):
        self.username = username
        self.password = password
        self.repository = repository
        self.realm = realm
        self.server = server

def encode_multipart(fields, files, boundary=None):
    """Prepare a multipart HTTP request.

    *fields* is a sequence of (name: str, value: str) elements for regular
    form fields, *files* is a sequence of (name: str, filename: str, value:
    bytes) elements for data to be uploaded as files.

    Returns (content_type: bytes, body: bytes) ready for httplib.HTTP.
    """
    # Taken from http://code.activestate.com/recipes/146306

    if boundary is None:
        boundary = _BOUNDARY
    elif not isinstance(boundary, str):
        raise TypeError('boundary must be str, not %r' % type(boundary))

    l = []
    for key, values in fields:
        # handle multiple entries for the same name
        if not isinstance(values, (tuple, list)):
            values = [values]

        for value in values:
            l.extend((
                six.b('--') + boundary,
                # XXX should encode to match packaging but it causes bugs
                ('Content-Disposition: form-data; name="%s"' % key).encode("utf-8"),
                six.b(''),
                value.encode("utf-8")))

    for key, filename, value in files:
        l.extend((
            six.b('--') + boundary,
            ('Content-Disposition: form-data; name="%s"; filename="%s"' %
             (key, filename)).encode("utf-8"),
            six.b(''),
            value))

    l.append(six.b('--') + boundary + six.b('--'))
    l.append(six.b(''))

    body = six.b('\r\n').join(l)
    content_type = six.b('multipart/form-data; boundary=') + boundary
    return content_type, body

def build_post_data(pkg, action):
    data = pkg_to_distutils_meta(pkg)
    data[":action"] = action
    return data

def post_to_server(post_data, config, auth=None):
    """Send the given post_data to the pypi server.

    Parameters
    ----------
    post_data: dict
        Usually the dict returned by build_post_data
    config: object
        A PyPIConfig instance
    auth: object or None
        HTTP authentification object.

    Returns
    -------
    code: int
        HTTP status code
    msg: str
        Message received back from the server
    """
    content_type, body = encode_multipart(post_data.items(), [])

    # build the Request
    headers = {
        'Content-type': content_type,
        'Content-length': str(len(body))
    }
    req = Request(config.repository, body, headers)

    # handle HTTP and include the Basic Auth handler
    opener = build_opener(HTTPBasicAuthHandler(password_mgr=auth))
    try:
        opener.open(req)
    except HTTPError:
        e = extract_exception()
        code, msg = e.code, e.msg
    except URLError:
        e = extract_exception()
        code, msg = 500, str(e)
    else:
        code, msg = 200, 'OK'

    return code, msg

########NEW FILE########
__FILENAME__ = test_register_utils
import os
import shutil
import tempfile

import os.path as op

from six.moves \
    import \
        StringIO

import mock

from bento.pypi.register_utils \
    import \
        build_post_data, encode_multipart, post_to_server, DEFAULT_REALM, \
        PyPIConfig, parse_pypirc
from bento.compat.api.moves \
    import \
        unittest
from bento.core \
    import \
        PackageDescription

from six \
    import \
        PY3

import six

if PY3:
    from urllib.request \
        import \
            HTTPPasswordMgr, urlparse, HTTPError, URLError
    _OPENER_DIRECTOR = "urllib.request.OpenerDirector"
else:
    from urllib2 \
        import \
            Request, HTTPPasswordMgr, HTTPError, URLError
    from urlparse \
        import \
            urlparse
    _OPENER_DIRECTOR = "urllib2.OpenerDirector"

class TestRegisterUtils(unittest.TestCase):
    def test_build_post_data(self):
        r_content = six.b("""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="maintainer"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="name"\r\n\r\n""" \
"""foo\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="license"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="author"\r\n\r\n""" \
"""John Doe\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="url"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name=":action"\r\n\r\n""" \
"""submit\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="download_url"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="maintainer_email"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="author_email"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="version"\r\n\r\n""" \
"""1.0\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="long_description"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254\r\n""" \
"""Content-Disposition: form-data; name="description"\r\n\r\n\r\n""" \
"""----------------GHSKFJDLGDS7543FJKLFHRE75642756743254--\r\n""" \
"""""")
        bento_info = """\
Name: foo
Version: 1.0
Author: John Doe
"""
        package = PackageDescription.from_string(bento_info)
        post_data = build_post_data(package, "submit")
        content_type, body = encode_multipart(post_data.items(), [])
        self.assertEqual(r_content, body)

    @mock.patch(_OPENER_DIRECTOR, mock.MagicMock())
    def test_register_server(self):
        package = PackageDescription(name="foo")
        repository = 'http://testpypi.python.org/pypi'
        realm = DEFAULT_REALM
        config = PyPIConfig(username="cdavid", password="yoyo", repository=repository, realm=realm)

        auth = HTTPPasswordMgr()
        host = urlparse(config.repository)[0]
        auth.add_password(config.realm, host, config.username, config.password)

        post_data = build_post_data(package, "submit")
        code, msg = post_to_server(post_data, config, auth)
        self.assertEqual(code, 200)
        self.assertEqual(msg, "OK")

    @mock.patch("%s.open" % _OPENER_DIRECTOR,
                mock.MagicMock(side_effect=HTTPError("", 404, "", {}, None)))
    def test_register_server_http_errors(self):
        code, msg = self._test_register_server_errors()
        self.assertEqual(code, 404)
        self.assertEqual(msg, "")

    @mock.patch("%s.open" % _OPENER_DIRECTOR,
                mock.MagicMock(side_effect=URLError("")))
    def test_register_server_url_errors(self):
        code, msg = self._test_register_server_errors()
        self.assertEqual(code, 500)

    def _test_register_server_errors(self):
        package = PackageDescription(name="foo")
        config = PyPIConfig.from_string("""
[distutils]
index-servers = pypi

[pypi]
username = cdavid
password = yoyo
server = http://testpypi.python.org
""")

        post_data = build_post_data(package, "submit")
        return post_to_server(post_data, config)

class TestReadPyPI(unittest.TestCase):
    def test_pypi_config(self):
        data = """\
[distutils]
index-servers =
    test

[test]
username:cdavid
password:yoyo
repository:http://testpypi.python.org
"""
        config = PyPIConfig.from_file(StringIO(data), repository="test")
        self.assertEqual(config.username, "cdavid")
        self.assertEqual(config.password, "yoyo")

    def test_new_format_default(self):
        data = """\
[distutils]
index-servers =

[pypi]
username:cdavid
password:yoyo
repository:http://testpypi.python.org
"""
        config = parse_pypirc(StringIO(data), 'pypi')
        self.assertEqual(config.username, "cdavid")
        self.assertEqual(config.password, "yoyo")
        self.assertEqual(config.repository, "http://testpypi.python.org")

    def test_invalid_format(self):
        self.assertRaises(ValueError, parse_pypirc, StringIO(""))

    def test_new_format(self):
        data = """\
[distutils]
index-servers =
    test
    pypi

[test]
username:cdavid
password:yoyo
repository:http://testpypi.python.org

[pypi]
username:david
password:yeye
repository:http://pypi.python.org
"""
        config = parse_pypirc(StringIO(data), "pypi")
        self.assertEqual(config.username, "david")
        self.assertEqual(config.password, "yeye")

        config = parse_pypirc(StringIO(data), repository="test")
        self.assertEqual(config.username, "cdavid")
        self.assertEqual(config.password, "yoyo")
        self.assertEqual(config.repository, "http://testpypi.python.org")

    def test_old_format(self):
        data = """\
[server-login]
username:cdavid
password:yoyo
repository:http://testpypi.python.org
"""
        config = parse_pypirc(StringIO(data))
        self.assertEqual(config.username, "cdavid")
        self.assertEqual(config.password, "yoyo")
        self.assertEqual(config.repository, "http://testpypi.python.org")

class TestReadPyPIDefault(unittest.TestCase):
    def setUp(self):
        self.d = tempfile.mkdtemp()
        try:
            self.p = mock.patch("os.path.expanduser", lambda x: self.d)
            self.p.start()
        except:
            shutil.rmtree(self.d)
            raise

    def tearDown(self):
        self.p.stop()
        shutil.rmtree(self.d)

    def test_read_pypirc(self):
        # Guard to make sure we don't write an actual file by mistake
        try:
            fp = open(op.join(op.expanduser("~"), ".pypirc"), "rt")
            fp.close()
            raise ValueError()
        except IOError:
            pass

        fp = open(op.join(op.expanduser("~"), ".pypirc"), "wt")
        try:
            fp.write("")
        finally:
            fp.close()

########NEW FILE########
__FILENAME__ = test_upload
import os
import shutil
import tempfile

import os.path as op

import mock

import bento.errors
import six

from bento.core.package \
    import \
        PackageDescription
from bento.pypi.register_utils \
    import \
        PyPIConfig
from bento.pypi.upload_utils \
    import \
        build_upload_post_data, build_request, upload

from bento.compat.api import moves

from six \
    import \
        PY3

if PY3:
    from urllib.request \
        import \
            HTTPPasswordMgr, urlparse, HTTPError, URLError
else:
    from urllib2 \
        import \
            Request, urlparse, HTTPError, URLError

# FIXME: there has to be a simpler way to do this
class MockedResult(object):
    def __init__(self, code, msg):
        self._code = code
        self.msg = msg

    def getcode(self):
        return self._code

#def raise_error_factory(url=None, code=200, msg=""):
#    if url is None:
#        url = "http://example.com"
#    def f(request):
#        raise urllib2.HTTPError(url, code, msg, {}, six.moves.StringIO())
#    return f

def my_urlopen_factory(exception):
    def my_urlopen(request):
        raise exception
    return my_urlopen

class TestUpload(moves.unittest.TestCase):
    def setUp(self):
        self.package = PackageDescription.from_string("""\
Name: foo
""")
        self.cwd = tempfile.mkdtemp()
        try:
            self.old_cwd = os.getcwd()
            os.chdir(self.cwd)

            filename = op.join(self.cwd, "foo.bin")
            fp = open(filename, "wb")
            try:
                fp.write(six.b("garbage"))
            finally:
                fp.close()

        except:
            shutil.rmtree(self.cwd)
            raise

    def tearDown(self):
        os.chdir(self.old_cwd)
        shutil.rmtree(self.cwd)

    def test_upload_post_data(self):
        post_data = build_upload_post_data("foo.bin", "bdist_dumb", self.package)
        self.assertEqual(post_data[":action"], "file_upload")
        self.assertEqual(post_data["content"], ("foo.bin", six.b("garbage")))

    def test_signing(self):
        self.assertRaises(NotImplementedError, build_upload_post_data, "foo.bin", "bdist_dumb", self.package, True)

    def test_build_request(self):
        repository = "http://localhost"
        post_data = build_upload_post_data("foo.bin", "bdist_dumb", self.package)
        request = build_request(repository, post_data, "dummy_auth")
        r_headers = {
                "Content-type": six.b("multipart/form-data; boundary=--------------GHSKFJDLGDS7543FJKLFHRE75642756743254"),
                "Content-length": "2229",
                "Authorization": "dummy_auth"}
        self.assertEqual(request.headers, r_headers)

    @mock.patch("bento.pypi.upload_utils.urlopen", lambda request: MockedResult(200, ""))
    def test_upload(self):
        config = PyPIConfig("john", "password", repository="http://localhost")
        upload("foo.bin", "bdist_dumb", self.package, config)

    @mock.patch("bento.pypi.upload_utils.urlopen", my_urlopen_factory(
        HTTPError("", 404, "url not found", {}, six.moves.StringIO())))
    def test_upload_error_404(self):
        config = PyPIConfig("john", "password", repository="http://localhost")
        self.assertRaises(bento.errors.PyPIError, upload, "foo.bin", "bdist_dumb", self.package, config)

    @mock.patch("bento.pypi.upload_utils.urlopen", my_urlopen_factory(URLError("dummy")))
    def test_upload_error_no_host(self):
        config = PyPIConfig("john", "password", repository="http://llocalhost")
        self.assertRaises(bento.errors.PyPIError, upload, "foo.bin", "bdist_dumb", self.package, config)

    @mock.patch("bento.pypi.upload_utils.urlopen", lambda request: MockedResult(200, ""))
    def test_upload_auth(self):
        config = PyPIConfig("john", "password", repository="http://localhost")
        self.assertRaises(NotImplementedError, upload, "foo.bin", "bdist_dumb", self.package, config, True)

########NEW FILE########
__FILENAME__ = upload_utils
import base64
import sys

import os.path as op

from bento.pypi.register_utils \
    import \
        encode_multipart, _BOUNDARY
from bento.conv \
    import \
        pkg_to_distutils_meta_pkg_info
from bento.utils.utils \
    import \
        extract_exception
from bento.errors \
    import \
        PyPIError, InvalidRepository

from six \
    import \
        PY3

import six

try:
    from hashlib import md5
except ImportError:
    from md5 import md5




if PY3:
    from urllib.request \
        import \
            Request, HTTPBasicAuthHandler, HTTPError, URLError, urlparse, urlopen
else:
    from urllib2 \
        import \
            Request, HTTPBasicAuthHandler, HTTPError, URLError, urlopen
    from urlparse \
        import \
            urlparse

def build_upload_post_data(filename, dist_type, package, sign=False, comment=""):
    pyversion = ".".join(str(i) for i in sys.version_info[:2])

    f = open(filename,'rb')
    try:
        content = f.read()
    finally:
        f.close()

    data = pkg_to_distutils_meta_pkg_info(package)
    data[":action"] = "file_upload"
    data["protocol_version"] = "1"
    data.update({
        # file content
        'content': (op.basename(filename), content),
        'filetype': dist_type,
        'pyversion': pyversion,
        'md5_digest': md5(content).hexdigest(),

        # additional meta-data
        'metadata_version' : '1.0',

        'comment': comment,
    })

    if sign:
        raise NotImplementedError("Signing not yet implemented.")
        data['gpg_signature'] = (op.basename(filename) + ".asc",
                                 open(filename+".asc").read())

    return data

def build_request(repository, post_data, auth):
    files = []
    for key in ('content', 'gpg_signature'):
        if key in post_data:
            filename_, value = post_data.pop(key)
            files.append((key, filename_, value))
    content_type, body = encode_multipart(post_data.items(), files)

    headers = {'Content-type': content_type,
               'Content-length': str(len(body)),
               'Authorization': auth}

    return Request(repository, data=body, headers=headers)

def upload(dist_filename, dist_type, package, config, sign=False):
    schema, netloc, url, params, query, fragments = urlparse(config.repository)
    if params or query or fragments:
        raise InvalidRepository("Incompatible url %s" % config.repository)

    if schema not in ('http', 'https'):
        raise InvalidRepository("unsupported schema " + schema)

    if sign:
        raise NotImplementedError()

    data = build_upload_post_data(dist_filename, dist_type, package)
    userpass = (config.username + ":" + config.password).encode("ascii")
    auth = six.b("Basic ") + base64.standard_b64encode(userpass)
    request = build_request(config.repository, data, auth)

    try:
        result = urlopen(request)
        status = result.getcode()
        reason = result.msg
    except HTTPError:
        e = extract_exception()
        status = e.code
        reason = e.msg
    except URLError:
        e = extract_exception()
        reason = e.reason
        raise PyPIError(
                "Could not upload to repository %r - error %s" \
                % (config.repository, reason))

    if status != 200:
        raise PyPIError(
                "Could not upload to repository %r - error %s (server answered '%s')" \
                % (config.repository, status, reason))

########NEW FILE########
__FILENAME__ = decorators
import warnings

from bento.compat.api \
    import \
        wraps
from bento.warnings \
    import \
        NoBentoInfoWarning

def disable_warning(f):
    def decorator_factory(warning_class=UserWarning):
        @wraps(f)
        def decorator(*a, **kw):
            filters = warnings.filters[:]
            warnings.simplefilter("ignore", warning_class)
            try:
                return f(*a, **kw)
            finally:
                warnings.filters = filters
        return decorator
    return decorator_factory

disable_missing_bento_warning = lambda f: disable_warning(f)(NoBentoInfoWarning)

########NEW FILE########
__FILENAME__ = misc
import os

import bento.testing.bentos

from bento.core.package \
    import \
        PackageDescription
from bento.installed_package_description \
    import \
        InstalledSection, build_manifest_meta_from_pkg

# FIXME: use correct install path instead of python package hack
BENTOS_DIR = os.path.dirname(bento.testing.bentos.__file__)
SPHINX_META = os.path.join(BENTOS_DIR, "sphinx_meta.info")

SPHINX_META_PKG = PackageDescription.from_file(SPHINX_META)

def create_simple_build_manifest_args(top_node):
    files = ["scripts/foo.py", "scripts/bar.py"]
    srcdir = "source"

    nodes = [top_node.make_node(os.path.join(srcdir, f)) for f in files]
    for n in nodes:
        n.parent.mkdir()
        n.write("")
    section = InstalledSection.from_source_target_directories("pythonfiles",
                    "section1", os.path.join("$_srcrootdir", srcdir), "$prefix/target", files)
    sections = {"pythonfiles": {"section1": section}}

    meta = build_manifest_meta_from_pkg(SPHINX_META_PKG)
    return meta, sections, nodes


########NEW FILE########
__FILENAME__ = sub_test_case
import os
import sys
import subprocess
import copy

from bento.compat.api.moves \
    import \
        unittest

TEMPLATE = """\
import sys

from bento.compat.api.moves \
    import \
        unittest

from %(module_name)s import %(class_name)s

if __name__ == "__main__":
    result = unittest.TestResult()
    suite = unittest.TestSuite()
    suite.addTest(%(class_name)s("%(test_function)s"))
    suite.run(result)
    if len(result.errors) > 0:
        assert len(result.errors) == 1
        sys.stderr.write(result.errors[0][1])
        sys.exit(1)
    elif len(result.failures) > 0:
        assert len(result.failures) == 1
        sys.stderr.write(result.failures[0][1])
        sys.exit(2)
    else:
        sys.exit(0)
"""

def _execute_test(module_name, class_name, function_name):
    code = TEMPLATE % {"module_name": module_name, "class_name": class_name,
            "test_function": function_name}

    cmd = [sys.executable]

    if sys.platform == "win32":
        env = copy.deepcopy(os.environ)
    else:
        env = {"PYTHONPATH": os.environ.get("PYTHONPATH", "")}
    env["SUBTESTPIPE"] = "1"
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
            stdin=subprocess.PIPE, env=env)
    out, err = p.communicate(code.encode())
    return out, err, p.returncode

class SubprocessTestCase(unittest.TestCase):
    """Simple unittest.TestCase subclass to execute every test in a different
    subprocess.
    
    Class-level tearDown and setUp are executed in the same subprocess as their
    corresponding test.
    """
    def run(self, result=None):
        is_sub = os.environ.get("SUBTESTPIPE", "0")
        if is_sub == "0":
            module_name = self.__module__
            class_name = self.__class__.__name__
            function_name = self._testMethodName
            result.startTest(self)
            try:
                out, err, st = _execute_test(module_name, class_name, function_name)
                out = out.decode()
                err = err.decode()
                sys.stdout.write(out)
                if st == 0:
                    result.addSuccess(self)
                elif st == 1:
                    result.errors.append([self, err])
                    sys.stderr.write("ERROR\n")
                elif st == 2:
                    result.failures.append([self, err])
                    sys.stderr.write("FAILED\n")
                else:
                    raise ValueError("Unexpected subprocess test failure")
            finally:
                result.stopTest(self)
        else:
            return super(SubprocessTestCase, self).run(result)

########NEW FILE########
__FILENAME__ = test_build_manifest
import os
import tempfile
import shutil

from six.moves import StringIO

from bento.compat.api \
    import \
        json
from bento.compat.api.moves \
    import \
        unittest
from bento.core.node \
    import \
        create_root_with_source_tree
from bento.testing.misc \
    import \
        create_simple_build_manifest_args
from bento.installed_package_description \
    import \
        BuildManifest, InstalledSection, iter_files

class TestInstalledSection(unittest.TestCase):
    def test_simple(self):
        files = [("scripts/foo.py", "scripts/foo"),
                 ("scripts/bar.py", "scripts/bar.py")]
        InstalledSection("pythonfiles", "section1", "source", "target", files)

    def test_from_source_target(self):
        files = [("scripts/foo.py", "scripts/foo.py"),
                 ("scripts/bar.py", "scripts/bar.py")]
        r_section = InstalledSection("pythonfiles", "section1", "source", "target", files)

        files = ["scripts/foo.py", "scripts/bar.py"]
        section = InstalledSection.from_source_target_directories("pythonfiles",
                        "section1", "source", "target", files)

        self.assertEqual(r_section.files, section.files)

class TestBUILD_MANIFEST(unittest.TestCase):
    def setUp(self):
        self.src_root = tempfile.mkdtemp()
        self.bld_root = self.src_root

        root = create_root_with_source_tree(self.src_root, self.bld_root)
        self.top_node = root.find_node(self.src_root)

        self.meta, self.sections, self.nodes = create_simple_build_manifest_args(self.top_node)

    def tearDown(self):
        shutil.rmtree(self.top_node.abspath())

    def test_simple_create(self):
        BuildManifest(self.sections, self.meta, {})

    def test_simple_roundtrip(self):
        # FIXME: we compare the loaded json to avoid dealing with encoding
        # differences when comparing objects, but this is kinda stupid
        r_build_manifest = BuildManifest(self.sections, self.meta, {})
        f = StringIO()
        r_build_manifest._write(f)
        r_s = f.getvalue()

        build_manifest = BuildManifest.from_string(r_s)
        f = StringIO()
        build_manifest._write(f)
        s = f.getvalue()
        
        self.assertEqual(json.loads(r_s), json.loads(s))

class TestIterFiles(unittest.TestCase):
    def setUp(self):
        self.src_root = tempfile.mkdtemp()
        self.bld_root = self.src_root

        root = create_root_with_source_tree(self.src_root, self.bld_root)
        self.top_node = root.find_node(self.src_root)

        self.meta, self.sections, nodes = create_simple_build_manifest_args(self.top_node)
        for n in nodes:
            print(n.abspath())

    def tearDown(self):
        shutil.rmtree(self.top_node.abspath())

    def test_simple(self):
        build_manifest = BuildManifest(self.sections, self.meta, {})
        sections = build_manifest.resolve_paths(self.top_node)
        res = sorted([(kind, source.abspath(), target.abspath()) \
                      for kind, source, target in iter_files(sections)])
        target_dir = build_manifest.resolve_path(os.path.join("$prefix", "target"))
        ref = [("pythonfiles", os.path.join(self.top_node.abspath(), "source", "scripts", "bar.py"),
                               os.path.join(target_dir, "scripts", "bar.py")),
               ("pythonfiles", os.path.join(self.top_node.abspath(), "source", "scripts", "foo.py"),
                               os.path.join(target_dir, "scripts", "foo.py"))]
        self.assertEqual(res, ref)

########NEW FILE########
__FILENAME__ = test_misc
import os

from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest
from bento.private.bytecode \
    import \
        bcompile, PyCompileError

def run_with_tempfile(content, function, mode="w"):
    """Create a temporary file with the given content, and execute tbe
    given function after the temporary file has been closed.

    The file is guaranteed to be deleted whether function succeeds or not
    """
    f = NamedTemporaryFile(mode=mode, delete=False)
    try:
        f.write(content)
        f.close()
        return function(f.name)
    finally:
        f.close()
        os.remove(f.name)

class TestBytecode(unittest.TestCase):
    def test_sanity(self):
        s = """print("foo")"""
        run_with_tempfile(s, lambda name: bcompile(name))

    def test_invalid(self):
        s = """print("""
        def f(filename):
            self.assertRaises(PyCompileError, lambda: bcompile(filename))
        run_with_tempfile(s, f)

########NEW FILE########
__FILENAME__ = io2
from bento.utils.os2 \
    import \
        rename

def safe_write(target, writer, mode="wb"):
    """a 'safe' way to write to files.

    Instead of writing directly into a file, this function writes to a
    temporary file, and then rename the file to the target. On sane
    platforms, rename is atomic, so this avoids leaving stale
    files in inconsistent states.

    Parameters
    ----------
    target: str
        destination to write to
    writer: callable
        function which takes one argument, a file descriptor, and
        writes content to it
    mode: str
        opening mode
    """
    f = open(target + ".tmp", mode)
    try:
        writer(f)
    finally:
        f.close()
        rename(target + ".tmp", target)

########NEW FILE########
__FILENAME__ = os2
import errno
import shutil

from bento.utils.utils \
    import \
        extract_exception
from bento.compat.api \
    import \
        rename as _rename

def rename(source, target):
    try:
        _rename(source, target)
    except OSError:
        e = extract_exception()
        if e.errno == errno.EXDEV:
            shutil.move(source, target)
        else:
            raise


########NEW FILE########
__FILENAME__ = path
import os

import os.path as op


def find_root(p):
    """Return the 'root' of the given path.

    Example
    -------
    >>> find_root("/Users/joe")
    '/'
    """
    while p != op.dirname(p):
        p = op.dirname(p)
    return p

def ensure_dir(path):
    d = op.dirname(path)
    if d and not op.exists(d):
        os.makedirs(d)

def unnormalize_path(path):
    if os.sep != "/":
        return path.replace("/", "\\")
    else:
        return path

def normalize_path(path):
    if os.sep != "/":
        return path.replace("\\", "/")
    else:
        return path

########NEW FILE########
__FILENAME__ = test_utils
import os
import sys
import mock
import tempfile
import shutil
import errno

import os.path as op

import mock

from six.moves \
    import \
        StringIO
from bento.compat.api \
    import \
        NamedTemporaryFile
from bento.compat.api.moves \
    import \
        unittest

from bento.utils.utils \
    import subst_vars, to_camel_case, explode_path, same_content, \
        cmd_is_runnable, memoized, comma_list_split, cpu_count, pprint, \
        virtualenv_prefix
from bento.utils.io2 \
    import \
        safe_write
from bento.utils.os2 \
    import \
        rename
import bento.utils.path

def raise_oserror(err):
    ex = OSError()
    ex.errno = err
    raise ex

class TestSubstVars(unittest.TestCase):
    def test_subst_vars_simple(self):
        d = {'prefix': '/usr/local'}
        self.assertEqual(subst_vars('$prefix', d), d['prefix'])

    def test_subst_vars_recursive(self):
        d = {'prefix': '/usr/local',
             'eprefix': '$prefix',
             'datarootdir': '$prefix/share',
             'datadir': '$datarootdir'}
        self.assertEqual(subst_vars('$eprefix', d), '/usr/local')
        self.assertEqual(subst_vars('$datadir', d), '/usr/local/share')

    def test_subst_vars_escaped(self):
        d = {'prefix': '/usr/local',
             'eprefix': '$prefix',
             'datarootdir': '$prefix/share',
             'datadir': '$datarootdir'}
        self.assertEqual(subst_vars('$datadir', d), '/usr/local/share')
        self.assertEqual(subst_vars('$$datadir', d), '$datadir')

    def test_to_camel_case(self):
        d = [("foo", "Foo"), ("foo_bar", "FooBar"), ("_foo_bar", "_FooBar"), ("__fubar", "__Fubar"),
             ("_fubar_", "_Fubar_")]
        for lower, camel in d:
            self.assertEqual(to_camel_case(lower), camel)

class TestExplodePath(unittest.TestCase):
    def test_simple(self):
        path = "/home/joe"
        self.assertEqual(explode_path(path), ["/", "home", "joe"])

        path = "home/joe"
        self.assertEqual(explode_path(path), ["home", "joe"])

    def test_ends_with_sep(self):
        path = "/home/joe/"
        self.assertEqual(explode_path(path), ["/", "home", "joe"])

    def test_empty(self):
        path = ""
        self.assertEqual(explode_path(path), [])

class TestSameFile(unittest.TestCase):
    def test_same(self):
        f1 = NamedTemporaryFile("wt", delete=False)
        try:
            f1.write("fofo")
            f1.close()
            f2 = NamedTemporaryFile("wt", delete=False)
            try:
                f2.write("fofo")
                f2.close()
                self.assertTrue(same_content(f1.name, f2.name))
            finally:
                os.remove(f2.name)
        finally:
            os.remove(f1.name)

    def test_different(self):
        f1 = NamedTemporaryFile("wt", delete=False)
        try:
            f1.write("fofo")
            f1.close()
            f2 = NamedTemporaryFile("wt", delete=False)
            try:
                f2.write("fofa")
                f2.close()
                self.assertFalse(same_content(f1.name, f2.name))
            finally:
                os.remove(f2.name)
        finally:
            os.remove(f1.name)

class TestMisc(unittest.TestCase):
    def test_cmd_is_runnable(self):
        st = cmd_is_runnable(["python", "-c", "''"])
        self.assertTrue(st)

    def test_cmd_is_not_runnable(self):
        st = cmd_is_runnable(["phython", "-c", "''"])
        self.assertFalse(st)

    def test_simple_ensure_dir(self):
        d = tempfile.mkdtemp()
        try:
            new_dir = op.join(d, "foo", "bar")
            new_file = op.join(new_dir, "fubar.txt")
            bento.utils.path.ensure_dir(new_file)
            self.assertTrue(op.exists(new_dir))
        finally:
            shutil.rmtree(d)

    def _test_rename(self):
        d = tempfile.mkdtemp()
        try:
            f = op.join(d, "f.txt")
            fid = open(f, "wt")
            try:
                fid.write("")
            finally:
                fid.close()
            g = op.join(d, "g.txt")
            rename(f, g)
            self.assertTrue(op.exists(g))
        finally:
            shutil.rmtree(d)

    def test_rename(self):
        self._test_rename()

    @mock.patch("bento.utils.os2._rename", lambda x, y: raise_oserror(errno.EXDEV))
    def test_rename_exdev_failure(self):
        self._test_rename()

    @mock.patch("bento.utils.os2._rename", lambda x, y: raise_oserror(errno.EBUSY))
    def test_rename_failure(self):
        self.assertRaises(OSError, self._test_rename)

class TestMemoize(unittest.TestCase):
    def test_simple_no_arguments(self):
        lst = []
        @memoized
        def dummy_function():
            lst.append(1)

        dummy_function()
        dummy_function()

        self.assertEqual(lst, [1])

    def test_simple(self):
        lst = []
        @memoized
        def dummy_function(x):
            lst.append(x)

        dummy_function(1)
        dummy_function(2)
        dummy_function(1)

        self.assertEqual(lst, [1, 2])

    def test_mutable_argument(self):
        lst = []
        @memoized
        def dummy_function(x):
            lst.extend(x)

        dummy_function([1])
        dummy_function([2])
        dummy_function([1])

        self.assertEqual(lst, [1, 2, 1])

    def test_meta(self):
        @memoized
        def dummy_function():
            """some dummy doc"""
        self.assertEqual(repr(dummy_function), "some dummy doc")

    def test_instance_method(self):
        lst = []
        class Foo(object):
            @memoized
            def dummy_function(self, x):
                lst.append(x)
        foo = Foo()
        foo.dummy_function(1)
        foo.dummy_function(1)

        self.assertEqual(lst, [1])

class TestCommaListSplit(unittest.TestCase):
    def test_simple(self):
        self.assertEqual(comma_list_split("-a,-b"), ["-a", "-b"])

class TestPathNormalization(unittest.TestCase):
    @mock.patch("os.sep", "\\")
    def test_simple_win32(self):
        self.assertEqual(bento.utils.path.normalize_path(r"foo\bar"), "foo/bar")
        self.assertEqual(bento.utils.path.unnormalize_path("foo/bar"), r"foo\bar")

    @mock.patch("os.sep", "/")
    def test_simple_unix(self):
        self.assertEqual(bento.utils.path.normalize_path(r"foo\bar"), "foo\\bar")
        self.assertEqual(bento.utils.path.unnormalize_path("foo/bar"), "foo/bar")

class TestPPrint(unittest.TestCase):
    @mock.patch("bento.utils.utils.COLORS_LST", {"USE": True, "RED": "\x1b[01;31m", "NORMAL": "\x1b[0m"})
    def test_simple(self):
        s = StringIO()
        pprint("RED", "foo", s)
        self.assertEqual(s.getvalue(), "\x1b[01;31mfoo\x1b[0m\n")

    @mock.patch("bento.utils.utils.COLORS_LST", {"USE": False})
    def test_simple_no_color(self):
        s = StringIO()
        pprint("RED", "foo", s)
        self.assertEqual(s.getvalue(), "foo\n")

class TestVirtualEnvPrefix(unittest.TestCase):
    @mock.patch("sys.real_prefix", sys.prefix, create=True)
    def test_no_virtualenv(self):
        self.assertEqual(virtualenv_prefix(), None)

    @mock.patch("sys.real_prefix", sys.prefix, create=True)
    @mock.patch("sys.prefix", "yoyo")
    def test_virtualenv(self):
        self.assertEqual(virtualenv_prefix(), "yoyo")

class TestCpuCount(unittest.TestCase):
    def test_native(self):
        self.assertTrue(cpu_count() > 0)

    @mock.patch("sys.platform", "win32")
    def test_win32(self):
        old = os.environ.get("NUMBER_OF_PROCESSORS", None)
        try:
            os.environ["NUMBER_OF_PROCESSORS"] = "2"
            self.assertEqual(cpu_count(), 2)
        finally:
            if old is not None:
                os.environ["NUMBER_OF_PROCESSORS"] = old

    @mock.patch("sys.platform", "bsd")
    @mock.patch("os.popen", lambda s: StringIO("3"))
    def test_bsd(self):
        self.assertEqual(cpu_count(), 3)

########NEW FILE########
__FILENAME__ = utils
import os
import sys
import stat
import re
import glob
import shutil
import errno
import subprocess
import shlex

from six.moves \
    import \
        cPickle

import os.path as op

from bento.compat.api \
    import \
        partial

try:
    from hashlib import md5
except ImportError:
    from md5 import md5



# Color handling for terminals (taken from waf)
COLORS_LST = {
        'USE' : True,
        'BOLD'  :'\x1b[01;1m',
        'RED'   :'\x1b[01;31m',
        'GREEN' :'\x1b[32m',
        'YELLOW':'\x1b[33m',
        'PINK'  :'\x1b[35m',
        'BLUE'  :'\x1b[01;34m',
        'CYAN'  :'\x1b[36m',
        'NORMAL':'\x1b[0m',
        'cursor_on'  :'\x1b[?25h',
        'cursor_off' :'\x1b[?25l',
}

GOT_TTY = not os.environ.get('TERM', 'dumb') in ['dumb', 'emacs']
if GOT_TTY:
    try:
        GOT_TTY = sys.stderr.isatty()
    except AttributeError:
        GOT_TTY = False
if not GOT_TTY or 'NOCOLOR' in os.environ:
    COLORS_LST['USE'] = False

def get_color(cl):
    if not COLORS_LST['USE']:
        return ''
    return COLORS_LST.get(cl, '')

class foo(object):
    def __getattr__(self, a):
        return get_color(a)
    def __call__(self, a):
        return get_color(a)
COLORS = foo()

def pprint(color, s, fout=None):
    if fout is None:
        fout = sys.stderr
    fout.write('%s%s%s\n' % (COLORS(color), s, COLORS('NORMAL')))

_IDPATTERN = "[a-zA-Z_][a-zA-Z_0-9]*"
_DELIM = "$"

def _simple_subst_vars(s, local_vars):
    """Like subst_vars, but does not handle escaping."""
    def _subst(m):
        var_name = m.group(1)
        if var_name in local_vars:
            return local_vars[var_name]
        else:
            raise ValueError("%s not defined" % var_name)
        
    def _resolve(d):
        ret = {}
        for k, v in d.items():
            ret[k] = re.sub("\%s(%s)" % (_DELIM, _IDPATTERN), _subst, v)
        return ret

    ret = _resolve(s)
    while not ret == s:
        s = ret
        ret = _resolve(s)
    return ret

def subst_vars (s, local_vars):
    """Perform shell/Perl-style variable substitution.

    Every occurrence of '$' followed by a name is considered a variable, and
    variable is substituted by the value found in the `local_vars' dictionary.
    Raise ValueError for any variables not found in `local_vars'.

    '$' may be escaped by using '$$'

    Parameters
    ----------
    s: str
        variable to substitute
    local_vars: dict
        dict of variables
    """
    # Resolve variable substitution within the local_vars dict
    local_vars = _simple_subst_vars(local_vars, local_vars)

    def _subst (match):
        named = match.group("named")
        if named is not None:
            if named in local_vars:
                return str(local_vars[named])
            else:
                raise ValueError("Invalid variable '%s'" % named)
        if match.group("escaped") is not None:
            return _DELIM
        raise ValueError("This should not happen")

    def _do_subst(v):
        pattern_s = r"""
        %(delim)s(?:
            (?P<escaped>%(delim)s) |
            (?P<named>%(id)s)
        )""" % {"delim": r"\%s" % _DELIM, "id": _IDPATTERN}
        pattern = re.compile(pattern_s, re.VERBOSE)
        return pattern.sub(_subst, v)

    try:
        return _do_subst(s)
    except KeyError:
        raise ValueError("invalid variable '$%s'" % ex_stack())

# Taken from multiprocessing code
def cpu_count():
    '''
    Returns the number of CPUs in the system
    '''
    if sys.platform == 'win32':
        try:
            num = int(os.environ['NUMBER_OF_PROCESSORS'])
        except (ValueError, KeyError):
            num = 0
    elif 'bsd' in sys.platform or sys.platform == 'darwin':
        try:
            num = int(os.popen('sysctl -n hw.ncpu').read())
        except ValueError:
            num = 0
    else:
        try:
            num = os.sysconf('SC_NPROCESSORS_ONLN')
        except (ValueError, OSError, AttributeError):
            num = 0

    if num >= 1:
        return num
    else:
        return 1
        #raise NotImplementedError('cannot determine number of cpus')

def same_content(f1, f2):
    """Return true if files in f1 and f2 has the same content."""
    fid1 = open(f1, "rb")
    try:
        fid2 = open(f2, "rb")
        try:
            return md5(fid1.read()).digest() == md5(fid2.read()).digest()
        finally:
            fid2.close()
    finally:
        fid1.close()

def virtualenv_prefix():
    """Return the virtual environment prefix if running python is "virtualized"
    (i.e. run inside virtualenv), None otherwise."""
    try:
        real_prefix = sys.real_prefix
    except AttributeError:
        return None
    else:
        if real_prefix != sys.prefix:
            return op.normpath(sys.prefix)
        else:
            return None

def to_camel_case(s):
    """Transform a string to camel case convention."""
    if len(s) < 1:
        return ""
    else:
        # XXX: could most likely be simpler with regex ?
        ret = []
        if s[0].isalpha():
            ret.append(s[0].upper())
            i = 1
        elif s[0] == "_":
            i = 0
            while i < len(s) and s[i] == "_":
                ret.append(s[i])
                i += 1
            if i < len(s) and s[i].isalpha():
                ret.append(s[i].upper())
                i += 1
        else:
            i = 0
        while i < len(s):
            c = s[i]
            if c == "_" and i < len(s) - 1 and s[i+1].isalpha():
                ret.append(s[i+1].upper())
                i += 1
            else:
                ret.append(c)
            i += 1
        return "".join(ret)


# We sometimes use keys from json dictionary as key word arguments. This may
# not work because python2.5 and below requires arguments to be string and not
# unicode while json may decode stuff as unicode, or alternatively encoding as
# ascii does not work in python3 which requires string (==unicode) there. Or
# maybe I just don't understand how this works...
def fix_kw(kw):
    """Make sure the given dictionary may be used as a kw dict independently on
    the python version and encoding of kw's keys."""
    return dict([(str(k), v) for k, v in kw.items()])

def cmd_is_runnable(cmd, **kw):
    """Test whether the given command can work.

    Notes
    -----
    Arguments are the same as subprocess.Popen, except for stdout/stderr
    defaults (to PIPE)
    """
    for stream in ["stdout", "stderr"]:
        if not stream in kw:
            kw[stream] = subprocess.PIPE
    try:
        p = subprocess.Popen(cmd, **kw)
        p.communicate()
        return p.returncode == 0
    except OSError:
        e = extract_exception()
        if e.errno == 2:
            return False
        else:
            raise

def explode_path(path):
    """Split a path into its components.

    If the path is absolute, the first value of the returned list will be '/',
    or the drive letter for platforms where it is applicable.

    Example
    -------
    >>> explode_path("/Users/joe")
    ["/", "Users", "joe"]
    """
    ret = []
    d, p = op.splitdrive(path)

    while p:
        head, tail = op.split(p)
        if head == p:
            ret.append(head)
            break
        if tail:
            ret.append(tail)
        p = head
    if d:
        ret.append(d)
    return ret[::-1]

if sys.version_info[0] < 3:
    def is_string(s):
        return isinstance(s, str) or isinstance(s, unicode)
else:
    def is_string(s):
        return isinstance(s, str)

def extract_exception():
    """Extract the last exception.

    Used to avoid the except ExceptionType as e, which cannot be written the
    same across supported versions. I.e::

        try:
            ...
        except Exception, e:
            ...

    becomes:

        try:
            ...
        except Exception:
            e = extract_exception()
    """
    return sys.exc_info()[1]

# We cannot use octal literal for compat with python 3.x
MODE_755 = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR | stat.S_IRGRP | stat.S_IXGRP | \
    stat.S_IROTH | stat.S_IXOTH
MODE_777 = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR | \
    stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP | \
    stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH

class CommaListLexer(object):
    def __init__(self, instream):
        self._lexer = shlex.shlex(instream, posix=True)
        self._lexer.whitespace += ','
        self._lexer.wordchars += './()*-'
        self.eof = self._lexer.eof

    def get_token(self):
        return self._lexer.get_token()

def comma_list_split(s):
    lexer = CommaListLexer(s)
    ret = []
    t = lexer.get_token()
    while t != lexer.eof:
        ret.append(t)
        t = lexer.get_token()

    return ret

class memoized(object):
    """Decorator that caches a function's return value each time it is called.
    If called later with the same arguments, the cached value is returned, and
    not re-evaluated.
    """
    def __init__(self, func):
       self.func = func
       self.cache = {}
    def __call__(self, *args):
       try:
          return self.cache[args]
       except KeyError:
          value = self.func(*args)
          self.cache[args] = value
          return value
       except TypeError:
          # uncachable -- for instance, passing a list as an argument.
          # Better to not cache than to blow up entirely.
          return self.func(*args)

    def __repr__(self):
       """Return the function's docstring."""
       return self.func.__doc__

    def __get__(self, obj, objtype):
       """Support instance methods."""
       return partial(self.__call__, obj)

def read_or_create_dict(filename):
    if op.exists(filename):
        fid = open(filename, "rb")
        try:
            return cPickle.load(fid)
        finally:
            fid.close()
    else:
        return {}

########NEW FILE########
__FILENAME__ = warnings
class BentoWarning(UserWarning):
    pass

class NoBentoInfoWarning(BentoWarning):
    pass

########NEW FILE########
__FILENAME__ = _config
"""
This module centralizes every internal configuration parameter used throughout
bento.
"""
import os
import sys

import bento

try:
    from bento.__config_py \
        import \
           PKGDATADIR, SITEDIR
except ImportError:
    # Arch-independent path
    PKGDATADIR = os.path.abspath(os.path.dirname(__file__))
    SITEDIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))

# Windows binaries
_CLI = os.path.join(PKGDATADIR, "commands", "cli.exe")
WININST_DIR = os.path.join(PKGDATADIR, "commands", "wininst")

# Parser parameters
_PICKLED_PARSETAB = os.path.join(PKGDATADIR, "parsetab")
_OPTIMIZE_LEX = 0
_DEBUG_YACC = 0

# Use subdist bento to avoid clashing with distutils ATM
_SUB_BUILD_DIR = "bento"

CONFIGURED_STATE_DUMP = os.path.join(_SUB_BUILD_DIR, ".config.bin")
DB_FILE = os.path.join(_SUB_BUILD_DIR, "cache.db")
DISTCHECK_DIR = os.path.join(_SUB_BUILD_DIR, "distcheck")
BUILD_MANIFEST_PATH = os.path.join(_SUB_BUILD_DIR, "build_manifest.info")

BENTO_SCRIPT = "bento.info"

USE_PRIVATE_MODULES = True

########NEW FILE########
__FILENAME__ = bentomaker
#! /usr/bin/env python
#import demandimport
#demandimport.enable()
import sys
import os
import traceback
import warnings

import bento

from bento.utils.utils \
    import \
        pprint, extract_exception
from bento._config \
    import \
        BENTO_SCRIPT, DB_FILE, _SUB_BUILD_DIR
from bento.core \
    import \
        PackageDescription
from bento.compat.api \
    import \
        defaultdict, input
import bento.core.node

from bento.commands.build \
    import \
        BuildCommand
from bento.commands.build_egg \
    import \
        BuildEggCommand
from bento.commands.build_pkg_info \
    import \
        BuildPkgInfoCommand
from bento.commands.build_wininst \
    import \
        BuildWininstCommand
from bento.commands.configure \
    import \
        ConfigureCommand
from bento.commands.core \
    import \
        HelpCommand
from bento.commands.dependency \
    import \
        CommandScheduler
from bento.commands.hooks \
    import \
        find_pre_hooks, find_post_hooks, find_startup_hooks, \
        find_shutdown_hooks, find_options_hooks, find_command_hooks
from bento.commands.install \
    import \
        InstallCommand
from bento.commands.parse \
    import \
        ParseCommand
from bento.commands.register \
    import \
        RegisterPyPI
from bento.commands.registries \
    import \
        CommandRegistry, ContextRegistry, OptionsRegistry
from bento.commands.sdist \
    import \
        SdistCommand
from bento.commands.sphinx_command \
    import \
        SphinxCommand
from bento.commands.upload \
    import \
        UploadPyPI
from bento.commands.options \
    import \
        OptionsContext, Option

from bento.backends.utils \
    import \
        load_backend
from bento.backends.yaku_backend \
    import \
        BuildYakuContext, ConfigureYakuContext
from bento.commands.command_contexts \
    import \
        HelpContext, SdistContext, ContextWithBuildDirectory
from bento.commands.wrapper_utils \
    import \
        set_main, run_with_dependencies
from bento.commands.contexts \
    import \
        GlobalContext
from bento.convert \
    import \
        ConvertCommand, DetectTypeCommand
import bento.errors
import bento.warnings

from bentomakerlib.package_cache \
    import \
        CachedPackage
from bentomakerlib.help \
    import \
        get_usage

if os.environ.get("BENTOMAKER_DEBUG", "0") != "0":
    BENTOMAKER_DEBUG = True
else:
    BENTOMAKER_DEBUG = False

SCRIPT_NAME = 'bentomaker'

# Path relative to build directory
CMD_DATA_DUMP = os.path.join(_SUB_BUILD_DIR, "cmd_data.db")

class GlobalOptions(object):
    def __init__(self, cmd_name, cmd_argv, show_usage, build_directory,
            bento_info, show_version, show_full_version, disable_autoconfigure):
        self.cmd_name = cmd_name
        self.cmd_argv = cmd_argv
        self.show_usage = show_usage
        self.build_directory = build_directory
        self.bento_info = bento_info
        self.show_version = show_version
        self.show_full_version = show_full_version
        self.disable_autoconfigure = disable_autoconfigure

#================================
#   Create the command line UI
#================================
def register_commands(global_context):
    global_context.register_command("help", HelpCommand())
    global_context.register_command("configure", ConfigureCommand())
    global_context.register_command("build", BuildCommand())
    global_context.register_command("install", InstallCommand())
    global_context.register_command("convert", ConvertCommand())
    global_context.register_command("sdist", SdistCommand())
    global_context.register_command("build_egg", BuildEggCommand())
    global_context.register_command("build_wininst", BuildWininstCommand())
    global_context.register_command("sphinx", SphinxCommand())
    global_context.register_command("register_pypi", RegisterPyPI())
    global_context.register_command("upload_pypi", UploadPyPI())

    global_context.register_command("build_pkg_info", BuildPkgInfoCommand(), public=False)
    global_context.register_command("parse", ParseCommand(), public=False)
    global_context.register_command("detect_type", DetectTypeCommand(), public=False)
 
    if sys.platform == "darwin":
        import bento.commands.build_mpkg
        global_context.register_command("build_mpkg",
            bento.commands.build_mpkg.BuildMpkgCommand(), public=False)
        global_context.set_before("build_mpkg", "build")

    if sys.platform == "win32":
        from bento.commands.build_msi \
            import \
                BuildMsiCommand
        global_context.register_command("build_msi", BuildMsiCommand())
        global_context.set_before("build_msi", "build")

def register_options(global_context, cmd_name):
    """Register options for the given command."""
    cmd = global_context.retrieve_command(cmd_name)
    context = OptionsContext.from_command(cmd)

    if not global_context.is_options_context_registered(cmd_name):
        global_context.register_options_context(cmd_name, context)

def register_options_special(global_context):
    # Register options for special topics not attached to a "real" command
    # (e.g. 'commands')
    context = OptionsContext()
    def print_usage():
        print(get_usage(global_context))
    context.parser.print_help = print_usage
    global_context.register_options_context_without_command("commands", context)

    context = OptionsContext()
    def print_help():
        global_options = global_context.retrieve_options_context("")
        p = global_options.parser
        return(p.print_help())
    context.parser.print_help = print_help
    global_context.register_options_context_without_command("globals", context)

def register_command_contexts(global_context):
   # global_context.register_default_context(CmdContext)
    default_mapping = defaultdict(lambda: ContextWithBuildDirectory)
    default_mapping.update(dict([
            ("configure", ConfigureYakuContext),
            ("build", BuildYakuContext),
            ("build_egg", ContextWithBuildDirectory),
            ("build_wininst", ContextWithBuildDirectory),
            ("build_mpkg", ContextWithBuildDirectory),
            ("install", ContextWithBuildDirectory),
            ("sdist", SdistContext),
            ("help", HelpContext)]))

    for cmd_name in global_context.command_names(public_only=False):
        if not global_context.is_command_context_registered(cmd_name):
            global_context.register_command_context(cmd_name, default_mapping[cmd_name])

# All the global state/registration stuff goes here
def register_stuff(global_context):
    register_commands(global_context)
    register_options_special(global_context)
    register_command_contexts(global_context)

def main(argv=None):
    if hasattr(os, "getuid"):
        if os.getuid() == 0:
            pprint("RED", "Using bentomaker under root/sudo is *strongly* discouraged - do you want to continue ? y/N")
            ans = input()
            if not ans.lower() in ["y", "yes"]:
                raise bento.errors.UsageException("bentomaker execution canceld (not using bentomaker with admin privileges)")

    if argv is None:
        argv = sys.argv[1:]

    options_context = create_global_options_context()
    popts = parse_global_options(options_context, argv)

    cmd_name = popts.cmd_name

    if popts.show_version:
        print(bento.__version__)
        return

    if popts.show_full_version:
        print(bento.__version__ + "git" + bento.__git_revision__)
        return

    source_root = os.path.join(os.getcwd(), os.path.dirname(popts.bento_info))
    build_root = os.path.join(os.getcwd(), popts.build_directory)

    top_node, build_node, run_node = bento.core.node.create_base_nodes(source_root, build_root)
    if run_node != top_node and run_node.is_src():
        raise bento.errors.UsageException("You cannot execute bentomaker in a subdirectory of the source tree !")
    if run_node != build_node and run_node.is_bld():
        raise bento.errors.UsageException("You cannot execute bentomaker in a subdirectory of the build tree !")

    global_context = GlobalContext(build_node.make_node(CMD_DATA_DUMP),
                                   CommandRegistry(), ContextRegistry(),
                                   OptionsRegistry(), CommandScheduler())
    global_context.register_options_context_without_command("", options_context)

    if not popts.disable_autoconfigure:
        global_context.set_before("build", "configure")
    global_context.set_before("build_egg", "build")
    global_context.set_before("build_wininst", "build")
    global_context.set_before("install", "build")

    if cmd_name and cmd_name not in ["convert"]:
        return _wrapped_main(global_context, popts, run_node, top_node, build_node)
    else:
        # XXX: is cached package necessary here ?
        cached_package = None
        register_stuff(global_context)
        for cmd_name in global_context.command_names():
            register_options(global_context, cmd_name)
        return _main(global_context, cached_package, popts, run_node, top_node, build_node)

def _wrapped_main(global_context, popts, run_node, top_node, build_node):
    # Some commands work without a bento description file (convert, help)
    # FIXME: this should not be called here then - clearly separate commands
    # which require bento.info from the ones who do not
    bento_info_node = top_node.find_node(BENTO_SCRIPT)
    if bento_info_node is not None:
        db_node = build_node.make_node(DB_FILE)
        cached_package = CachedPackage(db_node)
        package = cached_package.get_package(bento_info_node)
        package_options = cached_package.get_options(bento_info_node)

        if package.use_backends:
            if len(package.use_backends) > 1:
                raise ValueError("Only up to one backend supported for now")
            else:
                assert global_context.backend is None
                global_context.backend = load_backend(package.use_backends[0])()
        global_context.register_package_options(package_options)

        mods = set_main(package, top_node, build_node)

    else:
        warnings.warn("No %r file in current directory - only generic options "
                      "will be displayed" % BENTO_SCRIPT, bento.warnings.NoBentoInfoWarning)
        cached_package = None
        package_options = None
        mods = []

    startup_hooks = find_startup_hooks(mods)
    option_hooks = find_options_hooks(mods)
    shutdown_hooks = find_shutdown_hooks(mods)

    if startup_hooks:
        # FIXME: there should be an error or a warning if startup defined in
        # mods beyond the first one
        startup_hooks[0](global_context)

    if global_context.backend:
        global_context.backend.register_command_contexts(global_context)
    for command in find_command_hooks(mods):
        global_context.register_command(command.name, command)
    register_stuff(global_context)
    for cmd_name in global_context.command_names():
        register_options(global_context, cmd_name)

    if global_context.backend:
        global_context.backend.register_options_contexts(global_context)
    if option_hooks:
        # FIXME: there should be an error or a warning if shutdown defined in
        # mods beyond the first one
        option_hooks[0](global_context)

    # FIXME: this registered options for new commands registered in hook. It
    # should be made all in one place (hook and non-hook)
    for cmd_name in global_context.command_names(public_only=False):
        if not global_context.is_options_context_registered(cmd_name):
            register_options(global_context, cmd_name)

    for cmd_name in global_context.command_names():
        for hook in find_pre_hooks(mods, cmd_name):
            global_context.add_pre_hook(hook, cmd_name)
        for hook in find_post_hooks(mods, cmd_name):
            global_context.add_post_hook(hook, cmd_name)

    try:
        return _main(global_context, cached_package, popts, run_node, top_node, build_node)
    finally:
        if shutdown_hooks:
            shutdown_hooks[0](global_context)

def create_global_options_context():
    context = OptionsContext(usage="%prog [options] [cmd_name [cmd_options]]")
    context.add_option(Option("--version", "-v", dest="show_version", action="store_true",
                              help="Version"))
    context.add_option(Option("--full-version", dest="show_full_version", action="store_true",
                              help="Full version"))
    context.add_option(Option("--build-directory", dest="build_directory",
                              help="Build directory as relative path from cwd (default: '%default')"))
    context.add_option(Option("--bento-info", dest="bento_info",
                              help="Bento location as a relative path from cwd (default: '%default'). " \
                                   "The base name (without its component) must be 'bento.info("))
    context.add_option(Option("--disable-autoconfigure", dest="disable_autoconfigure",
                              action="store_true",
                              default=False,
                              help="""\
Do not automatically run configure before build. In this mode, the user is
expected to know what he is doing. This is mainly useful for developers, to
avoid running configure everytime (default: '%default')."""))
    context.add_option(Option("-h", "--help", dest="show_help", action="store_true",
                              help="Display help and exit"))
    context.parser.set_defaults(show_version=False, show_full_version=False, show_help=False,
                                build_directory="build", bento_info="bento.info")
    return context

def parse_global_options(context, argv):
    global_args, cmd_args = [], []
    for i, a in enumerate(argv):
        if a.startswith("-"):
            global_args.append(a)
        else:
            cmd_args = argv[i:]
            break

    cmd_name = None
    cmd_argv = None
    if cmd_args:
        cmd_name = cmd_args[0]
        cmd_argv = cmd_args[1:]

    o, a = context.parser.parse_args(global_args)
    show_usage = o.show_help
    build_directory = o.build_directory
    if not os.path.basename(o.bento_info) == BENTO_SCRIPT:
        context.parser.error("Invalid value for --bento-info: %r (basename should be %r)" % \
                             (o.bento_info, BENTO_SCRIPT))

    bento_info = o.bento_info
    show_version = o.show_version
    show_full_version = o.show_full_version

    global_options = GlobalOptions(cmd_name, cmd_argv, show_usage,
            build_directory, bento_info, show_version, show_full_version,
            o.disable_autoconfigure)
    return global_options

def _main(global_context, cached_package, popts, run_node, top_node, build_node):
    if popts.show_usage:
        context_klass = global_context.retrieve_command_context("help")
        cmd = global_context.retrieve_command('help')
        cmd.run(context_klass(global_context, [], global_context.retrieve_options_context('help'), None, None))
        return

    cmd_name = popts.cmd_name
    cmd_argv = popts.cmd_argv

    if not cmd_name:
        print("Type '%s help' for usage." % SCRIPT_NAME)
        return 1
    else:
        if not global_context.is_command_registered(cmd_name):
            raise bento.errors.UsageException("%s: Error: unknown command %r" % (SCRIPT_NAME, cmd_name))
        else:
            run_cmd(global_context, cached_package, cmd_name, cmd_argv, run_node, top_node, build_node)

def _get_package_user_flags(global_context, package_options, configure_argv):
    from bento.commands.configure import _get_flag_values

    p = global_context.retrieve_options_context("configure")
    o, a = p.parser.parse_args(configure_argv)
    flag_values = _get_flag_values(package_options.flag_options.keys(), o)

    return flag_values

def is_help_only(global_context, cmd_name, cmd_argv):
    p = global_context.retrieve_options_context(cmd_name)
    o, a = p.parser.parse_args(cmd_argv)
    return o.help is True

def get_running_package(global_context, cached_package, bento_info):
    """Return a PackageDescription instance after evaluation of the flags set
    up at configure time.

    Example
    -------
    For this bento.info::

        Name: foo

        Flag: bundle
            Description: foo
            Default: true

        Library:
            if flag(bundle):
                Packages: yeah

    if bentomaker was run as follows::

        bentomaker configure

    get_running_package will return a PackageDescription instance with
    package.packages set to ["yeah"], and if bentomaker was run as follows::

        bentomaker configure --bundle=false

    get_running_package will return a PackageDescription instance with
    package.packages set to [].
    """
    package_options = cached_package.get_options(bento_info)
    configure_argv = global_context.retrieve_command_argv("configure")
    flag_values = _get_package_user_flags(global_context, package_options, configure_argv)
    return cached_package.get_package(bento_info, flag_values)

def run_cmd(global_context, cached_package, cmd_name, cmd_argv, run_node, top_node, build_node):
    # XXX: fix this special casing (commands which do not need a pkg instance)
    if cmd_name in ["help", "convert"]:
        global_context.run_command(cmd_name, cmd_argv, PackageDescription(), run_node)
        return

    if is_help_only(global_context, cmd_name, cmd_argv):
        options_context = global_context.retrieve_options_context(cmd_name)
        options_context.parser.print_help()
        return

    bento_info = top_node.find_node(BENTO_SCRIPT)
    if bento_info is None:
        raise bento.errors.UsageException("Error: no %s found !" % os.path.join(top_node.abspath(), BENTO_SCRIPT))

    running_package = get_running_package(global_context, cached_package, bento_info)
    run_with_dependencies(global_context, cmd_name, cmd_argv, run_node, top_node, running_package)

    global_context.save_command_argv(cmd_name, cmd_argv)
    global_context.store()

def noexc_main(argv=None):
    def _print_debug():
        if BENTOMAKER_DEBUG:
            tb = sys.exc_info()[2]
            traceback.print_tb(tb)
    def _print_error(msg):
        pprint('RED', msg)
        if not BENTOMAKER_DEBUG:
            pprint('RED', "(You can see the traceback by setting the " \
                          "BENTOMAKER_DEBUG=1 environment variable)")

    try:
        main(argv)
    except bento.errors.BentoError:
        _print_debug()
        e = extract_exception()
        _print_error(str(e))
        sys.exit(2)
    except Exception:
        msg = """\
%s: Error: %s crashed (uncaught exception %s: %s).
Please report this on bento issue tracker:
    http://github.com/cournape/bento/issues"""
        if not BENTOMAKER_DEBUG:
            msg += "\nYou can get a full traceback by setting BENTOMAKER_DEBUG=1"
        else:
            _print_debug()
        e = extract_exception()
        pprint('RED',  msg % (SCRIPT_NAME, SCRIPT_NAME, e.__class__, str(e)))
        sys.exit(1)

if __name__ == '__main__':
    noexc_main()

########NEW FILE########
__FILENAME__ = help
import bento

from bento.commands.core \
    import \
        USAGE, fill_string

def get_usage(global_context):
    ret = [USAGE % {"name": "bentomaker",
                    "version": bento.__version__}]
    ret.append("Bento commands:")

    commands = []
    cmd_names = sorted(global_context.command_names())
    for name in cmd_names:
        v = global_context.retrieve_command(name)
        doc = v.short_descr
        if doc is None:
            doc = "undocumented"
        header = "  %s" % name
        commands.append((header, doc))

    minlen = max([len(header) for header, hlp in commands]) + 2
    for header, hlp in commands:
        ret.append(fill_string(header, minlen) + hlp)
    return "\n".join(ret)


########NEW FILE########
__FILENAME__ = package_cache
"""
Cache version 1

db["version"] : version number
db["magic"]   : "BENTOMAGIC"
db["bento_checkums"] : pickled dictionary {filename: checksum(filename)} for
                       each bento.info (including subentos)
db["package_description"] : pickled PackageDescription instance
db["user_flags"] : pickled user_flags dict
db["parsed_dict"]: pickled raw parsed dictionary (as returned by
                   raw_parse, before having been seen by the visitor)
"""
import os
import sys
import warnings

from bento.parser.misc \
    import \
        raw_parse
from bento.core.package \
    import \
        raw_to_pkg_kw, PackageDescription
from bento.core.options \
    import \
        raw_to_options_kw, PackageOptions
from bento.utils.utils import extract_exception
import bento.utils.path
import bento.utils.io2

if sys.version_info[0] < 3:
    import cPickle as pickle
else:
    import pickle

try:
    from hashlib import md5
except ImportError:
    from md5 import md5


class CachedPackage(object):
    def __init__(self, db_node):
        self._db_location = db_node

    def get_package(self, bento_info, user_flags=None):
        cache = _CachedPackageImpl(self._db_location.abspath())
        try:
            return cache.get_package(bento_info, user_flags)
        finally:
            cache.close()

    def get_options(self, bento_info):
        cache = _CachedPackageImpl(self._db_location.abspath())
        try:
            return cache.get_options(bento_info)
        finally:
            cache.close()

class _CachedPackageImpl(object):
    __version__ = "2"
    __magic__ = "CACHED_PACKAGE_BENTOMAGIC"

    def _has_valid_magic(self, db):
        try:
            magic = db["magic"]
            if not magic == self.__magic__:
                return False
            else:
                return True
        except KeyError:
            return False

    def _reset(self):
        self.db = {}
        self.db["magic"] = self.__magic__
        self.db["version"] = self.__version__
        self._first_time = True

    def _load_existing_cache(self, db_location):
        fid = open(db_location, "rb")
        try:
            db = pickle.load(fid)
            if not self._has_valid_magic(db):
                warnings.warn("Resetting invalid cached db")
                self._reset()
        finally:
            fid.close()

        version = db["version"]
        if version != self.__version__:
            warnings.warn("Resetting invalid version of cached db")
            self._reset()

        return db

    def __init__(self, db_location):
        self._location = db_location
        self._first_time = False
        if not os.path.exists(db_location):
            bento.utils.path.ensure_dir(db_location)
            self._reset()
        else:
            try:
                self.db = self._load_existing_cache(db_location)
            except Exception:
                e = extract_exception()
                warnings.warn("Resetting invalid cached db: (reason: %r)" % e)
                self._reset()

    def _has_invalidated_cache(self):
        if "bentos_checksums" in self.db:
            r_checksums = pickle.loads(self.db["bentos_checksums"])
            for f in r_checksums:
                checksum = md5(open(f, "rb").read()).hexdigest()
                if checksum != r_checksums[f]:
                    return True
            return False
        else:
            return True

    def get_package(self, bento_info, user_flags=None):
        try:
            return self._get_package(bento_info, user_flags)
        except Exception:
            e = extract_exception()
            warnings.warn("Resetting invalid cache (error was %r)" % e)
            self._reset()
            return self._get_package(bento_info, user_flags)

    def _get_package(self, bento_info, user_flags=None):
        if self._first_time:
            self._first_time = False
            return _create_package_nocached(bento_info, user_flags, self.db)
        else:
            if self._has_invalidated_cache():
                return _create_package_nocached(bento_info, user_flags, self.db)
            else:
                r_user_flags = pickle.loads(self.db["user_flags"])
                if user_flags is None:
                    # FIXME: this case is wrong
                    return pickle.loads(self.db["package_description"])
                elif r_user_flags != user_flags:
                    return _create_package_nocached(bento_info, user_flags, self.db)
                else:
                    raw = pickle.loads(self.db["parsed_dict"])
                    pkg, files = _raw_to_pkg(raw, user_flags, bento_info)
                    return pkg

    def get_options(self, bento_info):
        try:
            return self._get_options(bento_info)
        except Exception:
            e = extract_exception()
            warnings.warn("Resetting invalid cache (error was %r)" % e)
            self._reset()
            return self._get_options(bento_info)

    def _get_options(self, bento_info):
        if self._first_time:
            self._first_time = False
            return _create_options_nocached(bento_info, {}, self.db)
        else:
            if self._has_invalidated_cache():
                return _create_options_nocached(bento_info, {}, self.db)
            else:
                raw = pickle.loads(self.db["parsed_dict"])
                return _raw_to_options(raw)

    def close(self):
        bento.utils.io2.safe_write(self._location, lambda fd: pickle.dump(self.db, fd))

def _create_package_nocached(bento_info, user_flags, db):
    pkg, options = _create_objects_no_cached(bento_info, user_flags, db)
    return pkg

def _create_options_nocached(bento_info, user_flags, db):
    pkg, options = _create_objects_no_cached(bento_info, user_flags, db)
    return options

def _raw_to_options(raw):
    kw = raw_to_options_kw(raw)
    return PackageOptions(**kw)

def _raw_to_pkg(raw, user_flags, bento_info):
    kw, files = raw_to_pkg_kw(raw, user_flags, bento_info)
    pkg = PackageDescription(**kw)
    return pkg, files

def _create_objects_no_cached(bento_info, user_flags, db):
    d = os.path.dirname(bento_info.abspath())
    info_file = open(bento_info.abspath(), 'r')
    try:
        data = info_file.read()
        raw = raw_parse(data, bento_info.abspath())

        pkg, files = _raw_to_pkg(raw, user_flags, bento_info)
        files = [os.path.join(d, f) for f in files]
        options = _raw_to_options(raw)

        checksums = [md5(open(f, "rb").read()).hexdigest() for f in files]
        db["bentos_checksums"] = pickle.dumps(dict(zip(files, checksums)))
        db["package_description"] = pickle.dumps(pkg)
        db["user_flags"] = pickle.dumps(user_flags)
        db["parsed_dict"] = pickle.dumps(raw)

        return pkg, options
    finally:
        info_file.close()

########NEW FILE########
__FILENAME__ = test_bentomaker
import os
import sys
import tempfile
import shutil

import os.path as op

import mock
import multiprocessing

from bento.compat.api.moves \
    import \
        unittest
from bento.core.node \
    import \
        create_base_nodes
from bento.utils.utils \
    import \
        extract_exception
from bento.commands.contexts \
    import \
        GlobalContext
from bento.testing.decorators \
    import \
         disable_missing_bento_warning
from bento.testing.sub_test_case \
    import \
         SubprocessTestCase
from bento.errors \
    import \
        UsageException, CommandExecutionFailure, ConvertionError, ParseError

import bentomakerlib.bentomaker
import bento.commands.build_yaku
from bento.compat.dist \
    import \
        DistributionMetadata

from bentomakerlib.bentomaker \
    import \
        main, noexc_main, _wrapped_main, parse_global_options, create_global_options_context

# FIXME: nose is broken - needed to make it happy
if sys.platform == "darwin":
    import bento.commands.build_mpkg
# FIXME: nose is broken - needed to make it happy
# FIXME: nose is broken - needed to make it happy

class Common(unittest.TestCase):
    def setUp(self):
        super(Common, self).setUp()

        self.d = tempfile.mkdtemp()
        self.old = os.getcwd()

        try:
            os.chdir(self.d)
            self.top_node, self.build_node, self.run_node = \
                create_base_nodes(self.d, op.join(self.d, "build"), self.d)
        except:
            os.chdir(self.old)
            shutil.rmtree(self.d)

    def tearDown(self):
        os.chdir(self.old)
        shutil.rmtree(self.d)
        super(Common, self).tearDown()

class TestSpecialCommands(Common):
    @disable_missing_bento_warning
    def test_help_globals(self):
        main(["help", "globals"])

    @disable_missing_bento_warning
    def test_help_commands(self):
        main(["help", "commands"])

    def test_global_options_version(self):
        main(["--version"])

    def test_global_options_full_version(self):
        main(["--full-version"])

    def test_usage(self):
        main(["--help"])

    @disable_missing_bento_warning
    def test_command_help(self):
        main(["configure", "--help"])

class TestMain(Common):
    def test_no_bento(self):
        main([])

    @disable_missing_bento_warning
    def test_help_non_existing_command(self):
        self.assertRaises(UsageException, lambda: main(["help", "floupi"]))

    def test_configure_help(self):
        bento_info = """\
Name: foo
"""
        self.top_node.make_node("bento.info").write(bento_info)
        main(["configure", "--help"])

    def test_help_command(self):
        bento_info = """\
Name: foo
"""
        self.top_node.make_node("bento.info").write(bento_info)
        main(["help", "configure"])

    def test_configure(self):
        bento_info = """\
Name: foo
"""
        self.top_node.make_node("bento.info").write(bento_info)
        main(["configure"])

class TestMainCommands(Common):
    def setUp(self):
        super(TestMainCommands, self).setUp()

        bento_info = """\
Name: foo
"""
        self.top_node.make_node("bento.info").write(bento_info)

    def tearDown(self):
        super(TestMainCommands, self).tearDown()

    def test_configure(self):
        main(["configure"])

    def test_build(self):
        main(["build"])

    def test_install(self):
        main(["install"])

    def test_sdist(self):
        main(["sdist"])

    def test_build_egg(self):
        main(["build_egg"])

    @unittest.skipIf(sys.platform != "win32", "wininst is win32-only test")
    def test_wininst(self):
        main(["build_wininst"])

    @unittest.skipIf(sys.platform != "darwin", "mpkg is darwin-only test")
    def test_mpkg(self):
        main(["build_mpkg"])

# Add SubprocessTestCase mixin as convert depends on distutils which uses
# globals
class TestConvertCommand(Common, SubprocessTestCase):
    def test_convert(self):
        self.top_node.make_node("setup.py").write("""\
from distutils.core import setup

setup(name="foo")
""")
        main(["convert"])
        n = self.top_node.find_node("bento.info")
        r_bento = """\
Name: foo
Version: 0.0.0
Summary: UNKNOWN
Url: UNKNOWN
DownloadUrl: UNKNOWN
Description: UNKNOWN
Author: UNKNOWN
AuthorEmail: UNKNOWN
Maintainer: UNKNOWN
MaintainerEmail: UNKNOWN
License: UNKNOWN
Platforms: UNKNOWN

ExtraSourceFiles:
    setup.py
"""
        self.assertEqual(n.read(), r_bento)

class TestRunningEnvironment(Common):
    def test_in_sub_directory(self):
        bento_info = """\
Name: foo
"""
        self.top_node.make_node("bento.info").write(bento_info)

        subdir_node = self.top_node.make_node("subdir")
        subdir_node.mkdir()

        try:
            os.chdir(subdir_node.abspath())
            self.assertRaises(UsageException, lambda: main(["--bento-info=../bento.info", "configure"]))
        finally:
            os.chdir(self.top_node.abspath())

class TestCommandData(Common):
    def test_simple(self):
        # We use subprocesses to emulate how bentomaker would run itself - this
        # is more of a functional test than a unit test.
        bento_info = """\
Name: foo
"""
        self.top_node.make_node("bento.info").write(bento_info)

        p = multiprocessing.Process(target=noexc_main, args=(['configure', '--prefix=/fubar'],))
        p.start()
        p.join()

        def check_cmd_data(q):
            from bentomakerlib.bentomaker \
                import \
                    CMD_DATA_DUMP
            from bento.utils.utils \
                import \
                    read_or_create_dict

            cmd_data_db = self.build_node.find_node(CMD_DATA_DUMP)
            if cmd_data_db is None:
                raise IOError()
            cmd_data_store = read_or_create_dict(cmd_data_db.abspath())
            q.put(cmd_data_store.get("configure", []))

        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=check_cmd_data, args=(q,))
        p.start()
        self.assertEqual(q.get(timeout=1), ["--prefix=/fubar"])
        p.join()

    def test_flags(self):
        """Test that flag value specified on the command line are correctly
        stored between run."""
        # We use subprocesses to emulate how bentomaker would run itself - this
        # is more of a functional test than a unit test.
        bento_info = """\
Name: foo

Flag: debug
    Description: debug flag
    Default: true

HookFile: bscript

Library:
    if flag(debug):
        Modules: foo
    else:
        Modules: bar
"""
        self.top_node.make_node("bento.info").write(bento_info)
        self.top_node.make_node("bscript").write("""\
import sys
from bento.commands import hooks

@hooks.pre_build
def pre_build(context):
    if not context.pkg.py_modules == ['bar']:
        sys.exit(57)
""")
        self.top_node.make_node("foo.py").write("")
        self.top_node.make_node("bar.py").write("")

        p = multiprocessing.Process(target=main, args=(['configure', '--debug=false'],))
        p.start()
        p.join()

        p = multiprocessing.Process(target=main, args=(['build'],))
        p.start()
        p.join()

        self.assertEqual(p.exitcode, 0)

def raise_function(klass):
    raise klass()

class TestBentomakerError(Common):
    @mock.patch("bentomakerlib.bentomaker.pprint", lambda color, s, fout=None: None)
    def test_simple(self):
        errors = (
            (UsageException, 2),
            (ParseError, 2),
            (ConvertionError, 2),
            (CommandExecutionFailure, 2),
            (bento.errors.ConfigurationError, 2),
            (bento.errors.BuildError, 2),
            (bento.errors.InvalidPackage, 2),
            (Exception, 1),
        )
        for klass, error_code in errors:
            old_main = bentomakerlib.bentomaker.main
            bentomakerlib.bentomaker.main = lambda argv: raise_function(klass)
            try:
                try:
                    noexc_main()
                except SystemExit:
                    e = extract_exception()
                    self.assertEqual(e.code, error_code,
                                     "Expected error code %d for exception type(%r)" % \
                                             (error_code, klass))
            finally:
                bentomakerlib.bentomaker.main = old_main

class TestStartupHook(Common):
    def setUp(self):
        super(TestStartupHook, self).setUp()

        bento_info = """\
Name: foo

HookFile: bscript
"""
        self.top_node.make_node("bento.info").write(bento_info)

    def test_simple(self):
        bscript = """\
from bento.commands import hooks

@hooks.startup
def startup(context):
    context.seen = True
"""
        self.top_node.make_node("bscript").write(bscript)

        global_context = GlobalContext(None)
        options_context = create_global_options_context()
        popts = parse_global_options(options_context, ["configure"])

        _wrapped_main(global_context, popts, self.run_node, self.top_node,
                self.build_node)
        self.assertTrue(getattr(global_context, "seen", False))

    def test_register_command(self):
        bscript = """\
from bento.commands import hooks
from bento.commands.core import Command

@hooks.startup
def startup(context):
    context.register_command("foo", Command())
"""
        self.top_node.make_node("bscript").write(bscript)

        global_context = GlobalContext(None)
        options_context = create_global_options_context()
        popts = parse_global_options(options_context, ["configure"])

        _wrapped_main(global_context, popts, self.run_node, self.top_node,
                self.build_node)
        self.assertTrue(global_context.is_command_registered("foo"))

    def test_register_command_with_options(self):
        bscript = """\
from bento.commands import hooks
from bento.commands.core import Command
from bento.commands.options import OptionsContext, Option

class DocCommand(Command):
    def run(self, context):
        pass

@hooks.startup
def startup(context):
    cmd = DocCommand()
    context.register_command("doc", cmd)

    options_context = OptionsContext.from_command(cmd)
    options_context.add_option(Option("--some-weird-option"))
    context.register_options_context("doc", options_context)
"""
        self.top_node.make_node("bscript").write(bscript)

        global_context = GlobalContext(None)
        options_context = create_global_options_context()
        popts = parse_global_options(options_context, ["doc"])

        _wrapped_main(global_context, popts, self.run_node, self.top_node,
                self.build_node)
        p = global_context.retrieve_options_context("doc").parser
        o, a = p.parse_args(["--some-weird-option=46"])
        self.assertEqual(o.some_weird_option, "46")


    def test_register_existing_command(self):
        bscript = """\
from bento.commands import hooks
from bento.commands.core import Command

@hooks.startup
def startup(context):
    context.register_command("configure", Command)
"""
        self.top_node.make_node("bscript").write(bscript)

        global_context = GlobalContext(None)
        options_context = create_global_options_context()
        popts = parse_global_options(options_context, ["configure"])

        self.assertRaises(ValueError, _wrapped_main,
                          global_context, popts, self.run_node, self.top_node,
                           self.build_node)

########NEW FILE########
__FILENAME__ = bootstrap
import sys
import os

from bento.core \
    import \
        PackageDescription
from bento.utils.utils \
    import \
        pprint, MODE_755

from bento.core.node \
    import \
        create_root_with_source_tree
from bento.commands.script_utils \
    import \
        create_posix_script, create_win32_script

root = create_root_with_source_tree(os.getcwd(), os.path.join(os.getcwd(), "build"))

def _create_executable(name, executable, scripts_node):
    if sys.platform == "win32":
        nodes = create_win32_script(name, executable, scripts_node)
    else:
        nodes = create_posix_script(name, executable, scripts_node)
        for n in nodes:
            n.chmod(MODE_755)
    return nodes

def install_inplace(pkg):
    """Install scripts of pkg in the current directory."""
    for basename, executable in pkg.executables.items():
        version_str = ".".join([str(i) for i in sys.version_info[:2]])
        scripts_node = root._ctx.srcnode
        for name in [basename, "%s-%s" % (basename, version_str)]:
            nodes = _create_executable(name, executable, scripts_node)
            installed = ",".join([n.path_from(scripts_node) for n in nodes])
            pprint("GREEN", "installing %s in current directory" % installed)

def main():
    pkg = PackageDescription.from_file("bento.info")
    if pkg.executables:
        install_inplace(pkg)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = check_examples
import os
import subprocess
import shutil

from bento.core import PackageDescription
from bootstrap import main

main()

tests = []
for root, d, files in os.walk("examples"):
    if os.path.exists(os.path.join(root, "bento.info")):
        if not os.path.exists(os.path.join(root, os.pardir, "bento.info")):
            tests.append(root)

def use_waf(d):
    old_cwd = os.getcwd()
    try:
        os.chdir(d)
        package = PackageDescription.from_file("bento.info")
        return "Waf" in package.use_backends
    finally:
        os.chdir(old_cwd)

def test_package(d):
    def _run():
        for bcmd in [["configure"], ["build"], ["install", "--list-files"]]:
            cmd = [bentomaker] + bcmd
            p = subprocess.Popen(cmd, cwd=test, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
            p.wait()
            if p.returncode:
                print(p.stdout.read().decode())
                return False
        return True
    if use_waf(d) and not "WAFDIR" in os.environ:
        print("waf test and WAFDIR not set, skipped")
        return True
    if os.path.exists(os.path.join(d, "build")):
        shutil.rmtree(os.path.join(d, "build"))
    if _run():
        # We run twice to check that re-running commands with various bento caching
        # does not blow up
        return _run()
    else:
        return False

nerrors = 0
bentomaker = os.path.join(os.getcwd(), "bentomaker")
for test in tests:
    print("=============== testing %s ==============" % test)
    if not test_package(test):
        print("Failed")
        nerrors += 1
    else:
        print("Succeeded")
print("%d / %d example failed" % (nerrors, len(tests)))

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Bento documentation build configuration file, created by
# sphinx-quickstart on Sun Jan  3 12:53:13 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.append(os.path.abspath('.'))
import bento

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.intersphinx', 'sphinx.ext.todo', 'sphinx.ext.coverage', 'sphinx.ext.ifconfig']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['.templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'contents'

# General information about the project.
project = u'Bento'
copyright = u'2009-2011, David Cournapeau'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = bento.__version__
# The full version, including alpha/beta/rc tags.
release = "%s-git%s" % (version, bento.__git_revision__[:10])

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
on_rtd = os.environ.get('READTHEDOCS', None) == 'True'
if on_rtd:
    html_theme = 'default'
else:
    html_theme = 'nature'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['.static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Content template for the index page.
html_index = 'index.html'

# Custom sidebar templates, maps document names to template names.
html_sidebars = {'index': 'indexsidebar.html'}

# Additional templates that should be rendered to pages, maps page names to
# template names.
html_additional_pages = {'index': 'index.html'}

# If false, no module index is generated.
html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'Bentodoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Bento.tex', u'Bento Documentation',
   u'David Cournapeau', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

########NEW FILE########
__FILENAME__ = bar
def foo():
    print "foo"

########NEW FILE########
__FILENAME__ = bar
def foo():
    print "foo"

########NEW FILE########
__FILENAME__ = toytodist
"""bento version of setup.py."""
from distutils.core import setup

from bento.import PackageDescription

info_dict = PackageDescription.from_file('bento.info').to_dict()
setup(**info_dict)

########NEW FILE########
__FILENAME__ = toytodist
"""bento version of setup.py."""
from distutils.core import setup

from bento.import PackageDescription

info_dict = PackageDescription.from_setup('bento.info').to_dict()
setup(**info_dict)

########NEW FILE########
__FILENAME__ = commands
import sys

def main(args=sys.argv):
    print "main args are:", " ".join(args)

########NEW FILE########
__FILENAME__ = bar
def foo():
    print "foo"

########NEW FILE########
__FILENAME__ = toytodist
"""bento version of setup.py."""
from distutils.core import setup
from bento.import PackageDescription
from bento.conv import package_description_to_distutils

pkg = PackageDescription.from_file('bento.info')
info_dict = package_description_to_distutils(pkg)
print info_dict
setup(**info_dict)

########NEW FILE########
__FILENAME__ = bar
def foo():
    print "foo"

########NEW FILE########
__FILENAME__ = foo
def foo():
    print "foo"

########NEW FILE########
__FILENAME__ = toytodist
"""bento version of setup.py."""
from distutils.core import setup

from bento.import PackageDescription

info_dict = PackageDescription.from_file('bento.info').to_dict()
setup(**info_dict)

########NEW FILE########
__FILENAME__ = bar
def foo():
    print "foo"

########NEW FILE########
__FILENAME__ = toytodist
"""bento version of setup.py."""
from distutils.core import setup

from bento.import PackageDescription

info_dict = PackageDescription.form_file('bento.info').to_dict()
setup(**info_dict)

########NEW FILE########
__FILENAME__ = egg2wininst
import sys
import os
import zipfile
import shutil

from bento.core \
    import \
        PackageMetadata
from bento.installed_package_description \
    import \
        InstalledPkgDescription
from bento.commands.build_wininst \
    import \
        create_wininst, wininst_filename
from bento.commands.script_utils \
    import \
        create_scripts

if __name__ == "__main__":
    import tempfile

    TMPDIR = tempfile.mkdtemp()
    try:
        EGG_PATH = sys.argv[1]

        zid = zipfile.ZipFile(EGG_PATH)
        try:
            zid.extractall(path=TMPDIR)
        finally:
            zid.close()
        ipkg = InstalledPkgDescription.from_egg(EGG_PATH)

        # Build executables
        bdir = os.path.join(TMPDIR, "SCRIPTS")
        os.makedirs(bdir)
        create_scripts(ipkg.executables, bdir)
        # XXX: use internal API
        for k in ipkg.files["executables"]:
            ipkg.files["executables"][k].source_dir = bdir

        meta = PackageMetadata.from_ipkg(ipkg)
        wininst = wininst_filename(meta.fullname)
        create_wininst(ipkg, src_root_dir=TMPDIR, wininst=wininst)
    finally:
        #shutil.rmtree(TMPDIR)
        pass

########NEW FILE########
__FILENAME__ = wininst2egg
import os
import sys
import zipfile
import tempfile
import struct
import base64
import shutil

from cStringIO import StringIO

import bento
from bento.installed_package_description \
    import \
        InstalledPkgDescription
from bento.commands.build_egg \
    import \
        build_egg

def read_ipkg(wininst):
    # See eof_cdir size in archive.h of bdist_wininst sources
    eof_cdir_n = 22
    eof_cdir_tag = 0x06054b50

    meta_n = 12

    stat_info = os.stat(WININST)
    inst_size = stat_info.st_size

    with open(WININST, "rb") as fid:
        fid.seek(-eof_cdir_n, 2)
        s = fid.read(4)
        tag = struct.unpack("<l", s)[0]
        if not tag == eof_cdir_tag:
            raise ValueError("Unexpected bits")
        fid.read(2 * 4)
        s = fid.read(4)
        nBytesCDir = struct.unpack("<l", s)[0]
        s = fid.read(4)
        ofsCDir = struct.unpack("<l", s)[0]

        arc_start = inst_size - eof_cdir_n - nBytesCDir - ofsCDir
        ofs = arc_start - meta_n
        fid.seek(ofs, 0)
        s = fid.read(4)
        tag = struct.unpack("<l", s)[0]
        if not tag == 0x1234567B:
            raise ValueError("Unexpected bits")

        s = fid.read(4)
        uncomp_size = struct.unpack("<l", s)[0]

        s = fid.read(4)
        bitmap_size = struct.unpack("<l", s)[0]
        pexe_size = ofs - uncomp_size - bitmap_size

        fid.seek(pexe_size, 0)
        data = fid.read(uncomp_size)

        from ConfigParser import ConfigParser
        parser = ConfigParser()
        sdata = StringIO(data)

        def truncate_null(sdata):
            cur = sdata.tell()
            sdata.seek(0, 2)
            nbytes = sdata.tell()
            try:
                n = 10
                sdata.seek(-n, 2)
                null_ind = sdata.read().find("\0")
                sdata.truncate(nbytes - (n-null_ind))
            finally:
                sdata.seek(cur, 0)
        truncate_null(sdata)
        parser.readfp(sdata)
        raise ValueError("YO - fix wininst METADATA")
        ipkg_str = base64.b64decode(parser.get("IPKG_INFO", "value"))

        ipkg = InstalledPkgDescription.from_string(ipkg_str)
        return ipkg

if __name__ == "__main__":
    from bento.commands.wininst_utils import get_exe_bytes
    if len(sys.argv) < 2:
        WININST = "bento-0.0.3dev-py2.6.win32.exe"
    else:
        WININST = sys.argv[1]

    ipkg = read_ipkg(WININST)
    tmpdir = tempfile.mkdtemp()

    z = zipfile.ZipFile(WININST)
    try:
        z.extractall(path=tmpdir)
        build_egg(ipkg, source_root=os.path.join(tmpdir, "PURELIB"), path=".")
    finally:
        z.close()
        shutil.rmtree(tmpdir)

########NEW FILE########
__FILENAME__ = singledist
"""
Script to build a single-file distribution of bento. This code is
mostly taken from waf
"""
import base64
import os
import re
import sys
import glob
import optparse
import StringIO
from hashlib import md5

import os.path as op

sys.path.insert(0, op.abspath(op.join(op.dirname(__file__), os.pardir)))
try:
    import bento.core.node
    from bento.core import PackageDescription
finally:
    sys.path.pop(0)

ROOT = bento.core.node.Node("", None)

VERSION = "1"

def sfilter(path):
    f = open(path, "r")
    cnt = f.read()
    f.close()

    #cnt = process_decorators(cnt)
    #cnt = process_imports(cnt)
    #if path.endswith('Options.py') or path.endswith('Scripting.py'):
    #   cnt = cnt.replace('Utils.python_24_guard()', '')

    return (StringIO.StringIO(cnt), len(cnt), cnt)

def translate_entry_point(s):
    module, func = s.split(":", 1)
    return "import %s\n%s.%s()" % (module, module, func)

def read_config():
    import ConfigParser
    s = ConfigParser.ConfigParser()
    s.read("config.ini")

    def _parse_comma_list(s_list):
        return [i.strip() for i in s_list.split(",")]

    ret = {}
    section = "main"
    ret["script_template"] = s.get(section, "template")
    ret["script_name"] = s.get(section, "script_name")
    ret["script_pkg_root"] = s.get(section, "package_root")
    ret["script_version"] = s.get(section, "version")
    ret["script_entry_point"] = translate_entry_point(s.get(section, "entry_point"))
    try:
        ret["include_exe"] = s.getboolean(section, "include_exe")
    except ConfigParser.NoOptionError:
        ret["include_exe"] = False
    try:
        ret["include_waf"] = s.getboolean(section, "include_waf")
    except ConfigParser.NoOptionError:
        ret["include_waf"] = False

    ret["extra_files"] = []
    if ret["include_exe"]:
        ret["extra_files"].extend(_parse_comma_list(s.get("include_exe", "extra_files")))

    base_dir = s.get("waf", "base_dir")
    packages = _parse_comma_list(s.get("waf", "packages"))
    ret["waf"] = {"base_dir": base_dir, "packages": packages}
    return ret

def create_script_light(tpl, variables):
    # This is moronic...
    f = open(tpl, "r")
    try:
        cnt = f.read()
        r = {}
        for k, v in variables.items():
            r[k] = re.compile("@%s@" % k)
        lines = []
        for line in cnt.splitlines():
            for k, v in variables.items():
                v_lines = v.splitlines()
                if len(v_lines) > 1:
                    m = r[k].search(line)
                    if m:
                        indent = m.start()
                        v = "\n".join([v_lines[0]] + ["%s%s" % (" " * indent, v_line) for v_line in v_lines[1:]])
                line = r[k].sub(v, line)
            lines.append(line)
        return "\n".join(lines)
    finally:
        f.close()

def create_script(config):
    import tarfile, re

    script_name = config["script_name"]
    extra_files = config["extra_files"]
    print("Creating self-contained script %r in %s" % (script_name, os.getcwd()))
    mw = "tmp-foo-" + VERSION

    zip_type = "bz2"

    tar = tarfile.open('%s.tar.%s' % (mw, zip_type), "w:%s" % zip_type)

    # List of (source, arcname) pairs
    files = []
    nodes = []

    cwd_node = ROOT.find_node(os.getcwd())

    def list_nodes(packages, base_node=cwd_node):
        nodes = []
        for package_name in packages:
            init = os.path.join(*(package_name.split(".") + ["__init__.py"]))
            n = base_node.find_node(init)
            if n is None:
                raise IOError("init file for package %s not found (looked for %r)!" \
                              % (package_name, init))
            else:
                p = n.parent
                nodes.extend(p.find_node(f) for f in p.listdir() if f.endswith(".py"))
        return nodes

    package = PackageDescription.from_file("bento.info", user_flags={"bundle": True, "bundle_yaku": True})
    nodes.extend(list_nodes(package.packages))

    for module in package.py_modules:
        n = cwd_node.find_node(module + ".py")
        if n is None:
            raise IOError("init file for package %s not found (looked for %r)!" \
                          % (package_name, init))
        else:
            nodes.append(n)

    files.extend([(n.abspath(), n.path_from(cwd_node)) for n in nodes])

    if config["include_waf"]:
        base_dir = config["waf"]["base_dir"]
        packages = config["waf"]["packages"]
        base_node = ROOT.find_node(op.expanduser(base_dir))
        if base_node is None:
            raise ValueError("Waf base dir not found (misconfigured ?): %s" % base_dir)
        nodes = list_nodes(packages, base_node)
        files.extend([(n.abspath(), n.path_from(base_node)) for n in nodes])

    for pattern in extra_files:
        for f in glob.glob(pattern):
            files.append((f, f))

    for name, arcname in files:
        tarinfo = tar.gettarinfo(name, arcname)
        tarinfo.uid = tarinfo.gid = 1000
        tarinfo.uname = tarinfo.gname = "baka"
        (code, size, cnt) = sfilter(name)
        tarinfo.size = size
        tar.addfile(tarinfo, code)
    tar.close()

    variables = {}
    for k in ["script_name", "script_pkg_root", "script_version", "script_entry_point"]:
        variables[k] = config[k]
    variables["script_name"] = "'%s'" % variables["script_name"]
    variables["script_pkg_root"] = "'%s'" % variables["script_pkg_root"]

    code1 = create_script_light(config["script_template"], variables)

    f = open("%s.tar.%s" % (mw, zip_type), 'rb')
    cnt = f.read()
    f.close()

    m = md5()
    m.update(cnt)
    REVISION = m.hexdigest()
    reg = re.compile('^REVISION=(.*)', re.M)
    code1 = reg.sub(r'REVISION="%s"' % REVISION, code1)

    f = open(script_name, 'wb')
    #f.write(code1.replace("C1='x'", "C1='%s'" % C1).replace("C2='x'", "C2='%s'" % C2))
    f.write(code1)
    f.write('#==>\n')
    f.write('#')
    f.write(base64.b64encode(cnt))
    f.write('\n')
    f.write('#<==\n')
    f.close()

    if sys.platform != 'win32':
        from bento.utils.utils import MODE_755
        os.chmod(script_name, MODE_755)
    os.unlink('%s.tar.%s' % (mw, zip_type))

def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]

    parser = optparse.OptionParser()
    parser.add_option("--noinclude-exe", action="store_true",
            dest="include_exe", default=False,
            help="Include windows binaries to build bdist installers")
    parser.add_option("--include-waf", action="store_true",
            dest="include_waf", default=False,
            help="Include waf")
    parser.add_option("--waf-dir", dest="waf_dir",
            help="Waf sources (if included)")
    config = read_config()

    opts, args = parser.parse_args(argv)
    if opts.include_exe is not None:
        config["include_exe"] = opts.include_exe
    if opts.include_waf:
        config["include_waf"] = True
    if opts.waf_dir is not None:
        base_dir = opts.waf_dir
        packages = config.get("waf", {}).get("packages", [])
        config["waf"] = {"base_dir": base_dir, "packages": packages}

    create_script(config)

if __name__ == "__main__":
    main()

########NEW FILE########
