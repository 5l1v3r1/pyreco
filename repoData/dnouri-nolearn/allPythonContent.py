__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# nolearn documentation build configuration file, created by
# sphinx-quickstart on Wed Nov 21 19:35:42 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'nolearn'
copyright = u'2012-2014, Daniel Nouri'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.4'
# The full version, including alpha/beta/rc tags.
release = '0.4'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'nolearndoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'nolearn.tex', u'nolearn Documentation',
   u'Daniel Nouri', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'nolearn', u'nolearn Documentation',
     [u'Daniel Nouri'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'nolearn', u'nolearn Documentation',
   u'Daniel Nouri', 'nolearn', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


########NEW FILE########
__FILENAME__ = cache
"""This module contains a decorator :func:`cached` that can be used to
cache the results of any Python functions to disk.

This is useful when you have functions that take a long time to
compute their value, and you want to cache the results of those
functions between runs.

Python's :mod:`pickle` is used to serialize data.  All cache files go
into the `cache/` directory inside your working directory.

`@cached` uses a cache key function to find out if it has the value
for some given function arguments cached on disk.  The way it
calculates that cache key by default is to simply use the string
representation of all arguments passed into the function.  Thus, the
default cache key function looks like this:

.. code-block:: python

    def default_cache_key(*args, **kwargs):
        return str(args) + str(sorted(kwargs.items()))

Here is an example use of the :func:`cached` decorator:

.. code-block:: python

    import math
    @cached()
    def fac(x):
        print 'called!'
        return math.factorial(x)

    fac(20)
    called!
    2432902008176640000
    fac(20)
    2432902008176640000

Often you will want to use a more intelligent cache key, one that
takes more things into account.  Here's an example cache key function
for a cache decorator used with a `transform` method of a scikit-learn
:class:`~sklearn.base.BaseEstimator`:

.. doctest::

    >>> def transform_cache_key(self, X):
    ...     return ','.join([
    ...         str(X[:20]),
    ...         str(X[-20:]),
    ...         str(X.shape),
    ...         str(sorted(self.get_params().items())),
    ...         ])

This function puts the first and the last twenty rows of the matrix
`X` into the cache key.  On top of that, it adds the shape of the
matrix `X.shape` along with the items in `self.get_params`, which with
a scikit-learn :class:`~sklearn.base.BaseEstimator` class is the
dictionary of model parameters.  This makes sure that even though the
input matrix is the same, it will still calculate the value again if
the value of `self.get_params()` is different.

Your estimator class can then use the decorator like so:

.. code-block:: python

    class MyEstimator(BaseEstimator):
        @cached(transform_cache_key)
        def transform(self, X):
            # ...
"""

from functools import wraps
import hashlib
import logging
import random
import os
import string
import traceback

from joblib import numpy_pickle


CACHE_PATH = 'cache/'
if not os.path.exists(CACHE_PATH):  # pragma: no cover
    os.mkdir(CACHE_PATH)

logger = logging.getLogger(__name__)


def default_cache_key(*args, **kwargs):
    return str(args) + str(sorted(kwargs.items()))


class DontCache(Exception):
    pass


def cached(cache_key=default_cache_key, cache_path=None):
    def cached(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Calculation of the cache key is delegated to a function
            # that's passed in via the decorator call
            # (`default_cache_key` by default).
            try:
                key = str(cache_key(*args, **kwargs))
            except DontCache:
                return func(*args, **kwargs)

            hashed_key = hashlib.sha1(key).hexdigest()[:8]

            # We construct the filename using the cache key.  If the
            # file exists, unpickle and return the value.
            filename = os.path.join(
                cache_path or CACHE_PATH,
                '{}.{}-cache-{}'.format(
                    func.__module__, func.__name__, hashed_key))

            if os.path.exists(filename):
                filesize = os.path.getsize(filename)
                size = "%0.1f MB" % (filesize / (1024 * 1024.0))
                logger.debug(" * cache hit: {} ({})".format(filename, size))
                return numpy_pickle.load(filename)
            else:
                logger.debug(" * cache miss: {}".format(filename))
                value = func(*args, **kwargs)
                tmp_filename = '{}-{}.tmp'.format(
                    filename,
                    ''.join(random.sample(string.ascii_letters, 4)),
                    )
                try:
                    numpy_pickle.dump(value, tmp_filename)
                    os.rename(tmp_filename, filename)
                except Exception:
                    logger.exception(
                        "Saving pickle {} resulted in Exception".format(
                        filename))
                return value

        wrapper.uncached = func
        return wrapper
    return cached

########NEW FILE########
__FILENAME__ = caffe
from __future__ import absolute_import

from caffe.imagenet import wrapper
from joblib import Parallel
from joblib import delayed
from nolearn import cache
import numpy as np
from sklearn.base import BaseEstimator
from skimage.io import imread
from skimage.transform import resize

from .util import ChunkedTransform


def _forward_cache_key(self, X):
    if len(X) == 1:
        raise cache.DontCache
    return ','.join([
        str(X),
        self.model_def,
        self.pretrained_model,
        self.oversample,
        ])


def _transform_cache_key(self, X):
    if len(X) == 1 or not isinstance(X[0], str):
        raise cache.DontCache
    return ','.join([
        str(X),
        str(sorted(self.get_params().items())),
        ])


def _prepare_image(cls, image, oversample='center_only'):
    if isinstance(image, str):
        image = imread(image)
    if image.ndim == 2:
        image = np.tile(image[:, :, np.newaxis], (1, 1, 3))

    if oversample in ('center_only', 'corners'):
        # Resize and convert to BGR
        image = (resize(
            image, (cls.IMAGE_DIM, cls.IMAGE_DIM)) * 255)[:, :, ::-1]
        # subtract main
        image -= cls.IMAGENET_MEAN
        return wrapper.oversample(
            image, center_only=oversample == 'center_only')
    else:
        raise ValueError("oversample must be one of 'center_only', 'corners'")


_cached_nets = {}


class CaffeImageNet(ChunkedTransform, BaseEstimator):
    IMAGE_DIM = wrapper.IMAGE_DIM
    CROPPED_DIM = wrapper.CROPPED_DIM
    IMAGENET_MEAN = wrapper.IMAGENET_MEAN

    def __init__(
        self,
        model_def='examples/imagenet_deploy.prototxt',
        pretrained_model='caffe_reference_imagenet_model',
        gpu=True,
        oversample='center_only',
        num_output=1000,
        merge='max',
        batch_size=200,
        verbose=0,
        ):
        self.model_def = model_def
        self.pretrained_model = pretrained_model
        self.gpu = gpu
        self.oversample = oversample
        self.num_output = num_output
        self.merge = merge
        self.batch_size = batch_size
        self.verbose = verbose

    @classmethod
    def Net(cls):
        # soft dependency
        try:
            from caffe import CaffeNet
        except ImportError:
            from caffe import Net as CaffeNet
        return CaffeNet

    @classmethod
    def _create_net(cls, model_def, pretrained_model):
        key = (cls.__name__, model_def, pretrained_model)
        net = _cached_nets.get(key)
        if net is None:
            net = cls.Net()(model_def, pretrained_model)
        _cached_nets[key] = net
        return net

    def fit(self, X=None, y=None):
        self.net_ = self._create_net(self.model_def, self.pretrained_model)
        self.net_.set_phase_test()
        if self.gpu:
            self.net_.set_mode_gpu()
        return self

    @cache.cached(_forward_cache_key)
    def _forward(self, images):
        if isinstance(images[0], str):
            images = Parallel(n_jobs=-1)(delayed(_prepare_image)(
                self.__class__,
                image,
                oversample=self.oversample,
                ) for image in images)

        output_blobs = [
            np.empty((image.shape[0], self.num_output, 1, 1), dtype=np.float32)
            for image in images
            ]

        for i in range(len(images)):
            # XXX We would expect that we can send in a list of input
            #     blobs and output blobs.  However that produces an
            #     assertion error.
            self.net_.Forward([images[i]], [output_blobs[i]])

        return np.vstack([a[np.newaxis, ...] for a in output_blobs])

    @cache.cached(_transform_cache_key)
    def transform(self, X):
        return super(CaffeImageNet, self).transform(X)

    def _compute_features(self, images):
        output_blobs = self._forward(images)

        features = []
        for blob in output_blobs:
            blob = blob.reshape((blob.shape[0], blob.shape[1]))
            if self.merge == 'max':
                blob = blob.max(0)
            else:
                blob = self.merge(blob)
            features.append(blob)

        return np.vstack(features)

    prepare_image = classmethod(_prepare_image)

    def __getstate__(self):
        d = self.__dict__.copy()
        d.pop('net_')
        return d

    def __setstate__(self, state):
        self.__dict__.update(state)
        self.fit()

########NEW FILE########
__FILENAME__ = console
"""The console module contains the :class:`Command` class that's
useful for building command-line scripts.

Consider a function `myfunc` that you want to call directly from the
command-line, but you want to avoid writing glue that deals with
argument parsing, converting those arguments to Python types and
passing them to other functions.  Here's how `myfunc` could look like:

.. code-block:: python

    def myfunc(a_string, a_list):
        print a_string in a_list

`myfunc` takes two arguments, one is expeced to be a string, the other
one a list.

Let's use :class:`Command` to build a console script:

.. code-block:: python

    from nolearn.console import Command

    __doc__ = '''
    Usage:
      myprogram myfunc <config_file> [options]
    '''

    schema = '''
    [myfunc]
    a_string = string
    a_list = listofstrings
    '''

    class Main(Command):
        __doc__ = __doc__
        schema = schema
        funcs = [myfunc]

    main = Main()

Note how we define a `schema` that has a definition of `myfunc`'s
arguments and their types.  See :mod:`nolearn.inischema` for more
details on that.

We can then include this `main` function in our `setup.py` to get a
console script:

.. code-block:: python

    setup(
        name='myprogram',
        # ...
        entry_points='''
        [console_scripts]
        myprogram = myprogram.mymodule.main
        ''',
        )

With this in place, you can now call the `myprogram` script like so:

.. code-block:: bash

    $ myprogram myfunc args.ini

Where `args.ini` might look like:

.. code-block:: ini

    [myfunc]
    a_string = needle
    a_list = haystack haystack needle haystack haystack

These constitute the two named arguments that will be passed into
`myfunc`.  Passing of values is always done through `.ini` files.

You may also call your script with a `--profile=<fn>` option, which
you can use to profile your program using Python's standard
:mod:`cProfile` module.

A `--pdb` option is also available which allows you to automatically
enter post-mortem debugging when your script exits abnormally.
"""

import cProfile
import pdb
import os
import sys
import traceback

import docopt

from .inischema import parse_config

DEFAULT_OPTIONS = """
Options:
  -h --help           Show this screen
  --pdb               Do post mortem debugging on errors
  --profile=<fn>      Save a profile to <fn>
"""


class Command(object):
    __doc__ = None
    schema = None
    funcs = []

    def __init__(self, **kwargs):
        vars(self).update(kwargs)

    def doc(self):
        doc = self.__doc__
        if 'Options:' not in doc:
            doc = doc + DEFAULT_OPTIONS
        return doc

    def __call__(self, argv=sys.argv):
        doc = self.doc()
        arguments = docopt.docopt(doc, argv=argv[1:])
        self.arguments = arguments

        for func in self.funcs:
            if arguments[func.__name__]:
                break
        else:  # pragma: no cover
            raise KeyError("No function found to call.")

        with open(arguments['<config_file>']) as config_file:
            self.config = parse_config(self.schema, config_file.read())

        env = self.config.get('env', {})
        for key, value in env.items():
            os.environ[key.upper()] = value

        kwargs = self.config.get(func.__name__, {})

        # If profiling, wrap the function with another one that does the
        # profiling:
        if arguments.get('--profile'):
            func_ = func

            def prof(**kwargs):
                cProfile.runctx(
                    'func(**kwargs)',
                    globals(),
                    {'func': func_, 'kwargs': kwargs},
                    filename=arguments['--profile'],
                    )
            func = prof

        # If debugging, call pdb.post_mortem() in the except clause:
        try:
            func(**kwargs)
        except:
            if arguments.get('--pdb'):
                traceback.print_exc()
                pdb.post_mortem(sys.exc_traceback)
            else:  # pragma: no cover
                raise

########NEW FILE########
__FILENAME__ = convnet
import os
import sys

from nolearn import cache
import numpy as np
from sklearn.base import BaseEstimator


def _transform_cache_key(self, X):
    if len(X) == 1:
        raise cache.DontCache
    return ','.join([
        str(X),
        str(len(X)),
        str(sorted(self.get_params().items())),
        ])


class ConvNetFeatures(BaseEstimator):
    """Extract features from images using a pretrained ConvNet.

    Based on Yangqing Jia and Jeff Donahue's `DeCAF
    <https://github.com/UCB-ICSI-Vision-Group/decaf-release/wiki>`_.
    Please make sure you read and accept DeCAF's license before you
    use this class.

    If ``classify_direct=False``, expects its input X to be a list of
    image filenames or arrays as produced by
    `np.array(Image.open(filename))`.
    """
    verbose = 0

    def __init__(
        self,
        feature_layer='fc7_cudanet_out',
        pretrained_params='imagenet.decafnet.epoch90',
        pretrained_meta='imagenet.decafnet.meta',
        center_only=True,
        classify_direct=False,
        verbose=0,
        ):
        """
        :param feature_layer: The ConvNet layer that's used for
                              feature extraction.  Defaults to
                              `fc7_cudanet_out`.  A description of all
                              available layers for the
                              ImageNet-1k-pretrained ConvNet is found
                              in the DeCAF wiki.  They are:

                                - `pool5_cudanet_out`
                                - `fc6_cudanet_out`
                                - `fc6_neuron_cudanet_out`
                                - `fc7_cudanet_out`
                                - `fc7_neuron_cudanet_out`
                                - `probs_cudanet_out`

        :param pretrained_params: This must point to the file with the
                                  pretrained parameters.  Defaults to
                                  `imagenet.decafnet.epoch90`.  For
                                  the ImageNet-1k-pretrained ConvNet
                                  this file can be obtained from here:
                                  http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/

        :param pretrained_meta: Similar to `pretrained_params`, this
                                must file to the file with the
                                pretrained parameters' metadata.
                                Defaults to `imagenet.decafnet.meta`.

        :param center_only: Use the center patch of the image only
                            when extracting features.  If `False`, use
                            four corners, the image center and flipped
                            variants and average a total of 10 feature
                            vectors, which will usually yield better
                            results.  Defaults to `True`.

        :param classify_direct: When `True`, assume that input X is an
                                array of shape (num x 256 x 256 x 3)
                                as returned by `prepare_image`.
        """
        self.feature_layer = feature_layer
        self.pretrained_params = pretrained_params
        self.pretrained_meta = pretrained_meta
        self.center_only = center_only
        self.classify_direct = classify_direct
        self.net_ = None

        if (not os.path.exists(pretrained_params) or
            not os.path.exists(pretrained_meta)):
            raise ValueError(
                "Pre-trained ConvNet parameters not found.  You may"
                "need to download the files from "
                "http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/ and "
                "pass the path to the two files as `pretrained_params` and "
                "`pretrained_meta` to the `{}` estimator.".format(
                    self.__class__.__name__))

    def fit(self, X=None, y=None):
        from decaf.scripts.imagenet import DecafNet  # soft dep

        if self.net_ is None:
            self.net_ = DecafNet(
                self.pretrained_params,
                self.pretrained_meta,
                )
        return self

    @cache.cached(_transform_cache_key)
    def transform(self, X):
        features = []
        for img in X:
            if self.classify_direct:
                images = self.net_.oversample(
                    img, center_only=self.center_only)
                self.net_.classify_direct(images)
            else:
                if isinstance(img, str):
                    import Image  # soft dep
                    img = np.array(Image.open(img))
                self.net_.classify(img, center_only=self.center_only)
            feat = None
            for layer in self.feature_layer.split(','):
                val = self.net_.feature(layer)
                if feat is None:
                    feat = val
                else:
                    feat = np.hstack([feat, val])
            if not self.center_only:
                feat = feat.flatten()
            features.append(feat)
            if self.verbose:
                sys.stdout.write(
                    "\r[ConvNet] %d%%" % (100. * len(features) / len(X)))
                sys.stdout.flush()
        if self.verbose:
            sys.stdout.write('\n')
        return np.vstack(features)

    def prepare_image(self, image):
        """Returns image of shape `(256, 256, 3)`, as expected by
        `transform` when `classify_direct = True`.
        """
        from decaf.util import transform  # soft dep
        _JEFFNET_FLIP = True

        # first, extract the 256x256 center.
        image = transform.scale_and_extract(transform.as_rgb(image), 256)
        # convert to [0,255] float32
        image = image.astype(np.float32) * 255.
        if _JEFFNET_FLIP:
            # Flip the image if necessary, maintaining the c_contiguous order
            image = image[::-1, :].copy()
        # subtract the mean
        image -= self.net_._data_mean
        return image

########NEW FILE########
__FILENAME__ = dataset
"""A :class:`Dataset` is a simple abstraction around a `data` and a
`target` matrix.

A Dataset's :attr:`~Dataset.data` and :attr:`~Dataset.target`
attributes are available via attributes of the same name:

.. doctest::

  >>> data = np.array([[3, 2, 1], [2, 1, 0]] * 4)
  >>> target = np.array([3, 2] * 4)
  >>> dataset = Dataset(data, target)
  >>> dataset.data is data
  True
  >>> dataset.target is target
  True

Attribute :attr:`~Dataset.split_indices` gives us a cross-validation
generator:

.. doctest::

  >>> for train_index, test_index in dataset.split_indices:
  ...     X_train, X_test, = data[train_index], data[test_index]
  ...     y_train, y_test, = target[train_index], target[test_index]

An example of where a cross-validation generator like
:attr:`~Dataset.split_indices` returns it is expected is
:class:`sklearn.grid_search.GridSearchCV`.

If all you want is a train/test split of your data, you can simply
call :meth:`Dataset.train_test_split`:

.. doctest::

  >>> X_train, X_test, y_train, y_test = dataset.train_test_split()
  >>> X_train.shape, X_test.shape, y_train.shape, y_test.shape
  ((6, 3), (2, 3), (6,), (2,))
"""

import numpy as np
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn import preprocessing


class Dataset(object):
    n_iterations = 3
    test_size = 0.25
    random_state = 42

    def __init__(self, data, target):
        if isinstance(data, basestring):
            data = np.load(data)
        if isinstance(target, basestring):
            target = np.load(target)
        self.data, self.target = data, target

    def scale(self, **kwargs):
        self.data = preprocessing.scale(self.data, **kwargs)
        return self

    @property
    def split_indices(self):
        return StratifiedShuffleSplit(
            self.target,
            indices=True,
            n_iter=self.n_iterations,
            test_size=self.test_size,
            random_state=self.random_state,
            )

    def train_test_split(self):
        train_index, test_index = iter(self.split_indices).next()
        X_train, X_test, = self.data[train_index], self.data[test_index]
        y_train, y_test, = self.target[train_index], self.target[test_index]
        return X_train, X_test, y_train, y_test


########NEW FILE########
__FILENAME__ = dbn
from datetime import timedelta
from time import time

from gdbn.dbn import buildDBN
from gdbn import activationFunctions
import numpy as np
from sklearn.base import BaseEstimator


class DBN(BaseEstimator):
    """A scikit-learn estimator based on George Dahl's DBN
    implementation `gdbn`.
    """
    def __init__(
        self,
        layer_sizes=None,
        scales=0.05,
        fan_outs=None,
        output_act_funct=None,
        real_valued_vis=True,
        use_re_lu=True,
        uniforms=False,

        learn_rates=0.1,
        learn_rate_decays=1.0,
        learn_rate_minimums=0.0,
        momentum=0.9,
        l2_costs=0.0001,
        dropouts=0,
        nesterov=True,
        nest_compare=True,
        rms_lims=None,

        learn_rates_pretrain=None,
        momentum_pretrain=None,
        l2_costs_pretrain=None,
        nest_compare_pretrain=None,

        epochs=10,
        epochs_pretrain=0,
        loss_funct=None,
        minibatch_size=64,
        minibatches_per_epoch=None,

        pretrain_callback=None,
        fine_tune_callback=None,

        random_state=None,

        verbose=0,
        ):
        """
        Many parameters such as `learn_rates`, `dropouts` etc. will
        also accept a single value, in which case that value will be
        used for all layers.  To control the value per layer, pass a
        list of values instead; see examples below.

        Parameters ending with `_pretrain` may be provided to override
        the given parameter for pretraining.  Consider an example
        where you want the pre-training to use a lower learning rate
        than the fine tuning (the backprop), then you'd maybe pass
        something like::

          DBN([783, 300, 10], learn_rates=0.1, learn_rates_pretrain=0.005)

        If you don't pass the `learn_rates_pretrain` parameter, the
        value of `learn_rates` will be used for both pre-training and
        fine tuning.  (Which seems to not work very well.)

        :param layer_sizes: A list of integers of the form
                            ``[n_vis_units, n_hid_units1,
                            n_hid_units2, ..., n_out_units]``.

                            An example: ``[784, 300, 10]``

                            The number of units in the input layer and
                            the output layer will be set automatically
                            if you set them to -1.  Thus, the above
                            example is equivalent to ``[-1, 300, -1]``
                            if you pass an ``X`` with 784 features,
                            and a ``y`` with 10 classes.

        :param scales: Scale of the randomly initialized weights.  A
                       list of floating point values.  When you find
                       good values for the scale of the weights you
                       can speed up training a lot, and also improve
                       performance.  Defaults to `0.05`.

        :param fan_outs: Number of nonzero incoming connections to a
                         hidden unit.  Defaults to `None`, which means
                         that all connections have non-zero weights.

        :param output_act_funct: Output activation function.  Instance
                                 of type
                                 :class:`~gdbn.activationFunctions.Sigmoid`,
                                 :class:`~.gdbn.activationFunctions.Linear`,
                                 :class:`~.gdbn.activationFunctions.Softmax`
                                 from the
                                 :mod:`gdbn.activationFunctions`
                                 module.  Defaults to
                                 :class:`~.gdbn.activationFunctions.Softmax`.

        :param real_valued_vis: Set `True` (the default) if visible
                                units are real-valued.

        :param use_re_lu: Set `True` to use rectified linear units.
                          Defaults to `False`.

        :param uniforms: Not documented at this time.

        :param learn_rates: A list of learning rates, one entry per
                            weight layer.

                            An example: ``[0.1, 0.1]``

        :param learn_rate_decays: The number with which the
                                  `learn_rate` is multiplied after
                                  each epoch of fine-tuning.

        :param learn_rate_minimums: The minimum `learn_rates`; after
                                    the learn rate reaches the minimum
                                    learn rate, the
                                    `learn_rate_decays` no longer has
                                    any effect.

        :param momentum: Momentum

        :param l2_costs: L2 costs per weight layer.

        :param dropouts: Dropouts per weight layer.

        :param nesterov: Not documented at this time.

        :param nest_compare: Not documented at this time.

        :param rms_lims: Not documented at this time.

        :param learn_rates_pretrain: A list of learning rates similar
                                     to `learn_rates_pretrain`, but
                                     used for pretraining.  Defaults
                                     to value of `learn_rates` parameter.

        :param momentum_pretrain: Momentum for pre-training.  Defaults
                                  to value of `momentum` parameter.

        :param l2_costs_pretrain: L2 costs per weight layer, for
                                  pre-training.  Defaults to the value
                                  of `l2_costs` parameter.

        :param nest_compare_pretrain: Not documented at this time.

        :param epochs: Number of epochs to train (with backprop).

        :param epochs_pretrain: Number of epochs to pre-train (with CDN).

        :param loss_funct: A function that calculates the loss.  Used
                           for displaying learning progress and for
                           :meth:`score`.

        :param minibatch_size: Size of a minibatch.

        :param minibatches_per_epoch: Number of minibatches per epoch.
                                      The default is to use as many as
                                      fit into our training set.

        :param pretrain_callback: An optional function that takes as
                                  arguments the :class:`DBN` instance,
                                  the epoch and the layer index as its
                                  argument, and is called for each
                                  epoch of pretraining.

        :param fine_tune_callback: An optional function that takes as
                                   arguments the :class:`DBN` instance
                                   and the epoch, and is called for
                                   each epoch of fine tuning.

        :param random_state: An optional int used as the seed by the
                             random number generator.

        :param verbose: Debugging output.
        """

        if layer_sizes is None:
            layer_sizes = [-1, -1]

        if output_act_funct is None:
            output_act_funct = activationFunctions.Softmax()
        elif isinstance(output_act_funct, str):
            output_act_funct = getattr(activationFunctions, output_act_funct)()

        if random_state is not None:
            raise ValueError("random_sate must be an int")

        self.layer_sizes = layer_sizes
        self.scales = scales
        self.fan_outs = fan_outs
        self.output_act_funct = output_act_funct
        self.real_valued_vis = real_valued_vis
        self.use_re_lu = use_re_lu
        self.uniforms = uniforms

        self.learn_rates = learn_rates
        self.learn_rate_decays = learn_rate_decays
        self.learn_rate_minimums = learn_rate_minimums
        self.momentum = momentum
        self.l2_costs = l2_costs
        self.dropouts = dropouts
        self.nesterov = nesterov
        self.nest_compare = nest_compare
        self.rms_lims = rms_lims

        self.learn_rates_pretrain = learn_rates_pretrain
        self.momentum_pretrain = momentum_pretrain
        self.l2_costs_pretrain = l2_costs_pretrain
        self.nest_compare_pretrain = nest_compare_pretrain

        self.epochs = epochs
        self.epochs_pretrain = epochs_pretrain
        self.loss_funct = loss_funct
        self.use_dropout = True if dropouts else False
        self.minibatch_size = minibatch_size
        self.minibatches_per_epoch = minibatches_per_epoch

        self.pretrain_callback = pretrain_callback
        self.fine_tune_callback = fine_tune_callback
        self.random_state = random_state
        self.verbose = verbose

    def _fill_missing_layer_sizes(self, X, y):
        layer_sizes = self.layer_sizes
        if layer_sizes[0] == -1:  # n_feat
            layer_sizes[0] = X.shape[1]
        if layer_sizes[-1] == -1 and y is not None:  # n_classes
            layer_sizes[-1] = y.shape[1]

    def _vp(self, value):
        num_weights = len(self.layer_sizes) - 1
        if not hasattr(value, '__iter__'):
            value = [value] * num_weights
        return list(value)

    def _build_net(self, X, y=None):
        v = self._vp

        self._fill_missing_layer_sizes(X, y)
        if self.verbose:  # pragma: no cover
            print "[DBN] layers {}".format(self.layer_sizes)

        if self.random_state is not None:
            np.random.seed(self.random_state)

        net = buildDBN(
            self.layer_sizes,
            v(self.scales),
            v(self.fan_outs),
            self.output_act_funct,
            self.real_valued_vis,
            self.use_re_lu,
            v(self.uniforms),
            )

        return net

    def _configure_net_pretrain(self, net):
        v = self._vp

        self._configure_net_finetune(net)

        learn_rates = self.learn_rates_pretrain
        momentum = self.momentum_pretrain
        l2_costs = self.l2_costs_pretrain
        nest_compare = self.nest_compare_pretrain

        if learn_rates is None:
            learn_rates = self.learn_rates
        if momentum is None:
            momentum = self.momentum
        if l2_costs is None:
            l2_costs = self.l2_costs
        if nest_compare is None:
            nest_compare = self.nest_compare

        net.learnRates = v(learn_rates)
        net.momentum = momentum
        net.L2Costs = v(l2_costs)
        net.nestCompare = nest_compare

        return net

    def _configure_net_finetune(self, net):
        v = self._vp

        net.learnRates = v(self.learn_rates)
        net.momentum = self.momentum
        net.L2Costs = v(self.l2_costs)
        net.dropouts = v(self.dropouts)
        net.nesterov = self.nesterov
        net.nestCompare = self.nest_compare
        net.rmsLims = v(self.rms_lims)

        return net

    def _minibatches(self, X, y=None):
        while True:
            idx = np.random.randint(X.shape[0], size=(self.minibatch_size,))

            X_batch = X[idx]
            if hasattr(X_batch, 'todense'):
                X_batch = X_batch.todense()

            if y is not None:
                yield (X_batch, y[idx])
            else:
                yield X_batch

    def _onehot(self, y):
        if len(y.shape) == 1:
            num_classes = y.max() + 1
            y_new = np.zeros(
                (y.shape[0], num_classes), dtype=np.int)
            for index, label in enumerate(y):
                y_new[index][label] = 1
                y = y_new
        return y

    def _num_mistakes(self, targets, outputs):
        if hasattr(targets, 'as_numpy_array'):  # pragma: no cover
            targets = targets.as_numpy_array()
        if hasattr(outputs, 'as_numpy_array'):
            outputs = outputs.as_numpy_array()
        return np.sum(outputs.argmax(1) != targets.argmax(1))

    def _learn_rate_adjust(self):
        if self.learn_rate_decays == 1.0:
            return

        learn_rate_decays = self._vp(self.learn_rate_decays)
        learn_rate_minimums = self._vp(self.learn_rate_minimums)

        for index, decay in enumerate(learn_rate_decays):
            new_learn_rate = self.net_.learnRates[index] * decay
            if new_learn_rate >= learn_rate_minimums[index]:
                self.net_.learnRates[index] = new_learn_rate

        if self.verbose >= 2:
            print "Learn rates: {}".format(self.net_.learnRates)

    def fit(self, X, y):
        if self.verbose:
            print "[DBN] fitting X.shape=%s" % (X.shape,)
        y = self._onehot(y)

        self.net_ = self._build_net(X, y)

        minibatches_per_epoch = self.minibatches_per_epoch
        if minibatches_per_epoch is None:
            minibatches_per_epoch = X.shape[0] / self.minibatch_size

        loss_funct = self.loss_funct
        if loss_funct is None:
            loss_funct = self._num_mistakes

        errors_pretrain = self.errors_pretrain_ = []
        losses_fine_tune = self.losses_fine_tune_ = []
        errors_fine_tune = self.errors_fine_tune_ = []

        if self.epochs_pretrain:
            self.epochs_pretrain = self._vp(self.epochs_pretrain)
            self._configure_net_pretrain(self.net_)
            for layer_index in range(len(self.layer_sizes) - 1):
                errors_pretrain.append([])
                if self.verbose:  # pragma: no cover
                    print "[DBN] Pre-train layer {}...".format(layer_index + 1)
                time0 = time()
                for epoch, err in enumerate(
                    self.net_.preTrainIth(
                        layer_index,
                        self._minibatches(X),
                        self.epochs_pretrain[layer_index],
                        minibatches_per_epoch,
                        )):
                    errors_pretrain[-1].append(err)
                    if self.verbose:  # pragma: no cover
                        print "  Epoch {}: err {}".format(epoch + 1, err)
                        elapsed = str(timedelta(seconds=time() - time0))
                        print "  ({})".format(elapsed.split('.')[0])
                        time0 = time()
                    if self.pretrain_callback is not None:
                        self.pretrain_callback(
                            self, epoch + 1, layer_index)

        self._configure_net_finetune(self.net_)
        if self.verbose:  # pragma: no cover
            print "[DBN] Fine-tune..."
        time0 = time()
        for epoch, (loss, err) in enumerate(
            self.net_.fineTune(
                self._minibatches(X, y),
                self.epochs,
                minibatches_per_epoch,
                loss_funct,
                self.verbose,
                self.use_dropout,
                )):
            losses_fine_tune.append(loss)
            errors_fine_tune.append(err)
            self._learn_rate_adjust()
            if self.verbose:  # pragma: no cover
                print "Epoch {}:".format(epoch + 1)
                print "  loss {}".format(loss)
                print "  err  {}".format(err)
                elapsed = str(timedelta(seconds=time() - time0))
                print "  ({})".format(elapsed.split('.')[0])
                time0 = time()
            if self.fine_tune_callback is not None:
                self.fine_tune_callback(self, epoch + 1)

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

    def predict_proba(self, X):
        if hasattr(X, 'todense'):
            return self._predict_proba_sparse(X)
        res = np.zeros((X.shape[0], self.layer_sizes[-1]))
        for i, el in enumerate(self.net_.predictions(X, asNumpy=True)):
            res[i] = el
        return res

    def _predict_proba_sparse(self, X):
        batch_size = self.minibatch_size
        res = []
        for i in xrange(0, X.shape[0], batch_size):
            X_batch = X[i:min(i + batch_size, X.shape[0])].todense()
            res.extend(self.net_.predictions(X_batch))
        return np.array(res).reshape(X.shape[0], -1)

    def score(self, X, y):
        loss_funct = self.loss_funct
        if loss_funct is None:
            loss_funct = self._num_mistakes

        outputs = self.predict_proba(X)
        targets = self._onehot(y)
        mistakes = loss_funct(outputs, targets)
        return - float(mistakes) / len(y) + 1


########NEW FILE########
__FILENAME__ = grid_search
""":func:`grid_search` is a wrapper around
:class:`sklearn.grid_search.GridSearchCV`.

:func:`grid_search` adds a printed report to the standard
:class:`GridSearchCV` functionality, so you know about the best score
and parameters.

Usage example:

.. doctest::

  >>> import numpy as np
  >>> from nolearn.dataset import Dataset
  >>> from sklearn.linear_model import LogisticRegression
  >>> data = np.array([[1, 2, 3], [3, 3, 3]] * 20)
  >>> target = np.array([0, 1] * 20)
  >>> dataset = Dataset(data, target)

  >>> model = LogisticRegression()
  >>> parameters = dict(C=[1.0, 3.0])
  >>> grid_search(dataset, model, parameters)  # doctest: +ELLIPSIS
  parameters:
  {'C': [1.0, 3.0]}
  ...
  Best score: 1.0000
  Best grid parameters:
      C=1.0,
  ...
"""

from pprint import pprint

from sklearn.base import BaseEstimator
from sklearn.grid_search import GridSearchCV


def print_report(grid_search, parameters):
    print
    print "== " * 20
    print "All parameters:"
    best_parameters = grid_search.best_estimator_.get_params()
    for param_name, value in sorted(best_parameters.items()):
        if not isinstance(value, BaseEstimator):
            print "    %s=%r," % (param_name, value)

    print
    print "== " * 20
    print "Best score: %0.4f" % grid_search.best_score_
    print "Best grid parameters:"
    for param_name in sorted(parameters.keys()):
        print "    %s=%r," % (param_name, best_parameters[param_name])
    print "== " * 20

    return grid_search


def grid_search(dataset, clf, parameters, cv=None, verbose=4, n_jobs=1,
                **kwargs):
    # See http://scikit-learn.org/stable/modules/grid_search.html

    grid_search = GridSearchCV(
        clf,
        parameters,
        cv=cv,
        verbose=verbose,
        n_jobs=n_jobs,
        **kwargs
        )

    if verbose:
        print "parameters:"
        pprint(parameters)

    grid_search.fit(dataset.data, dataset.target)

    if verbose:
        print_report(grid_search, parameters)

    return grid_search

########NEW FILE########
__FILENAME__ = inischema
""":mod:`inischema` allows the definition of schemas for `.ini`
configuration files.

Consider this sample schema:

.. doctest::

    >>> schema = '''
    ... [first]
    ... value1 = int
    ... value2 = string
    ... value3 = float
    ... value4 = listofstrings
    ... value5 = listofints
    ...
    ... [second]
    ... value1 = string
    ... value2 = int
    ... '''

This schema defines the sections, names and types of values expected
in a schema file.

Using a concrete configuration, we can then use the schema to extract
values:

.. doctest::

    >>> config = '''
    ... [first]
    ... value1 = 2
    ... value2 = three
    ... value3 = 4.4
    ... value4 = five six seven
    ... value5 = 8 9
    ...
    ... [second]
    ... value1 = ten
    ... value2 = 100
    ... value3 = what?
    ... '''

    >>> result = parse_config(schema, config)
    >>> from pprint import pprint
    >>> pprint(result)
    {'first': {'value1': 2,
               'value2': 'three',
               'value3': 4.4,
               'value4': ['five', 'six', 'seven'],
               'value5': [8, 9]},
     'second': {'value1': 'ten', 'value2': 100, 'value3': 'what?'}}

Values in the config file that are not in the schema are assumed to be
strings.

This module is used in :mod:`nolearn.console` to allow for convenient
passing of values from `.ini` files as function arguments for command
line scripts.
"""

from ConfigParser import ConfigParser
from StringIO import StringIO


def string(value):
    return value.strip()


def listofstrings(value):
    return [string(v) for v in value.split()]


def listofints(value):
    return [int(v) for v in value.split()]


converters = {
    'int': int,
    'string': string,
    'float': float,
    'listofstrings': listofstrings,
    'listofints': listofints,
    }


def parse_config(schema, config):
    schemaparser = ConfigParser()
    schemaparser.readfp(StringIO(schema))
    cfgparser = ConfigParser()
    cfgparser.readfp(StringIO(config))

    result = {}
    for section in cfgparser.sections():
        result_section = {}
        schema = {}
        if section in schemaparser.sections():
            schema = dict(schemaparser.items(section))
        for key, value in cfgparser.items(section):
            converter = converters[schema.get(key, 'string')]
            result_section[key] = converter(value)
        result[section] = result_section
    return result

########NEW FILE########
__FILENAME__ = metrics
import numpy as np
from sklearn.base import clone
from sklearn.metrics import f1_score


def multiclass_logloss(actual, predicted, eps=1e-15):
    """Multi class version of Logarithmic Loss metric.

    :param actual: Array containing the actual target classes
    :param predicted: Matrix with class predictions, one probability per class
    """
    # Convert 'actual' to a binary array if it's not already:
    if len(actual.shape) == 1:
        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))
        for i, val in enumerate(actual):
            actual2[i, val] = 1
        actual = actual2

    clip = np.clip(predicted, eps, 1 - eps)
    rows = actual.shape[0]
    vsota = np.sum(actual * np.log(clip))
    return -1.0 / rows * vsota


class LearningCurve(object):
    score_func = staticmethod(f1_score)

    def __init__(self, score_func=None):
        if score_func is None:
            score_func = self.score_func
        self.score_func = score_func

    def predict(self, clf, X):
        return clf.predict(X)

    def __call__(self, dataset, classifier, steps=10,
                 verbose=0, random_state=42):
        """Create a learning curve that uses more training cases with
        each step.

        :param dataset: Dataset to work with
        :type dataset: :class:`~nolearn.dataset.Dataset`
        :param classifier: Classifier for fitting and making predictions.
        :type classifier: :class:`~sklearn.base.BaseEstimator`
        :param steps: Number of steps in the learning curve.
        :type steps: int

        :result: 3-tuple with lists `scores_train`, `scores_test`, `sizes`

        Drawing the resulting learning curve can be done like this:

        .. code-block:: python

          dataset = Dataset()
          clf = LogisticRegression()
          scores_train, scores_test, sizes = learning_curve(dataset, clf)
          pl.plot(sizes, scores_train, 'b', label='training set')
          pl.plot(sizes, scores_test, 'r', label='test set')
          pl.legend(loc='lower right')
          pl.show()
        """
        X_train, X_test, y_train, y_test = dataset.train_test_split()

        scores_train = []
        scores_test = []
        sizes = []

        if verbose:
            print "          n      train      test"

        for frac in np.linspace(0.1, 1.0, num=steps):
            frac_size = X_train.shape[0] * frac
            sizes.append(frac_size)
            X_train1 = X_train[:frac_size]
            y_train1 = y_train[:frac_size]

            clf = clone(classifier)
            clf.fit(X_train1, y_train1)

            predict_train = self.predict(clf, X_train1)
            predict_test = self.predict(clf, X_test)

            score_train = self.score_func(y_train1, predict_train)
            score_test = self.score_func(y_test, predict_test)

            scores_train.append(score_train)
            scores_test.append(score_test)

            if verbose:
                print "   %8d     %0.4f    %0.4f" % (
                    frac_size, score_train, score_test)

        return scores_train, scores_test, sizes


class LearningCurveProbas(LearningCurve):
    score_func = staticmethod(multiclass_logloss)

    def predict(self, clf, X):
        return clf.predict_proba(X)

learning_curve = LearningCurve().__call__
#: Same as :func:`learning_curve` but uses :func:`multiclass_logloss`
#: as the loss funtion.
learning_curve_logloss = LearningCurveProbas().__call__

########NEW FILE########
__FILENAME__ = model
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.externals.joblib import delayed
from sklearn.externals.joblib import Parallel


class AbstractModel(object):
    """A small abstraction around :class:`~sklearn.pipeline.Pipeline`
    objects.

    Allows the convenient parametrization of the underlying pipeline
    through :attr:`~AbstractModel.params`.
    """
    default_params = dict()

    def __init__(self, **kwargs):
        """
        :param kwargs: Keyword arguments correspond to pipeline
                       parameters, and will override parameters in
                       :attr:`~AbstractModel.default_params`.
        """
        params = self.default_params.copy()
        params.update(kwargs)
        self.params = params

    def __call__(self):
        """
        :rtype: :class:`~sklearn.pipeline.Pipeline`
        """
        pipeline = self.pipeline
        pipeline.set_params(**self.params)
        return pipeline

    @property
    def pipeline(self):  # pragma: no cover
        raise NotImplementedError()


def _avgest_fit_est(est, i, X, y, verbose):
    if verbose:
        print "[AveragingEstimator] estimator_%s.fit() ..." % i
    return est.fit(X, y)


def _avgest_predict_proba(est, i, X, verbose):
    if verbose:
        print "[AveragingEstimator] estimator_%s.predict_proba() ..." % i
    return est.predict_proba(X)


class AveragingEstimator(BaseEstimator):
    """An estimator that wraps a list of other estimators and returns
    their average for :meth:`fit`, :meth:`predict` and
    :meth:`predict_proba`.
    """
    def __init__(self, estimators, verbose=0, n_jobs=1):
        """
        :param estimators: List of estimator objects.
        """
        self.estimators = estimators
        self.verbose = verbose
        self.n_jobs = n_jobs

    def fit(self, X, y):
        result = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            delayed(_avgest_fit_est)(est, i, X, y, self.verbose)
            for i, est in enumerate(self.estimators))
        self.estimators = result
        return self

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

    def predict_proba(self, X):
        result = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
            delayed(_avgest_predict_proba)(est, i, X, self.verbose)
            for i, est in enumerate(self.estimators))
        for proba in result[1:]:
            result[0] += proba
        return result[0] / len(self.estimators)

########NEW FILE########
__FILENAME__ = overfeat
from __future__ import absolute_import

import subprocess

import Image
import ImageOps
import numpy as np

from nolearn import cache
from sklearn.base import BaseEstimator

from .util import ChunkedTransform


def _overfeat_cache_key(self, images):
    if len(images) == 1:
        raise cache.DontCache
    if isinstance(images[0], Image.Image):
        images = [im.filename for im in images]
    return ','.join([
        str(images),
        str(self.feature_layer),
        str(self.network_size),
        str(self.pretrained_params),
        ])


class OverFeatShell(ChunkedTransform, BaseEstimator):
    """Extract features from images using a pretrained ConvNet.

    Uses the executable from the OverFeat library by Sermanet et al.
    Please make sure you read and accept OverFeat's license before you
    use this software.
    """

    def __init__(
        self,
        feature_layer=21,
        overfeat_bin='overfeat',
        pretrained_params=None,
        network_size=0,
        merge='maxmean',
        batch_size=200,
        verbose=0,
        ):
        """
        :param feature_layer: The ConvNet layer that's used for
                              feature extraction.  Defaults to layer
                              `21`.

        :param overfeat_bin: The path to the `overfeat` binary.

        :param pretrained_params: The path to the pretrained
                                  parameters file.  These files come
                                  with the overfeat distribution and
                                  can be found in `overfeat/data`.

        :param network_size: Use the small (0) or large network (1).

        :param merge: How spatial features are merged.  May be one of
                      'maxmean', 'meanmax' or a callable.
        """
        self.feature_layer = feature_layer
        self.overfeat_bin = overfeat_bin
        self.pretrained_params = pretrained_params
        self.network_size = network_size
        self.merge = merge
        self.batch_size = batch_size
        self.verbose = verbose

    def fit(self, X=None, y=None):
        return self

    @cache.cached(_overfeat_cache_key)
    def _call_overfeat(self, fnames):
        cmd = [
            self.overfeat_bin,
            '-L', str(self.feature_layer),
            ]
        if self.network_size:
            cmd += ['-l']
        if self.pretrained_params:
            cmd += ['-d', self.pretrained_params]
        cmd += ["'{0}'".format(fn) for fn in fnames]

        output = subprocess.check_output(cmd, stderr=subprocess.STDOUT)

        if output == '':
            raise RuntimeError("Call failed; try lower 'batch_size'")
        elif "unable" in output or "Invalid" in output:
            print fnames
            raise RuntimeError("\n" + output)

        return output.splitlines()

    def _compute_features(self, fnames):
        data = self._call_overfeat(fnames)

        features = []
        for i in range(len(data) / 2):
            n_feat, n_rows, n_cols = data[i * 2].split()
            n_feat, n_rows, n_cols = int(n_feat), int(n_rows), int(n_cols)
            feat = np.fromstring(data[i * 2 + 1], dtype=np.float32, sep=' ')
            feat = feat.reshape(n_feat, n_rows, n_cols)
            if self.merge == 'maxmean':
                feat = feat.max(2).mean(1)
            elif self.merge == 'meanmax':
                feat = feat.mean(2).max(1)
            else:
                feat = self.merge(feat)
            features.append(feat)

        return np.vstack(features)


OverFeat = OverFeatShell  # BBB


class OverFeatPy(ChunkedTransform, BaseEstimator):
    """Extract features from images using a pretrained ConvNet.

    Uses the Python API from the OverFeat library by Sermanet et al.
    Please make sure you read and accept OverFeat's license before you
    use this software.
    """

    kernel_size = 231

    def __init__(
        self,
        feature_layer=21,
        pretrained_params='net_weight_0',
        network_size=None,
        merge='maxmean',
        batch_size=200,
        verbose=0,
        ):
        """
        :param feature_layer: The ConvNet layer that's used for
                              feature extraction.  Defaults to layer
                              `21`.  Please refer to `this post
                              <https://groups.google.com/forum/#!topic/overfeat/hQeI5hcw8f0>`_
                              to find out which layers are available
                              for the two different networks.

        :param pretrained_params: The path to the pretrained
                                  parameters file.  These files come
                                  with the overfeat distribution and
                                  can be found in `overfeat/data`.

        :param merge: How spatial features are merged.  May be one of
                      'maxmean', 'meanmax' or a callable.
        """
        if network_size is None:
            network_size = int(pretrained_params[-1])
        self.feature_layer = feature_layer
        self.pretrained_params = pretrained_params
        self.network_size = network_size
        self.merge = merge
        self.batch_size = batch_size
        self.verbose = verbose

    def fit(self, X=None, y=None):
        import overfeat  # soft dep
        overfeat.init(self.pretrained_params, self.network_size)
        return self

    @classmethod
    def prepare_image(cls, image):
        if isinstance(image, str):
            image = Image.open(image)

        if isinstance(image, Image.Image):
            if (image.size[0] < cls.kernel_size or
                image.size[1] < cls.kernel_size):
                image = ImageOps.fit(image, (cls.kernel_size, cls.kernel_size))
            image = np.array(image)

        image = image.swapaxes(1, 2).swapaxes(0, 1).astype(np.float32)
        return image

    @cache.cached(_overfeat_cache_key)
    def _compute_features(self, images):
        import overfeat  # soft dep

        features = []
        for image in images:
            image = self.prepare_image(image)
            overfeat.fprop(image)
            feat = overfeat.get_output(self.feature_layer)
            if self.merge == 'maxmean':
                feat = feat.max(2).mean(1)
            elif self.merge == 'meanmax':
                feat = feat.mean(2).max(1)
            else:
                feat = self.merge(feat)
            features.append(feat)
        return np.vstack(features)

    def __getstate__(self):
        return self.__dict__.copy()

    def __setstate__(self, state):
        self.__dict__.update(state)
        self.fit()

########NEW FILE########
__FILENAME__ = test_cache
from mock import patch


def test_cached(tmpdir):
    from ..cache import cached

    called = []

    @cached(cache_path=str(tmpdir))
    def add(one, two):
        called.append([one, two])
        return one + two

    assert add(2, 3) == 5
    assert len(called) == 1
    assert add(2, 3) == 5
    assert len(called) == 1
    assert add(2, 4) == 6
    assert len(called) == 2


def test_cache_with_cache_key(tmpdir):
    from ..cache import cached

    called = []

    def cache_key(one, two):
        return one

    @cached(cache_key=cache_key, cache_path=str(tmpdir))
    def add(one, two):
        called.append([one, two])
        return one + two

    assert add(2, 3) == 5
    assert len(called) == 1
    assert add(2, 3) == 5
    assert len(called) == 1
    assert add(2, 4) == 5
    assert len(called) == 1


def test_cache_systemerror(tmpdir):
    from ..cache import cached

    @cached(cache_path=str(tmpdir))
    def add(one, two):
        return one + two

    with patch('nolearn.cache.numpy_pickle.dump') as dump:
        dump.side_effect = SystemError()
        assert add(2, 3) == 5

########NEW FILE########
__FILENAME__ = test_console
import os
from tempfile import NamedTemporaryFile

from mock import patch

from test_inischema import SAMPLE_CONFIGURATION
from test_inischema import SAMPLE_SCHEMA


SAMPLE_CONFIGURATION += """
[env]
somekey = somevalue
"""


class TestCommand(object):

    with NamedTemporaryFile(delete=False) as config_file:
        config_file.write(SAMPLE_CONFIGURATION)

    def some_filename(self):
        with NamedTemporaryFile(delete=False) as some_file:
            return some_file.name

    def test_simple(self):
        from ..console import Command

        called = []

        def second(value1, value2=None):
            called.append((value1, value2))

        class MyCommand(Command):
            __doc__ = """
            Usage:
              script second <config_file>
            """
            schema = SAMPLE_SCHEMA
            funcs = [second]

        argv = ['script', 'second', self.config_file.name]
        MyCommand()(argv)
        assert len(called) == 1
        assert called[0][0].startswith('a few line breaks')
        assert called[0][1] is None
        assert os.environ['SOMEKEY'] == 'somevalue'

    def test_profiler(self):
        from ..console import Command

        called = []

        def second(value1, value2=None):
            called.append((value1, value2))

        class MyCommand(Command):
            __doc__ = """
            Usage:
              script second <config_file> [--profile=<file>]
            """
            schema = SAMPLE_SCHEMA
            funcs = [second]

        profile_filename = self.some_filename()
        argv = ['script', 'second', self.config_file.name,
                '--profile', profile_filename]
        MyCommand()(argv)
        assert len(called) == 1

        with open(profile_filename) as f:
            assert(len(f.read()) > 1)

    @patch('nolearn.console.pdb.post_mortem')
    @patch('nolearn.console.traceback.print_exc')
    def test_pdb(self, print_exc, post_mortem):
        from ..console import Command

        called = []

        def second(value1, value2=None):
            called.append((value1, value2))
            raise ValueError()

        class MyCommand(Command):
            __doc__ = """
            Usage:
              script second <config_file> [--pdb]
            """
            schema = SAMPLE_SCHEMA
            funcs = [second]

        argv = ['script', 'second', self.config_file.name, '--pdb']
        MyCommand()(argv)
        assert len(called) == 1

########NEW FILE########
__FILENAME__ = test_dataset
from mock import patch
import numpy as np


def test_dataset_simple():
    from ..dataset import Dataset

    data = object()
    target = object()
    dataset = Dataset(data, target)
    assert dataset.data is data
    assert dataset.target is target


@patch('nolearn.dataset.np.load')
def test_dataset_with_filenames(load):
    from ..dataset import Dataset

    data = 'datafile'
    target = 'targetfile'
    dataset = Dataset(data, target)
    assert load.call_count == 2
    assert dataset.target is load.return_value


def test_dataset_train_test_split():
    from ..dataset import Dataset

    data = np.arange(100)
    target = np.array([0] * 50 + [1] * 50)
    dataset = Dataset(data, target)

    assert dataset.split_indices.classes.tolist() == [0, 1]
    assert dataset.split_indices.n_train == 75
    assert dataset.split_indices.n_test == 25

    X_train, X_test, y_train, y_test = dataset.train_test_split()
    assert len(X_train) == len(y_train)
    assert len(X_test) == len(y_test)


def test_dataset_scale():
    from ..dataset import Dataset

    data = np.arange(100).astype('float')
    target = np.array([0] * 100)
    dataset = Dataset(data, target)

    dataset.scale()
    assert dataset.data[0] == -1.7148160424389376
    assert dataset.data[-1] == 1.7148160424389376

########NEW FILE########
__FILENAME__ = test_dbn
from scipy.sparse import csr_matrix
from sklearn.cross_validation import cross_val_score
from sklearn import datasets
from sklearn.metrics import f1_score

from ..dataset import Dataset


def pytest_funcarg__digits(request):
    digits = datasets.load_digits()
    n_samples = len(digits.images)
    data = digits.images.reshape((n_samples, -1))
    ds = Dataset(data, digits.target).scale()
    ds.test_size = 0.5
    return ds.train_test_split()


def pytest_funcarg__iris(request):
    iris = datasets.load_iris()
    ds = Dataset(iris.data, iris.target).scale()
    return ds


def test_callback(digits):
    from ..dbn import DBN

    fine_tune_call_args = []
    pretrain_call_args = []

    def fine_tune_callback(net, epoch):
        fine_tune_call_args.append((net, epoch))

    def pretrain_callback(net, epoch, layer):
        pretrain_call_args.append((net, epoch, layer))

    X_train, X_test, y_train, y_test = digits

    clf = DBN(
        [X_train.shape[1], 4, 10],
        epochs=3,
        epochs_pretrain=2,
        use_re_lu=False,
        fine_tune_callback=fine_tune_callback,
        pretrain_callback=pretrain_callback,
        )

    clf.fit(X_train, y_train)
    assert fine_tune_call_args == [
        (clf, 1), (clf, 2), (clf, 3)]
    assert pretrain_call_args == [
        (clf, 1, 0), (clf, 2, 0), (clf, 1, 1), (clf, 2, 1)]


def test_errors(digits):
    from ..dbn import DBN

    X_train, X_test, y_train, y_test = digits

    clf = DBN(
        [-1, 4, 10],
        epochs=3,
        epochs_pretrain=3,
        use_re_lu=False,
        )
    clf.fit(X_train, y_train)

    assert len(clf.errors_pretrain_) == 2
    assert len(clf.errors_pretrain_[0]) == 3
    assert len(clf.errors_pretrain_[1]) == 3

    assert len(clf.errors_fine_tune_) == 3
    assert len(clf.losses_fine_tune_) == 3


def test_functional_iris(iris):
    from ..dbn import DBN

    X_train, X_test, y_train, y_test = iris.train_test_split()

    clf = DBN(
        [-1, 4, 3],
        learn_rates=0.3,
        output_act_funct='Linear',
        epochs=50,
        )

    scores = cross_val_score(clf, iris.data, iris.target, cv=10)
    assert scores.mean() > 0.85


def test_functional_digits_no_pretrain(digits):
    from ..dbn import DBN

    X_train, X_test, y_train, y_test = digits
    clf = DBN(
        [64, 32, 10],
        verbose=0,
        )
    clf.fit(X_train, y_train)

    predicted = clf.predict(X_test)
    assert f1_score(y_test, predicted) > 0.9
    assert 0.9 < clf.score(X_test, y_test) < 1.0


def test_functional_digits_with_pretrain(digits):
    from ..dbn import DBN

    X_train, X_test, y_train, y_test = digits
    clf = DBN(
        [64, 32, 10],
        epochs_pretrain=10,
        use_re_lu=False,
        verbose=0,
        )
    clf.fit(X_train, y_train)

    predicted = clf.predict(X_test)
    assert f1_score(y_test, predicted) > 0.9
    assert 0.9 < clf.score(X_test, y_test) < 1.0


def test_sparse_support(digits):
    from ..dbn import DBN

    X_train, X_test, y_train, y_test = digits
    X_train = csr_matrix(X_train)
    X_test = csr_matrix(X_test)

    clf = DBN(
        [64, 32, 10],
        epochs_pretrain=10,
        use_re_lu=False,
        verbose=0,
        )
    clf.fit(X_train, y_train)

    predicted = clf.predict(X_test)
    assert f1_score(y_test, predicted) > 0.9
    assert 0.9 < clf.score(X_test, y_test) < 1.0


def test_layer_sizes_auto(iris):
    from ..dbn import DBN

    X_train, X_test, y_train, y_test = iris.train_test_split()

    clf = DBN(
        [-1, 4, -1],
        )
    clf.fit(X_train, y_train)

    assert clf.net_.weights[0].shape == (4, 4)
    assert clf.net_.weights[1].shape == (4, 3)


########NEW FILE########
__FILENAME__ = test_grid_search
from sklearn.linear_model import LogisticRegression
from sklearn.grid_search import GridSearchCV
from mock import Mock
import numpy as np


def test_grid_search():
    from ..grid_search import grid_search

    dataset = Mock()
    dataset.data = np.array([[1, 2, 3], [3, 3, 3]] * 20)
    dataset.target = np.array([0, 1] * 20)
    pipeline = LogisticRegression()
    parameters = dict(C=[1.0, 3.0])

    result = grid_search(dataset, pipeline, parameters)
    assert isinstance(result, GridSearchCV)
    assert hasattr(result, 'best_estimator_')
    assert hasattr(result, 'best_score_')

########NEW FILE########
__FILENAME__ = test_inischema
SAMPLE_SCHEMA = """
[first]
value1 = int
value2 = string
value3 = float
value4 = listofstrings
value5 = listofints

[second]
value1 = string
value2 = int
"""

SAMPLE_CONFIGURATION = """
[first]
value1 = 3
value2 = Three
value3 = 3.0
value4 = Three Drei
value5 = 3 3

[second]
value1 =
    a few line breaks

    are no problem

    neither is a missing value2
"""


def test_parse_config():
    from ..console import parse_config
    result = parse_config(SAMPLE_SCHEMA, SAMPLE_CONFIGURATION)
    assert result['first']['value1'] == 3
    assert result['first']['value2'] == u'Three'
    assert result['first']['value3'] == 3.0
    assert result['first']['value4'] == [u'Three', u'Drei']
    assert result['first']['value5'] == [3, 3]
    assert result['second']['value1'] == (
        u'a few line breaks\nare no problem\nneither is a missing value2')
    assert 'value2' not in result['second']

########NEW FILE########
__FILENAME__ = test_metrics
from mock import Mock
import numpy as np
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LogisticRegression


def test_multiclass_logloss():
    from ..metrics import multiclass_logloss

    act = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])
    pred = np.array([[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]])

    result = multiclass_logloss(act, pred)
    assert result == 0.69049112401021973


def test_multiclass_logloss_actual_conversion():
    from ..metrics import multiclass_logloss

    act = np.array([1, 0, 2])
    pred = np.array([[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]])

    result = multiclass_logloss(act, pred)
    assert result == 0.69049112401021973


def _learning_curve(learning_curve):
    X = np.array([[0, 1], [1, 0], [1, 1]] * 100, dtype=float)
    X[:, 1] += (np.random.random((300)) - 0.5)
    y = np.array([0, 0, 1] * 100)

    dataset = Mock()
    dataset.train_test_split.return_value = train_test_split(X, y)
    dataset.data = X
    dataset.target = y

    return learning_curve(dataset, LogisticRegression(), steps=5, verbose=1)
    #return scores_train, scores_test, sizes


def test_learning_curve():
    from ..metrics import learning_curve

    scores_train, scores_test, sizes = _learning_curve(learning_curve)
    assert len(scores_train) == 5
    assert len(scores_test) == 5
    assert sizes[0] == 22.5
    assert sizes[-1] == 225.0


def test_learning_curve_logloss():
    from ..metrics import learning_curve_logloss

    scores_train, scores_test, sizes = _learning_curve(learning_curve_logloss)
    assert len(scores_train) == 5
    assert len(scores_test) == 5
    assert sizes[0] == 22.5
    assert sizes[-1] == 225.0

########NEW FILE########
__FILENAME__ = test_model
from mock import Mock
import numpy as np
from scipy import sparse


def test_abstract_model():
    from ..model import AbstractModel

    pipeline = Mock()

    class MyModel(AbstractModel):
        default_params = {'param1': 1}
        @property
        def pipeline(self):
            return pipeline

    model = MyModel(param2=2)
    assert model() == pipeline

    pipeline.set_params.assert_called_with(param1=1, param2=2)


def test_averaging_estimator_fit():
    from ..model import AveragingEstimator

    estimator1 = Mock()
    estimator2 = Mock()
    X, y = Mock(), Mock()

    averaging = AveragingEstimator([estimator1, estimator2], verbose=1)
    assert averaging.fit(X, y) == averaging

    assert averaging.estimators == [
        estimator1.fit.return_value,
        estimator2.fit.return_value,
        ]

    estimator1.fit.assert_called_with(X, y)
    estimator2.fit.assert_called_with(X, y)


def test_averaging_estimator_predict():
    from ..model import AveragingEstimator

    estimator1 = Mock()
    estimator2 = Mock()
    X = Mock()

    estimator1.predict_proba.return_value = np.array([[0.1, 0.2, 0.3]])
    estimator2.predict_proba.return_value = np.array([[0.4, 0.5, 0.6]])

    averaging = AveragingEstimator([estimator1, estimator2], verbose=1)
    assert averaging.predict(X) == [2]

    estimator1.predict_proba.assert_called_with(X)
    estimator2.predict_proba.assert_called_with(X)

    assert (averaging.predict_proba(X) == [[0.45, 0.6, 0.75]]).all()

########NEW FILE########
__FILENAME__ = util
import sys

import numpy as np


def chunks(l, n):
    """ Yield successive n-sized chunks from l.
    """
    for i in xrange(0, len(l), n):
        yield l[i:i + n]


class ChunkedTransform(object):
    verbose = 0

    def transform(self, X):
        features = None
        for chunk in chunks(X, self.batch_size):
            if features is not None:
                features = np.vstack(
                    [features, self._compute_features(chunk)])
            else:
                features = self._compute_features(chunk)
            if self.verbose:
                sys.stdout.write(
                    "\r[%s] %d%%" % (
                        self.__class__.__name__,
                        100. * len(features) / len(X),
                        ))
                sys.stdout.flush()
        if self.verbose:
            sys.stdout.write('\n')
        return features

########NEW FILE########
