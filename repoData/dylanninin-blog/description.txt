---
layout: post
title: Get Port Status on Windows and Linux
category : Miscellanies
tags : [Linux, Windows, Utilities]
---

偶尔会碰到myeclipse突然崩溃的情况，但tomcat服务还为关闭，再次启动myeclipse和tomcat时，会提示Address Already Bind。稍总结了下Windows和Linux下查看系统端口使用的一般方法。

##进程与端口 

* 进程与端口一一对应；除了使用IP标识一台PC外，还需要使用端口标志与哪一个进程进行通信；
* 端口仅针对TCP、UDP应用。

##Window查找指定端口
    
###主要步骤
    
1）查看被占用的端口号

    netstat -aon | findstr port_number
    
    netstat -aon | findstr 5000
    TCP    127.0.0.1:5000         0.0.0.0:0              LISTENING       3988
    TCP    127.0.0.1:50000        0.0.0.0:0              LISTENING       316

最后一列是该端口对应的进程PID；
             
2）查看该端口对应的进程

    tasklist | findstr pid
                    
    tasklist | findstr 3988
    python.exe                  3988 Console                 0      2,968 K
            
3）关闭进程

打开任务管理器，菜单->查看->选择列，勾选"PID(进程标识符)"即可显示PID，找到后结束该进程即可。
            
###命令学习

netstat    
          
在命令行下输入: netstat -help，输出如下：

    显示协议统计信息和当前 TCP/IP 网络连接。


    NETSTAT [-a] [-b] [-e] [-n] [-o] [-p proto] [-r] [-s] [-v] [interval]


      -a            显示所有连接和监听端口。
      -b            显示包含于创建每个连接或监听端口的
                    可执行组件。在某些情况下已知可执行组件
                    拥有多个独立组件，并且在这些情况下
                    包含于创建连接或监听端口的组件序列
                    被显示。这种情况下，可执行组件名
                    在底部的 [] 中，顶部是其调用的组件，
                    等等，直到 TCP/IP 部分。注意此选项
                    可能需要很长时间，如果没有足够权限
                    可能失败。
      -e            显示以太网统计信息。此选项可以与 -s
                    选项组合使用。
      -n            以数字形式显示地址和端口号。
      -o            显示与每个连接相关的所属进程 ID。
      -p proto      显示 proto 指定的协议的连接；proto 可以是
                    下列协议之一: TCP、UDP、TCPv6 或 UDPv6。
                    如果与 -s 选项一起使用以显示按协议统计信息，proto 可以是下列协议之一:
                    IP、IPv6、ICMP、ICMPv6、TCP、TCPv6、UDP 或 UDPv6。
      -r            显示路由表。
      -s            显示按协议统计信息。默认地，显示 IP、
                    IPv6、ICMP、ICMPv6、TCP、TCPv6、UDP 和 UDPv6 的统计信息；
                    -p 选项用于指定默认情况的子集。
      -v            与 -b 选项一起使用时将显示包含于
                    为所有可执行组件创建连接或监听端口的
                    组件。
      interval      重新显示选定统计信息，每次显示之间
                    暂停时间间隔(以秒计)。按 CTRL+C 停止重新
                    显示统计信息。如果省略，netstat 显示当前
                    配置信息(只显示一次)

常用命令

* netstat -a 显示所有连接和端口号

* netstat -o  显示端口相关联的进程ID

* netstat -n 以数字形式显示地址和端口号

* netstat -r 显示路由表


##Linux下查找指定端口

###主要步骤

1）查看端口
    
    netstat -anp | grep "port_number"
    
    #netstat -anp | grep 8100
    tcp        0      0 127.0.0.1:8100              0.0.0.0:*                   LISTEN      2388/soffice.bin 
    
2）查看端口对应的服务
    
    lsof -i:"port_number"
    
    # lsof -i:8100
    COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
    soffice.b 2388   oa   15u  IPv4  12894      0t0  TCP dev.egolife.com:xprint-server (LISTEN)
    
端口服务对应列表可以通过/etc/services查找。


有些端口通过netstat查不出来，更可靠的办法是:  
    
    nmap -sT -O localhost
    
    # nmap -sT -O localhost

    Starting Nmap 5.21 ( http://nmap.org ) at 2013-11-01 08:49 CST
    Nmap scan report for localhost (127.0.0.1)
    Host is up (0.000036s latency).
    Hostname localhost resolves to 2 IPs. Only scanned 127.0.0.1
    rDNS record for 127.0.0.1: dev.egolife.com
    Not shown: 984 closed ports
    PORT     STATE SERVICE
    22/tcp   open  ssh
    25/tcp   open  smtp
    81/tcp   open  hosts2-ns
    111/tcp  open  rpcbind
    631/tcp  open  ipp
    873/tcp  open  rsync
    2048/tcp open  dls-monitor
    5222/tcp open  unknown
    5269/tcp open  unknown
    7070/tcp open  realserver
    7443/tcp open  unknown
    7777/tcp open  unknown
    8009/tcp open  ajp13
    8100/tcp open  unknown
    8300/tcp open  unknown
    9090/tcp open  zeus-admin
    No exact OS matches for host (If you know what OS is running on it, see http://nmap.org/submit/ ).
    TCP/IP fingerprint:
    OS:SCAN(V=5.21%D=11/1%OT=22%CT=1%CU=40338%PV=N%DS=0%DC=L%G=Y%TM=5272FA92%P=
    OS:x86_64-unknown-linux-gnu)SEQ(SP=105%GCD=1%ISR=10C%TI=Z%CI=Z%II=I%TS=A)OP
    OS:S(O1=M400CST11NW7%O2=M400CST11NW7%O3=M400CNNT11NW7%O4=M400CST11NW7%O5=M4
    OS:00CST11NW7%O6=M400CST11)WIN(W1=8000%W2=8000%W3=8000%W4=8000%W5=8000%W6=8
    OS:000)ECN(R=Y%DF=Y%T=40%W=8018%O=M400CNNSNW7%CC=Y%Q=)T1(R=Y%DF=Y%T=40%S=O%
    OS:A=S+%F=AS%RD=0%Q=)T2(R=N)T3(R=Y%DF=Y%T=40%W=8000%S=O%A=S+%F=AS%O=M400CST
    OS:11NW7%RD=0%Q=)T4(R=Y%DF=Y%T=40%W=0%S=A%A=Z%F=R%O=%RD=0%Q=)T5(R=Y%DF=Y%T=
    OS:40%W=0%S=Z%A=S+%F=AR%O=%RD=0%Q=)T6(R=Y%DF=Y%T=40%W=0%S=A%A=Z%F=R%O=%RD=0
    OS:%Q=)T7(R=Y%DF=Y%T=40%W=0%S=Z%A=S+%F=AR%O=%RD=0%Q=)U1(R=Y%DF=N%T=40%IPL=1
    OS:64%UN=0%RIPL=G%RID=G%RIPCK=G%RUCK=G%RUD=G)IE(R=Y%DFI=N%T=40%CD=S)

    Network Distance: 0 hops

    OS detection performed. Please report any incorrect results at http://nmap.org/submit/ .
    Nmap done: 1 IP address (1 host up) scanned in 11.87 seconds
    
3）关闭端口

iptables禁用端口

    iptables -A INPUT -p tcp --dport "port_number" -j DROP
    iptables -A OUTPUT -p tcp --dport "port_number" -j DROP 

关闭端口对应的进程

    kill -9 pid                         #9为信号量，表示立即强行删除一个进程
    pkill  "service_name"               #pkill会关闭所有同名服务，慎用

###命令学习

使用man/info等查询命令帮助即可

---
layout: post
title: MT Installation I
category : Blog
tags : [Blog, MovableType]
---

##MovableType 安装记录

安装环境

* 博客：MTOS-4.38-en
* 服务器：Linux dev.egolife.com 2.6.32-71.el6.i686 GNU/Linux
* 数据库： MySQL 5.1.47
* Web服务器: Apache/2.2.15 (CentOS) Server at localhost Port 80

##安装步骤

解压

	[root@dev]# unzip MTOS-4.38-en.zip /var/www/cgi-bin

创建cgi-bin软连接

	[root@dev]# ln -s /var/www/cgi-bin/MTOS-4.38-en /var/www/cgi-bin/mt
	[root@dev]# ll /var/www/cgi-bin/
	total 4
	lrwxrwxrwx 1 root root 13 Sep 22 23:57 mt -> MTOS-4.38-en/
	drwxr-xr-x 13 root root 4096 Sep 23 00:03 MTOS-4.38-en

创建静态文件软连接

	[root@dev]# ln -s /var/www/cgi-bin/MTOS-4.38-en/mt-static /var/www/html/mt-static
	[root@dev]# ll /var/www/html/
	total 0
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/

权限更改

	[root@dev]# chmod 777 /var/www/cgi-bin/MTOS-4.38-en/mt-static/support
	[root@dev]# chmod 777 /var/www/cgi-bin/MTOS-4.38-en/themes

##配置文件

主要配置Movable Type的cgi-bin、静态文件的路径，数据库连接认证信息。

查看当前主机名

	[root@dev mt]# hostname
	dev.egolife.com

重命名配置文件

	[root@dev]# cd /var/www/cgi-bin/MTOS-4.38-en
	[root@dev mt]# cp mt-config.cgi-original mt-config.cgi

编辑配置文件，配置数据库

	[root@dev mt]# vim mt-config.cgi
	CGIPath http://dev.egolife.com/cgi-bin/mt/
	StaticWebPath http://dev.egolife.com/mt-static/
	ObjectDriver DBI::mysql
	Database xxxxxxxxx
	DBUser xxxxxxxxx
	DBPassword xxxxxxxxx
	DBHost localhost
	EmailAddressMain dylanninin@gmail.com
	
	#####################################################################

##创建MT数据库、用户并授权

	[root@dev]# mysql -uroot -p
	mysql> create database mt character utf8;
	mysql> create user mt;
	mysql> grant all on mt.* to xxxxxxxxx@'localhost' identified by 'xxxxxxxxx';

##启动web服务器

	[root@dev mt]# apachectl start

##第一次访问

使用浏览器打开以下url:

	http://dev.egolife.com/cgi-bin/mt/mt.cgi

此时出现异常，信息如下：

	Forbidden
	
	You don't have permission to access /cgi-bin/mt/mt.cgi on this server.
	Apache/2.2.15 (CentOS) Server at dev.egolife.com Port 80

查看httpd日志信息

	[root@dev]# less /var/log/httpd/error_log
	[Sun Sep 23 00:11:31 2012] [error] [client 192.168.136.1] Directory index forbidden by Options directive: /var/www/html/
	[Sun Sep 23 00:11:39 2012] [error] [client 192.168.136.1] Symbolic link not allowed or link target not accessible: /var/www/cgi-bin/mt
	[Sun Sep 23 00:13:44 2012] [error] [client 127.0.0.1] File does not exist: /var/www/html/server-status

##主要错误信息

###不允许mt符号连接

更改`httpd.conf`中`/var/www/cgi-bin`的配置，启用符号链接，如下:
	
	581 #
	582 <Directory "/var/www/cgi-bin">
	583 AllowOverride None
	584 # Options None
	585 Options FollowSymLinks 
	586 Order allow,deny
	587 Allow from all
	588 </Directory>

重启apache

	[root@dev httpd]# apachectl restart

再次访问，则可以正常浏览。

###文件server-status不存在
	
	[root@dev httpd]# ll /var/www/html/
	total 0
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/
	[root@dev]# mkdir /var/www/html/server-status
	[root@dev]# chown apache:apache /var/www/html/server-status/
	[root@dev]# ll /var/www/html/
	total 4
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/
	drwxr-xr-x 2 apache apache 4096 Sep 23 00:31 server-status

访问时，已经显示Movable Type配置页面，但出现新的错误提示:
	
	An error occurred
	Can't connect to data source '' because I can't work out what driver to use (it doesn't seem to contain a 'dbi:driver:' prefix and the DBI_DRIVER env var is not set)

可能是因为没有安装DBI扩展库，但查看rpm包时已经安装:

	[root@dev httpd]# rpm -qa | grep DBI
	perl-DBI-1.609-4.el6.i686
	perl-DBIx-Simple-1.32-3.el6.noarch

有可能是前面的`mt-config.cgi`配置出现问题:

	27 ##### MYSQL #####
	28 ObjectDriver DBI::mysql
	29 Database mt
	30 DBUser mt
	31 DBPassword 000000
	32 DBHost localhost
	33 EmailAddressMain dylanninin@gmail.com
	34 
	35 ##### POSTGRESQL #####
	36 #ObjectDriver DBI::postgres
	37 #Database DATABASE_NAME
	38 #DBUser DATABASE_USERNAME
	39 #DBPassword DATABASE_PASSWORD
	40 #DBHost localhost
	41 
	42 ##### SQLITE #####
	43 #ObjectDriver DBI::sqlite
	44 #Database /path/to/sqlite/database/file

发现更改DB配置时，没有将PostgreSQL和SQLite的配置注释掉，注释掉后重启Apache，则可以正常配置。

##创建博客

创建博客时，出现异常。主要信息如下:

	Blog Name:
	My First Blog
	Blog URL
	http://dev.egolife.com/blog/
	Publishing Path
	/var/www/html/blog

错误信息：

	In order to properly publish your blog, you must provide Movable Type with your blog's URL and the path on the filesystem where its files should be published.
	--The path provided below is not writable.
	Blog Name
	Blog URL
	Publishing Path
	Your 'Publishing Path' is the path on your web server's file system where Movable Type will publish all the files for your blog. Your web server must have write access to this directory.

创建目录

	[root@dev html]# cd /var/www/html
	[root@dev html]# mkdir blog
	[root@dev html]# chown apache:apache blog/
	[root@dev html]# ll
	total 8
	drwxr-xr-x 2 apache apache 4096 Sep 23 00:46 blog

到此，初步安装MovableType成功。
---
layout: post
title: Pastime Paradise
category : Life
tags : [Music]
---

##Pastime Paradise 

	They've been spending most their lives
	Living in a pastime paradise
	They've been spending most their lives
	Living in a pastime paradise
	They've been wasting most their time
	Glorifying days long gone behind
	They've been wasting most their days
	In remembrance of ignorance oldest praise
	
	Tell me who of them will come to be
	How many of them are you and me
	Dissipation
	Race relations
	Consolation
	Segregation
	Dispensation
	Isolation
	Exploitation
	Mutilation
	Mutations
	Miscreation
	Confirmation...to the evils of the world
	
	They've been spending most their lives
	Living in a future paradise
	They've been spending most their lives
	Living in a future paradise
	They've been looking in their minds
	For the day that sorrows gone from time
	They keep telling of the day
	When the savior of love will come to stay
	
	Tell me who of them will come to be
	How many of them are you and me
	Proclamation
	Of race relations
	Consolation
	Integretion
	Verification
	Of revelations
	Acclamation
	World salvation
	Vibrations
	Stimulation
	Confirmation...to the peace of the world
	
	They've been spending most their lives
	Living in a pastime paradise
	They've been spending most their lives
	Living in a pastime paradise
	They've been spending most their lives
	Living in a future paradise
	They've been spending most their lives
	Living in a future paradise
	We've been spending too much of our lives
	Living in a pastime paradise
	
	Lets start living our lives
	Living for the future paradise
	Praise to our lives
	Living for the future paradise
	Shame to anyone's lives
	Living in the pastime paradise






---
layout: post
title: Blog Schedule
category : Blog
tags : [Blog]
---

##创站简介

最近两天集中查阅和比对了一些关于VPS和博客系统的资料，最终确定下来，在2012年9月28日购买了一年[Linode](http://www.linode.com/)的[VPS](http://en.wikipedia.org/wiki/Virtual_private_server)服务（VPS方案是Linode 512），紧接着使用自由软件[Movable Type](http://www.movabletype.org/)在VPS上搭建了博客系统（针对个人使用是免费的），使用MTOS自带的主题、样式，风格比较简单，当然也没有进行汉化。第二天，也就是2012年9月29日，在[Godaddy](http://www.godaddy.com/) 申请注册了一枚域名，即[dylanninin.com](http://dylanninin.com)，另外赠送[dylanninin.info](http://dylanninin.info) 免费域名。本博客正式使启用的域名是[www.dylanninin.com](http://www.dylanninin.com)，同时，[www.dylanninin.info](http://www.dylanninin.info)也指向本站。

##维护计划

目前本站刚刚起步，仅仅搭建起了一个框架，有很多关于Web Site的知识需要学习和应用， 今年中秋十一有八天长假，正好可以专心折腾下，希望能够在假期结束前能完成一个博客系 统的雏形。

具体说来，大概有以下内容需要在这段时间学习和完成：

* Movable Type日常管理，如博客、页面、图片等。
* Apache 服务器基本配置，如安全性，日志等。
* Linux主机安全，如防火墙，ssh等。
* UI设计，如页面框架，站点信息维护，配色方案等。
* 错误页面，如404,500等。
* 备份和恢复，如数据库，博客等。

在学习过程中，会陆续整理出一些笔记发在博客上，希望和大家多分享多交流。

##中秋佳节

今天正好中秋佳节，顺便送上祝福，希望人月两圆，幸福快乐。


---
layout: post
title: MT Installation II
category : Blog
tags : [Blog, MovableType]
---

##MovableType 安装记录（二）

接着[上一篇](http://dylanninin.com/blog/2012/09/29/mt_installation.html)继续配置Movable Type。

##Apache配置

###配置站点首页

打开`http://dev.egolife.com`时，默认显示为Apache的测试页面。当访问 `http://dev.egolife.com/blog`才会显示博客首页。

取消显示Apache测试页面,更改配置`/etc/httpd/conf.d/welcome.conf`,注释掉 LocationMatch配置：
	
	[root@dev html]# vim /etc/httpd/conf.d/welcome.conf
	1 #.
	2 # This configuration file enables the default "Welcome"
	3 # page if there is no default index page present for
	4 # the root URL. To disable the Welcome page, comment
	5 # out all the lines below.
	6 #
	7 #<LocationMatch "^/+$">
	8 # Options -Indexes
	9 # ErrorDocument 403 /error/noindex.html
	10 #</LocationMatch>

运行`service http reload`命令，重新加载配置后，直接访问主机，则出现的是 `/var/www/html`的页面列表信息。

###禁止列表文件

禁止列表信息，需要更改配置`/etc/httpd/conf/httpd.conf`：

	[root@dev html]# vim /etc/httpd/conf/httpd.conf
	317 <Directory "/var/www/html">
	318 
	319 #
	320 # Possible values for the Options directive are "None", "All",
	321 # or any combination of:
	322 # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews
	323 #
	324 # Note that "MultiViews" must be named *explicitly* --- "Options All"
	325 # doesn't give it to you.
	326 #
	327 # The Options directive is both complicated and important. Please see
	328 # http://httpd.apache.org/docs/2.2/mod/core.html#options
	329 # for more information.
	330 #
	331 # Options Indexes FollowSymLinks
	332 Options FollowSymLinks
	333 
	334 #
	335 # AllowOverride controls what directives may be placed in .htaccess files.
	336 # It can be "All", "None", or any combination of the keywords:
	337 # Options FileInfo AuthConfig Limit
	338 #
	339 AllowOverride None
	340 
	341 #
	342 # Controls who can get stuff from this server.
	343 #
	344 Order allow,deny
	345 Allow from all
	346 
	347 </Directory>

更改配置后，重新加载配置文件，则显示禁止访问，此时可以自定义显示的页面，这里采用 软链接的方式。

###设置软连接

	[root@dev html]# ln -s /var/www/html/blog/index.html /var/www/html/index.html
	[root@dev html]# ll
	total 12
	drwxr-xr-x 7 apache apache 4096 Sep 23 16:05 blog
	-rw-r--r-- 1 apache apache 1406 Sep 23 15:47 favicon.ico
	lrwxrwxrwx 1 root root 29 Sep 23 16:29 index.html -> /var/www/html/blog/index.html
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/
	drwxr-xr-x 2 apache apache 4096 Sep 23 00:31 server-status

这样访问`http://dev.egolife.com`时，会直接显示博客首页。当然，可以使用URL重写或 重定向来实现.

###错误提示页

主要是403,404,500页面的配置，更改`/etc/httpd/conf/httpd.conf`：

	[root@dev html]# vim /etc/httpd/conf/httpd.conf
	833 # Some examples:
	834 #ErrorDocument 500 "The server made a boo boo."
	835 #ErrorDocument 404 /missing.html
	836 #ErrorDocument 404 "/cgi-bin/missing_handler.pl"
	837 #ErrorDocument 402 http://www.example.com/subscription_info.html
	838 #
	839 #2012-09-22|dylanninin@gmail.com| ErrorDocument
	840 ErrorDocument 500 http://dev.egolife.com/errors/500.html
	841 ErrorDocument 404 http://dev.egolife.com/errors/404.html
	842 ErrorDocument 403 http://dev.egolife.com/errors/403.html

###添加错误页面

	[root@dev errors]# pwd
	/var/www/html/errors
	[root@dev errors]# ll
	total 0
	-rw-r--r-- 1 apache apache 0 Sep 23 16:58 403.html
	-rw-r--r-- 1 apache apache 0 Sep 23 16:58 404.html
	-rw-r--r-- 1 apache apache 0 Sep 23 17:00 500.html

以上页面内容待完善!

##参考

* Movable Type：[MTOS文件系统描述](http://www.movabletype.org/documentation/installation/file-system.html)

---
layout: post
title: Manually Creating Oracle Database On Linux
category : Oracle
tags : [Oracle, Database, DBA, Linux]
---

今年上半年，由于工作调整，开始接手DBA的工作，负责ERP、数据库、服务器等的管理和维 护工作。在粗略看完Oracle Database 2 Day DBA的官方文档后，结合一些中文书目以及工 作中遇到的问题，对Oracle数据库有了初步的了解。但发现单看中文数目以及解决一个个突 发问题，还远远不够，于是开始阅读2 Day DBA的升级版文档Administrator's Guide，并进 行一些尝试和实践，希望能够有进一步的理解。

在Adminstartor's Guide的第二章，讲到了如何使用`CREATE DATABASE`语句手工创建数据 库。这个语句比MySQL的复杂多了，涵盖了Oracle数据库最核心的概念，需要有一定的基础 知识。按照参考文档的提示进行操作时，还是出现过一些错误，最后参考CSDN上的一篇 [博客](http://blog.csdn.net/tianlesoftware/article/details/4680213)进行调整，成 功创建了数据库，这里记录如下。

##测试环境

* 操作系统 CentOS 6.0 x86 64bit
* 数据库	  Oracle 10.2

##创建步骤

###1. 规划sid,oracle home

	export ORACLE_SID=MANUAL
	export ORACLE_BASE=/db/oracle
	export ORACLE_HOME=/db/oracle/product/10.2.0/db_1

###2. 系统规划

	ORACLE_SID=manual
	DB_NAME=MANUAL
	DB_DOMAIN=egolife.com

###3. 手工创建必须的目录

dump目录
	
	mkdir /db/oracle/admin/MANUAL/adump
	mkdir /db/oracle/admin/MANUAL/bdump
	mkdir /db/oracle/admin/MANUAL/cdump
	mkdir /db/oracle/admin/MANUAL/udump
	mkdir /db/oracle/admin/MANUAL/pfile

数据文件目录

	mkdir /db/oracle/oradata/MANUAL

恢复目录
	
	mkdir /db/oracle/flash_recovery_area/MANUAL

###4. 建立密码文件

	orapwd file=/db/oracle/product/10.2.0/db_1/dbs/orapwMANUAL password=oracle

###5. 修改参数文件

编辑init.ora文件，更改主要配置：

	MANUAL.__db_cache_size=331350016
	MANUAL.__java_pool_size=4194304
	MANUAL.__large_pool_size=8388608
	MANUAL.__shared_pool_size=138412032
	MANUAL.__streams_pool_size=0
	*._kgl_large_heap_warning_threshold=8388608
	*.audit_file_dest='/db/oracle/admin/MANUAL/adump'
	*.background_dump_dest='/db/oracle/admin/MANUAL/bdump'
	*.compatible='10.2.0.1.0'
	*.control_files='/db/oracle/oradata/MANUAL/control01.ctl',
	'/db/oracle/oradata/MANUAL/control02.ctl',
	'/db/oracle/oradata/MANUAL/control03.ctl'
	*.core_dump_dest='/db/oracle/admin/MANUAL/cdump'
	*.db_2k_cache_size=33554432
	*.db_block_size=8192
	*.db_domain='egolife.com'
	*.db_file_multiblock_read_count=128
	*.db_files=4000
	*.db_name='MANUAL'
	*.db_recovery_file_dest_size=4294967296
	*.db_recovery_file_dest='/db/oracle/flash_recovery_area'
	*.log_checkpoints_to_alert=FALSE
	*.open_cursors=300
	*.parallel_execution_message_size=65535
	*.parallel_max_servers=128
	*.pga_aggregate_target=209715200
	*.processes=150
	*.recyclebin='OFF'
	*.remote_login_passwordfile='EXCLUSIVE'
	*.replication_dependency_tracking=FALSE
	*.session_cached_cursors=100
	*.sga_target=500m
	*.shared_pool_size=100m
	*.undo_management='AUTO'
	*.undo_retention=0
	*.undo_tablespace='UNDOTS'
	*.user_dump_dest='/db/oracle/admin/MANUAL/udump'
	*.workarea_size_policy='AUTO'
	_allow_resetlogs_corruption=true

###6. 启动数据库

	SQL> conn /as sysdba
	Connected to an idle instance.
	SQL> startup nomount pfile=/db/oracle/product/10.2.0/db_1/dbs/initMANUAL.ora;
	
	ORACLE instance started.
	
	Total System Global Area  524288000 bytes
	Fixed Size     1220336 bytes
	Variable Size   150995216 bytes
	Database Buffers   364904448 bytes
	Redo Buffers     7168000 bytes

###7. 运行创建数据库脚本

	CREATE DATABASE MANUAL
	LOGFILE
	GROUP 1 ('/db/oracle/oradata/MANUAL/redo01.log',
	'/db/oracle/oradata/MANUAL/redo01_1.log') size 100m reuse,
	GROUP 2 ('/db/oracle/oradata/MANUAL/redo02.log',
	'/db/oracle/oradata/MANUAL/redo02_1.log') size 100m reuse,
	GROUP 3 ('/db/oracle/oradata/MANUAL/redo03.log',
	'/db/oracle/oradata/MANUAL/redo03_1.log') size 100m reuse
	MAXLOGFILES 50
	MAXLOGMEMBERS 5
	MAXLOGHISTORY 200
	MAXDATAFILES 500
	MAXINSTANCES 5
	ARCHIVELOG
	CHARACTER SET UTF8
	NATIONAL CHARACTER SET UTF8
	DATAFILE '/db/oracle/oradata/MANUAL/system01.dbf' SIZE 1000M EXTENT MANAGEMENT LOCAL
	SYSAUX DATAFILE '/db/oracle/oradata/MANUAL/sysaux01.dbf' SIZE 1000M
	UNDO TABLESPACE UNDOTS DATAFILE '/db/oracle/oradata/MANUAL/undo.dbf' SIZE 500M
	DEFAULT TEMPORARY TABLESPACE TEMP TEMPFILE '/db/oracle/oradata/MANUAL/temp.dbf' SIZE 500M;

###8. 运行必要的sql脚本(注意按以下顺序)

	/db/oracle/product/10.2.0/db_1/rdbms/admin/catalog.sql
	/db/oracle/product/10.2.0/db_1/rdbms/admin/catproc.sql

###9. 创建相关表空间和用户
	
	create tablespace users datafile '/db/oracle/oradata/MANUAL/users01.dbf' size 500M;
	create tablespace indexes datafile '/db/oracle/oradata/MANUAL/index01.dbf' size 500M;

建立测试用户

	create user dylan identified by 000000 default tablespace users;
	grant connect,resource to dylan;

###10. 改为spfile启动

	create spfile from pfile;

重启数据库，并运行show parameter spfile，确认启动的参数文件类型。

###11. Windows客户端测试

tnsnames.ora配置：

	MANUAL =
	  (DESCRIPTION =
	    (ADDRESS = (PROTOCOL = TCP)(HOST = dev.egolife.com)(PORT = 1521))
	    (CONNECT_DATA =
	      (SERVER = DEDICATED)
	      (SERVICE_NAME = MANUAL.egolife.com )
	    )
	  )

运行tnsping MANUAL命令，并使用sqlplus进行连接测试。

##参考

* [Manually Creating an Oracle Database](http://docs.oracle.com/cd/B19306_01/server.102/b14231/create.htm#sthref220)
* [Linux下手工新建数据库](http://blog.csdn.net/tianlesoftware/article/details/4680213)

---
layout: post
title: Oracle Database Concepts
category : Oracle
tags : [Oracle, Database, DBA]
---

Oracle Database概念非常多，其中关于SID、NAMES以及监听参数的定义和关系经常让人混
淆。在[tianlesoftware](http://blog.csdn.net/tianlesoftware)的博客上，有一篇专门 讲解了这些概念，行文思路清晰，概念和实践分析相结合，易于理解；且文中大量引用官 方文档，来源可靠，可信度高。作者的博客很值得同行学习和借鉴。 

由于刚入行不久，秉承实用原则这里仅作粗略了解和记录，详情可以参见博客原文。

##基本概念

###db相关

* dbid, sid

###pfile参数

* `db_name`, `db_domain`, `instance_name`

* `db_unique_name`, `service_names`

* `global_name`, `global_names`

###listener.ora参数

* `sid_name`, `global_dbname`

###tnsnames.ora参数

* `service_name`, `sid`

###1. DB相关

####dbid

* `db_name`在数据库内部的表示，创建数据库时用`db_name`结合一种算法生成。

* `dbid`存在于数据文件、控制文件中，表示数据文件的归属，`dbid`唯一。不同数据库 `dbid`不同，但`db_name`有可能相同

####dbid 和 sid

查看dbid：

	select * from v$database;

####在controlfile中查看dbid和sid

转存控制文件到trace file：

	alter session set events 'immediate trace name CONTROLF level 8'; 

	select gettracefile() from dual; 

	Db ID=1873459096=0x6faab798, Db Name='EGOLIFE'

####修改`dbid`和`dbname`

主要步骤：

* a. `startup mount`;
* b. `nid TARGET=SYS/oracle@egolife DBNAME=one`
* c. `startup mount`;
* d. `alter database open resetlogs`;

更改`dbname`自后，之前的备份均无效。

####查看`sid`(system identifier)

	ORACLE_SID sid
	ORACLE_HOME oracle home

oracle用`sid`和`oracle_home`生成一个key，来创建共享内存。

###2. pfile参数

	SQL> show parameter name
	NAME TYPE VALUE
	-------------------------- ----------- --------------------
	db_file_name_convert       string
	db_name                    string       MANUAL
	db_unique_name             string       MANUAL
	global_names               boolean     	FALSE
	instance_name              string       MANUAL
	lock_name_space            string
	log_file_name_convert      string
	service_names              string       MANUAL.egolife.com
	
####db_name

* 不能超过8个字符
* 动态注册监听

###instance_name

* 在RAC环境中`DB_NAME`相同，但`INSTANCE_NAME`不同，用以区分唯一的实例
* 默认值为`SID`，一般与数据库名相同，也可以不同
* 与进程名相关 ,并且`initSID.ora/orapwSID`与`instance_name`保持一致

####db_domain

* 区分网络层次结构

####db_unique_name

* 在DataGuard环境中，DB_NAME相同，但`DB_UNIQUE_NAME`不同

####service_names

* 值：`db_unique_name.db_domain`

####global_name

* 由`db_name.db_domain`组成

####global_names

* 创建db link时是否强制使用远程数据库的`global_name`，多用于分布式系统

###3. listener.ora参数

####sid_name

* 数据库的运行的实例名，与`instance_name`一致

####global_dbname

* 配置静态监听需要使用
* `global_dbname`是listener对外的连接名称，我们可以写成任意值，客户端配置时， `service_name`需要与`global_dbname`一致
* 如果是动态监听，因`service_names`由多个，则会注册多个，每个对应着同一个 `instance_name`，这样配置任意一个即可访问

###4. tnsnames.ora参数

####service_name

* 静态监听：`service_name` = `global_dbname`
* 动态监听：`service_name` = `service_names`(在initSID.ora文件中)

####sid

* 直接指定`instance_name`

##参考

 * [SID、NAMES及监听参数说明](http://blog.csdn.net/tianlesoftware)
 * [Oracle小知识总结(一)](http://blog.csdn.net/tianlesoftware/article/details/5622268)

---
layout: post
title: Summary of My Blog
category : Blog
tags : [Blog, MovableType]
---

继9月30号作好[本站创建和维护计划](http://dylanninin.com/blog/2012/09/30/blog_schedule.html)后，经过这 两天的折腾，总算完成了一个博客系统的雏形，而且是一个十分朴素的雏形。 

在这两天，主要熟悉了MTOS的博客管理，自定义了几个模版或小工具，添加了几个插件，定 义了页首、页尾的的Contact和About信息，添加了HTTP请求常见的几个错误跳转页面，最后 确定了博客当前的布局和功能，也就是现在看到的这个样子。若没有异常情况，本站很长一 段时间内将保持当前布局和功能；这段时间以后，我会将更多的精力放在如何提高博客质量 上。

现在稍稍总结下这两天的工作，也记下生活流水，以作备忘。

##博客管理

MTOS的博客管理功能确实是十分强大，博客、模版、小工具等均有版本记录，在编辑的过程 中，若异常关闭浏览器，再次打开时还可以进行恢复。在折腾过程中，不知道是我心太急， 还是带宽有限，每一次编辑、发布就感觉系统的响应越来越慢，本以为十一假期大家都会在 高速公路上拥堵不堪，没想到互联网也是如此。无奈之下，Chrome都Oops不响应了，只好关 闭重启，好在MTOS有自动恢复功能，这样我就减少了许多体力劳动，不然还真会像某位急躁哥从车窗探出身子非常淡定地凝望这十分拥堵的traffic line，而且一凝望就是两小时。

##模版和小工具

###1. 相关文章

在博客系统中，相关文章是一个十分重要的功能，这些文章或在分类上相同，或在标签上相 同，使分散的文章在一定层次上进行了聚合，更具有知识性和传播性。当用户阅读完一篇博 客时，若饶有兴致，就可以根据列出的相关文章进行衍生阅读，很方便。

在网上搜索时，找到一篇关于不使用插件列出具有相同标签文章的博客，只需添加几行代码 到文章模版中即可。

本站采用的方式是新建一个模版`tag_related_entries`，代码如下：
	
	<mt:EntryIfTagged>
	<mt:SetVarBlock name="curentry"><mt:EntryID /></mt:SetVarBlock>
	<mt:SetVarBlock name="relatedtags"><mt:EntryTags glue=" OR "><mt:TagName /></mt:EntryTags></mt:SetVarBlock>
	<mt:SetVarBlock name="listitems"><mt:Entries tags="$relatedtags" unique="1" lastn="10"><mt:SetVarBlock name="listentry"><mt:EntryID /></mt:SetVarBlock><mt:Unless name="listentry" eq="$curentry"><li><a href="<mt:EntryPermaLink />"><mt:EntryTitle /></a></li></mt:Unless></mt:Entries></mt:SetVarBlock>
	<mt:If name="listitems">
	<div class="my_related_entries">
	<h4>Related Entries<span class="delimiter">:</span></h4>
	<ul>
	<mt:Var name="listitems">
	</ul>
	</div>
	</mt:If>
	</mt:EntryIfTagged>

保存后，在模版Entry中需要添加相关文章的位置导入该模版即可。

###2. 文章信息

看到互联网上很多博客在文章末尾会注上文章的版权信息，永久链接等，MTOS默认没有生成 这些信息，所以利用MTOS的标签自定义了一个模版entry_info，代码如下：

	<div class="my_entry_info">
	<h4>Entry Info<span class="delimiter">:</span></h4>
	<ul type="circle">
	<li>Name<span class="delimiter">:</span><a href="<$mt:EntryPermalink$>"><$mt:EntryTitle$></a></li>
	<li>Link<span class="delimiter">:</span><a href="<$mt:EntryPermalink$>"><$mt:EntryPermalink$></a></li>
	<li>Author<span class="delimiter">:</span>By <$mt:EntryAuthorLink show_hcard="1"$> on <$mt:EntryDate format="%x %X"$></li>
	</ul>
	</div>

在Entry模版的文章末尾处，导入该模版即可。

###3. 友情链接

为了让大家知道我经常阅读的一些博客，自定义了一个小工具friend_links，根据博客的布 局样式，在合适的位置拖入这个小工具即可。本站使用的是三栏式结构，友情链接添加在副
侧边栏的最后。

###4. 分享和订阅

为了方便分享和订阅本站，对比了一些常见的社会化分享工具，如[addthis](http://addthis.org.cn/)， [jiathis](http://www.jiathis.com/)，[bShare](http://www.bshare.cn/)，在分享服务、 自定义、数据统计、性能等方面各有长短，并未做十分仔细的对比和分析，因addthis在社 会化分享的同时，还提供了订阅按钮，为了保证本站页面的一致性，最终选择addthis。 

这里仍然采用创建模版或小工具的方式来添加自定义的分享和订阅代码，同样在Entry模版 合适的位置导入该模版或插件即可，最终效果见本站。

###5. 社会化评论

MTOS自动的评论系统因带有防Spam功能，响应比较慢，不是很友好。同样，为了提高响应速 度和友好性，对比了一些常见的社会化评论系统，如[disqus](http://disqus.com/), [多说](http://duoshuo.com/)等。这些评论系统均有常用的评论功能，不同的是还支持微 博等多账户登录；评论内容由第三方平台托管，支持邮件通知、评论统计、评论的导入导出 等。对于评论负荷较重的主机，可以采用社会化评论系统取代自带的评论功能。多说比较适 合国内的博客，在国外可能disqus使用得更为广泛，因本站VPS在国外，以及并没有将MTOS 中文化的计划，所以本站采用disqus。

disqus安装比较简单，注册后，根据向导设置网站，拷贝生成的评论代码，并创建模版，同 样在Entry模版合适的位置导入该模版即可，最终效果见本站。

##插件

MTOS自带的Web编辑器不太好用，尤其是添加图片和代码片段时。在官方网站中，找到了一 个所见即所得的Web编辑器CKEditor，进行配置后，只适用于Excerpt的编辑，而在Body编辑 时，没有CKeditor的踪影，不知是版本不正确，还是设置出了纰漏，目前尚未处理。

粘贴代码片段时，一款代码格式化和高亮插件，可以让代码更易读，博客也更整洁干净，目 前尚未处理。

##常用页面

###1. Contact

[联系页面](http://www.dylanninin.com/blog/pages/message.html)，大家若有任何意见 和建议，可以在这里留言，或者发邮件。

###2. About

[关于本站](http://www.dylanninin.com/blog/pages/about.html)，提供站点、站长的基 本信息，有助于理解和交流。

###3. 404

[Not Found](http://www.dylanninin.com/foo/bar.html)，当访问不存在的资源时，会提 示资源找不到。

###4. 403

[Forbidden](http://www.dylanninin.com/blog/errors)，当访问链接存在但由于某些原因 服务器拒绝请求时，会提示禁止访问。

###5. 500

当然，除了404,403页面，还有一个500页面，但用户正常访问时，最好不要出现此页面。

##参考

* [MT之旅](http://www.ezloo.com/mt/manual/entry_tag_entries.html)

---
layout: post
title: Root Security Introduction on Linux
category : Linux
tags : [Linux, Security]
---

root是Unix/Linux的超级用户，拥有绝对的权力。拥有root权限，你尽可以随心所欲而不逾 矩：你既是"矩"的制定者，也是"矩"的拥护者，当然稍有不慎，你也可能是"矩"的破坏者、 毁灭者，因为若被滥用，则会引发异常崩溃，甚至导致绝对的腐败。 

所以很多Unix/Linux教程都花了很大篇幅讲解系统权限，并强烈建议慎用root，而代之以普 通用户，如anybody。这样当anybody试图做某些逾矩的操作时，会提示Permission Denied 并无法执行，此时root制定的规则已经生效，并得到遵守，不管你是情愿，还是被迫。当 然，若有需要，anybody也可以切换为root身份，前提是需要知道root的口令。

根据以上信息，我们至少可以从三个方面增强root安全。

###1. 高强度口令

为root用户设置一个高强度的口令，大小写字母 + 数字 + 特殊字符，而且还要保证有足够 的位数。

去年国内几大网站相继出现[明文口令泄露事件](http://coolshell.cn/articles/6193.html)， 有很多可供引以为戒。但我相信，很多用户依然还没有引起重视，不信可以去测试下你的密 码强度：[How Security Is My Password](http://howsecureismypassword.net/) 

###2. 创建普通用户

使用root登录，新增用户anybody，并为anybody设置一定强度的密码。

	useradd anybody
	passwd anybody

注：创建修改用户需要root权限。

###3. 限制su/sudo

在Unix/Linux系统中，要获取其他用户的权限有两种方式，一是使用su(switch user)，切 换成为某用户，这样就拥有了该用户的所有权限，直到退出该用户；另外一种是sudo (do as another user)，以另外一个用户的身份执行某些操作，此时在短时间内你获取该用 户的权限，直到超时。

当然，su/sudo的细节可以由root进行控制。

####限制只有wheel组的用户可以使用su

编辑/etc/pam.d/su文件，启用用户组认证：

	# Uncomment the following line to require a user to be in the "wheel" group.
	
	auth            required        pam_wheel.so use_uid

将普通用户anybody添加到wheel组，wheel的gid为10

	usermod -G 10 anybody

这样就只有该用户和root可以使用su。

####限制普通用户使用sudo

sudo主要配置在/etc/sudoers，新增加的用户默认没有添加到sudoers中，若有需要，可以 参照配置中说明更改。

比较通用的做法是定制一些可以执行或禁止执行的命令集，而允许执行的命令集足以完成系 统的日常维护工作，禁止执行的命令集可能会使系统暴露不够安全，然后将这些命令集授权 给该用户，这样既分配适当权限，又作特殊限制。

当然，root涉及到系统安全远不止这些，以上仅是本站使用的一些策略，更详细的信息有待 进一步理解和实践。

##延伸阅读

* [Linux系统安全(一):安装与设置](http://www.ibm.com/developerworks/cn/linux/security/l-ossec/part1/)
* [权限安全使用和密码管理](http://www.ibm.com/developerworks/cn/linux/l-cn-rootadmin2/index.html)
* [了解和配置PAM](http://www.ibm.com/developerworks/cn/linux/l-pam/)
* [充分发挥sudo的作用](http://www.ibm.com/developerworks/cn/aix/library/au-sudo/)

---
layout: post
title: SSH Security Introduction on Linux
category : Linux
tags : [Linux, Security]
---

SSH (Secure Shell)可以说是每一台Unix/Linux主机的必备软件和服务，既可以用于远程主 机登录，也可以直接在远程主机上执行操作。

对于系统管理员来说，SSH就更显重要了。因为这意味着，当要进行服务器维护工作时，你 可以通过SSH在办公区直接管理服务器，而不必跑到机房，或者偏远的IDC。正如本站一样， 建站时即使用Xshell客户端远程登录到刚购买的Linode VPS，进行博客环境的搭建以及主机 基础安全防护工作；而无需翻山越岭跨不远万里地跑到Linode的某个数据中心。

SSH确实为我们开启了一扇方便之门，但使用不慎也会带来一些安全隐患。当你的主机暴露 在互联网上而不加任何防护，那么它很有可能就是某个好事者的下一道小菜。在开启SSH服 务时，我们可以适当更改某些设置，提高SSH服务的安全性，降低被攻击的可能性。

###1. 使用公钥私钥

使用公钥私钥对进行连接，而不需要输入口令，这样可以减少暴力破解的可能性。前提是需 要将私钥保护好。

####客户端

使用`ssh-keygen`命令，产生密钥对，并将公钥上传至服务端。

也可以使用shell客户端自带的Key管理工具来生成，本站即使用Xshell生成2048位的RSA密 钥对，并将公钥id_rsa.pub上传到服务端。

####服务端

在用户anybody下创建`.ssh`文件夹，并拷贝`id_rsa.pub`到`~/.ssh/authorized_keys`

	cd  /home/anybody
	mkdir .ssh
	cat id_rsa.pub >> .ssh/authorized_keys

	chown -R anybody:anybody .ssh

	chmod 700 .ssh
	chmod 600 .ssh/authorized_keys

注：`~/.ssh/authorized_keys`是ssh的默认配置，详情见`/etc/ssh/sshd_config`

###2. 修改默认端口

编号为1024以下的端口都是一些周知端口，SSH默认的端口号为22，更改SSH端口为非常规端 口，可以在一定程度上减少端口扫描的信息量，如改为5555，攻击者不一定能够很快猜测到 这个端口就是SSH服务，具有一定的不确定性。

更改配置 `/etc/ssh/sshd_config`：

	Port 5555

###3. 禁止root直接登录

root用户权限太大，直接使用root登录进行操作，稍有不慎就可能会引发意想不到的问题， 详见[root用户介绍和基础安全防护](http://www.dylanninin.com/blog/2012/10/root-info-and-basic-security.html)。 在SSH中禁用root登录：

	PermitRootLogin no

###4. 取消密码认证

前面已经通过ssh-key或SSH客户端工具生成了密钥对，就已经可以使用密钥进行登录认证了； 如果还留有密码认证，这显然会是好事者喜欢的盲点。在SSH中取消密码认证：

	PasswordAuthentication no

###5. 登录提醒

在用户成功登录后，sshd会进行一些的动作，其中可能会执行~/.ssh/rc和/etc/ssh/sshrc 脚本。利用这个特性，可以制定sshrc脚本，当有用户SSH远程登录时，发送邮件提醒，利于 尽早发现异常。

	echo "http://whatismyipaddress.com/ip/${SSH_CLIENT%% *}" | mail -s "$USER login from ${SSH_CLIENT%% *}" dylanninin@139.com

以上是本站使用SSH的一些基础安全防护策略。SSH的设计和功能很好很强大，Xshell也是一 款十分值得推荐的客户端，更多信息，可以参考延伸阅读。

##延伸阅读

* [SSH安全性和配置入门](http://www.ibm.com/developerworks/cn/aix/library/au-sshsecurity/index.html)
* [SSH原理与运用(一)](http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html)
* [SSH原理与运用(二)](http://www.ruanyifeng.com/blog/2011/12/ssh_port_forwarding.html)
* [与你潇洒人生路：SSH配置](http://www.cnblogs.com/shuaixf/archive/2012/05/25/2517947.html)
* [A挨个搞： Xshell十大技巧](http://actgod.com/archives/86/)

---
layout: post
title: Change the Name of Oracle Database
category : Oracle
tags : [Oracle, Database, DBA]
---

因新项目开发需要，现需建立新的测试数据库。但目前测试服务器没有到位，故暂且使用以 前的测试机代替，其中安装的Oracle数据库仅作学习测试使用，数据库名、实例名均为 dbtest。现在将此数据库的数据库名、实例名从dbtest改为CRMTEST。

以下是更改记录，以作备忘。

##测试环境

* 操作系统：CentOS 5.5
* 数据库: Oracle Database 10.2.0.1.0

##主要步骤
###1. 先更改dbname

修改oracle数据库的dbid和dbname，主要步骤如下：

* 1)将数据库启动到mount状态：`startup mount`;
* 2)使用nid命令修改：`nid target=/ DBNAME=CRMTEST`
* 3)更改初始化参数文件中的`db_name`：`db_name="CRMTEST"`
* 4)启动到mount：`startup mount`
* 5)以resetlogs打开数据库：`alter database open resetlogs`;

####1)重启数据库到mount

查看当前数据库names设置：

	SQL> show parameter name
	NAME                       TYPE      VALUE
	-------------------------- --------- ------------------------------
	db_file_name_convert       string
	db_name                    string    dbtest
	db_unique_name             string    dbtest
	global_names               boolean   FALSE
	instance_name              string    dbtest
	lock_name_space            string
	log_file_name_convert      string
	service_names              string    dbtest

查看当前数据库启动参数类型：

	SQL> show parameter spfile
	NAME                       TYPE      VALUE
	-------------------------- --------- ------------------------------
	spfile                     string     /db/oracle/product/10.2.0/db_1/
	                                      dbs/spfiledbtest.ora
	
从spfile创建pfile，便于直接编辑参数文件：

	SQL> create pfile from spfile;
	File created.
	
关闭数据库：

	SQL> shutdown immediate
	Database closed.
	Database dismounted.
	ORACLE instance shut down.

启动数据库到mount状态：

	SQL> startup mount;
	ORACLE instance started.
	Total System Global Area 603979776 bytes
	Fixed Size 1220796 bytes
	Variable Size 163581764 bytes
	Database Buffers 432013312 bytes
	Redo Buffers 7163904 bytes
	Database mounted.

####2)运行nid,更改`dbname`和`dbid`

nid即new database id，根据`dbname`生成新的`dbid`：

	[oracle@shoptest dbs]$ nid target=/ dbname=CRMTEST
	DBNEWID: Release 10.2.0.1.0 - Production on Tue Oct 9 20:02:52 2012
	Copyright (c) 1982, 2005, Oracle. All rights reserved.
	Connected to database DBTEST (DBID=1176995883)
	Connected to server version 10.2.0
	Control Files in database:
	/db/oracle/oradata/dbtest/control01.ctl
	/db/oracle/oradata/dbtest/control02.ctl
	/db/oracle/oradata/dbtest/control03.ctl
	Change database ID and database name DBTEST to CRMTEST? (Y/[N]) => y
	Proceeding with operation
	Changing database ID from 1176995883 to 889633516
	Changing database name from DBTEST to CRMTEST
	Control File /db/oracle/oradata/dbtest/control01.ctl - modified
	Control File /db/oracle/oradata/dbtest/control02.ctl - modified
	Control File /db/oracle/oradata/dbtest/control03.ctl - modified
	Datafile /db/oracle/oradata/dbtest/system01.dbf - dbid changed, wrote new name
	Datafile /db/oracle/oradata/dbtest/undotbs01.dbf - dbid changed, wrote new name
	Datafile /db/oracle/oradata/dbtest/sysaux01.dbf - dbid changed, wrote new name
	Datafile /db/oracle/oradata/dbtest/users01.dbf - dbid changed, wrote new name
	Datafile /db/oracle/oradata/dbtest/crm_data01.dbf - dbid changed, wrote new name
	Datafile /db/oracle/oradata/dbtest/crm_index01.dbf - dbid changed, wrote new name
	Datafile /db/oracle/oradata/dbtest/temp01.dbf - dbid changed, wrote new name
	Datafile /db/oracle/oradata/dbtest/crm_temp01.dbf - dbid changed, wrote new name
	Control File /db/oracle/oradata/dbtest/control01.ctl - dbid changed, wrote new name
	Control File /db/oracle/oradata/dbtest/control02.ctl - dbid changed, wrote new name
	Control File /db/oracle/oradata/dbtest/control03.ctl - dbid changed, wrote new name
	Instance shut down
	Database name changed to CRMTEST.
	Modify parameter file and generate a new password file before restarting.
	Database ID for database CRMTEST changed to 889633516.
	All previous backups and archived redo logs for this database are unusable.
	Database is not aware of previous backups and archived logs in Recovery Area.
	Database has been shutdown, open database with RESETLOGS option.
	Succesfully changed database name and ID.
	DBNEWID - Completed succesfully.

####3)更改initdbtest.ora，将`db_name`设置为CRMTEST

进入到$ORACLE_HOME/dbs目录：

	[oracle@shoptest dbs]$ pwd
	/db/oracle/product/10.2.0/db_1/dbs

更改`db_name`参数，改为CRMTEST：

	[oracle@shoptest dbs]$ vim initdbtest.ora
	... ...
	*.db_name='CRMTEST'
	... ...

####4)以`initdbtest.ora`启动数据库到mount状态

	SQL> startup mount pfile='/db/oracle/product/10.2.0/db_1/dbs/initdbtest.ora';
	ORACLE instance started.
	Total System Global Area 603979776 bytes
	Fixed Size 1220796 bytes
	Variable Size 163581764 bytes
	Database Buffers 432013312 bytes
	Redo Buffers 7163904 bytes
	Database mounted.

####5)以resetlogs开启数据库

	SQL> alter database open resetlogs;
	Database altered.

查看当前数据库的names：

	SQL> show parameter name
	NAME                       TYPE      VALUE
	-------------------------- --------- --------------------
	db_file_name_convert       string
	db_name                    string    CRMTEST
	db_unique_name             string    dbtest
	global_names               boolean 	 FALSE
	instance_name              string    dbtest
	lock_name_space            string
	log_file_name_convert      string
	service_names              string    dbtest

到这里，已经将`dbname`从dbtest更改为CRMTEST，但`instance_name`还是dbtest，接下来 将`intance_name`更改为CRMTEST。 其他的如`db_unique_name`，`service_names`同样更改为CRMTEST。

###2. 再更改`instance_name`

更改数据库的`instance_name`，主要步骤如下：

* 1)创建新的密码文件：`orapwd`
* 2)创建新的初始化参数文件:`cp init<SID>.ora to init<NEW_SID>.ora`，并作修改
* 3)以新的初始化参数文件启动数据库：`startup pfile='?/dbs/init<NEW_SID>.ora`
* 4)更改数据库以spfile启动:`create spfile from pfile`
* 5)修改监听：`vim $ORACLE_HOME/network/admin/listener.ora`

###1)创建新的密码文件`orapwCRMTEST`

	[oracle@shoptest dbs]$ pwd
	/db/oracle/product/10.2.0/db_1/dbs
	[oracle@shoptest dbs]$ orapwd file=orapwCRMTEST password=oracle entries=5
	[oracle@shoptest dbs]$ ll
	total 7252
	-rw-r----- 1 oracle oinstall 1544 Oct 9 20:20 hc_dbtest.dat
	-rw-r--r-- 1 oracle oinstall 1436 Oct 9 20:04 initdbtest.ora
	-r--r--r-- 1 oracle oinstall 982 May 24 15:20 initdbtest.ora.dist
	-rw-r----- 1 oracle oinstall 12920 May 3 2001 initdw.ora
	-rw-r----- 1 oracle oinstall 8385 Sep 11 1998 init.ora
	-rw-r----- 1 oracle oinstall 24 May 24 14:43 lkDBTEST
	-rw-r----- 1 oracle oinstall 2048 Oct 9 20:23 orapwCRMTEST
	-rw-r----- 1 oracle oinstall 1536 Sep 21 19:31 orapwdbtest
	-rw-r----- 1 oracle oinstall 7356416 Oct 9 20:06 snapcf_dbtest.f
	-rw-r----- 1 oracle oinstall 3584 Oct 9 20:17 spfiledbtest.ora

####2)创建initCRMTEST.ora文件，设置相关参数

拷贝initdbtest.ora到initCRMTEST.ora：
	
	[oracle@shoptest dbs]$ cp initdbtest.ora initCRMTEST.ora

更改initCRMTEST.ora文件，主要是instance_name，db_unique_name，dispatcher等：

	[oracle@shoptest dbs]$ vim initCRMTEST.ora 
	CRMTEST.__db_cache_size=432013312
	CRMTEST.__java_pool_size=4194304
	CRMTEST.__large_pool_size=4194304
	CRMTEST.__shared_pool_size=155189248
	CRMTEST.__streams_pool_size=0
	... ...
	*.db_name='CRMTEST'
	*.db_unique_name='CRMTEST'
	*.dispatchers='(PROTOCOL=TCP) (SERVICE=CRMTESTXDB)'
	... ...

####3)以initCRMTEST.ora启动数据库

因实例名已经从dbtest改为CRMTEST，需重新设置`ORACLE_SID`：

	[oracle@shoptest dbs]$ export ORACLE_SID=CRMTEST

以initCRMTEST.ora重启数据库：

	SQL> conn /as sysdba
	Connected.
	SQL> shutdown immediate;
	Database closed.
	Database dismounted.
	ORACLE instance shut down.
	SQL> startup pfile='/db/oracle/product/10.2.0/db_1/dbs/initCRMTEST.ora';
	ORACLE instance started.
	Total System Global Area 603979776 bytes
	Fixed Size 1220796 bytes
	Variable Size 163581764 bytes
	Database Buffers 432013312 bytes
	Redo Buffers 7163904 bytes
	Database mounted.
	Database opened.

查看当前数据库的names
	
	SQL> show parameter name
	NAME                        TYPE      VALUE
	--------------------------- --------- ------------------
	db_file_name_convert        string
	db_name                     string    CRMTEST
	db_unique_name              string    CRMTEST
	global_names                boolean   FALSE
	instance_name               string    CRMTEST
	lock_name_space             string
	log_file_name_convert       string
	service_names               string    dbtest

####4)创建spfile，并以spfile重启数据库

从pfile创建spfile：

	SQL> create spfile from pfile;
	File created.

关闭数据库：

	SQL> shutdown immediate;
	Database closed.
	Database dismounted.
	ORACLE instance shut down.

启动数据库：

	SQL> startup;
	ORACLE instance started.
	Total System Global Area 603979776 bytes
	Fixed Size 1220796 bytes
	Variable Size 163581764 bytes
	Database Buffers 432013312 bytes
	Redo Buffers 7163904 bytes
	Database mounted.
	Database opened.

确认当前数据库使用的参数文件类型：


	SQL> show parameter spfile;
	NAME                       TYPE     VALUE
	-------------------------- -------- ------------------------------
	spfile                     string   /db/oracle/product/10.2.0/db_1/
										dbs/spfileCRMTEST.ora

更改service_names：

	SQL> alter system set service_names=CRMTEST;
	
	System altered.

再次查看当前数据库的names：
	
	SQL> show parameter name
	NAME                        TYPE      VALUE
	--------------------------- --------- ------------------
	db_file_name_convert        string
	db_name                     string    CRMTEST
	db_unique_name              string    CRMTEST
	global_names                boolean   FALSE
	instance_name               string    CRMTEST
	lock_name_space             string
	log_file_name_convert       string
	service_names               string    CRMTEST

查看当前数据库的归档模式：

	SQL> archive log list;
	Database log mode Archive Mode
	Automatic archival Enabled
	Archive destination USE_DB_RECOVERY_FILE_DEST
	Oldest online log sequence 1
	Next log sequence to archive 1
	Current log sequence 1
	
####5)修改并重启监听

修改监听中的`SID_NAME`和`GLOBAL_DBNAME`：
	
	[oracle@shoptest admin]$ vim listener.ora
	# listener.ora Network Configuration File: /db/oracle/product/10.2.0/db_1/network/admin/listener.ora
	# Generated by Oracle configuration tools.
	
	SID_LIST_LISTENER =
	    (SID_LIST =
	        (SID_DESC =
	            (SID_NAME = PLSExtProc)
	            (ORACLE_HOME = /db/oracle/product/10.2.0/db_1)
	            (PROGRAM = extproc)
	        )
	        (SID_DESC =
	            (GLOBAL_DBNAME= CRMTEST)
	            (ORACLE_HOME = /db/oracle/product/10.2.0/db_1)
	            (SID_NAME = CRMTEST)
	        )
	    )
	
	LISTENER =
	    (DESCRIPTION_LIST =
	        (DESCRIPTION =
	            (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1))
	            (ADDRESS = (PROTOCOL = TCP)(HOST = shoptest.egolife.com)(PORT = 1521))
	        )
	)

重启监听：

	[oracle@shoptest admin]$ lsnrctl start
	LSNRCTL for Linux: Version 10.2.0.1.0 - Production on 10-OCT-2012 19:52:12
	Copyright (c) 1991, 2005, Oracle. All rights reserved.
	Starting /db/oracle/product/10.2.0/db_1/bin/tnslsnr: please wait...
	
	TNSLSNR for Linux: Version 10.2.0.1.0 - Production
	System parameter file is /db/oracle/product/10.2.0/db_1/network/admin/listener.ora
	Log messages written to /db/oracle/product/10.2.0/db_1/network/log/listener.log
	Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1)))
	Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=shoptest.egolife.com)(PORT=1521)))
	
	Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=EXTPROC1)))
	STATUS of the LISTENER
	------------------------
	Alias LISTENER
	Version TNSLSNR for Linux: Version 10.2.0.1.0 - Production
	Start Date 10-OCT-2012 19:52:13
	Uptime 0 days 0 hr. 0 min. 0 sec
	Trace Level off
	Security ON: Local OS Authentication
	SNMP OFF
	Listener Parameter File /db/oracle/product/10.2.0/db_1/network/admin/listener.ora
	Listener Log File /db/oracle/product/10.2.0/db_1/network/log/listener.log
	Listening Endpoints Summary...
	(DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1)))
	(DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=shoptest.egolife.com)(PORT=1521)))
	Services Summary...
	Service "CRMTEST" has 1 instance(s).
	Instance "CRMTEST", status UNKNOWN, has 1 handler(s) for this service...
	Service "PLSExtProc" has 1 instance(s).
	Instance "PLSExtProc", status UNKNOWN, has 1 handler(s) for this service...
	The command completed successfully

最后，注意更改bash环境变量，设置`ORACLE_SID=CRMTEST`，之前使用
`export ORACLE_SID=CRMTEST`仅对当前的用户有效

	[oracle@shoptest admin]$ vim ~/.bash_profile
	... ...
	export ORACLE_SID=CRMTEST
	... ...

##测试和确认

这样数据库的`instance_name`就更改完成，下面进行一些简单的测试，确认更改成功，主
要测试一下几个方面：
* 监听正常，可以接收连接请求：tnsping crmtest
* 密码文件生效：conn sys@crmtest /as sysdba

###1. 监听测试

####服务器端连接测试

配置tnsnames.ora：

	[oracle@shoptest admin]$ vim tnsnames.ora
	CRMTEST =
	    (DESCRIPTION =
	        (ADDRESS = (PROTOCOL = TCP)(HOST = shoptest.egolife.com)(PORT = 1521))
	        (CONNECT_DATA =
	            (SERVER = DEDICATED)
	            (SERVICE_NAME = CRMTEST)
	        )
	    )
	
	EXTPROC_CONNECTION_DATA =
	    (DESCRIPTION =
	        (ADDRESS_LIST =
	            (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1))
	        )
	        (CONNECT_DATA =
	            (SID = PLSExtProc)
	           (PRESENTATION = RO)
	        )
	    )

使用tnsping，测试crmtest连接是否正常：
	
	[oracle@shoptest admin]$ tnsping crmtest
	TNS Ping Utility for Linux: Version 10.2.0.1.0 - Production on 10-OCT-2012 19:54:31
	Copyright (c) 1997, 2005, Oracle. All rights reserved.
	
	Used parameter files:
	/db/oracle/product/10.2.0/db_1/network/admin/sqlnet.ora
	
	Used TNSNAMES adapter to resolve the alias
	Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = shoptest.egolife.com)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = CRMTEST)))
	OK (0 msec)

####Windows客户端连接测试

同样，先配置tnsnames.ora，然后运行tnsping crmtest，确认是否可以连接：

	C:\> tnsping crmtest
	TNS Ping Utility for 32-bit Windows: Version 11.2.0.1.0 - Production on 10-10月-2012 20:09:57
	Copyright (c) 1997, 2010, Oracle. All rights reserved.
	
	已使用的参数文件:
	D:\Oracle\11g\product\11.2.0\db_1\network\admin\sqlnet.ora
	
	已使用 TNSNAMES 适配器来解析别名
	尝试连接 (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = shoptest.egolife.com)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = CRMTEST)))
	OK (0 毫秒)

###2. orapwCRMTEST密码文件生效测试

再使用sqlplus以sysdba身份登录，检查密码文件是否生效：

	C:\>sqlplus /nolog
	SQL*Plus: Release 11.2.0.1.0 Production on 星期三 10月 10 20:11:38 2012
	Copyright (c) 1982, 2010, Oracle. All rights reserved.
	SQL> conn sys@crmtest/as sysdba
	输入口令:
	已连接。
	SQL>

##参考

* [Oracle 修改DB_NAME 和 DBID](http://blog.csdn.net/tianlesoftware/article/details/6087641)
* [如何修改数据库实例名](http://oonicedream.itpub.net/post/36905/457005)

---
layout: post
title: Firewall Introduction
category : Linux
tags : [Linux, Security]
---

Firewall，又称防火墙，顾名思义，即作为一道屏障防止火势蔓延。这正如战乱纷繁的诸侯 列国在重要的城池一般都有护城河、城墙、高山等层层屏障一样，若遇敌国侵袭，或者各种 动乱，则可以逐层抵挡以期缓解局势；说到这儿，你一定想到了，可以拿八达岭长城作比， 它似乎确凿已经起到了阻挡蛮夷隔离战火之功。

![greate wall](http://dylanninin.com/assets/images/2012/greate_wall.jpg)

既然防火墙可以抵挡外来入侵，从另外一个方面想，防火墙同样也可以限制内部资源向外蹦 蹿从而引发骚乱，想想《Prison Break》中监狱里的防电网，也想想万里长城的"庆长假， 量大，无侧翻"的稳定。

![overflow](http://dylanninin.com/assets/images/2012/greate_wall_overflow.jpg)

到这里，相信我们已经可以基本理解防火墙的最基本功能，即隔离，或在地域上，或在文化 上，或在网络上。在计算机领域，防火墙是一项协助信息安全的设备，可以制定一些特定的 规则，允许或限制数据的传输，从而将网络划分成不同的区域，制定出不同区域之间的访问 控制策略进而来控制不同区域间传送的数据流。通常，这些区域可以划分为两大类，即不可 信任的区域，和可信任或高信任的区域。典型的不可信区域即互联网，因为互联网对每个 人、每个设备都是开放的，可以说鱼龙混杂，好坏难分，一不小心你的对话就有可能人被窃 听，或者你的QQ号被盗，再或者服务器被黑客入侵等等。通常，一个内部网络可以被称为高 信任的区域，因为内部网络的组成十分单纯，网络工程师们已经尽力做了很多预期的设想和 限制，而且若不是别有用心，你绝不会冒失地去攻击内部网络的某某服务器。当内部网络即 可信任区域需要与外网即不可信任区域通信交流时，若没有任何保护措施，这几乎等同于把 服务器放在了黑客面前并说请随意入侵；可想而知，这种情形下，内部网络的可信任性就受 到了质疑。此时，防火墙即可担此重任，负责内外网的通信安全。

![firewall](http://dylanninin.com/assets/images/2012/firewall.png)

在现代操作系统中，大多已经自带了网络防火墙，通过定义一些分组规则，进行互联网、主 机之间数据过滤，来保证主机的相对安全。

在内核2.6.x的Linux系统中，自带了iptables防火墙，通常可以利用分组进行过滤，如来源 IP地址或端口号、目的IP地址或端口号、服务类型（如WWW或是FTP），也可以是通信协议、 来源网域、网段、网卡等。

##VPS防火墙设置

本站VPS使用的操作系统为CentOS 6.2，建站初期已经开启了防火墙服务，并制定了一些规 则，过滤掉一些不用或有危害的请求。

在一个简单的博客系统主机中，可以仅开启http（80端口）、https（443端口）、ssh（22 端口）这些服务，其中http/https用于web请求，加密或非加密；ssh用来远程登录，其他则 禁止。

检查防火墙是否应开启：

	[root@www web]# service iptables status

若没有开启，则运行以下命令，开启防火墙：

	[root@www web]# service iptables start

没有添加自定义规则的防火墙，也是采取了一些默认的接收或者拒绝请求的策略，但通常这 些策略太宽泛，因此我们需要自定义规则，进行严格的限制，从而创建一个相对安全的网络 环境。

创建文件，添加自定义规则：

	[root@www web]# vim /etc/iptables.rule
	
	*filter
	
	# Allow all loopback (lo0) traffic and drop all traffic to 127/8 that doesn't use lo0
	-A INPUT -i lo -j ACCEPT
	-A INPUT -d 127.0.0.0/8 -j REJECT
	
	# Accept all established inbound connections
	-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
	
	# Allow all outbound traffic - you can modify this to only allow certain traffic
	-A OUTPUT -j ACCEPT
	
	# Allow HTTP and HTTPS connections from anywhere (the normal ports for websites and SSL).
	-A INPUT -p tcp --dport 80 -j ACCEPT
	-A INPUT -p tcp --dport 443 -j ACCEPT
	
	# Allow SSH connections
	#
	# The -dport number should be the same port number you set in sshd_config
	#
	-A INPUT -p tcp -m state --state NEW --dport 22 -j ACCEPT
	
	# Allow ping
	-A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT
	
	# Log iptables denied calls
	-A INPUT -m limit --limit 5/min -j LOG --log-prefix "iptables denied: " --log-level 7
	
	# Reject all other inbound - default deny unless explicitly allowed policy
	-A INPUT -j REJECT
	-A FORWARD -j REJECT
	
	COMMIT
	
以上防火墙规则说明已经十分详细，不再赘述。

让防火墙规则生效

	[root@www web]# iptables-restore <  /etc/iptables.rule

查看防火墙规则是否生效：

	[root@www web]# iptables -L
	Chain INPUT (policy ACCEPT)
	target prot opt source destination 
	ACCEPT all -- anywhere anywhere 
	REJECT all -- anywhere loopback/8 reject-with icmp-port-unreachable 
	ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED 
	ACCEPT tcp -- anywhere anywhere tcp dpt:http 
	ACCEPT tcp -- anywhere anywhere tcp dpt:https 
	ACCEPT tcp -- anywhere anywhere state NEW tcp dpt:ssh 
	ACCEPT icmp -- anywhere anywhere icmp echo-request 
	LOG all -- anywhere anywhere limit: avg 5/min burst 5 LOG level debug prefix `iptables denied: ' 
	REJECT all -- anywhere anywhere reject-with icmp-port-unreachable
	
	Chain FORWARD (policy ACCEPT)
	target prot opt source destination 
	REJECT all -- anywhere anywhere reject-with icmp-port-unreachable
	
	Chain OUTPUT (policy ACCEPT)
	target prot opt source destination 
	ACCEPT all -- anywhere anywhere

最后，要保证服务器开机时就能够自动开启防火墙服务，并执行以上规则。此时需要做以下 两点：

###设置自动启动级别

	[root@www web]# chkconfig --list iptables
	iptables 0:off 1:off 2:on 3:on 4:on 5:on 6:off

在2,3,4,5级别上设置为自动开启即可，若有在这些级别上为off的，则设置为on：

	[root@www web]# chkconfig --level 2345  iptables on

###开机启动

编辑rc.local文件，让系统初始化完成后，自动加载以上规则

	[root@www web]# vim  /etc/rc.d/rc.local
	/sbin/iptables-restore <  /etc/iptables.rule

这样，简单的防火墙基础安全防护工作已经完成。

##延伸阅读

* [CentOS Wiki：HowTos/Network/IPTables](http://wiki.centos.org/HowTos/Network/IPTables)
* [新浪共享资料：2小时玩转iptables](http://ishare.iask.sina.com.cn/f/18503162.html)

##参考

* [Linode中文教程：做好VPS的基础安全防护工作](http://www.linode.im/1642.html)
* [维基百科：防火墙](http://zh.wikipedia.org/wiki/%E9%98%B2%E7%81%AB%E5%A2%99)
* [维基百科：runlevel](http://en.wikipedia.org/wiki/Runlevel)

##写在前面的话

今年上半年，由于工作调整，开始接手DBA的工作，负责ERP、数据库、服务器等的管理和维护工作。在粗略看完Oracle Database 2 Day DBA的官方文档后，结合一些中文书目以及工作中遇到的问题，对Oracle数据库有了初步的了解。但发现单看中文数目以及解决一个个突发问题，还远远不够，于是开始阅读2 Day DBA的升级版文档Administrator's Guide，并进行一些尝试和实践，希望能够有进一步的理解。

在Adminstartor's Guide的第二章，讲到了如何使用`CREATE DATABASE`语句手工创建数据库。这个语句比MySQL的复杂多了，涵盖了Oracle数据库最核心的概念，需要有一定的基础知识。按照参考文档的提示进行操作时，还是出现过一些错误，最后参考CSDN上的一篇[博客](http://blog.csdn.net/tianlesoftware/article/details/4680213)进行调整，成功创建了数据库，这里记录如下。

##测试环境

* 操作系统 CentOS 6.0 x86 64bit
* 数据库	  Oracle 10.2

##创建步骤

###1. 规划sid,oracle home

	export ORACLE_SID=MANUAL
	export ORACLE_BASE=/db/oracle
	export ORACLE_HOME=/db/oracle/product/10.2.0/db_1

###2. 系统规划

	ORACLE_SID=manual
	DB_NAME=MANUAL
	DB_DOMAIN=egolife.com

###3. 手工创建必须的目录

dump目录
	
	mkdir /db/oracle/admin/MANUAL/adump
	mkdir /db/oracle/admin/MANUAL/bdump
	mkdir /db/oracle/admin/MANUAL/cdump
	mkdir /db/oracle/admin/MANUAL/udump
	mkdir /db/oracle/admin/MANUAL/pfile

数据文件目录

	mkdir /db/oracle/oradata/MANUAL

恢复目录
	
	mkdir /db/oracle/flash_recovery_area/MANUAL

###4. 建立密码文件

	orapwd file=/db/oracle/product/10.2.0/db_1/dbs/orapwMANUAL password=oracle

###5. 修改参数文件

编辑init.ora文件，更改主要配置：

	MANUAL.__db_cache_size=331350016
	MANUAL.__java_pool_size=4194304
	MANUAL.__large_pool_size=8388608
	MANUAL.__shared_pool_size=138412032
	MANUAL.__streams_pool_size=0
	*._kgl_large_heap_warning_threshold=8388608
	*.audit_file_dest='/db/oracle/admin/MANUAL/adump'
	*.background_dump_dest='/db/oracle/admin/MANUAL/bdump'
	*.compatible='10.2.0.1.0'
	*.control_files='/db/oracle/oradata/MANUAL/control01.ctl',
	'/db/oracle/oradata/MANUAL/control02.ctl',
	'/db/oracle/oradata/MANUAL/control03.ctl'
	*.core_dump_dest='/db/oracle/admin/MANUAL/cdump'
	*.db_2k_cache_size=33554432
	*.db_block_size=8192
	*.db_domain='egolife.com'
	*.db_file_multiblock_read_count=128
	*.db_files=4000
	*.db_name='MANUAL'
	*.db_recovery_file_dest_size=4294967296
	*.db_recovery_file_dest='/db/oracle/flash_recovery_area'
	*.log_checkpoints_to_alert=FALSE
	*.open_cursors=300
	*.parallel_execution_message_size=65535
	*.parallel_max_servers=128
	*.pga_aggregate_target=209715200
	*.processes=150
	*.recyclebin='OFF'
	*.remote_login_passwordfile='EXCLUSIVE'
	*.replication_dependency_tracking=FALSE
	*.session_cached_cursors=100
	*.sga_target=500m
	*.shared_pool_size=100m
	*.undo_management='AUTO'
	*.undo_retention=0
	*.undo_tablespace='UNDOTS'
	*.user_dump_dest='/db/oracle/admin/MANUAL/udump'
	*.workarea_size_policy='AUTO'
	_allow_resetlogs_corruption=true

###6. 启动数据库

	SQL> conn /as sysdba
	Connected to an idle instance.
	SQL> startup nomount pfile=/db/oracle/product/10.2.0/db_1/dbs/initMANUAL.ora;
	
	ORACLE instance started.
	
	Total System Global Area  524288000 bytes
	Fixed Size     1220336 bytes
	Variable Size   150995216 bytes
	Database Buffers   364904448 bytes
	Redo Buffers     7168000 bytes

###7. 运行创建数据库脚本

	CREATE DATABASE MANUAL
	LOGFILE
	GROUP 1 ('/db/oracle/oradata/MANUAL/redo01.log',
	'/db/oracle/oradata/MANUAL/redo01_1.log') size 100m reuse,
	GROUP 2 ('/db/oracle/oradata/MANUAL/redo02.log',
	'/db/oracle/oradata/MANUAL/redo02_1.log') size 100m reuse,
	GROUP 3 ('/db/oracle/oradata/MANUAL/redo03.log',
	'/db/oracle/oradata/MANUAL/redo03_1.log') size 100m reuse
	MAXLOGFILES 50
	MAXLOGMEMBERS 5
	MAXLOGHISTORY 200
	MAXDATAFILES 500
	MAXINSTANCES 5
	ARCHIVELOG
	CHARACTER SET UTF8
	NATIONAL CHARACTER SET UTF8
	DATAFILE '/db/oracle/oradata/MANUAL/system01.dbf' SIZE 1000M EXTENT MANAGEMENT LOCAL
	SYSAUX DATAFILE '/db/oracle/oradata/MANUAL/sysaux01.dbf' SIZE 1000M
	UNDO TABLESPACE UNDOTS DATAFILE '/db/oracle/oradata/MANUAL/undo.dbf' SIZE 500M
	DEFAULT TEMPORARY TABLESPACE TEMP TEMPFILE '/db/oracle/oradata/MANUAL/temp.dbf' SIZE 500M;

###8. 运行必要的sql脚本(注意按以下顺序)

	/db/oracle/product/10.2.0/db_1/rdbms/admin/catalog.sql
	/db/oracle/product/10.2.0/db_1/rdbms/admin/catproc.sql

###9. 创建相关表空间和用户
	
	create tablespace users datafile '/db/oracle/oradata/MANUAL/users01.dbf' size 500M;
	create tablespace indexes datafile '/db/oracle/oradata/MANUAL/index01.dbf' size 500M;

建立测试用户

	create user dylan identified by 000000 default tablespace users;
	grant connect,resource to dylan;

###10. 改为spfile启动

	create spfile from pfile;

重启数据库，并运行show parameter spfile，确认启动的参数文件类型。

###11. Windows客户端测试

tnsnames.ora配置：

	MANUAL =
	  (DESCRIPTION =
	    (ADDRESS = (PROTOCOL = TCP)(HOST = dev.egolife.com)(PORT = 1521))
	    (CONNECT_DATA =
	      (SERVER = DEDICATED)
	      (SERVICE_NAME = MANUAL.egolife.com )
	    )
	  )

运行tnsping MANUAL命令，并使用sqlplus进行连接测试。

##参考

* [Manually Creating an Oracle Database](http://docs.oracle.com/cd/B19306_01/server.102/b14231/create.htm#sthref220)
* [Linux下手工新建数据库](http://blog.csdn.net/tianlesoftware/article/details/4680213)
##写在前面的话

Oracle Database概念非常多，其中关于SID、NAMES以及监听参数的定义和关系经常让人混淆。在[tianlesoftware](http://blog.csdn.net/tianlesoftware)的博客上，有一篇专门讲解了这些概念，行文思路清晰，概念和实践分析相结合，易于理解；且文中大量引用官方文档，来源可靠，可信度高。作者的博客很值得同行学习和借鉴。

由于刚入行不久，秉承实用原则这里仅作粗略了解和记录，详情可以参见博客原文。

##基本概念

###db相关

* dbid, sid

###pfile参数

* `db_name`, `db_domain`, `instance_name`

* `db_unique_name`, `service_names`

* `global_name`, `global_names`

###listener.ora参数

* `sid_name`, `global_dbname`

###tnsnames.ora参数

* `service_name`, `sid`

###1. DB相关

####dbid

* `db_name`在数据库内部的表示，创建数据库时用`db_name`结合一种算法生成。

* `dbid`存在于数据文件、控制文件中，表示数据文件的归属，`dbid`唯一。不同数据库`dbid`不同，但`db_name`有可能相同

####dbid 和 sid

查看dbid：

	select * from v$database;

####在controlfile中查看dbid和sid

转存控制文件到trace file：

	alter session set events 'immediate trace name CONTROLF level 8'; 

	select gettracefile() from dual; 

	Db ID=1873459096=0x6faab798, Db Name='EGOLIFE'

####修改`dbid`和`dbname`

主要步骤：

* a. `startup mount`;
* b. `nid TARGET=SYS/oracle@egolife DBNAME=one`
* c. `startup mount`;
* d. `alter database open resetlogs`;

更改`dbname`自后，之前的备份均无效。

####查看`sid`(system identifier)

	ORACLE_SID sid
	ORACLE_HOME oracle home

oracle用`sid`和`oracle_home`生成一个key，来创建共享内存。

###2. pfile参数

	SQL> show parameter name
	NAME TYPE VALUE
	-------------------------- ----------- --------------------
	db_file_name_convert       string
	db_name                    string       MANUAL
	db_unique_name             string       MANUAL
	global_names               boolean     	FALSE
	instance_name              string       MANUAL
	lock_name_space            string
	log_file_name_convert      string
	service_names              string       MANUAL.egolife.com
	
####`db_name`

* 不能超过8个字符
* 动态注册监听

###`instance_name`

* 在RAC环境中`DB_NAME`相同，但`INSTANCE_NAME`不同，用以区分唯一的实例
* 默认值为`SID`，一般与数据库名相同，也可以不同
* 与进程名相关 ,并且`initSID.ora/orapwSID`与`instance_name`保持一致

####`db_domain`

* 区分网络层次结构

####`db_unique_name`

* 在DataGuard环境中，DB_NAME相同，但`DB_UNIQUE_NAME`不同

####`service_names`

* 值：`db_unique_name.db_domain`

####`global_name`

* 由`db_name.db_domain`组成

####`global_names`

* 创建db link时是否强制使用远程数据库的`global_name`，多用于分布式系统

###3. listener.ora参数

####`sid_name`

* 数据库的运行的实例名，与`instance_name`一致

####`global_dbname`

* 配置静态监听需要使用
* `global_dbname`是listener对外的连接名称，我们可以写成任意值，客户端配置时，`service_name`需要与`global_dbname`一致
* 如果是动态监听，因`service_names`由多个，则会注册多个，每个对应着同一个`instance_name`，这样配置任意一个即可访问

###4. tnsnames.ora参数

###`service_name`

* 静态监听：`service_name` = `global_dbname`
* 动态监听：`service_name` = `service_names`(在initSID.ora文件中)

###`sid`

* 直接指定`instance_name`


##参考

 * [SID、NAMES及监听参数说明](http://blog.csdn.net/tianlesoftware)
 * [Oracle小知识总结(一)](http://blog.csdn.net/tianlesoftware/article/details/5622268)
---
layout: post
title: Mangement of Print Queues on Oracle EBS
category : Oracle
tags : [Oracle, EBS, DBA]
---

##写在前面的话

在ERP系统中，打印报表是不可或缺的应用之一。

由于Oracle本身的集成性，很多时候需要第三方工具的支持来一起完成某些任务，打印报表 即是其中一个的典型。

在Oracle EBS 11i以上版本中，一个典型的报表打印设置，就涉及到ERP应用中打印队列的 管理、ERP主机上打印队列的管理、打印服务器上Remote Print Manager打印队列的管理以 及远程打印机的管理。

Oracle ERP打印队列结构图

![Oracle ERP](http://dylanninin.com/assets/images/2012/printer_of_erp.png)

##打印服务器

###1. Windows打印机管理

在Windows中，有两种方式添加打印机：

* 1）图形界面方式:在控制面板中，添加打印机和传真

* 2）命令行方式:C:\Windows\System32\prnmngr.vbs           

当批量添加打印机时，使用图形界面的方式实在是一件体力活。如在做打印服务器迁移时， 将几台打印服务器上的远程打印机迁移到同一台打印服务器，此时使用向导界面则会非常耗 时，而且稍有不慎，就可能会勾选错误，导致打印机设置不正确；当然，出现问题也不好排 查。所幸的是，Windows也提供了vbs脚本来管理打印机。

####使用命令行管理打印机

使用命令行管理打印机需要Windows操作系统中有`prnmngr.vbs`脚本，一般Windows XP， Windows Server 2003中会自带这些脚本，以下为简单的学习笔记。

	cscript prnmngr.vbs
	cd  /D  C:\Windows\System32
	cscript prnmngr.vbs
	用法: prnmngr [-adxgtl?][c] [-s 服务器][-p 打印机][-m 驱动程序型号]
	             [-r 端口][-u 用户名][-w 密码]
	
	参数:
	-a     - 添加本地打印机
	-ac    - 添加打印机连接
	-d     - 删除打印机
	-g     - 获取默认打印机
	-l     - 列出打印机
	-m     - 驱动程序型号
	-p     - 打印机名
	-r     - 端口名
	-s     - 服务器名
	-t     - 设置默认打印机
	-u     - 用户名
	-w     - 密码
	-x     - 删除所有打印机
	-?     - 显示命令用法
 
	例如:
	prnmngr -a -p "打印机" -m "驱动程序" -r "lpt1:
	prnmngr -d -p "打印机" -s 服务器
	prnmngr -ac -p "\\服务器\打印机"
	prnmngr -d -p "\\服务器\打印机"
	prnmngr -x -s 服务器
	prnmngr -l -s 服务器
	prnmngr -g
	prnmngr -t -p "\\服务器\打印机"

#####主要用法

添加本地打印机

	cscript prnmngr.vbs -a -p PrinterName [-s RemoteComputer] -m DriverName -r PortName  [-u UserName] [-w Password]

示例

	cscript prnmngr.vbs -a -p testptq_1 -m "Epson LQ-680Pro ESC/P 2"  -r LPT1:

删除打印机

删除指定打印机

	cscript prnmngr.vbs -d -p PrinterName [-s RemoteComputer] [-u UserName] [-w Password]

删除所有打印机，慎用！

	cscript prnmngr.vbs -x [-s RemoteComputer] [-u UserName] [-w Password]

设置默认打印机

显示默认打印机

	cscript prnmngr.vbs -g
	
设置默认打印机

	cscript prnmngr.vbs -t -p PrinterName

列出所有打印机

	cscript prnmngr.vbs -l [-s RemoteComputer] [-u UserName -w Password]

示例

	cscript prnmngr.vbs -l

添加远程打印机

先创建tcp/ip端口

	cscript prnport.vbs -a -r IP_192.168.1.111_1 -h 192.168.1.111 -o lpr -q testptq

再添加远程打印机

	cscript prnmngr.vbs -a -p testptq -m "Epson LQ-680Pro ESC/P 2" -r IP_192.168.1.111_1

打印机服务脚本

* prnmngr.vbs：添加或删除打印机
* prncnfg.vbs：配置打印机
* prndrvr.vbs：添加或删除打印机驱动程序
* prnjobs.vbs：管理打印作业
* prnqctl.vbs：管理打印测试页、暂停打印机、继续运行打印机或清除打印机队列
* prnport.vbs：添加或删除打印机端口

###2. RPM打印队列管理

在Windows打印服务器管理打印队列，可以使用付费软件Remote Print Manager。用来接收 来自ERP服务器的打印请求，接收请求后，在RPM打印队列中进行一些格式的调整或转换，之 后，再将这些请求发送给远程打印机进行真正的打印作业。

目前主要接触过RPM Elite 4.5和RPM 5.1.1.99，新版本的功能更加强大，管理也方便。


###3. AIX打印队列管理

####基本概念

print job

* 在打印机上运行的作业单元
* 一个打印任务可以打印单个或多个文件
* 每个打印任务由job id 唯一标识

queue

* 打印队列用来管理打印任务
* 配置：/etc/qconfig
* 队列名和队列设备

queue device

* 队列设备，可以是本地、远程等设备

注：一个单独的打印队列可以与多个队列设备关联

Print spooler

* 管理打印的通用系统

####日常管理

#####create

* 命令:smit print

#####configure

* /etc/qconfig

* example:

	testptq:
	        device = @testptq
	        up = TRUE
	        host = testptq                                               
	        s_statfilter = /usr/lib/lpd/bsdshort
	        l_statfilter = /usr/lib/lpd/bsdlong
	        rq = testptq                                                             
	@testptq:
	        backend = /usr/lib/lpd/rembak

#####control

######lpstat 
	
lpstat：显示线性打印机的目前状态信息

	usage: lpstat [-drstW][-aDestination,...][-cClassname,...]
	              [-oOutRequirement,...][-pPrinter,...][-uUser,...]
	              [-vPrinter,...] [jobid,...]
			Prints LP status information.

qchk：显示某些打印任务、打印队列的目前状态信息

lpq：显示某些指定用户、任务号的打印任务的状态信息

lpr：使用打印系统打印文件

######stop

	enq  -D -P 'testoffice:@testptq'  #D Down print queue
	enq  -A -w Delay   #当打印队任务完成时每间隔多少秒更新打印状态信息

######start

	lpstat -vxxoffice              #status
	enq -U -P 'xxoffice:@testptq'  #start queue
	qchk -P  xxoffice              #status
	enabl    xxoffice:@testptq     #start queue

######flush

	lpstat -vxxoffice
	qcan -P xxoffice -x 3          #cancel job 3 on queue xxoffice
	cancel       3                 #cancel job 3
	lprm -P xxoffice 3             #cancel job 3 on queue xxoffice
	enq -P xxoffice  -x 3          #cancel job 3 on queue xxoffice

example:

	# lpstat -vxxoffice
	Queue   Dev   Status    Job Files              User         PP %   Blks  Cp Rnk
	------- ----- --------- --- ------------------ ---------- ---- -- ----- --- ---
	xxoffic @xxof DOWN    
	              QUEUED    505 STDIN.966664       testmgr                1   1   1
	              QUEUED    506 STDIN.622782       testmgr                1   1   2
	              QUEUED    507 STDIN.581720       testmgr                1   1   3
	: (FATAL ERROR) 0781-233 Unknown host xxoffice.

	# qcan -P xxoffice -x 505
	: (FATAL ERROR) 0781-233 Unknown host xxoffice.

	# lpstat -vxxoffice     
	Queue   Dev   Status    Job Files              User         PP %   Blks  Cp Rnk
	------- ----- --------- --- ------------------ ---------- ---- -- ----- --- ---
	xxoffic @xxof DOWN    
	
	              QUEUED    506 STDIN.622782       testmgr                1   1   1
	
	              QUEUED    507 STDIN.581720       testmgr                1   1   2
	
	: (FATAL ERROR) 0781-233 Unknown host xxoffice.
	# cancel 506
	# lpstat -vxxoffice
	Queue   Dev   Status    Job Files              User         PP %   Blks  Cp Rnk
	------- ----- --------- --- ------------------ ---------- ---- -- ----- --- ---
	xxoffic @xxof DOWN    
	              QUEUED    507 STDIN.581720       testmgr                1   1   1
	: (FATAL ERROR) 0781-233 Unknown host xxoffice.

######check

* lpstat       
* ps ef | grep qdaemon
* make sure that system date is correct   #enq -Y to register qconfig file
* make sure /tmp directory is not full
* obsolete queue:/var/spool/lpd/qdir and remove from /etc/qconfig
* hosts can be pinged 

#####命令汇总

Commands and equivalents

	Submit print jobs   Status print jobs    Cancel print jobs
	enq                 enq -A               enq -x
	qprt                qchk                 qcan
	lp                  lpstat               lprm
	lpr                 lpq                  cancel

###4. ERP打印队列的管理

在ERP应用中打印队列的设置主要包括打印机驱动、样式、类型以及注册等四个方面，用户 在提交报表请求时选择所需要的打印机即可完成想要的打印作业。

其中，打印驱动设置是重点。

打印机的驱动，是ERP应用中打印请求抛向外部（操作系统、打印机等）的出口，可以使用 操作系统命令、程序或者子例程的方式实现。

本例中，直接使用操作系统的打印命令，如下：

![printer_driver](http://dylanninin.com/assets/images/2012/checklp.png)

直接调用操作系统的打印命令进行打印，其中arguments内容如下：

	 /usr/bin/iconv -f UTF-8 -t GB18030 $PROFILES$.FILENAME | lpr -P$PROFILES$.PRINTER -#$PROFILES$.CONC_COPIES -T"$PROFILES$.TITLE"

 配置好打印驱动之后，还需要设置好打印样式、打印类型；之后，就可以在ERP应用中注册 打印队列。

###5.CentOS打印队列管理

Oracle ERP也是一个跨平台的应用，除了可以部署在Unix上，如IBM AIX，还可以部署在 Linux、Windows平台，如CentOS等。

在CentOS上，提供了与AIX系统上类似的lpstat、cancel等cups打印机管理工具，只要熟悉 了AIX上打印作业管理，在其他平台上上手十分容易。

 CentOS中，也提供了两种打印机管理方式，图形界面和命令行。在使用过程中，若批量新 建打印队列，则可以先以图形向导设置一个打印队列，测试通过后，再直接编辑配置文件， 重启打印服务器即可。

CentOS中，打印机配置文件：`/etc/cups/printers.conf`。编辑后，重启cups服务。

##测试

从Oracle ERP打印机设置中可以看出，在实际使用过程中，打印请求会首先从ERP应用中运 行完毕后以打印命令的方式抛向ERP服务器；然后，ERP服务器通过CUPS/lpd服务或协议将请 求发送给远程打印服务器，这里为RPM打印管理服务器；在RPM打印管理服务器上，又会进行 一些格式的调整或转换，最后才将请求抛给远程打印机进行真正的打印作业。

一个打印作业经过ERP应用、ERP服务器、RPM服务器、打印机最后才能真正变成手上拿着的 报表或者单据。若其中任何一个环节出现问题，打印作业则有可能出现异常，因此快速的排 查解决问题十分重要。这里提供一个思路，类似于单元测试、集成测试的概念，自底向上， 从依赖性最小的环节开始测试，确保没有问题，再测试依赖此服务的其他环节，直到最后能 够形成一套完整、可用的打印服务。

在此例中，测试可能需要经过以下步骤：

###1. Windows上打印机管理

新建TCP/IP端口，并创建远程打印机后，打印测试页，进行测试。

注：在测试前，确保远程打印机可以ping通。

###2. RPM上打印队列管理

目前暂未找到直接测试RPM上打印队列的方法

###3. AIX上打印队列管理

使用命令行测试

在ERP应用中，注册的打印队列使用命令行方式向ERP服务器抛送打印请求，这里可以直接在 OS上进行测试。

	[root@erpprod]# iconv -f UTF-8 -t GB18030 erp_printer.txt | lpr -Ptestptq -#1 -T"打印测试Oracle ERP"

然后，使用`lpstat`查看OS上打印作业状态，查看RPM上打印作业计数等

注：在测试前，确保打印服务器可以ping通，打印队列和打印服务已经启动；

打印时，可以打印中英文，测试下是否有乱码发生。

###4. ERP上打印队列管理

设置好打印驱动、打印队列后，提交打印请求，更改打印选项，选择测试的打印机，提交请 求，查看打印效果。

##脚本

在实际应用中，由于远程打印机、网络、断电重启等原因，时常会出现ERP服务器上打印队 列down掉的情况，这时需要手动检查远程打印机的网络是否正常，并重启打印队列。

对于日常维护来说，这会是一件例行事务，若用户要求打印业务需在上班时正常运行，则必 须把检查打印队列作为上班时的第一项任务，这样无疑也占用了宝贵的时间。

前面提到过AIX上一些打印队列管理的命令，因此可以编写简单的shell脚本，检查网络、打 印队列等，并尝试重启打印队列，再部署成定时任务，出现问题时发送邮件通知。这样可以 将时间留给其他更重要的工作。

###1.脚本示意图

![](http://dylanninin.com/blog/assets/themes/images/2012/checklp.png)

##2.checklp.sh 检查队列

	#!/bin/ksh
	#abstract:
	#check lpstat
	#history:
	#2012-09-04 dylanninin@gmail.com first release
	#variables
	script_basepath=/u1/PROD/prodora/itsection/sys/lpd
	mail_date=$(date +%Y-%m-%d\ %H:%M:%S)
	receipt=dylanninin@gmail.com
	hostname=$(hostname)
	
	#path
	export PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin
	
	#lpstat
	lpstat_basepath=/var/spool/lpd/stat
	lpstats=${lpstat_basepath}/s.*.@*
	
	#check hosts
	checklp_log=${script_basepath}/checklp.log
	if [ -e ${checklp_log} ]; then
	    true > ${checklp_log}
	else
	    touch ${checklp_log}
	fi
	
	#check ping function
	function check_ping {
	    v_ping=$1
	        if [ ${v_ping} == "" ]; then
	            echo "!!!!!! hostname or ip could not be null or empty!" >> ${checklp_log} 2>&1
	        else
	            ping -c 1 -w 1 ${v_ping} &> /dev/null && result=0 || result=1
	            if [ "${result}" == 0 ]; then
	                echo ">>>>>> Host ${v_ping} is Up!"
	            else
	                echo "!!!!!! Host ${v_ping} is Down!" >> ${checklp_log} 2>&1
	            fi
	        fi
		return
	}
	
	#host list of print queue
	host_list=$(ls -l ${lpstats} | cut -d "@" -f 2 | sort | uniq )
	for v_host in ${host_list}
	do
	     check_ping ${v_host}
	done
	
	office_ip=${script_basepath}/office.ip
	if [ ! -e ${office_ip} ]; then
	    echo "!!!!!! office ip file ${office_ip} does not exist!" >> ${checklp_log} 2>&1
	    exit 0
	fi
	
	#check lpstat
	lplist=$(ls -l ${lpstats} | awk -F "." '{print $2 ":" $3}')
	office_log=${script_basepath}/office.log
	for v_printer in ${lplist}
	do
	    v_queue=$(echo ${v_printer} | cut -d ":" -f 1)
	    v_host=$(echo ${v_printer} | cut -d "@" -f 2)
	    lpstat -W -v${v_queue} > ${office_log} 2>&1
	    v_down=$(grep "DOWN" ${office_log})
	    v_error=$(grep "ERROR" ${office_log})
	    if [ "${v_down}" != "" ]; then
	        v_office=$(echo ${v_queue} | cut -d "-" -f 1)
	        v_ip=$(grep ${v_office} ${officeip} | cut -d ":" -f 2)
	        if [ "${v_ip}" != "" ]; then
	            check_ping ${v_ip}
	        fi
	        echo "${v_down}" >> ${checklp_log} 2>&1
	        #stop queue
	        echo ">>>>>> enq -D -P${v_printer}"
	        enq -D -P${v_printer} > ${restart_log} 2>&1
	        #startup queue
	        echo ">>>>>> enq -U -P${v_printer} ..."
	        enq -U -P${v_printer} > ${restart_log} 2>&1
	    elif [ "${v_error}" != "" ]; then
	        echo "${v_error}" >> ${checklp_log} 2>&1
	    fi
	done
	if [ -e ${office_log} ]; then
	    true > ${office_log}
	fi
	
	#mail
	if [ $(cat ${checklp_log} | wc -l) -gt 0 ]; then
	    cat ${checklp_log} | mail -s "${mail_date}:${hostname} dba daily check of lpstat" ${receipt} 
	fi

###3.office.ip 办事处IP

	xxoffice:192.168.1.111
	xx1office: 192.168.2.111
	xx2office: 192.168.3.111
	... ...
	xxnoffice: 192.168.255.111

###4.restartlp.sh 重启所有队列

	#!/bin/ksh
	#abstract:
	#check lpstat
	#history:
	#2012-09-04     mis_ghb         first release
	#2012-12-04     mis_ghb         restart all lpd queue
	#variables
	script_basepath=/u1/PROD/prodora/itsection/sys/lpd
	mail_date=$(date +%Y-%m-%d\ %H:%M:%S)
	receipt=dylanninin@gmail.com
	hostname=$(hostname)
	
	#path
	export PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin
	
	#lpstat
	lpstat_basepath=/var/spool/lpd
	lpstats=${lpstat_basepath}/stat/s.*.@*
	lpd_qdir=${lpstat_basepath}/qdir
	
	#check restartlp log file
	restartlp_log=${script_basepath}/restartlp.log
	if [ -e ${restartlp_log} ]; then
	   true > ${restartlp_log}
	else
	   touch ${restartlp_log}
	fi
	
	#check qdir
	qdir_grep=$(ls ${lpd_qdir} | grep -v ".core")
	if [ -e "${qdir_grep}" == "" ]; then
	        ls -l ${lpd_qdir} > ${restartlp_log}
	fi
	
	#check lpstat
	lplist=$(ls -l ${lpstats} | awk -F "." '{print $2 ":" $3}')
	for v_printer in ${lplist}
	do
	    #stop queue
	    echo ">>>>>> enq -D -P${v_printer}"
	    enq -D -P${v_printer} >  ${restart_log} 2>&1
	    #startup queue
	    echo ">>>>>> enq -U -P${v_printer} ..."
	    enq -U -P${v_printer} >  ${restart_log} 2>&1
	done
	
	#mail
	if [ $(cat ${restartlp_log} | wc -l) -gt 0 ]; then
	   cat ${restartlp_log} | mail -s "${mail_date}:${hostname} dba daily check of lpstat" ${receipt} 
	fi

###5.testlp.sh测试打印队列

	#!/bin/ksh
	#abstract:
	#check lpstat
	#history:
	#2012-09-04     mis_ghb         first release
	#variables
	script_basepath=/u1/PROD/prodora/itsection/sys/lpd
	content=${script_basepath}/printer_test.txt
	current_date=$(date +%Y-%m-%d\ %H:%M:%S)
	
	#path
	export PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin
	
	lpq=$1
	if [ "${lpq}" == "" ]; then
	    echo "print queue must be specified!"
	elif [ ! -e ${content} ]; then
	    echo "print content file ${content} does not exist!"
	else
	    echo "iconv -f UTF-8 -t GB18030 ${content} | lpr -P${lpq} -#1 -T'Print Queue Test on Oracle EBS at ${current_date}'"
	    iconv -f UTF-8 -t GB18030 ${content} | lpr -P${lpq} -#1 -T'Print Queue Test on Oracle EBS at ${current_date}'
	fi

###6. 其他

以上脚本发送的邮件若出现中文乱码，可以使用iconv进行编码转换，如：

	if [ `cat ${sqllog} | wc -l` -gt 0 ]; then
	   iconv -f UTF-8 -t GB18030 ${sqllog} | mail -s "${mail_date}:${hostname} dba daily check of ${script_name}" ${receipt} 
	fi

##参考

* [Microsoft：prnmngr.vbs](http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/prnmngr.mspx?mfr=true)
* IBM AIX Redbook：IBM @server p5 and pSeries Administration and Support for AIX 5L Version 5.3

---
layout: post
title: Rsync Exception
category : Linux
tags : [Linux, Oracle, Exception]
---

[rsync](http://en.wikipedia.org/wiki/Rsync)是Unix/Linux下同步文件的一个高效算法， 它能同步更新两处计算机的文件与目录，并适当利用查找文件中的不同块以减少数据传输。 rsync中一项与其他大部分类似程序或协定中所未见的重要特性是镜像是只对有变更的部分 进行传送。rsync可拷贝/显示目录属性，以及拷贝文件，并可选择性的压缩以及递归拷贝， 同步速度快。

本文主要是记录在工作中使用rsync进行远程同步时遇到的问题及其解决方案，同时向大家 推荐关于rsync的两篇博客，一篇是酷壳陈皓的[rsync的核心算法](http://coolshell.cn/articles/7425.html)； 另一篇是51CTO上[抚琴煮酒的CentOS 5.5 下 rsync使用技巧与权限问题解读](http://os.51cto.com/art/201101/243374.htm)。

##环境

主机

	192.168.0.30 any.egolife.com  
	提供数据库服务，并部署rman增量备份
	使用inotify和rsync实时同步rman备份到备机，为rsync客户端

备机

	192.168.0.29 anybak.egolife.com
	部署数据库，配置与主机相同，但不提供数据库服务，仅在主机宕机时进行rman异机恢复
	部署rsync服务，为rsync服务端

以下是一次使用rsync出现异常的维护记录。

##第一次同步异常

查看某一系统主机和备机，发现2012-8-7的备份未进行同步，查看日志rsync连接异常，并手动进行测试。

rsync服务端输出日志如下

	[root@anybak rmanbak]# tail -f /var/log/rsyncd.log
	2012/08/08 09:18:05 [9301] params.c:Parameter() - Ignoring badly formed line in configuration file: ignore errors
	2012/08/08 09:18:05 [9301] name lookup failed for 192.168.0.30: Temporary failure in name resolution
	2012/08/08 09:18:05 [9301] connect from UNKNOWN (192.168.0.30)
	2012/08/08 09:18:05 [9301] rsync to anbak from oracle@unknown (192.168.0.30)
	2012/08/08 09:18:05 [9301] rmanbak/
	2012/08/08 09:18:28 [9301] inflate (token) returned -5
	2012/08/08 09:18:28 [9301] rsync error: error in rsync protocol data stream (code 12) at token.c(478) [receiver=2.6.8]
	2012/08/08 09:18:28 [9301] rsync: connection unexpectedly closed (2097 bytes received so far) [generator]
	2012/08/08 09:18:28 [9301] rsync error: error in rsync protocol data stream (code 12) at io.c(463) [generator=2.6.8]

###1.ignore errors 

查看`/etc/rsyncd.conf`，配置了ignore errors，注释掉即可；ignore errors 可以忽略掉一些无关的IO错误。

###2.name lookup failed for 192.168.0.30: Temporary failure in name resolution

rsync 启用了DNS反向解析，查询不到时，可能需要花很长时间。 在`/etc/hosts`文件中，添加`192.168.0.30 any.egolife.com` 配置重新启动rynsc服务和客户端脚本，即可正常同步。

##第二次同步异常

查看某一系统主机和备机，发现2012-9-25的备份未进行同步。

rsync客户端日志

	inflate (token) returned -5
	rsync: connection unexpectedly closed (229751 bytes received so far) [sender]
	rsync error: error in rsync protocol data stream (code 12) at io.c(463) [sender=2.6.8]

rsync服务端日志

	[root@anybak ~]# tail -f /var/log/rsyncd.log 
	2012/09/25 16:57:27 [8694] connect from any.egolife.com (192.168.0.30)
	2012/09/25 16:57:27 [8694] rsync to anybak from oracle@any.egolife.com (192.168.0.30)
	2012/09/25 16:57:27 [8694] rmanbak/
	2012/09/25 16:57:50 [8694] inflate (token) returned -5
	2012/09/25 16:57:50 [8694] rsync error: error in rsync protocol data stream (code 12) at token.c(478) [receiver=2.6.8]
	2012/09/25 16:57:50 [8694] rsync: connection unexpectedly closed (2096 bytes received so far) [generator]
	2012/09/25 16:57:50 [8694] rsync error: error in rsync protocol data stream (code 12) at io.c(463) [generator=2.6.8]

还是出现了上次出现过的异常。在网上搜索后，发现可能是rsync 2.6.8的bug，因传输的文件太多太大，此时需要升级rsync版本。

查看当前rsync版本

	[root@any rmanbak]# rpm -qa | grep rsync
	rsync-2.6.8-3.1
	
	[root@any rmanbak]# mount 192.168.1.100:/media/centos/5.7_64/1 /media/
	[root@any rmanbak]# rpm -ihv /media/CentOS/rsync-3.0.6-4.el5.x86_64.rpm
	warning: /media/CentOS/rsync-3.0.6-4.el5.x86_64.rpm: Header V3 DSA signature: NOKEY, key ID e8562897
	Preparing... ########################################### [100%]
	1:rsync ########################################### [100%]

###升级版本之后，仍有错误信息。

rsync客户端日志

	rsync: writefd_unbuffered failed to write 4 bytes to socket [sender]: Connection reset by peer (104)
	inflate (token) returned -5
	rsync error: error in rsync protocol data stream (code 12) at token.c(604) [receiver=3.0.6]
	rsync: connection unexpectedly closed (229747 bytes received so far) [sender]
	rsync error: error in rsync protocol data stream (code 12) at io.c(600) [sender=3.0.6]

rsync服务端日志

	2012/09/25 17:29:21 [9026] connect from any.egolife.com (192.168.0.30)
	2012/09/25 17:29:21 [9026] rsync to anybak from oracle@any.egolife.com (192.168.0.30)
	2012/09/25 17:29:21 [9026] receiving file list
	2012/09/25 17:29:21 [9026] rmanbak/
	2012/09/25 17:29:44 [9026] inflate (token) returned -5
	2012/09/25 17:29:44 [9026] rsync error: error in rsync protocol data stream (code 12) at token.c(604) [receiver=3.0.6]
	2012/09/25 17:29:44 [9026] rsync: connection unexpectedly closed (2017 bytes received so far) [generator]
	2012/09/25 17:29:44 [9026] rsync error: error in rsync protocol data stream (code 12) at io.c(600) [generator=3.0.6]

升级之后，可能还是传输的文件太大所导致的。

根据网上搜索到的文档，在rsync命令中加入参数 --no-iconv，重启后，客户端日志信息如下：

	sending incremental file list
	rmanbak/
	rmanbak/20120917_inc0_tfnlf4ok_1_1.bkp
	1052737280 22% 47.78MB/s 0:01:13
	rsync: writefd_unbuffered failed to write 4 bytes to socket [sender]: Connection reset by peer (104)
	inflate (token) returned -5
	rsync error: error in rsync protocol data stream (code 12) at token.c(604) [receiver=3.0.6]
	rsync: connection unexpectedly closed (229747 bytes received so far) [sender]
	rsync error: error in rsync protocol data stream (code 12) at io.c(600) [sender=3.0.6]

根据以上提示，文件`20120917_inc0_tfnlf4ok_1_1.bkp`太大，导致传输异常。

查看该文件大小

	[root@any rmanbak]# ll -h 20120917_inc0_tfnlf4ok_1_1.bkp
	-rw-r----- 1 oracle oinstall 4.4G Sep 17 22:21 20120917_inc0_tfnlf4ok_1_1.bkp

这里需要调整rman备份片的大小，限制在500M，实际最大能够正常传输多大的文件尚未明确。 在rman中设置备份集中备份片的大小

	[root@any rmanbak]# su - oracle
	[root@any rmanbak]# rman target/
	RMAN> CONFIGURE CHANNEL DEVICE TYPE DISK MAXPIECESIZE 500M;
	
	new RMAN configuration parameters:
	CONFIGURE CHANNEL DEVICE TYPE DISK MAXPIECESIZE 500 M;
	new RMAN configuration parameters are successfully stored

不过以上设置对手动分配过channel的备份脚本无效，此时可以在分配通道的脚本中手动指定每个备份片的大小

	run{
		sql 'alter system archive log current';
		allocate channel cha_inc0 type disk maxpiecesize=500M;
		backup incremental level 0 format '/apps/rmanbak/data/%T_inc0_%U.bkp' tag 'weekly inc0 backup' database plus archivelog delete input;
		release channel cha_inc0;
	}

将未同步的文件使用scp命令手动同步到备机，再进行小文件(大不予500M)的同步测试。

另外，因主机和备机停机过(2012-09-24 17:08)，而rsync的实时同步备份只有监测的路径下文件结构发生变更时才会触发远程同步，这样累计起来需要同步的文件太多(2012-9-24 正好周一，rman每周一晚十点都会进行数据库全备，因此产生的备份文件很多)，也可能导致rsync同步失败。

主机日志

	[root@any ~]# uptime
	20:37:04 up 1 day, 3:31, 1 user, load average: 0.07, 0.02, 0.11
	
	[root@any ~]# last | more
	root pts/1 dev.egolife.com Tue Sep 25 20:36 still logged in 
	root pts/1 dev.egolife.com Tue Sep 25 12:46 - 20:28 (07:42) 
	root pts/2 dev.egolife.com Tue Sep 25 09:06 - 20:28 (11:22) 
	root pts/1 dev.egolife.com Tue Sep 25 08:55 - 10:04 (01:09) 
	oracle pts/1 :0.0 Tue Sep 25 08:32 - 08:33 (00:00) 
	oracle :0 Tue Sep 25 08:31 - 08:33 (00:01) 
	oracle :0 Tue Sep 25 08:31 - 08:31 (00:00) 
	reboot system boot 2.6.18-194.el5 Mon Sep 24 17:08 (1+03:28)

备机日志

	[root@anybak ~]# date
	Tue Sep 25 20:30:25 CST 2012
	[root@anybak ~]# uptime
	20:30:27 up 1 day, 3:32, 1 user, load average: 0.00, 0.00, 0.00
	[root@anybak ~]# last | more
	root pts/1 dev.egolife.com Tue Sep 25 20:30 still logged in 
	root pts/2 192.128.1.100 Tue Sep 25 16:58 - 19:58 (03:00) 
	root pts/1 192.138.1.100 Tue Sep 25 12:37 - 20:21 (07:43) 
	reboot system boot 2.6.18-194.el5 Mon Sep 24 16:59 (1+03:31)

##脚本

rsync.sh

	#!/bin/sh
	#abstract:
	#rsync auto sync script
	#2012-06-11 dylanninin@gmail.com first_release
	#variables
	current_date=$(date +%Y%m%d_%H%M%S)
	rman_path=/apps/rmanbak
	log_file=/var/log/rsync.log

	#rsync
	rsync_server=192.168.0.29
	rsync_user=oracle
	rsync_pwd=/etc/rsync_client.pwd
	rsync_module=anybak
	#rsync_client password check
	if [ ! -e ${rsync_pwd} ]; then
		echo "rsync client password file does not exist!"
		exit 0
	fi

	#inotify function
	inotify_fun(){
		/usr/bin/inotifywait -mrq --timefmt '%d/%m/%y-%H:%M' --format '%T%w%f' \
	-e modify,delete,create,move ${rman_path} | while read file
		do
			/usr/bin/rsync -vrtzopg --progress --delete --password-file=${rsync_pwd} ${rman_path} ${rsync_user}@${rsync_server}::${rsync_module}
		done
	}

	#inotify
	inotify_fun >> ${log_file} 2<&1 &

##延伸阅读

* [抚琴煮酒：CentIOS 5.5 下rsync使用技巧和权限问题解读](http://os.51cto.com/art/201101/243374.htm)
* [陈皓：rsync的核心算法](http://coolshell.cn/articles/7425.html)

##参考文档

* [suchalin：在redhat5.4/5.5/5.6中默认的rsync出现的bug](http://suchalin.blog.163.com/blog/static/553046772011917112312684/)

---
layout: post
title: The Invalid Object of Oracle Database
category : Oracle
tags : [Oracle, Database, DBA, Exception]
---

##异常症状

在Oracle EBS系统中可以正常打开物料编辑页面，输入料号、描述等，选择物料模板，点击 保存按钮保存；此时切换到组织属性或组织分配页，再切换回物料属性页或者按料号查找该 物料，均无法找到该物料的信息；直接在数据库中查找该物料为空。

##异常确认

登录到Oracle ERP系统，输入物料确实无法保存；最大化编辑页面，Oracle Forms左下角没 有保存数据的事务操作提示。

##环境

* Oracle RDBMS : 11.1.0.7.0
* Oracle Applications : 12.1.1

##异常解决

在ERP服务器上部署了一些定时任务，其中每天结束前（23:55）会定时检查数据库系统的警 告日志，若有错误，则会发送邮件通知。

2012-10-29 收到来自此ERP系统的警告日志邮件通知。

###警告日志

alert_PROD.log.2012-10-28

	Fri Oct 26 14:24:24 2012
	Errors in file /u1/PROD/prodora/db/tech_st/11.1.0/admin/PROD_poprod/diag/rdbms/prod/
	PROD/trace/PROD_ora_14573.trc  (incident=22169):
	ORA-00600: internal error code, arguments: [qcsgpvc3], [], [], [], [], [], [], [], [], [], [], []
	Incident details in: /u1/PROD/prodora/db/tech_st/11.1.0/admin/PROD_poprod/diag/rdbms/
	prod/PROD/incident/incdir_22169/PROD_ora_14573_i22169.trc
	Non critical error ORA-48913 caught while writing to trace file "/u1/PROD/prodora/db/tech_st/11.1.0/admin/PROD_poprod/diag/rdbms/
	prod/PROD/incident/incdir_22169/PROD_ora_14573_i22169.trc"
	Error message: ORA-48913: Writing into trace file failed, file size limit [10485760] reached
	Writing to the above trace file is disabled for now on...

###跟踪日志

`PROD_ora_14573_i22169.trc`

	... ...
	*** 2012-10-26 14:24:24.811
	*** SESSION ID:(307.48840) 2012-10-26 14:24:24.811
	*** CLIENT ID:() 2012-10-26 14:24:24.811
	*** SERVICE NAME:(PROD) 2012-10-26 14:24:24.811
	*** MODULE NAME:(PL/SQL Developer) 2012-10-26 14:24:24.811
	*** ACTION NAME:(Main session) 2012-10-26 14:24:24.811

	Dump continued from file: /u1/PROD/prodora/db/tech_st/11.1.0/admin/PROD_poprod/diag/rdbms/PROD_ora_14573.trc

`PROD_ora_14573.trc`
	
	ORA-00600: internal error code, arguments: [qcsgpvc3], [], [], [], [], [], [], [], [], [], [], []
	========= Dump for incident 22169 (ORA 600 [qcsgpvc3]) ========
	----- Beginning of Customized Incident Dump(s) -----
	QCSGPVC3: icodefs yet to be processed = 1
	QCSGPVC3: ico = 0x7fa94803cad0
	QCSGPVC3: ico->icocop = 0x7fa9478e3ab8
	QCSGPVC3: ico->icocop->colkcc = (nil)
	QCDDMP: -------------------------------------------------------
	QCDDMP:  qcsgpvc3_CTX: [0x7fa94801ee10]
	QCDDMP:  {
	QCDDMP:    ->ctxqbc: [0x7fa94801d068]
	QCDDMP:    {
	QCDDMP:      ->qbcqtxt: N/A
	QCDDMP:      ->qbcfro: [0x7fa94801d3f0]
	QCDDMP:      {
	QCDDMP:        ->frooid: [(nil)]
	QCDDMP:        ->frotni: MTL_SYSTEM_ITEMS_INTERFACE
	QCDDMP:        ->froaid: MTL_SYSTEM_ITEMS_INTERFACE
	QCDDMP:        ->frotyp = 2
	QCDDMP:        ->froflg = 0x4b
	... ...

其中MTL_SYSTEM_ITEMS_INTERFACE表为物料接口表，用于临时保存物料信息。无法保存物 料，可能和此表异常相关。

在MOS上找到关于ORA-600[qcsgpvc3]的一篇文档，有可能是存储过程依赖的表结构发生了变 更，重新编译存储过程时会出现此错误。

根据此文档，在R12的测试环境上进行测试，过程如下：

	SQL> create table xx_qcsgpvc( c1 varchar2(60),c2 number);
	Table created.
	SQL> create or replace package xx_qcsgpvc_p is
	  2  procedure po(c1 varchar2, c2 number);
	  3  end xx_qcsgpvc_p;
	  4  /
	Package created.
	
	SQL> create or replace package body xx_qcsgpvc_p is
	  2  procedure po(c1 varchar2,c2 number)
	  3  is
	  4  begin
	  5  insert into xx_qcsgpvc(c1,c2) values (c1,c2);
	  6  end po;
	  7  end xx_qcsgpvc_p;
	  8  /
	Package body created.
	
	SQL> alter table xx_qcsgpvc rename column c2 to i_problem;
	Table altered.
	
	SQL> alter package xx_qcsgpvc_p compile body;
	Warning: Package Body altered with compilation errors.

###警告日志

alert_VIS.ora

	Mon Oct 29 09:15:01 2012
	Errors in file
	/u1/VIS/visora/db/tech_st/11.1.0/admin/VIS_demoerp/diag/rdbms/
	vis/VIS/trace/VIS_ora_925.trc  (incident=24961):
	ORA-00600: internal error code, arguments: [qcsgpvc3], [], [], [], [], [], [], [], [], [], [], []
	Incident details in:
	/u1/VIS/visora/db/tech_st/11.1.0/admin/VIS_demoerp/diag/rdbms/vis/
	VIS/incident/incdir_24961/VIS_ora_925_i24961.trc

###跟踪日志

`VIS_ora_925_i24961.trc`

	... ...
	*** 2012-10-29 09:15:01.536
	*** SESSION ID:(278.26930) 2012-10-29 09:15:01.536
	*** CLIENT ID:() 2012-10-29 09:15:01.536
	*** SERVICE NAME:(SYS$USERS) 2012-10-29 09:15:01.536
	*** MODULE NAME:(SQL*Plus) 2012-10-29 09:15:01.536
	*** ACTION NAME:() 2012-10-29 09:15:01.536
	
	Dump continued from file:
	/u1/VIS/visora/db/tech_st/11.1.0/admin/VIS_demoerp/diag/rdbms/
	vis/VIS/trace/VIS_ora_925.trc
	ORA-00600: internal error code, arguments: [qcsgpvc3], [], [], [], [], [], [], [], [], [], [], []
	========= Dump for incident 24961 (ORA 600 [qcsgpvc3]) ========
	----- Beginning of Customized Incident Dump(s) -----
	QCSGPVC3: icodefs yet to be processed = 1
	QCSGPVC3: ico = 0x7fc230514c90
	QCSGPVC3: ico->icocop = 0x7fc2305152b8
	QCSGPVC3: ico->icocop->colkcc = (nil)
	QCDDMP: -------------------------------------------------------
	QCDDMP:  qcsgpvc3_CTX: [0x7fc2305106b8]
	QCDDMP:  {
	QCDDMP:    ->ctxqbc: [0x7fc230514438]
	QCDDMP:    {
	QCDDMP:      ->qbcqtxt: N/A
	QCDDMP:      ->qbcfro: [0x7fc2305147c0]
	QCDDMP:      {
	QCDDMP:        ->frooid: [(nil)]
	QCDDMP:        ->frotni: XX_QCSGPVC
	QCDDMP:        ->froaid: XX_QCSGPVC
	QCDDMP:        ->frotyp = 2
	QCDDMP:        ->froflg = 0x43
	... ...

根据以上日志，可能是系统对象做过更改，导致依赖于这些对象的存储过程失效而无法正常 运行。

查看物料相关表创建编译时间

	SELECT owner,
	       object_name,
	       object_type,
	       created,
	       last_ddl_time,
	       TIMESTAMP,
	       status
	  FROM dba_objects do
	  WHERE do.object_name IN ('MTL_SYSTEM_ITEMS_INTERFACE', 'MTL_SYSTEM_ITEMS_B');

物料表和物料接口表近期均未更改和重编译。

###紧接着查找系统无效对象

无效对象

	SELECT owner,
	       object_name,
	       object_type,
	       status
	  FROM dba_objects
	 WHERE status = 'INVALID'
	 ORDER BY owner,
	          object_type,
	          object_name;
	output:
	APPS        OE_ITEMS_MV         MATERIALIZED VIEW       INVALID
	APPS        XX_IMPORT_MATERIEL_PCK   PACKAGE BODY        INVALID
	APPS        XX_ITEM_DEFAULT_SUBINVENTORY      TRIGGER INVALID

无效对象和依赖关系

	SELECT object_name,
	       object_type,
	       referenced_owner,
	       referenced_type,
	       referenced_name
	  FROM user_objects,
	       user_dependencies
	 WHERE object_name = NAME
	   AND status != 'VALID'
	 ORDER BY object_name,
	          object_type,
	          referenced_owner,
	          referenced_type,
	          referenced_name;

生成无效对象的编译脚本

	SELECT decode(object_type,
	              'PACKAGE BODY',
	              'alter package ' || owner || '.' || object_name ||
	              ' compile body;',
	              'alter ' || object_type || ' ' || owner || '.' || object_name ||
	              ' compile;')
	  FROM dba_objects
	 WHERE status = 'INVALID'
	   AND object_type IN ('PACKAGE BODY',
	                       'PACKAGE',
	                       'FUNCTION',
	                       'PROCEDURE',
	                       'TRIGGER',
	                       'VIEW')
	 ORDER BY object_type,
	          object_name;

编译后，还有无效触发器`APPS.XX_ITEM_DEFAULT_SUBINVENTORY`。

再次尝试新建物料时，依然无法保存。

通过与客制化开发人员沟通，了解到最近在尝试开发物料导入功能，以减轻物料录入的负 担。此触发器在11i中使用过，但在R12中不再使用，这里暂且drop掉该触发器或者禁用该触 发器：

	DROP TRIGGER apps.XX_ITEM_DEFAULT_SUBINVENTORY;

再次尝试新建物料时，可以正常保存。

根据ORA-600[qcsgpvc3]的错误提示以及近期有客制化功能开发可以总结，因对象结构发生 变更使得系统中存在一些无效对象从而导致物料无法保存。在本例中，引起物料无法保存的 应是依赖于物料基础表MTL_SYSTEMS_ITEM_B的触发器XX_ITEM_DEFAULT_SUBINVENTORY无效， 即当对物料基础表进行插入操作时该触发器无法运行，导致整个操作失败。在客制化开发过 程中，涉及到更改标准功能或者系统标准对象等，因Oracle系统的庞大和复杂性，无法确定 做出更改后所有功能都可以正常使用；但对数据库系统的日常维护来说，定期检查和编译无 效对象是必须的，否则某些功能将不能正常运行。

##脚本

检查数据库系统警告日志，结合crontab，若有ora error message，则发送邮件通知

checkalert.sh 

	#!/bin/sh
	#abstract:
	#oracle database daily alert check and send mail notifications if ora error messages occur
	#history:
	#2012-08-14     dylanninin@gmail.com         first release
	#variables
	script_basepath=/u1/PROD/prodora/itsection/adm/sql
	mail_date=$(date +%Y-%m-%d\ %H:%M:%S)
	receipt=dylanninin@gmail.com
	hostname=$(hostname)
	
	#path
	ORACLE_HOME=/u1/PROD/prodora/db/tech_st/11.1.0
	export ORACLE_HOME
	PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:
	$ORACLE_HOME/bin
	export PATH
	ORACLE_SID=PROD
	export ORACLE_SID
	
	#sql log file
	script_name=alert
	sqllog="${script_basepath}/${script_name}.log"
	
	#check alert log
	if [ "${script_name}" == "alert" ]; then
	   alert_basepath=/u1/PROD/prodora/db/tech_st/11.1.0/admin/PROD_poprod/diag/rdbms/prod/PROD/trace
	   alert_log=${alert_basepath}/alert_$ORACLE_SID.log
	   if [ -f ${alert_log} ]; then
	        current_date=$(date +%F)
	        current_log=${alert_log}.${current_date}
	        mv  ${alert_log} ${current_log}
	        touch ${alert_log}
	        grep -i ORA- ${current_log} > ${sqllog}
	   else
	        echo -e "Oracle Instance alert log file ${alert_log} does not exist!"
	        touch ${alert_log}
	   fi
	fi
	
	#mail
	if [ ! -e ${sqllog} ]; then
	   echo -e "sqllog file ${sqllog} does not exist!"
	   exit 0
	fi
	if [ `cat ${sqllog} | wc -l` -gt 0 ]; then
	   cat ${sqllog} | mail -s "${mail_date}:${hostname} dba daily check of ${script_name}" ${receipt}
	fi
	
##参考

* Bug 7172752 - OERI[qcsgpvc3] recompiling a package body [ID 7172752.8]
* Invalid Objects In Oracle Applications FAQ [ID 104457.1]

##写在前面的话

继9月30号作好[本站创建和维护计划](/blog/2012/09/30/blog_schedule.html)后，经过这两天的折腾，总算完成了一个博客系统的雏形，而且是一个十分朴素的雏形。

在这两天，主要熟悉了MTOS的博客管理，自定义了几个模版或小工具，添加了几个插件，定义了页首、页尾的的Contact和About信息，添加了HTTP请求常见的几个错误跳转页面，最后确定了博客当前的布局和功能，也就是现在看到的这个样子。若没有异常情况，本站很长一段时间内将保持当前布局和功能；这段时间以后，我会将更多的精力放在如何提高博客质量上。

现在稍稍总结下这两天的工作，也记下生活流水，以作备忘。

##博客管理

MTOS的博客管理功能确实是十分强大，博客、模版、小工具等均有版本记录，在编辑的过程中，若异常关闭浏览器，再次打开时还可以进行恢复。在折腾过程中，不知道是我心太急，还是带宽有限，每一次编辑、发布就感觉系统的响应越来越慢，本以为十一假期大家都会在高速公路上拥堵不堪，没想到互联网也是如此。无奈之下，Chrome都Oops不响应了，只好关闭重启，好在MTOS有自动恢复功能，这样我就减少了许多体力劳动，不然还真会像某位急躁哥从车窗探出身子非常淡定地凝望这十分拥堵的traffic line，而且一凝望就是两小时。

##模版和小工具

###1. 相关文章

在博客系统中，相关文章是一个十分重要的功能，这些文章或在分类上相同，或在标签上相同，使分散的文章在一定层次上进行了聚合，更具有知识性和传播性。当用户阅读完一篇博客时，若饶有兴致，就可以根据列出的相关文章进行衍生阅读，很方便。

在网上搜索时，找到一篇关于不使用插件列出具有相同标签文章的博客，只需添加几行代码到文章模版中即可。

本站采用的方式是新建一个模版`tag_related_entries`，代码如下：
	
	<mt:EntryIfTagged>
	<mt:SetVarBlock name="curentry"><mt:EntryID /></mt:SetVarBlock>
	<mt:SetVarBlock name="relatedtags"><mt:EntryTags glue=" OR "><mt:TagName /></mt:EntryTags></mt:SetVarBlock>
	<mt:SetVarBlock name="listitems"><mt:Entries tags="$relatedtags" unique="1" lastn="10"><mt:SetVarBlock name="listentry"><mt:EntryID /></mt:SetVarBlock><mt:Unless name="listentry" eq="$curentry"><li><a href="<mt:EntryPermaLink />"><mt:EntryTitle /></a></li></mt:Unless></mt:Entries></mt:SetVarBlock>
	<mt:If name="listitems">
	<div class="my_related_entries">
	<h4>Related Entries<span class="delimiter">:</span></h4>
	<ul>
	<mt:Var name="listitems">
	</ul>
	</div>
	</mt:If>
	</mt:EntryIfTagged>

保存后，在模版Entry中需要添加相关文章的位置导入该模版即可。

###2. 文章信息

看到互联网上很多博客在文章末尾会注上文章的版权信息，永久链接等，MTOS默认没有生成这些信息，所以利用MTOS的标签自定义了一个模版entry_info，代码如下：

	<div class="my_entry_info">
	<h4>Entry Info<span class="delimiter">:</span></h4>
	<ul type="circle">
	<li>Name<span class="delimiter">:</span><a href="<$mt:EntryPermalink$>"><$mt:EntryTitle$></a></li>
	<li>Link<span class="delimiter">:</span><a href="<$mt:EntryPermalink$>"><$mt:EntryPermalink$></a></li>
	<li>Author<span class="delimiter">:</span>By <$mt:EntryAuthorLink show_hcard="1"$> on <$mt:EntryDate format="%x %X"$></li>
	</ul>
	</div>

在Entry模版的文章末尾处，导入该模版即可。

###3. 友情链接

为了让大家知道我经常阅读的一些博客，自定义了一个小工具friend_links，根据博客的布局样式，在合适的位置拖入这个小工具即可。本站使用的是三栏式结构，友情链接添加在副侧边栏的最后。

###4. 分享和订阅

为了方便分享和订阅本站，对比了一些常见的社会化分享工具，如[addthis](http://addthis.org.cn/)，[jiathis](http://www.jiathis.com/)，[bShare](http://www.bshare.cn/)，在分享服务、自定义、数据统计、性能等方面各有长短，并未做十分仔细的对比和分析，因addthis在社会化分享的同时，还提供了订阅按钮，为了保证本站页面的一致性，最终选择addthis。

这里仍然采用创建模版或小工具的方式来添加自定义的分享和订阅代码，同样在Entry模版合适的位置导入该模版或插件即可，最终效果见本站。

###5. 社会化评论

MTOS自动的评论系统因带有防Spam功能，响应比较慢，不是很友好。同样，为了提高响应速度和友好性，对比了一些常见的社会化评论系统，如[disqus](http://disqus.com/),[多说](http://duoshuo.com/)等。这些评论系统均有常用的评论功能，不同的是还支持微博等多账户登录；评论内容由第三方平台托管，支持邮件通知、评论统计、评论的导入导出等。对于评论负荷较重的主机，可以采用社会化评论系统取代自带的评论功能。多说比较适合国内的博客，在国外可能disqus使用得更为广泛，因本站VPS在国外，以及并没有将MTOS中文化的计划，所以本站采用disqus。

disqus安装比较简单，注册后，根据向导设置网站，拷贝生成的评论代码，并创建模版，同样在Entry模版合适的位置导入该模版即可，最终效果见本站。

##插件

MTOS自带的Web编辑器不太好用，尤其是添加图片和代码片段时。在官方网站中，找到了一个所见即所得的Web编辑器CKEditor，进行配置后，只适用于Excerpt的编辑，而在Body编辑时，没有CKeditor的踪影，不知是版本不正确，还是设置出了纰漏，目前尚未处理。

粘贴代码片段时，一款代码格式化和高亮插件，可以让代码更易读，博客也更整洁干净，目前尚未处理。

##常用页面

###1. Contact

[联系页面](http://www.dylanninin.com/blog/pages/message.html)，大家若有任何意见和建议，可以在这里留言，或者发邮件。

###2. About

[关于本站](http://www.dylanninin.com/blog/pages/about.html)，提供站点、站长的基本信息，有助于理解和交流。

###3. 404

[Not Found](http://www.dylanninin.com/foo/bar.html)，当访问不存在的资源时，会提示资源找不到。

###4. 403

[Forbidden](http://www.dylanninin.com/blog/errors)，当访问链接存在但由于某些原因服务器拒绝请求时，会提示禁止访问。

###5. 500

当然，除了404,403页面，还有一个500页面，但用户正常访问时，最好不要出现此页面。

##参考

* [MT之旅](http://www.ezloo.com/mt/manual/entry_tag_entries.html)
##写在前面的话

root是Unix/Linux的超级用户，拥有绝对的权力。拥有root权限，你尽可以随心所欲而不逾矩：你既是"矩"的制定者，也是"矩"的拥护者，当然稍有不慎，你也可能是"矩"的破坏者、毁灭者，因为若被滥用，则会引发异常崩溃，甚至导致绝对的腐败。

所以很多Unix/Linux教程都花了很大篇幅讲解系统权限，并强烈建议慎用root，而代之以普通用户，如anybody。这样当anybody试图做某些逾矩的操作时，会提示Permission Denied并无法执行，此时root制定的规则已经生效，并得到遵守，不管你是情愿，还是被迫。当然，若有需要，anybody也可以切换为root身份，前提是需要知道root的口令。

根据以上信息，我们至少可以从三个方面增强root安全。

###1. 高强度口令

为root用户设置一个高强度的口令，大小写字母 + 数字 + 特殊字符，而且还要保证有足够的位数。

去年国内几大网站相继出现[明文口令泄露事件](http://coolshell.cn/articles/6193.html)，有很多可供引以为戒。但我相信，很多用户依然还没有引起重视，不信可以去测试下你的密码强度：[How Security Is My Password](http://howsecureismypassword.net/)

###2. 创建普通用户

使用root登录，新增用户anybody，并为anybody设置一定强度的密码。

	useradd anybody
	passwd anybody

注：创建修改用户需要root权限。

###3. 限制su/sudo

在Unix/Linux系统中，要获取其他用户的权限有两种方式，一是使用su(switch user)，切换成为某用户，这样就拥有了该用户的所有权限，直到退出该用户；另外一种是sudo(do as another user)，以另外一个用户的身份执行某些操作，此时在短时间内你获取该用户的权限，直到超时。

当然，su/sudo的细节可以由root进行控制。

####限制只有wheel组的用户可以使用su

编辑/etc/pam.d/su文件，启用用户组认证：

	# Uncomment the following line to require a user to be in the "wheel" group.
	
	auth            required        pam_wheel.so use_uid

将普通用户anybody添加到wheel组，wheel的gid为10

	usermod -G 10 anybody

这样就只有该用户和root可以使用su。

####限制普通用户使用sudo

sudo主要配置在/etc/sudoers，新增加的用户默认没有添加到sudoers中，若有需要，可以参照配置中说明更改。

比较通用的做法是定制一些可以执行或禁止执行的命令集，而允许执行的命令集足以完成系统的日常维护工作，禁止执行的命令集可能会使系统暴露不够安全，然后将这些命令集授权给该用户，这样既分配适当权限，又作特殊限制。

当然，root涉及到系统安全远不止这些，以上仅是本站使用的一些策略，更详细的信息有待进一步理解和实践。

##延伸阅读

* [Linux系统安全(一):安装与设置](http://www.ibm.com/developerworks/cn/linux/security/l-ossec/part1/)
* [权限安全使用和密码管理](http://www.ibm.com/developerworks/cn/linux/l-cn-rootadmin2/index.html)
* [了解和配置PAM](http://www.ibm.com/developerworks/cn/linux/l-pam/)
* [充分发挥sudo的作用](http://www.ibm.com/developerworks/cn/aix/library/au-sudo/)
##写在前面的话

SSH (Secure Shell)可以说是每一台Unix/Linux主机的必备软件和服务，既可以用于远程主机登录，也可以直接在远程主机上执行操作。

对于系统管理员来说，SSH就更显重要了。因为这意味着，当要进行服务器维护工作时，你可以通过SSH在办公区直接管理服务器，而不必跑到机房，或者偏远的IDC。正如本站一样，建站时即使用Xshell客户端远程登录到刚购买的Linode VPS，进行博客环境的搭建以及主机基础安全防护工作；而无需翻山越岭跨不远万里地跑到Linode的某个数据中心。

SSH确实为我们开启了一扇方便之门，但使用不慎也会带来一些安全隐患。当你的主机暴露在互联网上而不加任何防护，那么它很有可能就是某个好事者的下一道小菜。在开启SSH服务时，我们可以适当更改某些设置，提高SSH服务的安全性，降低被攻击的可能性。

###1. 使用公钥私钥

使用公钥私钥对进行连接，而不需要输入口令，这样可以减少暴力破解的可能性。前提是需要将私钥保护好。

####客户端

使用`ssh-keygen`命令，产生密钥对，并将公钥上传至服务端。

也可以使用shell客户端自带的Key管理工具来生成，本站即使用Xshell生成2048位的RSA密钥对，并将公钥id_rsa.pub上传到服务端。

####服务端

在用户anybody下创建`.ssh`文件夹，并拷贝`id_rsa.pub`到`~/.ssh/authorized_keys`

	cd  /home/anybody
	mkdir .ssh
	cat id_rsa.pub >> .ssh/authorized_keys

	chown -R anybody:anybody .ssh

	chmod 700 .ssh
	chmod 600 .ssh/authorized_keys

注：`~/.ssh/authorized_keys`是ssh的默认配置，详情见`/etc/ssh/sshd_config`

###2. 修改默认端口

编号为1024以下的端口都是一些周知端口，SSH默认的端口号为22，更改SSH端口为非常规端口，可以在一定程度上减少端口扫描的信息量，如改为5555，攻击者不一定能够很快猜测到这个端口就是SSH服务，具有一定的不确定性。

更改配置 `/etc/ssh/sshd_config`：

	Port 5555

###3. 禁止root直接登录

root用户权限太大，直接使用root登录进行操作，稍有不慎就可能会引发意想不到的问题，详见[root用户介绍和基础安全防护](http://www.dylanninin.com/blog/2012/10/root-info-and-basic-security.html)。在SSH中禁用root登录：

	PermitRootLogin no

###4. 取消密码认证

前面已经通过ssh-key或SSH客户端工具生成了密钥对，就已经可以使用密钥进行登录认证了；如果还留有密码认证，这显然会是好事者喜欢的盲点。在SSH中取消密码认证：

	PasswordAuthentication no

###5. 登录提醒

在用户成功登录后，sshd会进行一些的动作，其中可能会执行~/.ssh/rc和/etc/ssh/sshrc脚本。利用这个特性，可以制定sshrc脚本，当有用户SSH远程登录时，发送邮件提醒，利于尽早发现异常。

	echo "http://whatismyipaddress.com/ip/${SSH_CLIENT%% *}" | mail -s "$USER login from ${SSH_CLIENT%% *}" dylanninin@139.com

以上是本站使用SSH的一些基础安全防护策略。SSH的设计和功能很好很强大，Xshell也是一款十分值得推荐的客户端，更多信息，可以参考延伸阅读。

##延伸阅读

* [SSH安全性和配置入门](http://www.ibm.com/developerworks/cn/aix/library/au-sshsecurity/index.html)
* [SSH原理与运用(一)](http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html)
* [SSH原理与运用(二)](http://www.ruanyifeng.com/blog/2011/12/ssh_port_forwarding.html)
* [与你潇洒人生路：SSH配置](http://www.cnblogs.com/shuaixf/archive/2012/05/25/2517947.html)
* [A挨个搞： Xshell十大技巧](http://actgod.com/archives/86/)
---
layout: post
title: ORA-00257
category : Oracle
tags : [Oracle, Database, DBA, Exception]
---

##异常症状

使用PL/SQL Developer连接Oracle数据库时，出现ORA-00257错误

	ORA-00257: archiver error. Connect internal only, until freed.

##Error Message

ORA-00257: archiver error. Connect internal only, until freed.

Cause:  The archiver process received an error while trying to archive a redo log. If the problem is not resolved soon, the database will stop executing transactions.The most likely cause of this message is the destination device is out of space to store the redo log file.

Action:  Check archiver trace file for a detailed description of the problem. Also verify that the device specified in the initialization parameter ARCHIVE_LOG_DEST is set up properly for archiving.

ORA-00257，即存储重做日志的文件已经超出目标设备限制的大小

##异常确认

登录到服务器主机，操作如下：

检查启动参数

	SQL> show parameter spfile;
	NAME         TYPE       VALUE
	----------- ----------- --------------------------------------------------
	spfile      string     /db/oracle/product/10.2.0/db_1/dbs/spfiledbtest.ora

查看dest_size参数

	SQL> show parameter dest_size;
	NAME                        TYPE       VALUE
	-------------------------- ----------- ----------------------------
	
	db_recovery_file_dest_size big integer 2G

查看dest路径

	SQL> show parameter log_archive_dest;
	NAME                              TYPE       VALUE
	-------------------------------- ----------- ------------------------------
	log_archive_dest                 string
	log_archive_dest_1               string
	log_archive_dest_10              string
	log_archive_dest_2               string
	log_archive_dest_3               string
	log_archive_dest_4               string
	log_archive_dest_5               string
	log_archive_dest_6               string
	log_archive_dest_7               string
	log_archive_dest_8               string
	log_archive_dest_9               string
	
	NAME                            TYPE       VALUE
	------------------------------- ---------- ------------------------------
	log_archive_dest_state_1        string     enable
	log_archive_dest_state_10       string     enable
	log_archive_dest_state_2        string     enable
	log_archive_dest_state_3        string     enable
	log_archive_dest_state_4        string     enable
	log_archive_dest_state_5        string     enable
	log_archive_dest_state_6        string     enable
	log_archive_dest_state_7        string     enable
	log_archive_dest_state_8        string     enable
	log_archive_dest_state_9        string     enable

若dest为空，表明log_archive使用的是默认值，此时则可以使用archive log list查看归 档路径和序列，如下

	SQL> archive log list;
	Database log mode        		Archive Mode
	Automatic archival         		Enabled
	Archive destination        		USE_DB_RECOVERY_FILE_DEST
	Oldest online log sequence    	88
	Next log sequence to archive   	88
	Current log sequence            90

4）查询恢复目录参数

	SQL> show parameter recover;
	NAME                              TYPE        VALUE
	--------------------------------- ----------- ------------------------------
	db_recovery_file_dest             string     /db/oracle/flash_recovery_area
	db_recovery_file_dest_size        big integer 2G
	recovery_parallelism              integer     0

5）查看恢复目录文件大小

	SQL> host du -h --max-depth=1 /db/oracle/flash_recovery_area/DBTEST;
	4.0K          /db/oracle/flash_recovery_area/DBTEST/onlinelog
	2.0G          /db/oracle/flash_recovery_area/DBTEST/archivelog
	2.0G          /db/oracle/flash_recovery_area/DBTEST

6）查看恢复目录使用率

	SQL> select * from v$flash_recovery_area_usage;
	FILE_TYPE    PERCENT_SPACE_USED PERCENT_SPACE_RECLAIMABLE NUMBER_OF_FILES
	------------ ------------------ ------------------------- ---------------
	CONTROLFILE    				  0                         0               0
	ONLINELOG                     0                         0               0
	ARCHIVELOG                97.79                         0              44
	BACKUPPIECE                   0                         0               0
	IMAGECOPY                     0                         0               0
	FLASHBACKLOG                  0                         0               0
	6 rows selected.

flash_recovery的使用率达到97.79%，再有新的事务时，很可能会超出2G大小，故Oracle会 提示ORA-00257的错误。

##解决方案

出现ORA-00257错误，解决办法有两个，一是调整db_recovery_file_dest_size的大小；二 是移除或者转移一些归档日志，释放空间。    

###解决方案1

调整db_recovery_file_dest_size的大小

	SQL> alter system set db_recovery_file_dest_size=3G scope=both;
	
	System altered.

此时再尝试则可以正常连接；

###解决方案2

移除或者转移归档日志，释放空间

删除过期的归档日志：

	find  /db/oracle/flash_recovery_area/DBTEST/archivelog -name "2012-*" -mtime +14 -exec rm {} \;
	
使用rman同步控制文件，释放空间

检查一些无用的archivelog

	RMAN> crosscheck archivelog all;

删除过期的归档

	RMAN> delete expired archivelog all;

注：在OS级别删除归档日志后，还需要使用rman检查、删除，否则即使删除了归档日志，但 Oracle数据库记录的使用空间依然没有释放。

另外，也可以直接使用rman删除归档日志

	RMAN> delete noprompt archivelog all completed before 'sysdate - 14';

##参数设置

Oracle数据库中归档日志对于数据库的恢复十分重要，以下仅列出跟此错误相关的几个参数， 其中`LOG_ARCHIVE_DEST_N/LOG_ARCHIVE_DEST_STATE_N`多涉及到重做日志的多路复用、 Data Guard等，这里暂不列出。

###1.LOG_ARCHIVE_DEST

Parameter type: String

Syntax: `LOG_ARCHIVE_DEST = filespec`

Default value: Null

Modifiable: `ALTER SYSTEM`

Range of values: Any valid path or device name, except raw partitions

Basic: No

Real Application Cluster: Multiple instances can have different values.

`LOG_ARCHIVE_DEST` is applicable only if you are running the database in ARCHIVELOG mode or are recovering a database from archived redo logs.  `LOG_ARCHIVE_DEST` is incompatible with the `LOG_ARCHIVE_DEST_n` parameters, a nd must be defined as the null string ("") or (' ') when any `LOG_ARCHIVE_DEST_n` parameter has a value other than a null string. Use a text string to specify the default location and root of the disk file or tape device when archiving redo log files. (Archiving to tape is not supported on all operating systems.) The value cannot be a raw partition.

If `LOG_ARCHIVE_DEST` is not explicitly defined and all the `LOG_ARCHIVE_DEST_n` parameters have null string values, `LOG_ARCHIVE_DEST` is set to an operating system-specific default value on instance startup.

To override the destination that this parameter specifies, either specify a different destination for manual archiving or use the `SQL*Plus` statement `ARCHIVE LOG START` filespec for automatic archiving, where filespec is the new archive destination. To permanently change the destination, use the statement `ALTER SYSTEM SET LOG_ARCHIVE_DEST = filespec`, where filespec is the new archive destination.

Neither `LOG_ARCHIVE_DEST` nor `LOG_ARCHIVE_FORMAT` have to be complete file or directory specifiers themselves; they only need to form a valid file path after the variables are substituted into `LOG_ARCHIVE_FORMAT` and the two parameters are concatenated together.

###2.DB_RECOVERY_FILE_DEST_SIZE

Parameter type: Big integer

Syntax: `DB_RECOVERY_FILE_DEST_SIZE = integer [K | M | G]`

Default value: There is no default value.

Modifiable: `ALTER SYSTEM ... SID='*'`

Basic: Yes

Real Application Clusters: You must set this parameter for every instance, and multiple instances must have the same value 

`DB_RECOVERY_FILE_DEST_SIZE` specifies (in bytes) the hard limit on the total space to be used by target database recovery files created in the flash recovery area.

###3.DB_RECOVERY_FILE_DEST

Parameter type: String

Syntax: `DB_RECOVERY_FILE_DEST = directory | disk group`

Default value: There is no default value.

Modifiable: `ALTER SYSTEM ... SID='*'`

Basic: Yes

Real Application Clusters: You must set this parameter for every instance, and multiple instances must have the same value.

`DB_RECOVERY_FILE_DEST` specifies the default location for the flash recovery 
area.

The flash recovery area contains multiplexed copies of current control files and online redo logs, as well as archived redo logs, flashback logs, and RMAN backups.

Specifying this parameter without also specifying the `DB_RECOVERY_FILE_DEST_SIZE` parameter is not allowed.

##参考

* Oracle Database Reference
* Oracle Database Error Messages

---
layout: post
title: Kindle Fault
category : Miscellanies
tags : [Kindle, Exception]
---

去年11月份入手了一台6英寸 WiFi版的Kindle，电子水墨屏。用过一段时间，感觉还不错， 唯一不好的体验是在翻页时Kindle会刷屏，后来在国内的多看论坛上看到了针对Kindle开发 的多看应用，主要面向中文用户，做了一些优化，比如预加载，一次性可以加载5至10页， 改善了电纸书翻页的效果，还不错；主要缺憾是，在原生的Kindle系统和多看系统之间切换 比较繁琐。

但好景不长，不到两个月，这款Kindle就无法充电，也无法开机；好在还在保修期内，尽管 是在淘宝上找人代购，但商家还算给力，联系后可以发货给商家，进而退换一款新的。耗时 近一月，又入手一款全新的Kindle。

但无法充电、无法开机的故事还在继续，今年六月份，Kindle再次抽风，在网上找过一些帖 子，尝试过长时间充电、长按电源键，甚至丢到冰箱冷冻等方法，但到现在还没有折腾好。

故在此贴出Kindle症状，希望高人指点一二，若有一线生机，感激涕零。

1) 为Kindle专门买了保护皮套，方便外带，免受刮伤等。

![1_kindle_with_protection](http://dylanninin.com/assets/images/2012/1_kindle_with_protection.jpg)

2) 拆开保护皮套之后的Kindle，因保护不错，没有明显的损坏。

![2_kindle](http://dylanninin.com/assets/images/2012/2_kindle.jpg)

3) 使用试电笔测试，显示有电流，但按电源键，Kindle没有任何反应。

![3_kindle_inductor_test](http://dylanninin.com/assets/images/2012/3_kindle_inductor_test.jpg)

4) 使用电脑的USB接口和Kindle自带的USB线充电，但Kindle的充电指示灯没有反应。

![4_kindle_charge](http://dylanninin.com/assets/images/2012/4_kindle_charge.jpg)
---
layout: post
title: Oracle EBS Rapid Clone
category : Oracle
tags : [Oracle, Database, DBA,  EBS]
---

##克隆环境

* OS: IBM AIX 5.3 64bit
* EBS: 11.5.10.2 64bit
* DB: 9.2.0.6 64bit

ERPProd和ERPTest环境一样，在硬件方面ERPTest配置稍低

##准备工作

关闭克隆目标系统的数据库、应用等。目前ERP环境采用定期克隆，这里还需要删除克隆目标系统上以前的数据

1)关闭测试环境应用、监听、数据库

	--testmgr
	adstpall.sh apps/xxxxxx

	--testora
	addlnctl.sh stop ERP
	addbctl.sh stop immediate

2)删除测试环境数据

	--testora
	rm -rf /u2/TEST/testora/testdata /u2/TEST/testora/testdb &
	--testora
	rm -f /erpdata/redodata/redo*.dbf

	--testmgr
	rm -rf /u2/TEST/testmgr/testappl /u2/TEST/testmgr/testora &

3)删除测试环境上一次克隆时做的备份

	--testmgr
	cd /u2/TEST/testmgr/testcomn
	rm -rf _pages_120422bk &
	rm -rf html_120422bk &
	rm -rf java_120422bk &
	rm -rf clone_120422bk & 
	rm -rf util_120422bk &

4)备份测试环境上一次克隆的数据

	--testmgr
	mv /u2/TEST/testmgr/testcomn/_pages /u2/TEST/testmgr/testcomn/_pages_120527bk
	mv /u2/TEST/testmgr/testcomn/html /u2/TEST/testmgr/testcomn/html_120527bk
	mv /u2/TEST/testmgr/testcomn/java /u2/TEST/testmgr/testcomn/java_120527bk
	mv /u2/TEST/testmgr/testcomn/clone /u2/TEST/testmgr/testcomn/clone_120527bk
	mv /u2/TEST/testmgr/testcomn/util /u2/TEST/testmgr/testcomn/util_120527bk

##2.正式克隆

在执行rapid clone时，会从原系统创建克隆用的模板配置文件，当原系统文件复制到目标 系统时，rapid clone工具将使用目标心痛的配置来更新这些模板配置文件，达到克隆出另 外一套系统的目的。

1)关闭应用、监听、数据库
	
	--prodmgr
	adstpall.sh apps/xxxxxx
	--prodora
	addlnctl.sh stop PROD
	addbctl.sh stop immediate

2)数据库预克隆

	--prodora
	cd /u1/PROD/prodora/proddb/9.2.0/appsutil/scripts/PROD_erpprod
	perl adpreclone.pl dbTier

3)应用预克隆

	--prodmgr
	cd /u1/PROD/prodmgr/prodcomn/admin/scripts/PROD_erpprod
	perl adpreclone.pl appsTier

4)关闭原系统数据

因克隆后，数据库已经开启，再次关闭db

	--prodora
	addbctl.sh stop immediate

5)拷贝原系统数据到目标系统

目前ERP正式、测试环境使用AIX 5.3操作系统，共用磁阵，拷贝数据可以采用挂载文件系统 的方式进行

卸载正式环境上/u1文件系统

	--root@prod
	fuser -u /dev/fslv00
	umount /u1
	varyoffvg prodvg

挂载/u1到测试环境

	--root@test
	varyonvg prodvg
	mount /u1

复制原系统数据到目标系统

	--root@test
	cp -hrf /u1/PROD/prodora/proddb /u2/TEST/testora/testdb &
	cp -hrf /u1/PROD/prodora/proddata /u2/TEST/testora/testdata &
	cp -hrf /u1/PROD/prodmgr/prodappl /u2/TEST/testmgr/testappl &
	cp -hrf /u1/PROD/prodmgr/prodora /u2/TEST/testmgr/testora &
	
	cp -hrf /u1/PROD/prodmgr/prodcomn/_pages /u2/TEST/testmgr/testcomn/_pages &
	cp -hrf /u1/PROD/prodmgr/prodcomn/html /u2/TEST/testmgr/testcomn/html &
	cp -hrf /u1/PROD/prodmgr/prodcomn/java /u2/TEST/testmgr/testcomn/java &
	cp -hrf /u1/PROD/prodmgr/prodcomn/util /u2/TEST/testmgr/testcomn/util &
	cp -hrf /u1/PROD/prodmgr/prodcomn/clone /u2/TEST/testmgr/testcomn/clone &

检查复制数据任务是否完成
	
	--root@test
	jobs 或 ps -ef|grep cp

复制完成后，测试环境上卸载/u1，并将/u1挂载至原系统

	--root@test
	unmount /u1
	varyoffvg prodvg
	
	--root@prod
	varyonvg prodvg
	mount /u1

开启原系统数据库、应用等，使应用正常提供服务

开启DB、监听

	--prodora
	adautocfg.sh

备份jserv.properties文件

	--prodmgr
	cp /u1/PROD/prodmgr/prodora/iAS/Apache/Jserv/etc/jserv.properties /u1/PROD/prodmgr/prodora/iAS/Apache/Jserv/etc/jserv.properties.dist
	adautocfg.sh
	
正式环境加上第三方jar包
开启应用

	--prodmgr
	adstrtal.sh apps/xxxxxx

##3.配置目标系统

配置目标系统，主要是运行一些配置命令，设置目标系统的sid，路径，端口等

1)更改文件所属用户组

	--root@test
	chown -R testora:dba /u2/TEST/testora/testdb /u2/TEST/testora/testdata &
	chown -R testmgr:dba /u2/TEST/testmgr/testora /u2/TEST/testmgr/testcomn /u2/TEST/testmgr/testappl &

2)配置重做日志

原系统中新增了重做日志组，路径/erpdata/redodata

复制重做日志

	su - testora

切换时会提示找不到环境文件，属于正常现象(.profile[11]: /u2/TEST/testora/testdb/9.2.0/ERP_erp.env)
复制redolog到/erpdata/redodata下

	cp /u2/TEST/testora/testdata/redo01.dbf /erpdata/redodata
	cp /u2/TEST/testora/testdata/redo02.dbf /erpdata/redodata
	cd /erpdata/redodata
	
更改重做日志组设置

系统中重做日志组更改了路径，此时需要手动更改adcrdb.zip中的配置

	--testora
	cd /u2/TEST/testora/testdb/9.2.0/appsutil/clone/data/stage
	cp adcrdb.zip adcrdb.zip.120527bk
	unzip adcrdb.zip
	Archive: adcrdb.zip
	inflating: adcrdbclone.sql
	inflating: adcrdb.sh 
	inflating: dbfinfo.lst
	
修改redolog路径，共四处

	vi adcrdbclone.sql
	GROUP 1 ('%s_dbhome2%/redo01.dbf', '/erpdata/redodata/redo01.dbf') SIZE 314572800

重新打包adcrdb文件

	zip -o adcrdb.zip adcrdbclone.sql adcrdb.sh dbfinfo.lst

运行orainstRoot.sh脚本
	
	--root@test
	/tmp/orainstRoot.sh

配置和开启数据库层

从原系统克隆复制过来的配置需要根据目标系统进行一些调整，需谨慎

	--testora
	cd /u2/TEST/testora/testdb/9.2.0/appsutil/clone/bin
	perl adcfgclone.pl dbTier
	
	Do you want to use a virtual hostname for the target node (y/n) [n] ?:
	Target instance is a Real Application Cluster (RAC) instance (y/n) [n]:
	Target System database name [PROD]:ERP
	Target system RDBMS ORACLE_HOME directory [/u1/PROD/prodora/proddb/9.2.0]:/u2/TEST/testora/testdb/9.2.0
	Target system utl_file accessible directories list [/usr/tmp, /usr/tmp, /u1/PROD/prodora/proddb/9.2.0/appsutil/outbound/PROD_erpprod]:
	/usr/tmp,/usr/tmp,/u2/TEST/testora/testdb/9.2.0/appsutil/outbound/ERP_erp
	Number of DATA_TOP's on the target system [2]:1
	Target system DATA_TOP 1:/u2/TEST/testora/testdata
	Do you want to preserve the Display set to erpprod:1.0 (y/n) [y] ?:n
	Target system Display [erptest:0.0]::0.0
	Do you want to preserve the port values from the source system on the target system (y/n) [y] ?:n
	Enter the port pool number [0-99]:
	3
	... ...

再次运行orainstRoot.sh脚本

	--root@test
	/tmp/orainstRoot.sh

配置和开启应用层

	--testmgr
	cd /u2/TEST/testmgr/testcomn/clone/bin
	perl adcfgclone.pl appsTier
	
	Do you want to use a virtual hostname for the target node (y/n) [n] ?:
	Target system database SID [PROD]:ERP
	Target system database server node [erpprod]:erp
	Target system database domain name [egolife.com]:
	Does the target system have more than one application tier server node (y/n) [n] ?:
	Is the target system APPL_TOP divided into multiple mount points (y/n) [n] ?:
	Target system APPL_TOP mount point [/u1/PROD/prodmgr/prodappl]:/u2/TEST/testmgr/testappl
	Target system COMMON_TOP directory [/u1/PROD/prodmgr/prodcomn]:/u2/TEST/testmgr/testcomn
	Target system 8.0.6 ORACLE_HOME directory [/u1/PROD/prodmgr/prodora/8.0.6]:/u2/TEST/testmgr/testora/8.0.6
	Target system iAS ORACLE_HOME directory [/u1/PROD/prodmgr/prodora/iAS]:/u2/TEST/testmgr/testora/iAS
	Do you want to preserve the Display set to erpprod:1.0 (y/n) [y] ?:n
	Target system Display [erptest:0.0]::0.0
	Location of the JDK on the target system [/usr/java14]:
	Do you want to preserve the port values from the source system on the target system (y/n) [y] ?:n
	Enter the port pool number [0-99]:
	3
	UTL_FILE_DIR on database tier consists of the following directories.
	
	1. /usr/tmp
	2. /usr/tmp
	3. /usr/tmp
	4. /u2/TEST/testora/testdb/9.2.0/appsutil/outbound/TEST_erptest
	5. /u2/TEST/testora/testdb/9.2.0/appsutil/outbound/TEST_erptest
	6. /usr/tmp
	Choose a value which will be set as APPLPTMP value on the target node [1]:1
	
	
	addisctl.sh version 115.15
	
	/u2/TEST/testmgr/testora/8.0.6/vbroker/bin/osagent
	Started osagent.
	Osagent logs messages to the file /u2/TEST/testmgr/testora/8.0.6/discwb4/util/osagent.log.
	Waiting for OAD to start...
	Started OAD.
	OAD logs messages to the file /u2/TEST/testmgr/testora/8.0.6/discwb4/util/oad.log.
	Discoverer Locator Started.
	Locator logs messages to the file /u2/TEST/testmgr/testora/8.0.6/discwb4/util/locator.log.
	Registering Discoverer Session
	Completed registration of repository_id = IDL:DCISessionManager:1.0
	object_name = erptest.egolife.com_8003OracleDiscovererSession4
	reference data =
	path_name = /u2/TEST/testmgr/testora/8.0.6/discwb4/util/runses.sh
	activation_policy = UNSHARED_SERVER
	args = (length=4)[-session; erptestegolife.com_8003OracleDiscovererSession4; -prefere
	nce; erptest.egolife.com_8003OracleDiscovererPreferences4; ]
	env = NONE
	for OAD on host 10.20.1.14
	Registering the Collector
	Completed registration of repository_id = IDL:DCICollector:1.0
	object_name = erptest.egolife.com_8003OracleDiscovererCollector4
	reference data =
	path_name = /u2/TEST/testmgr/testora/8.0.6/discwb4/util/runcol.sh
	activation_policy = SHARED_SERVER
	args = (length=2)[-collector; erptest.egolife.com_8003OracleDiscovererCollector4; ]
	env = NONE
	for OAD on host 10.20.1.14
	Applying preferences from file : /u2/TEST/testmgr/testora/8.0.6/discwb4/util/pref.txt
	Finished applying preferences
	
	Closing down registry..
	Registry File sync...
	Registering Discoverer Preference Repository
	Completed registration of repository_id = IDL:DCICORBAInterface:1.0
	object_name = erptest.egolife.com_8003OracleDiscovererPreferences4
	reference data =
	path_name = /u2/TEST/testmgr/testora/8.0.6/discwb4/util/runpref.sh
	activation_policy = SHARED_SERVER
	args = (length=2)[-preference; erptest.egolife.com_8003OracleDiscovererPreferences4; ]
	env = NONE
	for OAD on host 10.20.1.14
	
	addisctl.sh: exiting with status 0
	
	
	.end std out.
	
	.end err out.

##4.完成目标系统最终设置

1)Profile参数设置

更改颜色和Site name

	System profile -> Java Color Scheme : blue
	Site Name : ERP

2)重新配置应用

关闭应用

	--testmgr
	adstpall.sh apps/xxxxxx

运行自动配置脚本

	--testmgr
	adautocfg.sh

删除原系统节点

	-apps
	select * from fnd_nodes ;
	DELETE FROM FND_NODES WHERE NODE_NAME='ERPPROD';

3)更改工作流设置

	--sysadmin
	Administrator -> workflow -> Notification Mailers -> Edit

将PROD改为ERP，用户名密码为testmgr的用户名密码
修改并发管理器的节点名为ERP Standard Manager

4)取消请求

取消所有定期跑的请求，如AIS、报表、定期请求等

5)更改密码

生产环境克隆到测试环境后，为安全起见，必须更改测试环境密码
更改数据库和应用用户密码，在更改密码前，先关闭应用

	--testmgr
	adstpall.sh apps/xxxxxx

运行fndcpass命令，更改密码

	FNDCPASS apps/oldpassword 0 Y system/manager SYSTEM APPLSYS newpassword

更改wdbsvr.app中记录的密码

	vi $IAS_ORACLE_HOME/Apache/modplsql/cfg/wdbsvr.app
	password = newpassword

运行自动配置脚本

	--testmgr
	adautocfg.sh

6)初始化参数设置

关闭数据库

	--testora
	addbctl.sh stop immediate

设置归档参数

	cd /u2/TEST/testora/testdb/9.2.0/dbs
	vi initERP.ora
	
	log_archive_start = true # if you want automatic archiving
	log_archive_dest_1 ='location=/u2/TEST/testora/arclog/'
	log_archive_format ='arch_%t_%s.arc'

开启数据库

	$ sqlplus /nolog
	SQL> conn /as sysdba
	SQL> startup mount pfile='/u2/TEST/testora/testdb/9.2.0/dbs/initERP.ora';
	SQL> alter database archivelog;
	SQL> archive log start;
	SQL> archive log list;
	SQL> alter database open;
	SQL> alter system switch logfile;
	SQL> create spfile from pfile;
	SQL> shutdown immediate
	SQL> startup
	SQL> alter system set sga_max_size = 3000M scope=spfile;
	SQL> shutdown
	SQL> startup

##5.错误记录

###1)2012-04-22 出错记录

将testvg挂载到正式环境后复制出现IO错误，重新挂载时出错

	# mount /u2
	Replaying log for /dev/fslv00.
	mount: 0506-324 Cannot mount /dev/fslv00 on /u2: The media is not formatted or the format is not correct.
	0506-342 The superblock on /dev/fslv00 is dirty. Run a full fsck to fix.

解决方法

在测试环境将testvg重新激活，使用fsck修复。复制时将prodvg挂载到测试环境进行复制。
testvg无法挂载到正式环境的原因待查。

	# fsck /u2
	The current volume is: /dev/fslv00
	Primary superblock is valid.
	J2_LOGREDO:log redo processing for /dev/fslv00 
	Primary superblock is valid.
	*** Phase 1 - Initial inode scan
	*** Phase 2 - Process remaining directories
	*** Phase 3 - Process remaining files
	*** Phase 4 - Check and repair inode allocation map
	File system inode map is corrupt; FIX? y
	Superblock marked dirty because repairs are about to be written.
	*** Phase 5 - Check and repair block allocation map
	File system is clean.
	Superblock is marked dirty; FIX? y
	All observed inconsistencies have been repaired.

###2)2010-11-21 出错记录

	Completed Apply...
	Sun Nov 21 15:30:38 2010
	
	Beginning APPSIAS_ERP registration to central inventory...
	
	ORACLE_HOME NAME : APPSIAS_ERP
	ORACLE_HOME PATH : /u2/TEST/testmgr/testora/iAS
	Using Inventory location in /etc/oraInst.loc
	Log file located at /etc/oraInventory/logs/OracleHomeCloner_11210330.log
	
	ERROR: Registration Failed... Please check log file.
	
	You can rerun this registration with the following script:
	/u2/TEST/testmgr/testappl/admin/out/ERP_erp/regOUI_APPSIAS_ERP.sh
	
	Skipping the starting of services
	INFO : Rapid Clone completed successfully , but the AutoConfig run recorded some errors. 
	Please review the AutoConfig section in the logfile. If required, you can re-run AutoConfig from command line after fixing the problem
	Once Autoconfig issue is fixed , you can start services$ 
	$ /u2/TEST/testmgr/testappl/admin/out/ERP_erp/regOUI_APPSIAS_ERP.sh
	Using Inventory location in /etc/oraInst.loc
	Log file located at /etc/oraInventory/logs/OracleHomeCloner_11210334.log
	Registration failed
	ERRORCODE = 1 ERRORCODE_END

	# cat /etc/oraInventory/logs/OracleHomeCloner_11210340.log|more
	Registering local Oracle Home located at /u2/TEST/testmgr/testora/iAS to central
	oracle inventory
	RC-00126: Update inventory failed. 
	Unable to create a new Oracle Home at /u2/TEST/testmgr/testora/iAS. Oracle Home 
	already exists at this location. Select another location.
	Raised by oracle.apps.ad.clone.util.OracleHomeCloner
	Registering local Oracle Home located at /u2/TEST/testmgr/testora/iAS to central
	oracle inventory
	RC-00126: Update inventory failed. 
	Unable to create a new Oracle Home at /u2/TEST/testmgr/testora/iAS. Oracle Home 
	already exists at this location. Select another location.
	Raised by oracle.apps.ad.clone.util.OracleHomeCloner

解决方法

	mv /tmp/oraInventory /tmp/oraInventory.bk
	/tmp/orainstRoot.sh

##参考

* Cloning Oracle Applications Release 11i with Rapid Clone
* EBS 11i Creating a Clone using Oracle Application Manager (OAM Clone)

---
layout: post
title: Python with Oracle Database
category : Python
tags : [Python, Libs, Database]
---

最近刚开始学习Python，学习了下基本语法，确实很灵活，不得不让只会写Java代码的人眼前一亮。

因工作中与Oracle数据打交道比较多，所以最先想到的就是如何用Python操作Oracle数据库，比如基本的数据库连接，增删改查操作，存储过程、函数调用，事务处理等，方便以后做一些测试、管理工作的迁移。在Java开发中，访问Oracle数据库可以采用JDBC，Oracle官方提供实现JDBC规范的ojdbc驱动；在Python开发中，找到了实现Python DB API V2.0规范的cx_Oracle模块，还有基于Tuxedo事务规范的tux_oracle。tux_oracle即基于tuxmodule和cx_Oracle。

##cx_Oracle学习计划

cx_Oracle 是Python版的Oracle Database操作扩展模块

1) 实现Python database API 2.0 规范

   curosr.nextset()、time data type尚未实现(Oracle Database不支持)

2) 增加一些Oracle Database扩展

3) 使用前提：安装Oracle客户端(服务端)，或者Instant Client

*  windows: Oracle Instant Client  +  cx_Oracle 5.1.2
* unix: cx_Oracle.pyd/cx_Oracle.so

4) 实现原理：OCI --> C --> Python

5) 开发者：doc -> sample -> test -> source code -> apps dev

6) 事务处理：tux_oracle

7) 总结：意见，建议

##参考

* [Python: Python DB API 2](http://www.python.org/dev/peps/pep-0249/)
* [Sourceforge: cx_Oracle](http://sourceforge.net/projects/cx-oracle/)
* [Sourceforge: tux_oracle](http://sourceforge.net/projects/cx-oracle/)
##克隆环境

* OS: IBM AIX 5.3 64bit
* EBS: 11.5.10.2 64bit
* DB: 9.2.0.6 64bit

ERPProd和ERPTest环境一样，在硬件方面ERPTest配置稍低

##准备工作

关闭克隆目标系统的数据库、应用等。
目前ERP环境采用定期克隆，这里还需要删除克隆目标系统上以前的数据

1)关闭测试环境应用、监听、数据库

	--testmgr
	adstpall.sh apps/xxxxxx

	--testora
	addlnctl.sh stop ERP
	addbctl.sh stop immediate

2)删除测试环境数据

	--testora
	rm -rf /u2/TEST/testora/testdata /u2/TEST/testora/testdb &
	--testora
	rm -f /erpdata/redodata/redo*.dbf

	--testmgr
	rm -rf /u2/TEST/testmgr/testappl /u2/TEST/testmgr/testora &

3)删除测试环境上一次克隆时做的备份

	--testmgr
	cd /u2/TEST/testmgr/testcomn
	rm -rf _pages_120422bk &
	rm -rf html_120422bk &
	rm -rf java_120422bk &
	rm -rf clone_120422bk & 
	rm -rf util_120422bk &

4)备份测试环境上一次克隆的数据

	--testmgr
	mv /u2/TEST/testmgr/testcomn/_pages /u2/TEST/testmgr/testcomn/_pages_120527bk
	mv /u2/TEST/testmgr/testcomn/html /u2/TEST/testmgr/testcomn/html_120527bk
	mv /u2/TEST/testmgr/testcomn/java /u2/TEST/testmgr/testcomn/java_120527bk
	mv /u2/TEST/testmgr/testcomn/clone /u2/TEST/testmgr/testcomn/clone_120527bk
	mv /u2/TEST/testmgr/testcomn/util /u2/TEST/testmgr/testcomn/util_120527bk

##2.正式克隆

在执行rapid clone时，会从原系统创建克隆用的模板配置文件，当原系统文件复制到目标系统时，rapid clone工具将使用目标心痛的配置来更新这些模板配置文件，达到克隆出另外一套系统的目的。

1)关闭应用、监听、数据库
	
	--prodmgr
	adstpall.sh apps/xxxxxx
	--prodora
	addlnctl.sh stop PROD
	addbctl.sh stop immediate

2)数据库预克隆

	--prodora
	cd /u1/PROD/prodora/proddb/9.2.0/appsutil/scripts/PROD_erpprod
	perl adpreclone.pl dbTier

3)应用预克隆

	--prodmgr
	cd /u1/PROD/prodmgr/prodcomn/admin/scripts/PROD_erpprod
	perl adpreclone.pl appsTier

4)关闭原系统数据

因克隆后，数据库已经开启，再次关闭db

	--prodora
	addbctl.sh stop immediate

5)拷贝原系统数据到目标系统

目前ERP正式、测试环境使用AIX 5.3操作系统，共用磁阵，拷贝数据可以采用挂载文件系统的方式进行

5.1)卸载正式环境上/u1文件系统

	--root@prod
	fuser -u /dev/fslv00
	umount /u1
	varyoffvg prodvg

5.2)挂载/u1到测试环境

	--root@test
	varyonvg prodvg
	mount /u1

5.3)复制原系统数据到目标系统

	--root@test
	cp -hrf /u1/PROD/prodora/proddb /u2/TEST/testora/testdb &
	cp -hrf /u1/PROD/prodora/proddata /u2/TEST/testora/testdata &
	cp -hrf /u1/PROD/prodmgr/prodappl /u2/TEST/testmgr/testappl &
	cp -hrf /u1/PROD/prodmgr/prodora /u2/TEST/testmgr/testora &
	
	cp -hrf /u1/PROD/prodmgr/prodcomn/_pages /u2/TEST/testmgr/testcomn/_pages &
	cp -hrf /u1/PROD/prodmgr/prodcomn/html /u2/TEST/testmgr/testcomn/html &
	cp -hrf /u1/PROD/prodmgr/prodcomn/java /u2/TEST/testmgr/testcomn/java &
	cp -hrf /u1/PROD/prodmgr/prodcomn/util /u2/TEST/testmgr/testcomn/util &
	cp -hrf /u1/PROD/prodmgr/prodcomn/clone /u2/TEST/testmgr/testcomn/clone &

5.4)检查复制数据任务是否完成
	
	--root@test
	jobs 或 ps -ef|grep cp

5.5)复制完成后，测试环境上卸载/u1，并将/u1挂载至原系统

	--root@test
	unmount /u1
	varyoffvg prodvg
	
	--root@prod
	varyonvg prodvg
	mount /u1

6)开启原系统数据库、应用等，使应用正常提供服务

开启DB、监听

	--prodora
	adautocfg.sh

备份jserv.properties文件

	--prodmgr
	cp /u1/PROD/prodmgr/prodora/iAS/Apache/Jserv/etc/jserv.properties /u1/PROD/prodmgr/prodora/iAS/Apache/Jserv/etc/jserv.properties.dist
	adautocfg.sh
	
正式环境加上第三方jar包
开启应用

	--prodmgr
	adstrtal.sh apps/xxxxxx

##3.配置目标系统

配置目标系统，主要是运行一些配置命令，设置目标系统的sid，路径，端口等

1)更改文件所属用户组

	--root@test
	chown -R testora:dba /u2/TEST/testora/testdb /u2/TEST/testora/testdata &
	chown -R testmgr:dba /u2/TEST/testmgr/testora /u2/TEST/testmgr/testcomn /u2/TEST/testmgr/testappl &

2)配置重做日志

原系统中新增了重做日志组，路径/erpdata/redodata

2.1)复制重做日志

	su - testora

切换时会提示找不到环境文件，属于正常现象(.profile[11]: /u2/TEST/testora/testdb/9.2.0/ERP_erp.env)
复制redolog到/erpdata/redodata下

	cp /u2/TEST/testora/testdata/redo01.dbf /erpdata/redodata
	cp /u2/TEST/testora/testdata/redo02.dbf /erpdata/redodata
	cd /erpdata/redodata
	
2.2)更改重做日志组设置

系统中重做日志组更改了路径，此时需要手动更改adcrdb.zip中的配置

	--testora
	cd /u2/TEST/testora/testdb/9.2.0/appsutil/clone/data/stage
	cp adcrdb.zip adcrdb.zip.120527bk
	unzip adcrdb.zip
	Archive: adcrdb.zip
	inflating: adcrdbclone.sql
	inflating: adcrdb.sh 
	inflating: dbfinfo.lst
	
修改redolog路径，共四处

	vi adcrdbclone.sql
	GROUP 1 ('%s_dbhome2%/redo01.dbf', '/erpdata/redodata/redo01.dbf') SIZE 314572800

重新打包adcrdb文件

	zip -o adcrdb.zip adcrdbclone.sql adcrdb.sh dbfinfo.lst

2.3)运行orainstRoot.sh脚本
	
	--root@test
	/tmp/orainstRoot.sh

2.4)配置和开启数据库层

从原系统克隆复制过来的配置需要根据目标系统进行一些调整，需谨慎

	--testora
	cd /u2/TEST/testora/testdb/9.2.0/appsutil/clone/bin
	perl adcfgclone.pl dbTier
	
	Do you want to use a virtual hostname for the target node (y/n) [n] ?:
	Target instance is a Real Application Cluster (RAC) instance (y/n) [n]:
	Target System database name [PROD]:ERP
	Target system RDBMS ORACLE_HOME directory [/u1/PROD/prodora/proddb/9.2.0]:/u2/TEST/testora/testdb/9.2.0
	Target system utl_file accessible directories list [/usr/tmp, /usr/tmp, /u1/PROD/prodora/proddb/9.2.0/appsutil/outbound/PROD_erpprod]:
	/usr/tmp,/usr/tmp,/u2/TEST/testora/testdb/9.2.0/appsutil/outbound/ERP_erp
	Number of DATA_TOP's on the target system [2]:1
	Target system DATA_TOP 1:/u2/TEST/testora/testdata
	Do you want to preserve the Display set to erpprod:1.0 (y/n) [y] ?:n
	Target system Display [erptest:0.0]::0.0
	Do you want to preserve the port values from the source system on the target system (y/n) [y] ?:n
	Enter the port pool number [0-99]:
	3
	... ...

2.5)再次运行orainstRoot.sh脚本

	--root@test
	/tmp/orainstRoot.sh

2.6)配置和开启应用层

	--testmgr
	cd /u2/TEST/testmgr/testcomn/clone/bin
	perl adcfgclone.pl appsTier
	
	Do you want to use a virtual hostname for the target node (y/n) [n] ?:
	Target system database SID [PROD]:ERP
	Target system database server node [erpprod]:erp
	Target system database domain name [egolife.com]:
	Does the target system have more than one application tier server node (y/n) [n] ?:
	Is the target system APPL_TOP divided into multiple mount points (y/n) [n] ?:
	Target system APPL_TOP mount point [/u1/PROD/prodmgr/prodappl]:/u2/TEST/testmgr/testappl
	Target system COMMON_TOP directory [/u1/PROD/prodmgr/prodcomn]:/u2/TEST/testmgr/testcomn
	Target system 8.0.6 ORACLE_HOME directory [/u1/PROD/prodmgr/prodora/8.0.6]:/u2/TEST/testmgr/testora/8.0.6
	Target system iAS ORACLE_HOME directory [/u1/PROD/prodmgr/prodora/iAS]:/u2/TEST/testmgr/testora/iAS
	Do you want to preserve the Display set to erpprod:1.0 (y/n) [y] ?:n
	Target system Display [erptest:0.0]::0.0
	Location of the JDK on the target system [/usr/java14]:
	Do you want to preserve the port values from the source system on the target system (y/n) [y] ?:n
	Enter the port pool number [0-99]:
	3
	UTL_FILE_DIR on database tier consists of the following directories.
	
	1. /usr/tmp
	2. /usr/tmp
	3. /usr/tmp
	4. /u2/TEST/testora/testdb/9.2.0/appsutil/outbound/TEST_erptest
	5. /u2/TEST/testora/testdb/9.2.0/appsutil/outbound/TEST_erptest
	6. /usr/tmp
	Choose a value which will be set as APPLPTMP value on the target node [1]:1
	
	
	addisctl.sh version 115.15
	
	/u2/TEST/testmgr/testora/8.0.6/vbroker/bin/osagent
	Started osagent.
	Osagent logs messages to the file /u2/TEST/testmgr/testora/8.0.6/discwb4/util/osagent.log.
	Waiting for OAD to start...
	Started OAD.
	OAD logs messages to the file /u2/TEST/testmgr/testora/8.0.6/discwb4/util/oad.log.
	Discoverer Locator Started.
	Locator logs messages to the file /u2/TEST/testmgr/testora/8.0.6/discwb4/util/locator.log.
	Registering Discoverer Session
	Completed registration of repository_id = IDL:DCISessionManager:1.0
	object_name = erptest.egolife.com_8003OracleDiscovererSession4
	reference data =
	path_name = /u2/TEST/testmgr/testora/8.0.6/discwb4/util/runses.sh
	activation_policy = UNSHARED_SERVER
	args = (length=4)[-session; erptestegolife.com_8003OracleDiscovererSession4; -prefere
	nce; erptest.egolife.com_8003OracleDiscovererPreferences4; ]
	env = NONE
	for OAD on host 10.20.1.14
	Registering the Collector
	Completed registration of repository_id = IDL:DCICollector:1.0
	object_name = erptest.egolife.com_8003OracleDiscovererCollector4
	reference data =
	path_name = /u2/TEST/testmgr/testora/8.0.6/discwb4/util/runcol.sh
	activation_policy = SHARED_SERVER
	args = (length=2)[-collector; erptest.egolife.com_8003OracleDiscovererCollector4; ]
	env = NONE
	for OAD on host 10.20.1.14
	Applying preferences from file : /u2/TEST/testmgr/testora/8.0.6/discwb4/util/pref.txt
	Finished applying preferences
	
	Closing down registry..
	Registry File sync...
	Registering Discoverer Preference Repository
	Completed registration of repository_id = IDL:DCICORBAInterface:1.0
	object_name = erptest.egolife.com_8003OracleDiscovererPreferences4
	reference data =
	path_name = /u2/TEST/testmgr/testora/8.0.6/discwb4/util/runpref.sh
	activation_policy = SHARED_SERVER
	args = (length=2)[-preference; erptest.egolife.com_8003OracleDiscovererPreferences4; ]
	env = NONE
	for OAD on host 10.20.1.14
	
	addisctl.sh: exiting with status 0
	
	
	.end std out.
	
	.end err out.

##4.完成目标系统最终设置

1)Profile参数设置

1.1)更改颜色和Site name

	System profile -> Java Color Scheme : blue
	Site Name : ERP

1.2)重新配置应用

关闭应用

	--testmgr
	adstpall.sh apps/xxxxxx

运行自动配置脚本

	--testmgr
	adautocfg.sh

删除原系统节点

	-apps
	select * from fnd_nodes ;
	DELETE FROM FND_NODES WHERE NODE_NAME='ERPPROD';

3)更改工作流设置

	--sysadmin
	Administrator -> workflow -> Notification Mailers -> Edit

将PROD改为ERP，用户名密码为testmgr的用户名密码
修改并发管理器的节点名为ERP Standard Manager

4)取消请求

取消所有定期跑的请求，如AIS、报表、定期请求等

5)更改密码

生产环境克隆到测试环境后，为安全起见，必须更改测试环境密码
更改数据库和应用用户密码，在更改密码前，先关闭应用

	--testmgr
	adstpall.sh apps/xxxxxx

运行fndcpass命令，更改密码

	FNDCPASS apps/oldpassword 0 Y system/manager SYSTEM APPLSYS newpassword

更改wdbsvr.app中记录的密码

	vi $IAS_ORACLE_HOME/Apache/modplsql/cfg/wdbsvr.app
	password = newpassword

5.3)运行自动配置脚本

	--testmgr
	adautocfg.sh

6)初始化参数设置

6.1)关闭数据库

	--testora
	addbctl.sh stop immediate

6.2)设置归档参数

	cd /u2/TEST/testora/testdb/9.2.0/dbs
	vi initERP.ora
	
	log_archive_start = true # if you want automatic archiving
	log_archive_dest_1 ='location=/u2/TEST/testora/arclog/'
	log_archive_format ='arch_%t_%s.arc'

6.3)开启数据库

	$ sqlplus /nolog
	SQL> conn /as sysdba
	SQL> startup mount pfile='/u2/TEST/testora/testdb/9.2.0/dbs/initERP.ora';
	SQL> alter database archivelog;
	SQL> archive log start;
	SQL> archive log list;
	SQL> alter database open;
	SQL> alter system switch logfile;
	SQL> create spfile from pfile;
	SQL> shutdown immediate
	SQL> startup
	SQL> alter system set sga_max_size = 3000M scope=spfile;
	SQL> shutdown
	SQL> startup

##5.错误记录

###1)2012-04-22 出错记录

将testvg挂载到正式环境后复制出现IO错误，重新挂载时出错

	# mount /u2
	Replaying log for /dev/fslv00.
	mount: 0506-324 Cannot mount /dev/fslv00 on /u2: The media is not formatted or the format is not correct.
	0506-342 The superblock on /dev/fslv00 is dirty. Run a full fsck to fix.

解决方法

在测试环境将testvg重新激活，使用fsck修复。复制时将prodvg挂载到测试环境进行复制。testvg无法挂载到正式环境的原因待查。

	# fsck /u2
	The current volume is: /dev/fslv00
	Primary superblock is valid.
	J2_LOGREDO:log redo processing for /dev/fslv00 
	Primary superblock is valid.
	*** Phase 1 - Initial inode scan
	*** Phase 2 - Process remaining directories
	*** Phase 3 - Process remaining files
	*** Phase 4 - Check and repair inode allocation map
	File system inode map is corrupt; FIX? y
	Superblock marked dirty because repairs are about to be written.
	*** Phase 5 - Check and repair block allocation map
	File system is clean.
	Superblock is marked dirty; FIX? y
	All observed inconsistencies have been repaired.

###2)2010-11-21 出错记录

	Completed Apply...
	Sun Nov 21 15:30:38 2010
	
	Beginning APPSIAS_ERP registration to central inventory...
	
	ORACLE_HOME NAME : APPSIAS_ERP
	ORACLE_HOME PATH : /u2/TEST/testmgr/testora/iAS
	Using Inventory location in /etc/oraInst.loc
	Log file located at /etc/oraInventory/logs/OracleHomeCloner_11210330.log
	
	ERROR: Registration Failed... Please check log file.
	
	You can rerun this registration with the following script:
	/u2/TEST/testmgr/testappl/admin/out/ERP_erp/regOUI_APPSIAS_ERP.sh
	
	Skipping the starting of services
	INFO : Rapid Clone completed successfully , but the AutoConfig run recorded some errors. 
	Please review the AutoConfig section in the logfile. If required, you can re-run AutoConfig from command line after fixing the problem
	Once Autoconfig issue is fixed , you can start services$ 
	$ /u2/TEST/testmgr/testappl/admin/out/ERP_erp/regOUI_APPSIAS_ERP.sh
	Using Inventory location in /etc/oraInst.loc
	Log file located at /etc/oraInventory/logs/OracleHomeCloner_11210334.log
	Registration failed
	ERRORCODE = 1 ERRORCODE_END

	# cat /etc/oraInventory/logs/OracleHomeCloner_11210340.log|more
	Registering local Oracle Home located at /u2/TEST/testmgr/testora/iAS to central
	oracle inventory
	RC-00126: Update inventory failed. 
	Unable to create a new Oracle Home at /u2/TEST/testmgr/testora/iAS. Oracle Home 
	already exists at this location. Select another location.
	Raised by oracle.apps.ad.clone.util.OracleHomeCloner
	Registering local Oracle Home located at /u2/TEST/testmgr/testora/iAS to central
	oracle inventory
	RC-00126: Update inventory failed. 
	Unable to create a new Oracle Home at /u2/TEST/testmgr/testora/iAS. Oracle Home 
	already exists at this location. Select another location.
	Raised by oracle.apps.ad.clone.util.OracleHomeCloner

解决方法

	mv /tmp/oraInventory /tmp/oraInventory.bk
	/tmp/orainstRoot.sh

##参考

* 230672.1 Cloning Oracle Applications Release 11i with Rapid Clone
* 398619.1 EBS 11i Creating a Clone using Oracle Application Manager (OAM Clone)
---
layout: post
title: Python PEP249
category : Python
tags : [Python, Database]
---

在项目开发中，数据库应用必不可少。[PEP 249](http://www.python.org/dev/peps/pep-0249/) 即定义了使用Python访问数据库的一组通用规范，统一了不同数据库系统的访问模型。该规 范使得数据库访问模块更易于理解，在提供广泛的数据库连接支持的同时，也增强了应用在 不同数据库之间的可移植性。

在Python Database API 2.0规范中，定义了API接口的各个部分，如模块接口，连接对象， 游标对象，类型对象和构造器，DB API的可选扩展以及可选的错误处理机制等。

##标准模块接口

根据PEP 249规范，做了一个简单的示意图，根据不同类型做了简单的划分，并不一定与 Python对象一一对应。

![pep249](http://dylanninin.com/assets/images/2012/pep249.jpg)

##1.模块接口

Module Interface
Python代码与数据库交互的模块接口，必须提供获取数据库连接的构造方法。

通过调用connect(parameters.. )即可返回数据库连接对象Connection Object，

* connect(parameters... )：获取数据库连接。其中parameters为数据源、用户、密码、数
据库等，根据不同数据库可选不同参数。

##2.数据库连接对象

Connection Object
数据库连接对象主要提供获取数据库游标对象、并提交/回滚事务的方法，以及关闭数据库
连接。

* close()：关闭数据库连接
* commit()：提交事务
* rollback()：回滚事务
* cursor()：获取游标对象，操作数据库，如执行DML操作，调用存储过程等

##3.全局常量

Gobals Constants
定义一些DB API相关的全局常量，在标准接口中，主要如下：

* apilevel：当前DB API版本，如"1.0"，或"2.0"
* threadsafety：整型常量，定义支持的线程安全类型，主要是是否可以在线程之间共享模块、连接对象以及游标对象。

	* 0        不可共享该模块
	* 1        可共享该模块，但不能共享连接对象
	* 2        可共享该模块和连接对象
	* 3        可共享该模块、连接对象和游标独享
	
* paramstyle：接口参数占位符所支持的样式。因易读性和灵活性，推荐使用numeric，named，pyformat
* qmark        问号符样式，如 ...WHERE name=?
* numeric     数字序号样式，如 ...WHERE name=:1
* named      命名参数样式，如 ...WHERE name=:name
* format      ANSI C语言printf格式化编码样式如 e.g. ...WHERE name=%s
* pyformat    Python扩展编码样式如 ...WHERE name=%(name)s

##4.异常体系

Exceptions
异常体系主要用于定义DB API相关的异常，包括警告，错误，数据相关的异常，接口相关的
异常等等。

* Warning ：数据在执行插入操作时被截断，等等
* Error ：这里提到的除 Warning 外的所有异常的基类
* InterfaceError ： 数据库接口而非数据库本身故障
* DatabaseError ： 严格意义上的数据库问题
* DataError ： 包含如下结果数据的问题除数为 0，值超出范围等
* OperationalError ： 与编程人员无关的数据库错误：连接丢失、内存分配错误、事务处理错误等
* IntegrityError ： 数据库的关系完整性受到了影响，例如，外键约束失败
* InternalError ： 数据库遇到内部错误，例如，游标无效、事务不同步
* ProgrammingError ： 未找到表、SQL 语句中的语法错误、指定参数的数量错误等
* NotSupportedError ： 调用的 API 部件并不存在

##5.游标对象

Cursor Object
游标对象代表数据库中的游标，用于指示抓取数据操作的上下文。主要提供执行SQL语句、
调用存储过程、获取查询结果等方法。

* description：数据库列类型和值的描述信息
* rowcount：回返结果的行数统计信息，如SELECT,UPDATE,CALLPROC等
* callproc(procname,[, parameters])：调用存储过程，需要数据库支持
* close()：关闭当前游标
* execute(operation[, parameters])：执行数据库操作，SQL语句或者数据库命令
* executemany(operation, seq_of_params)：用于批量操作，如批量更新
* fetchone()：获取查询结果集中的下一条记录
* fetchmany(size)：获取指定数量的记录
* fetchall()：获取结构集的所有记录
* arraysize：指定使用fetchmany()获取的行数，默认为1
* setinputsizes(sizes)：设置在调用`execute*()`方法时分配的内存区域大小
* setoutputsize(sizes)：设置列缓冲区大小，对大数据列入LONGS，BLOBS尤其有用

##6.数据类型和构造器

Type and Constructor

1）Type
Type定义了Python的数据类型和数据库中表列类型的转换和对应关系。

* STRING： 对应数据库中基于字符串的数据类型，如char，varchar
* BINARY： 对应数据库中的二进制类型，如long, row, blob
* NUMBER： 对应数据库中的数字类型，如number
* DATETIME： 对应数据库中的时间类型，如date, time
* ROWID：对应数据库中的row id
* None：对应数据库中的NULL

2）Constructor

Constructor是一组用于构造特殊数据类型的构造器，可以通过指定参数构造并返回预期对
象。

* Date(year, month, day)：构造日期对象
* Time(hour, minute, second)：构造时间对象
* Timestamp(year, month, day, hour, minute, second)：构造时间戳对象
* DateFromTicks(ticks)：通过从计算机计时开始经过的秒数构造日期对象；即从1970年1月1日0时0分0秒起计算
* TimeFromTicks(ticks)：同上，用于构造时间对象
* TimestampFromTicks(ticks)：同上，用于构造时间戳对象
* Binary(string)：从字符串构造二进制对象 扩展模块接口

扩展模块可以参考官方文档，这里暂不提及。

##参考

* [PEP 249](http://www.python.org/dev/peps/pep-0249/) 
* [PEP 248](http://www.python.org/dev/peps/pep-0248) 
* [DatabaseInterfaces](https://wiki.python.org/moin/DatabaseInterfaces) 

---
layout: post
title: Access Dapenti via VPN
category : Miscellanies
tags : [Windows, Network, Exception]
---

当登陆VPN时你已经可以正常访问一些在gfw列表中的网站；但很可能国内的一些网站却不能 打开了，比如[打喷嚏](http://dapenti.org)，此时你就需要看下本机的路由设置是否正 确。因为连接VPN后本机的默认路由可能已经更改，此时所有的连接都会经过默认路由出去； 而由于文化、版权等原因，国内很多网站往往是不对境外提供服务的。此时则需要针对这些 网站手动添加路由，这样访问时就不走VPN了；当然这样也可以减少不必要的流量浪费。

##查看打喷嚏ip

    C:\Users\dylanninin@gmail.com>nslookup dapenti.org
    服务器:  google-public-dns-a.google.com
    Address:  8.8.8.8

    非权威应答:
    名称:    dapenti.org
    Address:  122.226.227.53

##查看路由命令帮助

    C:\Users\dylanninin@gmail.com>route add

    操作网络路由表。

    ROUTE [-f] [-p] [-4|-6] command [destination]
                  [MASK netmask]  [gateway] [METRIC metric]  [IF interface]

    -f           清除所有网关项的路由表。如果与某个
                 命令结合使用，在运行该命令前，
                 应清除路由表。
 
    -p           与 ADD 命令结合使用时，将路由设置为
                 在系统引导期间保持不变。默认情况下，重新启动系统时，
                 不保存路由。忽略所有其他命令，
                 这始终会影响相应的永久路由。Windows 95
                 不支持此选项。

    -4           强制使用 IPv4。

    -6           强制使用 IPv6。

    command      其中之一:
                 PRINT     打印路由
                 ADD       添加路由
                 DELETE    删除路由
                 CHANGE    修改现有路由
    destination  指定主机。
    MASK         指定下一个参数为"网络掩码"值。
    netmask      指定此路由项的子网掩码值。
                 如果未指定，其默认设置为 255.255.255.255。
    gateway      指定网关。
    interface    指定路由的接口号码。
    METRIC       指定跃点数，例如目标的成本。

    用于目标的所有符号名都可以在网络数据库文件 NETWORKS 中进行查找。用于网关的符号名称都可以在主机名称数据库文件 HOSTS 中进行查找。

    如果命令为 PRINT 或 DELETE。目标或网关可以为通配符，(通配符指定为星号"*")，否则可能会忽略网关参数。

    如果 Dest 包含一个 * 或 ?，则会将其视为 Shell 模式，并且只打印匹配目标路由。"*"匹配任意字符串，而"?"匹配任意一个字符。示例: 157.*.1、157.*、127.*、*224*。

    只有在 PRINT 命令中才允许模式匹配。
    诊断信息注释:
    无效的 MASK 产生错误，即当 (DEST & MASK) != DEST 时。
    示例: > route ADD 157.0.0.0 MASK 155.0.0.0 157.55.80.1 IF 1
             路由添加失败: 指定的掩码参数无效。
             (Destination & Mask) != Destination。

    示例:

    > route PRINT
    > route PRINT -4
    > route PRINT -6
    > route PRINT 157*          .... 只打印那些匹配  157* 的项

    > route ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1 METRIC 3 IF 2
             destination^      ^mask      ^gateway     metric^    ^
                                                         Interface^
      如果未给出 IF，它将尝试查找给定网关的最佳接口。
    > route ADD 3ffe::/32 3ffe::1

    > route CHANGE 157.0.0.0 MASK 255.0.0.0 157.55.80.5 METRIC 2 IF 2

      CHANGE 只用于修改网关和/或跃点数。

    > route DELETE 157.0.0.0
    > route DELETE 3ffe::/32

##指定路由规则

因本机使用路由器连接，ip为192.168.1.123，子网掩码为255.255.255.0，网关为192.168.1.1； 这里指定访问打喷嚏走路由192.168.1.1；而非默认的VPN网关。

    C:\Users\dylanninin@gmail.com>route add 122.226.227.53 mask 255.255.255.255 192.168.1.1
 
    操作完成!

此时，就可以正常访问[打喷嚏](http://dapenti.org)了。

##参考

* chnroute [chnroute](https://github.com/jimmyxu/chnroutes)

---
layout: post
title: TS3100 Exception
category : Oracle
tags : [Database, Storage, Exception]
---

##环境

* OS：IBM Power System 550 AIX 5.3 (64 bit)
* Storage：TS 3100/Tivoli Storage Manager Server 5.4.3.0
* Application：Oracle EBS 11.5.10.2 (64 bit)/Oracle RDBMS 9.2.0.6 (64 bit)
* Backup：RMAN (Database/Archivelog)

##简介

IBM Tivoli Storage Manager（TSM）是为解决企业级数据及系统安全而设计的备份全面解 决方案。Tivoli ITSM 系统已成为当今数据备份领域的首选产品，并为包括金融、电信在内 的许多大型用户和广大的中小型企业用户，解决困扰信息技术部门的备份管理问题。 

为了满足企业所面临的上述问题，我们建议使用一台服务器作为 TSM 备份服务器，使用带 库作为备份介质，将整个企业 IT 环境内的备份操作交由 TSM 备份服务器完成，由 TSM 备 份服务器集中完成所有备份任务的调度、管理以及备份介质的集中管理。

为了解决企业所面临的数据备份问题，备份解决方案需要能够提供：

* 全自动的备份，所有的操作系统，应用，数据库的备份都可以通过设定计划自动运行；
* 集中的备份管理，集中进行整个备份系统的备份策略设置，存储池配置等工作，便于备份系统的管理和维护；
* 全面的数据备份支持，支持大量异构的系统（操作系统，数据库以及各种应用系统）和各种硬件存储设备，能够充分兼容企业内部几乎所有的软硬件平台;
* 友好的用户界面，方便使用和管理操作，提供 GUI，Web-based，以及 CLI 管理和操作平台；
* 支持 LAN、SAN、NAS 等多种存储环境；
* 其他众多先进的技术特性，如完善的介质管理技术、内置关系数据库、备份数据加密及压缩技术等。

##备份检查

以网页方式查看

检查备份路径：Server Administartion >> Object View >> Policy Domains >> Client schedules >> Query Client Events

输入查询条件，查询备份状态如下

    Operation Results
    Scheduled Start      Actual Start         Schedule Name   Node Name     Status   
    -------------------- -------------------- --------------- ------------- ---------
    12/11/12 07:00:00    12/11/12 07:11:09    DELETEEXPIRED   ORAPROD       Completed
    12/11/12 08:00:00    12/11/12 08:08:21    DELEXP_ORATEST  ORATEST       Completed                                
    12/11/12 20:00:00    12/11/12 20:00:08    ARCBAK          ORAPROD       Completed
    12/11/12 21:00:00    12/11/12 21:06:39    ARC_ORATEST     ORATEST       Completed
    12/11/12 23:00:00    12/11/12 23:01:55    FULLBAKUPORACLE ORAPROD       Failed  1                    
    12/12/12 07:00:00    12/12/12 07:04:07    DELETEEXPIRED   ORAPROD       Completed
    12/12/12 08:00:00    12/12/12 08:10:42    DELEXP_ORATEST  ORATEST       Completed

##异常信息

检查异常事件路径：Server Administartion >> Object View >> Server >> Activity Log

根据备份失败时间，输入条件；本例中备份出错为生产环境数据库全备阶段，时间为12/11/12 23:01:55左右。查询该时间段日志，错误信息如下：

    12/11/12 23:09:57     ANR1405W Scratch volume mount request denied - no scratch 
                          volume available. (SESSION: 8460)                        
    12/11/12 23:09:57     ANR0522W Transaction failed for session 8460 for node     
                          ORAPROD (TDP Oracle AIX) - no space available in storage 
                          pool ORADBPOOL and all successor pools. (SESSION: 8460)  
    12/11/12 23:09:57     ANR0403I Session 8460 ended for node ORAPROD (TDP Oracle  
                          AIX). (SESSION: 8460)                                    
    12/11/12 23:10:05     ANR0406I Session 8461 started for node ORAPROD (TDP Oracle
                          AIX) (Tcp/Ip 10.20.1.8(41271)). (SESSION: 8461)          
    12/11/12 23:10:05     ANE4994S (Session: 8461, Node: ORAPROD)  TDP Oracle AIX   
                          ANU0599 TDP for Oracle: (6475950): =>(oraprod) ANU2602E  
                          The object /adsmorc//XSDB_PROD_801788468_dcnskkhk_1_1 was
                          not found on the TSM Server (SESSION: 8461)              
    12/11/12 23:10:05     ANR0403I Session 8461 ended for node ORAPROD (TDP Oracle  
                          AIX). (SESSION: 8461)                                    
    12/11/12 23:10:06     ANR2579E Schedule FULLBAKUPORACLE in domain STANDARD for  
                          node ORAPROD failed (return code 1). (SESSION: 8447)     
    12/11/12 23:10:06     ANR0403I Session 8447 ended for node ORAPROD (AIX).       
                          (SESSION: 8447)                                          
    12/11/12 23:10:06     ANR0406I Session 8462 started for node ORAPROD (AIX)      
                          (Tcp/Ip x.x.x.x(41274)). (SESSION: 8462)               
    12/11/12 23:10:06     ANR0403I Session 8462 ended for node ORAPROD (AIX).       
                          (SESSION: 8462)                     
主要错误： no space available in storage pool ORADBPOOL and all successor pools，即带库中没有临时卷(空闲磁带)用于备份。

##异常处理

此时可进行登陆到带库管理服务器上，进行以下操作，确认备份异常，并进行相应处理。

###登陆服务器

以应用账户ssh登陆到服务器，并运行dsmadmc命令，进入带库管理环境。

	$ set -o vi
	$ dsmadmc
	IBM Tivoli Storage Manager
	Command Line Administrative Interface - Version 5, Release 4, Level 0.0
	(c) Copyright by IBM Corporation and other(s) 1990, 2007. All Rights Reserved.

	Enter your user id:  admin
	Enter your password:  

	Session established with server TSM: AIX-RS/6000
  	Server Version 5, Release 4, Level 3.0
  	Server date/time: 12/12/12   11:33:35  Last access: 12/12/12   11:23:06

###查看日志

q act begindate 可查看系统日志，同web页面相应功能。

	tsm: TSM>q act begindate=-1	

	Date/Time                Message                                                   
	--------------------     ----------------------------------------------------------
	...
	12/11/12   23:09:56      ANR1405W Scratch volume mount request denied - no scratch 
                          	 volume available. (SESSION: 8460)                        
	12/11/12   23:09:56      ANR1405W Scratch volume mount request denied - no scratch 
                          	 volume available. (SESSION: 8460)                        
	12/11/12   23:09:57      ANR1405W Scratch volume mount request denied - no scratch 
                          	 volume available. (SESSION: 8460)                        
	12/11/12   23:09:57      ANR0522W Transaction failed for session 8460 for node     
                          	 ORAPROD (TDP Oracle AIX) - no space available in storage 
                          	 pool ORADBPOOL and all successor pools. (SESSION: 8460)  
	12/11/12   23:09:57      ANR0403I Session 8460 ended for node ORAPROD (TDP Oracle  
                          	 AIX). (SESSION: 8460)                                      	 
	...

###查看磁带状态

q libvol 可查看当前在带库中注册的所有磁带的状态。

* 	Scratch表示此卷（磁带）已被回收，其内容被清除，且从存储池中注销，以备下次使用。
*	private表示此卷当前正在被使用。

如下

	tsm: TSM>q libvol

	------------     -----------     ----------------     ----------     ---------     -------     ------
	3573LIB          A00200L3        Private                             Data          4,096       LTO   
	3573LIB          A00201L3        Private                             Data          4,098       LTO   
	3573LIB          A00202L3        Private                             Data          4,100       LTO   
	3573LIB          A00203L3        Private                             Data          4,097       LTO   
	3573LIB          A00204L3        Private                             Data          4,099       LTO   
	3573LIB          A00205L3        Private                             Data          4,105       LTO   
	3573LIB          A00206L3        Private                             Data          4,104       LTO   
 
q vol   可查看当前存储池中的卷的大小及状态。

* Full状态的卷在达到回收阙值后将会被回收。 

如下

	tsm: TSM>q vol
	
	Volume Name                  Storage         Device         Estimated       Pct      Volume 
	                             Pool Name       Class Name      Capacity      Util      Status 
	------------------------     -----------     ----------     ---------     -----     --------
	/usr/tivoli/tsm/server/-     ARCHIVEPOOL     DISK               8.0 M       0.0     On-Line 
	 bin/archive.dsm                                                                            
	/usr/tivoli/tsm/server/-     BACKUPPOOL      DISK               8.0 M       0.0     On-Line 
	 bin/backup.dsm                                                                             
	/usr/tivoli/tsm/server/-     SPACEMGPOOL     DISK               8.0 M       0.0     On-Line 
	 bin/spcmgmt.dsm                                                                            
	A00200L3                     ORADBPOOL       3573CLASS          1.5 T       5.1       Full  
	A00201L3                     ORADBPOOL       3573CLASS          1.5 T      17.8       Full  
	A00202L3                     ORADBPOOL       3573CLASS          1.5 T      15.0       Full  
	A00203L3                     ORADBPOOL       3573CLASS          1.5 T      30.4       Full  
	A00204L3                     ORADBPOOL       3573CLASS          1.5 T       3.1       Full  
	A00205L3                     ORADBPOOL       3573CLASS          1.5 T      40.2     Filling 
	A00206L3                     ORADBPOOL       3573CLASS          1.5 T      13.2       Full  


###异常确认和处理

发生如上错误的原因则是所有磁带均在存储池中处于Full的状态，导致带库中没有Scratch 状态的磁带用于备份导致。

此时可执行以下命令手动删除磁带中的内容，更改其状态为Scratch。

####q con

* 可查看该卷中的内容，判断其中内容是否可以删除。
* 一般如果卷中只有少量的备份或是测试环境数据，可以考虑清空。

如下：

	tsm: TSM>q con A00200L3

	Node Name           Type     Filespace      FSID     Client's Name for File                
	                             Name                    
	---------------     ----     ----------     ----     --------------------------------------
	ORATEST             Bkup     /adsmorc          2     // q4nk09k9_1_1   
                    
delete volume volumename DISCARDDATA=YES

删除一些测试环境备份卷中数据，执行后该卷将从存储池中注销，恢复状态为Scratch。该 环境汇中标为TEST的即为测试环境备份卷。

删除测试环境卷数据

	tsm: TSM>delete volume A00200L3 DISCARDDATA=YES
	ANR2221W This command will result in the deletion of all inventory references to the data on volume A00200L3, thereby rendering the data
	unrecoverable.
	
	Do you wish to proceed? (Yes (Y)/No (N)) Y
	ANR2222I Discard Data process started for volume A00200L3 (process ID 40).
	ANS8003I Process number 40 started.

查看删除后卷状态

	tsm: TSM>q libvol
	Library Name     Volume Name     Status               Owner          Last Use      Home        Device
	                                                                                   Element     Type  
	------------     -----------     ----------------     ----------     ---------     -------     ------
	3573LIB          A00200L3        Scratch                                           4,096       LTO   
	3573LIB          A00201L3        Private                             Data          4,098       LTO   
	3573LIB          A00202L3        Private                             Data          4,100       LTO   
	3573LIB          A00203L3        Private                             Data          4,097       LTO   
	3573LIB          A00204L3        Private                             Data          4,099       LTO   
	3573LIB          A00205L3        Private                             Data          4,105       LTO   
	3573LIB          A00206L3        Private                             Data          4,104       LTO  

适当删除一些测试环境的备份数据后，有足够的空间则可以正常完成备份任务。

##参考

* IBM：IBM TSM backup case
* IBM：IBM Tivoli Storage Management Concepts

---
layout: post
title: SQLite Introduction
category : Database
tags : [Database, SQLite]
---

##简介

[SQLite](http://www.sqlite.org/)是遵守[ACID](http://en.wikipedia.org/wiki/ACID)的关系式数据库管理系统，它包含在一个相对小的C库中。它是[D.RichardHipp](http://en.wikipedia.org/wiki/D._Richard_Hipp)建立的公有领域项目。

![SQLite](http://www.ostools.net/uploads/apidocs/sqlite/images/sqlite370_banner.gif)

不像常见的客户-服务器范例，SQLite引擎不是个程序与之通信的独立进程，而是连接到程序中成为它的一个主要部分。所以主要的通信协议是在编程语言内的直接API调用。这在消耗总量、延迟时间和整体简单性上有积极的作用。整个数据库(定义、表、索引和数据本身)都在宿主主机上存储在一个单一的文件中。它的简单的设计是通过在开始一个事务的时候锁定整个数据文件而完成的。

##特征

库实现了多数的[SQL-92](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt)标准，包括事务，就是代表原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation ）和持久性（Durablity）的（ACID），触发器和多数的复杂查询。不进行类型检查。你可以把字符串插入到整数列中。例如，某些用户发现这是使数据库更加有用的创新，特别是与无类型的脚本语言一起使用的时候。其他用户认为这是主要的缺点。

多个进程或线程可以访问同一个数据而没有问题。可以并行的满足多个读访问。只有在其他访问当前不被服务的时候才能满足写访问；否则写访问失败并带有一个错误代码(也可以在可配置的超时过期之后自动的重试)。

提供了叫做sqlite的一个独立程序用来查询和管理SQLite数据库文件。 它也充当写使用SQLite库的应用的一个例子。

##语言绑定

可以从C/C++程序中使用这个库，还可以获得对Tcl和一些其他脚本语言的绑定。

在CPAN的DBD::SQLite上有一个Perl的DBI/DBD模块，它不是到SQLite的接口，而是包括整个SQLite数据库引擎在其中并不需要任何额外的软件。

还有一个Python模块叫做PySQLite(现在已经成为标准的内建模块，为sqlite3)。

PHP从PHP5.0开始包含了SQLite，但是自5.1版之后开始成为一个延伸函式库。SQLite能与PHP4一起工作但不包含在其中。

Rails2.0.3将缺省的数据库配置改为了SQLite 3

##SQLite管理客户端

SQLite亦可以作为桌面数据库使用，以下为第三方SQLite的GUI软件。例如：

* SQLiteMan，使用QT开发的一个SQLite客户端，支持多语言、跨平台。SQLiteMan
* SQLite Manager, 以 火狐浏览器的扩展形式提供的SQLite客户端。
* SQLite Database Browser, a graphical client to access SQLite databases
* SqlPro SQL Client, another graphical client to work with SQLite databases

##参考

 * 原文：[数据库服务器SQLite](http://www.oschina.net/p/sqlite)
 * Site：[SQLite](http://www.sqlite.org/)
 * Python：[Python With SQLite](http://docs.python.org/release/2.5.2/lib/module-sqlite3.html)
 * Tutorial：[SQLite Python tutorial](http://zetcode.com/db/sqlitepythontutorial/)

##简介

[SQLite](http://www.sqlite.org/)是遵守[ACID](http://en.wikipedia.org/wiki/ACID)的关系式数据库管理系统，它包含在一个相对小的C库中。它是[D.RichardHipp](http://en.wikipedia.org/wiki/D._Richard_Hipp)建立的公有领域项目。

![SQLite](http://www.ostools.net/uploads/apidocs/sqlite/images/sqlite370_banner.gif)

不像常见的客户-服务器范例，SQLite引擎不是个程序与之通信的独立进程，而是连接到程序中成为它的一个主要部分。所以主要的通信协议是在编程语言内的直接API调用。这在消耗总量、延迟时间和整体简单性上有积极的作用。整个数据库(定义、表、索引和数据本身)都在宿主主机上存储在一个单一的文件中。它的简单的设计是通过在开始一个事务的时候锁定整个数据文件而完成的。

##特征

库实现了多数的[SQL-92](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt)标准，包括事务，就是代表原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation
）和持久性（Durablity）的（ACID），触发器和多数的复杂查询。不进行类型检查。你可以把字符串插入到整数列中。例如，某些用户发现这是使数据库更加有用的创新，特别是与无类型的脚本语言一起使用的时候。其他用户认为这是主要的缺点。

多个进程或线程可以访问同一个数据而没有问题。可以并行的满足多个读访问。只有在其他访问当前不被服务的时候才能满足写访问；否则写访问失败并带有一个错误代码(也可以在可配置的超时过期之后自动的重试)。

提供了叫做sqlite的一个独立程序用来查询和管理SQLite数据库文件。 它也充当写使用SQLite库的应用的一个例子。

##语言绑定

可以从C/C++程序中使用这个库，还可以获得对Tcl和一些其他脚本语言的绑定。

在CPAN的DBD::SQLite上有一个Perl的DBI/DBD模块，它不是到SQLite的接口，而是包括整个SQLite数据库引擎在其中并不需要任何额外的软件。

还有一个Python模块叫做PySQLite(现在已经成为标准的内建模块，为sqlite3)。

PHP从PHP5.0开始包含了SQLite，但是自5.1版之后开始成为一个延伸函式库。SQLite能与PHP4一起工作但不包含在其中。

Rails2.0.3将缺省的数据库配置改为了SQLite 3

##SQLite管理客户端

SQLite亦可以作为桌面数据库使用，以下为第三方SQLite的GUI软件。例如：

* SQLiteMan，使用QT开发的一个SQLite客户端，支持多语言、跨平台。SQLiteMan
* SQLite Manager, 以 火狐浏览器的扩展形式提供的SQLite客户端。
* SQLite Database Browser, a graphical client to access SQLite databases
* SqlPro SQL Client, another graphical client to work with SQLite databases

##参考

 * 原文：[数据库服务器SQLite](http://www.oschina.net/p/sqlite)
 * Site：[SQLite](http://www.sqlite.org/)
 * Python：[Python With SQLite](http://docs.python.org/release/2.5.2/lib/module-sqlite3.html)
 * Tutorial：[SQLite Python tutorial](http://zetcode.com/db/sqlitepythontutorial/)
---
layout: post
title: Requests Introduction
category : Python
tags : [Python, Libs, Web]
---

Requests is an [Apache2 Licensed](http://docs.python-requests.org/en/latest/user/intro/#apache2) HTTP library, written in Python, for human beings.

Python's standard urllib2 module provides most of the HTTP capabilities you need, but the API is thoroughly broken. It was built for a different time -- and a different web. It requires an enormous amount of work (even method overrides) to perform the simplest of tasks.

Things shouldn't be this way. Not in Python.

	>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
	>>> r.status_code
	200
	>>> r.headers['content-type']
	'application/json; charset=utf8'
	>>> r.encoding
	'utf-8'
	>>> r.text
	u'{"type":"User"...'
	>>> r.json
	{u'private_gists': 419, u'total_private_repos': 77, ...}

similar code in urllib2.

	#!/usr/bin/env python
	# -*- coding: utf-8 -*-
	import urllib2
	gh_url = 'https://api.github.com'
	req = urllib2.Request(gh_url)
	 
	password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
	password_manager.add_password(None, gh_url, 'user', 'pass')
	 
	auth_manager = urllib2.HTTPBasicAuthHandler(password_manager)
	opener = urllib2.build_opener(auth_manager)
	urllib2.install_opener(opener)
	handler = urllib2.urlopen(req)
	 
	print handler.getcode()
	print handler.headers.getheader('content-type')
	# ------
	# 200
	# 'application/json'

Requests takes all of the work out of Python HTTP/1.1 -- making your integration with web services seamless. There's no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, powered by [urllib3](https://github.com/shazow/urllib3), which is embedded within Requests.

##Reference

* site: [request-python](http://docs.python-requests.org/en/latest/)
* Github: [requests](https://github.com/kennethreitz/requests)

---
layout: post
title: A Case of ATO Exception
category : Oracle
tags : [Oracle, Database, DBA, EBS]
---

##异常症状

2012-12-21下午Oracle ERP在销售模块做ATO产品订单进展时，没有弹出任何错误，也没有 弹出订单进展的"确认"提示，直接跳到订单头信息，行信息中也没有出现带`*`的标准成品料 号。且所有的ATO订单都无法完成订单进展。导致特制单不能及时录入系统，问题较为严重。

##解决思路

使用topas查看操作系统是否有异常进程；

登录系统查看并发请求状态；

查系统当前会话、进程数；

看数据库警告日志；

##详细过程

###1. OS进程检查

登录到erp，运行topas观察系统进程使用资源情况。

###2. 并发请求 

职责路径：system administrator >> Concurrent >> Manager >> Administer 此时再查看并发管理运行状态，等待处理的请求 因只有系统管理员有该权限，大家也可以通过SQL脚本进行查询，脚本如下(可以APPS运行)：

	WITH requests_count AS
	(SELECT COUNT(1) counts
   	FROM apps.fnd_concurrent_requests fcr
	WHERE fcr.request_date > trunc(SYSDATE - 1)
     AND fcr.requested_start_date < trunc(SYSDATE + 1)
     AND fcr.phase_code IN ('P', 'R'))
	SELECT 'Attention! Concurrent requests are up to ' || rc.counts "attention"
 	FROM requests_count rc
	WHERE rc.counts > 300;

###3. 系统当前进程、会话数

Oracle数据库当前数据库设置的最大进程数、会话数在数据初始参数中已经设置，可以查询如下：

1)使用PL/SQL Developer，打开Command Windows即可（以APPS运行）

	SQL> show parameter processes
	 
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	aq_tm_processes                      integer     2
	db_writer_processes                  integer     1
	job_queue_processes                  integer     10
	log_archive_max_processes            integer     2
	processes                            integer     800
	 
	SQL> show parameter sessions
	 
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	java_max_sessionspace_size           integer     0
	java_soft_sessionspace_limit         integer     0
	license_max_sessions                 integer     0
	license_sessions_warning             integer     0
	logmnr_max_persistent_sessions       integer     1
	mts_sessions                         integer     0
	sessions                             integer     1600
	shared_server_sessions               integer     0
	

2)使用SQL脚本查询，打开SQL Windows，或者SQLPlus(以APPS运行)

	select * from v$parameter p where p.NAME in ('processes','sessions');

再查看当前系统的进程数、会话数。

	SELECT COUNT(*) FROM v$process;	--当前进程数

	SELECT COUNT(*) FROM v$session; --当前会话数

当以上数目非常接近系统设定的最大值时，系统就可能出现类似以上的问题，主要是超出最 大会话数，无法进行新的会话、连接等响应。

此时一般会手动处理一些过期的会话，主要是以下方式

	--类似session/cookie的会话
	SELECT 'kill -9 ' || b.spid
	  FROM v$session a,
	       v$process b,
	       v$sqltext c
	 WHERE a.paddr = b.addr
	   AND a.username IS NOT NULL
	   AND c.hash_value = a.sql_hash_value
	   AND c.sql_text LIKE '%SESSION_COOKIE_DOMAIN%'
	   AND a.sid != (SELECT sid FROM v$mystat WHERE rownum = 1);

	--当前之前还没有正常回收的forms会话
	SELECT 'alter system kill session' || '''' || s.sid || ',' || s.serial# || ''';' oracle_level_kill
	  FROM v$session s,
	       v$process p
	 WHERE s.paddr = p.addr
	   AND s.sid IN (SELECT se.sid
	                   FROM v$session se
	                  WHERE sid IN (SELECT session_id FROM v$locked_object)
	                    AND se.logon_time < trunc(SYSDATE))
	   AND s.action LIKE 'FRM:%'
	   AND s.STATUS = 'INACTIVE';

###4.查看数据库警告日志

数据库警告日志中记录了数据库在运行过程中的状态信息，包括发生的错误。 登录到服务器主机，如prodora，警告日志文件路径：

	注：
	以上路径也是在数据库的初始化参数设定，查询如下：
	--PL/SQL Developer的Command Windows
	SQL> show parameter background_dump_dest;
	 
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	background_dump_dest                 string      /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/bdump

	--sql脚本
	select * from v$parameter p where p.NAME in ('background_dump_dest');

	如今天的警告日志截取如下：
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_5173482.trc:
	ORA-00600: 内部错误代码，参数: [15419], [severe error during PL/SQL execution], [], [], [], [], [], []
	ORA-06544: PL/SQL: 内部错误，参数: [77406], [], [], [], [], [], [], []
	ORA-06553: PLS-801: 内部错误 [77406]
	ORA-00018: 超出最大会话数
	ORA-06512: 在"APPS.FND_SIGNON", line 239
	Fri Dec 21 15:32:03 2012
	Timed out trying to start process J002.
	Fri Dec 21 15:32:43 2012
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_3883030.trc:
	ORA-00600: 内部错误代码，参数: [4414], [0], [0], [5454], [2], [], [], []
	ORA-00018: 超出最大会话数
	ORA-00018: 超出最大会话数
	Fri Dec 21 15:33:55 2012
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_688340.trc:
	ORA-00600: 内部错误代码，参数: [15419], [severe error during PL/SQL execution], [], [], [], [], [], []
	ORA-06544: PL/SQL: 内部错误，参数: [77406], [], [], [], [], [], [], []
	ORA-06553: PLS-801: 内部错误 [77406]
	ORA-00018: 超出最大会话数
	ORA-06512: 在"APPS.FND_SIGNON", line 239
	Fri Dec 21 15:33:55 2012
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_6520900.trc:
	ORA-00600: 内部错误代码，参数: [15419], [severe error during PL/SQL execution], [], [], [], [], [], []
	ORA-06544: PL/SQL: 内部错误，参数: [77406], [], [], [], [], [], [], []
	ORA-06553: PLS-801: 内部错误 [77406]
	ORA-20001: Oracle 错误 -18：FND_SIGNON.AUDIT_END 中检测到 ORA-00018: 超出最大会话数。
	ORA-06512: 在"APPS.APP_EXCEPTION", line 72
	ORA-06512: 在"APPS.FND_SIGNON", line 18
	ORA-06512: 在"APPS.FND_SIGNON", line 317
	Fri Dec 21 15:34:43 2012
	Completed checkpoint up to RBA [0x514c6.2.10], SCN: 0x0771.4352af6c
	Fri Dec 21 15:40:51 2012
	Restarting dead background process QMN0
	QMN0 started with pid=600
	Fri Dec 21 15:41:06 2012
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_6520900.trc:
	ORA-00600: 内部错误代码，参数: [17285], [0x1103D2D58], [4294967295], [0x70000008B9DF418], [], [], [], []
	Fri Dec 21 15:41:07 2012
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_6520900.trc:
	ORA-00600: 内部错误代码，参数: [17285], [0x1103D2D58], [4294967295], [0x70000008B9DF418], [], [], [], []
	ORA-00600: 内部错误代码，参数: [17285], [0x1103D2D58], [4294967295], [0x70000008B9DF418], [], [], [], []
	Fri Dec 21 15:42:37 2012
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_4214832.trc:
	ORA-00604: 递归 SQL 层 1 出现错误
	ORA-00018: 超出最大会话数
	ORA-06512: 在line 1
	Fri Dec 21 15:42:40 2012
	Errors in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_3190972.trc:
	ORA-00604: 递归 SQL 层 1 出现错误
	ORA-00018: 超出最大会话数
	ORA-06512: 在line 1
	
通过发生的ORA-错误，以及应用的异常时间，基本可以确定是由于会话引起的以上异常，清 理一些过期的会话就可以解决。

##特别说明

在操作系统中已经做了一些cron任务，定期预警或者执行会话清理。主要如下：

* 定期清理过期会话(session)
* 定期检查死锁(deadlock)
* 定期检查请求(requests)
* 定期检查无效对象、表空间等(invalidobject, tablespace, tablespaces)
* 定期检查数据库警告日志(checkalert.sh)

以上脚本均会发送邮件通知。

查看prodora定义的cron job

	$ crontab -l
	45 07,13,16 * * *   /u1/PROD/prodora/dailyduty.sh session  > /dev/null 2>&1
	00 08 * * *   /u1/PROD/prodora/dailyduty.sh database > /dev/null 2>&1
	05 08 * * *   /u1/PROD/prodora/dailyduty.sh instance > /dev/null 2>&1
	10 08 * * *   /u1/PROD/prodora/dailyduty.sh datafiles > /dev/null 2>&1
	15 08 * * *   /u1/PROD/prodora/dailyduty.sh tablespaces > /dev/null 2>&1
	20 08 * * 5   /u1/PROD/prodora/dailyduty.sh tablespace > /dev/null 2>&1
	00 10,16 * * *   /u1/PROD/prodora/dailyduty.sh deadlock > /dev/null 2>&1
	00 15,16,17 * * *   /u1/PROD/prodora/dailyduty.sh requests > /dev/null 2>&1
	00 08 * * 5   /u1/PROD/prodora/dailyduty.sh invalidobject > /dev/null 2>&1
	55 23 * * 0-4   /u1/PROD/prodora/checkalert.sh > /dev/null 2>&1
	
主要脚本dailyduty.sh

	#!/bin/ksh
	#abstract:
	#oracle database alert jobs
	#history:
	#2012-06-11     dylanninin@gmail.com         first release
	#variables
	script_basepath=/u1/PROD/prodora/sql
	mail_date=$(date +%Y-%m-%d\ %H:%M:%S)
	receipt=dylanninin@gmail.com
	hostname=$(hostname)
	
	#path
	ORACLE_HOME=/u1/PROD/prodora/proddb/9.2.0
	export ORACLE_HOME
	PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:$ORACLE_HOME/bin
	export PATH
	ORACLE_SID=PROD
	export ORACLE_SID
	
	
	#variabls
	script_name=$1
	if [ "${script_name}" == "" ]; then
	   echo "script file name was empty!"
	   exit 0
	fi
	script="${script_basepath}/${script_name}.sql"
	if [ ! -e ${script} ]; then
	   echo "sql script file ${script} does not exist!"
	   exit 0
	fi
	
	#sql spool log file
	sqllog="${script_basepath}/${script_name}.log"
	
	#sqlplus logon
	sqlplus -s '/as sysdba' << EOF
	set feed off
	set linesize 200
	set pagesize 200
	spool ${sqllog}
	@${script}
	spool off
	exit
	EOF
	
	#grep spool file and kill processes
	if [ "${script_name}" == "session" ]; then
	   grep "kill -9" ${sqllog} | awk 'NR >1 {print $0}' | ksh
	fi
	
	#mail
	if [ "${script_name}" != "session" ]; then
	   if [ `cat ${sqllog} | wc -l` -gt 0 ]; then
	       cat ${sqllog} | mail -s "${mail_date}:${hostname} dba daily check of ${script_name}" ${receipt} 
	   fi
	fi

##参考

* Oracle Database Administrator Guide
* Oracle Application Administrator Guide

---
layout: post
title: MongoDB Introduction
category : Database
tags : [Database, NoSQL, MongoDB]
---

##Introduction

MongoDB wasn't designed in a lab. We built MongoDB from our own experiences building large scale, high availability, robust systems. We didn't start from scratch, we really tried to figure out what was broken, and tackle that. So the way I think about MongoDB is that if you take MySql, and change the data model from relational to document based, you get a lot of great features: embedded docs for speed, manageability, agile development with schema-less databases, easier horizontal scalability because joins aren't as important. There are lots of things that work great in relational databases: indexes, dynamic queries and updates to name a few, and we haven't changed much there. For example, the way you design your indexes in MongoDB should be exactly the way you do it in MySql or Oracle, you just have the option of indexing an embedded field.

-- Eliot Horowitz, 10gen CTO and Co-founder

##Why MongoDB?

* Document-oriented
 	* Documents (objects) map nicely to programming language data types
 	* Embedded documents and arrays reduce need for joins
 	* Dynamically-typed (schemaless) for easy schema evolution
 	* No joins and no multi-document transactions for high performance and easy scalability
* High performance
 	* No joins and embedding makes reads and writes fast
 	* Indexes including indexing of keys from embedded documents and arrays
 	* Optional streaming writes (no acknowledgements)
* High availability
 	* Replicated servers with automatic master failover
* Easy scalability
 	* Automatic sharding (auto-partitioning of data across servers)
 	* Reads and writes are distributed over shards
 	* No joins or multi-document transactions make distributed queries easy and fast
 	* Eventually-consistent reads can be distributed over replicated servers
* Rich query language

##Large MongoDB deployment

1. One or more shards, each shard holds a portion of the total data (managed automatically). Reads and writes are automatically routed to the appropriate shard(s). Each shard is backed by a replica set - which just holds the data for that shard.

	A replica set is one or more servers, each holding copies of the same data. At any given time one is primary and the rest are secondaries. If the primary goes down one of the secondaries takes over automatically as primary. All writes and consistent reads go to the primary, and all eventually consistent reads are distributed amongst all the secondaries.

2. Multiple config servers, each one holds a copy of the meta data indicating which data lives on which shard.

3. One or more routers, each one acts as a server for one or more clients. Clients issue queries/updates to a router and the router routes them to the appropriate shard while consulting the config servers.

4. One or more clients, each one is (part of) the user's application and issues commands to a router via the mongo client library (driver) for its language.

	mongod is the server program (data or config). mongos is the router program.

	![mongodb](http://www.mongodb.org/download/attachments/2097393/sharding.png)

#Small deployment (no partitioning)
1. One replica set (automatic failover), or one server with zero or more slaves (no automatic failover).

2. One or more clients issuing commands to the replica set as a whole or the single master (the driver will manage which server in the replica set to send to).

#Mongo data model

* A Mongo system (see deployment above) holds a set of databases
* A database holds a set of collections
* A collection holds a set of documents
* A document is a set of fields
* A field is a key-value pair
* A key is a name (string)
* A value is a
	* basic type like string, integer, float, timestamp, binary, etc.,
	* a document, or
	* an array of values

##Mongo query language

To retrieve certain documents from a db collection, you supply a query document containing the fields the desired documents should match. For example, `{name: {first: 'John', last: 'Doe'}}` will match all documents in the collection with name of John Doe. Likewise,` {name.last: 'Doe'}` will match all documents with last name of Doe. Also, `{name.last: /^D/}` will match all documents with last name starting with 'D' (regular expression match).

Queries will also match inside embedded arrays. For example, `{keywords: 'storage'}` will match all documents with 'storage' in its keywords array. Likewise, `{keywords: {$in: ['storage', 'DBMS']}}` will match all documents with 'storage' or 'DBMS' in its keywords array.

If you have lots of documents in a collection and you want to make a query fast then build an index for that query. For example, `ensureIndex({name.last: 1})` or `ensureIndex({keywords: 1})`. Note, indexes occupy space and slow down updates a bit, so use them only when the tradeoff is worth it.

##See also:

* [source](http://www.mongodb.org/display/DOCS/Introduction)
* [Philosophy](http://www.mongodb.org/display/DOCS/Philosophy)

##Reference

* [Mongodb Reference](http://docs.mongodb.org/manual/reference/)
* [SQL to MongoDB Mapping Chart](http://docs.mongodb.org/manual/reference/sql-comparison/)
* [10gen](http://www.10gen.com/)
* [courses of 10gen](https://education.10gen.com/courses)
* [PyMongo](http://api.mongodb.org/python/current/)

---
layout: post
title: Web.py Introduction
category : Python
tags : [Python, Libs, Web]
---

##The web.py Philosophy

The web.py slogan is: "Think about the ideal way to write a web app. Write the code to make it happen."

This is literally how I developed web.py. I wrote a web application in Python just imagining how I wanted the API to be. It started with `import web`, of course, and then had a place to define URLs, simple functions for GET and POST, a thing to deal with input variables and so on. Once the code looked right to me, I did whatever it took to make it execute without changing the application code -- the result was web.py.

In response to someone complaining about web.py having "yet another template language", [I wrote a bit more about my philosophy](http://groups.google.com/group/webpy/msg/f266701d97e7ceb1):

You don't have to use it -- each part of web.py is completely separate from the others. But you're right, it is "yet another template language". And I'm not going to apologize for it.

The goal of web.py is to build the ideal way to make web apps. If reinventing old things with only small differences were necessary to achieve this goal, I would defend reinventing them. The difference between the ideal way and the almost-ideal way is, as Mark Twain suggested, the difference between the lightning and the lightning bug.

But these aren't just small differences. Instead of exposing Python objects, web.py allows you to build HTTP responses. Instead of trying to make the database look like an object, web.py makes the database easier to use. And instead of coming up with yet another way to write HTML, the web.py template system tries to bring Python into HTML. Not many other people are really trying to do that.

You can disagree that these ways are better and say why. But simply criticizing them for being different is a waste of time. Yes, they are different. That's the whole point.

##Hello World
	
	import web
	        
	urls = (
	    '/(.*)', 'hello'
	)
	app = web.application(urls, globals())
	
	class hello:        
	    def GET(self, name):
	        if not name: 
	            name = 'World'
	        return 'Hello, ' + name + '!'
	
	if __name__ == "__main__":
	    app.run()

##Reference

* [webpy.org](http://webpy.org/)
* [webpy.org on github](https://github.com/webpy/webpy.github.com)
* [web.py使用flup lighttpd优化过程](http://timyang.net/python/python-webpy-lighttpd/)
* [Pythonic Web应用平台对比](http://wiki.woodpecker.org.cn/moin/PyWebFrameVs)

---
layout: post
title: The Zen of Python
category : Python
tags : [Python, Easter]
---

Part of Python's documentation are the PEPs, or "Python Enhancement Proposals." PEP 20, "The Zen of Python," is something of an [easter egg](http://en.wikipedia.org/wiki/Easter_egg_(media)) hidden in the interpreter (you may have caught us quoting from it in instructional text or hints)

	[dylan@www ~]$ python
	Python 2.6.6 (r266:84292, Sep 11 2012, 08:28:27) 
	[GCC 4.4.6 20120305 (Red Hat 4.4.6-4)] on linux2
	Type "help", "copyright", "credits" or "license" for more information.
	>>> import this
	The Zen of Python, by Tim Peters
	
	Beautiful is better than ugly.
	Explicit is better than implicit.
	Simple is better than complex.
	Complex is better than complicated.
	Flat is better than nested.
	Sparse is better than dense.
	Readability counts.
	Special cases aren't special enough to break the rules.
	Although practicality beats purity.
	Errors should never pass silently.
	Unless explicitly silenced.
	In the face of ambiguity, refuse the temptation to guess.
	There should be one-- and preferably only one --obvious way to do it.
	Although that way may not be obvious at first unless you're Dutch.
	Now is better than never.
	Although never is often better than *right* now.
	If the implementation is hard to explain, it's a bad idea.
	If the implementation is easy to explain, it may be a good idea.
	Namespaces are one honking great idea -- let's do more of those!
	>>>


##Reference

* [Codecademy](http://www.codecademy.com/courses/python-beginner-c7VZg/0#!/exercises/0)

* [PythonZen](http://wiki.woodpecker.org.cn/moin/PythonZen)

##MovableType 安装记录

安装环境

* 博客：MTOS-4.38-en
* 服务器：Linux dev.egolife.com 2.6.32-71.el6.i686 GNU/Linux
* 数据库： MySQL 5.1.47
* Web服务器: Apache/2.2.15 (CentOS) Server at localhost Port 80

##安装步骤

解压

	[root@dev]# unzip MTOS-4.38-en.zip /var/www/cgi-bin

创建cgi-bin软连接

	[root@dev]# ln -s /var/www/cgi-bin/MTOS-4.38-en /var/www/cgi-bin/mt
	[root@dev]# ll /var/www/cgi-bin/
	total 4
	lrwxrwxrwx 1 root root 13 Sep 22 23:57 mt -> MTOS-4.38-en/
	drwxr-xr-x 13 root root 4096 Sep 23 00:03 MTOS-4.38-en

创建静态文件软连接

	[root@dev]# ln -s /var/www/cgi-bin/MTOS-4.38-en/mt-static /var/www/html/mt-static
	[root@dev]# ll /var/www/html/
	total 0
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/

权限更改

	[root@dev]# chmod 777 /var/www/cgi-bin/MTOS-4.38-en/mt-static/support
	[root@dev]# chmod 777 /var/www/cgi-bin/MTOS-4.38-en/themes

##配置文件

主要配置Movable Type的cgi-bin、静态文件的路径，数据库连接认证信息。

查看当前主机名

	[root@dev mt]# hostname
	dev.egolife.com

重命名配置文件

	[root@dev]# cd /var/www/cgi-bin/MTOS-4.38-en
	[root@dev mt]# cp mt-config.cgi-original mt-config.cgi

编辑配置文件，配置数据库

	[root@dev mt]# vim mt-config.cgi
	CGIPath http://dev.egolife.com/cgi-bin/mt/
	StaticWebPath http://dev.egolife.com/mt-static/
	ObjectDriver DBI::mysql
	Database xxxxxxxxx
	DBUser xxxxxxxxx
	DBPassword xxxxxxxxx
	DBHost localhost
	EmailAddressMain dylanninin@gmail.com
	
	#####################################################################

##创建MT数据库、用户并授权

	[root@dev]# mysql -uroot -p
	mysql> create database mt character utf8;
	mysql> create user mt;
	mysql> grant all on mt.* to xxxxxxxxx@'localhost' identified by 'xxxxxxxxx';

##启动web服务器

	[root@dev mt]# apachectl start

##第一次访问

使用浏览器打开以下url:

	http://dev.egolife.com/cgi-bin/mt/mt.cgi

此时出现异常，信息如下：

	Forbidden
	
	You don't have permission to access /cgi-bin/mt/mt.cgi on this server.
	Apache/2.2.15 (CentOS) Server at dev.egolife.com Port 80

查看httpd日志信息

	[root@dev]# less /var/log/httpd/error_log
	[Sun Sep 23 00:11:31 2012] [error] [client 192.168.136.1] Directory index forbidden by Options directive: /var/www/html/
	[Sun Sep 23 00:11:39 2012] [error] [client 192.168.136.1] Symbolic link not allowed or link target not accessible: /var/www/cgi-bin/mt
	[Sun Sep 23 00:13:44 2012] [error] [client 127.0.0.1] File does not exist: /var/www/html/server-status

##主要错误信息

###不允许mt符号连接
更改`httpd.conf`中`/var/www/cgi-bin`的配置，启用符号链接，如下:
	
	581 #
	582 <Directory "/var/www/cgi-bin">
	583 AllowOverride None
	584 # Options None
	585 Options FollowSymLinks 
	586 Order allow,deny
	587 Allow from all
	588 </Directory>

重启apache

	[root@dev httpd]# apachectl restart

再次访问，则可以正常浏览。

###文件server-status不存在
	
	[root@dev httpd]# ll /var/www/html/
	total 0
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/
	[root@dev]# mkdir /var/www/html/server-status
	[root@dev]# chown apache:apache /var/www/html/server-status/
	[root@dev]# ll /var/www/html/
	total 4
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/
	drwxr-xr-x 2 apache apache 4096 Sep 23 00:31 server-status

访问时，已经显示Movable Type配置页面，但出现新的错误提示:
	
	An error occurred
	Can't connect to data source '' because I can't work out what driver to use (it doesn't seem to contain a 'dbi:driver:' prefix and the DBI_DRIVER env var is not set)

可能是因为没有安装DBI扩展库，但查看rpm包时已经安装:

	[root@dev httpd]# rpm -qa | grep DBI
	perl-DBI-1.609-4.el6.i686
	perl-DBIx-Simple-1.32-3.el6.noarch

有可能是前面的`mt-config.cgi`配置出现问题:

	27 ##### MYSQL #####
	28 ObjectDriver DBI::mysql
	29 Database mt
	30 DBUser mt
	31 DBPassword 000000
	32 DBHost localhost
	33 EmailAddressMain dylanninin@gmail.com
	34 
	35 ##### POSTGRESQL #####
	36 #ObjectDriver DBI::postgres
	37 #Database DATABASE_NAME
	38 #DBUser DATABASE_USERNAME
	39 #DBPassword DATABASE_PASSWORD
	40 #DBHost localhost
	41 
	42 ##### SQLITE #####
	43 #ObjectDriver DBI::sqlite
	44 #Database /path/to/sqlite/database/file

发现更改DB配置时，没有将PostgreSQL和SQLite的配置注释掉，注释掉后重启Apache，则可以正常配置。

##创建博客

创建博客时，出现异常。主要信息如下:

	Blog Name:
	My First Blog
	Blog URL
	http://dev.egolife.com/blog/
	Publishing Path
	/var/www/html/blog

错误信息：

	In order to properly publish your blog, you must provide Movable Type with your blog's URL and the path on the filesystem where its files should be published.
	--The path provided below is not writable.
	Blog Name
	Blog URL
	Publishing Path
	Your 'Publishing Path' is the path on your web server's file system where Movable Type will publish all the files for your blog. Your web server must have write access to this directory.

创建目录

	[root@dev html]# cd /var/www/html
	[root@dev html]# mkdir blog
	[root@dev html]# chown apache:apache blog/
	[root@dev html]# ll
	total 8
	drwxr-xr-x 2 apache apache 4096 Sep 23 00:46 blog

到此，初步安装MovableType成功。
##MovableType 安装记录（二）

接着[上一篇](/blog/2012/09/29/mt_installation.html)继续配置Movable Type。

##Apache配置

###配置站点首页

打开`http://dev.egolife.com`时，默认显示为Apache的测试页面`。当访问`http://dev.egolife.com/blog`才会显示博客首页。

取消显示Apache测试页面,更改配置`/etc/httpd/conf.d/welcome.conf`,注释掉`LocationMatch`配置：
	
	[root@dev html]# vim /etc/httpd/conf.d/welcome.conf
	1 #.
	2 # This configuration file enables the default "Welcome"
	3 # page if there is no default index page present for
	4 # the root URL. To disable the Welcome page, comment
	5 # out all the lines below.
	6 #
	7 #<LocationMatch "^/+$">
	8 # Options -Indexes
	9 # ErrorDocument 403 /error/noindex.html
	10 #</LocationMatch>

运行`service http reload`命令，重新加载配置后，直接访问主机，则出现的是`/var/www/html`的页面列表信息。

###禁止列表文件
禁止列表信息，需要更改配置`/etc/httpd/conf/httpd.conf`：

	[root@dev html]# vim /etc/httpd/conf/httpd.conf
	317 <Directory "/var/www/html">
	318 
	319 #
	320 # Possible values for the Options directive are "None", "All",
	321 # or any combination of:
	322 # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews
	323 #
	324 # Note that "MultiViews" must be named *explicitly* --- "Options All"
	325 # doesn't give it to you.
	326 #
	327 # The Options directive is both complicated and important. Please see
	328 # http://httpd.apache.org/docs/2.2/mod/core.html#options
	329 # for more information.
	330 #
	331 # Options Indexes FollowSymLinks
	332 Options FollowSymLinks
	333 
	334 #
	335 # AllowOverride controls what directives may be placed in .htaccess files.
	336 # It can be "All", "None", or any combination of the keywords:
	337 # Options FileInfo AuthConfig Limit
	338 #
	339 AllowOverride None
	340 
	341 #
	342 # Controls who can get stuff from this server.
	343 #
	344 Order allow,deny
	345 Allow from all
	346 
	347 </Directory>

更改配置后，重新加载配置文件，则显示禁止访问，此时可以自定义显示的页面，这里采用软链接的方式。

###设置软连接

	[root@dev html]# ln -s /var/www/html/blog/index.html /var/www/html/index.html
	[root@dev html]# ll
	total 12
	drwxr-xr-x 7 apache apache 4096 Sep 23 16:05 blog
	-rw-r--r-- 1 apache apache 1406 Sep 23 15:47 favicon.ico
	lrwxrwxrwx 1 root root 29 Sep 23 16:29 index.html -> /var/www/html/blog/index.html
	lrwxrwxrwx 1 root root 40 Sep 22 23:58 mt-static -> /var/www/cgi-bin/MTOS-4.38-en/mt-static/
	drwxr-xr-x 2 apache apache 4096 Sep 23 00:31 server-status

这样访问`http://dev.egolife.com`时，会直接显示博客首页。当然，可以使用URL重写或重定向来实现.

###错误提示页

主要是403,404,500页面的配置，更改`/etc/httpd/conf/httpd.conf`：

	[root@dev html]# vim /etc/httpd/conf/httpd.conf
	833 # Some examples:
	834 #ErrorDocument 500 "The server made a boo boo."
	835 #ErrorDocument 404 /missing.html
	836 #ErrorDocument 404 "/cgi-bin/missing_handler.pl"
	837 #ErrorDocument 402 http://www.example.com/subscription_info.html
	838 #
	839 #2012-09-22|dylanninin@gmail.com| ErrorDocument
	840 ErrorDocument 500 http://dev.egolife.com/errors/500.html
	841 ErrorDocument 404 http://dev.egolife.com/errors/404.html
	842 ErrorDocument 403 http://dev.egolife.com/errors/403.html

###添加错误页面

	[root@dev errors]# pwd
	/var/www/html/errors
	[root@dev errors]# ll
	total 0
	-rw-r--r-- 1 apache apache 0 Sep 23 16:58 403.html
	-rw-r--r-- 1 apache apache 0 Sep 23 16:58 404.html
	-rw-r--r-- 1 apache apache 0 Sep 23 17:00 500.html

以上页面内容待完善!

##参考

* Movable Type：[MTOS文件系统描述](http://www.movabletype.org/documentation/installation/file-system.html)

##Pastime Paradise 

	They've been spending most their lives
	Living in a pastime paradise
	They've been spending most their lives
	Living in a pastime paradise
	They've been wasting most their time
	Glorifying days long gone behind
	They've been wasting most their days
	In remembrance of ignorance oldest praise
	
	Tell me who of them will come to be
	How many of them are you and me
	Dissipation
	Race relations
	Consolation
	Segregation
	Dispensation
	Isolation
	Exploitation
	Mutilation
	Mutations
	Miscreation
	Confirmation...to the evils of the world
	
	They've been spending most their lives
	Living in a future paradise
	They've been spending most their lives
	Living in a future paradise
	They've been looking in their minds
	For the day that sorrows gone from time
	They keep telling of the day
	When the savior of love will come to stay
	
	Tell me who of them will come to be
	How many of them are you and me
	Proclamation
	Of race relations
	Consolation
	Integretion
	Verification
	Of revelations
	Acclamation
	World salvation
	Vibrations
	Stimulation
	Confirmation...to the peace of the world
	
	They've been spending most their lives
	Living in a pastime paradise
	They've been spending most their lives
	Living in a pastime paradise
	They've been spending most their lives
	Living in a future paradise
	They've been spending most their lives
	Living in a future paradise
	We've been spending too much of our lives
	Living in a pastime paradise
	
	Lets start living our lives
	Living for the future paradise
	Praise to our lives
	Living for the future paradise
	Shame to anyone's lives
	Living in the pastime paradise






##创站简介

最近两天集中查阅和比对了一些关于VPS和博客系统的资料，最终确定下来，在2012年9月28日购买了一年[Linode](http://www.linode.com/)的[VPS](http://en.wikipedia.org/wiki/Virtual_private_server)服务（VPS方案是Linode 512），紧接着使用自由软件[Movable Type](http://www.movabletype.org/)在VPS上搭建了博客系统（针对个人使用是免费的），使用MTOS自带的主题、样式，风格比较简单，当然也没有进行汉化。第二天，也就是2012年9月29日，在[Godaddy](http://www.godaddy.com/)申请注册了一枚域名，即[dylanninin.com](http://dylanninin.com)，另外赠送[dylanninin.info](http://dylanninin.info)免费域名。本博客正式使启用的域名是[www.dylanninin.com](http://www.dylanninin.com)，同时，[www.dylanninin.info](http://www.dylanninin.info)也指向本站。

##维护计划

目前本站刚刚起步，仅仅搭建起了一个框架，有很多关于Web Site的知识需要学习和应用，今年中秋十一有八天长假，正好可以专心折腾下，希望能够在假期结束前能完成一个博客系统的雏形。

具体说来，大概有以下内容需要在这段时间学习和完成：

* Movable Type日常管理，如博客、页面、图片等。
* Apache 服务器基本配置，如安全性，日志等。
* Linux主机安全，如防火墙，ssh等。
* UI设计，如页面框架，站点信息维护，配色方案等。
* 错误页面，如404,500等。
* 备份和恢复，如数据库，博客等。

在学习过程中，会陆续整理出一些笔记发在博客上，希望和大家多分享多交流。

##中秋佳节

今天正好中秋佳节，顺便送上祝福，希望人月两圆，幸福快乐。


---
layout: post
title: DataPump Introduction
category : Oracle
tags : [Oracle, Database, DBA]
---

Oracle Data Pump is made up of three distinct parts:

	The command-line clients, expdp and impdp
	The DBMS_DATAPUMP PL/SQL package (also known as the Data Pump API)
	The DBMS_METADATA PL/SQL package (also known as the Metadata API)

The Data Pump clients, expdp and impdp, invoke the Data Pump Export utility and Data Pump Import utility, respectively.

The `expdp` and `impdp` clients use the procedures provided in the `DBMS_DATAPUMP` PL/SQL package to execute export and import commands, using the parameters entered at the command line. These parameters enable the exporting and importing of data and metadata for a complete database or for subsets of a database.

When metadata is moved, Data Pump uses functionality provided by the `DBMS_ METADATA` PL/SQL package. The `DBMS_METADATA` package provides a centralized facility for the extraction, manipulation, and re-creation of dictionary metadata.

The `DBMS_DATAPUMP`  and `DBMS_METADATA` PL/SQL packages can be used independently of the Data Pump clients.

![Oracle Data Pump Architecture](http://docs.oracle.com/cd/E11882_01/server.112/e10713/img/cncpt261.gif)

##Example

同一个数据库中克隆schema，本例中为克隆用户OA到OADev(以下操作均采用SYS用户)。

###1.新建用户
	
	create user oadev identified by "oadev";

###2.表空间和数据文件

单独创建表空间，数据文件

	create temporary tablespace oadev_tmp tempfile '/db/oracle/oradata/DBTEST/oadev_tmp01.dbf' size 1000M;
	
	create tablespace oadev_data datafile '/db/oracle/oradata/DBTEST/oadev_data01.dbf' size 1000M;

	create tablespace oadev_idx datafile '/db/oracle/oradata/DBTEST/oadev_idx01.dbf' size 1000M;

设置默认表空间


	SELECT 'alter user ' || 'OADEV' || ' default tablespace ' ||
		DU.DEFAULT_TABLESPACE || ' temporary tablespace ' ||
		DU.TEMPORARY_TABLESPACE || ' ;'
	FROM DBA_USERS DU
	WHERE DU.USERNAME IN ('OA');

	
	alter user OADEV default tablespace OADEV_DATA temporary tablespace OADEV_TMP;

验证
	
	SELECT * FROM DBA_USERS DU WHERE DU.USERNAME IN ('OA', 'OADEV');

####3.权限设置

查询已有系统权限

	SELECT 'grant ' || DSP.PRIVILEGE || ' to DEV;'
	  FROM DBA_SYS_PRIVS DSP
	 WHERE DSP.GRANTEE IN ('OA')
	 ORDER BY 1;

执行以上SQL输出的结果

	grant CREATE ANY PROCEDURE to oadev;
	grant CREATE ANY VIEW to oadev;
	grant DEBUG ANY PROCEDURE to oadev;
	grant DEBUG CONNECT SESSION to oadev;
	grant UNLIMITED TABLESPACE to oadev;

验证

	SELECT *
	  FROM DBA_SYS_PRIVS DSP
	 WHERE DSP.GRANTEE IN ('OA', 'OADEV')
	 ORDER BY 1;

###4.使用expdp/impdp准备工作

在OS上创建expdp/impdp目录:/dba/exp，并分配Oracle读写权限

创建Directory

	CREATE DIRECTORY EXPDP AS '/dba/exp';

授权
	
	GRANT READ, WRITE ON DIRECTORY EXPDP TO OA, OADEV;

验证
	
	SELECT * FROM DBA_DIRECTORIES;
	
	SELECT * FROM ALL_TAB_PRIVS ATP
	 WHERE ATP.TABLE_NAME = 'EXPDP';

###5.使用expdp导出oa数据

运行expdp命令，导出schema

	[oracle@oradb exp]$ expdp  oa/oatest directory=expdp dumpfile=oa_20121227.dmp logfile=oa_20121227.log parallel=2

###6.使用impdp导入5中导出的数据到oadev

运行impdp命令，导入数据，注意重新映射schema，tablespace

	[oracle@oradb exp]$ impdp oadev/oadev directory=expdp dumpfile=oa_20121227.dmp logfile=oadev_20121227.log remap_schema=oa:oadev remap_tablespace=oa_data:oadev_data,oa_idx:oadev_idx

###7.验证

查看导出、导入日志，有异常则进行检查即可。
使用oadev登录，更改应用数据库的用户即可，启动测试。

	SELECT 'All Objects owned by OA: ' || COUNT(1)
	  FROM ALL_OBJECTS O
	 WHERE O.OWNER = 'OA'
	UNION
	SELECT 'All Objects owned by OADev: ' || COUNT(1)
	  FROM ALL_OBJECTS O
	 WHERE O.OWNER = 'OADEV';


###8.Utility

进行data pump测试时，可能会反复重建用户，这里可以根据已有用户生成新用户的创建脚本，包括角色、系统权限、目录对象权限。

	--generate create user sq
	SELECT ' create user ' || DU.USERNAME || ' identified by "' || DU.USERNAME || '"' ||
	       ' profile ' || DU.PROFILE || ' default tablespace ' ||
	       DU.DEFAULT_TABLESPACE || ' temporary tablespace ' ||
	       DU.TEMPORARY_TABLESPACE
	  FROM DBA_USERS DU
	 WHERE DU.USERNAME = upper('&username');


	--generate grant roles and system or objects privileges sql
	SELECT 'grant ' || DRP.GRANTED_ROLE || ' to ' || '&new_grantee' || ' ;' -- roles
	  FROM DBA_ROLE_PRIVS DRP
	 WHERE DRP.GRANTEE = upper('&grantee')
	UNION ALL
	SELECT 'grant ' || DSP.PRIVILEGE || ' to ' || '&new_grantee' || ' ;' --sys privileges
	  FROM DBA_SYS_PRIVS DSP
	 WHERE DSP.GRANTEE = upper('&grantee')
	UNION ALL
	SELECT 'grant ' || ATP.PRIVILEGE || ' on directory ' || --directory privileges
	       ATP.TABLE_NAME || ' to ' || '&new_grantee' || ' ;'
	  FROM ALL_TAB_PRIVS ATP
	 WHERE ATP.TABLE_NAME = upper('&directory')
	   AND ATP.GRANTEE = upper('&grantee');

##expdp command line

[oracle@oradb exp]$ expdp help=Y
	
	Export: Release 11.2.0.1.0 - Production on Fri Jan 4 10:44:09 2013
	Copyright (c) 1982, 2009, Oracle and/or its affiliates.  All rights reserved.
	
	The Data Pump export utility provides a mechanism for transferring data objects
	between Oracle databases. The utility is invoked with the following command:
	
	   Example: expdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp
	
	You can control how Export runs by entering the 'expdp' command followed
	by various parameters. To specify parameters, you use keywords:
	
	   Format:  expdp KEYWORD=value or KEYWORD=(value1,value2,...,valueN)
	   Example: expdp scott/tiger DUMPFILE=scott.dmp DIRECTORY=dmpdir SCHEMAS=scott
	               or TABLES=(T1:P1,T1:P2), if T1 is partitioned table
	USERID must be the first parameter on the command line.

##impdp command line

[oracle@oradb exp]$ impdp help=Y

	Import: Release 11.2.0.1.0 - Production on Fri Jan 4 11:04:11 2013
	Copyright (c) 1982, 2009, Oracle and/or its affiliates.  All rights reserved.
	
	The Data Pump Import utility provides a mechanism for transferring data objects
	between Oracle databases. The utility is invoked with the following command:
	
	     Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp
	
	You can control how Import runs by entering the 'impdp' command followed
	by various parameters. To specify parameters, you use keywords:
	
	     Format:  impdp KEYWORD=value or KEYWORD=(value1,value2,...,valueN)
	     Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp
	USERID must be the first parameter on the command line.

##Reference

* [tianlesoftware](http://blog.csdn.net/tianlesoftware/article/details/4674224)
* Oracle Database Utilities

---
layout: post
title: Drop User in Oracle Database
category : Oracle
tags : [Oracle, Database, DBA]
---

Drop User in Oracle Database

Use the `DROP USER` statement to remove a database user and optionally remove the 
user's objects. 

When you drop a user, Oracle Database also purges all of that user's schema 
objects from the recycle bin.

##Prerequisites 

You must have the DROP USER system privilege. 

##syntax

	DROP USER user [cascade];

##semantics

* user

	Specify the user to be dropped. Oracle Database does not drop users whose 
	schemas contain objects unless you specify CASCADE or unless you first 
	explicitly drop the user's objects.

* CASCADE 

	Specify CASCADE to drop all objects in the user's schema before dropping the 
	user. You must specify this clause to drop a user whose schema contains any 
	objects. 
	
	■ If the user's schema contains tables, then Oracle Database drops the tables 
	and automatically drops any referential integrity constraints on tables in 
	other schemas that refer to primary and unique keys on these tables. 

	■ If this clause results in tables being dropped, then the database also 
	drops all domain indexes created on columns of those tables and invokes 
	appropriate drop routines. 

	■ Oracle Database invalidates, but does not drop, the following objects in 
	other schemas:

		– Views or synonyms for objects in the dropped user's schema
		– Stored procedures, functions, or packages that query objects in the 
		dropped user's schema

	■ Oracle Database does not drop materialized views in other schemas that are 
	based on tables in the dropped user's schema. However, because the base 
	tables no longer exist, the materialized views in the other schemas can no 
	longer be refreshed.

	■ Oracle Database drops all triggers in the user's schema.

	■ Oracle Database does not drop roles created by the user. 

##Examples

Dropping a Database User: Example   If user Sidney's schema contains no objects, 
then you can drop sidney by issuing the statement:

	DROP USER sidney; 

If Sidney's schema contains objects, then you must use the CASCADE clause to 
drop sidney and the objects:
	
	DROP USER sidney CASCADE; 

##Reference

* Oracle Database Adminstrator

---
layout: post
title: Breathe Still and Freely
category : Life
tags : [Blog, People, Music]
---

2012年12月29号晚，给一个高中同学打电话，聊了会儿天，得知病情恢复得不错，有说有笑，精神状态也不错。那时感觉一切正朝着良好的方向发展，尽管她已经接受治疗近三年。

2013年刚开始的第三天，即2013年1月3号下午，噩耗却突然传来：她走了。打电话给她爸爸时，听到的是作为一个父亲情感突然崩溃的哭泣，谁也无法想象这有多么难以接受。

无论是谁，听闻这些消息，都会有很多感触，而且并非言语所能表达。

这里冒昧引用她男朋友的几段话《会有天使替我爱你》：

* 10年的五一节以后一切都变了，他们的恋情就象小时候看的蓝色生死恋，一切来的是那个的突然，随着病情的一步步恶化，曾经的那个爱笑的，知性的，偶尔又略显风骚的大女孩，被病魔无情的摧残着，还在读书的他面对这样的噩耗，经济上也只能尽点绵薄之力，唯一能做的就是抽出一切可以抽出的时间去静静地守护她。
   
* 动完手术后的她选择了再也不理会他了，可他们各自心里都清楚谁也放不下谁。他通过朋友了解到做完手术的她恢复的还算可以，他也就放下了心。可是就在排异的危险期内，那让人无奈的三分之二复发几率无情地剥夺了她的生命。
       
* 他在2013年1月3日得知这个噩耗的时候，仿佛又回到了98年爷爷去世的那天。他的心很疼，那种熟悉又憎恶的感觉还是出现了。虽然之前已经作好最坏打算的他，听到这个后还是哭了，而且都没有勇气去送她最后一程。请她能够原谅她，因为他会为她在天堂找个天使替他爱她，在那里她将远离病痛，永远都开开心心的。

* 一路走好，对不起，那些实现不了的承诺，拜托天使了。
	
至于我，因表达的障碍，目前也只能听听几首歌，聊以缓解。过些时日我再好好整理下思绪，也算是一种悼念。

没有想到的是，老周的这些歌，听得让人无法忘却。

##沉默如谜的呼吸

	千钧一发的呼吸， 
	水滴石穿的呼吸， 
	蒸汽机粗重的呼吸， 
	玻璃切割玻璃的呼吸。 
	 
		我的疼是肉体的疼， 
		我怕一只肮脏的烟头摁在肺上， 
		我吱吱冒烟缩成一团， 
		又彻底松弛， 
		如崩溃的大坝， 
		任疼痛的洪水泛滥。 
		我怕二十万根生锈的针插遍全身， 
		每一根敏感的神经都被疼痛拨响， 
		折断的竹筷从鼻孔插入脑组织， 
		思索的大脑变成疼痛的蚂蚁窝。
	 
	鱼死网破的呼吸， 
	火焰痉挛的呼吸， 
	刀尖上跳舞的呼吸， 
	彗星般消逝的呼吸。 
	 
		我怕在铁水沸腾的熔炉里永生， 
		在热与疼的颠峰我清醒、我存在， 
		我奢望昏迷和死亡， 
		逆着时间的湍流追寻， 
		我怕温暖过游子的母亲不是起点。
	 
	沉默如鱼的呼吸， 
	沉默如石的呼吸， 
	沉默如睡的呼吸， 
	沉默如谜的呼吸。 
	 
		我怕时间将一切锈蚀， 
		而让追寻者独自锃亮......
	
###悬棺

	悬在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子

	悬棺
	悬棺在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子
	我们就住在云彩上面的棉花里
	我们在棉花里偷偷地喝酒

	踉踉跄跄一脚高山
	踉踉跄跄一脚平原
	踉踉跄跄一脚海洋
	踉踉跄跄一脚沙滩

	我们的身体是一只水桶
	升上去空空荡荡
	落下来装满了水
	升上去空空荡荡
	落下来装满了水

	悬在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子

	悬棺
	悬棺在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子


##山鬼

	有一个无人居住的老屋， 
	孤单的卧在荒野上。 
	它还保留着古老的门和窗， 
	却已没有炊烟和灯光。 

	春草在它的身旁长啊长， 
	那时我还没离开故乡。 
	蟋蟀在它的身旁唱啊唱， 
	那时我刚准备着去远方。 

	有一个无人祭奠的灵魂， 
	独自在荒山间游荡， 
	月光是她洁白的衣裳， 
	却没人为她点一柱香。 

	夜露是她莹莹的泪光， 
	那时爱情正栖息在我心上， 
	辰星是她憔悴的梦想， 
	那时爱人已长眠在他乡。 

	上帝坐在空荡荡的天堂， 
	诗人走在寂寞的世上， 
	时间慢慢的在水底凝固， 
	太阳疲倦的在极地驻足。 

	这时冰山醒来呼唤着生长， 
	这时巨树展翅渴望着飞翔， 
	这时我们离家去流浪， 
	长发宛若战旗在飘扬， 
	俯瞰逝去的悲欢和沧桑， 
	扛着自己的墓碑走遍四方。 


##鱼相忘于江湖

	鱼忘记了沧海， 
	虫忘记了尘埃， 
	神忘记了永恒， 
	人忘记了现在。 

	也是没有人的空山， 
	也是没有鹰的青天， 
	也是没有梦的睡眠， 
	也是没有故事的流年。 

	忘了此地是何地， 
	忘了今昔是何昔。 
	睁开眼睛就亮天， 
	闭上眼睛就黑天。 

	太阳出来，为了生活出去， 
	太阳落了，为了爱情回去。 
	班若波罗揭谛......

##幻觉支撑我们活下去

	那是一片蓝葡萄， 
	挂在戈壁的天尽头， 
	云外有片大草原， 
	有个孩子在放牛。 

	道路死在我身后， 
	离开河床水更自由， 
	为了不断的向前走， 
	我得相信那不是蜃楼。
 
	梦里全是湖水绿洲， 
	醒来满地是跳舞的石头。 
	啊，我的饥渴映红起伏的沙丘， 
	我不要清醒的水， 
	我只要晕眩的酒。 

	清醒的人倒在路旁， 
	幻觉带着我们向前走， 
	大风淘尽了我的衣兜， 
	失明的灵魂更加自由。
 
	我是世界壮丽的伤口， 
	伤口是我身上奔腾的河流。 
	啊，我的饥渴映红起伏的沙丘， 
	我不要清醒的水， 
	我只要晕眩的酒。 
	我不要清醒的水， 
	我只要如梦的酒。 

##参考

* 豆瓣小站  [周云蓬](http://site.douban.com/zhouyunpeng/room/418171/)
* 豆瓣读书  [绿皮火车](http://book.douban.com/subject/10743137/)
* 豆瓣读书  [春天责备](http://book.douban.com/subject/5333599/)

---
layout: post
title: Aaron Swartz Commits Suidice
category : Miscellanies
tags : [Python, People]
---

学习Python的第一个Web框架就是[web.py](http://webpy.org/)，没想到它的作者Aaron Swartz已经于2013年1月11日自杀。

2012年年末计划用Python将此博客重写，其中Web框架经过一翻[选型](http://wiki.woodpecker.org.cn/moin/PyWebFrameList)，便确定使用[web.py](http://webpy.org/)。本打算利用周末时间，希望能在年前完成，但因迫近年关诸事繁忙，现在才刚刚搭建起基本的项目框架。真是诸行无常，现在唯一能纪念的，就是作为一个程序员，学习吸收Aaron Swartz的思想，尽快的完成这个项目，并发布到[github](http://github.com/)，也算是对web.py的一种推介，对Aaron Swartz的一种追思。

[@程序员杂志](http://e.weibo.com/programmermag)：
震惊！Aaron Swartz自杀身亡，年仅26。他是Reddit的联合创始人，web.py的设计者，14岁参与创造了RSS 1.0规范，又与John Gruber共同设计了Markdown。2011年曾因下载480万篇JSTOR学术论文而被捕。[http://t.cn/zjrzsyb](http://t.cn/zjrzsyb)

##News

Aaron Swartz commits suicide By Anne Cai -- NEWS EDITOR; UPDATED AT 2:15 A.M. 1/12/13

Computer activist Aaron H. Swartz committed suicide in New York City yesterday, Jan. 11, according to his uncle, Michael Wolf, in a comment to The Tech. Swartz was 26.

"The tragic and heartbreaking information you received is, regrettably, true," confirmed Swartz' attorney, Elliot R. Peters of Kecker and Van Nest, in an email to The Tech.

Swartz was indicted in July 2011 by a federal grand jury for allegedly mass downloading documents from the JSTOR online journal archive with the intent to distribute them. He subsequently moved to Brooklyn, New York, where he then worked for Avaaz Foundation, a nonprofit "global web movement to bring people-powered politics to decision-making everywhere." Swartz appeared in court on Sept. 24, 2012 and pleaded not guilty.

The accomplished Swartz co-authored the now widely-used RSS 1.0 specification at age 14, was one of the three co-owners of the popular social news site Reddit, and completed a fellowship at Harvard's Ethics Center Lab on Institutional Corruption. In 2010, he founded [DemandProgress.org](http://demandprogress.org/), a "campaign against the Internet censorship bills SOPA/PIPA."

##About Aaron Swartz

Aaron Swartz is the founder of [Demand Progress](http://demandprogress.org), which launched the campaign against the Internet censorship bills (SOPA/PIPA) and now has over a million members. He is also a Contributing Editor to [The Baffler](http://thebaffler.com/) and on the Council of Advisors to The Rules.

He is a frequent television commentator and the author of numerous articles on a variety of topics, especially the corrupting influence of big money on institutions including [nonprofits](http://aaronsw.jottit.com/rachelcarson), [the media](http://www.aaronsw.com/weblog/newobjectivity), [politics](http://crookedtimber.org/2009/05/01/political-entrepreneurs-and-lunatics-with-money/), and public opinion. From 2010-11, he researched these topics as a Fellow at the Harvard Ethics Center Lab on Institutional Corruption. He also served on the board of Change Congress, a good government nonprofit.

He has also developed the site [theinfo.org](http://theinfo.org/). His landmark analysis of Wikipedia, [Who Writes Wikipedia?](http://www.aaronsw.com/weblog/whowriteswikipedia), has been widely cited.[ Working with Web inventor Tim Berners-Lee at MIT](http://www.w3.org/2001/sw/RDFCore/members.html), he helped [develop](https://tools.ietf.org/html/rfc3870) and [popularize](http://logicerror.com/semanticWeb-long) standards for sharing data on the Web. He also coauthored the [RSS 1.0 specification](http://purl.org/rss/1.0/), now widely used for publishing news stories.

His piece with photographer Taryn Simon, Image Atlas (2012), is has [been featured](http://www.newmuseum.org/exhibitions/view/taryn-simon-cultural-differences) in the New Museum. In 2007, he led the development of the nonprofit Open Library, an ambitious project to collect information about every book ever published. He also cofounded the online news site Reddit, where he released as free software the web framework he developed, [web.py](http://webpy.org/).

##Reference

* MIT News: [Aaron Swartz commits suicide](http://tech.mit.edu/V132/N61/swartz.html?comments#comments)
* Site: [Aaron Swartz](http://www.aaronsw.com/)

---
layout: post
title: Oracle EBS Adadmin Scripts
category : Oracle
tags : [Oracle, DBA, EBS]
---

## Admin Scripts for instance

location: `<RDBMS ORACLE_HOME>/appsutil/scripts/dbname_node`

	Control 		Functionality
	addbctl.sh 		Control database server
	addlnctl.sh 	Control Oracle Net listener for the database server

## RDBMS ORACLE_HOME Install scripts

location: `<RDBMS ORACLE_HOME>/appsutil/install/dbname_node`

	Install   		Functionality
	adsvdlsn.sh	 	Start Oracle Net listener during installation
	adcrdb.sh	 	Start database and create database control files
	addbprf.sh	 	Set profile option values
	adsvdcnv.sh	 	Perform character set conversion and licensing tasks
	adsvdb.sh	 	Start database during installation

##COMMON_TOP Control scripts 

location: `<COMMON_TOP>/admin/scripts/dbname_node)`

	Install Script  Functionality
	adalnctl.sh	 	Control Oracle Net8 listener for Applications services	 
					All application tier server nodes
	adstrtal.sh	 	Start all Applications server processes	 
					All application tier server nodes
	adstpall.sh	 	Stop all Applications server processes	 
					All application tier server nodes
	adfrmctl.sh	 	Control Forms server	 
					Forms server node
	adfmcctl.sh	 	Control Forms Metrics Client	 
					Forms server node
	adfmsctl.sh	 	Control Forms Metrics Server	 
					HTTP server node
	adtcfctl.sh	 	Control TCF SocketServer	 
					Concurrent processing server node
	adcmctl.sh	 	Control Concurrent managers	 
					Concurrent processing server node
	adrepctl.sh	 	Control Reports server	 
					Concurrent processing server node
	adapcctl.sh	 	Control Apache processes	 
					HTTP server node
	jtffmctl.sh	 	Control Oracle fulfillment server	 
					HTTP server node

##COMMON_TOP Install scripts 

location: `<COMMON_TOP>/admin/install/dbname_node`

	Install		 	Functionality
	adsvalsn.sh	 	Start Net8 listener processes for Applications services	 
					All application tier server nodes
	adsvfrm.sh	 	Start Forms server during install	 
					Forms server node
	adsvfmc.sh	 	Start Forms server during install	 
					Forms server node
	adsvfms.sh	 	Start Forms metric server during install	 
					HTTP server node
	adsvcm.sh	 	Start Concurrent manager during install	 
					Concurrent processing server node
	adsvtcf.sh	 	Start TCF server during install	 
					Concurrent processing server node
	adsvrep.sh	 	Start Reports server during install	 
					Concurrent processing server node
	adsvapc.sh	 	Start Apache server during install	 
					HTTP server node
	jtfsvfm.sh	 	Start Fulfillment server during install	 
					HTTP server node


---
layout: post
title: A Case of NTP Exception
category : Linux
tags : [NTP, Linux, Exception]
---

最近虚拟化服务器出现时钟同步异常：定时同步，但发现时间比个人电脑时间相差10几个小 时。看了下鸟哥的私房菜，觉得没太大问题，于是决定测试NTP服务以排查问题。因网络限 制，无法连接到互联网，就采用其中的一台虚拟机作为NTP服务器，并以本机时间为准。测 试后才知道原来是系统安装时没有选择时区（默认为美国纽约），而使用的NTP服务器时区 为中国，所以导致虚拟化服务器时间相差13个小时。

从这次经历，再次得到一些经验教训：

* 安装Linux时注意选择合适的时区;
* 仔细阅读官方文档，这些才是最权威的文档;
* 结合理论进行测试，绝知此事要躬行;
* 要掌握一个理论，不是一朝一夕之事;

##搭建NTP服务器

###主要配置

启用本机时钟作为NTP基准时间，主要需取消注释`server	127.127.1.0	# local clock`和`fudge	127.127.1.0 stratum 10`。

完整配置如下：

	[root@server ~]# cat /etc/ntp.conf
	# For more information about this file, see the man pages
	# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).
	
	driftfile /var/lib/ntp/drift
	
	# Permit time synchronization with our time source, but do not
	# permit the source to query or modify the service on this system.
	#restrict default kod nomodify notrap nopeer noquery
	#restrict -6 default kod nomodify notrap nopeer noquery
	
	# Permit all access over the loopback interface.  This could
	# be tightened as well, but to do so would effect some of
	# the administrative functions.
	
	# Hosts on local network are less restricted.
	restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
	
	# Use public servers from the pool.ntp.org project.
	# Please consider joining the pool (http://www.pool.ntp.org/join.html).
	#server 0.centos.pool.ntp.org
	#server 1.centos.pool.ntp.org
	#server 2.centos.pool.ntp.org
	
	#broadcast 192.168.1.255 autokey	# broadcast server
	#broadcastclient			# broadcast client
	#broadcast 224.0.1.1 autokey		# multicast server
	#multicastclient 224.0.1.1		# multicast client
	#manycastserver 239.255.254.254		# manycast server
	#manycastclient 239.255.254.254 autokey # manycast client
	
	# Undisciplined Local Clock. This is a fake driver intended for backup
	# and when no outside source of synchronized time is available. 
	server	127.127.1.0	# local clock
	fudge	127.127.1.0 stratum 10	
	
	# Enable public key cryptography.
	#crypto
	
	includefile /etc/ntp/crypto/pw
	
	# Key file containing the keys and key identifiers used when operating
	# with symmetric key cryptography. 
	keys /etc/ntp/keys
	
	# Specify the key identifiers which are trusted.
	#trustedkey 4 8 42
	
	# Specify the key identifier to use with the ntpdc utility.
	#requestkey 8
	
	# Specify the key identifier to use with the ntpq utility.
	#controlkey 8
	
	# Enable writing of statistics records.
	#statistics clockstats cryptostats loopstats peerstats

###开启服务

开启ntp服务

	[root@server ~]# service ntpd restart
	Shutting down ntpd:                                        [  OK  ]
	Starting ntpd:                                             [  OK  ]

注意：
	
每次重启NTP服务之后大约要5分钟客户端才能建立正常的NTP通讯连接，否则在执行ntpdate时候将返回：

	[root@oradb ~]# ntpdate -d 192.168.1.162
	... ...
	192.168.1.162: Server dropped: strata too high
	server 192.168.1.162, port 123
	stratum 16, precision -23, leap 11, trust 000
	... ...
	8 Jan 19:28:07 ntpdate[17795]: no server suitable for synchronization found

在ntp客户端用`ntpdate –d`查看，发现有“Server dropped: strata too high”的错误，并且显示“stratum 16”。而正常情况下stratum这个值得范围是“0~15”。

这是因为NTP server还没有和其自身或者它的server同步上。详见[NTP常见错误](http://www.blogjava.net/spray/archive/2008/07/10/213964.html)。

##同步测试

###1. NTP服务器(192.168.1.162)
当前时间：

	[root@server ~]# date
	Tue Jan  8 19:32:42 EST 2013
 
###2. NTP客户端时间(192.168.1.163)：
当前时间：

	[root@oradb ~]# date
	Tue Jan  8 19:33:37 EST 2013

与192.168.1.162同步：

	[root@oradb ~]# ntpdate 192.168.1.162
	 8 Jan 19:33:51 ntpdate[24174]: adjust time server 192.168.1.162 offset -0.000002 sec

与172.31.1.1同步，时间滞后，大概是13个小时：

	[root@oradb ~]# ntpdate 172.31.1.1
	 8 Jan 06:34:10 ntpdate[24270]: step time server 172.31.1.1 offset -46801.549132 sec

在与172.31.1.1同步时，出现时钟同步异常，因与192.168.1.162同步的时钟相差太大。导致这种情况大致推测有两个原因：1）172.31.1.1上时间不对；2）192.168.1.162、163时间不对。不过这两点很快都被排除了，经过同事提点，查看[鸟哥的私房菜](http://linux.vbird.org/linux_server/0440ntp.php)，对比[世界时差表](http://www.timedate.cn/time/time_diff.asp)，才知道是192.168.1.162、163系统的时区不正确。由于系统安装时没有选择时区（默认为美国纽约），与中国上海大概相差13个小时。
 
###3.调整时区

系统当前时区

	[root@server ~]# cat /etc/sysconfig/clock.20130109 
	# The time zone of the system is defined by the contents of /etc/localtime.
	# This file is only for evaluation by system-config-date, do not rely on its
	# contents elsewhere.
	ZONE="America/New York"

调整后系统时区

	[root@server ~]# cat /etc/sysconfig/clock
	# The time zone of the system is defined by the contents of /etc/localtime.
	# This file is only for evaluation by system-config-date, do not rely on its
	# contents elsewhere.
	ZONE="Asia/Shanghai"
 
调整后时，再次与172.31.1.1同步：

	[root@oradb ~]# date
	Wed Jan  9 21:59:23 CST 2013
	[root@oradb ~]# cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 
	cp: overwrite `/etc/localtime'? y
	[root@oradb ~]# clock -w
	[root@oradb ~]# date
	Wed Jan  9 21:59:53 CST 2013
	[root@oradb ~]# ntpdate 172.31.1.1
	 9 Jan 09:04:46 ntpdate[5898]: step time server 172.31.1.1 offset -46801.989143 sec
 
设置定时同步

	[root@server ~]# crontab -l
	00 07 * * *  /usr/sbin/ntpdate 172.31.1.1 && /sbin/hwclock -w

##鸟哥的讲解

例題： 

假設你的筆記型電腦安裝 CentOS 這套系統，而且選擇的時區為台灣。現在，你將有一個月的時間要出差到美國的紐約去， 你會帶著這個筆電，那麼到了美國之後，時間會不一致啊！你該如何手動的調整時間參數呢？

答： 

因為時區資料檔在 `/usr/share/zoneinfo` 內，在該目錄內會找到 `/usr/share/zoneinfo/America/New_York` 這個時區檔。 而時區設定檔在 `/etc/sysconfig/clock` ，且目前的時間格式在 `/etc/localtime` ，所以你應該這樣做： 

	[root@www ~]# date
	Thu Jul 28 15:08:39 CST 2011  <==重點是 CST 這個時區喔！
	
	[root@www ~]# vim /etc/sysconfig/clock
	ZONE="America/New_York"       <==改的是這裡啦！
	
	[root@www ~]# cp /usr/share/zoneinfo/America/New_York /etc/localtime
	[root@www ~]# date
	Thu Jul 28 03:09:21 EDT 2011  <==時區與時間都改變了！

 
這個範例做完之後，記得將這兩個檔案改回來！不然以後你的時間都是美國時間啦！ 

##参考
* [鸟哥的私房菜：时间服务器](http://linux.vbird.org/linux_server/0440ntp.php)
* [NTP Project](http://www.eecis.udel.edu/~mills/ntp.html)
* [NTP常见错误](http://www.blogjava.net/spray/archive/2008/07/10/213964.html)

---
layout: post
title: Complex Password Utility
category : Miscellanies
tags : [Python, Utilities]
---

很早以前就在使用[Lastpass](https://lastpass.com)进行密码管理，如自动生成复杂密码，Web页面自动填写和登陆；并且在去年开始付费（目前仅支付一年，主要是因为Android移动平台上lastpass免费版使用时间有限，仅16天）。

自[CSDN明文密码泄露](http://coolshell.cn/articles/6193.html)后，密码管理进一步加强，主要是加强密码复杂度，并且定期更换密码。很不幸的是，那是我的CSDN账户也在公布之列，后来一度想找回，无奈CSDN博客的ID已经从`dylanninin`变为`dylan_ninin`，心中还是稍有不悦。

最近在一些项目的测试过程中，发现复杂的密码其实有助于测试用户认证模块中很容易被忽视的小问题，尤其是在调用操作系统的命令时。

目前因密码包含特殊字符(如`&`)导致认证异常的情况，已经遇到过两例。

第一例，是在使用Java构建基于Samba文件共享的搜索系统时，因调用Windows系统的`net use \\samba_server_ip password /user:username`时，`password`没有加引号，导致密码复杂时无法成功认证。

第二例，也是因少了引号，在Linux平台上同步修改LDAP和Samba密码，PHP调用`rtrim(shell_exec("/usr/bin/mkntpwd -N $new_pwd"));`，如果`new_pwd`包含`&`符号，`mkntpwd`就只能在后台运行，密码也就从`&`处截断了。

想想这些问题其实很简单，但也很容易被忽视。当然了，从发现这些问题的测试来看，一定复杂度的密码确实起到不少作用，至少一个很容易被忽视的死角马上就暴露出来了。

##程序

因在日常工作中，有一部分是负责用户账号和权限分配，使用Lastpass自动生成密码容易导致密码繁多难以管理，且也有泄露的危险。于是动手写了一个简单的自动生成密码工具，主要使用Python的random和string标准库。

程序主要配置在`passwdgen.py`的`config`中，可以自定义：

	"""password generator config"""
	config={
	  'min':8,			#密码最小长度
	  'max':32,			#密码最大长度
	  'length':8,		#生成密码长度
	  'cs':2,			#字符character最小个数
	  'ds':2,			#数字digit最小个数
	  'ps':2,			#特殊字符punctuation最小个数
	  'users':[			#待生成密码的用户列表
	     'dylanninin',
		 'dylan',
		 'ninin'
	  ],
	  'log':{
	     'file':'passwdgen.log',
	     'format':'%(asctime)s - %(name)s - %(levelname)s - %(message)s'
	   }
	}
	

运行效果如下：
	
	$python passwdgen.py
	USERNAME  	PASSWORD  
	dylanninin	U'13Dz/m  
	dylan     	`9K7x\yJ  
	ninin     	8x%df-06  

为避免清空控制台导致生成的密码忘记，已经做了日志记录，`passwdgen.log`：

	2013-01-17 20:37:25,937 - root - INFO - password generation started ... ...
	2013-01-17 20:37:25,937 - root - INFO - USERNAME  	PASSWORD  
	2013-01-17 20:37:25,937 - root - INFO - dylanninin	U'13Dz/m  
	2013-01-17 20:37:25,937 - root - INFO - dylan     	`9K7x\yJ  
	2013-01-17 20:37:25,937 - root - INFO - ninin     	8x%df-06  
	2013-01-17 20:37:25,937 - root - INFO - password generation ended ... ...

源代码：[dylanninin/utils/passwdgen.py](https://github.com/dylanninin/utils/blob/master/passwdgen.py)

##参考

* [如何管理并设计你的口令](如何管理并设计你的口令)
* [CSDN明文口令泄露的启示](http://coolshell.cn/articles/6193.html)
* [CSDN明文密码 dylanninin@gmail.com](https://dazzlepod.com/csdn/?email=dylanninin%40gmail.com)
* [破解你的口令](http://coolshell.cn/articles/3801.html)

---
layout: post
title: Fucked By GFW
category : Miscellanies
tags : [Network, Exception]
---

Today there's a network problem within my [Linode](http://www.linode.com/) VPS. And now this issue has been resolved with its' support.

##traceroute within my pc

traceroute to 198.74.50.175 from my PC within PRC.

	Xshell:\> tracert -d 198.74.50.175
	tracerote to 198.74.50.175, 30 hops max

	  1    <1 ms   <1 ms   <1 ms 	192.168.1.1 
	  2     5 ms     6 ms     1 ms  14.153.124.1 
	  3    46 ms     1 ms     4 ms  113.106.43.221 
	  4     3 ms     3 ms     8 ms  119.145.220.58 
	  5    50 ms     9 ms     7 ms  119.145.45.177 
	  6     4 ms     4 ms     4 ms  202.97.33.214 
	  7     *        *        *     timeout
	  8     *        *        *     timeout
	  9     *        *        *     timeout
	 10     *        *        *     timeout
	 11     *        *        *     timeout
	 12     *        *        *     timeout
	 13     *        *        *     timeout

##the last hop

The last accessible hop is 202.97.33.214:

	Hostname:	202.97.33.214
	ISP:	Chinanet Backbone Network

![backbone_of_chinanet](http://dylanninin.com/assets/images/2013/backbone_of_chinanet.png)


##my ip status

check blacklist status

![blacklist_status](http://dylanninin.com/assets/images/2013/blacklist_status.png)

##traceroute within linode

traceroute to 8.8.8.8 from my linode(login with Lish Web Console)

	[root@www ~]# traceroute 8.8.8.8                                                                  
	traceroute to 8.8.8.8 (8.8.8.8), 30 hops max, 60 byte packets                                        
	 1  router1-fmt.linode.com (184.105.143.85) 1.843ms  1.808ms  1.892ms                            
	 2  10gigabitethernet2-3.core1.fmt1.he.net (64.62.250.5) 8.029 ms  8.260ms  8.496ms               
	 3  10gigabitethernet1-1.core1.pao1.he.net (184.105.213.66) 1.830ms  1.862ms  1.888ms            
	 4  184.105.224.254 (184.105.224.254) 1.706ms  1.678ms  1.652ms                                  
	 5  216.239.49.250 (216.239.49.250) 4.857ms 2.088 ms 209.85.240.114 (209.85.240.114)  2.172ms    
	 6  209.85.250.63 (209.85.250.63) 2.145ms 209.85.250.64(209.85.250.64) 2.047ms 209.85.250.63(20
	9.85.250.63)  1.489ms                                                                               
	 7  216.239.49.198 (216.239.49.198)  21.191 ms  21.493 ms 216.239.47.186 (216.239.47.186)  21.266 ms 
	 8  72.14.233.202 (72.14.233.202)  24.267 ms 72.14.233.200 (72.14.233.200)  23.998 ms 72.14.233.202 (
	72.14.233.202)  24.180 ms                                                                            
	 9  216.239.48.165 (216.239.48.165)  24.205 ms 64.233.174.129 (64.233.174.129)  24.362 ms 64.233.174.
	131 (64.233.174.131)  25.005 ms                                                                      
	10  * * *                                                                                            
	11  google-public-dns-a.google.com (8.8.8.8)  24.454 ms  24.407 ms  24.473 ms  
	
##route within linode

route within my linode(login with Lish Web Console)

	[root@www ~]# route -n                                                               
	Kernel IP routing table                                                                       
	Destination     Gateway         Genmask         Flags Metric Ref    Use Iface                        
	0.0.0.0         198.74.50.1     0.0.0.0         UG    0      0        0 eth0                         
	169.254.0.0     0.0.0.0         255.255.0.0     U     1003   0        0 eth0                         
	198.74.50.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0                         


##ifconfig within linode

ifconfig within my linode(login with Lish Web Console)
                                                     
	[root@www ~]# ifconfig                                                                                                          
	eth0      Link encap:Ethernet  HWaddr F2:3C:91:AE:F2:41                                                                         
	          inet addr:198.74.50.175  Bcast:198.74.50.255  Mask:255.255.255.0                                                      
	          inet6 addr: 2600:3c01::f03c:91ff:feae:f241/64 Scope:Global                                                            
	          inet6 addr: fe80::f03c:91ff:feae:f241/64 Scope:Link                                                                   
	          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1                                                                    
	          RX packets:21495 errors:0 dropped:0 overruns:0 frame:0                                                                
	          TX packets:21738 errors:0 dropped:0 overruns:0 carrier:0                                                              
	          collisions:0 txqueuelen:1000                                                                                          
	          RX bytes:2800173 (2.6 MiB)  TX bytes:3993300 (3.8 MiB)                                                                
	          Interrupt:48                                                                                                          
	                                                                                                                                
	lo        Link encap:Local Loopback                                                                                             
	          inet addr:127.0.0.1  Mask:255.0.0.0                                                                                   
	          inet6 addr: ::1/128 Scope:Host                                                                                        
	          UP LOOPBACK RUNNING  MTU:16436  Metric:1                                                                              
	          RX packets:669 errors:0 dropped:0 overruns:0 frame:0                                                                  
	          TX packets:669 errors:0 dropped:0 overruns:0 carrier:0                                                                
	          collisions:0 txqueuelen:0                                                                                             
	          RX bytes:72979 (71.2 KiB)  TX bytes:72979 (71.2 KiB)  

Well, there's some issues lately with the GFW.

##reference

* [Using the Linode Shell(Lish)](http://library.linode.com/troubleshooting/using-lish-the-linode-shell)
* [whatismyipaddress.com](http://whatismyipaddress.com)

---
layout: post
title: Subversion Administration
category : Linux
tags : [SVN, Linux]
---

##Subversion Administration 

###Administrator's Toolkit

安装Subversion后，查看下svn提供的实用工具：

	[root@server ~]# svn [Tab][Tab]
	svn   svnadmin  svndumpfilter  svnlook   svnserve    svnsync   svnversion    

##简介：

* svn：命令行客户端程序
* svnversion：查看当前项目的修订版本的工具
* svnadmin：Subversion版本库管理程序，可以建立、调整和修复版本库等
* svndumpfilter：过滤Subversion版本库转储数据流的工具
* svnlook：直接查看版本库的工具
* svnserve：一个单独运行的服务器程序，可以作为守护进程或者由SSH调用
* svnsync：一个通过网络增量镜像版本库的程序

这里参考《Version Control with SUbversion》的“Chapter 5. Repository Administration”进行整理，并进行简单的测试，主要面向fsfs类型的版本库的备份和恢复；有Subversion的概念、组织、授权等请参考本书其他章节。

###1. svn

svn is the official command-line client of Subversion. Its functionality is offered via a collection of task-specific subcommands, most of which accept a number of options for fine-grained control of the program's behavior.  

###2. svnversion

svnversion is a program for summarizing the revision mixture of a working copy. The resultant revision number, or revision range, is written to standard output.

###3. svnadmin

The svnadmin program is the repository administrator's best friend. Besides providing the ability to create Subversion repositories, this program allows you to perform several maintenance operations on those repositories.

###4. svnserve

svnserve allows access to Subversion repositories using Subversion's custom network protocol.

You can run svnserve as a standalone server process (for clients that are using the `svn://` access method); you can have a daemon such as inetd or xinetd launch it for you on demand (also for `svn://`), or you can have sshd launch it on demand for the `svn+ssh://` access method.

###5. svnlook

svnlook is a tool provided by Subversion for examining the various revisions and transactions (which are revisions in the making) in a repository. No part of this program attempts to change the repository. svnlook is typically used by the repository hooks for re- porting the changes that are about to be committed (in the case of the pre-commit hook) or that were just committed (in the case of the post-commit hook) to the repository.  

###6. svndumpfilter

While it won't be the most commonly used tool at the administrator's disposal, svndumpfilter provides a very particular brand of useful functionality—the ability to quickly and easily modify streams of Subversion repository history data by acting as a path- based filter.

###7. svnsync

The svnsync program provides all the functionality required for maintaining a read-only mirror of a Subversion repository. The program really has one job—to transfer one repository's versioned history into another repository. And while there are few ways to do that, its primary strength is that it can operate remotely—the “source” and “sink”6 repositories may be on different computers from each other and from svnsync itself.

svnsync is the Subversion remote repository mirroring tool. Put simply, it allows you to replay the revisions of one repository intoanother one.

In any mirroring scenario, there are two repositories: the source repository, and the mirror (or “sink”) repository. The source repository is the repository from which svnsync pulls revisions. The mirror repository is the destination for the revisions pulled from the source repository. Each of the repositories may be local or remote—they are only ever addressed by their URLs.

The svnsync process requires only read access to the source repository; it never attempts to modify it. But obviously, svnsync requires both read and write access to the mirror repository.

##Migration Repositories

###1. svnadmin dump

	[root@server repository]# svnadmin help dump
	dump: usage: svnadmin dump REPOS_PATH [-r LOWER[:UPPER] [--incremental]]
	
	Dump the contents of filesystem to stdout in a 'dumpfile'
	portable format, sending feedback to stderr.  Dump revisions
	LOWER rev through UPPER rev.  If no revisions are given, dump all
	revision trees.  If only LOWER is given, dump that one revision tree.
	If --incremental is passed, the first revision dumped will describe
	only the paths changed in that revision; otherwise it will describe
	every path present in the repository as of that revision.  (In either
	case, the second and subsequent revisions, if any, describe only paths
	changed in those revisions.)
	
	Valid options:
	  -r [--revision] ARG      : specify revision number ARG (or X:Y range)
	  --incremental            : dump incrementally
	  --deltas                 : use deltas in dump output
	  -q [--quiet]             : no progress (only errors) to stderr

####Examples

转储存整个版本库：

	[root@server ~]# svnadmin dump /data/repository/Test > /data/backup/Test_full.dump
	* Dumped revision 0.
	* Dumped revision 1.
	* Dumped revision 2.
	* Dumped revision 3.
	* Dumped revision 4.

基于修订版本进行增量转储：

	[root@server ~]# svnadmin dump /data/repository/Test --incremental > /data/backup/Test_base.dump
	* Dumped revision 0.
	* Dumped revision 1.
	* Dumped revision 2.
	* Dumped revision 3.
	* Dumped revision 4.
	* Dumped revision 5.
	* Dumped revision 6.
	* Dumped revision 7.
	* Dumped revision 8.

检出版本库：

	[root@server Workspace]# pwd
	/root/Workspace
	[root@server Workspace]# svn checkout --username 'gonghaibing@tp-link.net' --password 'Us*+2&:9L/C,8^.?' http://172.29.88.162/Test .

	[root@server Workspace]# ls
	DBA  Notes  svn  Test

提交更新：

	[root@server Workspace]# mkdir Incremental
	[root@server Workspace]# cd Incremental/
	[root@server Incremental]# ls
	[root@server Incremental]# touch 1
	[root@server Incremental]# touch 2
	[root@server Incremental]# cd ..
	[root@server Workspace]# svn add Incremental/
	A         Incremental
	A         Incremental/1
	A         Incremental/2
	[root@server Workspace]# svn commit -m "add incremental"
	Adding         Incremental
	Adding         Incremental/1
	Adding         Incremental/2


增量转存：

	[root@server Workspace]# svnlook history /data/repository/Test / --show-ids
	REVISION   PATH <ID>
	--------   ---------
	       9   / <0.0.r9/713>
	       8   / <0.0.r8/4674>
	       7   / <0.0.r7/710>
	       6   / <0.0.r6/112>
	       5   / <0.0.r5/201>
	       4   / <0.0.r4/174>
	       3   / <0.0.r3/144851702>
	       2   / <0.0.r2/138>
	       1   / <0.0.r1/106>
	       0   / <0.0.r0/17>

	[root@server ~]# svnadmin dump /data/repository/Test -r 9 --incremental > /data/backup/Test_r9.dump
	* Dumped revision 9.

导入初始增量：

	[root@server ~]# svnadmin create /data/repository/Inc
	[root@server ~]# svnadmin load /data/repository/Inc/ < /data/backup/Test_base.dump 

查看目录结构：	

	[root@server ~]# svnlook tree /data/repository/Inc/ --full-paths
	/
	Test/
	DBA/
	DBA/LaTeX and WinEdit Tutorial.pdf
	... ...
	svn/
	svn/svn.png
	svn/svn.dot
	Notes/
		
导入版本为9的增量：

	[root@server ~]# svnadmin load /data/repository/Inc/ < /data/backup/Test_r9.dump 
	<<< Started new transaction, based on original revision 9
	     * adding path : Incremental ... done.
	     * adding path : Incremental/1 ... done.
	     * adding path : Incremental/2 ... done.
	
	------- Committed revision 9 >>>

查看目录结构：

	[root@server ~]# svnlook tree /data/repository/Inc/ --full-paths
	/
	Test/
	Incremental/
	Incremental/1
	Incremental/2
	DBA/
	DBA/LaTeX and WinEdit Tutorial.pdf
	... ...
	svn/
	svn/svn.png
	svn/svn.dot
	Notes/

注：

查看版本库的目录结构，：

	[root@server ~]# svnlook tree /data/repository/Test --full-paths --show-ids
	/ <0.0.r4/174>
	Test/ <0-4.0.r4/0>
	DBA/ <0-1.0.r3/144851472>
	DBA/LaTeX and WinEdit Tutorial.pdf <1-3.0.r3/144826285>
	... ...


以相应目录的修订版本历史：

	[root@server ~]# svnlook history /data/repository/Test / --show-ids
	REVISION   PATH <ID>
	--------   ---------
	       4   / <0.0.r4/174>
	       3   / <0.0.r3/144851702>
	       2   / <0.0.r2/138>
	       1   / <0.0.r1/106>
	       0   / <0.0.r0/17>
	[root@server ~]# 


###2. svnadmin load

	[root@server ~]# svnadmin help load
	load: usage: svnadmin load REPOS_PATH

	Read a 'dumpfile'-formatted stream from stdin, committing
	new revisions into the repository's filesystem.  If the repository
	was previously empty, its UUID will, by default, be changed to the
	one specified in the stream.  Progress feedback is sent to stdout.
	
	Valid options:
	  -q [--quiet]             : no progress (only errors) to stderr
	  --ignore-uuid            : ignore any repos UUID found in the stream
	  --force-uuid             : set repos UUID to that found in stream, if any
	  --use-pre-commit-hook    : call pre-commit hook before committing revisions
	  --use-post-commit-hook   : call post-commit hook after committing revisions
	  --parent-dir ARG         : load at specified directory in repository

####Examples

导入转储的版本库：

	[root@server ~]# svnadmin load /data/repository/Test_load < /data/backup/Test_full.dump 
	svnadmin: Can't open file '/data/repository/Test_load/format': No such file or directory

需先创建目标版本库：

	[root@server ~]# svnadmin create /data/repository/Test_load
	[root@server ~]# svnadmin load /data/repository/Test_load < /data/backup/Test_full.dump 
	<<< Started new transaction, based on original revision 1
	     * adding path : DBA ... done.
	
	------- Committed revision 1 >>>
	
	<<< Started new transaction, based on original revision 2
	     * adding path : Notes ... done.
	
	------- Committed revision 2 >>>
	
	<<< Started new transaction, based on original revision 3
	     * adding path : DBA/CTeX FAQ.pdf ... done.
	     ... ...
	     * adding path : DBA/template ... done.
	     * adding path : DBA/template/CASthesis-v0.1j.zip ... done.
	     * adding path : DBA/template/CASthesis-v0.2.zip ... done.
	
	------- Committed revision 3 >>>
	
	<<< Started new transaction, based on original revision 4
	     * adding path : Test ... done.
	
	------- Committed revision 4 >>>


###3. svndumpfilter

	[root@server ~]# svndumpfilter help
	general usage: svndumpfilter SUBCOMMAND [ARGS & OPTIONS ...]

	Type 'svndumpfilter help <subcommand>' for help on a specific subcommand.
	Type 'svndumpfilter --version' to see the program version.

	Available subcommands:
	   exclude
	   include
	   help (?, h)

####Examples

使用svndumpfilter对转储文件进行过滤，`include`模式，其他的都排除：

	[root@server repository]# svndumpfilter include "DBA/template" "DBA/pdf" < /data/backup/Test_full.dump > Test_filter.dump
	Including prefixes:
	   '/DBA/template'
	   '/DBA/pdf'
	
	Revision 0 committed as 0.
	Revision 1 committed as 1.
	Revision 2 committed as 2.
	Revision 3 committed as 3.
	Revision 4 committed as 4.
	
	Dropped 72 nodes:
	   '/DBA'
	   '/DBA/CTeX FAQ.pdf'
	   '/DBA/LaTeX and WinEdit Tutorial.pdf'
	   '/DBA/LaTeX2ε 插图指南.pdf'
	   ... ...

`--pattern`模式，使用正则表达式：

	[root@server ~]# svndumpfilter exclude --pattern "*template" < /data/backup/Test_full.dump > Test_filter.dump
	svndumpfilter: invalid option: --pattern
	Type 'svndumpfilter help' for usage.

Note: 

Beginning in Subversion 1.7, `svndumpfilter` can optionally treat the PATH_PREFIXs not merely as explicit substrings, but as file patterns instead。

也就是说，在1.7以上版本的subversion中，才能使用基于正则表达式的路径语法；在低版本中，使用正则表达式语法，只会被当做普通的字符串处理。

查看当前subversion版本信息：

	[root@server ~]# svnadmin --version
	svnadmin, version 1.6.11 (r934486)
	   compiled Sep 27 2011, 15:29:25
	Copyright (C) 2000-2009 CollabNet.

	Subversion is open source software, see http://subversion.tigris.org/
	This product includes software developed by CollabNet (http://www.Collab.Net/).

	The following repository back-end (FS) modules are available:
	* fs_base : Module for working with a Berkeley DB repository.
	* fs_fs : Module for working with a plain file (FSFS) repository.

###4. svnadmin hotcopy

	[root@server repository]# svnadmin help hotcopy
	hotcopy: usage: svnadmin hotcopy REPOS_PATH NEW_REPOS_PATH
	Makes a hot copy of a repository.
	Valid options:
	  --clean-logs             : remove redundant Berkeley DB log files
	                             from source repository [Berkeley DB]

####Examples

热拷贝一个版本库：

	[root@server ~]# svnadmin hotcopy /data/repository/Test /data/repository/Test_hotcopy

使用svnlook tree输出源版本库、新版本库的目录结构，并用diff比较

	[root@server ~]# svnlook tree /data/repository/Test_hotcopy/ > Test_hotcopy.tree
	[root@server ~]# svnlook tree /data/repository/Test/ > Test.tree
	[root@server ~]# diff Test_hotcopy.tree Test.tree 

新版本库路径加到Apache的`subversion.conf`中，调整authz配置即可正常访问。

###5. svnsync

	[root@server ~]# svnsync help
	general usage: svnsync SUBCOMMAND DEST_URL  [ARGS & OPTIONS ...]
	Type 'svnsync help <subcommand>' for help on a specific subcommand.
	Type 'svnsync --version' to see the program version and RA modules.
	
	Available subcommands:
	   initialize (init)
	   synchronize (sync)
	   copy-revprops
	   info
	   help (?, h)

####Examples

	[root@server ~]# svnsync init file:///data/repository/Test_mirror file:///data/repository/Test
	svnsync: Unable to open an ra_local session to URL
	svnsync: Unable to open repository 'file:///data/repository/Test_mirror'

	[root@server ~]# svnadmin create /data/repository/Test_mirror

	[root@server ~]# svnsync init file:///data/repository/Test_mirror file:///data/repository/Test
	svnsync: Repository has not been enabled to accept revision propchanges;
	ask the administrator to create a pre-revprop-change hook

	[root@server ~]# cd /data/repository/Test_mirror/hooks
	[root@server hooks]# cp pre-revprop-change.tmpl pre-revprop-change

	[root@server ~]# svnsync init file:///data/repository/Test_mirror file:///data/repository/Test
	svnsync: Revprop change blocked by pre-revprop-change hook (exit code 255) with no output.
	
	[root@server ~]# chmod +x /data/repository/Test_mirror/hooks/pre-revprop-change
	[root@server ~]# svnsync init file:///data/repository/Test_mirror file:///data/repository/Test
	svnsync: Revprop change blocked by pre-revprop-change hook (exit code 1) with output:
	Changing revision properties other than svn:log is prohibited

有待继续测试... ...

###6.svnadmin upgrade

	[root@server repository]# svnadmin help upgrade
	upgrade: usage: svnadmin upgrade REPOS_PATH
	
	Upgrade the repository located at REPOS_PATH to the latest supported
	schema version.
	
	This functionality is provided as a convenience for repository
	administrators who wish to make use of new Subversion functionality
	without having to undertake a potentially costly full repository dump
	and load operation.  As such, the upgrade performs only the minimum
	amount of work needed to accomplish this while still maintaining the
	integrity of the repository.  It does not guarantee the most optimized
	repository state as a dump and subsequent load would.


###7. svnadmin verify

	[root@server ~]# svnadmin help verify
	verify: usage: svnadmin verify REPOS_PATH

	Verifies the data stored in the repository.

	Valid options:
	  -r [--revision] ARG      : specify revision number ARG (or X:Y range)
	  -q [--quiet]             : no progress (only errors) to stderr


##Sumary

Backup or migrate a Whole Repository：

* hotcopy

	`svnadmin hotcopy src_repo_path dist_repo_path`

* dump with increment

	`svnadmin dump -> dump file --> svndumpfilter --> filtered dump file --> svnadmin load`

* sync

	`svnsync`

![svn_backup_and_recovery](http://dylanninin.com/assets/images/2013/svn_backup_and_recovery.png)

##Reference

* Version Control with Subversion

---
layout: post
title: Integration SVN with HTTPD and LDAP
category : Linux
tags : [SVN, Apache, LDAP, Linux]
---

##Subversion Architecture

![svn_arch](http://dylanninin.com/assets/images/2013/svn_architecture.png)

##HTTP

To network your repository over HTTP, you basically need four components, available in two packages. You'll need Apache httpd 2.0 or newer, the mod_dav DAV module that comes with it, Subversion, and the `mod_dav_svn` filesystem provider module distributed with Subversion. Once you have all of those components, the process of networking your repository is as simple as:
 
* Getting httpd up and running with the mod_dav module
* Installing the mod_dav_svn backend to mod_dav, which uses Subversion's libraries to access the repository
* Configuring your httpd.conffile to export (or expose) the repository

###`mod_dav_svn`

mod_dav_svn Configuration Directives — Apache configuration directives for serving Subversion repositories through the Apache
HTTP Server.

###`mod_authz_svn`

mod_authz_svn Configuration Directives — Apache configuration directives for configuring path-based authorization for Subversion repositories served through the Apache HTTP Server.

##LDAP 

This module provides authentication front-ends such as mod_auth_basic to authenticate users through an ldap directory.

`mod_authnz_ldap` supports the following features:

* Known to support the OpenLDAP SDK (both 1.x and 2.x), Novell LDAP SDK and the iPlanet (Netscape) SDK.
* Complex authorization policies can be implemented by representing the policy with LDAP filters.
* Uses extensive caching of LDAP operations via mod_ldap.
* Support for LDAP over SSL (requires the Netscape SDK) or TLS (requires the OpenLDAP 2.x SDK or Novell LDAP SDK).

##Sample Configure of SVN, HTTP, LDAP

###Load Module to support SVN

modify /etc/httpd/conf.d/subversion.conf:

	LoadModule dav_module         modules/mod_dav.so
	LoadModule dav_svn_module     modules/mod_dav_svn.so
	LoadModule authz_svn_module   modules/mod_authz_svn.so

###Basic Config

The easiest way to authenticate a client is via the HTTP Basic authentication mechanism, which simply uses a username and password to verify a user's identity. Apache provides the htpasswd utility for managing files containing usernames and passwords.

create a password file:

	$ htpasswd -c -m /apps/svnroot/passwd harry
	New password: *****
	Re-type new password: *****
	Adding password for user harry
	$ htpasswd -m /apps/svnroot/passwd sally
	New password: *******
	Re-type new password: *******
	Adding password for user sally
	
modify /etc/httpd/conf.d/subversion.conf:

	<Location /svn>
		DAV svn
		SVNParentPath /apps/svnroot/
		AuthzSVNAccessFile /apps/svnroot/authz.conf
		AuthType Basic
		AuthName "Subversion welcome to svn"
		AuthUserFile /apps/svnroot/passwd
		Require valid-user
	</Location>

###Load Module to support LDAP

modify /etc/httpd/conf/httpd.conf:

	LoadModule ldap_module modules/mod_ldap.so
	LoadModule authnz_ldap_module modules/mod_authnz_ldap.so

modify /etc/httpd/conf.d/subversion.conf:
	
		<Location /IT>
		DAV svn
		SVNParentPath  /apps/svnroot/
		AuthType Basic
		AuthName "Subversion Repository"
	#   Auth by LDAP
		AuthBasicProvider ldap
		AuthzLDAPAuthoritative off
		AuthLDAPURL  "ldap://192.168.1.111:389/dc=test-it,dc=net?mail?sub"
		AuthLDAPBindDN  "cn=it,ou=admin,dc=test-it,dc=net"
		AuthLDAPBindPassword "itpassword"
	
	#   Auth by passwd file
	#	AuthUserFile /apps/svnroot/passwd
		AuthzSVNAccessFile /apps/svnroot/authz_svn
		Require  valid-user
	</Location>


An [RFC 2255](https://tools.ietf.org/html/rfc2255) URL which specifies the LDAP search parameters to use. The syntax of the URL is

	ldap://host:port/basedn?attribute?scope?filter

##Reference

* Version Control with Subversion
* [`mod_authz_svn`](http://svn.apache.org/repos/asf/subversion/trunk/subversion/mod_authz_svn/INSTALL)
* [`mod_authnz_ldap`](http://httpd.apache.org/docs/2.2/mod/mod_authnz_ldap.html)
* [RFC 2255 - The LDAP URL Format](https://tools.ietf.org/html/rfc2255)

---
layout: post
title: SQL*Plus Help Facility
category : Oracle
tags : [Oracle, Database, DBA]
---

Unix/Linux下可以用`man`、`info`查询命令的具体用法，十分方便，既不需要联网要求， 也不需要额外打开所谓的命令大全，对于系统管理员来说，确实是一个不可或缺的好帮手。

在数据库方面，MySQL、SQLite、MongoDB对SQL、Utility等都提供十分友好的help输出； Oracle Database也有，但仅有`SQL*Plus`，SQL的帮助需要额外安装。

查了下MOS，找到一篇比较古老的文档，最后更新时间是在2003年，那应该是针对Oracle 8i 的，用`SQL*Loader`导入。用AvaFind查了下硬盘，发现在我的电脑上还真有8i的help。后来， 也在网络上找了下，走了一些弯路，故写下备忘，当然也算是一种吐槽。

##Oracle's `SQL*Plus` Help Facility

###Problem Description
 
You try to  use the HELP facility in `SQL*Plus` and you receive the following 
error: 
 
    'HELP not accessible.'  

###Solution Description: 

The `SQL*Plus` HELP facility has not been enabled. 

The `SQL*Plus` HELP facility can be created by performing the following steps: 
 
1. Go to the following directory: 
         
	% cd $ORACLE_HOME/sqlplus/admin/help 
 
2. Login to `SQL*Plus` as the user SYSTEM. 
 
	% sqlplus system/password
  
3. Run the "helptbl.sql" script. 
 
	SQL> @helptbl 
 
4. Exit `SQL*Plus` and execute the following `SQL*Loader` commands:  
 
	% sqlldr system/manager control=plushelp.ctl	
	
	% sqlldr system/manager control=sqlhelp.ctl 
	
	% sqlldr system/manager control=plshelp.ctl 
	 
5. Log back into `SQL*Plus` as the user SYSTEM and run the "helpindx.sql" 
   script.  
 
   SQL> @helpindx 
 
 
###Solution Explanation: 

Performing these steps will build and load the `SQL*Plus` HELP table.

##Install SQL HELP on Oracle 11g

安装前，仅有`SQL*Pus`的help（安装数据库时一般会自动安装）：

	SQL> help index
	help index
	
	Enter Help [topic] for help.
	
	 @             COPY         PAUSE                    SHUTDOWN
	 @@            DEFINE       PRINT                    SPOOL
	 /             DEL          PROMPT                   SQLPLUS
	 ACCEPT        DESCRIBE     QUIT                     START
	 APPEND        DISCONNECT   RECOVER                  STARTUP
	 ARCHIVE LOG   EDIT         REMARK                   STORE
	 ATTRIBUTE     EXECUTE      REPFOOTER                TIMING
	 BREAK         EXIT         REPHEADER                TTITLE
	 BTITLE        GET          RESERVED WORDS (SQL)     UNDEFINE	
	 CHANGE        HELP         RESERVED WORDS (PL/SQL)  VARIABLE	
	 CLEAR         HOST         RUN                      WHENEVER OSERROR	
	 COLUMN        INPUT        SAVE                     WHENEVER SQLERROR	
	 COMPUTE       LIST         SET                      XQUERY	
	 CONNECT       PASSWORD     SHOW


	SQL> help topics
	help topics
	Help is available on the following topics:
	/
	@
	@@
	ACCEPT
	APPEND
	ARCHIVE LOG
	ATTRIBUTE
	BREAK
	BTITLE
	CHANGE
	... ...
	XQUERY


HELP表`system.help`结构：
	
	SQL> desc system.help
	desc system.help

	 Name			  Null?     Type	
	 ---------------- --------- ------------------
	 TOPIC		      NOT NULL  VARCHAR2(50)
	 SEQ			  NOT NULL  NUMBER
	 INFO					    VARCHAR2(80)

查看当前Help数：

	select count(1) from system.help;
	  COUNT(*)
	----------
	       919

`help create`相关主题：

	SQL> help create
	SP2-0172: No HELP matching this topic was found.

`SQL*Plus`帮助文件路径：

	[oracle@oradb ~]$ cd /db/oracle/product/11.2.0/db_1/sqlplus/admin/help/
	[oracle@oradb help]$ ll
	total 80
	-rw-r--r--. 1 oracle oinstall   265 Feb 17  2003 helpbld.sql
	-rw-r--r--. 1 oracle oinstall   337 Jun 28  2000 helpdrop.sql
	-rw-r--r--. 1 oracle oinstall 65975 Jun 29  2009 helpus.sql
	-rw-r--r--. 1 oracle oinstall  2086 Jan  6  2009 hlpbld.sql

以上脚本简介：

* helpbld.sql：Invoke and execute the script to loads the `SQL*Plus` HELP system and upon completion, exit the `SQL*Plus` connection. Code: @@&1/hlpbld.sql &2
* helpdrop.sql：Drops the `SQL*Plus` HELP table
* helpus.sql：Inserts `SQL*Plus` HELP text in English. This script is called from helpbld.sql
* hlpbld.sql：Builds the `SQL*Plus` HELP table and loads the HELP data from a data file. The data file must exist before this script is run.

在安装Oracle数据库时默认已经安装一些Help，不过主要是针对`SQL*Plus`，对Oracle SQL 的Help没有。查MOS，暂且仅找到8i自带的SQL Help。在CSDN上找了下，有一个help.sql脚 本，针对SQL语法的，对DBA日常管理应该已经够用。下载链接见文末[参考](#reference)。

上传help.sql文件：

	[oracle@oradb help]$ rz
	rz waiting to receive.**B0100000023be50
	?

	[oracle@oradb help]$ ll
	total 596
	-rw-r--r--. 1 oracle oinstall    265 Feb 17  2003 helpbld.sql
	-rw-r--r--. 1 oracle oinstall    337 Jun 28  2000 helpdrop.sql
	-rw-r--r--. 1 oracle oinstall 527523 May 11  2005 help.sql
	-rw-r--r--. 1 oracle oinstall  65975 Jun 29  2009 helpus.sql
	-rw-r--r--. 1 oracle oinstall   2086 Jan  6  2009 hlpbld.sql

安装help.sql：

	[oracle@oradb help]$ pwd
	/db/oracle/product/11.2.0/db_1/sqlplus/admin/help
	[oracle@oradb help]$ sqlplus system/oracle
	
	SQL*Plus: Release 11.2.0.1.0 Production on Tue Jan 22 17:15:58 2013
	
	Copyright (c) 1982, 2009, Oracle.  All rights reserved.
	
	Connected to:
	Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
	With the Partitioning, OLAP, Data Mining and Real Application Testing options
	
	SQL> @helpbld
	Enter value for 1: /db/oracle/product/11.2.0/db_1/sqlplus/admin/help
	Enter value for 2: /db/oracle/product/11.2.0/db_1/sqlplus/admin/help/help.sql
	
	PL/SQL procedure successfully completed.


安装后，查看help：

	oracle@oradb help]$ sqlplus  /nolog
	
	SQL*Plus: Release 11.2.0.1.0 Production on Tue Jan 22 17:17:54 2013
	
	Copyright (c) 1982, 2009, Oracle.  All rights reserved.
	
	SQL> conn /as sysdba
	Connected.
	SQL> select count(1) from system.help;
	
	  COUNT(1)
	----------
	      5085
	
	SQL> help index
	 Use the HELP TOPIC command for a list of help topics.
	
	SQL> help topic
	 Help is available on the following topics:
	
	%ROWTYPE ATTRIBUTE
	%TYPE ATTRIBUTE
	/
	@
	@@
	ACCEPT
	ALLOCATE (EMBEDDED SQL)
	ALTER CLUSTER
	ALTER DATABASE
	ALTER FUNCTION
	ALTER INDEX
	ALTER PACKAGE
	ALTER PROCEDURE
	ALTER PROFILE
	ALTER RESOURCE COST
	ALTER ROLE
	ALTER ROLLBACK SEGMENT
	ALTER SEQUENCE
	ALTER SESSION
	... ...

`help create database`示例：

	SQL> help create database
	
	 CREATE DATABASE
	 ---------------
	
	 Use this command to create a database, making it available for
	 general use, with the following options:
	
	   *  to establish a maximum number of instances, data files, redo
	      log files groups, or redo log file members
	   *  to specify names and sizes of data files and redo log files
	   *  to choose a mode of use for the redo log
	   *  to specify the national and database character sets
	
	 Warning: This command prepares a database for initial use and erases
	 any data currently in the specified files. Only use this command
	 when you understand its ramifications.
	
	 CREATE DATABASE [database]
	   { CONTROLFILE REUSE
	   | LOGFILE [GROUP integer] filespec
	           [,[GROUP integer] filespec] ...
	   | MAXLOGFILES integer
	   | MAXLOGMEMBERS integer
	   | MAXLOGHISTORY integer
	   | MAXDATAFILES integer
	   | MAXINSTANCES integer
	   | {ARCHIVELOG | NOARCHIVELOG}
	   | EXCLUSIVE
	   | CHARACTER SET charset
	   | NATIONAL CHARACTER SET charset
	   | DATAFILE filespec [AUTOEXTEND {OFF | ON [NEXT integer [K | M] ]
	                       [MAXSIZE { UNLIMITED | integer [K | M]} ] } ]
	           [, filespec [AUTOEXTEND {OFF | ON [NEXT integer [K | M] ]
	                       [MAXSIZE { UNLIMITED | integer [K | M]} ] } ] ] ...} ...
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.
	
	
	 CREATE DATABASE LINK
	 --------------------
	
	 Use this command to create a database link. A database link is a
	 schema object in the local database that allows you to access
	 objects on a remote database or to mount a secondary database in
	 read-only mode. The remote database can be either an Oracle or a
	 non-Oracle system.
	
	 CREATE [PUBLIC] DATABASE LINK dblink
	   [CONNECT TO user IDENTIFIED BY password]
	   [USING 'connect_string']
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.


##Reference

* `SQL*Plus` HELP Message 'HELP NOT ACCESSIBLE' [ID 1054547.6]
* [Alter Oracle's `SQL*Plus` Help Facility](http://it.toolbox.com/blogs/database-solutions/altering-oracles-sqlplus-help-facility-7697)
* [Oracle之常用FAQ V1.0](http://www.itpub.net/thread-180363-1-1.html)
* [Google help.sql](http://www.google.fr/search?hl=zh-CN&newwindow=1&safe=strict&tbo=d&biw=1440&bih=762&noj=1&q=site%3Adownload.csdn.net+help.sql&btnG=Google+%E6%90%9C%E7%B4%A2)

---
layout: post
title: Proxy Authentication
category : Oracle
tags : [Oracle, Database, DBA, Security]
---

在学习[cx_Oracle](http://sourceforge.net/projects/cx-oracle/)访问Oracle数据库时， 下载作者的源代码，发现作者在测试的数据库脚本中创建用户即使用了代理授权。当时没有 太注意，最近正好总结作半年以来的工作，想起来就先测试下，以作备忘。

##代理认证

###cx_Oracle中创建用户脚本

	... ...
	create user cx_Oracle identified by "password"
	quota unlimited on dba_data
	default tablespace dba_data
	temporary tablespace dba_tmp;
	
	
	create user cx_Oracle_proxy identified by "password";
	alter user cx_Oracle_proxy grant connect through cx_Oracle;
	
	grant create session to cx_Oracle_proxy;
	
	grant
	  create session,
	  create table,
	  create procedure,
	  create type
	to cx_Oracle;
	... ...

###代理认证测试

####创建代理/被代理用户

查看DEV所有的系统权限

	SQL> conn /as sysdba
	Connected.
	
	SQL> SELECT * FROM dba_sys_privs dsp WHERE dsp.grantee = upper('&grantee');
	Enter value for grantee: dev
	old   1: SELECT * FROM dba_sys_privs dsp WHERE dsp.grantee = upper('&grantee')
	new   1: SELECT * FROM dba_sys_privs dsp WHERE dsp.grantee = upper('dev')
	
	GRANTEE 		    PRIVILEGE			    ADM
	------------------ ------------------------ ---
	DEV			       CREATE ANY VIEW			NO
	DEV			       DEBUG CONNECT SESSION	NO
	DEV			       CREATE ANY PROCEDURE		NO
	DEV			       UNLIMITED TABLESPACE		NO
	DEV			       DEBUG ANY PROCEDURE		NO
	

创建账户`DEV_APP`，并授权代理。`DEV_APP`通过`DEV`代理数据库连接

	SQL> create user dev_app default tablespace dba_data temporary tablespace dba_tmp identified by APP;
	User created.
	
	SQL> alter user dev_app grant connect through dev;
	User altered.


使用代理用户连接数据库
	
	SQL> conn dev[dev_app]  --conn proxy_user[real_user]/password_of_proxy_user
	Enter password: 
	ERROR:
	ORA-01045: user DEV_APP lacks CREATE SESSION privilege; logon denied
	Warning: You are no longer connected to ORACLE.
	SQL> alter create session to dev_app;
	SP2-0640: Not connected
	SQL> conn /as sysdba
	Connected.
	
DEV_APP需要`CREATE SESSION`权限

	SQL> grant create session to dev_app;
	Grant succeeded.
	
查看当前用户和代理用户

	SQL> conn dev[dev_app]		
	Enter password: 			
	Connected.
	SQL> select sys_context('userenv','current_user') from dual;
	SYS_CONTEXT('USERENV','CURRENT_USER')
	------------------------------------------------------------
	DEV_APP
	
	SQL> select sys_context('userenv','proxy_user') from dual;
	SYS_CONTEXT('USERENV','PROXY_USER')
	------------------------------------------------------------
	DEV

从这部分测试可用看出，被代理的用户(即`DEV_APP`)通过代理用户(即`DEV`)进行连接时，使用代理用户的用户名、密码进行认证，但被代理用户需要有连接数据库创建会话的权限才能连接。

####对象访问权限

因`DEV_APP`通过代理用户`DEV`进行连接，不知`DEV_APP`是否也会拥有DEV的权限。故测试下被代理用户的权限是如何处理的。

DEV拥有的表：

	SQL> conn dev
	Enter password: 
	Connected.
	SQL> select table_name from user_tables;	
	TABLE_NAME
	------------------------------
	DBA_TEST

查看`DEV_APP`拥有的表，并访问`DEV`的对象：

	SQL> conn dev[dev_app]/
	Enter password: 
	Connected.
	SQL> select table_name from user_tables;
	no rows selected

	SQL> select count(1) from dev.dba_test;
	select count(1) from dev.dba_test
	                         *
	ERROR at line 1:
	ORA-00942: table or view does not exist

	SQL> SELECT * FROM session_privs;
	PRIVILEGE
	----------------------------------------
	CREATE SESSION
	
	SQL> create table app_test(id number);
	create table app_test(id number)
	*
	ERROR at line 1:
	ORA-01031: insufficient privileges
	
给`DEV_APP`用户授权：

	SQL> conn /as sysdba
	Connected.
	SQL> grant create table to dev_app;
	Grant succeeded.
	
	SQL> conn dev[dev_app]
	Enter password: 
	Connected.
	SQL> create table app_test(id number);	
	Table created.
	
通过这部分测试可用看出，被代理用户`DEV_APP`不会继承代理用户`DEV`的权限。要权限需要额外授予。

##小结

One should understand that a database proxied user behaves just like the user itself. The connection is created by the proxy, but the session's privileges are limited to the privileges of the proxied user, who is after all a database user.

意即被代理用户(如`DEV_APP`)仅仅是通过代理用户(如`DEV`)身份认证并创建数据库会话连接，在该连接过程中所有的操作都受限于被代理用户的权限。

代理认证的作用：

1. 多用户(被代理用户)可以使用单一用户(代理用户)进行认证连接，因统一使用代理用户的用户名/密码进行认证，简化了多用户的密码管理。
2. 当忘记用户密码(如`DEV_APP`)，而又想以该身份登陆数据库时，可以使用代理用户（如`DEV`）。

---
layout: post
title: UTL_MAIL Introduction
category : Oracle
tags : [Oracle, Database, DBA]
---

##UTL_MAIl reference

###Security Model

`UTL_MAIL` is not installed by default because of the `SMTP_OUT_SERVER ` configuration requirement and the security exposure this involves. In installing `UTL_MAIL`, you should take steps to prevent the port defined by `SMTP_OUT_SERVER ` being swamped by data transmissions.

This package is now an invoker's rights package and the invoking user will need the connect privilege granted in the access control list assigned to the remote network host to which he wants to connect. 

###Operations

You must both install `UTL_MAIL` and define the `SMTP_OUT_SERVER`.

■ To install UTL_MAIL:

	sqlplus sys/<pwd>
	SQL> @$ORACLE_HOME/rdbms/admin/utlmail.sql
	SQL> @$ORACLE_HOME/rdbms/admin/prvtmail.plb

■ You define the `SMTP_OUT_SERVER` parameter in the init.ora rdbms initialization file. However, if `SMTP_OUT_SERVER` is not defined, this invokes a default of `DB_DOMAIN` which is guaranteed to be defined to perform appropriately

###Rules and Limits

Use `UTL_MAIL` only within the context of the ASCII (American Standard Code for Information Interchange) and EBCDIC (Extended Binary-Coded Decimal Interchange Code) codes.

###Summary of UTL_MAIL
* SEND：Packages an email message into the appropriate format, locates SMTP information, and delivers the message to the SMTP server for forwarding to the recipients
* SEND_ATTACH_RAW：Represents the SEND Procedure overloaded for RAW attachments
* SEND_ATTACH_VARCHAR2：Represents the SEND Procedure overloaded for VARCHAR2 attachments

##UTL_MAIL in Action

###安装UTL_MAIL包

切换到Oracle身份，以sys用户登录：

	[oracle@oradb]$ sqlplus /nolog
	SQL*Plus: Release 11.2.0.1.0 Production on Wed Jan 23 17:23:38 2013
	Copyright (c) 1982, 2009, Oracle.  All rights reserved.
	
	SQL> conn /as sysdba
	Connected.
	SQL> desc UTL_MAIL.SEND
	ERROR:
	ORA-04043: object UTL_MAIL.SEND does not exist
	
正如前面提到的, `UTL_MAIL`包在安装数据库时并不会默认安装；此时需要手动安装，安装过程如下：

切换到`UTL_MAIL`包路径，确认代码是否存在：

	[oracle@oradb admin]$ cd $ORACLE_HOME/rdbms/admin
	[oracle@oradb admin]$ pwd
	/db/oracle/product/11.2.0/db_1/rdbms/admin
	[oracle@oradb admin]$ ls *mail*
	prvtmail.plb  utlmail.sql

安装`UTL_MAIL`包：

	[oracle@oradb admin]$ sqlplus /nolog
	SQL*Plus: Release 11.2.0.1.0 Production on Wed Jan 23 17:46:24 2013
	Copyright (c) 1982, 2009, Oracle.  All rights reserved.
	
	SQL> conn /as sysdba
	Connected.
	SQL> @utlmail.sql
	Package created.
	Synonym created.
	
	SQL> @prvtmail.plb
	Package created.
	Package body created.
	Grant succeeded.
	Package body created.
	No errors.
	
	SQL> desc utl_mail
	PROCEDURE SEND
	 Argument Name			Type			In/Out Default?
	 ------------------------------ ----------------------- ------ --------
	 SENDER 			VARCHAR2		IN
	 RECIPIENTS			VARCHAR2		IN
	 CC					VARCHAR2		IN     DEFAULT
	 BCC				VARCHAR2		IN     DEFAULT
	 SUBJECT			VARCHAR2		IN     DEFAULT
	 MESSAGE			VARCHAR2		IN     DEFAULT
	 MIME_TYPE			VARCHAR2		IN     DEFAULT
	 PRIORITY			BINARY_INTEGER	IN     DEFAULT
	 REPLYTO			VARCHAR2		IN     DEFAULT
	 PROCEDURE SEND_ATTACH_RAW
	 Argument Name			Type			In/Out Default?
	 ------------------------------ ----------------------- ------ --------
	 SENDER 			VARCHAR2		IN
	 RECIPIENTS			VARCHAR2		IN
	 CC					VARCHAR2		IN     DEFAULT
	 BCC				VARCHAR2		IN     DEFAULT
	 SUBJECT			VARCHAR2		IN     DEFAULT
	 MESSAGE			VARCHAR2		IN     DEFAULT
	 MIME_TYPE			VARCHAR2		IN     DEFAULT
	 PRIORITY			BINARY_INTEGER	IN     DEFAULT
	 ATTACHMENT			RAW				IN
	 ATT_INLINE			BOOLEAN 		IN     DEFAULT
	 ATT_MIME_TYPE		VARCHAR2		IN     DEFAULT
	 ATT_FILENAME		VARCHAR2		IN     DEFAULT
	 REPLYTO			VARCHAR2		IN     DEFAULT
	 PROCEDURE SEND_ATTACH_VARCHAR2
	 Argument Name			Type			In/Out Default?
	 ------------------------------ ----------------------- ------ --------
	 SENDER 			VARCHAR2		IN
	 RECIPIENTS			VARCHAR2		IN
	 CC					VARCHAR2		IN     DEFAULT
	 BCC				VARCHAR2		IN     DEFAULT
	 SUBJECT			VARCHAR2		IN     DEFAULT
	 MESSAGE			VARCHAR2		IN     DEFAULT
	 MIME_TYPE			VARCHAR2		IN     DEFAULT
	 PRIORITY			BINARY_INTEGER	IN     DEFAULT
	 ATTACHMENT			VARCHAR2		IN
	 ATT_INLINE			BOOLEAN 		IN     DEFAULT
	 ATT_MIME_TYPE		VARCHAR2		IN     DEFAULT
	 ATT_FILENAME		VARCHAR2		IN     DEFAULT
	 REPLYTO			VARCHAR2		IN     DEFAULT


###给用户授权

非SYS用户没有权限调用`UTL_MAIL`:

	SQL> disc
	Disconnected from Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
	With the Partitioning, OLAP, Data Mining and Real Application Testing options
	SQL> conn dev
	Enter password: 
	Connected.
	SQL> desc utl_mail
	ERROR:
	ORA-04043: object "SYS"."UTL_MAIL" does not exist

授权给PUBLIC或某一个特定的用户，以使用户有权限调用该程序包：

	SQL> conn /as sysdba
	Connected.
	SQL> grant execute on utl_mail to dev;
	Grant succeeded.

确认授权后，用户可以访问该程序包：

	SQL> conn dev
	Enter password: 
	Connected.
	SQL> desc utl_mail
	 PROCEDURE SEND
	 Argument Name			Type			In/Out Default?
	 ... ...

###邮件服务器测试

####使用telnet测试smtp是否正常

	[oracle@oradb ]$ telnet smtp.test-it.net 25
	Trying 192.168.1.111...
	Connected to smtp.test-it.net.
	Escape character is '^]'.
	220 test-it.net (IMail 9.23 574278-2) NT-ESMTP Server X1
	EHLO oradb.test-it.net
	250-test-it.net says hello
	250-SIZE 0
	250-8BITMIME
	250-DSN
	250-ETRN
	250-AUTH LOGIN CRAM-MD5
	250-AUTH LOGIN
	250-AUTH=LOGIN
	250 EXPN
	MAIL FROM:dylanninin@test-it.net
	250 ok
	RCPT TO:anyone@test-it.net
	250 ok its for <anyone@test-it.net>
	DATA
	354 ok, send it; end with <CRLF>.<CRLF>
	Subject:telnet smtp test
	
	This is a test message with telnet smtp
	.
	250 Message queued
	quit
	221 Goodbye
	Connection closed by foreign host.

查看邮件箱，或者`/var/mail/log`确保可以收到以上邮件。

若邮件服务有问题，需先配置好，才能进行`UTL_MAIL`的测试。

###UTL_MAIl使用示例

###发送第一封邮件

	SQL> conn dev
	Enter password: 
	Connected.
	SQL> begin
	  2  utl_mail.send(sender=>'dylanninin@test-it.net',
	  3  recipients=>'anyone@test-it.net',
      4  subject=>'utl_mail smtp test',
      5  message=>'This is a test message with utl_mail');
	  6  end;
	  7  /
	begin
	*
	ERROR at line 1:
	ORA-06502: PL/SQL: numeric or value error
	ORA-06512: at "SYS.UTL_MAIL", line 654
	ORA-06512: at "SYS.UTL_MAIL", line 671
	ORA-06512: at line 2

出现以上错误，是因为`smtp_out_server`没有设置。

设置`smtp_out_server`参数：

	SQL> conn /as sysdba
	Connected.
	SQL> show parameter smtp_out_server
	
	NAME				     TYPE	 	 VALUE
	------------------------ ----------- ---------------------------
	smtp_out_server 		 string

	SQL> alter system set smtp_out_server='smtp.it-test.net';
	System altered.

发送以上同样的邮件，出现新的错误：

	ERROR at line 1:
	ORA-24247: network access denied by access control list (ACL)
	ORA-06512: at "SYS.UTL_MAIL", line 654
	ORA-06512: at "SYS.UTL_MAIL", line 671
	ORA-06512: at line 2

查看MOS，原来是Oracle做了ACL限制，需要授权允许用户DEV访问外部网络。 执行以下脚本，授权DEV访问外部网络(若是其他用户，则将DEV改为对应账户即可)：

	BEGIN
	
	  -- Only uncomment the following line if ACL "network_services.xml" has already been created
	  --DBMS_NETWORK_ACL_ADMIN.DROP_ACL('network_services.xml');
	
	  DBMS_NETWORK_ACL_ADMIN.CREATE_ACL(
	    acl => 'network_services.xml',
	    description => 'SMTP ACL',
	    principal => 'DEV',
	    is_grant => true,
	    privilege => 'connect');
	
	  DBMS_NETWORK_ACL_ADMIN.ADD_PRIVILEGE(
	    acl => 'network_services.xml',
	    principal => 'DEV',
	    is_grant => true,
	    privilege => 'resolve');
	
	  DBMS_NETWORK_ACL_ADMIN.ASSIGN_ACL(
	    acl => 'network_services.xml',
	    host => '*');
	
	  COMMIT;
	
	END;

	/


执行后，重新运行上述发邮件代码，运行无错误，查看收件箱也可以成功收到邮件。到此，使用`UTL_MAIL`就成功发送了第一封邮件。

##Reference

* [PL/SQL Packages and Types Reference](http://docs.oracle.com/cd/B28359_01/appdev.111/b28419/u_mail.htm#i1001258)
* Master Note For PL/SQL `UTL_SMTP` and `UTL_MAIL` Packages [ID 1137673.1]

---
layout: post
title: About Information in EBS
category : Oracle
tags : [Oracle, DBA, EBS]
---

About Menu in Oracle EBS 11i

	Oracle Applications
	Copyright (c) 2004 Oracle Corporation,
	Redwood Shores, California.
	All Rights Reserved.

	----------------------------------------
	Login
	----------------------------------------
	Site : ERP
	Application : System Administration
	Responsibility : System Administrator
	Security Group : Standard
	User Name : MIS_XXX

	----------------------------------------
	Database Server
	----------------------------------------
	RDBMS : 9.2.0.6.0
	Oracle Applications : 11.5.10.2
	Machine : erp
	User : APPS
	Oracle SID : ERP
	System Date : 29-JAN-2013 10:05:54
	Database Server PID : 1188030
	Session SID : 176
	SERIAL# : 1751
	AUDSID : 64845869
	Database CPU Usage (in secs) : 0.33


	----------------------------------------
	Forms Server
	----------------------------------------
	Oracle Forms Version : 6.0.8.28.0
	Application Object Library : 11.5.0
	Machine : ERP
	Forms User CPU (secs) : 0.385144
	Forms System CPU (secs) : 0.154454
	Forms Process ID : 1581242

	----------------------------------------
	Forms Server Environment Variables
	----------------------------------------
	AU_TOP : /u2/TEST/testmgr/testappl/au/11.5.0
	FDBDMCHK : [Unset]
	FDFGCXDBG : [Unset]
	FDSQLCHK : [Unset]
	FDUDEBUG : [Unset]
	FNDNAM : APPS
	FND_TOP : /u2/TEST/testmgr/testappl/fnd/11.5.0
	FORMS60_APPSLIBS : APPCORE FNDSQF APPDAYPK APPFLDR GLCORE HR_GEN HR_SPEC ARXCOVER
	FORMS60_CATCHTERM : 1
	FORMS60_ERROR_DATETIME_FORMAT : [Unset]
	FORMS60_ERROR_DATE_FORMAT : [Unset]
	FORMS60_FORCE_MENU_MNEMONICS : 0
	FORMS60_MAPPING : http://erp.egolife.net:8003/OA_TEMP
	FORMS60_MMAP : [Unset]
	FORMS60_OUTPUT : /u2/TEST/testmgr/testcomn/temp
	FORMS60_OUTPUT_DATETIME_FORMAT : [Unset]
	FORMS60_OUTPUT_DATE_FORMAT : [Unset]
	FORMS60_PATH : /u2/TEST/testmgr/testappl/au/11.5.0/resource:/u2/TEST/testmgr/testappl/au/11.5.0/resource/stub
	FORMS60_RESOURCE : [Unset]
	FORMS60_TIMEOUT : 5
	FORMS60_USER_DATETIME_FORMAT : DD-MON-RRRR HH24:MI:SS
	FORMS60_USER_DATE_FORMAT : DD-MON-RRRR
	FORMS60_USE_CBO : [Unset]
	GWYUID : APPLSYSPUB/PUB
	NLS_DATE_FORMAT : DD-MON-RR
	NLS_DATE_LANGUAGE : AMERICAN
	NLS_LANG : AMERICAN_AMERICA.UTF8
	NLS_NUMERIC_CHARACTERS : .,
	ORACLE_HOME : /u2/TEST/testmgr/testora/8.0.6
	ORACLE_PATH : [Unset]
	ORA_NLS_CHARSET_CONVERSION : [Unset]
	TNS_ADMIN : /u2/TEST/testmgr/testora/8.0.6/network/admin/ERP_erp
	TWO_TASK : ERP

	----------------------------------------
	Current Form
	----------------------------------------
	Form Application : Application Object Library
	Form Name : FNDSCSGN
	Form Path : UNKNOWN
	Form Version : 11.5.119
	Form Last Modified : $Date: 2005/10/05 12:50  $

	----------------------------------------
	Forms
	----------------------------------------
	APPSTAND : 11.5.33
	FNDCPVCM : 11.5.27
	FNDSCSGN : 11.5.119

	----------------------------------------
	Form Menus
	----------------------------------------
	FNDMENU : 11.5.49

	----------------------------------------
	Forms PL/SQL
	----------------------------------------
	APPCORE : 11.5.146
	CUSTOM : 11.5.5.1150.1
	FNDCONC : 11.5.27
	FNDSQF : 11.5.111
	GHR : 11.5.150
	GLOBE : 11.5.44.115103.4
	GMS : 11.5.230.90.7
	IGILUTIL2 : 11.5.97
	IGILUTIL : 11.5.29
	OPM : 11.5.33.115100.2
	PQH_GEN : 11.5.118
	PSA : 11.5.162
	PSAC : 11.5.16
	PSB : 11.5.24
	VERT : 11.5.6


---
layout: post
title: Rebuild Indexes
category : Oracle
tags : [Oracle, Database, DBA, Exception]
---

##异常日志

警告日志

	Wed Feb 20 11:00:20 2013
	GATHER_STATS_JOB encountered errors.  Check the trace file.
	Errors in file /db/oracle/diag/rdbms/dbtest/DBTEST/trace/DBTEST_j000_21813.trc:
	ORA-20000: index "BARCODE"."BC_BOXES_I"  or partition of such index is in unusable state

在执行`GATHER_STATS_JOB`任务时，检查到索引 `"BARCODE"."BC_BOXES_I"`存在异常，状态为`unusable`。

跟踪日志：

	ORA-20000: index "BARCODE"."BC_BOXES_I"  or partition of such index is in unusable state
	
	*** 2013-02-20 11:00:20.158
	GATHER_STATS_JOB: GATHER_TABLE_STATS('"BARCODE"','"BC_BOXES"','""', ...)
	ORA-20000: index "BARCODE"."BC_BOXES_I"  or partition of such index is in unusable state

在跟踪日志文件中，可以看到同样的错误信息。

##异常确认

检查索引状态：

	SELECT C.INDEX_NAME,
	       I.UNIQUENESS,
	       C.COLUMN_NAME,
	       C.COLUMN_POSITION,
	       I.STATUS
	  FROM DBA_IND_COLUMNS C, DBA_INDEXES I
	 WHERE I.INDEX_NAME = C.INDEX_NAME
	   AND I.OWNER = 'BARCODE'
	  9   ORDER BY C.INDEX_NAME, C.COLUMN_POSITION;
	
	INDEX_NAME	     UNIQUENESS  COLUMN_NAME	COLUMN_POSITION STATUS
	--------------- 	------------ -------------- --------------- ----------
	BC_BOXES_I	     NONUNIQUE	 BOX_NUMBER 				1 UNUSABLE
	ITEM_MODEL_I	 NONUNIQUE	 MODEL						1 VALID
	ITEM_NUMBER_I	 NONUNIQUE	 ITEM_NUMBER				1 VALID

执行统计信息收集程序，确认异常：

	SQL> conn /as sysdba
	Connected.
	SQL> begin
	  2  dbms_stats.gather_table_stats('"BARCODE"','"BC_BOXES"','""');
	  3  end;
	  4  /
	begin
	*
	ERROR at line 1:
	ORA-20000: index "BARCODE"."BC_BOXES_I"  or partition of such index is in
	unusable state
	ORA-06512: at "SYS.DBMS_STATS", line 20337
	ORA-06512: at "SYS.DBMS_STATS", line 20360
	ORA-06512: at line 2

##解决异常

重建索引：

	SQL> alter index "BARCODE"."BC_BOXES_I" rebuild tablespace barcode_idx nologging;
	
	Index altered.


再次执行搜集程序：

	SQL> begin
	  2  dbms_stats.gather_table_stats('"BARCODE"','"BC_BOXES"','""');
	  3  end;
	  4  /
	
	PL/SQL procedure successfully completed.

##关于重建索引
	
When you rebuild an index, you use an existing index as the data source. Creating an index in this manner enables you to change storage characteristics or move to a new tablespace. Rebuilding an index based on an existing data source removes intra-block fragmentation. Compared to dropping the index and using the `CREATE INDEX` statement, re-creating an existing index offers better performance.

即：重建(rebuild)索引使用已有的索引作为数据源，可以调整存储参数，移除数据块之间的碎片。另外，重要的一点是，相比先删除(drop)再创建索引(create index)，重建(rebuild)能够提供更好的性能。

##参考

 * Oracle Database Administrator's Guide#Rebuilding an Existing Index
---
layout: post
title: Blocked by Sangfor
category : Miscellanies
tags : [Network, Exception]
---

##查询住房公积金

Browse with Chrome

![gjjcx.png](http://dylanninin.com/assets/images/2013/gjjcx.png)

code

	<html><frameset> <frame id='top' src="http://200.200.20.61/disable/disable.htm"><noframes> <body>Your browse does not support frame!</body> </noframes> </frameset> </html>

##ipaddress

![ipaddress](http://dylanninin.com/assets/images/2013/ipaddress.png)

##查询社保

Browse with Chrome

![sbcx.png](http://dylanninin.com/assets/images/2013/sbcx.png)

google 

![google_ip](http://dylanninin.com/assets/images/2013/google_ip.png)


##参考

* [深信服](http://www.sangfor.com.cn/)


---
layout: post
title: Trace Requests in EBS
category : Oracle
tags : [Oracle, DBA, EBS]
---

##Trace File

命名规则：`[oracle_sid]_ora_[server_process_id]_[trace_id].trc` 

* a) oracle_sid：可简单理解为数据库实例id，通过v$instance的instance_name来确定；    
* b) server_process_id：oracle内部标示进程的id，通过v$session的spid来确定；
* c) trace_id：可由`tracefile_identifier`参数指定，通过v$process的traceid来确定，默认为空。

存放路径：由参数 `user_dump_dest`指定，可以通过`show user_dump_dest；`或 
`SELECT NAME, VALUE FROM V$PARAMETER WHERE NAME = 'user_dump_dest';` 来确定。

大小设定：由参数 `max_dump_file_size`指定，以OS block为单位，跟踪时不确定文件大小，可以增加文件大小，或设为 unlimited。

##跟踪请求会话

**根据request_id查找并发请求信息：**

	SELECT D.VALUE || '/' || LOWER(RTRIM(I.INSTANCE, CHR(0))) || '_ora_' || FCR.ORACLE_PROCESS_ID || '.trc' TRACE_FILE,
	       REQUEST_ID,                                                                                                  
	       OS_PROCESS_ID,                                                                                               
	       ORACLE_PROCESS_ID,                                                                                           
	       ORACLE_SESSION_ID,                                                                                           
	       ACTUAL_START_DATE,                                                                                           
	       ACTUAL_COMPLETION_DATE
	  FROM FND_CONCURRENT_REQUESTS FCR,
	       (SELECT T.INSTANCE
	          FROM V$THREAD T, V$PARAMETER V
	         WHERE V.NAME = 'thread'
	           AND (V.VALUE = 0 OR T.THREAD# = TO_NUMBER(V.VALUE))) I,
	       (SELECT VALUE FROM V$PARAMETER WHERE NAME = 'user_dump_dest') D
	 WHERE FCR.REQUEST_ID = 19584084;
 
**根据SPID查询并发进程的当前会话信息：**

	SELECT S.SID,
	       S.SERIAL#,
	       'exec DBMS_SYSTEM.SET_SQL_TRACE_IN_SESSION(' || S.SID || ',' || S.SERIAL# || ',' || 'TRUE' || ');' START_SQL_TRACE,
	       'exec DBMS_SYSTEM.SET_SQL_TRACE_IN_SESSION(' || S.SID || ',' || S.SERIAL# || ',' || 'FALSE' || ');' END_SQL_TRACE,
	       S.STATUS,
	       S.MODULE,                           
	       S.ACTION,                             
	       S.CLIENT_INFO,
	       S.SQL_HASH_VALUE,
	       P.PGA_USED_MEM,
	       P.PGA_MAX_MEM
	  FROM V$SESSION S, V$PROCESS P
	 WHERE S.PADDR = P.ADDR
	   AND S.PROCESS = '2031636'
	   AND P.SPID = 3199034;
 
**跟踪会话，以 ‘225,27128’ 为例：**

开始跟踪，执行 START_SQL_TRACE即可：

	SQL> exec DBMS_SYSTEM.SET_SQL_TRACE_IN_SESSION(225,27128,TRUE);
	 
	PL/SQL procedure successfully completed.
 
结束跟踪，执行 END_SQL_TRACE即可(注意，对于长会话，记得手动结束跟踪，否则会浪费服务器资源)：

	SQL> exec DBMS_SYSTEM.SET_SQL_TRACE_IN_SESSION(225,27128,FALSE);
	 
	PL/SQL procedure successfully completed.

查看跟踪文件：

	$ ll /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_3199034.trc
	-rw-r--r--   1 prodora  dba          745495 Feb 25 15:20 /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_3199034.trc
 
##使用tkprof格式化跟踪文件

因sql trace产生的跟踪文件阅读不太友好，Oracle提供了tkprof工具，可以对此格式化，便于阅读。

**用法：**

	$ tkprof 
	Usage: tkprof tracefile outputfile [explain= ] [table= ]
	              [print= ] [insert= ] [sys= ] [sort= ]
	  table=schema.tablename   Use 'schema.tablename' with 'explain=' option.
	  explain=user/password    Connect to ORACLE and issue EXPLAIN PLAN.
	  print=integer    List only the first 'integer' SQL statements.
	  aggregate=yes|no
	  insert=filename  List SQL statements and data inside INSERT statements.
	  sys=no           TKPROF does not list SQL statements run as user SYS.
	  record=filename  Record non-recursive statements found in the trace file.
	  waits=yes|no     Record summary for any wait events found in the trace file.
	  sort=option      Set of zero or more of the following sort options:
	    prscnt  number of times parse was called
	    ... ...

**使用示例：**

	$ tkprof explain=apps/password
	trace = prod_ora_3199034.trc
	output = prod_ora_3199034.out                           
	 
	TKPROF: Release 9.2.0.6.0 - Production on Mon Feb 25 15:58:26 2013
	 
	Copyright (c) 1982, 2002, Oracle Corporation.  All rights reserved.
 
 
	$ ll prod_ora_3199034*        
	-rw-r--r--   1 prodora  dba          150112 Feb 25 15:58 prod_ora_3199034.out
	-rw-r--r--   1 prodora  dba          745495 Feb 25 15:20 prod_ora_3199034.trc

**查看输出：**

	********************************************************************************
	
	SELECT SOURCE_TYPE 
	FROM
	 MTL_SYSTEM_ITEMS_B WHERE ORGANIZATION_ID = :B2 AND INVENTORY_ITEM_ID = :B1 

	call     count       cpu    elapsed       disk      query    current        rows
	------- ------  -------- ---------- ---------- ---------- ----------  ----------
	Parse        0      0.00       0.00          0          0          0           0
	Execute    156      0.00       0.00          0          0          0           0
	Fetch      156      0.00       0.00          0        780          0         156
	------- ------  -------- ---------- ---------- ---------- ----------  ----------
	total      312      0.00       0.00          0        780          0         156
	
	Misses in library cache during parse: 0
	Optimizer goal: CHOOSE
	Parsing user id: 44  (APPS)   (recursive depth: 1)
	
	Rows     Execution Plan
	-------  ---------------------------------------------------
	      0  SELECT STATEMENT   GOAL: CHOOSE
	      0   TABLE ACCESS   GOAL: ANALYZED (BY INDEX ROWID) OF 
	              'MTL_SYSTEM_ITEMS_B'
	      0    INDEX   GOAL: ANALYZED (UNIQUE SCAN) OF 
	               'MTL_SYSTEM_ITEMS_B_U1' (UNIQUE)
	
	********************************************************************************

##延伸阅读

* [Oracle EBS SQL Trace日志收集的方法](http://blog.csdn.net/pan_tian/article/details/7677120)
* [How to Generate SQL Trace in OAF](http://blog.csdn.net/pan_tian/article/details/8555503)
* [Oracle SQL Trace和10046事件](http://blog.csdn.net/tianlesoftware/article/details/5857023)
* [使用TKProf分析Oracle跟踪文件](http://blog.csdn.net/tianlesoftware/article/details/5632003)

---
layout: post
title: Python Array
category : Python
tags : [Python, Libs]
---

Help on module array:

NAME
    array

FILE
    /usr/lib64/python2.6/lib-dynload/arraymodule.so

DESCRIPTION

    This module defines an object type which can efficiently represent
    an array of basic values: characters, integers, floating point
    numbers.  Arrays are sequence types and behave very much like lists,
    except that the type of objects stored in them is constrained.  The
    type is specified at object creation time by using a type code, which
    is a single character.  The following type codes are defined:
    
        Type code   C Type             Minimum size in bytes 
        'c'         character          1 
        'b'         signed integer     1 
        'B'         unsigned integer   1 
        'u'         Unicode character  2 
        'h'         signed integer     2 
        'H'         unsigned integer   2 
        'i'         signed integer     2 
        'I'         unsigned integer   2 
        'l'         signed integer     4 
        'L'         unsigned integer   4 
        'f'         floating point     4 
        'd'         floating point     8 
    
    The constructor is:
    
    array(typecode [, initializer]) -- create a new array
CLASSES
    __builtin__.object
        array
        array
    
    ArrayType = class array(__builtin__.object)
     |  array(typecode [, initializer]) -> array
     |  
     |  Return a new array whose items are restricted by typecode, and
     |  initialized from the optional initializer value, which must be a list,
     |  string. or iterable over elements of the appropriate type.
     |  
     |  Arrays represent basic values and behave very much like lists, except
     |  the type of objects stored in them is constrained.
     |  
     |  Methods:
     |  
     |  append() -- append a new item to the end of the array
     |  buffer_info() -- return information giving the current memory info
     |  byteswap() -- byteswap all the items of the array
     |  count() -- return number of occurrences of an object
     |  extend() -- extend array by appending multiple elements from an iterable
     |  fromfile() -- read items from a file object
     |  fromlist() -- append items from the list
     |  fromstring() -- append items from the string
     |  index() -- return index of first occurrence of an object
     |  insert() -- insert a new item into the array at a provided position
     |  pop() -- remove and return item (default last)
     |  read() -- DEPRECATED, use fromfile()
     |  remove() -- remove first occurrence of an object
     |  reverse() -- reverse the order of the items in the array
     |  tofile() -- write all items to a file object
     |  tolist() -- return the array converted to an ordinary list
     |  tostring() -- return the array converted to a string
     |  write() -- DEPRECATED, use tofile()

     |  Attributes:
     |  
     |  typecode -- the typecode character used to create the array
     |  itemsize -- the length in bytes of one array item


	>>> c = array.array('c','abcdefg')
	>>> h = array.array('H',c.tostring())
	Traceback (most recent call last):
	  File "<stdin>", line 1, in <module>
	ValueError: string length not a multiple of item size
	>>> h = array.array('H',c.tostring()+'\x00')
	>>> h
	array('H', [25185, 25699, 26213, 103])

	>>> for i in c.tostring():
	...     print '%s%5d%10s' % (i, ord(i),bin(ord(i)))
	... 
	a   97 0b1100001
	b   98 0b1100010
	c   99 0b1100011
	d  100 0b1100100
	e  101 0b1100101
	f  102 0b1100110
	g  103 0b1100111

	>>> for i in h.tolist():
	...     f = chr(i&0x00ff)
	...     l = chr(i>>8)
	...     print '%s%5d%10s' % (f,ord(f),bin(ord(f)))
	...     print '%s%5d%10s' % (l,ord(l),bin(ord(l)))
	... 
	a   97 0b1100001
	b   98 0b1100010
	c   99 0b1100011
	d  100 0b1100100
	e  101 0b1100101
	f  102 0b1100110
	g  103 0b1100111
	    0       0b0
---
layout: post
title: A Case of RMAN Exception
category : Oracle
tags : [Oracle, Database, DBA, Exception]
---

##RMAN备份异常

备份日志：

	[oracle@oradb data]$ cat 20130226_220001-inc2.log

	Recovery Manager: Release 11.2.0.1.0 - Production on Tue Feb 26 22:00:01 2013
	
	Copyright (c) 1982, 2009, Oracle and/or its affiliates.  All rights reserved.
	
	connected to target database: DBTEST (DBID=1193549399)
	
	RMAN> run{
	2>    sql 'alter system archive log current';
	3>    backup 
	4>    incremental level 2
	5>    format '/db/rmanbak/data/%T_inc2_%U.bkp'
	6>    tag 'weekly inc2 backup'
	7>    database plus archivelog;
	8> }
	9> 
	using target database control file instead of recovery catalog
	sql statement: alter system archive log current
	
	
	Starting backup at 26-FEB-13
	current log archived
	allocated channel: ORA_DISK_1
	channel ORA_DISK_1: SID=494 device type=DISK
	allocated channel: ORA_DISK_2
	channel ORA_DISK_2: SID=16 device type=DISK
	allocated channel: ORA_DISK_3
	channel ORA_DISK_3: SID=516 device type=DISK
	allocated channel: ORA_DISK_4
	channel ORA_DISK_4: SID=33 device type=DISK
	RMAN-00571: ===========================================================
	RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS ===============
	RMAN-00571: ===========================================================
	RMAN-03002: failure of backup plus archivelog command at 02/26/2013 22:00:05
	RMAN-06059: expected archived log not found, loss of archived log compromises recoverability
	ORA-19625: error identifying file /db/oracle/arch/1_1263_800686553.dbf
	ORA-27037: unable to obtain file status
	Linux-x86_64 Error: 2: No such file or directory
	Additional information: 3
	
	Recovery Manager complete.

归档日志`/db/oracle/arch/1_1263_800686553.dbf`不存在，导致2013-2-26的RMAN增量备份失败。

确认此文件不存在：

	[oracle@oradb data]# ll /db/oracle/arch/1_1263_800686553.dbf
	ls: cannot access /db/oracle/arch/1_1263_800686553.dbf: No such file or directory
	
查看所有RMAN备份日志（保留最近两周）：

	[oracle@oradb data]$ ll *.log
	-rw-r--r--. 1 oracle oinstall  9450 Feb 12 22:35 20130212_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 10347 Feb 13 22:36 20130213_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 11700 Feb 14 22:36 20130214_220001-inc1.log
	-rw-r--r--. 1 oracle oinstall 12210 Feb 15 22:36 20130215_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 13107 Feb 16 22:36 20130216_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 14073 Feb 17 22:37 20130217_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 24527 Feb 18 22:17 20130218_220001-inc0.log
	-rw-r--r--. 1 oracle oinstall 33349 Feb 19 02:00 20130219_020001-obsolete.log
	-rw-r--r--. 1 oracle oinstall  9450 Feb 19 22:36 20130219_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 10416 Feb 20 22:36 20130220_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 11383 Feb 21 22:36 20130221_220001-inc1.log
	-rw-r--r--. 1 oracle oinstall 12279 Feb 22 22:37 20130222_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 13245 Feb 23 22:37 20130223_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 14210 Feb 24 22:38 20130224_220001-inc2.log
	-rw-r--r--. 1 oracle oinstall 25402 Feb 25 22:16 20130225_220001-inc0.log
	-rw-r--r--. 1 oracle oinstall 33258 Feb 26 02:00 20130226_020001-obsolete.log
	-rw-r--r--. 1 oracle oinstall  1492 Feb 26 22:00 20130226_220001-inc2.log

搜索丢失的归档日志：

	[oracle@oradb data]$ grep 1_1263_800686553.dbf *.log
	20130225_220001-inc0.log:archived log file name=/db/oracle/arch/1_1263_800686553.dbf RECID=1249 STAMP=808334795
	20130226_220001-inc2.log:ORA-19625: error identifying file /db/oracle/arch/1_1263_800686553.dbf

以上结果表明，在20130225_220001-inc0.log中该归档日志可能已经备份。下面进一步确认。

查看20130225_220001-inc0.log：

	[oracle@oradb data]$ less 20130225_220001-inc0.log 

	... ...
	input archived log thread=1 sequence=1263 RECID=1249 STAMP=808334795
	input archived log thread=1 sequence=1264 RECID=1247 STAMP=808334794
	input archived log thread=1 sequence=1265 RECID=1248 STAMP=808334794
	channel ORA_DISK_4: starting piece 1 at 25-FEB-13
	channel ORA_DISK_4: finished piece 1 at 25-FEB-13
	piece handle=/db/rmanbak/data/20130225_inc0_qro2stf6_1_1.bkp tag=WEEKLY INC0 BACKUP comment=NONE
	channel ORA_DISK_4: backup set complete, elapsed time: 00:01:59
	channel ORA_DISK_4: deleting archived log(s)
	... ...
	archived log file name=/db/oracle/arch/1_1262_800686553.dbf RECID=1244 STAMP=808284272
	archived log file name=/db/oracle/arch/1_1263_800686553.dbf RECID=1249 STAMP=808334795
	archived log file name=/db/oracle/arch/1_1264_800686553.dbf RECID=1247 STAMP=808334794
	archived log file name=/db/oracle/arch/1_1265_800686553.dbf RECID=1248 STAMP=808334794
	... ...

列出备份的归档日志：

	RMAN> list backup of archivelog until time 'sysdate - 1';
	
	
	List of Backup Sets
	===================
	
	
	BS Key  Size       Device Type Elapsed Time Completion Time
	------- ---------- ----------- ------------ ---------------
	856     187.71M    DISK        00:01:49     25-FEB-13      
	        BP Key: 922   Status: AVAILABLE  Compressed: YES  Tag: WEEKLY INC0 BACKUP
	        Piece Name: /db/rmanbak/data/20130225_inc0_qro2stf6_1_1.bkp
	
	  List of Archived Logs in backup set 856
	  Thrd Seq     Low SCN    Low Time  Next SCN   Next Time
	  ---- ------- ---------- --------- ---------- ---------
	  ... ...
	  1    1262    58178567   25-FEB-13 58246233   25-FEB-13
	  1    1263    58246233   25-FEB-13 58270069   25-FEB-13
	  1    1264    58270069   25-FEB-13 58271064   25-FEB-13
	  1    1265    58271064   25-FEB-13 58273071   25-FEB-13
	  ... ...

以上结果表明，序号为1263的归档日志已经备份在`20130225_inc0_qro2stf6_1_1.bkp`文件中。在本例中，`inc0`为RMAN增量备份的初始增量，全备时会删除归档日志。具体脚本如下：

	[oracle@oradb ~]$ cat /dba/scripts/inc0.rman 
	run{
	   sql 'alter system archive log current';
	   backup 
	   incremental level 0
	   format '/db/rmanbak/data/%T_inc0_%U.bkp'
	   tag 'weekly inc0 backup'
	   database plus archivelog delete input;
	}

出现`RMAN-06059`错误提示归档日志丢失，可能是控制文件中记录的归档日志信息和实际存在的归档日志不同步。按理在RMAN备份时会自动同步控制文件，但该归档日志并非人为的丢失，故怀疑这里可能出现类BUG的异常，不知能否重现。此时需要手动运行`crosscheck` 和 `delete`命令，使之同步。

交叉校验所有归档日志：

	RMAN> crosscheck archivelog all;

	using target database control file instead of recovery catalog
	allocated channel: ORA_DISK_1
	channel ORA_DISK_1: SID=56 device type=DISK
	allocated channel: ORA_DISK_2
	channel ORA_DISK_2: SID=51 device type=DISK
	allocated channel: ORA_DISK_3
	channel ORA_DISK_3: SID=504 device type=DISK
	allocated channel: ORA_DISK_4
	channel ORA_DISK_4: SID=14 device type=DISK
	validation succeeded for archived log
	
	Crosschecked 19 objects
	
	validation failed for archived log
	archived log file name=/db/oracle/arch/1_1263_800686553.dbf RECID=1245 STAMP=808334320
	validation failed for archived log
	archived log file name=/db/oracle/arch/1_1264_800686553.dbf RECID=1246 STAMP=808334457
	Crosschecked 2 objects

删除失效的归档日志：

	RMAN> delete expired archivelog all;
	
	released channel: ORA_DISK_1
	released channel: ORA_DISK_2
	released channel: ORA_DISK_3
	released channel: ORA_DISK_4
	allocated channel: ORA_DISK_1
	channel ORA_DISK_1: SID=56 device type=DISK
	allocated channel: ORA_DISK_2
	channel ORA_DISK_2: SID=51 device type=DISK
	allocated channel: ORA_DISK_3
	channel ORA_DISK_3: SID=504 device type=DISK
	allocated channel: ORA_DISK_4
	channel ORA_DISK_4: SID=14 device type=DISK
	List of Archived Log Copies for database with db_unique_name DBTEST
	=====================================================================
	
	Key     Thrd Seq     S Low Time 
	------- ---- ------- - ---------
	1245    1    1263    X 25-FEB-13
	        Name: /db/oracle/arch/1_1263_800686553.dbf
	
	1246    1    1264    X 25-FEB-13
	        Name: /db/oracle/arch/1_1264_800686553.dbf
	
	
	Do you really want to delete the above objects (enter YES or NO)? YES
	deleted archived log
	archived log file name=/db/oracle/arch/1_1263_800686553.dbf RECID=1245 STAMP=808334320
	deleted archived log
	archived log file name=/db/oracle/arch/1_1264_800686553.dbf RECID=1246 STAMP=808334457
	Deleted 2 EXPIRED objects

手动执行RMAN增量备份，则可以正常完成。

##Reference

* [RMAN命令详解](http://blog.csdn.net/tianlesoftware/article/details/4976998)
* [Oracle RMAN备份与恢复](http://www.dylanninin.com/blog/2012/10/rman-backup-and-recovery-of-oracle.html)
---
layout: post
title: DeadLock in Oracle Database
category : Oracle
tags : [Oracle, Database, DBA]
---

##死锁

在事务运行高峰期，由于DDL、DML使用锁资源十分频繁，争用时甚至会发生死锁，阻塞的锁可能导致系统延迟，甚至长时间不响应，影响业务系统的正常运行。若死锁频频发生，则有可能是应用设计不当。这里暂且初步整理下锁争用的有关资料。

###1.锁介绍

####概述

锁：并发访问、更新时容易出现

概念：当数据库对象正在被其他进程或用户修改时，可以保护它不被修改 		

结构：“排队”的队列，先到先服务，串行工作

功能：

* 坚持一致性和完整性
* 队列结构管理请求的会话
* 自动处理锁机制
* 锁的持续时间等于被提交事务的长度或处理时间

####类型

DDL

* 在修改过程中保护模式对象
* DDL锁，由Oracle自动发布和释放

DML

* 在事务处理过程中保护对象
* 当发布rollback或commit时，认为完成了事务

内部锁

* 由Oracle管理，以保护内部数据库结构，如数据文件

####模式

锁模式描述 ...
 
锁模式和DML语句 ...

锁模式和DDL语句 ...

####级别

#####数据库级别

锁定数据库以禁止任何新会话和新事务，可以分为以下三种类型。

	ALTER SYSTEM ENABLE RESTRICTED SESSION
	
	* 使数据库进入限制模式
	* 仅有RESTRICTED SESSION权限用户才能登陆
	* 已经登陆的会话不受此印象
	* 登陆的用户可以正常进行DDL/DML操作
	
	ALTER SYSTEM QUIESCE RESTRICTED
	
	* 使数据库进入静默的限制模式
	* 从用户的活动中锁定数据库，锁定所有操作（处于等待状态），直到UNQUICESCE
	* 不允许非特权用户登陆
	* 特权用户可以正常进行操作
	
	ALTER DATABASE OPEN READ ONLY
	
	* 使数据库以只读模式打开
	* UNDO段处于脱机状态
	* 不允许任何更新、事务操作

#####表级别

* 通过DML或者LOCK语句发出

#####行级别

* DML更新一行或多行时
* Oracle支持的最低级别
* SELECT ... FOR UPDATE只锁定返回的行

#####列级别

* Oracle不支持

####锁语句

LOCK语法

	 LOCK TABLE
	   [ schema.]{table | view} [@dblink]
	   [, [schema.]{table | view} [@dblink] ] ...
	 IN lockmode MODE
	   [ NOWAIT]

示例

	* SHARE(S)
	* ROW SHARE(RS)
	* ROW EXCLUSIVE(RX)
	* SHARE ROW EXCLUSIVE(SRX)
	* EXCLUSIVE(X)
	* SELECT ... FOR UPDATE

###2.死锁日志

####警告日志

	Wed Feb 27 09:34:58 2013
	ORA-000060: Deadlock detected. More info in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_4513878.trc.

####跟踪日志

	... ...
	*** 2013-02-11 03:01:37.846
	*** SESSION ID:(25.18) 2013-02-11 03:01:37.846
	Undo Segment 11 Onlined 
	*** 2013-02-11 08:41:27.866 
	Undo Segment 19 Onlined 
	*** 2013-02-16 08:46:16.482
	Undo Segment 134 Onlined
	*** 2013-02-16 09:55:31.314
	Undo Segment 195 Onlined
	*** 2013-02-27 09:34:58.033
	DEADLOCK DETECTED
	Current SQL statement for this session:
	UPDATE FND_CONCURRENT_REQUESTS SET PP_END_DATE = SYSDATE, POST_REQUEST_STATUS = 'E' WHERE REQUEST_ID = :B1 
	----- PL/SQL Call Stack -----
	  object      			line  		object
	  handle    			number  	name
	700000095f756d0        		67  	package body APPS.FND_CP_OPP_CMD
	70000008caf2ea0         	1  		anonymous block
	The following deadlock is not an ORACLE error. It is a
	deadlock due to user error in the design of an application
	or from issuing incorrect ad-hoc SQL. The following
	information may aid in determining the deadlock:
	Deadlock graph:
	                       ---------Blocker(s)--------  ---------Waiter(s)---------
	Resource Name          process session holds waits  process session holds waits
	TX-0159001e-000075b3        24      25     X             24      25           X
	session 25: DID 0001-0018-00000006	session 25: DID 0001-0018-00000006
	Rows waited on:
	Session 25: obj - rowid = 00009050 - AAAJBQAAmAAAKPzAAG
	  (dictionary objn - 36944, file - 38, block - 41971, slot - 6)
	Information on the OTHER waiting sessions:
	End of information on OTHER waiting sessions.
	... ...

###3.死锁情形模拟

若在请求锁资源时出现循环等待，则产生死锁现象。如下，以列出顺序先后执行更新操作。

1.获取并保持锁，如LOCK1，且为排他锁，锁定更新行1：
	
	SQL(49,180)>update dt_char set name = '1A' where id = 1;
	
	1 row updated.

2.获取并保持锁，如LOCK2，且为排他锁，锁定更新行2：	

	SQL(462,13)> update dt_char set name = '2B' where id = 2;
	
	1 row updated.

3.请求更新行2，因处于等待，等待锁LOCK2

	SQL(49,180)>update dt_char set name = '2A' where id = 2;
	
4.请求更新行1，因处于等待，等待锁LOCK1
	
	SQL(462,13)> update dt_char set name = '1B' where id = 1;

此时，会话(49,180)和(462,13)在锁资源LOCK1和LOCK2上存在循环等待的情形，故出现死锁,3中的语句终止；此时4中的语句处于等待状态。

	SQL(49,180)>update dt_char set name = '2A' where id = 2;
	update dt_char set name = '2A' where id = 2
	*
	ERROR at line 1:
	ORA-00060: deadlock detected while waiting for resource

出现死锁时，系统会


###4.死锁解决方案

**ORA-00060 deadlock detected while waiting for resource**

**Cause**:  Your session and another session are waiting for a resource locked by the other. This condition is known as a deadlock. To resolve the deadlock, one or more statements were rolled back for the other session to continue work.

**Action**:  Either:

*  Enter a ROLLBACK statement and re-execute all statements since the last commit or
* Wait until the lock is released, possibly a few minutes, and then re-execute the rolled back statements.

除了执行ROLLBACK，或者等待锁资源释放，并重新执行所需要的语句外；还有一种方案就是清理掉死锁中阻塞的一方的数据库会话。

	--生成kill session脚本，杀掉死锁中阻塞一方的会话
	--ALTER SYSTEM KILL SESSION 'sid,serial#';
	SELECT 'alter system kill session ''' || SID || ',' || SERIAL# || ''';' "Deadlock"
	  FROM V$SESSION
	 WHERE SID IN (SELECT SID FROM V$LOCK WHERE BLOCK = 1);
	
- - -

##DDL Lock

DDL语句在发布时为获取数据对象的内部结构而需要排他锁，如果锁不可用则会更新失败。

###1.直接发布ALTER TABLE语句

建立`SQL*Plus`会话(27，0），发出ALTER TABLE语句：

	SQL> SELECT * FROM V$MYSTAT WHERE ROWNUM = 1;

	       SID STATISTIC#	   VALUE
	---------- ---------- ----------
		27	    0	       0

	SQL> desc dt_char;
	 Name					   Null?    Type
	 ----------------------------------------- -------- ----------------------------
	 NAME						    CHAR(10)
	
	SQL> alter table dt_char modify (name varchar(20));
	
	Table altered.
	
	SQL> desc dt_char;
	 Name					   Null?    Type
	 ----------------------------------------- -------- ----------------------------
	 NAME						    VARCHAR2(20)

###2.有DML更新表数据时发布DDL语句

####默认

新开一个`SQL*Plus`会话(513,22540)，发出UPDATE语句：

	SQL> SELECT * FROM V$MYSTAT WHERE ROWNUM = 1;

	       SID STATISTIC#	   VALUE
	---------- ---------- ----------
		513	    22540	       0

	SQL> update dt_char set name = 'DDL Lock';

	0 rows updated.

此时，在会话(27,0)中再次发布ALTER TABLE语句：

	SQL> alter table dt_char modify (name char(10));
	alter table dt_char modify (name char(10))
	            *
	ERROR at line 1:
	ORA-00054: resource busy and acquire with NOWAIT specified or timeout expired

此时，提示要求更新的表结构正处于繁忙状态(被其他会话占用)，因不愿意等待锁释放或者等待超时，故更新失败。

查看哪些对象处于争用之中：

	SQL> SELECT lo.session_id,
       s.serial#,
       lo.oracle_username,
       o.owner,
       o.object_name,
       o.object_type,
       decode(lo.locked_mode,
              0,
              'None', /* Mon Lock equivalent */
              1,
              'Null', /* N */
              2,
              'Row-S (SS)', /* L */
              3,
              'Row-X (SX)', /* R */
              4,
              'Share', /* S */
              5,
              'S/Row-X (SSX)', /* C */
              6,
              'Exclusive', /* X */
              lo.locked_mode) locked_mode
  	FROM v$locked_object lo,
	       dba_objects     o,
	       v$session       s
	 WHERE lo.object_id = o.object_id
	   AND s.sid = lo.session_id;
	
   	SESSION_ID 	  SERIAL# ORACLE_USERNAME		OWNER  OBJECT_NAME  OBJECT_TYPE	LOCKED_MODE
	---------- ---------- ---------------- ---------- ------------ ------------ -----------
		513			22540 DEV			   DEV		  DT_CHAR	   TABLE		Row-X (SX)

从以上查询可以看出，表DEV.DT_CHAR正在被会话(513.22540)占用，锁模式为Row-X，即排他锁，为了更新，在某个事务中锁定了行，则不允许其他事务锁定这个表。

在这个例子中，发布的DML后，已经获取了锁，并锁定了表DEV.DT_CHAR，因没有发布ROLLBACK、COMMIT，该会话一直持有锁资源。

####指定TIMEOUT

在会话(27，0）中，发布超时为30s:

	SQL@（27，0）> alter session set ddl_lock_timeout=30;		--DDL_LOCK_TIMEOUT参数值默认为0
	
	Session altered.
	
	SQL@（27，0）> alter table dt_char modify (name char(10));	--处于等待状态，超时30s

此时，若在发布DDL时起30s内，会话(513,22540)释放锁资源，则DDL可以更新成功：

	SQL@(513,22540)> commit;

	Commit complete.

	SQL@（27，0）> alter table dt_char modify (name char(10));	--处于等待状态，超时30s
	
	Table altered.

否则，DDL失败：

	SQL@（27，0）> alter table dt_char modify (name char(10));	--处于等待状态，超时30s
	alter table dt_char modify (name char(10))
	            *
	ERROR at line 1:
	ORA-00054: resource busy and acquire with NOWAIT specified or timeout expired

- - -

##DML Lock

###1.事务和保存点

原始数据：

	SQL> select * from dt_char;
	
	NAME
	----------
	1
	2
	3
	4
	5
	
更新行：

	SQL> update dt_char set name = '1A' where name = '1';
	
	1 row updated.

	SQL> select * from dt_char;
	
	NAME
	----------
	1A
	2
	3
	4
	5

创建保存点，用来控制事务	

	SQL> savepoint A;
	
	Savepoint created.
	
更新行：

	SQL> update dt_char set name = '2A' where name = '2';
	
	1 row updated.
	
	SQL> select * from dt_char;
	
	NAME
	----------
	1A
	2A
	3
	4
	5

回滚到保存点A，此时在A之前的锁没有释放，在A之后的锁则被释放，更新将会回滚到保存A（若不指定保存点，则回滚到更新的初始状态）：	

	SQL> rollback to savepoint A;
	
	Rollback complete.
	
	SQL> select * from dt_char;
	
	NAME
	----------
	1A
	2
	3
	4
	5

##常用脚本

查询当前会话ID

	--查询当前会话ID
	SELECT * FROM V$MYSTAT WHERE ROWNUM = 1;

查询会话锁

	--查询会话锁
	SELECT * FROM DBA_LOCKS WHERE SESSION_ID IN (513);
	

检测锁争用

	--request:如果request值非0，则表示在等待一个锁
	--block:如果block为1，则表示此会话持有一个锁，并阻塞别人获得此锁
	SELECT *
	  FROM V$LOCK
	 WHERE BLOCK = 1
	    OR REQUEST > 0;
	
被锁定的对象

	--被锁定的对象
	SELECT * FROM V$LOCKED_OBJECT;
	
仅列出用户所保持的锁

	--仅列出用户所保持的锁
	SELECT S.USERNAME,
	       L.SESSION_ID,
	       L.LOCK_TYPE,
	       L.MODE_HELD,
	       L.MODE_REQUESTED,
	       L.LOCK_ID1,
	       L.LOCK_ID2,
	       L.LAST_CONVERT,
	       L.BLOCKING_OTHERS
	  FROM V$SESSION S, DBA_LOCKS L
	 WHERE S.USERNAME IS NOT NULL
	   AND (S.SID = L.SESSION_ID AND L.MODE_REQUESTED != 'NONE')
	    OR (S.SID = L.SESSION_ID AND L.MODE_REQUESTED = 'NONE' AND
	       L.MODE_HELD != 'Share' AND
	       (LOCK_ID1, LOCK_ID2) IN
	       (SELECT A.LOCK_ID1, A.LOCK_ID2
	           FROM DBA_LOCKS A
	          WHERE A.MODE_REQUESTED != 'NONE'
	            AND A.LOCK_ID1 = L.LOCK_ID1
	            AND A.LOCK_ID2 = L.LOCK_ID2))
	 ORDER BY 6, 7, 5;
	
哪些对象处于争用中

	--v$locked_object
	--哪些对象处于争用中
	SELECT LO.SESSION_ID,
	       S.SERIAL#,
	       LO.ORACLE_USERNAME,
	       S.PROCESS,
	       S.PROGRAM,
	       O.OWNER,
	       O.OBJECT_NAME,
	       O.OBJECT_TYPE,
	       DECODE(LO.LOCKED_MODE,
	              0,
	              'None', /* Mon Lock equivalent */
	              1,
	              'Null', /* N */
	              2,
	              'Row-S (SS)', /* L */
	              3,
	              'Row-X (SX)', /* R */
	              4,
	              'Share', /* S */
	              5,
	              'S/Row-X (SSX)', /* C */
	              6,
	              'Exclusive', /* X */
	              LO.LOCKED_MODE) LOCKED_MODE
	  FROM V$LOCKED_OBJECT LO, DBA_OBJECTS O, V$SESSION S
	 WHERE LO.OBJECT_ID = O.OBJECT_ID
	   AND S.SID = LO.SESSION_ID
	   AND S.SID IN (336, 1360);
	
死锁检测

	--dba_blockers
	--阻塞其他用户的会话的id
	SELECT * FROM DBA_BLOCKERS;
	
	--dba_waiters
	--显示等待将被阻塞会话释放锁的会话id
	SELECT * FROM DBA_WAITERS;

	--使用dba_waiters视图
	信息格式：Blocker|Waiter(sid,serial#,username,sql)
	SELECT 'Blocker(' || BW.HOLDING_SESSION || ':' || SB.USERNAME ||
	       ') - SQL: ' || BQ.SQL_TEXT BLOCKERS,
	       'Waiter(' || BW.WAITING_SESSION || ':' || SW.USERNAME || ') - SQL: ' ||
	       WQ.SQL_TEXT BLOCKERS
	  FROM DBA_WAITERS BW,
	       V$SESSION   SB,
	       V$SESSION   SW,
	       V$SQLAREA   BQ,
	       V$SQLAREA   WQ
	 WHERE BW.HOLDING_SESSION = SB.SID
	   AND BW.WAITING_SESSION = SW.SID
	   AND SB.PREV_SQL_ADDR = BQ.ADDRESS
	   AND SW.SQL_ADDRESS = WQ.ADDRESS
	   AND BW.MODE_HELD <> 'None';

	--使用dba_locks视图
	--信息格式：Blocker|Waiter(sid,serial#,username,sql)
	SELECT DISTINCT 'Blocker(' || LB.SESSION_ID || ',' || SB.SERIAL# || ':' ||
	                SB.USERNAME || ') - SQL: ' || BQ.SQL_TEXT BLOCKERS,
	                'Waiter(' || LW.SESSION_ID || ',' || SW.SERIAL# ||
	                SW.USERNAME || ') - SQL: ' || WQ.SQL_TEXT BLOCKERS
	  FROM DBA_LOCKS LB,
	       V$SESSION SB,
	       DBA_LOCKS LW,
	       V$SESSION SW,
	       V$SQL     BQ,
	       V$SQL     WQ
	 WHERE LB.SESSION_ID = SB.SID
	   AND LW.SESSION_ID = SW.SID
	   AND SB.PREV_SQL_ADDR = BQ.ADDRESS
	   AND SW.SQL_ADDRESS = WQ.ADDRESS
	   AND LB.LOCK_ID1 = LW.LOCK_ID1
	   AND SW.LOCKWAIT IS NOT NULL
	   AND SB.LOCKWAIT IS NULL
	   AND LB.BLOCKING_OTHERS = 'Blocking';


生成kill session脚本

	--生成kill session脚本，杀掉死锁中阻塞一方的会话
	--ALTER SYSTEM KILL SESSION 'sid,serial#';
	SELECT 'alter system kill session ''' || SID || ',' || SERIAL# || ''';' "Deadlock"
	  FROM V$SESSION
	 WHERE SID IN (SELECT SID FROM V$LOCK WHERE BLOCK = 1);


查看导致死锁的SQL	

	--导致死锁的SQL 
	SELECT S.SID, Q.SQL_TEXT
	  FROM V$SQLTEXT Q, V$SESSION S
	 WHERE Q.ADDRESS = S.SQL_ADDRESS
	   AND S.SID = &SID
	 ORDER BY PIECE;

##延伸阅读

* [锁 死锁 阻塞 Latch 等待](http://blog.csdn.net/tianlesoftware/article/details/5822674)

##Reference

* Database Error Message
* Oracle 9i数据库性能优化与调整
---
layout: post
title: LogMiner Introduction
category : Oracle
tags : [Oracle, Database, DBA]
---

##LogMiner介绍

OracleLogMiner是 Oracle 公司从产品 8i 以后提供的一个实际非常有用的分析工具，使用该工具可以轻松获得 Oracle 重作日志文件（归档日志文件）中的具体内容，特别是，该工具可以分析出所有对于数据库操作的 DML（insert、update、delete 等）语句，9i 后可以分析 DDL 语句，另外还可分析得到一些必要的回滚 SQL 语句。其中一个最重要的用途就是不用全部恢复数据库就可以恢复数据库的某个变化。该工具特别适用于调试、审计或者回退某个特定的事务。

Log Miner工具即可以用来分析在线，也可以用来分析离线日志文件，即可以分析本身自己数据库的重作日志文件，也可以用来分析其他数据库的重作日志文件。

总的说来，LogMiner 工具的主要用途有:

* 跟踪数据库的变化，可以离线的跟踪数据库的变化，而不会影响在线系统的性能。
* 回退数据库的变化，回退特定的变化数据，减少 point-in-time recovery 的执行。
* 优化和扩容计划，可通过分析日志文件中的数据以分析数据增长模式。
* 确定数据库的逻辑损坏时间，准确定位操作执行的时间和SCN(基于时间和SCN 的恢复)
* 确定事务级要执行的精细逻辑恢复操作(取得相应的 UNDO 操作)
* 执行后续审计(DML、DDL、DCL、执行时间、用户)

需要注意的是，在没有启动supplemental log的情况下，Mining出来的`SQL_REDO`和`SQL_UNDO`数据是没有经过数据字典进行转换的（使用Oracle的内部ID标示），可读性很差。一般生产环境不会启动supplemental log。所以用Logminer 方法来做数据恢复不是一种常用的方法。 对于DML操作，还是有一定的可行性。

##安装LogMiner

要安装 LogMiner 工具，必须首先要运行下面这样两个脚本：

* `$ORACLE_HOME/rdbms/admin/dbmslm.sql` 	创建`DBMS_LOGMNR`包，分析日志
* `$ORACLE_HOME/rdbms/admin/dbmslmd.sql` 	创建`DBMS_LOGMNR_D`包，创建数据字典

注：这两个脚本必须均以SYS用户身份运行。


##LogMiner使用步骤

###设置日志文件存放位置
因`UTL_FILE_DIR`为静态参数，设置后，需要重启数据库才能生效。

	SQL> alter system set utl_file_dir='/dba' scope=spfile;

###弹回数据库

	SQL> shutdown immediate;

	SQL> startup

###建立LogMiner分析数据字典

	SQL> exec dbms_logmnr_d.build(dictionary_filename=>'logmnr_dict.dat',dictionary_location=>'/dba');

###添加用于分析的日志文件

	SQL> exec dbms_logmnr.add_logfile(options=>dbms_logmnr.new,logfilename=>'/db/oracle/arch/1_9_808334794.arc');	

###执行LogMiner分析

	SQL> exec dbms_logmnr.start_logmnr(dictfilename=>'/dba/logmnr_dict.dat');

###查看分析结果

	SQL> select operation,sql_redo,sql_undo from v$logmnr_contents;
	
	... ...

###结束分析

	SQL> exec dbms_logmnr.end_logmnr();

###保存分析结果

因`v$logmnr_content`只对执行过`dbms_logmnr`程序包的会话有效，这里可以通过创建表的方式保存结果，以便长期查看和分析。这里需要注意以下两点：统一标准时间格式，复制分析结构数据时不要成日志记录(无需再分析)。

修改时间格式：

	SQL>alter session set nls_date_format='yyyy-mm-dd hh24:mi:ss';

创建表结构：
	 
	SQL> create table hn_logmnr nologging as select * from v$logmnr_contents where 1=2;
	
使用APPEND直接插入数据：

	SQL> insert /*+append */ into hn_logmnr select * from v$logmnr_contents;
	 
提交事务：

	SQL> commit;

##主要视图

	--历史日志文件信息
	select * from v$loghist;
	
	--LogMnr字典文件信息
	select * from v$logmnr_dictionary;
	
	--LogMnr参数信息
	select * from v$logmnr_parameters;
	
	--显示用于分析的日志列表
	select * from v$logmnr_logs;
	
	--LogMnr分析结果 仅对执行分析的会话有效
	select * from v$logmnr_contents;

##延伸阅读

* [Oracle Redo Log机制小结](http://blog.csdn.net/tianlesoftware/article/details/7346212)
* [LogMiner说明](http://blog.csdn.net/tianlesoftware/article/details/5604497)
* [LogMiner做数据恢复 说明示例](http://blog.csdn.net/tianlesoftware/article/details/6554674)
---
layout: post
title: Privileges in PL/SQL
category : Oracle
tags : [Oracle, Database, DBA]
---

##监控临时段使用

最近一个Web应用后端的数据库经常出现ORA-01652异常，即扩展临时段失败；关键在于该临时段在不久前已经扩展过一次，所以这里不再继续扩展，而是计划监控临时段的使用情况，并从中发现不合理的语句。

	ORA-1652: unable to extend temp segment by 128 in tablespace APPS_TMP 

数据库版本

	DEV> select * from v$version;
		BANNER
	----------------------------------------------------------------------------
		Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
		PL/SQL Release 11.2.0.1.0 - Production
		CORE	11.2.0.1.0	Production
		TNS for Linux: Version 11.2.0.1.0 - Production
		NLSRTL Version 11.2.0.1.0 - Production


##创建表结构

`TEMP_SEG_USAGE`用于存放临时段使用统计信息，包括使用的用户、SQL语句等。

	DEV> CREATE TABLE TEMP_SEG_USAGE(
	  2         DATE_TIME DATE,
	  3         USERNAME VARCHAR2(30),
	  4         SID VARCHAR2(6),
	  5         SERIAL# VARCHAR2(6),
	  6         OS_USER VARCHAR2(30),
	  7         SPACE_USED NUMBER,
	  8         SQL_TEXT VARCHAR2(1000)
	  9  );
 
	Table created
 
##创建存储过程

`TEMP_SEG_USAGE_P`存储过程处理`INSERT`操作，方便JOB调用。

	DEV> CREATE OR REPLACE PROCEDURE TEMP_SEG_USAGE_P IS
	  2  BEGIN
	  3      INSERT INTO TEMP_SEG_USAGE
	  4          SELECT SYSDATE,
	  5                 A.USERNAME,
	  6                 A.SID,
	  7                 A.SERIAL#,
	  8                 A.OSUSER,
	  9                 B.BLOCKS,
	 10                 C.SQL_TEXT
	 11            FROM V$SESSION A, V$TEMPSEG_USAGE B, V$SQLAREA C
	 12           WHERE B.TABLESPACE = 'TMP'
	 13             AND A.SADDR = B.SESSION_ADDR
	 14             AND C.ADDRESS = A.SQL_ADDRESS
	 15             AND C.HASH_VALUE = A.SQL_HASH_VALUE
	 16             AND B.BLOCKS * 8192 > 1024
	 17           ORDER BY B.TABLESPACE, B.BLOCKS;
	 18      COMMIT;
	 19  END TEMP_SEG_USAGE_P;
	 20  /
	 
	Warning: Procedure created with compilation errors
	 
查询错误信息

	DEV> SELECT UO.OBJECT_NAME NAME,
	  2         UO.OBJECT_TYPE TYPE,
	  3         UE.LINE        LINENO,
	  4         UE.TEXT        ERR
	  5    FROM USER_OBJECTS UO, USER_ERRORS UE
	  6   WHERE UO.STATUS != 'VALID'
	  7     AND UE.NAME = UO.OBJECT_NAME
	  8   ORDER BY UO.OBJECT_NAME, UO.OBJECT_TYPE, UE.LINE;
	 
	NAME                TYPE         LINENO   ERR
	------------------- ------------ -------- -------------------------------
	TEMP_SEG_USAGE_P    PROCEDURE           3 PL/SQL: SQL Statement ignored
	TEMP_SEG_USAGE_P    PROCEDURE          11 PL/SQL: ORA-00942: 表或视图不存在
	 
单独执行

	DEV>  SELECT SYSDATE,
	  2          A.USERNAME,
	  3          A.SID,
	  4          A.SERIAL#,
	  5          A.OSUSER,
	  6          B.BLOCKS,
	  7          C.SQL_TEXT
	  8    FROM V$SESSION A, V$TEMPSEG_USAGE B, V$SQLAREA C
	  9   WHERE B.TABLESPACE = 'TMP'
	 10     AND A.SADDR = B.SESSION_ADDR
	 11     AND C.ADDRESS = A.SQL_ADDRESS
	 12     AND C.HASH_VALUE = A.SQL_HASH_VALUE
	 13     AND B.BLOCKS * 8192 > 1024
	 14  ORDER BY B.TABLESPACE, B.BLOCKS;

	 no rows selected

从以上输出可以看出，以DEV身份单独单独执行存储过程中的关键SQL时没有问题，但编译存储过程时，却提示表或视图不存在。在这个存储过程中，访问到非该用户拥有的表或视图的只有`V$SESSION`，`V$TEMPSEG_USAGE`，`V$SQLAREA`，推测有可能是访问权限不足导致。
	 
单独授权

	SYS> grant select on v_$session  to dev;
	
	Grant succeeded.
	
	SYS> grant select on v_$sort_usage  to dev;
	
	Grant succeeded.
	
	SYS> grant select on v_$sqlarea  to dev;
	
	Grant succeeded.

重新编译

	DEV> alter procedure TEMP_SEG_USAGE_P compile;
	 
	Procedure altered

##提交JOB

	DEV> BEGIN
			DBMS_JOB.ISUBMIT(JOB=>8001,WHAT=> 'TEMP_SEG_USAGE_P;',NEXT_DATE => SYSDATE,INTERVAL  => 'sysdate + (5/1440)');
			COMMIT;
		END;
		/

	PL/SQL procedure successfully completed.

到这里，监控临时段使用情况的任务已经创建完成，监控一段时间时候，即可获取临时段使用的统计信息，进而发现异常的语句。


##遇到的几个问题

###1.权限异常

在存储过程TEMP_SEG_USAGE_P中，使用到了`V$SESSION`，`V$TEMPSEG_USAGE`,`V$SQLAREA`动态性能视图，但查询当前用户授予的直接角色时，已经有系统标准的DBA角色，有权限查询这些视图。但在存储过程编译时，却提示表或视图不存在。

	SQL> SELECT * FROM USER_ROLE_PRIVS;
	
	USERNAME	 GRANTED_ROLE	 ADMIN_OPTION DEFAULT_ROLE OS_GRANTED
	------------ --------------- ------------ ------------ ----------
	DEV			 CONNECT	     NO  		  NO  		   NO
	DEV			 DBA			 NO  		  YES          NO
	DEV			 RESOURCE 		 NO  		  YES          NO

查看文档才知道，授予role的对象权限，仅在用户会话中有效；在用户创建的存储过程，视图等对象中并不能继承这些角色权限，必须单独为此授权才行。

Note : The privileges assigned to a role can be associated with a user session only and cannot be inherited by any objects (views, stored procedures) that are owned by a user who is granted the role. You cannot use the privileges granted via a role while creating a stored PL/SQL object or a view. For this you have to be granted the object privileges directly. Furthermore the object privileges granted through roles cannot be used by the scheduler jobs. 

It is important to note that the privileges acquired via roles can be exercised when running a procedure with invoker's rights but cannot be used when running a procedure with definer's rights.


###2.同义词授权

在创建上监控程序的过程中，出现过授权访问v$session错误的情况，如下：

	SYS> grant select on v$session to dev;
	grant select on v$session to dev
	                *
	ERROR at line 1:
	ORA-02030: can only select from fixed tables/views

通过查询`dba_synonyms`视图，可以知道v$session是动态性能视图v_$session的同义词，它本身并不是表或者视图。

	SYS> select * from dba_synonyms where synonym_name = 'V$SESSION';

	OWNER	   SYNONYM_NAME 	     TABLE_OWNE TABLE_NAME		  DB_LINK
	---------- ------------------------- ---------- ------------------------- ----------
	PUBLIC	   V$SESSION		     SYS	V_$SESSION

故这里应针对V_$SESSION授权，如下：

	SYS> grant select on v_$session to dev;

	Grant succeeded.


注意

在授权访问V_$动态性能视图以及其他数据字典时，也可以直接授予所有权限：

	SYS> grant select any dictionary to username;

这样将给用户足够的权限去访问所有的数据字典。

###3.什么是Fixed Views

Throughout its operation, Oracle maintains a set of virtual tables that record current database activity. These tables are called dynamic performance tables.

Dynamic performance tables are not true tables, and they should not be accessed by 
most users. However, database administrators can query and create views on the 
tables and grant access to those views to other users. These views are sometimes called fixed views because they cannot be altered or removed by the database administrator. 

In most cases, the information available in fixed views persists across instance shutdowns. However, certain fixed view information is reset when the instance is shut down.

SYS owns the dynamic performance tables; their names all begin with V_$(or GV_$). Views are created on these tables, and then public synonyms are created for the views. The synonym names begin with V$(or GV$). For example, the `V$DATAFILE` view contains information about the database’s datafiles, and the `V$FIXED_TABLE` view contains information about all of the dynamic performance tables and views in the database. 

Standard dynamic performance views (V$ fixed views) store information on the local instance. In contrast, global dynamic performance views (GV$ fixed views) store information on all open instances. Each V$ fixed view has a corresponding GV$ fixed view.

动态性能视图

	SYS> select count(1) from v$fixed_table;
	
	  COUNT(1)
	----------
	      1968

###4.什么是Synonym

A synonym is an alias for any table, view, materialized view, sequence, procedure, function, package, type, Java class schema object, user-defined object type, or another synonym. Because a synonym is simply an alias, it requires no storage other than its definition in the data dictionary.

Synonyms allow underlying objects to be renamed or moved, where only the synonym needs to be redefined and applications based on the synonym continue to function without modification.

You can create both public and private synonyms. A public synonym is owned by the special user group named PUBLIC and is accessible to every user in a database. A private synonym is contained in the schema of a specific user and available only to the user and the user's grantees.

创建语法

	DEV> ? create synonym
	
	 CREATE SYNONYM
	 --------------
	
	 Use this command to create a synonym. A synonym is an alternative
	 name for a table, view, sequence, procedure, stored function,
	 package, snapshot, or another synonym.
	
	 CREATE [PUBLIC] SYNONYM [schema.]synonym
	   FOR [schema.]object [@dblink]


创建私有同义词


	DEV> desc role_t;
	 Name			 Null?	  Type
	 ----------------------- -------- ----------------
	 ID				  NUMBER

	DEV> create synonym synonym_role for role_t;
	
	Synonym created.
	
	DEV> select * from synonym_role;
	
	no rows selected

另外一个用户APP访问

	APP> select * from dev.synonym_role;
	select * from dev.synonym_role
	                  *
	ERROR at line 1:
	ORA-01031: insufficient privileges
	

	APP> select * from dev.role_t;
	select * from dev.role_t
	                  *
	ERROR at line 1:
	ORA-01031: insufficient privileges

给APP添加权限

	DEV> grant select on synonym_role to app;

	Grant succeeded.

APP再次访问

	APP> select * from dev.synonym_role;

	no rows selected
	
	APP> select * from dev.role_t;
	
	no rows selected

这里感觉比较奇怪的是同样是同义词，授权却不一样，`grant select on v$session to username；`提示select操作的对象不对（即select仅能在表或视图上操作），这样就只能通过`grant select on v_$session to username；`授权；而`grant select on synonym_role to username;`却可以正常授权。

##小结

在这个监控临时段使用脚本的创建执行过程中，遇到的主要是权限方面的问题，针对动态性能视图，同义词，以及存储过程。不过，即使根据文档解决了这些问题，还是没理解它们为什么会存在，为什么要这样设计。

这里先记录以作备忘：

* Oracle数据库的权限和角色，以及它们的限制；
* 函数、存储过程的授权；
* Job,Database Link中的权限问题；
* 同义词的授权；

##参考

* Oracle Database Concepts
* [Oracle Temp临时表空间](http://blog.csdn.net/tianlesoftware/article/details/4697417)
* [浅谈Job和Database Link的一个特点](http://yangtingkun.itpub.net/post/468/7984)
* [DBMS_JOB用法](http://blog.csdn.net/tianlesoftware/article/details/4703133)
---
layout: post
title:  Users, Roles, Privileges in Oracle Database
category : Oracle
tags : [Oracle, Database, DBA]
---

##权限

权限(Privilege)：即执行特定语句的能力。权限允许用户访问数据库中其他用户的对象，执行存储过程，或者执行一些系统级的操作。在Oracle数据库系统中，一般分为系统权限，和对象权限。

###1.系统权限

与具体的对象无关，是在任何对象上执行操作的权利，以及运行批处理、改变系统参数、创建角色等方面的权限。
	
	--查询所有系统权限
	SELECT * FROM SYSTEM_PRIVILEGE_MAP;
	
	--查询某个用户/角色的所有系统权限
	SELECT * FROM DBA_SYS_PRIVS DSP WHERE DSP.GRANTEE = UPPER('&grantee');	
	
###2.对象权限

在特定对象上执行特定操作的权限，如SELECT,INSERT,UPDATE等。对象：表、视图、过程等。
	
	--所有对象权限
	SELECT * FROM DBA_TAB_PRIVS;
	
	--某角色被授予的相关表的权限
	SELECT * FROM ROLE_TAB_PRIVS RTP WHERE RTP.ROLE = UPPER('&role');
	
	--使用密码文件的用户
	SELECT * FROM V$PWFILE_USERS;
	
###3.角色相关

一组权限的集合，简化权限的管理。被授予给角色的用户，将继承该角色所授予的所有权限以及角色。角色可以使用密码认证加强安全性。
	
	--所有角色
	SELECT * FROM DBA_ROLES;
	
	--某一角色被授予的系统权限
	SELECT * FROM ROLE_SYS_PRIVS RSP WHERE RSP.ROLE = UPPER('&role');
	
	--用户角色分配关系
	SELECT * FROM DBA_ROLE_PRIVS DRP WHERE DRP.GRANTEE = UPPER('&user');
	
###4.当前用户

与当前登录用户相关的系统权限、对象权限、角色等相关的视图。

	--查看当前用户
	SELECT USER FROM DUAL;
	
	--当前用户授予的系统权限
	SELECT * FROM USER_SYS_PRIVS;
	
	--当前用户所拥有的全部权限
	SELECT * FROM SESSION_PRIVS;
	
	--当前用户所授予的直接角色
	SELECT * FROM USER_ROLE_PRIVS;
	
	--当前用户被授予的所有角色
	SELECT * FROM SESSION_ROLES;
	
	--当前用户所有角色被授予的角色
	SELECT * FROM ROLE_ROLE_PRIVS;
	
	--当前用户的对象权限
	SELECT * FROM TABLE_PRIVILEGES;
	
##权限、角色配置与管理

###1.授权

	SQL> ? grant
	
	 GRANT (Object Privileges)
	 -------------------------
	
	 Use this command to grant privileges for a particular object to
	 users and roles. To grant system privileges and roles, use the GRANT
	 command (System Privileges and Roles).
	
	 GRANT
	   { object_priv | ALL [PRIVILEGES] }
	   [ ( column [, column] ...) ]
	   [, { object_priv | ALL [PRIVILEGES] }
	      [ ( column [, column] ...) ] ] ...
	 ON [ schema.| DIRECTORY] object
	 TO { user | role | PUBLIC} ...
	    [ WITH GRANT OPTION]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.
	
	
	 GRANT (System Privileges and Roles)
	 -----------------------------------
	
	 Use this command to grant system privileges and roles to users and
	 roles. To grant object privileges, use the GRANT command (Object
	 Privileges).
	
	 GRANT
	   { system_priv | role}
	   [, { system_priv | role} ] ...
	 TO
	   { user | role | PUBLIC}
	   [, { user | role | PUBLIC} ] ...
	   [ WITH ADMIN OPTION]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.

注：

*  WITH ADMIN OPTION：带有该选项，则被授权用户也可以管理该权限，即授权、取消授权给其他用户。
* GRANT ANY PRIVILEGE：带有该选项，则被授权用户可以对任何权限进行授权或取消授权。

###2.取消授权

	SQL> ? revoke
	
	 REVOKE (Schema Object Privileges)
	 ---------------------------------
	
	 Use this command to revoke object privileges for a particular object
	 from users and roles. To revoke system privileges or roles, use the
	 REVOKE command (System Privileges and Roles).
	
	 REVOKE
	   { object_priv | ALL [PRIVILEGES] }
	   [, {object_priv | ALL [PRIVILEGES] } ] ...
	 ON
	   [ schema.| DIRECTORY] object
	 FROM
	   { user | role | PUBLIC}
	   [, {user | role | PUBLIC} ] ...
	   [ CASCADE CONSTRAINTS]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.
	
	
	 REVOKE (System Privileges and Roles)
	 ------------------------------------
	
	 Use this command to revoke system privileges and roles from users
	 and roles. To revoke object privileges from users and roles, use the
	 REVOKE command (Object Privileges).
	
	 REVOKE
	   { system_priv | role}
	   [, { system_priv | role} ] ...
	 FROM
	   { user | role | PUBLIC}
	   [, {user | role | PUBLIC} ] ...
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.

###3.角色

####创建角色

	SQL> ? create role
	
	 CREATE ROLE
	 -----------
	
	 Use this command to create a role. A role is a set of privileges
	 that can be granted to users or to other roles.
	
	 CREATE ROLE role [NOT IDENTIFIED | IDENTIFIED {BY password |
	   EXTERNALLY | GLOBALLY} ]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.

####角色选择

创建角色后，可以使用GRANT/REVOKE给角色授予、取消授予适当权限，并最终分配给用户。一个用户可以拥有多个角色，在一个用户会话过程中，用户可以选择性的使某些角色生效或禁用，以控制对系统、对象的访问。

设置当前角色

	SQL> set role connect;
	
	Role set.
	
	SQL> create table role_t(id number);
	create table role_t(id number)
	*
	ERROR at line 1:
	ORA-01031: insufficient privileges
	

使用户所有角色生效

	SQL> set role all;
	
	Role set.

	SQL> create table role_t(id number);
	
	Table created.
	
排除某些角色

	SQL> select count(1) from v$session;
	
	  COUNT(1)
	----------
		88
	
	SQL> set role all except dba;
	
	Role set.
	
	SQL> select count(1) from v$session;
	select count(1) from v$session
	                     *
	ERROR at line 1:
	ORA-00942: table or view does not exist

####默认角色

当设置用户默认角色时，这些角色会在用户初始化会话时自动生效。

	alter user username defautl role role_list;

##延伸阅读

* [Oracle用户及角色介绍](http://blog.csdn.net/tianlesoftware/article/details/4786956)
---
layout: post
title: Temp Segment Usage Monitor
category : Oracle
tags : [Oracle, Database, DBA]
---

How to Monitor the usage of temp segment.

create tablespace for dba_test
	
	SQL> conn /as sysdba
	Connected.
	SQL> create tablespace dba_data datafile '/db/oracle/oradata/wxprod/dba_data01.dbf' size 1000M;
	
	Tablespace created.
	
	SQL> create tablespace dba_idx datafile '/db/oracle/oradata/wxprod/dba_idx01.dbf' size 500M;
	
	Tablespace created.
	
	
	SQL> create temporary tablespace dba_tmp tempfile '/db/oracle/oradata/wxprod/dba_tmp01.dbf' size 500M;
	
	Tablespace created.
	
	SQL> alter system switch logfile;
	
	System altered.
	

create user dba_test and grant privileges
	
	SQL> create user dba_test identified by dba_test
	  2  default tablespace dba_data
	  3  temporary tablespace dba_tmp;
	
	User created.
	
	SQL> grant resource,connect,dba to dba_test;
	
	Grant succeeded.
	
	SQL> grant select on v_$session to dba_test;
	
	Grant succeeded.
	
	SQL> grant select on v_$sort_usage to dba_test;
	
	Grant succeeded.
	
	SQL> grant select on v_$sqlarea to dba_test;
	
	Grant succeeded.
	
create table `TEMP_SEG_USAGE` for storing temp segment usage statistics
	
	SQL> conn dba_test
	Enter password: 
	Connected.
	SQL> CREATE TABLE TEMP_SEG_USAGE(
	 2      DATE_TIME DATE,
	 3      USERNAME VARCHAR2(30),
	 4      SID VARCHAR2(6),
	 5      SERIAL# VARCHAR2(6),
	 6      OS_USER VARCHAR2(30),
	 7      SPACE_USED NUMBER,
	 8      SQL_TEXT VARCHAR2(1000)
	 9   );   
	
	Table created.
	
create procudure and submit as a job
	
	SQL> CREATE OR REPLACE PROCEDURE TEMP_SEG_USAGE_P IS
	  2  BEGIN
	  3      INSERT INTO TEMP_SEG_USAGE
	  4          SELECT SYSDATE,
	  5                 A.USERNAME,
	  6                 A.SID,
	  7                 A.SERIAL#,
	  8                 A.OSUSER,
	  9                 B.BLOCKS,
	 10                 C.SQL_TEXT
	 11            FROM V$SESSION A, V$TEMPSEG_USAGE B, V$SQLAREA C
	 12           WHERE B.TABLESPACE = 'TMP'
	 13             AND A.SADDR = B.SESSION_ADDR
	 14             AND C.ADDRESS = A.SQL_ADDRESS
	 15             AND C.HASH_VALUE = A.SQL_HASH_VALUE
	 16             AND B.BLOCKS * 8192 > 1024
	 17           ORDER BY B.TABLESPACE, B.BLOCKS;
	 18      COMMIT;
	 19  END TEMP_SEG_USAGE_P;
	 20  /
	
	Procedure created.
	
	SQL> BEGIN
			DBMS_JOB.ISUBMIT(JOB=>2,WHAT=> 'TEMP_SEG_USAGE_P;',NEXT_DATE => SYSDATE,INTERVAL  => 'sysdate + (5/1440)');
			COMMIT;
		END;
		/

	PL/SQL procedure successfully completed.
	
##Reference

* MOS: How to Monitor the usage of temp segment.
* [Privileges in PL/SQL](http://dylanninin.com/blog/2013/03/07/privileges_in_plsql.html)
---
layout: post
title: Gzip and Bzip2 on Linux
category : Linux
tags : [Linux, Utilities]
---

gzip 可以说是应用度最广的压缩命令了！目前 gzip 可以解开 compress, zip 与 gzip 等软件所压缩的文件。

###gzip help

	[root@server Workspace]# gzip -h
	Usage: gzip [OPTION]... [FILE]...
	Compress or uncompress FILEs (by default, compress FILES in-place).
	
	Mandatory arguments to long options are mandatory for short options too.
	
	  -c, --stdout      write on standard output, keep original files unchanged
	  -d, --decompress  decompress
	  -f, --force       force overwrite of output file and compress links
	  -h, --help        give this help
	  -l, --list        list compressed file contents
	  -L, --license     display software license
	  -n, --no-name     do not save or restore the original name and time stamp
	  -N, --name        save or restore the original name and time stamp
	  -q, --quiet       suppress all warnings
	  -r, --recursive   operate recursively on directories
	  -S, --suffix=SUF  use suffix SUF on compressed files
	  -t, --test        test compressed file integrity
	  -v, --verbose     verbose mode
	  -V, --version     display version number
	  -1, --fast        compress faster
	  -9, --best        compress better
	    --rsyncable   Make rsync-friendly archive
	
	With no FILE, or when FILE is -, read standard input.
	
	Report bugs to <bug-gzip@gnu.org>.

###用法

压缩

	[test@server ~]$ gzip -v netstat.log 
	netstat.log:	 93.0% -- replaced with netstat.log.gz
	[test@server ~]$ ll
	total 4
	-rw-rw-r-- 1 test test 1868 Mar 11 17:11 netstat.log.gz

继续压缩

	[test@server ~]$ gzip -v netstat.log.gz 
	gzip: netstat.log.gz already has .gz suffix -- unchanged
	[test@server ~]$ gzip -v -S .gz netstat.log.gz -c > netstat.log.gz.gz
	netstat.log.gz:	  0.2%
	[test@server ~]$ ll
	total 8
	-rw-rw-r-- 1 test test 1799 Mar 11 16:12 netstat.log.gz
	-rw-rw-r-- 1 test test 1837 Mar 11 16:17 netstat.log.gz.gz

解压缩

	[test@server ~]$ gzip -v -d netstat.log.gz 
	netstat.log.gz:	 93.0% -- replaced with netstat.log

测试完整性
	
	[test@server ~]$ gzip -v -t netstat.log.gz 
	netstat.log.gz:	 OK

	[test@server ~]$ touch dummy.gz;gzip -v -t dummy.gz

	gzip: dummy.gz: unexpected end of file

列表文件

	[test@server ~]$ gzip -v -l netstat.log.gz 
	method  crc     date  time           compressed        uncompressed  ratio uncompressed_name
	defla 73d6b845 Mar 11 16:12                1799               25013  93.0% netstat.log

###特点

1.仅能对单个文件（即输入一个文件，输出一个文件）

	[test@server ~]$ mkdir -p foo/bar
	[test@server ~]$ free > foo/free.log
	[test@server ~]$ netstat > foo/bar/netstat.log
	[test@server ~]$ tree .
	.
	└── foo
	    ├── bar
	    │   └── netstat.log
	    └── free.log
	
	2 directories, 2 files
	[test@server ~]$ gzip foo/
	gzip: foo/ is a directory -- ignored
	[test@server ~]$ gzip foo/*
	gzip: foo/bar is a directory -- ignored

2.通过递归，支持对文件夹的压缩(`gzip -r`)，但解压缩需单独一一进行

	[test@server ~]$ gzip -v -r foo/
	foo//bar/netstat.log:	 92.5% -- replaced with foo//bar/netstat.log.gz
	foo//free.log:	 44.8% -- replaced with foo//free.log.gz

3.默认情况下，gzip压缩后的文件后缀名为.gz(`gzip -S ".your_suffix"`)

	[test@server ~]$ gzip -v -S ".gzip" passwd 
	passwd:	 61.4% -- replaced with passwd.gzip

gzip默认后缀为.gz，当更改后缀名后，解压时，需要指定后缀名

	[test@server ~]$ gzip -v -d passwd.gzip 
	gzip: passwd.gzip: unknown suffix -- ignored
	[test@server ~]$ gzip -v -d -S ".gzip" passwd.gzip 
	passwd.gzip:	 61.4% -- replaced with passwd

4.压缩、解压缩时，保留原有文件权限

压缩前，文件权限

	[root@server ~]# ll
	total 4
	-rw-r--r-- 1 root root   5 Mar 11 16:49 whoami.log

更改权限为777

	[root@server ~]# chmod 777 whoami.log 
	[root@server ~]# chown test:test whoami.log 

压缩后，文件权限

	[root@server ~]# gzip -v whoami.log 
	whoami.log:	120.0% -- replaced with whoami.log.gz
	[root@server ~]# ll
	total 4
	-rwxrwxrwx 1 test test  36 Mar 11 16:49 whoami.log.gz

解压后，文件权限

	[root@server ~]# gzip -v -d whoami.log.gz 
	whoami.log.gz:	120.0% -- replaced with whoami.log
	[root@server ~]# ll
	total 4
	-rwxrwxrwx 1 test test   5 Mar 11 16:49 whoami.log

注：在使用 gzip -c 配合 > 重定向时，相当于创建文件，此时文件权限为于当前用户设定的权限。

5.压缩时，默认状态下，原文件会被压缩后的文件替换；解压时类似

此时，可以使用`gzip -c > your_file_name`来重定向输出文件，同时保存原有文件。不指定文件时，默认会输出到标准输出stdout上。

压缩

	[test@server ~]$ gzip -v -S .gz netstat.log.gz -c > netstat.log.gz.gz
	netstat.log.gz:	  0.2%
	[test@server ~]$ ll
	total 8
	-rw-rw-r-- 1 test test 1799 Mar 11 16:12 netstat.log.gz
	-rw-rw-r-- 1 test test 1837 Mar 11 16:17 netstat.log.gz.gz

解压缩

	[root@server ~]# gzip -vdc whoami.log.gz > whoami
	whoami.log.gz:	120.0%
	[root@server Workspace]# ll
	total 8
	-rw-r--r-- 1 root root  5 Mar 11 17:08 whoami
	-rw-r--r-- 1 test test 36 Mar 11 17:04 whoami.log.gz


6.压缩后的文本文件可以直接使用zcat读出(zcat your_file_name.gz)

	[test@server ~]$ zcat netstat.log.gz 
	Active Internet connections (w/o servers)
	Proto Recv-Q Send-Q Local Address               Foreign Address             State  
	... ...

##bzip2

若说 gzip 是为了取代 compress 并提供更好的压缩比而成立的，那么 bzip2 则是为了取代 gzip 并提供更佳的压缩比而来的。 bzip2 真是很不错用的东西～这玩意的压缩比竟然比 gzip 还要好～至於 bzip2 的用法几乎与 gzip 相同！

##bzip2 help

	[root@server svn]# bzip2 --help
	bzip2, a block-sorting file compressor.  Version 1.0.5, 10-Dec-2007.
	
	   usage: bzip2 [flags and input files in any order]
	
	   -h --help           print this message
	   -d --decompress     force decompression
	   -z --compress       force compression
	   -k --keep           keep (don't delete) input files
	   -f --force          overwrite existing output files
	   -t --test           test compressed file integrity
	   -c --stdout         output to standard out
	   -q --quiet          suppress noncritical error messages
	   -v --verbose        be verbose (a 2nd -v gives more)
	   -L --license        display software version & license
	   -V --version        display software version & license
	   -s --small          use less memory (at most 2500k)
	   -1 .. -9            set block size to 100k .. 900k
	   --fast              alias for -1
	   --best              alias for -9
	
	   If invoked as `bzip2', default action is to compress.
	              as `bunzip2',  default action is to decompress.
	              as `bzcat', default action is to decompress to stdout.
	
	   If no file names are given, bzip2 compresses or decompresses
	   from standard input to standard output.  You can combine
	   short flags, so `-v -4' means the same as -v4 or -4v, &c.

###使用

gzip的替代工具bzip2，压缩比较gzip高，用法与gzip类似，不再赘述。

1.压缩后的文本文件可以直接使用bzcat读出

	[root@server ~]# bzcat netstat.log.bz2 
	Active Internet connections (w/o servers)
	Proto Recv-Q Send-Q Local Address               Foreign Address             State  
	... ...

##参考

* [鸟哥的私房菜 文件与文件系统的压缩与打包](http://vbird.dic.ksu.edu.tw/linux_basic/0240tarcompress.php)
* [gzip wikipedia](http://en.wikipedia.org/wiki/Gzip)
* [bzip2 wikipedia](http://en.wikipedia.org/wiki/Bzip2)
---
layout: post
title: AWR Introduction
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

Oracle Database 10g 提供了一个新的工具：(AWR:Automatic Workload Repository)。Oracle 建议用户用这个取代 Statspack。AWR 实质上是一个 Oracle 的内置工具，它采集与性能相关的统计数据，并从那些统计数据中导出性能量度，以跟踪潜在的问题。

与 Statspack 不同，快照由一个称为 `MMON` 的新的后台进程及其从进程自动地每小时采集一次。为了节省空间，采集的数据在 7 天后自动清除。快照频率和保留时间都可以由用户修改。它产生两种类型的输出：文本格式（类似于 Statspack 报表的文本格式但来自于 AWR 信息库）和默认的 HTML 格式（拥有到部分和子部分的所有超链接），从而提供了非常用户友好的报表。
 
AWR 使用几个表来存储采集的统计数据，所有的表都存储在新的名称为`SYSAUX` 的特定表空间中的 `SYS` 模式下，并且以 `WRM$_*` 和 `WRH$_*` 的格式命名。前一种类型存储元数据信息（如检查的数据库和采集的快照），后一种类型保存实际采集的统计数据。H 代表“历史数据 (historical)”而 M 代表“元数据 (metadata)”。
            
在这些表上构建了几种带前缀`DBA_HIST_` 的视图，这些视图可以用来编写您自己的性能诊断工具。视图的名称直接与表相关；例如，视图 `DBA_HIST_SYSMETRIC_SUMMARY` 是在`WRH$_SYSMETRIC_SUMMARY` 表上构建的。
 
###注意
 
`statistics_level`默认是typical，在10g中表监控是激活的，强烈建议在10g中此参数的值是typical。

如果`statistics_level`设置为basic，不仅不能监控表，而且将禁掉如下一些10g的新功能：

* ASH(Active Session History)
* ASSM(Automatic Shared Memory Management)
* AWR(Automatic Workload Repository)
* ADDM(Automatic Database Diagnostic Monitor)

- - -

##AWR使用

运行awr报告

	SQL> @?/rdbms/admin/awrrpt
	
	Current Instance
	~~~~~~~~~~~~~~~~
	
	   DB Id    DB Name	 Inst Num Instance
	----------- ------------ -------- ------------
	 1193549399 DBTEST		1 DBTEST
	

指定报告类型	

	Specify the Report Type
	~~~~~~~~~~~~~~~~~~~~~~~
	Would you like an HTML report, or a plain text report?
	Enter 'html' for an HTML report, or 'text' for plain text
	Defaults to 'html'
	Enter value for report_type: 
	
	Type Specified:  html
	
	
	Instances in this Workload Repository schema
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	   DB Id     Inst Num DB Name	   Instance	Host
	------------ -------- ------------ ------------ ------------
	* 1193549399	    1 DBTEST	   DBTEST	oradb.tp-lin
							k.net
	
	Using 1193549399 for database Id
	Using	       1 for instance number
	

指定选取快照的天数	

	Specify the number of days of snapshots to choose from
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Entering the number of days (n) will result in the most recent
	(n) days of snapshots being listed.  Pressing <return> without
	specifying a number lists all completed snapshots.
	
	
	Enter value for num_days: 1
	
	Listing the last day's Completed Snapshots
	
								Snap
	Instance     DB Name	    Snap Id    Snap Started    Level
	------------ ------------ --------- ------------------ -----
	DBTEST	     DBTEST	       2409 14 Mar 2013 00:00	   1
				       2410 14 Mar 2013 01:00	   1
				       2411 14 Mar 2013 02:00	   1
				       2412 14 Mar 2013 03:00	   1
				       2413 14 Mar 2013 04:00	   1
				       2414 14 Mar 2013 05:00	   1
				       2415 14 Mar 2013 06:00	   1
				       2416 14 Mar 2013 07:00	   1
				       2417 14 Mar 2013 08:00	   1
				       2418 14 Mar 2013 09:00	   1
				       2419 14 Mar 2013 10:00	   1
				       2420 14 Mar 2013 11:00	   1
				       2421 14 Mar 2013 12:00	   1
				       2422 14 Mar 2013 13:00	   1
				       2423 14 Mar 2013 14:00	   1
	
指定生产报告所需快照的起始和结束编号	
	
	Specify the Begin and End Snapshot Ids
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Enter value for begin_snap: 2418
	Begin Snapshot Id specified: 2418
	
	Enter value for end_snap: 2421
	End   Snapshot Id specified: 2421
	
指定生成的报告名
	
	Specify the Report Name
	~~~~~~~~~~~~~~~~~~~~~~~
	The default report file name is awrrpt_1_2418_2421.html.  To use this name,
	press <return> to continue, otherwise enter an alternative.
	
	Enter value for report_name: 
	
	Using the report name awrrpt_1_2418_2421.html
	... ...
	Report written to awrrpt_1_2418_2421.html

- - -

##AWR报告分析

这部分内容，可以参考statspack report的分析，这2个内容都差不多。

AWR报告样例，可以参考[eygle](http://www.eygle.com)的[AWR报告分析之二：ges inquiry response](http://www.eygle.com/pdf/awrrpt_1_143_157.html)；

statspack报告可以参考[Dave](http://blog.csdn.net/tianlesoftware)的[statspack安装使用和report分析](http://blog.csdn.net/tianlesoftware/article/details/4682329)。

##AWR操作

###AWR保存策略
 
	SQL> col snap_interval for a20
	SQL> col retention for a20
	SQL> select * from dba_hist_wr_control;
	
	      DBID SNAP_INTERVAL	    RETENTION	         TOPNSQL
	---------- -------------------- -------------------- ----------
	1193549399 +00000 01:00:00.0	+00008 00:00:00.0    DEFAULT

以上结果表示,每小时产生一个SNAPSHOT，保留8天。
 
###调整AWR配置
 
AWR配置都是通过`dbms_workload_repository`包进行配置。
 
调整AWR产生snapshot的频率和保留策略，如将收集间隔时间改为30 分钟一次。并且保留5天时间（单位为分钟）：

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30, retention=>5*24*60);
 
关闭AWR，把interval设为0则关闭自动捕捉快照

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>0);
 
手工创建一个快照

	SQL> exec dbms_workload_repository.create_snapshot();
 
查看快照

	SQL> select * from sys.wrh$_active_session_history;
 
手工删除指定范围的快照
	
	SQL> exec dbms_workload_repository.drop_snapshot_range(low_snap_id => 973, high_snap_id => 999, dbid => 262089084);
 
创建baseline，保存这些数据用于将来分析和比较

	SQL> exec dbms_workload_repository.create_baseline(start_snap_id => 1003, end_snap_id => 1013, 'apply_interest_1');
 
删除baseline

	SQL> exec dbms_workload_repository.drop_baseline(baseline_name => 'apply_interest_1', cascade => FALSE);
 
将AWR数据导出并迁移到其它数据库以便于以后分析

	SQL> exec dbms_swrf_internal.awr_extract(dmpfile => 'awr_data.dmp', mpdir => 'DIR_BDUMP', bid => 1003, eid => 1013);
 
迁移AWR数据文件到其他数据库

	SQL> exec dbms_swrf_internal.awr_load(schname => 'AWR_TEST', dmpfile => 'awr_data.dmp', dmpdir => 'DIR_BDUMP');

把AWR数据转移到SYS模式中：

	SQL> exec dbms_swrf_internal.move_to_awr(schname => 'TEST');

##错误

在学习AWR配置调整过程中，出现了一例异常，如下。

###ORA-13541

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60);
	BEGIN dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60); END;
	
	*
	ERROR at line 1:
	ORA-13541: system moving window baseline size (691200) greater than retention (604800)
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 174
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 222
	ORA-06512: at line 1



	SQL> select 691200/24/60/60 baseline,604800/24/60/70 retention from dual;
	
	  BASELINE  RETENTION
	---------- ----------
		 8	    6

检查当前系统移动窗口的基线设置

	SQL>  SELECT dbid, baseline_name, baseline_type, moving_window_size from dba_hist_baseline;
	
	      DBID BASELINE_NAME	BASELINE_TYPE MOVING_WINDOW_SIZE
	---------- -------------------- ------------- ------------------
	1193549399 SYSTEM_MOVING_WINDOW MOVING_WINDOW		       8

根据以上信息，即移动窗口(moving window baseline)的大小大于数据保留(retention)时间，故报`ORA-13541`。要弄清楚这两者之间的关系，必须要弄清楚几个概念。这里仅作简单的理解：AWR是通过比较不同时间点的性能数据来分析系统的运行状态的，所选取的时间点跨度(最大值即移动窗口的大小)必须在这些性能数据的保留周期之内，否则会出现以上异常。

这里调整下以上两个参数的大小，使移动窗口的大小，这个值要等于或小于AWR保留天数即可。

如调整移动窗口大小，从8天改为7天：

	SQL> exec dbms_workload_repository.modify_baseline_window_size(window_size=>7);
	
	PL/SQL procedure successfully completed.
	
	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60);
	
	PL/SQL procedure successfully completed.

如调整保留时间，从7改为8天：

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60);
	BEGIN dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60); END;
	
	*
	ERROR at line 1:
	ORA-13541: system moving window baseline size (691200) greater than retention (604800)
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 174
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 222
	ORA-06512: at line 1
	
	
	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>8*24*60);
	
	PL/SQL procedure successfully completed.

####基线(Baselines)

基线包含了一个特定时间范围的性能数据，用来在性能问题发生时，与其他类似的时间段进行比较。基线中的快照会被自动AWR清除进程排除，并无限期保留。

oracle数据库中包含了三种类型的基线：
 
#####1.固定基线(Fixed Baselines)
	
固定基线相当于被指定的过去的一个固定的、连续的时间范围。在创建固定基线以前，要慎重考虑这个时间段，因为基线代表了一个理想状态的系统状态。之后，你可以用这个基线和其他基线或者某个时间范围内的快照来分析性能上的退化情况。

#####移动窗口基线(Moving Window Baseline)

移动窗口基线相当于AWR保留期间内存在的所有AWR数据。在使用自适应阈值时，这将很有用处，因为数据库可以使用AWR保留期间的所有AWR数据来计算出度量阈值。oracle数据库自动维护一个系统定义的移动窗口基线。系统定义的移动窗口基线的默认窗口大小等于当前AWR保留的时间，默认为8天。如果你要使用自适应阈值，可以考虑使用更大的移动窗口，例如30天，可以更精确地计算出阈值。你可以改变移动窗口的大小，这个值要等于或小于AWR保留天数。因此若你需要增大移动窗口的大小，首先需要增加AWR的保留时间。

#####基线模板(Baseline Templates)

 你可以创建一个基线，作为未来一个时间连续的时间段可以使用的基线模板。有两种类型的基线模板：单一的和重复的。你可以为未来一个单独的连续时间段的基线创建单一基线模板。如果你要提前准备获取一个未来的时间段，这个技术会很有用处。例如，你安排好要在周末进行一个系统测试，并准备获取AWR数据，这种情况下，你可以创建一个单一基线模板，用以在测试时自动获取该时间范围内的数据。你也可以使用重复基线模板来创建或者删除一个重复的时间计划，当你想自动获取一个连续的时间范围，这将很有用。例如，你可能希望在一个月里的每周一早晨获取AWR数据，这种情况下，你可以创建一个重复基线模板来自动为每个周一创建基线，并且在设置了过期时间(例如一个月)后，自动删除过期的基线。

##参考

* [Oracle AWR 简介](http://blog.csdn.net/wildwave/article/details/6838906)
* [Oracle AWR 说明](http://blog.csdn.net/tianlesoftware/article/details/4682300)
* [AWR报告分析之二：ges inquiry response](http://www.eygle.com/pdf/awrrpt_1_143_157.html)
* [AWR报告样例](http://www.eygle.com/pdf/awrrpt_1_143_157.html)
* [statspack安装使用和report分析](http://blog.csdn.net/tianlesoftware/article/details/4682329)
---
layout: post
title: Statspack Introduction
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

Oracle  Statspack 从 Oracle8.1.6 开始被引入 Oracle,并马上成为 DBA 和 Oracle 专家用来诊断数据库性能的强有力的工具。通过 Statspack 我们可以很容易的确定 Oracle 数据库的瓶颈所在，记录数据库性能状态，也可以使远程技术支持人员迅速了解你的数据库运行状况。因此了解和使用 Statspack 对于 DBA 来说至关重要。

Oracle 10g之前对数据库做性能检测使用statspack工具，自10g 提供了一个新的工具：(AWR:Automatic Workload Repository)。Oracle 建议用户用这个取代 Statspack。AWR 实质上是一个 Oracle 的内置工具，它采集与性能相关的统计数据，并从那些统计数据中导出性能量度，以跟踪潜在的问题。

在数据库中 Statspack 的脚本位于`$ORACLE_HOME/RDBMS/ADMIN` 目录下。

##基本使用

###1.安装statspack.

在$ORACLE_HOME/rdbms/admin/目录下运行：

	SQL> @spcreate.sql

若创建失败则在同一目录下运行： 
	
	SQL> @spdrop.sql
 
###2.测试

	SQL>execute statspack.snap
	  PL/SQL procedure successfully completed.
	SQL>execute statspack.snap
	  PL/SQL procedure successfully completed.
	SQL>@spreport.sql
 
	SQL>exec statspack.snap; 

进行信息收集统计，每次运行都将产生一个快照号，获得快照号，必须要有两个以上的快照，才能生成报表
 
###3.查选快照信息

	SQL>select SNAP_ID, SNAP_TIME from STATS$SNAPSHOT;
 
###4.获取statspack 报告

	SQL>@spreport.sql          
                                                         
按照提示，输入需要查看的开始快照号与结束快照号即可。
 
###5.其他相关脚本

* spauto.sql： 利用dbms_job提交一个作业，自动的进行STATPACK的信息收集统计
* sppurge.sql ：清除一段范围内的统计信息，需要提供开始快照与结束快照号
* sptrunc.sql ： 清除(truncate)所有统计信息
 
###6.查看Statspack 生成源代码

在oracle 9i里面，我们可以通过查看statspack 生成脚本来帮助我们理解report，但是10g的AWR是通过`dbms_workload_repository`包来实现AWR的。包把代码都封装了起来，我们无法查看。
 	
statspack的生成脚本位置：`$ORACLE_HOME/rdbms/admin/sprepins.sql`
代码很长，不过看懂了，能帮助我们理解statspack中各个数据的意义。


##检查系统参数

为了能够顺利安装和运行 Statspack 你可能需要设置以下系统参数：

###1.`job_queue_processes` 

为了能够建立自动任务，执行数据收集，该参数需要大于 0。你可以在初试化参数文件中修改该参数(使该参数在重起后以然有效)。 

该参数可以在系统级动态修改。

	SQL> show parameter job_queue_processes;
	
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	job_queue_processes                  integer     2
	SQL> alter system set job_queue_processes=6;
	
	System altered.
	
	SQL> show parameter job_queue_processes;
	
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	job_queue_processes                  integer     6

在 Oracle9i 当中，可以指定范围，如 both,这样该修改在当前及之后保持有效(仅当你使用 spfile时，如果在 9i 中仍然使用 pfile，那么更改方法同 8i 相同): 

	SQL> alter system set job_queue_processes = 6 scope=both;   
	System altered.

###2.`timed_statistics` 

收集操作系统的计时信息，这些信息可被用来显示时间等统计信息、优化数据库和 SQL 语句。要防止因从操作系统请求时间而引起的开销，请将该值设置为 False。 

使用 statspack 收集统计信息时建议将该值设置为 True，否则收集的统计信息大约只能起到10%的作用，将 timed_statistics 设置为 True 所带来的性能影响与好处相比是微不足道的。 

该参数使收集的时间信息存储在在 V$SESSTATS 和 V$SYSSTATS 等动态性能视图中。

timed_statistics 参数可以在实例级进行更改

	SQL> alter system set timed_statistics = true; 
	System altered 
	SQL> show parameter timed_statistics;
	
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	timed_statistics                     boolean     TRUE

如果你担心一致启用 timed_statistics  对于性能的影响，你可以在使用 statspack 之前在 system 更改，采样过后把该参数动态修改成 false。


##检查Statspack
因测试环境中之前做过statspack监控，并未删除statspack对象，因此这里仅确认下是否安装正确。要看安装、卸载等，请查阅参考文档。

###1.statspack检查

statspack脚本均在$ORACLE_HOME/rdbms/admin下。首先切换到该目录，下面执行脚本时会比较方便。

切换到该路径

	$ pwd
	/u2/TEST/testora/testdb/9.2.0/rdbms/admin

sp脚本

	$ ls sp*
	sp__.lst          spcpkg.sql        spctab.sql        spdoc.txt         spdusr.sql        spreport.sql      sprepsql.sql      spup816.sql
	spauto.sql        spcreate.sql      spcusr.lis        spdrop.sql        sppurge.sql       spreport0902.txt  sptrunc.sql       spup817.sql
	spcpkg.lis        spctab.lis        spcusr.sql        spdtab.sql        sprepins.sql      spreport0907.txt  spuexp.par        spup90.sql

###2.测试sp脚本

	SQL> execute statspack.snap
	
	PL/SQL procedure successfully completed.
	
	SQL> execute statspack.snap
	
	PL/SQL procedure successfully completed.
	SQL> @spreport.sql
	……	

###3.检查statspach表空间

	SQL> SELECT tablespace_name,file_name, round(dbf.BYTES / (1024 * 1024),0) "Total_space(M)" FROM dba_data_files dbf where dbf.TABLESPACE_NAME = 'STATSPACK';  
			TABLESPACE_NAME   FILE_NAME                     Total_space(M)
	-------------------- ------------------------------------------------------------ --------------
	STATSPACK  /u2/TEST/testora/testdata/statspack_01.dbf               500.00

##规划自动任务

Statspack 正确安装以后，我们就可以设置定时任务，开始收集数据了。可以使用 spatuo.sql 来定义自动任务。

先来看看 spauto.sql 的关键内容：

	variable jobno number;
	variable instno number;
	begin
	  select instance_number into :instno from v$instance;
	  dbms_job.submit(:jobno, 'statspack.snap;',
	  trunc(sysdate+1/24,'HH'), 'trunc(SYSDATE+1/24,'HH')', 
	TRUE, :instno);
	  commit;
	end;

这个 job 任务定义了收集数据的时间间隔：

	一天有 24 个小时，1440 分钟，那么：
		1/24   HH           每小时一次
		1/48   MI           每半小时一次
		1/144  MI           每十分钟一次
		1/288  MI           每五分钟一次

	SQL> @spauto
	
	PL/SQL procedure successfully completed.
	
	
	Job number for automated statistics collection for this instance
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Note that this job number is needed when modifying or removing
	the job:
	
	     JOBNO
	----------
	     87890
	
	
	Job queue process
	~~~~~~~~~~~~~~~~~
	Below is the current setting of the job_queue_processes init.ora
	parameter - the value for this parameter must be greater
	than 0 to use automatic statistics gathering:
	
	NAME_COL_PLUS_SHOW_PARAM                                         TYPE
	---------------------------------------------------------------- -----------
	VALUE_COL_PLUS_SHOW_PARAM
	------------------------------------------------------------------------------
	job_queue_processes                                              integer
	6
	
	
	Next scheduled run
	~~~~~~~~~~~~~~~~~~
	The next scheduled run for this job is:
	
	       JOB NEXT_DATE       NEXT_SEC
	---------- --------------- ------------------------
	     87890 28-JUN-12       10:00:00

关于采样间隔，我们通常建议以 1 小时为时间间隔，对于有特殊需要的环境，可以设置更短的，如半小时作为采样间隔，但是不推荐更短。因为 statspack 的执行本身需要消耗资源，对于繁忙的生产系统，太短的采样对系统的性能会产生较大的影响（甚至会使 statspack 的执行出现在采样数据中）。

##生成分析报告

运行spreport脚本，输入起始和结束的快照ID，生成分析报告。

	SQL> @spreport
	Current Instance
	~~~~~~~~~~~~~~~~
	   DB Id    DB Name      Inst Num Instance
	---------- ------------ -------- ------------
	   33115540 ERP                 1 ERP 
	Instances in this Statspack schema
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	   DB Id    Inst Num DB Name      Instance     Host
	----------- -------- ------------ ------------ ------------
	   33115540        1 ERP          ERP          erp
	   56114082        1 PROD         PROD         erpprod
	
	Using   33115540 for database Id
	Using          1 for instance number
	Completed Snapshots
	                               Snap                    Snap
	Instance     DB Name             Id   Snap Started    Level Comment
	------------ ------------ --------- ----------------- ----- --------------------
	ERP          ERP                753 28 Jun 2012 10:30     5
	                                754 28 Jun 2012 11:30     5
	                                755 28 Jun 2012 12:30     5
	                                756 28 Jun 2012 13:30     5
	                                757 28 Jun 2012 14:30     5
	                                758 28 Jun 2012 15:30     5 
	Specify the Begin and End Snapshot Ids
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Enter value for begin_snap: 753
	Begin Snapshot Id specified: 753
	Enter value for end_snap: 758
	End   Snapshot Id specified: 758
	Specify the Report Name
	~~~~~~~~~~~~~~~~~~~~~~~
	The default report file name is sp_753_758.  To use this name,
	press <return> to continue, otherwise enter an alternative.
	Enter value for report_name: sp_753_758.txt
	Using the report name sp_753_758.txt SNATSPACK report for DB Name         DB Id    Instance     Inst Num Release     Cluster Host
	----------- ----------- ------------ -------- ----------- ------- ------------
	ERP             33115540 ERP                 1 9.2.0.6.0   NO      erp
	              Snap Id     Snap Time      Sessions Curs/Sess Comment
	            --------- ------------------ -------- --------- -------------------
	Begin Snap:       753 28-Jun-12 10:30:16      153      91.9
	  End Snap:       758 28-Jun-12 15:30:17      160      90.5
	   Elapsed:              300.02 (mins) ……

一个 statspack 的报告不能跨越一次停机，但是之前或之后的连续区间，收集的信息依然有效。你可以选择之前或之后的采样声称 report。

##移除定时任务

运行dbms_job.remove(‘job_id’)，移除定时任务。

	SQL> select job,log_user,priv_user,last_date,next_date,interval from user_jobs;
	    JOB LOG_USER  PRIV_USER LAST_DATE NEXT_DATE INTERVAL
	------------ --------------- -------------- ----------------  -------------- --------------------------
	87890 	SYS   SYS 	    28-JUN-12 28-JUN-12  trunc(SYSDATE+1/24,'HH') 

	SQL> execute dbms_job.remove('87890');
	PL/SQL procedure successfully completed.


##删除历史数据

删除stats$snapshot数据表中的数据，其他表中的数据会相应的级联删除

	SQL> select max(snap_id) from stats$snapshot;
	MAX(SNAP_ID)
	------------
	        758
	SQL> delete from stats$snapshot where snap_id < = 758;

如果采样了大量的数据，直接delete是非常会慢的，可以考虑使用sptrunc脚本，清空stats历史数据。

	SQL> @sptrunc.sql 
	Warning
	~~~~~~~
	Running sptrunc.sql removes ALL data from Statspack tables.  You may
	wish to export the data before continuing. 
	About to Truncate Statspack Tables ……

##延伸阅读

* [statspack安装使用和report分析](http://blog.csdn.net/tianlesoftware/article/details/4682329)
* [Oracle AWR说明](http://blog.csdn.net/tianlesoftware/article/details/4682300)

##参考

* [statspack使用指南](http://www.eygle.com/pdf/Statspack-v3.0.pdf)
---
layout: post
title: Tar on Linux
category : Linux
tags : [Linux, Utilities]
---

在Linux下，文件以及文件系统的压缩与打包工具十分丰富，如compress,gzip,bzip2,tar,dump,restore,dd,cpio等等。这里仅针对文件的增量备份做一个简单的测试，主要使用tar。

命令组合：tar,bzip2,split。其中tar负责打包文件，bzip2负责压缩，split负责文件分割。

其他工具：rsync,inotify,sha1sum。rsync负责文件镜像，inotify负责文件实时监控，sha1sum负责文件校验。

##测试

边打包、压缩，边分割

	[dylan@server ~]# tar -g snapshot/snapshot.snar -cjpvf - source | split -a 3 -b 1K - backup/backup_201303121100.tar.bz2.

tar 选项

	-c  创建新的归档文件
	-j  使用bzip2压缩归档文件
	-g  即--listed-incremental 指定增量备份
	-p  归档时保留文件权限
	-f  指定归档文件
	-   stdout
	
split 选项

	-a  指定后缀名的长度。根据数字或字母，确定分割后的最大文件数
		如果后缀为数字[0-9]，则分割后最多有 10 ** ${suffix_length};
		如果后缀为字母[a-z]，则分割后最多有 26 ** ${suffix_length}；
	-b  指定分割大小，以字节为单位
	-   stdin

查看目录结构

	[dylan@server ~]# tree .
	.
	├── backup
	│   ├── backup_201303121100.tar.bz2.aaa
	│   ├── backup_201303121100.tar.bz2.aab
	│   └── backup_201303121100.tar.bz2.aac
	├── snapshot
	│   └── snapshot.snar
	└── source
	     ... ...

测试分割后的文件	

	[dylan@server backup]# tar -tf backup_201303121100.tar.bz2.aaa 

	bzip2: Compressed file ends unexpectedly;
		perhaps it is corrupted?  *Possible* reason follows.
	bzip2: Inappropriate ioctl for device
		Input file = (stdin), output file = (stdout)
	
	It is possible that the compressed file(s) have become corrupted.
	You can use the -tvv option to test integrity of such files.
	
	You can use the `bzip2recover' program to attempt to recover
	data from undamaged sections of corrupted files.
	
	tar: Child returned status 2
	tar: Error is not recoverable: exiting now

合并分割后的文件

	[dylan@server backup]# cat * > backup_201303121100.tar.bz2

查看要恢复的文件

	[dylan@server backup]# tar -tf backup_201303121100.tar.bz2
	source/
	source/foo/
	source/foo/bar/
	source/foo/bar/fooo/
	source/0
	source/1

	[dylan@server backup]# tar -tf backup_201303121100.tar.bz2 | grep 50
	source/50

解压指定的文件

	[dylan@server backup]# tar -xjvf backup_201303121100.tar.bz2 source/50
	source/50

查看文件

	[dylan@server backup]# tree source/
	source/
	└── 50
	
	0 directories, 1 file


##脚本

backup.sh

	#!/bin/sh
	#abstract:
	#backup files using tar, bzip2
	#history:
	#2013-3-12    dylanninin@gmail.com    first release
	
	source=/fs
	#backup settings
	dist=/backup
	prefix=''
	suffix='.tar.bz2.'
	timestamp=$(date +%Y%m%d%H%M%S)
	current=$(date +%Y/%m)
	size=500M
	recipients=dba
	base=${dist}/${current}
	pieces=${base}/${prefix}${timestamp}${suffix}
	snapshot=${base}/snapshot.snar
	log=${base}/${timestamp}.log    
	
	start=$(date +%s)
	checksum="sha1sum started:"
	ls ${source} ${dist} && mkdir -p ${base} && tar -g ${snapshot} -cjpvf - ${source} | split -b ${size} - ${pieces} && echo -e ${checksum} > ${log} && sha1sum ${pieces}* >> ${log}
	end=$(date +%s)
	elapsed=$(expr ${end} - ${start})
	summary="sha1sum ended.\nsummary:\n\tstarted: ${start}\tended: ${end}\n\telapsed: ${elapsed} seconds."
	echo -e ${summary}  >> ${log}
	
	#mail log 
	cat ${log} | mail -a ${log} -s "$(date +%Y-%m-%d\ %H:%M:%S):$(hostname) dba daily check of fs backup" ${recipients}

以上脚本每个月都会产生新的快照文件（snapshot每个月会重新创建）；增量备份的频率，即脚本运行的频率。若以月为周期太长，则可以更改`current=$(date +%Y/%m)`，（如`current=$(date +%Y/%M)`，周期为一个星期）自行定义。

log

	sha1sum started:
	24c00b3eff967fec250ac7663ac03cbd35cbe40e  /backup/2013/03/20130312163458.tar.bz2.aa
	c2ccb143c514d1afcebfb7a84b4642e168bdd93d  /backup/2013/03/20130312163458.tar.bz2.ab
	7313889f5057365b168980c8f1765f97016fe913  /backup/2013/03/20130312163458.tar.bz2.ac
	a186a9aee90606d53755719a63bed1f407ce6d88  /backup/2013/03/20130312163458.tar.bz2.ad
	945979757291b2f1ac588cbf5c82637f20e37eb3  /backup/2013/03/20130312163458.tar.bz2.ae
	642732865fcc1d43b620b255bf9f32d504f02079  /backup/2013/03/20130312163458.tar.bz2.af
	9cf6a651d4d084d5fe3dc595aa57dc4d61484e3c  /backup/2013/03/20130312163458.tar.bz2.ag
	9eae089684016860aec91b50913d451c5a8689ce  /backup/2013/03/20130312163458.tar.bz2.ah
	dce088dcdd7e3f6e8d7a58007e590cfdc9047b9e  /backup/2013/03/20130312163458.tar.bz2.ai
	5db4a8f6196e1c11cc9dcf29422f56248111dc6e  /backup/2013/03/20130312163458.tar.bz2.aj
	sha1sum ended.
	summary:
		started:1363077298	ended at:1363077466
		elapsed: 168 seconds.

##参考

* [鸟哥的私房菜 文件与文件系统的压缩与打包](http://vbird.dic.ksu.edu.tw/linux_basic/0240tarcompress.php)
* [rsync wikipedia](http://en.wikipedia.org/wiki/Rsync)
* [inotify wikipedia](http://en.wikipedia.org/wiki/Inotify)
---
layout: post
title: INSTR or Like
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

在51CTO上看到一篇文章：[在Oracle数据库中使用instr代替like实操](http://database.51cto.com/art/201005/197734.htm)，看到之后很质疑，以下是我做的简单测试，以对原文作对比验证。

##1.创建索引前

	 SQL> select count(1) from t;
	 
	  COUNT(1)
	----------
	  11905920
	 
	Elapsed: 00:00:11.38

	SQL> select count(1) from t where instr(object_name,'A') >0;
	 
	  COUNT(1)
	----------
	   3947520
	 
	Elapsed: 00:00:10.46

	SQL> select count(1) from t where object_name like '%A%';
	 
	  COUNT(1)
	----------
	   3947520
	 
	Elapsed: 00:00:12.31

	SQL> select count(1) from t where instr(object_name,'A') = 0;
	 
	  COUNT(1)
	----------
	   7958400
	 
	Elapsed: 00:00:10.39

	SQL> select count(1) from t where object_name not like '%A%';
	 
	  COUNT(1)
	----------
	   7958400
	Elapsed: 00:00:10.94
 

从以上结果看出，没有创建索引时，instr和like效率差不多，instr效率略高一点，但也不是文中提到的相差巨大。
 
##2.创建索引后

	SQL> create index t_i on t(object_name);
	 
	Index created.
	 
	Elapsed: 00:02:08.92

	SQL> select count(1) from t;
	 
	  COUNT(1)
	----------
	  11905920
	 
	Elapsed: 00:00:11.07

	SQL> select count(1) from t where instr(object_name,'A') >0;
	 
	  COUNT(1)
	----------
	   3947520
	 
	Elapsed: 00:00:12.04

	SQL> select count(1) from t where object_name like '%A%';
	 
	  COUNT(1)
	----------
	   3947520
	 
	Elapsed: 00:00:07.33

	SQL> select count(1) from t where instr(object_name,'A') = 0;
	 
	  COUNT(1)
	----------
	   7958400
	 
	Elapsed: 00:00:11.57

	SQL> select count(1) from t where object_name not like '%A%';
	 
	  COUNT(1)
	----------
	   7958400
	 
	Elapsed: 00:00:06.47
 

从以上测试看出，添加索引后，like比instr效率高，费时是instr的一半，可以用相差巨大来形容。

##小结

instr,like都是Oracle已经实现的功能，严格来说instr为内部函数，like为SQL标准，效率都很高，但具体如何实现，暂且不知。但两者之间的性能差别还是要看具体的数据库环境，如表结构，数据分布，索引，数据库版本，数据库当时的负载情况。另外，效率高也是以数据库、服务器的资源消耗为代价的，在时间同数量级或用户允许的情况下，如何控制效能才是关键。

由此，也可以看出数据库性能调优应该是一门艺术，需要不断学习、实践。

##参考

* [在Oracle数据库中使用instr代替like实操](http://database.51cto.com/art/201005/197734.htm)
* [INSTR Oracle SQL Language Reference](http://docs.oracle.com/cd/B28359_01/server.111/b28286/functions073.htm#i77598)
* [LIKE Oracle SQL Language Reference](http://docs.oracle.com/cd/B28359_01/server.111/b28286/conditions007.htm#sthref2907)
---
layout: post
title: A Case of OPP Exception
category : Oracle
tags : [Oracle, DBA, EBS, Exception]
---

##Output Post Processor

Concurrent Processing now uses the Output Post Processor (OPP) to enforce post-processing actions for concurrent requests. Post-processing actions are actions taken on concurrent request output. An example of a post-processing action is that used in Concurrent Processing support of XML Publisher. If a request is submitted with an XML Publisher template specified as a layout for the concurrent request output, then after the concurrent manager finishes running the concurrent program, it will contact the OPP to apply the XML Publisher template and create the final output.

###Manager Log

Responsibility Path：System Administartor >> Concurrent >> Manager >> Administer，找到 Output Post Processor，选择Processes，查看日志时，选择 Manager Log即可。

日志如下：
	
	... ...
	[3/18/13 11:06:25 AM] [OPPServiceThread0] Post-processing request 20000374.
	[3/18/13 11:06:26 AM] [123757:RT20000374] Executing post-processing actions for request 20000374.
	[3/18/13 11:06:26 AM] [123757:RT20000374] Starting XML Publisher post-processing action.
	[3/18/13 11:06:26 AM] [123757:RT20000374] 
	Template code: XX_INV_ORGDIAOBO
	Template app:  ONT
	Language:      zh
	Territory:     CN
	Output type:   EXCEL
	[3/18/13 11:06:56 AM] [123757:RT20000374] XML Publisher post-processing action complete.
	[3/18/13 11:06:57 AM] [123757:RT20000374] Completed post-processing actions for request 20000374.
	[3/18/13 11:25:18 AM] [OPPServiceThread0] Post-processing request 20000836.
	[3/18/13 12:04:00 PM] [OPPServiceThread1] Post-processing request 20002627.
	[3/18/13 1:33:00 PM] [OPPServiceThread1] Post-processing request 20004039.
	[3/18/13 2:11:45 PM] [OPPServiceThread0] Post-processing request 20004816.
	[3/18/13 2:32:03 PM] [OPPServiceThread0] Post-processing request 20005431.
	[3/18/13 2:35:31 PM] [OPPServiceThread1] Post-processing request 20005534.
	[3/18/13 2:37:35 PM] [OPPServiceThread1] Post-processing request 20005554.
	[3/18/13 2:49:06 PM] [OPPServiceThread0] Post-processing request 20006105.
	[3/18/13 3:01:03 PM] [OPPServiceThread1] Post-processing request 20006657.
	[3/18/13 3:02:10 PM] [OPPServiceThread1] Post-processing request 20006688.
	[3/18/13 3:03:49 PM] [OPPServiceThread0] Post-processing request 20006730.
	[3/18/13 3:04:47 PM] [OPPServiceThread0] Post-processing request 20004000.
	[3/18/13 3:06:09 PM] [OPPServiceThread1] Post-processing request 20006802.
	[3/18/13 3:13:06 PM] [OPPServiceThread1] Post-processing request 20007065.
	[3/18/13 3:19:41 PM] [OPPServiceThread0] Post-processing request 20007441.
	[3/18/13 3:22:22 PM] [OPPServiceThread1] Post-processing request 20007546.
	[3/18/13 3:24:18 PM] [OPPServiceThread1] Post-processing request 20007695.
	[3/18/13 3:24:24 PM] [OPPServiceThread0] Post-processing request 20007707.
	[3/18/13 3:33:50 PM] [OPPServiceThread0] Post-processing request 20008124.
	[3/18/13 3:39:46 PM] [OPPServiceThread1] Post-processing request 20008351.
	[3/18/13 3:41:24 PM] [OPPServiceThread1] Post-processing request 20008494.
	[3/18/13 3:41:52 PM] [OPPServiceThread1] Post-processing request 20008507.
	[3/18/13 3:52:04 PM] [OPPServiceThread0] Post-processing request 20008915.
	[3/18/13 3:54:02 PM] [OPPServiceThread0] Post-processing request 20009040.
	[3/18/13 3:56:12 PM] [OPPServiceThread1] Post-processing request 20009153.
	[3/18/13 4:00:27 PM] [OPPServiceThread0] Post-processing request 20009271.
	[3/18/13 4:03:57 PM] [OPPServiceThread0] Post-processing request 20009434.
	[3/18/13 4:15:23 PM] [OPPServiceThread0] Post-processing request 20009946.
	[3/18/13 4:17:05 PM] [OPPServiceThread1] Post-processing request 20010020.
	[3/18/13 4:35:11 PM] [OPPServiceThread0] Post-processing request 20010758.
	[3/18/13 4:39:43 PM] [OPPServiceThread1] Post-processing request 20010989.
	[3/18/13 4:39:50 PM] [OPPServiceThread0] Post-processing request 20011015.
	[3/18/13 4:43:12 PM] [OPPServiceThread1] Post-processing request 20011195.
	[3/18/13 4:58:42 PM] [OPPServiceThread1] Post-processing request 20012085.
	[3/18/13 5:03:34 PM] [OPPServiceThread1] Post-processing request 20011940.
	[3/18/13 5:06:45 PM] [OPPServiceThread0] Post-processing request 20012221.
	[3/18/13 5:25:44 PM] [OPPServiceThread0] Post-processing request 20013863.
	[3/18/13 5:55:50 PM] [OPPServiceThread1] Post-processing request 20016234.
	[3/18/13 6:29:53 PM] [OPPServiceThread0] Post-processing request 20018561.
	[3/18/13 7:12:45 PM] [OPPServiceThread1] Post-processing request 20020489.
	[3/18/13 8:21:06 PM] [OPPServiceThread1] Post-processing request 20023085.
	[3/18/13 9:26:56 PM] [OPPServiceThread0] Post-processing request 20023160.
	[3/19/13 7:56:25 AM] [OPPServiceThread0] Post-processing request 20023315.
	[3/19/13 7:56:57 AM] [OPPServiceThread1] Post-processing request 20023317.
	[3/19/13 8:04:21 AM] [OPPServiceThread0] Post-processing request 20023331.
	[3/19/13 8:10:08 AM] [OPPServiceThread0] Post-processing request 20023438.
	[3/19/13 8:18:31 AM] [OPPServiceThread0] Post-processing request 20023572.
	[3/19/13 8:22:35 AM] [OPPServiceThread1] Post-processing request 20023622.
	[3/19/13 8:27:45 AM] [OPPServiceThread0] Post-processing request 20023721.
	[3/19/13 8:29:12 AM] [OPPServiceThread0] Post-processing request 20023796.
	[3/19/13 8:34:49 AM] [OPPServiceThread1] Post-processing request 20023960.
	[3/19/13 8:39:52 AM] [OPPServiceThread0] Post-processing request 20024067.
	[3/19/13 8:40:41 AM] [GSMServiceController:123757] Received shutdown request.
	[3/19/13 8:40:41 AM] [GSMServiceController:123757] Preparing to shutdown service.
	[3/19/13 8:40:41 AM] [GSMServiceController:123757] Stopping all Service Threads.
	[3/19/13 8:40:41 AM] [OPPServiceThread0] Preparing to shut down service thread.
	[3/19/13 8:40:41 AM] [OPPServiceThread1] Preparing to shut down service thread.
	[3/19/13 8:40:41 AM] [OPPServiceThread0] Received immediate shutdown request. Service Thread will shutdown once all running requests have completed.
	... ...

###异常确认

重新提交ECO XML报表，结束状态为 WARNING。查看日志：

	Executing request completion options...
	
	+------------- 1) PUBLISH -------------+
	Beginning post-processing of request 20024067 on node ERPPROD at 19-MAR-2013 08:39:51.
	Post-processing of request 20024067 failed at 19-MAR-2013 08:59:52 with the error message:
	The Output Post-processor is running but has not picked up this request. 
	No further attempts will be made to post-process this request, and the request will be marked 
	with Warning status.
	Setting the profile option Concurrent: OPP Response Timeout to a higher value may be necessary.
	+--------------------------------------+
	
	+------------- 2) PRINT   -------------+
	+--------------------------------------+


	Finished executing request completion options.

##error message

Error Message：ONC-PP NO RESPONSE and TIMEOUT

The Output Post-processor is running but has not picked up this request. 
No further attempts will be made to post-process this request, and the request will be marked  with Warning status.
Setting the profile option Concurrent: OPP Response Timeout to a higher value may be necessary.

###Cause: ONC-PP NO RESPONSE and TIMEOUT

1.The Output Post Processor (OPP) service is not picking up any new requests.

The concurrent manager process generates the XML data file. Upon completion it will trigger the Output Post Processor in order to merge the XML data file and the template which was selected on the Submit Request form.

The number of concurrent requests that the Output Post Processor can handle in parallel depends upon:

* the number of Processes
* the number of Threads Per Process

The default values are 2 Processes and 5 Threads Per Process so a total of 10 reports can be processed in parallel.

In case there are other concurrent requests running which have already invoked the OPP then it might happen that no additional requests can be picked up for a period of time. The pending request will be picked up as soon as one of the running jobs completes. By default a timeout will occur if it takes longer then 120 seconds (2 min.) for the Output Post Processor to pick up the request from the concurrent manager process. In that case, the concurrent request will complete with status Warning and the request log file will contain Error Message (see above).

2.The Output Post Processor JAVA process is still running, but it is unresponsive. 

This can be due to errors in previous requests or just to the amount of time the manager has been running. The OPP becomes unresponsive (stale) after running for more than one week.

###Solution

* Option 1: Increase the value (in seconds) for the profile option 'Concurrent:OPP Response Timeout'.
* Option 2: Increase the number of processes or threads (or both) of the OPP via Oracle Applications Manager.


###ERPProd

####查看当前设置

在Administer Concurrent Managers中找到Output Post Processor，可以看到该程序的OS Pid，即Concurrent值；并在OS上查找该进行，可以看到OPP的设置信息。如下：

	# ps -ef | grep 123757
	 prodmgr 5488788 1269866   0   Feb 03      - 114:55 /usr/java14/bin/java -mx512m -Ddbcfile=/u1/PROD/prodmgr/prodappl/fnd/11.5.0/secure/PROD_erpprod/prod.dbc -Dcpid=123757 -Dlogfile=/u1/PROD/prodmgr/prodcomn/admin/log/PROD_erpprod/FNDOPP123757.txt -DLONG_RUNNING_JVM=true -DOVERRIDE_DBC=true -DFND_JDBC_BUFFER_MIN=1 -DFND_JDBC_BUFFER_MAX=2 oracle.apps.fnd.cp.gsf.GSMServiceController 


或者在数据库中执行以下SQL：

	SELECT *
	FROM fnd_concurrent_queues fcq, fnd_concurrent_processes fcp
	WHERE process_status_code not in ('K', 'S')
	AND fcq.concurrent_queue_id = fcp.concurrent_queue_id
	AND fcq.application_id = fcp.queue_application_id
	and concurrent_queue_name = 'FNDCPOPP';

####更改OPP设置

登陆ERP，选择System Administrator >> Dashboard >> Site Map >> Administration : Application Services : Generic Services，选择 Output Post Processor，即可查看更改OPP设置。

The OPP Service is multi-threaded and will start a new thread for each concurrent request it processes. You can control the number of simultaneous threads for an OPP Service Instance by adjusting the Threads per Process parameter for the instance. If all the OPP process have reached their respective maximum number of threads, the requests waiting to be processed remain in a queue to be processed as soon as threads become available. If request throughput has become slow, you may want to increase the number of Threads per Process for the OPP. We recommend you keep the number of Threads per Process between 1 and 20. You may find that you get even better performance by increasing the number of OPP processes.

系统原先设置为： 1 Process * 5 Threads Per Process。

现调整为： 2 Process * 10 Threads Per Process。

####重启OPP

在Administer Concurrent Managers中找到Output Post Processor，可以选择Restart，但长期不响应，很长时间处于restarting状态，这里就只能采取杀死OPP进程，再重新启动OPP的办法。

####报表测试

再次提交ECO XML报表，则可以正常完成和输出报表。

##Reference

* Concurrent Requests Fail Due to Output Post Processing (OPP) Timeout [ID 352518.1]

---
layout: post
title: How to trace sessions in Oracle Database
category : Oracle
tags : [Oracle, Database, DBA]
---

当Oracle数据库出现性能问题时，我们通常可以利用一些性能诊断工具来跟踪SQL的执行情况，进而根据输出的跟踪文件来了解和分析数据库内部的一些操作过程和统计信息，找出性能瓶颈。

针对不同的需求场景，有不同的诊断工具，或者同一种工具有不同的用法。这里就汇总下一些常用的性能诊断工具和使用场景。

##跟踪文件

Oracle跟踪文件分为三种类型

* 一种是后台报警日志文件，记录数据库在启动、关闭和运行期间后台进程的活动情况,如表空间创建、回滚段创建、某些alter命令、日志切换、错误消息等。在数据库出现故障时，应首先查看该文件，但文件中的信息与任何错误状态没有必然的联系。后台报警日志文件保存`BACKGROUND_DUMP_DEST`参数指定的目录中。
* 另一种类型是DBWR、LGWR、SMON等后台进程创建的后台跟踪文件。后台跟踪文件根据后台进程运行情况产生，后台跟踪文件也保存在`BACKGROUND_DUMP_DEST`参数指定的目录中。
* 还有一种类型是由连接到Oracle的用户进程(Server Processes)生成的用户跟踪文件。这些文件仅在用户会话期间遇到错误时产生。此外，用户可以通过执行oracle跟踪事件(如10046事件）来生成该类文件，用户跟踪文件保存在`USER_DUMP_DEST`参数指定的目录中。

###命名规则

用户进程跟踪文件的命名规则

`[oracle_sid]_ora_[server_process_id]_[trace_id].trc`

* `oracle_sid`，可以简单理解为数据库实例id，erpprod为prod；    
* `server_process_id`，oracle内部标示进程的id，可以通过查询v$session的spid来确定；
* `trace_id`可由 `tracefile_identifier`参数指定，通过查询v$process的traceid来确定，默认为空。

###存放路径

* 由参数 `user_dump_dest`指定。
* 在`SQL*Plus`中可以通过`show user_dump_dest；`查询；
或执行 `SELECT NAME, VALUE FROM V$PARAMETER WHERE NAME = 'user_dump_dest';`来确定。


##初始化参数

为了在跟踪文件中更好的收集数据库性能诊断信息，必须先调整以下两个参数：

* `TIMED_STATISTICS` 用于收集与时间有关的信息，主要从操作系统请求时间而引起的时间开销，若需要显示这些信息，需设置为TRUE。
* `MAX_DUMP_FILE_SIZE` 指定跟踪文件的最大大小；如果跟踪文件很大或者不确定，需设置为足够的大小，或者`UNLIMITED`。


##跟踪当前会话

###1.使用 SQL_TRACE

在`SQL*Plus`中使用`SQL_TRACE`，跟踪当前会话

	-- Start tracing the current session
	ALTER SESSION SET SQL_TRACE = TRUE ;
	
	-- execute your SQL to be traced --
	
	-- Stop tracing the current session
	ALTER SESSION SET SQL_TRACE = FALSE;

对实例上所有的SQL做跟踪

	-- Start tracing the whole instance
	ALTER DATABASE SET SQL_TRACE = TRUE ;

	-- execute your SQL to be traced --

	-- Stop tracing the whole instance
	ALTER SESSION SET SQL_TRACE = FALSE;

注：

* 在session级别设置，只对当前session进行跟踪，在实例级别，会对实例上所有的SQL做跟踪，这种方式跟踪的SQL太多，代价是非常大的，所有很少用。
* `sql_trace`也可以在初始化文件里面设置。

###2.10046事件

10046 事件主要用来跟踪SQL语句，它并不是ORACLE官方提供给用户的命令，在官方文档上也找不到事件的说明信息。但是用的却比较多，因为10046事件获取SQL的信息比SQL_TRACE 更多。 更有利于我们对SQL的判断。

	-- Start tracing the current session
	alter session set events '10046 trace name context forever, level 12';
	
	-- execute your SQL to be traced --
	
	--Stop tracing the current session
	alter session set events '10046 trace name context off';


####10046事件级别

根据要收集信息的详细程度，10046事件分为了很多不同的级别，使用整数定义，常见的如下：

* Level 1： 等同于SQL_TRACE 的功能
* Level 4： 在Level 1的基础上增加收集绑定变量的信息
* Level 8： 在Level 1 的基础上增加等待事件的信息
* Level 12：等同于Level 4+Level 8, 即同时收集绑定变量信息和等待事件信息。

一般来说，使用`Level 12`就可以收集到比较详细的诊断信息，包括绑定变量，等待事件等。

####使用示例

初始参数设置

	alter session set tracefile_identifier = '10046';
	alter session set timed_statistics = true;
	alter session set statistics_level=all;
	alter session set max_dump_file_size = unlimited;

启动10046跟踪事件

	alter session set events '10046 trace name context forever, level 12';
	
	-- execute your SQL to be traced --

在当前会话中，停用10046跟踪事件，有以下两种方法：

1)退出当前会话

	select * from dual;
	--exit;
	

2)手动使跟踪失效

	alter session set events '10046 trace name context off';


###3.DBMS_SUPPORT

跟踪当前会话除了以上两种方式外，Oracle还提供了实用工具`DBMS_SUPPORT`包，如果没有需要手动到`$ORACLE_HOME/rdbms/admin`下安装。

主要用法如下：

	--Start tracing the current session
	exec sys.dbms_support.start_trace ;
	
	-- execute your SQL to be traced --
	
	--Stop tracing  the current session
	exec sys.dbms_support.stop_trace ;

##跟踪非当前会话

在性能诊断过程中，跟踪当前会话更多的是重现问题，以确认是否存在异常情况，但限制太多，而且很难模拟生产系统当时的运行情况。因此，Oracle也提供了丰富的工具盒方法来监控正在运行的会话，以更好的适应实际应用的需求。

以下汇总一些常用方的工具，在不同的数据库版本中，支持程度可能不一样。

###1.使用`dbms_system.set_bool_param_in_session`

	--Start tracing other session (28,226)
	exec sys.dbms_system.set_bool_param_in_session(28, 226, 'sql_trace', TRUE);
	
	-- execute SQL in that session to be traced --	

	--Stop tracing other session (28,226)
	exec sys.dbms_system.set_bool_param_in_session(28, 226, 'sql_trace', FALSE);


###2.使用`dbms_system.set_ev`

	--Start tracing other session (28,226)
	exec dbms_system.set_ev(28, 226, 10046, 12, '');
	
	-- execute SQL in that session to be traced --	

	--Stop tracing other session (28,226)
	exec dbms_system.set_ev(28, 226, 10046, 0, '');


###3.使用`dbms_system.set_sql_trace_in_session`

	-Start tracing other session (28,226)
	exec dbms_system.set_sql_trace_in_session(1166,30993,TRUE);

	-- execute SQL in that session to be traced --	

	-Start tracing other session (28,226)
	exec dbms_system.set_sql_trace_in_session(1166,30993,FALSE);

####4.使用`dbms_monitor`

	-Start tracing other session (28,226)
	exec dbms_monitor.session_trace_enable(session_id=>28,serial_num=>226, waits=>true, binds=>true);
	
	-- execute SQL in that session to be traced --	
	
	-Start tracing other session (28,226)
	exec dbms_monitor.session_trace_disable(session_id=>28,serial_num=>226);


####5.使用oradebug

	--Start tracing process with pid = os_process_pid
	oradebug setospid os_process_pid
	oradebug event 10046 trace name context forever, level 12;
	
	-- execute SQL in that process to be traced --	
	
	--Stop tracing process with pid = os_process_pid
	oradebug event 10046 trace name context off ;


####6.使用数据库触发器

除了以上方式，也可以结合数据库级别触发器，如logon/logoff，针对某一用户进行跟踪。

	--Start tracing when logon database
	create or replace trigger user_logon_t
	after logon on database
	begin
		if USER = 'username' then
		execute immediate
		'alter session set events ''10046 trace name context forever, level 8''';
		end if;
	end;
	/
	
	-- execute SQL with that user to be traced */
	
	--Stop tracing before logoff database
	create or replace trigger user_logoff_t
	before logoff on database
	begin
		if USER = 'username' then
		execute immediate
		'alter session set events ''10046 trace name context off''';
		end if;
	end;
	/

##TKPROF

以上跟踪会话过程中产生的跟踪文件阅读性不是太友好，很难阅读。于是Oracle提供了专门的格式化工具tkprof。

tkprof是一个用于分析Oracle跟踪文件并且产生一个更加清晰合理的输出结果的可执行工具，使用tkprof工具使用排序功能格式化输出，可以使阅读更加友好。

###用法帮助
	
	$ tkprof 
	Usage: tkprof tracefile outputfile [explain= ] [table= ]
	              [print= ] [insert= ] [sys= ] [sort= ]
	  table=schema.tablename   Use 'schema.tablename' with 'explain=' option.
	  explain=user/password    Connect to ORACLE and issue EXPLAIN PLAN.
	  print=integer    List only the first 'integer' SQL statements.
	  aggregate=yes|no
	  insert=filename  List SQL statements and data inside INSERT statements.
	  sys=no           TKPROF does not list SQL statements run as user SYS.
	  record=filename  Record non-recursive statements found in the trace file.
	  waits=yes|no     Record summary for any wait events found in the trace file.
	  sort=option      Set of zero or more of the following sort options:
	    prscnt  number of times parse was called
	    ... ...

###使用示例

	$ cd $ORACLE_USER_DUMP_DEST
	$ tkprof explain=apps/password
	trace = prod_ora_3199034.trc
	output = prod_ora_3199034.out                           
	 
	TKPROF: Release 9.2.0.6.0 - Production on Mon Feb 25 15:58:26 2013
	 
	Copyright (c) 1982, 2002, Oracle Corporation.  All rights reserved.
 
	$ ll prod_ora_3199034*        
	-rw-r--r--   1 prodora  dba          150112 Feb 25 15:58 prod_ora_3199034.out
	-rw-r--r--   1 prodora  dba          745495 Feb 25 15:20 prod_ora_3199034.trc


关于tkprof更详细的说明参见[tkprof使用](http://blog.csdn.net/tianlesoftware/article/details/5632003)

##Reference

* [Oracle跟踪文件和转储命令详解](http://blog.csdn.net/newhappy2008/article/details/6864284)
* [`TIMED_STATISTICS`](http://docs.oracle.com/cd/B28359_01/server.111/b28320/initparams245.htm)
* [`MAX_DUMP_FILE_SIZE`](http://docs.oracle.com/cd/B28359_01/server.111/b28320/initparams129.htm)
* [Oracle SQL Trace 和 10046 事件](http://blog.csdn.net/tianlesoftware/article/details/5857023)
* [`DBMS_SUPPORT` reference](http://psoug.org/reference/dbms_support.html)
* [`DBMS_SYSTEM` reference](http://psoug.org/reference/dbms_system.html)
* [`DBMS_MONITOR` reference](http://psoug.org/reference/dbms_monitor.html)
* [Oracle oradebug命令](http://blog.csdn.net/tianlesoftware/article/details/6525628)
* [Using Triggers in Oracle Database](http://docs.oracle.com/cd/B28359_01/appdev.111/b28370/triggers.htm)
* [`Oracle TKPROF and SQL_TRACE`](http://psoug.org/reference/trace_tkprof.html)
* [tkprof使用](http://blog.csdn.net/tianlesoftware/article/details/5632003)
---
layout: post
title: Telnet Introduction
category : Linux
tags : [Linux, Network, Utilities]
---

Telnet协议是TCP/IP协议族中的一员，是Internet远程登陆服务的标准协议和主要方式。
但由于telnet通信未加密，存在很大的安全隐患，一般很少使用。

##测试smtp
SMTP 电子邮件从客户机传输到服务器或从某一个服务器传输到另外一个服务器所使用的传输协议。

请求/响应协议，命令和响应都基于ASCII文本，并以CR/LF结束。响应包括一个表示返回状态的三位数字代码。

SMTP在TCP协议25号端口监听连接请求。

###SMTP 命令

* HELO 向服务器标识用户身份。发送者能欺骗，说谎，但一般情况下服务器都能检测到。 
* EHLO 向服务器标识用户身份。发送者能欺骗，说谎，但一般情况下服务器都能检测到。
* MAIL FROM 命令中指定的地址是发件人地址
* RCPT TO 标识单个的邮件接收人；可有多个 RCPT TO；常在 MAIL 命令后面。
* DATA 在单个或多个 RCPT 命令后，表示所有的邮件接收人已标识，并初始化数据传输，以 CRLF.CRLF 结束 
* VRFY 用于验证指定的用户/邮箱是否存在；由于安全方面的原因，服务器常禁止此命令 
* EXPN 验证给定的邮箱列表是否存在，扩充邮箱列表，也常被禁用 
* HELP 查询服务器支持什么命令 
* NOOP 无操作，服务器应响应 OK 
* RSET 重置会话，当前传输被取消
* QUIT 结束会话 

###简单测试

	[vismgr@demoerp ~]# telnet mail.egolife.com 25
	Trying 192.168.1.10...
	Connected to mail.egolife.com.
	Escape character is '^]'.
	220 egolife.com (IMail 9.23 574278-2) NT-ESMTP Server X1
	EHLO demoerp.egolife.com
	250-egolife.com says hello
	250-SIZE 0
	250-8BITMIME
	250-DSN
	250-ETRN
	250-AUTH LOGIN CRAM-MD5
	250-AUTH LOGIN
	250-AUTH=LOGIN
	250 EXPN
	MAIL FROM:me@egolife.com
	250 ok
	RCPT TO:you@egolife.com
	250 ok its for <you@egolife.com>
	DATA
	354 ok, send it; end with <CRLF>.<CRLF>
	Subject:telnet smtp test
	
	This is a test message with telnet smtp
	.
	250 Message queued
	quit
	221 Goodbye
	Connection closed by foreign host.


##测试pop3

POP3协议规定个人计算机连接到邮件服务器和下载电子邮件的协议。是Internet电子邮件的第一个离线协议标准，允许用户从服务器上把邮件存储到本地，同时删除保存在邮件服务器上的邮件。

POP3服务器遵循POP3协议，用来接收电子邮件。

###POP3 命令

* USER username 认证用户名         
* PASS password 认证密码认证，认证通过则状态转换         
* APOP name,digest 认可一种安全传输口令的办法，执行成功导致状态转换，请参见 RFC 1321 。         
* STAT 处理请求 server 回送邮箱统计资料，如邮件数、 邮件总字节数         
* UIDL n 处理 server 返回用于该指定邮件的唯一标识， 如果没有指定，返回所有的。         
* LIST n 处理 server 返回指定邮件的大小等         
* RETR n 处理 server 返回邮件的全部文本         
* DELE n 处理 server 标记删除，QUIT 命令执行时才真正删除         
* RSET 处理撤消所有的 DELE 命令         
* TOP n,m 处理 返回 n 号邮件的前 m 行内容，m 必须是自然数         
* NOOP 处理 server 返回一个肯定的响应         
* QUIT 希望结束会话。如果 server 处于"处理" 状态，则现在进入"更新"状态，删除那些标记成删除的邮件。如果 server 处于"认可"状态，则结束会话时 server 不进入"更新"状态 。         

###简单测试

	[root@oatest ~]$ telnet mail.egolife.com 110
	Trying 192.168.1.10...
	Connected to mail.egolife.com.
	Escape character is '^]'.
	+OK X1 NT-POP3 Server egolife.com (IMail 9.23 29760096-6710)
	user gfw
	+OK send your password
	pass noitcesti
	+OK maildrop locked and ready
	list
	+OK 304 messages (5041576 octets)
	1 700
	2 749
	... ...
	retr 1 
	+OK 700 octets
	Received: from dev.egolife.com [192.168.1.10] by egolife.com with ESMTP
	  (SMTPD-9.23) id AC332EF0; Mon, 20 Aug 2012 23:48:03 +0800
	Received: by dev.egolife.com (Postfix, from userid 501)
		id 469EE78050; Mon, 20 Aug 2012 23:55:01 +0800 (CST)
	To: gfw@egolife.com
	Subject: 2012-08-20 23:55:01:dev.egolife.com dba daily check of alert
	Message-Id: <20120820155501.469EE78050@dev.egolife.com>
	Date: Mon, 20 Aug 2012 23:55:01 +0800 (CST)
	From: oracle@dev.egolife.com
	X-RCPT-TO: <gfw@egolife.com>
	Status: `
	X-UIDL: 639033796
	X-IMail-ThreadID: 5c33009b0000b7e5
	
	ORA-959 signalled during: alter tablespace dev add datafile '/db/oracle/oradata/prod/dev.dbf' size 100k...
	
	.
	dele 1
	+OK msg deleted
	stat
	+OK 303 5040876
	quit
	+OK POP3 Server saying Good-Bye
	Connection closed by foreign host.

##测试IMAP

IMAP与POP协议有很大的不同，最明显的一点是发送的每条命令前面都带有一个标识，发送一条命令后可以紧接着发送另一条命令，服务端返回命令的处理结果的顺序是未知的。

待补充...

##参考

* [telnet wikipedia](http://en.wikipedia.org/wiki/Telnet)

---
layout: post
title: MySQL DBA in 20 Days
category : MySQL
tags : [DBA, MySQL]
---

偶然在在[Fenng](http://dbanotes.net/)的 [Startup News](http://news.dbanotes.net/)上看到这篇关于MySQL DBA的文章：[MySQL DBA 20天速成教程](http://lutaf.com/130.htm)，虽然对作者[鲁塔弗](http://lutaf.com/)不熟，但文中最后几段话，还是很让人感触，摘抄如下：

* 吐个槽，我招募过很多DBA，mysql DBA的工作知识就这么些，其他DBA也差不多，看一遍操作一遍你就学会了，基本上不用智商。所谓亿万用户海量计算，哪只是一种经历，你在现场你就会有。DBA只是一个”看门狗”的角色:有事能打电话找到人，出了事故有人可以被罚款，没事就一边凉快去。

* 互联网让获取知识的成本越来越低，数据库越来越像一个普通软件，你见过浏览器，office需要专人操作么？

* 所以，DBA不是一个可以承载梦想的职业，年轻人如果想在技术领域有发展，想要”改变世界”，老老实实写代码当程序员去吧。

##参考

* [MySQL DBA 20天速成教程](http://lutaf.com/130.htm)
---
layout: post
title: Oracle Instant Client Introduction
category : Oracle
tags : [Oracle, Database, DBA]
---

利用 Instant Client，您无需安装标准的 Oracle 客户端或拥有 ORACLE_HOME 就可运行应用程序。OCI、OCCI、`Pro*C`、ODBC 和 JDBC 应用程序无需进行修改即可运行，同时显著节省磁盘空间。甚至 `SQL*Plus` 也可与 Instant Client 一起使用。无需重新编译，也就没有烦恼。

独立软件供应商与合作伙伴通过将 Instant Client 与应用程序打包在一起，为客户省去了安装和配置 Oracle 客户端的额外步骤，这使得独立软件供应商和合作伙伴获益颇丰。其中 Oracle ISV 与合作伙伴提供的功能完善的高性能应用程序将不受任何影响，工作如常。

客户可以快速试用新的打包应用程序和 Oracle 客户端特性，而无需担忧其他的安装事项。较大的企业可以通过使用安装脚本来访问中央 IT 信息库，以自动进行 Instant Client 的安装和配置。总而言之，空间使用减少后，每个人都可从中受益。

关于Oracle Instant Client，请访问：[Oracle Database Instant Client](http://www.oracle.com/technetwork/cn/database/features/instant-client/index-092537-zhs.html)

##参考

* [Oracle Database Instant Client](http://www.oracle.com/technetwork/cn/database/features/instant-client/index-092537-zhs.html)
* [Oracle Instant Client使用说明](http://blog.csdn.net/tianlesoftware/article/details/7244641)
* [Windows XP下 oracle instant client 配置指南](http://blog.csdn.net/guoguo1980/article/details/2647424)
---
layout: post
title: Oracle Audit
category : Oracle
tags : [Oracle, Database, DBA, Security, Exception]
---

某天突然发现一个Oracle数据库中system表空间大小有点异常，经过统计，原来是启用了Oracle的Audit功能，并将审计的信息存放在系统表空间。

数据文件大小

	[oracle@dev DEV]$ ll -h
	total 11G
	-rw-r-----. 1 oracle oinstall  9.8M Mar 28 09:56 control01.ctl
	-rw-r-----. 1 oracle oinstall  101M Mar 28 09:03 example01.dbf
	-rw-r-----. 1 oracle oinstall  2.0G Mar 28 09:56 oa_data01.dbf
	-rw-r-----. 1 oracle oinstall  2.0G Mar 28 09:55 oa_index01.dbf
	-rw-r-----. 1 oracle oinstall  2.0G Mar 28 09:21 oa_temp01.dbf
	-rw-r-----. 1 oracle oinstall   51M Mar 28 08:19 redo01.log
	-rw-r-----. 1 oracle oinstall   51M Mar 28 08:58 redo02.log
	-rw-r-----. 1 oracle oinstall   51M Mar 28 09:56 redo03.log
	-rw-r-----. 1 oracle oinstall   51M Mar 28 08:19 redo11.log
	-rw-r-----. 1 oracle oinstall   51M Mar 28 08:58 redo12.log
	-rw-r-----. 1 oracle oinstall   51M Mar 28 09:56 redo13.log
	-rw-r-----. 1 oracle oinstall 1001M Mar 28 09:55 sysaux01.dbf
	-rw-r-----. 1 oracle oinstall  5.2G Mar 28 09:56 system01.dbf
	-rw-r-----. 1 oracle oinstall   30M Mar 28 05:56 temp01.dbf
	-rw-r-----. 1 oracle oinstall  106M Mar 28 09:55 undotbs01.dbf
	-rw-r-----. 1 oracle oinstall  5.1M Mar 28 09:03 users01.dbf

从以上输出可用看出，system01.dbf大小为5.2G，相比其他数据文件，有点不正常。

表空间使用统计

	SELECT DS.OWNER,
	       DS.SEGMENT_NAME,
	       DS.SEGMENT_TYPE,
	       (SUM(BYTES) / 1024 / 1024) "SEGMENT_SIZE(MB)",
	       MIN(UPPER('&tablespace')) "TABLESPACE"
	  FROM DBA_SEGMENTS DS
	 WHERE DS.TABLESPACE_NAME = UPPER('&tablespace')
	 GROUP BY DS.OWNER, DS.SEGMENT_NAME, DS.SEGMENT_TYPE
	 ORDER BY SUM(BYTES) DESC;

	   	OWNER	SEGMENT_NAME	SEGMENT_TYPE	SEGMENT_SIZE(MB) TABLESPACE
	1	SYS		AUD$			TABLE			4549			 SYSTEM
	2	SYS		IDL_UB1$		TABLE			240				 SYSTEM
	3	SYS		SOURCE$			TABLE			64				 SYSTEM
	4	SYS		IDL_UB2$		TABLE			30				 SYSTEM
	5	SYS		C_TOID_VERSION#	CLUSTER			23				 SYSTEM
	6	SYS		C_OBJ#_INTCOL#	CLUSTER			19				 SYSTEM
	7	SYS		C_OBJ#			CLUSTER			12				 SYSTEM
	8	SYS		I_SOURCE1		INDEX			12				 SYSTEM
	9	SYS		ARGUMENT$		TABLE			11				 SYSTEM
	10	SYS		JAVA$MC$		TABLE			11				 SYSTEM

其中SYS.AUD$表占用4549MB，大约是整个SYSTEM表空间的90%。AUD$是Oracle数据库的审计跟踪表，专门用于存储审计跟踪信息。

##异常处理

审计设置

	SQL> show parameter audit
	
	NAME				    TYPE	   VALUE
	---------------------- ----------- ------------------------------
	audit_file_dest 	   string	   /db/oracle/admin/DEV/adump
	audit_sys_operations   boolean	   FALSE
	audit_syslog_level	   string
	audit_trail			   string	   DB

数据库审计是否启用通过参数`AUDIT_TRAIL`来设置，该参数为静态参数，要使更改生效，必须重启数据库。

在本数据库中，启用了审计，且审计的数据存储在DB，即表`SYS.AUD$`中。

清除所有审计数据

	SQL> conn /as sysdba
	Connected.
	SQL> truncate table aud$;
	
	Table truncated.
	
	SQL> alter table aud$ shrink;
	alter table aud$ shrink
	                      *
	ERROR at line 1:
	ORA-10630: Illegal syntax specified with SHRINK clause

The `SYSTEM` tablespace is created with manual segment allocation and as such it is not possible to run the `SHRINK` command for the objects that are located within. However, if the `AUD$` table is moved to another tablespace (locally managed with automatic segment space management) then it can be shrunk. 

It is recommended to use shrink on the `AUD$` only during a downtime window, since part of the shrink operation will use incompatible locks.

	SQL> begin
	  2  dbms_audit_mgmt.set_audit_trail_location(
	  3  audit_trail_type => dbms_audit_mgmt.audit_trail_db_std,
	  4  audit_trail_location_value => 'USERS'
	  5  );
	  6  end;
	  7  /

	PL/SQL procedure successfully completed.
	
	SQL> alter table aud$ enable row movement;
	
	Table altered.
	
	SQL> alter table sys.aud$ shrink space cascade;
	
	Table altered.

	
	SQL> begin
	  2  dbms_audit_mgmt.set_audit_trail_location(
	  3  audit_trail_type => dbms_audit_mgmt.audit_trail_db_std,
	  4  audit_trail_location_value => 'SYSTEM'
	  5  );
	  6  end;
	  7  /

	PL/SQL procedure successfully completed.


查看`AUD$`大小

	 SELECT DS.OWNER,
	        DS.SEGMENT_NAME,
	        DS.SEGMENT_TYPE,
	        (SUM(BYTES) / 1024 / 1024) "SEGMENT_SIZE(MB)"
	   FROM DBA_SEGMENTS DS
	  WHERE DS.SEGMENT_NAME = UPPER('&segment')
	  GROUP BY DS.OWNER, DS.SEGMENT_NAME, DS.SEGMENT_TYPE;

	   	OWNER	SEGMENT_NAME	SEGMENT_TYPE	SEGMENT_SIZE(MB)
	1	SYS		AUD$			TABLE					  0.125


除了使用`TRUNCATE`，Oracle也提供了`DBMS_AUDIT_MGMT`程序包，可用结合`DBMS_SCHEDULE`定期清理过期的审计信息。具体用法参见[Auditing Enhancements in Oracle Database 11gR2](http://www.oracle-base.com/articles/11g/auditing-enhancements-11gr2.php)。


##关于审计

###Concepts and Overview

Database auditing is the process of recording, monitoring and reporting of the actions performed on a database. It allows the security auditors to observe whether the database users are using the database according to the established policies and that there are no policy violations. Database Auditing facilitates the analysis of  the database activity patterns/trends and it can help in the process of gathering the historical data about a particular database user or activity.  

One can use standard auditing to audit SQL statements, privileges, schemas, objects, and network and multitier activity. Alternatively, one can use Fine Grained Auditing (available only in Enterprise Edition) to monitor specific database activities based on factors such as actions on a database table or times when those activities occur. FGA is very flexible since the audit_condition can reference a PL/SQL function that will control whether the audit record is produced or not.  

Reasons for using auditing include:

- Enabling future accountability for current actions
- Deterring users (or others, such as intruders) from inappropriate actions based on their accountability
- Investigating, monitoring, and recording suspicious activity
- Addressing auditing requirements for compliance
 
###Auditing Installation

The database standard auditing is a feature available by default in all the database editions.

###Configuration and Administration

To use auditing one must first enable it and then define exactly what must be audited. The audited actions  are recorded either in the SYS.AUD$ table or in operating system files.

###Enabling Auditing

The auditing is enabled by setting the `AUDIT_TRAIL` parameter to a value different than NONE followed by a restart of the database.  The following table presents all the possible legal values for the `AUDIT_TRAIL` parameter:

* `NONE`：Auditing is disabled
* `DB`：The auditing is enabled and the audit data is written to the `SYS.AUD$` table
* `DB_EXTENDED`：Behaves as DB but also populates the `SQL_TEXT` and `SQL_BIND` columns
* `OS`：The auditing is enabled. On Unix the audit data is written to text files which are located in the directory specified via `AUDIT_FILE_DEST`. On Windows the audit data will be sent to the Event Viewer.
* `XML`：The auditing is enabled and the audit data is written to XML files which are located in the directory/folder specified via `AUDIT_FILE_DEST`. This is the case for Windows as well.
* `XML_EXTENDED`：Behaves as XML but also populates the `SQL_TEXT` and `SQL_BIND` tags


As seen in the above table the location where the audit data is sent to is controlled by the `AUDIT_TRAIL` parameter. When this parameter is set to `OS`, `XML` or `XML_EXTENDED` the exact location of the audit data is controlled by `AUDIT_FILE_DEST` and  `AUDIT_SYSLOG_LEVEL`. Other factors that influence the exact location for the audit data are : 

 - the status of the database (started or shutdown)
 - whether the user running the audited event is a privileged user or not

The following table shows what will happen when using different combinations for these parameters/factors: 

![auditing_location_corr.jpg](http://dylanninin.com/assets/images/2013/auditing_location_corr.jpg)


##参考

* Master Note For Oracle Database Auditing
* [Oracle Database Auditing Performance](http://www.oracle.com/technetwork/database/audit-vault/learnmore/twp-security-auditperformance-166655.pdf)
* [Auditing Enhancements in Oracle Database 11gR2](http://www.oracle-base.com/articles/11g/auditing-enhancements-11gr2.php)
* [Oracle® Audit Vault Administrator's Guide](http://docs.oracle.com/cd/E11062_01/admin.1023/e11059/avadm_mng_admin_tasks.htm#BEIJCFED)
* [Oracle `DBMS_AUDIT_MGMT PL/SQL Package`](http://docs.oracle.com/cd/E11062_01/admin.1023/e11059/avadm_app_d_audit_mgmt.htm#BABBAGGI)
---
layout: post
title: Stty and rlwrap on Linux
category : Linux
tags : [Linux, Utilities]
---

##退格键
      
在Linux环境下，使用`SQL*Plus`时有时会出现退格键不好使用的情况。此时，可以一般可用使用stty命令来解决。

stty(set tty，设置tty)命令用于检查和修改当前注册的终端的通信参数。UNIX系统为键盘的输入和终端的输出提供了重要的控制手段，可以通过stty命令对特定终端或通信线路设置选项。该命令可以改变并打印终端行设置。

看这个命令的帮助：

	[oracle@dev ~]$ stty --help
	Usage: stty [-F DEVICE] [--file=DEVICE] [SETTING]...
	  or:  stty [-F DEVICE] [--file=DEVICE] [-a|--all]
	  or:  stty [-F DEVICE] [--file=DEVICE] [-g|--save]
	Print or change terminal characteristics.
	
	  -a, --all          print all current settings in human-readable form
	  -g, --save         print all current settings in a stty-readable form
	  -F, --file=DEVICE  open and use the specified DEVICE instead of stdin
	      --help     display this help and exit
	      --version  output version information and exit
	
	Optional - before SETTING indicates negation.  An * marks non-POSIX
	settings.  The underlying system defines which settings are available.
	
	Special characters:
	 * dsusp CHAR    CHAR will send a terminal stop signal once input flushed
	   eof CHAR      CHAR will send an end of file (terminate the input)
	   eol CHAR      CHAR will end the line
	 * eol2 CHAR     alternate CHAR for ending the line
	   erase CHAR    CHAR will erase the last character typed
	   intr CHAR     CHAR will send an interrupt signal
	   kill CHAR     CHAR will erase the current line
	 * lnext CHAR    CHAR will enter the next character quoted
	   quit CHAR     CHAR will send a quit signal
	 * rprnt CHAR    CHAR will redraw the current line
	   start CHAR    CHAR will restart the output after stopping it
	   stop CHAR     CHAR will stop the output
	   susp CHAR     CHAR will send a terminal stop signal
	 * swtch CHAR    CHAR will switch to a different shell layer
	 * werase CHAR   CHAR will erase the last word typed
	
	Special settings:
	  N             set the input and output speeds to N bauds
	 * cols N        tell the kernel that the terminal has N columns
	 * columns N     same as cols N
	   ispeed N      set the input speed to N
	 * line N        use line discipline N
	   min N         with -icanon, set N characters minimum for a completed read
	   ospeed N      set the output speed to N
	 * rows N        tell the kernel that the terminal has N rows
	 * size          print the number of rows and columns according to the kernel
	   speed         print the terminal speed
	   time N        with -icanon, set read timeout of N tenths of a second
	……
	Handle the tty line connected to standard input.  Without arguments,
	prints baud rate, line discipline, and deviations from stty sane.  In
	settings, CHAR is taken literally, or coded as in ^c, 0x37, 0177 or
	127; special values ^- or undef used to disable special characters.
	
	Report bugs to <bug-coreutils@gnu.org>. 
 
与退格键相关的设置是erase，它表示删除最后一个字符。

	$stty erase ^H

说明：按下退格键会显示成^H。 如果在当前窗口执行的话，只对当前的窗口有效，下次登陆的时候还需要重新设置，可以把这个命令写入shell 的配置文件，如`~/.bashrc` 中，这样每次都能生效了。
 
##方向键

Windows下使用方向键是没有问题的，但是在Linux下，方向键是使用不了。此时可以安装一下rlwrap工具。

	SQL> ^[[A	----[按方向键会显示异常]

rlwrap本身是个遵循GPL 标准的Shell 脚本，可以运行任何你提供给它的命令包括参数，并添加命令历史浏览功能。

1) 关于rlwrap

	[root@dev ~]# yum info rlwrap
	Loaded plugins: fastestmirror, refresh-packagekit, security
	Determining fastest mirrors
	base                                                                                                                                             | 1.3 kB     00:00     
	extras                                                                                                                                           | 1.3 kB     00:00     
	Installed Packages
	Name        : rlwrap
	Arch        : x86_64
	Version     : 0.37
	Release     : 1.el6
	Size        : 196 k
	Repo        : installed
	From repo   : extras
	Summary     : Wrapper for GNU readline
	URL         : http://utopia.knoware.nl/~hlub/rlwrap/
	License     : GPLv2+
	Description : rlwrap is a 'readline wrapper' that uses the GNU readline library to
	            : allow the editing of keyboard input for any other command. Input
	            : history is remembered across invocations, separately for each command;
	            : history completion and search work as in bash and completion word
	            : lists can be specified on the command line.

2) 安装rlwrap

	[root@dev soft]# yum install -y rlwrap
 
3) 测试

	[oracle@dev ~]$ rlwrap sqlplus / as sysdba
	
	SQL*Plus: Release 10.2.0.1.0 - Production on Wed Jun 27 17:42:50 2012
	
	Copyright (c) 1982, 2005, Oracle.  All rights reserved.
	
	
	Connected to:
	Oracle Database 10g Enterprise Edition Release 10.2.0.1.0 - Production
	With the Partitioning, OLAP and Data Mining options
	
	SQL> select * from v$version;
	
	BANNER
	----------------------------------------------------------------
	Oracle Database 10g Enterprise Edition Release 10.2.0.1.0 - Prod
	PL/SQL Release 10.2.0.1.0 - Production
	CORE	10.2.0.1.0	Production
	TNS for Linux: Version 10.2.0.1.0 - Production
	NLSRTL Version 10.2.0.1.0 - Production
	
	SQL> select * from v$version; --[按方向键可用正常输入]

现在就可以上下翻动了。 

4) 设置别名

但是这样没事都需要加上rlwrap 也是很麻烦的，可以对rlwrap 做一个别名，放到shell 的配置文件里，在`~/.bashrc` 文件里添加如下内容：

	alias sqlplus='rlwrap sqlplus'

5) 让参数生效

	[oracle@dev ~]$  source ~/.bashrc   

6) 测试

	[oracle@dev ~]$ type sqlplus
	sqlplus is aliased to `rlwrap sqlplus'
	
	[oracle@dev ~]$ $ sqlplus / as sysdba
	SQL*Plus: Release 10.2.0.1.0 - Production on Wed Jun 27 17:42:50 2012
	
	Copyright (c) 1982, 2005, Oracle.  All rights reserved.
	
	
	Connected to:
	Oracle Database 10g Enterprise Edition Release 10.2.0.1.0 - Production
	With the Partitioning, OLAP and Data Mining options
	
	SQL> select * from v$version;
	
	BANNER
	----------------------------------------------------------------
	Oracle Database 10g Enterprise Edition Release 10.2.0.1.0 - Prod
	PL/SQL Release 10.2.0.1.0 - Production
	CORE	10.2.0.1.0	Production
	TNS for Linux: Version 10.2.0.1.0 - Production
	NLSRTL Version 10.2.0.1.0 - Production
	
	SQL> select * from v$version; --[按方向键可用正常输入]
 
其他一些组合键：

	Ctrl+A：ahead，到行的顶端，相当于 Home
	Ctrl+E：end，到行的末端，相当于end
	Ctrl+B：behind，后退一个字符，相当于left
	Ctrl+F：forward，前进一个子放入，相当于right
	Ctrl+P：prev.，上一行历史记录，相当于up
	Ctrl+N：next.，下一行历史记录，相当于down
	Ctrl+U：undo，回复操作，这行就被清空掉了
	Ctrl+W：剪切
	Ctrl+Y：粘贴
	Ctrl+L：cLear，清屏

##参考

* [Linux下`SQL*Plus`退格、方向键问题](http://blog.csdn.net/tianlesoftware/article/details/6168219)
* [利用Uniread解决Linux下的`SQL*Plus`命令历史回调功能](http://dbanotes.net/tech-memo/uniread-howto.html)
* [Another tool for ‘Command Line History’](http://dbanotes.net/database/another_tool_for_command_line_history.html#more-99)
---
layout: post
title: Oracle Security Introduction
category : Oracle
tags : [Oracle, DBA]
---

数据库安全涉及的主题比较多，基础方面的安全即有验证、授权和审计，理解这些有助于做好生产环境安全部署的第一步。

下图即根据《Oracle 10g DBA手册》整理。

![oracle_database_basic_security.jpg](http://dylanninin.com/assets/images/2013/oracle_database_basic_security.jpg)

##延伸阅读

* [Oracle Database Security](http://www.oracle.com/us/products/database/security/resources/index.html)
* [Oracle Database Security Guide](http://docs.oracle.com/cd/E11882_01/network.112/e16543/toc.htm)
* [Oracle DBA手记4](http://book.douban.com/subject/10946310/)

##参考

* [Oracle 10g DBA手册](http://book.douban.com/subject/1938839/)

---
layout: post
title: Configure Rsync and Inotify on Linux
category : Linux
tags : [Linux, Utilities]
---

##测试环境

* wxbak.egolife.com CentOS 5.5 	inotify-tools-3.14-1.el5
* nxbak.egolife.com CentOS 5.5  xinetd-2.3.14-10.el5

测试前，先安装好所需要的软件包。测试中，进行了wxbak、nxbak的双向同步测试。

这里仅列出单向测试记录。

* nxbak上安装xinetd服务，充当rsync服务端；
* wxbak上安装inotify-tools，可以实时监测wxbak机上指定文件夹的变化，并触发相应的事件，激活rsync以实时同步文件系统的变化到nxbak机上。
   
具体如下：

![rsync_arch](http://dylanninin.com/assets/images/2013/rsync_arch.png)

##服务端

###1.开启rsync服务

编辑	/etc/xinetd.d/rsync文件

	[root@nxbak]# cat /etc/xinetd.d/rsync 
	# default: off
	# description: The rsync server is a good addition to an ftp server, as it \
	#	allows crc checksumming etc.
	service rsync
	{
		disable	= no 
		socket_type     = stream
		wait            = no
		user            = root
		server          = /usr/bin/rsync
		server_args     = --daemon
		log_on_failure  += USERID
	}

将`disable = yes` 改为 `disable = no`

###2.配置rsync服务

创建/etc/rsyncd.conf

	[root@nxbak]# vim /etc/rsyncd.conf 
	#2012-06-11	dylanninin@gmail.com	settings for rsync server
	#rsync backup side settings
	uid = root 
	gid = root
	use chroot = no
	max connections = 200
	timeout = 600
	strict modes = yes
	port = 873
	pid file = /var/run/rsyncd.pid
	lock file = /var/run/rsyncd.lock
	log file = /var/log/rsyncd.log
	
	#backup fs settings
	[nxbak]
	path = /data/nxbak
	ignore errors
	comment = rsync rman backup
	auth users=oracle
	uid = root
	gid = root
	secrets file = /etc/rsync_server.pwd 
	read only = no
	list = no
	hosts allow = 192.168.1.118
	hosts deny = 0.0.0.0/32	

###3.密码文件

创建/etc/rsync_server.pwd

	[root@nxbak]# vim /etc/rsync_server.pwd 
	oracle:security

修改权限

	[root@nxbak]# chmod 600 /etc/rsync_server.pwd 
	[root@nxbak]# ll /etc/rsync_server.pwd 
	-rw------- 1 root root 17 Jun 12 11:43 /etc/rsync_server.pwd

##客户端

###1.密码文件

	[root@wxbak ]# vim /etc/rsync_client.pwd 
	security

修改权限

	[root@wxbak]# chmod 600 /etc/rsync_client.pwd
	[root@wxbak]# ll /etc/rsync_client.pwd 
	-rw------- 1 root root 10 Jun 12 11:42 /etc/rsync_client.pwd
	
###2.rsync+inotify脚本

	[root@wxbak ]# cat /apps/scripts/rsync.sh 
	#!/bin/sh
	#abstract:
	#rsync auto sync script
	#2012-06-11	mis_ghb		first_release
	#variables
	current_date=$(date +%Y%m%d_%H%M%S)
	rman_path=/apps/rmanbak
	log_file=/var/log/rsync.log
	
	#rsync
	rsync_server=192.168.1.119
	rsync_user=oracle
	rsync_pwd=/etc/rsync_client.pwd
	rsync_module=nxbak
	#rsync_client password check
	if [ ! -e ${rsync_pwd} ]; then
	   echo -e "rsync client password file ${rsync_pwd} does not exist!"
	   exit 0
	fi
	
	#inotify function
	inotify_fun(){
	    /usr/bin/inotifywait -mrq --timefmt '%d/%m/%y-%H:%M' --format '%T%w%f' \
	-e modify,delete,create,move ${rman_path} | while read file
	   do
	        /usr/bin/rsync -vrtzopg --progress --delete --password-file=${rsync_pwd} ${rman_path} ${rsync_user}@${rsync_server}::${rsync_module}
	   done
	}

	#inotify
	inotify_fun >> ${log_file}  2>&1 &

##测试

###1.开启xinetd服务（服务端）

	 [root@nxbak]# service xinetd restart
	Stopping xinetd:                                           [  OK  ]
	Starting xinetd:                                           [  OK  ]
 
查看服务监听状态netstat

	 [root@nxbak]# netstat -nap | grep xinetd
	tcp     0     0 0.0.0.0:873    0.0.0.0:*      LISTEN      5310/xinetd   

或者lsof

	[root@nxbak]# lsof -i:873
	COMMAND  PID USER   FD   TYPE DEVICE SIZE NODE NAME
	xinetd  5310 root    5u  IPv4 597328       TCP *:rsync (LISTEN)

###2.运行rsync脚本（客户端）

	[root@wxbak ]# /apps/scripts/rsync.sh

查看脚本运行状态

	[root@wxbak ]# ps -ef | grep rsync
	root     29896     1  0 09:42 pts/1    00:00:00 /bin/sh /apps/scripts/rsync.sh
	root     29898 29896  0 09:42 pts/1    00:00:00 /bin/sh /apps/scripts/rsync.sh
	root     29902 18433  0 09:42 pts/1    00:00:00 grep rsync

###3.变更同步目录（客户端）

	[root@wxbak rmanbak]# pwd
	/apps/rmanbak
	[root@wxbak rmanbak]# touch rsync
	[root@wxbak rmanbak]# touch inotify
	[root@wxbak rmanbak]# ll
	total 1211696
	-rw-r--r-- 1 oracle oinstall       3414 Jun 12 13:46 20120612_134619-inc0.log
	-rw-r--r-- 1 oracle oinstall       3038 Jun 12 13:50 20120612_134935-inc2.log
	-rw-r--r-- 1 oracle oinstall       1664 Jun 12 23:00 20120612_230001-obsolete.log
	-rw-r--r-- 1 oracle oinstall       3240 Jun 12 23:45 20120612_234501-inc2.log
	-rw-r----- 1 oracle oinstall 1112170496 Jun 12 13:46 20120612_inc0_87ndbqle_1_1.bkp
	-rw-r----- 1 oracle oinstall       4096 Jun 12 13:46 20120612_inc0_89ndbqlv_1_1.bkp
	-rw-r----- 1 oracle oinstall      37888 Jun 12 13:49 20120612_inc2_8andbqrg_1_1.bkp
	-rw-r----- 1 oracle oinstall     319488 Jun 12 13:50 20120612_inc2_8bndbqri_1_1.bkp
	-rw-r----- 1 oracle oinstall   11927552 Jun 12 13:50 20120612_inc2_8cndbqsl_1_1.bkp
	-rw-r----- 1 oracle oinstall       7168 Jun 12 13:50 20120612_inc2_8dndbqsn_1_1.bkp
	-rw-r----- 1 oracle oinstall   55731200 Jun 12 23:45 20120612_inc2_8endctnv_1_1.bkp
	-rw-r----- 1 oracle oinstall   47374336 Jun 12 23:45 20120612_inc2_8fndcto0_1_1.bkp
	-rw-r----- 1 oracle oinstall   11927552 Jun 12 23:45 20120612_inc2_8gndctp3_1_1.bkp
	-rw-r----- 1 oracle oinstall       6656 Jun 12 23:45 20120612_inc2_8hndctp6_1_1.bkp
	-rw-r--r-- 1 root   root              0 Jun 13 09:48 inotify
	-rw-r--r-- 1 root   root              0 Jun 13 09:48 rsync

###4.查看同步效果（服务端）

	[oracle@nxbak rmanbak]$ pwd
	/data/nxbak/rmanbak
	[oracle@nxbak rmanbak]$ ll
	total 1211696
	-rw-r--r-- 1 oracle oinstall       3414 Jun 12 13:46 20120612_134619-inc0.log
	-rw-r--r-- 1 oracle oinstall       3038 Jun 12 13:50 20120612_134935-inc2.log
	-rw-r--r-- 1 oracle oinstall       1664 Jun 12 23:00 20120612_230001-obsolete.log
	-rw-r--r-- 1 oracle oinstall       3240 Jun 12 23:45 20120612_234501-inc2.log
	-rw-r----- 1 oracle oinstall 1112170496 Jun 12 13:46 20120612_inc0_87ndbqle_1_1.bkp
	-rw-r----- 1 oracle oinstall       4096 Jun 12 13:46 20120612_inc0_89ndbqlv_1_1.bkp
	-rw-r----- 1 oracle oinstall      37888 Jun 12 13:49 20120612_inc2_8andbqrg_1_1.bkp
	-rw-r----- 1 oracle oinstall     319488 Jun 12 13:50 20120612_inc2_8bndbqri_1_1.bkp
	-rw-r----- 1 oracle oinstall   11927552 Jun 12 13:50 20120612_inc2_8cndbqsl_1_1.bkp
	-rw-r----- 1 oracle oinstall       7168 Jun 12 13:50 20120612_inc2_8dndbqsn_1_1.bkp
	-rw-r----- 1 oracle oinstall   55731200 Jun 12 23:45 20120612_inc2_8endctnv_1_1.bkp
	-rw-r----- 1 oracle oinstall   47374336 Jun 12 23:45 20120612_inc2_8fndcto0_1_1.bkp
	-rw-r----- 1 oracle oinstall   11927552 Jun 12 23:45 20120612_inc2_8gndctp3_1_1.bkp
	-rw-r----- 1 oracle oinstall       6656 Jun 12 23:45 20120612_inc2_8hndctp6_1_1.bkp
	-rw-r--r-- 1 root   root              0 Jun 13 09:48 inotify
	-rw-r--r-- 1 root   root              0 Jun 13 09:48 rsync

###5.查看日志

服务端日志

	[root@nxbak]# tail -f /var/log/rsyncd.log 
	2012/06/12 23:46:07 [1633] params.c:Parameter() - Ignoring badly formed line in configuration file: ignore errors
	2012/06/12 23:46:07 [1633] name lookup failed for 192.168.1.118: Temporary failure in name resolution
	2012/06/12 23:46:07 [1633] connect from UNKNOWN (192.168.1.118)
	2012/06/12 23:46:07 [1633] rsync to nxbak from oracle@unknown (192.168.1.118)
	2012/06/12 23:46:07 [1633] sent 69 bytes  received 589 bytes  total size 1239517788
	2012/06/12 23:46:07 [1635] params.c:Parameter() - Ignoring badly formed line in configuration file: ignore errors
	2012/06/12 23:46:07 [1635] name lookup failed for 192.168.1.118: Temporary failure in name resolution
	2012/06/12 23:46:07 [1635] connect from UNKNOWN (192.168.1.118)
	2012/06/12 23:46:07 [1635] rsync to nxbak from oracle@unknown (192.168.1.118)
	2012/06/12 23:46:07 [1635] sent 69 bytes  received 589 bytes  total size 1239517788
	2012/06/13 09:49:06 [5342] params.c:Parameter() - Ignoring badly formed line in configuration file: ignore errors
	2012/06/13 09:49:06 [5342] name lookup failed for 192.168.1.118: Temporary failure in name resolution
	2012/06/13 09:49:06 [5342] connect from UNKNOWN (192.168.1.118)
	2012/06/13 09:49:06 [5342] rsync to nxbak from oracle@unknown (192.168.1.118)
	2012/06/13 09:49:06 [5342] rmanbak/
	2012/06/13 09:49:06 [5342] sent 69 bytes  received 675 bytes  total size 1239517788
	2012/06/13 09:49:12 [5344] params.c:Parameter() - Ignoring badly formed line in configuration file: ignore errors
	2012/06/13 09:49:12 [5344] name lookup failed for 192.168.1.118: Temporary failure in name resolution
	2012/06/13 09:49:12 [5344] connect from UNKNOWN (192.168.1.118)
	2012/06/13 09:49:12 [5344] rsync to nxbak from oracle@unknown (192.168.1.118)
	2012/06/13 09:49:12 [5344] rmanbak/
	2012/06/13 09:49:12 [5344] sent 69 bytes  received 729 bytes  total size 1239517788

客户端日志

	[root@wxbak]tail -f /var/log/rsync.log
	sent 505 bytes  received 16 bytes  1042.00 bytes/sec
	total size is 1239517788  speedup is 2379112.84
	building file list ... 
	16 files to consider
	rmanbak/
	rmanbak/rsync
	           0 100%    0.00kB/s    0:00:00 (xfer#1, to-check=0/16)
	
	sent 591 bytes  received 44 bytes  1270.00 bytes/sec
	total size is 1239517788  speedup is 1951996.52
	building file list ... 
	17 files to consider
	rmanbak/
	rmanbak/inotify
	           0 100%    0.00kB/s    0:00:00 (xfer#1, to-check=1/17)
	
	sent 645 bytes  received 44 bytes  1378.00 bytes/sec
	total size is 1239517788  speedup is 1799009.85

##异常

昨天在配置rsync时，出现一个新问题，提示`mkdir failed: Permission denied (13)`和`mkstemp failed: Permission`，后来查看Stackoverflow，以及以前的[CentOS 5.5下rsync使用技巧与权限问题解读](http://os.51cto.com/art/201101/243374.htm)，发现可能是开启了SELinux导致的，因对SELinux的权限控制不熟悉，关闭SELinux，rsync即可以正常同步文件、文件夹。

rsync client log

	sending incremental file list
	scripts/checkalert.sh
	        1288 100%    0.00kB/s    0:00:00 (xfer#1, to-check=39/41)
	scripts/checkbak.sh
	        1275 100%    1.22MB/s    0:00:00 (xfer#2, to-check=38/41)
	scripts/cleaner.sh
	         257 100%  250.98kB/s    0:00:00 (xfer#3, to-check=37/41)
	scripts/dailyduty.sh
	        1773 100%    1.69MB/s    0:00:00 (xfer#4, to-check=36/41)
	scripts/full.rman
	         263 100%  256.84kB/s    0:00:00 (xfer#5, to-check=35/41)
	scripts/historycleaner.sh
	        1721 100%    1.64MB/s    0:00:00 (xfer#6, to-check=34/41)
	scripts/inc0.rman
	         287 100%  280.27kB/s    0:00:00 (xfer#7, to-check=33/41)
	scripts/inc1.rman
	         275 100%  268.55kB/s    0:00:00 (xfer#8, to-check=32/41)
	scripts/inc2.rman
	         274 100%  267.58kB/s    0:00:00 (xfer#9, to-check=31/41)
	scripts/inotify
	           0 100%    0.00kB/s    0:00:00 (xfer#10, to-check=30/41)
	scripts/obsolete.rman
	         192 100%  187.50kB/s    0:00:00 (xfer#11, to-check=29/41)
	scripts/rman.sh
	        1256 100%    1.20MB/s    0:00:00 (xfer#12, to-check=28/41)
	scripts/rsync.sh
	         844 100%  824.22kB/s    0:00:00 (xfer#13, to-check=27/41)
	scripts/sql/
	rsync: recv_generator: mkdir "scripts/sql" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.checkalert.sh.J5S0do" (in oa_fs) failed: Permission denied (13)
	*** Skipping any contents from this failed directory ***
	rsync: mkstemp "scripts/.checkbak.sh.evfA21" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.cleaner.sh.L6ccRF" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.dailyduty.sh.cShQFj" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.full.rman.tn4vuX" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.historycleaner.sh.O2mdjB" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.inc0.rman.xLgW7e" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.inc1.rman.kDyGWS" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.inc2.rman.BBHsLw" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.inotify.w68fAa" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.obsolete.rman.ZAO4oO" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.rman.sh.KkEVds" (in oa_fs) failed: Permission denied (13)
	rsync: mkstemp "scripts/.rsync.sh.hp2Q25" (in oa_fs) failed: Permission denied (13)
	
	sent 6010 bytes  received 260 bytes  12540.00 bytes/sec
	total size is 20476  speedup is 3.27
	rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1039) [sender=3.0.6]
	
rsync server log

	2013/03/28 20:14:58 [12524] connect from oaprod.tp-link.net (172.31.1.90)
	2013/03/28 20:14:58 [12524] rsync to oa_fs/ from oa@oaprod.tp-link.net (172.31.1.90)
	2013/03/28 20:14:58 [12524] receiving file list
	2013/03/28 20:14:58 [12524] rsync: recv_generator: mkdir "scripts/sql" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] *** Skipping any contents from this failed directory ***
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.checkalert.sh.vOJStU" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.checkbak.sh.kpYRTQ" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.cleaner.sh.LwyTjN" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.dailyduty.sh.2ZJWJJ" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.full.rman.f3w29F" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.historycleaner.sh.K1tbAC" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.inc0.rman.lGzm0y" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.inc1.rman.2cSzqv" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.inc2.rman.VFAOQr" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.obsolete.rman.4WV6go" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.rman.sh.phPqHk" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] rsync: mkstemp "scripts/.rsync.sh.kslM7g" (in oa_fs) failed: Permission denied (13)
	2013/03/28 20:14:58 [12524] sent 1546 bytes  received 6003 bytes  total size 20476

Stackoverflow

Even though you got this working, I recently had a similar encounter and no SO or google searching was any help as they all dealt with basic permission issues wheres the solution below is somewhat of an off setting that you would even think to check in most situations.

One thing to check for with permission denied that I recently found having issues with rsync myself where permissions were exactly the same on both servers including the owner and group but rsync transfers worked one way on one server but not the other way.

It turned out the server with problems that I was getting permission denied from had SELinux enabled which in turn overrides POSIX permissions on files/folders. So even though the folder in question could of been 777 with root running the command SELinux was enabled and would in turn overwrite those permissions and was producing a permission denied error with rsync.

You can run the command getenforce to see if SELinux is enabled on the machine.

In my situation I ended up just disabling SELINUX completely because it wasn't needed and already disabled on the server that was working fine and just caused problems being enabled. To disable, open `/etc/selinux/config` and set `SELINUX=disabled`. To temporarily disable you can run the command `setenforce 0` which will set SELinux into a permissive state rather then enforcing state which causes it to print warnings instead of enforcing.

##参考

* [Inotify](http://en.wikipedia.org/wiki/Inotify)
* [Inotify: Efficient, Real-Time Linux File System Event Monitoring](http://www.infoq.com/articles/inotify-linux-file-system-event-monitoring)
* [Security-Enhanced Linux](http://en.wikipedia.org/wiki/Security-Enhanced_Linux)
* [SELinux](http://wiki.centos.org/HowTos/SELinux)
* [rsync-mkstemp-failed-permission-denied-13](http://stackoverflow.com/questions/11039559/rsync-mkstemp-failed-permission-denied-13)
* [CentOS 5.5下rsync使用技巧与权限问题解读](http://os.51cto.com/art/201101/243374.htm)
* [rsync日常维护](http://dylanninin.com/blog/2012/10/26/rsync_exception.html)

---
layout: post
title: Split on Linux
category : Linux
tags : [Linux, Utilities]
---

split即一款分割文件的小工具，可以根据设定的大小（如行数、字节数等）将一个文件等分成更小的文件。若文件大小超出文件系统支持的单文件最大值，或由于网络传输的限制，此时将大文件切分成同等大小的小文件，则可以很好的解决这些问题。

##split help

	[test@server ~]$ split --help
	Usage: split [OPTION]... [INPUT [PREFIX]]
	Output fixed-size pieces of INPUT to PREFIXaa, PREFIXab, ...; default
	size is 1000 lines, and default PREFIX is `x'.  With no INPUT, or when INPUT
	is -, read standard input.
	
	Mandatory arguments to long options are mandatory for short options too.
	  -a, --suffix-length=N   use suffixes of length N (default 2)
	  -b, --bytes=SIZE        put SIZE bytes per output file
	  -C, --line-bytes=SIZE   put at most SIZE bytes of lines per output file
	  -d, --numeric-suffixes  use numeric suffixes instead of alphabetic
	  -l, --lines=NUMBER      put NUMBER lines per output file
	      --verbose           print a diagnostic just before each
	                            output file is opened
	      --help     display this help and exit
	      --version  output version information and exit
	
	SIZE may be (or may be an integer optionally followed by) one of following:
	KB 1000, K 1024, MB 1000*1000, M 1024*1024, and so on for G, T, P, E, Z, Y.
	
	Report split bugs to bug-coreutils@gnu.org
	GNU coreutils home page: <http://www.gnu.org/software/coreutils/>
	General help using GNU software: <http://www.gnu.org/gethelp/>
	For complete documentation, run: info coreutils 'split invocation'

##使用
查看文件大小

	[root@server ~]# ll -h
	total 4.0K
	-rw-r--r-- 1 root root 1.5K Mar 12 10:19 netstat.log.bz2

按bytes分割文件

	[root@server ~]# split -d -b 1K netstat.log.bz2 netstat.log.bz2.
	[root@server ~]# ll -h
	total 12K
	-rw-r--r-- 1 root root 1.5K Mar 12 10:19 netstat.log.bz2
	-rw-r--r-- 1 root root 1.0K Mar 12 10:22 netstat.log.bz2.00
	-rw-r--r-- 1 root root  500 Mar 12 10:22 netstat.log.bz2.01

测试分割后文件的完整性

	[root@server ~]# bzip2 -v -t netstat.log.bz2.00 
	  netstat.log.bz2.00: file ends unexpectedly
	
	You can use the `bzip2recover' program to attempt to recover
	data from undamaged sections of corrupted files.
	
	[root@server ~]# bzip2 -v -t netstat.log.bz2.01 
	  netstat.log.bz2.01: bad magic number (file not created by bzip2)
	
	You can use the `bzip2recover' program to attempt to recover
	data from undamaged sections of corrupted files.
	
合并分割后的文件

	[root@server ~]# cat netstat.log.bz2.0[0-1] > netstat.log.recover.bz2

测试合并后文件的完整性

	[root@server ~]# bzip2 -v -t netstat.log.recover.bz2 
	  netstat.log.recover.bz2: ok

注意

	-a  指定后缀名的长度。根据数字或字母，可以确定分割后的最大文件数
		如果后缀为数字[0-9]，则分割后最多有 10 ** ${suffix_length};
		如果后缀为字母[a-z]，则分割后最多有 26 ** ${suffix_length};

通常在使用split分割文件前，根据原文件大小，分割后大小，估算下分割后文件数量，以此确定合适的分割分割后缀和后缀长度，否则可能出现后缀不够用的情况。

查看要分割的文件大小

	[test@server ~]$ wc netstat.log 
	  302  1918 23945 netstat.log

按行分割文件

	[test@server ~]$ split -a 2 -d -l 2 --verbose netstat.log netstat.log.
	creating file `netstat.log.00'
	... ...
	creating file `netstat.log.99'
	split: output file suffixes exhausted

该文件有302行，按2行一个文件进行分割，则会产生151(302/2)个文件。但在分割时，使用数字为后缀，长度为2，则最多能够产生 10 ** 2 = 100个文件，显然不够用。

##参考

* [ext3 wikipedia](http://en.wikipedia.org/wiki/Ext3)
* [rsync日常维护](http://www.dylanninin.com/blog/2012/10/rsync-maintenance-info.html)

---
layout: post
title: YUM on Linux
category : Linux
tags : [Linux, Utilities]
---

##yum介绍

###为什么要使用yum

Linux系统维护中令管理员很头疼的就是软件包之间的依赖性了，往往是你要安装A软件，但是编译的时候告诉你X软件安装之前需要B软件，而当你安装Y软件的时候，又告诉你需要Z库了，好不容易安装好Z库，发现版本还有问题等。由于历史原因，[RPM](http://en.wikipedia.org/wiki/RPM_Package_Manager)软件包管理系统对软件之间的依存关系没有内部定义，造成安装RPM软件时经常出现令人无法理解的软件依赖问题。其实开源社区早就对这个问题尝试进行解决了，不同的发行版推出了各自的工具，比如[Yellow Dog](http://en.wikipedia.org/wiki/Yellow_Dog_Linux)的[YUM（Yellow dog Updater, Modified)](http://en.wikipedia.org/wiki/Yellowdog_Updater,_Modified)，[Debian](http://www.debian.org/)的[APT(Advanced Packaging Tool)](http://en.wikipedia.org/wiki/Advanced_Packaging_Tool)等。开发这些工具的目的都是为了要解决安装RPM时的依赖性问题，而不是额外再建立一套安装模式。这些软件也被开源软件爱好者们逐渐移植到别的发行版上。

目前，APT和YUM都可以运行在Red Hat系统上。目前 yum 是Red Hat/Fedora系统上默认安装的更新系统。

###什么是yum
YUM是 Yellow dog Updater, Modified 的简称，起初是由yellow dog 发行版的开发者 Terra Soft 研发，用 python 写成，那时叫做 yup (yellow dog updater)，后经杜克大学的[Linux@Duke](http://sites.duke.edu/linux/) 开发团队进行改进，遂有此名。yum 的宗旨是自动化地升级，安装/移除rpm包，收集rpm包的相关信息，检查依赖性并自动提示用户解决。yum 的关键之处是要有可靠的 repository，顾名思义，这是软件的仓库，它可以是 http 或 ftp 站点，也可以是本地软件池，但必须包含 rpm 的 header，header 包括了rpm 包的各种信息，包括描述，功能，提供的文件，依赖性等.正是收集了这些 header并加以分析，才能自动化地完成余下的任务。

yum 具有如下特点：

* 自动解决包的倚赖性问题能更方便的添加/删除/更新RPM包
* 便于管理大量系统的更新问题
* 可以同时配置多个资源库(Repository)
* 简洁的配置文件(/etc/yum.conf)
* 保持与RPM数据库的一致性
* 有一个比较详细的log，可以查看何时升级安装了什么软件包等
* 使用方便

###yum使用流程

CentOS 先将释出的软件放置到 YUM 服务器内，然后分析这些软件的相依属性问题，将软件内的记录资讯写下来 (header)。 然后再将这些资讯分析后记录成软件相关性的清单列表。这些列表数据与软件所在的位置可以称呼为容器 (repository)。 当用户端有软件安装的需求时，用户端主机会主动的向网络上面的 yum 服务器的容器网址下载清单列表， 然后透过清单列表的数据与本机 RPM 数据库已存在的软件数据相比较，就能够一口气安装所有需要的具有相依属性的软件了。

 整个流程可以简单的如下图说明：

![yum](http://vbird.dic.ksu.edu.tw/linux_basic/0520rpm_and_srpm_files/yum-01.gif)

当用户端有升级、安装的需求时， yum 会向容器要求清单的升级，等到清单升级到本机的 /var/cache/yum 里面后， 等一下升级时就会用这个本机清单与本机的 RPM 数据库进行比较，这样就知道该下载什么软件。接下来 yum 会跑到容器服务器 (yum server) 下载所需要的软件，然后再透过 RPM 的机制开始安装软件啦！这就是整个流程。


##内网YUM配置

在很多公司中，出于网络安全、维护成本等方面的考虑，服务器并没有接入到公共网络。从“yum使用流程”示意图可以看出，没有接入公网的服务器默认是无法访问外部的yum服务器的，这样yum带给系统管理员的维护便利似乎一下子荡然无存了。好在yum提供了简洁的资源库配置，结合ftp、http服务，我们就可以很方便快速的打造出一个内网的yum服务器，以供其他内网Linux系统使用。

以下即是一个简单的内网yum环境搭建示例。

###环境

* YUM Server： CentOS 6.2，httpd-2.2.15，192.168.1.111(yum.egolife.com)
* YUM Client： CentOS 6.2，192.168.2.111

###仓库规划

仓库（repository）是一个预备好的目录，或是一个网站，包含了软件包和索引文件。 yum 可以在仓库中自动地定位并获取正确的 RPM 软件包。这样，您就不必手动搜索和安装新应用程序和升级补丁了。只用一个命令，您就可以更新系统中所有软件，也可以根据指定搜索目标来查找安装新软件。
	
	[root@yum]# pwd
	/apps/yum	

	[root@yum]# tree -L 3 .
	.
	└── centos
		└── 6.2_64
	    	├── addons
		    ├── base
		    ├── centosplus
		    ├── contrib
		    ├── extras
		    └── updates
	
	8 directories, 0 files

在本例中，YUM镜像服务器为每个版本的 CentOS 分别提供了一些仓库。其中，为CentOS 6.2 64bit提供了
以下几个仓库：

* base: 构成 CentOS 发行版的软件包，和光盘上内容相同
* updates: base 仓库中软件包的更新版本
* addons: 已编译的但不在发行版中的软件包
* extras: 一大批附加的软件包
* centosplus: 用于增强一些现有软件包的功能
* contrib：社区用户构建贡献的软件包

###创建仓库

CentOS提供了`createrepo`工具，可以从rpm包来创建yum仓库。

拷贝CentOS ISO镜像，创建仓库

	[root@yum]# createrepo centos/6.2_64/base
	2762/6295 - gthumb-2.10.11-8.el6.x86_64.rpm 

当需要增加RPM包时，拷贝到相应的路径，并使用`createrepo`更新仓库即可

	[root@yum]# pwd
	/apps/yum/centos/6.2_64/extras
	
	[root@yum extras]# ll
	total 92
	-rw-r--r-- 1 root root 92328 Dec 13 11:02 rlwrap-0.37-1.el6.x86_64.rpm
	
	[root@yum]# createrepo --update .
	1/1 - rlwrap-0.37-1.el6.x86_64.rpm                                              
	Saving Primary metadata
	Saving file lists metadata
	Saving other metadata

	[root@yum]# ll
	total 96
	drwxr-xr-x 2 root root  4096 Apr  1 18:09 repodata
	-rw-r--r-- 1 root root 92328 Dec 13 11:02 rlwrap-0.37-1.el6.x86_64.rpm
	

###配置http

这里以Apache httpd作为http服务器，其他Web服务器配置可以自行查询参考手册。

创建连接文件，指向`/apps/yum`
	
	[root@yum html]# grep DocumentRoot /etc/httpd/conf/httpd.conf
	# DocumentRoot: The directory out of which you will serve your
	DocumentRoot "/var/www/html"
	# This should be changed to whatever you set DocumentRoot to.
	#    DocumentRoot /www/docs/dummy-host.example.com	

	[root@yum html]# pwd
	/var/www/html
	[root@yum html]# ln -s /apps/yum/ yum
	[root@yum html]# ll
	total 0
	lrwxrwxrwx 1 root root 10 Apr  1 17:39 yum -> /apps/yum/

开启http服务

	[root@yum]# service httpd start
	Starting httpd: 

###yum测试

####服务端测试

1.确认连接

	[root@client]# ping yum.egolife.com
	PING yum.egolife.com (192.168.1.111) 56(84) bytes of data.
	64 bytes from yum.egolife.com (192.168.1.111): icmp_seq=1 ttl=64 time=0.041 ms
	64 bytes from yum.egolife.com (192.168.1.111): icmp_seq=2 ttl=64 time=0.039 ms
	64 bytes from yum.egolife.com (192.168.1.111): icmp_seq=3 ttl=64 time=0.038 ms
	^C
	--- yum.egolife.com ping statistics ---
	3 packets transmitted, 3 received, 0% packet loss, time 2879ms
	rtt min/avg/max/mdev = 0.038/0.039/0.041/0.005 ms


2.更改yum仓库配置

	[root@yum]# vim /etc/yum.repos.d/CentOS-Base.repo
	[base]
	name=CentOS-$releasever - Base
	baseurl=http://yum.egolife.com/yum/centos/6.2_64/base
	gpgcheck=0
	enabled=1

	[updates]
	name=CentOS-$releasever - Updates
	baseurl=http://yum.egolife.com/yum/centos/6.2_64/updates
	gpgcheck=0
	enabled=0

	[extras]
	name=CentOS-$releasever - Extras
	baseurl=http://yum.egolife.com/yum/centos/6.2_64/extras
	gpgcheck=0
	enabled=1

	[centosplus]
	name=CentOS-$releasever - Plus
	baseurl=http://yum.egolife.com/yum/centos/6.2_64/centosplus
	gpgcheck=0
	enabled=1

	[contrib]
	name=CentOS-$releasever - Contrib
	baseurl=http://yum.egolife.com/yum/centos/6.2_64/contrib
	gpgcheck=0
	enabled=1

以上配置主要说明如下：

* repositoryid ： 用于指定一个仓库
* name： 用于指定易读的仓库名称
* mirrorlist： 用于指定仓库的镜像站点
* baseurl： 用于指定本仓库的 URL，可以是如下的几种类型：
	* http： 用于指定远程 HTTP 协议的源
	* ftp： 用于指定远程 FTP 协议的源
	* file： 用于本地镜像或 NFS 挂装文件系统
* enabled： 用于指定是否使用本仓库，默认值为1，即可用
* gpgcheck： 用于指定是否检查软件包的 GPG 签名
* gpgkey： 用于指定 GPG 签名文件的 URL

在本例中，`baseurl`为`http://yum.egolife.com/yum/centos/6.2_64/repositoryid`。

注：

在此次测试中，以硬编码的形式指定了具体的URL，包括yum服务器，Linux平台和发行版。在yum配置中，`releasever`,`basearch`以变量的形式从OS中获取，来确定不同Linux的平台和发行版本，从很大程度上保证了配置的灵活性。

在配置`baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/`中，可以看到有几个变量，`releaseve`和`basearch`，该变量的值，即对应当前的Linux发行版，可以从`/etc/yum.conf`中找到。

	[root@yum etc]# head /etc/yum.conf 
	[main]
	cachedir=/var/cache/yum/$basearch/$releasever
	keepcache=0
	debuglevel=2
	logfile=/var/log/yum.log
	exactarch=1
	obsoletes=1
	gpgcheck=1
	plugins=1
	installonly_limit=5

	[root@yum etc]# ll /var/cache/yum/x86_64/6/
	total 12
	drwxr-xr-x. 3 root root 4096 Apr  1 17:15 base
	drwxr-xr-x. 3 root root 4096 Apr  1 17:15 extras
	-rw-r--r--  1 root root    0 Apr  1 17:15 timedhosts.txt
	drwxr-xr-x. 3 root root 4096 Jan  7 00:03 updates
	
从以上输出，可以看到本例中，`releasever=6`,`basearch=x86_64`。

3.yum测试

	[root@yum conf]# yum info yum
	Loaded plugins: downloadonly, fastestmirror, refresh-packagekit, security
	Determining fastest mirrors
	base                                                                                                                                                             | 1.3 kB     00:00     
	extras                                                                                                                                                           | 1.3 kB     00:00     
	Installed Packages
	Name        : yum
	Arch        : noarch
	Version     : 3.2.29
	Release     : 22.el6.centos
	Size        : 4.5 M
	Repo        : installed
	From repo   : anaconda-CentOS-201112091719.x86_64
	Summary     : RPM package installer/updater/manager
	URL         : http://yum.baseurl.org/
	License     : GPLv2+
	Description : Yum is a utility that can check for and automatically download and
	            : install updated RPM packages. Dependencies are obtained and downloaded
	            : automatically, prompting the user for permission as necessary.


####客户端测试

其他Linux客户机中，在保证到yum.egolife.com正常的网络连接的前提下，按照上述配置，即可以正常使用yum服务。

	[root@client ~]# yum install -y lrzsz
	Loaded plugins: fastestmirror, refresh-packagekit
	Loading mirror speeds from cached hostfile
	Setting up Install Process
	Resolving Dependencies
	There are unfinished transactions remaining. You might consider running yum-complete-transaction first to finish them.
	The program yum-complete-transaction is found in the yum-utils package.
	--> Running transaction check
	---> Package lrzsz.x86_64 0:0.12.20-27.1.el6 will be installed
	--> Finished Dependency Resolution
	
	Dependencies Resolved
	
	============================================================================================
	 Package                                  Arch                                      Version                                               Repository                               Size
	============================================================================================
	Installing:
	 lrzsz                                    x86_64                                    0.12.20-27.1.el6                                      base                                     71 k
	
	Transaction Summary
	============================================================================================
	Install       1 Package(s)
	
	Total download size: 71 k
	Installed size: 159 k
	Downloading Packages:
	lrzsz-0.12.20-27.1.el6.x86_64.rpm                                                                                                                                |  71 kB     00:00     
	Running rpm_check_debug
	Running Transaction Test
	Transaction Test Succeeded
	Running Transaction
	  Installing : lrzsz-0.12.20-27.1.el6.x86_64                                                                                                                                        1/1 
	
	Installed:
	  lrzsz.x86_64 0:0.12.20-27.1.el6                                                                                                                                                       
	
	Complete!


##参考

* [软件安装：RPM、SRPM与YUM功能](http://vbird.dic.ksu.edu.tw/linux_basic/0520rpm_and_srpm.php)

---
layout: post
title: Bash Shell Startup Files
category : Linux
tags : [Linux]
---

bash shell startup files

	[root@oracle]# man bash
	... ...
	FILES
	  	/bin/bash
              The bash executable
       /etc/profile
              The systemwide initialization file, executed for login shells
       ~/.bash_profile
              The personal initialization file, executed for login shells
       ~/.bashrc
              The individual per-interactive-shell startup file
       ~/.bash_logout
              The individual login shell cleanup file, executed when a login shell exits
       ~/.inputrc
              Individual readline initialization file
	... ...


##参考

* [The Bash Shell Startup Files](http://www.linuxfromscratch.org/blfs/view/svn/postlfs/profile.html)

---
layout: post
title: Redmine Installation on CentOS
category : Linux
tags : [Linux, Utilities]
---

##bitnami-redmine

[Redmine](http://www.redmine.org/)是一个开源的项目管理系统，基于[ROR](http://rubyonrails.org/)开发，在小型项目管理中比较实用。[BitNami](http://bitnami.com/)提供了打包好的[Redmine](http://bitnami.com/stack/redmine)程序，集成Apache, Mysql，简化了安装配置过程。

以下是安装示例。

更改权限

	[root@redmine]# chmod +x bitnami-redmine-2.3.0-0-linux-x64-installer.run 
	
开始安装
	
	[root@redmine]# ./bitnami-redmine-2.3.0-0-linux-x64-installer.run 
	Language Selection

安装语言	

	Please select the installation language
	[1] English - English
	[2] Spanish - Español
	[3] Japanese - 日本語
	[4] Korean - 한국어
	[5] Simplified Chinese - 简体中文
	[6] Hebrew - עברית
	[7] German - Deutsch
	[8] Romanian - Română
	[9] Russian - Русский
	Please choose an option [1] : 
	----------------------------------------------------------------------------
	Welcome to the BitNami Redmine Stack Setup Wizard.
	
选择组件

	----------------------------------------------------------------------------
	Select the components you want to install; clear the components you do not want 
	to install. Click Next when you are ready to continue.
	
	PhpMyAdmin [Y/n] :n
	
	Redmine : Y (Cannot be edited)
	
	Is the selection above correct? [Y/n]: Y
	
安装路径

	----------------------------------------------------------------------------
	Installation folder
	
	Please, choose a folder to install BitNami Redmine Stack
	
	Select a folder [/opt/redmine-2.3.0-0]: /apps/redmine 

管理员账户
	
	----------------------------------------------------------------------------
	Create Admin account
	
	BitNami Redmine Stack admin user creation
	
	Login [user]: admin
	
	Password :z9AZz7cz
	Please confirm your password :z9AZz7cz
	Your real name [User Name]: gonghaibing
	
	Email Address [mail@example.com]: gonghaibing@tp-link.net
	
配置语言

	----------------------------------------------------------------------------
	Language for default data configuration
	
	Select your language for default data configuration:
	
	[1] Bulgarian
	[2] Czech
	[3] German
	[4] English
	[5] Spanish
	[6] French
	[7] Hebrew
	[8] Italian
	[9] Japanese
	[10] Korean
	[11] Dutch
	[12] Polish
	[13] Portuguese
	[14] Portuguese/Brazilian
	[15] Romanian
	[16] Russian
	[17] Serbian
	[18] Swedish
	[19] Chinese
	[20] Chinese/Taiwan
	Please choose an option [4] : 
	
	Do you want to configure mail support? [y/N]: y
	
邮件支持	

	----------------------------------------------------------------------------
	Configure SMTP Settings
	
	This is required so your application can send notifications via email.
	
	Default email provider:
	
	[1] GMail
	[2] Custom
	Please choose an option [1] : 2
	
	----------------------------------------------------------------------------
	Configure SMTP Settings
	
	Default mail server configuration.
	
	Username []: dylanninin
	
	Password :
	Re-enter :
	SMTP Host []: smtp.egolife.com
	
	SMTP Port []: 25
	
	Secure connection
	
	[1] None
	[2] SSL
	[3] TLS
	Please choose an option [3] : 1
	
开始安装

	----------------------------------------------------------------------------
	Setup is now ready to begin installing BitNami Redmine Stack on your computer.
	
	Do you want to continue? [Y/n]: Y
	
	----------------------------------------------------------------------------
	Please wait while Setup installs BitNami Redmine Stack on your computer.
	
	 Installing
	 0% ______________ 50% ______________ 100%
	 #########################################
	
	----------------------------------------------------------------------------
	Setup has finished installing BitNami Redmine Stack on your computer.
	
	Launch Redmine application. [Y/n]: Y
	
	Info: To access the BitNami Redmine Stack, go to
	http://localhost:80 from your browser.
	Press [Enter] to continue :

##导出数据

	[root@old]# /opt/rubystack-2.1-0/apps/redmine/config/database.yml

	production:
	  adapter: mysql
	  database: bitnami_redmine
	  host: localhost
	  username: bn_redmine
	  password: b3eae0f5e6 
	  socket: /opt/rubystack-2.1-0/mysql/tmp/mysql.sock

	[root@old rubystack-2.1-0]# mysql/bin/mysqldump -ubn_redmine -p --opt bitnami_redmine > ~/redmine_20130402.sql
	Enter password: 

##导入数据

	[root@redmine]# less /apps/rubystack/apps/redmine/htdocs/config/database.yml
	
	production:
	  adapter: mysql
	  database: bitnami_redmine
	  host: localhost
	  username: bn_redmine
	  password: 7e37e93290 
	  socket: /apps/rubystack/mysql/tmp/mysql.sock

	[root@redmine ~]# /apps/redmine/mysql/bin/mysql -ubitnami -p bitnami_redmine < redmine_20130402.sql 
	Enter password: 7e37e93290

##参考

* [Installing Redmine](http://www.redmine.org/projects/redmine/wiki/RedmineInstall)
* [bitnami Redmine]((http://bitnami.com/stack/redmine))

---
layout: post
title: MySQL Backup Tools Summary
category : Database
tags : [MySQL, DBA]
---

##Comparison of Backup Tools

<table class="table table-striped">
	<tr>
		<td>Characteristics</td><td>mylvmbackup</td><td>mysqldump</td><td>mk-parallel-dump</td><td>mysqlhotcopy</td><td>ibbackup</td>
	</tr>
	<tr>
		<td>Blocks processing</td><td>Optional</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td>
	</tr>
	<tr>
		<td>Logical or Raw</td><td>Raw</td><td>Logical</td><td>Logical</td><td>Raw</td><td>Raw</td>
	</tr>
	<tr>
		<td>Engines</td><td>All</td><td>All</td><td>All</td><td>MyISAM/Archive</td><td>InnoDB</td>
	</tr>
	<tr>
		<td>Speed</td><td>Very Good</td><td>Slow</td><td>Good</td><td>Very Good</td><td>Very Good</td>
	</tr>
	<tr>
		<td>Remote Backups</td><td>No</td><td>Yes</td><td>Yes</td><td>No</td><td>No</td>
	</tr>
	<tr>
		<td>Availability</td><td>Free</td><td>Free</td><td>Free</td><td>Free</td><td>Commercial</td>
	</tr>
	<tr>
		<td>License</td><td>GPL</td><td>GPL</td><td>GPL</td><td>GPL</td><td>Proprietary</td>
	</tr>
</table>

##Supplement

###Delimited file backups

To create a text file containing a table's data, you can use [`SELECT * INTO OUTFILE 'file_name' FROM tbl_name`](http://dev.mysql.com/doc/refman/5.1/en/select-into.html). The file is created on the MySQL server host, not the client host. For this statement, the output file cannot already exist because permitting files to be overwritten constitutes a security risk. See [`SELECT Syntax`](http://dev.mysql.com/doc/refman/5.1/en/select.html). This method works for any kind of data file, but saves only table data, not the table structure.

Another way to create text data files (along with files containing [`CREATE TABLE`](http://dev.mysql.com/doc/refman/5.1/en/create-table.html) statements for the backed up tables) is to use `mysqldump` with the [`--tab`](http://dev.mysql.com/doc/refman/5.1/en/mysqldump.html#option_mysqldump_tab) option. See [Section 1.4.3, “Dumping Data in Delimited-Text Format with mysqldump”](https://dev.mysql.com/doc/mysql-backup-excerpt/5.1/en/mysqldump-delimited-text.html).

To reload a delimited-text data file, use [`LOAD DATA INFILE`](http://dev.mysql.com/doc/refman/5.1/en/load-data.html) or `mysqlimport`.

backup

	mysql> select * into outfile '/db/backup/table1.txt'
		 > fields terminated by ',' optionally enclosed by '"'
		 > lines terminated by '\n'
		 > from test.table1;

recovery

	mysql> load data infile '/db/backup/table1.txt'
		 > into table test.table1
		 > fields terminated by ',' optionally enclosed by '"'
		 > lines terminated by '\n';

##Reference

* [High Performance MySQL Backup and Recovery](http://book.douban.com/subject/1495763/)
* [MySQL Backup and Recovery](https://dev.mysql.com/doc/mysql-backup-excerpt/5.1/en/index.html)
* [mysqldump command reference](https://dev.mysql.com/doc/refman/5.1/en/mysqldump.html)
* [mysqlhotcopy command reference](https://dev.mysql.com/doc/refman/5.0/en/mysqlhotcopy.html)
* [mylvmbackup](http://www.lenzg.net/mylvmbackup/)
* [mk-parallel-dump](http://www.maatkit.org/doc/mk-parallel-dump.html)
* [ibbackup command reference](https://dev.mysql.com/doc/mysql-enterprise-backup/3.5/en/options.html)

---
layout: post
title: HWM Introduction
category : Oracle
tags : [Oracle, Database, DBA]
---

在ORACLE Database中，执行对表的删除操作不会降低该表的高水位线。而全表扫描将始终读取一个段(extent)中所有低于高水位线标记的块。如果在执行删除操作后不降低高水位线标记，则将导致查询语句的性能低下。rebuild, truncate, shrink,move 等操作会降低高水位。

##实验

###创建表

	[oracle@dev ~]$ sqlplus /nolog
	
	SQL*Plus: Release 11.2.0.1.0 Production on Wed Apr 17 14:25:48 2013
	
	Copyright (c) 1982, 2009, Oracle.  All rights reserved.
	
	SQL> conn dev
	Enter password: 
	Connected.
	SQL> create table hwm (id number);
	
	Table created.
	
	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	no rows selected

	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME			 			NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------------------ ---------- ---------- ------------
	HWM

疑问：在查询dba_segments时没有找到数据，延时？？？

	
###插入数据

	SQL> declare
	  2      i number;
	  3  begin
	  4      for i in 1..10000 loop
	  5          insert into hwm values(i);
	  6      end loop;
	  7      commit;
	  8  end;
	  9  /
	
	PL/SQL procedure successfully completed.
	

	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       TABLE			  		  24
	

	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   NUM_ROWS       BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM
	
此时表HWM已经占有了数据，24个数据块。其他统计信息为空。这些信息需要做统计分析之后才会有。

###搜集统计信息

	SQL> exec dbms_stats.gather_table_stats('DEV','HWM');
	
	PL/SQL procedure successfully completed.
	
	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       	TABLE			  		  24
	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   	 NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM				    	10000	  	  20			0
	
使用`dbms_stats`收集统计信息后，显示表HWM有1000行，占用20个数据块。但`EMPTY_BLOCKS`为空，该列需要在`ANALYZE`之后才会有数据。


###分析表
	
	
	SQL> analyze table hwm compute statistics;
	
	Table analyzed.


	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       TABLE			  		  24
	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   	 NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM				    	10000	  	  20			4
	

###删除数据

	SQL> delete from hwm;
	
	10000 rows deleted.
	
	SQL> commit;
	
	Commit complete.
	
	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       TABLE			  		  24
	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   	 NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM				        10000	      20			4
	

删除数据后再分析表

	SQL> analyze table hwm compute statistics;
	
	Table analyzed.
	
	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       TABLE			  	      24
	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   	 NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM							0	  	  20			4
	

delete数据，不会降低高水位。


###truncate表

	SQL> truncate table hwm;
	
	Table truncated.

	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       TABLE			   		   8
	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   	 NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM						    0	  	  20			4
	

runcate后再次收集统计信息


	SQL> exec dbms_stats.gather_table_stats('DEV','HWM');
	
	PL/SQL procedure successfully completed.

	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       TABLE			   		   8
	
	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   	 NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM						    0	       0			4
	

再次分析表

	SQL> analyze table hwm compute statistics;
	
	Table analyzed.

	SQL> select segment_name,segment_type,blocks from dba_segments where segment_name = 'HWM';
	
	SEGMENT_NAME	   SEGMENT_TYPE	  	  	  BLOCKS
	------------------ ------------------ ----------
	HWM			       TABLE			   		   8
	
	SQL> select table_name,num_rows, blocks, empty_blocks from user_tables where table_name = 'HWM';
	
	TABLE_NAME		   	 NUM_ROWS     BLOCKS EMPTY_BLOCKS
	------------------ ---------- ---------- ------------
	HWM						    0	   	   0		    8

truncate后，高水位降低。

---
layout: post
title: LDAP Introduction
category : Linux
tags : [Linux, Utilities, LDAP]
---

##基本介绍

LDAP是轻量目录访问协议，英文全称是Lightweight Directory Access Protocol，一般都简称为LDAP。它是基于X.500标准的，但是简单多了并且可以根据需要定制。与X.500不同，LDAP支持TCP/IP，这对访问Internet是必须的。LDAP的核心规范在RFC中都有定义，所有与LDAP相关的RFC都可以在LDAPman RFC网页中找到。

简单说来，LDAP是一个得到关于人或者资源的集中、静态数据的快速方式。

LDAP是一个用来发布目录信息到许多不同资源的协议。通常它都作为一个集中的地址本使用，不过根据组织者的需要，它可以做得更加强大。目前已经被很多流行的应用和服务支持，如Apache、Oracle、Linux等。

##LDAP模型

###1.信息模型

在LDAP中信息以树状方式组织，在树状信息中的基本数据单元是条目，而每个条目由属性构成，属性中存储有属性值。

LDAP中的信息模式，类似于面向对象的概念，在LDAP中每个条目必须属于某个或多个`objectClass`，每个`objectClass`由多个属性类型组成，每个属性类型有所对应的语法和匹配规则；对象类和属性类型的定义均可以使用继承的概念。

每个条目创建时，必须定义所属的对象类，必须提供对象类中的必选属性类型的属性值，在LDAP中一个属性类型可以对应多个值

在LDAP中对象类、属性类型、语法和匹配规则统称为Schema.
在LDAP中有许多系统对象类、属性类型、语法和匹配规则，这些系统 Schema在LDAP标准中进行了规定，同时不同的应用领域也可以自定义Schema；同时用户在应用时，也可根据需要自定义Schema.

###2.命名模型

即LDAP中的条目定位方式。在LDAP中每个条目均有自己的DN(Distinguish Name)和RDN(Relative Distinguish Name)。
		
DN是该条目在整个树中的唯一名称标识，RDN是条目在父节点下的唯一名称标识，如同文件系统中，带路径的文件名就是DN，文件名就是RDN。

###3.功能模型

在LDAP中共有四类10种操作：

* 查询类操作:搜索、比较
* 更新类操作:如添加、删除、修改条目，修改条目名
* 认证类操作:如绑定、解绑定；
* 其它操作:放弃和扩展操作。

除扩展操作，另外9种是LDAP的标准操作；扩展操作是LDAP中为了增加新的功能，提供的一种标准的扩展框架，在新的RFC标准和草案中正在增加一些新的扩展操作，不同的LDAP厂商也均定义了自己的扩展操作。

###4.安全模型

主要通过身份认证、安全通道和访问控制来实现。

####身份认证

* 匿名：
	不对用户进行认证，该方法仅对完全公开的方式适用
* 基本认证：
	通过用户名和密码进行身份识别，又分为简单密码和摘要密码认证
* SASL（Simple Authentication and Secure Layer）：
	在SSL和TLS安全通道基础上进行的身份认证，包括数字证书认证

####安全通道

在LDAP中提供了基于SSL/TLS的通讯安全保障。SSL/TLS是基于PKI信息安全技术，是目前Internet上广泛采用的安全服务。 LDAP通过StartTLS方式启动TLS服务，可以提供通讯中的数据保密性、完整性保护；通过强制客户端证书认证的TLS服务，同时可以实现对客户端身份和服务器端身份的双向验证。

####访问控制

虽然LDAP目前并无访问控制的标准，但从一些草案中或是事实上LDAP产品的访问控制情况，我们不难看出：LDAP访问控制异常的灵活和丰富，在 LDAP中是基于访问控制策略语句来实现访问控制的，这不同于现有的关系型数据库系统和应用系统，它是通过基于访问控制列表来实现的，无论是基于组模式或角色模式，都摆脱不了这种限制。


##过滤器

LDAP是一个查询为主的记录结构，无论是何种查询方式，最终都由过滤器确定查询的条件。过滤器相当于SQL中的`WHERE`子句。任何LDAP的类过滤和字符串都必须放在括号内，如`（objectclass=*）`,指列出所有类型的记录。

可以使用`=`，`>=`，`<=`，`~=`（约等于）进行比较，如`(number<=100)`。

合并条件是最怪的，必须把操作符放在两个操作对象的前面而不是中间，单一操作对象用括号括起来。如：

	A与B，不是A&B，而是（&(A)(B)）；
	或使用“|"表示；
	非使用“!"表示。

对于"与"，或"或"在操作符后可以跟多个条件表达式，但非后则只能是单个表达式。


##工具

* 使用python操作ldap：[ldapsearch.py@github](https://github.com/dylanninin/utils/blob/master/ldapsearch.py)
* Jarek Gawor's [LDAP Browser in Java](http://www.novell.com/communities/node/8652/gawors-excellent-ldap-browsereditor-v282)

---
layout: post
title: CUPS Introduction
category : Linux
tags : [Linux, Utilities]
---

`CUPS`全称为Common Unix Printing System,是类Unix系统上的通用打印系统，支持本地、远程打印。`CUPS`主要有以下组件或子系统组成：

* print spooler/scheduler: convert LPD requests to IPP; provide a web-based interface for managing print jobs, configurations
* filter system：convert the print data to specified formats
* backend system：send data to print devices

示意图如下：

![CUPS Simple](https://upload.wikimedia.org/wikipedia/commons/6/64/Cups_simple.svg)

##配置文件

	[root@cups]# pwd
	/etc/cups

	[root@cups]# tree 
	.
	├── classes.conf					
	├── client.conf
	├── cupsd.conf						--> CUPS服务配置
	├── cupsd.conf.default
	├── interfaces
	├── lpoptions
	├── paps.convs
	├── ppd
	├── printers.conf					--> 打印队列配置
	├── snmp.conf
	├── ssl
	└── subscriptions.conf
	
	3 directories, 10 files

以上配置文件介绍、作用、指令，大都可以使用`man file`来查看；若还有不明确的，可以访问`CUPS`服务的Online Help页面，内容比较详细。

##认证示例

When you enable remote administration, the server will use Basic authentication for adminstration tasks. The current CUPS server supports Basic, Digest, Kerberos, and local certificate authentication:

* *Basic authentication* essentially places the clear text of the username and password on the network.
Since CUPS uses the system username and password account information, the authentication information could be used to gain access to possibly privileged accounts on the server.

	Recommendation: Enable encryption to hide the username and password information - this is the default on MacOS X and systems with GNU TLS or OpenSSL installed.

* *Digest authentication* uses an MD5 checksum of the username, password, and domain ("CUPS"), so the original username and password is not sent over the network.
The current implementation does not authenticate the entire message and uses the client's IP address for the nonce value, making it possible to launch "man in the middle" and replay attacks from the same client.

	Recommendation: Enable encryption to hide the username and password information.

* *Local certificate authentication* passes 128-bit "certificates" that identify an authenticated user. Certificates are created on-the-fly from random data and stored in files under /var/run/cups/certs. They have restricted read permissions: root + system-group(s) for the root certificate, and lp + lp for CGI certificates.
Because certificates are only available on the local system, the CUPS server does not accept local authentication unless the client is connected to the loopback interface (127.0.0.1 or ::1) or domain socket.

	Recommendation: Ensure that unauthorized users are not added to the system group(s).


这里使用"Basic Authentication"介绍下使用系统账户访问`CUPS`服务的方式。

创建用户/用户组

	[root@cups]# groupadd cupsadmin

	[root@cups]# useradd -g cupsadmin -s /sbin/nologin cupsuser

	[root@cups]# passwd  cupsuser

更改`/etc/cups/cupsd.conf`配置

	[root@cups]# vim /etc/cups/cupsd.conf
	... ...
	
	# Administrator user group...
	# 2013-05-06 dylanninin@gmail.com  customize SystemGroup to cupsadmin
	SystemGroup cupsadmin
	
	
	# Only listen for connections from the local machine.
	#Listen localhost:631
	# 2013-05-06 dylanninin@gmail.com listen on 631 port of all the interfaces
	Listen 631
	Listen /var/run/cups/cups.sock
	
	# Show shared printers on the local network.
	Browsing On
	BrowseOrder allow,deny
	BrowseAllow all
	BrowseLocalProtocols CUPS dnssd
	
	# Default authentication type, when authentication is required...
	DefaultAuthType Basic
	
	# Restrict access to the server...
	# 2013-05-06 dylanninin@gmail.com access control
	<Location />
	  Order allow,deny
	  Allow all
	  Require valid-user
	  Require user @SYSTEM
	</Location>
	
	# Restrict access to the admin pages...
	# 2013-05-06 dylanninin@gmail.com access control
	<Location /admin>
	  Order allow,deny
	  Allow all
	  Require valid-user
	  Require user @SYSTEM
	</Location>
	
	# Restrict access to configuration files...
	# 2013-05-06 dylanninin@gmail.com access control
	<Location /admin/conf>
	  AuthType Default
	  Require user @SYSTEM
	  Order allow,deny
	  Allow all
	  Require valid-user
	  Require user @SYSTEM
	</Location>
	... ...

	
重启`CUPS`服务
	
	[root@cups]# service cups restart
	Stopping cups:                                             [  OK  ]
	Starting cups:                                             [  OK  ]


使用浏览器打开`https://yourhostname:631`，输入用户名、密码即可看到管理界面，截图为证。

![CUPS_Server](http://dylanninin.com/assets/images/2013/cups_server.png)

##参考

* [CUPS wikipedia](https://en.wikipedia.org/wiki/CUPS)
* [利用CUPS配置Linux打印机](http://vbird.dic.ksu.edu.tw/linux_basic/0610hardware_2.php)

---
layout: post
title: Configure iscsi Storage on Linux
category : Linux
tags : [Linux, Storage]
---

##ISCSI

Internet SCSI (iSCSI) is a network protocol s that allows you to use of the SCSI protocol over TCP/IP networks. It is good alternative to Fibre Channel-based SANs. You can easily manage, mount and format iSCSI Volume under Linux. It allows access to SAN storage over Ethernet.

##Open-iSCSI Project

Open-iSCSI project is a high-performance, transport independent, multi-platform implementation of iSCSI. Open-iSCSI is partitioned into user and kernel parts.

Instructions are tested on:

* [a] RHEL 5
* [b] CentOS 5
* [c] Fedora 7
* [d] Debian / Ubuntu Linux

###Install Required Package

iscsi-initiator-utils RPM package - The iscsi package provides the server daemon for the iSCSI protocol, as well as the utility programs used to manage it. iSCSI is a protocol for distributed disk access using SCSI commands sent over Internet Protocol networks. This package is available under Redhat Enterprise Linux / CentOS / Fedora Linux and can be installed using yum command:

	# yum install iscsi-initiator-utils

A note about Debian / Ubuntu Linux

If you are using Debian / Ubuntu Linux install open-iscsi package, enter:

	$ sudo apt-get install open-iscsi

##CentOS / Red Hat Linux: Install and manage iSCSI Volume

###iSCSI Configuration

There are three steps needed to set up a system to use iSCSI storage:

* iSCSI startup using the init script or manual startup. You need to edit and configure iSCSI via `/etc/iscsi/iscsid.conf` file
* Discover targets.
* Automate target logins for future system reboots.

You also need to obtain iSCSI username, password and storage server IP address (target host)

####Step # 1: Configure iSCSI

Open /etc/iscsi/iscsid.conf with vi text editor:

	# vi /etc/iscsi/iscsid.conf

Setup username and password:

	node.session.auth.username = My_ISCSI_USR_NAME
	node.session.auth.password = MyPassword
	discovery.sendtargets.auth.username = My_ISCSI_USR_NAME
	discovery.sendtargets.auth.password = MyPassword

Where,

* node.session.* is used to set a CHAP username and password for initiator authentication by the target(s).
* discovery.sendtargets.* is used to set a discovery session CHAP username and password for the initiator authentication by the target(s)

You may also need to tweak and set other options. Refer to man page for more information. Now start the iscsi service:

	# /etc/init.d/iscsid start

####Step # 2: Discover targets

Now use iscsiadm command, which is a command-line tool allowing discovery and login to iSCSI targets, as well as access and management of the open-iscsi database. If your storage server IP address is 192.168.1.5, enter:

	# iscsiadm -m discovery -t sendtargets -p 192.168.1.5

	# /etc/init.d/iscsid restart

Now there should be a block device under /dev directory. To obtain new device name, type:

	# fdisk -l

or

	# tail -f /var/log/messages

Output:

	Oct 10 12:42:20 ora9is2 kernel:   Vendor: EQLOGIC   Model: 100E-00           Rev: 3.2
	Oct 10 12:42:20 ora9is2 kernel:   Type:   Direct-Access                      ANSI SCSI revision: 05
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: 41963520 512-byte hdwr sectors (21485 MB)
	Oct 10 12:42:20 ora9is2 kernel: sdd: Write Protect is off
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: drive cache: write through
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: 41963520 512-byte hdwr sectors (21485 MB)
	Oct 10 12:42:20 ora9is2 kernel: sdd: Write Protect is off
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: drive cache: write through
	Oct 10 12:42:20 ora9is2 kernel:  sdd: unknown partition table
	Oct 10 12:42:20 ora9is2 kernel: sd 3:0:0:0: Attached scsi disk sdd
	Oct 10 12:42:20 ora9is2 kernel: sd 3:0:0:0: Attached scsi generic sg3 type 0
	Oct 10 12:42:20 ora9is2 kernel: rtc: lost some interrupts at 2048Hz.
	Oct 10 12:42:20 ora9is2 iscsid: connection0:0 is operational now

`/dev/sdd` is my new block device.

####Step # 3: Format and Mount iSCSI Volume

You can now partition and create a filesystem on the target using usual fdisk and mkfs.ext3 commands:

	# fdisk /dev/sdd
	# mke2fs -j -m 0 -O dir_index /dev/sdd1

OR

	# mkfs.ext3 /dev/sdd1

Tip: If your volume is large size like 1TB, run mkfs.ext3 in background using nohup:

	# nohup mkfs.ext3 /dev/sdd1 &

Mount new partition:

	# mkdir /mnt/iscsi
	# mount /dev/sdd1 /mnt/iscsi

####Step #4: Mount iSCSI drive automatically at boot time

First make sure iscsi service turned on at boot time:

	# chkconfig iscsi on

Open `/etc/fstab` file and append config directive:

	/dev/sdd1 /mnt/iscsi ext3 _netdev 0 0

Save and close the file.

##测试

###1.关于多重联机

多重联机即一个或多个客户端连接一块SAN分配的存储单元，不过为避免数据受损，必须确认在集群 架构下使用才行。

在测试过程中，使用新购置的两台ERP测试服务器共享一块网络存储，发现存在以下问题：

####多写

mount 均采用  `mount -o acl,rw /dev/sdb5 /data` 进行挂载；写数据时，都是使用root写入；

* a. 多台客户端可以同时挂载，但挂载后，写入的数据仅有自身可以看到，其他客户端不可见；
* b. 多台客户端可以同时挂载时，各客户端均有写入数据时，当时数据没有丢失，但是重新挂载网络设备时，会出现数据丢失的情况（只能看到最后写入的数据）；

####一写多读

* a. 多个客户端同时挂载，仅一个客户端写数据，其他读；在写入时，其他客户端暂时不可见，必须重新挂载；
* b. 关于AIX和Oracle Linux之间共享该网络存储（一写一读），Oracle Linux挂载jfs2文件系统时不识别，添加jfs相关的软件包后挂载失败；在CentOS 6.2上，添加jfs2软件包后可以挂载jfs2文件系统。对比后，发现Oracle Linux启动的内核为uek内核，可能和当前系统运行的内核相关。
 
###2.其他

另外，测试了下使用SAN存储搭建NFS，多客户端可以同时读写，但速率很低。

##Reference

* [Linux ISCSI howto](http://www.cyberciti.biz/tips/rhel-centos-fedora-linux-iscsi-howto.html)
* [AIX 5.3/6.1 ISCSI howto](http://dawangliang.blog.163.com/blog/static/1879031682011543446756/)
* [AIX 5.3 iSCSI softwre initiator](http://publib.boulder.ibm.com/infocenter/pseries/v5r3/index.jsp?topic=/com.ibm.aix.commadmn/doc/commadmndita/iscsi_config.htm)
* [AIX 5.3 iSCSI target file reference](http://publib.boulder.ibm.com/infocenter/pseries/v5r3/index.jsp?topic=/com.ibm.aix.commadmn/doc/commadmndita/iscsi_config.htm)
---
layout: post
title: Key Problems in Oracle EBS Upgrade
category : Oracle
tags : [Oracle, DBA, EBS]
---

##ERP痛点问题

###1.进程会话监控和异常处理

	1.监控进程会话正在执行的SQL和资源使用情况
	2.过期或死掉进程会话的清理


###2.工作流监控和异常处理

	1.工作流以及其附属服务(如邮件，通知)的架构和处理流程
	2.工作流、邮件、通知服务运行状态的监控和异常处理
	3.主要单据审批工作流监控和异常处理
	4.工作流运行数据的检查和清理


###3.并发请求监控和异常处理

	1.主要处理器的设计原理，架构
	2.主要处理器运行状态的监控和异常处理
	3.请求任务运行状态的监控和异常处理
	4.请求运行数据的检查和清理


###4.数据库性能优化

	1.SQL监控及调优的一般过程和工具
	2.数据库内存的监控和优化
	3.数据库存储的监控和优化，如表，索引，表空间等
	4.数据库性能优化的主动监控和建议


###5.数据库开发和安全

	1.数据库开发的建议和注意点，如表、视图的创建等
	2.客制化开发中DBA应担任的角色和主要任务
	3.数据库权限分配和资源限制
	4.用户操作审计和分7，如记录执行DDL的用户、ip、时间、DDL具体内容等


###6.服务器健康检查和维护

	1.ERP应用各模块的健康检查和日常维护
	2.ERP主要服务的健康检查和日常维护，如Apache、Forms、Reports等
	3.ERP数据库的健康检查和日常维护
	4.每月或季度需对应用、数据库以及服务器的健康检查和参数调整，如重启应用、数据库，增加内存等

---
layout: post
title: Discover V7000 on Redhat 
category : Linux
tags : [Linux, Storage]
---

##Steps to configure the Linux Host

Follow these steps to configure the Linux host:

* Use the latest firmware levels on your host system.
* Install the HBA or HBAs on the Linux server.
* Install the supported HBA driver/firmware and upgrade the kernel, if required.
* Connect the Linux server FC host adapters to the switches.
* Configure the switches (zoning) if needed.
* Install SDD for Linux.
* Configure the host, volumes, and host mapping in the IBM Flex System V7000 Storage Node.
* Rescan for LUNs on the Linux server to discover the volumes that were created on the IBM Flex System V7000 Storage Node.

###os kernel

	[root@dev1 ~]# uname -a
	Linux dev1.egolife.com 2.6.39-400.17.1.el6uek.x86_64 #1 SMP Fri Feb 22 18:16:18 PST 2013 x86_64 x86_64 x86_64 GNU/Linux

###hba details

fc device

	[root@dev1 ~]# lspci  | grep Fibre
	8b:00.0 Fibre Channel: Emulex Corporation Saturn-X: LightPulse Fibre Channel Host Adapter (rev 03)
	8b:00.1 Fibre Channel: Emulex Corporation Saturn-X: LightPulse Fibre Channel Host Adapter (rev 03)

	[root@dev1 ~]# grep Fibre /var/log/dmesg
	Emulex LightPulse Fibre Channel SCSI driver 8.3.5.86.2p
	scsi3 : Emulex LPe12000 PCIe Fibre Channel Adapter  on PCI bus 8b device 00 irq 48
	scsi4 : Emulex LPe12000 PCIe Fibre Channel Adapter  on PCI bus 8b device 01 irq 58

fc hosts

	[root@dev1 ~]# ll /sys/class/fc_host/host*
	lrwxrwxrwx. 1 root root 0 Jun 18 10:42 /sys/class/fc_host/host3 -> ../../devices/pci0000:80/0000:80:03.0/0000:8b:00.0/host3/fc_host/host3
	lrwxrwxrwx. 1 root root 0 Jun 18 10:42 /sys/class/fc_host/host4 -> ../../devices/pci0000:80/0000:80:03.0/0000:8b:00.1/host4/fc_host/host4

fc_host directory

	[root@dev1 ~]# ll /sys/class/fc_host/host3/
	total 0
	-r--r--r--. 1 root root 4096 Jun 18 10:43 active_fc4s
	lrwxrwxrwx. 1 root root    0 Jun 18 10:43 device -> ../../../host3
	-rw-r--r--. 1 root root 4096 Jun 18 10:43 dev_loss_tmo
	-r--r--r--. 1 root root 4096 Jun 18 10:43 fabric_name
	--w-------. 1 root root 4096 Jun 18 10:43 issue_lip
	-r--r--r--. 1 root root 4096 Jun 18 10:43 maxframe_size
	-r--r--r--. 1 root root 4096 Jun 18 10:43 max_npiv_vports
	-r--r--r--. 1 root root 4096 Jun 18 10:43 node_name
	-r--r--r--. 1 root root 4096 Jun 18 10:43 npiv_vports_inuse
	-r--r--r--. 1 root root 4096 Jun 18 10:43 port_id
	-r--r--r--. 1 root root 4096 Jun 18 10:43 port_name
	-r--r--r--. 1 root root 4096 Jun 18 10:43 port_state
	-r--r--r--. 1 root root 4096 Jun 18 10:43 port_type
	drwxr-xr-x. 2 root root    0 Jun 18 10:43 power
	-r--r--r--. 1 root root 4096 Jun 18 10:43 speed
	drwxr-xr-x. 2 root root    0 Jun 18 10:43 statistics
	lrwxrwxrwx. 1 root root    0 Jun 18 10:43 subsystem -> ../../../../../../../class/fc_host
	-r--r--r--. 1 root root 4096 Jun 18 10:43 supported_classes
	-r--r--r--. 1 root root 4096 Jun 18 10:43 supported_fc4s
	-r--r--r--. 1 root root 4096 Jun 18 10:43 supported_speeds
	-r--r--r--. 1 root root 4096 Jun 18 10:43 symbolic_name
	-rw-r--r--. 1 root root 4096 Jun 18 10:43 tgtid_bind_type
	-rw-r--r--. 1 root root 4096 Jun 18 10:43 uevent
	--w-------. 1 root root 4096 Jun 18 10:43 vport_create
	--w-------. 1 root root 4096 Jun 18 10:43 vport_delete

hba wwn

	[root@dev1 ~]# cat /sys/class/fc_host/host*/node_name 
	0x20000090fa3436de
	0x20000090fa3436df

hba hosts in v7000

operation:V7000 -> Cluster -> Hosts -> Ports by Host -> Host Details:APP01

hba name:
	10000090FA3436DE
	10000090FA3436DF

![APP01 hba details](http://dylanninin.com/assets/images/2013/app01_ports.png)

###rpms

install multipath packages

	[root@dev1 ~]# yum install -y device-mapper-multipath

	[root@dev1 ~]# rpm -qa | grep device-mapper
	device-mapper-persistent-data-0.1.4-1.el6.x86_64
	device-mapper-event-1.02.77-9.el6.x86_64
	device-mapper-1.02.77-9.el6.x86_64
	device-mapper-multipath-0.4.9-64.0.1.el6.x86_64
	device-mapper-event-libs-1.02.77-9.el6.x86_64
	device-mapper-multipath-libs-0.4.9-64.0.1.el6.x86_64
	device-mapper-libs-1.02.77-9.el6.x86_64

###multipath configure

DM_Multipath

Device mapper multipathing (DM-Multipath) allows you to configure multiple I/O paths between server nodes and storage arrays into a single device. 
These I/O paths are physical SAN connections that can include separate cables, switches, and controllers. Multipathing aggregates the I/O paths, 
creating a new device that consists of the aggregated paths

enable multipath

	[root@dev1 ~]# mpathconf --enable --with_multipathd y

scan disks

	[root@dev1 ~]# fdisk -l

	WARNING: GPT (GUID Partition Table) detected on '/dev/sda'! The util fdisk doesn't support GPT. Use GNU Parted.


	Disk /dev/sda: 299.0 GB, 298999349248 bytes
	255 heads, 63 sectors/track, 36351 cylinders
	Units = cylinders of 16065 * 512 = 8225280 bytes
	Sector size (logical/physical): 512 bytes / 4096 bytes
	I/O size (minimum/optimal): 4096 bytes / 4096 bytes
	Disk identifier: 0x00000000

	   Device Boot      Start         End      Blocks   Id  System
	/dev/sda1               1       36352   291991551+  ee  GPT
	Partition 1 does not start on physical sector boundary.

	Disk /dev/sdb: 107.4 GB, 107374182400 bytes
	255 heads, 63 sectors/track, 13054 cylinders
	Units = cylinders of 16065 * 512 = 8225280 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 512 bytes / 512 bytes
	Disk identifier: 0x00000000


	Disk /dev/sdc: 107.4 GB, 107374182400 bytes
	255 heads, 63 sectors/track, 13054 cylinders
	Units = cylinders of 16065 * 512 = 8225280 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 512 bytes / 512 bytes
	Disk identifier: 0x00000000


	Disk /dev/mapper/mpathc: 107.4 GB, 107374182400 bytes
	255 heads, 63 sectors/track, 13054 cylinders
	Units = cylinders of 16065 * 512 = 8225280 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 512 bytes / 512 bytes
	Disk identifier: 0x00000000

note:sdb,sdc and /dev/mapper/mpathc are new devices added.

show multipath topology

	[root@dev1 ~]# multipath -l
	mpathc (36005076802810b121800000000000002) dm-0 IBM,2145
	size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw
	|-+- policy='round-robin 0' prio=0 status=active
	| `- 4:0:0:0 sdc 8:32 active undef running
	`-+- policy='round-robin 0' prio=0 status=enabled
	  `- 3:0:0:0 sdb 8:16 active undef running

note:multipathing aggregates the sdb,sdc, and creates a new device mpathc that consists of the aggregated paths.

v7000 volumes detail
operation: V7000 -> Cluster -> Volumes -> Volumes --> Volume Details:volume01

!![volume01 details](http://dylanninin.com/assets/images/2013/volume01_details.png)

chkconfig 

	[root@dev1 ~]# chkconfig multipathd on

###fdisk and makefs

fdisk
	
	[root@dev1 ~]# fdisk /dev/mapper/mpathc 


makefs	

	[root@dev1 ~]# mkfs.ext4 /dev/mapper/mpathc 


mount
	
	[root@dev1 ~]# mkdir /v7000 
	[root@dev1 ~]# mount /dev/mapper/mpathc /v7000/
	
	[root@dev1 ~]# df -h
	Filesystem            Size  Used Avail Use% Mounted on
	/dev/sda4             179G  3.0G  167G   2% /
	tmpfs                  32G   88K   32G   1% /dev/shm
	/dev/sda2             485M   55M  405M  12% /boot
	/dev/sda1             200M  260K  200M   1% /boot/efi
	/dev/mapper/mpathc     99G  188M   94G   1% /v7000

dd test
	
	[root@dev1 v7000]# time dd if=/dev/mapper/mpathc of=/dev/null bs=8k
	13107200+0 records in
	13107200+0 records out
	107374182400 bytes (107 GB) copied, 270.831 s, 396 MB/s
	
	real	4m30.834s
	user	0m2.224s
	sys	2m43.163s

##Reference

* [IBM V7000 Introduction and Implementation Guide](http://www.redbooks.ibm.com/redpieces/abstracts/sg248068.html?Open&pdfbookmark)
* [DM Multipath on Redhat](https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/DM_Multipath/MPIO_Overview.html)

---
layout: post
title: CPU Temperature
category : Linux
tags : [Linux, Utilities]
---

##Detect CPU Temperature

os kernel

	[root@dev ~]# uname -a
	Linux dev.egolife.com 2.6.32-358.el6.x86_64 #1 SMP Fri Feb 22 13:35:02 PST 2013 x86_64 x86_64 x86_64 GNU/Linux

install lm_sensors

	[root@dev ~]# yum install -y lm_sensors
	Loaded plugins: refresh-packagekit, security
	Setting up Install Process
	Resolving Dependencies
	--> Running transaction check
	---> Package lm_sensors.x86_64 0:3.1.1-17.el6 will be installed
	--> Finished Dependency Resolution
	
	Dependencies Resolved
	
	========================================================================================================================================
	 Package                           Arch                          Version                              Repository                   Size
	========================================================================================================================================
	Installing:
	 lm_sensors                        x86_64                        3.1.1-17.el6                         base                        123 k
	
	Transaction Summary
	========================================================================================================================================
	Install       1 Package(s)
	
	Total download size: 123 k
	Installed size: 350 k
	Downloading Packages:
	lm_sensors-3.1.1-17.el6.x86_64.rpm                                                                               | 123 kB     00:00     
	Running rpm_check_debug
	Running Transaction Test
	Transaction Test Succeeded
	Running Transaction
	  Installing : lm_sensors-3.1.1-17.el6.x86_64                                                                                       1/1 
	  Verifying  : lm_sensors-3.1.1-17.el6.x86_64                                                                                       1/1 
	
	Installed:
	  lm_sensors.x86_64 0:3.1.1-17.el6                                                                                                      
	
	Complete!

detect sensors

	[root@dev ~]# sensors-detect 
	# sensors-detect revision 1.1
	# System: IBM System x3650 M4 -[7915I31]-
	# Board: IBM 00J6520
	
	This program will help you determine which kernel modules you need
	to load to use lm_sensors most effectively. It is generally safe
	and recommended to accept the default answers to all questions,
	unless you know what you're doing.
	
	Some south bridges, CPUs or memory controllers contain embedded sensors.
	Do you want to scan for them? This is totally safe. (YES/no): Y
	Silicon Integrated Systems SIS5595...                       No
	VIA VT82C686 Integrated Sensors...                          No
	VIA VT8231 Integrated Sensors...                            No
	AMD K8 thermal sensors...                                   No
	AMD Family 11h thermal sensors...                           No
	Intel digital thermal sensor...                             Success!
	    (driver `coretemp')
	Intel AMB FB-DIMM thermal sensor...                         No
	VIA C7 thermal and voltage sensors...                       No
	
	Some Super I/O chips contain embedded sensors. We have to write to
	standard I/O ports to probe them. This is usually safe.
	Do you want to scan for Super I/O sensors? (YES/no): Y
	Probing for Super-I/O at 0x2e/0x2f
	Trying family `National Semiconductor'...                   Yes
	Found unknown chip with ID 0x3711
	Probing for Super-I/O at 0x4e/0x4f
	Trying family `National Semiconductor'...                   No
	Trying family `SMSC'...                                     No
	Trying family `VIA/Winbond/Nuvoton/Fintek'...               No
	Trying family `ITE'...                                      No
	
	Some systems (mainly servers) implement IPMI, a set of common interfaces
	through which system health data may be retrieved, amongst other things.
	We first try to get the information from SMBIOS. If we don't find it
	there, we have to read from arbitrary I/O ports to probe for such
	interfaces. This is normally safe. Do you want to scan for IPMI
	interfaces? (YES/no): Y
	Found `IPMI BMC KCS' at 0xcc0...                            Success!
	    (confidence 8, driver `ipmisensors')
	
	Some hardware monitoring chips are accessible through the ISA I/O ports.
	We have to write to arbitrary I/O ports to probe them. This is usually
	safe though. Yes, you do have ISA I/O ports even if you do not have any
	ISA slots! Do you want to scan the ISA I/O ports? (YES/no): Y
	Probing for `National Semiconductor LM78' at 0x290...       No
	Probing for `National Semiconductor LM79' at 0x290...       No
	Probing for `Winbond W83781D' at 0x290...                   No
	Probing for `Winbond W83782D' at 0x290...                   No
	
	Lastly, we can probe the I2C/SMBus adapters for connected hardware
	monitoring devices. This is the most risky part, and while it works
	reasonably well on most systems, it has been reported to cause trouble
	on some systems.
	Do you want to probe the I2C/SMBus adapters now? (YES/no): Y
	Found unknown SMBus adapter 8086:1d22 at 0000:00:1f.3.
	Sorry, no supported PCI bus adapters found.
	Module i2c-dev loaded successfully.
	
	Next adapter: SMBus I801 adapter at 4000 (i2c-0)
	Do you want to scan it? (YES/no/selectively): Y
	Client found at address 0x48
	Probing for `National Semiconductor LM75'...                No
	Probing for `Dallas Semiconductor DS75'...                  No
	Probing for `National Semiconductor LM77'...                No
	Probing for `Dallas Semiconductor DS1621/DS1631'...         No
	Probing for `Maxim MAX6650/MAX6651'...                      No
	Probing for `National Semiconductor LM73'...                No
	Probing for `National Semiconductor LM92'...                No
	Probing for `National Semiconductor LM76'...                No
	Probing for `Maxim MAX6633/MAX6634/MAX6635'...              No
	
	Now follows a summary of the probes I have just done.
	Just press ENTER to continue: 
	
	Driver `coretemp':
	  * Chip `Intel digital thermal sensor' (confidence: 9)
	
	Driver `ipmisensors':
	  * ISA bus, address 0xcc0
	    Chip `IPMI BMC KCS' (confidence: 8)
	
	Warning: the required module ipmisensors is not currently installed
	on your system. If it is built into the kernel then it's OK.
	Otherwise, check http://www.lm-sensors.org/wiki/Devices for
	driver availability.
	
	Do you want to overwrite /etc/sysconfig/lm_sensors? (YES/no): Y
	Starting lm_sensors: loading module ipmi-si coretemp       [  OK  ]
	Unloading i2c-dev... OK

note: yes|sensors-detect

get current cup temperature

	[root@dev ~]# sensors
	coretemp-isa-0000
	Adapter: ISA adapter
	Physical id 0: +39.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 0:        +35.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 1:        +36.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 2:        +36.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 3:        +39.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 4:        +33.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 5:        +38.0°C  (high = +81.0°C, crit = +91.0°C) 

##Reference

* [Linux下查看CPU温度](http://www.linuxsong.org/2010/09/linux-look-cpu-temperature/)
---
layout: post
title: Configure Sendmail, Dovecot  for EBS
category : Oracle
tags : [Oracle, DBA, EBS, Mail, Utilities]
---

##Workflow Notification Mailer

Operation: System Administrator -> Oracle Application Manager: Workflow -> Notification Mailers -> Edit

Basic Configuration:

Details:
	Name: Workflow Notification Mailer
	
Outbount EMail Account(SMTP):
	Server Name: smtp.egolife.com
	Username:itsection
	Password:secret
	
Inbount Email Account(IMAP):
	check Inbound Processing
	Server Name:dev.egolife.com
	Username:me
	Password:secret
	Reply-To Address: me@dev.egolife.com

##Sendmail Configuration

check sendmail service 

	[root@dev ~]# yum install -y sendmail
	Loaded plugins: refresh-packagekit, security
	Setting up Install Process
	Package sendmail-8.14.4-8.el6.x86_64 already installed and latest version
	Nothing to do

	[root@dev ~]# service sendmail status
	sendmail dead but subsys locked
	sm-client (pid  32048) is running...

	[root@dev ~]# service sendmail stop
	Shutting down sm-client:                                   [  OK  ]
	Shutting down sendmail:                                    [FAILED]

	[root@dev ~]# service sendmail status
	sendmail is stopped
	sm-client is stopped

	[root@dev ~]# service sendmail start
	Starting sendmail:                                         [  OK  ]
	Starting sm-client:                                        [  OK  ]

	[root@dev ~]# chkconfig sendmail on

	[root@dev ~]# less /var/log/maillog
	Jun 18 14:33:45 dev sendmail[32366]: NOQUEUE: SYSERR(root): opendaemonsocket: daemon MTA: cannot bind: Address already in use
	Jun 18 14:33:45 dev sendmail[32366]: daemon MTA: problem creating SMTP socket
	Jun 18 14:33:50 dev sendmail[32366]: NOQUEUE: SYSERR(root): opendaemonsocket: daemon MTA: cannot bind: Address already in use
	Jun 18 14:33:50 dev sendmail[32366]: daemon MTA: problem creating SMTP socket
	Jun 18 14:33:55 dev sendmail[32366]: NOQUEUE: SYSERR(root): opendaemonsocket: daemon MTA: cannot bind: Address already in use
	Jun 18 14:33:55 dev sendmail[32366]: daemon MTA: problem creating SMTP socket
	Jun 18 14:34:00 dev sendmail[32366]: NOQUEUE: SYSERR(root): opendaemonsocket: daemon MTA: cannot bind: Address already in use
	Jun 18 14:34:00 dev sendmail[32366]: daemon MTA: problem creating SMTP socket
	Jun 18 14:34:00 dev sendmail[32366]: NOQUEUE: SYSERR(root): opendaemonsocket: daemon MTA: server SMTP socket wedged: exiting

stop postfix and restart sendmail

	[root@dev ~]# lsof -i:25
	COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
	master  31226 root   12u  IPv4 734327      0t0  TCP *:smtp (LISTEN)
	master  31226 root   13u  IPv6 734329      0t0  TCP *:smtp (LISTEN)

	[root@dev ~]# service postfix status
	master (pid  31226) is running...

	[root@dev ~]# service postfix stop
	Shutting down postfix:                                     [  OK  ]

	[root@dev ~]# lsof -i:25

	[root@dev ~]# service sendmail restart
	Shutting down sm-client:                                   [  OK  ]
	Shutting down sendmail:                                    [FAILED]
	Starting sendmail:                                         [  OK  ]
	Starting sm-client:                                        [  OK  ]

	[root@dev ~]# lsof -i:25

reinstall sendmail packages

	[root@dev ~]# rpm -qa | grep sendmail
	sendmail-cf-8.14.4-8.el6.noarch
	sendmail-8.14.4-8.el6.x86_64

	[root@dev ~]# rpm -e --nodeps sendmail
	[root@dev ~]# rpm -e --nodeps sendmail-cf


	[root@dev ~]# yum install -y sendmail
	[root@dev ~]# yum install -y sendmail-cf

add listen ip

	[root@dev ~]# vim /etc/mail/sendmail.mc
	DAEMON_OPTIONS(`Port=smtp,Addr=127.0.0.1, Name=MTA')dnl
	DAEMON_OPTIONS(`Port=smtp,Addr=192.168.1.6, Name=MTA')dnl

	[root@dev ~]# service sendmail restart
	Shutting down sm-client:                                   [  OK  ]
	Shutting down sendmail:                                    [FAILED]
	Starting sendmail:                                         [  OK  ]
	Starting sm-client:                                        [  OK  ]

	[root@dev ~]# lsof -i:25
	COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
	sendmail 3948 root    4u  IPv4 763157      0t0  TCP localhost:smtp (LISTEN)
	sendmail 3948 root    5u  IPv4 763158      0t0  TCP dev.egolife.com:smtp (LISTEN)
	
sendmail test

	[root@dev ~]# cat /etc/mail/sendmail.mc | mail -s "no-reply:sendmail test from `hostname`" me@egolife.com

	[root@dev ~]# tail /var/log/maillog
	Jun 18 17:34:38 dev sendmail[4201]: r5I9YcUV004201: from=root, size=7516, class=0, nrcpts=1, msgid=<201306180934.r5I9YcUV004201@dev.egolife.com>, relay=root@localhost
	Jun 18 17:34:38 dev sendmail[4202]: r5I9YcOE004202: from=<root@dev.egolife.com>, size=7772, class=0, nrcpts=1, msgid=<201306180934.r5I9YcUV004201@dev.egolife.com>, proto=ESMTP, daemon=MTA, relay=localhost [127.0.0.1]
	Jun 18 17:34:39 dev sendmail[4201]: r5I9YcUV004201: to=me@egolife.com, ctladdr=root (0/0), delay=00:00:01, xdelay=00:00:01, mailer=relay, pri=37516, relay=[127.0.0.1] [127.0.0.1], dsn=2.0.0, stat=Sent (r5I9YcOE004202 Message accepted for delivery)
	Jun 18 17:34:39 dev sendmail[4204]: r5I9YcOE004202: to=<me@egolife.com>, ctladdr=<root@dev.egolife.com> (0/0), delay=00:00:01, xdelay=00:00:00, mailer=esmtp, pri=127772, relay=smtp.egolife.com. [172.29.88.10], dsn=2.0.0, stat=Sent (Message queued)

##Dovecot Configuration

add listen ip and set mail location

	[root@dev ~]# vim /etc/dovecot/dovecot.conf 
	... ...
	listen = 127.0.0.1, 192.168.1.6
	mail_location = maildir:~/mail
	... ...

set auth mechanisms

	[root@dev ~]# vim /etc/dovecot/conf.d/10-auth.conf 
	... ...
	disable_plaintext_auth = no
	... ...
	# NOTE: See also disable_plaintext_auth setting.
	auth_mechanisms = plain login
	... ...
	##
	## Password and user databases
	##
	passdb {
	    driver = shadow
	}
	... ...

restart dovecot

	[root@dev ~]# service dovecot restart
	Stopping Dovecot Imap:                                     [  OK  ]
	Starting Dovecot Imap:                                     [  OK  ]
	[root@dev ~]# chkconfig dovecot on

	[root@dev ~]# lsof -i:143
	COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
	dovecot 5661 root   20u  IPv4 771014      0t0  TCP localhost:imap (LISTEN)
	dovecot 5661 root   21u  IPv4 771015      0t0  TCP dev.egolife.com:imap (LISTEN)
	
view maillog

	[root@dev ~]# tail /var/log/maillog
	Jun 18 18:16:21 dev dovecot: imap-login: Disconnected (no auth attempts): rip=192.168.1.6, lip=192.168.1.6, secured
	Jun 18 18:16:22 dev dovecot: imap-login: Login: user=<me>, method=PLAIN, rip=192.168.1.6, lip=192.168.1.6, mpid=5768, secured
	Jun 18 18:16:24 dev dovecot: imap-login: Disconnected (no auth attempts): rip=192.168.1.6, lip=192.168.1.6, secured
	Jun 18 18:16:26 dev dovecot: imap(me): Disconnected: Logged out bytes=76/773

##Test Mailer in EBS App Node

Operation: System Administrator -> Oracle Application Manager: Workflow -> Notification Mailers -> Edit -> Test Mailer

WF Mailer NOTIFICATIONS

Workflow Mailer Perference Settings

	SQL> SELECT NAME, DISPLAY_NAME, NOTIFICATION_PREFERENCE, EMAIL_ADDRESS, STATUS FROM WF_LOCAL_ROLES WHERE NAME = UPPER('&fnd_user_name');
	Enter value for fnd_user_name: me
	old   1: SELECT NAME, DISPLAY_NAME, NOTIFICATION_PREFERENCE, EMAIL_ADDRESS, STATUS FROM WF_LOCAL_ROLES WHERE NAME = UPPER('&fnd_user_name')
	new   1: SELECT NAME, DISPLAY_NAME, NOTIFICATION_PREFERENCE, EMAIL_ADDRESS, STATUS FROM WF_LOCAL_ROLES WHERE NAME = UPPER('me')

	NAME	   DISPLAY_NA NOTIFICATION_PREFERE EMAIL_ADDRESS		  STATUS
	---------- ---------- -------------------- ------------------------------ --------------------
	me    		我自己,    MAILHTML             me@egolife.com        ACTIVE

	SQL> SELECT NAME, DISPLAY_NAME, NOTIFICATION_PREFERENCE, EMAIL_ADDRESS, STATUS FROM WF_LOCAL_ROLES WHERE NAME = UPPER('&fnd_user_name');
	Enter value for fnd_user_name: sysadmin
	old   1: SELECT NAME, DISPLAY_NAME, NOTIFICATION_PREFERENCE, EMAIL_ADDRESS, STATUS FROM WF_LOCAL_ROLES WHERE NAME = UPPER('&fnd_user_name')
	new   1: SELECT NAME, DISPLAY_NAME, NOTIFICATION_PREFERENCE, EMAIL_ADDRESS, STATUS FROM WF_LOCAL_ROLES WHERE NAME = UPPER('sysadmin')

	NAME	   DISPLAY_NA NOTIFICATION_PREFERE EMAIL_ADDRESS		  STATUS
	---------- ---------- -------------------- ------------------------------ --------------------
	SYSADMIN   SYSADMIN   MAILHTML						  ACTIVE

WF Mailer Notification Status

	SQL> SELECT NOTIFICATION_ID, MESSAGE_NAME, RECIPIENT_ROLE, MAIL_STATUS FROM WF_NOTIFICATIONS WFN WHERE WFN.SENT_DATE > SYSDATE - 1;
	
	... ...
	
##Reference

* [sendmail](http://www.sendmail.com/)
* [dovecot](http://www.dovecot.org/)

---
layout: post
title: DM Multipath Introduction
category : Linux
tags : [Linux, Storage]
---

##DM Multipath介绍

###1.1概念

设备映射器多路径（DM-Multipath）可让您将服务器节点和存储阵列间的多个 I/O 路径配置为一个单一设备。这些 I/O 路径是可包含独立电缆、交换机以及控制器的物理 SAN 连接。多路径集合了 I/O 路径，并生成由这些整合路径组成的新设备。

###1.2概述

可使用 DM-Multipath 提供：

* 冗余
DM-Multipath 可在主动/被动配置中提供出错冗余。在主动/被动配置中，只有一半的路径在每次 I/O 时都使用。如果 I/O 路径的任意元素（电缆、交换机或者控制器）出现故障，就会将 DM-Multipath 切换到备用路径。

* 改进的性能
可将 DM-Multipath 配置为主动/主动模式，其中将 I/O 以轮叫调度算法方式分布到所有路径中。在有些配置中，DM-Multipath 可在 I/O 路径中检测负载并动态重新平衡负载。

“带一个 RAID 设备的主动/被动多路径配置” 演示在服务器和 RAID 设备之间有两个 I/O 路径的主动/被动配置。这里服务器中有两个 HBA，两个 SAN 交换机以及两个 RAID 控制器。

!![multipath-server](http://dylanninin.com/assets/images/2013/multipath-server1.png)

###1.3存储阵列支持

默认情况下，DM-Multipath 支持大多数常用的、支持 DM-Multipath 的存储阵列。您可在 multipath.conf.defaults 文件中找到这些支持的设备。如果您的存储阵列支持 DM-Multipath 且未在这个文件中默认设置，可能需要将其添加到 DM-Multipath 配置文件 multipath.conf 中。

###1.4组件。


	----------------------  ------------------------------------------------
	组件	 					描述	
	---------------------- -------------------------------------------------
	dm-multipath 内核模块	 为路径和路径组群重新指定 I/O 并支持出错冗余。
	mpathconf 程序	 	 配置并启用设备映射器多路径
	multipath 命令		 列出并配置 multipath 设备。通常使用 /etc/rc.sysinit 启动，还可以在添加块设备时使用 udev 程序启动。
	multipathd 守护进程	   监视器路径，如果路径故障并返回，它可能会启动路径组群切换。可为多路径设备提供互动修改。对 /etc/multipath.conf 文件的任何修改都必须启动它。
	kpartx 命令	 		 为设备中的分区生成设备映射器设备。这个命令对带 DM-MP 的 DOS 分区是很必要的。kpartx 在其自身软件包中就存在，但 device-mapper-multipath 软件包要依赖它

###1.5设置概述

DM-Multipath 包含适用于常见多路径配置已编译的默认设置。安装 DM-multipath 通常很简单。

以下是为您的系统配置 DM-multipath 的基本步骤：

* 安装 device-mapper-multipath rpm。
* 使用 mpathconf 命令创建配置文件并启用多路径。如果您不需要编辑该配置文件，您还可以使用这个命令启动多路径守护进程。
* 如需要，请编辑 multipath.conf 配置文件，修改默认值并保存更新的文件：
* 启动多路径守护进程

##Reference

* [Configuration and Administration zh ](https://access.redhat.com/site/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html-single/DM_Multipath/)
* [Configuration and Administration en ](https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/DM_Multipath/index.html)

---
layout: post
title: DM Multipath Configuration on Linux
category : Linux
tags : [Linux, Storage]
---

##unique scsi identifiers

list all scsi devices

	[root@devdb ~]# cat /proc/partitions 
	major minor  #blocks  name

	   8        0  291991552 sda
	   8        1     204800 sda1
	   8        2     512000 sda2
	   8        3   20971520 sda3
	   8        4  270301184 sda4
	   8       16    5242880 sdb
	   8       64  104857600 sde
	   8       32  104857600 sdc
	   8       80  104857600 sdf
	   8      112  104857600 sdh
	   8       48  104857600 sdd
	   8      128  157286400 sdi
	   8      144  209715200 sdj
	   8      160  419430400 sdk
	   8       96  104857600 sdg
	   8      176    5242880 sdl
	   8      208  104857600 sdn
	   8      224  104857600 sdo
	   8      240  104857600 sdp
	   8      192  104857600 sdm
	  65        0  104857600 sdq
	  65       16  104857600 sdr
	  65       32  157286400 sds
	  65       48  209715200 sdt
	  65       64  419430400 sdu
	 252        0   33554432 dm-0
	 252        1    5242880 dm-1
	 252        2  104857600 dm-2
	 252        3  104857600 dm-3
	 252        4  104857600 dm-4
	 252        5  104857600 dm-5
	 252        6  104857600 dm-6
	 252        7  157286400 dm-7
	 252        8  209715200 dm-8
	 252        9  419430400 dm-9
	 252       10  104857600 dm-10
	 252       11          1 dm-11
	 252       12  209712447 dm-12
	 252       13  122880000 dm-13

obtain clusterware device unique scsi identifiers

	[root@devdb ~]# for i in `ls /dev/sd[a-z]` ; do   echo $i "`/sbin/scsi_id i --whitelisted --device=$i`"; done | sort -k2
	/dev/sdc 36005076802810b121800000000000005
	/dev/sdm 36005076802810b121800000000000005
	/dev/sdd 36005076802810b121800000000000006
	/dev/sdn 36005076802810b121800000000000006
	/dev/sde 36005076802810b121800000000000007
	/dev/sdo 36005076802810b121800000000000007
	/dev/sdf 36005076802810b121800000000000008
	/dev/sdp 36005076802810b121800000000000008
	/dev/sdg 36005076802810b121800000000000009
	/dev/sdq 36005076802810b121800000000000009
	/dev/sdh 36005076802810b12180000000000000a
	/dev/sdr 36005076802810b12180000000000000a
	/dev/sdb 36005076802810b12180000000000000b
	/dev/sdl 36005076802810b12180000000000000b
	/dev/sdi 36005076802810b12180000000000000c
	/dev/sds 36005076802810b12180000000000000c
	/dev/sdj 36005076802810b12180000000000000e
	/dev/sdt 36005076802810b12180000000000000e
	/dev/sdk 36005076802810b12180000000000000f
	/dev/sdu 36005076802810b12180000000000000f
	/dev/sda 3600605b0061b8b301944b4a80d86a2f9

##configure multipathing

enable the multipath configuration file and starts the multipathd daemon

	[root@devdb ~]# mpathconf --enable --with_multipathd y

	[root@devdb ~]# tree -f /etc/multipath*
	/etc/multipath.conf
	/etc/multipath
	├── /etc/multipath/bindings  
	└── /etc/multipath/wwids

multipath.conf 

	[root@devdb ~]# cat /etc/multipath.conf 
	# This is a basic configuration file with some examples, for device mapper
	# multipath.
	# For a complete list of the default configuration values, see
	# /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf.defaults
	# For a list of configuration options with descriptions, see
	# /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf.annotated
	#
	# REMEMBER: After updating multipath.conf, you must run
	#
	# service multipathd reload
	#
	# for the changes to take effect in multipathd
	... ...
	blacklist {
		wwid  3600605b0061b8b301944b4a80d86a2f9
	}

wwids

	[root@devdb ~]# cat /etc/multipath/wwids 
	# Multipath wwids, Version : 1.0
	# NOTE: This file is automatically maintained by multipath and multipathd.
	# You should not need to edit this file in normal circumstances.
	#
	# Valid WWIDs:
	/36005076802810b121800000000000005/

bindings

	[root@devdb ~]# cat /etc/multipath/bindings 
	# Multipath bindings, Version : 1.0
	# NOTE: this file is automatically maintained by the multipath program.
	# You should not need to edit this file in normal circumstances.
	#
	# Format:
	# alias wwid
	#
	mpatha 3600605b0061b8b301944b4a80d86a2f9
	mpathb SIBM     2145           0200a042c486XX25
	mpathc 36005076802810b121800000000000005

multipath -v3

	[root@devdb ~]# multipath -v3
	... ...
	===== paths list =====
	uuid                              hcil    dev dev_t pri dm_st chk_st vend/prod
	3600605b0061b8b301944b4a80d86a2f9 0:2:0:0 sda 8:0   1   undef ready  IBM,Serve
	36005076802810b121800000000000005 3:0:0:0 sdb 8:16  50  undef ready  IBM,2145 
	36005076802810b121800000000000005 4:0:0:0 sdc 8:32  10  undef ready  IBM,2145 
	... ...

##alias

alias settings

	[root@devdb ~] vim /etc/multipath.conf
	... ...
	defaults {
		user_friendly_names yes
	}
	... ...
	multipaths {
		multipath {
			wwid			36005076802810b121800000000000005
			alias			data
		}
	}
	... ...

stop device mapping

	[root@devdb ~] service multipathd stop

flush device mappings

	[root@devdb ~] multipath -F

start device mapping

	[root@devdb ~] service multipathd start
	
show multipath topology

	[root@devdb ~]# multipath -ll
	data (36005076802810b121800000000000005) dm-5 IBM,2145
	size=400G features='0' hwhandler='0' wp=rw
	|-+- policy='round-robin 0' prio=50 status=active
	| `- 4:0:0:9 sdd 65:64 active ready running
	`-+- policy='round-robin 0' prio=10 status=enabled
	  `- 3:0:0:9 sdc 8:160 active ready running

show created devices

	[root@devdb ~]# dmsetup ls | sort
	data	(252:9)
	erpdb_asm_crsdg	(252:1)
	erpdb_asm_dg_1	(252:3)
	erpdb_asm_dg_2	(252:6)
	erpdb_asm_dg_3	(252:2)
	erpdb_asm_dg_4	(252:4)
	erpdb_asm_dg_5	(252:10)
	erpdb_asm_fradg	(252:5)
	erpdb_fs_arch_db1	(252:7)
	patch	(252:8)
	patchp1	(252:11)
	patchp5	(252:12)
	vg_devdb-LV_DB	(252:13)
	vg_devdb-LV_SWAP	(252:0)
	
##adminstartion commands

mpathconf

	[root@devdb ~]# mpathconf --help
	usage: /sbin/mpathconf <command>

	Commands:
	Enable: --enable 
	Disable: --disable
	Set user_friendly_names (Default n): --user_friendly_names <y|n>
	Set find_multipaths (Default n): --find_multipaths <y|n>
	Load the dm-multipath modules on enable (Default y): --with_module <y|n>
	start/stop/reload multipathd (Default n): --with_multipathd <y|n>
	chkconfig on/off multipathd (Default y): --with_chkconfig <y|n>

multipath

	[root@dev1 etc]# multipath -l
	multipath-tools v0.4.9 (04/04, 2009)
	Usage:
	  multipath [-c] [-d] [-r] [-v lvl] [-p pol] [-b fil] [-q] [dev]
	  multipath -l|-ll|-f [-v lvl] [-b fil] [dev]
	  multipath -F [-v lvl]
	  multipath -h
	
	Where:
	  -h      print this usage text
	  -l      show multipath topology (sysfs and DM info)
	  -ll     show multipath topology (maximum info)
	  -f      flush a multipath device map
	  -F      flush all multipath device maps
	  -c      check if a device should be a path in a multipath device
	  -q      allow queue_if_no_path when multipathd is not running
	  -d      dry run, do not create or update devmaps
	  -r      force devmap reload
	  -p      policy failover|multibus|group_by_serial|group_by_prio
	  -b fil  bindings file location
	  -p pol  force all maps to specified path grouping policy :
	          . failover            one path per priority group
	          . multibus            all paths in one priority group
	          . group_by_serial     one priority group per serial
	          . group_by_prio       one priority group per priority lvl
	          . group_by_node_name  one priority group per target node
	  -v lvl  verbosity level
	          . 0 no output
	          . 1 print created devmap names only
	          . 2 default verbosity
	          . 3 print debug information
	  dev     action limited to:
	          . multipath named 'dev' (ex: mpath0) or
	          . multipath whose wwid is 'dev' (ex: 60051..)
	          . multipath including the path named 'dev' (ex: /dev/sda)
	          . multipath including the path with maj:min 'dev' (ex: 8:0)

multipathd

	[root@devdb ~]# multipathd -k
	multipathd> help
	multipath-tools v0.4.9 (04/04, 2009)
	CLI commands reference:
	 list|show paths
	 list|show paths format $format
	 list|show status
	 list|show daemon
	 list|show maps|multipaths
	 list|show maps|multipaths status
	 list|show maps|multipaths stats
	 list|show maps|multipaths format $format
	 list|show maps|multipaths topology
	 list|show topology
	 list|show map|multipath $map topology
	 list|show config
	 list|show blacklist
	 list|show devices
	 list|show wildcards
	 add path $path
	 remove|del path $path
	 add map|multipath $map
	 remove|del map|multipath $map
	 switch|switchgroup map|multipath $map group $group
	 reconfigure
	 suspend map|multipath $map
	 resume map|multipath $map
	 resize map|multipath $map
	 disablequeueing map|multipath $map
	 restorequeueing map|multipath $map
	 disablequeueing maps|multipaths
	 restorequeueing maps|multipaths
	 reinstate path $path
	 fail path $path
	 paths count
	 forcequeueing daemon
	 restorequeueing daemon
	 quit|exit
	 map|multipath $map getprstatus
	 map|multipath $map setprstatus
	 map|multipath $map unsetprstatus

##full configure

	[root@devdb ~]# grep -v '^#' /etc/multipath.conf
	defaults {
		polling_interval 	30
		path_selector		"round-robin 0"
		path_grouping_policy	multibus
		path_checker		readsector0
		rr_min_io		100
		max_fds			8192
		rr_weight		priorities
		failback		immediate
		no_path_retry		fail
		user_friendly_names	yes
	}
	blacklist {
			wwid 3600605b0061b8b301944b4a80d86a2f9
		devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"
		devnode "^hd[a-z]"
	}
	devices {
		device {
			vendor "IBM"
			product "2145"
			path_grouping_policy group_by_prio
			getuid_callout "/lib/udev/scsi_id --whitelisted --device=/dev/%n"
			path_selector "round-robin 0"
			path_checker tur
			features "1 queue_if_no_path"
			hardware_handler "0"
			prio alua
			failback immediate
			rr_weight uniform
			rr_min_io 1000
			rr_min_io_rq 1
		}
	}

	multipaths {
		multipath {
			wwid			36005076802810b121800000000000005
			alias			erpdb_asm_dg_1
		}
		multipath {
			wwid			36005076802810b121800000000000006
			alias			erpdb_asm_dg_2
		}
		multipath {
			wwid			36005076802810b121800000000000007
			alias			erpdb_asm_dg_3
		}
		multipath {
			wwid			36005076802810b121800000000000008
			alias			erpdb_asm_dg_4
		}
		multipath {
			wwid			36005076802810b121800000000000009
			alias			erpdb_asm_dg_5
		}
		multipath {
			wwid			36005076802810b12180000000000000a
			alias			erpdb_asm_fradg
		}
		multipath {
			wwid			36005076802810b12180000000000000b
			alias			erpdb_asm_crsdg
		}
		multipath {
			wwid			36005076802810b12180000000000000c
			alias			erpdb_fs_arch_db1
		}
		multipath {
			wwid			36005076802810b12180000000000000e
			alias			patch
		}
		multipath {
			wwid			36005076802810b12180000000000000f
			alias			data
		}
	}
---
layout: post
title: ORA-12514
category : Oracle
tags : [Oracle, Database, DBA, Exception]
---

tnsping  dbtest

	[oracle@oradb ~]$ tnsping dbtest

	TNS Ping Utility for Linux: Version 11.2.0.1.0 - Production on 20-JUN-2013 08:43:11

	Copyright (c) 1997, 2009, Oracle.  All rights reserved.

	Used parameter files:
	/db/oracle/product/11.2.0/db_1/network/admin/sqlnet.ora


	Used TNSNAMES adapter to resolve the alias
	Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = oradb.egolife.com)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = DBTEST.oradb.egolife.com)))
	OK (0 msec)

log on database

	[oracle@oradb ~]$ sqlplus  /nolog

	SQL*Plus: Release 11.2.0.1.0 Production on Thu Jun 20 08:43:17 2013

	Copyright (c) 1982, 2009, Oracle.  All rights reserved.

	SQL> conn barcode@dbtest
	Enter password: 
	ERROR:
	ORA-12514: TNS:listener does not currently know of service requested in connect
	descriptor

check names parameter

	SQL> show parameter name

	NAME				     TYPE			 VALUE
	------------------------ --------------- --------
	db_file_name_convert	 string
	db_name					 string			 DBTEST
	db_unique_name			 string			 DBTEST
	global_names			 boolean		 TRUE
	instance_name			 string			 DBTEST
	lock_name_space 		 string
	log_file_name_convert	 string
	service_names			 string			 DBTEST

service name registered is dbtest, and 'DBTEST.oradb.egolife.com' is not registered

	SQL> alter system set service_names = 'DBTEST.oradb.egolife.com' scope=both;

	System altered.

retest

	[oracle@oradb ~]$ sqlplus  /nolog

	SQL*Plus: Release 11.2.0.1.0 Production on Thu Jun 20 08:57:55 2013

	Copyright (c) 1982, 2009, Oracle.  All rights reserved.

	SQL> conn barcode@dbtest
	Enter password: 
	Connected.

check database

	SQL> alter session set NLS_DATE_FORMAT = 'mm-dd-yyyy hh24:mi:ss';

	Session altered.

	SQL> select instance_name,startup_time,status from v$instance;

	INSTANCE_NAME	     STARTUP_TIME		 STATUS
	-------------------- ------------------- ------------------------------------
	DBTEST		         06-19-2013 14:22:48 OPEN

alert log

	[oracle@oradb trace]$ grep ORA- alert_DBTEST.log.2013-06-19  | sort | uniq
	opiodr aborting process unknown ospid (21424) as a result of ORA-1092
	opiodr aborting process unknown ospid (21484) as a result of ORA-1092
	opiodr aborting process unknown ospid (21818) as a result of ORA-1092
	opiodr aborting process unknown ospid (21872) as a result of ORA-1092
	opiodr aborting process unknown ospid (21931) as a result of ORA-1092
	opiodr aborting process unknown ospid (21987) as a result of ORA-1092
	opiodr aborting process unknown ospid (22051) as a result of ORA-1092
	ORA-00020: maximum number of processes 50 exceeded
	ORA-00020: No more process state objects available
	ORA-00202: control file: '/db/oracle/oradata/DBTEST/ctl01DBTEST.ora'
	ORA-00202: control file: '/db/oracle/oradata/DBTEST/ctl02DBTEST.ora'
	ORA-00202: control file: '/db/oracle/oradata/DBTEST/ctl03DBTEST.ora'
	ORA-00210: cannot open the specified control file
	ORA-01565: error in identifying file '/db/oracle/product/11.2.0/db_1/dbs/spfileDBTEST.ora'
	ORA-01565: Unable to open Spfile /db/oracle/product/11.2.0/db_1/dbs/spfileDBTEST.ora.
	ORA-1031 signalled during: create undo tablespace undotbs datafile '/db/oracle/oradata/DBTEST/undotbs02.dbf' size 300M...
	ORA-1092 : opitsk aborting process
	ORA-1092 signalled during: ALTER DATABASE OPEN...
	ORA-1507 signalled during: ALTER DATABASE CLOSE NORMAL...
	ORA-1543 signalled during: CREATE TABLESPACE USERS LOGGING DATAFILE
	ORA-201 signalled during: ALTER DATABASE   MOUNT...
	ORA-205 signalled during: ALTER DATABASE   MOUNT...
	ORA-20 errors will not be written to the alert log for
	ORA-27037: unable to obtain file status
	ORA-27300: OS system dependent operation:open failed with status: 2
	ORA-27301: OS failure message: No such file or directory
	ORA-27302: failure occurred at: sskgmsmr_7
	ORA-30012: undo tablespace 'AUTO' does not exist or of wrong type
	ORA-30012: undo tablespace '/db/oracle/oradata/DBTEST/undo' does not exist or of wrong type
	ORA-30012: undo tablespace 'UNDO' does not exist or of wrong type
	ORA-30012: undo tablespace 'undotbs01.dbf' does not exist or of wrong type
	 the ORA-20 errors.
	WARNING: inbound connection timed out (ORA-3136)

Error Message

	ORA-12514: TNS:listener does not currently know of service requested in connect descriptor
	Cause: The listener received a request to establish a connection to a database or other service. The connect descriptor received by the listener specified a service name for a service (usually a database service) that either has not yet dynamically registered with the listener or has not been statically configured for the listener. This may be a temporary condition such as after the listener has started, but before the database instance has registered with the listener.
	Action:
	- Wait a moment and try to connect a second time.

	- Check which services are currently known by the listener by executing: lsnrctl services <listener name>

	- Check that the SERVICE_NAME parameter in the connect descriptor of the net service name used specifies a service known by the listener.

	- If an easy connect naming connect identifier was used, check that the service name specified is a service known by the listener.

	- Check for an event in the listener.log file.

##Reference

* Oracle Error Messages 11gR2


---
layout: post
title: LogWatch Introduction
category : Linux
tags : [Linux, Utilities]
---

##介绍

在维护Linux服务器时，经常需要查看系统中各种服务的日志，以检查服务器的运行状态。 如登陆历史、邮件、软件安装等日志。系统管理员一个个去检查会十分不方便；且大多时 候，这会是一种被动的检查，即只有在发现系统运行异常时才会想到去查看日志以获取异常 的信息。那么如何主动、集中的分析这些日志，并生产报告，定时发送给管理员就会显得十 分重要。[LogWatch](http://www.logwatch.org/)即提供了这样的功能。

本文即介绍LogWatch的简单使用，更详细的用法请参见[Logwatch -- a syslog analyzer written in Perl](http://www.softpanorama.info/Logs/Log_analysers/logwatch.shtml)。

##安装与配置

###安装

os kernel

	[root@tplat1 ~]# uname -a
	Linux tplat1.egolife.net 2.6.32-358.el6.x86_64 #1 SMP Fri Feb 22 13:35:02 PST 2013 x86_64 x86_64 x86_64 GNU/Linux

yum install

	[root@tplat1 ~]# yum install -y logwatch
	... ...
	================================================================
	Installing:
	 logwatch            noarch   7.3.6-49.el6     base       297 k
	Installing for dependencies:
	 perl-Date-Manip     noarch   6.24-1.el6       base       1.3 M
	 perl-YAML-Syck      x86_64   1.07-4.el6       base       75 k
	... ...

在安装logwatch时，会同时安装依赖包perl-Date-Manip和perl-YAML-Syck。

###初次使用

命令帮助

	[root@tplat1 ~]# logwatch --help
	
	Usage: /usr/sbin/logwatch [--detail <level>] [--logfile <name>]
	   [--print] [--mailto <addr>] [--archives] [--range <range>] [--debug <level>]
	   [--save <filename>] [--help] [--version] [--service <name>]
	   [--numeric] [--output <output_type>]
	   [--splithosts] [--multiemail] [--no-oldfiles-log]
	
	--detail <level>: Report Detail Level - High, Med, Low or any #.
	--logfile <name>: *Name of a logfile definition to report on.
	--logdir <name>: Name of default directory where logs are stored.
	--service <name>: *Name of a service definition to report on.
	--print: Display report to stdout.
	--mailto <addr>: Mail report to <addr>.
	--archives: Use archived log files too.
	--save <filename>: Save to <filename>.
	--range <range>: Date range: Yesterday, Today, All, Help
	                             where help will describe additional options
	--numeric: Display addresses numerically rather than symbolically and numerically
	           (saves  a  nameserver address-to-name lookup).
	--debug <level>: Debug Level - High, Med, Low or any #.
	--splithosts: Create a report for each host in syslog.
	--multiemail: Send each host report in a separate email.  Ignored if 
	              not using --splithosts.
	--output <output type>: Report Format - mail, html or unformatted#.
	--encode: Use base64 encoding on output mail.
	--no-oldfiles-log: Suppress the logwatch log, which informs about the
	                   old files in logwatch tmpdir.
	--version: Displays current version.
	--help: This message.
	* = Switch can be specified multiple times...

从以上帮助，可以看出，LogWatch整个原理就是，LogWatch 首先要知道针对哪一个服务, 从这个服务中得到需要处理的 Log 文件信息, 然后这个文件送给过滤脚本处理, 之后把处 理后格式化的信息展现出。

显示SSH登陆历史

	[root@tplat1 ~]# logwatch --service sshd --print

###配置

查看logwatch package的主要文件

	[root@tplat1 ~]# rpm -ql logwatch
	/etc/cron.daily/0logwatch					
	/etc/logwatch								
	/etc/logwatch/conf
	/etc/logwatch/conf/ignore.conf
	/etc/logwatch/conf/logfiles
	/etc/logwatch/conf/logwatch.conf
	/etc/logwatch/conf/override.conf
	/etc/logwatch/conf/services
	/etc/logwatch/scripts
	/etc/logwatch/scripts/services
	/usr/sbin/logwatch
	/usr/share/doc/logwatch-7.3.6
	... ...
	/var/cache/logwatch

从以上输出，可以看出logwatch是以cron job的方式定时运行的，默认在`/etc/cron.daily`
目录下，即每天运行一次。

	[root@tplat1 ~]# cat /etc/cron.daily/0logwatch 
	#!/bin/bash
	
	DailyReport=`grep -e "^[[:space:]]*DailyReport[[:space:]]*=[[:space:]]*" /usr/share/logwatch/default.conf/logwatch.conf | head -n1 | sed -e "s|^\s*DailyReport\s*=\s*||"`
	
	if [ "$DailyReport" != "No" ] && [ "$DailyReport" != "no" ]
	then
	    logwatch
	fi
	[root@tplat1 ~]# 

主要配置文件

	[root@tplat1 ~]# tree /etc/logwatch/
	/etc/logwatch/
	├── conf
	│   ├── ignore.conf
	│   ├── logfiles
	│   ├── logwatch.conf		
	│   ├── override.conf
	│   └── services
	└── scripts
	    └── services
	
	5 directories, 3 files

* logwatch.conf	自定义LogWatch主配置，如报告分析时间，级别，收件人等，默认设置在`/usr/share/logwatch/default.conf/logwatch.conf`文件中
* ignore.conf 过滤配置，定义正则表达式，过滤输出报告内容
* override.conf 覆盖或者重写配置，针对`/etc/logwatch/conf/services`下自定义的服务
* conf/services 自定义需分析日志的Service目录，默认支持的Service在`/usr/share/logwatch/default.conf/services`下。
* logfiles 定义待分析服务的日志路径，默认配置在`/usr/share/logwatch/default.conf/logfiles/`下。
* scripts/services 定义Service的可执行脚本。

邮件通知

	[root@tplat1 ~]# less /usr/share/logwatch/default.conf/logwatch.conf
	# Default person to mail reports to.  Can be a local account or a
	# complete email address.  Variable Print should be set to No to
	# enable mail feature.
	MailTo = root

LogWatch默认将分析的日志报告发送给本机的root用户，此时要查看则需登陆到服务器上， 使用`mail`指令查看。

另外，也可以将报告发送到外部邮箱，如`sa.logwatch@gmail.com`，此时需在服务器上配 置简单的邮件服务，如Postfix,Sendmail，编辑`/etc/logwatch/conf/logwatch.conf`覆盖 `MailTo`配置，或者在`/etc/aliase`中定义账户别名，使`root`为`sa.logwatch@gmail.com` 别名，则LogWatch会将日志报告发送给`sa.logwatch@gmail.com`，这样不用登陆到服务器 就可以查看日志报告了。

##小结

LogWatch安装后，基本不用配置即可使用，即可达到主动、集中的分析系统日志，并生产 报告，定时发送给管理员的目的。

更详细的用法请参见[Logwatch -- a syslog analyzer written in Perl](http://www.softpanorama.info/Logs/Log_analysers/logwatch.shtml)。

##Reference

* [用Logwatch工具监控Linux系统Log日志](http://dbanotes.net/opensource/logwatch_linux_log.html)
* [Logwatch -- a syslog analyzer written in Perl](http://www.softpanorama.info/Logs/Log_analysers/logwatch.shtml)

---
layout: post
title: NIC Channel Bond on Linux
category : Linux
tags : [Linux, Network]
---

##Binding Configure

1.服务器

	dev1 为服务端，ip为 192.168.1.7，计划使用eth2,eth3聚合绑定成虚拟网卡bond0。
	dev2 为客户端，ip为192.168.1.10。
	若无指明，则传输文件测试是从 dev1 传输到 dev2，传输为单文件，大小为9.7G。

2.网络结构

    dev1，dev2的网卡均连接到同一个物理交换机。

3.测试要点

   a.带宽：主要测试双网卡绑定时，带宽是否有增加，传输是否更快；主要使用time查看时间，scp带实时的传输速率显示。
   b.冗余：主要测试双网卡绑定时，其中一个网卡工作异常，是否会走另外一个网卡；主要使用netstat查看数据包。

4.测试点

主要分为单网卡、网卡绑定（分模式0，1,6，以及在6下禁用其中一个网卡）的测试：
  
  * a.单网卡测试
  * b.模式0绑定：负载均衡方式，两块网卡都工作，需要交换机作支持
  * c.模式1绑定：冗余方式，网卡只有一个工作，一个出问题启用另外的
  * d.模式6绑定：负载均衡方式，两块网卡都工作，不需要交换机作支持
  * e.模式6绑定时禁用一个网卡
  * f.模式6绑定时禁用一个网卡，从客户端到服务端传输
  * 注：a,b,c,d,e均是从服务端到客户端传输文件。
  
5. mode

Allows you to specify the bonding policy. The 'value' can be one of:

* balance-rr or 0 — Sets a round-robin policy for fault tolerance and load balancing. Transmissions are received and sent out sequentially on each bonded slave interface beginning with the first one available.
* active-backup or 1 — Sets an active-backup policy for fault tolerance. Transmissions are received and sent out via the first available bonded slave interface. Another bonded slave interface is only used if the active bonded slave interface fails.
* balance-xor or 2 — Sets an XOR (exclusive-or) policy for fault tolerance and load balancing. Using this method, the interface matches up the incoming request's MAC address with the MAC address for one of the slave NICs. Once this link is established, transmissions are sent out sequentially beginning with the first available interface.
* broadcast or 3 — Sets a broadcast policy for fault tolerance. All transmissions are sent on all slave interfaces.
* `802.3ad` or 4 — Sets an IEEE 802.3ad dynamic link aggregation policy. Creates aggregation groups that share the same speed and duplex settings. Transmits and receives on all slaves in the active aggregator. Requires a switch that is 802.3ad compliant.
* balance-tlb or 5 — Sets a Transmit Load Balancing (TLB) policy for fault tolerance and load balancing. The outgoing traffic is distributed according to the current load on each slave interface. Incoming traffic is received by the current slave. If the receiving slave fails, another slave takes over the MAC address of the failed slave.
* balance-alb or 6 — Sets an Active Load Balancing (ALB) policy for fault tolerance and load balancing. Includes transmit and receive load balancing for IPV4 traffic. Receive load balancing is achieved through ARP negotiation.

6.测试结论

 * a.带宽：单网卡和双网卡绑定时相比，带宽并无明显差异。
 * b.冗余：双网卡绑定有冗余效果，正在传输时，其中一个网卡工作异常，会自动走另外一个网卡，暂且没有出现数据包丢失的现象。

Network Interface

	[root@dev1 etc]# uname -a
	Linux dev1.egolife.com 2.6.39-400.17.1.el6uek.x86_64 #1 SMP Fri Feb 22 18:16:18 PST 2013 x86_64 x86_64 x86_64 GNU/Linux

	[root@dev1 etc]# lspci | grep Ethernet
	04:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
	04:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
	0e:00.0 Ethernet controller: Intel Corporation 82580 Gigabit Network Connection (rev 01)
	0e:00.1 Ethernet controller: Intel Corporation 82580 Gigabit Network Connection (rev 01)

##Single Interface

ifconfig

	[root@dev1 ~]# ifconfig -a
	eth0      Link encap:Ethernet  HWaddr 6C:AE:8B:78:4C:44  
			  inet addr:172.29.73.7  Bcast:172.29.73.255  Mask:255.255.255.0
			  inet6 addr: fe80::6eae:8bff:fe78:4c44/64 Scope:Link
			  UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
			  RX packets:8679827 errors:0 dropped:0 overruns:0 frame:0
			  TX packets:11695363 errors:0 dropped:0 overruns:0 carrier:0
			  collisions:0 txqueuelen:1000 
			  RX bytes:10981189708 (10.2 GiB)  TX bytes:11622829842 (10.8 GiB)

	eth1      Link encap:Ethernet  HWaddr 6C:AE:8B:78:4C:46  
			  BROADCAST MULTICAST  MTU:1500  Metric:1
			  RX packets:0 errors:0 dropped:0 overruns:0 frame:0
			  TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
			  collisions:0 txqueuelen:1000 
			  RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)

	eth2      Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
			  BROADCAST MULTICAST  MTU:1500  Metric:1
			  RX packets:0 errors:0 dropped:0 overruns:0 frame:0
			  TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
			  collisions:0 txqueuelen:1000 
			  RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)

	eth3      Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CB  
			  BROADCAST MULTICAST  MTU:1500  Metric:1
			  RX packets:0 errors:0 dropped:0 overruns:0 frame:0
			  TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
			  collisions:0 txqueuelen:1000 
			  RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)

	lo        Link encap:Local Loopback  
			  inet addr:127.0.0.1  Mask:255.0.0.0
			  inet6 addr: ::1/128 Scope:Host
			  UP LOOPBACK RUNNING  MTU:16436  Metric:1
			  RX packets:16 errors:0 dropped:0 overruns:0 frame:0
			  TX packets:16 errors:0 dropped:0 overruns:0 carrier:0
			  collisions:0 txqueuelen:0 
			  RX bytes:960 (960.0 b)  TX bytes:960 (960.0 b)

route

	[root@dev1 ~]# route 
	Kernel IP routing table
	Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
	default         172.29.73.1     0.0.0.0         UG    0      0        0 eth0
	link-local      *               255.255.0.0     U     1002   0        0 eth0
	link-local      *               255.255.0.0     U     1007   0        0 eth1
	172.29.73.0     *               255.255.255.0   U     0      0        0 eth0
	192.168.1.0     *               255.255.255.0   U     0      0        0 eth1

ping

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	^C
	--- 192.168.1.10 ping statistics ---
	43 packets transmitted, 0 received, 100% packet loss, time 42421ms

eth2

	[root@dev1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth2
	DEVICE=eth2
	HWADDR=90:E2:BA:43:4F:CA
	TYPE=Ethernet
	UUID=c57d0153-e9d7-42a0-bc6b-4cc3295d4796
	IPADDR=192.168.1.7
	BROADCAST=192.168.1.255
	NETMASK=255.255.255.0
	ONBOOT=no
	NM_CONTROLLED=no
	BOOTPROTO=static
	IPV6INIT=no
	USERCTL=no

	[root@dev1 ~]# ifup eth2

	[root@dev1 ~]# ifconfig eth2
	eth2      Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          inet addr:192.168.1.7  Bcast:192.168.1.255  Mask:255.255.255.0
	          inet6 addr: fe80::92e2:baff:fe43:4fca/64 Scope:Link
	          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
	          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:1000 
	          RX bytes:0 (0.0 b)  TX bytes:412 (412.0 b)
	
	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=1.42 ms
	^C
	--- 192.168.1.10 ping statistics ---
	1 packets transmitted, 1 received, 0% packet loss, time 626ms
	rtt min/avg/max/mdev = 1.423/1.423/1.423/0.000 ms

speed test

	[root@dev1 ~]# ll -h /u2
	total 9.7G
	drwx------. 2 root root  16K Jun 26 15:28 lost+found
	-rw-r--r--. 1 root root 9.7G Jun 26 16:39 oracle_1213_apps.tar.gz

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	The authenticity of host '192.168.1.10 (192.168.1.10)' can't be established.
	RSA key fingerprint is d2:6f:03:40:a0:78:a9:71:1e:c4:6f:73:e4:d8:4f:7b.
	Are you sure you want to continue connecting (yes/no)? yes
	Warning: Permanently added '192.168.1.10' (RSA) to the list of known hosts.
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 110.7MB/s   01:29    
	
	real	1m35.725s
	user	0m58.625s
	sys		0m39.648s

##Configure bond0 with eth2, eth3

###Channel Bonding Configure

bond0

	[root@dev1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-bond0
	DEVICE=bond0
	BOOTPROTO=none
	IPADDR=192.168.1.7
	BROADCAST=192.168.1.255
	NETMASK=255.255.255.0
	ONBOOT=yes
	TYPE=Ethernet
	USERCTL=no
	IPV6INIT=no
	PEERDNS=yes

binding eth2,eth3 to bond0

	[root@dev1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth2
	DEVICE=eth2
	TYPE=Ethernet
	ONBOOT=yes
	BOOTPROTO=no

	[root@dev1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth3
	DEVICE=eth3
	TYPE=Ethernet
	ONBOOT=yes
	BOOTPROTO=no

modprobe with mode 0


	[root@dev1 ~]# cat /etc/modprobe.conf 
	alias eth2 pcnet32
	alias eth3 pcnet32
	alias scsi_hostadapter mptbase
	alias scsi_hostadapter3 mptspi
	alias scsi_hostadapter4 ata_piix
	alias peth0 pcnet32
	alias bond0 bonding
	options bond0 miimon=100 mode=0
	
注：

* miimon 是链路监测的时间间隔单位是毫秒，miimon=100的意思就是，每100毫秒检测网卡和交换机之间是否连通，如不通则使用另外的链路。
* mode:
	mode=0 表示负载均衡方式，两块网卡都工作，需要交换机作支持  
  	mode=1 表示冗余方式，网卡只有一个工作，一个出问题启用另外的
	mode=6 表示负载均衡方式，两块网卡都工作，不需要交换机作支持

enslave

    [root@dev1 ~]# ifenslave bond1 eth2 eth3
    Master 'bond1': Error: handshake with driver failed. Aborting

    [root@dev1 ~]# dmesg | tail
    ADDRCONF(NETDEV_UP): bond0: link is not ready
    Loading kernel module for a network device with CAP_SYS_MODULE (deprecated).  Use CAP_NET_ADMIN and alias netdev-bond1 instead
    ADDRCONF(NETDEV_UP): eth2: link is not ready
    ADDRCONF(NETDEV_UP): eth3: link is not ready
    igb: eth2 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None
    ADDRCONF(NETDEV_CHANGE): eth2: link becomes ready
    igb: eth3 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None
    ADDRCONF(NETDEV_CHANGE): eth3: link becomes ready
    eth2: no IPv6 routers present
    
restart network

	[root@dev1 ~]# service network restart
	Shutting down interface bond0:                             [  OK  ]
	Shutting down interface eth0:                              [  OK  ]
	Shutting down loopback interface:                          [  OK  ]
	Bringing up loopback interface:                            [  OK  ]
	Bringing up interface bond0:                               [  OK  ]
	Bringing up interface eth0:                                [  OK  ]
	Bringing up interface eth2:                                [  OK  ]
	Bringing up interface eth3:                                [  OK  ]

enslave
    
    [root@dev1 ~]# ifenslave bond1 eth2 eth3
    
ping

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	^C
	--- 192.168.1.10 ping statistics ---
	43 packets transmitted, 0 received, 100% packet loss, time 42421ms
    
	[root@dev1 ~]# ifconfig bond0
	bond0     Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          inet addr:192.168.1.7  Bcast:192.168.1.255  Mask:255.255.255.0
	          inet6 addr: fe80::92e2:baff:fe43:4fca/64 Scope:Link
	          UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
	          RX packets:729735 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:7551346 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:0 
	          RX bytes:52531377 (50.0 MiB)  TX bytes:10862582133 (10.1 GiB)

注：
	
	bond0: the mac address is set to the mac address of the first slave nic.

route

	[root@dev1 ~]# route 
	Kernel IP routing table
	Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
	default         172.29.73.1     0.0.0.0         UG    0      0        0 eth0
	link-local      *               255.255.0.0     U     1002   0        0 eth0
	link-local      *               255.255.0.0     U     1004   0        0 eth2
	link-local      *               255.255.0.0     U     1005   0        0 eth3
	link-local      *               255.255.0.0     U     1007   0        0 bond0
	172.29.73.0     *               255.255.255.0   U     0      0        0 eth0
	192.168.1.0     *               255.255.255.0   U     0      0        0 bond0

###mode 0

ping

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=1.69 ms
	64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.287 ms
	^C
	--- 192.168.1.10 ping statistics ---
	2 packets transmitted, 2 received, 0% packet loss, time 1395ms
	rtt min/avg/max/mdev = 0.287/0.992/1.697/0.705 ms

test

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 109.5MB/s   01:30    
	
	real	1m32.922s
	user	0m58.353s
	sys		0m46.081s

ifconfig to view traffic throughout

	[root@dev1 ~]# ifconfig bond0
	bond0     Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          inet addr:192.168.1.7  Bcast:192.168.1.255  Mask:255.255.255.0
	          inet6 addr: fe80::92e2:baff:fe43:4fca/64 Scope:Link
	          UP BROADCAST MASTER MULTICAST  MTU:1500  Metric:1
	          RX packets:3879393 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:14996073 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:0 
	          RX bytes:307412626 (293.1 MiB)  TX bytes:21719128132 (20.2 GiB)

	[root@dev1 ~]# ifconfig eth2
	eth2      Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          inet addr:192.168.1.7  Bcast:192.168.1.255  Mask:255.255.255.0
	          UP BROADCAST RUNNING SLAVE MULTICAST  MTU:1500  Metric:1
	          RX packets:3879390 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:10718053 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:1000 
	          RX bytes:307412446 (293.1 MiB)  TX bytes:15398686700 (14.3 GiB)
	
	[root@dev1 ~]# ifconfig eth3	
	eth3      Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          UP BROADCAST RUNNING SLAVE MULTICAST  MTU:1500  Metric:1
	          RX packets:3 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:4278020 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:1000 
	          RX bytes:180 (180.0 b)  TX bytes:6320441432 (5.8 GiB)
	
bring down eth2, eth3

	[root@dev1 ~]# ifdown eth2
	
	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=1.13 ms
	64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.256 ms
	64 bytes from 192.168.1.10: icmp_seq=3 ttl=64 time=0.236 ms
	^C
	--- 192.168.1.10 ping statistics ---
	3 packets transmitted, 3 received, 0% packet loss, time 2596ms
	rtt min/avg/max/mdev = 0.236/0.542/1.134/0.418 ms


	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 112.0MB/s   01:28    
	
	real	1m31.001s
	user	0m59.142s
	sys		0m30.835s

	[root@dev1 ~]# ifdown eth3

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	^C
	--- 192.168.1.10 ping statistics ---
	7 packets transmitted, 0 received, 100% packet loss, time 6996ms

	[root@dev1 ~]# ifup eth2

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=2005 ms
	64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=1005 ms
	64 bytes from 192.168.1.10: icmp_seq=3 ttl=64 time=5.55 ms
	64 bytes from 192.168.1.10: icmp_seq=4 ttl=64 time=0.283 ms
	^C
	--- 192.168.1.10 ping statistics ---
	4 packets transmitted, 4 received, 0% packet loss, time 3950ms
	rtt min/avg/max/mdev = 0.283/754.118/2005.167/830.199 ms, pipe 3

	[root@dev1 ~]# ifup eth3

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=0.253 ms
	64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.290 ms
	^C
	--- 192.168.1.10 ping statistics ---
	2 packets transmitted, 2 received, 0% packet loss, time 1639ms
	rtt min/avg/max/mdev = 0.253/0.271/0.290/0.024 ms
	
	[root@dev1 ~]# ifconfig 
	bond0     Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          inet addr:192.168.1.7  Bcast:192.168.1.255  Mask:255.255.255.0
	          inet6 addr: fe80::92e2:baff:fe43:4fca/64 Scope:Link
	          UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
	          RX packets:4544721 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:22217942 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:0 
	          RX bytes:355677703 (339.2 MiB)  TX bytes:32559983059 (30.3 GiB)
	
	eth0      Link encap:Ethernet  HWaddr 6C:AE:8B:78:4C:44  
	          inet addr:172.29.73.7  Bcast:172.29.73.255  Mask:255.255.255.0
	          inet6 addr: fe80::6eae:8bff:fe78:4c44/64 Scope:Link
	          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
	          RX packets:854 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:497 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:1000 
	          RX bytes:80825 (78.9 KiB)  TX bytes:76148 (74.3 KiB)
	
	eth2      Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          UP BROADCAST SLAVE MULTICAST  MTU:1500  Metric:1
	          RX packets:3879467 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:10718143 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:1000 
	          RX bytes:307419536 (293.1 MiB)  TX bytes:15398694004 (14.3 GiB)
	
	eth3      Link encap:Ethernet  HWaddr 90:E2:BA:43:4F:CA  
	          UP BROADCAST RUNNING SLAVE MULTICAST  MTU:1500  Metric:1
	          RX packets:665254 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:11499799 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:1000 
	          RX bytes:48258167 (46.0 MiB)  TX bytes:17161289055 (15.9 GiB)
	
	lo        Link encap:Local Loopback  
	          inet addr:127.0.0.1  Mask:255.0.0.0
	          inet6 addr: ::1/128 Scope:Host
	          UP LOOPBACK RUNNING  MTU:16436  Metric:1
	          RX packets:293 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:293 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:0 
	          RX bytes:31960 (31.2 KiB)  TX bytes:31960 (31.2 KiB)

bring down bind0

	[root@dev1 ~]# ifdown bond0

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	^C
	--- 192.168.1.10 ping statistics ---
	3 packets transmitted, 0 received, 100% packet loss, time 2719ms

bring up bond0

	[root@dev1 ~]# ifup bond0

	[root@dev1 ~]# ping 192.168.1.10
	PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
	64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=0.402 ms
	64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.293 ms
	64 bytes from 192.168.1.10: icmp_seq=3 ttl=64 time=0.290 ms
	^C
	--- 192.168.1.10 ping statistics ---
	3 packets transmitted, 3 received, 0% packet loss, time 2490ms
	rtt min/avg/max/mdev = 0.290/0.328/0.402/0.054 ms

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0  4544721      0      0      0 22217942      0      0      0 BMmRU
	eth0       1500   0      995      0      0      0      622      0      0      0 BMRU
	eth2       1500   0  3879467      0      0      0 10718143      0      0      0 BMsRU
	eth3       1500   0   665254      0      0      0 11499799      0      0      0 BMsRU
	lo        16436   0      293      0      0      0      293      0      0      0 LRU

###mode 1

	[root@dev1 ~]# cat /etc/modprobe.conf 
	alias eth2 pcnet32
	alias eth3 pcnet32
	alias scsi_hostadapter mptbase
	alias scsi_hostadapter3 mptspi
	alias scsi_hostadapter4 ata_piix
	alias peth0 pcnet32
	alias bond0 bonding
	options bond0 miimon=100 mode=1

	[root@dev1 ~]# service network restart
	... ...
	
	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 110.7MB/s   01:29    
	
	real	1m31.298s
	user	0m55.667s
	sys		0m43.423s

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0  8764945      0      0      0 29542976      0      0      0 BMmRU
	eth0       1500   0      201      0      0      0      167      0      0      0 BMRU
	eth2       1500   0  3879467      0      0      0 14399423      0      0      0 BMsRU
	eth3       1500   0  4885478      0      0      0 15143553      0      0      0 BMsRU
	lo        16436   0      293      0      0      0      293      0      0      0 LRU

	[root@dev1 ~]# ifdown eth2

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 112.0MB/s   01:28    
	
	real	1m31.059s
	user	0m59.240s
	sys		0m30.670s

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0  9432589      0      0      0 36767111      0      0      0 BMmRU
	eth0       1500   0      440      0      0      0      329      0      0      0 BMRU
	eth3       1500   0  5553122      0      0      0 22367688      0      0      0 BMsRU
	lo        16436   0      293      0      0      0      293      0      0      0 LRU

###mode 6

	[root@dev1 ~]# cat /etc/modprobe.conf 
	alias eth2 pcnet32
	alias eth3 pcnet32
	alias scsi_hostadapter mptbase
	alias scsi_hostadapter3 mptspi
	alias scsi_hostadapter4 ata_piix
	alias peth0 pcnet32
	alias bond0 bonding
	options bond0 miimon=100 mode=6

	[root@dev1 ~]# service network restart
	... ...

scp test

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 110.7MB/s   01:29    
	
	real	1m31.287s
	user	0m56.114s
	sys		0m44.767s

netstat

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0 13269295      0      0      0 44134140      0      0      0 BMmRU
	eth0       1500   0      211      0      0      0      162      0      0      0 BMRU
	eth2       1500   0  3879467      0      0      0 18195589      0      0      0 BMsRU
	eth3       1500   0  9389828      0      0      0 25938551      0      0      0 BMsRU
	lo        16436   0      293      0      0      0      293      0      0      0 LRU

bring down eth3

	[root@dev1 ~]# ifdown eth3
	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 112.0MB/s   01:28    
	
	real	1m30.996s
	user	0m58.047s
	sys		0m39.067s

netstat

	[root@dev1 ~]# ifup eth3
	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0 13990944      0      0      0 51618873      0      0      0 BMmRU
	eth0       1500   0      447      0      0      0      324      0      0      0 BMRU
	eth2       1500   0  4601116      0      0      0 25680322      0      0      0 BMsRU
	eth3       1500   0  9389828      0      0      0 25938551      0      0      0 BMsRU
	lo        16436   0      293      0      0      0      293      0      0      0 LRU

when file transfering, bring down eth2

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 100.6MB/s   01:38    
	
	real	1m40.639s
	user	0m59.232s
	sys		0m38.922s

	[root@dev1 ~]# ifup eth2

after completing transfering

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0 14854993      0      0      0 58942784      0      0      0 BMmRU
	eth0       1500   0     5220      0      0      0     2745      0      0      0 BMRU
	eth2       1500   0  4868378      0      0      0 26232685      0      0      0 BMsU
	eth3       1500   0  9986615      0      0      0 32710099      0      0      0 BMsRU
	lo        16436   0      325      0      0      0      325      0      0      0 LRU

from client to server, while file transfering, bring down eth2
	
	[root@erpdb2 /]# time scp /tmp/oracle_1213_apps.tar.gz 192.168.1.7:/tmp
	root@192.168.1.7's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 108.3MB/s   01:31    
	
	real	1m37.377s
	user	0m58.969s
	sys		0m33.968s

	[root@dev1 ~]# ifdown eth3

after completing transfering

	[root@dev1 ~]# ifup eth3

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0 22419759      0      0      0 59667621      0      0      0 BMmRU
	eth0       1500   0     5412      0      0      0     2882      0      0      0 BMRU
	eth2       1500   0 11898827      0      0      0 26932401      0      0      0 BMsRU
	eth3       1500   0 10520932      0      0      0 32735220      0      0      0 BMsU
	lo        16436   0      325      0      0      0      325      0      0      0 LRU

##Test Summary

eth2

	[root@dev1 ~]# ll -h /u2
	total 9.7G
	drwx------. 2 root root  16K Jun 26 15:28 lost+found
	-rw-r--r--. 1 root root 9.7G Jun 26 16:39 oracle_1213_apps.tar.gz

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	The authenticity of host '192.168.1.10 (192.168.1.10)' can't be established.
	RSA key fingerprint is d2:6f:03:40:a0:78:a9:71:1e:c4:6f:73:e4:d8:4f:7b.
	Are you sure you want to continue connecting (yes/no)? yes
	Warning: Permanently added '192.168.1.10' (RSA) to the list of known hosts.
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 110.7MB/s   01:29    
	
	real	1m35.725s
	user	0m58.625s
	sys		0m39.648s

bonding mode 0 载均衡方式，两块网卡都工作，需要交换机作支持

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 110.7MB/s   01:29    
	
	real	1m31.298s
	user	0m55.667s
	sys		0m43.423s

bonding mode 1 表示冗余方式，网卡只有一个工作，一个出问题启用另外的

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 110.7MB/s   01:29    
	
	real	1m31.298s
	user	0m55.667s
	sys		0m43.423s

bonding mode 6 表示负载均衡方式，两块网卡都工作，不需要交换机作支持

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 110.7MB/s   01:29    
	
	real	1m31.287s
	user	0m56.114s
	sys		0m44.767s

bonding mode 6, when file transfering, bring down eth2

	[root@dev1 ~]# time scp /u2/oracle_1213_apps.tar.gz 192.168.1.10:/tmp
	root@192.168.1.10's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 100.6MB/s   01:38    
	
	real	1m40.639s
	user	0m59.232s
	sys		0m38.922s

after completing transfering

	[root@dev1 ~]# ifup eth2

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0 14854993      0      0      0 58942784      0      0      0 BMmRU
	eth0       1500   0     5220      0      0      0     2745      0      0      0 BMRU
	eth2       1500   0  4868378      0      0      0 26232685      0      0      0 BMsU
	eth3       1500   0  9986615      0      0      0 32710099      0      0      0 BMsRU
	lo        16436   0      325      0      0      0      325      0      0      0 LRU

binding mode 6, from client to server, while file transfering, bring down eth2
	
	[root@erpdb2 /]# time scp /tmp/oracle_1213_apps.tar.gz 192.168.1.7:/tmp
	root@192.168.1.7's password: 
	oracle_1213_apps.tar.gz                                                                                                                 100% 9855MB 108.3MB/s   01:31    
	
	real	1m37.377s
	user	0m58.969s
	sys		0m33.968s

	[root@dev1 ~]# ifdown eth3

after completing transfering

	[root@dev1 ~]# ifup eth3

	[root@dev1 ~]# netstat -i
	Kernel Interface table
	Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
	bond0      1500   0 22419759      0      0      0 59667621      0      0      0 BMmRU
	eth0       1500   0     5412      0      0      0     2882      0      0      0 BMRU
	eth2       1500   0 11898827      0      0      0 26932401      0      0      0 BMsRU
	eth3       1500   0 10520932      0      0      0 32735220      0      0      0 BMsU
	lo        16436   0      325      0      0      0      325      0      0      0 LRU

##Reference

* [Oracle RAC与网卡绑定](http://blog.csdn.net/tianlesoftware/article/details/6189639)
* [Linux 双网卡绑定测试](http://www.cnblogs.com/killkill/archive/2009/02/15/1390717.html)
* [Redhat Deployment_Guide](https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/Deployment_Guide/index.html#sec-Using_Channel_Bonding)
---
layout: post
title: Network Configure on Linux
category : Linux
tags : [Linux, Network]
---

##Configuration Files

Before delving into the interface configuration files, let us first itemize the primary configuration files used in network configuration. Understanding the role these files play in setting up the network stack can be helpful when customizing a Red Hat Enterprise Linux system.

The primary network configuration files are as follows:

 * `/etc/hosts`
The main purpose of this file is to resolve hostnames that cannot be resolved any other way. It can also be used to resolve hostnames on small networks with no DNS server. Regardless of the type of network the computer is on, this file should contain a line specifying the IP address of the loopback device (127.0.0.1) as localhost.localdomain. For more information, refer to the hosts(5) manual page.
 *  `/etc/resolv.conf`
This file specifies the IP addresses of DNS servers and the search domain. Unless configured to do otherwise, the network initialization scripts populate this file. For more information about this file, refer to the resolv.conf(5) manual page.
 *  `/etc/sysconfig/network`
This file specifies routing and host information for all network interfaces. It is used to contain directives which are to have global effect and not to be interface specific. For more information about this file and the directives it accepts, refer to Section D.1.13, “/etc/sysconfig/network”.
 * `/etc/sysconfig/network-scripts/ifcfg-interface-name`
For each network interface, there is a corresponding interface configuration script. Each of these files provide information specific to a particular network interface. Refer to Section 9.2, “Interface Configuration Files” for more information on this type of file and the directives it accepts.

##Interface Configuration Files

Interface configuration files control the software interfaces for individual network devices. As the system boots, it uses these files to determine what interfaces to bring up and how to configure them. These files are usually named ifcfg-name, where name refers to the name of the device that the configuration file controls.

##Channel Bonding Interfaces

Red Hat Enterprise Linux allows administrators to bind multiple network interfaces together into a single channel using the bonding kernel module and a special network interface called a channel bonding interface. Channel bonding enables two or more network interfaces to act as one, simultaneously increasing the bandwidth and providing redundancy.

To create a channel bonding interface, create a file in the `/etc/sysconfig/network-scripts/` directory called ifcfg-bondN, replacing N with the number for the interface, such as 0.

The contents of the file can be identical to whatever type of interface is getting bonded, such as an Ethernet interface. The only difference is that the DEVICE directive is bondN, replacing N with the number for the interface.
The following is a sample channel bonding configuration file:

Example 9.1. Sample ifcfg-bond0 interface configuration file

	DEVICE=bond0
	IPADDR=192.168.1.1
	NETMASK=255.255.255.0
	ONBOOT=yes
	BOOTPROTO=none
	USERCTL=no
	BONDING_OPTS="bonding parameters separated by spaces"

After the channel bonding interface is created, the network interfaces to be bound together must be configured by adding the MASTER and SLAVE directives to their configuration files. The configuration files for each of the channel-bonded interfaces can be nearly identical.

For example, if two Ethernet interfaces are being channel bonded, both eth0 and eth1 may look like the following example:

	DEVICE=ethN
	BOOTPROTO=none
	ONBOOT=yes
	MASTER=bond0
	SLAVE=yes
	USERCTL=no

In this example, replace N with the numerical value for the interface.
For a channel bonding interface to be valid, the kernel module must be loaded. To ensure that the module is loaded when the channel bonding interface is brought up, create a new file as root named bonding.conf in the `/etc/modprobe.d/` directory. Note that you can name this file anything you like as long as it ends with a .conf extension. Insert the following line in this new file:

	alias bondN bonding

Replace N with the interface number, such as 0. For each configured channel bonding interface, there must be a corresponding entry in your new /`etc/modprobe.d/bonding.conf` file.

##Reference

 * [REL Deployment Guide](https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/Deployment_Guide/index.html#ch-Network_Interfaces)
---
layout: post
title: Oracle RAC Command
category : Oracle
tags : [Oracle, Database, DBA]
---

##Oracle Clusterware命令

集群层次

* 节点：osnodes
* 网络：oifcfg
* 集群：csrctl, ocrcheck, orchdump, orcconfig
* 应用：srvctl, crs_stat, onsctl

##节点

olsnodes

    [grid@dev1 ~]$ olsnodes -help
    ... ...

##网络

oifcfg

    [grid@dev1 ~]$ oifcfg -help
	... ...


##集群

###crs

	crsctl start|stop crs	
    crsctl enable|disable crs
	crsctl check crs
    
    [grid@dev1 ~]$ crsctl -help
	... ...

##应用

###crs

    crs_stat 已经deprecated，请使用crsctl status resource

	crs_stat
	crs_stat -v
	crs_stat -v|p service_name
	crs_stat -ls
    
    [grid@dev1 ~]$ crs_stat -help
	... ...

###onsctl
	
	[grid@dev1 ~]$ onsctl help
	... ...

###srvctl	

srvctl help	

	[grid@dev1 ~]$ srvctl --help
	... ...

trace srvctl

	export SRVM_TRACE=TRUE
	srvctl config database -d prod -a

srvctl database:	

 	srvctl config database -d prod -a		
	srvctl start database -d prod
	srvctl start database -d prod -i prod1 -o mount
	
srvctl	instance

	srvctl stop instance -d prod -i prod1 -o immediate
	
srvctl	service

	srvctl start service -d prod -v
	srvctl start service -d prod -v -s service -i prod1
	srvctl stop service -d prod -v -s service -i prod1
	srvctl status service -d prod -v
				
srvctl	asm	
	
	srvctl start asm -n dev1
	srcctl start asm -n erpdb2

    
##常用命令

###停止/启动

停止顺序：grid停止database或instance -> root用户停止crs
启动顺序：root用户启动crs -> grid用户启动database或instance

停止/启动RAC所有实例

    srvctl stop|start database -d dbname

停止/启动RAC单个实例

    srvctl stop|start instance -d dbname -i instancename
 
停止/启动CRS

    crsctl stop|start crs
    
###检查crs resource状态

    crs_stat -t -v
    crsctl stat res -t
    
###检查HAS资源是否正常

    crsctl check crs
    
###检查vodedisk, orc的位置

    crsctl query css votedisk
    ocrcheck
    
###检查orc备份

    ocrconfig -showbackup
    
###检查网络配置

    srvctl config nodeapps -a
    oifcfg getif
    oifcfg iflist
---
layout: post
title: Truncate Queue Table in Oracle EBS
category : Oracle
tags : [Oracle, Database, DBA, EBS, Excepiton]
---

##Truncate Table

database version

	SQL> select * from v$version;

	BANNER
	-------------------------------------------------------------------------
	Oracle9i Enterprise Edition Release 9.2.0.6.0 - 64bit Production
	PL/SQL Release 9.2.0.6.0 - Production
	CORE    9.2.0.6.0       Production
	TNS for IBM/AIX RISC System/6000: Version 9.2.0.6.0 - Production
	NLSRTL Version 9.2.0.6.0 - Production

truncate table

	truncate table ASO.ASO_ORDER_FEEDBACK_T;
    
segment size

	SELECT DS.OWNER,
	       DS.SEGMENT_NAME,
	       DS.SEGMENT_TYPE,
	       (SUM(BYTES) / 1024 / 1024) "SEGMENT_SIZE(MB)"
	  FROM DBA_SEGMENTS DS
	 WHERE DS.SEGMENT_NAME = UPPER('&segment')
	 GROUP BY DS.OWNER, DS.SEGMENT_NAME, DS.SEGMENT_TYPE;

   	OWNER	SEGMENT_NAME			SEGMENT_TYPE	SEGMENT_SIZE(MB)
    ------ ------------------------ --------------- ------------------------
	ASO		ASO_ORDER_FEEDBACK_T	TABLE			34908
 


##Metalink Note
 
suggested truncate operation 

1.truncate table reuse storage

    Connect as apps

    truncate table ASO.ASO_ORDER_FEEDBACK_T REUSE STORAGE;
    truncate table ASO.AQ$_ASO_ORDER_FEEDBACK_T_I REUSE STORAGE; 
    truncate table ASO.AQ$_ASO_ORDER_FEEDBACK_T_H REUSE STORAGE;
    truncate table ASO.AQ$_ASO_ORDER_FEEDBACK_T_T REUSE STORAGE;

    COMMIT;


2.grant execute privilege

    Connect as sysdba
    
    grant execute on SYS.DBMS_AQADM to ASO WITH GRANT OPTION;
    grant execute on SYS.DBMS_AQADM to APPS WITH GRANT OPTION;
    grant execute on SYS.DBMS_AQ to ASO WITH GRANT OPTION;
    grant execute on SYS.DBMS_AQ to APPS WITH GRANT OPTION;
 
3.execute asoqueue.sql
 
    # su - me
    $ cd $ASO_TOP/patch/115/sql/
    $ pwd
    /u2/TEST/me/testappl/aso/11.5.0/patch/115/sql
    $ ls -l asoqueue.sql
    -rwxr-xr-x   1 me  dba           60010 Jun 16 12:55 asoqueue.sql

    $ sqlplus apps/p0809ad @asoqueue apps p0809ad aso aso system manager
	... ...
    Connected.

    Grant succeeded.

    GRANT EXECUTE ON SYS.DBMS_AQ TO aso
                         *
    ERROR at line 1:
    ORA-04021: timeout occurred while waiting to lock object SYS.DBMS_AQ
	... ...

4.deallocate unused space
 
    ALTER TABLE ASO.ASO_ORDER_FEEDBACK_T DEALLOCATE UNUSED;
    ALTER TABLE ASO.AQ$_ASO_ORDER_FEEDBACK_T_I DEALLOCATE UNUSED;
    ALTER TABLE ASO.AQ$_ASO_ORDER_FEEDBACK_T_H DEALLOCATE UNUSED;
    ALTER TABLE ASO.AQ$_ASO_ORDER_FEEDBACK_T_T DEALLOCATE UNUSED;

##Error Message

ORA-04021: Timeout occurred while waiting to lock object SYS.DBMS_AQADM.

You encounter this error when running the Runtime Repository Assistant. There are two possible resolutions to this issue.

Cause: The Runtime Assistant grants 'Execute' privileges on the SYS user dbms_aq packages to the Runtime Repository users being created. The database server waits for a lock on the dbms_aq package before it can apply this grant. If the package is in use by another user, the Runtime Repository will encounter error ORA-04021 from the database server.

Action: Using Oracle Enterprise Manager, connect to the database server and identify the session that is using the dbms_aq. Exit the application that is holding the lock and then recreate a new set of Warehouse Builder Runtime Repository users.

Action: Wait until the application holding the lock finishes running.

From `SQL*Plus`, connect as SYS user and execute the following SQL statements:

    grant execute on sys.dbms_aq to scott;
    grant execute on sys.dbms_aqadm to scott;

When the above statements are successful, execute the following SQL statements:

    revoke execute on sys.dbms_aq from scott;
    revoke execute on sys.dbms_aqadm from scott;

Recreate a new set of Warehouse Builder Runtime Repository users.
Restart the Runtime Platform Service by running the ORACLE_HOME\owb\rtp\sql\start_service.sql script.
    
##Diagnose while re-executing asoqueue.sql
    
    $ whoami
    testora
    $ sqlplus "/ as sysdba"

    SQL*Plus: Release 9.2.0.6.0 - Production on Wed Jul 3 16:25:02 2013

    Copyright (c) 1982, 2002, Oracle Corporation.  All rights reserved.


    Connected to:
    Oracle9i Enterprise Edition Release 9.2.0.6.0 - 64bit Production
    With the Partitioning, OLAP and Oracle Data Mining options
    JServer Release 9.2.0.6.0 - Production
    
    SQL> oradebug setmypid 
    oradebug unlimit 
    oradebug hanganalyze 3 
    Statement processed.
    SQL> Statement processed.
    SQL> Hang Analysis in /u2/TEST/testora/testdb/9.2.0/admin/ERP_erp/udump/erp_ora_1863752.trc
    SQL> 
    SQL> oradebug setmypid 
    oradebug unlimit 
    oradebug hanganalyze 3 
    Statement processed.
    SQL> Statement processed.
    SQL> Hang Analysis in /u2/TEST/testora/testdb/9.2.0/admin/ERP_erp/udump/erp_ora_1863752.trc
    SQL> 
    SQL> select sid,event from v$session_wait;

           SID EVENT
    ---------- ----------------------------------------------------------------
             1 pmon timer
             2 rdbms ipc message
             3 rdbms ipc message
             6 rdbms ipc message
             7 rdbms ipc message
            10 rdbms ipc message
             9 rdbms ipc message
             4 rdbms ipc message
             5 smon timer
            31 library cache pin
            32 library cache pin

           SID EVENT
    ---------- ----------------------------------------------------------------
            38 library cache pin
            46 library cache pin
            92 library cache pin
           114 library cache pin
           157 library cache pin
           184 library cache pin
           190 library cache pin
           186 library cache pin
           170 library cache pin
           127 library cache pin
           106 library cache pin

           SID EVENT
    ---------- ----------------------------------------------------------------
            80 library cache pin
            20 pipe get
            43 pipe get
            63 pipe get
            65 pipe get
           223 pipe get
           210 pipe get
           155 pipe get
           150 pipe get
           144 pipe get
           140 pipe get

           SID EVENT
    ---------- ----------------------------------------------------------------
           126 pipe get
           124 pipe get
           116 pipe get
           113 pipe get
            79 SQL*Net message to client
             8 SQL*Net message from client
            11 SQL*Net message from client
            12 SQL*Net message from client
            21 SQL*Net message from client
            24 SQL*Net message from client
            27 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
            29 SQL*Net message from client
            41 SQL*Net message from client
            40 SQL*Net message from client
            39 SQL*Net message from client
            37 SQL*Net message from client
            36 SQL*Net message from client
            35 SQL*Net message from client
            34 SQL*Net message from client
            33 SQL*Net message from client
            30 SQL*Net message from client
            61 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
            60 SQL*Net message from client
            59 SQL*Net message from client
            58 SQL*Net message from client
            57 SQL*Net message from client
            56 SQL*Net message from client
            54 SQL*Net message from client
            53 SQL*Net message from client
            52 SQL*Net message from client
            97 SQL*Net message from client
            96 SQL*Net message from client
            95 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
            94 SQL*Net message from client
            91 SQL*Net message from client
            90 SQL*Net message from client
            88 SQL*Net message from client
            86 SQL*Net message from client
            84 SQL*Net message from client
           122 SQL*Net message from client
           121 SQL*Net message from client
           120 SQL*Net message from client
           119 SQL*Net message from client
           118 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           117 SQL*Net message from client
           110 SQL*Net message from client
           109 SQL*Net message from client
           108 SQL*Net message from client
           145 SQL*Net message from client
           143 SQL*Net message from client
           142 SQL*Net message from client
           141 SQL*Net message from client
           139 SQL*Net message from client
           138 SQL*Net message from client
           137 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           136 SQL*Net message from client
           135 SQL*Net message from client
           227 SQL*Net message from client
           225 SQL*Net message from client
           222 SQL*Net message from client
           221 SQL*Net message from client
           220 SQL*Net message from client
           219 SQL*Net message from client
           218 SQL*Net message from client
           217 SQL*Net message from client
           216 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           215 SQL*Net message from client
           214 SQL*Net message from client
           213 SQL*Net message from client
           212 SQL*Net message from client
           211 SQL*Net message from client
           209 SQL*Net message from client
           208 SQL*Net message from client
           207 SQL*Net message from client
           205 SQL*Net message from client
           204 SQL*Net message from client
           203 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           202 SQL*Net message from client
           201 SQL*Net message from client
           199 SQL*Net message from client
           198 SQL*Net message from client
           197 SQL*Net message from client
           196 SQL*Net message from client
           195 SQL*Net message from client
           194 SQL*Net message from client
           193 SQL*Net message from client
           189 SQL*Net message from client
           188 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           187 SQL*Net message from client
           185 SQL*Net message from client
           183 SQL*Net message from client
           180 SQL*Net message from client
           179 SQL*Net message from client
           178 SQL*Net message from client
           177 SQL*Net message from client
           176 SQL*Net message from client
           175 SQL*Net message from client
           174 SQL*Net message from client
           173 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           172 SQL*Net message from client
           168 SQL*Net message from client
           167 SQL*Net message from client
           166 SQL*Net message from client
           165 SQL*Net message from client
           164 SQL*Net message from client
           163 SQL*Net message from client
           162 SQL*Net message from client
           161 SQL*Net message from client
           160 SQL*Net message from client
           156 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           154 SQL*Net message from client
           153 SQL*Net message from client
           152 SQL*Net message from client
           151 SQL*Net message from client
           149 SQL*Net message from client
           148 SQL*Net message from client
           147 SQL*Net message from client
           146 SQL*Net message from client
           134 SQL*Net message from client
           133 SQL*Net message from client
           131 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
           130 SQL*Net message from client
           129 SQL*Net message from client
           128 SQL*Net message from client
           125 SQL*Net message from client
           123 SQL*Net message from client
           105 SQL*Net message from client
           104 SQL*Net message from client
           103 SQL*Net message from client
           102 SQL*Net message from client
           101 SQL*Net message from client
           100 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
            99 SQL*Net message from client
            98 SQL*Net message from client
            83 SQL*Net message from client
            82 SQL*Net message from client
            81 SQL*Net message from client
            77 SQL*Net message from client
            76 SQL*Net message from client
            75 SQL*Net message from client
            74 SQL*Net message from client
            73 SQL*Net message from client
            72 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
            71 SQL*Net message from client
            70 SQL*Net message from client
            69 SQL*Net message from client
            68 SQL*Net message from client
            67 SQL*Net message from client
            66 SQL*Net message from client
            62 SQL*Net message from client
            51 SQL*Net message from client
            50 SQL*Net message from client
            49 SQL*Net message from client
            48 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
            47 SQL*Net message from client
            45 SQL*Net message from client
            44 SQL*Net message from client
            42 SQL*Net message from client
            28 SQL*Net message from client
            26 SQL*Net message from client
            22 SQL*Net message from client
            14 SQL*Net message from client
            16 SQL*Net message from client
            17 SQL*Net message from client
            18 SQL*Net message from client

           SID EVENT
    ---------- ----------------------------------------------------------------
            19 queue messages
           206 queue messages
           171 queue messages
            78 queue messages
            23 wakeup time manager

    203 rows selected.
    
    SQL> select /*+ ordered */ w1.sid  waiting_session,
             h1.sid  holding_session,
             w.kgllktype lock_or_pin,
             w.kgllkhdl address,
             decode(h.kgllkmod,  0, 'None', 1, 'Null', 2, 'Share', 3, 'Exclusive',
                'Unknown') mode_held,
             decode(w.kgllkreq,  0, 'None', 1, 'Null', 2, 'Share', 3, 'Exclusive',
              'Unknown') mode_requested
       from dba_kgllock w, dba_kgllock h, v$session w1, v$session h1
      where
       (((h.kgllkmod != 0) and (h.kgllkmod != 1)
          and ((h.kgllkreq = 0) or (h.kgllkreq = 1)))
        and
          (((w.kgllkmod = 0) or (w.kgllkmod= 1))
          and ((w.kgllkreq != 0) and (w.kgllkreq != 1))))
       and  w.kgllktype      =  h.kgllktype
       and  w.kgllkhdl =  h.kgllkhdl
       and  w.kgllkuse     =   w1.saddr
       and  h.kgllkuse     =   h1.saddr
     /
    WAITING_SESSION HOLDING_SESSION LOCK ADDRESS          MODE_HELD MODE_REQU
    --------------- --------------- ---- ---------------- --------- ---------
                175              38 Pin  07000000ACBBE940 Share     Share
                127              38 Pin  07000000ACBBE940 Share     Share
                114              38 Pin  07000000ACBBE940 Share     Share
                110              38 Pin  07000000ACBBE940 Share     Share
                106              38 Pin  07000000ACBBE940 Share     Share
                 80              38 Pin  07000000ACBBE940 Share     Exclusive
                 46              38 Pin  07000000ACBBE940 Share     Share
                 31              38 Pin  07000000ACBBE940 Share     Share
                 30              38 Pin  07000000ACBBE940 Share     Share
                 28              38 Pin  07000000ACBBE940 Share     Share
                175              78 Pin  07000000ACBBE940 Share     Share

    WAITING_SESSION HOLDING_SESSION LOCK ADDRESS          MODE_HELD MODE_REQU
    --------------- --------------- ---- ---------------- --------- ---------
                127              78 Pin  07000000ACBBE940 Share     Share
                114              78 Pin  07000000ACBBE940 Share     Share
                110              78 Pin  07000000ACBBE940 Share     Share
                106              78 Pin  07000000ACBBE940 Share     Share
                 80              78 Pin  07000000ACBBE940 Share     Exclusive
                 46              78 Pin  07000000ACBBE940 Share     Share
                 31              78 Pin  07000000ACBBE940 Share     Share
                 30              78 Pin  07000000ACBBE940 Share     Share
                 28              78 Pin  07000000ACBBE940 Share     Share
                175              92 Pin  07000000ACBBE940 Share     Share
                127              92 Pin  07000000ACBBE940 Share     Share

    WAITING_SESSION HOLDING_SESSION LOCK ADDRESS          MODE_HELD MODE_REQU
    --------------- --------------- ---- ---------------- --------- ---------
                114              92 Pin  07000000ACBBE940 Share     Share
                110              92 Pin  07000000ACBBE940 Share     Share
                106              92 Pin  07000000ACBBE940 Share     Share
                 80              92 Pin  07000000ACBBE940 Share     Exclusive
                 46              92 Pin  07000000ACBBE940 Share     Share
                 31              92 Pin  07000000ACBBE940 Share     Share
                 30              92 Pin  07000000ACBBE940 Share     Share
                 28              92 Pin  07000000ACBBE940 Share     Share
                175             170 Pin  07000000ACBBE940 Share     Share
                127             170 Pin  07000000ACBBE940 Share     Share
                114             170 Pin  07000000ACBBE940 Share     Share

    WAITING_SESSION HOLDING_SESSION LOCK ADDRESS          MODE_HELD MODE_REQU
    --------------- --------------- ---- ---------------- --------- ---------
                110             170 Pin  07000000ACBBE940 Share     Share
                106             170 Pin  07000000ACBBE940 Share     Share
                 80             170 Pin  07000000ACBBE940 Share     Exclusive
                 46             170 Pin  07000000ACBBE940 Share     Share
                 31             170 Pin  07000000ACBBE940 Share     Share
                 30             170 Pin  07000000ACBBE940 Share     Share
                 28             170 Pin  07000000ACBBE940 Share     Share
                175             184 Pin  07000000ACBBE940 Share     Share
                127             184 Pin  07000000ACBBE940 Share     Share
                114             184 Pin  07000000ACBBE940 Share     Share
                110             184 Pin  07000000ACBBE940 Share     Share

    WAITING_SESSION HOLDING_SESSION LOCK ADDRESS          MODE_HELD MODE_REQU
    --------------- --------------- ---- ---------------- --------- ---------
                106             184 Pin  07000000ACBBE940 Share     Share
                 80             184 Pin  07000000ACBBE940 Share     Exclusive
                 46             184 Pin  07000000ACBBE940 Share     Share
                 31             184 Pin  07000000ACBBE940 Share     Share
                 30             184 Pin  07000000ACBBE940 Share     Share
                 28             184 Pin  07000000ACBBE940 Share     Share

    50 rows selected.
    
    SQL> oradebug setmypid 
    oradebug unlimit 
    oradebug hanganalyze 3 
    Statement processed.
    SQL> Statement processed.
    SQL> Hang Analysis in /u2/TEST/testora/testdb/9.2.0/admin/ERP_erp/udump/erp_ora_1863752.trc
    SQL> host vi /u2/TEST/testora/testdb/9.2.0/admin/ERP_erp/udump/erp_ora_1863752.trc
    "/u2/TEST/testora/testdb/9.2.0/admin/ERP_erp/udump/erp_ora_1863752.trc" 880 lines, 45434 characters Dump file /u2/TEST/testora/testdb/9.2.0/admin/ERP_erp/udump/erp_ora_1863752.trc
    Oracle9i Enterprise Edition Release 9.2.0.6.0 - 64bit Production
    With the Partitioning, OLAP and Oracle Data Mining options
    JServer Release 9.2.0.6.0 - Production
    ORACLE_HOME = /u2/TEST/testora/testdb/9.2.0
    System name:    AIX
    Node name:erp
    Release:3
    Version:5
    Machine:00066345D600
    Instance name: ERP
    Redo thread mounted by this instance: 1
    Oracle process number: 70
    Unix process pid: 1863752, image: oracle@erp (TNS V1-V3)

    *** SESSION ID:(79.3983) 2013-07-03 16:25:16.197
    *** 2013-07-03 16:25:16.197
    ==============
    HANG ANALYSIS:
    ==============
    Open chains found:
    Chain 1 : <cnode/sid/sess_srno/proc_ptr/ospid/wait_event> :
        <0/38/37/0xa87b3b68/2388080/queue messages>
     -- <0/80/7524/0xa879b848/593972/library cache pin>
    Chain 2 : <cnode/sid/sess_srno/proc_ptr/ospid/wait_event> :
        <0/78/1346/0xa87b4ab0/2027618/PL/SQL lock timer>
     -- <0/80/7524/0xa879b848/593972/library cache pin>
    Chain 3 : <cnode/sid/sess_srno/proc_ptr/ospid/wait_event> :
        <0/92/60842/0xa87b4598/2056366/queue messages>
     -- <0/80/7524/0xa879b848/593972/library cache pin>
    Other chains found:
    Chain 4 : <cnode/sid/sess_srno/proc_ptr/ospid/wait_event> :"/u2/TEST/testora/testdb/9.2.0/admin/ERP_erp/udump/erp_ora_1863752.trc" 880 lines, 45434 characters
    
    
    SQL> select username,program,sql_hash_value from v$session where sid=38;

    USERNAME   PROGRAM          SQL_HASH_VALUE
    ---------- ---------------- ---------------------------
    APPS      JDBC Thin Client  4251209219


    SQL> select sql_text from v$sql where hash_value=4251209219;
    SQL_TEXT
    -------------------------------------------------------------------------
    BEGIN FND_CP_GSM_IPC.Get_Message(:1,:2,:3,:4,:5,:6,:7,:8,:9,:10); END;
    
    SQL> select s.sid,p.spid from v$session s,v$process p where s.paddr=p.addr and s.sid in (38,78,92);

           SID SPID
    ---------- ------------
            38 2388080
            92 2056366
            78 2027618

    SQL> host kill -9 2388080

    SQL> host kill -9 2056366

    SQL> host kill -9 2027618
  
##Retest
  
retest and the issue remains

stop apps and asoqueue.sql can be executed successfully.

segment size

	SELECT DS.OWNER,
	       DS.SEGMENT_NAME,
	       DS.SEGMENT_TYPE,
	       (SUM(BYTES) / 1024 / 1024) "SEGMENT_SIZE(MB)"
	  FROM DBA_SEGMENTS DS
	 WHERE DS.SEGMENT_NAME = UPPER('&segment')
	 GROUP BY DS.OWNER, DS.SEGMENT_NAME, DS.SEGMENT_TYPE;

   	OWNER	SEGMENT_NAME			SEGMENT_TYPE	SEGMENT_SIZ
    ------ ------------------------ --------------- -------------------------
    ASO		ASO_ORDER_FEEDBACK_T	TABLE			0.125
   
##Reference:

* Release 11.5.10 / R12 Quoting/Order Capture Order Feedback Queue FAQ [ID 181410.1]
---
layout: post
title: First Repository on Github
category : Dev
tags : [Python, Git, Github]
---

##Generating SSH Keys

check for ssh keys

	[dylan@www .ssh]$ cd ~/.ssh

	[dylan@www .ssh]$ pwd
	/home/dylan/.ssh

generate a new ssh key

	[dylan@www .ssh]$ ssh-keygen -t rsa -C 'dylanninin@gmail.com'

	[dylan@www .ssh]$ ll
	total 16
	-rw------- 1 dylan dylan  380 Sep 28  2012 authorized_keys
	-rw------- 1 dylan dylan 1675 Jan 27 14:39 id_rsa
	-rw-r--r-- 1 dylan dylan  402 Jan 27 14:39 id_rsa.pub

add ssh key to github

* GitHub -> Accout Settings -> SSH Keys -> Add SSH Key -> Paste your public key

test everthing out

	[dylan@www .ssh]$ ssh -T git@github.com
	Warning: Permanently added the RSA host key for IP address '204.232.175.90' to the list of known hosts.
	Hi dylanninin! You've successfully authenticated, but GitHub does not provide shell access.

##Repository Quick setup

* GitHub -> Create a new repo

	git@github.com:dylanninin/blog.git


Configure username and email

	git config --global user.name 'dylanninin'
	git config --global user.email 'dylanninin@gmail.com'

Create a new repository on the command line

	touch README.md
	git init
	git add README.md
	git commit -m "first commit"
	git remote add origin git@github.com:dylanninin/blog.git
	git push -u origin master
	
Push an existing repository from the command line

	git remote add origin git@github.com:dylanninin/blog.git
	git push -u origin master

##Example

init local repo

	[dylan@www ~]$ ll
	total 4
	drwxrwxr-x 6 dylan dylan   4096 Jul  2 22:18 blog

	[dylan@www ~]$ cd blog
	
	[dylan@www blog]$ git init
	Initialized empty Git repository in /home/dylan/blog/.git/

	[dylan@www blog]$ git add *
	
	[dylan@www blog]$ git commit -m 'blog init with web.py'
	[master (root-commit) 58899a8] blog init with web.py
	 128 files changed, 22775 insertions(+), 0 deletions(-)
	 create mode 100644 __init__.py
	 create mode 100644 __init__.pyc
	 create mode 100644 blog.log
	 create mode 100644 blog.py
	 create mode 100644 blog.pyc
	 create mode 100644 config.py
	 create mode 100644 config.pyc
	 create mode 100644 controller.py
	 create mode 100644 controller.pyc
	 create mode 100644 model.py
	 create mode 100644 model.pyc
	 ... ...

push to remote repo

	[dylan@www blog]$ git remote add origin git@github.com:dylanninin/blog.git
	
	[dylan@www blog]$ git push -u origin master
	Counting objects: 144, done.
	Delta compression using up to 8 threads.
	Compressing objects: 100% (142/142), done.
	Writing objects: 100% (144/144), 335.43 KiB, done.
	Total 144 (delta 2), reused 0 (delta 0)
	To git@github.com:dylanninin/blog.git
	 * [new branch]      master -> master
	Branch master set up to track remote branch master from origin.

delete pyc

	[dylan@www blog]$ rm *.pyc
	[dylan@www blog]$ git rm *.pyc
	rm '__init__.pyc'
	rm 'blog.pyc'
	rm 'config.pyc'
	rm 'controller.pyc'
	rm 'model.pyc'
	rm 'service.pyc'
	rm 'tool.pyc'
	
	[dylan@www blog]$ git commit -m 'delete pyc'
	
	[master fbfa542] delete pyc
	 7 files changed, 0 insertions(+), 0 deletions(-)
	 delete mode 100644 __init__.pyc
	 delete mode 100644 blog.pyc
	 delete mode 100644 config.pyc
	 delete mode 100644 controller.pyc
	 delete mode 100644 model.pyc
	 delete mode 100644 service.pyc
	 delete mode 100644 tool.pyc
	 
	[dylan@www blog]$ git push -u origin master
	Counting objects: 3, done.
	Delta compression using up to 8 threads.
	Compressing objects: 100% (2/2), done.
	Writing objects: 100% (2/2), 238 bytes, done.
	Total 2 (delta 1), reused 0 (delta 0)
	To git@github.com:dylanninin/blog.git
	   58899a8..fbfa542  master -> master
	Branch master set up to track remote branch master from origin.

##Reference

* [Generating SSH Keys](https://help.github.com/articles/generating-ssh-keys#platform-linux)
---
layout: post
title: Raw Markdown Blog with Web.py
category : Python
tags : [Python, Markdown, Blog]
---

##设计思路

传统的博客程序，一般都有后台数据库。当新发布一篇博客时，基本步骤就需要：1.登陆->2.新建博客-3.填写标题-4.复制内容-5.填写关键字-6.选择分类-7.点击发布等，过程十分繁琐。

前段时间正好看到轻量级博客[Letterpress](https://github.com/an0/Letterpress)中提到的想法：直接上传markdown格式的博客原文，让程序自动去处理和渲染。这样一来发布博客确实可省去很多体力活；另外，作为python新手，也想动手实践下，至于实现好坏暂且不表。

主要思路：

* 页面模板：

	* 去年10月起折腾过Movable Type 5，虽博客写得少，但还是看过官方的文档；以及近来流行的Bootstrap项目，也动手进行过一些定制化，Movable Type后台的模板设计很值得学习和借鉴，所以照搬过来。

	* 博客主要采用典型的三栏设计，据此对模板进行抽象和分类，即可分为主模板、layout、modules、widgets和misc等，模板之间可以相互引用。这些模板根据URL请求获取处理后的参数，完成最终渲染并响应给用户。

* 核心逻辑：

	* 初始化。EntryService读取指定路径下的markdown文件，进行初始化，以`2013-7-3_github_tips.md`为例：
		
		* 1.生成URL，即根据文件名生成URL。如 `2013-7-3_github_tips.md` -> `/blog/2013/07/03/github_tips.html`，对应的raw文件URL则为`/raw/2013/07/03/github_tips.md`。
		* 2.获取时间，优先从文件名中提取，否则获取文件的创建或者更新时间，这里暂未处理好。
	    * 3.获取内容，包括markdown原文内容，以及经过markdown到html转换后的内容。
		* 4.其他初始化，如tag, category, calendar, archive等。
		* 5.存储博客： url(key) -> entry(value) 为一一对一个关系，这里使用dict将entry保存在内存中。

	* 响应请求。当浏览器客户端进行访问时，Web.py应用根据URL路由到指定的Controller，并由EntryService处理逻辑（包括搜索、查看博客、查博客的Raw内容、博客和Raw归档等），返回响应参数（针对模板，该参数也创建了数据模型，可以简化参数传递）；再由控制器根据响应参数判断渲染的模板文件，响应给用户。若请求博客的Raw内容，则直接返回文本内容不用模板渲染。

    * 动态监听。使用pyinotify监听文件系统中的指定博客路径，当该路径下有新增、修改、删除博客的操作时，触发监听事件，并通知EntryService进行相应的新增、修改、删除操作，以更新内存中的博客内容。在本项目中，仅搭建了一个可用的模型，pyinotify使用了一个单独的线程，EntryService在整个程序运行过程，仅有一个实例。

##项目结构

	blog/
	├── __init__.py            #初始化文件，主要加载pyinotify，监听blog/raw/entry路径下md的新增、修改、删除
	├── blog.py                #博客程序入口，运行： $ cd blog; python blog.py > blog.log 2>&1 &
	├── config.py			   #博客配置，主要是站点、文件系统路径、博客URL路由以及全局环境的配置
	├── controller.py		   #控制器模块，分发URL请求，并响应响应的视图
	├── service.py			   #服务模块，初始化博客系统，处理URL请求逻辑
	├── model.py			   #模型模块，针对博客系统的Entry,Page,Tag,Category等设置的数据模型，以及模板中的参数模型
	├── tool.py				   #工具类，如自动提取关键字，将字典转换为对象等，待完善
	├── README.md
	├── raw					   #raw格式文件路径，主要是markdown文件
	│   ├── entry			   #博客文件
	│   ├── page			   #页面文件，相对于博客而言，页面处理简单很多
	│   └── tweet				
	├── static				   #静态文件，web.py框架的要求，此路径下的文件可以有web.py当做静态文件处理
	│   ├── css
	│   ├── favicon.ico
	│   ├── img
	│   └── js
	└─── template			   #模板库，主要参考Movable Type 5的设计。layout,misc,modules,widgets为子模板。
	    ├── index.html	       #首页模板
	    ├── search.html		   #搜索模板
	    ├── entry.html		   #Entry/Page详情模板
	    ├── archive.html	   #归档模板
	    ├── atom.xml		   #RSS订阅模板
	    ├── error.html		   #错误页面模板
	    ├── layout			   #布局模板，这里采用 `AA|B|C`的三栏布局，AA放博客主要内容，B、C作为右侧边栏显示widgets
	    │   ├── footer.html
	    │   ├── header.html
	    │   ├── navbar.html
	    │   ├── three
	    │   │   ├── primary.html
	    │   │   └── secondary.html
	    │   └── two
	    ├── misc			  #杂项资源
	    │   ├── ads
	    │   │   └── ad.html
	    │   └── analytics.html
		├── modules			   #模板模板，在AA布局中使用
		│   ├── archive.html
		│   ├── category.html
		│   ├── comment.html
		│   ├── entry.html
		│   ├── error.html
		│   ├── excerpt.html
		│   ├── info.html
		│   ├── pager.html
		│   ├── related.html
		│   ├── search.html
		│   └── tag.html
		└── widgets           #小工具模板，在B、C侧边栏中使用
		    ├── about.html
		    ├── archive.html
		    ├── calendar.html
		    ├── category.html
		    ├── link.html
		    ├── powered.html
		    ├── recently.html
		    └── tag.html
	
##要求

* 仅限于Linux平台。因使用[pyinotify](https://github.com/seb-m/pyinotify)有平台限制。Pyinotify监听指定路径，即可自动处理新增、更新以及删除的博客。详见[pyinotify](https://github.com/seb-m/pyinotify)。

* Markdown。将markdown格式文件渲染成HTML，详见[markdown](https://github.com/waylan/Python-Markdown)。

* Web.py。使用的Web框架，由已故的[Aaron Swartz](http://www.aaronsw.com/)开发，详见[Web.py](http://webpy.org)。

* Python 2.7.5。本博客程序仅在Python 2.7.5下测试通过。

##运行

克隆博客代码

	$ cd ~

	$ git clone https://github.com/dylanninin/blog.git
 
切换路径
	
	$ cd blog

启动博客

	$ python blog.py > blog.log 2>&1 &  #Listen on 0.0.0.0:8080 default


##博客

环境准备

	$ ln -s ~/blog/raw/entry  entry

	$ cd entry

发布博客

	$ rz 				# use ZMODEM (Batch) file receive tool to send your local markdown file
						# to blog server. md format is usually yyyy-mm-dd_file_name.md

删除博客

	$ rm 2013-07-02_github_tips.md

##效果

 * Online Demo: [http://dylanninin.com:8080/](http://dylanninin.com:8080)
 * Movable Type 5：[http://dylanninin.com/](http://dylanninin.com/)	

##计划

* 自动提取关键字

* 自动摘要

* 自动查找相关博客

* 功能完善和bug排除

* 代码重构

##参考

* [Web.py](http://webpy.org)
* [pyinotify](https://github.com/seb-m/pyinotify)
* [Bootstrap](http://twitter.github.com/bootstrap)
* [Movable Type](http://dylanninin.com)

---
layout: post
title: ORA-12514 on Oracle RAC
category : Oracle
tags : [Oracle, Database, DBA, Exception]
---

##异常症状

erpapp connect issue 
   
    [applmgr@app1 network]$ tnsping prod

    TNS Ping Utility for Linux: Version 10.1.0.5.0 - Production on 05-JUL-2013 10:29:20

    Copyright (c) 1997, 2003, Oracle.  All rights reserved.

    Used parameter files:


    Used TNSNAMES adapter to resolve the alias
    Attempting to contact (DESCRIPTION= (ADDRESS=(PROTOCOL=tcp)(HOST=dev1-vip.egolife.com)(PORT=1521)) (CONNECT_DATA= (SERVICE_NAME=PROD1) (INSTANCE_NAME=PROD1)))
    OK (10 msec)

##异常确认

    [applmgr@app1 network]$ sqlplus /nolog

    SQL*Plus: Release 10.1.0.5.0 - Production on Fri Jul 5 10:29:23 2013

    Copyright (c) 1982, 2005, Oracle.  All rights reserved.

    SQL> conn apps
    Enter password: 
    ERROR:
    ORA-12514: TNS:listener does not currently know of service requested in connect
    descriptor

service on dev1

    [grid@dev1 ~]$ lsnrctl

    LSNRCTL for Linux: Version 11.2.0.3.0 - Production on 05-JUL-2013 10:48:14

    Copyright (c) 1991, 2011, Oracle.  All rights reserved.

    Welcome to LSNRCTL, type "help" for information.

    LSNRCTL> services
    Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=LISTENER)))
    Services Summary...
    Service "+ASM" has 1 instance(s).
      Instance "+ASM1", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:0 refused:0 state:ready
             LOCAL SERVER
    Service "PROD" has 1 instance(s).
      Instance "PROD1", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:3 refused:0 state:ready
             LOCAL SERVER
    The command completed successfully
    
service on dev2

    [grid@dev2 ~]$ lsnrctl 

    LSNRCTL for Linux: Version 11.2.0.3.0 - Production on 05-JUL-2013 10:48:37

    Copyright (c) 1991, 2011, Oracle.  All rights reserved.

    Welcome to LSNRCTL, type "help" for information.

    LSNRCTL> services
    Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=LISTENER)))
    Services Summary...
    Service "+ASM" has 1 instance(s).
      Instance "+ASM2", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:0 refused:0 state:ready
             LOCAL SERVER
    Service "PROD" has 1 instance(s).
      Instance "PROD2", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:29 refused:0 state:ready
             LOCAL SERVER
    Service "prod1" has 1 instance(s).
      Instance "PROD2", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:29 refused:0 state:ready
             LOCAL SERVER
    Service "prod2" has 1 instance(s).
      Instance "PROD2", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:29 refused:0 state:ready
             LOCAL SERVER
    The command completed successfully

##异常解决 
 
the service prod1 is running on instance PROD2(host node is dev2)

relocate service prod1 from instance PROD2 to PROD1

    [grid@dev2 ~]$ srvctl relocate service -d PROD -s prod1 -i PROD2 -t PROD1
    
service on dev1
    
    [grid@dev1 ~]$ lsnrctl 

    LSNRCTL for Linux: Version 11.2.0.3.0 - Production on 05-JUL-2013 10:57:10

    Copyright (c) 1991, 2011, Oracle.  All rights reserved.

    Welcome to LSNRCTL, type "help" for information.

    LSNRCTL> services
    Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=LISTENER)))
    Services Summary...
    Service "+ASM" has 1 instance(s).
      Instance "+ASM1", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:0 refused:0 state:ready
             LOCAL SERVER
    Service "PROD" has 1 instance(s).
      Instance "PROD1", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:73 refused:0 state:ready
             LOCAL SERVER
    Service "prod1" has 1 instance(s).
      Instance "PROD1", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:73 refused:0 state:ready
             LOCAL SERVER
    The command completed successfully


service on dev2

    [grid@dev2 ~]$ lsnrctl 

    LSNRCTL for Linux: Version 11.2.0.3.0 - Production on 05-JUL-2013 10:57:24

    Copyright (c) 1991, 2011, Oracle.  All rights reserved.

    Welcome to LSNRCTL, type "help" for information.

    LSNRCTL> services
    Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=LISTENER)))
    Services Summary...
    Service "+ASM" has 1 instance(s).
      Instance "+ASM2", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:0 refused:0 state:ready
             LOCAL SERVER
    Service "PROD" has 1 instance(s).
      Instance "PROD2", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:52 refused:0 state:ready
             LOCAL SERVER
    Service "prod2" has 1 instance(s).
      Instance "PROD2", status READY, has 1 handler(s) for this service...
        Handler(s):
          "DEDICATED" established:52 refused:0 state:ready
             LOCAL SERVER
    The command completed successfully
    
##Error Message

ORA-12514: TNS:listener does not currently know of service requested in connect descriptor

Cause: The listener received a request to establish a connection to a database or other service. The connect descriptor received by the listener specified a service name for a service (usually a database service) that either has not yet dynamically registered with the listener or has not been statically configured for the listener. This may be a temporary condition such as after the listener has started, but before the database instance has registered with the listener.

Action:
- Wait a moment and try to connect a second time.

- Check which services are currently known by the listener by executing: lsnrctl services listener_name

- Check that the SERVICE_NAME parameter in the connect descriptor of the net service name used specifies a service known by the listener.

- If an easy connect naming connect identifier was used, check that the service name specified is a service known by the listener.

- Check for an event in the listener.log file.

##Reference

* Oracle 11g Error Message
---
layout: post
title: OS Preparation for Oracle Database
category : Oracle
tags : [Oracle, Database, DBA, Linux]
---

##OS Configuration

### kernel

    [root@db ~]# uname -a
    Linux db.egolife.com 2.6.32-220.el6.x86_64 #1 SMP Tue Dec 6 19:48:22 GMT 2011 x86_64 x86_64 x86_64 GNU/Linux

### hosts

	[root@db ~]# cat /etc/hosts
	127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
	::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
	172.17.1.100  db.egolife.com db
	172.29.88.135 wxbak.egolife.com wxbak

##Database 11gR2

### group and user

    [root@db ~]# groupadd -g 501 oinstall
    [root@db ~]# groupadd -g 502 dba
    [root@db ~]# useradd -u 501 -g oinstall -G dba oracle
    [root@db ~]# id oracle
    uid=501(oracle) gid=501(oinstall) groups=501(oinstall),502(dba)
    [root@db ~]# passwd oracle

###	kernel parameters

* `/proc/sys/.../file`: current value of these parameters; non persistent * `/etc/sysctl.conf`: configure file of these parameters; persistent during each startup * `sysctl -p`: persistent during each startup 

#### parameters

shared memory, semaphores, file handles parameter, net parameter 
	
    [root@db ~]# sysctl -p
    net.ipv4.ip_forward = 0
    net.ipv4.conf.default.rp_filter = 1
    net.ipv4.conf.default.accept_source_route = 0
    kernel.sysrq = 0
    kernel.core_uses_pid = 1
    net.ipv4.tcp_syncookies = 1
    error: "net.bridge.bridge-nf-call-ip6tables" is an unknown key
    error: "net.bridge.bridge-nf-call-iptables" is an unknown key
    error: "net.bridge.bridge-nf-call-arptables" is an unknown key
    kernel.msgmnb = 65536
    kernel.msgmax = 65536
    kernel.shmmax = 68719476736
    kernel.shmall = 4294967296

    [root@db ~]# vim /etc/sysctl.conf
    ... ...
    #2013-7-19  kernel setting for oracle database 11gR2
    fs.aio-max-nr = 1048576
    fs.file-max = 6815744
    kernel.shmall = 2097152
    kernel.shmmax = 4294967295
    kernel.shmmni = 4096
    kernel.sem = 250 32000 100 128
    net.ipv4.ip_local_port_range = 9000 65500
    net.core.rmem_default = 262144
    net.core.rmem_max = 4194304
    net.core.wmem_default = 262144
    net.core.wmem_max = 1048576
    
    [root@db ~]# sysctl -p
    net.ipv4.ip_forward = 0
    net.ipv4.conf.default.rp_filter = 1
    net.ipv4.conf.default.accept_source_route = 0
    kernel.sysrq = 0
    kernel.core_uses_pid = 1
    net.ipv4.tcp_syncookies = 1
    kernel.msgmnb = 65536
    kernel.msgmax = 65536
    kernel.shmmax = 68719476736
    kernel.shmall = 4294967296
    fs.aio-max-nr = 1048576
    fs.file-max = 6815744
    kernel.shmall = 2097152
    kernel.shmmax = 4294967295
    kernel.shmmni = 4096
    kernel.sem = 250 32000 100 128
    net.ipv4.ip_local_port_range = 9000 65500
    net.core.rmem_default = 262144
    net.core.rmem_max = 4194304
    net.core.wmem_default = 262144
    net.core.wmem_max = 1048576

### resource limits
* `/etc/security.conf`: resource limits configuration file 
* `/etc/pam.d/login`: login settings * `ulimit`: utility to check or change resource limits 

open file descriptors, number of processes available to a single user, size of the 
stack segment of the process.


    [root@db ~]# ulimit -Sn; ulimit -Hn
    1024
    1024
    [root@db ~]# ulimit -Su; ulimit -Hu
    1024
    62638
    [root@db ~]# ulimit -Ss; ulimit -Hs
    10240
    unlimited

    [root@db ~]# vim /etc/security/limits.conf 
    ... ...
    #2013-7-19  resource limits for oracle database 11gR2
    oracle              soft    nproc   2047
    oracle              hard    nproc   16384
    oracle              soft    nofile  1024
    oracle              hard    nofile  65536
    oracle              soft    stack   10240
    
    [root@db ~]# vim /etc/pam.d/login 
    ... ...
    #2013-7-19 shell limits for oracle database 11gR2
    session    required     /lib64/security/pam_limits.so
    session    required     pam_limits.so
    
### managing packages
for Oracle Linux 6 and Red Hat Enterprise Linux 6 x86-64, the following packages (or later versions) must be installed:

* binutils-2.20.51.0.2-5.11.el6 (x86_64)
* compat-libcap1-1.10-1 (x86_64)
* compat-libstdc++-33-3.2.3-69.el6 (x86_64)
* compat-libstdc++-33-3.2.3-69.el6.i686
* gcc-4.4.4-13.el6 (x86_64)
* gcc-c++-4.4.4-13.el6 (x86_64)
* glibc-2.12-1.7.el6 (i686)
* glibc-2.12-1.7.el6 (x86_64)
* glibc-devel-2.12-1.7.el6 (x86_64)
* glibc-devel-2.12-1.7.el6.i686
* ksh
* libgcc-4.4.4-13.el6 (i686)
* libgcc-4.4.4-13.el6 (x86_64)
* libstdc++-4.4.4-13.el6 (x86_64)
* libstdc++-4.4.4-13.el6.i686
* libstdc++-devel-4.4.4-13.el6 (x86_64)
* libstdc++-devel-4.4.4-13.el6.i686
* libaio-0.3.107-10.el6 (x86_64)
* libaio-0.3.107-10.el6.i686
* libaio-devel-0.3.107-10.el6 (x86_64)
* libaio-devel-0.3.107-10.el6.i686
* make-3.81-19.el6
* sysstat-9.0.4-11.el6 (x86_64)

query packages installed

    [root@db ~]#  rpm -qa --queryformat "%{NAME}-%{VERSION}-%{RELEASE} (%{ARCH})\n" | egrep 'binutils|compat-db43|gcc|glibc|libgcc|libstdc++|libXi|libXp|libaio|libgomp|make|gdbm|sysstat|util-linux-ng|unzip|compat-libstdc++|compat-libstdc++|openmotif21|xorg-x11-libs-compat' | sort
    automake-1.11.1-1.2.el6 (noarch)
    binutils-2.20.51.0.2-5.28.el6 (x86_64)
    binutils-devel-2.20.51.0.2-5.28.el6 (x86_64)
    compat-db43-4.3.29-15.el6 (x86_64)
    compat-glibc-2.5-46.2 (x86_64)
    compat-glibc-headers-2.5-46.2 (x86_64)
    compat-libstdc++-296-2.96-144.el6 (i686)
    compat-libstdc++-33-3.2.3-69.el6 (x86_64)
    gcc-4.4.6-3.el6 (x86_64)
    gcc-c++-4.4.6-3.el6 (x86_64)
    gcc-gfortran-4.4.6-3.el6 (x86_64)
    gdbm-1.8.0-36.el6 (x86_64)
    glibc-2.12-1.47.el6 (i686)
    glibc-2.12-1.47.el6 (x86_64)
    glibc-common-2.12-1.47.el6 (x86_64)
    glibc-devel-2.12-1.47.el6 (x86_64)
    glibc-headers-2.12-1.47.el6 (x86_64)
    libaio-0.3.107-10.el6 (x86_64)
    libgcc-4.4.6-3.el6 (i686)
    libgcc-4.4.6-3.el6 (x86_64)
    libgomp-4.4.6-3.el6 (x86_64)
    libstdc++-4.4.6-3.el6 (x86_64)
    libstdc++-devel-4.4.6-3.el6 (x86_64)
    libXi-1.3-3.el6 (x86_64)
    libXi-devel-1.3-3.el6 (x86_64)
    libXinerama-1.1-1.el6 (x86_64)
    libXinerama-devel-1.1-1.el6 (x86_64)
    libXp-1.0.0-15.1.el6 (x86_64)
    libXp-devel-1.0.0-15.1.el6 (x86_64)
    make-3.81-19.el6 (x86_64)
    sysstat-9.0.4-18.el6 (x86_64)
    unzip-6.0-1.el6 (x86_64)
    util-linux-ng-2.17.2-12.4.el6 (x86_64)
    
to be installed packages: 

* compat-libcap1-1.10-1 (x86_64)
* glibc-devel-2.12-1.47.el6
* ksh
* libstdc++-4.4.6-3.el6
* libstdc++-devel-4.4.6-3.el6
* libaio-0.3.107-10.el6
* libaio-devel-0.3.107-10.el6, libaio-devel-0.3.107-10.el6 (x86_64)
* make-3.81-19.el6

### OFA

    [root@db ~]# mkdir -p /db/oracle/product/11.2.0/db_1
    [root@db ~]# chown -R oracle:oinstall /db/oracle/product/11.2.0/db_1/
    [root@db ~]# chmod -R 755 /db/oracle/

### Profile

    [root@db ~]# su - oracle
    
    [oracle@db ~]$ vim .bash_profile 
    # .bash_profile

    # Get the aliases and functions
    if [ -f ~/.bashrc ]; then
        . ~/.bashrc
    fi

    # User specific environment and startup programs

    PATH=$PATH:$HOME/bin

    export PATH

    #2013-7-19 settings for oracle database 11gR2
    export TMP=/tmp
    export TMPDIR=$TMP
    export ORACLE_BASE=/db/oracle
    export ORACLE_HOME=/db/oracle/product/11.2.0/db_1
    export ORACLE_SID=WXPROD
    export PATH=$PATH:$ORACLE_HOME/bin

### X Window System
### nobody

### Install Database

logon as oracle with X Manager or VNC

## Basic Security

### root account

    [root@db ~]# useradd itsection
    [root@db ~]# usermod -G 10 itsection
    [root@db ~]# grep '^\s*[^# \t].*$' /etc/pam.d/su
    auth		sufficient	pam_rootok.so
    auth		required	pam_wheel.so use_uid
    auth		include		system-auth
    account		sufficient	pam_succeed_if.so uid = 0 use_uid quiet
    account		include		system-auth
    password	include		system-auth
    session		include		system-auth
    session		optional	pam_xauth.so

itsection su test    
   
    [itsection@db ~]$ su - 
    Password: 
    [root@db ~]# 
    
oracle su test

    [oracle@db ~]$ su - 
    Password: 
    su: incorrect password
    [oracle@db ~]$ 
    
### ssh

use oracle as example

    [root@db ~]# su - oracle
    
    [oracle@db ~]$ ssh-keygen 
    Generating public/private rsa key pair.
    Enter file in which to save the key (/home/oracle/.ssh/id_rsa): 
    Enter passphrase (empty for no passphrase): 
    Enter same passphrase again: 
    Passphrases do not match.  Try again.
    Enter passphrase (empty for no passphrase): 
    Enter same passphrase again: 
    Your identification has been saved in /home/oracle/.ssh/id_rsa.
    Your public key has been saved in /home/oracle/.ssh/id_rsa.pub.
    The key fingerprint is:
    e5:d9:51:82:97:d9:4d:11:1a:57:af:aa:5f:f0:36:e6 oracle@db.egolife.com
    The key's randomart image is:
    +--[ RSA 2048]----+
    |           ..=.**|
    |          . +o= o|
    |          ....  .|
    |         o o . . |
    |        S o o .  |
    |             +   |
    |            . *  |
    |           . = . |
    |          ... E  |
    +-----------------+
    [oracle@db ~]$ cd .ssh/
    [oracle@db .ssh]$ ll
    total 8
    -rw-------. 1 oracle oinstall 1743 Jul 22 09:23 id_rsa
    -rw-r--r--. 1 oracle oinstall  406 Jul 22 09:23 id_rsa.pub
    [oracle@db .ssh]$ cat id_rsa.pub > authorized_keys
    [oracle@db .ssh]$ chmod 600 authorized_keys 
    [root@db ~]# grep '^\s*[^# \t].*$' /etc/ssh/sshd_config 
    Protocol 2
    SyslogFacility AUTHPRIV
    PermitRootLogin no
    PasswordAuthentication no
    ChallengeResponseAuthentication no
    GSSAPIAuthentication yes
    GSSAPICleanupCredentials yes
    UsePAM yes
    AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
    AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
    AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE
    AcceptEnv XMODIFIERS
    X11Forwarding yes
    Subsystem	sftp	/usr/libexec/openssh/sftp-server
    [root@db ~]# service sshd restart
    
### iptables

    [root@db ~]# cat /etc/iptables.rule 
    # Generated by iptables-save v1.4.7 on Mon Jul 22 09:56:41 2013
    *filter
    :INPUT ACCEPT [0:0]
    :FORWARD ACCEPT [0:0]
    :OUTPUT ACCEPT [0:0]
    -A INPUT -i lo -j ACCEPT 
    -A INPUT -d 127.0.0.0/8 -j REJECT --reject-with icmp-port-unreachable 
    -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT 
    -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT 
    -A INPUT -p tcp -m tcp --dport 8080 -j ACCEPT 
    -A INPUT -s 172.29.73.0/24 -p tcp -m tcp --dport 25 -j ACCEPT 
    -A INPUT -s 172.29.73.0/24 -p tcp -m tcp --dport 1521 -j ACCEPT 
    -A INPUT -s 172.29.73.0/24 -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT 
    -A INPUT -s 172.29.88.135/32 -j ACCEPT 
    -A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT 
    -A INPUT -m limit --limit 5/min -j LOG --log-prefix "iptables denied: " --log-level 7 
    -A INPUT -j REJECT --reject-with icmp-port-unreachable 
    -A FORWARD -j REJECT --reject-with icmp-port-unreachable 
    -A OUTPUT -j ACCEPT 
    COMMIT
    # Completed on Mon Jul 22 09:56:41 2013
    # Generated by iptables-save v1.4.7 on Mon Jul 22 09:56:41 2013
    *nat
    :PREROUTING ACCEPT [1:64]
    :POSTROUTING ACCEPT [12:1378]
    :OUTPUT ACCEPT [12:1378]
    -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8080 
    COMMIT
    # Completed on Mon Jul 22 09:56:41 2013

    [root@db ~]# iptables-restore < /etc/iptables.rule 
    [root@db ~]# iptables-save > /etc/iptables.rule
    
    [root@db ~]# cat /etc/rc.local 
    #!/bin/sh
    #
    # This script will be executed *after* all the other init scripts.
    # You can put your own initialization stuff in here if you don't
    # want to do the full Sys V style init stuff.

    touch /var/lock/subsys/local
    su - c "iptables-restore < /etc/iptables.rule"
    
---
layout: post
title: Configure Postfix, Cyrus-SASL and MySQL
category : Linux
tags : [Linux, Utilities, Mail]
---

### OS Configuration

kernel

    [dev@dev ~]$ uname -a
    Linux oa.egolife.com 2.6.32-220.el6.x86_64 #1 SMP Tue Dec 6 19:48:22 GMT 2011 x86_64 x86_64 x86_64 GNU/Linux
    
account

    [root@dev ~]# useradd -s /sbin/nologin me
    [root@dev ~]# passwd me
    Changing password for user me.
    New password: 
    Retype new password: 
    passwd: all authentication tokens updated successfully.
    
## postfix sasl mysql

required packages

    [dev@dev ~]$ rpm -qa | egrep 'postfix|sasl|mysql'
    cyrus-sasl-sql-2.1.23-13.el6.x86_64
    cyrus-sasl-ldap-2.1.23-13.el6.x86_64
    cyrus-sasl-md5-2.1.23-13.el6.x86_64
    cyrus-sasl-gssapi-2.1.23-13.el6.x86_64
    mysql-5.1.52-1.el6_0.1.x86_64
    mysql-libs-5.1.52-1.el6_0.1.x86_64
    mysql-devel-5.1.52-1.el6_0.1.x86_64
    cyrus-sasl-lib-2.1.23-13.el6.x86_64
    cyrus-sasl-plain-2.1.23-13.el6.x86_64
    mysql-server-5.1.52-1.el6_0.1.x86_64
    cyrus-sasl-2.1.23-13.el6.x86_64
    cyrus-sasl-devel-2.1.23-13.el6.x86_64
    postfix-2.6.6-2.2.el6_1.x86_64

config postfix
    
    postconf -e 'home_mailbox = Maildir/'
    postconf -e 'smtpd_sasl_path = postfix'
    postconf -e 'smtpd_sasl_security_options = noanonymous'
    postconf -e 'broken_sasl_auth_clients = yes'
    postconf -e 'smtpd_sasl_auth_enable = yes'
    postconf -e 'smtpd_recipient_restrictions = permit_sasl_authenticated, permit_mynetworks,reject_unauth_destination'
    postconf -e 'inet_interfaces = all'
    postconf -e 'mynetworks = 172.31.0.0/16, 127.0.0.0/8'
    postconf -e 'myorigin = egolife.com'

    [dev@dev ~]$ grep '^\s*[^# \t].*$' /etc/postfix/main.cf 
    queue_directory = /var/spool/postfix
    command_directory = /usr/sbin
    daemon_directory = /usr/libexec/postfix
    data_directory = /var/lib/postfix
    mail_owner = postfix
    myhostname = oa.egolife.com
    myorigin = egolife.com
    inet_interfaces = all
    inet_protocols = all
    mydestination = $myhostname, localhost.$mydomain, localhost
    unknown_local_recipient_reject_code = 550
    mynetworks = 172.31.0.0/16, 127.0.0.0/8
    alias_maps = hash:/etc/aliases
    alias_database = hash:/etc/aliases
    home_mailbox = Maildir/
    debug_peer_level = 7
    debugger_command =
         PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin
         ddd $daemon_directory/$process_name $process_id & sleep 5
    sendmail_path = /usr/sbin/sendmail.postfix
    newaliases_path = /usr/bin/newaliases.postfix
    mailq_path = /usr/bin/mailq.postfix
    setgid_group = postdrop
    html_directory = no
    manpage_directory = /usr/share/man
    sample_directory = /usr/share/doc/postfix-2.6.6/samples
    readme_directory = /usr/share/doc/postfix-2.6.6/README_FILES
    smtpd_recipient_restrictions = permit_sasl_authenticated, permit_mynetworks,reject_unauth_destination
    smtpd_sasl_auth_enable      =  yes
    smtpd_sasl_security_options = noanonymous
    smtpd_sasl_path             = postfix
    broken_sasl_auth_clients    =  yes
    smtpd_sasl_authenticated_header =  yes

config sasl2

    [dev@dev ~]$ cat /etc/sasl2/postfix.conf
    log_level: 5
    pwcheck_method: auxprop
    auxprop_plugin: sql
    mech_list: CRAM-MD5 PLAIN LOGIN
    sql_engine: mysql
    sql_hostnames: localhost
    sql_user: email
    sql_passwd: email
    sql_database: email
    sql_verbose: yes
    sql_select: SELECT clear_password FROM users WHERE username='%u@%r'  AND active=1
    
config mysql

    [root@dev ~]# mysql -uroot -p
    Enter password: 
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 240512
    Server version: 5.1.52-log Source distribution

    Copyright (c) 2000, 2010, Oracle and/or its affiliates. All rights reserved.
    This software comes with ABSOLUTELY NO WARRANTY. This is free software,
    and you are welcome to modify and redistribute it under the GPL v2 license

    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

    mysql> show databases;
    +--------------------+
    | Database           |
    +--------------------+
    | information_schema |
    | email              |
    | mysql              |
    | test               |
    +--------------------+
    4 rows in set (0.07 sec)

    mysql> use email;
    Reading table information for completion of table and column names
    You can turn off this feature to get a quicker startup with -A

    Database changed
    mysql> show tables;
    +-----------------+
    | Tables_in_email |
    +-----------------+
    | users           |
    +-----------------+
    1 row in set (0.00 sec)

    mysql> desc users;
    +----------------+--------------+------+-----+-------------------+-------+
    | Field          | Type         | Null | Key | Default           | Extra |
    +----------------+--------------+------+-----+-------------------+-------+
    | username       | varchar(255) | NO   | PRI | NULL              |       |
    | password       | varchar(255) | NO   |     | $$$$$$            |       |
    | clear_password | varchar(255) | NO   |     | 888888            |       |
    | maildir        | varchar(255) | NO   |     |                   |       |
    | mailquota      | int(11)      | NO   |     | 20                |       |
    | created        | timestamp    | NO   |     | CURRENT_TIMESTAMP |       |
    | modified_by    | varchar(255) | NO   |     |                   |       |
    | active         | int(11)      | NO   |     | 1                 |       |
    +----------------+--------------+------+-----+-------------------+-------+
    8 rows in set (0.01 sec)
    
    mysql> select * from users;
    +-------------------------+----------+----------------+---------+-----------+---------------------+-------------+--------+
    | username                | password | clear_password | maildir | mailquota | created             | modified_by | active |
    +-------------------------+----------+----------------+---------+-----------+---------------------+-------------+--------+
    | Dev@dev.egolife.com    | $$$$$$   | dbVm116#       |         |        20 | 2012-12-19 11:32:41 |             |      1 |
    | Mailer@dev.egolife.com | $$$$$$   | 6$!Xxuw4       |         |        20 | 2012-12-19 10:44:12 |             |      1 |
    +-------------------------+----------+----------------+---------+-----------+---------------------+-------------+--------+
    2 rows in set (0.00 sec)
    
    mysql> grant all on email.* to email@'%' identified by 'password';
  
## chkconfig and start service
  
chkconfig 
    
    [root@db ~]# chkconfig postfix on
    [root@db ~]# chkconfig saslauthd on
    [root@db ~]# chkconfig mysqld on

start service

    [root@db ~]# service postfix restart
    Shutting down postfix:                                     [  OK  ]
    Starting postfix:                                          [  OK  ]
    [root@db ~]# service saslauthd restart
    Stopping saslauthd Imap:                                     [  OK  ]
    Starting saslauthd Imap:                                     [  OK  ]
    
## mail test

script 

    F:\Workspace\python\pydev\mail\eml\mail.py

client 

    cd /d  F:\Workspace\python\pydev\mail\eml\
    python mail.py
    send mail successfully!
    
server

    [root@db ~]# tail -f /var/log/maillog
    Dec 19 17:57:34 oa postfix/smtpd[4397]: connect from unknown[192.168.1.1]
    Dec 19 17:57:34 oa postfix/smtpd[4397]: C683287A03: client=unknown[192.168.1.1], sasl_method=CRAM-MD5, sasl_username=me
    Dec 19 17:57:34 oa postfix/cleanup[4404]: C683287A03: message-id=<>
    Dec 19 17:57:34 oa postfix/qmgr[2159]: C683287A03: from=<me@egolife.com>, size=4331, nrcpt=1 (queue active)
    Dec 19 17:57:34 oa postfix/smtpd[4397]: disconnect from unknown[192.168.1.1]
    Dec 19 17:57:35 oa postfix/smtp[4405]: C683287A03: to=<me@egolife.com>, relay=smtp.egolife.com[192.168.1.10]:25, delay=0.28, delays=0.04/0/0.02/0.22, dsn=2.0.0, status=sent (250 Message queued)
    Dec 19 17:57:35 oa postfix/qmgr[2159]: C683287A03: removed
	
##Reference

* [postfix](http://www.postfix.org/)
* [SASL](http://asg.web.cmu.edu/sasl/)
---
layout: post
title: Configure Posfix  with Dovecot
category : Linux
tags : [Linux, Utilities, Mail]
---

### OS Configuration

kernel

    [root@db ~]# uname -a
    Linux db.egolife.com 2.6.32-220.el6.x86_64 #1 SMP Tue Dec 6 19:48:22 GMT 2011 x86_64 x86_64 x86_64 GNU/Linux

account

    [root@db ~]# useradd -s /sbin/nologin me
    [root@db ~]# passwd me
    Changing password for user me.
    New password: 
    Retype new password: 
    passwd: all authentication tokens updated successfully.
    

### postfix and dovecot 

required packages

    [root@db ~]# rpm -qa | egrep 'sendmail|postfix|dovecot'
    dovecot-2.0.9-2.el6_1.1.x86_64
    postfix-2.6.6-2.2.el6_1.x86_64

config postfix
    
    postconf -e 'home_mailbox = Maildir/'
    postconf -e 'smtpd_sasl_type = dovecot'
    postconf -e 'smtpd_sasl_path = private/auth'
    postconf -e 'smtpd_sasl_security_options = noanonymous'
    postconf -e 'broken_sasl_auth_clients = yes'
    postconf -e 'smtpd_sasl_auth_enable = yes'
    postconf -e 'smtpd_recipient_restrictions = permit_sasl_authenticated, permit_mynetworks,reject_unauth_destination'
    postconf -e 'inet_interfaces = all'
    postconf -e 'mynetworks = 127.0.0.0/8, 172.17.1.100/32, 172.29.73.0/24'
    postconf -e 'myorigin = egolife.com'

    [root@db ~]#  grep '^\s*[^# \t].*$' /etc/postfix/main.cf 
    queue_directory = /var/spool/postfix
    command_directory = /usr/sbin
    daemon_directory = /usr/libexec/postfix
    data_directory = /var/lib/postfix
    mail_owner = postfix
    myorigin = egolife.com
    inet_interfaces = all
    inet_protocols = all
    mydestination = $myhostname, localhost.$mydomain, localhost
    unknown_local_recipient_reject_code = 550
    mynetworks = 127.0.0.0/8, 172.17.1.100/32, 172.29.73.0/24
    alias_maps = hash:/etc/aliases
    alias_database = hash:/etc/aliases
    debug_peer_level = 2
    debugger_command =
         PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin
         ddd $daemon_directory/$process_name $process_id & sleep 5
    sendmail_path = /usr/sbin/sendmail.postfix
    newaliases_path = /usr/bin/newaliases.postfix
    mailq_path = /usr/bin/mailq.postfix
    setgid_group = postdrop
    html_directory = no
    manpage_directory = /usr/share/man
    sample_directory = /usr/share/doc/postfix-2.6.6/samples
    readme_directory = /usr/share/doc/postfix-2.6.6/README_FILES
    home_mailbox = Maildir/
    smtpd_sasl_type = dovecot
    smtpd_sasl_path = private/auth
    smtpd_sasl_security_options = noanonymous
    broken_sasl_auth_clients = yes
    smtpd_sasl_auth_enable = yes
    smtpd_recipient_restrictions = permit_sasl_authenticated, permit_mynetworks,reject_unauth_destination

config dovecot
    
dovecot.conf
    
    [root@db ~]# vim /etc/dovecot/dovecot.conf 
    ... ...
    listen = 127.0.0.1, 172.17.1.100
    mail_location = maildir:~/mail
    ... ...

    [root@db ~]#  grep '^\s*[^# \t].*$' /etc/dovecot/dovecot.conf 
    listen = 127.0.0.1, 172.17.1.100
    mail_location = maildir:~/mail
    dict {
    }
    !include conf.d/*.conf

10-master.conf
    
    [root@db ~]# vim /etc/dovecot/conf.d/10-master.conf 
    ... ...
    service auth {
      # auth_socket_path points to this userdb socket by default. It's typically
      # used by dovecot-lda, doveadm, possibly imap process, etc. Its default
      # permissions make it readable only by root, but you may need to relax these
      # permissions. Users that have access to this socket are able to get a list
      # of all usernames and get results of everyone's userdb lookups.
      unix_listener auth-userdb {
        #mode = 0600
        #user = 
        #group = 
      }

      # Postfix smtp-auth
      unix_listener /var/spool/postfix/private/auth {
        mode = 0666
      }

      # Auth process is run as this user.
      #user = $default_internal_user
    }

    [root@db ~]#  grep '^\s*[^# \t].*$' /etc/dovecot/conf.d/10-master.conf 
    service imap-login {
      inet_listener imap {
      }
      inet_listener imaps {
      }
    }
    service pop3-login {
      inet_listener pop3 {
      }
      inet_listener pop3s {
      }
    }
    service lmtp {
      unix_listener lmtp {
      }
    }
    service imap {
    }
    service pop3 {
    }
    service auth {
      unix_listener auth-userdb {
      }
      unix_listener /var/spool/postfix/private/auth {
        mode = 0666
      }
    }
    service auth-worker {
    }
    service dict {
      unix_listener dict {
      }
    }

10-auth.conf
    
    [root@db ~]# vim /etc/dovecot/conf.d/10-auth.conf 
    ... ...
    disable_plaintext_auth = no
    ... ...
    auth_mechanisms = plain login
    ... ...
    passdb {
        driver = shadow
    }
    ... ...

    [root@db ~]#  grep '^\s*[^# \t].*$' /etc/dovecot/conf.d/10-auth.conf 
    disable_plaintext_auth = no
    auth_mechanisms = plain login
    passdb {
        driver = shadow
    }
    !include auth-system.conf.ext

### chkconfig and start service
    
chkconfig 
    
    [root@db ~]# chkconfig postfix on
    [root@db ~]# chkconfig dovecot on

start service

    [root@db ~]# service postfix restart
    Shutting down postfix:                                     [  OK  ]
    Starting postfix:                                          [  OK  ]
    [root@db ~]# service dovecot restart
    Stopping Dovecot Imap:                                     [  OK  ]
    Starting Dovecot Imap:                                     [  OK  ]
    
### mail test

script 

    F:\Workspace\python\pydev\mail\eml\mail.py

client 

    cd /d  F:\Workspace\python\pydev\mail\eml\
    python mail.py
    send mail successfully!
    
server log

    [root@db ~]# tail -f /var/log/maillog
    Jul 19 17:35:09 db postfix/smtpd[16361]: connect from unknown[192.168.1.1]
    Jul 19 17:35:09 db postfix/smtpd[16361]: 160E68047E: client=unknown[192.168.1.1], sasl_method=PLAIN, sasl_username=me
    Jul 19 17:35:09 db postfix/cleanup[16370]: 160E68047E: message-id=<>
    Jul 19 17:35:09 db postfix/qmgr[16304]: 160E68047E: from=<me@egolife.com>, size=4298, nrcpt=1 (queue active)
    Jul 19 17:35:09 db postfix/smtpd[16361]: disconnect from unknown[192.168.1.1]
    Jul 19 17:35:09 db postfix/smtp[16371]: connect to gmail-smtp-in.l.google.com[2607:f8b0:400e:c03::1a]:25: Network is unreachable
    Jul 19 17:35:13 db postfix/smtp[16371]: 160E68047E: to=<dylanninin@gmail.com>, relay=gmail-smtp-in.l.google.com[74.125.25.26]:25, delay=4.2, delays=0.04/0/0.93/3.2, dsn=2.0.0, status=sent (250 2.0.0 OK 1374226522 pf2si10055347pac.244 - gsmtp)
    Jul 19 17:35:13 db postfix/qmgr[16304]: 160E68047E: removed

##Reference

* [postfix](http://www.postfix.org/)
* [dovecot](http://www.dovecot.org/)
---
layout: post
title: A Case of TightVNC Exception
category : Linux
tags : [Linux, Utilities, Exception]
---

configure tightvnc

    [root@db ~]# su - oracle

    [oracle@db ~]$ vncpasswd 
    Password:
    Verify:
	
start vncserver
	
    [oracle@db ~]$ vncserver 

    WARNING: The first attempt to start Xvnc failed, possibly because the font
    catalog is not properly configured.  Attempting to determine an appropriate
    font path for this system and restart Xvnc using that font path ...
    Could not start Xvnc.


    Xvnc TigerVNC 1.1.0 - built Feb 22 2013 22:28:37
    Copyright (C) 1999-2011 TigerVNC Team and many others (see README.txt)
    See http://www.tigervnc.org for information on TigerVNC.
    Underlying X server release 11300000, The X.Org Foundation

    Initializing built-in extension Generic Event Extension
    Initializing built-in extension SHAPE
    Initializing built-in extension MIT-SHM
    Initializing built-in extension XInputExtension
    Initializing built-in extension XTEST
    Initializing built-in extension BIG-REQUESTS
    Initializing built-in extension SYNC
    Initializing built-in extension XKEYBOARD
    Initializing built-in extension XC-MISC
    Initializing built-in extension XFIXES
    Initializing built-in extension RENDER
    Initializing built-in extension RANDR
    Initializing built-in extension DAMAGE
    Initializing built-in extension MIT-SCREEN-SAVER
    Initializing built-in extension DOUBLE-BUFFER
    Initializing built-in extension RECORD
    Initializing built-in extension DPMS
    Initializing built-in extension X-Resource
    Initializing built-in extension XVideo
    Initializing built-in extension XVideo-MotionCompensation
    Initializing built-in extension VNC

    Fri Jul 19 11:32:29 2013
     vncext:      VNC extension running!
     vncext:      Listening for VNC connections on all interface(s), port 5901
     vncext:      created VNC server for screen 0
    [dix] Could not init font path element catalogue:/etc/X11/fontpath.d, removing from list!
    [dix] Could not init font path element built-ins, removing from list!

    Fatal server error:
    could not open default font 'fixed'

    Xvnc TigerVNC 1.1.0 - built Feb 22 2013 22:28:37
    Copyright (C) 1999-2011 TigerVNC Team and many others (see README.txt)
    See http://www.tigervnc.org for information on TigerVNC.
    Underlying X server release 11300000, The X.Org Foundation

    Initializing built-in extension Generic Event Extension
    Initializing built-in extension SHAPE
    Initializing built-in extension MIT-SHM
    Initializing built-in extension XInputExtension
    Initializing built-in extension XTEST
    Initializing built-in extension BIG-REQUESTS
    Initializing built-in extension SYNC
    Initializing built-in extension XKEYBOARD
    Initializing built-in extension XC-MISC
    Initializing built-in extension XFIXES
    Initializing built-in extension RENDER
    Initializing built-in extension RANDR
    Initializing built-in extension DAMAGE
    Initializing built-in extension MIT-SCREEN-SAVER
    Initializing built-in extension DOUBLE-BUFFER
    Initializing built-in extension RECORD
    Initializing built-in extension DPMS
    Initializing built-in extension X-Resource
    Initializing built-in extension XVideo
    Initializing built-in extension XVideo-MotionCompensation
    Initializing built-in extension VNC

    Fri Jul 19 11:32:32 2013
     vncext:      VNC extension running!
     vncext:      Listening for VNC connections on all interface(s), port 5901
     vncext:      created VNC server for screen 0
    [dix] Could not init font path element /usr/share/X11/fonts/misc, removing from list!
    [dix] Could not init font path element /usr/share/X11/fonts/100dpi, removing from list!
    [dix] Could not init font path element /usr/share/X11/fonts/Type1, removing from list!
    [dix] Could not init font path element /usr/share/fonts/default/Type1, removing from list!
    [dix] Could not init font path element built-ins, removing from list!

    Fatal server error:
    could not open default font 'fixed'

reinstall vncserver
    
    [root@db yum.repos.d]# rpm -qa | grep vnc
    tigervnc-server-1.1.0-5.el6.x86_64
    [root@db yum.repos.d]# rpm -e tigervnc-server-1.1.0-5.el6.x86_64

    [root@db sofware]# yum groupinstall -y "X Window System"
    [root@db sofware]# yum -y install tigervnc-server pixman pixman-devel libXfont
   
config and start vncserver
    
    [root@db sofware]# chkconfig vncserver on
    [root@db sofware]# vncpasswd 
    Password:
    Verify:
    [root@db sofware]# vncserver 

    New 'db.egolife.com:1 (root)' desktop is db.egolife.com:1

    Creating default startup script /root/.vnc/xstartup
    Starting applications specified in /root/.vnc/xstartup
    Log file is /root/.vnc/db.egolife.com:1.log
---
layout: post
title: Oracle ROWID
category : Oracle
tags : [Oracle, Database, DBA]
---

##Rowid Data Types

Every row stored in the database has an address. Oracle Database uses a  ROWID  data type to store the address (rowid) of every row in the database. Rowids fall into the following categories:

 * Physical rowids store the addresses of rows in heap-organized tables, table clusters, and table and index partitions.
 * Logical rowids store the addresses of rows in index-organized tables.
 * Foreign rowids are identifiers in foreign tables, such as DB2 tables accessed through a gateway. They are not standard Oracle Database rowids.

A data type called the universal rowid, or  UROWID , supports all kinds of rowids.

##Use of Rowids  

Oracle Database uses rowids internally for the construction of indexes. 

A B-tree index, which is the most common type, contains an ordered list of keys divided into ranges. Each key is associated with a rowid that points to the associated row's address for fast access. End users and application developers can also use rowids for several important functions:

 * Rowids are the fastest means of accessing particular rows.
 * Rowids provide the ability to see how a table is organized.
 * Rowids are unique identifiers for rows in a given table.

You can also create tables with columns defined using the  ROWID  data type. For example, you can define an exception table with a column of data type  ROWID  to store the rowids of rows that violate integrity constraints. Columns defined using the  ROWID data type behave like other table columns: values can be updated, and so on.

##ROWID Pseudocolumn  

Every table in an Oracle database has a pseudocolumn named ROWID . A pseudocolumn behaves like a table column, but is not actually stored in the table. You can select from pseudocolumns, but you cannot insert, update, or delete their values. A pseudocolumn is also similar to a SQL function without arguments. Functions without arguments typically return the same value for every row in the result set, whereas pseudocolumns typically return a different value for each row. Values of the  ROWID  pseudocolumn are strings representing the address of each row. These strings have the data type  ROWID . This pseudocolumn is not evident when listing the structure of a table by executing  SELECT  or  DESCRIBE , nor does the pseudocolumn consume space. However, the rowid of each row can be retrieved with a SQL query using the reserved word  ROWID  as a column name.


##Rowid Format  

Oracle Database uses a rowid to uniquely identify a row. Internally, the rowid is a structure that holds information that the database needs to access a row. A rowid is not physically stored in the database, but is inferred from the file and block on which the data is stored.

An extended rowid includes a data object number. This rowid type uses a base 64 encoding of the physical address for each row. The encoding characters are  A-Z ,  a-z , 0-9 ,  + , and  / . 

After a rowid is assigned to a row piece, the rowid can change in special circumstances. For example, if row movement is enabled, then the rowid can change because of partition key updates, Flashback Table operations, shrink table operations, and so on. If row movement is disabled, then a rowid can change if the row is exported and imported using Oracle Database utilities.


    --rowid
    ROWID	            Object	File	Block	Row
    AAAVYeAAMAAAAMcAAk	AAAVYe	AAM	    AAAAMc	AAk


    --查询RowID信息 10进制
    select dbms_rowid.rowid_object('&rowid')       data_object_id,
           dbms_rowid.rowid_relative_fno('&rowid') data_file_id,
           dbms_rowid.rowid_block_number('&rowid') data_block_id,
           dbms_rowid.rowid_row_number('&rowid')   data_num
      FROM dual;
	--output
    DATA_OBJECT_ID	DATA_FILE_ID	DATA_BLOCK_ID	DATA_NUM
    87582	        12	            796	            36

	
##The Base64 Alphabet
	
                    Table 1: The Base64 Alphabet

     Value Encoding  Value Encoding  Value Encoding  Value Encoding
         0 A            17 R            34 i            51 z
         1 B            18 S            35 j            52 0
         2 C            19 T            36 k            53 1
         3 D            20 U            37 l            54 2
         4 E            21 V            38 m            55 3
         5 F            22 W            39 n            56 4
         6 G            23 X            40 o            57 5
         7 H            24 Y            41 p            58 6
         8 I            25 Z            42 q            59 7
         9 J            26 a            43 r            60 8
        10 K            27 b            44 s            61 9
        11 L            28 c            45 t            62 +
        12 M            29 d            46 u            63 /
        13 N            30 e            47 v
        14 O            31 f            48 w         (pad) =
        15 P            32 g            49 x
        16 Q            33 h            50 y
	
##Reference

* Oracle Database Administrator's Guide
* [The Base16, Base32, and Base64 Data Encodings](http://tools.ietf.org/html/rfc4648)
---
layout: post
title: Configure cx_Oracle on CentOS
category : Python
tags : [Python, Database, Libs]
---

uname

    [root@oradb]~# uname -a
    Linux oradb.egolife.com 2.6.32-220.el6.x86_64 #1 SMP Tue Dec 6 19:48:22 GMT 2011 x86_64 x86_64 x86_64 GNU/Linux

python version
	
    [root@oradb]~# python     
    Python 2.6.6 (r266:84292, Dec  7 2011, 20:48:22) 
    [GCC 4.4.6 20110731 (Red Hat 4.4.6-3)] on linux2
    Type "help", "copyright", "credits" or "license" for more information.
    >>> exit()

cx_Oracle package
	
    [root@oradb]~# ll software 
    total 276
    -rw-r--r--. 1 root   root     275117 Jul 25 10:16 cx_Oracle-5.1.2-11g-py26-1.x86_64.rpm
    drwxr-xr-x. 8 oracle oinstall   4096 Aug 21  2009 database

install cx_Oracle
	
    [root@oradb]~# rpm -ihv software/cx_Oracle-5.1.2-11g-py26-1.x86_64.rpm 
    Preparing...                ########################################### [100%]
       1:cx_Oracle              ########################################### [100%]

cx_Oracle test
	   
    [root@oradb]~# su - oracle
    [oracle@oradb]~# python
    Python 2.6.6 (r266:84292, Dec  7 2011, 20:48:22) 
    [GCC 4.4.6 20110731 (Red Hat 4.4.6-3)] on linux2
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import cx_Oracle
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    ImportError: libclntsh.so.11.1: cannot open shared object file: No such file or directory
    >>> exit()

    [oracle@oradb]~# locate libclntsh.so.11.1
    /db/oracle/product/11.2.0/db_1/inventory/Scripts/ext/lib/libclntsh.so.11.1
    /db/oracle/product/11.2.0/db_1/inventory/backup/2012-11-30_04-38-40AM/Scripts/ext/lib/libclntsh.so.11.1
    /db/oracle/product/11.2.0/db_1/lib/libclntsh.so.11.1
    /root/software/database/stage/ext/lib/libclntsh.so.11.1

    [oracle@oradb ~]$ cat ~/.bash_profile 
    # .bash_profile

    # Get the aliases and functions
    if [ -f ~/.bashrc ]; then
        . ~/.bashrc
    fi

    # User specific environment and startup programs

    PATH=$PATH:$HOME/bin

    export PATH

    #2013-1-22	oracle 11gR2 settings
    export TMP=/tmp
    export TMPDIR=$TMP
    export ORACLE_BASE=/db/oracle
    export ORACLE_HOME=/db/oracle/product/11.2.0/db_1
    export ORACLE_SID=DBTEST
    export LD_LIBRARY_PATH=/db/oracle/product/11.2.0/db_1/lib
    export PATH=$PATH:$ORACLE_HOME/bin

    alias sqlplus='rlwrap sqlplus'
	
    [oracle@oradb ~]$ python
    Python 2.6.6 (r266:84292, Dec  7 2011, 20:48:22) 
    [GCC 4.4.6 20110731 (Red Hat 4.4.6-3)] on linux2
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import cx_Oracle
    >>> connection = cx_Oracle.connect('dev/dev@dbtest')
    >>> connection = cx_Oracle.connect('dev/dev@172.29.88.164:1521:/dbtest')
    >>> connection.version
    '11.2.0.1.0'
	
##Reference

* [Python cx_Oracle 5.0 New Features Overview](http://www.oracle.com/technetwork/articles/tuininga-cx-oracle-084866.html)
* [cx_Oracle Tutorial](http://dbaportal.eu/sidekicks/sidekick-cx_oracle-code-paterns/)


---
layout: post
title: A Case of Java Headless Exception
category : Java
tags : [Java, Exception]
---

##reproduce exception
    
    [dev@db ~]$ java -version
    java version "1.6.0_33"
    Java(TM) SE Runtime Environment (build 1.6.0_33-b03)
    Java HotSpot(TM) 64-Bit Server VM (build 20.8-b03, mixed mode)
    
    [dev@db ~]$ pwd
    /apps/test/

    [dev@db ~]$ ls -l
    total 40
    drwxrwxr-x. 8 db db 4096 Jul 24 16:21 bin
    drwxrwxr-x. 6 db db 4096 Jul 24 16:21 file
    -rw-rw-r--. 1 db db 1105 Jul 24 16:21 index.htm
    -rw-rw-r--. 1 db db  836 Jul 24 16:21 index.jsp
    drwxrwxr-x. 2 db db 4096 Jul 24 16:21 jsp
    drwxrwxr-x. 2 db db 4096 Jul 24 16:21 logs
    drwxrwxr-x. 2 db db 4096 Jul 24 16:20 META-INF
    -rw-r--r--. 1 db db 4508 Jul 25 15:28 ReportTest.java
    drwxrwxr-x. 7 db db 4096 Jul 25 15:36 WEB-INF
	
exception
	
    [dev@db ~]$ javac ReportTest.java 
    [dev@db ~]$ java ReportTest
    ... ...
    3154 [main] INFO org.hibernate.impl.SessionFactoryObjectFactory - Not binding factory to JNDI, no JNDI name configured
    Exception in thread "main" java.lang.InternalError: Can't connect to X11 window server using 'localhost:10.0' as the value of the DISPLAY variable.
        at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
        at sun.awt.X11GraphicsEnvironment.access$100(X11GraphicsEnvironment.java:52)
        at sun.awt.X11GraphicsEnvironment$1.run(X11GraphicsEnvironment.java:155)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:131)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:68)
        at net.sf.jasperreports.engine.util.JRStyledTextParser.<clinit>(JRStyledTextParser.java:83)
        at net.sf.jasperreports.engine.fill.JRBaseFiller.<init>(JRBaseFiller.java:174)
        at net.sf.jasperreports.engine.fill.JRVerticalFiller.<init>(JRVerticalFiller.java:74)
        at net.sf.jasperreports.engine.fill.JRVerticalFiller.<init>(JRVerticalFiller.java:56)
        at net.sf.jasperreports.engine.fill.JRFiller.createFiller(JRFiller.java:143)
        at net.sf.jasperreports.engine.fill.JRFiller.fillReport(JRFiller.java:53)
        at net.sf.jasperreports.engine.JasperFillManager.fillReport(JasperFillManager.java:417)
        at ReportTest.main(ReportTest.java:77)
        
        
    [dev@db ~]$ java  -Djava.awt.headless=true ReportTest
    ... ...
    3154 [main] INFO org.hibernate.impl.SessionFactoryObjectFactory - Not binding factory to JNDI, no JNDI name configured
    [B@65de41c3

set profile and restart tomcat
    
    [dev@db ~]$ cat ~/.bash_profile 
    # .bash_profile

    # Get the aliases and functions
    if [ -f ~/.bashrc ]; then
        . ~/.bashrc
    fi

    # User specific environment and startup programs
    export CATALINA_HOME=/apps/test/tomcat
    PATH=$PATH:$HOME/bin:$CATALINA_HOME/bin

    export PATH
	
    alias java='java -Djava.awt.headless=true'
    unset DISPLAY

    [dev@db ~]$ ps -ef | grep tomcat
    db     4003     1  1 14:53 ?        00:00:46 /usr/java/jdk1.6.0_33/bin/java -Djava.util.logging.config.file=/apps/test/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.endorsed.dirs=/apps/test/tomcat/endorsed -classpath /apps/test/tomcat/bin/bootstrap.jar -Dcatalina.base=/apps/db/tomcat -Dcatalina.home=/apps/db/tomcat -Djava.io.tmpdir=/apps/test/tomcat/temp org.apache.catalina.startup.Bootstrap start
    db    13305  6465  0 15:56 pts/3    00:00:00 grep tomcat
    [dev@db ~]$ kill -9 4003
    [dev@db ~]$ catalina.sh start
    Using CATALINA_BASE:   /apps/db/tomcat
    Using CATALINA_HOME:   /apps/db/tomcat
    Using CATALINA_TMPDIR: /apps/test/tomcat/temp
    Using JRE_HOME:        /usr/java/jdk1.6.0_33
    Using CLASSPATH:       /apps/test/tomcat/bin/bootstrap.jar
    
##awt test
   
source code
   
    [root@db sofware]# cat GETest.java 
    import java.awt.GraphicsEnvironment;

    public class GETest{

        public static void main(String[] args){

            GraphicsEnvironment ge = GraphicsEnvironment.getLocalGraphicsEnvironment();
            System.out.println("class: " + ge.getClass());
            System.out.println("isHeadless: " + ge.isHeadless());
        }

    }
	
compile and run test
	
    [root@db sofware]# javac GETest.java 
    [root@db sofware]# java GETest
    Exception in thread "main" java.lang.InternalError: Can't connect to X11 window server using 'localhost:10.0' as the value of the DISPLAY variable.
        at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
        at sun.awt.X11GraphicsEnvironment.access$100(X11GraphicsEnvironment.java:52)
        at sun.awt.X11GraphicsEnvironment$1.run(X11GraphicsEnvironment.java:155)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:131)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:68)
        at GETest.main(GETest.java:7)

    [root@db sofware]# java -Djava.awt.headless=true GETest
    class: class sun.java2d.HeadlessGraphicsEnvironment
    isHeadless: true
    [root@db sofware]# echo $DISPLAY
    localhost:12.0
    [root@db sofware]# java GETest
    Exception in thread "main" java.lang.InternalError: Can't connect to X11 window server using 'localhost:12.0' as the value of the DISPLAY variable.
        at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
        at sun.awt.X11GraphicsEnvironment.access$100(X11GraphicsEnvironment.java:52)
        at sun.awt.X11GraphicsEnvironment$1.run(X11GraphicsEnvironment.java:155)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:131)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:68)
        at GETest.main(GETest.java:7)

unset DISPLAY
		
    [root@db sofware]# unset DISPLAY
    [root@db sofware]# java GETest
    class: class sun.java2d.HeadlessGraphicsEnvironment
    isHeadless: true

##Summary
    
1) Java Technology Community

How to resolve java Exceptions ? :java.awt.HeadlessException

2) java.awt.HeadlessException
 
I have seen this Exception being encountered by many applications, which use UI (AWT/Swing) APIs .

3) When is this exception thrown ? 

java.awt.HeadlessException is thrown on a machine which is headless  when -Djava.awt.headless commandline option is set to true and  Heavyweight AWT components like Applet, Button, Checkbox, Choice, FileDialog, Label, List, Menu, MenuBar, MenuComponent, MenuItem, PopupMenu, Scrollbar,ScrollPane, TextArea, TextComponent, Frame, Window, Dialog, JApplet, JFrame, JWindow, JDialog and TextField are used in the environment.

4) What is the option -Djava.awt.headless all about? 

Many environments, such as mainframe machines and dedicated servers, do not support a display, keyboard, or mouse. On Such machines if we try to to use AWT package,we will get an error like this
"Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
at sun.awt.X11GraphicsEnvironment.initDisplay"
This error is thrown as the AWT package is causing the X11 server to open the DISPLAY.
To correct this problem, We need to tell the Java AWT package that the X11 server is headless (has no display).
To run our environment with a headless implementation, the follow property may be specified at the java command line: 
        
	-Djava.awt.headless=true
		
This will tell the AWT package about the X11 server being headless.
This option is available from Java 1.4 onwards.

5) Why we get a "HeadlessException" after setting this option ?

Headless support is enabled by the GraphicsEnvironment methods "isHeadless" and "isHeadlessInstance".

These methods check whether X11 server has a DISPLAY or its headless(without a DISPLAY).

The heavyweight AWT components like Applet, Button, Checkbox, Choice, FileDialog, Label, List, Menu, MenuBar, MenuComponent, MenuItem, PopupMenu, Scrollbar, ScrollPane, TextArea, TextComponent, Frame, Window, Dialog, JApplet, JFrame, JWindow, JDialog and TextField are affected by the lack of DISPLAY,keyboard  and mouse etc.
Therefore ,if used in a "headless" environment they all will throw a "HeadlessException" like this -
	
	java.awt.HeadlessException 
	at java.awt.GraphicsEnvironment.checkHeadless(GraphicsEnvironment.java:150) 
	at java.awt.Window.<init>(Window.java:311) 
	at java.awt.Frame.<init>(Frame.java:431) 
	at java.awt.Frame.<init>(Frame.java:396)

6) How can we resolve this exception/related erros ?

-Check whether the X11 server is installed 
-Check whether DISPLAY variable is properly set 
-Heavyweight awt/swing components which requires display, mouse keyboard should be run on client side rather than server side.
---
layout: post
title: Oracle ADDM Example
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

##pl/sql packages

	[oracle@oradb ~]$ ls -l /db/oracle/product/11.2.0/db_1/rdbms/admin/addm*
	-rw-r--r--. 1 oracle oinstall 4748 Jan  6  2005 /db/oracle/product/11.2.0/db_1/rdbms/admin/addmrpti.sql
	-rw-r--r--. 1 oracle oinstall 3168 Oct 16  2003 /db/oracle/product/11.2.0/db_1/rdbms/admin/addmrpt.sql
	-rw-r--r--. 1 oracle oinstall 6196 Mar  6  2007 /db/oracle/product/11.2.0/db_1/rdbms/admin/addmtmig.sql

##dbms_advisor

###statistics_level
     
    SQL> show parameter statistics_level;
     
    NAME                                 TYPE        VALUE
    ------------------------------------ ----------- ------------------------------
    statistics_level                     string      TYPICAL
     
###tables
     
    SQL> create table bigtab as select rownum as "id", a.* from dba_objects a;
     
    Table created
     
    SQL> create table smalltab as select  rownum as "id", a.* from dba_tables a;
     
    Table created
     
    SQL> declare
      2  n number;
      3  begin
      4     for n in 1 .. 100
      5     loop
      6       insert into bigtab select rownum as "id", a.* from dba_objects a;
      7       commit;
      8     end loop;
      9  end;
     10  /
     
    PL/SQL procedure successfully completed

###create first snapshot
    
    SQL> exec dbms_workload_repository.create_snapshot('TYPICAL');
     
    PL/SQL procedure successfully completed
     
###high-load operation   
  
    SQL> declare
      2  v_var number;
      3  begin
      4     for n in 1 .. 6
      5     loop
      6       select count(*) into v_var from bigtab a, smalltab b;
      7     end loop;
      8  end;
      9  /
     
    PL/SQL procedure successfully completed

    SQL> /

    PL/SQL procedure successfully completed

###create second snapshot
    
    SQL> exec dbms_workload_repository.create_snapshot('TYPICAL');
     
    PL/SQL procedure successfully completed

###create advisor task
    
get last two snap_id
    
    SQL> select * from(select * from dba_hist_snapshot order by snap_id desc)where rownum <= 2;
     
       SNAP_ID       DBID INSTANCE_NUMBER STARTUP_TIME                                                                     BEGIN_INTERVAL_TIME                                                              END_INTERVAL_TIME                                                                FLUSH_ELAPSED                                                                   SNAP_LEVEL ERROR_COUNT  SNAP_FLAG
    ---------- ---------- --------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- ------------------------------------------------------------------------------- ---------- ----------- ----------
          4220 1193549399               1 08-JUL-13 05.34.27.000 PM                                                        26-JUL-13 01.00.41.115 PM                                                        26-JUL-13 01.40.28.363 PM                                                        +00000 00:00:00.6                                                                        1           0          1
          4219 1193549399               1 08-JUL-13 05.34.27.000 PM                                                        26-JUL-13 11.47.51.301 AM                                                        26-JUL-13 01.00.41.115 PM                                                        +00000 00:00:00.9                                                                        1           0          0
  
get dbid
  
    SQL> select dbid from v$database;
     
          DBID
    ----------
    1193549399

###create advisor task and execute
    
    SQL> declare
      2     task_name varchar2(30) := 'ADDM_01';
      3     task_desc varchar2(30) := 'ADDM Feature Test';
      4     task_id number;
      5  begin
      6     dbms_advisor.create_task('ADDM', task_id, task_name, task_desc, null);
      7     dbms_advisor.set_task_parameter(task_name, 'START_SNAPSHOT', 4219);
      8     dbms_advisor.set_task_parameter(task_name, 'END_SNAPSHOT', 4220);
      9     dbms_advisor.set_task_parameter(task_name, 'DB_ID', 1193549399);
     10     dbms_advisor.execute_task(task_name);
     11  end;
     12  /
     
    PL/SQL procedure successfully completed
    
###display task report
    
    SQL> set long 1000000 pagesize 0 longchunksize 1000
    SQL> column get_clob for a80
    SQL> select dbms_advisor.get_task_report('ADDM_01', 'TEXT', 'All') from dual;
     
    DBMS_ADVISOR.GET_TASK_REPORT('
    --------------------------------------------------------------------------------
              ADDM Report for Task 'ADDM_01'
              ------------------------------
     
    Analysis Period
    ---------------
    AWR snapshot range from 4219 to 4220.
    Time period starts at 26-JUL-13 01.00.41 PM
    Time period ends at 26-JUL-13 01.40.28 PM
     
    Analysis Target
    ---------------
    Database 'DBTEST' with DB ID 1193549399.
    Database version 11.2.0.1.0.
    Analysis was requested for all instances, but ADDM analyzed instance DBTEST,
    numbered 1 and hosted at oradb.egolife.com.
    See the "Additional Information" section for more information on the requested
    instances.
     
    Activity During the Analysis Period
    -----------------------------------
    Total database time was 2303 seconds.
    The average number of active sessions was .96.
    ADDM analyzed 1 of the requested 1 instances.
     
    Summary of Findings
    -------------------
       Description            Active Sessions      Recommendations
                              Percent of Activity
       ---------------------  -------------------  ---------------
    1  Top SQL Statements     .96 | 100            1
    2  "User I/O" wait Class  .02 | 2.35           0
     
     
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     
     
              Findings and Recommendations
              ----------------------------
     
    Finding 1: Top SQL Statements
    Impact is .96 active sessions, 100% of total activity.
    ------------------------------------------------------
    SQL statements consuming significant database time were found. These
    statements offer a good opportunity for performance improvement.
     
       Recommendation 1: SQL Tuning
       Estimated benefit is .96 active sessions, 100% of total activity.
       -----------------------------------------------------------------
       Action
          Run SQL Tuning Advisor on the SELECT statement with SQL_ID
          "4mfwm7psk02pc".
          Related Object
             SQL statement with SQL_ID 4mfwm7psk02pc.
             SELECT COUNT(*) FROM BIGTAB A, SMALLTAB B
       Rationale
          The SQL spent 100% of its database time on CPU, I/O and Cluster waits.
          This part of database time may be improved by the SQL Tuning Advisor.
       Rationale
          Database time for this SQL was divided as follows: 100% for SQL
          execution, 0% for parsing, 0% for PL/SQL execution and 0% for Java
          execution.
       Rationale
          SQL statement with SQL_ID "4mfwm7psk02pc" was executed 2 times and had
          an average elapsed time of 1150 seconds.
       Rationale
          Top level calls to execute the PL/SQL statement with SQL_ID
          "b8zqgfq0kz0hv" are responsible for 100% of the database time spent on
          the SELECT statement with SQL_ID "4mfwm7psk02pc".
          Related Object
             SQL statement with SQL_ID b8zqgfq0kz0hv.
             declare
             v_var number;
             begin
             for n in 1 .. 6
             loop
             select count(*) into v_var from bigtab a, smalltab b;
             end loop;
             end;
     
     
    Finding 2: "User I/O" wait Class
    Impact is .02 active sessions, 2.35% of total activity.
    -------------------------------------------------------
    Wait class "User I/O" was consuming significant database time.
    The throughput of the I/O subsystem was not significantly lower than expected.
    The Oracle instance memory (SGA and PGA) was adequately sized.
     
       No recommendations are available.
     
     
     
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     
              Additional Information
              ----------------------
     
    Miscellaneous Information
    -------------------------
    Wait class "Application" was not consuming significant database time.
    Wait class "Commit" was not consuming significant database time.
    Wait class "Concurrency" was not consuming significant database time.
    Wait class "Configuration" was not consuming significant database time.
    CPU was not a bottleneck for the instance.
    Wait class "Network" was not consuming significant database time.
    Session connect and disconnect calls were not consuming significant database
    time.
    Hard parsing of SQL statements was not consuming significant database time.
     
    The database's maintenance windows were active during 100% of the analysis
    period.

##Alternative

script path

    [oracle@oradb ~]$ pwd
    /home/oracle
    [oracle@oradb ~]$ ls -l /db/oracle/product/11.2.0/db_1/rdbms/admin/addm*
    -rw-r--r--. 1 oracle oinstall 4748 Jan  6  2005 /db/oracle/product/11.2.0/db_1/rdbms/admin/addmrpti.sql
    -rw-r--r--. 1 oracle oinstall 3168 Oct 16  2003 /db/oracle/product/11.2.0/db_1/rdbms/admin/addmrpt.sql
    -rw-r--r--. 1 oracle oinstall 6196 Mar  6  2007 /db/oracle/product/11.2.0/db_1/rdbms/admin/addmtmig.sql
    
execute addmrpt
    
    [oracle@oradb ~]$ sqlplus  dev/dev

    `SQL*Plus`: Release 11.2.0.1.0 Production on Fri Jul 26 14:07:16 2013

    Copyright (c) 1982, 2009, Oracle.  All rights reserved.


    Connected to:
    Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
    With the Partitioning, OLAP, Data Mining and Real Application Testing options

    SQL> @?/rdbms/admin/addmrpt

    Current Instance
    ~~~~~~~~~~~~~~~~

       DB Id    DB Name	 Inst Num Instance
    ----------- ------------ -------- ------------
     1193549399 DBTEST		1 DBTEST


    Instances in this Workload Repository schema
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

       DB Id     Inst Num DB Name	   Instance	 Host
    ------------ -------- ------------ --------- ------------------
      1193549399	    1 DBTEST	   DBTEST	 oradbbak.egolife.com
    * 1193549399	    1 DBTEST	   DBTEST	 oradb.egolife.com

    Using 1193549399 for database Id
    Using	       1 for instance number


    Specify the number of days of snapshots to choose from
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Entering the number of days (n) will result in the most recent
    (n) days of snapshots being listed.  Pressing <return> without
    specifying a number lists all completed snapshots.



    Listing the last 3 days of Completed Snapshots

                                Snap
    Instance     DB Name	    Snap Id    Snap Started    Level
    ------------ ------------ --------- ------------------ -----
    DBTEST	     DBTEST	       4158 24 Jul 2013 00:00	   1
                       4159 24 Jul 2013 01:00	   1
                       4160 24 Jul 2013 02:00	   1
                       4161 24 Jul 2013 03:00	   1
                       4162 24 Jul 2013 04:00	   1
                       4163 24 Jul 2013 05:00	   1
                       4164 24 Jul 2013 06:00	   1
                       4165 24 Jul 2013 07:00	   1
                       4166 24 Jul 2013 08:00	   1
                       4167 24 Jul 2013 09:00	   1
                       4168 24 Jul 2013 10:00	   1
                       4169 24 Jul 2013 11:00	   1
                       4170 24 Jul 2013 12:00	   1
                       4171 24 Jul 2013 13:00	   1
                       4172 24 Jul 2013 14:00	   1
                       4173 24 Jul 2013 15:00	   1
                       4174 24 Jul 2013 16:00	   1
                       4175 24 Jul 2013 17:00	   1
                       4176 24 Jul 2013 18:00	   1
                       4177 24 Jul 2013 19:00	   1
                       4178 24 Jul 2013 20:00	   1
                       4179 24 Jul 2013 21:00	   1
                       4180 24 Jul 2013 22:00	   1
                       4181 24 Jul 2013 23:00	   1
                       4182 25 Jul 2013 00:00	   1
                       4183 25 Jul 2013 01:00	   1
                       4184 25 Jul 2013 02:00	   1
                       4185 25 Jul 2013 03:00	   1
                       4186 25 Jul 2013 04:00	   1
                       4187 25 Jul 2013 05:00	   1
                       4188 25 Jul 2013 06:00	   1
                       4189 25 Jul 2013 07:00	   1
                       4190 25 Jul 2013 08:00	   1
                       4191 25 Jul 2013 09:00	   1
                       4192 25 Jul 2013 10:00	   1
                       4193 25 Jul 2013 11:00	   1
                       4194 25 Jul 2013 12:00	   1
                       4195 25 Jul 2013 13:00	   1
                       4196 25 Jul 2013 14:00	   1
                       4197 25 Jul 2013 15:00	   1
                       4198 25 Jul 2013 16:00	   1
                       4199 25 Jul 2013 17:00	   1
                       4200 25 Jul 2013 18:00	   1
                       4201 25 Jul 2013 19:00	   1
                       4202 25 Jul 2013 20:00	   1
                       4203 25 Jul 2013 21:00	   1
                       4204 25 Jul 2013 22:00	   1
                       4205 25 Jul 2013 23:00	   1
                       4206 26 Jul 2013 00:00	   1
                       4207 26 Jul 2013 01:00	   1
                       4208 26 Jul 2013 02:00	   1
                       4209 26 Jul 2013 03:00	   1
                       4210 26 Jul 2013 04:00	   1
                       4211 26 Jul 2013 05:00	   1
                       4212 26 Jul 2013 06:00	   1
                       4213 26 Jul 2013 07:00	   1

                                Snap
    Instance     DB Name	    Snap Id    Snap Started    Level
    ------------ ------------ --------- ------------------ -----
    DBTEST	     DBTEST	       4214 26 Jul 2013 08:00	   1
                       4215 26 Jul 2013 09:00	   1
                       4216 26 Jul 2013 10:00	   1
                       4217 26 Jul 2013 11:00	   1
                       4218 26 Jul 2013 11:47	   1
                       4219 26 Jul 2013 13:00	   1
                       4220 26 Jul 2013 13:40	   1



    Specify the Begin and End Snapshot Ids
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Enter value for begin_snap: 4217
    Begin Snapshot Id specified: 4217

    Enter value for end_snap: 4220
    End   Snapshot Id specified: 4220



    Specify the Report Name
    ~~~~~~~~~~~~~~~~~~~~~~~
    The default report file name is addmrpt_1_4217_4220.txt.  To use this name,
    press <return> to continue, otherwise enter an alternative.

    Enter value for report_name: 

    Using the report name addmrpt_1_4217_4220.txt


    Running the ADDM analysis on the specified pair of snapshots ...


    Generating the ADDM report for this analysis ...


          ADDM Report for Task 'TASK_4690'
          --------------------------------

    Analysis Period
    ---------------
    AWR snapshot range from 4217 to 4220.
    Time period starts at 26-JUL-13 11.00.36 AM
    Time period ends at 26-JUL-13 01.40.28 PM

    Analysis Target
    ---------------
    Database 'DBTEST' with DB ID 1193549399.
    Database version 11.2.0.1.0.
    ADDM performed an analysis of instance DBTEST, numbered 1 and hosted at
    oradb.egolife.com.

    Activity During the Analysis Period
    -----------------------------------
    Total database time was 6692 seconds.
    The average number of active sessions was .7.

    Summary of Findings
    -------------------
       Description		  Active Sessions      Recommendations
                  Percent of Activity
       ---------------------  -------------------  ---------------
    1  Top SQL Statements	  .51 | 72.49	       1
    2  "User I/O" wait Class  .02 | 2.41	       0


    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


          Findings and Recommendations
          ----------------------------

    Finding 1: Top SQL Statements
    Impact is .51 active sessions, 72.49% of total activity.
    --------------------------------------------------------
    SQL statements consuming significant database time were found. These
    statements offer a good opportunity for performance improvement.

       Recommendation 1: SQL Tuning
       Estimated benefit is .51 active sessions, 72.49% of total activity.
       -------------------------------------------------------------------
       Action
          Run SQL Tuning Advisor on the SELECT statement with SQL_ID
          "4mfwm7psk02pc".
          Related Object
         SQL statement with SQL_ID 4mfwm7psk02pc.
         SELECT COUNT(*) FROM BIGTAB A, SMALLTAB B
       Rationale
          The SQL spent 100% of its database time on CPU, I/O and Cluster waits.
          This part of database time may be improved by the SQL Tuning Advisor.
       Rationale
          Database time for this SQL was divided as follows: 100% for SQL
          execution, 0% for parsing, 0% for PL/SQL execution and 0% for Java
          execution.
       Rationale
          SQL statement with SQL_ID "4mfwm7psk02pc" was executed 4 times and had
          an average elapsed time of 1652 seconds.
       Rationale
          Top level calls to execute the PL/SQL statement with SQL_ID
          "b8zqgfq0kz0hv" are responsible for 100% of the database time spent on
          the SELECT statement with SQL_ID "4mfwm7psk02pc".
          Related Object
         SQL statement with SQL_ID b8zqgfq0kz0hv.
         declare
         v_var number;
         begin
         for n in 1 .. 6
         loop
         select count(*) into v_var from bigtab a, smalltab b;
         end loop;
         end;


    Finding 2: "User I/O" wait Class
    Impact is .02 active sessions, 2.41% of total activity.
    -------------------------------------------------------
    Wait class "User I/O" was consuming significant database time.
    The throughput of the I/O subsystem was not significantly lower than expected.
    The Oracle instance memory (SGA and PGA) was adequately sized.

       No recommendations are available.



    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

          Additional Information
          ----------------------

    Miscellaneous Information
    -------------------------
    Wait class "Application" was not consuming significant database time.
    Wait class "Commit" was not consuming significant database time.
    Wait class "Concurrency" was not consuming significant database time.
    Wait class "Configuration" was not consuming significant database time.
    CPU was not a bottleneck for the instance.
    Wait class "Network" was not consuming significant database time.
    Session connect and disconnect calls were not consuming significant database
    time.
    Hard parsing of SQL statements was not consuming significant database time.

    The database's maintenance windows were active during 100% of the analysis
    period.


    End of Report
    Report written to addmrpt_1_4217_4220.txt
	
##Reference

* Oracle Database Performance Tuning Guide
---
layout: post
title: Oracle Explain Plan Introduction
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

##Execution plan

To execute a SQL statement, Oracle Database may need to perform many steps. Each step either retrieves rows of data physically from the database or prepares them in some way for the user issuing the statement. The combination of the steps that Oracle Database uses to execute a statement is an execution plan. 

An execution plan includes an access path for each table that the statement accesses and an ordering of the tables (the join order) with the appropriate join method.

	PLAN_TABLE --> EXPLAIN PLAN FOR statement --> Displaying PLAN_TABLE Output

These are the basics of using the  EXPLAIN PLAN  statement:

* Use the SQL script  CATPLAN . SQL  to create a sample output table called  PLAN_TABLE in your schema. 
* Include the  EXPLAIN PLAN FOR  clause before the SQL statement. 
* After issuing the  EXPLAIN PLAN  statement, use one of the scripts or package provided by Oracle Database to display the most recent plan table output.
* The execution order in  EXPLAIN PLAN  output begins with the line that is the furthest indented to the right. The next step is the parent of that line. If two lines are indented equally, then the top line is normally executed first.

##Using EXPLAIN PLAN  

This chapter introduces execution plans, describes the SQL statement  `EXPLAIN PLAN` , and explains how to interpret its output. This chapter also provides procedures for managing outlines to control application performance characteristics. This chapter contains the following sections: 

* Understanding EXPLAIN PLAN
* The PLAN_TABLE Output Table
* Running EXPLAIN PLAN
* Displaying PLAN_TABLE Output
* Reading EXPLAIN PLAN Output
* Viewing Parallel Execution with EXPLAIN PLAN
* Viewing Bitmap Indexes with EXPLAIN PLAN
* Viewing Result Cache with EXPLAIN PLAN
* Viewing Partitioned Objects with EXPLAIN PLAN
* PLAN_TABLE Columns

###Understanding EXPLAIN PLAN

The  `EXPLAIN PLAN`  statement displays execution plans chosen by the optimizer for` SELECT` ,  `UPDATE` , `INSERT` , and  `DELETE`  statements. A statement execution plan is the sequence of operations that the database performs to run the statement. 

The row source tree is the core of the execution plan. The tree shows the following information:

* An ordering of the tables referenced by the statement
* An access method for each table mentioned in the statement
* A join method for tables affected by join operations in the statement
* Data operations like filter, sort, or aggregationUnderstanding EXPLAIN PLAN

In addition to the row source tree, the plan table contains information about the following:

* Optimization, such as the cost and cardinality of each operation
* Partitioning, such as the set of accessed partitions
* Parallel execution, such as the distribution method of join inputs

The  `EXPLAIN PLAN`  results let you determine whether the optimizer selects a particular execution plan, such as, nested loops join. The results also help you to understand the optimizer decisions, such as why the optimizer chose a nested loops join instead of a hash join, and lets you understand the performance of a query.

###How Execution Plans Can Change

With the query optimizer, execution plans can and do change as the underlying optimizer inputs change.  EXPLAIN PLAN  output shows how Oracle Database would run the SQL statement when the statement was explained. This plan can differ from the actual execution plan a SQL statement because of differences in the execution environment and explain plan environment

Different Schemas 

* The execution and explain plan occur on different databases.
* The user explaining the statement is different from the user running the statement. Two users might be pointing to different objects in the same database, resulting in different execution plans.
* Schema changes (usually changes in indexes) between the two operations.

Different Costs 

Even if the schemas are the same, the optimizer can choose different execution plans when the costs are different. Some factors that affect the costs include the following:

* Data volume and statistics
* Bind variable types and values
* Initialization parameters set globally or at session level

Minimizing Throw-Away

Examining an explain plan lets you look for throw-away in cases such as the following:

* Full scans
* Unselective range scans
* Late predicate filters
* Wrong join order
* Late filter operations

###Looking Beyond Execution Plans

The execution plan operation alone cannot differentiate between well-tuned statements and those that perform poorly. For example, an  EXPLAIN PLAN  output that shows that a statement uses an index does not necessarily mean that the statement runs efficiently. Sometimes indexes are extremely inefficient. In this case, you should examine the following:

* The columns of the index being used
* Their selectivity (fraction of table being accessed)

It is best to use  EXPLAIN PLAN  to determine an access plan, and then later prove that it is the optimal plan through testing. When evaluating a plan, examine the statement's actual resource consumption. 

##Using V$SQL_PLAN Views

In addition to running the  EXPLAIN PLAN  command and displaying the plan, you can use the  V$SQL_PLAN  views to display the execution plan of a SQL statement: After the statement has executed, you can display the plan by querying the  V$SQL_ PLAN  view.  V$SQL_PLAN  contains the execution plan for every statement stored in the shared SQL area.

The advantage of  V$SQL_PLAN  over  EXPLAIN PLAN  is that you do not need to know the compilation environment that was used to execute a particular statement. For  EXPLAIN PLAN , you would need to set up an identical environment to get the same plan when executing the statement.

The  V$SQL_PLAN_STATISTICS  view provides the actual execution statistics for every operation in the plan, such as the number of output rows and elapsed time. All statistics, except the number of output rows, are cumulative. For example, the statistics for a join operation also includes the statistics for its two inputs. The statistics in V$SQL_PLAN_STATISTICS  are available for cursors that have been compiled with the STATISTICS_LEVEL  initialization parameter set to  ALL .

The  V$SQL_PLAN_STATISTICS_ALL  view enables side by side comparisons of the estimates that the optimizer provides for the number of rows and elapsed time. This view combines both  V$SQL_PLAN  and  V$SQL_PLAN_STATISTICS  information for every cursor.

The  PLAN_TABLE  is automatically created as a public synonym to a global temporary table. This temporary table holds the output of  EXPLAIN PLAN  statements for all users. PLAN_TABLE  is the default sample output table into which the  EXPLAIN PLAN  statement inserts rows describing execution plans.

After you have explained the plan, use the following SQL scripts or PL/SQL package 
provided by Oracle Database to display the most recent plan table output:

* UTLXPLS.SQL : This script displays the plan table output for serial processing.

* UTLXPLP.SQL: This script displays the plan table output including parallel execution columns.

* DBMS_XPLAN.DISPLAY  table function: This function accepts options for displaying the plan table output. You can specify:

	* A plan table name if you are using a table different than  PLAN_TABLE
	* A statement ID if you have set a statement ID with the  EXPLAIN PLAN
	* A format option that determines the level of detail:  BASIC ,  SERIAL , and TYPICAL ,  ALL ,

Some examples of the use of  DBMS_XPLAN  to display  PLAN_TABLE  output are:

	SELECT PLAN_TABLE_OUTPUT FROM TABLE(DBMS_XPLAN.DISPLAY());
	
	SELECT PLAN_TABLE_OUTPUT FROM TABLE(DBMS_XPLAN.DISPLAY('MY_PLAN_TABLE', 'st1','TYPICAL'));
  

###Customizing PLAN_TABLE Output

If you have specified a statement identifier, then you can write your own script to query the  PLAN_TABLE . For example:

* Start with ID = 0 and given  STATEMENT_ID .
* Use the  CONNECT BY  clause to walk the tree from parent to child, the join keys being STATEMENT_ID  =  PRIOR STATEMENT_ID  and  PARENT_ID  =  PRIOR ID .
* Use the pseudo-column  LEVEL  (associated with  CONNECT BY ) to indent the children.

sql

	SELECT cardinality "Rows",
	   lpad(' ',level-1)||operation||' '||options||' '||object_name "Plan"
	  FROM PLAN_TABLE
	CONNECT BY prior id = parent_id AND prior statement_id = statement_id
	  START WITH id = 0 AND statement_id = 'st1'
	  ORDER BY id;
	   Rows Plan
	------- ----------------------------------------
			SELECT STATEMENT
			 TABLE ACCESS FULL EMPLOYEES
  
##Reference

* Oracle Database Performance Guide

---
layout: post
title: Oracle Explain Plan Example
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

unlock hr

	SQL> alter user hr account unlock;
	 
	User altered
	 
	SQL> alter user hr identified by hr;
	 
	User altered

##example 1

	SQL> explain plan for
	  2  select e.employee_id, j.job_title,e.salary,d.department_name
	  3  from employees e, jobs j, departments d
	  4  where e.employee_id < 103
	  5  and e.job_id  = j.job_id
	  6  and e.department_id = d.department_id
	  7  /
	 
	Explained
	 
display plan
	 
	SQL> select * from table(dbms_xplan.display);
	 
	PLAN_TABLE_OUTPUT
	--------------------------------------------------------------------------------
	Plan hash value: 2963623819
	--------------------------------------------------------------------------------
	| Id  | Operation                       | Name          | Rows  | Bytes | Cost (
	--------------------------------------------------------------------------------
	|   0 | SELECT STATEMENT                |               |     3 |   189 |     8
	|   1 |  NESTED LOOPS                   |               |       |       |
	|   2 |   NESTED LOOPS                  |               |     3 |   189 |     8
	|   3 |    MERGE JOIN                   |               |     3 |   141 |     5
	|   4 |     TABLE ACCESS BY INDEX ROWID | JOBS          |    19 |   513 |     2
	|   5 |      INDEX FULL SCAN            | JOB_ID_PK     |    19 |       |     1
	|*  6 |     SORT JOIN                   |               |     3 |    60 |     3
	|   7 |      TABLE ACCESS BY INDEX ROWID| EMPLOYEES     |     3 |    60 |     2
	|*  8 |       INDEX RANGE SCAN          | EMP_EMP_ID_PK |     3 |       |     1
	|*  9 |    INDEX UNIQUE SCAN            | DEPT_ID_PK    |     1 |       |     0
	|  10 |   TABLE ACCESS BY INDEX ROWID   | DEPARTMENTS   |     1 |    16 |     1
	--------------------------------------------------------------------------------
	Predicate Information (identified by operation id):
	---------------------------------------------------
	 
	PLAN_TABLE_OUTPUT
	--------------------------------------------------------------------------------
	   6 - access("E"."JOB_ID"="J"."JOB_ID")
		   filter("E"."JOB_ID"="J"."JOB_ID")
	   8 - access("E"."EMPLOYEE_ID"<103)
	   9 - access("E"."DEPARTMENT_ID"="D"."DEPARTMENT_ID")
	 
	25 rows selected


##example 2
	
sql
						   
	 SQL> explain plan
	  2  set statement_id = 'exp_1' for
	  3  select phone_number
	  4  from employees
	  5  where phone_number like '650%'
	  6  /
	 
	Explained
	 
display plan
	 
	SQL> select plan_table_output
	  2  from table(dbms_xplan.display(null,'exp_1','All'))
	  3  /
	 
	PLAN_TABLE_OUTPUT
	--------------------------------------------------------------------------------
	Plan hash value: 1445457117
	-------------------------------------------------------------------------------
	| Id  | Operation         | Name      | Rows  | Bytes | Cost (%CPU)| Time     |
	-------------------------------------------------------------------------------
	|   0 | SELECT STATEMENT  |           |     1 |    15 |     3   (0)| 00:00:01 |
	|*  1 |  TABLE ACCESS FULL| EMPLOYEES |     1 |    15 |     3   (0)| 00:00:01 |
	-------------------------------------------------------------------------------
	Query Block Name / Object Alias (identified by operation id):
	-------------------------------------------------------------
	   1 - SEL$1 / EMPLOYEES@SEL$1
	Predicate Information (identified by operation id):
	---------------------------------------------------
	   1 - filter("PHONE_NUMBER" LIKE '650%')
	Column Projection Information (identified by operation id):
	 
	PLAN_TABLE_OUTPUT
	--------------------------------------------------------------------------------
	-----------------------------------------------------------
	   1 - "PHONE_NUMBER"[VARCHAR2,20]
	 
	23 rows selected
	 
	SQL> 

##Reference

* Oracle Database Performance Guide
---
layout: post
title: Oracle SQL Tuning Overview
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

##Introduction to SQL Tuning

SQL tuning involves the following basic steps:

* Identifying high load or top SQL statements that are responsible for a large share of the application workload and system resources, by reviewing past SQL execution history available in the system
* Verifying that the execution plans produced by the query optimizer for these statements perform reasonably
* Implementing corrective actions to generate better execution plans for poorly performing SQL statements

The previous steps are repeated until the system performance reaches a satisfactory level or no more statements can be tuned.


##Goals

* Reduce the workload. SQL tuning commonly involves finding more efficient ways to process the same workload. It is possible to change the execution plan of the statement without altering the functionality to reduce the resource consumption

* Balance the Workload. Systems often tend to have peak usage in the daytime when real users are connected to the system, and low usage in the nighttime. If you can schedule noncritical reports and batch jobs to run in the nighttime and reduce their concurrency during day time, then the database frees up resources for the more critical programs in the day.

* Parallelize the Workload. Queries that access large amounts of data (typical data warehouse queries) can often run in parallel. Parallelism is extremely useful for reducing response time in a low concurrency data warehouse. However, for OLTP environments, which tend to be high concurrency, parallelism can adversely impact other users by increasing the overall resource usage of the program.


##Identifying High-Load SQL

This section describes the steps involved in identifying and gathering data on high-load SQL statements. High-load SQL are poorly-performing, resource-intensive SQL statements that impact the performance of an Oracle database. The following tools can identify high-load SQL statements:

* Automatic Database Diagnostic Monitor
* Automatic SQL tuning
* Automatic Workload Repository
* V$SQL  view
* Custom Workload
* SQL Trace

###Identifying Resource-Intensive SQL

####Tuning a Specific Program.

If you are tuning a specific program (GUI or 3GL), then identifying the SQL to examine is a simple matter of looking at the SQL executed within the program. Oracle Enterprise Manager (Enterprise Manager) provides tools for identifying resource intensive SQL statements, generating explain plans, and evaluating SQL performance. If it is not possible to identify the SQL (for example, the SQL is generated dynamically), then use  SQL_TRACE  to generate a trace file that contains the SQL executed, then use  TKPROF  to generate an output file.

The SQL statements in the  TKPROF  output file can be ordered by various parameters, such as the execution elapsed time ( exeela ), which usually assists in the identification by ordering the SQL statements by elapsed time (with highest elapsed time SQL statements at the top of the file). This makes the job of identifying the poorly performing SQL easier if there are many SQL statements in the file.

####Tuning an Application / Reducing Load.

If the whole application is performing poorly, or if you are attempting to reduce the overall CPU or I/O load on the database server, then identifying resource-intensive SQL involves the following steps:

* Determine which period in the day you would like to examine; typically this is the application's peak processing time.
* Gather operating system and Oracle Database statistics at the beginning and end of that period. The minimum of Oracle Database statistics gathered should be file I/O ( V$FILESTAT ), system statistics ( V$SYSSTAT ), and SQL statistics ( V$SQLAREA , V$SQL , or  V$SQLSTATS ,  V$SQLTEXT ,  V$SQL_PLAN , and  V$SQL_PLAN_STATISTICS ).
* Using the data collected in step two, identify the SQL statements using the most resources. A good way to identify candidate SQL statements is to query V$SQLSTATS .  V$SQLSTATS  contains resource usage information for all SQL statements in the shared pool. The data in  V$SQLSTATS  should be ordered by resource usage. The most common resources are: 

	* Buffer gets ( V$SQLSTATS . BUFFER_GETS , for high CPU using statements) 
	* Disk reads ( V$SQLSTATS . DISK_READS , for high I/O statements) 
	* Sorts ( V$SQLSTATS . SORTS , for many sorts)

One method to identify which SQL statements are creating the highest load is to compare the resources used by a SQL statement to the total amount of that resource used in the period. For  BUFFER_GETS , divide each SQL statement's  BUFFER_GETS  by the total number of buffer gets during the period. The total number of buffer gets in the system is available in the  V$SYSSTAT  table, for the statistic session logical reads. Similarly, it is possible to apportion the percentage of disk reads a statement performs out of the total disk reads performed by the system by dividing  V$SQL_STATS.DISK_ READS  by the value for the  V$SYSSTAT  statistic physical reads. The SQL sections of the Automatic Workload Repository report include this data, so you do not need to 
perform the percentage calculations manually.


###Gathering Data on the SQL Identified

If you are most concerned with CPU, then examine the top SQL statements that performed the most  BUFFER_GETS  during that interval. Otherwise, start with the SQL statement that performed the most  DISK_READS .

####Information to Gather During Tuning

The tuning process begins by determining the structure of the underlying tables and indexes. The information gathered includes the following:

* Complete SQL text from  V$SQLTEXT
* Structure of the tables referenced in the SQL statement, usually by describing the table in `SQL*Plus`
* Definitions of any indexes (columns, column orders), and whether the indexes are unique or non-unique
* Optimizer statistics for the segments (including the number of rows each table, selectivity of the index columns), including the date when the segments were last analyzed
* Definitions of any views referred to in the SQL statement
* Repeat steps two, three, and four for any tables referenced in the view definitions 
* Optimizer plan for the SQL statement (either from  EXPLAIN PLAN ,  V$SQL_PLAN , or the  TKPROF  output)
* Any previous optimizer plans for that SQL statement

Note:

It is important to generate and review execution plans for all of the key SQL statements in your application. Doing so lets you compare the optimizer execution plans of a SQL statement when the statement performed well to the plan when that the statement is not performing well. Having the comparison, along with information such as changes in data volumes, can assist in identifying the cause of performance degradation.


##Automatic SQL Tuning Features

Because the manual SQL tuning process poses many challenges to the application developer, the SQL tuning process has been automated by the automatic SQL tuning features of Oracle Database. These features are designed to work equally well for OLTP and Data Warehouse type applications:

* ADDM
* SQL Tuning Advisor
* SQL Tuning Sets
* SQL Access Advisor

The Automatic Database Diagnostic Monitor (ADDM) analyzes the information collected by the AWR for possible performance problems with Oracle Database, including high-load SQL statements.

SQL Tuning Advisor optimizes SQL statements that have been identified as high-load SQL statements. By default, Oracle Database automatically identifies problematic SQL statements and implements tuning recommendations using SQL Tuning Advisor during system maintenance windows as an automated maintenance task, searching for ways to improve the execution plans of the high-load SQL statements. You can also choose to run SQL Tuning Advisor at any time on any given SQL workload to improve performance.

When multiple SQL statements serve as input to ADDM, SQL Tuning Advisor, or SQL Access Advisor, the database constructs and stores a SQL tuning set (STS). The STS includes the set of SQL statements along with their associated execution context and basic execution statistics.

In addition to SQL Tuning Advisor, SQL Access Advisor provides advice on materialized views, indexes, and materialized view logs. SQL Access Advisor helps you achieve performance goals by recommending the proper set of materialized views, materialized view logs, and indexes for a given workload. In general, as the number of materialized views and indexes and the space allocated to them is increased, query performance improves. SQL Access Advisor considers the trade-offs between space usage and query performance, and recommends the most cost-effective configuration of new and existing materialized views and indexes.

##Developing Efficient SQL Statements

This section describes ways you can improve SQL statement efficiency:

* Verifying Optimizer Statistics
* Reviewing the Execution Plan
* Restructuring the SQL Statements
* Restructuring the Indexes
* Modifying or Disabling Triggers and Constraints
* Restructuring the Data
* Maintaining Execution Plans Over Time
* Visiting Data as Few Times as Possible

###Verifying Optimizer Statistics

The query optimizer uses statistics gathered on tables and indexes when determining the optimal execution plan. If these statistics have not been gathered, or if the statistics are no longer representative of the data stored within the database, then the optimizer does not have sufficient information to generate the best plan. Things to check:

* If you gather statistics for some tables in your database, then it is probably best to gather statistics for all tables. This is especially true if your application includes SQL statements that perform joins.

* If the optimizer statistics in the data dictionary are no longer representative of the data in the tables and indexes, then gather new statistics. One way to check whether the dictionary statistics are stale is to compare the real cardinality (row count) of a table to the value of  DBA_TABLES.NUM_ROWS . Additionally, if there is significant data skew on predicate columns, then consider using histograms.

###Reviewing the Execution Plan

When tuning (or writing) a SQL statement in an OLTP environment, the goal is to drive from the table that has the most selective filter. This means that there are fewer rows passed to the next step. If the next step is a join, then this means that fewer rows are joined. Check to see whether the access paths are optimal.

When examining the optimizer execution plan, look for the following:

* The driving table has the best filter.
* The join order in each step returns the fewest number of rows to the next step (that is, the join order should reflect, where possible, going to the best not-yet-used filters).
* The join method is appropriate for the number of rows being returned. For example, nested loop joins through indexes may not be optimal when the statement returns many rows.
* The database uses views efficiently. Look at the  SELECT  list to see whether access to the view is necessary.
* There are any unintentional Cartesian products (even with small tables).
* Each table is being accessed efficiently: Consider the predicates in the SQL statement and the number of rows in the table. Look for suspicious activity, such as a full table scans on tables with large number of rows, which have predicates in the where clause. Determine why an index is not used for such a selective predicate.A full table scan does not mean inefficiency. It might be more efficient to perform a full table scan on a small table, or to perform a full table scan to leverage a better join method (for example, hash_join) for the number of rows returned. If any of these conditions are not optimal, then consider restructuring the SQL statement or the indexes available on the tables.

####Restructuring the SQL Statements

Often, rewriting an inefficient SQL statement is easier than modifying it. If you understand the purpose of a given statement, then you might be able to quickly and easily write a new statement that meets the requirement.

* Compose Predicates Using AND and =
* To improve SQL efficiency, use equijoins whenever possible. Statements that perform equijoins on untransformed column values are the easiest to tune. 
* Avoid Transformed Columns in the WHERE Clause
* Write Separate SQL Statements for Specific Tasks. SQL is not a procedural language. Using one piece of SQL to do many different things usually results in a less-than-optimal result for each task. If you want SQL to accomplish different things, then write various statements, rather than writing one statement to do different things depending on the parameters you give it. 

####Controlling the Access Path and Join Order with Hints

You can influence the optimizer's choices by setting the optimizer approach and goal, and by gathering representative statistics for the query optimizer. Sometimes, the application designer, who has more information about a particular application's data than is available to the optimizer, can choose a more effective way to execute a SQL statement. You can use hints in SQL statements to instruct the optimizer about how the statement should be executed. 

Join order can have a significant effect on performance. The main objective of SQL tuning is to avoid performing unnecessary work to access rows that do not affect the result. This leads to three general rules:
* Avoid a full-table scan if it is more efficient to get the required rows through an index.
* Avoid using an index that fetches 10,000 rows from the driving table if you could instead use another index that fetches 100 rows.
* Choose the join order so as to join fewer rows to tables later in the join order.

Filter conditions dominate the choice of driving table and index. In general, the driving table is the one containing the filter condition that eliminates the highest percentage of the table. 

####Restructuring the Indexes 

Often, there is a beneficial impact on performance by restructuring indexes. This can involve the following:

* Remove nonselective indexes to speed the DML.
* Index performance-critical access paths.
* Consider reordering columns in existing concatenated indexes.
* Add columns to the index to improve selectivity.

 Do not use indexes as a panacea. Application developers sometimes think that performance improves when they create more indexes. If a single programmer creates an appropriate index, then this index may improve the application's performance. However, if 50 developers each create an index, then application performance will probably be hampered.

####Modifying or Disabling Triggers and Constraints

Using triggers consumes system resources. If you use too many triggers, then performance may be adversely affected. In this case, you might need to modify or disable the triggers. 

####Restructuring the Data

After restructuring the indexes and the statement, consider restructuring the data:
* Introduce derived values. Avoid  GROUP BY  in response-critical code.
* Review your data design. Change the design of your system if it can improve performance.

####Maintaining Execution Plans Over Time

You can maintain the existing execution plan of SQL statements over time either using stored statistics or SQL plan baselines. Storing optimizer statistics for tables will apply to all SQL statements that refer to those tables. Storing an execution plan as a SQL plan baseline maintains the plan for set of SQL statements. If both statistics and a SQL plan baseline are available for a SQL statement, then the optimizer first uses a cost-based search method to build a best-cost plan, and then tries to find a matching plan in the SQL plan baseline. If a match is found, then the optimizer proceeds using this plan. Otherwise, it evaluates the cost of each of the accepted plans in the SQL plan baseline and selects the plan with the lowest cost.

####Visiting Data as Few Times as Possible 

Applications should try to access each row only once. This reduces network traffic and reduces database load. Consider doing the following:

* Combine Multiples Scans Using CASE Expressions
* Use DML with RETURNING Clause
* Modify All the Data Needed in One Statement


##Building SQL Test Cases

For many SQL-related problems, obtaining a reproducible test case makes it easier to resolve the problem. Starting with the 11g Release 2 (11.2), Oracle Database contains the SQL Test Case Builder, which automates the somewhat difficult and time-consuming process of gathering and reproducing as much information as possible about a problem and the environment in which it occurred.

SQL Test Case Builder captures information pertaining to a SQL-related problem, along with the exact environment under which the problem occurred, so that you can reproduce and test the problem on a separate database. After the test case is ready, you can upload the problem to Oracle Support to enable support personnel to reproduce and troubleshoot the problem.

The information gathered by SQL Test Case Builder includes the query being executed, table and index definitions (but not the actual data), PL/SQL functions, procedures, and packages, optimizer statistics, and initialization parameter settings.

##Reference

* Oracle Database Performance Tuning Guide
---
layout: post
title: Oracle STA Example
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

explain plan

    oracle@oradb ~]$ sqlplus  dev

    `SQL*Plus`: Release 11.2.0.1.0 Production on Fri Jul 26 14:15:15 2013

    Copyright (c) 1982, 2009, Oracle.  All rights reserved.

    Enter password: 

    Connected to:
    Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
    With the Partitioning, OLAP, Data Mining and Real Application Testing options

    SQL> set timing on
    SQL> set autot on
    SQL> select count(1)
      2  from bigtab a,
      3       smalltab b
      4  where a.object_name = b.table_name
      5  /

      COUNT(1)
    ----------
        389961

    Elapsed: 00:00:04.15

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 3089226980

    --------------------------------------------------------------------------------
    | Id  | Operation	    | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    --------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT    |	       |     1 |    83 | 30501	 (1)| 00:06:07 |
    |   1 |  SORT AGGREGATE     |	       |     1 |    83 |	    |	       |
    |*  2 |   HASH JOIN	    |	       |  5096K|   403M| 30501	 (1)| 00:06:07 |
    |   3 |    TABLE ACCESS FULL| SMALLTAB |  2986 | 50762 |    35	 (0)| 00:00:01 |
    |   4 |    TABLE ACCESS FULL| BIGTAB   |  7955K|   500M| 30440	 (1)| 00:06:06 |
    --------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("A"."OBJECT_NAME"="B"."TABLE_NAME")

    Note
    -----
       - dynamic sampling used for this statement (level=2)


    Statistics
    ----------------------------------------------------------
         32  recursive calls
          0  db block gets
     112161  consistent gets
     111877  physical reads
          0  redo size
        528  bytes sent via SQL*Net to client
        524  bytes received via SQL*Net from client
          2  SQL*Net roundtrips to/from client
          0  sorts (memory)
          0  sorts (disk)
          1  rows processed

    SQL> 
    
1) create tuning task
    
    SQL> set autot off
    SQL> set timing off
    SQL> declare
      2      my_task_name varchar2(50);
      3      my_sql_text clob;
      4  begin
      5      my_sql_text := 'select count(1) from bigtab a,smalltab b where a.object_name = b.table_name';
      6      my_task_name := dbms_sqltune.create_tuning_task(sql_text => my_sql_text, task_name => 'sql_tuning_advisor_test');
      7      dbms_sqltune.execute_tuning_task(task_name => 'sql_tuning_advisor_test');
      8  end;
      9  /
     
    PL/SQL procedure successfully completed.

2) execute tuning task

    SQL> exec dbms_sqltune.execute_tuning_task('sql_tuning_advisor_test');

    PL/SQL procedure successfully completed.

3) view tuning task status

    SQL> select task_name, status from user_advisor_tasks where task_name = 'sql_tuning_advisor_test';
    
    TASK_NAME		       STATUS
    ------------------------------ -----------
    sql_tuning_advisor_test        COMPLETED

4) display tuning report

    SQL> set long 999999
    SQL> set serveroutput on size 999999
    SQL> set linesize 100
    SQL> select dbms_sqltune.report_tuning_task('sql_tuning_advisor_test') from dual;

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
    GENERAL INFORMATION SECTION
    -------------------------------------------------------------------------------
    Tuning Task Name   : sql_tuning_advisor_test
    Tuning Task Owner  : DEV
    Workload Type	   : Single SQL Statement
    Execution Count    : 2
    Current Execution  : EXEC_4845
    Execution Type	   : TUNE SQL
    Scope		   : COMPREHENSIVE
    Time Limit(seconds): 1800
    Completion Status  : COMPLETED

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
    Started at	   : 07/26/2013 14:33:47
    Completed at	   : 07/26/2013 14:34:26

    -------------------------------------------------------------------------------
    Schema Name: DEV
    SQL ID	   : 1fyx6kqg3csfb
    SQL Text   : select count(1) from bigtab a,smalltab b where a.object_name =
             b.table_name

    -------------------------------------------------------------------------------
    FINDINGS SECTION (3 findings)

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
    -------------------------------------------------------------------------------

    1- Statistics Finding
    ---------------------
      Table "DEV"."SMALLTAB" was not analyzed.

      Recommendation
      --------------
      - Consider collecting optimizer statistics for this table.
        execute dbms_stats.gather_table_stats(ownname => 'DEV', tabname =>
            'SMALLTAB', estimate_percent => DBMS_STATS.AUTO_SAMPLE_SIZE,
            method_opt => 'FOR ALL COLUMNS SIZE AUTO');

      Rationale
      ---------
        The optimizer requires up-to-date statistics for the table in order to
        select a good execution plan.

    2- Statistics Finding
    ---------------------
      Table "DEV"."BIGTAB" was not analyzed.


    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
      Recommendation
      --------------
      - Consider collecting optimizer statistics for this table.
        execute dbms_stats.gather_table_stats(ownname => 'DEV', tabname =>
            'BIGTAB', estimate_percent => DBMS_STATS.AUTO_SAMPLE_SIZE,
            method_opt => 'FOR ALL COLUMNS SIZE AUTO');

      Rationale
      ---------
        The optimizer requires up-to-date statistics for the table in order to
        select a good execution plan.

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------

    3- Index Finding (see explain plans section below)
    --------------------------------------------------
      The execution plan of this statement can be improved by creating one or more
      indices.

      Recommendation (estimated benefit: 98.99%)
      ------------------------------------------
      - Consider running the Access Advisor to improve the physical schema design
        or creating the recommended index.
        create index DEV.IDX$$_12540001 on DEV.SMALLTAB("TABLE_NAME");

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------

      - Consider running the Access Advisor to improve the physical schema design
        or creating the recommended index.
        create index DEV.IDX$$_12540002 on DEV.BIGTAB("OBJECT_NAME");

      Rationale
      ---------
        Creating the recommended indices significantly improves the execution plan
        of this statement. However, it might be preferable to run "Access Advisor"
        using a representative SQL workload as opposed to a single statement. This
        will allow to get comprehensive index recommendations which takes into

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
        account index maintenance overhead and additional space consumption.

    -------------------------------------------------------------------------------
    EXPLAIN PLANS SECTION
    -------------------------------------------------------------------------------

    1- Original
    -----------
    Plan hash value: 3089226980

    --------------------------------------------------------------------------------

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------

    | Id  | Operation	    | Name     | Rows  | Bytes | Cost (%CPU)| Time     |

    --------------------------------------------------------------------------------

    |   0 | SELECT STATEMENT    |	       |     1 |    83 | 30501	 (1)| 00:06:07 |

    |   1 |  SORT AGGREGATE     |	       |     1 |    83 |	    |	       |

    |*  2 |   HASH JOIN	    |	       |  5096K|   403M| 30501	 (1)| 00:06:07 |


    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
    |   3 |    TABLE ACCESS FULL| SMALLTAB |  2986 | 50762 |    35	 (0)| 00:00:01 |

    |   4 |    TABLE ACCESS FULL| BIGTAB   |  7955K|   500M| 30440	 (1)| 00:06:06 |

    --------------------------------------------------------------------------------


    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("A"."OBJECT_NAME"="B"."TABLE_NAME")

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------

    2- Using New Indices
    --------------------
    Plan hash value: 1674279188

    --------------------------------------------------------------------------------
    ---------
    | Id  | Operation	       | Name		| Rows	| Bytes | Cost (%CPU)| T
    ime	|
    --------------------------------------------------------------------------------
    ---------

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT       |		|     1 |    83 |   306   (9)| 0
    0:00:04 |
    |   1 |  SORT AGGREGATE        |		|     1 |    83 |	     |
        |
    |*  2 |   HASH JOIN	       |		|  5096K|   403M|   306   (9)| 0
    0:00:04 |
    |   3 |    INDEX FAST FULL SCAN| IDX$$_12540001 |  2986 | 50762 |    12   (0)| 0
    0:00:01 |
    |   4 |    INDEX FAST FULL SCAN| IDX$$_12540002 |  7955K|   500M|   269   (1)| 0
    0:00:04 |
    --------------------------------------------------------------------------------

    DBMS_SQLTUNE.REPORT_TUNING_TASK('SQL_TUNING_ADVISOR_TEST')
    --------------------------------------------------------------------------------
    ---------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("A"."OBJECT_NAME"="B"."TABLE_NAME")

    -------------------------------------------------------------------------------


    SQL> 
    
5) implement tuning advisor actions

gather table statistics

    SQL> begin
      2      dbms_stats.gather_table_stats(
      3          ownname => 'DEV',
      4          tabname => 'smalltab',
      5          estimate_percent => dbms_stats.auto_sample_size,
      6          method_opt => 'for all columns size auto'
      7      );
      8  end;
      9  /

    PL/SQL procedure successfully completed.

    SQL> begin
      2      dbms_stats.gather_table_stats(
      3          ownname => 'DEV',
      4          tabname => 'bigtab',
      5          estimate_percent => dbms_stats.auto_sample_size,
      6          method_opt => 'for all columns size auto'
      7      );
      8  end;
      9  /

    PL/SQL procedure successfully completed.
    
create index on smalltab(table_name)

    SQL> create index smalltab_idx on dev.smalltab(table_name);

    Index created.
    
retest

    SQL> set timing on
    SQL> set autot on
    SQL> select count(1)
      2  from bigtab a,
      3       smalltab b
      4  where a.object_name = b.table_name
      5  /

      COUNT(1)
    ----------
        389961

    Elapsed: 00:00:01.46

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 998718878

    ---------------------------------------------------------------------------------------
    | Id  | Operation	       | Name	      | Rows  | Bytes | Cost (%CPU)| Time     |
    ---------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT       |	      |     1 |    45 | 30468	(1)| 00:06:06 |
    |   1 |  SORT AGGREGATE        |	      |     1 |    45 | 	   |	      |
    |*  2 |   HASH JOIN	       |	      |   527K|    22M| 30468	(1)| 00:06:06 |
    |   3 |    INDEX FAST FULL SCAN| SMALLTAB_IDX |  3157 | 63140 |     6	(0)| 00:00:01 |
    |   4 |    TABLE ACCESS FULL   | BIGTAB       |  7521K|   179M| 30438	(1)| 00:06:06 |
    ---------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("A"."OBJECT_NAME"="B"."TABLE_NAME")


    Statistics
    ----------------------------------------------------------
          1  recursive calls
          0  db block gets
         111909  consistent gets
         111891  physical reads
          0  redo size
        528  bytes sent via SQL*Net to client
        524  bytes received via SQL*Net from client
          2  SQL*Net roundtrips to/from client
          0  sorts (memory)
          0  sorts (disk)
          1  rows processed

create index on bigtab(object_name)
    
    SQL> create index bigtab_idx on dev.bigtab(object_name);

    Index created.

retest
    
    SQL> select count(1) 
      2  from bigtab a,
      3       smalltab b
      4  where a.object_name = b.table_name
      5  /

      COUNT(1)
    ----------
        389961

    Elapsed: 00:00:00.32

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 995607616

    ---------------------------------------------------------------------------------------
    | Id  | Operation	       | Name	      | Rows  | Bytes | Cost (%CPU)| Time     |
    ---------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT       |	      |     1 |    45 |  5766	(1)| 00:01:10 |
    |   1 |  SORT AGGREGATE        |	      |     1 |    45 | 	   |	      |
    |   2 |   NESTED LOOPS	       |	      |   527K|    22M|  5766	(1)| 00:01:10 |
    |   3 |    INDEX FAST FULL SCAN| SMALLTAB_IDX |  3157 | 63140 |     6	(0)| 00:00:01 |
    |*  4 |    INDEX RANGE SCAN    | BIGTAB_IDX   |   167 |  4175 |     2	(0)| 00:00:01 |
    ---------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       4 - access("A"."OBJECT_NAME"="B"."TABLE_NAME")


    Statistics
    ----------------------------------------------------------
          1  recursive calls
          0  db block gets
           7310  consistent gets
           3095  physical reads
          0  redo size
        528  bytes sent via SQL*Net to client
        524  bytes received via SQL*Net from client
          2  SQL*Net roundtrips to/from client
          0  sorts (memory)
          0  sorts (disk)
          1  rows processed
    
6) delele sql tuning advisor task record
    
    SQL> exec dbms_sqltune.drop_tuning_task('sql_tuning_advisor_test');
    
    PL/SQL procedure successfully completed.
---
layout: post
title: Oracle SQl Access Advisor Example
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

##dbms_advisor

The DBMS_ADVISOR package can be used to create and execute any advisor tasks, including SQL Access Advisor tasks. The following example shows how it is used to create, execute and display a typical SQL Access Advisor script for the current workload. 

	DECLARE
	  taskname varchar2(30) := 'SQLACCESS3638195';
	  task_desc varchar2(256) := 'SQL Access Advisor';
	  task_or_template varchar2(30) := 'SQLACCESS_EMTASK';
	  task_id number := 0;
	  num_found number;
	  sts_name varchar2(256) := 'SQLACCESS3638195_sts';
	  sts_cursor dbms_sqltune.sqlset_cursor;
	BEGIN
	  /* Create Task */
	  dbms_advisor.create_task(DBMS_ADVISOR.SQLACCESS_ADVISOR,
							   task_id,
							   taskname,
							   task_desc,
							   task_or_template);

	  /* Reset Task */
	  dbms_advisor.reset_task(taskname);

	  /* Delete Previous STS Workload Task Link */
	  select count(*)
	  into   num_found
	  from   user_advisor_sqla_wk_map
	  where  task_name = taskname
	  and    workload_name = sts_name;
	  IF num_found > 0 THEN
		dbms_advisor.delete_sqlwkld_ref(taskname,sts_name,1);
	  END IF;

	  /* Delete Previous STS */
	  select count(*)
	  into   num_found
	  from   user_advisor_sqlw_sum
	  where  workload_name = sts_name;
	  IF num_found > 0 THEN
		dbms_sqltune.delete_sqlset(sts_name);
	  END IF;

	  /* Create STS */
	  dbms_sqltune.create_sqlset(sts_name, 'Obtain workload from cursor cache');

	  /* Select all statements in the cursor cache. */
	  OPEN sts_cursor FOR
		SELECT VALUE(P)
		FROM TABLE(dbms_sqltune.select_cursor_cache) P;

	  /* Load the statements into STS. */
	  dbms_sqltune.load_sqlset(sts_name, sts_cursor);
	  CLOSE sts_cursor;

	  /* Link STS Workload to Task */
	  dbms_advisor.add_sqlwkld_ref(taskname,sts_name,1);

	  /* Set STS Workload Parameters */
	  dbms_advisor.set_task_parameter(taskname,'VALID_ACTION_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'VALID_MODULE_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'SQL_LIMIT','25');
	  dbms_advisor.set_task_parameter(taskname,'VALID_USERNAME_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'VALID_TABLE_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'INVALID_TABLE_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'INVALID_ACTION_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'INVALID_USERNAME_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'INVALID_MODULE_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'VALID_SQLSTRING_LIST',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'INVALID_SQLSTRING_LIST','"@!"');

	  /* Set Task Parameters */
	  dbms_advisor.set_task_parameter(taskname,'ANALYSIS_SCOPE','ALL');
	  dbms_advisor.set_task_parameter(taskname,'RANKING_MEASURE','PRIORITY,OPTIMIZER_COST');
	  dbms_advisor.set_task_parameter(taskname,'DEF_PARTITION_TABLESPACE',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'TIME_LIMIT',10000);
	  dbms_advisor.set_task_parameter(taskname,'MODE','LIMITED');
	  dbms_advisor.set_task_parameter(taskname,'STORAGE_CHANGE',DBMS_ADVISOR.ADVISOR_UNLIMITED);
	  dbms_advisor.set_task_parameter(taskname,'DML_VOLATILITY','TRUE');
	  dbms_advisor.set_task_parameter(taskname,'WORKLOAD_SCOPE','PARTIAL');
	  dbms_advisor.set_task_parameter(taskname,'DEF_INDEX_TABLESPACE',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'DEF_INDEX_OWNER',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'DEF_MVIEW_TABLESPACE',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'DEF_MVIEW_OWNER',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'DEF_MVLOG_TABLESPACE',DBMS_ADVISOR.ADVISOR_UNUSED);
	  dbms_advisor.set_task_parameter(taskname,'CREATION_COST','TRUE');
	  dbms_advisor.set_task_parameter(taskname,'JOURNALING','4');
	  dbms_advisor.set_task_parameter(taskname,'DAYS_TO_EXPIRE','30');

	  /* Execute Task */
	  dbms_advisor.execute_task(taskname);
	END;
	/

	-- Display the resulting script.
	SET LONG 100000
	SET PAGESIZE 50000
	SELECT DBMS_ADVISOR.get_task_script('test_sql_access_task') AS script FROM   dual;
	SET PAGESIZE 24


##quick tune

If you just want to tune an individual statement you can use the QUICK_TUNE procedure as follows.

	BEGIN
	  DBMS_ADVISOR.quick_tune(
		advisor_name => DBMS_ADVISOR.SQLACCESS_ADVISOR, 
		task_name    => 'bigtab_quick_tune_test',
		attr1        => 'SELECT b.* FROM bigtab b WHERE UPPER(b.object_name) = ''BIGTAB''');
	END;
	/


##views definition

* DBA_ADVISOR_TASKS - Basic information about existing tasks.
* DBA_ADVISOR_LOG - Status information about existing tasks.
* DBA_ADVISOR_FINDINGS - Findings identified for an existing task.
* DBA_ADVISOR_RECOMMENDATIONS - Recommendations for the problems identified by an existing task.

##Reference

* Oracle Database Performance Tuning Guide
---
layout: post
title: Oracle SQL Access Path Introduction
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

##Access Path

Access paths are ways in which data is retrieved from the database. In general, index access paths are useful for statements that retrieve a small subset of table rows, whereas full scans are more efficient when accessing a large portion of the table. Online transaction processing (OLTP) applications, which consist of short-running SQL statements with high selectivity, often are characterized by the use of index access paths. Decision support systems, however, tend to use partitioned tables and perform full scans of the relevant partitions. 

The query optimizer chooses an access path based on the following factors: 

* The available access paths for the statement
* The estimated cost of executing the statement, using each access path or combination of paths

To choose an access path, the optimizer first determines which access paths are available by examining the conditions in the statement's  WHERE  clause and its  FROM clause. The optimizer then generates a set of possible execution plans using available access paths and estimates the cost of each plan, using the statistics for the index, columns, and tables accessible to the statement. Finally, the optimizer chooses the execution plan with the lowest estimated cost. 

When choosing an access path, the query optimizer is influenced by the following:

* Optimizer Hints. You can instruct the optimizer to use a specific access path using a hint, except when the statement's  FROM  clause contains  `SAMPLE`  or  `SAMPLE BLOCK` . 
* Old Statistics. For example, if a table has not been analyzed since it was created, and if it has less than  `DB_FILE_MULTIBLOCK_READ_COUNT` blocks under the high water mark, then the optimizer thinks that the table is small and uses a full table scan. Review the  `LAST_ ANALYZED`  and  `BLOCKS`  columns in the  `ALL_TABLES` table to examine the statistics.

###Full Table Scans

This type of scan reads all rows from a table and filters out those that do not meet the selection criteria. During a full table scan, all blocks in the table that are under the high water mark are scanned. The high water mark indicates the amount of used space, or space that had been formatted to receive data. Each row is examined to determine whether it satisfies the statement's  `WHERE  clause`. 

When Oracle Database performs a full table scan, the blocks are read sequentially. Because the blocks are adjacent, the database can make I/O calls larger than a single block to speed up the process. The size of the read calls range from one block to the number of blocks indicated by the initialization parameter  `DB_FILE_MULTIBLOCK_READ_ COUNT` . Using multiblock reads, the database can perform a full table scan very efficiently. The database reads each block only once. 
 
Full table scans are cheaper than index range scans when accessing a large fraction of the blocks in a table. Full table scans can use larger I/O calls, and making fewer large I/O calls is cheaper than making many smaller calls.

###Rowid Scan

The rowid of a row specifies the data file and data block containing the row and the location of the row in that block. Locating a row by specifying its rowid is the fastest way to retrieve a single row, because the exact location of the row in the database is specified. 

To access a table by rowid, Oracle Database first obtains the rowids of the selected rows, either from the statement's  WHERE  clause or through an index scan of one or more of the table's indexes. Oracle Database then locates each selected row in the table based on its rowid. 

###Index Scans

In this method, a row is retrieved by traversing the index, using the indexed column values specified by the statement. An index scan retrieves data from an index based on the value of one or more columns in the index. To perform an index scan, Oracle Database searches the index for the indexed column values accessed by the statement. If the statement accesses only columns of the index, then Oracle Database reads the indexed column values directly from the index, rather than from the table. 

The index contains not only the indexed value, but also the rowids of rows in the table having that value. Therefore, if the statement accesses other columns in addition to the indexed columns, then Oracle Database can find the rows in the table by using either a table access by rowid or a cluster scan. 

In this method, a row is retrieved by traversing the index, using the indexed column values specified by the statement. An index scan retrieves data from an index based on the value of one or more columns in the index. To perform an index scan, Oracle Database searches the index for the indexed column values accessed by the statement. If the statement accesses only columns of the index, then Oracle Database reads the indexed column values directly from the index, rather than from the table. 

The index contains not only the indexed value, but also the rowids of rows in the table having that value. Therefore, if the statement accesses other columns in addition to the indexed columns, then Oracle Database can find the rows in the table by using either a table access by rowid or a cluster scan. 

* Assessing I/O for Blocks, not Rows
* Index Unique Scans
* Index Range Scans
* Index Range Scans Descending
* Index Skip Scans
* Full Scans
* Fast Full Index Scans
* Index Joins
* Bitmap Indexes

####I/O Blocks

Oracle Database performs I/O by blocks. Therefore, the optimizer's decision to use full table scans is influenced by the percentage of blocks accessed, not rows. This is called the index clustering factor. If blocks contain single rows, then rows accessed and blocks accessed are the same.

However, most tables have multiple rows in each block. Consequently, the desired number of rows may be clustered in a few blocks or spread out over a larger number of blocks.
 
Although the clustering factor is a property of the index, the clustering factor actually relates to the spread of similar indexed column values within data blocks in the table.
 
A lower clustering factor indicates that the individual rows are concentrated within fewer blocks in the table. Conversely, a high clustering factor indicates that the individual rows are scattered more randomly across blocks in the table. Therefore, a high clustering factor means that it costs more to use a range scan to fetch rows by rowid, because more blocks in the table need to be visited to return the data. 

####Index Unique Scans

This scan returns, at most, a single rowid. Oracle Database performs a unique scan if a statement contains a  `UNIQUE`  or a  `PRIMARY KEY`  constraint that guarantees that only a single row is accessed. The database uses this access path when the user specifies all columns of a unique (B-tree) index or an index created as a result of a primary key constraint with equality conditions.

####Index Range Scans

An index range scan is a common operation for accessing selective data. It can be bounded (bounded on both sides) or unbounded (on one or both sides). Data is returned in the ascending order of index columns. Multiple rows with identical values are sorted in ascending order by rowid.

If you require the data to be sorted by order, then use the  ORDER BY  clause, and do not rely on an index. If an index can satisfy an  ORDER BY  clause, then the optimizer uses this option and avoids a sort. The optimizer uses a range scan when it finds one or more leading columns of an index specified in conditions

####Index Skip Scans

Index skip scans improve index scans by nonprefix columns. Often, scanning index blocks is faster than scanning table data blocks.

Skip scanning lets a composite index be split logically into smaller subindexes. In skip scanning, the initial column of the composite index is not specified in the query. In other words, it is skipped.

The database determines the number of logical subindexes by the number of distinct values in the initial column. Skip scanning is advantageous when there are few distinct values in the leading column of the composite index and many distinct values in the nonleading key of the index.

####Full Scans

A full index scan eliminates a sort operation, because the data is ordered by the index key. It reads the blocks singly. Oracle Database may use a full scan in any of the following situations:

An  ORDER BY  clause that meets the following requirements is present in the query:

* All of the columns in the  ORDER BY  clause must be in the index.
* The order of the columns in the  ORDER BY  clause must match the order of the leading index columns.

The  ORDER BY  clause can contain all of the columns in the index or a subset of the 	columns in the index. 
	
The query requires a sort merge join. The database can perform a full index scan instead of doing a full table scan followed by a sort when the query meets the following requirements:

* All of the columns referenced in the query must be in the index.
* The order of the columns referenced in the query must match the order of the leading index columns.

The query can contain all of the columns in the index or a subset of the columns in the index.

* A  GROUP BY  clause is present in the query, and the columns in the  GROUP BY  clause are present in the index. The columns do not need to be in the same order in the index and the  GROUP BY  clause. The  GROUP BY  clause can contain all of the columns in the index or a subset of the columns in the index.

####Index Joins

An index join is a hash join of several indexes that together contain all the table columns referenced in the query. If the database uses an index join, then table access is not needed because the database can retrieve all the relevant column values from the indexes. The database cannot use an index join cannot to eliminate a sort operation. 

####Bitmap Indexes

A bitmap join uses a bitmap for key values and a mapping function that converts each bit position to a rowid. Bitmaps can efficiently merge indexes that correspond to several conditions in a  WHERE  clause, using Boolean operations to resolve  AND  and  OR conditions.

####Cluster Access

The database uses a cluster scan to retrieve all rows that have the same cluster key value from a table stored in an indexed cluster. In an indexed cluster, the database stores all rows with the same cluster key value in the same data block. To perform a cluster scan, Oracle Database first obtains the rowid of one of the selected rows by scanning the cluster index. Oracle Database then locates the rows based on this rowid. 

####Hash Access

The database uses a hash scan to locate rows in a hash cluster based on a hash value. In a hash cluster, all rows with the same hash value are stored in the same data block. To perform a hash scan, Oracle Database first obtains the hash value by applying a hash function to a cluster key value specified by the statement. Oracle Database then scans the data blocks containing rows with that hash value. 

####Sample Table Scans

A sample table scan retrieves a random sample of data from a simple table or a complex  `SELECT  statement`, such as a statement involving joins and views. The database uses this access path when a statement's  `FROM  clause` includes the  `SAMPLE clause` or the  `SAMPLE BLOCK  clause`. To perform a sample table scan when sampling by rows with the  `SAMPLE  clause`, the database reads a specified percentage of rows in the table. To perform a sample table scan when sampling by blocks with the  `SAMPLE BLOCK clause`, the database reads a specified percentage of table blocks. 

##Reference

* Oracle Database Performance Tuning Guide
---
layout: post
title: Oracle SQL Access Path Examples
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

##Init `SQL*Plus`

    SQL> set timing on
    SQL> set autot on exp
    SQL> set linesize 200

##Full Table Scan

    SQL> select count(1) from bigtab where owner='DEV';

      COUNT(1)
    ----------
          2827

    Elapsed: 00:00:01.85

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2140185107

    -----------------------------------------------------------------------------
    | Id  | Operation	   | Name   | Rows  | Bytes | Cost (%CPU)| Time     |
    -----------------------------------------------------------------------------
    |   0 | SELECT STATEMENT   |	    |	  1 |	  6 | 30445   (1)| 00:06:06 |
    |   1 |  SORT AGGREGATE    |	    |	  1 |	  6 |		 |	    |
    |*  2 |   TABLE ACCESS FULL| BIGTAB |	203K|  1191K| 30445   (1)| 00:06:06 |
    -----------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - filter("OWNER"='DEV')

##Rowid Scan   
   
    SQL> select rowid from bigtab where owner='DEV' and rownum <= 1;

    ROWID
    ------------------
    AAAXKzAAoAAAATVAA+

    Elapsed: 00:00:00.03

    SQL> select count(1) from bigtab where  rowid = 'AAAXKzAAoAAAATVAA+';

      COUNT(1)
    ----------
         1

    Elapsed: 00:00:00.01

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 989695095

    --------------------------------------------------------------------------------------
    | Id  | Operation		    | Name   | Rows  | Bytes | Cost (%CPU)| Time     |
    --------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |	     |	   1 |	  12 |	   1   (0)| 00:00:01 |
    |   1 |  SORT AGGREGATE 	    |	     |	   1 |	  12 |		  |	     |
    |   2 |   TABLE ACCESS BY USER ROWID| BIGTAB |	   1 |	  12 |	   1   (0)| 00:00:01 |
    --------------------------------------------------------------------------------------

create non-unique index
    
    SQL> create index bigtab_idx on bigtab(owner);
    
    Elapsed: 00:00:17.00
    
    SQL> select count(1) from bigtab where owner='DEV';

      COUNT(1)
    ----------
          2827

    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 397841597

    --------------------------------------------------------------------------------
    | Id  | Operation	  | Name       | Rows  | Bytes | Cost (%CPU)| Time     |
    --------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT  |	       |     1 |     6 |   480	 (1)| 00:00:06 |
    |   1 |  SORT AGGREGATE   |	       |     1 |     6 |	    |	       |
    |*  2 |   INDEX RANGE SCAN| BIGTAB_IDX |   203K|  1191K|   480	 (1)| 00:00:06 |
    --------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("OWNER"='DEV')

    SQL> select object_name from bigtab where owner='DEV';
    ... ... 
    Elapsed: 00:00:00.20

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 346967093

    ------------------------------------------------------------------------------------------
    | Id  | Operation		    | Name	 | Rows  | Bytes | Cost (%CPU)| Time	 |
    ------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |		 |   203K|  6154K|  6143   (1)| 00:01:14 |
    |   1 |  TABLE ACCESS BY INDEX ROWID| BIGTAB	 |   203K|  6154K|  6143   (1)| 00:01:14 |
    |*  2 |   INDEX RANGE SCAN	    | BIGTAB_IDX |   203K|	 |   480   (1)| 00:00:06 |
    ------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("OWNER"='DEV')

    SQL> select owner from bigtab where owner='DEV';
    ... ...
    Elapsed: 00:00:00.04

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 3859841128

    -------------------------------------------------------------------------------
    | Id  | Operation	 | Name       | Rows  | Bytes | Cost (%CPU)| Time     |
    -------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT |	      |   203K|  1191K|   480	(1)| 00:00:06 |
    |*  1 |  INDEX RANGE SCAN| BIGTAB_IDX |   203K|  1191K|   480	(1)| 00:00:06 |
    -------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - access("OWNER"='DEV')

##Index Scans

###Index Unique Scans

    SQL> select  * from smalltab where id = 1;
    ... ...
    Elapsed: 00:00:00.01

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2765411029

    ------------------------------------------------------------------------------
    | Id  | Operation	  | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    ------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT  |	     |	   1 |	 245 |	  35   (0)| 00:00:01 |
    |*  1 |  TABLE ACCESS FULL| SMALLTAB |	   1 |	 245 |	  35   (0)| 00:00:01 |
    ------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - filter("ID"=1)

add primary key
       
    SQL> alter table smalltab add constraint smalltab_pk primary key (id);

    Table altered.

    Elapsed: 00:00:00.17
    SQL> select  * from smalltab where id = 1;
    ... ...
    Elapsed: 00:00:00.02

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 3259460393

    -------------------------------------------------------------------------------------------
    | Id  | Operation		    | Name	  | Rows  | Bytes | Cost (%CPU)| Time	  |
    -------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |		  |	1 |   245 |	2   (0)| 00:00:01 |
    |   1 |  TABLE ACCESS BY INDEX ROWID| SMALLTAB	  |	1 |   245 |	2   (0)| 00:00:01 |
    |*  2 |   INDEX UNIQUE SCAN	    | SMALLTAB_PK |	1 |	  |	1   (0)| 00:00:01 |
    -------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("ID"=1)
       
create unique index

    SQL> alter table smalltab drop constraint smalltab_pk;

    Table altered.

    Elapsed: 00:00:00.08
    SQL> create unique index smalltab_idx on smalltab(id);

    Index created.

    Elapsed: 00:00:00.02
    
    SQL> select  * from smalltab where id = 1;
    ... ...
    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 3320624552

    --------------------------------------------------------------------------------------------
    | Id  | Operation		    | Name	   | Rows  | Bytes | Cost (%CPU)| Time	   |
    --------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |		   |	 1 |   245 |	 2   (0)| 00:00:01 |
    |   1 |  TABLE ACCESS BY INDEX ROWID| SMALLTAB	   |	 1 |   245 |	 2   (0)| 00:00:01 |
    |*  2 |   INDEX UNIQUE SCAN	    | SMALLTAB_IDX |	 1 |	   |	 1   (0)| 00:00:01 |
    --------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("ID"=1)

###Index Range Scans

    SQL> create index smalltab_owner_idx on smalltab(owner);

    Index created.

    Elapsed: 00:00:00.03
    
`=`
    
    SQL> select * from smalltab where owner = 'dev';

    no rows selected

    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2274244531

    --------------------------------------------------------------------------------------------------
    | Id  | Operation		    | Name		 | Rows  | Bytes | Cost (%CPU)| Time	 |
    --------------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |			 |    48 | 11760 |     5   (0)| 00:00:01 |
    |   1 |  TABLE ACCESS BY INDEX ROWID| SMALLTAB		 |    48 | 11760 |     5   (0)| 00:00:01 |
    |*  2 |   INDEX RANGE SCAN	    | SMALLTAB_OWNER_IDX |    48 |	 |     1   (0)| 00:00:01 |
    --------------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("OWNER"='dev')

`<`
       
    SQL> select * from smalltab where owner < 'abc';
    ... ...
    Elapsed: 00:00:12.26

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2765411029

    ------------------------------------------------------------------------------
    | Id  | Operation	  | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    ------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT  |	     |	3157 |	 755K|	  35   (0)| 00:00:01 |
    |*  1 |  TABLE ACCESS FULL| SMALLTAB |	3157 |	 755K|	  35   (0)| 00:00:01 |
    ------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - filter("OWNER"<'abc')
  
`>`
  
    SQL> select * from smalltab where owner > 'abc';

    no rows selected

    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2274244531

    --------------------------------------------------------------------------------------------------
    | Id  | Operation		    | Name		 | Rows  | Bytes | Cost (%CPU)| Time	 |
    --------------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |			 |    61 | 14945 |     7   (0)| 00:00:01 |
    |   1 |  TABLE ACCESS BY INDEX ROWID| SMALLTAB		 |    61 | 14945 |     7   (0)| 00:00:01 |
    |*  2 |   INDEX RANGE SCAN	    | SMALLTAB_OWNER_IDX |    61 |	 |     2   (0)| 00:00:01 |
    --------------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("OWNER">'abc')
 
`like` 
 
    SQL> select * from smalltab where owner like 'abc%';

    no rows selected

    Elapsed: 00:00:00.01

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2274244531

    --------------------------------------------------------------------------------------------------
    | Id  | Operation		    | Name		 | Rows  | Bytes | Cost (%CPU)| Time	 |
    --------------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |			 |   102 | 24990 |    10   (0)| 00:00:01 |
    |   1 |  TABLE ACCESS BY INDEX ROWID| SMALLTAB		 |   102 | 24990 |    10   (0)| 00:00:01 |
    |*  2 |   INDEX RANGE SCAN	    | SMALLTAB_OWNER_IDX |   102 |	 |     2   (0)| 00:00:01 |
    --------------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("OWNER" LIKE 'abc%')
           filter("OWNER" LIKE 'abc%')
       
    SQL> select * from smalltab where owner like '%abc';

    no rows selected

    Elapsed: 00:00:00.01

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2765411029

    ------------------------------------------------------------------------------
    | Id  | Operation	  | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    ------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT  |	     |	 158 | 38710 |	  35   (0)| 00:00:01 |
    |*  1 |  TABLE ACCESS FULL| SMALLTAB |	 158 | 38710 |	  35   (0)| 00:00:01 |
    ------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - filter("OWNER" LIKE '%abc')
       
### Index Range Scans Descending

    SQL> select /*+index_desc(smalltab smalltab_owner_idx) */ count(1) from smalltab where owner = 'dev';

      COUNT(1)
    ----------
         0

    Elapsed: 00:00:00.01

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 3707260210

    ---------------------------------------------------------------------------------------------------
    | Id  | Operation		     | Name		  | Rows  | Bytes | Cost (%CPU)| Time	  |
    ---------------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	     |			  |	1 |	7 |	1   (0)| 00:00:01 |
    |   1 |  SORT AGGREGATE 	     |			  |	1 |	7 |	       |	  |
    |*  2 |   INDEX RANGE SCAN DESCENDING| SMALLTAB_OWNER_IDX |    48 |   336 |	1   (0)| 00:00:01 |
    ---------------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("OWNER"='dev')
           filter("OWNER"='dev')

### Index Skip Scans

    SQL> create index smalltab_skip_idx on smalltab(owner, tablespace_name);

    Index created.

    Elapsed: 00:00:00.02

    SQL> select * from smalltab where tablespace_name = 'x' ;

    no rows selected

    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2078826789

    -------------------------------------------------------------------------------------------------
    | Id  | Operation		    | Name		| Rows	| Bytes | Cost (%CPU)| Time	|
    -------------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT	    |			|     1 |   245 |    33   (0)| 00:00:01 |
    |   1 |  TABLE ACCESS BY INDEX ROWID| SMALLTAB		|     1 |   245 |    33   (0)| 00:00:01 |
    |*  2 |   INDEX SKIP SCAN	    | SMALLTAB_SKIP_IDX |     1 |	|    32   (0)| 00:00:01 |
    -------------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("TABLESPACE_NAME"='x')
           filter("TABLESPACE_NAME"='x')

### Full Scans

#### Fast Full Index Scans

    SQL> drop index smalltab_owner_idx;

    Index dropped.

    Elapsed: 00:00:00.02
    
    SQL> create index smalltab_ffs_idx on smalltab(owner,status);

    Index created.

    Elapsed: 00:00:00.03
    
    SQL> select count(1) from smalltab where status = 'VALID' ;

      COUNT(1)
    ----------
          3157

    Elapsed: 00:00:00.01

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2296131484

    -------------------------------------------------------------------------------------------
    | Id  | Operation	      | Name		  | Rows  | Bytes | Cost (%CPU)| Time	  |
    -------------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT      | 		  |	1 |	6 |	5   (0)| 00:00:01 |
    |   1 |  SORT AGGREGATE       | 		  |	1 |	6 |	       |	  |
    |*  2 |   INDEX FAST FULL SCAN| SMALLTAB_FFS_IDX |  3157 | 18942 |	5   (0)| 00:00:01 |
    -------------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - filter("STATUS"='VALID')

## Index Joins

Todo ...

## Bitmap Indexes

Todo ...

## Cluster Access

## Hash Access

## Sample Table Scans

    SQL> select * from smalltab where owner = 'dev';

    no rows selected

    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 2765411029

    ------------------------------------------------------------------------------
    | Id  | Operation	  | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    ------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT  |	     |	  48 | 11760 |	  35   (0)| 00:00:01 |
    |*  1 |  TABLE ACCESS FULL| SMALLTAB |	  48 | 11760 |	  35   (0)| 00:00:01 |
    ------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - filter("OWNER"='dev')

    SQL> select * from smalltab sample block (1) where owner = 'dev';

    no rows selected

    Elapsed: 00:00:00.03

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 1616664914

    --------------------------------------------------------------------------------
    | Id  | Operation	    | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    --------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT    |	       |     1 |   245 |     2	 (0)| 00:00:01 |
    |*  1 |  TABLE ACCESS SAMPLE| SMALLTAB |     1 |   245 |     2	 (0)| 00:00:01 |
    --------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - filter("OWNER"='dev')

    SQL> select * from smalltab sample block (10) where owner = 'dev';

    no rows selected

    Elapsed: 00:00:00.01

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 1616664914

    --------------------------------------------------------------------------------
    | Id  | Operation	    | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    --------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT    |	       |     5 |  1225 |     5	 (0)| 00:00:01 |
    |*  1 |  TABLE ACCESS SAMPLE| SMALLTAB |     5 |  1225 |     5	 (0)| 00:00:01 |
    --------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - filter("OWNER"='dev')

    SQL> select * from smalltab sample block (10) where owner = 'DEV';

    no rows selected

    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 1616664914

    --------------------------------------------------------------------------------
    | Id  | Operation	    | Name     | Rows  | Bytes | Cost (%CPU)| Time     |
    --------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT    |	       |    10 |  2450 |     5	 (0)| 00:00:01 |
    |*  1 |  TABLE ACCESS SAMPLE| SMALLTAB |    10 |  2450 |     5	 (0)| 00:00:01 |
    --------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       1 - filter("OWNER"='DEV')
	   
##Reference

* Oracle Database Performance Tuning Guide
---
layout: post
title: Oracle SQL Join Introduction
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

##Joins

Joins are statements that retrieve data from multiple tables. A join is characterized by multiple tables in the  FROM  clause. The existence of a join condition in the  WHERE  clause defines the relationship between the tables. In a join, one row set is called inner, and the other is called outer.

To choose an execution plan for a join statement, the optimizer must make these interrelated decisions:
* Access Paths. As for simple statements, the optimizer must choose an access path to retrieve data from each table in the join statement. 
* Join Method. To join each pair of row sources, Oracle Database must perform a join operation. Join methods include nested loop, sort merge, cartesian, and hash joins.
* Join Order. To execute a statement that joins more than two tables, Oracle Database joins two of the tables and then joins the resulting row source to the next table. This process continues until all tables are joined into the result. 


How the Query Optimizer Chooses Execution Plans for Joins

The query optimizer considers the following when choosing an execution plan: 

* The optimizer first determines whether joining two or more tables definitely results in a row source containing at most one row. The optimizer recognizes such situations based on  UNIQUE  and  PRIMARY KEY  constraints on the tables. If such a situation exists, then the optimizer places these tables first in the join order. The optimizer then optimizes the join of the remaining set of tables. 
* For join statements with outer join conditions, the table with the outer join operator must come after the other table in the condition in the join order. The optimizer does not consider join orders that violate this rule. Similarly, when a subquery has been converted into an antijoin or semijoin, the tables from the subquery must come after those tables in the outer query block to which they were connected or correlated. However, hash antijoins and semijoins are able to override this ordering condition in certain circumstances.

With the query optimizer, the optimizer generates a set of execution plans, according to possible join orders, join methods, and available access paths. The optimizer then estimates the cost of each plan and chooses the one with the lowest cost. The optimizer estimates costs in the following ways: 

* The cost of a nested loops operation is based on the cost of reading each selected row of the outer table and each of its matching rows of the inner table into memory. The optimizer estimates these costs using the statistics in the data dictionary. 
* The cost of a sort merge join is based largely on the cost of reading all the sources into memory and sorting them. 
* The cost of a hash join is based largely on the cost of building a hash table on one of the input sides to the join and using the rows from the other of the join to probe it.

The optimizer also considers other factors when determining the cost of each operation. For example: 

* A smaller sort area size is likely to increase the cost for a sort merge join because sorting takes more CPU time and I/O in a smaller sort area.
* A larger multiblock read count is likely to decrease the cost for a sort merge join in relation to a nested loop join. If the database can read a large number of sequential blocks from disk in a single I/O, then an index on the inner table for the nested loop join is less likely to improve performance over a full table scan. The multiblock read count is specified by the initialization parameter  DB_FILE_ MULTIBLOCK_READ_COUNT . 

You can use the  ORDERED  hint to override the optimizer's choice of join orders. If the ORDERED  hint specifies a join order that violates the rule for an outer join, then the optimizer ignores the hint and chooses the order. Also, you can override the optimizer's choice of join method with hints. 

##Nested Loop Joins

Nested loop joins are useful when the following conditions are true:

* The database joins small subsets of data.
* The join condition is an efficient method of accessing the second table.

It is important to ensure that the inner table is driven from (dependent on) the outer table. If the inner table's access path is independent of the outer table, then the same rows are retrieved for every iteration of the outer loop, degrading performance considerably. In such cases, hash joins joining the two independent row sources perform better.

A nested loop join involves the following steps:

* The optimizer determines the driving table and designates it as the outer table.
* The other table is designated as the inner table.
* For every row in the outer table, Oracle Database accesses all the rows in the inner table. The outer loop is for every row in the outer table and the inner loop is for every row in the inner table. The outer loop appears before the inner loop in the execution plan, as follows:

    NESTED LOOPS 
    outer_loop
    inner_loop
    
When the Optimizer Uses Nested Loop Joins

The optimizer uses nested loop joins when joining small number of rows, with a good driving condition between the two tables. You drive from the outer loop to the inner loop, so the order of tables in the execution plan is important.

The outer loop is the driving row source. It produces a set of rows for driving the join condition. The row source can be a table accessed using an index scan or a full table scan. Also, the rows can be produced from any other operation. For example, the output from a nested loop join can serve as a row source for another nested loop join.

The inner loop is iterated for every row returned from the outer loop, ideally by an index scan. If the access path for the inner loop is not dependent on the outer loop, then you can end up with a Cartesian product; for every iteration of the outer loop, the inner loop produces the same set of rows. Therefore, you should use other join methods when two independent row sources are joined together.

##Hash Joins

The database uses hash joins to join large data sets. The optimizer uses the smaller of two tables or data sources to build a hash table on the join key in memory. It then scans the larger table, probing the hash table to find the joined rows.

This method is best when the smaller table fits in available memory. The cost is then limited to a single read pass over the data for the two tables.

When the Optimizer Uses Hash Joins

The optimizer uses a hash join to join two tables if they are joined using an equijoin and if either of the following conditions are true:

* A large amount of data must be joined.
* A large fraction of a small table must be joined.

##Sort Merge Joins

Sort merge joins can join rows from two independent sources. Hash joins generally perform better than sort merge joins. However, sort merge joins can perform better than hash joins if both of the following conditions exist:

* The row sources are sorted already.
* A sort operation does not have to be done.

However, if a sort merge join involves choosing a slower access method (an index scan as opposed to a full table scan), then the benefit of using a sort merge might be lost. Sort merge joins are useful when the join condition between two tables is an inequality condition such as  < ,  <= ,  > , or  >= . Sort merge joins perform better than nested loop joins for large data sets. You cannot use hash joins unless there is an equality condition. In a merge join, there is no concept of a driving table. The join consists of two steps:

* Sort join operation: Both the inputs are sorted on the join key.
* Merge join operation: The sorted lists are merged together.

If the input is sorted by the join column, then a sort join operation is not performed for that row source. However, a sort merge join always creates a positionable sort buffer for the right side of the join so that it can seek back to the last match in the case where duplicate join key values come out of the left side of the join.

When the Optimizer Uses Sort Merge Joins

The optimizer can choose a sort merge join over a hash join for joining large amounts of data if any of the following conditions are true:

* The join condition between two tables is not an equijoin.
* Because of sorts required by other operations, the optimizer finds it is cheaper to use a sort merge than a hash join.

Sort Merge Join Hints

To instruct the optimizer to use a sort merge join, apply the  USE_MERGE  hint. You might also need to give hints to force an access path. 

There are situations where it makes sense to override the optimizer with the  USE_ MERGE  hint. For example, the optimizer can choose a full scan on a table and avoid a sort operation in a query. However, there is an increased cost because a large table is accessed through an index and single block reads, as opposed to faster access through a full table scan.

##Cartesian Joins

The database uses a Cartesian join when one or more of the tables does not have any join conditions to any other tables in the statement. The optimizer joins every row from one data source with every row from the other data source, creating the Cartesian product of the two sets.

When the Optimizer Uses Cartesian Joins

The optimizer uses Cartesian joins when it is asked to join two tables with no join conditions. In some cases, a common filter condition between the two tables could be picked up by the optimizer as a possible join condition. In other cases, the optimizer may decide to generate a Cartesian product of two very small tables that are both joined to the same large table.

Cartesian Join Hints

Applying the  ORDERED  hint, instructs the optimizer to use a Cartesian join. By specifying a table before its join table is specified, the optimizer does a Cartesian join. 

##Outer Joins

An outer join extends the result of a simple join. An outer join returns all rows that satisfy the join condition and also returns some or all of those rows from one table for which no rows from the other satisfy the join condition.

Nested Loop Outer Joins

The database uses this operation to loop through an outer join between two tables. The outer join returns the outer (preserved) table rows, even when no corresponding rows are in the inner (optional) table. 

In a regular outer join, the optimizer chooses the order of tables (driving and driven) based on the cost. However, in a nested loop outer join, the join condition determines the order of tables. The database uses the outer table, with rows that are being preserved, to drive to the inner table.

The optimizer uses nested loop joins to process an outer join in the following circumstances:

* It is possible to drive from the outer table to inner table.
* Data volume is low enough to make the nested loop method efficient.

##Reference

* Oracle Database Performance Tuning Guide
---
layout: post
title: Oracle SQL Join Examples
category : Oracle
tags : [Oracle, Database, DBA, Performance]
---

Create tables

    SQL> create table smalltab as select * from user_tables;

    Table created.

    Elapsed: 00:00:00.27

    SQL> create table bigtab as select * from dba_objects;

    Table created.

    Elapsed: 00:00:00.96
    SQL> insert into bigtab select * from dba_objects;

    74472 rows created.

    Commit complete.
    Elapsed: 00:00:00.83
    
Nested Loops
    
    SQL> select count(1)
      2  from smalltab a,
      3       smalltab b
      4  where a.table_name = b.table_name;

      COUNT(1)
    ----------
         4

    Elapsed: 00:00:00.00

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 3911555197

    -----------------------------------------------------------------------------------
    | Id  | Operation	   | Name	  | Rows  | Bytes | Cost (%CPU)| Time	  |
    -----------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT   |		  |	1 |    34 |	1   (0)| 00:00:01 |
    |   1 |  SORT AGGREGATE    |		  |	1 |    34 |	       |	  |
    |   2 |   NESTED LOOPS	   |		  |	4 |   136 |	1   (0)| 00:00:01 |
    |   3 |    INDEX FULL SCAN | SMALLTAB_IDX |	4 |    68 |	1   (0)| 00:00:01 |
    |*  4 |    INDEX RANGE SCAN| SMALLTAB_IDX |	1 |    17 |	0   (0)| 00:00:01 |
    -----------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       4 - access("A"."TABLE_NAME"="B"."TABLE_NAME")
       
Hash Join

    SQL> select count(1)
      2  from smalltab s,
      3       bigtab b
      4  where s.table_name = b.object_name;

      COUNT(1)
    ----------
         7

    Elapsed: 00:00:00.03

    Execution Plan
    ----------------------------------------------------------
    Plan hash value: 178863919

    ------------------------------------------------------------------------------------
    | Id  | Operation	    | Name	   | Rows  | Bytes | Cost (%CPU)| Time	   |
    ------------------------------------------------------------------------------------
    |   0 | SELECT STATEMENT    |		   |	 1 |	83 |   298   (1)| 00:00:04 |
    |   1 |  SORT AGGREGATE     |		   |	 1 |	83 |		|	   |
    |*  2 |   HASH JOIN	    |		   |	50 |  4150 |   298   (1)| 00:00:04 |
    |   3 |    INDEX FULL SCAN  | SMALLTAB_IDX |	 4 |	68 |	 1   (0)| 00:00:01 |
    |   4 |    TABLE ACCESS FULL| BIGTAB	   | 65625 |  4229K|   297   (1)| 00:00:04 |
    ------------------------------------------------------------------------------------

    Predicate Information (identified by operation id):
    ---------------------------------------------------

       2 - access("S"."TABLE_NAME"="B"."OBJECT_NAME")
	   
##Reference

* Oracle Database Performance Tuning Guide
---
layout: post
title: Oracle Undo Introduction
category : Oracle
tags : [Oracle, Database, DBA]
---

##Undo

Oracle Database creates and manages information that is used to roll back, or undo, changes to the database. Such information consists of records of the actions of transactions, primarily before they are committed. These records are collectively referred to as undo. 

Undo records are used to:

* Roll back transactions when a ROLLBACK statement is issued
* Recover the database
* Provide read consistency
* Analyze data as of an earlier point in time by using Oracle Flashback Query
* Recover from logical corruptions using Oracle Flashback features

When a `ROLLBACK` statement is issued, undo records are used to undo changes that were made to the database by the uncommitted transaction. During database recovery, undo records are used to undo any uncommitted changes applied from the redo log to the datafiles. Undo records provide read consistency by maintaining the before image of the data for users who are accessing the data at the same time that another user is changing it.

##Undo Retention

After a transaction is committed, undo data is no longer needed for rollback or transaction recovery purposes. However, for consistent read purposes, long-running queries may require this old undo information for producing older images of data blocks. Furthermore, the success of several Oracle Flashback features can also depend upon the availability of older undo information. For these reasons, it is desirable to retain the old undo information for as long as possible.

When automatic undo management is enabled, there is always a current undo retention period, which is the minimum amount of time that Oracle Database attempts to retain old undo information before overwriting it. Old (committed) undo information that is older than the current undo retention period is said to be expired and its space is available to be overwritten by new transactions. Old undo information with an age that is less than the current undo retention period is said to be unexpired and is retained for consistent read and Oracle Flashback operations.

To guarantee the success of long-running queries or Oracle Flashback operations, you can enable retention guarantee. If retention guarantee is enabled, the specified minimum undo retention is guaranteed; the database never overwrites unexpired undo data even if it means that transactions fail due to lack of space in the undo tablespace. If retention guarantee is not enabled, the database can overwrite unexpired undo when space is low, thus lowering the undo retention for the system. This option is disabled by default.

##Reference

* Oracle Database Administrator's Guide
---
layout: post
title: Oracle Flashbackup Examples
category : Oracle
tags : [Oracle, Database, DBA]
---

##flashback database

check flashback settings

    SQL> conn /as sysdba
    Connected.
    SQL> show parameter db_recovery

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    db_recovery_file_dest		     string	 /db/oracle/flash_recovery_area
    db_recovery_file_dest_size	     big integer 3882M
    SQL> archive log list;
    Database log mode	       Archive Mode
    Automatic archival	       Enabled
    Archive destination	       /db/oracle/arch
    Oldest online log sequence     1019
    Next log sequence to archive   1021
    Current log sequence	       1021
    SQL> select flashback_on from v$database;

    FLASHBACK_ON
    ------------------
    YES

current scn or time
    
    SQL> select current_scn from v$database;

    CURRENT_SCN
    -----------
       90142039

    SQL> select to_char(sysdate, 'yy-mm-dd hh24:mi:ss') time from dual;

    TIME
    -----------------
    13-08-05 15:53:37

drop table
    
    SQL> select count(1) from dev.test;

      COUNT(1)
    ----------
         73448

    SQL> drop table dev.test;

    Table dropped.

    SQL> commit;

    Commit complete.

flashback database 
    
    SQL> shutdown immediate;
    Database closed.
    Database dismounted.
    ORACLE instance shut down.
    SQL> startup mount;
    ORACLE instance started.

    Total System Global Area 1043886080 bytes
    Fixed Size		    2219952 bytes
    Variable Size		  763363408 bytes
    Database Buffers	  272629760 bytes
    Redo Buffers		    5672960 bytes
    Database mounted.
    
    SQL> flashback database to scn 90142039; 
    -- or flashback database to timestamp to_timestamp('13-08-05 15:53:37','yy-mm-dd hh24:mi:ss');

    Flashback complete.

open database
    
    SQL> alter database open resetlogs; --指定scn 或者timestamp 时间点之后产生的数据统统丢失

    Database altered.

    SQL> select count(1) from dev.test;

      COUNT(1)
    ----------
         73448

         
##flashback drop

recyclebin

    SQL> show parameter recycle

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    buffer_pool_recycle		     string
    db_recycle_cache_size		     big integer 0
    recyclebin			     string	 on
    
    SQL> select original_name, object_name from recyclebin;

    no rows selected

flashback drop test
    
    SQL> select * from test;

    TABLE_NAME
    ------------------------------
    SMALLTAB

    SQL> drop table test;

    Table dropped.

    SQL> select * from test;
    select * from test
                  *
    ERROR at line 1:
    ORA-00942: table or view does not exist

    SQL> select original_name, object_name from recyclebin;

    ORIGINAL_NAME			 OBJECT_NAME
    -------------------------------- ------------------------------
    TEST				 BIN$4y/v6z2YgJXgQB2spFgMLQ==$0

    SQL> flashback table test to before drop;

    Flashback complete.

    SQL> select * from test;

    TABLE_NAME
    ------------------------------
    SMALLTAB

    SQL> select original_name, object_name from recyclebin;

    no rows selected

flashback drop test2
    
    SQL> drop table test;

    Table dropped.

    SQL> create table test as select object_id from dba_objects where rownum <= 1;

    Table created.

    SQL> select * from test;

     OBJECT_ID
    ----------
        20

    SQL> drop table test;

    Table dropped.

    SQL> select original_name, object_name from recyclebin;

    ORIGINAL_NAME			 OBJECT_NAME
    -------------------------------- ------------------------------
    TEST				 BIN$4y/v6z2agJXgQB2spFgMLQ==$0
    TEST				 BIN$4y/v6z2bgJXgQB2spFgMLQ==$0

    SQL> flashback table test to before drop;

    Flashback complete.

    SQL> drop table test;

    Table dropped.

    SQL> flashback table test to before drop;

    Flashback complete.

    SQL> select original_name, object_name from recyclebin;

    ORIGINAL_NAME			 OBJECT_NAME
    -------------------------------- ------------------------------
    TEST				 BIN$4y/v6z2agJXgQB2spFgMLQ==$0

    SQL> select * from test;

     OBJECT_ID
    ----------
        20

    SQL> flashback table test to before drop;
    flashback table test to before drop
    *
    ERROR at line 1:
    ORA-38312: original name is used by an existing object


    SQL> flashback table test to before drop rename to test_t;

    Flashback complete.

    SQL> select * from test_t;

    TABLE_NAME
    ------------------------------
    SMALLTAB
    
    SQL> select original_name, object_name from recyclebin;

    no rows selected

flashback drop test3

    SQL> drop table test;

    Table dropped.

    SQL> select original_name, object_name from recyclebin;

    ORIGINAL_NAME			 OBJECT_NAME
    -------------------------------- ------------------------------
    TEST				 BIN$4y/v6z2dgJXgQB2spFgMLQ==$0

    SQL> flashback table "BIN$4y/v6z2dgJXgQB2spFgMLQ==$0" to before drop;

    Flashback complete.

    SQL> select * from test;

     OBJECT_ID
    ----------
        20

    SQL> select original_name, object_name from recyclebin;

    no rows selected

flashback drop test4

    SQL> select * from test;

     OBJECT_ID
    ----------
        20

    SQL> create index test_idx on test(object_id);

    Index created.

    SQL> drop table test;

    Table dropped.

    SQL> select original_name, object_name from recyclebin;

    ORIGINAL_NAME			 OBJECT_NAME
    -------------------------------- ------------------------------
    TEST_IDX			 BIN$4y/v6z2egJXgQB2spFgMLQ==$0
    TEST				 BIN$4y/v6z2fgJXgQB2spFgMLQ==$0

    SQL> select index_name from user_indexes where table_name = 'TEST';

    no rows selected

    SQL> select * from test;
    select * from test
                  *
    ERROR at line 1:
    ORA-00942: table or view does not exist


    SQL> flashback table test to before drop;

    Flashback complete.

    SQL> select original_name, object_name from recyclebin;

    no rows selected

    SQL> select index_name from user_indexes where table_name = 'TEST';

    INDEX_NAME
    ------------------------------
    BIN$4y/v6z2egJXgQB2spFgMLQ==$0

    SQL> alter index "BIN$4y/v6z2egJXgQB2spFgMLQ==$0" rename to test_idx;

    Index altered.

    SQL> select index_name from user_indexes where table_name = 'TEST';

    INDEX_NAME
    ------------------------------
    TEST_IDX
    
##flashback query

nls date format

    SQL> alter session set nls_date_format = 'YYYY-MM-DD hh24:mi:ss';

    Session altered.

flashback query test based on timestamp
    
    SQL> select sysdate from dual;

    SYSDATE
    -------------------
    2013-08-05 16:59:13

    SQL> select * from test;

     OBJECT_ID
    ----------
        20

    SQL> delete from test;

    1 row deleted.

    SQL> commit;

    Commit complete.

    SQL> select * from test;

    no rows selected

    SQL> select * from test as of timestamp sysdate - 5/1440;

    no rows selected

    SQL> select * from test as of timestamp sysdate - 10/1440;

     OBJECT_ID
    ----------
        20

    SQL> select * from test as of timestamp to_timestamp('2013-08-05 16:59:13','YYYY-MM-DD hh24:mi:ss');

     OBJECT_ID
    ----------
        20

flashback query test based on scn
        
    SQL> select dbms_flashback.get_system_change_number from dual;

    GET_SYSTEM_CHANGE_NUMBER
    ------------------------
            90146709

    SQL> select current_scn from v$database;

    CURRENT_SCN
    -----------
       90146895

    SQL> select current_scn from v$database;

    CURRENT_SCN
    -----------
       90146897

    SQL> select * from test_t;

    TABLE_NAME
    ------------------------------
    SMALLTAB

    SQL> delete from test_t;

    1 row deleted.

    SQL> commit;

    Commit complete.

    SQL> select * from test_t;

    no rows selected

    SQL> select * from test_t as of scn 90146897;

    TABLE_NAME
    ------------------------------
    SMALLTAB

    SQL> select * from test_t as of scn 90146709;

    TABLE_NAME
    ------------------------------
    SMALLTAB

    SQL> insert into test_t select * from test_t as of scn 90146709;

    1 row created.

    SQL> commit;

    Commit complete.

    SQL> select * from test_t;

    TABLE_NAME
    ------------------------------
    SMALLTAB

    
    SQL> create or replace function getdate return date
      2  as 
      3      v_date date;
      4  begin
      5      select sysdate into v_date from dual;
      6      return v_date;
      7  end;
      8  /

    Function created.

    SQL> alter session set nls_date_format = 'YYYY-MM-DD hh24:mi:ss';

    Session altered.

    SQL> select getdate() from dual;

    GETDATA()
    -------------------
    2013-08-05 17:10:29

    SQL> select text from dba_source where name='GETDATE' order by line;

    TEXT
    --------------------------------------------------------------------------------
    function getdate return date
    as
        v_date date;
    begin
        select sysdate into v_date from dual;
        return v_date;
    end;

    7 rows selected.

    SQL> drop function getdate;

    Function dropped.

    SQL> select text from dba_source where name='GETDATE' order by line;

    no rows selected

    SQL> select text from dba_source as of timestamp to_timestamp('2013-08-05 17:10:29','YYYY-MM-DD hh24:mi:ss') where name='GETDATE' order by line;
    select text from dba_source as of timestamp to_timestamp('2013-08-05 17:10:29','YYYY-MM-DD hh24:mi:ss') where name='GETDATE' order by line
                     *
    ERROR at line 1:
    ORA-01031: insufficient privileges

    SQL> conn /as sysdba
    Connected.
    SQL> select text from dba_source as of timestamp to_timestamp('2013-08-05 17:10:29','YYYY-MM-DD hh24:mi:ss') where name='GETDATE' order by line;

    TEXT
    --------------------------------------------------------------------------------
    function getdata return date
    as
        v_date date;
    begin
        select sysdate into v_date from dual;
        return v_date;
    end;

    7 rows selected.
    
##flashbak table

    SQL> alter session set nls_date_format = 'YYYY-MM-DD hh24:mi:ss';

    Session altered.

    SQL> select current_scn from v$database;

    CURRENT_SCN
    -----------
       90148131
    
    SQL> select row_movement from user_tables where table_name = 'TEST_T';

    ROW_MOVE
    --------
    DISABLED

    SQL> select * from test_t;

    TABLE_NAME
    ------------------------------
    SMALLTAB

    SQL> delete from test_t;

    1 row deleted.

    SQL> commit;

    Commit complete.

    SQL> select * from test_t;

    no rows selected

    SQL> flashback table test_t to scn 90148131;
    flashback table test_t to scn 90148131
                    *
    ERROR at line 1:
    ORA-08189: cannot flashback the table because row movement is not enabled


    SQL> alter table test_t enable row movement;

    Table altered.

    SQL> flashback table test_t to scn 90148131;

    Flashback complete.

    SQL> select * from test_t;

    TABLE_NAME
    ------------------------------
    SMALLTAB

##Reference

* [Oracle 11gR2 中 Flashback 说明](http://blog.csdn.net/tianlesoftware/article/details/7229802)
* [Oracle Flashback 技术总结](http://blog.csdn.net/tianlesoftware/article/details/4677378)
---
layout: post
title: Oracle Cursor Introduction
category : Oracle
tags : [Oracle, Database, DBA]
---

##Cursor

Cursors are one of the most common and fundamental terms in the database terminology. 

It is one of the core database programming concepts, which forms a basic unit of execution of SQL statement.

A cursor is a pointer, which points towards a pre allocated memory location in the SGA. For transparent understanding, it is a handle or gateway adopted by Oracle to execute a SQL query. The memory location to which it points is known as Context area. Oracle associates every SELECT statement with a cursor to hold the query information in this context area.

Cursor follows a defined execution cycle to execute the SQL statement associated with it. The article describes the Oracle cursors and their usage.

There are two types of cursors: Implicit cursors and explicit cursors.

###Implicit Cursors

Oracle server processes every SQL statement in a PL/SQL block as an implicit cursor. All the DML statements (INSERT, UPDATE or DELETE) and SELECT query with INTO or BULK COLLECT clauses are candidates for implicit cursors. Whenever a SQL statement is executed, Oracle automatically allocates a memory area (known as context area) in Oracle database PGA i.e. Process Global Area. This allocated memory space is the query work area which holds the query related information.

For implicit cursor, the complete execution cycle is internally handled and maintained by the oracle server. For developers, implicit cursor appears to be an abstract concept. Only thing which is physically available and visible to them is the cursor status flags and information. Cursor attributes reveal the cursor related information and status. Following are the cursor attributes available:

* SQL%ROWCOUNT – Number of rows returned/changed in the last executed query. Applicable for SELECT as well as DML statement
* SQL%ISOPEN – Boolean TRUE if the cursor is still open, else FALSE. For implicit cursor it is FALSE only
* SQL%FOUND – Boolean TRUE, if the cursor fetch points to a record, else FALSE
* SQL%NOTFOUND – Inverse of SQL%FOUND. The flag is set as FALSE when the cursor pointer does not point to a record in the result set.

These attributes are set at the different stages of execution cycle and retained in the context area.

###Explicit Cursors

These cursors are explicitly declared in the DECLARE section of the block. They possess a specific name and a static SELECT statement attached to them. Explicit cursors are manually executed by the developers and follow complete execution cycle.

Explicit cursor information is also captured in cursor attributes, which are set during the cursor processing and reveal essential information about the cursors. These attributes, as listed below, are same as that in implicit cursors but specific to the cursors.

* CURSOR%ROWCOUNT
* CURSOR%ISOPEN
* CURSOR%FOUND
* CURSOR%NOTFOUND

##Reference

* Oracle Database Administrator's Guide
---
layout: post
title: Retry PO Approval Workflow Process in EBS
category : Oracle
tags : [Oracle, DBA, EBS, Exception]
---

Oracle EBS 11i中，经常会出现PO在审批的工作流中卡掉的情况，即到某一个审批节点时出 现异常，无法进行下一步。此时，一般可以通过运行指定的脚本，Retry这个工作流节点。 Retry都可以将异常的工作流跑通，如果还是出现异常，则可能需要进行reset操作，即将审 批中的单据退回到提交审批前的状态。这样的话工作人员需要重新进行审批。

##PO模块工作流异常Retry的办法。

0）查询单据的item_type,item_key，wf_approval_process

    --PR
    SELECT HR.NAME, PRH.SEGMENT1, PRH.WF_ITEM_TYPE, PRH.WF_ITEM_KEY
      FROM PO_REQUISITION_HEADERS_ALL PRH, HR_ALL_ORGANIZATION_UNITS HR
     WHERE PRH.ORG_ID = HR.ORGANIZATION_ID
     AND PRH.SEGMENT1 = '&Enter_PR_Number';
 
    --PO
    SELECT HR.NAME, POH.SEGMENT1, POH.WF_ITEM_TYPE, POH.WF_ITEM_KEY
      FROM PO_HEADERS_ALL POH, HR_ALL_ORGANIZATION_UNITS HR
     WHERE POH.ORG_ID = HR.ORGANIZATION_ID
       AND POH.SEGMENT1 = '&Enter_PO_Number';
 
    --PO Relese
    SELECT HR.NAME,
           POH.SEGMENT1,
           POR.RELEASE_NUM,
           POR.WF_ITEM_TYPE,
           POR.WF_ITEM_KEY
      FROM PO_HEADERS_ALL            POH,
           PO_RELEASES_ALL           POR,
           HR_ALL_ORGANIZATION_UNITS HR
     WHERE POH.ORG_ID = HR.ORGANIZATION_ID
       AND POR.ORG_ID = POH.ORG_ID
       AND POH.PO_HEADER_ID = POR.PO_HEADER_ID
       AND POH.SEGMENT1 = '&Enter_PO_Number'
       AND POR.RELEASE_NUM = '&Enter_Release_Num';
 
    --wf approval process
    SELECT DOCUMENT_TYPE_CODE,
           DOCUMENT_SUBTYPE,
           WF_APPROVAL_ITEMTYPE,
           WF_APPROVAL_PROCESS,     --确认该值
           WF_CREATEDOC_ITEMTYPE,
           WF_CREATEDOC_PROCESS
      FROM PO_DOCUMENT_TYPES_ALL_B
     WHERE DOCUMENT_TYPE_CODE IN('REQUISITION','PO','RELEASE')
     ORDER BY 1,2,3;
     
 
1）在管理界面Retry，需要sysadmin权限。

* 以sysadmin登陆；
 
* 操作路径： Workflow Administrator Web Applications --> Status Monitor -> 输入查询条件 (Type Internal Name 即对应 item_type） -->　Action History -> Retry 即可。
 
2）登录到服务器的`SQL*Plus`，运行wfretry.sql脚本。

* 以mgr用户，登录到服务器，并切换到 $FND_TOP/sql 路径下；
* 以apps用户登录到``SQL*Plus``；
* 运行脚本: @wfretry，并按提示输入参数即可。

示例，以 PR 81000902 为例：

    SQL> conn apps
    Enter password: 
    Connected.
    
    SQL> @wfretry
    Enter value for 1: REQAPPRV
    Enter value for 2: 267857-933577
    Select from list of error activities
     
    LABEL                          RESULT
    ------------------------------ ------------------------------
    VERIFY_APPROVER_AUTHORITY      #EXCEPTION
     
    Label: MAIN_REQAPPRV_PROCESS
    SKIP, RETRY or RESET activity?
    Command: RETRY
    Result of activity if command is SKIP
    Result: 
     
    PL/SQL procedure successfully completed.
     
     
    Commit complete.
     
    Disconnected from Oracle9i Enterprise Edition Release 9.2.0.6.0 - 64bit Production
    With the Partitioning, OLAP and Oracle Data Mining options
    JServer Release 9.2.0.6.0 - Production
 
3）在`SQL*Plus`或者PL/SQL Developer中运行wf_engine.HandleError存储过程。

以PR 81000902 为例，确认好上述参数后，运行如脚本：

    BEGIN
       wf_engine.HandleError('REQAPPRV','267857-933577','MAIN_REQAPPRV_PROCESS','RETRY');
    END;
 
    commit;
 
4）确认时，查看该单据的工作流详情即可： 

    --查看工作流详情
    SELECT WIAV.NAME ATTR_NAME,
           SUBSTR(NVL(WIAV.TEXT_VALUE,
                      NVL(TO_CHAR(WIAV.NUMBER_VALUE),
                          TO_CHAR(WIAV.DATE_VALUE, 'DD-MON-YYYY hh24:mi:ss'))),
                  1,
                  30) VALUE
      FROM WF_ITEM_ATTRIBUTE_VALUES WIAV, WF_ITEM_ATTRIBUTES WIA
     WHERE WIAV.ITEM_TYPE = '&item_type'
       AND WIAV.ITEM_KEY = '&item_key'
       AND WIA.ITEM_TYPE = WIAV.ITEM_TYPE
       AND WIA.NAME = WIAV.NAME
    --   AND WIAV.NAME = 'SYSADMIN_ERROR_MSG'
       AND WIA.TYPE <> 'EVENT';

       
##小结
 
一般情况下，retry都可以将异常的工作流跑通，如果还是出现异常，则可能需要进行reset 操作，即将审批中的单据退回到提交审批前的状态。这样的话工作人员需要重新进行审批。

---
layout: post
title: ORA-01002
category : Oracle
tags : [Oracle, Database, DBA, Exception]
---

reproduct ORA-01002

create table

	create table test as select object_id,object_name from dba_objects where rownum <= 100;

execute script
	
	declare
	   cursor v_cursor is select * from test for update;
	begin
	   for v in v_cursor
	   loop
		update test set object_id = 0 where current of v_cursor;
		commit;
	   end loop;
	end;
	/

	ERROR at line 1:
	ORA-01002: fetch out of sequence
	ORA-06512: at line 4


	
ORA-01002: fetch out of sequence

Cause: This error means that a fetch has been attempted from a cursor which is no longer valid. Note that a PL/SQL cursor loop implicitly does fetches, and thus may also cause this error. There are a number of possible causes for this error, including: 

* Fetching from a cursor after the last row has been retrieved and the ORA-1403 error returned. 
* If the cursor has been opened with the FOR UPDATE clause, fetching after a COMMIT has been issued will return the error. 
* Rebinding any placeholders in the SQL statement, then issuing a fetch before reexecuting the statement.
	
Action: 

* Do not issue a fetch statement after the last row has been retrieved - there are no more rows to fetch. 
* Do not issue a COMMIT inside a fetch loop for a cursor that has been opened FOR UPDATE. 
* Reexecute the statement after rebinding, then attempt to fetch again.

##Reference

* Oracle Database Error Message
---
layout: post
title: Use Wget to Detect Dead Links
category : Linux
tags : [Linux, Utilities]
---

Typical wget usage:

	wget -r --spider -b -o /var/tmp/wget.log http://target.site.com/

	-r - Recursive download
	--spider - Check downloaded content for new links, then discard
	-b - Run in the background
	-o /var/tmp/wget.log - Target log file

	http://target.site.com/ - Replace with the site you want to check

wget will create a log file detailing the status codes reported while downloading content from the target site - note that this method will catch links to nonexistent content from within the domain, however, off-site links will not be checked (which is why it pays to monitor your site's error logs).

##Reference

* [Wget下载中级用法和15个详细的例子](http://www.cnblogs.com/dzh-stuff/archive/2012/02/16/2354611.html)

---
layout: post
title: Oracle DataGuard Example
category : Oracle
tags : [Oracle, Database, DBA]
---

##Environment

* primary    192.168.1.100    CentOS 6.0 64bit  Oracle 11gR2  primary database
* standby    192.168.2.100    Redhat 5.5 64bit  Oracle 10g    standby database

##Primary Database

force logging

    [root@primary conf]# su - oracle
    [oracle@primary ~]$ sqlplus  /nolog

    SQL*Plus: Release 11.2.0.1.0 Production on Sun Sep 8 14:56:17 2013

    Copyright (c) 1982, 2009, Oracle.  All rights reserved.

    SQL> conn /as sysdba
    Connected.
    SQL> select force_logging from v$database;

    FOR
    ---
    NO

    SQL> alter database force logging;

    Database altered.

enable archivelog
    
    SQL> archive log list;
    Database log mode	       No Archive Mode
    Automatic archival	       Disabled
    Archive destination	       USE_DB_RECOVERY_FILE_DEST
    Oldest online log sequence     634
    Current log sequence	       636
    SQL> shutdown immediate; 
    Database closed.
    Database dismounted.
    ORACLE instance shut down.
    SQL> startup mount; 
    ORACLE instance started.

    Total System Global Area 3290345472 bytes
    Fixed Size		    2217832 bytes
    Variable Size		 2365589656 bytes
    Database Buffers	  905969664 bytes
    Redo Buffers		   16568320 bytes
    Database mounted.
    SQL> alter database archivelog;

    Database altered.

    SQL> alter database open;

    Database altered.

    SQL> archive log list;
    Database log mode	       Archive Mode
    Automatic archival	       Enabled
    Archive destination	       USE_DB_RECOVERY_FILE_DEST
    Oldest online log sequence     634
    Next log sequence to archive   636
    Current log sequence	       636
    SQL> show parameter log_archive

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    log_archive_config		     string
    log_archive_dest		     string	 /db/oracle/arch/
    log_archive_dest_1		     string
    log_archive_dest_10		     string
    log_archive_dest_11		     string
    log_archive_dest_12		     string
    log_archive_dest_13		     string
    log_archive_dest_14		     string
    log_archive_dest_15		     string
    log_archive_dest_16		     string
    log_archive_dest_17		     string

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    log_archive_dest_18		     string
    log_archive_dest_19		     string
    log_archive_dest_2		     string
    log_archive_dest_20		     string
    log_archive_dest_21		     string
    log_archive_dest_22		     string
    log_archive_dest_23		     string
    log_archive_dest_24		     string
    log_archive_dest_25		     string
    log_archive_dest_26		     string
    log_archive_dest_27		     string

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    log_archive_dest_28		     string
    log_archive_dest_29		     string
    log_archive_dest_3		     string
    log_archive_dest_30		     string
    log_archive_dest_31		     string
    log_archive_dest_4		     string
    log_archive_dest_5		     string
    log_archive_dest_6		     string
    log_archive_dest_7		     string
    log_archive_dest_8		     string
    log_archive_dest_9		     string

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    log_archive_dest_state_1	     string	 enable
    log_archive_dest_state_10	     string	 enable
    log_archive_dest_state_11	     string	 enable
    log_archive_dest_state_12	     string	 enable
    log_archive_dest_state_13	     string	 enable
    log_archive_dest_state_14	     string	 enable
    log_archive_dest_state_15	     string	 enable
    log_archive_dest_state_16	     string	 enable
    log_archive_dest_state_17	     string	 enable
    log_archive_dest_state_18	     string	 enable
    log_archive_dest_state_19	     string	 enable

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    log_archive_dest_state_2	     string	 enable
    log_archive_dest_state_20	     string	 enable
    log_archive_dest_state_21	     string	 enable
    log_archive_dest_state_22	     string	 enable
    log_archive_dest_state_23	     string	 enable
    log_archive_dest_state_24	     string	 enable
    log_archive_dest_state_25	     string	 enable
    log_archive_dest_state_26	     string	 enable
    log_archive_dest_state_27	     string	 enable
    log_archive_dest_state_28	     string	 enable
    log_archive_dest_state_29	     string	 enable

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    log_archive_dest_state_3	     string	 enable
    log_archive_dest_state_30	     string	 enable
    log_archive_dest_state_31	     string	 enable
    log_archive_dest_state_4	     string	 enable
    log_archive_dest_state_5	     string	 enable
    log_archive_dest_state_6	     string	 enable
    log_archive_dest_state_7	     string	 enable
    log_archive_dest_state_8	     string	 enable
    log_archive_dest_state_9	     string	 enable
    log_archive_duplex_dest 	     string
    log_archive_format		     string	 %t_%s_%r.dbf

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    log_archive_local_first 	     boolean	 TRUE
    log_archive_max_processes	     integer	 4
    log_archive_min_succeed_dest	     integer	 1
    log_archive_start		     boolean	 FALSE
    log_archive_trace		     integer	 0
    
add standby logfile group
    
    SQL> select group#,sequence#,members,archived,status from v$log;

        GROUP#  SEQUENCE#	 MEMBERS ARC STATUS
    ---------- ---------- ---------- --- ----------------
         1	  634	       1 YES INACTIVE
         2	  635	       1 YES INACTIVE
         3	  636	       1 NO  CURRENT

    SQL> alter database add standby logfile group 4 ('/db/oracle/oradata/test/log4a.log','/db/oracle/oradata/test/log4b.log')size 50M;

    Database altered.

    SQL> alter database add standby logfile group 5 ('/db/oracle/oradata/test/log5a.log','/db/oracle/oradata/test/log5b.log')size 50M;

    Database altered.

    SQL> alter database add standby logfile group 6 ('/db/oracle/oradata/test/log6a.log','/db/oracle/oradata/test/log6b.log')size 50M;

    Database altered.

    SQL> alter database add standby logfile group 7 ('/db/oracle/oradata/test/log7a.log','/db/oracle/oradata/test/log7b.log')size 50M;

    Database altered.

    SQL> select group#,thread#,sequence#,archived,status from v$standby_log;

        GROUP#    THREAD#  SEQUENCE# ARC STATUS
    ---------- ---------- ---------- --- ----------
         4	    0	       0 YES UNASSIGNED
         5	    0	       0 YES UNASSIGNED
         6	    0	       0 YES UNASSIGNED
         7	    0	       0 YES UNASSIGNED

add dataguard configuration in pfile      
         
    SQL> create pfile from spfile;

    File created.

    SQL> shutdown immediate;
    Database closed.
    Database dismounted.
    ORACLE instance shut down.
    SQL> 
  
    [oracle@primary primary]$ cd /db/oracle/product/11.2.0/db_1/dbs/
    [oracle@primary dbs]$ ls -al
    total 9840
    drwxr-xr-x.  2 oracle oinstall     4096 Sep  8 15:01 .
    drwxr-xr-x. 74 oracle oinstall     4096 Jul 19 14:30 ..
    -rw-rw----.  1 oracle oinstall     1544 Sep  8 15:01 hc_primary.dat
    -rw-r--r--.  1 oracle oinstall     2851 May 15  2009 init.ora
    -rw-r--r--.  1 oracle oinstall      993 Sep  8 15:01 initprimary.ora
    -rw-r-----.  1 oracle oinstall       24 Jul 19 14:27 lkprimary
    -rw-r-----.  1 oracle oinstall     1536 Sep  8 14:25 orapwprimary
    -rw-r-----.  1 oracle oinstall 10043392 Sep  8 13:20 snapcf_primary.f
    -rw-r-----.  1 oracle oinstall     3584 Sep  8 14:57 spfileprimary.ora
    [oracle@primary dbs]$ cp initprimary.ora initprimary.ora.dist
    [oracle@primary dbs]$ vim initprimary.ora
    ... ...
    #2013-9-8    dylanninin    settings for primary database
    *.db_unique_name='primary'
    *.log_archive_config='dg_config=(primary,standby)'
    *.log_archive_dest_1='location=/db/oracle/arch valid_for=(all_logfiles,all_roles)'
    *.log_archive_dest_2='service=standby arch async valid_for=(online_logfiles,primary_role) db_unique_name=standby'
    *.log_archive_dest_state_1='enable'
    *.log_archive_dest_state_2='enable'
    *.standby_file_management='auto'
    *.standby_archive_dest='/db/oracle/arch'
    *.fal_client='primary'
    *.fal_server='standby'

    SQL> conn /as sysdba
    Connected to an idle instance.
    SQL> startup pfile=?/dbs/initprimary.ora
    ORACLE instance started.

    Total System Global Area 3290345472 bytes
    Fixed Size		    2217832 bytes
    Variable Size		 2365589656 bytes
    Database Buffers	  905969664 bytes
    Redo Buffers		   16568320 bytes
    Database mounted.
    Database opened.
    SQL> create spfile from pfile; 

    File created.

    SQL> shutdown immediate;
    Database closed.
    Database dismounted.
    ORACLE instance shut down.
    SQL> startup
    ORA-32004: obsolete or deprecated parameter(s) specified for RDBMS instance
    ORACLE instance started.

    Total System Global Area 3290345472 bytes
    Fixed Size		    2217832 bytes
    Variable Size		 2365589656 bytes
    Database Buffers	  905969664 bytes
    Redo Buffers		   16568320 bytes
    Database mounted.
    Database opened.
    SQL> show parameter spfile

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    spfile				     string	 /db/oracle/product/11.2.0/db_1
                             /dbs/spfileprimary.ora
                 
create standby controlfile                 
                     
    SQL> alter database create standby controlfile as '/db/oracle/standby.ctl';

    Database altered.

shutdown database
    
    SQL> shutdown immediate
    Database closed.
    Database dismounted.
    ORACLE instance shut down.

edit listener.ora and tnsnames.ora
    
    [oracle@primary admin]$ pwd
    /db/oracle/product/11.2.0/db_1/network/admin

    [oracle@primary admin]$ cat listener.ora 
    # listener.ora Network Configuration File: /db/oracle/product/11.2.0/db_1/network/admin/listener.ora
    # Generated by Oracle configuration tools.

    LISTENER =
      (DESCRIPTION_LIST =
        (DESCRIPTION =
          (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))
          (ADDRESS = (PROTOCOL = TCP)(HOST = primary.egolife.com)(PORT = 1521))
        )
      )

    SID_LIST_LISTENER =
      (SID_LIST =
        (SID_DESC =
          (SID_NAME = PLSExtProc)
          (ORACLE_HOME = /db/oracle/product/11.2.0/db_1)
          (PROGRAM = extproc)
        )
        (SID_DESC =
          (GLOBAL_DBNAME = primary)
          (ORACLE_HOME = /db/oracle/product/11.2.0/db_1)
          (SID_NAME = primary)
        )
      )

    ADR_BASE_LISTENER = /db/oracle


    [oracle@primary admin]$ cat tnsnames.ora
    # tnsnames.ora Network Configuration File: /db/oracle/product/11.2.0/db_1/network/admin/tnsnames.ora
    # Generated by Oracle configuration tools.

    primary =
      (DESCRIPTION =
        (ADDRESS = (PROTOCOL = TCP)(HOST =192.168.1.100)(PORT = 1521))
        (CONNECT_DATA =
          (SERVER = DEDICATED)
          (SERVICE_NAME = primary)
        )
      )

    standby =
      (DESCRIPTION =
        (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.2.100)(PORT = 1521))
        (CONNECT_DATA =
          (SERVER = DEDICATED)
          (SERVICE_NAME = primary)
        )
      )
  
  
##Copy files to Standby Database
  
scp datafile,pfile/spfile/orapwd,tnsnames,listener to standby database

##Standby Database

edut dataguard configuration in pfile

    [oracle@standby ~]$ cd /db/oracle/product/11.2.0/db_1/dbs/
    [oracle@standby dbs]$ vim initprimary.ora
    ... ...
    *.control_files='/db/oracle/oradata/test/standby.ctl'
    ... ...
    #2013-9-8    dylanninin    settings for primary database
    *.db_unique_name='standby'
    *.log_archive_config='dg_config=(primary,standby)'
    *.log_archive_dest_1='location=/db/oracle/arch valid_for=(all_logfiles,all_roles) db_unique_name=standby'
    *.log_archive_dest_2='service=primary db_unique_name=primary'
    *.log_archive_dest_state_1='enable'
    *.log_archive_dest_state_2='enable'
    *.standby_file_management='auto'
    *.standby_archive_dest='/db/oracle/arch'
    *.fal_client='standby'
    *.fal_server='primary'

mount standby database and start listener

    [oracle@standby ~]$ sqlplus  /nolog

    SQL*Plus: Release 11.2.0.1.0 Production on Sun Sep 8 17:28:26 2013

    Copyright (c) 1982, 2009, Oracle.  All rights reserved.

    SQL> conn /as sysdba
    Connected to an idle instance.
    SQL> startup nomount pfile=?/dbs/initprimary.ora
    ORA-32006: STANDBY_ARCHIVE_DEST initialization parameter has been deprecated
    ORACLE instance started.

    Total System Global Area 1653518336 bytes
    Fixed Size		    2213896 bytes
    Variable Size		  939526136 bytes
    Database Buffers	  704643072 bytes
    Redo Buffers		    7135232 bytes

    SQL> alter database mount standby database;

    Database altered.

    [oracle@standby ~]$ lsnrctl start

start primary database and listener

##test archivelog sync and apply

primary database

    SQL> archive log list;
    Database log mode	       Archive Mode
    Automatic archival	       Enabled
    Archive destination	       /db/oracle/arch
    Oldest online log sequence     638
    Next log sequence to archive   640
    Current log sequence	       640
    SQL> alter system switch logfile;

    System altered.

    SQL> alter system switch logfile;

    System altered.

    SQL> archive log list;
    Database log mode	       Archive Mode
    Automatic archival	       Enabled
    Archive destination	       /db/oracle/arch
    Oldest online log sequence     640
    Next log sequence to archive   642
    Current log sequence	       642


standby database

    SQL> conn /as sysdba
    Connected.
    SQL> archive log list;
    Database log mode	       Archive Mode
    Automatic archival	       Enabled
    Archive destination	       /db/oracle/arch
    Oldest online log sequence     640
    Next log sequence to archive   0
    Current log sequence	       642


##common operations

switch logfile to archive log

    SQL> alter system switch logfile;

views to check logfile,archivelog in primary/standby database
    
    select max(sequence#) from v$archived_log; 
    select max(sequence#) from v$log_history; 
    select group#,sequence#,archived,status from v$log; 
    select name,sequence#,applied from v$archived_log; 

switch between primary/standby database

    
sync failure
    
1）check whether archivelogs are missing; if then to copy and register missing archivelogs manually. 

	--register archivelog
	alter database register logfile '/path/to/the/missing/archivelog';

2）apply archivelog in standby database： 

	--cancel applying archivelog
	alter database recover managed standby database cancel; 

	--apply archivelog
	alter database recover managed standby database disconnect from session; 
	
##Reference

* [如何搭建一个DataGuard环境](http://blog.csdn.net/tianlesoftware/article/details/6195771)
* [David Dave -- DataGuard 专题](http://blog.csdn.net/tianlesoftware/article/category/700326)
##写在前面的话

学习Python的第一个Web框架就是[web.py](http://webpy.org/)，没想到它的作者Aaron Swartz已经于2013年1月11日自杀。

2012年年末计划用Python将此博客重写，其中Web框架经过一翻[选型](http://wiki.woodpecker.org.cn/moin/PyWebFrameList)，便确定使用[web.py](http://webpy.org/)。本打算利用周末时间，希望能在年前完成，但因迫近年关诸事繁忙，现在才刚刚搭建起基本的项目框架。真是诸行无常，现在唯一能纪念的，就是作为一个程序员，学习吸收Aaron Swartz的思想，尽快的完成这个项目，并发布到[github](http://github.com/)，也算是对web.py的一种推介，对Aaron Swartz的一种追思。

[@程序员杂志](http://e.weibo.com/programmermag)：
震惊！Aaron Swartz自杀身亡，年仅26。他是Reddit的联合创始人，web.py的设计者，14岁参与创造了RSS 1.0规范，又与John Gruber共同设计了Markdown。2011年曾因下载480万篇JSTOR学术论文而被捕。[http://t.cn/zjrzsyb](http://t.cn/zjrzsyb)

##News

###Aaron Swartz commits suicide By Anne Cai

* NEWS EDITOR; UPDATED AT 2:15 A.M. 1/12/13

Computer activist Aaron H. Swartz committed suicide in New York City yesterday, Jan. 11, according to his uncle, Michael Wolf, in a comment to The Tech. Swartz was 26.

"The tragic and heartbreaking information you received is, regrettably, true," confirmed Swartz' attorney, Elliot R. Peters of Kecker and Van Nest, in an email to The Tech.

Swartz was indicted in July 2011 by a federal grand jury for allegedly mass downloading documents from the JSTOR online journal archive with the intent to distribute them. He subsequently moved to Brooklyn, New York, where he then worked for Avaaz Foundation, a nonprofit "global web movement to bring people-powered politics to decision-making everywhere." Swartz appeared in court on Sept. 24, 2012 and pleaded not guilty.

The accomplished Swartz co-authored the now widely-used RSS 1.0 specification at age 14, was one of the three co-owners of the popular social news site Reddit, and completed a fellowship at Harvard's Ethics Center Lab on Institutional Corruption. In 2010, he founded [DemandProgress.org](http://demandprogress.org/), a "campaign against the Internet censorship bills SOPA/PIPA."

##About Aaron Swartz

Aaron Swartz is the founder of [Demand Progress](http://demandprogress.org), which launched the campaign against the Internet censorship bills (SOPA/PIPA) and now has over a million members. He is also a Contributing Editor to [The Baffler](http://thebaffler.com/) and on the Council of Advisors to The Rules.

He is a frequent television commentator and the author of numerous articles on a variety of topics, especially the corrupting influence of big money on institutions including [nonprofits](http://aaronsw.jottit.com/rachelcarson), [the media](http://www.aaronsw.com/weblog/newobjectivity), [politics](http://crookedtimber.org/2009/05/01/political-entrepreneurs-and-lunatics-with-money/), and public opinion. From 2010-11, he researched these topics as a Fellow at the Harvard Ethics Center Lab on Institutional Corruption. He also served on the board of Change Congress, a good government nonprofit.

He has also developed the site [theinfo.org](http://theinfo.org/). His landmark analysis of Wikipedia, [Who Writes Wikipedia?](http://www.aaronsw.com/weblog/whowriteswikipedia), has been widely cited.[ Working with Web inventor Tim Berners-Lee at MIT](http://www.w3.org/2001/sw/RDFCore/members.html), he helped [develop](https://tools.ietf.org/html/rfc3870) and [popularize](http://logicerror.com/semanticWeb-long) standards for sharing data on the Web. He also coauthored the [RSS 1.0 specification](http://purl.org/rss/1.0/), now widely used for publishing news stories.

His piece with photographer Taryn Simon, Image Atlas (2012), is has [been featured](http://www.newmuseum.org/exhibitions/view/taryn-simon-cultural-differences) in the New Museum. In 2007, he led the development of the nonprofit Open Library, an ambitious project to collect information about every book ever published. He also cofounded the online news site Reddit, where he released as free software the web framework he developed, [web.py](http://webpy.org/).

##Reference

* MIT News: [Aaron Swartz commits suicide](http://tech.mit.edu/V132/N61/swartz.html?comments#comments)
* Site: [Aaron Swartz](http://www.aaronsw.com/)
##Data Pump Introduce

Oracle Data Pump is made up of three distinct parts:

	The command-line clients, expdp and impdp
	The DBMS_DATAPUMP PL/SQL package (also known as the Data Pump API)
	The DBMS_METADATA PL/SQL package (also known as the Metadata API)

The Data Pump clients, expdp and impdp, invoke the Data Pump Export utility and 
Data Pump Import utility, respectively.

The `expdp` and `impdp` clients use the procedures provided in the `DBMS_DATAPUMP`
PL/SQL package to execute export and import commands, using the parameters 
entered at the command line. These parameters enable the exporting and importing of data and metadata for a complete database or for subsets of a database.

When metadata is moved, Data Pump uses functionality provided by the `DBMS_
METADATA` PL/SQL package. The `DBMS_METADATA` package provides a centralized 
facility for the extraction, manipulation, and re-creation of dictionary metadata.

The `DBMS_DATAPUMP`  and `DBMS_METADATA` PL/SQL packages can be used 
independently of the Data Pump clients.


![Oracle Data Pump Architecture](http://docs.oracle.com/cd/E11882_01/server.112/e10713/img/cncpt261.gif)


##Example

同一个数据库中克隆schema，本例中为克隆用户OA到OADev(以下操作均采用SYS用户)。

###1.新建用户
	
	create user oadev identified by "oadev";

###2.表空间和数据文件

单独创建表空间，数据文件

	create temporary tablespace oadev_tmp tempfile '/db/oracle/oradata/DBTEST/oadev_tmp01.dbf' size 1000M;
	
	create tablespace oadev_data datafile '/db/oracle/oradata/DBTEST/oadev_data01.dbf' size 1000M;

	create tablespace oadev_idx datafile '/db/oracle/oradata/DBTEST/oadev_idx01.dbf' size 1000M;

设置默认表空间


	SELECT 'alter user ' || 'OADEV' || ' default tablespace ' ||
		DU.DEFAULT_TABLESPACE || ' temporary tablespace ' ||
		DU.TEMPORARY_TABLESPACE || ' ;'
	FROM DBA_USERS DU
	WHERE DU.USERNAME IN ('OA');

	
	alter user OADEV default tablespace OADEV_DATA temporary tablespace OADEV_TMP;

验证
	
	SELECT * FROM DBA_USERS DU WHERE DU.USERNAME IN ('OA', 'OADEV');

####3.权限设置

查询已有系统权限

	SELECT 'grant ' || DSP.PRIVILEGE || ' to DEV;'
	  FROM DBA_SYS_PRIVS DSP
	 WHERE DSP.GRANTEE IN ('OA')
	 ORDER BY 1;

执行以上SQL输出的结果

	grant CREATE ANY PROCEDURE to oadev;
	grant CREATE ANY VIEW to oadev;
	grant DEBUG ANY PROCEDURE to oadev;
	grant DEBUG CONNECT SESSION to oadev;
	grant UNLIMITED TABLESPACE to oadev;

验证

	SELECT *
	  FROM DBA_SYS_PRIVS DSP
	 WHERE DSP.GRANTEE IN ('OA', 'OADEV')
	 ORDER BY 1;

###4.使用expdp/impdp准备工作

在OS上创建expdp/impdp目录:/dba/exp，并分配Oracle读写权限

创建Directory

	CREATE DIRECTORY EXPDP AS '/dba/exp';

授权
	
	GRANT READ, WRITE ON DIRECTORY EXPDP TO OA, OADEV;

验证
	
	SELECT * FROM DBA_DIRECTORIES;
	
	SELECT * FROM ALL_TAB_PRIVS ATP
	 WHERE ATP.TABLE_NAME = 'EXPDP';

###5.使用expdp导出oa数据

运行expdp命令，导出schema

	[oracle@oradb exp]$ expdp  oa/oatest directory=expdp dumpfile=oa_20121227.dmp logfile=oa_20121227.log parallel=2

###6.使用impdp导入5中导出的数据到oadev

运行impdp命令，导入数据，注意重新映射schema，tablespace

	[oracle@oradb exp]$ impdp oadev/oadev directory=expdp dumpfile=oa_20121227.dmp logfile=oadev_20121227.log remap_schema=oa:oadev remap_tablespace=oa_data:oadev_data,oa_idx:oadev_idx

###7.验证

查看导出、导入日志，有异常则进行检查即可。
使用oadev登录，更改应用数据库的用户即可，启动测试。

	SELECT 'All Objects owned by OA: ' || COUNT(1)
	  FROM ALL_OBJECTS O
	 WHERE O.OWNER = 'OA'
	UNION
	SELECT 'All Objects owned by OADev: ' || COUNT(1)
	  FROM ALL_OBJECTS O
	 WHERE O.OWNER = 'OADEV';


###8.Utility

进行data pump测试时，可能会反复重建用户，这里可以根据已有用户生成新用户的创建脚本，包括角色、系统权限、目录对象权限。

	--generate create user sq
	SELECT ' create user ' || DU.USERNAME || ' identified by "' || DU.USERNAME || '"' ||
	       ' profile ' || DU.PROFILE || ' default tablespace ' ||
	       DU.DEFAULT_TABLESPACE || ' temporary tablespace ' ||
	       DU.TEMPORARY_TABLESPACE
	  FROM DBA_USERS DU
	 WHERE DU.USERNAME = upper('&username');


	--generate grant roles and system or objects privileges sql
	SELECT 'grant ' || DRP.GRANTED_ROLE || ' to ' || '&new_grantee' || ' ;' -- roles
	  FROM DBA_ROLE_PRIVS DRP
	 WHERE DRP.GRANTEE = upper('&grantee')
	UNION ALL
	SELECT 'grant ' || DSP.PRIVILEGE || ' to ' || '&new_grantee' || ' ;' --sys privileges
	  FROM DBA_SYS_PRIVS DSP
	 WHERE DSP.GRANTEE = upper('&grantee')
	UNION ALL
	SELECT 'grant ' || ATP.PRIVILEGE || ' on directory ' || --directory privileges
	       ATP.TABLE_NAME || ' to ' || '&new_grantee' || ' ;'
	  FROM ALL_TAB_PRIVS ATP
	 WHERE ATP.TABLE_NAME = upper('&directory')
	   AND ATP.GRANTEE = upper('&grantee');

##expdp command line

[oracle@oradb exp]$ expdp help=Y
	
	Export: Release 11.2.0.1.0 - Production on Fri Jan 4 10:44:09 2013
	Copyright (c) 1982, 2009, Oracle and/or its affiliates.  All rights reserved.
	
	The Data Pump export utility provides a mechanism for transferring data objects
	between Oracle databases. The utility is invoked with the following command:
	
	   Example: expdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp
	
	You can control how Export runs by entering the 'expdp' command followed
	by various parameters. To specify parameters, you use keywords:
	
	   Format:  expdp KEYWORD=value or KEYWORD=(value1,value2,...,valueN)
	   Example: expdp scott/tiger DUMPFILE=scott.dmp DIRECTORY=dmpdir SCHEMAS=scott
	               or TABLES=(T1:P1,T1:P2), if T1 is partitioned table
	USERID must be the first parameter on the command line.

##impdp command line

[oracle@oradb exp]$ impdp help=Y

	Import: Release 11.2.0.1.0 - Production on Fri Jan 4 11:04:11 2013
	Copyright (c) 1982, 2009, Oracle and/or its affiliates.  All rights reserved.
	
	The Data Pump Import utility provides a mechanism for transferring data objects
	between Oracle databases. The utility is invoked with the following command:
	
	     Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp
	
	You can control how Import runs by entering the 'impdp' command followed
	by various parameters. To specify parameters, you use keywords:
	
	     Format:  impdp KEYWORD=value or KEYWORD=(value1,value2,...,valueN)
	     Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp
	USERID must be the first parameter on the command line.

##Reference

* [tianlesoftware](http://blog.csdn.net/tianlesoftware/article/details/4674224)
* Oracle Database Utilities
##如此无常

2012年12月29号晚，给一个高中同学打电话，聊了会儿天，得知病情恢复得不错，有说有笑，精神状态也不错。那时感觉一切正朝着良好的方向发展，尽管她已经接受治疗近三年。

2013年刚开始的第三天，即2013年1月3号下午，噩耗却突然传来：她走了。打电话给她爸爸时，听到的是作为一个父亲情感突然崩溃的哭泣，谁也无法想象这有多么难以接受。

无论是谁，听闻这些消息，都会有很多感触，而且并非言语所能表达。

这里冒昧引用她男朋友的几段话《会有天使替我爱你》：

* 10年的五一节以后一切都变了，他们的恋情就象小时候看的蓝色生死恋，一切来的是那个的突然，随着病情的一步步恶化，曾经的那个爱笑的，知性的，偶尔又略显风骚的大女孩，被病魔无情的摧残着，还在读书的他面对这样的噩耗，经济上也只能尽点绵薄之力，唯一能做的就是抽出一切可以抽出的时间去静静地守护她。
   
* 动完手术后的她选择了再也不理会他了，可他们各自心里都清楚谁也放不下谁。他通过朋友了解到做完手术的她恢复的还算可以，他也就放下了心。可是就在排异的危险期内，那让人无奈的三分之二复发几率无情地剥夺了她的生命。
       
* 他在2013年1月3日得知这个噩耗的时候，仿佛又回到了98年爷爷去世的那天。他的心很疼，那种熟悉又憎恶的感觉还是出现了。虽然之前已经作好最坏打算的他，听到这个后还是哭了，而且都没有勇气去送她最后一程。请她能够原谅她，因为他会为她在天堂找个天使替他爱她，在那里她将远离病痛，永远都开开心心的。

* 一路走好，对不起，那些实现不了的承诺，拜托天使了。
	
至于我，因表达的障碍，目前也只能听听几首歌，聊以缓解。过些时日我再好好整理下思绪，也算是一种悼念。

没有想到的是，老周的这些歌，听得让人无法忘却。

##沉默如谜的呼吸

	千钧一发的呼吸， 
	水滴石穿的呼吸， 
	蒸汽机粗重的呼吸， 
	玻璃切割玻璃的呼吸。 
	 
		我的疼是肉体的疼， 
		我怕一只肮脏的烟头摁在肺上， 
		我吱吱冒烟缩成一团， 
		又彻底松弛， 
		如崩溃的大坝， 
		任疼痛的洪水泛滥。 
		我怕二十万根生锈的针插遍全身， 
		每一根敏感的神经都被疼痛拨响， 
		折断的竹筷从鼻孔插入脑组织， 
		思索的大脑变成疼痛的蚂蚁窝。
	 
	鱼死网破的呼吸， 
	火焰痉挛的呼吸， 
	刀尖上跳舞的呼吸， 
	彗星般消逝的呼吸。 
	 
		我怕在铁水沸腾的熔炉里永生， 
		在热与疼的颠峰我清醒、我存在， 
		我奢望昏迷和死亡， 
		逆着时间的湍流追寻， 
		我怕温暖过游子的母亲不是起点。
	 
	沉默如鱼的呼吸， 
	沉默如石的呼吸， 
	沉默如睡的呼吸， 
	沉默如谜的呼吸。 
	 
		我怕时间将一切锈蚀， 
		而让追寻者独自锃亮......
	
###悬棺

	悬在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子

	悬棺
	悬棺在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子
	我们就住在云彩上面的棉花里
	我们在棉花里偷偷地喝酒

	踉踉跄跄一脚高山
	踉踉跄跄一脚平原
	踉踉跄跄一脚海洋
	踉踉跄跄一脚沙滩

	我们的身体是一只水桶
	升上去空空荡荡
	落下来装满了水
	升上去空空荡荡
	落下来装满了水

	悬在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子

	悬棺
	悬棺在云间
	上看不到你的天
	下见不到我的地
	秋天不用收粮食
	冬天不必盖房子


##山鬼

	有一个无人居住的老屋， 
	孤单的卧在荒野上。 
	它还保留着古老的门和窗， 
	却已没有炊烟和灯光。 

	春草在它的身旁长啊长， 
	那时我还没离开故乡。 
	蟋蟀在它的身旁唱啊唱， 
	那时我刚准备着去远方。 

	有一个无人祭奠的灵魂， 
	独自在荒山间游荡， 
	月光是她洁白的衣裳， 
	却没人为她点一柱香。 

	夜露是她莹莹的泪光， 
	那时爱情正栖息在我心上， 
	辰星是她憔悴的梦想， 
	那时爱人已长眠在他乡。 

	上帝坐在空荡荡的天堂， 
	诗人走在寂寞的世上， 
	时间慢慢的在水底凝固， 
	太阳疲倦的在极地驻足。 

	这时冰山醒来呼唤着生长， 
	这时巨树展翅渴望着飞翔， 
	这时我们离家去流浪， 
	长发宛若战旗在飘扬， 
	俯瞰逝去的悲欢和沧桑， 
	扛着自己的墓碑走遍四方。 


##鱼相忘于江湖

	鱼忘记了沧海， 
	虫忘记了尘埃， 
	神忘记了永恒， 
	人忘记了现在。 

	也是没有人的空山， 
	也是没有鹰的青天， 
	也是没有梦的睡眠， 
	也是没有故事的流年。 

	忘了此地是何地， 
	忘了今昔是何昔。 
	睁开眼睛就亮天， 
	闭上眼睛就黑天。 

	太阳出来，为了生活出去， 
	太阳落了，为了爱情回去。 
	班若波罗揭谛......

##幻觉支撑我们活下去

	那是一片蓝葡萄， 
	挂在戈壁的天尽头， 
	云外有片大草原， 
	有个孩子在放牛。 

	道路死在我身后， 
	离开河床水更自由， 
	为了不断的向前走， 
	我得相信那不是蜃楼。
 
	梦里全是湖水绿洲， 
	醒来满地是跳舞的石头。 
	啊，我的饥渴映红起伏的沙丘， 
	我不要清醒的水， 
	我只要晕眩的酒。 

	清醒的人倒在路旁， 
	幻觉带着我们向前走， 
	大风淘尽了我的衣兜， 
	失明的灵魂更加自由。
 
	我是世界壮丽的伤口， 
	伤口是我身上奔腾的河流。 
	啊，我的饥渴映红起伏的沙丘， 
	我不要清醒的水， 
	我只要晕眩的酒。 
	我不要清醒的水， 
	我只要如梦的酒。 

##参考

* 豆瓣小站  [周云蓬](http://site.douban.com/zhouyunpeng/room/418171/)
* 豆瓣读书  [绿皮火车](http://book.douban.com/subject/10743137/)
* 豆瓣读书  [春天责备](http://book.douban.com/subject/5333599/)

---
layout: post
title: RSS to Email
category: Miscellanies
tags: [RSS, Amazon, Mail]
---

##介绍

一般网站在提供RSS/Atom订阅的同时，也提供邮件订阅。受此启发，可以采用邮件的形式来订阅RSS，Google一下，还真有很多"RSS to Email"的文章， 稍微准备以下即可，具体可以参考[Turning Gmail into Google Reader](http://wcm1.web.rice.edu/turning-gmail-into-google-reader.html)。

###现状

一般公司邮箱分内网、外网，所有人都有内网邮箱，大部分人拥有公司域名的外网邮箱，但有些限制，如仅开放25/110端口，导致无法直接接收Gmail的邮件。 好在Gmail可以设置Forwarding规则，将Gmail收到的邮件以副本形式转发到个人的公司外网邮箱，这样就可以查收Gmail邮件。

只是最近https被限制，但Google Account登录时必须使用https协议，继而导致使用Google Account授权访问的RSS Reader无法登录。 无奈之下，只好另辟蹊径，于是找到了上文提到的"Turning Gmail into Google Reader"方法，将RSS以邮件形式发送到Gmail账户，方便存档。只是这样的话，邮件量会 持续增长，眼下迫在眉睫的是*如何高效处理邮件*。

###前提条件

* 拥有Gmail账号;
* 拥有联网主机/个人电脑，这里可以使用Amazon Web Services提供的Free Tier，如EC2，SES，不过有默认的容量限制;
* 运行环境：Python

###EC2 Account Notification

	Dear EC2 Customer,
	You recently reached a limit on the volume of email you were able to send out of SMTP port 25 on your instance:

	Instance ID: i-qwertbnm
	* IP Address: www.xxx.yyy.zz
	* Start date: 2013-10-07 05:37 +0000

	In order to maintain the quality of EC2 addresses for sending email, we enforce default limits on the amount of email that can be sent from EC2 accounts. If you wish to send larger amounts of email from EC2, you can apply to have these limits removed from your account by filling out our online request form.

	If you are unaware of your instance having sent emails, we advise checking your instance application(s) to confirm that this activity was intended. It is your responsibility to ensure that your instances and all applications are secured against unauthorized use. For suggestions on securing your instances, visit aws.amazon.com/security.

	Regards,
	Your Amazon Web Services EC2 team

##参考

* [Turning Gmail into Google Reader](http://wcm1.web.rice.edu/turning-gmail-into-google-reader.html)
* [Amazon Web Services](https://aws.amazon.com/console/)

---
layout: post
title: Java Resource
category: Miscellanies
tags: [Java, Resource]
---

大学期间主要学的编程语言是Java，做课程设计、毕业项目都是基于Java的；其他语言或多或少接触过（如C、C++、C#、VB），也写过一些小程序或小项目，但学得不深。

毕业后工作的一年时间内，除去培训，剩下大半年的时间都是在使用Java开发Web应用，使用的还是大学学到的一些技术和框架，不过引入了Flex、Ext JS等前端框架。

之后因工作调整，从小白开始逐步转向Oracle DBA，学习大量的主机、存储、网络、数据库等方面的知识，偶尔也打打杂，不直接参与项目，到现在已经又是一年的光阴了。

在做DBA这一年中，利用业余时间学习了Python，可以动手快速、简洁的实现一些自动化脚本、偶然想法、项目原型或者真正投入使用，很喜欢[The Zen of Python](http://www.python.org/dev/peps/pep-0020/)，也很喜欢“人 生苦短，我用Python”的格言。

最近几个月还学习了Ruby，十分喜欢Ruby、Ruby On Rails中随处可见的DSL，也开始折腾Github和Jekyll。

今天有同学问起Java学习的一些推荐资料，乘此机会正好整理一份，方便以后参考，以下是自己看过和值得推荐的一些资料清单。

##基础
	
主要熟悉Java语言历史、特点、基础语法以及常用标准库，推荐官方的参考Java SE5.0/6.0 [Java Specifications](http://docs.oracle.com/javase/specs/)， 另外，台湾人[良葛格](http://openhome.cc/Gossip/)从事Java教育经验丰富，他的Java Essence和Java上下教程写得浅显易懂，很适合Java初学者，看起来不费力，入门快。

[《Thinking in Java 4th》](http://pervasive2.morselli.unimo.it/~nicola/courses/IngegneriaDelSoftware/java/ThinkingInJava.pdf)很经典，可以作参考书，从Java的基础语法到高级特性（面向对象概念、容器、多线程等）都有涉及，但讲述详实，比较厚，不适合快速入门。

网上有针对Python程序员的Java教程，对比了概念、语法，可供Python程序员快速浏览和熟悉之用，详见[Welcome to Java for Python Programmers](http://interactivepython.org/runestone/static/java4python/index.html)。

编码规范：参考[Java Code Conversion](http://www.oracle.com/technetwork/java/codeconv-138413.html)。
    
##进阶

在了解Java语言基础之后，接下来可以熟悉下Java的网络和数据库编程，一般的Java项目都离不开这两项。

网络编程：参考[《Java Network Programming》](http://www.mdp.ac.id/materi/2012-2013-1/TI425/111061/TI425-111061-908-1.pdf)。

数据库：参考[《Java Database Programming with JDBC》](http://eduunix.ccut.edu.cn/index2/pdf/O'Reilly%20-%20Java%20Database%20Programming%20with%20JDBC.pdf)，[MongoDB Getting Started with Java Drive](http://docs.mongodb.org/ecosystem/tutorial/getting-started-with-java-driver/)。

##高级

技术点：参考《Java核心技术》[卷1](http://www.china-pub.com/208978)，[卷2](http://www.china-pub.com/508881)，[Volume 1](http://www.amazon.com/Core-Java-I-Fundamentals-8th-Sun/dp/0132354764/ref=sr_11_1?ie=UTF8&qid=1215592737&sr=11-1)，[Volume 2](http://www.amazon.com/Core-Java-Vol-Advanced-Features/dp/0132354799/ref=sr_1_1?ie=UTF8&s=books&qid=1227751671&sr=1-1)，介绍了Java技术的方方面面，阐述精确到位，叙述深入浅出，并包含大量示例，可作参考书。

并发编程：参考[《Java Concurrency in Practice》](http://cs.famaf.unc.edu.ar/~nicolasw/Docencia/PCeJ/jcip.pdf)，熟悉Java的并发模型和机制，有助于设计和编写并发性能良好的程序。

经验规则：参考[《Effective Java》](http://uet.vnu.edu.vn/~chauttm/e-books/java/Effective.Java.2nd.Edition.May.2008.3000th.Release.pdf),熟悉常见问题的解决方案，有助于编写高效、健壮、清晰的代码。

Java解惑：参考[《Java Puzzles》](http://www.javapuzzlers.com/)，熟悉Java或其类库的陷阱和缺陷而导致的bug，可以帮助了解实现细节。

设计模式：参考[《Head First Design Patterns》](http://isa.unomaha.edu/wp-content/uploads/2012/08/Design-Patterns.pdf)、[JDK里的设计模式](http://coolshell.cn/articles/3320.html)，Java编程更偏向设计和架构，因此必须熟悉常用的设计模式，并灵活运用于实际项目中。

JVM：参考[《The JVM Specification》](http://docs.oracle.com/javase/specs/jvms/se5.0/html/VMSpecTOC.doc.html)，Java虚拟机的规范和细节，对深入编译器和字节码有兴趣的可以参考。

##Web开发

主要介绍Java在Web开发方面的基础技术和框架。

####1.前端

HTML/CSS/JS：这些是基础，BS架构的应用离不开

####2.基础

JSP/Servlet：参考良葛格的[Servlet/JSP Gossip](http://openhome.cc/Gossip/ServletJSP/)，Java服务端编程基础，必须掌握。

XML：参考 [Java XML Tutorial](http://www.mkyong.com/tutorials/java-xml-tutorials/)，[《XML开发技术教程》](http://book.douban.com/subject/3246748/)，配套的[PPT](https://drive.google.com/folderview?id=0B_BiAoqYvwURRVJsSWRHcEx1MzA&usp=sharing&tid=0B_BiAoqYvwURUnVISmVDX3hZTGc)，编辑配置文件、自动化脚本、WebService等都离不开XML，必须掌握。

Web Container：参考[《How Tomcat Works》](http://files.cnblogs.com/wasp520/tomcat/HowTomcatWorks.pdf)，熟悉Java Web容器的原理以及实现细节，源代码很适合阅读。
        
####3.一些框架

大学时做过一些Web项目基本都是SSH框架，对中小型企业已经够用，其他的暂未接触。

前端框架: [Struts2+](http://struts.apache.org/development/2.x/)，参考[2010-3-10_Struts2_讲义_IBM](https://drive.google.com/folderview?id=0B_BiAoqYvwURTDBrR0kyZ0hKcDQ&usp=sharing&tid=0B_BiAoqYvwURUnVISmVDX3hZTGc)。
    
数据库ORM框架：[Hibernate3+](http://www.hibernate.org/)，参考[2012-9-12_Hibernate.ppt](https://docs.google.com/file/d/0B_BiAoqYvwURcFZ3LWRzdXlDQm8/edit?usp=drive_web)。

全能框架：[Spring2.5+](http://spring.io/)，参考[2012-9-12_Spring.ppt](https://docs.google.com/file/d/0B_BiAoqYvwURcWxzdGRtTE9FRkE/edit?usp=drive_web&pli=1)。

##工具

IDE: [Vim](http://www.vim.org/)，[Eclipse](http://www.eclipse.org/)，[MyEclipse](http://www.myeclipseide.com/)，Java开发并不难，主要在于设计，写代码方面更多的是在使用IDE（如自动补全、生成代码等），推荐使用MyEclipse，参考[MyEclipse10优化](http://www.cnblogs.com/batys/archive/2012/02/23/2364832.html)。

反编译：[JAD](http://varaneckas.com/jad/)，[jadeclipse(Eclipse插件)](http://sourceforge.net/projects/jadclipse/)，反编译.class文件，查看源代码。

自动化工具：[Ant](http://ant.apache.org/)，[Maven](http://maven.apache.org/)，用于项目的自动化编译、测试、打包、部署等。

##其他

UML：设计核心、参考手册, [UML相关工具一览](http://www.umlchina.com/tools/newindex1.htm)。从不同维度和层次对项目进行建模（分析和设计），有助于熟悉项目的整体架构。

我用得比较多的是[StarUML](staruml.sourceforge.net/‎)，现在也有很多Web版的UML工具，推荐[LucidChart](https://www.lucidchart.com/)，Google Docs现在也支持LucidChart。

##参考
	
* [Welcome to Java for Python Programmers](http://interactivepython.org/runestone/static/java4python/index.html)
* [Java Specifications](http://docs.oracle.com/javase/specs/)
* 良葛格 [Java Essence](http://openhome.cc/Gossip/JavaEssence/)
* 良葛格 [Java 上](http://openhome.cc/Gossip/JavaGossip-V1/)
* 良葛格 [Java 下](http://openhome.cc/Gossip/JavaGossip-V2/)
* 良葛格 [Servlet/JSP Gossip](http://openhome.cc/Gossip/ServletJSP/)
* [Java Code Conversion](http://www.oracle.com/technetwork/java/codeconv-138413.html)
* [《Thinking in Java 4th》](http://pervasive2.morselli.unimo.it/~nicola/courses/IngegneriaDelSoftware/java/ThinkingInJava.pdf)
* [《Java Network Programming》](http://www.mdp.ac.id/materi/2012-2013-1/TI425/111061/TI425-111061-908-1.pdf)
* [《Java Database Programming with JDBC》](http://eduunix.ccut.edu.cn/index2/pdf/O'Reilly%20-%20Java%20Database%20Programming%20with%20JDBC.pdf)
* [MongoDB Getting Started with Java Drive](http://docs.mongodb.org/ecosystem/tutorial/getting-started-with-java-driver/)
* 《Java核心技术》[卷1](http://www.china-pub.com/208978)，[卷2](http://www.china-pub.com/508881)，[Volume 1](http://www.amazon.com/Core-Java-I-Fundamentals-8th-Sun/dp/0132354764/ref=sr_11_1?ie=UTF8&qid=1215592737&sr=11-1),[Volume 2](http://www.amazon.com/Core-Java-Vol-Advanced-Features/dp/0132354799/ref=sr_1_1?ie=UTF8&s=books&qid=1227751671&sr=1-1)
* [《Java Concurrency in Practice》](http://cs.famaf.unc.edu.ar/~nicolasw/Docencia/PCeJ/jcip.pdf)
* [《The JVM Specification》](http://docs.oracle.com/javase/specs/jvms/se5.0/html/VMSpecTOC.doc.html)
* [Java XML Tutorial](http://www.mkyong.com/tutorials/java-xml-tutorials/)
* [《XML开发技术教程》](http://book.douban.com/subject/3246748/)
* [《How Tomcat Works》](http://files.cnblogs.com/wasp520/tomcat/HowTomcatWorks.pdf)
* [《Head First Design Patterns》](http://isa.unomaha.edu/wp-content/uploads/2012/08/Design-Patterns.pdf)
* [JDK里的设计模式](http://coolshell.cn/articles/3320.html)
* [Lisp的永恒之道](http://coolshell.cn/articles/7526.html)
* [Java SE5 API](http://docs.oracle.com/javase/1.5.0/docs/api/), [Java SE6 API](http://docs.oracle.com/javase/6/docs/api/)
* [Apache Tomcat 6](http://tomcat.apache.org/tomcat-6.0-doc/index.html)
* [UML](http://www.uml.org/)
* [陈皓 Java数据Top10](http://coolshell.cn/articles/14.html)
* [MyEclipse10优化](http://www.cnblogs.com/batys/archive/2012/02/23/2364832.html)
* [Google Drive](https://drive.google.com/folderview?id=0B_BiAoqYvwURUnVISmVDX3hZTGc&usp=sharing)
---
layout: post
title: Oracle Database RAC2Single Recovery 
category : Oracle
tags : [Oracle, Database, DBA]
---

###环境介绍

11gR2 RAC（原系统）

                           db1        db2
    public ip         192.168.1.9      192.168.1.10
    vip               192.168.1.11     192.168.1.12
    private ip        192.168.1.9      192.168.1.10
    scan ip                   192.168.1.13

target （目标系统）

    public ip            192.168.1.5

主要步骤（除去post步骤，耗时120~150min，可以多分配几个channel，提高处理速度）

    1）rac backup：原系统RMAN备份，耗时 30min
    
    2）copy backup set to target system(耗时30min) or shared by nfs(耗时5min，可提前准备好)
    
    3）rac2single recovery：RAC恢复到单节点数据库(耗时 80min，可以通过多分配通道提速)
                                     
        prepare：--耗时5min
            path:backup,datafile
            env:ORACLE_SID
            pfile: --耗时5min
                SID
                non-rac
                log_file_name_convert
                db_file_name_convert
                service_names
                other:*_dest*
        recovery：--耗时 70min
            set dbid;
            nomount, restore controlfile;
            mount;
            setnew name,restore database,switch datafile all,recover database; --耗时60min ~ 70min
            restore archivelogs; --不需要此步骤
            recreate controlfile; --不需要此步骤
            open database resetlogs; --耗时5min
    
    4）post：监听配置，spfile启动，全备
        listener configuration
        restart target system
        rman full backup targets system

## 11gR2 RMAN Backup
    
rman backup
    
    [oracle@db1 backup]$ rman target/

    Recovery Manager: Release 11.2.0.3.0 - Production on Thu Oct 17 10:17:42 2013

    Copyright (c) 1982, 2011, Oracle and/or its affiliates.  All rights reserved.

    connected to target database: PROD (DBID=240240185)

    RMAN> show all;

    using target database control file instead of recovery catalog
    RMAN configuration parameters for database with db_unique_name PROD are:
    CONFIGURE RETENTION POLICY TO REDUNDANCY 7;
    CONFIGURE BACKUP OPTIMIZATION OFF; # default
    CONFIGURE DEFAULT DEVICE TYPE TO DISK; # default
    CONFIGURE CONTROLFILE AUTOBACKUP ON;
    CONFIGURE CONTROLFILE AUTOBACKUP FORMAT FOR DEVICE TYPE DISK TO '/data/controlfile/ctl_file_%F';
    CONFIGURE DEVICE TYPE DISK PARALLELISM 1 BACKUP TYPE TO BACKUPSET; # default
    CONFIGURE DATAFILE BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default
    CONFIGURE ARCHIVELOG BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default
    CONFIGURE CHANNEL DEVICE TYPE DISK MAXPIECESIZE 10240 M;
    CONFIGURE MAXSETSIZE TO UNLIMITED; # default
    CONFIGURE ENCRYPTION FOR DATABASE OFF; # default
    CONFIGURE ENCRYPTION ALGORITHM 'AES128'; # default
    CONFIGURE COMPRESSION ALGORITHM 'BASIC' AS OF RELEASE 'DEFAULT' OPTIMIZE FOR LOAD TRUE ; # default
    CONFIGURE ARCHIVELOG DELETION POLICY TO NONE; # default
    CONFIGURE SNAPSHOT CONTROLFILE NAME TO '/u1/app/oracle/product/11.2.0/db1/dbs/snapcf_PROD1.f'; # default

    RMAN> run
    {
    ###backup database###
    backup  incremental level=0 tag='db_bak_lev0'        
            format='/data/backup/db_bak_lev0_%t_%p_%s'
            database;
    ###switch out the current logfile###
    sql 'alter system archive log thread 1 current';
    sql 'alter system archive log thread 2 current';
    ###backup archived log######
    backup archivelog all tag='arc_bak'
           format='/data/backup/archive_log_%t_%p_%s'
           skip inaccessible 
           filesperset 4
           not  backed up 1 times
           delete  input;
    ###backup current controlfile that contains the records for the backups just made###
    backup current controlfile tag='bak_ctlfile' format='/data/backup/ctl_file_%t_%p_%s';
    crosscheck backup;
    delete force noprompt expired backup;
    delete force noprompt obsolete;
    }

    ... ...


    RMAN> list backup of controlfile;


    List of Backup Sets
    ===================


    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    268     Full    18.42M     DISK        00:00:00     17-OCT-13      
            BP Key: 308   Status: AVAILABLE  Compressed: NO  Tag: TAG20131017T104019
            Piece Name: /data/controlfile/ctl_file_c-240240185-20131017-00
      Control File Included: Ckp SCN: 101311620    Ckp time: 17-OCT-13

    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    273     Full    18.39M     DISK        00:00:01     17-OCT-13      
            BP Key: 313   Status: AVAILABLE  Compressed: NO  Tag: BAK_CTLFILE
            Piece Name: /data/backup/ctl_file_829046450_1_273
      Control File Included: Ckp SCN: 101313008    Ckp time: 17-OCT-13

    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    274     Full    18.42M     DISK        00:00:00     17-OCT-13      
            BP Key: 314   Status: AVAILABLE  Compressed: NO  Tag: TAG20131017T104052
            Piece Name: /data/controlfile/ctl_file_c-240240185-20131017-01
      Control File Included: Ckp SCN: 101313063    Ckp time: 17-OCT-13

## copy backup set

###use scp

copy backup set using scp (estimated time: 0.55h (214GB * 1024MB / 110MB / 3600 ))

file system

    [root@target ~]# df -h
    Filesystem            Size  Used Avail Use% Mounted on
    /dev/mapper/vg_erptest1-LV_ROOT
                           50G   16G   31G  35% /
    tmpfs                  32G  144K   32G   1% /dev/shm
    /dev/mapper/vg_erptest1-LV_APPS
                           99G  188M   94G   1% /apps
    /dev/sda2             485M   59M  401M  13% /boot
    /dev/sda1             200M  260K  200M   1% /boot/efi
    /dev/mapper/vg_erptest1-LV_TMP
                          9.9G  151M  9.2G   2% /tmp
    /dev/mapper/vg_erptest1-LV_U1
                          985G  212G  724G  23% /u1

make directory                          
                                       
    [root@target ~]# mkdir /u1/backup
    [root@target ~]# ls -l /u1
    total 35686692
    drwxrwx---. 10 testora dba         4096 Aug 15 14:04 11gR2_BASE
    drwxrwx---.  7 testora dba         4096 Jun  1 09:33 11gR2_BASE_11201
    drwxr-xr-x.  2 root    root        4096 Oct 15 13:49 backup
    drwxr-xr-x.  4 testora dba         4096 Jun 15 14:45 db_expimp
    drwx------.  2 root    root       16384 May 28 01:24 lost+found
    drwxr-xr-x.  3 testmgr dba         4096 Aug 15 14:01 oracle
    drwxr-xr-x. 10 testmgr dba         4096 Sep  5 21:44 patches
    -rw-r--r--.  1 root    root 36543116257 Jun 16 02:34 R1213_DB.tar.gz
    drwxr-xr-x.  3 root    root        4096 May 29 18:18 redhat

copy parameter file

    [oracle@db1 ~]$ sqlplus  /nolog

    SQL*Plus: Release 11.2.0.3.0 Production on Tue Oct 15 14:09:56 2013

    Copyright (c) 1982, 2011, Oracle.  All rights reserved.

    SQL> conn /as sysdba
    Connected.
    SQL> show parameter spfile

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    spfile				     string	 /u1/app/oracle/product/11.2.0/
                             db1/dbs/spfilePROD1.ora
    SQL> create pfile='/home/oracle/initPROD1.ora' from spfile;

    File created.

    SQL> exit
    Disconnected from Oracle Database 11g Enterprise Edition Release 11.2.0.3.0 - 64bit Production
    With the Partitioning, Real Application Clusters, Automatic Storage Management, OLAP,
    Data Mining and Real Application Testing options
    
    [oracle@db1 ~]$ scp initPROD1.ora root@192.168.1.5:/u1/backup/
    root@192.168.1.5's password: 
    initPROD1.ora                                                                                              100% 3121     3.1KB/s   00:00    

copy backup set
    
    [root@db1 ~]# scp /data/backup/* root@192.168.1.5:/u1/backup
    root@192.168.1.5's password: 
    20131016_inc0_7somi1rp_1_1.bkp                                                                      100% 1932KB   1.9MB/s   00:00    
    20131016_inc0_7tomi1rt_1_1.bkp                                                                      100%   72GB 110.7MB/s   11:07    
    20131016_inc0_7uomi2a5_1_1.bkp                                                                      100%   71GB 110.4MB/s   11:01    
    20131016_inc0_7vomi2n6_1_1.bkp                                                                      100%   67GB 109.6MB/s   10:27    
    20131016_inc0_80omi33a_1_1.bkp                                                                      100%   16MB  16.2MB/s   00:01    
    20131016_inc0_81omi33h_1_1.bkp                                                                      100% 2203KB   2.2MB/s   00:00 
    
    [testora@target ~]$ ll /u1/backup/
    total 220713556
    -rwxr-xr-x. 1 root    root     1978368 Oct 16 15:55 20131016_inc0_7somi1rp_1_1.bkp
    -rwxr-xr-x. 1 root    root 77434830848 Oct 16 16:06 20131016_inc0_7tomi1rt_1_1.bkp
    -rwxr-xr-x. 1 root    root 76532965376 Oct 16 16:18 20131016_inc0_7uomi2a5_1_1.bkp
    -rwxr-xr-x. 1 root    root 72021614592 Oct 16 16:28 20131016_inc0_7vomi2n6_1_1.bkp
    -rwxr-xr-x. 1 root    root    17006592 Oct 16 16:28 20131016_inc0_80omi33a_1_1.bkp
    -rwxr-xr-x. 1 root    root     2255872 Oct 16 16:28 20131016_inc0_81omi33h_1_1.bkp
    -rw-r--r--. 1 testora dba         3121 Oct 15 14:11 initPROD1.ora

make soft link    

    [testora@target ~]$ ln -s /u1/backup/  /data/backup
    [testora@target ~]$ ll /data/backup
    lrwxrwxrwx. 1 testora dba 11 Oct 16 16:33 /data/backup -> /u1/backup/
    [testora@target ~]$ ll /data/backup/
    total 220713556
    -rwxr-xr-x. 1 root    root     1978368 Oct 16 15:55 20131016_inc0_7somi1rp_1_1.bkp
    -rwxr-xr-x. 1 root    root 77434830848 Oct 16 16:06 20131016_inc0_7tomi1rt_1_1.bkp
    -rwxr-xr-x. 1 root    root 76532965376 Oct 16 16:18 20131016_inc0_7uomi2a5_1_1.bkp
    -rwxr-xr-x. 1 root    root 72021614592 Oct 16 16:28 20131016_inc0_7vomi2n6_1_1.bkp
    -rwxr-xr-x. 1 root    root    17006592 Oct 16 16:28 20131016_inc0_80omi33a_1_1.bkp
    -rwxr-xr-x. 1 root    root     2255872 Oct 16 16:28 20131016_inc0_81omi33h_1_1.bkp
    -rw-r--r--. 1 testora dba         3121 Oct 15 14:11 initPROD1.ora
    
###shared with nfs 

configure exportfs

    [root@db1 ~]# cat /etc/exports 
    /patch *(rw,root_squash,no_all_squash,sync)
    /data *(rw,no_root_squash,no_all_squash,sync)
    
restart nfs
    
    [root@db1 ~]# service nfs restart
    Shutting down NFS daemon:                                  [  OK  ]
    Shutting down NFS mountd:                                  [  OK  ]
    Shutting down NFS quotas:                                  [  OK  ]
    Shutting down NFS services:                                [  OK  ]
    Starting NFS services:                                     [  OK  ]
    Starting NFS quotas:                                       [  OK  ]
    Starting NFS mountd:                                       [  OK  ]
    Stopping RPC idmapd:                                       [  OK  ]
    Starting RPC idmapd:                                       [  OK  ]
    Starting NFS daemon:                                       [  OK  ]
    [root@db1 ~]# exportfs 
    /patch        	<world>
    /data    	<world>

mount on target system
    
    [root@target ~]# showmount -e 192.168.1.9
    Export list for 192.168.1.9:
    /data/pre1 *
    /patch     *
    [root@target ~]# mount 192.168.1.9:/data/ /data/
    [root@target ~]# ll /data/backup
    total 220713556
    -rw-r-----. 1 nobody nobody     1978368 Oct 16 12:32 20131016_inc0_7somi1rp_1_1.bkp
    -rw-r-----. 1 nobody nobody 77434830848 Oct 16 12:40 20131016_inc0_7tomi1rt_1_1.bkp
    -rw-r-----. 1 nobody nobody 76532965376 Oct 16 12:47 20131016_inc0_7uomi2a5_1_1.bkp
    -rw-r-----. 1 nobody nobody 72021614592 Oct 16 12:54 20131016_inc0_7vomi2n6_1_1.bkp
    -rw-r-----. 1 nobody nobody    17006592 Oct 16 12:54 20131016_inc0_80omi33a_1_1.bkp
    -rw-r-----. 1 nobody nobody     2255872 Oct 16 12:54 20131016_inc0_81omi33h_1_1.bkp

chmod backup directory    
   
    [root@target ~]# chmod -R 777 /data/backup/
    [root@target ~]# ll /data/backup/
    total 220713556
    -rwxrwxrwx. 1 nobody nobody     1978368 Oct 16 12:32 20131016_inc0_7somi1rp_1_1.bkp
    -rwxrwxrwx. 1 nobody nobody 77434830848 Oct 16 12:40 20131016_inc0_7tomi1rt_1_1.bkp
    -rwxrwxrwx. 1 nobody nobody 76532965376 Oct 16 12:47 20131016_inc0_7uomi2a5_1_1.bkp
    -rwxrwxrwx. 1 nobody nobody 72021614592 Oct 16 12:54 20131016_inc0_7vomi2n6_1_1.bkp
    -rwxrwxrwx. 1 nobody nobody    17006592 Oct 16 12:54 20131016_inc0_80omi33a_1_1.bkp
    -rwxrwxrwx. 1 nobody nobody     2255872 Oct 16 12:54 20131016_inc0_81omi33h_1_1.bkp
    
##recovery on target system

###edit parameter file

copy pfile to destination

    [testora@target ~]$ export ORACLE_SID=PROD
    [testora@target ~]$ echo $ORACLE_HOME
    /u1/11gR2_BASE/product/11.2.0
    [testora@target ~]$ cp /data/backup/initPROD1.ora /u1/11gR2_BASE/product/11.2.0/dbs/initPROD.ora
    [testora@target ~]$ cd /u1/11gR2_BASE/product/11.2.0/dbs/

before edit

    [testora@target dbs]$ cat initPROD.ora 
    PROD1.__db_cache_size=40802189312
    PROD1.__java_pool_size=939524096
    PROD1.__large_pool_size=134217728
    PROD1.__oracle_base='/u1/app/oracle'#ORACLE_BASE set from environment
    PROD1.__pga_aggregate_target=1073741824
    PROD1.__sga_target=51539607552
    PROD1.__shared_io_pool_size=402653184
    PROD1.__shared_pool_size=8724152320
    PROD1.__streams_pool_size=268435456
    *._b_tree_bitmap_plans=FALSE# Required 11i setting
    *._disable_fast_validate=TRUE#required by 761570.1
    *._fast_full_scan_enabled=FALSE
    *._immediate_commit_propagation=TRUE
    *._like_with_bind_as_equality=TRUE
    *._lm_global_posts=TRUE
    *._optimizer_autostats_job=false# Turning off auto statistics 
    *._pga_max_size=1048576000#required by 761570.1
    *._sort_elimination_cost_ratio=5
    *._system_trig_enabled=true
    *._trace_files_public=TRUE
    *._use_adaptive_log_file_sync='TRUE'# 'FALSE'
    *.aq_tm_processes=1
    *.audit_file_dest='/u1/app/oracle/admin/prod/adump'
    *.AUDIT_SYS_OPERATIONS=FALSE
    *.cluster_database=TRUE
    *.cluster_database_instances=2
    *.compatible='11.2.0'
    *.control_files='+DATADG/PROD/CONTROLFILE/control01.ctl','+FRADG/PROD/CONTROLFILE/control02.ctl'
    *.cursor_sharing='EXACT'# Required 11i settting
    *.db_block_checking='FALSE'
    *.db_block_checksum='TRUE'
    *.db_block_size=8192
    *.db_files=2000# Max. no. of database files
    *.db_name='PROD'
    *.db_writer_processes=6# improve IO performance
    *.diagnostic_dest='/u1/app/oracle/product/11.2.0/db1/admin/PROD1_db1'
    *.dml_locks=10000
    *.event='44951 trace name context forever, level 1024','31156 trace name context forever, level 0x400'# ref 3-4941349801
    *.filesystemio_options='SETALL'
    *.instance_name='PROD1'
    *.instance_number=1
    *.job_queue_processes=64
    *.log_archive_dest_1='LOCATION=+FRADG/PROD/archivelog/PROD1'
    *.log_archive_format='%t_%s_%r.dbf'
    *.log_buffer=10485760
    *.log_checkpoint_interval=100000
    *.log_checkpoint_timeout=1200# Checkpoint at least every 20 mins.
    *.log_checkpoints_to_alert=TRUE
    *.max_dump_file_size='20480'# trace file size 
    *.nls_comp='binary'# Required 11i setting 
    *.nls_date_format='DD-MON-RR'
    *.nls_length_semantics='BYTE'# Required 11i setting  
    *.nls_numeric_characters='.,'
    *.nls_sort='binary'# Required 11i setting 
    *.nls_territory='america'
    *.o7_dictionary_accessibility=FALSE#MP
    *.olap_page_pool_size=4194304
    *.open_cursors=1600# Consumes process memory, unless using MTS. 
    *.optimizer_secure_view_merging=false
    *.OS_AUTHENT_PREFIX=''
    *.parallel_max_servers=64
    *.parallel_min_servers=0
    *.pga_aggregate_target=1G
    *.plsql_code_type='INTERPRETED'# Default 11i setting
    *.plsql_optimize_level=2# Required 11i setting
    *.processes=5000# Max. no. of users x 2
    *.remote_listener='erpdb-cluster-scan:1521'
    *.sec_case_sensitive_logon=FALSE
    *.service_names='PROD1'
    *.session_cached_cursors=500
    *.sessions=4400# 2 X processes  
    *.sga_target=48G
    *.shared_pool_reserved_size=440M
    *.shared_pool_size=4400M
    *.SQL92_SECURITY=TRUE
    *.thread=1
    *.timed_statistics=true
    *.undo_management='AUTO'# Required 11i setting
    *.undo_tablespace='APPS_UNDOTS1'# Required 11i setting
    *.utl_file_dir='/usr/tmp','/u1/11gR2_BASE/product/11.2.0/appsutil/outbound/PROD1_db1'
    *.workarea_size_policy='AUTO'# Required 11i setting

after edit
    
    [testora@target dbs]$ cat initPROD.ora 
    PROD.__db_cache_size=40802189312
    PROD.__java_pool_size=939524096
    PROD.__large_pool_size=134217728
    PROD.__oracle_base='/u1/11gR2_BASE'#ORACLE_BASE set from environment
    PROD.__pga_aggregate_target=1073741824
    PROD.__sga_target=51539607552
    PROD.__shared_io_pool_size=402653184
    PROD.__shared_pool_size=8724152320
    PROD.__streams_pool_size=268435456
    *._b_tree_bitmap_plans=FALSE# Required 11i setting
    *._disable_fast_validate=TRUE#required by 761570.1
    *._fast_full_scan_enabled=FALSE
    *._immediate_commit_propagation=TRUE
    *._like_with_bind_as_equality=TRUE
    *._lm_global_posts=TRUE
    *._optimizer_autostats_job=false# Turning off auto statistics 
    *._pga_max_size=1048576000#required by 761570.1
    *._sort_elimination_cost_ratio=5
    *._system_trig_enabled=true
    *._trace_files_public=TRUE
    *._use_adaptive_log_file_sync='TRUE'# 'FALSE'
    *.aq_tm_processes=1
    *.audit_file_dest='/u1/11gR2_BASE/admin/prod/adump'
    *.AUDIT_SYS_OPERATIONS=FALSE
    *.compatible='11.2.0'
    *.control_files='/u1/TEST/testora/testdata/control01.ctl','/u1/TEST/testora/testdata/control02.ctl'
    *.cursor_sharing='EXACT'# Required 11i settting
    *.db_block_checking='FALSE'
    *.db_block_checksum='TRUE'
    *.db_block_size=8192
    *.db_files=2000# Max. no. of database files
    *.db_name='PROD'
    *.db_writer_processes=6# improve IO performance
    *.diagnostic_dest='/u1/11gR2_BASE/admin/prod'
    *.dml_locks=10000
    *.event='44951 trace name context forever, level 1024','31156 trace name context forever, level 0x400'# ref 3-4941349801
    *.filesystemio_options='SETALL'
    *.instance_name='PROD'
    *.job_queue_processes=64
    *.log_archive_dest_1='LOCATION=/u1/TEST/testora/archive'
    *.log_archive_format='%t_%s_%r.dbf'
    *.log_buffer=10485760
    *.log_checkpoint_interval=100000
    *.log_checkpoint_timeout=1200# Checkpoint at least every 20 mins.
    *.log_checkpoints_to_alert=TRUE
    *.max_dump_file_size='20480'# trace file size 
    *.nls_comp='binary'# Required 11i setting 
    *.nls_date_format='DD-MON-RR'
    *.nls_length_semantics='BYTE'# Required 11i setting  
    *.nls_numeric_characters='.,'
    *.nls_sort='binary'# Required 11i setting 
    *.nls_territory='america'
    *.o7_dictionary_accessibility=FALSE#MP
    *.olap_page_pool_size=4194304
    *.open_cursors=1600# Consumes process memory, unless using MTS. 
    *.optimizer_secure_view_merging=false
    *.OS_AUTHENT_PREFIX=''
    *.parallel_max_servers=64
    *.parallel_min_servers=0
    *.pga_aggregate_target=1G
    *.plsql_code_type='INTERPRETED'# Default 11i setting
    *.plsql_optimize_level=2# Required 11i setting
    *.processes=5000# Max. no. of users x 2
    *.sec_case_sensitive_logon=FALSE
    *.service_names='PROD'
    *.session_cached_cursors=500
    *.sessions=4400# 2 X processes  
    *.sga_target=48G
    *.shared_pool_reserved_size=440M
    *.shared_pool_size=4400M
    *.SQL92_SECURITY=TRUE
    *.thread=1
    *.timed_statistics=true
    *.undo_management='AUTO'# Required 11i setting
    *.undo_tablespace='APPS_UNDOTS1'# Required 11i setting
    *.utl_file_dir='/usr/tmp','/u1/11gR2_BASE/product/11.2.0/appsutil/PROD'
    *.workarea_size_policy='AUTO'# Required 11i setting
    *.log_file_name_convert=('+DATADG/prodora/proddata','/u1/TEST/testora/testdata')
    *.db_file_name_convert=('+DATADG/prodora/proddata','/u1/TEST/testora/testdata')

copy initPROD.ora to init.ora

    [testora@target dbs]$ cp initPROD.ora init.ora
    
diff between initPROD1.ora and initPROD.ora

    [testora@target ~]$ diff  /data/backup/initPROD1.ora /u1/11gR2_BASE/product/11.2.0/dbs/initPROD.ora
    1,9c1,9
    < PROD1.__db_cache_size=40802189312
    < PROD1.__java_pool_size=939524096
    < PROD1.__large_pool_size=134217728
    < PROD1.__oracle_base='/u1/app/oracle'#ORACLE_BASE set from environment
    < PROD1.__pga_aggregate_target=1073741824
    < PROD1.__sga_target=51539607552
    < PROD1.__shared_io_pool_size=402653184
    < PROD1.__shared_pool_size=8724152320
    < PROD1.__streams_pool_size=268435456
    ---
    > PROD.__db_cache_size=40802189312
    > PROD.__java_pool_size=939524096
    > PROD.__large_pool_size=134217728
    > PROD.__oracle_base='/u1/11gR2_BASE'#ORACLE_BASE set from environment
    > PROD.__pga_aggregate_target=1073741824
    > PROD.__sga_target=51539607552
    > PROD.__shared_io_pool_size=402653184
    > PROD.__shared_pool_size=8724152320
    > PROD.__streams_pool_size=268435456
    23c23
    < *.audit_file_dest='/u1/app/oracle/admin/prod/adump'
    ---
    > *.audit_file_dest='/u1/11gR2_BASE/admin/prod/adump'
    25,26d24
    < *.cluster_database=TRUE
    < *.cluster_database_instances=2
    28c26
    < *.control_files='+DATADG/PROD/CONTROLFILE/control01.ctl','+FRADG/PROD/CONTROLFILE/control02.ctl'
    ---
    > *.control_files='/u1/TEST/testora/testdata/control01.ctl','/u1/TEST/testora/testdata/control02.ctl'
    36c34
    < *.diagnostic_dest='/u1/app/oracle/product/11.2.0/db1/admin/PROD1_db1'
    ---
    > *.diagnostic_dest='/u1/11gR2_BASE/admin/prod'
    40,41c38
    < *.instance_name='PROD1'
    < *.instance_number=1
    ---
    > *.instance_name='PROD'
    43c40
    < *.log_archive_dest_1='LOCATION=+FRADG/PROD/archivelog/PROD1'
    ---
    > *.log_archive_dest_1='LOCATION=/u1/TEST/testora/archive'
    67d63
    < *.remote_listener='erpdb-cluster-scan:1521'
    69c67
    < *.service_names='PROD1'
    ---
    > *.service_names='PROD'
    80c76
    < *.utl_file_dir='/usr/tmp','/u1/11gR2_BASE/product/11.2.0/appsutil/outbound/PROD1_db1'
    ---
    > *.utl_file_dir='/usr/tmp','/u1/11gR2_BASE/product/11.2.0/appsutil/PROD'
    81a78,79
    > *.log_file_name_convert=('+DATADG/prodora/proddata','/u1/TEST/testora/testdata')
    > *.db_file_name_convert=('+DATADG/prodora/proddata','/u1/TEST/testora/testdata')
    
###recovery test
    
    [testora@target ~]$ export ORACLE_SID=PROD
    [testora@target ~]$ rman target/

    Recovery Manager: Release 11.2.0.3.0 - Production on Thu Oct 17 10:48:57 2013

    Copyright (c) 1982, 2011, Oracle and/or its affiliates.  All rights reserved.

    connected to target database (not started)

    RMAN> set dbid=240240185;

    executing command: SET DBID

    RMAN> startup nomount;

    Oracle instance started

    Total System Global Area   51309510656 bytes

    Fixed Size                     2240344 bytes
    Variable Size              10066329768 bytes
    Database Buffers           41204842496 bytes
    Redo Buffers                  36098048 bytes

    RMAN> restore controlfile from '/data/controlfile/ctl_file_c-240240185-20131017-01';

    Starting restore at 17-OCT-13
    using target database control file instead of recovery catalog
    allocated channel: ORA_DISK_1
    channel ORA_DISK_1: SID=2 device type=DISK

    channel ORA_DISK_1: restoring control file
    channel ORA_DISK_1: restore complete, elapsed time: 00:00:01
    output file name=/u1/TEST/testora/testdata/control01.ctl
    output file name=/u1/TEST/testora/testdata/control02.ctl
    Finished restore at 17-OCT-13

    RMAN> alter database mount;

    database mounted
    released channel: ORA_DISK_1

    RMAN> list backup of controlfile;


    List of Backup Sets
    ===================


    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    262     Incr 0  18.42M     DISK        00:00:01     16-OCT-13      
            BP Key: 282   Status: AVAILABLE  Compressed: NO  Tag: TEST INC0 BACKUP
            Piece Name: /data/backup/20131016_inc0_86omij5d_1_1.bkp
      Control File Included: Ckp SCN: 99193819     Ckp time: 16-OCT-13

    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    264     Full    18.39M     DISK        00:00:01     17-OCT-13      
            BP Key: 284   Status: AVAILABLE  Compressed: NO  Tag: CF1
            Piece Name: /data/backup/ctl_file_20131017_88omkbta
      Control File Included: Ckp SCN: 100644641    Ckp time: 17-OCT-13

    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    268     Full    18.42M     DISK        00:00:00     17-OCT-13      
            BP Key: 308   Status: AVAILABLE  Compressed: NO  Tag: TAG20131017T104019
            Piece Name: /data/controlfile/ctl_file_c-240240185-20131017-00
      Control File Included: Ckp SCN: 101311620    Ckp time: 17-OCT-13

    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    273     Full    18.39M     DISK        00:00:01     17-OCT-13      
            BP Key: 313   Status: AVAILABLE  Compressed: NO  Tag: BAK_CTLFILE
            Piece Name: /data/backup/ctl_file_829046450_1_273
      Control File Included: Ckp SCN: 101313008    Ckp time: 17-OCT-13

    RMAN> list backup of spfile;


    List of Backup Sets
    ===================


    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    262     Incr 0  18.42M     DISK        00:00:01     16-OCT-13      
            BP Key: 282   Status: AVAILABLE  Compressed: NO  Tag: TEST INC0 BACKUP
            Piece Name: /data/backup/20131016_inc0_86omij5d_1_1.bkp
      SPFILE Included: Modification time: 15-OCT-13
      SPFILE db_unique_name: PROD

    BS Key  Type LV Size       Device Type Elapsed Time Completion Time
    ------- ---- -- ---------- ----------- ------------ ---------------
    268     Full    18.42M     DISK        00:00:00     17-OCT-13      
            BP Key: 308   Status: AVAILABLE  Compressed: NO  Tag: TAG20131017T104019
            Piece Name: /data/controlfile/ctl_file_c-240240185-20131017-00
      SPFILE Included: Modification time: 15-OCT-13
      SPFILE db_unique_name: PROD

    RMAN> run {set newname for datafile 1 to "/u1/TEST/testora/testdata/system01.dbf"; restore datafile 1;recover datafile 1;}

    executing command: SET NEWNAME

    Starting restore at 17-OCT-13
    allocated channel: ORA_DISK_1
    channel ORA_DISK_1: SID=2 device type=DISK

    channel ORA_DISK_1: starting datafile backup set restore
    channel ORA_DISK_1: specifying datafile(s) to restore from backup set
    channel ORA_DISK_1: restoring datafile 00001 to /u1/TEST/testora/testdata/system01.dbf
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829046009_1_267
    channel ORA_DISK_1: piece handle=/data/backup/db_bak_lev0_829046009_1_267 tag=DB_BAK_LEV0
    channel ORA_DISK_1: restored backup piece 1
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829046009_2_267
    channel ORA_DISK_1: piece handle=/data/backup/db_bak_lev0_829046009_2_267 tag=DB_BAK_LEV0
    channel ORA_DISK_1: restored backup piece 2
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829046009_3_267
    channel ORA_DISK_1: piece handle=/data/backup/db_bak_lev0_829046009_3_267 tag=DB_BAK_LEV0
    channel ORA_DISK_1: restored backup piece 3
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829046009_4_267
    channel ORA_DISK_1: piece handle=/data/backup/db_bak_lev0_829046009_4_267 tag=DB_BAK_LEV0
    channel ORA_DISK_1: restored backup piece 4
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829046009_5_267
    channel ORA_DISK_1: piece handle=/data/backup/db_bak_lev0_829046009_5_267 tag=DB_BAK_LEV0
    channel ORA_DISK_1: restored backup piece 5
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829046009_6_267
    channel ORA_DISK_1: piece handle=/data/backup/db_bak_lev0_829046009_6_267 tag=DB_BAK_LEV0
    channel ORA_DISK_1: restored backup piece 6
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829046009_7_267
    channel ORA_DISK_1: piece handle=/data/backup/db_bak_lev0_829046009_7_267 tag=DB_BAK_LEV0
    channel ORA_DISK_1: restored backup piece 7
    channel ORA_DISK_1: restore complete, elapsed time: 00:10:35
    Finished restore at 17-OCT-13

    Starting recover at 17-OCT-13
    using channel ORA_DISK_1

    RMAN-00571: ===========================================================
    RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS ===============
    RMAN-00571: ===========================================================
    RMAN-03002: failure of recover command at 10/17/2013 11:01:15
    RMAN-06067: RECOVER DATABASE required with a backup or created control file
    
    [root@target testdata]# ll -h
    total 1.1G
    -rw-r-----. 1 testora dba   19M Oct 17 11:01 control01.ctl
    -rw-r-----. 1 testora dba   19M Oct 17 11:01 control02.ctl
    -rw-r-----. 1 testora dba 1001M Oct 17 11:01 system01.dbf
    
    
rman-06067
    
    Error:	RMAN 6067
    Text:	RECOVER DATABASE required with a backup or created controlfile 
    ---------------------------------------------------------------------------
    Cause:	The controlfile has been restored from a backup or was created via 
        ALTER DATABASE CREATE CONTROLFILE. 
    Action:	Use the RECOVER DATABASE command to perform the recovery.
    
clean failure instance and datafiles

    RMAN> shutdown immediate;
    
    [root@target testdata]# rm control01.ctl control02.ctl system01.dbf
    
###recover database
  
sql script to set newname for datafile  
  
    SELECT FILE#,
           NAME,
           '''' ||NEWNAME || ''',' newname,
           'set newname for datafile ' || FILE# || ' to "' || NEWNAME ||
           '";' RECOVERY
      FROM (SELECT FILE#,
                   NAME,
                   REPLACE(REPLACE(NAME,
                           '+DATADG/prod/datafile/',
                           '/u1/TEST/testora/testdata/'),
                           '+DATADG/prod/undofile/',
                           '/u1/TEST/testora/testdata/'
                           ) NEWNAME
              FROM V$DATAFILE);
              
sql script output

    --new path
    '/u1/TEST/testora/testdata/system01.dbf',
    '/u1/TEST/testora/testdata/system02.dbf',
    '/u1/TEST/testora/testdata/system03.dbf',
    '/u1/TEST/testora/testdata/system04.dbf',
    '/u1/TEST/testora/testdata/system05.dbf',
    '/u1/TEST/testora/testdata/system06.dbf',
    '/u1/TEST/testora/testdata/system07.dbf',
    '/u1/TEST/testora/testdata/system08.dbf',
    '/u1/TEST/testora/testdata/system09.dbf',
    '/u1/TEST/testora/testdata/system10.dbf',
    '/u1/TEST/testora/testdata/system11.dbf',
    '/u1/TEST/testora/testdata/sysaux01.dbf',
    '/u1/TEST/testora/testdata/undo01.dbf',
    '/u1/TEST/testora/testdata/undo02.dbf',
    '/u1/TEST/testora/testdata/undo03.dbf',
    '/u1/TEST/testora/testdata/a_archive01.dbf',
    '/u1/TEST/testora/testdata/a_archive02.dbf',
    '/u1/TEST/testora/testdata/a_archive03.dbf',
    '/u1/TEST/testora/testdata/a_archive04.dbf',
    '/u1/TEST/testora/testdata/a_archive05.dbf',
    '/u1/TEST/testora/testdata/a_int01.dbf',
    '/u1/TEST/testora/testdata/a_int02.dbf',
    '/u1/TEST/testora/testdata/a_int03.dbf',
    '/u1/TEST/testora/testdata/a_int04.dbf',
    '/u1/TEST/testora/testdata/a_int05.dbf',
    '/u1/TEST/testora/testdata/a_int06.dbf',
    '/u1/TEST/testora/testdata/a_int07.dbf',
    '/u1/TEST/testora/testdata/a_int08.dbf',
    '/u1/TEST/testora/testdata/a_media01.dbf',
    '/u1/TEST/testora/testdata/a_media02.dbf',
    '/u1/TEST/testora/testdata/a_media03.dbf',
    '/u1/TEST/testora/testdata/a_media04.dbf',
    '/u1/TEST/testora/testdata/a_media05.dbf',
    '/u1/TEST/testora/testdata/a_media06.dbf',
    '/u1/TEST/testora/testdata/a_media07.dbf',
    '/u1/TEST/testora/testdata/a_media08.dbf',
    '/u1/TEST/testora/testdata/a_media09.dbf',
    '/u1/TEST/testora/testdata/a_media10.dbf',
    '/u1/TEST/testora/testdata/a_nolog01.dbf',
    '/u1/TEST/testora/testdata/a_queue01.dbf',
    '/u1/TEST/testora/testdata/a_queue02.dbf',
    '/u1/TEST/testora/testdata/a_queue03.dbf',
    '/u1/TEST/testora/testdata/a_queue04.dbf',
    '/u1/TEST/testora/testdata/a_queue05.dbf',
    '/u1/TEST/testora/testdata/a_queue06.dbf',
    '/u1/TEST/testora/testdata/a_queue07.dbf',
    '/u1/TEST/testora/testdata/a_queue08.dbf',
    '/u1/TEST/testora/testdata/a_queue09.dbf',
    '/u1/TEST/testora/testdata/a_queue10.dbf',
    '/u1/TEST/testora/testdata/a_queue11.dbf',
    '/u1/TEST/testora/testdata/a_queue12.dbf',
    '/u1/TEST/testora/testdata/a_queue13.dbf',
    '/u1/TEST/testora/testdata/a_queue14.dbf',
    '/u1/TEST/testora/testdata/a_queue15.dbf',
    '/u1/TEST/testora/testdata/a_queue16.dbf',
    '/u1/TEST/testora/testdata/a_queue17.dbf',
    '/u1/TEST/testora/testdata/a_queue18.dbf',
    '/u1/TEST/testora/testdata/a_queue19.dbf',
    '/u1/TEST/testora/testdata/a_queue20.dbf',
    '/u1/TEST/testora/testdata/a_queue21.dbf',
    '/u1/TEST/testora/testdata/a_queue22.dbf',
    '/u1/TEST/testora/testdata/a_queue23.dbf',
    '/u1/TEST/testora/testdata/a_queue24.dbf',
    '/u1/TEST/testora/testdata/a_queue25.dbf',
    '/u1/TEST/testora/testdata/a_ref01.dbf',
    '/u1/TEST/testora/testdata/a_ref02.dbf',
    '/u1/TEST/testora/testdata/a_summ01.dbf',
    '/u1/TEST/testora/testdata/a_summ02.dbf',
    '/u1/TEST/testora/testdata/a_summ03.dbf',
    '/u1/TEST/testora/testdata/a_summ04.dbf',
    '/u1/TEST/testora/testdata/a_summ05.dbf',
    '/u1/TEST/testora/testdata/a_tools01.dbf',
    '/u1/TEST/testora/testdata/a_txn_data01.dbf',
    '/u1/TEST/testora/testdata/a_txn_data02.dbf',
    '/u1/TEST/testora/testdata/a_txn_data03.dbf',
    '/u1/TEST/testora/testdata/a_txn_data04.dbf',
    '/u1/TEST/testora/testdata/a_txn_data05.dbf',
    '/u1/TEST/testora/testdata/a_txn_data06.dbf',
    '/u1/TEST/testora/testdata/a_txn_data07.dbf',
    '/u1/TEST/testora/testdata/a_txn_data08.dbf',
    '/u1/TEST/testora/testdata/a_txn_data09.dbf',
    '/u1/TEST/testora/testdata/a_txn_data10.dbf',
    '/u1/TEST/testora/testdata/a_txn_data11.dbf',
    '/u1/TEST/testora/testdata/a_txn_data12.dbf',
    '/u1/TEST/testora/testdata/a_txn_data13.dbf',
    '/u1/TEST/testora/testdata/a_txn_data14.dbf',
    '/u1/TEST/testora/testdata/a_txn_data15.dbf',
    '/u1/TEST/testora/testdata/a_txn_data16.dbf',
    '/u1/TEST/testora/testdata/a_txn_data17.dbf',
    '/u1/TEST/testora/testdata/a_txn_data18.dbf',
    '/u1/TEST/testora/testdata/a_txn_data19.dbf',
    '/u1/TEST/testora/testdata/a_txn_data20.dbf',
    '/u1/TEST/testora/testdata/a_txn_data21.dbf',
    '/u1/TEST/testora/testdata/a_txn_data22.dbf',
    '/u1/TEST/testora/testdata/a_txn_data23.dbf',
    '/u1/TEST/testora/testdata/a_txn_data24.dbf',
    '/u1/TEST/testora/testdata/a_txn_data25.dbf',
    '/u1/TEST/testora/testdata/a_txn_data26.dbf',
    '/u1/TEST/testora/testdata/a_txn_data28.dbf',
    '/u1/TEST/testora/testdata/a_txn_data29.dbf',
    '/u1/TEST/testora/testdata/a_txn_data30.dbf',
    '/u1/TEST/testora/testdata/a_txn_data31.dbf',
    '/u1/TEST/testora/testdata/a_txn_data32.dbf',
    '/u1/TEST/testora/testdata/a_txn_data33.dbf',
    '/u1/TEST/testora/testdata/a_txn_data34.dbf',
    '/u1/TEST/testora/testdata/a_txn_data35.dbf',
    '/u1/TEST/testora/testdata/a_txn_data36.dbf',
    '/u1/TEST/testora/testdata/a_txn_data37.dbf',
    '/u1/TEST/testora/testdata/a_txn_data38.dbf',
    '/u1/TEST/testora/testdata/a_txn_data39.dbf',
    '/u1/TEST/testora/testdata/a_txn_data40.dbf',
    '/u1/TEST/testora/testdata/a_txn_data41.dbf',
    '/u1/TEST/testora/testdata/a_txn_data42.dbf',
    '/u1/TEST/testora/testdata/a_txn_data43.dbf',
    '/u1/TEST/testora/testdata/a_txn_data44.dbf',
    '/u1/TEST/testora/testdata/a_txn_data45.dbf',
    '/u1/TEST/testora/testdata/a_txn_data46.dbf',
    '/u1/TEST/testora/testdata/a_txn_data47.dbf',
    '/u1/TEST/testora/testdata/a_txn_data48.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind01.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind02.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind03.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind04.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind05.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind06.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind07.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind08.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind09.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind10.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind11.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind12.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind13.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind14.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind15.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind16.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind17.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind18.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind19.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind20.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind21.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind22.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind23.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind24.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind25.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind26.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind27.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind28.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind29.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind30.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind31.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind32.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind33.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind34.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind35.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind36.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind37.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind38.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind39.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind40.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind41.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind42.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind43.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind44.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind45.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind46.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind47.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind48.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind49.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind50.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind51.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind52.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind53.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind54.dbf',
    '/u1/TEST/testora/testdata/a_txn_ind55.dbf',
    '/u1/TEST/testora/testdata/ctxd01.dbf',
    '/u1/TEST/testora/testdata/odm.dbf',
    '/u1/TEST/testora/testdata/olap.dbf',
    '/u1/TEST/testora/testdata/owad01.dbf',
    '/u1/TEST/testora/testdata/portal01.dbf',
    '/u1/TEST/testora/testdata/statspack_01.dbf',
    '/u1/TEST/testora/testdata/tplcux_data_01.dbf',
    '/u1/TEST/testora/testdata/tplcux_idx_01.dbf',
    '/u1/TEST/testora/testdata/interim.dbf',
    '/u1/TEST/testora/testdata/system12.dbf',
    '/u1/TEST/testora/testdata/a_txn_data49.dbf',
    '/u1/TEST/testora/testdata/a_ref03.dbf',
    '/u1/TEST/testora/testdata/a_summ06.dbf',
    '/u1/TEST/testora/testdata/undo04.dbf',
    
    --set newname;
    set newname for datafile 1 to "/u1/TEST/testora/testdata/system01.dbf";
    set newname for datafile 2 to "/u1/TEST/testora/testdata/system02.dbf";
    set newname for datafile 3 to "/u1/TEST/testora/testdata/system03.dbf";
    set newname for datafile 4 to "/u1/TEST/testora/testdata/system04.dbf";
    set newname for datafile 5 to "/u1/TEST/testora/testdata/system05.dbf";
    set newname for datafile 6 to "/u1/TEST/testora/testdata/system06.dbf";
    set newname for datafile 7 to "/u1/TEST/testora/testdata/system07.dbf";
    set newname for datafile 8 to "/u1/TEST/testora/testdata/system08.dbf";
    set newname for datafile 9 to "/u1/TEST/testora/testdata/system09.dbf";
    set newname for datafile 10 to "/u1/TEST/testora/testdata/system10.dbf";
    set newname for datafile 11 to "/u1/TEST/testora/testdata/system11.dbf";
    set newname for datafile 12 to "/u1/TEST/testora/testdata/sysaux01.dbf";
    set newname for datafile 13 to "/u1/TEST/testora/testdata/undo01.dbf";
    set newname for datafile 14 to "/u1/TEST/testora/testdata/undo02.dbf";
    set newname for datafile 15 to "/u1/TEST/testora/testdata/undo03.dbf";
    set newname for datafile 16 to "/u1/TEST/testora/testdata/a_archive01.dbf";
    set newname for datafile 17 to "/u1/TEST/testora/testdata/a_archive02.dbf";
    set newname for datafile 18 to "/u1/TEST/testora/testdata/a_archive03.dbf";
    set newname for datafile 19 to "/u1/TEST/testora/testdata/a_archive04.dbf";
    set newname for datafile 20 to "/u1/TEST/testora/testdata/a_archive05.dbf";
    set newname for datafile 21 to "/u1/TEST/testora/testdata/a_int01.dbf";
    set newname for datafile 22 to "/u1/TEST/testora/testdata/a_int02.dbf";
    set newname for datafile 23 to "/u1/TEST/testora/testdata/a_int03.dbf";
    set newname for datafile 24 to "/u1/TEST/testora/testdata/a_int04.dbf";
    set newname for datafile 25 to "/u1/TEST/testora/testdata/a_int05.dbf";
    set newname for datafile 26 to "/u1/TEST/testora/testdata/a_int06.dbf";
    set newname for datafile 27 to "/u1/TEST/testora/testdata/a_int07.dbf";
    set newname for datafile 28 to "/u1/TEST/testora/testdata/a_int08.dbf";
    set newname for datafile 29 to "/u1/TEST/testora/testdata/a_media01.dbf";
    set newname for datafile 30 to "/u1/TEST/testora/testdata/a_media02.dbf";
    set newname for datafile 31 to "/u1/TEST/testora/testdata/a_media03.dbf";
    set newname for datafile 32 to "/u1/TEST/testora/testdata/a_media04.dbf";
    set newname for datafile 33 to "/u1/TEST/testora/testdata/a_media05.dbf";
    set newname for datafile 34 to "/u1/TEST/testora/testdata/a_media06.dbf";
    set newname for datafile 35 to "/u1/TEST/testora/testdata/a_media07.dbf";
    set newname for datafile 36 to "/u1/TEST/testora/testdata/a_media08.dbf";
    set newname for datafile 37 to "/u1/TEST/testora/testdata/a_media09.dbf";
    set newname for datafile 38 to "/u1/TEST/testora/testdata/a_media10.dbf";
    set newname for datafile 39 to "/u1/TEST/testora/testdata/a_nolog01.dbf";
    set newname for datafile 40 to "/u1/TEST/testora/testdata/a_queue01.dbf";
    set newname for datafile 41 to "/u1/TEST/testora/testdata/a_queue02.dbf";
    set newname for datafile 42 to "/u1/TEST/testora/testdata/a_queue03.dbf";
    set newname for datafile 43 to "/u1/TEST/testora/testdata/a_queue04.dbf";
    set newname for datafile 44 to "/u1/TEST/testora/testdata/a_queue05.dbf";
    set newname for datafile 45 to "/u1/TEST/testora/testdata/a_queue06.dbf";
    set newname for datafile 46 to "/u1/TEST/testora/testdata/a_queue07.dbf";
    set newname for datafile 47 to "/u1/TEST/testora/testdata/a_queue08.dbf";
    set newname for datafile 48 to "/u1/TEST/testora/testdata/a_queue09.dbf";
    set newname for datafile 49 to "/u1/TEST/testora/testdata/a_queue10.dbf";
    set newname for datafile 50 to "/u1/TEST/testora/testdata/a_queue11.dbf";
    set newname for datafile 51 to "/u1/TEST/testora/testdata/a_queue12.dbf";
    set newname for datafile 52 to "/u1/TEST/testora/testdata/a_queue13.dbf";
    set newname for datafile 53 to "/u1/TEST/testora/testdata/a_queue14.dbf";
    set newname for datafile 54 to "/u1/TEST/testora/testdata/a_queue15.dbf";
    set newname for datafile 55 to "/u1/TEST/testora/testdata/a_queue16.dbf";
    set newname for datafile 56 to "/u1/TEST/testora/testdata/a_queue17.dbf";
    set newname for datafile 57 to "/u1/TEST/testora/testdata/a_queue18.dbf";
    set newname for datafile 58 to "/u1/TEST/testora/testdata/a_queue19.dbf";
    set newname for datafile 59 to "/u1/TEST/testora/testdata/a_queue20.dbf";
    set newname for datafile 60 to "/u1/TEST/testora/testdata/a_queue21.dbf";
    set newname for datafile 61 to "/u1/TEST/testora/testdata/a_queue22.dbf";
    set newname for datafile 62 to "/u1/TEST/testora/testdata/a_queue23.dbf";
    set newname for datafile 63 to "/u1/TEST/testora/testdata/a_queue24.dbf";
    set newname for datafile 64 to "/u1/TEST/testora/testdata/a_queue25.dbf";
    set newname for datafile 65 to "/u1/TEST/testora/testdata/a_ref01.dbf";
    set newname for datafile 66 to "/u1/TEST/testora/testdata/a_ref02.dbf";
    set newname for datafile 67 to "/u1/TEST/testora/testdata/a_summ01.dbf";
    set newname for datafile 68 to "/u1/TEST/testora/testdata/a_summ02.dbf";
    set newname for datafile 69 to "/u1/TEST/testora/testdata/a_summ03.dbf";
    set newname for datafile 70 to "/u1/TEST/testora/testdata/a_summ04.dbf";
    set newname for datafile 71 to "/u1/TEST/testora/testdata/a_summ05.dbf";
    set newname for datafile 72 to "/u1/TEST/testora/testdata/a_tools01.dbf";
    set newname for datafile 73 to "/u1/TEST/testora/testdata/a_txn_data01.dbf";
    set newname for datafile 74 to "/u1/TEST/testora/testdata/a_txn_data02.dbf";
    set newname for datafile 75 to "/u1/TEST/testora/testdata/a_txn_data03.dbf";
    set newname for datafile 76 to "/u1/TEST/testora/testdata/a_txn_data04.dbf";
    set newname for datafile 77 to "/u1/TEST/testora/testdata/a_txn_data05.dbf";
    set newname for datafile 78 to "/u1/TEST/testora/testdata/a_txn_data06.dbf";
    set newname for datafile 79 to "/u1/TEST/testora/testdata/a_txn_data07.dbf";
    set newname for datafile 80 to "/u1/TEST/testora/testdata/a_txn_data08.dbf";
    set newname for datafile 81 to "/u1/TEST/testora/testdata/a_txn_data09.dbf";
    set newname for datafile 82 to "/u1/TEST/testora/testdata/a_txn_data10.dbf";
    set newname for datafile 83 to "/u1/TEST/testora/testdata/a_txn_data11.dbf";
    set newname for datafile 84 to "/u1/TEST/testora/testdata/a_txn_data12.dbf";
    set newname for datafile 85 to "/u1/TEST/testora/testdata/a_txn_data13.dbf";
    set newname for datafile 86 to "/u1/TEST/testora/testdata/a_txn_data14.dbf";
    set newname for datafile 87 to "/u1/TEST/testora/testdata/a_txn_data15.dbf";
    set newname for datafile 88 to "/u1/TEST/testora/testdata/a_txn_data16.dbf";
    set newname for datafile 89 to "/u1/TEST/testora/testdata/a_txn_data17.dbf";
    set newname for datafile 90 to "/u1/TEST/testora/testdata/a_txn_data18.dbf";
    set newname for datafile 91 to "/u1/TEST/testora/testdata/a_txn_data19.dbf";
    set newname for datafile 92 to "/u1/TEST/testora/testdata/a_txn_data20.dbf";
    set newname for datafile 93 to "/u1/TEST/testora/testdata/a_txn_data21.dbf";
    set newname for datafile 94 to "/u1/TEST/testora/testdata/a_txn_data22.dbf";
    set newname for datafile 95 to "/u1/TEST/testora/testdata/a_txn_data23.dbf";
    set newname for datafile 96 to "/u1/TEST/testora/testdata/a_txn_data24.dbf";
    set newname for datafile 97 to "/u1/TEST/testora/testdata/a_txn_data25.dbf";
    set newname for datafile 98 to "/u1/TEST/testora/testdata/a_txn_data26.dbf";
    set newname for datafile 99 to "/u1/TEST/testora/testdata/a_txn_data28.dbf";
    set newname for datafile 100 to "/u1/TEST/testora/testdata/a_txn_data29.dbf";
    set newname for datafile 101 to "/u1/TEST/testora/testdata/a_txn_data30.dbf";
    set newname for datafile 102 to "/u1/TEST/testora/testdata/a_txn_data31.dbf";
    set newname for datafile 103 to "/u1/TEST/testora/testdata/a_txn_data32.dbf";
    set newname for datafile 104 to "/u1/TEST/testora/testdata/a_txn_data33.dbf";
    set newname for datafile 105 to "/u1/TEST/testora/testdata/a_txn_data34.dbf";
    set newname for datafile 106 to "/u1/TEST/testora/testdata/a_txn_data35.dbf";
    set newname for datafile 107 to "/u1/TEST/testora/testdata/a_txn_data36.dbf";
    set newname for datafile 108 to "/u1/TEST/testora/testdata/a_txn_data37.dbf";
    set newname for datafile 109 to "/u1/TEST/testora/testdata/a_txn_data38.dbf";
    set newname for datafile 110 to "/u1/TEST/testora/testdata/a_txn_data39.dbf";
    set newname for datafile 111 to "/u1/TEST/testora/testdata/a_txn_data40.dbf";
    set newname for datafile 112 to "/u1/TEST/testora/testdata/a_txn_data41.dbf";
    set newname for datafile 113 to "/u1/TEST/testora/testdata/a_txn_data42.dbf";
    set newname for datafile 114 to "/u1/TEST/testora/testdata/a_txn_data43.dbf";
    set newname for datafile 115 to "/u1/TEST/testora/testdata/a_txn_data44.dbf";
    set newname for datafile 116 to "/u1/TEST/testora/testdata/a_txn_data45.dbf";
    set newname for datafile 117 to "/u1/TEST/testora/testdata/a_txn_data46.dbf";
    set newname for datafile 118 to "/u1/TEST/testora/testdata/a_txn_data47.dbf";
    set newname for datafile 119 to "/u1/TEST/testora/testdata/a_txn_data48.dbf";
    set newname for datafile 120 to "/u1/TEST/testora/testdata/a_txn_ind01.dbf";
    set newname for datafile 121 to "/u1/TEST/testora/testdata/a_txn_ind02.dbf";
    set newname for datafile 122 to "/u1/TEST/testora/testdata/a_txn_ind03.dbf";
    set newname for datafile 123 to "/u1/TEST/testora/testdata/a_txn_ind04.dbf";
    set newname for datafile 124 to "/u1/TEST/testora/testdata/a_txn_ind05.dbf";
    set newname for datafile 125 to "/u1/TEST/testora/testdata/a_txn_ind06.dbf";
    set newname for datafile 126 to "/u1/TEST/testora/testdata/a_txn_ind07.dbf";
    set newname for datafile 127 to "/u1/TEST/testora/testdata/a_txn_ind08.dbf";
    set newname for datafile 128 to "/u1/TEST/testora/testdata/a_txn_ind09.dbf";
    set newname for datafile 129 to "/u1/TEST/testora/testdata/a_txn_ind10.dbf";
    set newname for datafile 130 to "/u1/TEST/testora/testdata/a_txn_ind11.dbf";
    set newname for datafile 131 to "/u1/TEST/testora/testdata/a_txn_ind12.dbf";
    set newname for datafile 132 to "/u1/TEST/testora/testdata/a_txn_ind13.dbf";
    set newname for datafile 133 to "/u1/TEST/testora/testdata/a_txn_ind14.dbf";
    set newname for datafile 134 to "/u1/TEST/testora/testdata/a_txn_ind15.dbf";
    set newname for datafile 135 to "/u1/TEST/testora/testdata/a_txn_ind16.dbf";
    set newname for datafile 136 to "/u1/TEST/testora/testdata/a_txn_ind17.dbf";
    set newname for datafile 137 to "/u1/TEST/testora/testdata/a_txn_ind18.dbf";
    set newname for datafile 138 to "/u1/TEST/testora/testdata/a_txn_ind19.dbf";
    set newname for datafile 139 to "/u1/TEST/testora/testdata/a_txn_ind20.dbf";
    set newname for datafile 140 to "/u1/TEST/testora/testdata/a_txn_ind21.dbf";
    set newname for datafile 141 to "/u1/TEST/testora/testdata/a_txn_ind22.dbf";
    set newname for datafile 142 to "/u1/TEST/testora/testdata/a_txn_ind23.dbf";
    set newname for datafile 143 to "/u1/TEST/testora/testdata/a_txn_ind24.dbf";
    set newname for datafile 144 to "/u1/TEST/testora/testdata/a_txn_ind25.dbf";
    set newname for datafile 145 to "/u1/TEST/testora/testdata/a_txn_ind26.dbf";
    set newname for datafile 146 to "/u1/TEST/testora/testdata/a_txn_ind27.dbf";
    set newname for datafile 147 to "/u1/TEST/testora/testdata/a_txn_ind28.dbf";
    set newname for datafile 148 to "/u1/TEST/testora/testdata/a_txn_ind29.dbf";
    set newname for datafile 149 to "/u1/TEST/testora/testdata/a_txn_ind30.dbf";
    set newname for datafile 150 to "/u1/TEST/testora/testdata/a_txn_ind31.dbf";
    set newname for datafile 151 to "/u1/TEST/testora/testdata/a_txn_ind32.dbf";
    set newname for datafile 152 to "/u1/TEST/testora/testdata/a_txn_ind33.dbf";
    set newname for datafile 153 to "/u1/TEST/testora/testdata/a_txn_ind34.dbf";
    set newname for datafile 154 to "/u1/TEST/testora/testdata/a_txn_ind35.dbf";
    set newname for datafile 155 to "/u1/TEST/testora/testdata/a_txn_ind36.dbf";
    set newname for datafile 156 to "/u1/TEST/testora/testdata/a_txn_ind37.dbf";
    set newname for datafile 157 to "/u1/TEST/testora/testdata/a_txn_ind38.dbf";
    set newname for datafile 158 to "/u1/TEST/testora/testdata/a_txn_ind39.dbf";
    set newname for datafile 159 to "/u1/TEST/testora/testdata/a_txn_ind40.dbf";
    set newname for datafile 160 to "/u1/TEST/testora/testdata/a_txn_ind41.dbf";
    set newname for datafile 161 to "/u1/TEST/testora/testdata/a_txn_ind42.dbf";
    set newname for datafile 162 to "/u1/TEST/testora/testdata/a_txn_ind43.dbf";
    set newname for datafile 163 to "/u1/TEST/testora/testdata/a_txn_ind44.dbf";
    set newname for datafile 164 to "/u1/TEST/testora/testdata/a_txn_ind45.dbf";
    set newname for datafile 165 to "/u1/TEST/testora/testdata/a_txn_ind46.dbf";
    set newname for datafile 166 to "/u1/TEST/testora/testdata/a_txn_ind47.dbf";
    set newname for datafile 167 to "/u1/TEST/testora/testdata/a_txn_ind48.dbf";
    set newname for datafile 168 to "/u1/TEST/testora/testdata/a_txn_ind49.dbf";
    set newname for datafile 169 to "/u1/TEST/testora/testdata/a_txn_ind50.dbf";
    set newname for datafile 170 to "/u1/TEST/testora/testdata/a_txn_ind51.dbf";
    set newname for datafile 171 to "/u1/TEST/testora/testdata/a_txn_ind52.dbf";
    set newname for datafile 172 to "/u1/TEST/testora/testdata/a_txn_ind53.dbf";
    set newname for datafile 173 to "/u1/TEST/testora/testdata/a_txn_ind54.dbf";
    set newname for datafile 174 to "/u1/TEST/testora/testdata/a_txn_ind55.dbf";
    set newname for datafile 175 to "/u1/TEST/testora/testdata/ctxd01.dbf";
    set newname for datafile 176 to "/u1/TEST/testora/testdata/odm.dbf";
    set newname for datafile 177 to "/u1/TEST/testora/testdata/olap.dbf";
    set newname for datafile 178 to "/u1/TEST/testora/testdata/owad01.dbf";
    set newname for datafile 179 to "/u1/TEST/testora/testdata/portal01.dbf";
    set newname for datafile 180 to "/u1/TEST/testora/testdata/statspack_01.dbf";
    set newname for datafile 181 to "/u1/TEST/testora/testdata/tplcux_data_01.dbf";
    set newname for datafile 182 to "/u1/TEST/testora/testdata/tplcux_idx_01.dbf";
    set newname for datafile 183 to "/u1/TEST/testora/testdata/interim.dbf";
    set newname for datafile 184 to "/u1/TEST/testora/testdata/system12.dbf";
    set newname for datafile 185 to "/u1/TEST/testora/testdata/a_txn_data49.dbf";
    set newname for datafile 186 to "/u1/TEST/testora/testdata/a_ref03.dbf";
    set newname for datafile 187 to "/u1/TEST/testora/testdata/a_summ06.dbf";
    set newname for datafile 188 to "/u1/TEST/testora/testdata/undo04.dbf";

run recover rman script
    
    RMAN> run {
    set newname for datafile 1 to "/u1/TEST/testora/testdata/system01.dbf";
    set newname for datafile 2 to "/u1/TEST/testora/testdata/system02.dbf";
    set newname for datafile 3 to "/u1/TEST/testora/testdata/system03.dbf";
    set newname for datafile 4 to "/u1/TEST/testora/testdata/system04.dbf";
    set newname for datafile 5 to "/u1/TEST/testora/testdata/system05.dbf";
    set newname for datafile 6 to "/u1/TEST/testora/testdata/system06.dbf";
    set newname for datafile 7 to "/u1/TEST/testora/testdata/system07.dbf";
    set newname for datafile 8 to "/u1/TEST/testora/testdata/system08.dbf";
    set newname for datafile 9 to "/u1/TEST/testora/testdata/system09.dbf";
    set newname for datafile 10 to "/u1/TEST/testora/testdata/system10.dbf";
    set newname for datafile 11 to "/u1/TEST/testora/testdata/system11.dbf";
    set newname for datafile 12 to "/u1/TEST/testora/testdata/sysaux01.dbf";
    set newname for datafile 13 to "/u1/TEST/testora/testdata/undo01.dbf";
    set newname for datafile 14 to "/u1/TEST/testora/testdata/undo02.dbf";
    set newname for datafile 15 to "/u1/TEST/testora/testdata/undo03.dbf";
    set newname for datafile 16 to "/u1/TEST/testora/testdata/a_archive01.dbf";
    set newname for datafile 17 to "/u1/TEST/testora/testdata/a_archive02.dbf";
    set newname for datafile 18 to "/u1/TEST/testora/testdata/a_archive03.dbf";
    set newname for datafile 19 to "/u1/TEST/testora/testdata/a_archive04.dbf";
    set newname for datafile 20 to "/u1/TEST/testora/testdata/a_archive05.dbf";
    set newname for datafile 21 to "/u1/TEST/testora/testdata/a_int01.dbf";
    set newname for datafile 22 to "/u1/TEST/testora/testdata/a_int02.dbf";
    set newname for datafile 23 to "/u1/TEST/testora/testdata/a_int03.dbf";
    set newname for datafile 24 to "/u1/TEST/testora/testdata/a_int04.dbf";
    set newname for datafile 25 to "/u1/TEST/testora/testdata/a_int05.dbf";
    set newname for datafile 26 to "/u1/TEST/testora/testdata/a_int06.dbf";
    set newname for datafile 27 to "/u1/TEST/testora/testdata/a_int07.dbf";
    set newname for datafile 28 to "/u1/TEST/testora/testdata/a_int08.dbf";
    set newname for datafile 29 to "/u1/TEST/testora/testdata/a_media01.dbf";
    set newname for datafile 30 to "/u1/TEST/testora/testdata/a_media02.dbf";
    set newname for datafile 31 to "/u1/TEST/testora/testdata/a_media03.dbf";
    set newname for datafile 32 to "/u1/TEST/testora/testdata/a_media04.dbf";
    set newname for datafile 33 to "/u1/TEST/testora/testdata/a_media05.dbf";
    set newname for datafile 34 to "/u1/TEST/testora/testdata/a_media06.dbf";
    set newname for datafile 35 to "/u1/TEST/testora/testdata/a_media07.dbf";
    set newname for datafile 36 to "/u1/TEST/testora/testdata/a_media08.dbf";
    set newname for datafile 37 to "/u1/TEST/testora/testdata/a_media09.dbf";
    set newname for datafile 38 to "/u1/TEST/testora/testdata/a_media10.dbf";
    set newname for datafile 39 to "/u1/TEST/testora/testdata/a_nolog01.dbf";
    set newname for datafile 40 to "/u1/TEST/testora/testdata/a_queue01.dbf";
    set newname for datafile 41 to "/u1/TEST/testora/testdata/a_queue02.dbf";
    set newname for datafile 42 to "/u1/TEST/testora/testdata/a_queue03.dbf";
    set newname for datafile 43 to "/u1/TEST/testora/testdata/a_queue04.dbf";
    set newname for datafile 44 to "/u1/TEST/testora/testdata/a_queue05.dbf";
    set newname for datafile 45 to "/u1/TEST/testora/testdata/a_queue06.dbf";
    set newname for datafile 46 to "/u1/TEST/testora/testdata/a_queue07.dbf";
    set newname for datafile 47 to "/u1/TEST/testora/testdata/a_queue08.dbf";
    set newname for datafile 48 to "/u1/TEST/testora/testdata/a_queue09.dbf";
    set newname for datafile 49 to "/u1/TEST/testora/testdata/a_queue10.dbf";
    set newname for datafile 50 to "/u1/TEST/testora/testdata/a_queue11.dbf";
    set newname for datafile 51 to "/u1/TEST/testora/testdata/a_queue12.dbf";
    set newname for datafile 52 to "/u1/TEST/testora/testdata/a_queue13.dbf";
    set newname for datafile 53 to "/u1/TEST/testora/testdata/a_queue14.dbf";
    set newname for datafile 54 to "/u1/TEST/testora/testdata/a_queue15.dbf";
    set newname for datafile 55 to "/u1/TEST/testora/testdata/a_queue16.dbf";
    set newname for datafile 56 to "/u1/TEST/testora/testdata/a_queue17.dbf";
    set newname for datafile 57 to "/u1/TEST/testora/testdata/a_queue18.dbf";
    set newname for datafile 58 to "/u1/TEST/testora/testdata/a_queue19.dbf";
    set newname for datafile 59 to "/u1/TEST/testora/testdata/a_queue20.dbf";
    set newname for datafile 60 to "/u1/TEST/testora/testdata/a_queue21.dbf";
    set newname for datafile 61 to "/u1/TEST/testora/testdata/a_queue22.dbf";
    set newname for datafile 62 to "/u1/TEST/testora/testdata/a_queue23.dbf";
    set newname for datafile 63 to "/u1/TEST/testora/testdata/a_queue24.dbf";
    set newname for datafile 64 to "/u1/TEST/testora/testdata/a_queue25.dbf";
    set newname for datafile 65 to "/u1/TEST/testora/testdata/a_ref01.dbf";
    set newname for datafile 66 to "/u1/TEST/testora/testdata/a_ref02.dbf";
    set newname for datafile 67 to "/u1/TEST/testora/testdata/a_summ01.dbf";
    set newname for datafile 68 to "/u1/TEST/testora/testdata/a_summ02.dbf";
    set newname for datafile 69 to "/u1/TEST/testora/testdata/a_summ03.dbf";
    set newname for datafile 70 to "/u1/TEST/testora/testdata/a_summ04.dbf";
    set newname for datafile 71 to "/u1/TEST/testora/testdata/a_summ05.dbf";
    set newname for datafile 72 to "/u1/TEST/testora/testdata/a_tools01.dbf";
    set newname for datafile 73 to "/u1/TEST/testora/testdata/a_txn_data01.dbf";
    set newname for datafile 74 to "/u1/TEST/testora/testdata/a_txn_data02.dbf";
    set newname for datafile 75 to "/u1/TEST/testora/testdata/a_txn_data03.dbf";
    set newname for datafile 76 to "/u1/TEST/testora/testdata/a_txn_data04.dbf";
    set newname for datafile 77 to "/u1/TEST/testora/testdata/a_txn_data05.dbf";
    set newname for datafile 78 to "/u1/TEST/testora/testdata/a_txn_data06.dbf";
    set newname for datafile 79 to "/u1/TEST/testora/testdata/a_txn_data07.dbf";
    set newname for datafile 80 to "/u1/TEST/testora/testdata/a_txn_data08.dbf";
    set newname for datafile 81 to "/u1/TEST/testora/testdata/a_txn_data09.dbf";
    set newname for datafile 82 to "/u1/TEST/testora/testdata/a_txn_data10.dbf";
    set newname for datafile 83 to "/u1/TEST/testora/testdata/a_txn_data11.dbf";
    set newname for datafile 84 to "/u1/TEST/testora/testdata/a_txn_data12.dbf";
    set newname for datafile 85 to "/u1/TEST/testora/testdata/a_txn_data13.dbf";
    set newname for datafile 86 to "/u1/TEST/testora/testdata/a_txn_data14.dbf";
    set newname for datafile 87 to "/u1/TEST/testora/testdata/a_txn_data15.dbf";
    set newname for datafile 88 to "/u1/TEST/testora/testdata/a_txn_data16.dbf";
    set newname for datafile 89 to "/u1/TEST/testora/testdata/a_txn_data17.dbf";
    set newname for datafile 90 to "/u1/TEST/testora/testdata/a_txn_data18.dbf";
    set newname for datafile 91 to "/u1/TEST/testora/testdata/a_txn_data19.dbf";
    set newname for datafile 92 to "/u1/TEST/testora/testdata/a_txn_data20.dbf";
    set newname for datafile 93 to "/u1/TEST/testora/testdata/a_txn_data21.dbf";
    set newname for datafile 94 to "/u1/TEST/testora/testdata/a_txn_data22.dbf";
    set newname for datafile 95 to "/u1/TEST/testora/testdata/a_txn_data23.dbf";
    set newname for datafile 96 to "/u1/TEST/testora/testdata/a_txn_data24.dbf";
    set newname for datafile 97 to "/u1/TEST/testora/testdata/a_txn_data25.dbf";
    set newname for datafile 98 to "/u1/TEST/testora/testdata/a_txn_data26.dbf";
    set newname for datafile 99 to "/u1/TEST/testora/testdata/a_txn_data28.dbf";
    set newname for datafile 100 to "/u1/TEST/testora/testdata/a_txn_data29.dbf";
    set newname for datafile 101 to "/u1/TEST/testora/testdata/a_txn_data30.dbf";
    set newname for datafile 102 to "/u1/TEST/testora/testdata/a_txn_data31.dbf";
    set newname for datafile 103 to "/u1/TEST/testora/testdata/a_txn_data32.dbf";
    set newname for datafile 104 to "/u1/TEST/testora/testdata/a_txn_data33.dbf";
    set newname for datafile 105 to "/u1/TEST/testora/testdata/a_txn_data34.dbf";
    set newname for datafile 106 to "/u1/TEST/testora/testdata/a_txn_data35.dbf";
    set newname for datafile 107 to "/u1/TEST/testora/testdata/a_txn_data36.dbf";
    set newname for datafile 108 to "/u1/TEST/testora/testdata/a_txn_data37.dbf";
    set newname for datafile 109 to "/u1/TEST/testora/testdata/a_txn_data38.dbf";
    set newname for datafile 110 to "/u1/TEST/testora/testdata/a_txn_data39.dbf";
    set newname for datafile 111 to "/u1/TEST/testora/testdata/a_txn_data40.dbf";
    set newname for datafile 112 to "/u1/TEST/testora/testdata/a_txn_data41.dbf";
    set newname for datafile 113 to "/u1/TEST/testora/testdata/a_txn_data42.dbf";
    set newname for datafile 114 to "/u1/TEST/testora/testdata/a_txn_data43.dbf";
    set newname for datafile 115 to "/u1/TEST/testora/testdata/a_txn_data44.dbf";
    set newname for datafile 116 to "/u1/TEST/testora/testdata/a_txn_data45.dbf";
    set newname for datafile 117 to "/u1/TEST/testora/testdata/a_txn_data46.dbf";
    set newname for datafile 118 to "/u1/TEST/testora/testdata/a_txn_data47.dbf";
    set newname for datafile 119 to "/u1/TEST/testora/testdata/a_txn_data48.dbf";
    set newname for datafile 120 to "/u1/TEST/testora/testdata/a_txn_ind01.dbf";
    set newname for datafile 121 to "/u1/TEST/testora/testdata/a_txn_ind02.dbf";
    set newname for datafile 122 to "/u1/TEST/testora/testdata/a_txn_ind03.dbf";
    set newname for datafile 123 to "/u1/TEST/testora/testdata/a_txn_ind04.dbf";
    set newname for datafile 124 to "/u1/TEST/testora/testdata/a_txn_ind05.dbf";
    set newname for datafile 125 to "/u1/TEST/testora/testdata/a_txn_ind06.dbf";
    set newname for datafile 126 to "/u1/TEST/testora/testdata/a_txn_ind07.dbf";
    set newname for datafile 127 to "/u1/TEST/testora/testdata/a_txn_ind08.dbf";
    set newname for datafile 128 to "/u1/TEST/testora/testdata/a_txn_ind09.dbf";
    set newname for datafile 129 to "/u1/TEST/testora/testdata/a_txn_ind10.dbf";
    set newname for datafile 130 to "/u1/TEST/testora/testdata/a_txn_ind11.dbf";
    set newname for datafile 131 to "/u1/TEST/testora/testdata/a_txn_ind12.dbf";
    set newname for datafile 132 to "/u1/TEST/testora/testdata/a_txn_ind13.dbf";
    set newname for datafile 133 to "/u1/TEST/testora/testdata/a_txn_ind14.dbf";
    set newname for datafile 134 to "/u1/TEST/testora/testdata/a_txn_ind15.dbf";
    set newname for datafile 135 to "/u1/TEST/testora/testdata/a_txn_ind16.dbf";
    set newname for datafile 136 to "/u1/TEST/testora/testdata/a_txn_ind17.dbf";
    set newname for datafile 137 to "/u1/TEST/testora/testdata/a_txn_ind18.dbf";
    set newname for datafile 138 to "/u1/TEST/testora/testdata/a_txn_ind19.dbf";
    set newname for datafile 139 to "/u1/TEST/testora/testdata/a_txn_ind20.dbf";
    set newname for datafile 140 to "/u1/TEST/testora/testdata/a_txn_ind21.dbf";
    set newname for datafile 141 to "/u1/TEST/testora/testdata/a_txn_ind22.dbf";
    set newname for datafile 142 to "/u1/TEST/testora/testdata/a_txn_ind23.dbf";
    set newname for datafile 143 to "/u1/TEST/testora/testdata/a_txn_ind24.dbf";
    set newname for datafile 144 to "/u1/TEST/testora/testdata/a_txn_ind25.dbf";
    set newname for datafile 145 to "/u1/TEST/testora/testdata/a_txn_ind26.dbf";
    set newname for datafile 146 to "/u1/TEST/testora/testdata/a_txn_ind27.dbf";
    set newname for datafile 147 to "/u1/TEST/testora/testdata/a_txn_ind28.dbf";
    set newname for datafile 148 to "/u1/TEST/testora/testdata/a_txn_ind29.dbf";
    set newname for datafile 149 to "/u1/TEST/testora/testdata/a_txn_ind30.dbf";
    set newname for datafile 150 to "/u1/TEST/testora/testdata/a_txn_ind31.dbf";
    set newname for datafile 151 to "/u1/TEST/testora/testdata/a_txn_ind32.dbf";
    set newname for datafile 152 to "/u1/TEST/testora/testdata/a_txn_ind33.dbf";
    set newname for datafile 153 to "/u1/TEST/testora/testdata/a_txn_ind34.dbf";
    set newname for datafile 154 to "/u1/TEST/testora/testdata/a_txn_ind35.dbf";
    set newname for datafile 155 to "/u1/TEST/testora/testdata/a_txn_ind36.dbf";
    set newname for datafile 156 to "/u1/TEST/testora/testdata/a_txn_ind37.dbf";
    set newname for datafile 157 to "/u1/TEST/testora/testdata/a_txn_ind38.dbf";
    set newname for datafile 158 to "/u1/TEST/testora/testdata/a_txn_ind39.dbf";
    set newname for datafile 159 to "/u1/TEST/testora/testdata/a_txn_ind40.dbf";
    set newname for datafile 160 to "/u1/TEST/testora/testdata/a_txn_ind41.dbf";
    set newname for datafile 161 to "/u1/TEST/testora/testdata/a_txn_ind42.dbf";
    set newname for datafile 162 to "/u1/TEST/testora/testdata/a_txn_ind43.dbf";
    set newname for datafile 163 to "/u1/TEST/testora/testdata/a_txn_ind44.dbf";
    set newname for datafile 164 to "/u1/TEST/testora/testdata/a_txn_ind45.dbf";
    set newname for datafile 165 to "/u1/TEST/testora/testdata/a_txn_ind46.dbf";
    set newname for datafile 166 to "/u1/TEST/testora/testdata/a_txn_ind47.dbf";
    set newname for datafile 167 to "/u1/TEST/testora/testdata/a_txn_ind48.dbf";
    set newname for datafile 168 to "/u1/TEST/testora/testdata/a_txn_ind49.dbf";
    set newname for datafile 169 to "/u1/TEST/testora/testdata/a_txn_ind50.dbf";
    set newname for datafile 170 to "/u1/TEST/testora/testdata/a_txn_ind51.dbf";
    set newname for datafile 171 to "/u1/TEST/testora/testdata/a_txn_ind52.dbf";
    set newname for datafile 172 to "/u1/TEST/testora/testdata/a_txn_ind53.dbf";
    set newname for datafile 173 to "/u1/TEST/testora/testdata/a_txn_ind54.dbf";
    set newname for datafile 174 to "/u1/TEST/testora/testdata/a_txn_ind55.dbf";
    set newname for datafile 175 to "/u1/TEST/testora/testdata/ctxd01.dbf";
    set newname for datafile 176 to "/u1/TEST/testora/testdata/odm.dbf";
    set newname for datafile 177 to "/u1/TEST/testora/testdata/olap.dbf";
    set newname for datafile 178 to "/u1/TEST/testora/testdata/owad01.dbf";
    set newname for datafile 179 to "/u1/TEST/testora/testdata/portal01.dbf";
    set newname for datafile 180 to "/u1/TEST/testora/testdata/statspack_01.dbf";
    set newname for datafile 181 to "/u1/TEST/testora/testdata/tplcux_data_01.dbf";
    set newname for datafile 182 to "/u1/TEST/testora/testdata/tplcux_idx_01.dbf";
    set newname for datafile 183 to "/u1/TEST/testora/testdata/interim.dbf";
    set newname for datafile 184 to "/u1/TEST/testora/testdata/system12.dbf";
    set newname for datafile 185 to "/u1/TEST/testora/testdata/a_txn_data49.dbf";
    set newname for datafile 186 to "/u1/TEST/testora/testdata/a_ref03.dbf";
    set newname for datafile 187 to "/u1/TEST/testora/testdata/a_summ06.dbf";
    set newname for datafile 188 to "/u1/TEST/testora/testdata/undo04.dbf";
    restore database;
    switch datafile all;
    recover database;
    }
    
###power off failure （中间遇电路检修，短暂停电）
    
    [testora@target testora]$ du -sh testdata  
    205G	testdata
    
    [testora@target testora]$ ll -rst testdata/  
    total 214433112
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 14:54 olap.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 14:54 a_queue25.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 14:54 odm.dbf
     6144012 -rw-r-----. 1 testora dba  6291464192 Oct 17 14:55 undo04.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 14:55 interim.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 14:56 a_txn_ind32.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 14:56 a_int07.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 14:56 undo01.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 14:56 a_queue10.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 14:57 a_txn_ind41.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 14:57 a_txn_data41.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 14:57 a_txn_data01.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 14:57 a_queue11.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 14:58 a_txn_ind42.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 14:58 a_txn_data42.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 14:58 a_txn_data02.dbf
       40972 -rw-r-----. 1 testora dba    41951232 Oct 17 14:58 ctxd01.dbf
       10252 -rw-r-----. 1 testora dba    10493952 Oct 17 14:58 owad01.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 14:58 portal01.dbf
    23068684 -rw-r-----. 1 testora dba 23622328320 Oct 17 15:01 a_txn_data49.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:01 a_queue12.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:02 a_txn_ind43.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:03 a_txn_data43.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:03 a_txn_data03.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:03 a_queue13.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:03 a_txn_ind44.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:04 a_txn_data44.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:04 a_txn_data04.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:04 a_queue14.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:05 a_txn_ind45.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:05 a_txn_data45.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:05 a_txn_data05.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:05 a_queue15.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:06 a_txn_ind46.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:06 a_txn_data46.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:06 a_txn_data06.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:06 a_queue16.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:07 a_txn_ind47.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:08 a_txn_data47.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:08 a_txn_data07.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:08 a_queue17.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:08 a_txn_ind48.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:09 a_txn_data48.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:09 a_txn_data08.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:09 a_queue18.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:10 a_txn_ind49.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:10 a_txn_ind01.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:10 a_txn_data09.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:10 a_queue19.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:11 a_txn_ind50.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:11 a_txn_ind02.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:11 a_txn_data10.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:11 a_queue20.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:12 a_txn_ind51.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:12 a_txn_ind03.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:12 a_txn_data11.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:12 a_queue21.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:13 a_txn_ind52.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:13 a_txn_ind04.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:13 a_txn_data12.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:13 a_queue22.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:14 a_txn_ind53.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:14 a_txn_ind05.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:14 a_txn_data13.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:14 a_queue23.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:15 a_txn_ind54.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:15 a_txn_ind06.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:15 a_txn_data14.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:15 a_queue09.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:16 a_media03.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:16 a_media04.dbf
     5242892 -rw-r-----. 1 testora dba  5368717312 Oct 17 15:16 system12.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:16 a_queue24.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:17 a_txn_ind55.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:17 a_txn_ind07.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:17 a_txn_data15.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:17 a_queue01.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:18 a_txn_ind33.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:18 a_int08.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:18 undo02.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:18 a_queue02.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:18 a_summ01.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:18 a_txn_ind34.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:19 undo03.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:19 a_queue03.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:19 a_txn_ind35.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:20 a_int01.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:20 a_txn_data35.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:20 a_queue04.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:20 a_txn_ind36.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:21 a_int02.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:21 a_txn_data36.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:21 a_queue06.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:21 a_txn_ind38.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:22 a_int04.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:22 a_txn_data38.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:22 a_queue05.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:22 a_txn_ind37.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:23 a_int03.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:23 a_txn_data37.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:23 a_queue07.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:24 a_txn_ind39.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:24 a_int05.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:24 a_txn_data39.dbf
      102412 -rw-r-----. 1 testora dba   104865792 Oct 17 15:24 a_queue08.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:25 a_txn_ind40.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:25 a_int06.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:25 a_txn_data40.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:25 a_media05.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:26 a_txn_ind26.dbf
     1843212 -rw-r-----. 1 testora dba  1887444992 Oct 17 15:26 a_txn_ind19.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:26 a_txn_data29.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:26 a_media06.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:27 a_txn_ind27.dbf
     1843212 -rw-r-----. 1 testora dba  1887444992 Oct 17 15:27 a_txn_ind20.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:27 a_txn_data30.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:27 a_media07.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:28 a_txn_ind28.dbf
     1843212 -rw-r-----. 1 testora dba  1887444992 Oct 17 15:28 a_txn_ind21.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:28 a_txn_data31.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:28 a_media08.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:29 a_txn_ind29.dbf
     1843212 -rw-r-----. 1 testora dba  1887444992 Oct 17 15:29 a_txn_ind22.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:29 a_txn_data32.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:29 a_media09.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:30 a_txn_ind30.dbf
     1843212 -rw-r-----. 1 testora dba  1887444992 Oct 17 15:30 a_txn_ind23.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:30 a_txn_data33.dbf
      204812 -rw-r-----. 1 testora dba   209723392 Oct 17 15:30 a_media10.dbf
     1536012 -rw-r-----. 1 testora dba  1572872192 Oct 17 15:31 a_txn_ind31.dbf
     1843212 -rw-r-----. 1 testora dba  1887444992 Oct 17 15:31 a_txn_ind24.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:31 a_txn_data34.dbf
      307212 -rw-r-----. 1 testora dba   314580992 Oct 17 15:32 a_summ02.dbf
     1024012 -rw-r-----. 1 testora dba  1048584192 Oct 17 15:32 system02.dbf
     2048012 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:32 a_txn_ind09.dbf
     2097164 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:32 a_txn_data17.dbf
       18804 -rw-r-----. 1 testora dba    19251200 Oct 17 15:33 control01.ctl
       18804 -rw-r-----. 1 testora dba    19251200 Oct 17 15:33 control02.ctl
      233476 -rw-r-----. 1 testora dba  2097160192 Oct 17 15:33 a_txn_ind08.dbf
      233476 -rw-r-----. 1 testora dba  1048584192 Oct 17 15:33 system01.dbf
      235524 -rw-r-----. 1 testora dba   314580992 Oct 17 15:33 a_archive05.dbf
      234500 -rw-r-----. 1 testora dba  2147491840 Oct 17 15:33 a_txn_data16.dbf
    
    [testora@target testora]$ sqlplus  /nolog
    SQL*Plus: Release 11.2.0.3.0 Production on Thu Oct 17 17:05:35 2013

    Copyright (c) 1982, 2011, Oracle.  All rights reserved.

    SQL> conn /as sysdba
    Connected to an idle instance.
    SQL> startup mount;
    ORACLE instance started.

    Total System Global Area 5.1310E+10 bytes
    Fixed Size		    2240344 bytes
    Variable Size		 1.0066E+10 bytes
    Database Buffers	 4.1205E+10 bytes
    Redo Buffers		   36098048 bytes
    Database mounted.
    
### continue recovering

    RMAN> run {
    set newname for datafile 1 to "/u1/TEST/testora/testdata/system01.dbf";
    set newname for datafile 2 to "/u1/TEST/testora/testdata/system02.dbf";
    set newname for datafile 3 to "/u1/TEST/testora/testdata/system03.dbf";
    set newname for datafile 4 to "/u1/TEST/testora/testdata/system04.dbf";
    set newname for datafile 5 to "/u1/TEST/testora/testdata/system05.dbf";
    set newname for datafile 6 to "/u1/TEST/testora/testdata/system06.dbf";
    set newname for datafile 7 to "/u1/TEST/testora/testdata/system07.dbf";
    set newname for datafile 8 to "/u1/TEST/testora/testdata/system08.dbf";
    set newname for datafile 9 to "/u1/TEST/testora/testdata/system09.dbf";
    set newname for datafile 10 to "/u1/TEST/testora/testdata/system10.dbf";
    set newname for datafile 11 to "/u1/TEST/testora/testdata/system11.dbf";
    set newname for datafile 12 to "/u1/TEST/testora/testdata/sysaux01.dbf";
    set newname for datafile 13 to "/u1/TEST/testora/testdata/undo01.dbf";
    set newname for datafile 14 to "/u1/TEST/testora/testdata/undo02.dbf";
    set newname for datafile 15 to "/u1/TEST/testora/testdata/undo03.dbf";
    set newname for datafile 16 to "/u1/TEST/testora/testdata/a_archive01.dbf";
    set newname for datafile 17 to "/u1/TEST/testora/testdata/a_archive02.dbf";
    set newname for datafile 18 to "/u1/TEST/testora/testdata/a_archive03.dbf";
    set newname for datafile 19 to "/u1/TEST/testora/testdata/a_archive04.dbf";
    set newname for datafile 20 to "/u1/TEST/testora/testdata/a_archive05.dbf";
    set newname for datafile 21 to "/u1/TEST/testora/testdata/a_int01.dbf";
    set newname for datafile 22 to "/u1/TEST/testora/testdata/a_int02.dbf";
    set newname for datafile 23 to "/u1/TEST/testora/testdata/a_int03.dbf";
    set newname for datafile 24 to "/u1/TEST/testora/testdata/a_int04.dbf";
    set newname for datafile 25 to "/u1/TEST/testora/testdata/a_int05.dbf";
    set newname for datafile 26 to "/u1/TEST/testora/testdata/a_int06.dbf";
    set newname for datafile 27 to "/u1/TEST/testora/testdata/a_int07.dbf";
    set newname for datafile 28 to "/u1/TEST/testora/testdata/a_int08.dbf";
    set newname for datafile 29 to "/u1/TEST/testora/testdata/a_media01.dbf";
    set newname for datafile 30 to "/u1/TEST/testora/testdata/a_media02.dbf";
    set newname for datafile 31 to "/u1/TEST/testora/testdata/a_media03.dbf";
    set newname for datafile 32 to "/u1/TEST/testora/testdata/a_media04.dbf";
    set newname for datafile 33 to "/u1/TEST/testora/testdata/a_media05.dbf";
    set newname for datafile 34 to "/u1/TEST/testora/testdata/a_media06.dbf";
    set newname for datafile 35 to "/u1/TEST/testora/testdata/a_media07.dbf";
    set newname for datafile 36 to "/u1/TEST/testora/testdata/a_media08.dbf";
    set newname for datafile 37 to "/u1/TEST/testora/testdata/a_media09.dbf";
    set newname for datafile 38 to "/u1/TEST/testora/testdata/a_media10.dbf";
    set newname for datafile 39 to "/u1/TEST/testora/testdata/a_nolog01.dbf";
    set newname for datafile 40 to "/u1/TEST/testora/testdata/a_queue01.dbf";
    set newname for datafile 41 to "/u1/TEST/testora/testdata/a_queue02.dbf";
    set newname for datafile 42 to "/u1/TEST/testora/testdata/a_queue03.dbf";
    set newname for datafile 43 to "/u1/TEST/testora/testdata/a_queue04.dbf";
    set newname for datafile 44 to "/u1/TEST/testora/testdata/a_queue05.dbf";
    set newname for datafile 45 to "/u1/TEST/testora/testdata/a_queue06.dbf";
    set newname for datafile 46 to "/u1/TEST/testora/testdata/a_queue07.dbf";
    set newname for datafile 47 to "/u1/TEST/testora/testdata/a_queue08.dbf";
    set newname for datafile 48 to "/u1/TEST/testora/testdata/a_queue09.dbf";
    set newname for datafile 49 to "/u1/TEST/testora/testdata/a_queue10.dbf";
    set newname for datafile 50 to "/u1/TEST/testora/testdata/a_queue11.dbf";
    set newname for datafile 51 to "/u1/TEST/testora/testdata/a_queue12.dbf";
    set newname for datafile 52 to "/u1/TEST/testora/testdata/a_queue13.dbf";
    set newname for datafile 53 to "/u1/TEST/testora/testdata/a_queue14.dbf";
    set newname for datafile 54 to "/u1/TEST/testora/testdata/a_queue15.dbf";
    set newname for datafile 55 to "/u1/TEST/testora/testdata/a_queue16.dbf";
    set newname for datafile 56 to "/u1/TEST/testora/testdata/a_queue17.dbf";
    set newname for datafile 57 to "/u1/TEST/testora/testdata/a_queue18.dbf";
    set newname for datafile 58 to "/u1/TEST/testora/testdata/a_queue19.dbf";
    set newname for datafile 59 to "/u1/TEST/testora/testdata/a_queue20.dbf";
    set newname for datafile 60 to "/u1/TEST/testora/testdata/a_queue21.dbf";
    set newname for datafile 61 to "/u1/TEST/testora/testdata/a_queue22.dbf";
    set newname for datafile 62 to "/u1/TEST/testora/testdata/a_queue23.dbf";
    set newname for datafile 63 to "/u1/TEST/testora/testdata/a_queue24.dbf";
    set newname for datafile 64 to "/u1/TEST/testora/testdata/a_queue25.dbf";
    set newname for datafile 65 to "/u1/TEST/testora/testdata/a_ref01.dbf";
    set newname for datafile 66 to "/u1/TEST/testora/testdata/a_ref02.dbf";
    set newname for datafile 67 to "/u1/TEST/testora/testdata/a_summ01.dbf";
    set newname for datafile 68 to "/u1/TEST/testora/testdata/a_summ02.dbf";
    set newname for datafile 69 to "/u1/TEST/testora/testdata/a_summ03.dbf";
    set newname for datafile 70 to "/u1/TEST/testora/testdata/a_summ04.dbf";
    set newname for datafile 71 to "/u1/TEST/testora/testdata/a_summ05.dbf";
    set newname for datafile 72 to "/u1/TEST/testora/testdata/a_tools01.dbf";
    set newname for datafile 73 to "/u1/TEST/testora/testdata/a_txn_data01.dbf";
    set newname for datafile 74 to "/u1/TEST/testora/testdata/a_txn_data02.dbf";
    set newname for datafile 75 to "/u1/TEST/testora/testdata/a_txn_data03.dbf";
    set newname for datafile 76 to "/u1/TEST/testora/testdata/a_txn_data04.dbf";
    set newname for datafile 77 to "/u1/TEST/testora/testdata/a_txn_data05.dbf";
    set newname for datafile 78 to "/u1/TEST/testora/testdata/a_txn_data06.dbf";
    set newname for datafile 79 to "/u1/TEST/testora/testdata/a_txn_data07.dbf";
    set newname for datafile 80 to "/u1/TEST/testora/testdata/a_txn_data08.dbf";
    set newname for datafile 81 to "/u1/TEST/testora/testdata/a_txn_data09.dbf";
    set newname for datafile 82 to "/u1/TEST/testora/testdata/a_txn_data10.dbf";
    set newname for datafile 83 to "/u1/TEST/testora/testdata/a_txn_data11.dbf";
    set newname for datafile 84 to "/u1/TEST/testora/testdata/a_txn_data12.dbf";
    set newname for datafile 85 to "/u1/TEST/testora/testdata/a_txn_data13.dbf";
    set newname for datafile 86 to "/u1/TEST/testora/testdata/a_txn_data14.dbf";
    set newname for datafile 87 to "/u1/TEST/testora/testdata/a_txn_data15.dbf";
    set newname for datafile 88 to "/u1/TEST/testora/testdata/a_txn_data16.dbf";
    set newname for datafile 89 to "/u1/TEST/testora/testdata/a_txn_data17.dbf";
    set newname for datafile 90 to "/u1/TEST/testora/testdata/a_txn_data18.dbf";
    set newname for datafile 91 to "/u1/TEST/testora/testdata/a_txn_data19.dbf";
    set newname for datafile 92 to "/u1/TEST/testora/testdata/a_txn_data20.dbf";
    set newname for datafile 93 to "/u1/TEST/testora/testdata/a_txn_data21.dbf";
    set newname for datafile 94 to "/u1/TEST/testora/testdata/a_txn_data22.dbf";
    set newname for datafile 95 to "/u1/TEST/testora/testdata/a_txn_data23.dbf";
    set newname for datafile 96 to "/u1/TEST/testora/testdata/a_txn_data24.dbf";
    set newname for datafile 97 to "/u1/TEST/testora/testdata/a_txn_data25.dbf";
    set newname for datafile 98 to "/u1/TEST/testora/testdata/a_txn_data26.dbf";
    set newname for datafile 99 to "/u1/TEST/testora/testdata/a_txn_data28.dbf";
    set newname for datafile 100 to "/u1/TEST/testora/testdata/a_txn_data29.dbf";
    set newname for datafile 101 to "/u1/TEST/testora/testdata/a_txn_data30.dbf";
    set newname for datafile 102 to "/u1/TEST/testora/testdata/a_txn_data31.dbf";
    set newname for datafile 103 to "/u1/TEST/testora/testdata/a_txn_data32.dbf";
    set newname for datafile 104 to "/u1/TEST/testora/testdata/a_txn_data33.dbf";
    set newname for datafile 105 to "/u1/TEST/testora/testdata/a_txn_data34.dbf";
    set newname for datafile 106 to "/u1/TEST/testora/testdata/a_txn_data35.dbf";
    set newname for datafile 107 to "/u1/TEST/testora/testdata/a_txn_data36.dbf";
    set newname for datafile 108 to "/u1/TEST/testora/testdata/a_txn_data37.dbf";
    set newname for datafile 109 to "/u1/TEST/testora/testdata/a_txn_data38.dbf";
    set newname for datafile 110 to "/u1/TEST/testora/testdata/a_txn_data39.dbf";
    set newname for datafile 111 to "/u1/TEST/testora/testdata/a_txn_data40.dbf";
    set newname for datafile 112 to "/u1/TEST/testora/testdata/a_txn_data41.dbf";
    set newname for datafile 113 to "/u1/TEST/testora/testdata/a_txn_data42.dbf";
    set newname for datafile 114 to "/u1/TEST/testora/testdata/a_txn_data43.dbf";
    set newname for datafile 115 to "/u1/TEST/testora/testdata/a_txn_data44.dbf";
    set newname for datafile 116 to "/u1/TEST/testora/testdata/a_txn_data45.dbf";
    set newname for datafile 117 to "/u1/TEST/testora/testdata/a_txn_data46.dbf";
    set newname for datafile 118 to "/u1/TEST/testora/testdata/a_txn_data47.dbf";
    set newname for datafile 119 to "/u1/TEST/testora/testdata/a_txn_data48.dbf";
    set newname for datafile 120 to "/u1/TEST/testora/testdata/a_txn_ind01.dbf";
    set newname for datafile 121 to "/u1/TEST/testora/testdata/a_txn_ind02.dbf";
    set newname for datafile 122 to "/u1/TEST/testora/testdata/a_txn_ind03.dbf";
    set newname for datafile 123 to "/u1/TEST/testora/testdata/a_txn_ind04.dbf";
    set newname for datafile 124 to "/u1/TEST/testora/testdata/a_txn_ind05.dbf";
    set newname for datafile 125 to "/u1/TEST/testora/testdata/a_txn_ind06.dbf";
    set newname for datafile 126 to "/u1/TEST/testora/testdata/a_txn_ind07.dbf";
    set newname for datafile 127 to "/u1/TEST/testora/testdata/a_txn_ind08.dbf";
    set newname for datafile 128 to "/u1/TEST/testora/testdata/a_txn_ind09.dbf";
    set newname for datafile 129 to "/u1/TEST/testora/testdata/a_txn_ind10.dbf";
    set newname for datafile 130 to "/u1/TEST/testora/testdata/a_txn_ind11.dbf";
    set newname for datafile 131 to "/u1/TEST/testora/testdata/a_txn_ind12.dbf";
    set newname for datafile 132 to "/u1/TEST/testora/testdata/a_txn_ind13.dbf";
    set newname for datafile 133 to "/u1/TEST/testora/testdata/a_txn_ind14.dbf";
    set newname for datafile 134 to "/u1/TEST/testora/testdata/a_txn_ind15.dbf";
    set newname for datafile 135 to "/u1/TEST/testora/testdata/a_txn_ind16.dbf";
    set newname for datafile 136 to "/u1/TEST/testora/testdata/a_txn_ind17.dbf";
    set newname for datafile 137 to "/u1/TEST/testora/testdata/a_txn_ind18.dbf";
    set newname for datafile 138 to "/u1/TEST/testora/testdata/a_txn_ind19.dbf";
    set newname for datafile 139 to "/u1/TEST/testora/testdata/a_txn_ind20.dbf";
    set newname for datafile 140 to "/u1/TEST/testora/testdata/a_txn_ind21.dbf";
    set newname for datafile 141 to "/u1/TEST/testora/testdata/a_txn_ind22.dbf";
    set newname for datafile 142 to "/u1/TEST/testora/testdata/a_txn_ind23.dbf";
    set newname for datafile 143 to "/u1/TEST/testora/testdata/a_txn_ind24.dbf";
    set newname for datafile 144 to "/u1/TEST/testora/testdata/a_txn_ind25.dbf";
    set newname for datafile 145 to "/u1/TEST/testora/testdata/a_txn_ind26.dbf";
    set newname for datafile 146 to "/u1/TEST/testora/testdata/a_txn_ind27.dbf";
    set newname for datafile 147 to "/u1/TEST/testora/testdata/a_txn_ind28.dbf";
    set newname for datafile 148 to "/u1/TEST/testora/testdata/a_txn_ind29.dbf";
    set newname for datafile 149 to "/u1/TEST/testora/testdata/a_txn_ind30.dbf";
    set newname for datafile 150 to "/u1/TEST/testora/testdata/a_txn_ind31.dbf";
    set newname for datafile 151 to "/u1/TEST/testora/testdata/a_txn_ind32.dbf";
    set newname for datafile 152 to "/u1/TEST/testora/testdata/a_txn_ind33.dbf";
    set newname for datafile 153 to "/u1/TEST/testora/testdata/a_txn_ind34.dbf";
    set newname for datafile 154 to "/u1/TEST/testora/testdata/a_txn_ind35.dbf";
    set newname for datafile 155 to "/u1/TEST/testora/testdata/a_txn_ind36.dbf";
    set newname for datafile 156 to "/u1/TEST/testora/testdata/a_txn_ind37.dbf";
    set newname for datafile 157 to "/u1/TEST/testora/testdata/a_txn_ind38.dbf";
    set newname for datafile 158 to "/u1/TEST/testora/testdata/a_txn_ind39.dbf";
    set newname for datafile 159 to "/u1/TEST/testora/testdata/a_txn_ind40.dbf";
    set newname for datafile 160 to "/u1/TEST/testora/testdata/a_txn_ind41.dbf";
    set newname for datafile 161 to "/u1/TEST/testora/testdata/a_txn_ind42.dbf";
    set newname for datafile 162 to "/u1/TEST/testora/testdata/a_txn_ind43.dbf";
    set newname for datafile 163 to "/u1/TEST/testora/testdata/a_txn_ind44.dbf";
    set newname for datafile 164 to "/u1/TEST/testora/testdata/a_txn_ind45.dbf";
    set newname for datafile 165 to "/u1/TEST/testora/testdata/a_txn_ind46.dbf";
    set newname for datafile 166 to "/u1/TEST/testora/testdata/a_txn_ind47.dbf";
    set newname for datafile 167 to "/u1/TEST/testora/testdata/a_txn_ind48.dbf";
    set newname for datafile 168 to "/u1/TEST/testora/testdata/a_txn_ind49.dbf";
    set newname for datafile 169 to "/u1/TEST/testora/testdata/a_txn_ind50.dbf";
    set newname for datafile 170 to "/u1/TEST/testora/testdata/a_txn_ind51.dbf";
    set newname for datafile 171 to "/u1/TEST/testora/testdata/a_txn_ind52.dbf";
    set newname for datafile 172 to "/u1/TEST/testora/testdata/a_txn_ind53.dbf";
    set newname for datafile 173 to "/u1/TEST/testora/testdata/a_txn_ind54.dbf";
    set newname for datafile 174 to "/u1/TEST/testora/testdata/a_txn_ind55.dbf";
    set newname for datafile 175 to "/u1/TEST/testora/testdata/ctxd01.dbf";
    set newname for datafile 176 to "/u1/TEST/testora/testdata/odm.dbf";
    set newname for datafile 177 to "/u1/TEST/testora/testdata/olap.dbf";
    set newname for datafile 178 to "/u1/TEST/testora/testdata/owad01.dbf";
    set newname for datafile 179 to "/u1/TEST/testora/testdata/portal01.dbf";
    set newname for datafile 180 to "/u1/TEST/testora/testdata/statspack_01.dbf";
    set newname for datafile 181 to "/u1/TEST/testora/testdata/tplcux_data_01.dbf";
    set newname for datafile 182 to "/u1/TEST/testora/testdata/tplcux_idx_01.dbf";
    set newname for datafile 183 to "/u1/TEST/testora/testdata/interim.dbf";
    set newname for datafile 184 to "/u1/TEST/testora/testdata/system12.dbf";
    set newname for datafile 185 to "/u1/TEST/testora/testdata/a_txn_data49.dbf";
    set newname for datafile 186 to "/u1/TEST/testora/testdata/a_ref03.dbf";
    set newname for datafile 187 to "/u1/TEST/testora/testdata/a_summ06.dbf";
    set newname for datafile 188 to "/u1/TEST/testora/testdata/undo04.dbf";
    restore database;
    switch datafile all;
    recover database;
    }
    Starting restore at 17-OCT-13
    using target database control file instead of recovery catalog
    allocated channel: ORA_DISK_1
    channel ORA_DISK_1: SID=629 device type=DISK

    datafile 2 is already restored to file /u1/TEST/testora/testdata/system02.dbf
    ... ...
    datafile 188 is already restored to file /u1/TEST/testora/testdata/undo04.dbf
    channel ORA_DISK_1: starting datafile backup set restore
    channel ORA_DISK_1: specifying datafile(s) to restore from backup set
    channel ORA_DISK_1: restoring datafile 00001 to /u1/TEST/testora/testdata/system01.dbf
    channel ORA_DISK_1: restoring datafile 00020 to /u1/TEST/testora/testdata/a_archive05.dbf
    channel ORA_DISK_1: restoring datafile 00088 to /u1/TEST/testora/testdata/a_txn_data16.dbf
    channel ORA_DISK_1: restoring datafile 00127 to /u1/TEST/testora/testdata/a_txn_ind08.dbf
    channel ORA_DISK_1: reading from backup piece /data/backup/db_bak_lev0_829058166_1_318
    ... ...
    channel ORA_DISK_1: restore complete, elapsed time: 00:00:35
    Finished restore at 17-OCT-13
    
    datafile 1 switched to datafile copy
    input datafile copy RECID=190 STAMP=829070406 file name=/u1/TEST/testora/testdata/system01.dbf
    ... ...
    
    datafile 188 switched to datafile copy
    input datafile copy RECID=377 STAMP=829070448 file name=/u1/TEST/testora/testdata/undo04.dbf

    Starting recover at 17-OCT-13
    using channel ORA_DISK_1

    starting media recovery

    channel ORA_DISK_1: starting archived log restore to default destination
    channel ORA_DISK_1: restoring archived log
    archived log thread=2 sequence=88
    channel ORA_DISK_1: reading from backup piece /data/backup/archive_log_829058535_1_334
    channel ORA_DISK_1: piece handle=/data/backup/archive_log_829058535_1_334 tag=ARC_BAK
    channel ORA_DISK_1: restored backup piece 1
    channel ORA_DISK_1: restore complete, elapsed time: 00:00:03
    channel ORA_DISK_1: starting archived log restore to default destination
    channel ORA_DISK_1: restoring archived log
    archived log thread=1 sequence=1094
    channel ORA_DISK_1: restoring archived log
    archived log thread=1 sequence=1095
    channel ORA_DISK_1: restoring archived log
    archived log thread=2 sequence=89
    channel ORA_DISK_1: reading from backup piece /data/backup/archive_log_829058537_1_335
    channel ORA_DISK_1: piece handle=/data/backup/archive_log_829058537_1_335 tag=ARC_BAK
    channel ORA_DISK_1: restored backup piece 1
    channel ORA_DISK_1: restore complete, elapsed time: 00:00:01
    archived log file name=/u1/TEST/testora/archive/1_1094_828401849.dbf thread=1 sequence=1094
    archived log file name=/u1/TEST/testora/archive/2_88_828401849.dbf thread=2 sequence=88
    archived log file name=/u1/TEST/testora/archive/1_1095_828401849.dbf thread=1 sequence=1095
    archived log file name=/u1/TEST/testora/archive/2_89_828401849.dbf thread=2 sequence=89
    unable to find archived log
    archived log thread=1 sequence=1096
    RMAN-00571: ===========================================================
    RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS ===============
    RMAN-00571: ===========================================================
    RMAN-03002: failure of recover command at 10/17/2013 17:21:31
    RMAN-06054: media recovery requesting unknown archived log for thread 1 with sequence 1096 and starting SCN of 101987199
    
list backup of archivelog
    
    RMAN> list backup of archivelog all;


    List of Backup Sets
    ===================


    BS Key  Size       Device Type Elapsed Time Completion Time
    ------- ---------- ----------- ------------ ---------------
    330     156.45M    DISK        00:00:01     17-OCT-13      
            BP Key: 390   Status: AVAILABLE  Compressed: NO  Tag: ARC_BAK
            Piece Name: /data/backup/archive_log_829058535_1_334

      List of Archived Logs in backup set 330
      Thrd Seq     Low SCN    Low Time  Next SCN   Next Time
      ---- ------- ---------- --------- ---------- ---------
      1    1092    101805928  17-OCT-13 101861298  17-OCT-13
      1    1093    101861298  17-OCT-13 101914951  17-OCT-13
      2    87      101311879  17-OCT-13 101861641  17-OCT-13
      2    88      101861641  17-OCT-13 101987191  17-OCT-13

    BS Key  Size       Device Type Elapsed Time Completion Time
    ------- ---------- ----------- ------------ ---------------
    331     441.13M    DISK        00:00:02     17-OCT-13      
            BP Key: 391   Status: AVAILABLE  Compressed: NO  Tag: ARC_BAK
            Piece Name: /data/backup/archive_log_829058535_1_333

      List of Archived Logs in backup set 331
      Thrd Seq     Low SCN    Low Time  Next SCN   Next Time
      ---- ------- ---------- --------- ---------- ---------
      1    1091    101311755  17-OCT-13 101805928  17-OCT-13

    BS Key  Size       Device Type Elapsed Time Completion Time
    ------- ---------- ----------- ------------ ---------------
    332     27.28M     DISK        00:00:00     17-OCT-13      
            BP Key: 392   Status: AVAILABLE  Compressed: NO  Tag: ARC_BAK
            Piece Name: /data/backup/archive_log_829058537_1_335

      List of Archived Logs in backup set 332
      Thrd Seq     Low SCN    Low Time  Next SCN   Next Time
      ---- ------- ---------- --------- ---------- ---------
      1    1094    101914951  17-OCT-13 101987096  17-OCT-13
      1    1095    101987096  17-OCT-13 101987199  17-OCT-13
      2    89      101987191  17-OCT-13 101987202  17-OCT-13
    
datafile size
    
    [testora@target testora]$ du -sh testdata
    271G	testdata
    
###recreate controlfile

    [testora@target ~]$ export ORACLE_SID=PROD
    [testora@target ~]$ sqlplus  /nolog

    SQL*Plus: Release 11.2.0.3.0 Production on Thu Oct 17 11:09:21 2013

    Copyright (c) 1982, 2011, Oracle.  All rights reserved.

    SQL> conn /as sysdba
    Connected.
    SQL> alter database backup controlfile to trace;

    Database altered.

    SQL> show parameter user_dump

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    user_dump_dest			     string	 /u1/11gR2_BASE/admin/prod/diag
                             /rdbms/prod/PROD/trace
    SQL> exit
    Disconnected from Oracle Database 11g Enterprise Edition Release 11.2.0.3.0 - 64bit Production
    With the Partitioning, OLAP, Data Mining and Real Application Testing options
    
    [testora@target ~]$ ll -rst /u1/11gR2_BASE?admin/prod/diag/rdbms/prod/PROD/trace/ | tail
     3268 -rw-r--r--. 1 testora dba  3346203 Oct 17 10:51 PROD_m000_7617.trc
       12 -rw-r--r--. 1 testora dba     8760 Oct 17 11:01 PROD_dbw0_7573.trm
       72 -rw-r--r--. 1 testora dba    70015 Oct 17 11:01 PROD_dbw0_7573.trc
       84 -rw-r--r--. 1 testora dba    83406 Oct 17 11:01 PROD_ora_7609.trm
      476 -rw-r--r--. 1 testora dba   486962 Oct 17 11:01 PROD_ora_7609.trc
      652 -rw-r--r--. 1 testora dba   663569 Oct 17 11:01 PROD_m000_7779.trm
     4352 -rw-r--r--. 1 testora dba  4454728 Oct 17 11:01 PROD_m000_7779.trc
        4 -rw-r--r--. 1 testora dba      268 Oct 17 11:09 PROD_ora_7826.trm
       28 -rw-r--r--. 1 testora dba    26359 Oct 17 11:09 PROD_ora_7826.trc
      112 -rw-r--r--. 1 testora dba   107376 Oct 17 11:09 alert_PROD.log
      
    [testora@target ~]$ pwd
    /home/testora
    [testora@target ~]$ cp /u1/11gR2_BASE/admin/prod/diag/rdbms/prod/PROD/trace/PROD_ora_7826.trc .
    [testora@target ~]$ ls -l
    total 11952
    drwxr-xr-x. 10 testora dba     4096 Jun 21 17:12 itsection
    drwxr-xr-x.  3 testora dba     4096 May 31 00:43 oradiag_testora
    -rw-r--r--.  1 testora dba    26359 Oct 17 11:10 PROD_ora_7826.trc
    -rw-r--r--.  1 testora dba     1139 Jul 15 16:30 rdccs_m000_15683.out
    -rw-r--r--.  1 testora dba 12196573 Jul 15 16:28 rdccs_m000_15683.trc
    [testora@target ~]$ cp PROD_ora_7826.trc controlfile.sql
    
edit controlfile

    [testora@target ~]$ cat controlfile.sql 
    CREATE CONTROLFILE REUSE DATABASE "PROD" RESETLOGS  ARCHIVELOG
        MAXLOGFILES 16
        MAXLOGMEMBERS 2
        MAXDATAFILES 512
        MAXINSTANCES 63
        MAXLOGHISTORY 2920
    LOGFILE
      GROUP 1 (
        '/u1/TEST/testora/testdata/redo1_1.dbf',
        '/u1/TEST/testora/testdata/redo1_2.dbf'
      ) SIZE 500M BLOCKSIZE 512,
      GROUP 2 (
        '/u1/TEST/testora/testdata/redo2_1.dbf',
        '/u1/TEST/testora/testdata/redo2_2.dbf'
      ) SIZE 500M BLOCKSIZE 512,
      GROUP 3 (
        '/u1/TEST/testora/testdata/redo3_1.dbf',
        '/u1/TEST/testora/testdata/redo3_2.dbf'
      ) SIZE 500M BLOCKSIZE 512
    -- STANDBY LOGFILE
    DATAFILE
        '/u1/TEST/testora/testdata/system01.dbf',
        '/u1/TEST/testora/testdata/system02.dbf',
        '/u1/TEST/testora/testdata/system03.dbf',
        '/u1/TEST/testora/testdata/system04.dbf',
        '/u1/TEST/testora/testdata/system05.dbf',
        '/u1/TEST/testora/testdata/system06.dbf',
        '/u1/TEST/testora/testdata/system07.dbf',
        '/u1/TEST/testora/testdata/system08.dbf',
        '/u1/TEST/testora/testdata/system09.dbf',
        '/u1/TEST/testora/testdata/system10.dbf',
        '/u1/TEST/testora/testdata/system11.dbf',
        '/u1/TEST/testora/testdata/sysaux01.dbf',
        '/u1/TEST/testora/testdata/undo01.dbf',
        '/u1/TEST/testora/testdata/undo02.dbf',
        '/u1/TEST/testora/testdata/undo03.dbf',
        '/u1/TEST/testora/testdata/a_archive01.dbf',
        '/u1/TEST/testora/testdata/a_archive02.dbf',
        '/u1/TEST/testora/testdata/a_archive03.dbf',
        '/u1/TEST/testora/testdata/a_archive04.dbf',
        '/u1/TEST/testora/testdata/a_archive05.dbf',
        '/u1/TEST/testora/testdata/a_int01.dbf',
        '/u1/TEST/testora/testdata/a_int02.dbf',
        '/u1/TEST/testora/testdata/a_int03.dbf',
        '/u1/TEST/testora/testdata/a_int04.dbf',
        '/u1/TEST/testora/testdata/a_int05.dbf',
        '/u1/TEST/testora/testdata/a_int06.dbf',
        '/u1/TEST/testora/testdata/a_int07.dbf',
        '/u1/TEST/testora/testdata/a_int08.dbf',
        '/u1/TEST/testora/testdata/a_media01.dbf',
        '/u1/TEST/testora/testdata/a_media02.dbf',
        '/u1/TEST/testora/testdata/a_media03.dbf',
        '/u1/TEST/testora/testdata/a_media04.dbf',
        '/u1/TEST/testora/testdata/a_media05.dbf',
        '/u1/TEST/testora/testdata/a_media06.dbf',
        '/u1/TEST/testora/testdata/a_media07.dbf',
        '/u1/TEST/testora/testdata/a_media08.dbf',
        '/u1/TEST/testora/testdata/a_media09.dbf',
        '/u1/TEST/testora/testdata/a_media10.dbf',
        '/u1/TEST/testora/testdata/a_nolog01.dbf',
        '/u1/TEST/testora/testdata/a_queue01.dbf',
        '/u1/TEST/testora/testdata/a_queue02.dbf',
        '/u1/TEST/testora/testdata/a_queue03.dbf',
        '/u1/TEST/testora/testdata/a_queue04.dbf',
        '/u1/TEST/testora/testdata/a_queue05.dbf',
        '/u1/TEST/testora/testdata/a_queue06.dbf',
        '/u1/TEST/testora/testdata/a_queue07.dbf',
        '/u1/TEST/testora/testdata/a_queue08.dbf',
        '/u1/TEST/testora/testdata/a_queue09.dbf',
        '/u1/TEST/testora/testdata/a_queue10.dbf',
        '/u1/TEST/testora/testdata/a_queue11.dbf',
        '/u1/TEST/testora/testdata/a_queue12.dbf',
        '/u1/TEST/testora/testdata/a_queue13.dbf',
        '/u1/TEST/testora/testdata/a_queue14.dbf',
        '/u1/TEST/testora/testdata/a_queue15.dbf',
        '/u1/TEST/testora/testdata/a_queue16.dbf',
        '/u1/TEST/testora/testdata/a_queue17.dbf',
        '/u1/TEST/testora/testdata/a_queue18.dbf',
        '/u1/TEST/testora/testdata/a_queue19.dbf',
        '/u1/TEST/testora/testdata/a_queue20.dbf',
        '/u1/TEST/testora/testdata/a_queue21.dbf',
        '/u1/TEST/testora/testdata/a_queue22.dbf',
        '/u1/TEST/testora/testdata/a_queue23.dbf',
        '/u1/TEST/testora/testdata/a_queue24.dbf',
        '/u1/TEST/testora/testdata/a_queue25.dbf',
        '/u1/TEST/testora/testdata/a_ref01.dbf',
        '/u1/TEST/testora/testdata/a_ref02.dbf',
        '/u1/TEST/testora/testdata/a_summ01.dbf',
        '/u1/TEST/testora/testdata/a_summ02.dbf',
        '/u1/TEST/testora/testdata/a_summ03.dbf',
        '/u1/TEST/testora/testdata/a_summ04.dbf',
        '/u1/TEST/testora/testdata/a_summ05.dbf',
        '/u1/TEST/testora/testdata/a_tools01.dbf',
        '/u1/TEST/testora/testdata/a_txn_data01.dbf',
        '/u1/TEST/testora/testdata/a_txn_data02.dbf',
        '/u1/TEST/testora/testdata/a_txn_data03.dbf',
        '/u1/TEST/testora/testdata/a_txn_data04.dbf',
        '/u1/TEST/testora/testdata/a_txn_data05.dbf',
        '/u1/TEST/testora/testdata/a_txn_data06.dbf',
        '/u1/TEST/testora/testdata/a_txn_data07.dbf',
        '/u1/TEST/testora/testdata/a_txn_data08.dbf',
        '/u1/TEST/testora/testdata/a_txn_data09.dbf',
        '/u1/TEST/testora/testdata/a_txn_data10.dbf',
        '/u1/TEST/testora/testdata/a_txn_data11.dbf',
        '/u1/TEST/testora/testdata/a_txn_data12.dbf',
        '/u1/TEST/testora/testdata/a_txn_data13.dbf',
        '/u1/TEST/testora/testdata/a_txn_data14.dbf',
        '/u1/TEST/testora/testdata/a_txn_data15.dbf',
        '/u1/TEST/testora/testdata/a_txn_data16.dbf',
        '/u1/TEST/testora/testdata/a_txn_data17.dbf',
        '/u1/TEST/testora/testdata/a_txn_data18.dbf',
        '/u1/TEST/testora/testdata/a_txn_data19.dbf',
        '/u1/TEST/testora/testdata/a_txn_data20.dbf',
        '/u1/TEST/testora/testdata/a_txn_data21.dbf',
        '/u1/TEST/testora/testdata/a_txn_data22.dbf',
        '/u1/TEST/testora/testdata/a_txn_data23.dbf',
        '/u1/TEST/testora/testdata/a_txn_data24.dbf',
        '/u1/TEST/testora/testdata/a_txn_data25.dbf',
        '/u1/TEST/testora/testdata/a_txn_data26.dbf',
        '/u1/TEST/testora/testdata/a_txn_data28.dbf',
        '/u1/TEST/testora/testdata/a_txn_data29.dbf',
        '/u1/TEST/testora/testdata/a_txn_data30.dbf',
        '/u1/TEST/testora/testdata/a_txn_data31.dbf',
        '/u1/TEST/testora/testdata/a_txn_data32.dbf',
        '/u1/TEST/testora/testdata/a_txn_data33.dbf',
        '/u1/TEST/testora/testdata/a_txn_data34.dbf',
        '/u1/TEST/testora/testdata/a_txn_data35.dbf',
        '/u1/TEST/testora/testdata/a_txn_data36.dbf',
        '/u1/TEST/testora/testdata/a_txn_data37.dbf',
        '/u1/TEST/testora/testdata/a_txn_data38.dbf',
        '/u1/TEST/testora/testdata/a_txn_data39.dbf',
        '/u1/TEST/testora/testdata/a_txn_data40.dbf',
        '/u1/TEST/testora/testdata/a_txn_data41.dbf',
        '/u1/TEST/testora/testdata/a_txn_data42.dbf',
        '/u1/TEST/testora/testdata/a_txn_data43.dbf',
        '/u1/TEST/testora/testdata/a_txn_data44.dbf',
        '/u1/TEST/testora/testdata/a_txn_data45.dbf',
        '/u1/TEST/testora/testdata/a_txn_data46.dbf',
        '/u1/TEST/testora/testdata/a_txn_data47.dbf',
        '/u1/TEST/testora/testdata/a_txn_data48.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind01.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind02.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind03.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind04.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind05.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind06.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind07.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind08.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind09.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind10.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind11.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind12.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind13.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind14.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind15.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind16.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind17.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind18.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind19.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind20.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind21.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind22.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind23.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind24.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind25.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind26.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind27.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind28.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind29.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind30.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind31.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind32.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind33.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind34.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind35.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind36.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind37.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind38.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind39.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind40.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind41.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind42.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind43.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind44.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind45.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind46.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind47.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind48.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind49.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind50.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind51.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind52.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind53.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind54.dbf',
        '/u1/TEST/testora/testdata/a_txn_ind55.dbf',
        '/u1/TEST/testora/testdata/ctxd01.dbf',
        '/u1/TEST/testora/testdata/odm.dbf',
        '/u1/TEST/testora/testdata/olap.dbf',
        '/u1/TEST/testora/testdata/owad01.dbf',
        '/u1/TEST/testora/testdata/portal01.dbf',
        '/u1/TEST/testora/testdata/statspack_01.dbf',
        '/u1/TEST/testora/testdata/tplcux_data_01.dbf',
        '/u1/TEST/testora/testdata/tplcux_idx_01.dbf',
        '/u1/TEST/testora/testdata/interim.dbf',
        '/u1/TEST/testora/testdata/system12.dbf',
        '/u1/TEST/testora/testdata/a_txn_data49.dbf',
        '/u1/TEST/testora/testdata/a_ref03.dbf',
        '/u1/TEST/testora/testdata/a_summ06.dbf',
        '/u1/TEST/testora/testdata/undo04.dbf'
    CHARACTER SET UTF8
    ;
    
create controlfile in `SQL*Plus`

    [testora@target ~]$ ll
    total 11964
    -rw-r--r--.  1 testora dba    10773 Oct 17 11:19 controlfile.sql
    drwxr-xr-x. 10 testora dba     4096 Jun 21 17:12 itsection
    drwxr-xr-x.  3 testora dba     4096 May 31 00:43 oradiag_testora
    -rw-r--r--.  1 testora dba    26359 Oct 17 11:10 PROD_ora_7826.trc
    -rw-r--r--.  1 testora dba     1139 Jul 15 16:30 rdccs_m000_15683.out
    -rw-r--r--.  1 testora dba 12196573 Jul 15 16:28 rdccs_m000_15683.trc

    [testora@target ~]$ sqlplus  /nolog

    SQL*Plus: Release 11.2.0.3.0 Production on Thu Oct 17 17:30:33 2013

    Copyright (c) 1982, 2011, Oracle.  All rights reserved.

    SQL> conn /as sysdba
    Connected.
    SQL> shutdown immediate;
    ORA-01109: database not open


    Database dismounted.
    ORACLE instance shut down.
   
    SQL> startup nomount;
    ORACLE instance started.

    Total System Global Area 5.1310E+10 bytes
    Fixed Size		    2240344 bytes
    Variable Size		 1.0066E+10 bytes
    Database Buffers	 4.1205E+10 bytes
    Redo Buffers		   36098048 bytes
    SQL> @controlfile.sql

    Control file created.
    
open database
 
    SQL> alter database open resetlogs;
    alter database open resetlogs
    *
    ERROR at line 1:
    ORA-38856: cannot mark instance UNNAMED_INSTANCE_2 (redo thread 2) as enabled
    
create logfile group for thread 2 and then disable it

    [testora@tplat2 oradata]$ sqlplus  /nolog

    SQL*Plus: Release 11.2.0.3.0 Production on Wed Jul 31 16:49:15 2013

    Copyright (c) 1982, 2011, Oracle.  All rights reserved.

    SQL> conn /as sysdba
    Connected.

    SQL> ALTER DATABASE ADD LOGFILE THREAD 2
      GROUP 4 (
        '/u1/TEST/testora/testdata/redo4_1.dbf',
        '/u1/TEST/testora/testdata/redo4_2.dbf'
      ) SIZE 500M BLOCKSIZE 512,
      GROUP 5 (
        '/u1/TEST/testora/testdata/redo5_1.dbf',
        '/u1/TEST/testora/testdata/redo5_2.dbf'
      ) SIZE 500M BLOCKSIZE 512,
       GROUP 6 (
        '/u1/TEST/testora/testdata/redo6_1.dbf',
        '/u1/TEST/testora/testdata/redo6_2.dbf'
      ) SIZE 500M BLOCKSIZE 512;

    Database altered.
    
open database

    SQL> alter database open resetlogs;

    Database altered.

    SQL> alter database disable thread 2;

    Database altered.

    SQL> alter database drop logfile group 4, group 5, group 6;
    
    Database altered.
    
    SQL> shutdown immediate;
    Database closed.
    Database dismounted.
    ORACLE instance shut down.
    SQL> startup;
    ORACLE instance started.

    Total System Global Area 5.1310E+10 bytes
    Fixed Size		    2240344 bytes
    Variable Size		 1.0066E+10 bytes
    Database Buffers	 4.1205E+10 bytes
    Redo Buffers		   36098048 bytes
    Database mounted.
    Database opened.

alert.log

    [testora@target trace]$ pwd
    /u1/11gR2_BASE/admin/prod/diag/rdbms/prod/PROD/trace
    [testora@target trace]$ cat alert_PROD.log
    ...
    WARNING: The following temporary tablespaces contain no files.
             This condition can occur when a backup controlfile has
             been restored.  It may be necessary to add files to these
             tablespaces.  That can be done using the SQL statement:
     
             ALTER TABLESPACE <tablespace_name> ADD TEMPFILE
     
             Alternatively, if these temporary tablespaces are no longer
             needed, then they can be dropped.
               Empty temporary tablespace: TEMP
    ...
    
    SQL> alter tablespace temp add tempfile '/u1/TEST/testora/testdata/temp01.dbf' size 2000M, '/u1/TEST/testora/testdata/temp02.dbf' size
    alter tablespace temp add tempfile '/u1/TEST/testora/testdata/temp01.dbf' size 2000M, '/u1/TEST/testora/testdata/temp02.dbf' size 2000M
    *
    ERROR at line 1:
    ORA-00600: internal error code, arguments: [kghstack_free1], [list of datafile
    #'s], [], [], [], [], [], [], [], [], [], []
    
    SQL> SELECT TABLESPACE_NAME,FILE_ID,FILE_NAME,BYTES,BLOCKS,STATUS FROM DBA_TEMP_FILES ORDER BY 1, 2;
    
    TABLESPACE_NAME   FILE_ID FILE_NAME						         BYTES	     BLOCKS      STATUS    
    ----------------- ------- -------------------------------------- ----------- ----------- -------
    TEMP				     1 /u1/TEST/testora/testdata/temp01.dbf	  2097152000	  256000 ONLINE    
    TEMP				     2 /u1/TEST/testora/testdata/temp02.dbf	  2097152000	  256000 ONLINE    
    
##Post Steps
 
###listener configuration
    
show service_names
    
    SQL> show parameter name;

    NAME				     TYPE	 VALUE
    ------------------------------------ ----------- ------------------------------
    db_file_name_convert		     string	 +DATADG/prodora/proddata, /u1/
                             TEST/testora/testdata
    db_name 			     string	 PROD
    db_unique_name			     string	 PROD
    global_names			     boolean	 FALSE
    instance_name			     string	 PROD
    lock_name_space 		     string
    log_file_name_convert		     string	 +DATADG/prodora/proddata, /u1/
                             TEST/testora/testdata
    processor_group_name		     string
    service_names			     string	 PROD
    
listener configuration
    
    [testora@target ~]$ lsnrctl start

    LSNRCTL for Linux: Version 11.2.0.3.0 - Production on 18-OCT-2013 08:41:27

    Copyright (c) 1991, 2011, Oracle.  All rights reserved.

    Starting /u1/11gR2_BASE/product/11.2.0/bin/tnslsnr: please wait...

    TNSLSNR for Linux: Version 11.2.0.3.0 - Production
    System parameter file is /u1/11gR2_BASE/product/11.2.0/network/admin/ERP_erp/listener.ora
    Log messages written to /u1/11gR2_BASE/diag/tnslsnr/target/listener/alert/log.xml
    Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=target.egolife.com)(PORT=1521)))

    Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))
    STATUS of the LISTENER
    ------------------------
    Alias                     LISTENER
    Version                   TNSLSNR for Linux: Version 11.2.0.3.0 - Production
    Start Date                18-OCT-2013 08:41:27
    Uptime                    0 days 0 hr. 0 min. 0 sec
    Trace Level               off
    Security                  ON: Local OS Authentication
    SNMP                      OFF
    Listener Parameter File   /u1/11gR2_BASE/product/11.2.0/network/admin/ERP_erp/listener.ora
    Listener Log File         /u1/11gR2_BASE/diag/tnslsnr/target/listener/alert/log.xml
    Listening Endpoints Summary...
      (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=target.egolife.com)(PORT=1521)))
    The listener supports no services
    The command completed successfully

    [testora@target ~]$ env | grep TNS_ADMIN
    TNS_ADMIN=/u1/11gR2_BASE/product/11.2.0/network/admin/ERP_erp
    [testora@target ~]$ export TNS_ADMIN=TNS_ADMIN=/u1/11gR2_BASE/product/11.2.0/network/admin
    [testora@target ~]$ echo $TNS_ADMIN
    TNS_ADMIN=/u1/11gR2_BASE/product/11.2.0/network/admin
    [testora@target ~]$ lsnrctl stop

    LSNRCTL for Linux: Version 11.2.0.3.0 - Production on 18-OCT-2013 08:46:09

    Copyright (c) 1991, 2011, Oracle.  All rights reserved.

    Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))
    The command completed successfully
    
listener.ora and tnsnames.ora

    [testora@target admin]$ pwd
    /u1/11gR2_BASE/product/11.2.0/network/admin
    [testora@target admin]$ cat listener.ora 
    PROD =
      (DESCRIPTION_LIST =
        (DESCRIPTION =
          (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.1.5)(PORT = 1521))
        )
      )

    SID_LIST_ERP =
      (SID_LIST =
        (SID_DESC =
          (ORACLE_HOME= /u1/11gR2_BASE/product/11.2.0)
          (SID_NAME = PROD)
        )
      )

    [testora@target admin]$ cat tnsnames.ora 
    PROD =
      (DESCRIPTION =
        (ADDRESS_LIST =
          (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.1.5)(PORT = 1521))
        )
        (CONNECT_DATA =
          (SERVICE_NAME = prod)
        )
      )
    
start listener
    
    [testora@target ~]$ lsnrctl start

    LSNRCTL for Linux: Version 11.2.0.3.0 - Production on 18-OCT-2013 08:46:14

    Copyright (c) 1991, 2011, Oracle.  All rights reserved.

    Starting /u1/11gR2_BASE/product/11.2.0/bin/tnslsnr: please wait...

    TNSLSNR for Linux: Version 11.2.0.3.0 - Production
    System parameter file is /u1/11gR2_BASE/product/11.2.0/network/admin/listener.ora
    Log messages written to /u1/11gR2_BASE/diag/tnslsnr/target/listener/alert/log.xml
    Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.5)(PORT=1521)))

    Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))
    STATUS of the LISTENER
    ------------------------
    Alias                     LISTENER
    Version                   TNSLSNR for Linux: Version 11.2.0.3.0 - Production
    Start Date                18-OCT-2013 08:46:14
    Uptime                    0 days 0 hr. 0 min. 0 sec
    Trace Level               off
    Security                  ON: Local OS Authentication
    SNMP                      OFF
    Listener Parameter File   /u1/11gR2_BASE/product/11.2.0/network/admin/listener.ora
    Listener Log File         /u1/11gR2_BASE/diag/tnslsnr/target/listener/alert/log.xml
    Listening Endpoints Summary...
      (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.5)(PORT=1521)))
    The listener supports no services
    The command completed successfully
    [testora@target ~]$ tnsping prod

    TNS Ping Utility for Linux: Version 11.2.0.3.0 - Production on 18-OCT-2013 08:46:26

    Copyright (c) 1997, 2011, Oracle.  All rights reserved.

    Used parameter files:
    /u1/11gR2_BASE/product/11.2.0/network/admin/ERP_erp/sqlnet_ifile.ora


    Used TNSNAMES adapter to resolve the alias
    Attempting to contact (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = 192.168.1.5)(PORT = 1521))) (CONNECT_DATA = (SERVICE_NAME = prod)))
    OK (10 msec)
    
client test with `PL*SQL` Developer ...

###restart database

    SQL> create pfile from spfile;

    File created.

    SQL> shutdown immediate;
    Database closed.
    Database dismounted.
    ORACLE instance shut down.
    
    SQL> startup;  
    ORACLE instance started.

    Total System Global Area 5.1310E+10 bytes
    Fixed Size		    2240344 bytes
    Variable Size		 1.0066E+10 bytes
    Database Buffers	 4.1205E+10 bytes
    Redo Buffers		   36098048 bytes
    Database mounted.
    Database opened.

##full backup of database ...

##Reference

* Oracle Database Backup and Reference User's Guide
---
layout: post
title: Watchdog Introduction
category : Linux
tags : [Linux, Utilities]
---

The watchdog package provides a user-space application which can be configured to provide updates to a hardware or software watchdog timer via the Linux kernel's watchdog interface.

In addition to the standard provisions of watchdog timer behavior, the watchdog package can be configured to voluntarily cease operation based on a number of administrator-defined heuristics.


watchdog 

    [root@dev ~]# ps -ef | grep watchdog
    root         6     2  0 Oct17 ?        00:00:00 [watchdog/0]
    root        10     2  0 Oct17 ?        00:00:00 [watchdog/1]
    root        14     2  0 Oct17 ?        00:00:00 [watchdog/2]
    root        18     2  0 Oct17 ?        00:00:00 [watchdog/3]
    root        22     2  0 Oct17 ?        00:00:00 [watchdog/4]
    root        26     2  0 Oct17 ?        00:00:00 [watchdog/5]
    root        30     2  0 Oct17 ?        00:00:00 [watchdog/6]
    root        34     2  0 Oct17 ?        00:00:00 [watchdog/7]
    root        38     2  0 Oct17 ?        00:00:00 [watchdog/8]
    root        42     2  0 Oct17 ?        00:00:00 [watchdog/9]
    root        46     2  0 Oct17 ?        00:00:00 [watchdog/10]
    root        50     2  0 Oct17 ?        00:00:00 [watchdog/11]
    root     16881 16304  0 11:23 pts/1    00:00:00 grep watchdog
 
stop watchdog
    
    [root@dev ~]# cat /proc/sys/kernel/nmi_watchdog 
    [root@dev ~]# echo 0 > /proc/sys/kernel/nmi_watchdog 
    [root@dev ~]# ps -ef | grep watchdog
    root     16894 16304  0 11:24 pts/1    00:00:00 grep watchdog

To start monitoring the system with OProfile, execute the following command as root:
    
	~]# opcontrol --start
    
Output similar to the following is displayed:

    Using log file /var/lib/oprofile/oprofiled.log Daemon started. Profiler running.
    The settings in /root/.oprofile/daemonrc are used.
    The OProfile daemon, oprofiled, is started; it periodically writes the sample data to the /var/lib/oprofile/samples/ directory. The log file for the daemon is located at /var/lib/oprofile/oprofiled.log.
    Disable the nmi_watchdog registers
	
On a Red Hat Enterprise Linux 6 system, the nmi_watchdog registers with the perf subsystem. Due to this, the perf subsystem grabs control of the performance counter registers at boot time, blocking OProfile from working.

To resolve this, either boot with the nmi_watchdog=0 kernel parameter set, or run the following command to disable nmi_watchdog at run time:
    
	~]# echo 0 > /proc/sys/kernel/nmi_watchdog
	
To re-enable nmi_watchdog, use the following command:

    ~]# echo 1 > /proc/sys/kernel/nmi_watchdog
	
To stop the profiler, execute the following command as root:

    ~]# opcontrol --shutdown

##reference

* [New Package: Watchdog](http://rhn.redhat.com/errata/RHEA-2007-0715.html)
* [OProfile on Deployment Guide](https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/Deployment_Guide/index.html)
---
layout: post
title: Linux Administrator
category: Miscellanies
tags: [Linux, Resource]
---

刚接触电脑始于从大学时代，大学以前对电脑可谓一无所知，07年高考填报志愿都是同学帮忙填的，可见在 没人指导的情况下毅然填报一无所知、学费昂贵的软件工程专业需要多么大的勇气，也足见无知者无畏。

和很多人一样，接触的第一个操作系统是Windows，不过是Vista号称最失败的Windows OS，不知它是不是 Windows家族中短命的一个。大二时，开始接触[Ubuntu](http://www.ubuntu.com/)，一直用到现在。在学 “Unix系统程序设计”选修课时，是[Fedora](http://fedoraproject.org/)，但后来很少关注。大四时经同 学介绍，了解到[Amazon AWS](http://aws.amazon.com/)的存在，真正使用是在有了工作办了信用卡之后； 毕业后做Java开发、DBA时，用到最多的是[AIX 5.3](http://www-03.ibm.com/systems/power/software/aix/v53/)， [Redhat 6](http://www.redhat.com/promo/Red_Hat_Enterprise_Linux6/), (包括 [CentOS 6](http://www.centos.org/modules/newbb/index.php?cat=9), [Oracle Linux 6](http://www.oracle.com/us/technologies/linux/overview/index.html))； 期间接触过一些虚拟化的知识，如[Xen](http://www.xenproject.org/)、[vSphere](http://www.vmware.com/products/vsphere/)、 [HyperV](http://www.microsoft.com/hyper-v-server/)。业余时间，购买过一年的[Linode](http://www.linode.com/‎) [VPS](http://en.wikipedia.org/wiki/Virtual_private_server)服务，用于搭建个人博客和XX，不 过积累少，很少有输出，现在博客也迁移到[Github](https://github.com/dylanninin/dylanninin.github.com)。

在DBA工作中，有一部分是在做Linux系统管理员和开发运维，从工作内容上讲，主要包括服务器硬件的配置和 安装，操作系统和应用软件的安装与配置，用户权限、文件系统、网络、内存等资源的管理与维护，日常问题 的诊断和处理，服务器性能的监控和调优，系统日志的收集和分析，备份与恢复策略的定制与测试，高可用性 环境的规划与实施，服务器软硬件的维护与升级等，确保服务器可用、安全和可靠。工作时，为系统的学习 Linux，花重金买了鸟哥的Linux私房菜系列，作为循序渐进、深入浅出的Linux教程，确实物有所值；另外，鸟 哥的书中有大量的学习方法、有用链接，可供探索和深究。

##Roadmap to Linux Administrator

花几分钟整理了一份简单的Linux资料，偏向Redhat和SA，可供快速入门，轻松存活。

* fundamentals (shell, vi/vim, ssh, Xshell)
    
	* [Explain Shell](http://explainshell.com/)
	* [简明Vim练级攻略](http://coolshell.cn/articles/5426.html‎)
	* [VIM Hacks by Cornelius](http://www.slideshare.net/c9s/vim-hacks‎)
	* [Git时代的VIM不完全使用教程](http://beiyuu.com/git-vim-tutorial/)
	* [SSH原理与运用(一)](http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html)
	* [SSH原理与运用(二)](http://www.ruanyifeng.com/blog/2011/12/ssh_port_forwarding.html)
	* [A挨个搞： Xshell十大技巧](http://actgod.com/archives/86/)

* administration

	* [51CTO电子杂志《Linux运维趋势》](http://os.51cto.com/art/201011/233915.htm)

* tuning & diagnostics

	* [First 5 minutes Troubleshooting A Server](http://devo.ps/blog/2013/03/06/troubleshooting-5minutes-on-a-yet-unknown-box.html)
	* [Linux System Performance Monitoring Tools](http://dylanninin.com/assets/images/2013/tools.png)

* book

	* [A Byte of VIM v0.51](http://swaroopch.com/notes/vim/‎)
	* [Hack Proofing Linux](http://book.douban.com/subject/10512634/)
	* [Understanding The Linux Kernel](http://www.douban.com/subject/1776614/)
	* [SSH, The Secure Shell: The Definitive Guide](http://book.douban.com/subject/2299605/)

* blog

	* [鸟哥的Linux私房菜：基础学习篇](http://vbird.dic.ksu.edu.tw/linux_basic/)
	* [鸟哥的Linux私房菜：服务器架设篇](http://vbird.dic.ksu.edu.tw/linux_server/)

* reference

	* [Linux网站导航](http://linux.ubuntu.org.cn/)
	* [Redhat Enterprise Linux - Document Center](https://access.redhat.com/site/documentation/Red_Hat_Enterprise_Linux/)
	* [IBM Redbooks](http://www.redbooks.ibm.com/‎)

---
layout: post
title: Oracle ERP DBA
category: Miscellanies
tags: [Oracle, DBA, EBS, Resource]
---

Oracle EBS全称Oracle E-Business Suite，是有名的[ERP](http://en.wikipedia.org/wiki/Enterprise_resource_planning) 产品之一，其他的还有SAP ERP，用友ERP等，具体参见[List of ERP software packages](http://en.wikipedia.org/wiki/List_of_ERP_software_packages) 。

从2012-7月至今，做了一年有余的ERP DBA，主要负责管理和维护Oracle E-Business 11i和R12，服务全 公司，用于生产制造、销售管理等。其中11i基于IBM Power5，OS为AIX 5.3；R12基于x86，OS为Oracle Linux 6。

最近半年，在Oracle Advanced Customer Support的Team带领下，完成了11i到R12的升级项目，主要包 括硬件平台的迁移(主机从Power5到x86，存储从DS4700到V7000)，部署架构的调整(应用、数据库分离， 单节点到多节点，数据库实施RAC + ASM)，应用、数据库、客制化程序的升级等。

经过一年的折腾，可以说入门了，积累不多，但体会不少，以下仅针对ERP DBA吐吐槽，不同人体会可能 不一样：

* ERP DBA也是一个SDE，是Someone Do Everything，也是Miscellaneous Transaction；
* Oracle EBS坑太多，软件臃肿，文档泛滥，Reference Cross Reference，而且还是个大内存杀手；
* Oracle 有个藏经阁叫Metalink，皓首穷经始成绝技，但人生苦短时不我待，容易练成小无相功；
* Oracle ACS Team经验丰富，但多是重复性的、支持性的工作，远远望去，不免无聊乏味，也不免骑虎难下；

当然，即使这么多坑坑槽槽，也可以从中学到很多，比如耐下心来看英文Guide和Note，熟悉行业的业务流程， 理解复杂系统的技术架构；其中也有犯过很多错误，走过很多弯路，所谓“犯完这些错误，再牛逼起来”是也。 只是Oracle ERP DBA入行需慎重再慎重。

##ERP DBA

以下引用朱龙春在[《Oracle E-Business Suite: ERP DBA实践指南》]((http://book.douban.com/subject/10795733/)) 中的话，介绍ERP DBA：

ERP 在中国的应用与推广已有近20 年，而Oracle 电子商务套件（E-Business Suite）作 为世界上最有 名的ERP 产品之一，在中国也已经有了十几年的发展历程，并且已经广泛应用于各个行业和领域，在国内市 场上占有很大份额。
	
随着ERP 的发展，市场上对于能担任重要角色的ERP DBA 有着大量的需求。ERP DBA 与一般意义上的DBA和 其他技术人员是有区别的：ERP DBA 需要有更多的综合知识，不仅 要精通ERP 产品和数据库技术，还要掌 握操作系统、网络、存储设备及程序开发等相关知识，甚至对模块功能也需要有一定的了解。可见，ERP DBA人才是需要有较长实践的持续积累的。 	

然而，像Oracle 电子商务套件这样重要的ERP 产品，由于是西方人研发的，相关的文档基本上都是英文的， 至今也没有一本写给ERP DBA 看的中文书籍，这也就进一步抬高了进入ERP 领域的门槛，让很多爱好者无从 下手或望而却步。
	
本书是国内第一本适用于ERP DBA 的中文书籍，全书从介绍如何架构一个大型ERP 系统开始，并配以ERP系 统管理员的基础知识，最后以概述如何优化和维护一个大型ERP 系统结尾。漫漫人生路，笔者总结自己十几 年的ERP 从业经历，越发感悟到：“越是最基础的东西，往往就是最精髓的”，所谓的“高级技术”只不过是基 础知识的衍生而已。当然，技能是ERP DBA 存在的价值，但思路可以决定一个人的未来，所以本书推翻了初 稿中大量的所谓的“高级技术”内容，以“最基础的知识”为引子，力求使读者不仅能掌握基础知识，更能拓宽 思路，从而一步一步走向架构师的职业发展道路。“授人以鱼，不如授人以渔”，这是笔者一贯秉承的理念， 也是始终贯穿于本书的指导思想，希望本书能够降低广大技术爱好者的学习难度，并使更多的技术爱好者走 进ERP 顾问行业，促进中国ERP 事业的进一步发展。

##Roadmap to Oracle ERP DBA

Oracle ERP的相关文档基本上都是英文的，但因为缺少中文文档而望而却步：What a shame it is!

阅读官方文档和同行博客是熟悉其产品和技术的最有效的不二法门，以下是个人收集整理的一些资料，大部 分都可以在[Oracle R12.1 Documentation Library](http://docs.oracle.com/cd/B53825_08/current/html/docset.html) 和[Oracle Blogs](https://blogs.oracle.com/)中找到，其他请自行Google。

* technology	

	* R12 Technology Stack Roadmap
	* R12 Technology Essentials
	* Oracle Application Installation Guide:Using Rapid Install
	* Oracle Applications Concepts
	* Oracle Applications Multiple Organizations Implementation Guide

* administration	

	* Oracle Applications System Administrator's Guide - Configuration
	* Oracle Applications System Administrator's Guide - Maintenance
	* Oracle Applications System Administrator's Guide - Security
	* Oracle Web Applications Desktop Intergrator Implementation and Administrator's Guide

* tuning & diagnostics

	* Tuning All Layers of EBS
	* Oracle EBS Diagnostics User's Guide

* guide

	* Oracle Applications User's Guide
	* Oracle Applications Developer's Guide
	* Oracle Application Framework Personalization Guide
	* Oracle Applications Flexfields Guide
	* Oracle Workflow Administrator's Guide
	* Oracle Workflow User's Guide
	* Oracle Application User Interface Standards for Forms-Based Products

* book
    
	* [Oracle E-Business Suite: ERP DBA实践指南](http://book.douban.com/subject/10795733/)

* blog

	* [Oracle E-Business Suite Technology](https://blogs.oracle.com/stevenChan/)
	* [Oracle E-Business Suite Support Blog](https://blogs.oracle.com/ebs/)
	* [Apps DBA & Architect Knowledge Warehouse](https://blogs.oracle.com/longchun/)
	* [Paul Tian on CSDN](http://blog.csdn.net/pan_tian)
	* [OracleSeeker](http://oracleseeker.com/)

* reference

	* [Oracle R12.1 Documentation Library](http://docs.oracle.com/cd/B53825_08/current/html/docset.html)
	* [Oracle Metalink](https://support.oracle.com)
	* [Oracle eTRM Technical Reference](http://etrm.oracle.com/pls/etrm/etrm_search.search)
---
layout: post
title: Oracel DBA
category: Miscellanies
tags: [Oracle, Database, DBA, Resource]
---

DBA，即Database Administrator，数据库管理员，是负责管理和维护数据库的人。

DBA负责全面管理和维护数据库，从工作内容上讲，贯穿了一个数据库的整个服务周期，如数据库安装和创建， 网络和客户端配置，用户权限、数据库对象、存储和实例的管理与维护，日常问题的诊断和处理，系统性能监 控和调优，备份与恢复策略的定制与测试，高可用性环境的规划与实施，历史数据的切割与归档，数据库软件 的维护与升级，以及数据库的迁移等，确保数据库可用、安全和可靠。

不同公司分工明细度不同，DBA的职责也不尽相同，管理和维护数据库是DBA的必备技能，但随着发展，每个DBA 会有各自的研究方向和专长，如性能调优，备份与恢复，高可用性，内部原理等。一般来讲，DBA的定义更为宽 泛，除了数据库的全面管理和维护，DBA还需兼任SA(System Administrator)、SDE(Software Development Engineer)的部分工作；因此按面向的工作内容，DBA又可以分为运维DBA、开发DBA、应用DBA（如ERP DBA）等。

DBA的成长不易，DBA要经过入门、初级、中级、高级、资深、顶级等阶段，算起来，可能又是一个本硕博连读 的时间。只是不少人在入门阶段就会身陷泥淖左右彷徨而不得其法，导致多走弯路、耗费青春。陈吉平在《构 建Oracle高可用环境--企业级高可用数据库架构、实战与经验总结》中提到，“不管任何行业，高级的人才总是 奇缺，而低端的人总是遍地都是，永恒的金字塔结构持续了一年又一年。如果你不想做金字塔的最低层，就必 须及早自我修炼”。修炼的方法，就是多读书、多实践、多思考、多总结，逐步提升自己在管理与维护、技术与 经验、规划与设计、视野与思路等方面的水平。

知乎上有一个提问[“DBA应该具备什么样的特质？招聘DBA时应看谢什么？”](http://www.zhihu.com/question/20112937), [涛吴](http://www.zhihu.com/people/Metaphox)依重要程度依次列举了一些DBA应该具有的性格、知识特点， 可供准备走入DBA行业的人参考，也可供已是DBA的人反思和鞭策。

因上一任DBA离职和工作调整，2012-7月临时从Java开发转Oracle DBA，至今一年有余。期间，主要维护Oracle EBS 11i和R12，包括Apps和DB，以及一些IT系统的数据库，包括Oracle Database 9i, 10g, 11g以及MySQL 5.1， 需要学习大量关于Oracle Database、MySQL、主机、存储和网络等方面的知识，因时间精力有限，只是初窥门径 而已。

这一年多时间，看了一些官方文档，也看了一些个人博客，官方的英文文档依然是首先，中文圈重点推荐下 [Eygle](http://www.eygle.com/)，他是[Oracle ACE](http://www.oracle.com/technetwork/community/oracle-ace/index.html) 总监，[云和恩墨](http://www.enmotech.com/)创始人，现在专注于以数据为中心的企业服务和传道授业解惑； [Dave](http://blog.csdn.net/tianlesoftware)，熟读官方文档，原创和实践很多，是学习Oracle的益友；更 多Oracle牛人，可以参考[中国Oracle用户组](http://www.acoug.org/)。

##Roadmap to Oracle DBA

阅读官方文档和同行博客是熟悉其产品和技术的最有效的不二法门，以下是个人收集整理的一些资料，大部 分都可以在[Oracle Database Documentation Library](http://www.oracle.com/pls/db112/homepage) 和[Oracle Blogs](https://blogs.oracle.com/)中找到。

* roadmap

	* [Maclean：学习Oracle Database的自由之翼](http://www.askmaclean.com/archives/linked-to-oracle-world.html)
	* [Tom: the road map to 10g documentation](http://dylanninin.com/assets/images/2013/tom.jpg)
	* [High Level Vision of DSI](http://dylanninin.com/assets/images/2013/dsi.jpg)

* fundamentals (SQL, PL/SQL, `SQL*Plus`, PL/SQL Developer)

	* Oracle SQL教程
	* Oracle SQL & PL/SQL 教程
	* `SQL*Plus` Quick Reference
	* PL/SQL Language Reference
	* PL/SQL Developer 8.0 User’s Guide 

* administration

	* 2 Day DBA
	* 2 Day + Security Guide
	* Concepts
	* Administrator's Guide
	* Net Services Administrator's Guide
	* Error Messages

* tuning & diagnostics

	* 2 Day + Performance Tuning Guide
	* Performance Tuning Guide
	* Cost Based Oracle Fundamentals

* high availability

	* High Availability Best Practices
	* The White Paper: Oracle Database 11g Release 2 High Availability 
	* Data Guard Concepts and Administration
	* Backup and Recovery User's Guide
	* Real Application Clusters Administration and Deployment Guide
   
* development

	* 2 Day + Java Developer's Guide
	* Java Developer's Guide
	* JDBC Developer's Guide
   
* book

	* [陈吉平：构建Oracle高可用环境--企业级高可用数据库架构、实战与经验总结](http://www.douban.com/subject/2531036/)
	* [Oracle DBA手记1：数据库诊断案例与性能优化实践](http://www.douban.com/subject/4209919/)
	* [Oracle DBA手记2：数据库诊断案例与内部恢复实践](http://book.douban.com/subject/5362865/)
	* [Oracle DBA手记3：数据库性能优化与内部原理解析](http://book.douban.com/subject/6849408/)
	* [Fenng维护的豆列：Oracle优秀图书](http://www.douban.com/doulist/10940/)

* blog

	* [Oracle DB/EM Support](https://blogs.oracle.com/db/)
	* [Oracle Optimizer](https://blogs.oracle.com/optimizer/)
	* [Dave](http://blog.csdn.net/tianlesoftware)
	* [Eygle](http://www.eygle.com/)

* reference
    
	* [Tahiti Documentation](http://tahiti.oracle.com/)
	* [Oracle Metalink](https://support.oracle.com)

---
layout: post
title: Free Blog with Github Pages
category : Miscellanies
tags : [Git, Github, Markdown, Python, Resource]
---

##Github

阳志平："正是Github，让社会化编程成为现实。本文尝试谈谈GitHub的文化、技巧与影响。" 在[阳志平：如何高效利用GitHub](http://www.yangzhiping.com/tech/github.html)文中介 绍了Github的方方面面，包括一些很有意思的创意，如用Github做PPT，发布婚宴的邀请函等， 可以让人快速拥抱Github，十分不错。

* [How to build a Github](http://zachholman.com/talk/how-to-build-a-github/)，Github一名早期员工介绍Github的历史，5年108名员工无人离职。
* [阳志平：如何高效利用GitHub](http://www.yangzhiping.com/tech/github.html)，信息量很大，值得深入学习和实践。
* [Got Github](http://www.worldhello.net/gotgithub/)，蒋鑫，介绍Github的开源书籍，你真的了解Github吗，如果不了解，可以认真看看此书。

##Git

* [Pro Git](http://git-scm.com/book)，Git深入浅出教程，不过[git-scm]被blocked了，不信你试试。
* [Pro Git 中文版](http://git-scm.com/book/zh)
* [Git Reference](http://gitref.org/)
* [Linus讲解git](http://www.youtube.com/watch?v=4XpnKHJAok8)，Google大会演讲，Linus介绍他创造git的原因，对比了git和svn。

##Markdown

语法

* [Official Markdown Syntax](http://daringfireball.net/projects/markdown/syntax)，Markdown的官方教程，讲了Markdown的起源，设计哲学和语法。
* [Markdown Syntax on Stackoverflow](http://stackoverflow.com/editing-help)，Stackoverflow上的Markdown教程，介绍得更为简单实用。

写作

* [阳志平：Markdown写作浅谈](http://www.yangzhiping.com/tech/r-markdown-knitr.html)，介绍了常见科技写作工具遇到的问题，如何使用Markdown替代并解决这些问题。

编辑器

* [Dillinger: Online Markdown Editor](http://dillinger.io/)，Web版的在线编辑器，很漂亮，体验也不错，支持连接Dropbox、Github、Google Drive。
* [MarkdownPad for Windows](http://markdownpad.com/)，左右两栏格式，可以实时预览渲染的效果，也支持自定义Markdown的CSS样式，可以把Github风格的Markdown CSS拷贝一份。
* [Markdown Vim Mode](https://github.com/plasticboy/vim-markdown) Vim的Markdown插件，提供语法高亮和折叠。
* [Vim + Chrome预览](http://howiefh.github.io/2013/05/16/vim-markdown-preview/)，我现在都使用这种方案。
* [Git时代的VIM不完全使用教程](http://beiyuu.com/git-vim-tutorial/)，介绍了Vim的基本用法，相关资料；重点在插件管理工具[Vundle](https://github.com/gmarik/vundle)，结合Git，就类似Python的PIP、Ruby的Gem了，十分简便。补充下，这个人的博客风格也很简洁、漂亮。
* [Notepad++](http://notepad-plus-plus.org/)，Markdown语法熟悉后，在日常学习工作时，多文件编辑会频繁，这时使用Notepad++占用内存小，也很方便。

更多编辑器参见：[阳志平：Markdown写作浅谈](http://www.yangzhiping.com/tech/r-markdown-knitr.html)，[阳志平：Markdown生态链整理](http://www.yangzhiping.com/tech/markdown-ecosystem.html)。

Evernote

平时用Evernote比较多，顺便整理下和Markdown相关的方案。

* [马克飞象：专为印象笔记打造的Markdown编辑器](http://maxiang.info/)，这好像是最早让Evernote支持Markdown的方案，不过是国人开发的，慎用。
* [Evernote](http://evernote.com/)，中文版为[印象笔记](http://www.yinxiang.com/)。PC端、移动端有很多相关的应用、插件。推荐[Chrome](https://www.google.com/intl/zh-CN/chrome/browser/) + [Evernote Web Clipper](http://evernote.com/webclipper/)组合，在浏览网页时，可以随心所欲的剪辑收藏，很不错。
* [Ruby China:有人用Evernote整合Markdown吗？](http://ruby-china.org/topics/759)


其他

* [Ruby China: 不选择Markdown的理由](http://ruby-china.org/topics/10734)，讲Markdown的坑，用了一年多，我也感觉该换换了。 
* [Rei](http://blog.chloerei.com/authors/chloerei)是[Writings.io](https://writings.io/)的创建者，不过它[失败了](http://blog.chloerei.com/articles/79-writings-io-is-failure),马上就要[关闭](https://writings.io/)；[36氪](http://www.36kr.com/)上的采访[我犯了过早产品化的错误——Writings.io创始人谈Writings.io的关闭](http://www.36kr.com/p/206974.html)。源代码在[Github](https://github.com/chloerei/writings)上。

##Blog

为什么要写博客

* [阮一峰：为什么要写Blog？](http://www.ruanyifeng.com/blog/2006/12/why_i_keep_blogging.html)
* [BeiYuu：我为什么写博客？](http://beiyuu.com/why-blog/)


Github Pages

* [阳志平：理想的写作环境：Git + Github + Markdown + Jekyll](http://www.yangzhiping.com/tech/writing-space.html)
* [阮一峰：使用Github Pages搭建免费的、无限流量的Blog](http://www.ruanyifeng.com/blog/2012/08/blogging_with_jekyll.html)
* [Github Pages Help](https://help.github.com/categories/20/articles)
* [Official Jekyll Site](http://jekyllrb.com/docs/home/)
* [jekyll-bootstrap](http://jekyllbootstrap.com/) 减少折腾过程，可以快速创建Github Pages博客。


Dropbox

* [Dropbox-Hosted Websites](https://news.ycombinator.com/item?id=6387242)

##About Me

我写博客起于2012年9月底，至今一年有余，但积累少，输出少，也无法大放厥词，无奈之下只好用工作中累积的一些Note、Log来充数。

* 2012-10-1 ~ 2013-10-1 使用[Linode VPS](http://www.linode.com/)，刚好一年到期。博客程序为[MovableType](http://www.movabletype.org/)，之所以没有选择[Wordpress](wordpress.org)，是受[阮一峰](http://www.ruanyifeng.com/)影响，详见[四种Blog程序](http://www.ruanyifeng.com/blog/2004/01/blog.html)，[Wordpress和MovableType的比较](http://www.ruanyifeng.com/blog/2007/04/wordpress_vs_movable_type.html)。当时的博客计划见[Blog Schedule](http://dylanninin.com/blog/2012/09/30/blog_schedule.html)。
* 2013-7月初  有个小插曲，自学Python接触的第一个Web框架的[Web.py](http://webpy.org/)的创始人[Aaron Swartz](http://www.aaronsw.com/)自杀了，他也是Markdown的设计者之一。作为敬意，用[Web.py](http://webpy.org/)写了一个简单的博客程序，渲染Markdown生成HTML，放在[Github](https://github.com/dylanninin/blog)上，现在因VPS到期，也没有提供在线预览，缘起见[Aaron Swartz Commits Suidice](http://dylanninin.com/blog/2013/01/12/aaron_swartz_commits_suidice.html)。关于Aaron Swartz见[Fenng](http://dbanotes.net/siteinfo.html)的介绍[少年心气 － 艾伦·斯沃兹(Aaron Swartz) 的传奇](http://dbanotes.net/geek/aaron-swartz_smells-like-teen-spirit.html)。
* Since 2013-10-1 博客写得少，也懒于fq，所以选择了免费的方案，将博客用Markdown重写了一遍，并迁移到Github Pages，博客地址[dylanninin.com](http://dylanninin.com/)，[代码库](https://github.com/dylanninin/dylanninin.github.com)。


---
layout: post
title: Good Resumes
category : Miscellanies
tags : [Resume]
---


有幸看到[打喷嚏](http://www.dapenti.com/)上的一篇[奇葩简历](http://www.dapenti.com/blog/more.asp?name=xilei&id=83363)，[@天才小熊猫](http://weibo.com/panada)都笑尿了...。

工作中打杂的活不少，其中一项就是筛选校招简历，目前已经筛选过13届秋季、春季，以及14届秋季的部分简历，以本科生、硕士生居多，偶尔还会出现几个博士（竟然还有博士应聘我司，可见如今的教育和就业市场多么混乱）。回想想起来，自己还真遇到一些有趣的简历，今儿正好汇总一份，以供娱乐。

直接上料，有图有真相。

特别注明：其中不涉及个人隐私，若有雷同，纯属巧合！

![education](http://dylanninin.com/assets/images/2013/resume/2.png)

![professional](http://dylanninin.com/assets/images/2013/resume/3.png)

![professional](http://dylanninin.com/assets/images/2013/resume/4.png)

![skills](http://dylanninin.com/assets/images/2013/resume/5.png)

![skills](http://dylanninin.com/assets/images/2013/resume/6.png)

![skills](http://dylanninin.com/assets/images/2013/resume/7.png)

![activities](http://dylanninin.com/assets/images/2013/resume/8.png)

![projects](http://dylanninin.com/assets/images/2013/resume/9.png)

![self_evaluation](http://dylanninin.com/assets/images/2013/resume/10.png)

![self_evaluation](http://dylanninin.com/assets/images/2013/resume/11.png)

![self_evaluation](http://dylanninin.com/assets/images/2013/resume/12.png)

![trial](http://dylanninin.com/assets/images/2013/resume/13.png)

##参考

* [[八卦一下]奇葩简历](http://www.dapenti.com/blog/more.asp?name=xilei&id=83363)
---
layout: post
title: Sentences in SQLite 3
category : Miscellanies
tags : [SQLite, Rails]
---

拖同学和朋友帮忙，最近看Rails，第一份教材自然是[Agile Web Development with Rails 4](http://book.douban.com/subject/24718727/)。

其中谈到SQLite 3是Rails的默认数据库，使用SQLite省去了安装和创建数据库、创建账户、分配权限等步骤，可以使开发人员的精力集中在项目开发上；等到部署和投入使用时，再根据应用和业务特点选择合适的数据库产品，此时SQLite就不一定适合了。

比起SQLite这类嵌入式数据库，常见的CS架构的数据库产品的安装、配置、使用、维护等一般是非常耗时的，此时可能需要一个专职的DBA来支撑，安装数据库软件、创建数据库、创建用户、分配权限等看似很简单，但看看[OS Preparation for Oracle Database](http://dylanninin.com/blog/2013/07/19/os_preparation_for_oracle.html)、 [Manually Creating Oracle Database On Linux](http://dylanninin.com/blog/2012/10/01/manually_creating_oradb_on_linux.html)、[Users, Roles, Privileges in Oracle Database](http://dylanninin.com/blog/2013/03/07/user_roles_privileges.html)就知道这些步骤有多么复杂了。 对于适合Rails开发的Mac、Ubuntu本本来说，运行Oracle这类数据库实在太暴殄天物，估计没人干这事儿吧！此时选择SQLite是多么的合适，不需要单独运行的数据库，也不需要额外的网络配置。Rails选择SQLite作为默认的数据库可以说是它宣称"Agile"的一个有力例证。 

旧习难改，忍不住去看了下SQLite的介绍，设计哲学，SQL教程，日常使用以及开发教程，不得不说，[The Definitive Guide to SQLite](http://book.douban.com/subject/5392299/)是一本好书，概念阐述得清晰到位，配图也很经典，让人真正体会到什么是一图胜万言。

比如这张讲`SELECT`语句处理过程的：

![SELECT](http://dylanninin.com/assets/images/2013/sqlite_sql_select.png)

当然，书中文笔也是很优美、很流畅的，所以看到一图胜万言时，也不要忘记认细读。这不，我就顺手摘抄了其中的一些好句子，与大家共赏。

##The Definitive Guide to SQLite 

The Definitive Guide to SQLite 

* The eXperT’s Voice ® in open source. Take control of this compact and powerful tool to embed sophisticated SQL databases within your applications.

The API is both well documented and intuitive.

* As a programming library, SQLite’s API is one of the simplest and easiest to use. The API is both well documented and intuitive.It is designed to help you customize SQLite in many ways, such as implementing your own custom SQL functions in C. The open source community also has a created a vast number of language and library interfaces with which to use SQLite. There are extensions for Perl, Python, Ruby, Tcl/Tk, Java, PHP, Visual Basic, ODBC, Delphi, C#, VB .NET, Smalltalk, Ada, Objective C, Eiffel, Rexx, Lisp, Scheme, Lua, Pike, Objective Camel, Qt, WxWindows, REALBASIC, and others. You can find an exhaustive list on the SQLite wiki: www.sqlite.org/cvstrac/wiki?p=SqliteWrappers. 

For their particular application, Informix was somewhat overkill. For an experienced database administrator (DBA) at the time, it could take almost an entire day to install or upgrade. To the uninitiated application programmer, it might take forever.

* SQLite was conceived on a battleship...well, sort of. SQLite’s author, D. Richard Hipp, was working for General Dynamics on a program for the U.S. Navy developing software for use on board guided missile destroyers. The program originally ran on Hewlett-Packard Unix (HP-UX) and used an Informix database as the back end. For their particular application, Informix was somewhat overkill. For an experienced database administrator (DBA) at the time, it could take almost an entire day to install or upgrade. To the uninitiated application programmer, it might take forever. What was really needed was a self-contained database that was easy to use and that could travel with the program and run anywhere regardless of what other software was or wasn’t installed on the system. 

SQLite contains just enough features to fit in a single programmer’s brain, and like its library, it requires as small a footprint in the gray matter as it does in RAM.

* From its initial conception, SQLite has been designed so that it can be incorporated and used without the need of a DBA. Configuring and administering SQLite is as simple as it gets. SQLite contains just enough features to fit in a single programmer’s brain, and like its library, it requires as small a footprint in the gray matter as it does in RAM.

It is not that SQLite is incapable of working over a network file system because of anything in its implementation. Rather, SQLite is at the mercy of the underlying file system and wire protocol, and those technologies are not always perfect. 

* Although SQLite databases can be shared over network file systems, the latency associated with such file systems can cause performance to suffer. Worse, bugs in network file system implementations can also make opening and modifying remote files—SQLite or otherwise—error prone. If the file system’s locking does not work properly, two clients may be allowed to simultaneously modify the same database file, which will almost certainly result in database corruption. It is not that SQLite is incapable of working over a network file system because of anything in its implementation. Rather, SQLite is at the mercy of the underlying file system and wire protocol, and those technologies are not always perfect. For instance, many versions of NFS have a flawed fcntl() implementation, meaning that locking does not behave as intended. Newer NFS versions, such are Solaris NFS v4, work just fine and reliably implement the requisite locking mechanisms needed by SQLite. However, the SQLite developers have neither the time nor the resources to certify that any given network file system works flawlessly in all cases.  

These limitations are in line with its intended purpose.

* So, there are situations where SQLite is not as fast as larger databases. But many if not all of these conditions are to be expected. SQLite is an embedded database designed for small to medium-sized applications. These limitations are in line with its intended purpose. Many new users make the mistake of assuming that they can use SQLite as a drop-in replacement for larger relational databases. Sometimes you can; sometimes you can’t. It all depends on what you are trying to do. 

In short, what SQLite can’t do is a direct result of what it can.

* Again, most of these limitations are intentional, resulting from SQLite’s design. Supporting high write concurrency, for example, brings with it great deal of complexity, and this runs counter to SQLite’s simplicity in design. Similarly, being an embedded database, SQLite intentionally does not support networking. This should come as no surprise. In short, what SQLite can’t do is a direct result of what it can. It was designed to operate as a modular, simple, compact, and easy-to-use embedded relational database whose code base is within the reach of the programmers using it. And in many respects, it can do what many other databases cannot, such as run in embedded environments where actual power consumption is a limiting factor. 

No matter how good you think your chosen backup approach is, remember you are only as good as your last successful restore. Test your restore procedure if you need to rely on it—otherwise, you’ll be remembered for one failed restore, regardless of how many successful backups you took. 

##SQLite in Rails Application

###Development

For this application, we’ll use the open source SQLite database (which you’ll need if you’re following along with the code). We’re using SQLite version 3 here.

SQLite 3 is the default database for Rails development and was installed along with Rails (in Chapter 1, Installing Rails, on page 3). With SQLite 3 there are no steps required to create a database, and there are no special user accounts or passwords to deal with. So, now you get to experience one of the benefits of going with the flow (or, convention over configuration, as Rails folks say...ad nauseam).

If it’s important to you to use a database server other than SQLite 3, the commands you’ll need to create the database and grant permissions will be different. You will find some helpful hints in the Getting Started Rails Guide.

###Deployment and Production

The SQLite website is refreshingly honest when it comes to describing what this database is good at and what is not good at. In particular, SQLite is not recommended for high-volume, high-concurrency websites with large datasets. And, of course, we want our website to be such a website . 

##Reference

* [Agile Web Development with Rails 4](http://book.douban.com/subject/24718727/)
* [The Definitive Guide to SQLite](http://book.douban.com/subject/5392299/) 
* [SQLite Introduction](http://dylanninin.com/blog/2012/12/19/sqlite.html)
* [为什么会有这么多中的数据库](http://www.aqee.net/what-databases-fix/)
* [What Databases Fix](http://cargocultcoder.blogspot.se/2012/12/what-databases-fix.html)
---
layout: post
title: For Rails
category : Miscellanies
tags : [Java, Ruby, Rails]
---

我是一枚Java攻城狮，工作两年多，主要做Java Web开发，期间转去做了一年多的[Oracle DBA](http://dylanninin.com/blog/2013/10/26/oracle_dba.html)，维护[Oracle EBS](http://dylanninin.com/blog/2013/10/25/oracle_ebs.html)，也当过[Linux Administrator](http://dylanninin.com/blog/2013/10/25/linux.html)。

Sun早就被Oracle收购了，Java和MySQL成了Oracle的小儿子；Oracle Database和EBS自不必说，Oracle的亲亲亲生孩子，庞然大物，组件众多；Oracle推出了基于Redhat的Oracle Linux，
号称Unbreakable，所以Linux Administrator就成了Oracle Linux Administrator。看这趋势，一路朝Oracle走来一路坑，深有被Oracle绑架的感觉，所以希望能够从Oracle中解脱出来，呼吸下
新鲜空气。

最近王垠新写了一篇博文[再见Voxer，你好Sourcegraph](http://www.yinwang.org/blog-cn/2013/11/08/voxer-sg)，讲述他最近的一段经历和想法，顺便小议了下Oracle：“想当年 Oracle 的那些烂东西也是用同样的方式发家的吧”。
这是不是在黑Oracle，我想不是的。在公司刚结束的一个由华南地区Oracle ACS的team主导的Oracle EBS升级项目中，有一位技术深厚、经验丰富的顾问（专攻Oracle Database，经历过8i, 9i, 10g, 11g，现在12c开始着火了）半开玩笑的说道：Oracle Database就是一堆Patch堆积起
来的。这是在食堂排队前的一点闲谈，但不无道理，以不才一年之误尚不足以体会这么深，可顾问该是有些说服力的。

甲骨文叫Oracle，而不取类似“Apache”的名字，是不是很神秘？即使这样，公司每年还是要签Oracle的License，为Oracle测试、提Bug。

在做Oracle DBA时看了阮一峰的[Java语言学校的危险性(译文)](http://www.ruanyifeng.com/blog/2008/12/the_perils_of_javaschools.html)，似乎识迷途其未远；偶尔碰见[Lisp的永恒之道](http://coolshell.cn/articles/7526.html)，编程语言真的会影响人的思维方式；昨天还看了下Hacker News上的提问[Ask HN: Is Java still worth learning?](https://news.ycombinator.com/item?id=6706402)又验证了
某些想法。现在开始觉得是时候换种语言了，在同学、朋友的影响与帮助下，开始了[RoR](http://rubyonrails.org/)之旅。

---

##Java

先简单总结下个人Java Web开发的经历。基本有三个阶段：

* 四叶草网上书店：大二数据库课程设计的项目，用到的技术很基础，JSP, Servlet, JavaBean, 数据库为MySQL。 现在早已不知道代码在哪里，好在当时开了新浪博客[四叶草团队](http://blog.sina.com.cn/s/blog_604d17d30100e0qn.html)，还留有一些可寻的印象，现在看过去文字好生硬。
* 俱乐部会员管理系统：一个暑期的项目，应该是大二结束后的暑假，由IBM教育学院派人进行培训，为其40天。从这起就是开始接触 Java Web开发框架(Struts2, Hibernate, Spring，合称SSH)；那时啥也不知道，学会了一点SSH的使用技巧后，就开始抛弃纯JSP, Servlet, JavaBean的开发了。
* 生活信息分享系统：大四毕业答辩的项目，分微博(集成新浪微博)、房产(模仿搜房)、景点(不记得模仿哪个系统了)和天气预报(集成雅虎天气)四大子系统， 因为那时LBS渐热，所以这些模块都要结合Google Maps，根据我们的想法就把它们硬生生地扯在一起了。技术框架自然是SSH，但需要整合的第三方系统实在太多， 新浪微博(OAuth和开放API)，雅虎天气(Web Service，XML或JSON)，还有Google Maps(开放API)；因为种种原因，系统完成得并不理想，庆幸的是并没有影响大家正常毕业。

工作中接触的Java Web项目，都是采用SSH的基础框架。偶尔还有点变化，前端从Adobe Flex到Ext JS3，Java FX也曾一度惊现，但缺少纯HTML和JavaScript；开始集成Lucene做简单的站内搜索， 或者干脆使用Solr；最近又要集成IM，做基于Openfire、Spark、FastPath的开发；用够了Hibernate，终于开始调研MyBatis；听说PHP和Java可以强强联合，所以又开始尝试Quercus及php-java的桥接... ...

以个人有限的知识和经历，我认为对中小型企业来说SSH框架已经基本够用。作为补充，上个月整理了一份[Java Resource](http://dylanninin.com/blog/2013/10/09/java_resource.html)可供参考。

---

##Rails

以上是从大学到工作至今接触过的一些Java Web开发的技术或框架，它们堆积在一起很吓人，说不上完全掌握，但理解还是有一点的；要说是啃老族，那我啃的还是大学的底，只是不再认为“Java Web == SSH”。

在回头审视这些项目或阅读源代码时，我有经历过一些想法的摩擦和碰撞，而这些想法又体现在Rails中，但Rails做得很好，也很优雅，当之无愧的"魔幻"。

看Rails的第一本书是[Agile Web Development with Rails 4](http://book.douban.com/subject/24718727/)，它除了教你如何认识和使用Rails，也讲了敏捷开发的流程，不得不说让人感受和启发良多。如果要说一句话，我想那会是相见恨晚。


##Java vs Rails

下文将说说这几天学Rails时脑海中时而涌现的念头，作为对Java Web开发的一次小结，也算是对Rails的一种拥抱。

这些想法很基础，可以说任何一个项目都会遇到；但也很重要，我认为只有理解和解决了它们，一个项目的基石才是可靠的。

主要包括：

* 项目结构：Top directory and packages in Java, Rails and Django;
* 命名规范：Naming Conversion; NamingStrategy in Hibernate and why plurals for tables in Rails;
* 日志处理：Log4j in Java and Logger in Rails
* 单元测试：jUnit, DbUnit in Java and Fixtures in Rails
* 构建工具：Ant in Java and Rake in Rails

特别说明：

* 在[Ask HN: Is Java still worth learning?](https://news.ycombinator.com/item?id=6706402)中看到基于Java和Scala的框架[Play Framework](http://www.playframework.com/)，对以上问题似乎解决得也很不错，具体如何，需要试用、[与Rails对比](http://vschart.com/compare/play-framework/vs/ruby-on-rails)才知道；
* 为方便引用，Agile Web Development with Rails 4以下简称AWDR4；
* 因为啃老，下文可能停留在“Java Web == SSH”的水平，欢迎拍砖和评论；
* 个人知识水平有限，若有错误还请大侠们不吝赐教；

---

##项目结构

###目录结构

####Java

Java Web应用并没有明确规定该采用怎样的目录结构，唯一可寻的规范似乎是[WAR file format](http://en.wikipedia.org/wiki/WAR_%28Sun_file_format%29)中提及的[web.xml](http://en.wikipedia.org/wiki/Web.xml)， 其中web.xml的具体内容又由[web-app_2_5](http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd)进行定义。

一个打包好的Java Web应用应有如下的目录结构，以便部署到Tomcat、Weblogic等容器中。

     WebRoot/					    
        css/							
        images/                     
        js/						
        logs/						
        META-INF/						
        WEB-INF/					
            classes/
            lib/				    
            web.xml		
        index.jsp
        404.jsp
        500.jsp

项目开发的目录结构就真没有规范可寻了，最简单的只需要一个src目录和一个WebRoot目录即可搞定；但随着时间推移，或许是半年，一年，也或许是两年，这样的目录结构并不能满足需求。 于是经过调整，可能会发展成这样：

    my_project/
        src/
            java源码包，一般按域名继续划分，防止包冲突，如 com.egolife.ssh；
        test/
            java测试源码包；
        resource/
            各类资源或配置文件，如struts, spring, system properties, freemarker templates, jbpm processes；
        doc/
            项目文档目录；
        lib/        
            测试时需要的额外的库，如jsp,servlet,junit,spring等；运行时需要的lib在WebRoot下；
            单独列出此lib，方便正式部署时排出这些包；
        reports/
            报告目录，如单元测试报告；
        WebRoot/
            Web根目录，包含运行所需的web.xml，lib，classes等；
        build.xml - Ant build文件；

但也可能会是这样：

    my_project/
        src/
            java源码包，一般按域名继续划分，防止包冲突，如 com.egolife.ssh；
        test/
            java测试源码包；
        config/
            各类资源或配置文件，如struts, spring, system properties, freemarker templates, jbpm processes；
        lib/        
            测试时需要的额外的库，如jsp,servlet,junit,spring等；运行时需要的lib在WebRoot下；
            单独列出此lib，方便正式部署时排出这些包；
        sql/ 
            数据库schema，或者实用的script；
        WebRoot/
            Web根目录，包含运行所需的web.xml，lib，classes等；
        
####Rails

Rails框架已经约定好了项目的目录结构，你只需要输入 `rails new my_app`，它就会自动帮你生成，每个文件/目录的定义清晰可见。

    my_app/
        app/
            Model, view, and controller files go here.
        bin/
            Wrapper scripts
        config/
            Configuration and database connection par
        config.ru - Rack server configuration.
        db/
            Schema and migration information.
        Gemfile - Gem Dependencies.
        lib/
            Shared code.
        log/
            Log files produced by your application.
        public/
            Web-accessible directory.
        Rakefile - Build script.
        README.rdoc - Installation and usage information.
        test/
            Unit, functional, and integration tests, fixtures
        tmp/
            Runtime temporary files.
        vendor/
            Imported code.

###包或模块结构

除了项目的顶级目录结构，包或模块结构的划分往往是一件令人头疼的事儿。

最早开始注意到这个问题，还是在11年工作后开发第一个Java Web应用时。那是四个人的小团队针对老系统进行再次开发，抱着动大刀的心态， 总觉得原有项目的包结构十分别扭，修改一个模块时经常需要跨越多个包，而且前端还使用了Adode Flex 3的框架，IDE提供的Refactor功能顿 时失灵了，在修改service的一个方法名、model的一个属性时别提会有多么恶心（深受其害之后动手写了一个小工具，可以批量将Java的Model映射成 Flex的valueObject，copy->paste->fix这种方式实在太ugly）。当然，很有可能是再次开发增强了这种恶心感，手里拿着锤 子什么的，那自然一切都成了钉子。不过，开发第二个Java Web项目时，还是沿用了原有 的包结构，SSH框架，前端由Adobe Flex 3换成了Ext JS3，但明显没有以前那种恶心感。如果这个系统再由一批新人去维护，很有可能他们也会受罪，正如我们一样，只不过这次是我们制造的。

言归正传，还是谈谈Web项目的包或模块结构，先引一篇老文，[JavaEye](http://www.iteye.com)新闻月刊2008年07月总第05期[四个有害的Java习惯之包的命名和划分](http://www.javaeye.com/news/3058)。

####包的命名和划分

包(package)的命名和划分按照行为和层次划分(package-by-layer)，而不是根据特征和功能划分(package-by-feature)

这个问题在我刚学java的时候就遇到了，在看了众多的网上开源程序后，我也慢慢习惯了按层次命名包。

作者举了个例子:

    com.blah.action
    com.blah.dao
    com.blah.model
    com.blah.util

我们已经习惯了按照层次分类或者叫按照行为分类，model一个包，dao一个包，service一个包，action一个 包。这样就把具有同样特征或者功能的类划分到了不同的包里。这样的习惯，把java的包内私有(package- private)这个作用域给完全扔掉了，而包内私有是java的默认作用域（ps:我学java来好像很少用过java的包内 私有这个作用域，汗一个）。

这种包的划分习惯也违反了面向对象编程的核心原则之--尽量保持私有以减少影响,因为这种习惯强迫你必须扩大类的作用域.

下面的包命名方式是按照特征划分命名:

    com.blah.painting
    com.blah.buyer
    com.blah.seller
    com.blah.auction
    com.blah.webmaster
    com.blah.useraccess
    com.blah.util

举个例子，在一个web应用中，`com.blah.painting` 包可能包含下面的成员:

    Painting.java: model对象
    PaintingDAO.java: dao对对象
    PaintingAction.java: controller or action 对象
    statements.sql: SQL文件
    view.jsp: JSP文件

值得注意的是这种情况下，包里包含的不仅仅是java源码文件，同时也包含其他与该特征相关的文件。这点上好像违反大多数java程序员的习惯，并且如果要打包为jar好像也不方便，真实环境中如何应用，有没有别的麻烦，还要待实践一下。

作者列举了这种包划分方式的优点:

* 包是高内聚的，并且模块化，包与包之间的耦合性被降到最低。
* 代码的自文档性(或自描述性 self-documenting)增强. 读者只需看包的名字就对程序有些什么功能或特 征有了大概的印象。在《代码大全》中, Steve McConnell 将自文档化(self-documenting)的代码比作 "the Holy Grail of legibility."(不知道怎么翻译)
* 把类按照每个特征和功能区分开可以很容易实现分层设计。
* 相关的成员在同一个位置。不需要为了编辑一个相关的成员而去浏览整个源码树。
* 成员的作用域默认是包内私有。只有当另外的包需要访问某个成员的时候，才把它修改为public. (需要 注意的是修改一个类为public，并不意味着它的所有类成员都应该改为public。public成员和包内私有 (package-private)成员是可以在同一个类里共存的。)
* 删除一个功能或特征只需要简单的删除一个文件夹。
* 每个包内一般只有很少的成员，这样包可以很自然的按照进化式发展。如果包慢慢变的太大，就可以再 进行细分，把它重构为两个或者更多新的包，类似于物种进化。而按照层次划分的方式，就没办法进化 式发展，重构也不容易。

作者引用了一句Effective Java中的名言:

	"The single most important factor that distinguishes a well-designed module from a poorly designed one is the degree to which the module hides its internal data and other implementation details from other modules." 

	-- Joshua Bloch, Effective Java
    
####Java, Rails and Django

在按照行为和层次划分还是根据特征和功能划分的争论上，我们的Java项目优先选择了前者，在MVC层级的结构之下再按特征和功能划分模块；Rails与之类似。那么，有没有反过来的呢，即正如上文作者提到的建议，先按特征和功能划分，再按行为和层次划分。答案是
Django，在Django 1.5.3 Documentation中提到的项目结构：

    tutorial/
        mysite/
            __init__.py
            settings.py
            urls.py
            wsgi.py
        polls/
            __init__.py
            urls.py
            views.py
            models.py
            admin.py
            tests.py
            static/
                polls/
                    style.css
            templates/
                404.html
                500.html
                polls/
                    detail.html
                    index.html
                    result.html
        hn/
            __init__.py
            urls.py
            views.py
            models.py
            admin.py
            tests.py
            static/
                hn/
                    style.css
            templates/
                404.html
                500.html
                hn/
                    detail.html
                    index.html
                    result.html
        templates/
            admin/
                base_site.html
        manage.py
		
目录结构，包结构十分重要，它们从文件、模块的层次体现着项目的设计水平和权衡，在这方面感觉Ruby和Django要略胜一筹，因为设计或不设计，它就在那里。

---	
	
##命名规范    

对程序猿来说，Coding并非是最难的事情；那是什么？命名！！！对，是命名偷走程序猿的大把时间和精力。

记得在一个OA项目开始初期，整理过一分Java和Ext JS的编码规范，其中命名规范是重点，实践起来实际上是困难重重。想一想，单是找出一个合适的英文单词 似乎就要绞尽脑汁，随意和不杀脑细胞的后果是瞅瞅国内一些开放平台的API就知道他们的英语水平和设计素养；当然，如果你直接使用中文编程就另当别论了。

###Java

Java命名规范举例（参考了AWDR4）

Model Naming

    Table           image_sprite    
    Package         com.egolife.ssh.po.media
    Class           ImageSprite
    File            my_project/src/com/egolife/ssh/po/media/ImageSprite.java
    ORM             my_project/src/com/egolife/ssh/hbm/media/ImageSprite.hbm.xml

Service Naming

    Package         com.egolife.ssh.service.media
    Interface       ImageSpriteService
    Method          ImageSprite findById(String id);
    File            my_project/src/com/egolife/ssh/service/media/ImageSpriteService.java
    
    Class           ImageSpriteServiceImpl
    Method          public ImageSprite findById(Srting id){ ... ... }
    File            my_project/src/com/egolife/ssh/service/media/impl/ImageSpriteServiceImpl.java    
    
    Spring          <bean id="ImageSpriteDAO" class="com.egolife.ssh.dao.media.ImageSpriteDAO">
                        <property name="sessionFactory">
                            <ref bean="sessionFactory" />
                        </property>
                    </bean>
                    
                    <bean id="imageSpriteService" class="com.egolife.ssh.service.media.ImageSpriteService">
                        <property name="imageSpriteDAO" ref="ImageSpriteDAO"/>
                    </bean>
                    
                    <bean  id="imageSpriteAction" class="com.egolife.ssh.action.media.ImageSpriteAction">
                        <property name="imageSpriteService" ref="imageSpriteService"/>
                    </bean>
                    
    File            my_project/resource/spring/applicationContext-media.xml
    
Action Naming

    URL             http://../media/findImageSpriteById.action?id=DEADBEEF
    Package         com.egolife.ssh.action.media
    Class           ImageSpriteAction
    Property        private String id;
    Method          public String findImageSpriteById(){ ... ...}
    File            my_project/src/com/egolife/ssh/action/media/ImageSpriteAction.java    
    
    Struts          <action name="*" class="imageSpriteAction" method="{1}">
                        <result type="json"></result>
                    </action>
    File            my_project/resource/struts/struts-media.xml    

除了Java文件，还涉及到Struts, Hibernate, Spring一堆XML的配置文件，当然，你也可以使用Annotation实现“零配置”。
    
###Rails

Rails命名规范举例（直接引用AWDR4）：All these conventions are shown in the following tables.

Model Naming

    Table           line_items
    File            app/models/line_item.rb
    Class           LintItem

Controller Naming

    URL             http://../store/list
    File            app/controllers/store_controller.rb
    Class           StoreController
    Method          list
    Layout          apps/views/layouts/store.html.erb

View Naming

    URL             http://../store/list
    File            app/views/store/list.html.erb (or .builder)
    Helper          module StoreHelper
    File            app/helpers/store_helper.rb
    
###ORM 命名策略

由于关系数据库与面向对象的不匹配，ORM应运而生。在Java中比较典型的ORM框架是Hibernate，采用XML文件描述Java POJO和数据库表之间的对应关系， 可以让开发人员以面向对象的方式来完成Java与数据库的交互，而不再纠缠于JDBC，写大量的原生SQL和参数的setter/getter；是的，这时最满意的要数程序员的 手指头。在Rails中，ActiveRecord担任了此角色，但省去了XML文件，这时你会更觉满意。

随着ORM的出现，Model和Table的命名问题也逐渐浮出水面：

* Table: line_item, line_items, or tb_line_items ?
* Id:    id, line_item_id, or line_items_id ?
* Model: LineItem or LineItems ?

在以SSH框架为基础的Java Web项目开发中，一般有两种选择：

* ddl2java：先进行数据库设计，然后通过逆向工程生成Java代码，包括Model, XML, DAO, 甚至是Service。此时Java层的命名由数据库层决定。
* java2ddl: 标准的面向对象设计，手写Model, XML，或直接使用Annotation，然后通过正向工程生成数据库DDL。此时数据库层的命名由Java层决定。

在接触过的项目中，以ddl2java的选择居多，逆向工程可以帮忙生成很多代码，这是自底向上的设计思路。

无论是ddl2java，还是java2ddl，都有一个叫[NamingStrategy](http://docs.jboss.org/hibernate/orm/3.2/api/org/hibernate/cfg/NamingStrategy.html)的工具 在帮你进行一些默认的转换，毕竟Java和数据库各有各的命名规范，迁就哪一个都觉得有所不妥。

在Rails中同样有NamingStragety，只不过这是一个很隐匿的家伙，约定俗成以至于不需要它了。

关于Model和Table的命名问题，最大的要数单数还是复数了。Java Web开发中暂未确定，随开发人员高兴，但一般还是挺一致的，要么都采用单数，要么都采用复数， 但这样又似乎确凿有些别扭：

* Singular: User(Class) -> user(Table)，表user有多条记录，每个记录代表一个User，也就是多个User，这样表名用user是否有些不妥？
* Plurals : Users(Class) -> users(Table)，表users有多条记录，每个记录代表一个User，似乎比上面好多了。但1）根据id查找用户时，应返回null或者User，此时却是Users，是多个用户吗，有点奇怪？ 2）模糊查找时返回List，此时却`List<Users>`，Users不是已经是复数了吗，这时你不会有一种使用foreach嵌套循环的冲动？这样似乎也有些不妥？

终极方案似乎是这样的：

* Mixing: User(Class) -> users(Table)，但此时需要你自定义Hibernate的NamingStragety，谁有时间去干这事儿；再说满足需求就行了，这事儿谁在意呢。

这问题在AWDR4中解释得很好，摘抄如下：
    
	David says: Why Plurals for Tables?

	Because it sounds good in conversation. Really. “Select a Product from products.” And “Order has_many :line_items.”

	The intent is to bridge programming and conversation by creating a domain language that can be shared by both. Having such a language means cutting down on the mental translation that otherwise confuses the discussion of a product description with the client when it’s really implemented as merchandise body. These communica- tions gaps are bound to lead to errors.

	Rails sweetens the deal by giving you most of the configuration for free if you follow the standard conventions. Developers are thus rewarded for doing the right thing, so it’s less about giving up “your ways” and more about getting productivity for free.

---
	
##日志处理

先讲一个笑话，第一次见到[Log4j](http://logging.apache.org/log4j/)，你会怎么读？"Log|Four|J"，还是"Log|四|J"？饱受中文和阿拉伯汉子熏陶的Java程序猿，若是第一次碰到，不出意外，大都 会读错，即读成 "Log|四|J"，在我不长的Java开发生涯里，已经碰到好几例。

即使联想到"J"是"Java"的缩写，也难以想到"4"代表什么；但若知道这世上还有Log4cxx，Log4php，那大概可以猜到"4"其实代表"For"，只是"LogForJava"写起来太长了，"Java"缩写成"J"，"For"干脆就用"4"代替了；更进一步，可以说Log4j所代表的是一种思想、一种设计，只不过刚好是Java语言的实现。 这和"B2B"、"B2C"是一个道理，但君不见也有某主持人把"B2B"读成"B二B"的。

调试Java程序，难免会有打印输出，使用`System.out.println(message)`和`System.err.println(error)`就可以满足需求；但你一定干过这事儿：1）第一版的代码里为了调试写满println，调试好后觉得输出的无关信息太多，看着怪怪的，手痒了就非常勤快地把它们删掉；但使用时还是莫名地出错，于是又手贱地再写了一遍`println`。 2）正式上线前，经理突然说把项目里的调试代码注释掉，以免影响运行的性能；好家伙，这一天没写一行代码，反倒删除了几千行。

这时，Log4j可以帮你免除手痒手贱之苦。使用Log4j可以自定义输出级别，方便控制项目在测试阶段、上线运行时的日志输出；你也可以在日志输出到标准输出的同时，保存一份到文件或数据库中，以便定时查看和统计分析；碰到Error/Fatal类的错误时，发送一封日志邮件给开发维护人员也很不错，何乐而不为呢。

Log4j是很多Java程序里默认的日志组件，在Hibernate逆向工程生成的DAO里即可找到它的身影。看起来很不错，但要在自己的Java代码用到，还是得稍微花点功夫。这时你需要引入Log4j的配置，熟悉rootLogger、LogLevel、Appender、Layout等概念，若有兴趣，似乎也想看下Log4j的设计原理和具体实现，趁机“Pick Up”一些设计模式， 然后为己所用。不仔细看文档，直接上配置和代码的人可能要折腾一小会儿了，好在打造一份Log4j.properties之后，就可以随你“Copy and Paste”。

Rails中有类似的日志组件，叫Logger -- supporting rollover，零配置即可用；当然了，遇到项目上线运行时还是需要定制下的。只是在此不得不感叹下，在Rails中使用Logger似乎出乎天性、理所当然，但Log4j却要一些折腾，一些配置。

Gists：

* [Log4j Gist](https://gist.github.com/dylanninin/7426118)

---

##单元测试

在最初的Java Web开发里，是没有单元测试的概念的，即使到现在开发人员也依然习惯说：功能我写好了，你看！

jUnit是什么？为什么要单元测试？即使我看了jUnit in Action，理解测试的重要性，也不见得会按部就班的进行严格的单元测试。

在Java Web项目中，可以借助IDE自动生成单元测试用例，虽然功能有限，但也省了不少时间（如果连这个功能都不知道，那可以说你还生活在原始时代）。 出于对IDE自动生成的单元测试用例的不满，自己写过两个版本的小工具。一个是Java版的，粗野的将文本扫描和正则表达式相结合，渲染freeMarker模板，批量生成 TestCase和Tests，并自动填充Parameters和Asserts，接下来所要做的工作就是更改Parameters和Asserts，并运行测试即可。第二个是Python版的，最初是想用Java重写，但还是 脱离不了扫描源文件（想过使用反射来实现，但看Java提供的反射API，运行时源代码中定义的参数名丢失了，实在不忍用arg1, arg2来代替start, pageSize），当时正好在学Python， 所以改用Python重新实现。代码只能捂脸见人，所以就不贴出来了，思路在模板，见[jUnit TestCase Gist](https://gist.github.com/dylanninin/7426041)。

对涉及到数据交互的单元测试，jUnit已经不够用了。当时找到一个叫DbUnit的小框架，是jUnit的扩展，可以用XML事先构建一些测试数据集，在单元测试的时候载入数据库， 测试完成后从数据库中清除，有效的控制了测试数据库的状态。使用DbUnit大概有两个好处：1）构建测试数据集，可重复测试，再也不用担心测试数据被修改或丢失；2）测试完成后清除测试数据，保证了测试前后数据库的数据是一致的。
    
当然，使用jUnit已经很折腾，加上DbUnit，好像又得花几天的时间了。

那么，Rails中是怎么解决的呢？

Rails提供了scaffold和generator，可以自动生成测试用例，而且把我想通过编写小工具去实现的功能完成得更加优雅；在构建测试数据时，Rails一致地选择了YAML，对了，这应该称作Fixtures，你可以在test/fixtures目录下找到它们。

YAML是什么呢，YAML is a recursive acronym for "YAML Ain't Markup Languade". Early in its development, YAML was said to mean "Yet Another Markup Language", but it was then reinterpreted(backronyming the original acronym) to distinguish its purpose as data-oriented, rather than document markup. 详情请见[Wikipedia: YAML](http://en.wikipedia.org/wiki/YAM)。

为什么不使用XML，因为YAML面向数据、自解释，你再也不用写一堆标签和属性。

这还不是最重要的，最重要的是，在这种内在的测试支持的帮助下，你会很乐于做这件事而无需额外折腾，正文AWDR4文中所言“One of the real joys of the Rails framework is that it has support for testing baked right in from the start of every project.”

##构建工具

Java中用于自动化构建的工具有Ant和Maven。Ant起步比较麻烦，需要手写一大堆东西；Maven无需配置，可直接上手使用。但Maven的网络特性更强一些，在隔离互联网的内网开发环境下是难以生存的，所以要用也只用到Ant。

Ant的介绍：Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. Ant supplies a number of built-in tasks allowing to compile, assemble, test and run Java applications. Ant can also be used effectively to build non Java applications, for instance C or C++ applications. More generally, Ant can be used to pilot any type of process which can be described in terms of targets and tasks.

Ant可以编译、打包、测试和运行Java程序，但不限于此，你还可以编写自己的扩展，加入到Ant Lib中，实现更多更丰富的任务，比如创建Java项目的目录和包结构；一些开源的项目也提供了Ant Lib，比如Hibernate，jUnit。

从头开始写一个Ant配置有点麻烦，但写好之后基本可以复用，有不一样的地方只需根据项目需求进行调整即可，这样似乎有点一劳永逸了，也似乎有点不够优雅，更别提创建一个新项目的时候你还记得有这事儿吗？踩进与Ant有点关系的深坑，要数实习期间的一个Web项目，前端使用Adobe Flex 3， 在给MyEclipse装上一大堆插件之后，编译Flex代码变得奇慢无比；若是你不小心勾选了自动编译的选项，Flex代码的任何一点改动都可以让你等上一二十分钟。这时想到了Ant，查到Ant和Adobe Flex的官网，确实提供有编译Flex的Lib，也有一些教程，接下来就是测试和移植工作了。那会儿顿时觉得很兴奋， 希望一天内就能完成，实际上零零散散花了一周多的时间还没搞定，记得好像是某些style资源始终无法编译到SWF，最后就不了了之。

Gists：

* [Ant Gist](https://gist.github.com/dylanninin/7425918)
* [Flex with Ant Gist](https://gist.github.com/dylanninin/7425972)

那么Rails表现如何呢？Rails提供了Rake，比起Ant需从头编写build.xml以及使用Flex Task时遇到的困难，Rake更像Maven，内置了很多任务，无需配置；到目前为止，我还没踩进Rake的坑。

Rake的介绍：Rake is like having a reliable assistant on hand all the time: you tell it to do some task, and that task gets done. 

Rake在你键入"rails new my_apps"时，已经陪伴你左右，随时待命。见[Rake Gist](https://gist.github.com/dylanninin/7431140)。

是的，Rake是一个值得信赖的好助手，随时随地就在身边供你差遣；而Ant需要你手工打造，好与坏完全取决于自身的水平。

##参考

* [Agile Web Development with Rails 4](http://book.douban.com/subject/24718727/)
* [四个有害的Java习惯之包的命名和划分](http://www.javaeye.com/news/3058)

##死锁

在事务运行高峰期，由于DDL、DML使用锁资源十分频繁，争用时甚至会发生死锁，阻塞的锁可能导致系统延迟，甚至长时间不响应，影响业务系统的正常运行。若死锁频频发生，则有可能是应用设计不当。这里暂且初步整理下锁争用的有关资料。

###1.锁介绍

####概述

锁：并发访问、更新时容易出现

概念：当数据库对象正在被其他进程或用户修改时，可以保护它不被修改 		

结构：“排队”的队列，先到先服务，串行工作

功能：

* 坚持一致性和完整性
* 队列结构管理请求的会话
* 自动处理锁机制
* 锁的持续时间等于被提交事务的长度或处理时间

####类型

DDL

* 在修改过程中保护模式对象
* DDL锁，由Oracle自动发布和释放

DML

* 在事务处理过程中保护对象
* 当发布rollback或commit时，认为完成了事务

内部锁

* 由Oracle管理，以保护内部数据库结构，如数据文件

####模式

锁模式描述 ...
 
锁模式和DML语句 ...

锁模式和DDL语句 ...

####级别

#####数据库级别

锁定数据库以禁止任何新会话和新事务，可以分为以下三种类型。

	ALTER SYSTEM ENABLE RESTRICTED SESSION
	
	* 使数据库进入限制模式
	* 仅有RESTRICTED SESSION权限用户才能登陆
	* 已经登陆的会话不受此印象
	* 登陆的用户可以正常进行DDL/DML操作
	
	ALTER SYSTEM QUIESCE RESTRICTED
	
	* 使数据库进入静默的限制模式
	* 从用户的活动中锁定数据库，锁定所有操作（处于等待状态），直到UNQUICESCE
	* 不允许非特权用户登陆
	* 特权用户可以正常进行操作
	
	ALTER DATABASE OPEN READ ONLY
	
	* 使数据库以只读模式打开
	* UNDO段处于脱机状态
	* 不允许任何更新、事务操作

#####表级别

* 通过DML或者LOCK语句发出

#####行级别

* DML更新一行或多行时
* Oracle支持的最低级别
* SELECT ... FOR UPDATE只锁定返回的行

#####列级别

* Oracle不支持

####锁语句

LOCK语法

	 LOCK TABLE
	   [ schema.]{table | view} [@dblink]
	   [, [schema.]{table | view} [@dblink] ] ...
	 IN lockmode MODE
	   [ NOWAIT]

示例

	* SHARE(S)
	* ROW SHARE(RS)
	* ROW EXCLUSIVE(RX)
	* SHARE ROW EXCLUSIVE(SRX)
	* EXCLUSIVE(X)
	* SELECT ... FOR UPDATE

###2.死锁日志

####警告日志

	Wed Feb 27 09:34:58 2013
	ORA-000060: Deadlock detected. More info in file /u1/PROD/prodora/proddb/9.2.0/admin/PROD_erpprod/udump/prod_ora_4513878.trc.

####跟踪日志

	... ...
	*** 2013-02-11 03:01:37.846
	*** SESSION ID:(25.18) 2013-02-11 03:01:37.846
	Undo Segment 11 Onlined 
	*** 2013-02-11 08:41:27.866 
	Undo Segment 19 Onlined 
	*** 2013-02-16 08:46:16.482
	Undo Segment 134 Onlined
	*** 2013-02-16 09:55:31.314
	Undo Segment 195 Onlined
	*** 2013-02-27 09:34:58.033
	DEADLOCK DETECTED
	Current SQL statement for this session:
	UPDATE FND_CONCURRENT_REQUESTS SET PP_END_DATE = SYSDATE, POST_REQUEST_STATUS = 'E' WHERE REQUEST_ID = :B1 
	----- PL/SQL Call Stack -----
	  object      			line  		object
	  handle    			number  	name
	700000095f756d0        		67  	package body APPS.FND_CP_OPP_CMD
	70000008caf2ea0         	1  		anonymous block
	The following deadlock is not an ORACLE error. It is a
	deadlock due to user error in the design of an application
	or from issuing incorrect ad-hoc SQL. The following
	information may aid in determining the deadlock:
	Deadlock graph:
	                       ---------Blocker(s)--------  ---------Waiter(s)---------
	Resource Name          process session holds waits  process session holds waits
	TX-0159001e-000075b3        24      25     X             24      25           X
	session 25: DID 0001-0018-00000006	session 25: DID 0001-0018-00000006
	Rows waited on:
	Session 25: obj - rowid = 00009050 - AAAJBQAAmAAAKPzAAG
	  (dictionary objn - 36944, file - 38, block - 41971, slot - 6)
	Information on the OTHER waiting sessions:
	End of information on OTHER waiting sessions.
	... ...

###3.死锁情形模拟

若在请求锁资源时出现循环等待，则产生死锁现象。如下，以列出顺序先后执行更新操作。

1.获取并保持锁，如LOCK1，且为排他锁，锁定更新行1：
	
	SQL(49,180)>update dt_char set name = '1A' where id = 1;
	
	1 row updated.

2.获取并保持锁，如LOCK2，且为排他锁，锁定更新行2：	

	SQL(462,13)> update dt_char set name = '2B' where id = 2;
	
	1 row updated.

3.请求更新行2，因处于等待，等待锁LOCK2

	SQL(49,180)>update dt_char set name = '2A' where id = 2;
	
4.请求更新行1，因处于等待，等待锁LOCK1
	
	SQL(462,13)> update dt_char set name = '1B' where id = 1;

此时，会话(49,180)和(462,13)在锁资源LOCK1和LOCK2上存在循环等待的情形，故出现死锁,3中的语句终止；此时4中的语句处于等待状态。

	SQL(49,180)>update dt_char set name = '2A' where id = 2;
	update dt_char set name = '2A' where id = 2
	*
	ERROR at line 1:
	ORA-00060: deadlock detected while waiting for resource

出现死锁时，系统会


###4.死锁解决方案

**ORA-00060 deadlock detected while waiting for resource**

**Cause**:  Your session and another session are waiting for a resource locked by the other. This condition is known as a deadlock. To resolve the deadlock, one or more statements were rolled back for the other session to continue work.

**Action**:  Either:

*  Enter a ROLLBACK statement and re-execute all statements since the last commit or
* Wait until the lock is released, possibly a few minutes, and then re-execute the rolled back statements.

除了执行ROLLBACK，或者等待锁资源释放，并重新执行所需要的语句外；还有一种方案就是清理掉死锁中阻塞的一方的数据库会话。

	--生成kill session脚本，杀掉死锁中阻塞一方的会话
	--ALTER SYSTEM KILL SESSION 'sid,serial#';
	SELECT 'alter system kill session ''' || SID || ',' || SERIAL# || ''';' "Deadlock"
	  FROM V$SESSION
	 WHERE SID IN (SELECT SID FROM V$LOCK WHERE BLOCK = 1);
	
- - -

##DDL Lock

DDL语句在发布时为获取数据对象的内部结构而需要排他锁，如果锁不可用则会更新失败。

###1.直接发布ALTER TABLE语句

建立`SQL*Plus`会话(27，0），发出ALTER TABLE语句：

	SQL> SELECT * FROM V$MYSTAT WHERE ROWNUM = 1;

	       SID STATISTIC#	   VALUE
	---------- ---------- ----------
		27	    0	       0

	SQL> desc dt_char;
	 Name					   Null?    Type
	 ----------------------------------------- -------- ----------------------------
	 NAME						    CHAR(10)
	
	SQL> alter table dt_char modify (name varchar(20));
	
	Table altered.
	
	SQL> desc dt_char;
	 Name					   Null?    Type
	 ----------------------------------------- -------- ----------------------------
	 NAME						    VARCHAR2(20)

###2.有DML更新表数据时发布DDL语句

####默认

新开一个`SQL*Plus`会话(513,22540)，发出UPDATE语句：

	SQL> SELECT * FROM V$MYSTAT WHERE ROWNUM = 1;

	       SID STATISTIC#	   VALUE
	---------- ---------- ----------
		513	    22540	       0

	SQL> update dt_char set name = 'DDL Lock';

	0 rows updated.

此时，在会话(27,0)中再次发布ALTER TABLE语句：

	SQL> alter table dt_char modify (name char(10));
	alter table dt_char modify (name char(10))
	            *
	ERROR at line 1:
	ORA-00054: resource busy and acquire with NOWAIT specified or timeout expired

此时，提示要求更新的表结构正处于繁忙状态(被其他会话占用)，因不愿意等待锁释放或者等待超时，故更新失败。

查看哪些对象处于争用之中：

	SQL> SELECT lo.session_id,
       s.serial#,
       lo.oracle_username,
       o.owner,
       o.object_name,
       o.object_type,
       decode(lo.locked_mode,
              0,
              'None', /* Mon Lock equivalent */
              1,
              'Null', /* N */
              2,
              'Row-S (SS)', /* L */
              3,
              'Row-X (SX)', /* R */
              4,
              'Share', /* S */
              5,
              'S/Row-X (SSX)', /* C */
              6,
              'Exclusive', /* X */
              lo.locked_mode) locked_mode
  	FROM v$locked_object lo,
	       dba_objects     o,
	       v$session       s
	 WHERE lo.object_id = o.object_id
	   AND s.sid = lo.session_id;
	
   	SESSION_ID 	  SERIAL# ORACLE_USERNAME		OWNER  OBJECT_NAME  OBJECT_TYPE	LOCKED_MODE
	---------- ---------- ---------------- ---------- ------------ ------------ -----------
		513			22540 DEV			   DEV		  DT_CHAR	   TABLE		Row-X (SX)

从以上查询可以看出，表DEV.DT_CHAR正在被会话(513.22540)占用，锁模式为Row-X，即排他锁，为了更新，在某个事务中锁定了行，则不允许其他事务锁定这个表。

在这个例子中，发布的DML后，已经获取了锁，并锁定了表DEV.DT_CHAR，因没有发布ROLLBACK、COMMIT，该会话一直持有锁资源。

####指定TIMEOUT

在会话(27，0）中，发布超时为30s:

	SQL@（27，0）> alter session set ddl_lock_timeout=30;		--DDL_LOCK_TIMEOUT参数值默认为0
	
	Session altered.
	
	SQL@（27，0）> alter table dt_char modify (name char(10));	--处于等待状态，超时30s

此时，若在发布DDL时起30s内，会话(513,22540)释放锁资源，则DDL可以更新成功：

	SQL@(513,22540)> commit;

	Commit complete.

	SQL@（27，0）> alter table dt_char modify (name char(10));	--处于等待状态，超时30s
	
	Table altered.

否则，DDL失败：

	SQL@（27，0）> alter table dt_char modify (name char(10));	--处于等待状态，超时30s
	alter table dt_char modify (name char(10))
	            *
	ERROR at line 1:
	ORA-00054: resource busy and acquire with NOWAIT specified or timeout expired

- - -

##DML Lock

###1.事务和保存点

原始数据：

	SQL> select * from dt_char;
	
	NAME
	----------
	1
	2
	3
	4
	5
	
更新行：

	SQL> update dt_char set name = '1A' where name = '1';
	
	1 row updated.

	SQL> select * from dt_char;
	
	NAME
	----------
	1A
	2
	3
	4
	5

创建保存点，用来控制事务	

	SQL> savepoint A;
	
	Savepoint created.
	
更新行：

	SQL> update dt_char set name = '2A' where name = '2';
	
	1 row updated.
	
	SQL> select * from dt_char;
	
	NAME
	----------
	1A
	2A
	3
	4
	5

回滚到保存点A，此时在A之前的锁没有释放，在A之后的锁则被释放，更新将会回滚到保存A（若不指定保存点，则回滚到更新的初始状态）：	

	SQL> rollback to savepoint A;
	
	Rollback complete.
	
	SQL> select * from dt_char;
	
	NAME
	----------
	1A
	2
	3
	4
	5

##常用脚本

查询当前会话ID

	--查询当前会话ID
	SELECT * FROM V$MYSTAT WHERE ROWNUM = 1;

查询会话锁

	--查询会话锁
	SELECT * FROM DBA_LOCKS WHERE SESSION_ID IN (513);
	

检测锁争用

	--request:如果request值非0，则表示在等待一个锁
	--block:如果block为1，则表示此会话持有一个锁，并阻塞别人获得此锁
	SELECT *
	  FROM V$LOCK
	 WHERE BLOCK = 1
	    OR REQUEST > 0;
	
被锁定的对象

	--被锁定的对象
	SELECT * FROM V$LOCKED_OBJECT;
	
仅列出用户所保持的锁

	--仅列出用户所保持的锁
	SELECT S.USERNAME,
	       L.SESSION_ID,
	       L.LOCK_TYPE,
	       L.MODE_HELD,
	       L.MODE_REQUESTED,
	       L.LOCK_ID1,
	       L.LOCK_ID2,
	       L.LAST_CONVERT,
	       L.BLOCKING_OTHERS
	  FROM V$SESSION S, DBA_LOCKS L
	 WHERE S.USERNAME IS NOT NULL
	   AND (S.SID = L.SESSION_ID AND L.MODE_REQUESTED != 'NONE')
	    OR (S.SID = L.SESSION_ID AND L.MODE_REQUESTED = 'NONE' AND
	       L.MODE_HELD != 'Share' AND
	       (LOCK_ID1, LOCK_ID2) IN
	       (SELECT A.LOCK_ID1, A.LOCK_ID2
	           FROM DBA_LOCKS A
	          WHERE A.MODE_REQUESTED != 'NONE'
	            AND A.LOCK_ID1 = L.LOCK_ID1
	            AND A.LOCK_ID2 = L.LOCK_ID2))
	 ORDER BY 6, 7, 5;
	
哪些对象处于争用中

	--v$locked_object
	--哪些对象处于争用中
	SELECT LO.SESSION_ID,
	       S.SERIAL#,
	       LO.ORACLE_USERNAME,
	       S.PROCESS,
	       S.PROGRAM,
	       O.OWNER,
	       O.OBJECT_NAME,
	       O.OBJECT_TYPE,
	       DECODE(LO.LOCKED_MODE,
	              0,
	              'None', /* Mon Lock equivalent */
	              1,
	              'Null', /* N */
	              2,
	              'Row-S (SS)', /* L */
	              3,
	              'Row-X (SX)', /* R */
	              4,
	              'Share', /* S */
	              5,
	              'S/Row-X (SSX)', /* C */
	              6,
	              'Exclusive', /* X */
	              LO.LOCKED_MODE) LOCKED_MODE
	  FROM V$LOCKED_OBJECT LO, DBA_OBJECTS O, V$SESSION S
	 WHERE LO.OBJECT_ID = O.OBJECT_ID
	   AND S.SID = LO.SESSION_ID
	   AND S.SID IN (336, 1360);
	
死锁检测

	--dba_blockers
	--阻塞其他用户的会话的id
	SELECT * FROM DBA_BLOCKERS;
	
	--dba_waiters
	--显示等待将被阻塞会话释放锁的会话id
	SELECT * FROM DBA_WAITERS;

	--使用dba_waiters视图
	信息格式：Blocker|Waiter(sid,serial#,username,sql)
	SELECT 'Blocker(' || BW.HOLDING_SESSION || ':' || SB.USERNAME ||
	       ') - SQL: ' || BQ.SQL_TEXT BLOCKERS,
	       'Waiter(' || BW.WAITING_SESSION || ':' || SW.USERNAME || ') - SQL: ' ||
	       WQ.SQL_TEXT BLOCKERS
	  FROM DBA_WAITERS BW,
	       V$SESSION   SB,
	       V$SESSION   SW,
	       V$SQLAREA   BQ,
	       V$SQLAREA   WQ
	 WHERE BW.HOLDING_SESSION = SB.SID
	   AND BW.WAITING_SESSION = SW.SID
	   AND SB.PREV_SQL_ADDR = BQ.ADDRESS
	   AND SW.SQL_ADDRESS = WQ.ADDRESS
	   AND BW.MODE_HELD <> 'None';

	--使用dba_locks视图
	--信息格式：Blocker|Waiter(sid,serial#,username,sql)
	SELECT DISTINCT 'Blocker(' || LB.SESSION_ID || ',' || SB.SERIAL# || ':' ||
	                SB.USERNAME || ') - SQL: ' || BQ.SQL_TEXT BLOCKERS,
	                'Waiter(' || LW.SESSION_ID || ',' || SW.SERIAL# ||
	                SW.USERNAME || ') - SQL: ' || WQ.SQL_TEXT BLOCKERS
	  FROM DBA_LOCKS LB,
	       V$SESSION SB,
	       DBA_LOCKS LW,
	       V$SESSION SW,
	       V$SQL     BQ,
	       V$SQL     WQ
	 WHERE LB.SESSION_ID = SB.SID
	   AND LW.SESSION_ID = SW.SID
	   AND SB.PREV_SQL_ADDR = BQ.ADDRESS
	   AND SW.SQL_ADDRESS = WQ.ADDRESS
	   AND LB.LOCK_ID1 = LW.LOCK_ID1
	   AND SW.LOCKWAIT IS NOT NULL
	   AND SB.LOCKWAIT IS NULL
	   AND LB.BLOCKING_OTHERS = 'Blocking';


生成kill session脚本

	--生成kill session脚本，杀掉死锁中阻塞一方的会话
	--ALTER SYSTEM KILL SESSION 'sid,serial#';
	SELECT 'alter system kill session ''' || SID || ',' || SERIAL# || ''';' "Deadlock"
	  FROM V$SESSION
	 WHERE SID IN (SELECT SID FROM V$LOCK WHERE BLOCK = 1);


查看导致死锁的SQL	

	--导致死锁的SQL 
	SELECT S.SID, Q.SQL_TEXT
	  FROM V$SQLTEXT Q, V$SESSION S
	 WHERE Q.ADDRESS = S.SQL_ADDRESS
	   AND S.SID = &SID
	 ORDER BY PIECE;

##延伸阅读

* [锁 死锁 阻塞 Latch 等待](http://blog.csdn.net/tianlesoftware/article/details/5822674)

##Reference

* Database Error Message
* Oracle 9i数据库性能优化与调整
##gzip

gzip 可以说是应用度最广的压缩命令了！目前 gzip 可以解开 compress, zip 与 gzip 等软件所压缩的文件。

###gzip help

	[root@server Workspace]# gzip -h
	Usage: gzip [OPTION]... [FILE]...
	Compress or uncompress FILEs (by default, compress FILES in-place).
	
	Mandatory arguments to long options are mandatory for short options too.
	
	  -c, --stdout      write on standard output, keep original files unchanged
	  -d, --decompress  decompress
	  -f, --force       force overwrite of output file and compress links
	  -h, --help        give this help
	  -l, --list        list compressed file contents
	  -L, --license     display software license
	  -n, --no-name     do not save or restore the original name and time stamp
	  -N, --name        save or restore the original name and time stamp
	  -q, --quiet       suppress all warnings
	  -r, --recursive   operate recursively on directories
	  -S, --suffix=SUF  use suffix SUF on compressed files
	  -t, --test        test compressed file integrity
	  -v, --verbose     verbose mode
	  -V, --version     display version number
	  -1, --fast        compress faster
	  -9, --best        compress better
	    --rsyncable   Make rsync-friendly archive
	
	With no FILE, or when FILE is -, read standard input.
	
	Report bugs to <bug-gzip@gnu.org>.

###用法

压缩

	[test@server ~]$ gzip -v netstat.log 
	netstat.log:	 93.0% -- replaced with netstat.log.gz
	[test@server ~]$ ll
	total 4
	-rw-rw-r-- 1 test test 1868 Mar 11 17:11 netstat.log.gz

继续压缩

	[test@server ~]$ gzip -v netstat.log.gz 
	gzip: netstat.log.gz already has .gz suffix -- unchanged
	[test@server ~]$ gzip -v -S .gz netstat.log.gz -c > netstat.log.gz.gz
	netstat.log.gz:	  0.2%
	[test@server ~]$ ll
	total 8
	-rw-rw-r-- 1 test test 1799 Mar 11 16:12 netstat.log.gz
	-rw-rw-r-- 1 test test 1837 Mar 11 16:17 netstat.log.gz.gz

解压缩

	[test@server ~]$ gzip -v -d netstat.log.gz 
	netstat.log.gz:	 93.0% -- replaced with netstat.log

测试完整性
	
	[test@server ~]$ gzip -v -t netstat.log.gz 
	netstat.log.gz:	 OK

	[test@server ~]$ touch dummy.gz;gzip -v -t dummy.gz

	gzip: dummy.gz: unexpected end of file

列表文件

	[test@server ~]$ gzip -v -l netstat.log.gz 
	method  crc     date  time           compressed        uncompressed  ratio uncompressed_name
	defla 73d6b845 Mar 11 16:12                1799               25013  93.0% netstat.log

###特点

1.仅能对单个文件（即输入一个文件，输出一个文件）

	[test@server ~]$ mkdir -p foo/bar
	[test@server ~]$ free > foo/free.log
	[test@server ~]$ netstat > foo/bar/netstat.log
	[test@server ~]$ tree .
	.
	└── foo
	    ├── bar
	    │   └── netstat.log
	    └── free.log
	
	2 directories, 2 files
	[test@server ~]$ gzip foo/
	gzip: foo/ is a directory -- ignored
	[test@server ~]$ gzip foo/*
	gzip: foo/bar is a directory -- ignored

2.通过递归，支持对文件夹的压缩(`gzip -r`)，但解压缩需单独一一进行

	[test@server ~]$ gzip -v -r foo/
	foo//bar/netstat.log:	 92.5% -- replaced with foo//bar/netstat.log.gz
	foo//free.log:	 44.8% -- replaced with foo//free.log.gz

3.默认情况下，gzip压缩后的文件后缀名为.gz(`gzip -S ".your_suffix"`)

	[test@server ~]$ gzip -v -S ".gzip" passwd 
	passwd:	 61.4% -- replaced with passwd.gzip

gzip默认后缀为.gz，当更改后缀名后，解压时，需要指定后缀名

	[test@server ~]$ gzip -v -d passwd.gzip 
	gzip: passwd.gzip: unknown suffix -- ignored
	[test@server ~]$ gzip -v -d -S ".gzip" passwd.gzip 
	passwd.gzip:	 61.4% -- replaced with passwd

4.压缩、解压缩时，保留原有文件权限

压缩前，文件权限

	[root@server ~]# ll
	total 4
	-rw-r--r-- 1 root root   5 Mar 11 16:49 whoami.log

更改权限为777

	[root@server ~]# chmod 777 whoami.log 
	[root@server ~]# chown test:test whoami.log 

压缩后，文件权限

	[root@server ~]# gzip -v whoami.log 
	whoami.log:	120.0% -- replaced with whoami.log.gz
	[root@server ~]# ll
	total 4
	-rwxrwxrwx 1 test test  36 Mar 11 16:49 whoami.log.gz

解压后，文件权限

	[root@server ~]# gzip -v -d whoami.log.gz 
	whoami.log.gz:	120.0% -- replaced with whoami.log
	[root@server ~]# ll
	total 4
	-rwxrwxrwx 1 test test   5 Mar 11 16:49 whoami.log

注：在使用 gzip -c 配合 > 重定向时，相当于创建文件，此时文件权限为于当前用户设定的权限。

5.压缩时，默认状态下，原文件会被压缩后的文件替换；解压时类似

此时，可以使用`gzip -c > your_file_name`来重定向输出文件，同时保存原有文件。不指定文件时，默认会输出到标准输出stdout上。

压缩

	[test@server ~]$ gzip -v -S .gz netstat.log.gz -c > netstat.log.gz.gz
	netstat.log.gz:	  0.2%
	[test@server ~]$ ll
	total 8
	-rw-rw-r-- 1 test test 1799 Mar 11 16:12 netstat.log.gz
	-rw-rw-r-- 1 test test 1837 Mar 11 16:17 netstat.log.gz.gz

解压缩

	[root@server ~]# gzip -vdc whoami.log.gz > whoami
	whoami.log.gz:	120.0%
	[root@server Workspace]# ll
	total 8
	-rw-r--r-- 1 root root  5 Mar 11 17:08 whoami
	-rw-r--r-- 1 test test 36 Mar 11 17:04 whoami.log.gz


6.压缩后的文本文件可以直接使用zcat读出(zcat your_file_name.gz)

	[test@server ~]$ zcat netstat.log.gz 
	Active Internet connections (w/o servers)
	Proto Recv-Q Send-Q Local Address               Foreign Address             State  
	... ...

##bzip2

若说 gzip 是为了取代 compress 并提供更好的压缩比而成立的，那么 bzip2 则是为了取代 gzip 并提供更佳的压缩比而来的。 bzip2 真是很不错用的东西～这玩意的压缩比竟然比 gzip 还要好～至於 bzip2 的用法几乎与 gzip 相同！

##bzip2 help

	[root@server svn]# bzip2 --help
	bzip2, a block-sorting file compressor.  Version 1.0.5, 10-Dec-2007.
	
	   usage: bzip2 [flags and input files in any order]
	
	   -h --help           print this message
	   -d --decompress     force decompression
	   -z --compress       force compression
	   -k --keep           keep (don't delete) input files
	   -f --force          overwrite existing output files
	   -t --test           test compressed file integrity
	   -c --stdout         output to standard out
	   -q --quiet          suppress noncritical error messages
	   -v --verbose        be verbose (a 2nd -v gives more)
	   -L --license        display software version & license
	   -V --version        display software version & license
	   -s --small          use less memory (at most 2500k)
	   -1 .. -9            set block size to 100k .. 900k
	   --fast              alias for -1
	   --best              alias for -9
	
	   If invoked as `bzip2', default action is to compress.
	              as `bunzip2',  default action is to decompress.
	              as `bzcat', default action is to decompress to stdout.
	
	   If no file names are given, bzip2 compresses or decompresses
	   from standard input to standard output.  You can combine
	   short flags, so `-v -4' means the same as -v4 or -4v, &c.

###使用

gzip的替代工具bzip2，压缩比较gzip高，用法与gzip类似，不再赘述。

1.压缩后的文本文件可以直接使用bzcat读出

	[root@server ~]# bzcat netstat.log.bz2 
	Active Internet connections (w/o servers)
	Proto Recv-Q Send-Q Local Address               Foreign Address             State  
	... ...

##参考

* [鸟哥的私房菜 文件与文件系统的压缩与打包](http://vbird.dic.ksu.edu.tw/linux_basic/0240tarcompress.php)
* [gzip wikipedia](http://en.wikipedia.org/wiki/Gzip)
* [bzip2 wikipedia](http://en.wikipedia.org/wiki/Bzip2)
##介绍

Oracle Database 10g 提供了一个新的工具：(AWR:Automatic Workload Repository)。Oracle 建议用户用这个取代 Statspack。AWR 实质上是一个 Oracle 的内置工具，它采集与性能相关的统计数据，并从那些统计数据中导出性能量度，以跟踪潜在的问题。

与 Statspack 不同，快照由一个称为 `MMON` 的新的后台进程及其从进程自动地每小时采集一次。为了节省空间，采集的数据在 7 天后自动清除。快照频率和保留时间都可以由用户修改。它产生两种类型的输出：文本格式（类似于 Statspack 报表的文本格式但来自于 AWR 信息库）和默认的 HTML 格式（拥有到部分和子部分的所有超链接），从而提供了非常用户友好的报表。
 
AWR 使用几个表来存储采集的统计数据，所有的表都存储在新的名称为`SYSAUX` 的特定表空间中的 `SYS` 模式下，并且以 `WRM$_*` 和 `WRH$_*` 的格式命名。前一种类型存储元数据信息（如检查的数据库和采集的快照），后一种类型保存实际采集的统计数据。H 代表“历史数据 (historical)”而 M 代表“元数据 (metadata)”。
            
在这些表上构建了几种带前缀`DBA_HIST_` 的视图，这些视图可以用来编写您自己的性能诊断工具。视图的名称直接与表相关；例如，视图 `DBA_HIST_SYSMETRIC_SUMMARY` 是在`WRH$_SYSMETRIC_SUMMARY` 表上构建的。
 
###注意
 
`statistics_level`默认是typical，在10g中表监控是激活的，强烈建议在10g中此参数的值是typical。

如果`statistics_level`设置为basic，不仅不能监控表，而且将禁掉如下一些10g的新功能：

* ASH(Active Session History)
* ASSM(Automatic Shared Memory Management)
* AWR(Automatic Workload Repository)
* ADDM(Automatic Database Diagnostic Monitor)

- - -

##AWR使用

运行awr报告

	SQL> @?/rdbms/admin/awrrpt
	
	Current Instance
	~~~~~~~~~~~~~~~~
	
	   DB Id    DB Name	 Inst Num Instance
	----------- ------------ -------- ------------
	 1193549399 DBTEST		1 DBTEST
	

指定报告类型	

	Specify the Report Type
	~~~~~~~~~~~~~~~~~~~~~~~
	Would you like an HTML report, or a plain text report?
	Enter 'html' for an HTML report, or 'text' for plain text
	Defaults to 'html'
	Enter value for report_type: 
	
	Type Specified:  html
	
	
	Instances in this Workload Repository schema
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	   DB Id     Inst Num DB Name	   Instance	Host
	------------ -------- ------------ ------------ ------------
	* 1193549399	    1 DBTEST	   DBTEST	oradb.tp-lin
							k.net
	
	Using 1193549399 for database Id
	Using	       1 for instance number
	

指定选取快照的天数	

	Specify the number of days of snapshots to choose from
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Entering the number of days (n) will result in the most recent
	(n) days of snapshots being listed.  Pressing <return> without
	specifying a number lists all completed snapshots.
	
	
	Enter value for num_days: 1
	
	Listing the last day's Completed Snapshots
	
								Snap
	Instance     DB Name	    Snap Id    Snap Started    Level
	------------ ------------ --------- ------------------ -----
	DBTEST	     DBTEST	       2409 14 Mar 2013 00:00	   1
				       2410 14 Mar 2013 01:00	   1
				       2411 14 Mar 2013 02:00	   1
				       2412 14 Mar 2013 03:00	   1
				       2413 14 Mar 2013 04:00	   1
				       2414 14 Mar 2013 05:00	   1
				       2415 14 Mar 2013 06:00	   1
				       2416 14 Mar 2013 07:00	   1
				       2417 14 Mar 2013 08:00	   1
				       2418 14 Mar 2013 09:00	   1
				       2419 14 Mar 2013 10:00	   1
				       2420 14 Mar 2013 11:00	   1
				       2421 14 Mar 2013 12:00	   1
				       2422 14 Mar 2013 13:00	   1
				       2423 14 Mar 2013 14:00	   1
	
指定生产报告所需快照的起始和结束编号	
	
	Specify the Begin and End Snapshot Ids
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Enter value for begin_snap: 2418
	Begin Snapshot Id specified: 2418
	
	Enter value for end_snap: 2421
	End   Snapshot Id specified: 2421
	
指定生成的报告名
	
	Specify the Report Name
	~~~~~~~~~~~~~~~~~~~~~~~
	The default report file name is awrrpt_1_2418_2421.html.  To use this name,
	press <return> to continue, otherwise enter an alternative.
	
	Enter value for report_name: 
	
	Using the report name awrrpt_1_2418_2421.html
	... ...
	Report written to awrrpt_1_2418_2421.html

- - -

##AWR报告分析

这部分内容，可以参考statspack report的分析，这2个内容都差不多。

AWR报告样例，可以参考[eygle](http://www.eygle.com)的[AWR报告分析之二：ges inquiry response](http://www.eygle.com/pdf/awrrpt_1_143_157.html)；

statspack报告可以参考[Dave](http://blog.csdn.net/tianlesoftware)的[statspack安装使用和report分析](http://blog.csdn.net/tianlesoftware/article/details/4682329)。

##AWR操作

###AWR保存策略
 
	SQL> col snap_interval for a20
	SQL> col retention for a20
	SQL> select * from dba_hist_wr_control;
	
	      DBID SNAP_INTERVAL	    RETENTION	         TOPNSQL
	---------- -------------------- -------------------- ----------
	1193549399 +00000 01:00:00.0	+00008 00:00:00.0    DEFAULT

以上结果表示,每小时产生一个SNAPSHOT，保留8天。
 
###调整AWR配置
 
AWR配置都是通过`dbms_workload_repository`包进行配置。
 
调整AWR产生snapshot的频率和保留策略，如将收集间隔时间改为30 分钟一次。并且保留5天时间（单位为分钟）：

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30, retention=>5*24*60);
 
关闭AWR，把interval设为0则关闭自动捕捉快照

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>0);
 
手工创建一个快照

	SQL> exec dbms_workload_repository.create_snapshot();
 
查看快照

	SQL> select * from sys.wrh$_active_session_history;
 
手工删除指定范围的快照
	
	SQL> exec dbms_workload_repository.drop_snapshot_range(low_snap_id => 973, high_snap_id => 999, dbid => 262089084);
 
创建baseline，保存这些数据用于将来分析和比较

	SQL> exec dbms_workload_repository.create_baseline(start_snap_id => 1003, end_snap_id => 1013, 'apply_interest_1');
 
删除baseline

	SQL> exec dbms_workload_repository.drop_baseline(baseline_name => 'apply_interest_1', cascade => FALSE);
 
将AWR数据导出并迁移到其它数据库以便于以后分析

	SQL> exec dbms_swrf_internal.awr_extract(dmpfile => 'awr_data.dmp', mpdir => 'DIR_BDUMP', bid => 1003, eid => 1013);
 
迁移AWR数据文件到其他数据库

	SQL> exec dbms_swrf_internal.awr_load(schname => 'AWR_TEST', dmpfile => 'awr_data.dmp', dmpdir => 'DIR_BDUMP');

把AWR数据转移到SYS模式中：

	SQL> exec dbms_swrf_internal.move_to_awr(schname => 'TEST');

##错误

在学习AWR配置调整过程中，出现了一例异常，如下。

###ORA-13541

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60);
	BEGIN dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60); END;
	
	*
	ERROR at line 1:
	ORA-13541: system moving window baseline size (691200) greater than retention (604800)
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 174
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 222
	ORA-06512: at line 1



	SQL> select 691200/24/60/60 baseline,604800/24/60/70 retention from dual;
	
	  BASELINE  RETENTION
	---------- ----------
		 8	    6

检查当前系统移动窗口的基线设置

	SQL>  SELECT dbid, baseline_name, baseline_type, moving_window_size from dba_hist_baseline;
	
	      DBID BASELINE_NAME	BASELINE_TYPE MOVING_WINDOW_SIZE
	---------- -------------------- ------------- ------------------
	1193549399 SYSTEM_MOVING_WINDOW MOVING_WINDOW		       8

根据以上信息，即移动窗口(moving window baseline)的大小大于数据保留(retention)时间，故报`ORA-13541`。要弄清楚这两者之间的关系，必须要弄清楚几个概念。这里仅作简单的理解：AWR是通过比较不同时间点的性能数据来分析系统的运行状态的，所选取的时间点跨度(最大值即移动窗口的大小)必须在这些性能数据的保留周期之内，否则会出现以上异常。

这里调整下以上两个参数的大小，使移动窗口的大小，这个值要等于或小于AWR保留天数即可。

如调整移动窗口大小，从8天改为7天：

	SQL> exec dbms_workload_repository.modify_baseline_window_size(window_size=>7);
	
	PL/SQL procedure successfully completed.
	
	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60);
	
	PL/SQL procedure successfully completed.

如调整保留时间，从7改为8天：

	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60);
	BEGIN dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>7*24*60); END;
	
	*
	ERROR at line 1:
	ORA-13541: system moving window baseline size (691200) greater than retention (604800)
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 174
	ORA-06512: at "SYS.DBMS_WORKLOAD_REPOSITORY", line 222
	ORA-06512: at line 1
	
	
	SQL> exec dbms_workload_repository.modify_snapshot_settings(interval=>30,retention=>8*24*60);
	
	PL/SQL procedure successfully completed.

####基线(Baselines)

基线包含了一个特定时间范围的性能数据，用来在性能问题发生时，与其他类似的时间段进行比较。基线中的快照会被自动AWR清除进程排除，并无限期保留。

oracle数据库中包含了三种类型的基线：
 
#####1.固定基线(Fixed Baselines)
	
固定基线相当于被指定的过去的一个固定的、连续的时间范围。在创建固定基线以前，要慎重考虑这个时间段，因为基线代表了一个理想状态的系统状态。之后，你可以用这个基线和其他基线或者某个时间范围内的快照来分析性能上的退化情况。

#####移动窗口基线(Moving Window Baseline)

移动窗口基线相当于AWR保留期间内存在的所有AWR数据。在使用自适应阈值时，这将很有用处，因为数据库可以使用AWR保留期间的所有AWR数据来计算出度量阈值。oracle数据库自动维护一个系统定义的移动窗口基线。系统定义的移动窗口基线的默认窗口大小等于当前AWR保留的时间，默认为8天。如果你要使用自适应阈值，可以考虑使用更大的移动窗口，例如30天，可以更精确地计算出阈值。你可以改变移动窗口的大小，这个值要等于或小于AWR保留天数。因此若你需要增大移动窗口的大小，首先需要增加AWR的保留时间。

#####基线模板(Baseline Templates)

 你可以创建一个基线，作为未来一个时间连续的时间段可以使用的基线模板。有两种类型的基线模板：单一的和重复的。你可以为未来一个单独的连续时间段的基线创建单一基线模板。如果你要提前准备获取一个未来的时间段，这个技术会很有用处。例如，你安排好要在周末进行一个系统测试，并准备获取AWR数据，这种情况下，你可以创建一个单一基线模板，用以在测试时自动获取该时间范围内的数据。你也可以使用重复基线模板来创建或者删除一个重复的时间计划，当你想自动获取一个连续的时间范围，这将很有用。例如，你可能希望在一个月里的每周一早晨获取AWR数据，这种情况下，你可以创建一个重复基线模板来自动为每个周一创建基线，并且在设置了过期时间(例如一个月)后，自动删除过期的基线。

##参考

* [Oracle AWR 简介](http://blog.csdn.net/wildwave/article/details/6838906)
* [Oracle AWR 说明](http://blog.csdn.net/tianlesoftware/article/details/4682300)
* [AWR报告分析之二：ges inquiry response](http://www.eygle.com/pdf/awrrpt_1_143_157.html)
* [AWR报告样例](http://www.eygle.com/pdf/awrrpt_1_143_157.html)
* [statspack安装使用和report分析](http://blog.csdn.net/tianlesoftware/article/details/4682329)
##概述

Oracle  Statspack 从 Oracle8.1.6 开始被引入 Oracle,并马上成为 DBA 和 Oracle 专家用来诊断数据库性能的强有力的工具。通过 Statspack 我们可以很容易的确定 Oracle 数据库的瓶颈所在，记录数据库性能状态，也可以使远程技术支持人员迅速了解你的数据库运行状况。因此了解和使用 Statspack 对于 DBA 来说至关重要。

Oracle 10g之前对数据库做性能检测使用statspack工具，自10g 提供了一个新的工具：(AWR:Automatic Workload Repository)。Oracle 建议用户用这个取代 Statspack。AWR 实质上是一个 Oracle 的内置工具，它采集与性能相关的统计数据，并从那些统计数据中导出性能量度，以跟踪潜在的问题。

在数据库中 Statspack 的脚本位于`$ORACLE_HOME/RDBMS/ADMIN` 目录下。

##基本使用

###1.安装statspack.

在$ORACLE_HOME/rdbms/admin/目录下运行：

	SQL> @spcreate.sql

若创建失败则在同一目录下运行： 
	
	SQL> @spdrop.sql
 
###2.测试

	SQL>execute statspack.snap
	  PL/SQL procedure successfully completed.
	SQL>execute statspack.snap
	  PL/SQL procedure successfully completed.
	SQL>@spreport.sql
 
	SQL>exec statspack.snap; 

进行信息收集统计，每次运行都将产生一个快照号，获得快照号，必须要有两个以上的快照，才能生成报表
 
###3.查选快照信息

	SQL>select SNAP_ID, SNAP_TIME from STATS$SNAPSHOT;
 
###4.获取statspack 报告

	SQL>@spreport.sql          
                                                         
按照提示，输入需要查看的开始快照号与结束快照号即可。
 
###5.其他相关脚本

* spauto.sql： 利用dbms_job提交一个作业，自动的进行STATPACK的信息收集统计
* sppurge.sql ：清除一段范围内的统计信息，需要提供开始快照与结束快照号
* sptrunc.sql ： 清除(truncate)所有统计信息
 
###6.查看Statspack 生成源代码

在oracle 9i里面，我们可以通过查看statspack 生成脚本来帮助我们理解report，但是10g的AWR是通过`dbms_workload_repository`包来实现AWR的。包把代码都封装了起来，我们无法查看。
 	
statspack的生成脚本位置：`$ORACLE_HOME/rdbms/admin/sprepins.sql`
代码很长，不过看懂了，能帮助我们理解statspack中各个数据的意义。


##检查系统参数

为了能够顺利安装和运行 Statspack 你可能需要设置以下系统参数：

###1.`job_queue_processes` 

为了能够建立自动任务，执行数据收集，该参数需要大于 0。你可以在初试化参数文件中修改该参数(使该参数在重起后以然有效)。 

该参数可以在系统级动态修改。

	SQL> show parameter job_queue_processes;
	
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	job_queue_processes                  integer     2
	SQL> alter system set job_queue_processes=6;
	
	System altered.
	
	SQL> show parameter job_queue_processes;
	
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	job_queue_processes                  integer     6

在 Oracle9i 当中，可以指定范围，如 both,这样该修改在当前及之后保持有效(仅当你使用 spfile时，如果在 9i 中仍然使用 pfile，那么更改方法同 8i 相同): 

	SQL> alter system set job_queue_processes = 6 scope=both;   
	System altered.

###2.`timed_statistics` 

收集操作系统的计时信息，这些信息可被用来显示时间等统计信息、优化数据库和 SQL 语句。要防止因从操作系统请求时间而引起的开销，请将该值设置为 False。 

使用 statspack 收集统计信息时建议将该值设置为 True，否则收集的统计信息大约只能起到10%的作用，将 timed_statistics 设置为 True 所带来的性能影响与好处相比是微不足道的。 

该参数使收集的时间信息存储在在 V$SESSTATS 和 V$SYSSTATS 等动态性能视图中。

timed_statistics 参数可以在实例级进行更改

	SQL> alter system set timed_statistics = true; 
	System altered 
	SQL> show parameter timed_statistics;
	
	NAME                                 TYPE        VALUE
	------------------------------------ ----------- ------------------------------
	timed_statistics                     boolean     TRUE

如果你担心一致启用 timed_statistics  对于性能的影响，你可以在使用 statspack 之前在 system 更改，采样过后把该参数动态修改成 false。


##检查Statspack
因测试环境中之前做过statspack监控，并未删除statspack对象，因此这里仅确认下是否安装正确。要看安装、卸载等，请查阅参考文档。

###1.statspack检查

statspack脚本均在$ORACLE_HOME/rdbms/admin下。首先切换到该目录，下面执行脚本时会比较方便。

切换到该路径

	$ pwd
	/u2/TEST/testora/testdb/9.2.0/rdbms/admin

sp脚本

	$ ls sp*
	sp__.lst          spcpkg.sql        spctab.sql        spdoc.txt         spdusr.sql        spreport.sql      sprepsql.sql      spup816.sql
	spauto.sql        spcreate.sql      spcusr.lis        spdrop.sql        sppurge.sql       spreport0902.txt  sptrunc.sql       spup817.sql
	spcpkg.lis        spctab.lis        spcusr.sql        spdtab.sql        sprepins.sql      spreport0907.txt  spuexp.par        spup90.sql

###2.测试sp脚本

	SQL> execute statspack.snap
	
	PL/SQL procedure successfully completed.
	
	SQL> execute statspack.snap
	
	PL/SQL procedure successfully completed.
	SQL> @spreport.sql
	……	

###3.检查statspach表空间

	SQL> SELECT tablespace_name,file_name, round(dbf.BYTES / (1024 * 1024),0) "Total_space(M)" FROM dba_data_files dbf where dbf.TABLESPACE_NAME = 'STATSPACK';  
			TABLESPACE_NAME   FILE_NAME                     Total_space(M)
	-------------------- ------------------------------------------------------------ --------------
	STATSPACK  /u2/TEST/testora/testdata/statspack_01.dbf               500.00

##规划自动任务

Statspack 正确安装以后，我们就可以设置定时任务，开始收集数据了。可以使用 spatuo.sql 来定义自动任务。

先来看看 spauto.sql 的关键内容：

	variable jobno number;
	variable instno number;
	begin
	  select instance_number into :instno from v$instance;
	  dbms_job.submit(:jobno, 'statspack.snap;',
	  trunc(sysdate+1/24,'HH'), 'trunc(SYSDATE+1/24,'HH')', 
	TRUE, :instno);
	  commit;
	end;

这个 job 任务定义了收集数据的时间间隔：

	一天有 24 个小时，1440 分钟，那么：
		1/24   HH           每小时一次
		1/48   MI           每半小时一次
		1/144  MI           每十分钟一次
		1/288  MI           每五分钟一次

	SQL> @spauto
	
	PL/SQL procedure successfully completed.
	
	
	Job number for automated statistics collection for this instance
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Note that this job number is needed when modifying or removing
	the job:
	
	     JOBNO
	----------
	     87890
	
	
	Job queue process
	~~~~~~~~~~~~~~~~~
	Below is the current setting of the job_queue_processes init.ora
	parameter - the value for this parameter must be greater
	than 0 to use automatic statistics gathering:
	
	NAME_COL_PLUS_SHOW_PARAM                                         TYPE
	---------------------------------------------------------------- -----------
	VALUE_COL_PLUS_SHOW_PARAM
	------------------------------------------------------------------------------
	job_queue_processes                                              integer
	6
	
	
	Next scheduled run
	~~~~~~~~~~~~~~~~~~
	The next scheduled run for this job is:
	
	       JOB NEXT_DATE       NEXT_SEC
	---------- --------------- ------------------------
	     87890 28-JUN-12       10:00:00

关于采样间隔，我们通常建议以 1 小时为时间间隔，对于有特殊需要的环境，可以设置更短的，如半小时作为采样间隔，但是不推荐更短。因为 statspack 的执行本身需要消耗资源，对于繁忙的生产系统，太短的采样对系统的性能会产生较大的影响（甚至会使 statspack 的执行出现在采样数据中）。

##生成分析报告

运行spreport脚本，输入起始和结束的快照ID，生成分析报告。

	SQL> @spreport
	Current Instance
	~~~~~~~~~~~~~~~~
	   DB Id    DB Name      Inst Num Instance
	---------- ------------ -------- ------------
	   33115540 ERP                 1 ERP 
	Instances in this Statspack schema
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	   DB Id    Inst Num DB Name      Instance     Host
	----------- -------- ------------ ------------ ------------
	   33115540        1 ERP          ERP          erp
	   56114082        1 PROD         PROD         erpprod
	
	Using   33115540 for database Id
	Using          1 for instance number
	Completed Snapshots
	                               Snap                    Snap
	Instance     DB Name             Id   Snap Started    Level Comment
	------------ ------------ --------- ----------------- ----- --------------------
	ERP          ERP                753 28 Jun 2012 10:30     5
	                                754 28 Jun 2012 11:30     5
	                                755 28 Jun 2012 12:30     5
	                                756 28 Jun 2012 13:30     5
	                                757 28 Jun 2012 14:30     5
	                                758 28 Jun 2012 15:30     5 
	Specify the Begin and End Snapshot Ids
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Enter value for begin_snap: 753
	Begin Snapshot Id specified: 753
	Enter value for end_snap: 758
	End   Snapshot Id specified: 758
	Specify the Report Name
	~~~~~~~~~~~~~~~~~~~~~~~
	The default report file name is sp_753_758.  To use this name,
	press <return> to continue, otherwise enter an alternative.
	Enter value for report_name: sp_753_758.txt
	Using the report name sp_753_758.txt SNATSPACK report for DB Name         DB Id    Instance     Inst Num Release     Cluster Host
	----------- ----------- ------------ -------- ----------- ------- ------------
	ERP             33115540 ERP                 1 9.2.0.6.0   NO      erp
	              Snap Id     Snap Time      Sessions Curs/Sess Comment
	            --------- ------------------ -------- --------- -------------------
	Begin Snap:       753 28-Jun-12 10:30:16      153      91.9
	  End Snap:       758 28-Jun-12 15:30:17      160      90.5
	   Elapsed:              300.02 (mins) ……

一个 statspack 的报告不能跨越一次停机，但是之前或之后的连续区间，收集的信息依然有效。你可以选择之前或之后的采样声称 report。

##移除定时任务

运行dbms_job.remove(‘job_id’)，移除定时任务。

	SQL> select job,log_user,priv_user,last_date,next_date,interval from user_jobs;
	    JOB LOG_USER  PRIV_USER LAST_DATE NEXT_DATE INTERVAL
	------------ --------------- -------------- ----------------  -------------- --------------------------
	87890 	SYS   SYS 	    28-JUN-12 28-JUN-12  trunc(SYSDATE+1/24,'HH') 

	SQL> execute dbms_job.remove('87890');
	PL/SQL procedure successfully completed.


##删除历史数据

删除stats$snapshot数据表中的数据，其他表中的数据会相应的级联删除

	SQL> select max(snap_id) from stats$snapshot;
	MAX(SNAP_ID)
	------------
	        758
	SQL> delete from stats$snapshot where snap_id < = 758;

如果采样了大量的数据，直接delete是非常会慢的，可以考虑使用sptrunc脚本，清空stats历史数据。

	SQL> @sptrunc.sql 
	Warning
	~~~~~~~
	Running sptrunc.sql removes ALL data from Statspack tables.  You may
	wish to export the data before continuing. 
	About to Truncate Statspack Tables ……

##延伸阅读

* [statspack安装使用和report分析](http://blog.csdn.net/tianlesoftware/article/details/4682329)
* [Oracle AWR说明](http://blog.csdn.net/tianlesoftware/article/details/4682300)

##参考

* [statspack使用指南](http://www.eygle.com/pdf/Statspack-v3.0.pdf)
##权限

权限(Privilege)：即执行特定语句的能力。权限允许用户访问数据库中其他用户的对象，执行存储过程，或者执行一些系统级的操作。在Oracle数据库系统中，一般分为系统权限，和对象权限。

###1.系统权限

与具体的对象无关，是在任何对象上执行操作的权利，以及运行批处理、改变系统参数、创建角色等方面的权限。
	
	--查询所有系统权限
	SELECT * FROM SYSTEM_PRIVILEGE_MAP;
	
	--查询某个用户/角色的所有系统权限
	SELECT * FROM DBA_SYS_PRIVS DSP WHERE DSP.GRANTEE = UPPER('&grantee');	
	
###2.对象权限

在特定对象上执行特定操作的权限，如SELECT,INSERT,UPDATE等。对象：表、视图、过程等。
	
	--所有对象权限
	SELECT * FROM DBA_TAB_PRIVS;
	
	--某角色被授予的相关表的权限
	SELECT * FROM ROLE_TAB_PRIVS RTP WHERE RTP.ROLE = UPPER('&role');
	
	--使用密码文件的用户
	SELECT * FROM V$PWFILE_USERS;
	
###3.角色相关

一组权限的集合，简化权限的管理。被授予给角色的用户，将继承该角色所授予的所有权限以及角色。角色可以使用密码认证加强安全性。
	
	--所有角色
	SELECT * FROM DBA_ROLES;
	
	--某一角色被授予的系统权限
	SELECT * FROM ROLE_SYS_PRIVS RSP WHERE RSP.ROLE = UPPER('&role');
	
	--用户角色分配关系
	SELECT * FROM DBA_ROLE_PRIVS DRP WHERE DRP.GRANTEE = UPPER('&user');
	
###4.当前用户

与当前登录用户相关的系统权限、对象权限、角色等相关的视图。

	--查看当前用户
	SELECT USER FROM DUAL;
	
	--当前用户授予的系统权限
	SELECT * FROM USER_SYS_PRIVS;
	
	--当前用户所拥有的全部权限
	SELECT * FROM SESSION_PRIVS;
	
	--当前用户所授予的直接角色
	SELECT * FROM USER_ROLE_PRIVS;
	
	--当前用户被授予的所有角色
	SELECT * FROM SESSION_ROLES;
	
	--当前用户所有角色被授予的角色
	SELECT * FROM ROLE_ROLE_PRIVS;
	
	--当前用户的对象权限
	SELECT * FROM TABLE_PRIVILEGES;
	
##权限、角色配置与管理

###1.授权

	SQL> ? grant
	
	 GRANT (Object Privileges)
	 -------------------------
	
	 Use this command to grant privileges for a particular object to
	 users and roles. To grant system privileges and roles, use the GRANT
	 command (System Privileges and Roles).
	
	 GRANT
	   { object_priv | ALL [PRIVILEGES] }
	   [ ( column [, column] ...) ]
	   [, { object_priv | ALL [PRIVILEGES] }
	      [ ( column [, column] ...) ] ] ...
	 ON [ schema.| DIRECTORY] object
	 TO { user | role | PUBLIC} ...
	    [ WITH GRANT OPTION]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.
	
	
	 GRANT (System Privileges and Roles)
	 -----------------------------------
	
	 Use this command to grant system privileges and roles to users and
	 roles. To grant object privileges, use the GRANT command (Object
	 Privileges).
	
	 GRANT
	   { system_priv | role}
	   [, { system_priv | role} ] ...
	 TO
	   { user | role | PUBLIC}
	   [, { user | role | PUBLIC} ] ...
	   [ WITH ADMIN OPTION]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.

注：

*  WITH ADMIN OPTION：带有该选项，则被授权用户也可以管理该权限，即授权、取消授权给其他用户。
* GRANT ANY PRIVILEGE：带有该选项，则被授权用户可以对任何权限进行授权或取消授权。

###2.取消授权

	SQL> ? revoke
	
	 REVOKE (Schema Object Privileges)
	 ---------------------------------
	
	 Use this command to revoke object privileges for a particular object
	 from users and roles. To revoke system privileges or roles, use the
	 REVOKE command (System Privileges and Roles).
	
	 REVOKE
	   { object_priv | ALL [PRIVILEGES] }
	   [, {object_priv | ALL [PRIVILEGES] } ] ...
	 ON
	   [ schema.| DIRECTORY] object
	 FROM
	   { user | role | PUBLIC}
	   [, {user | role | PUBLIC} ] ...
	   [ CASCADE CONSTRAINTS]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.
	
	
	 REVOKE (System Privileges and Roles)
	 ------------------------------------
	
	 Use this command to revoke system privileges and roles from users
	 and roles. To revoke object privileges from users and roles, use the
	 REVOKE command (Object Privileges).
	
	 REVOKE
	   { system_priv | role}
	   [, { system_priv | role} ] ...
	 FROM
	   { user | role | PUBLIC}
	   [, {user | role | PUBLIC} ] ...
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.

###3.角色

####创建角色

	SQL> ? create role
	
	 CREATE ROLE
	 -----------
	
	 Use this command to create a role. A role is a set of privileges
	 that can be granted to users or to other roles.
	
	 CREATE ROLE role [NOT IDENTIFIED | IDENTIFIED {BY password |
	   EXTERNALLY | GLOBALLY} ]
	
	 For detailed information on this command, see the Oracle8 Server SQL
	 Reference.

####角色选择

创建角色后，可以使用GRANT/REVOKE给角色授予、取消授予适当权限，并最终分配给用户。一个用户可以拥有多个角色，在一个用户会话过程中，用户可以选择性的使某些角色生效或禁用，以控制对系统、对象的访问。

设置当前角色

	SQL> set role connect;
	
	Role set.
	
	SQL> create table role_t(id number);
	create table role_t(id number)
	*
	ERROR at line 1:
	ORA-01031: insufficient privileges
	

使用户所有角色生效

	SQL> set role all;
	
	Role set.

	SQL> create table role_t(id number);
	
	Table created.
	
排除某些角色

	SQL> select count(1) from v$session;
	
	  COUNT(1)
	----------
		88
	
	SQL> set role all except dba;
	
	Role set.
	
	SQL> select count(1) from v$session;
	select count(1) from v$session
	                     *
	ERROR at line 1:
	ORA-00942: table or view does not exist

####默认角色

当设置用户默认角色时，这些角色会在用户初始化会话时自动生效。

	alter user username defautl role role_list;

##延伸阅读

* [Oracle用户及角色介绍](http://blog.csdn.net/tianlesoftware/article/details/4786956)
##ISCSI

Internet SCSI (iSCSI) is a network protocol s that allows you to use of the SCSI protocol over TCP/IP networks. It is good alternative to Fibre Channel-based SANs. You can easily manage, mount and format iSCSI Volume under Linux. It allows access to SAN storage over Ethernet.

##Open-iSCSI Project

Open-iSCSI project is a high-performance, transport independent, multi-platform implementation of iSCSI. Open-iSCSI is partitioned into user and kernel parts.

Instructions are tested on:

* [a] RHEL 5
* [b] CentOS 5
* [c] Fedora 7
* [d] Debian / Ubuntu Linux

###Install Required Package

iscsi-initiator-utils RPM package - The iscsi package provides the server daemon for the iSCSI protocol, as well as the utility programs used to manage it. iSCSI is a protocol for distributed disk access using SCSI commands sent over Internet Protocol networks. This package is available under Redhat Enterprise Linux / CentOS / Fedora Linux and can be installed using yum command:

	# yum install iscsi-initiator-utils

A note about Debian / Ubuntu Linux

If you are using Debian / Ubuntu Linux install open-iscsi package, enter:
	
	$ sudo apt-get install open-iscsi

##CentOS / Red Hat Linux: Install and manage iSCSI Volume

###iSCSI Configuration

There are three steps needed to set up a system to use iSCSI storage:

* iSCSI startup using the init script or manual startup. You need to edit and configure iSCSI via `/etc/iscsi/iscsid.conf` file
* Discover targets.
* Automate target logins for future system reboots.

You also need to obtain iSCSI username, password and storage server IP address (target host)

####Step # 1: Configure iSCSI

Open /etc/iscsi/iscsid.conf with vi text editor:

	# vi /etc/iscsi/iscsid.conf

Setup username and password:
node.session.auth.username = My_ISCSI_USR_NAME
node.session.auth.password = MyPassword
discovery.sendtargets.auth.username = My_ISCSI_USR_NAME
discovery.sendtargets.auth.password = MyPassword

Where,

* node.session.* is used to set a CHAP username and password for initiator authentication by the target(s).
* discovery.sendtargets.* is used to set a discovery session CHAP username and password for the initiator authentication by the target(s)

You may also need to tweak and set other options. Refer to man page for more information. Now start the iscsi service:

	# /etc/init.d/iscsi start

####Step # 2: Discover targets

Now use iscsiadm command, which is a command-line tool allowing discovery and login to iSCSI targets, as well as access and management of the open-iscsi database. If your storage server IP address is 192.168.1.5, enter:
	
	# iscsiadm -m discovery -t sendtargets -p 192.168.1.5

	# /etc/init.d/iscsi restart

Now there should be a block device under /dev directory. To obtain new device name, type:

	# fdisk -l

or

	# tail -f /var/log/messages

Output:

	Oct 10 12:42:20 ora9is2 kernel:   Vendor: EQLOGIC   Model: 100E-00           Rev: 3.2
	Oct 10 12:42:20 ora9is2 kernel:   Type:   Direct-Access                      ANSI SCSI revision: 05
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: 41963520 512-byte hdwr sectors (21485 MB)
	Oct 10 12:42:20 ora9is2 kernel: sdd: Write Protect is off
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: drive cache: write through
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: 41963520 512-byte hdwr sectors (21485 MB)
	Oct 10 12:42:20 ora9is2 kernel: sdd: Write Protect is off
	Oct 10 12:42:20 ora9is2 kernel: SCSI device sdd: drive cache: write through
	Oct 10 12:42:20 ora9is2 kernel:  sdd: unknown partition table
	Oct 10 12:42:20 ora9is2 kernel: sd 3:0:0:0: Attached scsi disk sdd
	Oct 10 12:42:20 ora9is2 kernel: sd 3:0:0:0: Attached scsi generic sg3 type 0
	Oct 10 12:42:20 ora9is2 kernel: rtc: lost some interrupts at 2048Hz.
	Oct 10 12:42:20 ora9is2 iscsid: connection0:0 is operational now

`/dev/sdd` is my new block device.

####Step # 3: Format and Mount iSCSI Volume

You can now partition and create a filesystem on the target using usual fdisk and mkfs.ext3 commands:

	# fdisk /dev/sdd
	# mke2fs -j -m 0 -O dir_index /dev/sdd1

OR
	
	# mkfs.ext3 /dev/sdd1

Tip: If your volume is large size like 1TB, run mkfs.ext3 in background using nohup:

	# nohup mkfs.ext3 /dev/sdd1 &

Mount new partition:
	
	# mkdir /mnt/iscsi
	# mount /dev/sdd1 /mnt/iscsi

####Step #4: Mount iSCSI drive automatically at boot time

First make sure iscsi service turned on at boot time:

	# chkconfig iscsi on

Open `/etc/fstab` file and append config directive:
	
	/dev/sdd1 /mnt/iscsi ext3 _netdev 0 0

Save and close the file.

###Mount ISCSI on CentOS

1.discover

	[root@erptest1 ~]# iscsiadm -m discovery -t sendtargets -p 172.29.88.61
	172.29.88.61:3260,0 iqn.2000-01.com.synology:themain-2nd.oemfile
	169.254.146.25:3260,0 iqn.2000-01.com.synology:themain-2nd.oemfile
	172.29.88.61:3260,0 iqn.2000-01.com.synology:themain-2nd.cluster
	169.254.146.25:3260,0 iqn.2000-01.com.synology:themain-2nd.cluster
	172.29.88.61:3260,0 iqn.2000-01.com.synology:themain-2nd.transmission
	169.254.146.25:3260,0 iqn.2000-01.com.synology:themain-2nd.transmission
	172.29.88.61:3260,0 iqn.2000-01.com.synology:themain-2nd.mobile
	169.254.146.25:3260,0 iqn.2000-01.com.synology:themain-2nd.mobile
	172.29.88.61:3260,0 iqn.2000-01.com.synology:themain-2nd.mobilefile
	169.254.146.25:3260,0 iqn.2000-01.com.synology:themain-2nd.mobilefile
	172.29.88.61:3260,0 iqn.2000-01.com.synology:themain-2nd.test
	169.254.146.25:3260,0 iqn.2000-01.com.synology:themain-2nd.test
	172.29.88.61:3260,0 iqn.2000-01.com.synology:themain-2nd.erp
	169.254.146.25:3260,0 iqn.2000-01.com.synology:themain-2nd.erp

2.iscsid.conf

	vim /etc/iscsi/iscsid.conf

	. . .
	# *************
	# CHAP Settings
	# *************
	 
	# To enable CHAP authentication set node.session.auth.authmethod
	# to CHAP. The default is None.
	node.session.auth.authmethod = CHAP                
	 
	# To set a CHAP username and password for initiator
	# authentication by the target(s), uncomment the following lines:
	node.session.auth.username = backup
	node.session.auth.password = storageoabak
	. . .
	

3.login

	[root@erptest1 ~]# ll /dev/sd*
	brw-rw----. 1 root disk 8, 0 May 29 18:07 /dev/sda
	brw-rw----. 1 root disk 8, 1 May 29 10:48 /dev/sda1
	brw-rw----. 1 root disk 8, 2 May 29 10:48 /dev/sda2
	brw-rw----. 1 root disk 8, 3 May 29 10:48 /dev/sda3

	[root@erptest1 ~]#iscsiadm -m node -T iqn.2000-01.com.synology:themain-2nd.erp -p 172.29.88.61 --login
	Logging in to [iface: default, target: iqn.2000-01.com.synology:themain-2nd.erp, portal: 172.29.88.61,3260] (multiple)
	Login to [iface: default, target: iqn.2000-01.com.synology:themain-2nd.erp, portal: 172.29.88.61,3260] successful.

	[root@erptest1 ~]# ll /dev/sd*
	brw-rw----. 1 root disk 8,  0 May 29 18:07 /dev/sda
	brw-rw----. 1 root disk 8,  1 May 29 10:48 /dev/sda1
	brw-rw----. 1 root disk 8,  2 May 29 10:48 /dev/sda2
	brw-rw----. 1 root disk 8,  3 May 29 10:48 /dev/sda3
	brw-rw----. 1 root disk 8, 16 May 29 18:00 /dev/sdb

4.fdisk,mkfs ...

	fdisk ...

	mkfs ...

	[root@erptest1 ~]# fdisk -l /dev/sdb

	Disk /dev/sdb: 1099.5 GB, 1099511627776 bytes
	255 heads, 63 sectors/track, 133674 cylinders
	Units = cylinders of 16065 * 512 = 8225280 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 512 bytes / 512 bytes
	Disk identifier: 0xc1aa5473
	
	   Device Boot      Start         End      Blocks   Id  System
	/dev/sdb1               1      133674  1073736373+   5  Extended
	/dev/sdb5               1      133674  1073736342   83  Linux

5.mount

	[root@erptest1 ~]# mount -o acl,rw /dev/sdb5 /data/

##测试

###1.关于多重联机

多重联机即一个或多个客户端连接一块SAN分配的存储单元，不过为避免数据受损，必须确认在集群
架构下使用才行。

在测试过程中，使用新购置的两台ERP测试服务器共享一块网络存储，发现存在以下问题：

####多写

mount 均采用  `mount -o acl,rw /dev/sdb5 /data` 进行挂载；写数据时，都是使用root写入；

* a. 多台客户端可以同时挂载，但挂载后，写入的数据仅有自身可以看到，其他客户端不可见；
* b. 多台客户端可以同时挂载时，各客户端均有写入数据时，当时数据没有丢失，但是重新挂载网络设备时，会出现数据丢失的情况（只能看到最后写入的数据）；

####一写多读

* a. 多个客户端同时挂载，仅一个客户端写数据，其他读；在写入时，其他客户端暂时不可见，必须重新挂载；
* b. 关于AIX和Oracle Linux之间共享该网络存储（一写一读），Oracle Linux挂载jfs2文件系统时不识别，添加jfs相关的软件包后挂载失败；在CentOS 6.2上，添加jfs2软件包后可以挂载jfs2文件系统。对比后，发现Oracle Linux启动的内核为uek内核，可能和当前系统运行的内核相关。
 
###2.其他

另外，测试了下使用SAN存储搭建NFS，多客户端可以同时读写，但速率很低。


##Reference

* [Linux ISCSI howto](http://www.cyberciti.biz/tips/rhel-centos-fedora-linux-iscsi-howto.html)

##mount iscsi storage on aix

1.os and iscsi package

	# uname -a
	AIX erp 3 5 00066345D600	

	# lslpp -L | grep iscsi
	  devices.common.IBM.iscsi.rte
	  devices.iscsi.disk.rte    5.3.0.60    C     F    iSCSI Disk Software 
	  devices.iscsi.tape.rte    5.3.0.30    C     F    iSCSI Tape Software 
	  devices.iscsi_sw.rte      5.3.0.64    C     F    iSCSI Software Device Driver

2.iscsi targets configure

	# vi /etc/iscsi/targets
	"/etc/iscsi/targets" 104 lines, 3562 characters 
	# @(#)25        1.2  src/bos/usr/lib/methods/cfgiscsi/targets.sh, sysxiscsi, bos53A, a2004_37B1 8/30/04 13:58:05
	# IBM_PROLOG_BEGIN_TAG
	# This is an automatically generated prolog.
	#
	# bos53A src/bos/usr/lib/methods/cfgiscsi/targets.sh 1.2
	#
	# Licensed Materials - Property of IBM
	#
	# (C) COPYRIGHT International Business Machines Corp. 2003,2004
	# All Rights Reserved
	#
	# US Government Users Restricted Rights - Use, duplication or
	# disclosure restricted by GSA ADP Schedule Contract with IBM Corp.
	#
	# IBM_PROLOG_END_TAG
	# iSCSI targets file
	#
	# Comments may be used in the file, the comment character is '#', 0x23.
	# Anything from a comment char to the end of the line is ignored.
	#
	# Blank lines are ignored.
	#
	# The format for a target line is defined according to the Augmented BNF
	# syntax as # described in rfc2234.
	#
	# The format for the IPv4Address is taken from rfc2373.
	#
	# The line continuation character '\' (i.e., back-slash) can be used to make
	# each TargetLine easier to read. To ensure no parsing errors, the '\'
	# character must be the last character and must be preceeded by white space.
	#
	# ---
	#
	# comment        = %x23 *CHARS LF
	#                  ; #
	#
	# TargetLine     = *WSP HostNameOrAddr 1*WSP PortNumber 1*WSP ISCSIName *WSP LF
	#       or
	"/etc/iscsi/targets" 104 lines, 3562 characters
	#       Assume the target is at address 10.2.1.105
	#       the valid port is 3260
	#       the name of the target is iqn.com.ibm-K167-42.fc1a
	#       the CHAP secret is "This is my password."
	# The target line would look like:
	# 10.2.1.105 3260 iqn.com.ibm-K167-42.fc1a "This is my password."
	#
	# EXAMPLE 3: iSCSI Target with CHAP(MD5) authentication and line continuation
	#       Assume the target is at address 10.2.1.106
	#       the valid port is 3260
	#       the name of the target is iqn.2003-01.com.ibm:00.fcd0ab21.shark128
	#       the CHAP secret is "123ismysecretpassword.fc1b"
	# The target line would look like:
	# 10.2.1.106 3260 iqn.2003-01.com.ibm:00.fcd0ab21.shark128 \
	#               "123ismysecretpassword.fc1b"
	#
	
	172.29.88.61 3260 iqn.2000-01.com.synology:TheMain-2nd.erp

3.cfgmgr discover hardware

	# cfgmgr -v -l iscsi0
	----------------
	attempting to configure device 'iscsi0'
	Time: 0 LEDS: 0x25b0
	invoking /usr/lib/methods/cfgiscsi -l iscsi0 
	Number of running methods: 1
	----------------
	Completed method for: iscsi0, Elapsed time = 1
	return code = 0
	****************** stdout ***********
	hdisk10 
	****************** no stderr ***********
	----------------
	Time: 1 LEDS: 0x539
	Number of running methods: 0
	----------------
	attempting to configure device 'hdisk10'
	Time: 1 LEDS: 0x25f3
	invoking /usr/lib/methods/cfgscsidisk -l hdisk10 
	Number of running methods: 1
	----------------
	Completed method for: hdisk10, Elapsed time = 4
	return code = 0
	****************** no stdout ***********
	****************** no stderr ***********
	----------------
	Time: 5 LEDS: 0x539
	Number of running methods: 0
	----------------
	calling savebase
	return code = 0
	****************** no stdout ***********
	****************** no stderr ***********
	Configuration time: 5 seconds

	# lspv
	hdisk0          00066345812c3186                    rootvg          active
	hdisk1          000663453bb98660                    rootvg          active
	hdisk2          00066345863e157b                    testvg          active
	hdisk3          000664cd863d5728                    prodvg          
	hdisk6          00066345a372bab2                    testvg1         active
	hdisk7          000664cd9ac59525                    None            
	hdisk8          00066345e6513be8                    testvg          active
	hdisk9          none                                None            
	hdisk10         none                    			None            

	# lsattr -El hdisk10          
	clr_q         no                                       Device CLEARS its Queue on error True
	host_addr     172.29.88.61                             Hostname or IP Address           False
	location                                               Location Label                   True
	lun_id        0x0                                      Logical Unit Number ID           False
	max_transfer  0x40000                                  Maximum TRANSFER Size            True
	port_num      0xcbc                                    PORT Number                      False
	pvid          none                                     Physical volume identifier       False
	q_err         yes                                      Use QERR bit                     True
	q_type        simple                                   Queuing TYPE                     True
	queue_depth   1                                        Queue DEPTH                      True
	reassign_to   120                                      REASSIGN time out value          True
	rw_timeout    30                                       READ/WRITE time out value        True
	start_timeout 60                                       START unit time out value        True
	target_name   iqn.2000-01.com.synology:themain-2nd.erp Target NAME                      False

4.mkvg

	#chdev -l hdisk10 -a pv=yes
	hdisk10 changed

	# lsattr -El hdisk10
	clr_q         no                                       Device CLEARS its Queue on error True
	host_addr     172.29.88.61                             Hostname or IP Address           False
	location                                               Location Label                   True
	lun_id        0x0                                      Logical Unit Number ID           False
	max_transfer  0x40000                                  Maximum TRANSFER Size            True
	port_num      0xcbc                                    PORT Number                      False
	pvid          00066345e9028f430000000000000000         Physical volume identifier       False
	q_err         yes                                      Use QERR bit                     True
	q_type        simple                                   Queuing TYPE                     True
	queue_depth   1                                        Queue DEPTH                      True
	reassign_to   120                                      REASSIGN time out value          True
	rw_timeout    30                                       READ/WRITE time out value        True
	start_timeout 60                                       START unit time out value        True
	target_name   iqn.2000-01.com.synology:themain-2nd.erp Target NAME                      False

	# lquerypv -h /dev/hdisk10
	00000000   C9C2D4C1 00000000 00000000 00000000  |................|
	00000010   00000000 00000000 00000000 00000000  |................|
	00000020   00000000 00000000 00000000 00000000  |................|
	00000030   00000000 00000000 00000000 00000000  |................|
	00000040   00000000 00000000 00000000 00000000  |................|
	00000050   00000000 00000000 00000000 00000000  |................|
	00000060   00000000 00000000 00000000 00000000  |................|
	00000070   00000000 00000000 00000000 00000000  |................|
	00000080   00066345 E9028F43 00000000 00000000  |..cE...C........|
	00000090   00000000 00000000 00000000 00000000  |................|
	000000A0   00000000 00000000 00000000 00000000  |................|
	000000B0   00000000 00000000 00000000 00000000  |................|
	000000C0   00000000 00000000 00000000 00000000  |................|
	000000D0   00000000 00000000 00000000 00000000  |................|
	000000E0   00000000 00000000 00000000 00000000  |................|
	000000F0   00000000 00000000 00000000 00000000  |................|

	# mkvg -y datavg hdisk10
	0516-1184 mkvg: IO failure on hdisk10.
	0516-862 mkvg: Unable to create volume group.

	# lsdev -l hdisk10                             
	hdisk10 Available  Other iSCSI Disk Drive

	# errpt -a | more
	---------------------------------------------------------------------------
	LABEL:          SC_DISK_ERR2
	IDENTIFIER:     B6267342
	
	Date/Time:       Wed May 29 14:24:25 BEIST 2013
	Sequence Number: 154487
	Machine Id:      00066345D600
	Node Id:         erp
	Class:           H
	Type:            PERM
	Resource Name:   hdisk10         
	Resource Class:  disk
	Resource Type:   osdisk
	Location:        
	VPD:             
	        Manufacturer................SYNOLOGY
	        Machine Type and Model......iSCSI Storage
	        ROS Level and ID............332E3100
	        Device Specific.(Z0)........000005021FB00032
	
	Description
	DISK OPERATION ERROR
	
	Probable Causes
	DASD DEVICE
	
	Failure Causes
	DISK DRIVE
	DISK DRIVE ELECTRONICS
	
	        Recommended Actions
	        PERFORM PROBLEM DETERMINATION PROCEDURES
	
	Detail Data
	PATH ID
	           0
	SENSE DATA
	0A00 2E00 0000 0047 0000 0100 0000 0000 0000 0000 0000 0000 0102 0000 7000 0500 
	0000 0000 0000 0000 2000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 
	0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 
	0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 
	0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 
	0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 
	0000 0000 0000 0000 0000 0000 1C00 0000 0004 0000 0000 0000 0000 0000 0000 0000 
	0000 0029 0017 
	---------------------------------------------------------------------------

5.mkfs

	# mkfs -o log=INLINE -Vjfs2 /dev/hdisk10
	mkfs: destroy /dev/hdisk10 (yes)? yes
	logform: Format inline log for  <y>?y
	File system created successfully.
	1072660308 kilobytes total disk space.
	Device /dev/hdisk10:
	  Standard empty filesystem
	  Size:           2145320616 512-byte (DEVBLKSIZE) blocks

6.mount/umount

	# mount -o log=NULL /dev/hdisk10 /data
	mount: 0506-323 Cannot get information about log device NULL.
	# mount -V jfs2 -o log=INLINE /dev/hdisk10 /data

	# umount /data

	# rmdev -Rdl hdisk10
	hdisk10 deleted

	# cfgmgr -l iscsi0
	# lspv
	hdisk0          00066345812c3186                    rootvg          active
	hdisk1          000663453bb98660                    rootvg          active
	hdisk2          00066345863e157b                    testvg          active
	hdisk3          000664cd863d5728                    prodvg          
	hdisk6          00066345a372bab2                    testvg1         active
	hdisk7          000664cd9ac59525                    None            
	hdisk8          00066345e6513be8                    testvg          active
	hdisk9          none                                None            
	hdisk10         00066345eef33fa1                    None        
    
	# mount -V jfs2 -o log=INLINE /dev/hdisk10 /data

##Reference

* [AIX 5.3/6.1 ISCSI howto](http://dawangliang.blog.163.com/blog/static/1879031682011543446756/)
* [AIX 5.3 iSCSI softwre initiator](http://publib.boulder.ibm.com/infocenter/pseries/v5r3/index.jsp?topic=/com.ibm.aix.commadmn/doc/commadmndita/iscsi_config.htm)
* [AIX 5.3 iSCSI target file reference](http://publib.boulder.ibm.com/infocenter/pseries/v5r3/index.jsp?topic=/com.ibm.aix.commadmn/doc/commadmndita/iscsi_config.htm)

##Detect CPU Temperatur

os kernel

	[root@dev ~]# uname -a
	Linux dev.egolife.com 2.6.32-358.el6.x86_64 #1 SMP Fri Feb 22 13:35:02 PST 2013 x86_64 x86_64 x86_64 GNU/Linux

install lm_sensors

	[root@dev ~]# yum install -y lm_sensors
	Loaded plugins: refresh-packagekit, security
	Setting up Install Process
	Resolving Dependencies
	--> Running transaction check
	---> Package lm_sensors.x86_64 0:3.1.1-17.el6 will be installed
	--> Finished Dependency Resolution
	
	Dependencies Resolved
	
	========================================================================================================================================
	 Package                           Arch                          Version                              Repository                   Size
	========================================================================================================================================
	Installing:
	 lm_sensors                        x86_64                        3.1.1-17.el6                         base                        123 k
	
	Transaction Summary
	========================================================================================================================================
	Install       1 Package(s)
	
	Total download size: 123 k
	Installed size: 350 k
	Downloading Packages:
	lm_sensors-3.1.1-17.el6.x86_64.rpm                                                                               | 123 kB     00:00     
	Running rpm_check_debug
	Running Transaction Test
	Transaction Test Succeeded
	Running Transaction
	  Installing : lm_sensors-3.1.1-17.el6.x86_64                                                                                       1/1 
	  Verifying  : lm_sensors-3.1.1-17.el6.x86_64                                                                                       1/1 
	
	Installed:
	  lm_sensors.x86_64 0:3.1.1-17.el6                                                                                                      
	
	Complete!

detect sensors

	[root@dev ~]# sensors-detect 
	# sensors-detect revision 1.1
	# System: IBM System x3650 M4 -[7915I31]-
	# Board: IBM 00J6520
	
	This program will help you determine which kernel modules you need
	to load to use lm_sensors most effectively. It is generally safe
	and recommended to accept the default answers to all questions,
	unless you know what you're doing.
	
	Some south bridges, CPUs or memory controllers contain embedded sensors.
	Do you want to scan for them? This is totally safe. (YES/no): Y
	Silicon Integrated Systems SIS5595...                       No
	VIA VT82C686 Integrated Sensors...                          No
	VIA VT8231 Integrated Sensors...                            No
	AMD K8 thermal sensors...                                   No
	AMD Family 11h thermal sensors...                           No
	Intel digital thermal sensor...                             Success!
	    (driver `coretemp')
	Intel AMB FB-DIMM thermal sensor...                         No
	VIA C7 thermal and voltage sensors...                       No
	
	Some Super I/O chips contain embedded sensors. We have to write to
	standard I/O ports to probe them. This is usually safe.
	Do you want to scan for Super I/O sensors? (YES/no): Y
	Probing for Super-I/O at 0x2e/0x2f
	Trying family `National Semiconductor'...                   Yes
	Found unknown chip with ID 0x3711
	Probing for Super-I/O at 0x4e/0x4f
	Trying family `National Semiconductor'...                   No
	Trying family `SMSC'...                                     No
	Trying family `VIA/Winbond/Nuvoton/Fintek'...               No
	Trying family `ITE'...                                      No
	
	Some systems (mainly servers) implement IPMI, a set of common interfaces
	through which system health data may be retrieved, amongst other things.
	We first try to get the information from SMBIOS. If we don't find it
	there, we have to read from arbitrary I/O ports to probe for such
	interfaces. This is normally safe. Do you want to scan for IPMI
	interfaces? (YES/no): Y
	Found `IPMI BMC KCS' at 0xcc0...                            Success!
	    (confidence 8, driver `ipmisensors')
	
	Some hardware monitoring chips are accessible through the ISA I/O ports.
	We have to write to arbitrary I/O ports to probe them. This is usually
	safe though. Yes, you do have ISA I/O ports even if you do not have any
	ISA slots! Do you want to scan the ISA I/O ports? (YES/no): Y
	Probing for `National Semiconductor LM78' at 0x290...       No
	Probing for `National Semiconductor LM79' at 0x290...       No
	Probing for `Winbond W83781D' at 0x290...                   No
	Probing for `Winbond W83782D' at 0x290...                   No
	
	Lastly, we can probe the I2C/SMBus adapters for connected hardware
	monitoring devices. This is the most risky part, and while it works
	reasonably well on most systems, it has been reported to cause trouble
	on some systems.
	Do you want to probe the I2C/SMBus adapters now? (YES/no): Y
	Found unknown SMBus adapter 8086:1d22 at 0000:00:1f.3.
	Sorry, no supported PCI bus adapters found.
	Module i2c-dev loaded successfully.
	
	Next adapter: SMBus I801 adapter at 4000 (i2c-0)
	Do you want to scan it? (YES/no/selectively): Y
	Client found at address 0x48
	Probing for `National Semiconductor LM75'...                No
	Probing for `Dallas Semiconductor DS75'...                  No
	Probing for `National Semiconductor LM77'...                No
	Probing for `Dallas Semiconductor DS1621/DS1631'...         No
	Probing for `Maxim MAX6650/MAX6651'...                      No
	Probing for `National Semiconductor LM73'...                No
	Probing for `National Semiconductor LM92'...                No
	Probing for `National Semiconductor LM76'...                No
	Probing for `Maxim MAX6633/MAX6634/MAX6635'...              No
	
	Now follows a summary of the probes I have just done.
	Just press ENTER to continue: 
	
	Driver `coretemp':
	  * Chip `Intel digital thermal sensor' (confidence: 9)
	
	Driver `ipmisensors':
	  * ISA bus, address 0xcc0
	    Chip `IPMI BMC KCS' (confidence: 8)
	
	Warning: the required module ipmisensors is not currently installed
	on your system. If it is built into the kernel then it's OK.
	Otherwise, check http://www.lm-sensors.org/wiki/Devices for
	driver availability.
	
	Do you want to overwrite /etc/sysconfig/lm_sensors? (YES/no): Y
	Starting lm_sensors: loading module ipmi-si coretemp       [  OK  ]
	Unloading i2c-dev... OK

note: yes|sensors-detect

get current cup temperature

	[root@dev ~]# sensors
	coretemp-isa-0000
	Adapter: ISA adapter
	Physical id 0: +39.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 0:        +35.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 1:        +36.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 2:        +36.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 3:        +39.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 4:        +33.0°C  (high = +81.0°C, crit = +91.0°C)  
	Core 5:        +38.0°C  (high = +81.0°C, crit = +91.0°C) 

##Reference

* [Linux下查看CPU温度](http://www.linuxsong.org/2010/09/linux-look-cpu-temperature/)

#设计思路

传统的博客程序，一般都有后台数据库。当新发布一篇博客时，基本步骤就需要：1.登陆->2.新建博客-3.填写标题-4.复制内容-5.填写关键字-6.选择分类-7.点击发布等，过程十分繁琐。

前段时间正好看到轻量级博客[Letterpress](https://github.com/an0/Letterpress)中提到的想法：直接上传markdown格式的博客原文，让程序自动去处理和渲染。这样一来发布博客确实可省去很多体力活；另外，作为python新手，也想动手实践下，至于实现好坏暂且不表。

主要思路：

* 页面模板：

	* 去年10月起折腾过Movable Type 5，虽博客写得少，但还是看过官方的文档；以及近来流行的Bootstrap项目，也动手进行过一些定制化，Movable Type后台的模板设计很值得学习和借鉴，所以照搬过来。

	* 博客主要采用典型的三栏设计，据此对模板进行抽象和分类，即可分为主模板、layout、modules、widgets和misc等，模板之间可以相互引用。这些模板根据URL请求获取处理后的参数，完成最终渲染并响应给用户。

* 核心逻辑：

	* 初始化。EntryService读取指定路径下的markdown文件，进行初始化，以`2013-7-3_github_tips.md`为例：
		
		* 1.生成URL，即根据文件名生成URL。如 `2013-7-3_github_tips.md` -> `/blog/2013/07/03/github_tips.html`，对应的raw文件URL则为`/raw/2013/07/03/github_tips.md`。
		* 2.获取时间，优先从文件名中提取，否则获取文件的创建或者更新时间，这里暂未处理好。
	    * 3.获取内容，包括markdown原文内容，以及经过markdown到html转换后的内容。
		* 4.其他初始化，如tag, category, calendar, archive等。
		* 5.存储博客： url(key) -> entry(value) 为一一对一个关系，这里使用dict将entry保存在内存中。

	* 响应请求。当浏览器客户端进行访问时，Web.py应用根据URL路由到指定的Controller，并由EntryService处理逻辑（包括搜索、查看博客、查博客的Raw内容、博客和Raw归档等），返回响应参数（针对模板，该参数也创建了数据模型，可以简化参数传递）；再由控制器根据响应参数判断渲染的模板文件，响应给用户。若请求博客的Raw内容，则直接返回文本内容不用模板渲染。

    * 动态监听。使用pyinotify监听文件系统中的指定博客路径，当该路径下有新增、修改、删除博客的操作时，触发监听事件，并通知EntryService进行相应的新增、修改、删除操作，以更新内存中的博客内容。在本项目中，仅搭建了一个可用的模型，pyinotify使用了一个单独的线程，EntryService在整个程序运行过程，仅有一个实例。

#项目结构

	blog/
	├── __init__.py            #初始化文件，主要加载pyinotify，监听blog/raw/entry路径下md的新增、修改、删除
	├── blog.py                #博客程序入口，运行： $ cd blog; python blog.py > blog.log 2>&1 &
	├── config.py			   #博客配置，主要是站点、文件系统路径、博客URL路由以及全局环境的配置
	├── controller.py		   #控制器模块，分发URL请求，并响应响应的视图
	├── service.py			   #服务模块，初始化博客系统，处理URL请求逻辑
	├── model.py			   #模型模块，针对博客系统的Entry,Page,Tag,Category等设置的数据模型，以及模板中的参数模型
	├── tool.py				   #工具类，如自动提取关键字，将字典转换为对象等，待完善
	├── README.md
	├── raw					   #raw格式文件路径，主要是markdown文件
	│   ├── entry			   #博客文件
	│   ├── page			   #页面文件，相对于博客而言，页面处理简单很多
	│   └── tweet				
	├── static				   #静态文件，web.py框架的要求，此路径下的文件可以有web.py当做静态文件处理
	│   ├── css
	│   ├── favicon.ico
	│   ├── img
	│   └── js
	└─── template			   #模板库，主要参考Movable Type 5的设计。layout,misc,modules,widgets为子模板。
	    ├── index.html	       #首页模板
	    ├── search.html		   #搜索模板
	    ├── entry.html		   #Entry/Page详情模板
	    ├── archive.html	   #归档模板
	    ├── atom.xml		   #RSS订阅模板
	    ├── error.html		   #错误页面模板
	    ├── layout			   #布局模板，这里采用 `AA|B|C`的三栏布局，AA放博客主要内容，B、C作为左侧边栏显示widgets
	    │   ├── footer.html
	    │   ├── header.html
	    │   ├── navbar.html
	    │   ├── three
	    │   │   ├── primary.html
	    │   │   └── secondary.html
	    │   └── two
	    ├── misc			  #杂项资源
	    │   ├── ads
	    │   │   └── ad.html
	    │   └── analytics.html
		├── modules			   #模板模板，在AA布局中使用
		│   ├── archive.html
		│   ├── category.html
		│   ├── comment.html
		│   ├── entry.html
		│   ├── error.html
		│   ├── excerpt.html
		│   ├── info.html
		│   ├── pager.html
		│   ├── related.html
		│   ├── search.html
		│   └── tag.html
		└── widgets           #小工具模板，在B、C侧边栏中使用
		    ├── about.html
		    ├── archive.html
		    ├── calendar.html
		    ├── category.html
		    ├── link.html
		    ├── powered.html
		    ├── recently.html
		    └── tag.html
	
#要求

* 仅限于Linux平台。因使用[pyinotify](https://github.com/seb-m/pyinotify)有平台限制。Pyinotify监听指定路径，即可自动处理新增、更新以及删除的博客。详见[pyinotify](https://github.com/seb-m/pyinotify)。

* Markdown。将markdown格式文件渲染成HTML，详见[markdown](https://github.com/waylan/Python-Markdown)。

* Web.py。使用的Web框架，由已故的[Aaron Swartz](http://www.aaronsw.com/)开发，详见[Web.py](http://webpy.org)。

* Python 2.7.5。本博客程序仅在Python 2.7.5下测试通过。

#运行

克隆博客代码

	$ cd ~

	$ git clone git@github.com:dylanninin/blog.git
 
切换路径
	
	$ cd blog

启动博客

	$ python blog.py > blog.log 2>&1 &  #Listen on 0.0.0.0:8080 default


#博客

环境准备

	$ ln -s ~/blog/raw/entry  entry

	$ cd entry

发布博客

	$ rz 				# use ZMODEM (Batch) file receive tool to send your local markdown file
						# to blog server. md format is usually yyyy-mm-dd_file_name.md

删除博客

	$ rm 2013-07-02_github_tips.md

#效果

 * Online Demo: [http://dylanninin.com:8080/](http://dylanninin.com:8080)
 * Movable Type 5：[http://dylanninin.com/](http://dylanninin.com/)	

#计划

* 自动提取关键字

* 自动摘要

* 自动查找相关博客

* 功能完善和bug排除

* 代码重构

#参考

* [Web.py](http://webpy.org)
* [pyinotify](https://github.com/seb-m/pyinotify)
* [Bootstrap](http://twitter.github.com/bootstrap)
* [Movable Type](http://dylanninin.com)

##About

This blog is about IT technology such as web, database and development.
Additional, I'll share some of my individual life, especially about reading and
varied opinions. Hope you will like it and discuss.

##Work

IT Engineer 2011.7 ~ Now

##Info

* ![douban](/static/img/about/douban.png) [douban.com](http://www.douban.com/people/dylanninin/)
* ![weibo](/static/img/about/weibo.png) [weibo.com](http://weibo.com/dylanninin)
* ![twitter](/static/img/about/twitter.png) [twitter.com](https://twitter.com/dylanninin)
* ![facebook](/static/img/about/facebook.png) [facebook.com](https://www.facebook.com/dylanninin)
* ![blog](/static/img/about/dylanninin.png) [dylanninin.com](http://www.dylanninin.com)
* ![me](/static/img/about/me.png) [about.me](http://about.me/dylanninin)

##Contact

* ![gmail](/static/img/about/gmail.png) dylanninin@gmail.com

#设计思路

传统的博客程序，一般都有后台数据库。当新发布一篇博客时，基本步骤就需要：1.登陆->2.新建博客-3.填写标题-4.复制内容-5.填写关键字-6.选择分类-7.点击发布等，过程十分繁琐。

前段时间正好看到轻量级博客[Letterpress](https://github.com/an0/Letterpress)中提到的想法：直接上传markdown格式的博客原文，让程序自动去处理和渲染。这样一来发布博客确实可省去很多体力活；另外，作为python新手，也想动手实践下，至于实现好坏暂且不表。

主要思路：

* 页面模板：

	* 去年10月起折腾过Movable Type 5，虽博客写得少，但还是看过官方的文档；以及近来流行的Bootstrap项目，也动手进行过一些定制化，Movable Type后台的模板设计很值得学习和借鉴，所以照搬过来。

	* 博客主要采用典型的三栏设计，据此对模板进行抽象和分类，即可分为主模板、layout、modules、widgets和misc等，模板之间可以相互引用。这些模板根据URL请求获取处理后的参数，完成最终渲染并响应给用户。

* 核心逻辑：

	* 初始化。EntryService读取指定路径下的markdown文件，进行初始化，以`2013-7-3_github_tips.md`为例：
		
		* 1.生成URL，即根据文件名生成URL。如 `2013-7-3_github_tips.md` -> `/blog/2013/07/03/github_tips.html`，对应的raw文件URL则为`/raw/2013/07/03/github_tips.md`。
		* 2.获取时间，优先从文件名中提取，否则获取文件的创建或者更新时间，这里暂未处理好。
	    * 3.获取内容，包括markdown原文内容，以及经过markdown到html转换后的内容。
		* 4.其他初始化，如tag, category, calendar, archive等。
		* 5.存储博客： url(key) -> entry(value) 为一一对一个关系，这里使用dict将entry保存在内存中。

	* 响应请求。当浏览器客户端进行访问时，Web.py应用根据URL路由到指定的Controller，并由EntryService处理逻辑（包括搜索、查看博客、查博客的Raw内容、博客和Raw归档等），返回响应参数（针对模板，该参数也创建了数据模型，可以简化参数传递）；再由控制器根据响应参数判断渲染的模板文件，响应给用户。若请求博客的Raw内容，则直接返回文本内容不用模板渲染。

    * 动态监听。使用pyinotify监听文件系统中的指定博客路径，当该路径下有新增、修改、删除博客的操作时，触发监听事件，并通知EntryService进行相应的新增、修改、删除操作，以更新内存中的博客内容。在本项目中，仅搭建了一个可用的模型，pyinotify使用了一个单独的线程，EntryService在整个程序运行过程，仅有一个实例。

#项目结构

	blog/
	├── __init__.py            #初始化文件，主要加载pyinotify，监听blog/raw/entry路径下md的新增、修改、删除
	├── blog.py                #博客程序入口，运行： $ cd blog; python blog.py > blog.log 2>&1 &
	├── config.py			   #博客配置，主要是站点、文件系统路径、博客URL路由以及全局环境的配置
	├── controller.py		   #控制器模块，分发URL请求，并响应响应的视图
	├── service.py			   #服务模块，初始化博客系统，处理URL请求逻辑
	├── model.py			   #模型模块，针对博客系统的Entry,Page,Tag,Category等设置的数据模型，以及模板中的参数模型
	├── tool.py				   #工具类，如自动提取关键字，将字典转换为对象等，待完善
	├── README.md
	├── raw					   #raw格式文件路径，主要是markdown文件
	│   ├── entry			   #博客文件
	│   ├── page			   #页面文件，相对于博客而言，页面处理简单很多
	│   └── tweet				
	├── static				   #静态文件，web.py框架的要求，此路径下的文件可以有web.py当做静态文件处理
	│   ├── css
	│   ├── favicon.ico
	│   ├── img
	│   └── js
	└─── template			   #模板库，主要参考Movable Type 5的设计。layout,misc,modules,widgets为子模板。
	    ├── index.html	       #首页模板
	    ├── search.html		   #搜索模板
	    ├── entry.html		   #Entry/Page详情模板
	    ├── archive.html	   #归档模板
	    ├── atom.xml		   #RSS订阅模板
	    ├── error.html		   #错误页面模板
	    ├── layout			   #布局模板，这里采用 `AA|B|C`的三栏布局，AA放博客主要内容，B、C作为右侧边栏显示widgets
	    │   ├── footer.html
	    │   ├── header.html
	    │   ├── navbar.html
	    │   ├── three
	    │   │   ├── primary.html
	    │   │   └── secondary.html
	    │   └── two
	    ├── misc			  #杂项资源
	    │   ├── ads
	    │   │   └── ad.html
	    │   └── analytics.html
		├── modules			   #模板模板，在AA布局中使用
		│   ├── archive.html
		│   ├── category.html
		│   ├── comment.html
		│   ├── entry.html
		│   ├── error.html
		│   ├── excerpt.html
		│   ├── info.html
		│   ├── pager.html
		│   ├── related.html
		│   ├── search.html
		│   └── tag.html
		└── widgets           #小工具模板，在B、C侧边栏中使用
		    ├── about.html
		    ├── archive.html
		    ├── calendar.html
		    ├── category.html
		    ├── link.html
		    ├── powered.html
		    ├── recently.html
		    └── tag.html
	
#要求

* 仅限于Linux平台。因使用[pyinotify](https://github.com/seb-m/pyinotify)有平台限制。Pyinotify监听指定路径，即可自动处理新增、更新以及删除的博客。详见[pyinotify](https://github.com/seb-m/pyinotify)。

* Markdown。将markdown格式文件渲染成HTML，详见[markdown](https://github.com/waylan/Python-Markdown)。

* Web.py。使用的Web框架，由已故的[Aaron Swartz](http://www.aaronsw.com/)开发，详见[Web.py](http://webpy.org)。

* Python 2.7.5。本博客程序仅在Python 2.7.5下测试通过。

#运行

克隆博客代码

	$ cd ~

	$ git clone https://github.com/dylanninin/blog.git
 
切换路径
	
	$ cd blog

启动博客

	$ python blog.py > blog.log 2>&1 &  #Listen on 0.0.0.0:8080 default


#博客

环境准备

	$ ln -s ~/blog/raw/entry  entry

	$ cd entry

发布博客

	$ rz 				# use ZMODEM (Batch) file receive tool to send your local markdown file
						# to blog server. md format is usually yyyy-mm-dd_file_name.md

删除博客

	$ rm 2013-07-02_github_tips.md

#效果

 * Online Demo: [http://dylanninin.com:8080/](http://dylanninin.com:8080)
 * Movable Type 5：[http://dylanninin.com/](http://dylanninin.com/)	

#计划

* 自动提取关键字

* 自动摘要

* 自动查找相关博客

* 功能完善和bug排除

* 代码重构

#更新 2013-11-23

本计划使用[TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf)以及[Jieba](https://github.com/fxsjy/jieba)，自动提取博客的关键字、摘要以及相似文章，但效果并不理想。

这里模仿 Githuh Pages中的做法，在博客的开始声明一些属性：

	---
	title: the title, default None if it's empty
	category: category, default Uncategorised if it's empty.
	tags: [tag1, tag2], default [Untagged] if it's empty.
	---

因使用[YAML](http://en.wikipedia.org/wiki/Yaml)解析，需要新增[PyYAML](http://pyyaml.org/)包。

另外，此博客的在线[demo](http://ec2-54-254-45-254.ap-southeast-1.compute.amazonaws.com/)。


* 阮一峰的网络日志—— TF-IDF与余弦相似性的应用系列：[自动提取关键字](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)；[找出相似文章](http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html)；[自动摘要](http://www.ruanyifeng.com/blog/2013/03/automatic_summarization.html)
* Python 中文分词[Jieba](https://github.com/fxsjy/jieba)

#参考

* [Web.py](http://webpy.org)
* [pyinotify](https://github.com/seb-m/pyinotify)
* [Bootstrap](http://twitter.github.com/bootstrap)
* [Movable Type](http://dylanninin.com)

