Avant Propos
==========

Performance ou performance ?
---------------------------------------------

En collectant un peu les avis de chacun sur les performances des sites
web, je me suis aperçu que le terme de performance en français recoupe
beaucoup de concepts. Chacun y voit quelque chose de différent, et
parfois même les gens parlent entre eux sans comprendre qu'ils parlent
de sujets entièrement différents.

Voici un peu ce que les gens entendent par « performance des sites web » :

-   bénéfices tirés du site, souvent financiers,
-   taux de conversion pour les ventes ou les envois formulaires,
-   nombre de visiteurs uniques, ou de pages vues,
-   ressources utilisées sur les serveurs web ou base de données,
-   temps de réaction des serveurs web ou des équipements réseau,
-   le temps d'exécution d'une page web PHP ou Java sur le serveur,
-   temps de chargement des pages web.

Alors précisons : ce livre ne s'occupe que du dernier item, le temps de
chargement des pages web par le navigateur sur le poste de l'internaute,
et uniquement de cela. Même si vous avez probablement déjà feuilleté le
livre ou regardé le sommaire, cette précision reste importante.
N'oubliez pas de la faire vous-même quand vous parlez de performance des
sites web à quelqu'un après m'avoir lu.

Vous le verrez, ce livre est même plus pointu que cela, puisque dans le
temps de chargement des pages web je ne m'occuperai ni d'architecture
système, ni de programmation Java ou PHP, ni de bases de données... mais
vous aurez l'occasion de découvrir que je vous propose bien mieux.

Ce que nous allons voir ensemble
-------------------------------------------------

Cet ouvrage est découpé en quatre sections et treize chapitres. Les deux
premiers sont des chapitres d'introduction, ils vous permettront de vous
familiariser avec la notion de performance et les concepts techniques
indispensables pour la suite de votre lecture :

1. Users really respond to speed
2. Premiers concepts

Les six chapitres suivants, qui composent le corps du livre, décriront 
tous les principes techniques, les recommandations de performance, 
la mise en œuvre, le pourquoi et le comment. Ils sont plus ou moins placés par 
ordre de priorité, aussi je vous conseille de les lire séquentiellement 
au moins une fois.

3. Travailler avec les caches HTTP
4. Moins de requêtes HTTP
5. Contenus plus petits
6. Parallélisations 
7. Réduire les espaces vides
8. Applicatif

Vient ensuite un chapitre plus conversationnel qui a pour but de
discuter des différentes approches et de gérer l'aspect non technique de
la mise en œuvre :

9. Optimiser  

Enfin, les derniers chapitres collectent toutes les références utiles.
Ils référencent les recommandations de performance, les termes et
concepts techniques, les outils de mesure ou de correction, et les sites
ou documentations externes. Ces chapitres vous serviront d'aide au cours
de votre lecture et d'index par la suite lors de la mise en œuvre :

10. Mesurer la performance
11. Définitions et concepts techniques
12. Outils utiles
13. Références externes

Ce dont vous aurez besoin
--------------------------------

Avant la lecture, préparez un poste de développement qui vous servira
pour vos tests et vos analyses. Sur ce poste, désactivez tous les
logiciels de vie privée ou les options agressives de surveillance réseau
de votre anti-virus, ainsi que tous les logiciels ou extensions
anti-publicités. Ces logiciels pourraient modifier et fausser toutes les
mesures. Ils sont aussi connus pour parfois modifier le trafic réseau en
profondeur et vous empêcheront de réaliser vos expériences.

Côté navigateurs, vous aurez besoin de Mozilla Firefox ou Chrome pour
plusieurs extensions. Créez un profil dédié à vos tests performance sous
Mozilla Firefox afin de ne pas être impacté par les différentes autres
extensions que vous avez. Pour cela, lancez Firefox avec l'option
« -profilemanager ».

-   Mozilla Firefox :
    [http://www.mozilla-europe.org/](http://www.mozilla-europe.org/) 
-   Gérer les profils Firefox :
    [http://support.mozilla.com/fr/kb/managing+profiles](http://support.mozilla.com/fr/kb/managing+profiles) 
-   Chrome :
    [http://www.google.com/chrome?hl=fr](http://www.google.com/chrome?hl=fr) 

Dans Mozilla Firefox, installez la toute dernière version de Firebug
ainsi que les extensions Yslow et Pagespeed. Pour Chrome, installez
l'extension Speed Tracer.

-   Firebug : [http://getfirebug.com/](http://getfirebug.com/) 
-   Yslow :
    [http://developer.yahoo.com/yslow/](http://developer.yahoo.com/yslow/) 
-   Pagespeed :
    [http://code.google.com/speed/page-speed/](http://code.google.com/speed/page-speed/) 
-   Speed Tracer :
    [http://code.google.com/intl/fr/webtoolkit/speedtracer/](http://code.google.com/intl/fr/webtoolkit/speedtracer/) 

Vous aurez aussi besoin, tôt ou tard, d'un proxy de débogage, qui vous
permet d'analyser le trafic réseau. Vous pouvez par exemple utiliser
Fidler ou Charles. Pour jouer avec le navigateur vous pouvez aussi
installer l'extension TamperData de Firefox.

-   Fiddler :
    [http://www.fiddler2.com/fiddler2/](http://www.fiddler2.com/fiddler2/) 
-   Charles :
    [http://www.charlesproxy.com/](http://www.charlesproxy.com/) 
-   TamperData :
    [https://addons.mozilla.org/fr/firefox/addon/966/](https://addons.mozilla.org/fr/firefox/addon/966/) 

Enfin, vous finirez très probablement par avoir un raccourci vers le
service en ligne webpagetest.org, alors autant le créer maintenant :

-   Webpagetest :
    [http://www.webpagetest.org/](http://www.webpagetest.org/) 

Les dernières versions de Microsoft Internet Explorer, Safari et Opera
ont aussi des outils pour tester et analyser la performance des pages
web. Il peut être intéressant de les installer aussi, afin de pouvoir
étudier des ralentissements spécifiques à l'un ou l'autre de ces
navigateurs.

Parti pris et organisation
-------------------------------

Enfin, j'ai dans ce livre trois partis pris très explicites :

-   ne laisser aucun internaute de côté, que ce soit à cause de son
    navigateur (même s'il est faiblement utilisé), de ses difficultés
    matérielles (faible bande passante, mauvaise connectivité, petit
    écran) ou de ses difficultés physiques (handicap divers) ;
-   faire de la qualité, donc éviter de proposer des solutions qui
    provoqueront des incompatibilités avec les normes et standards ou
    qui seront impossibles à maintenir ensuite ;
-   tout vous expliquer en détail, quitte à parfois plus me rapprocher
    de l'encyclopédie que du cahier de recettes à appliquer sans
    comprendre, parce que les choses changent très vite sur le web,
    qu'un simple cahier de recettes risquerait de devenir obsolète, et que
    j'adhère totalement au principe qu'il vaut mieux apprendre à pêcher
    que de donner du poisson.

Le but final restant tout de même de vous permettre de mettre en œuvre
rapidement et simplement ce que vous apprenez, je ponctue le livre de
recommandations claires mises en surbrillance. Si vous voulez aller
vite, il vous suffit de les suivre et de lire les détails uniquement
quand vous en avez besoin.

Compromis
---------------

Au fil de ce livre je vous proposerai des recommandations.
Généralement elles sont applicables sans avoir à dégrader l'expérience
utilisateur ou vos choix graphiques. Par contre, dans de rares cas, il
vous faudra peut être faire des compromis entre la performance, la
richesse des pages, la qualité graphique, la complexité et le temps de
développement. 

Habituellement, et les graphistes vous y encourageront, tous choisissent
assez rapidement de sacrifier la performance en trouvant inacceptable de
« dégrader l'expérience utilisateur ». On privilégie ce qui est
visible : le graphisme et le contenu de la page.

Dites-vous bien cependant que la performance fait justement partie de
l'expérience utilisateur, et elle en est même un des critères majeurs. Faites
donc attention à ne pas vous focaliser uniquement sur l'évident visible
dans la page. Acceptez de faire des compromis avec vous-même et avec vos
interlocuteurs.

Si à un moment vous tranchez en faveur d'un des critères (performance,
graphisme, contenu, temps de développement, complexité) de manière
radicale sans avoir accepté de compromis qui fasse faire un pas sur
chaque domaine, c'est probablement une erreur.

Si les compromis sont difficiles ce n'est pas qu'ils sont complexes à
trouver ou à réaliser, c'est qu'il est difficile de se convaincre qu'il
faut abandonner notre position intégriste du « je ne lâche rien là-dessus ».
Ce qui est délicat, c'est de se convaincre soi-même (et les
autres) de faire un compromis.

Pour votre bien, et pour celui de vos visiteurs, acceptez les compromis,
c'est indispensable (et oui, cela veut aussi dire parfois ne pas
appliquer toutes mes recommandations mais les adapter, tant que ce n'est
pas « à chaque fois » et que vous faites vous aussi un pas dans ma
direction).

78 % des statistiques sont fausses (les miennes aussi, dont celle-ci)
---------------------------------------------------------------------------------

Les statistiques et valeurs ne sont significatives que dans le contexte
dans lequel elles ont été créées : version du navigateur, latence de la
connexion, nombre de files de téléchargement simultanées, date et heure
de mesure, etc. Je ne prétends donc pas avoir une représentation
objective des sites testés dans les différents tableaux.

Pire, le logiciel de mesure utilisé peut avoir un impact sur la mesure
elle-même. Chacun étant imparfait d'une façon différente, le résultat
peut varier fortement. Il est donc parfois vain de comparer des chiffres
obtenus avec deux logiciels différents. Pour cette raison, deux
paragraphes de ce livre peuvent donner des résultats différents pour un même
site, sans forcément qu'il y ait contradiction.

Les statistiques réalisées permettent d’établir des comparaisons entre
les différents sites dans les mêmes conditions, ou de vérifier les gains
sur un même site avec le même logiciel avant et après les actions
correctrices. Il ne faut parfois pas leur donner plus de poids : ce ne
sont pas les chiffres objectifs mais des mesures qui permettent de
donner des références et des guides, rien de plus.


Users really respond to speed
=====================

(en français : les utilisateurs réagissent vraiment à la vitesse)

Ce titre est une citation de Marissa Mayer, lorsqu'elle était vice-présidente de
la section recherche et expérience utilisateur de Google. Nous le
verrons, Google a publié des chiffres impressionnants, tout comme Amazon ; nous les
découvrirons dans ce chapitre.

Il nous a semblé important d'insérer la question du pourquoi en tout premier chapitre.
Quiconque évangélise au sujet de la performance des sites web peut se retrouver
face à un mur à chaque fois. Tout le monde s’accorde à dire
qu’il s’agit d’un sujet important mais dès qu’on entre dans le concret
il y a toujours une bonne raison pour laisser ça de côté, souvent une
très bonne raison, même. Bref, « c’est important mais pas pour moi, ou
pas maintenant ».

L'évidence émerge : il faut
d’abord tordre le cou aux idées reçues avant de le développer. C’est la
raison d’être de ce chapitre. Lisez-le en entier avant de passer à la
suite, n’en sautez pas une page. Après sa lecture vous devriez envisager
de bousculer l’ordre de vos priorités et insérer la performance dans les
premières lignes. Si à un quelconque moment vous doutez, revenez faire
un tour sur ce chapitre.

Vous verrez de nombreux liens en bas de page. Il n'est pas nécessaire
d'aller lire les présentations ou les études qui sont liées, même si ça
ne fait pas de mal, mais il me paraissait important de montrer les
sources de chaque affirmation présente ici. Ces liens permettent de
prouver qu'il ne s'agit pas de chiffres jetés au hasard : il s'agit
d'études et tests réalisés par les plus gros acteurs du web, diffusés
publiquement par eux-mêmes, et recoupés par des retours d'expérience de
nombreux autres. Les chapitres suivants seront moins envahis par ces
liens en bas de page, promis.

Les performances impactent votre business, Amazon et Aberdeen
--------------------------------------------------------------------------------

Pour attirer votre attention, voici les premiers chiffres du livre : 

> + 100 ms de latence → - 1 % de ventes [^1]

  [^1]: Greg Linden, Amazon.com, dans sa présentation [« Make Data Useful » donnée en novembre 2006, page 15](http://sites.google.com/site/glinden/Home/StanfordDataMining.2006-11-28.ppt)

Amazon a constaté dans des tests utilisateurs que pour chaque
100 millisecondes de latence en plus lors du chargement de leurs pages
web, s'ensuivait une chute de 1 % des ventes. C’est factuel, simple, et
brutal : on parle de ventes, de l'objectif du site d'Amazon. Perdre des
ventes, c'est un échec qui doit retenir l'attention de tous, du
développeur au directeur.

Une étude Aberdeen Group est tout aussi claire :

> Un délai d'une seconde dans le chargement de la page entraîne une baisse
> de 11 % du nombre de pages vues, une baisse de 16 % de la satisfaction
> client, et une perte de 7 % en conversions. [^2]

  [^2]: Aberdeen Group, étude [« The Performance of Web Applications: Customers are Won or Lost in One Second » du 30 novembre 2008](http://www.aberdeen.com/Aberdeen-Library/5136/RA-performance-web-application.aspx) 

Votre site met probablement plus de deux secondes à se charger
entièrement une fois que l’utilisateur a cliqué sur un lien. À
l’écriture de ces lignes, la page d’accueil de TF1.fr met 5 secondes.
Pour prendre plus extrême, FranceTelecom.com met parfois plus de 11 
secondes.

Bien que tout ne soit pas transposable ou comparable, quand un site web
met plus de 5 secondes, que des centaines de millisecondes se
traduisent déjà en 1 % des ventes fait réfléchir. Combien perdez-vous à
cause des mauvaises performances de votre site web ?

Ce qui est sûr, c’est que les utilisateurs réagissent en fonction des
performances des sites web. Si votre site est lent, ce sont vos
utilisateurs qui partent et leurs intentions d’achat qui s’écroulent.
Bref, ils abandonnent ou se freinent.

Charger la page en une à trois secondes
------------------------------------------------

Vous prendrez le TGV ou le bus pour vos vacances ? Le TGV bien entendu,
sauf pour les marchés de niche comme les étudiants pendant les vacances
d’été. Et encore, le train devient de plus en plus la première option
considérée. Ce n’est pas tellement pour le confort, ce n’est
certainement pas pour le prix, c’est pour la vitesse.

Il y a dix ans la vitesse était un critère exprimé dans les sondages à
propos des sites Internet. Jakob Nielsen, gourou de l’utilisabilité du
web, en parlait déjà en mai 1996 comme une des dix plus grosses erreurs
du design web [^3]. Il en reparle en 1997, 1999, 2000, 2004, 2010 [^4], mais
finalement pas grand chose n’a changé. Malgré l’explosion des débits,
entre 100 et 2 000 fois supérieurs à ceux de 1996, on trouve encore des
sites de multinationales avec des temps de chargement approchant la
dizaine de secondes.

  [^3]: Jakob Nielsen, dans son AlertBox de mai 1996 intitulée [« Original Top Ten Mistakes in Web Design »](http://www.useit.com/alertbox/9605a.html)
  [^4]: Alert box [« Website Response Times » du 21 juin 2010](http://www.useit.com/alertbox/response-times.html)

Le problème est que les usages ont changé, eux. À l’époque le simple
fait de se connecter était considéré comme une réussite. Maintenant
c’est une action quotidienne, voire instinctive pour certains, et on
attend une réactivité en conséquence. La page doit arriver vite, très
vite. 

Lors de recherches sur Internet les visiteurs décident souvent en
quelques dixièmes de secondes si la page est potentiellement
intéressante ou s’ils repartent ailleurs. Au bout de quelques secondes à
attendre, la confiance s’effrite, l’intérêt s’effondre. Après trois à
quatre secondes les abandons deviennent fréquents et la situation peut
devenir catastrophique. C’est parfois le site complet qui est abandonné
par l’utilisateur et l’image négative se diffuse par bouche-à-oreille.

Jakob Nielsen identifie de manière stable depuis plus de quinze ans
trois paliers de perception de la vitesse :

-   0,1 seconde : En dessous du dixième de seconde, la réponse est perçue
    comme instantanée. C'est la limite haute pour la manipulation de
    l'interface (clic sur un bouton, glisser/déposer, menu déroulant,
    etc.)
-   1 seconde : La seconde est le palier de fluidité. Jusque là,
    l'utilisateur perçoit le délai, mais a encore une navigation fluide
    sans avoir l'impression d'attendre la machine. C'est cette limite
    qu'il faudrait viser dans l'idéal pour une navigation interne dans
    un logiciel. Au-delà, il y a une impression d'attente et donc de
    frustration.
-   10 secondes : C'est la limite de l'attention. Jusqu'à 10 secondes,
    l'utilisateur se sent à la merci de la machine, mais peut accepter
    d'attendre. Au-delà, il commence à penser à autre chose, voire à
    réaliser d'autres activités et revient à la machine plus tard pour
    constater la progression.

![Perception en fonction du temps de réponse](img/chap01-perception-en-fonction-du-temps-de-reponse.png)

Bien entendu ces paliers ne sont pas précis. Ils dépendent du contexte
et de l'utilisateur, mais les ordres de grandeur sont valables pour
quasiment toute situation.

Le web a ses habitudes et ses usages. Il est fréquent que les sites
s'affichent en plus d'une seconde. Certains pourtant,
comme Google sur leur page d'accueil, s'attachent à ne pas dépasser
ce palier symbolique.

Nous pouvons probablement parler d'un autre palier intermédiaire entre la
seconde et les 10 secondes. L'aspect fortement concurrentiel du web incite
l'utilisateur à ne pas attendre 10 secondes, il a plus vite
fait d'aller sur un site concurrent qui lui se charge rapidement. Au-delà de 3 à 4 secondes, les conséquences négatives deviennent non
négligeables.

Plus récemment, on a pu voir une statistique tirée de Google Analytics
mettant en rapport, sur un échantillon de pages, le temps de chargement
avec le taux d'abandon. Ce dernier monte progressivement, quasiment
proportionnellement au temps de chargement, sur l'ensemble des valeurs de 0 à 5 secondes.

![Abandons en fonction du temps de chargement](img/chap01-abandons-en-fonction-du-temps-de-chargement.png)[^5]

  [^5]: Illustration originale dans la conférence [« Varnish – A State of the Art High-Performance Reverse Proxy »](http://www.oscon.com/oscon2009/public/schedule/detail/10433) d'Artur Bergman donnée à l'OSCON le 23 juillet 2009

Votre commerce et votre marque en pâtissent, l’étude Akamaï
----------------------------------------------------------------------------

Dans une étude de 2006, Akamaï et JupiterResearch ont mesuré l’impact
des performances auprès de 1 000 marchands en ligne. Ils ont trouvé que
la lenteur d’un site marchand est le second motif principal d’insatisfaction,
juste après les prix trop élevés.

Un tiers des clients ayant estimé leur expérience mauvaise a abandonné
le site, et les trois quarts ne reviendront probablement plus
par la suite. Le palier d'insatisfaction se situait à 4 secondes.
Au-delà, les abandons deviennent très importants.

Akamaï a relancé une étude similaire en 2009 avec Forrester Consulting.
Là c'est à partir de 3 secondes pour le chargement de la page qu'on
identifie 57 % d'abandon parmi les acheteurs, et 80 % qui ne reviendront
plus s'ils ont eu un résultat inapproprié. Ces paliers de 3 ou
4 secondes sont une limite haute à ne pas dépasser, pas un
objectif à atteindre.

De plus, ces mauvaises performances rejaillissent sur la perception de
la boutique et de la marque. 30 % des visiteurs vont ainsi développer
une perception négative de la société et potentiellement partager leur (mauvaise) expérience avec leurs amis ou leur famille.

Une autre étude, menée par Forrester et sponsorisée par Akamai en 2010, concerne
cette fois-ci les sites bancaires : 56% des utilisateurs s'attendent à
des temps de réponse inférieurs à 2 secondes.[^6]

  [^6]: Étude « The Impact of Poor Web Site Performance in Financial Services » citée dans [« Online Banking Customers Expect Fast Website Performance, Survey Finds »](http://www.banktech.com/business-intelligence/online-banking-customers-expect-fast-web/222500093)

Vos utilisateurs ne vous le diront pas forcément, mais ils attendent de
votre site qu'il soit rapide. C’est tellement naturel qu’ils ne
nommeront probablement pas le critère. Partout pourtant, quand on teste
et que des études sont conduites, la vitesse est un critère de choix même s’il
n’est pas exprimé. Plus qu’un critère, c’est en fait un pré-requis.

Votre visibilité est aussi impactée
-----------------------------------------

Depuis quelques temps, Google a rejoint Yahoo! pour communiquer auprès
des développeurs de sites web et leur enseigner comment améliorer les
performances. C'est ainsi que Steve Souders, autrefois à Yahoo! est
passé chez Google. Depuis Google a sorti l'extension de Firefox « Page
Speed », concurrent de Yslow de Yahoo!. Les deux servent à analyser une
page pour y trouver les pistes d'amélioration de performance.

Quelques voix le soupçonnaient auparavant mais Google l'a désormais
annoncé officiellement : la performance des sites web influe sur les
métriques internes du moteur et sur votre référencement.

Les performances jouent par exemple sur le positionnement du site dans
les résultats de recherche (activité SEO). Le temps de chargement des
pages devient un des 200 critères de tri de Google. Avoir un site qui se
charge à la vitesse de la lumière ne permettra pas d'avoir la première
place, mais de mauvaises performances rétrograderont le site à une plus
mauvaise place.

Les performances jouent aussi sur le « quality score » (le score
qualité) de la partie adWords de Google (activité SEM). Ce score est
affecté à chaque site et à chaque page ciblée par une publicité (page
destination quand on suit la publicité). Il permet de déterminer la
place de la publicité dans le moteur de recherche et modifie le coût
d'insertion dans le système AdWords. Plus le site et la page destination
seront lents, moins la publicité aura une bonne place, et plus elle
coûtera cher.

Si ce critère de performance n'est pas le seul, et pas forcément le plus
important, il est un de ceux qui sont facilement manipulables par les
équipes techniques des sites web.

Google aide même ces équipes en proposant l'outil Google Page Speed et
en l'intégrant dans ses outils en ligne Webmaster Tools ou Google
Analytics. Il est extrêmement probable que les métriques générées par ces
outils soient exactement celles prises en compte en interne dans les
moteurs de Google.

Les cas Google, Microsoft, AOL et Yahoo!
---------------------------------------------------

Plus haut, je vous ai cité le cas d’Amazon, qui voit ses ventes chuter de
1 % pour chaque tranche de 100 ms de latence. Il y a en fait d'autres
poids lourds qui ont donné des chiffres sur ce sujet : Google,
Microsoft, AOL et Yahoo! Ce dernier a même créé une équipe dédiée au
sujet (nommée « exceptional performance ») et a communiqué le résultat
de ses recherches auprès de la communauté des développeurs web. Tous les
quatre ont un ressenti similaire, même si les chiffres et leur
signification sont différents à chaque fois.

Google a mené plusieurs expérimentations sur les performances dans leur
page de recherche. Un premier résultat fait état d'une perte de 20 % de
trafic pour un ralentissement d'une demi-seconde de la page[^7].

> + 500 ms au chargement → - 20 % de trafic

  [^7]: Marissa Mayer, VP section recherche et expérience utilisateur, Google, [Conferences Scalability 2007 de Seattle](http://www.techpresentations.org/Scaling_Google_for_Every_User)

D'autres facteurs sont peut-être intervenus en parallèle pour affecter à
ce point le test (il présente par exemple 30 résultats par page au lieu
des 10 habituels), et d'autres tests plus récents ont des résultats
beaucoup plus mesurés. La corrélation entre performance et fidélité du
trafic est toutefois plus que certaine, mais il est difficile
d'individualiser tous les facteurs.

Par contre même ces derniers tests amènent un résultat complémentaire
inquiétant : ceux qui ont un jour fait partie des utilisateurs « test »
ayant reçu une page plus lente ont tendance à faire moins de recherches
par la suite, même plusieurs semaines après que le test soit fini et que
la vitesse des pages soit revenue à la normale. De mauvaises
performances, même temporaires, peuvent vous faire perdre durablement la
confiance des internautes.

Leur second retour d’expérience concerne le service Google Maps. Après
avoir réduit le poids de leur page de 30 %, le trafic a augmenté de 10 %
la première semaine, puis encore 25 % par la suite[^8]. 

> - 30 % sur le poids de la page → + 10 % puis + 25 % de trafic

  [^8]: Marissa Mayer, VP section recherche et expérience utilisateur, Google, [conférences Web 2.0 en novembre 2006](http://www.zdnet.com/blog/btl/googles-marissa-mayer-speed-wins/3925)

Le retour d’expérience suivant vient de Yahoo! Sur leurs tests
comparatifs, un délai de 400 ms induit immédiatement 5 à 9 % d’abandon[^9],
c’est à dire de gens qui ne vont pas plus loin et qui ne cliquent sur
aucun lien, ni aucune publicité.

> + 400 ms au chargement → - 5 à 9 % d’abandon

  [^9]: Présentation de Stoyan Stefanov (Yahoo!)  [« Yslow 2.0 early preview » en décembre 2008 en Chine](http://www.techpresentations.org/YSlow_2.0_early_preview_in_China)

Pour AOL, la découverte se fait plus en termes d'intérêt et de motivation
du visiteur. Plus les pages sont lentes à charger, moins le visiteur va
en visiter. La courbe est assez franche, presque droite au départ.
Ainsi, les 10 % de visiteurs qui ont les pages les plus rapides lisent
en moyenne 8 pages par visite. Cela descend entre 6 et 7 pour les 10 %
suivants, puis 5 pages par visite pour le troisième décile, 4 pages pour
le quatrième, etc.[^10] Les 50 % de visiteurs qui ont les plus mauvaises
performances visitent moins de 3 pages par visite en moyenne. C'est
presque trois fois moins que le premier décile. L'influence du temps de
chargement est donc indéniablement perceptible.

  [^10]: Présentation Dave Artz, Director of optimization, AOL, aux [conférences Velocity 2009](http://en.oreilly.com/velocity2009/public/schedule/detail/7579)

![Influence du temps de chargement sur le nombre de pages visitées pour AOL](img/chap01-influence-du-temps-de-chargement-sur-le-nombre-de-pages-visitees-pour-aol.png)

Enfin, sur Bing, un ralentissement influe sur le visiteur dès 200 ms. À
partir de 500 ms on voit déjà une baisse de plus de 1,2 % du revenu par
visiteur[^11]. Avec une ou deux secondes de ralentissement ce sont
respectivement 2,8 % et 4,3 % de perte de revenu par utilisateur. Ces
effets négatifs sont accompagnés de pertes de satisfaction utilisateur,
de plus faibles taux de clics, et d'une diminution de la propension des
visiteurs à affiner et approfondir leur recherche.

> + 500 ms au chargement → - 1,2 % de revenus

> + 1 s au chargement  → - 2,8 % de revenus

> + 2 s au chargement  → - 4,3 % de revenus

  [^11]: Intervention Bing aux [conférences Velocity 2009](http://en.oreilly.com/velocity2009/public/schedule/detail/8523)

![Évolution des performances suivant le ralentissement sur Bing](img/chap01-evolution-des-performances-suivant-le-ralentissement-sur-bing.png)

La baisse de revenus publicitaires est d'ailleurs corrélée par une étude
de Jackob Nielsen en juin 2010 : après un grand délai d'attente,
l'utilisateur se concentre sur le contenu et est bien moins à même de
s'arrêter sur les contenus promotionnels. Jackob évoque le passage de
20 % d'attention au promotionnel pour 1 seconde d'attente à 1 %
d'attention au promotionnel seulement après 8 secondes d'attente.

Certes, tout le monde n’est pas Google, Microsoft, AOL, Amazon ou Yahoo!
Il n’est pas possible de transposer immédiatement ces chiffres sur votre
site. En revanche, ce qui est certain c’est que vos utilisateurs
réagiront aux performances de votre site. Il y aura une influence réelle
sur votre trafic, vos ventes, la fidélité de vos utilisateurs, et la
confiance qu’ils auront dans votre site et votre marque. Toutes les
études confirment que c’est significatif, même pour des chiffres en
apparence négligeables. Ne vous croyez pas différents, votre site ne
l’est probablement pas.

Malgré l’augmentation des débits, le web est lent
------------------------------------------------------------

Le web est malheureusement en mauvais état par rapport aux performances.
Je vous ai cité en début de chapitre les cas de TF1 et de France Telecom
avec respectivement un peu plus de 5 secondes et un peu moins de 10 
secondes en moyenne. On se situe presque toujours au delà des 2
secondes, quel que soit le site, et trop souvent au-delà de 3, voire
de 5 secondes. 

Ici, dans une liste arbitraire de sites mesurés sur une connexion ADSL
de très bonne qualité, presque la moitié sont au dessus des 4 secondes :

![Performance de grosses audiences françaises et internationales](img/chap01-performance-de-grosses-audiences-francaises-et-internationales.gif)

Si les débits et la puissance des machines ont explosé, les développeurs
ont aussi fait moins attention au poids des pages, à leur contenu, à
leur utilisation. En cinq ans (2003 à 2008) on a entre autres triplé le
poids des pages [^12], doublé le nombre de composants par page, et doublé le
nombre de balises HTML. Entre 2008 et 2009 l'augmentation a été encore
plus importante. 

Au total, en moyenne, le poids des pages est de plus de 500 kilo octets,
avec près de 65 composants par page et pour seulement 500 mots. Si
l'étude ne va pas jusqu'en 2010, avec la multiplication des
bibliothèques javascript et des effets visuels, on peut sans risques
continuer les courbes sur leur progression d'alors.

  [^12]: Étude avec sources sur websiteoptimizations.com, [« Average Web Page Size Triples Since 2003 » d’avril 2008](http://www.websiteoptimization.com/speed/tweak/average-web-page/)

![Croissance du poids et du nombre d'objets d'une page web moyenne](img/chap01-croissance-du-poids-et-du-nombre-d-objets-d-une-page-web-moyenne.png)

Bref, on a un gros gâchis mais c’est aussi une chance : cela signifie
que nous avons beaucoup de marge pour améliorer les choses et qu’il y a
certainement des actions simples et peu coûteuses à appliquer, avec des
effets qui seront exceptionnels. Ce livre est là pour vous donner ces
informations, et bien d’autres.

Le problème n’est pas sur le serveur
--------------------------------------------

Mais agir sur quoi d’abord ? Demandez en interne, autour de vous, on
vous proposera de mesurer le temps de génération des pages web,
d’optimiser votre code PHP ou Java, de mettre des index dans votre base
de données.

Ne vous en privez surtout pas. Une fois le gros du travail fait votre
page se générera en moins de 100 millisecondes mais votre site risquera
d'être toujours lent pour l’utilisateur. Ce qu’ont montré les études
récentes, c’est que la génération de la page HTML n’occupe en fait
qu’une toute petite partie du ressenti de l’utilisateur. C’est au niveau
du navigateur et du réseau que nos problèmes se logent.

### Moins de 20% du temps est passé sur la page HTML**

Steve Souders a mené des études pour mesurer le poids de la génération
de la page HTML dans le temps de chargement total. D'après ses résultats
seul 10 à 20 % du temps est du à la génération de la page par le serveur
applicatif, 80 % du temps est passé sur le navigateur ou sur le réseau.
Pour les gros sites français le rapport est même plus souvent de 95 / 5
que de 80 / 20.

![Importance du fichier HTML sur le temps de chargement total de la page](img/chap01-importance-du-fichier-html-sur-le-temps-de-chargement-total-de-la-page.gif)

Sur des mesures faites avec une douzaine de sites à fort trafic, on
remarque bien que, le plus souvent, le chargement de la page HTML de
base ne prend qu’un très faible pourcentage du temps total de
chargement. À part France Telecom qui a définitivement un problème de
redirections HTTP, les seuls pour qui la page principale est
significative sont ceux qui ont déjà de bonnes performances, donc qui
ont déjà travaillé pour optimiser le rendu côté client.

### Travailler sur la partie cliente est le meilleur investissement

Les mathématiques sont assez simples, et il vaut mieux gagner 5% sur la
partie cliente (90% du total) que 30% sur le temps de génération du
fichier HTML source (qui ne représente que 10% du total). Pour ne rien
gâcher, si votre applicatif et votre base de données ne sont pas
utilisés à tort et à travers, il est beaucoup plus facile de faire des
améliorations sur la partie cliente que sur la partie serveur.

Nous occuper du client – navigateur et réseau – offre de loin le
meilleur retour sur investissement. Ce livre porte donc tout
naturellement sur ces aspects, sur le côté navigateur et le côté réseau.
Nous discuterons ensemble de cache, du protocole HTTP, du fonctionnement
interne des navigateurs, et de compression.

Comme un peu de publicité ne fait jamais de mal, si vous souhaitez tout
de même travailler votre programmation serveur en PHP, je ne peux que
vous conseiller le livre *PHP 5 avancé* dans la collection blanche des
éditions Eyrolles. J'ai eu le plaisir de le co-écrire avec Cyril Pierre
de Geyer. Nous l'avons maintenu à jour et enrichi presque chaque année.
Ce sera le seul passage promotionnel, promis.

Une stratégie gagnante, d'autres chiffres
--------------------------------------------------

Plus que ces chiffres, c’est aussi l’assurance que la démarche a été un
succès de nombreux sites, dont Yahoo! qui en a fait un cheval de
bataille. Travailler sur les performances web vues du navigateur a
permis de réduire le temps de chargement des pages, et ainsi éviter de
perdre des clients et des visiteurs.

Les différences en trafic et en nombre de ventes sur les sites ont été
prouvées par des tests puis confirmées après mise en œuvre. Bien que
l’influence dépende du ressenti subjectif de l’utilisateur, ces
recherches ont permis de mettre en lumière des gains objectifs sur le
temps de chargement et de voir une réelle réaction positive des
utilisateurs se traduisant en taux de rebond, nombre de visites, ou taux
de transformation pour les ventes.

Pour finir sur une note positive, l'exemple Shopzilla est intéressant.
En refondant leur site web, ils ont amélioré le chargement des pages en
passant de 6 secondes à 1,2 secondes. Pour résultat tous leurs
indicateurs sont passés au vert : taux de transformation amélioré de 7 à
12 % et nombre de pages vues augmenté de 25 % [^13].

  [^13]: Intervention de Philip Dixon, VP of Engineering, Shopzilla, aux [conférences Velocity 2009](http://en.oreilly.com/velocity2009/public/schedule/detail/7709)

Mieux encore, cette expérience leur a permis de diviser par deux le
nombre de serveurs nécessaires et a diminué les coûts de déploiement de
plus de 90 %. On fait mieux pour moins cher.

![Impact de l'amélioration des performances pour Shopzilla](img/chap01-impact-de-l-amelioration-des-performances-pour-shopzilla.gif)

Même chose chez Mozilla, leur équipe a aussi pu constater les effets
positifs du travail sur les performances web. En réduisant de
2,2 secondes le temps de chargement de leur page d'arrivée (celle qui
incite à télécharger le navigateur), ils ont pu augmenter de 15,4 % le
taux de transformation[^14].

  [^14]: Publication de Blake Cutler sur le blog officiel Mozilla, [le 5 avril 2010](http://blog.mozilla.com/metrics/2010/04/05/firefox-page-load-speed-%E2%80%93-part-ii/)

![Impact et détail de l'amélioration de performance pour Mozilla](img/chap01-impact-de-l-amelioration-des-performances-pour-mozilla.png)

Strangeloops Networks, une société qui travaille sur les performances, a
aussi publié ses chiffres. Si, parce qu'ils sont intéressés aux
résultats qu'ils donnent, on ne peut pas forcément prendre leurs
graphiques pour argent comptant, ils montrent bien une amélioration sur
tous les fronts : taux de rebond, nombre de visites, pages par visite,
temps passé sur le site, fidélité (retour des anciens visiteurs) et taux
de conversion. C'est sur toutes ces métriques que vous pouvez espérer
jouer, en travaillant les points techniques détaillés dans le reste de
ce livre.

Impact des optimisations de performance par Strangeloops Networks :

* \- 7 % en rebonds (de 14,35% à 13,38%)
* \+ 42 % en pages par visite (de 11,04 à 15,64)
* \+ 27 % en temps passé sur le site (de 25 minutes 50 secondes à 30 minutes 10 secondes)
* \+ 16 % en conversions en achats en augmentation
* \+ 5,5 % en montant moyen des commandes

Retour sur investissement
--------------------------------

Nous avons vu des chiffres qui font peur, l'impact négatif de mauvaises
performances, où quelques dixièmes de secondes ont des conséquences
importantes. Nous avons vu que pour améliorer ces performances il vaut
mieux travailler sur la façade de l'application. Mais combien cela
coûte-t-il ? 

### Augmenter le bénéfice

En prenant l'exemple d'une boutique en ligne pour améliorer les
bénéfices on peut jouer sur le nombre de visiteurs attirés dans la
boutique, sur leur propension à faire des achats, et sur les coûts de la
boutique elle-même.

Le nombre de visiteurs dépend souvent fortement de la visibilité dans
les moteurs de recherche mais aussi du taux de rebond, de la fidélité
des visiteurs qui restent, ainsi que du nombre de pages par visite. On a
vu qu'en jouant sur la performance on augmentait ces 4 facteurs.
Cette augmentation de trafic elle-même, mathématiquement augmentera les
ventes.

La propension à faire des achats dépend de l'incitation à déclencher le
panier (taux de transformation), mais pas uniquement. Le temps passé sur
le site incite à découvrir et donc à trouver finalement quelque chose
d'intéressant, ou à se laisser tenter, voire à augmenter la taille de son
panier. Là aussi on a vu que tous ces indicateurs sont améliorés quand
on améliore la vitesse du site.

Enfin, contrairement à ce qu'on pourrait penser, une fois
l'investissement de développement réalisé, les coûts sont eux aussi
diminués. Améliorer la vitesse passe souvent simplement par une
diminution du volume de données échangé entre le serveur et les
navigateurs. La bande passante étant une des ressources les plus chères,
la diminution des coûts est sensible. On diminue aussi le nombre de
requêtes vers le serveur, et donc le besoin en processeurs.

### Mettre en rapport avec les coûts d'investissement

Améliorer la vitesse du site est pratiquement la seule action
d'investissement qui permet d'agir positivement sur tous les
indicateurs, sans exception. Bien évidemment, les impacts de ces
indicateurs ne s'additionnent pas, ils se multiplient ! Le bénéfice
d'une augmentation de trafic est multiplié par le bénéfice d'une
augmentation du panier moyen. Vous trouverez peu, voire aucune,
manipulation qui ont une telle influence.

La conséquence est assez simple : votre investissement sera certainement
vite amorti. Tout ceci est prouvé par des chiffres concrets en situation
réelle, par des poids lourds du marché.

Dans son intervention publique, Stangeloops Networks calcule son retour
sur investissement en comparant le coût investit avec le nombre de
commandes multipliée par l'augmentation du taux de conversion et
l'augmentation du panier moyen. Pour 10 000 $ de revenus quotidiens et
50 000 $ investis dans la performance, le ROI est de 24 jours, moins
d'un mois. Leur calcul ne tient même pas compte de la diminution des
coûts (SEM, hébergement), ou de l'augmentation de trafic (SEO,
fidélité). On peut donc penser que l'amortissement réel des 50 000 $
est atteint en moins de 20 jours.

Vous verrez qu'il est possible de réaliser des investissements beaucoup
plus faibles, avec des résultats qui restent spectaculaires. La mise en
oeuvre des deux préconisations les plus importantes prennent moins d'une
journée chacune. En comptant les tests, l'apprentissage et la
surveillance, on peut les mettre en pratique en une semaine.

Plus qu'une fonctionnalité
--------------------------------

C'est peut être Fred Wilson qui résume le mieux tout cela. Fred Wilson est
un investisseur bien connu aux États Unis d'Amérique de par ses
participations dans de nombreuses sociétés « web 2.0 » comme Twitter,
Del.icio.us ou Feedburner. Lors des conférences « Future of Web Apps »
de 2010, dans ses dix principes pour des applications web gagnantes la
vitesse arrive en premier critère. Pour lui c'est bien plus qu'une
fonctionnalité, c'est le point le plus important, un pré-requis. Les
entreprises avec des applications lentes ne grossissent pas aussi vite.

> First and foremost, we believe that speed is more than a feature. Speed
> is the most important feature. […]
> 
> […] When we see some of our portfolio company’s applications getting
> bogged down, we also note that they don’t grow as quickly. There is real
> empirical evidence that substantiates the fact that speed is more than a
> feature. It’s a requirement. 

À retenir
-----------

-   De bonnes performances amélioreront votre visibilité dans les
    moteurs de recherche et diminueront votre coût de publicité en ligne
    sur Google
-   Quelques dixièmes de secondes de ralentissement peuvent avoir un
    impact négatif concret et important sur votre site
-   Il vaut mieux se concentrer sur la façade (réseau, navigateur) que
    sur la génération des pages (programmation Java, PHP, base de
    données)
-   Améliorer la vitesse de votre site fera immédiatement progresser
    votre taux de rebond, la fidélité de vos visiteurs, le temps passé
    sur votre site, et la conversion finale
-   L'investissement réalisé est généralement vite amorti, les premiers
    jours de travail sont rentabilisés en moins d'un mois

Premiers concepts
=================

Avant d’entrer dans le détail des recommandations et du travail 
à réaliser, il est important de comprendre ce qu’il se passe entre 
le navigateur et le serveur web. Vous connaissez peut-être le 
principal mais jetez tout de même un œil attentif à l’analyse 
réseau. Il y a de nombreux points sous-estimés qui méritent votre 
attention. 

Composition d’une requête HTTP
------------------------------

Le protocole utilisé par le navigateur web et le serveur web s’appelle 
HTTP : HyperText Transfer Protocol. Il s’agit d’un protocole 
simple à comprendre, basé sur des lignes de texte et des associations 
clef – valeur. 

La première particularité de HTTP est d’être un protocole dit 
_stateless_, sans état. Il n’y a en effet qu’un seul échange : 
le navigateur envoie une requête puis attend la réponse du serveur. 
Il n’y a aucun autre échange entre les deux intervenants. Lorsque 
le navigateur demandera une seconde ressource (ou rechargera 
la même que précédemment) il fera une nouvelle requête, indépendante, 
sans aucun lien avec la première. 

### Exemple de requête HTTP

Une requête du navigateur au serveur est constituée de trois 
parties : la ligne de requête, un bloc d’en-têtes, et éventuellement 
un bloc de données, le corps de la requête. 

~~~~~~~ {.http .request}
GET /index.html HTTP/1.1
Host: example.org
User-Agent: Mozilla/5.0 ([...]) Gecko/2008092414 Firefox/3.0.3
Accept: text/html,application/xml;q=0.9,*/*;q=0.8
Accept-Language: fr,fr-fr;q=0.8,en-us;q=0.5,en;q=0.3
Accept-Encoding: gzip,deflate
Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
Keep-Alive: 300
Connection: keep-alive
~~~~~~~

La première ligne est la ligne de requête. Le terme `GET` indique 
qu’on souhaite récupérer une ressource (la page web) ; c’est 
ce qui est fait le plus souvent. Vous pourrez aussi trouver `POST`, 
qui indique qu’on souhaite envoyer une ressource, par exemple 
un message sur un forum. [D’autres méthodes existent](https://fr.wikipedia.org/wiki/Http#M.C3.A9thodes) mais l'essentiel  du web
est conçu autour de ces deux-là donc nous nous en contenterons.

Vient ensuite l’adresse de la ressource que nous souhaitons 
récupérer, ou vers laquelle nous souhaitons envoyer des informations. 
Cette adresse commence toujours par `/`, et ne contient pas le 
nom de domaine[^proxyrequests]. Notons que la spécification HTTP n'impose qu'une
chaîne de caractères arbitraire. 
Si habituellement on considère qu’il y a des répertoires, un 
nom de fichier puis une extension, l’adresse peut en fait correspondre 
à tout à fait autre chose. Il s’agit en fait d’un simple identifiant, 
charge au serveur de savoir à quoi il correspond. 

[^proxyrequests]: Cela n'est vrai que pour les requêtes à destination des 
serveurs HTTP finaux. Lorsqu'on utilise un proxy HTTP, on envoie aussi des requêtes HTTP,
mais qui contiennent cette fois le domaine-cible.

La troisième information de la première ligne de requête est 
le protocole utilisé. Ce sera quasiment toujours `HTTP/1.1` 
pour les navigateurs, même si quelques rares scripts ou robots 
utilisent encore `HTTP/1.0`. 

Le reste de l’exemple donné constitue le bloc d’en-tête. Chaque 
ligne contient un couple clef – valeur, séparés par le caractère 
« : ». Notons que les clefs ne sont pas sensibles à la casse.
Certaines valeurs ont de multiples valeurs séparées par 
des virgules, et parfois une valeur est paramétrée, les paramètres 
étant séparés par un point virgule. 

Une ligne vide indique la fin du bloc d’en-tête. Dans le cas d’une 
requête de type `POST` vous trouverez aussi un bloc de contenu 
juste après. Il peut contenir des valeurs de formulaire par exemple. 

### Exemple de réponse HTTP

Une réponse HTTP n’est pas très différente d’une requête. On 
a une ligne de statut, un bloc d’en-têtes et un bloc de contenu, 
le corps de la réponse. 

~~~~~~~ {.http .response}
HTTP/1.1 200 OK
Date: Sun, 02 Nov 2008 15:54:27 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 15 Nov 2005 13:24:10 GMT
Etag: "280100-1b6-80bfd280"
Accept-Ranges: bytes
Content-Length: 438
Connection: close
Content-Type: text/html; charset=UTF-8

<HTML>
[…]
</HTML>
~~~~~~~

La ligne de statut contient le protocole, code de retour et un 
message explicatif. Le protocole sera presque toujours HTTP 
1.1, exceptionnellement HTTP 1.0. Le code de retour 200 indique 
que le serveur a traité la requête avec succès et renvoyé la page 
demandée, d’où le message explicatif « OK ». Vous connaissez 
aussi probablement le code de retour 404 « Document Not Found », 
qui correspond à une page non trouvée. Dans ce livre nous croiserons 
aussi les codes 301 et 302 qui sont des redirections, et le code 
304 qui est lié aux mécanismes de cache. Pour information, la RFC 2616
définit [la liste des codes HTTP](http://fr.wikipedia.org/wiki/Liste_des_codes_HTTP).

La suite de la réponse est vraiment similaire à ce que nous avons 
vu pour une requête : une suite d’en-têtes avec clef - valeur, une 
ligne vide, et le corps de la réponse. Le corps de la réponse, c'est 
ce que le serveur a fait correspondre à la ressource demandée,
par exemple un fichier HTML, du code javascript, ou du contenu binaire de 
du fichier image. 

### Les outils

Pour explorer les requêtes et les réponses HTTP, les navigateurs web
actuels ont maintenant des outils directement intégrés: Chrome et Firefox
ont chacun leurs *Developer Tools*.

Pour beaucoup d'usages ces outils sont suffisants, mais pour aller plus loin,
on pourra télécharger l’extension Firefox nommée HTTPfox, ou encore 
l’extension LiveHttpHeaders. L’extension Firebug,ou le site [Web Page Test](http://www.webpagetest.org/)
peuvent aussi vous montrer le détail 
de ce qui est envoyé et reçu par les navigateurs. L’extension 
Firefox nommée TamperData permet même de modifier dynamiquement 
les requêtes HTTP au moment où elles sont envoyées, pour tester 
différents résultats.

![Exemple de visualisation d’une requête HTTP avec Firebug](img/chap02-exemple-de-visualisation-dune-requete-http-avec-firebug.png)

Pour les plus geeks, l'outil en ligne de commandes `curl` est indispensable.

Analyse d’une requête réseau
----------------------------

La requête HTTP n’est qu’une partie de ce qui est fait sur le réseau. 
Voici les concepts principaux à retenir. 

![Schéma d’un échange réseau](img/chap02-schema-d-un-echange-reseau.png)

### Requête DNS

Le début d’un échange commence toujours par une requête DNS. 
Quand vous demandez http://www.example.org/ il s’agit de déterminer 
quelle est l’adresse IP de la machine hébergeant www.example.org. 
C’est un serveur DNS (serveur de nom de domaine) chez votre fournisseur 
d’accès qui va vous répondre. C’est généralement rapide, mais 
ça peut tout de même prendre quelques dizaines de millisecondes. 
En fait tout dépend de la latence avec le DNS de votre société, 
et éventuellement de la latence entre ce DNS et celui du site que 
vous cherchez à joindre. Sur un réseau lent ou avec un site très 
éloigné, cela peut dépasser 150 ms. 

### Connexion TCP

Une fois que votre navigateur connait l’adresse du serveur à 
joindre, on peut établir une connexion TCP avec le serveur web. 
TCP est le protocole de transport utilisé par HTTP. Il s'agit ni plus ni moins 
que de mettre en place une sorte de fil de discussion entre le serveur 
et le client, et pour ça il faut l'accord des deux avec une phase 
d'initialisation. C’est rapide, mais c’est encore 
une étape à franchir, qui dépend elle aussi de la latence. En effet, 
l'établissement d'une connexion TCP se fait en trois temps, et dure
donc trois fois la latence.

C’est seulement après ces deux premières étapes qu’on peut avoir 
un échange requête - réponse entre le navigateur et le serveur 
web. Mais attention: juste après l'établissement de la connexion,
le transfert ne se fait pas encore à plein régime: c'est ce qu'on appelle
l'algorithme *slow start*, algorithme utilisé pour ne pas engorger
les réseaux.

Pour HTTP, l'un des principes de base est que pour chaque requête on
crée une nouvelle connexion TCP. Pour compenser et comme solution de contournement, les navigateurs 
opèrent plusieurs téléchargements en parallèle (de 2 à 8 suivant 
le navigateurs). Ils ouvrent simplement plusieurs fils TCP 
et y envoient des requêtes différentes. Ils permettent ainsi 
d'optimiser la bande passante et de ne pas se tourner les pouces 
pendant les temps d'attente dus à la latence. 

Pour combler ce problème HTTP prévoit 
une fonctionnalité (connexions persistantes, keep-alive) 
pour garder le fil TCP ouvert et pouvoir y enchaîner plusieurs 
requêtes, l'une après l'autre. Cette possibilité est un gros avantage, et permet
de compenser les problèmes dus au *slow start*,
mais n'est pas toujours activée sur les serveurs à cause de la 
consommation en ressources qu'elle implique. Une seconde fonctionnalité 
existe dans la version 1.1 de HTTP, le pipelining, mais elle est 
rarement activée. Nous y reviendrons plus loin dans ce livre. 


### Génération de la page HTML par le serveur

Sur le schéma, le temps de génération de la page sur le serveur 
se verrait par un espace blanc entre la fin du parallélogramme 
labellisé « requête HTTP » et le début du parallélogramme « réponse 
HTTP ». 

Comme on l'a déjà vu dans le chapitre d'introduction, le temps 
de génération de la page, ce que j'ai appelé la partie « back-end 
», est généralement inférieur à la demie-seconde (voire inférieur au dizième de seconde), 
et au final négligeable par rapport au temps de chargement total de la page. Quand ce n'est pas le cas,
des techniques d'optimisation existent, mais ce n'est pas le sujet de ce livre, nous n'aborderons
donc plus ce point.

### Temps de transfert, débit et latence

Vous voyez que sur le schéma on dessine des obliques et non des 
traits verticaux. Plus la latence est importante, plus les obliques 
sont écrasées, et donc plus l’aller-retour prend du temps. 

Vous voyez aussi que la requête et la réponse se dessinent ici 
avec des parallélogrammes et non de simples flèches. C’est parce 
que les requêtes prennent du temps à être envoyées entièrement, 
et les réponses prennent encore plus de temps à être téléchargées. 
Augmenter ou diminuer la bande passante réseau permet d’influer 
sur ces deux durées. 

### Les outils

Des outils comme Firebug, les outils de développements natifs des navigateurs ou WebPagetest (http://www.webpagetest.org ) vous montreront les différentes étapes d’une requêtes, pour vous permettre de constater où est 
votre problème. Les navigateurs Chrome, Safari et Firefox possèdent nativement des outils équivalents à Firebug. Plusieurs proxy de développement comme HttpWatch peuvent aussi exporter des données similaires. 

Le graphique alors représenté est appelé vue en cascade. Il est 
un peu différent du schéma explicatif général car il ne prend 
en compte que les temps d’attente vues du navigateur et permet 
d’établir les mesures pour chaque requête HTTP faite sur la page. 
On y remarque si une étape prend un temps trop important mais aussi 
si une requête bloque les suivantes par exemple. 

![Exemple d’analyse faite par webpagetest.org](img/chap02-exemple-danalyse-faite-par-webpagetestorg.png)

### Plafonnement du débit lors d'une session HTTP

#### Gestion de la bande passante par TCP

Sur Internet le serveur ne connait pas la bande passante disponible 
sur le client, ou sur les éléments réseaux entre le serveur et 
le client. Il est donc impossible de savoir à priori à quelle vitesse 
envoyer les données. Comme les équipements réseaux ne peuvent 
se permettre de stocker temporairement les données de tout le 
monde, si des données sont envoyées plus vite qu'on ne peut les 
recevoir, une grande partie est simplement perdue, ignorée 
sur le trajet. 

Pour palier ce problème, le protocole TCP met en œuvre une communication 
entre le client et le serveur. Le serveur commence par transmettre 
une certaine quantité de données au client puis attend confirmation 
de la bonne réception. Si tout va bien, il va transmettre un peu plus 
de données à la fois et attendre là aussi confirmation, et ainsi 
de suite. Quand la vitesse devient trop grande des données sont 
perdues et la confirmation n'arrive pas. Le serveur diminue 
alors la quantité de données et recommence. Après la phase d'initialisation, 
la bande passante utilisée oscille donc (on monte jusqu'à dépasser 
la bande passante disponible, on diminue d'un coup pour remonter 
progressivement et recommencer). 

Comme ce système nécessite quelques allers-retours pour arriver 
en rythme optimal et que la latence influe sur le temps que prennent 
ces allers-retours, plus la latence est forte, plus la phase 
préalable de faible débit perdure. 

#### Influence sur HTTP et les pages web

Sur le web l'essentiel des contenus sont très petits, souvent 
moins de 10 ko, rarement plus de 25 ko (http://httparchive.org/interesting.php#responsesizes). Seuls quelques pages ou composants 
javascript montent à 100 ko. Le résultat c'est que très souvent 
TCP n'a pas le temps d'échanger assez de données pour monter à 
la bande passante optimale entre le client et le serveur. 

Si on ajoute que pour télécharger plusieurs contenus le serveur 
et le client établissent plusieurs connexions TCP (chacune 
prenant un temps initial fixe pour l'initialisation avant le 
transfert d'une quelconque donnée) on comprend qu'assez rapidement 
augmenter la bande passante disponible n'aura aucune influence, 
ou très peu. 

D'après une étude de Google[^1], à 1 Mb/s le trafic HTTP occupe 
à peu près 70 % de la bande passante disponible. On tombe à 55 % pour 
une bande passante disponible de 2 Mb/s. Pour 4 Mb/s disponible 
on n'utilise que 1,45 Mb/s en réalité. Ce nombre n'augmente quasiment 
pas par la suite quand bien même on augmenterait encore la bande 
passante disponible. Pour la page de référence de Google, même 
avec une connexion qui peut réaliser 10 Mb/s, le trafic web n'en 
utilise pas beaucoup plus de 1,6 Mb/s. 

![Bande passante réellement utilisée en fonction de la bande passante disponible](img/chap02-bande-passante-reellement-utilisee-en-fonction-de-la-bande-passante-disponible.png)

[^1]: [More bandwidth doesn't matter (much), Google, Mike Belshe](http://www.belshe.com/2010/05/24/more-bandwidth-doesnt-matter-much/), avril 2010. Les graphiques suivants sont tirés de cette étude.

#### Plafonnement 

On observe un plafonnement de la bande passante à cause de la latence, 
du fonctionnement de TCP, des contraintes de HTTP et de la petite 
taille des composants échangés. Ce plafonnement rend peu utile 
d'avoir une bande passante de plus de 4 à 5 Mb/s. 

![Bénéfice de l'augmentation de la bande passante pour une page web](img/chap02-benefice-de-laugmentation-de-la-bande-passante-pour-une-page-web.png)

Analyse réseau du chargement d’une page
---------------------------------------

Comme nous l’avons vu au premier chapitre, la page HTML principale 
n’est pas forcément la source du problème de performance. Généralement 
elle ne l’est pas du tout même. On ne peut pas se limiter à une requête 
indépendamment des autres. 

Le navigateur commence par charger la page HTML. Après quelques 
millisecondes, le navigateur initie le téléchargement des 
composants qui sont référencés dans la page. 

Un certain nombre de ces téléchargements sont fait en parallèles, 
les autres sont mis en attente le temps que les premiers soient 
terminés. De plus, certains composants sont bloquants, c’est 
à dire qu’ils empêchent tout chargement d’un autre composant 
en parallèle. La réorganisation des références dans la page 
peut donc permettre de charger plus vite les éléments visibles, 
ou d’éviter de bloquer le navigateur à un moment critique. 

### Les outils

Les graphiques en cascade sont un des outils principaux, essentiellement 
Firebug ou les outils de développements de Chrome, Google Page 
Speed ou Yslow (tous les deux des extensions de Firefox), un proxy 
de débogage comme Charles, et WebPagetest. 
On y liste toutes les requêtes sur un axe de temps. On voit s’il 
y en a trop, lesquelles bloquent les autres, lesquelles sont 
lentes, etc. 

Si dans l’analyse d’une requête réseau particulière nous nous 
intéressions au graphique ligne à ligne, ici nous nous intéressons 
surtout à la cascade elle-même, à l’enchaînement et à l’organisation 
des requêtes. 

### Exemple de chargement

Un graphique en cascade du chargement de Yahoo! France accompagne 
ces pages. Vous pouvez y constater quelques particularités 
propres à une analyse macroscopiques. 

Tout d’abord vous voyez que certaines requêtes bloquent tout 
téléchargement. C’est le cas de la troisième ligne, un fichier 
javascript. Tant que ce javascript n’est pas entièrement téléchargé, 
rien d’autre n’avance. Il y a même un espace blanc entre la fin 
de ce javascript et le téléchargement suivant. C’est que le javascript 
prend un certain temps à s’exécuter, et bloque le navigateur 
pendant ce temps. 

![Graphique en cascade de la page d’accueil Yahoo! France](img/chap02-graphique-en-cascade-de-la-page-daccueil-yahoo-france.png)

Ensuite vous voyez que le rendu de la page ne commence que longtemps 
après que la page HTML soit téléchargée. La page HTML principale 
c’est la première ligne. Le début du rendu sur le navigateur c’est 
la première ligne verticale, entre 3 s. et 3,5 s. La seconde ligne 
verticale correspond à l’événement `onLoad` du navigateur, 
c’est à dire quand le navigateur considère avoir entièrement 
chargé la page. 

Notre objectif est d’avancer au maximum le début du rendu, première 
ligne verticale, et la fin des téléchargements, seconde ligne 
verticale. Pour cela on tente d’éliminer des requêtes HTTP, 
surtout les plus lentes, ainsi que de réduire le temps de chargement 
et d’améliorer la parallélisation de celles qui restent. 

On retrouve sur le graphique en cascade les différentes étapes 
d'un connexion HTTP sur chaque ligne. Les différentes couleurs 
permettent de repérer la requête DNS, l'établissement de la 
connexion TCP, l'envoi de la requête et l'attente du navigateur, 
puis la réception elle-même. 

Suivant les lignes, la requête DNS et l'établissement de la connexion 
TCP peuvent être inutile (respectivement si le domaine a déjà 
été résolu en adresse IP, et si le serveur réutilise une connexion 
persistante). La réception des données est parfois tellement 
rapide qu'elle semble ne pas apparaître dans le graphique. 

On voit aussi deux lignes verticales. La première symbolise 
le début du rendu dans le navigateur (la page commence à ne plus 
être blanche) et la seconde indique l'événement « onload » dans 
le navigateur. 

Dans le navigateur
------------------

Si le réseau occupe une place prépondérante dans le temps de d'affichage d'une page, 
ce n'est pas le seul domaine qui entre en jeu. 
A titre d'exemple, Microsoft identifie onze sous-système pour son navigateur Internet 
Explorer :

* Réseau : Ce sous-système est responsable de toute la communication 
  entre le navigateur et les serveurs web. Il s'occupe des requêtes 
  HTTP, des requêtes DNS, des files d'attente de téléchargement, 
  et des caches. 

* HTML : Une fois le code HTML téléchargé il est envoyé au sous-système 
  HTML pour être analysée et découpé. Ce module a pour rôle de 
  créer une structure en mémoire représentant le code HTML. 

* CSS : De même que pour HTML, le module CSS a pour rôle d'analyser 
  le code source pour créer une structure qui sera réutilisée 
  plus tard par le navigateur. 

* Collections : Ce sous-système est responsable du stockage 
  et de l'accès à toutes les méta-données, pour la page (dans 
  les entêtes HTML par exemple) ou pour les éléments HTML (attributs) 

* JavaScript : C'est le sous-sytème chargé d'exécuter le code 
  JavaScript. 

* Routage (marshalling) : Si le module javascript n'est pas 
  directement intégré dans le navigateur, il faut passer par 
  une couche intermédiaire pour toutes les interactions avec 
  le DOM ou les éléments du navigateur. Ce sous-système de routage 
  est là pour faire le pont entre les deux. 

* Modèle objet natif : C'est la représentation native du document 
  et de ses éléments dans le navigateur, à laquelle on accède 
  souvent via JavaScript. 

* Formatage : Une fois les structures du document créées un sous-système 
  est chargé d'y appliquer les différents styles. 

* Construction des blocs : À partir du formatage on construit 
  les différents blocs à afficher, par exemple avec leur taille. 

* Agencement (layout) : Couplé avec le module précédent, le 
  sous-système d'agencement est chargé de préparer le rendu 
  en organisant les blocs entre eux. 

* Rendu : Le rendu est l'étape finale. Il s'agit d'afficher à 
  l'écran ce que l'utilisateur verra. C'est aussi ce sous-système 
  qui s'occupe éventuellement de l'accélération graphique. 

La construction de la page en mémoire et l'affichage dans le navigateur passe donc
par de nombreuses étapes, susceptibles de générer des ralentissements ou blocages
qu'il faudra donc eviter.

Pour une application basée fortement sur ajax avec Microsoft 
Internet Explorer 8, hors réseau, les répartitions sont de l'ordre 
de 30 % pour le rendu, 20 % pour JavaScript, 15 % pour l'agencement, 
et 12 % pour le formatage. Pour un site web plus classique le rendu, 
l'agencement et le formatage ont bien moins d'importance car 
ils interviennent une seule fois (le JavaScript ne provoque 
pas de changements fréquents dans la page).(notabene: il faudrait trouver un exemple plus moderne)

Cependant quelques points meritent une attention particulière.

En premier lieu, on mentionnera :
* l'ordre d'apparition des composants dans la page qui peut provoquer des blocages
(attente qu'un composant - js, fonte - soit chargé et interprété avant de charger le suivant)
* les composants dont le navigateur ne peut pas connaitre les dimensions sans les avoir
complètement telechargés, et qui obligent a redessiner toute la page plusieurs fois 
(images sans dimenssions, animations flash chargées par javascript, publicités...)

En second ordre, mais qui peuvent etre problematique sur des grosses pages :
* la taille du DOM
* la complexité des selecteurs CSS

 

Ressenti utilisateur, temps de réponse
--------------------------------------

### Valeurs objectives de temps de réponse

Lors des explorations nous nous attacherons à plusieurs mesures 
objectives : 

* Temps de chargement complet (PLT, page load time) : temps nécessaire 
  pour charger, exécuter et afficher toute la page et ses composants. 

* OnLoad : déclenchement de l'événement DOM du même nom dans 
  la page en cours de chargement. Naturellement cet événement 
  se déclenche quand toute la page et ses ressources sont chargées 
  et exécutées, toutefois, il est possible de différer l'exécution 
  ou le chargement de certains composants, sauf ce qu'on a volontairement 
  différé. 

* ContentLoaded : déclenchement de l'événement DOM du même 
  nom, quand la page HTML a été téléchargée et analysée entièrement, 
  disponible pour utilisation. Les ressources annexes peuvent 
  être encore en téléchargement. 

* Temps de téléchargement réseau : temps nécessaire pour le 
  transfert complet réseau de la page HTML principale. 

* Début du rendu (start render) : temps à partir duquel le navigateur 
  commence à tracer la page à l'écran. 

### Autres valeurs mesurables

Outre les mesures objectives, il est possible de mesurer sur 
d'autres critères : 

* Page visible (above the fold, ATF) : temps nécessaire pour 
  charger tous les composants nécessaires à tracer la partie 
  visible de la page. 

* Temps pour agir (time to act, TTA) : temps nécessaire pour commencer 
  à utiliser la page (le contenu utile est donc chargé et le navigateur 
  réactif). 

* Chargement du contenu principal : temps nécessaire pour afficher 
  le contenu principal du site (généralement le corps textuel 
  de l'article, le menu principal sur la page d'accueil, parfois 
  l'illustration ou le formulaire central). 

### Pièges et priorité entre les différentes mesures

Si nous abordons autant de mesures différentes c'est qu'il n'y 
en a aucune qui répond à toutes les problématiques. Par facilité 
nous parlons souvent du temps de chargement complet et du temps 
de chargement du contenu principal, parce que ce sont les mesures 
significatives les plus parlantes. 

C'est toutefois la combinaison de toutes ces mesures qui constitue 
la performance d'une page. Lors de vos propres études, faites 
attention à ne pas vous focaliser uniquement sur les deux que 
nous mettons souvent en avant. Si le navigateur exécute encore 
beaucoup de javascript (temps pour agir très long) ou si c'est 
le contenu utile de la fin de page qui se charge avant celui de la 
partie visible, vous aurez échoué à efficacement améliorer 
les performances du point de vue de l'utilisateur. 

Faire attention aux mesures est aussi indispensable quand elles 
ne sont faussés par un artefact peu significatif. C'est par exemple 
le cas quand une ressource unique met très longtemps à charger 
alors que tout le reste de la page est fonctionnel : les mesures 
onload et temps de chargement complet seront mauvaises mais 
cela ne reflétera pas la performance réelle de la page et l'expérience 
utilisateur. À l'inverse, il est possible de différer au dernier 
moment ou passer en tâche de fond beaucoup de traitements : cela 
améliore immédiatement quasiment toutes les mesures, mais 
ça pourra aussi ralentir toutes les interactions futures avec 
l'utilisateur et donc dégrader la performance réelle de la page. 

Enfin, il est indispensable de vérifier les mesures sur différents 
navigateurs. Leurs fonctionnalités sont différentes et ils 
peuvent réagir de manière tout à fait opposée sur une même page. 

### Ressenti utilisateur

Avant de passer à la suite, il est important d'ajouter encore 
d'autres considérations. Même avec une analyse intelligente 
et en prenant en compte toutes les mesures brutes, le ressenti 
de l'utilisateur peut ne pas varier dans le même sens que nos mesures. 

Ce ressenti utilisateur peut être impacté par les _reflow_, 
ces recalculs du rendu de la page au fur et à mesure de son chargement, 
qui lui donnent l'impression de « bouger ». Il peut aussi être 
impacté par la présence d'un FLOUC (flash of unstyled content, 
affichage bref d'une page non stylée le temps que les styles soient 
appliqués ou téléchargés), d'un temps avec une page blanche, 
ou d'une construction trop progressive de la page. Même la présence 
d'un indicateur de chargement peut changer du tout au tout le 
ressenti de performance de l'utilisateur. 

Quand nous améliorons les performances il ne faut pas perdre 
de vue que le plus souvent c'est ce ressenti qui est notre objectif. 
Les mesures brutes ne servent qu'à nous donner une indication 
de notre avancée. Il arrivera que l'amélioration des indicateurs 
chiffrés entraine une dégradation du ressenti, et inversement. 
Parfois il peut ainsi être une bonne idée de retarder le chargement 
complet de la page si cela permet d'afficher plus tôt les éléments 
importants, de diminuer les « bougés » dans le chargement et de 
continuer des choses invisibles en tâche de fond. 

Les navigateurs font très attention à ce ressenti de performance. 
Mozilla Firefox a très bien compris l'importance de l'impression 
de vitesse. Ainsi l'image animée qui tourne quand une page est 
en cours de chargement a été accéléré au fur et à mesure des versions. 
Ce changement renforce et donne une visibilité aux améliorations 
concrètes et réelles de performance dans le moteur. C'est sur 
l'impression de performance que le navigateur a joué mais la 
satisfaction pour l'utilisateur est elle très concrète. 

**Recommandation :** Il est heureusement rare que les chiffres 
donnent une impression totalement fausse, mais attention à 
ne pas leur donner une importance démesurée. L'interprétation 
de l'utilisateur et le subjectif ont eux aussi une grande influence. 

Directives principales
----------------------

De tout ceci on peut tirer cinq directives principales qui devront 
guider toutes nos actions : 

1. Réduire le nombre de requêtes HTTP 

2. Réduire le poids des composants 

3. Améliorer la parallélisation des téléchargements 

4. Améliorer le ressenti utilisateur 

5. Améliorer l'applicatif et les temps de traitement 

Il est difficile d'établir une priorité entre ces cinq directives, 
parce que toutes peuvent être la source de votre problème de performance. 
Toutefois, en général les deux premières directives sont les 
plus simples à traiter dans un premier temps et offrent un retour 
sur investissement excellent. Le troisième point permet lui 
aussi d'améliorer considérablement les performances mais 
reste parfois plus complexe à gérer. 

Installations et mesures préalables
-----------------------------------

Ce chapitre sur les concepts de base touche à sa fin, et avant de 
passer aux détails techniques il est temps de vous inciter à installer 
les outils nécessaires et à prendre vos premières mesures. 

### Outils

Tout d'abord si vous n'avez pas installé Mozilla Firefox, faites 
le, tout de suite. C'est ce navigateur qui sera votre principal 
outil. Gardez à l'esprit qu'appuyer sur la touche _shift_ pendant 
que vous réactualisez une page permet de ne pas utiliser le cache 
du navigateur. L'adresse about:config vous permettra d'accéder 
aux options de configurations internes ; nous en utiliserons 
plusieurs. 

Profitez en pour installer au minimum les extensions Firebug, 
Yslow, Google Page Speed et HTTPfox. Prenez les dernières versions, 
n'hésitez pas à utiliser les bétas ou versions en développement. 
Google Page Speed vous donnera un premier graphique en cascade, 
Yslow vous donnera les statistiques et HTTPfox vous permettra 
d'analyser les entêtes HTTP. 

Installez aussi Chrome, Safari, Microsoft Internet Explorer, 
et Opera. Pour avoir les différentes versions de chacun de ces 
logiciels vous aurez certainement à installer des machines 
virtuelles. Certaines versions ayant des paramétrages vraiment 
différent, ce n'est pas superflu. 

Si vous avez en plus un Iphone et quelques téléphones portables 
avec accès web, cela peut être intéressant (mais pas indispensable). 
Sur Android l'installation de Browser2 ([http://www.5o9mm.com/](http://www.5o9mm.com/)) 
sera aussi utile pour avoir des métriques fiables. 

Téléchargez et installez ensuite les outils IBM Page Detailer 
et Page Test, qui vous donneront des cascades précises et bourrées 
d'informations. 

Les proxys de déboguage Charles et Fiddler peuvent aussi vous 
aider à intercepter le trafic HTTP et collecter quelques statistiques. 
Charles vous permettra par exemple de simuler une latence et 
un débit réduit entre votre serveur et votre navigateur, quand 
bien même les deux sont sur la même machine. Pour tester des configurations 
plus complexes, vous pouvez aussi installer Wireshark, qui 
surveillera tout le réseau et pourra extraire les sessions HTTP, 
TCP et DNS. 

Enfin, prévoyez un bout de serveur avec Apache et PHP quelque 
part, avec un accès en écriture à la configuration. C'est ce qui 
vous permettra de tester vos tentatives. 

Un chapitre dédié aux outils est présent en fin de livre, n'hésitez 
pas à y jeter un oeil pour savoir comment utiliser tout ça. 

### Mesures

Il est maintenant temps de prendre vos premières mesures. Prenez 
quelques sites exemples : le votre, mais aussi quelques uns que 
vous visitez régulièrement. Faites un répertoire pour chaque 
et sauvegardez le code source et les mesures. Quand vous aurez 
exploré tout ce livre vous pourrez voir le chemin parcouru. 

![Écran statistique de Yslow](img/chap02-ecran-statistique-de-yslow.png)

Si vous faites des mesures manuelles, pensez à faire un test avec 
le cache vide, et un test avec le cache pré-initialisé. Des informations 
sont disponible à ce sujet dans le chapitre lié au cache. Si vous 
oubliez de vider votre cache avant la première mesure, vous risquez 
de fausser tous vos résultats. 

Yslow nous donne pratiquement toutes les mesures utiles à sauvegarder. 
Vous trouverez un lien « vue imprimable » dans le menu outils de 
l'extension : sauvegardez-en le résultat. Vous aurez le nombre 
et la répartition des requêtes HTTP, le poids de la page et des 
différents composants, et le temps de chargement. 

Tout ce qu'il vous manque éventuellement c'est un graphique 
en cascade avec différents valeur de bande passante et de latence. 
Webpagetest.org devrait vous donner ça. Pensez à activer la 
vidéo et à sauvegarder les résultats. 

**Recommandation :** Commencez par faire des mesures Yslow 
sur chacun de vos sites de travail, puis des mesures avec webpagetest.org 
avec différentes qualités de connexion Internet. 

Sauvegardez le tout pour pouvoir constater vos avancées au fur 
et à mesure. 

### Yslow et Google Page Speed en fil directeur

Quand vous ne savez pas quoi faire, ou que vous voulez constater 
vos progrès, jetez un oeil à Yslow, puis éventuellement à Google 
Page Speed. Yslow n'est pas parfait, mais c'est un des meilleurs 
guides qui existe sur le sujet et Google Page Speed comble les 
quelques manques. Les statistiques, la mesure du temps de réponse 
et les différentes notes (de F à A, A étant la meilleure) vous donneront 
un aperçu rapide de là où vous en êtes. 

![L'écran d'évaluation Yslow](img/chap02-lecran-devaluation-yslow.png)

Quand vous voulez avancer indépendamment de ce livre, prenez 
un sujet où vous avez une mauvaise note, et suivez les recommandations 
correspondantes. 

Débit et latence d'une connexion Internet type
----------------------------------------------

### Débit, soyons humbles

Ne soyez pas trop optimistes sur les valeurs de bande passante 
et latence. Pensez que 2 Mb/s est déjà une bonne moyenne. En réalité 
beaucoup ont une moins bonne connexion parce qu'ils utilisent 
une connexion WIFI, un téléphone portable, un ordinateur lent, 
ou un logiciel P2P en tâche de fond par exemple. Générez des mesures 
et des tests pour 56 kb/s (modem téléphonique classique, RTC), 
512 kb/s (ligne ADSL basique, mauvais wifi, ligne partagée), 
1 Mb/s, 2 Mb/s, éventuellement pour 3 Mb/s et 5 Mb/s. 

Malheureusement il n'est pas la peine de monter plus haut. Plus 
les gens ont une grande bande passante, plus ils ont tendance 
à utiliser de logiciels consommateurs simultanément (messagerie, 
téléchargements, onglets multiples dans le navigateur, etc.) 
divisant d'autant la bande passante réellement disponible 
pour la session HTTP. Comme nous l'avons vu, le débit réellement 
utilisé pour une unique session TCP plafonne de toute façon assez 
rapidement quand on télécharge beaucoup de petits contenus 
(comme c'est souvent le cas pour le web) : Entre un tuyau de 4 Mb/s 
et un tuyau de 10 Mb/s, il n'y a quasiment aucune différence 
pour le chargement d'une page web (sauf à charger plusieurs pages 
simultanément), la bande passante réellement utilisée par 
le navigateur sera presque la même. 

### Latence, prévoyez large

Les latences moyennes sont elles entre 30 ms et 60 ms pour des connexions filaires
correctes en France métropolitaine et des sites hébergés eux 
aussi en France métropolitaine ou dans la proche Europe. La latence 
dépasse rarement les 100 ms pour des sites en Europe mais peut 
facilement exploser s'il y a un problème réseau quelconque, et dès que l'on
utilise des connexions aeriennes telles que Wifi public ou par mobiles. 

Vous pouvez travailler avec le jeu de latence suivant : 30 ms, 
45 ms, 60 ms, 90 ms et 120 ms voire 150 ms. 
Si vous travaillez exclusivement avec un réseau interne vous pouvez éventuellement utiliser 15 ms. 
Inversement n'hésitez pas à tester jusqu'à 200 ms pour tenir compte des connexions mobiles degradées
ou vous savez que les conditions réseau risquent d'être très mauvaises. 

#### Quelques explications sur la latence

La vitesse de la lumière est de 300 000 km/s. Pour réaliser le trajet 
jusqu'à un serveur Californien, il faudrait 20 000 km pour un trajet 
direct aller-retour, soit 67 ms. Dans le meilleur des cas nous 
utilisons des fibres optiques qui ralentissent de moitié la 
vitesse de propagation, ce qui donne 100 ms théoriques. En général, 
il faut environ doubler ces temps pour une connexion réelle, 
donc 200 ms dans notre cas. Même pour joindre New York nous avons 
dans les 12 000 km en ligne direct aller-retour, donc dans les 
120 ms possibles. 

Ce sont ces temps là que nous obtenons régulièrement depuis la 
France pour joindre des serveurs américains. Toute difficulté 
ou tout équipement réseau superflu viendrait en sus, et bien 
entendu cela ne comprend pas le délai de réaction de votre serveur 
lui-même. 

Les équipements réseaux aux deux extrémités amènent forcément 
un minimum de latence et pour un particulier, même avec des serveurs 
géographiquement proches, il sera peu commun de tomber en dessous 
de 30 ms. 

### Combiner latence et débit dans vos tests

Il n'est pas la peine de générer toutes les combinaisons entre 
débit et latence. Malheureusement un faible débit va très souvent 
avec une forte latence et inversement. Si on trouve parfois des 
connexions à forte latence et haut débit (les terminaux 3G+ par 
exemple), la bande passante restera de toute façon sous utilisée. 
Du fait du plafonnement des débits TCP, la bande passante utilisée 
est en fait plus dépendante de la latence que de la bande passante 
disponible. 

![Bande passante réellement utilisable pour du web en fonction de la latence](img/chap02-bande-passante-reellement-utilisable-pour-du-web-en-fonction-de-la-latence1.png)[^2]

  [^2]: Graphique en provenance de « More Bandwidth Doesn't Matter (Much) » par Mike Belshe le 4 août 2010

Travaillez avec quelques combinaisons qui vous semblent réalistes. 
Une latence moyenne se situe entre 30 et 60 ms pour des sites hébergés 
en France ou dans la proche Europe. 

Tout cela prend du temps à générer mais vous saurez ce qu'il en 
est sans vous baser uniquement sur votre cas. Faites ceci pour 
votre page d'accueil, mais aussi pour quelques types de pages 
internes significatives de votre trafic. La page d'accueil 
n'est pas forcément la mesure la plus pertinente car elle est 
très spécifique. 

À retenir
---------

Concernant le réseau 

* Un échange HTTP nécessite au préalable une résolution DNS 
  et l'établissement d'une connexion TCP, qui prennent tous 
  deux du temps ; 

* Ce qui est envoyé du navigateur au serveur et inversement prend 
  du temps, on parle de latence réseau ; 

* L'analyse des échanges réseau peut être faite entre autres 
  à l'aide de l'extension Firebug ou de webpagetest.org ; 

* La connexion de référence fait environ 2 Mb/s avec une latence 
  de 50 ms ; 

Concernant HTTP 

* Un échange HTTP est composé d'une requête et d'une réponse ; 

* Requête et réponse HTTP sont composés d'une ligne de statut, 
  d'une ou plusieurs entêtes clef:valeur, d'une ligne vide, 
  et éventuellement d'un contenu ; 

* Le détail HTTP (requête, réponse, entêtes, réseau) peut être 
  visualisé à l'aide de l'extension Firebug sur Mozilla Firefox ; 

Concernant les mesures 

* Les 5 directives principales de performance qui doivent vous 
  guider ; 

* Le ressenti de performance est aussi important que la mesure 
  objective ; 

* Utilisez les extensions Yslow et Google Page Speed comme fil 
  conducteur ; 

* Ne soyez pas trop optimistes sur la qualité des connexions 
  internet de vos visiteurs. 

Travailler avec les caches HTTP
===============================

Ce premier chapitre technique aborde les caches HTTP. Notre objectif va
être de réduire, presque à zéro, le temps de chargement des composants
d'une page web. En fait nous allons faire en sorte que le navigateur
n'ait presque rien à télécharger sur le réseau. Comme chacun peut
l'imaginer le gain pour le navigateur, et donc pour le visiteur, est
impressionnant.

Le principe du cache
--------------------

Dans une session de navigation classique, le visiteur repasse souvent
sur la même page, ou du moins sur des pages similaires d'un même site.
Il demande plusieurs fois l'affichage d'une même ressource, d'une même
icône, d'une même feuille de style.

Sur une page web, le trafic réseau représente l'essentiel du temps
d'attente et de chargement. Couper les temps d'attente réseaux et vous
aurez une réactivité quasi instantanée.

Pour cela le navigateur stocke certaines ressources dans un cache local
au premier accès. Il stocke en fait simplement les fichiers
correspondants sur disque ou en mémoire. Quand il a besoin une nouvelle
fois de cette même ressource, il peut la prendre dans cet espace
temporaire rapide, sans avoir à la re-télécharger et à subir les délais
dus au réseau.

![Système de cache HTTP local](img/chap03-systeme-de-cache-http-local.png)


Importance du cache
-------------------

Sur deux pages d'un même site, on peut facilement avoir 80 % des
composants identiques. Si le cache est correctement utilisé, la seconde
page ne télécharge donc que les 20 % qui lui manquent. C'est ça qui peut
faire la différence entre un site lent et un site instantané.

Pour un utilisateur, passer de 81 requêtes HTTP à 2, ou de 400 ko à
75 ko, c'est la différence entre un camion et une voiture de course en terme de performance.
Le résultat est visible immédiatement.

Mais en plus du gain visible pour l'utilisateur, vous avez un bonus.
Vérifiez combien vous coûtent vos serveurs et votre bande passante.
Imaginez diviser par 40 le nombre de hits ou par 5 votre bande passante.
L'investissement est très vite rentable.

![Téléchargements en premier et second accès sur Amazon.fr](img/chap03-telechargements-en-premier-et-second-acces-sur-amazon-fr.png)

Sur l'échantillon de sites français testés, on identifie le nombre de
requêtes et le poids total du document. Pour chaque site, le premier
camembert montre la proportion de requêtes HTTP économisées si le cache
est initialisé (en foncé) par rapport au total des requêtes HTTP
réalisées. Le second camembert montre ce même rapport en prenant en
compte le volume de données téléchargé plutôt que le nombre de requêtes
HTTP. Un graphique essentiellement foncé montre que le cache permet
d'éviter l'essentiel des requêtes et des téléchargements. Un graphique
majoritairement clair montre à l'inverse que le cache est inefficace ou
mal exploité.

Un mauvais résultat (graphique clair) n'est toutefois pas toujours
facile à interpréter. Ce peut aussi être un type de page qui profite peu
du cache mais c'est aussi le plus souvent simplement que l'équipe de
développement n'a pas exploité le cache du navigateur et laissé de
mauvaises performances là où on aurait pu avoir de très bons résultats.
On ne peut donc pas les ériger en contre-exemples. C'est en fait plus le
symptôme d'une mauvaise qualité des sites français.

À l'inverse, les bons résultats (graphique foncé) sont une preuve de
l'effet qui peut être obtenu en travaillant les performances du site
web. Wikipedia, Facebook, Microsoft, Apple, Dailymotion, la Fnac,
permettent d'économiser plus de 90 % de la bande passante lors du second
accès. Ils représentent des sites de contenu, des sites vitrine, des
sites de commerce, des sites orientés vidéo. Très peu de catégories sont
exclues et même la page d'accueil de Google qui n'a pour ainsi dire
aucun composant complexe arrive à diviser par 6 se bande passante.

![Influence du cache sur le poids des pages et le nombre de requêtes HTTP](img/chap03-influence-du-cache-sur-le-poids-des-pages-et-le-nombre-de-requetes-http.png)

Cache initialisé et cache vide, utilisateurs concernés
------------------------------------------------------

On parlera de cache vide ou de premier accès quand le cache du
navigateur ne contient pas encore les éléments que vous souhaitez y
entreposer. Il est alors obligé de télécharger tous les composants
nécessaires. À l'inverse on parlera de cache initialisé ou de second
accès quand le navigateur a déjà tous les composants dans son cache et
peut les réutiliser sans avoir à les re-télécharger.

### Nombre d'utilisateurs concernés

Sur le papier, un utilisateur arrive la première fois avec un cache vide.
Les composants de la page sont mis en cache par le navigateurs puis
toutes les pages suivantes, dans la même session ou dans toutes ses
sessions futures, sont téléchargées avec un cache pré-initialisé.

En pratique, les choses sont un peu différentes. Le cache des navigateurs
est limité en taille et vos utilisateurs ne naviguent pas que sur votre
site. Au fur et à mesure de leur surf le cache se remplit, et certains
anciens éléments sont effacés pour laisser de la place, peut être les
vôtres. Le résultat c'est qu'à la prochaine visite sur votre site, on se
retrouve dans la situation d'un cache vide : le navigateur doit
retélécharger tous vos composants.

Ensuite, certaines politiques de sécurité d'entreprise ou de gestion de
la vie privée vident automatiquement et régulièrement le cache de
certains utilisateurs. Ces utilisateurs vont devoir réinitialiser
souvent leur cache et se retrouveront régulièrement dans une situation
qui correspond à nos mesures « cache vide ».

Enfin, sur un site web public, une part non négligeable des utilisateurs
viennent des moteurs de recherche. Ce sont des visiteurs qui viennent
pour la première fois sur votre site et qui, pour la plupart, ne
visiteront que peu de pages. Eux se retrouvent très souvent dans une
situation de cache vide.

### L'étude Yahoo!

L'équipe performance de Yahoo! a mené une étude [^16] pour connaître la
proportion de caches vides et de caches pré-initialisés lors de l'accès
à leur page d'accueil. Les résultats publiés montrent que 40 à 60 % des
visiteurs ont subi à un moment où un autre une expérience de cache vide
mais surtout 20 % des pages sont chargées avec un cache vide. 20 % c'est
une page sur cinq. Un nombre important des utilisateurs ne profitent
donc pas du cache : les optimisations du cache sont essentielles, mais
ne perdez pas de vue qu'une partie des utilisateurs n'en profiteront
pas.

La page d'accueil de Yahoo! est très spécifique et ces chiffres ne sont
probablement pas généralisables à tous les sites. Toutefois, la tendance
est, elle, certainement généralisable sur la plupart des sites web
publics : les accès à votre site avec un cache vide sont fréquents et
non négligeables. Si vous avez un doute, faites vos propres mesures. La
réalisation est assez simple et Yahoo! explique comment faire dans son
étude.

[Proportion d'utilisateurs ou pages vues avec un cache vide](img/chap03-proportion-d-utilisateurs-ou-pages-vues-avec-un-cache-vide)[^17]

  [^16]: [Performance Research, Part 2: Browser Cache Usage – Exposed!, Yahoo!, janvier 2007](http://yuiblog.com/blog/2007/01/04/performance-research-part-2/)
  [^17]: © 2007- 2009  Reprinted with permission from Yahoo! Inc. YAHOO! and the YAHOO! logo are trademarks of Yahoo! Inc.

### Utilisateurs mobiles

Les terminaux mobiles ont des navigateurs de qualité très variable,
globalement plutôt mauvais. Android WebKit, Safari (iPhone) et Opera
Mobile sont à peu près au niveau mais les autres navigateurs sont
parfois catastrophiques, même quand ils sont eux aussi basés sur le même
code source.

Cloud Four a mené un test à grande échelle. Les résultats sont assez
mauvais. Près d'un tiers des utilisateurs testés possèdent un terminal qui ne
met pas en cache les ressources web. Les résultats sont très disparates
suivant les versions d'un même navigateur. Ainsi une installation de la
version 3 d'Opera Mini fonctionne correctement alors qu'une
installation de la version 4 ne gère aucun cache. Parmi les terminaux
les plus répandus, les Blackberry et Opera Mini 4 ont un sérieux
problème de cache. IE Mobile 7.x et Opera Mobile 8.x ont quelques
installations qui elles aussi ne permettent pas le cache (respectivement
sur 30% et 10% des tests).

Le résultat c'est que vous ne pouvez pas vous reposer sur le cache comme
palliatif des connexions bas débit des terminaux mobiles. Il sera
important de réduire au maximum la taille des composants, en tenant
compte du faible débit mais aussi de la probable absence de cache.

**Recommandation :** prévoyez qu'une grande partie de vos utilisateurs
mobiles ne pourront pas profiter du cache. Limitez le nombre de vos
composants ainsi que leur taille.

### Résultats globaux

Toutefois, gardez en tête les résultats de Apple ou de Microsoft. Même
si 20 % des utilisateurs n'en profitent pas, le gain reste intéressant
en moyenne : gagner 90 % de la bande passante sur 80 % des pages, ça
permet encore de diviser la bande passante globale par 3 à 4.


Requête conditionnelle
----------------------

La principale problématique du navigateur est de savoir quoi mettre en
cache et combien de temps. Un élément qui ne part pas en cache, c'est un
téléchargement en plus. Un élément qui reste trop longtemps en cache,
c'est un élément qui risque de ne pas être mis à jour dans le rendu du
navigateur quand bien même il aurait changé sur le serveur.

Le premier mécanisme mis en oeuvre pour répondre à ces questions est la
requête conditionnelle. Il s'agit pour le serveur d'informer le
navigateur sur la date de dernière mise à jour du contenu. Dans les
téléchargements suivants, le navigateur peut demander au serveur si la
ressource a changé. Si ce n'est pas le cas, alors on évite de la
retélécharger.

### Détails HTTP

Lors du premier téléchargement, le serveur web envoie un identifiant ou
une date de mise à jour du contenu. L'entête en jeu pour la date de mise
à jour s'appelle `Last-Modified`. Elle utilise le format de la RFC 1123 :
`Sun, 06 Nov 1994 08:49:37 GMT`.

Requête :

~~~~~~~ {.http .request}
GET /index.html HTTP/1.1
Host: example.org
~~~~~~~

Réponse :

~~~~~ {.http .response}
HTTP/1.1 200 OK
Date: Sun, 02 Nov 2008 15:54:27 GMT
Last-Modified: Tue, 15 Nov 2005 13:24:10 GMT
Etag: "280100-1b6-80bfd280"
Content-Type: text/html; charset=UTF-8

<HTML>
[…]
</HTML>
~~~~~

Lors des téléchargements suivants, le navigateur renvoie cet identifiant
ou cette date et demande un téléchargement conditionnel. Cela se fait
avec l'entête `If-Modified-Since`, en reprenant la date fournie la
dernière fois par le serveur. Si le contenu a changé, tout se passe
comme habituellement, et le serveur renvoie une nouvelle date.

Seconde requête :

~~~~~~~ {.http .request}
GET /index.html HTTP/1.1
Host: example.org
If-Modified-Since: Mon, 29 Apr 2013 13:24:10 GMT
Accept-Encoding: gzip,deflate
~~~~~~~

Réponse :

~~~~~ {.http .response}
HTTP/1.1 200 OK
Date: Sun, 28 Apr 2013 15:54:27 GMT
Last-Modified: Mon, 22 Apr 2013 18:44:18 GMT
Content-Type: text/html; charset=UTF-8

<HTML>
[…]
</HTML>
~~~~~

Si au contraire le contenu n'a pas été mis à jour depuis la dernière
fois, le serveur web se contente de signaler que rien n'a changé. Pour
cela il renvoie un code de retour 304 au lieu de l'habituel code 200 et
du contenu. Il y a toujours une requête HTTP, et donc un léger délai,
mais on évite de re-télécharger le contenu lui-même. Si la ressource
dépasse un ou deux kilo-octets, cela fait une différence.

Seconde requête :

~~~~~~~ {.http .request}
GET /index.html HTTP/1.1
Host: example.org
If-Modified-Since: Mon, 29 Apr 2013 13:24:10 GMT
~~~~~~~

Réponse (seules des entêtes sont envoyées, aucun contenu) :

~~~~~ {.http .response}
HTTP/1.1 304 NOT MODIFIED
Date: Mon, 29 Apr 2013 15:54:27 GMT
Content-Type: text/html; charset=UTF-8
~~~~~

### Support des navigateurs

Tous les navigateurs savent gérer les requêtes conditionnelles. Les
seules exceptions sont certains mauvais robots de téléchargement de fils
RSS. La situation évolue toutefois sur ces derniers logiciels et on peut
considérer que les mauvais clients HTTP se font rares.

Le protocole HTTP laisse toutefois une marge d'appréciation au
navigateur. Ainsi, certains navigateurs vérifient s'il y a eu une mise à jour à
chaque visite, ou à chaque session, ou encore à chaque page. C'est la raison
pour laquelle parfois sur Microsoft Internet Explorer, on continue pendant
quelques temps à voir un ancien contenu au lieu de voir la mise à jour.

Pour palier ce problème, il est possible de demander explicitement au
navigateur de revalider son contenu à chaque fois. Cela se fait avec le
paramètre `must-revalidate` de l'entête `Cache-Control` envoyée par le
serveur :

~~~~~ {.http .partial .response .oneline}
Cache-Control: public; must-revalidate
~~~~~

Il existe aussi un paramètre `proxy-revalidate`, qui a le même effet mais
qui s'adresse uniquement aux serveurs proxy. Les autres paramètres de
cette entête sont détaillées plus loin dans ce chapitre.

### Mise en œuvre sur le serveur web

La configuration par défaut de votre serveur web s'occupe de gérer tout
cela avec les contenus statiques. Si vous utilisez du contenu dynamique
(PHP, Java, Ruby, Python, etc.) c'est à vous de déterminer la dernière
date de modification de vos contenus, de comparer avec celle que vous
recevez, et d'envoyer soit le code de retour 304 soit l'entête
Last-Modified.

**Recommandation :** gérez manuellement les entêtes HTTP de cache quand
vous servez du contenu dynamiquement (PHP, .Net, Ruby, Python, etc.) et
principalement des contenus réutilisables changeant peu (CSS,
Javascript, images).


Avec Ruby On Rails, un mécanisme est prévu pour cela dans le framework
via la méthode `fresh_when`, qui prend une date de dernière modification
et un ETag :

~~~~~~~ {.ruby .rails}
fresh_when :last_modified => @article.published_at.utc, 
           :etag => @article
~~~~~~~

En PHP vous pouvez utiliser une fonction comme la suivante :

~~~~~~~ {.php}
<?php
function HttpCache($last_modified_timestamp)  {
  if( isset($_SERVER['HTTP_IF_MODIFIED_SINCE']) ) {
    $from_browser = $_SERVER['HTTP_IF_MODIFIED_SINCE'] ;
    $from_browser = strtotime($from_browser) ;
    if ($from_browser = = $last_modified_timestamp) {
      header('Not Modified', true, 304) ;
      exit ;
    }
  }
  $format = 'D, d M Y H:i:s' ;
  $to_browser = gmdate($format, $last_modified_timestamp) . ' GMT' ;
  header("Last-Modified: $to_browser");
}
~~~~~~~

ETag
----

La gestion du cache par la date de dernière mise à jour est un mécanisme dynamique proposé par la version 1.0 du protocole HTTP.
HTTP 1.1, quant à lui, apporte un mécanisme plus étendu : l'ETag (pour Entity Tag, en anglais).

### Détails HTTP

Au lieu d'une date avec `Last-Modified`, on envoie un identifiant textuel
avec l'entête `ETag`. Cet identifiant est ensuite renvoyé avec
`If-None-Match` au lieu de `If-Modified-Since`.

Première réponse :

~~~~~~~ {.http .response}
HTTP/1.1 200 OK
Date: Sun, 02 Nov 2008 15:54:27 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Sun, 01 Feb 2009 18:44:18 GMT
Etag: "29244f-45d-3819bb2e"
Content-Type: text/html; charset=UTF-8

<HTML>
[…]
</HTML>
~~~~~~~

Requête suivante :

~~~~~~~ {.http .request}
GET /index.html HTTP/1.1
Host: example.org
If-Modified-Since: Tue, 15 Nov 2005 13:24:10 GMT
If-None-Match: "29244f-45d-3819bb2e"
~~~~~~~

### Avantages des ETags sur les dates de dernière modification

Il y a deux avantages à ce nouveau procédé. Tout d'abord on peut gérer
des mises à jour plus fines, éventuellement plusieurs mises à jour pour
une même seconde, ce qui n'était pas possible auparavant. Ensuite, HTTP
1.1 permet de gérer plusieurs représentations d'un même document à la
même adresse, par exemple, un même contenu en plusieurs langues. Il est
ainsi possible d'avoir plusieurs identifiants pour une même adresse (un
par représentation), ce qui n'était pas réalisable avec les dates de
modification.

En réalité tous les clients HTTP ne savent pas gérer cette négociation
avec ETag. Il faut donc toujours la doubler avec la négociation par date
de dernière modification quand on le peut.

Au final, comme peu de sites utilisent plusieurs représentations pour
une même adresse, et comme on doit de toutes façons doubler avec la date
de dernière modification, la gestion des ETag a un intérêt limité.

### Désactiver les ETags ?

L'équipe performance de Yahoo! a proposé dès le départ de désactiver la
gestion des ETags sur les serveurs web. Outre leur intérêt limité, ils
posent en effet quelques problèmes.

L'ETag est un identifiant unique textuel. Pour trouver cet identifiant
unique, le serveur web le plus courant sur le marché, Apache, utilise
l'identifiant interne du fichier sur le disque (l'inode) associé à la
date de modification du fichier. Si vous avez un gros site avec
plusieurs serveurs, chacun a son disque, et l'identifiant interne du
fichier sera différent sur chaque serveur.

Chaque serveur aura donc son propre identifiant, différent de celui des
autres. Pour peu que la première et la seconde requête ne touchent pas
le même serveur, vous re-téléchargerez inutilement des contenus. Bref,
non seulement inutiles, les ETags vont pénaliser vos performances.

Désactiver les ETags sous Apache peut être réalisé avec la directive :

~~~~~~~ {.apache .oneline}
FileETag none
~~~~~~~

En réalité, il existe d'autres méthodes de génération des ETags, qui ne
comportent pas ce défaut. Il suffit de ne pas utiliser l'identifiant
interne du fichier (l'inode). On peut par exemple recommander d'utiliser
la date de dernière modification associée à la taille du fichier :

~~~~~~~ {.apache .oneline}
FileETag MTime Size
~~~~~~~

**Recommandation :** si votre site utilise plusieurs serveurs web,
désactivez les ETags ou assurez-vous qu'ils ne se basent pas sur l'inode
des fichiers.

Désactiver les ETags est plus simple, et vu leur faible utilité, c'est
probablement le plus simple, mais le choix vous revient. Si votre site
n'utilise qu'un seul serveur pour l'instant, ne vous en préoccupez pas.

### Statistiques

L'analyse d'un échantillon d'une trentaine de sites français ne dégage
pas de consensus fort sur la questions des ETags. À peu près autant
fonctionnent avec ETag que sans ETag. Tout au plus, on peut noter que
ceux qui ont les meilleures performances, donc qui ont probablement le
plus réfléchi à la question, ont plutôt tendance à les désactiver.

Il n'y a pas non plus de spécificité suivant le type de contenu (HTML,
javascript, feuille de style, images, autres). Seul Cdiscount fait une
différence en mettant des ETag sur 60 images mais aucun des 25 fichiers
javascript ou des 19 fichiers CSS.

![Statistiques sur la présence d'ETag sur un échantillon de sites français](img/chap03-statistiques-sur-la-presence-d-etag-sur-un-echantillon-de-sites-francais.png)

Note : sur la plupart des sites on peut voir qu'il y a toujours quelques
contenus sans ETags, et quelques contenus avec ETag. Il s'agit le plus
souvent de publicités et contenus externes, non contrôlés par le site
lui-même, ou de contenu très dynamique regénéré à chaque accès (dans le
cas de l'absence des ETags).

Expiration explicite des contenus
---------------------------------

Le système des requêtes conditionnelles reste imparfait du point de vue
des performances, qu'il soit basé sur la date de dernière modification
ou l'ETag. Chaque nouvelle utilisation d'une ressource implique une
requête conditionnelle sur le serveur.

On évite de transférer le contenu de la ressource mais on attend quand
même une réponse réseau. Pour une page classique avec cinquante
composants, deux téléchargements simultanés, et une latence de 40 ms,
nous devrons attendre au mieux une bonne seconde pour rien.

~~~~~~~ {.oneline}
attente minimum = nbr ressources / connex. simultanées * latence
~~~~~~~

La solution, c'est l'expiration explicite des contenus. Il s'agit
d'informer le navigateur que la ressource ne changera pas avant une
certaine date. Jusque là, le navigateur récupère directement son
ancienne version sans faire aucune requête réseau.

### Détails HTTP

Lors de la première réponse, le serveur envoie une entête HTTP nommée
`Expires` ou la propriété `max-age` de l'entête `Cache-Control`. La première
permet de définir une date d'expiration au format HTTP. La seconde est
réservée à HTTP 1.1 et permet de définir plutôt une période de validité
avec un nombre de secondes. On évite ainsi les problèmes de synchronisation
horaire.

En général, on définit les deux. En cas de conflit, c'est `Cache-Control`
qui prime. `Expires` ne servira que pour un client HTTP 1.0 qui ne
comprend pas `Cache-Control`. Si vous n'en choisissez qu'une, utilisez
`Expires`.

Réponse HTTP :

~~~~~~~ {.http .response}
HTTP/1.1 200 OK
Date: Sun, 02 Nov 2008 15:54:27 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Sun, 01 Feb 2009 18:44:18 GMT
Expires: Sun, 01, Feb, 20010 18:44:18 GMT
Cache-Control: max-age=31536000

[…]
~~~~~~~

Jusqu'à l'expiration des contenus – ou si le visiteur renouvelle le
cache de son navigateur – le navigateur ne fera plus aucune requête pour
ces ressources. Cela implique aussi que si vous modifiez l'image ou le
fichier concerné, les visiteurs qui l'ont déjà en cache ne verront pas
la mise à jour. Aucun procédé ne vous permettra de forcer vos visiteurs
à mettre à jour leur cache. Cette problématique est abordée plus loin
dans cette section, avec deux solutions qui sont utilisées par
l'essentiel des sites web.

Si le cache d'une ressource a expiré, le navigateur utilisera une
requête conditionnelle plus classique pour revalider son contenu (et
éventuellement ne pas avoir à le retélécharger). Il obtiendra alors une
nouvelle date d'expiration explicite, et éventuellement le nouveau
contenu si la ressource a été mise à jour entre temps.

Si la directive `max-age` est présente en même temps que `must-revalidate`,
alors le client peut utiliser le cache pendant le temps indiqué, mais il
doit ensuite absolument revalider son contenu et ne pas réutiliser le
cache plus longtemps. Cela s'adresse particulièrement à certains proxy
intermédiaires qui sont configurés pour imposer leurs propres durées
d'expiration sans prendre en compte ce qui est demandé par le serveur. 

Note : pour `Cache-Control`, il existe aussi une directive `s-maxage`,
identique à la précédente, mais qui s'adresse uniquement aux serveurs
proxy.

### Que mettre en cache et pour combien de temps

En pratique, on sépare d'un côté les ressources qui ne changent que
rarement, voire jamais (images de graphisme, feuilles de style,
javascript et plus généralement tous les fichiers statiques), et d'un
autre côté les ressources dynamiques ou qui changent fréquemment
(principalement le code HTML et quelques échanges ajax).

Les ressources statiques peuvent le plus souvent être mises en cache à
vie, c'est à dire une dizaine d'années. Les ressources dynamiques
dépendent de votre application :

Le plus souvent les pages HTML contiennent des sections ou des
informations liées à une authentification, des commentaires, des
actualités, des publicités ou d'autres éléments qui nécessitent d'être
réactualisés par le serveur à chaque accès. Ainsi sur l'essentiel des
sites, les pages HTML elles-même ne sont pas mises en cache.

**Recommandation** : Pour tous les fichiers statiques (qui ne devraient
pas être mis à jour plus d'une à deux fois par semaine dans les périodes
de plus grand changements), définissez une expiration explicite très
importante, par exemple dix ans.

Attention particulièrement au fichier favicon.ico qu'on oublie
fréquemment. Ce fichier est d'autant plus spécial qu'il est préférable
de ne pas en changer l'adresse. Une préconisation spécifique est prévue
plus loin dans ce chapitre concernant sa date d'expiration.

### Mise en œuvre sur le serveur web

Par défaut, votre serveur web ne définit aucune expiration explicite.
C'est à vous de spécifier ce que vous souhaitez dans votre
configuration. Pour Apache il est possible d'utiliser 
[mod_expires](http://httpd.apache.org/docs/2.2/mod/mod_expires.html).
Vous pourrez, pour un répertoire précis ou pour certains types de
ressources précis, définir une date d'expiration ou une durée de
validité pour vos contenus.

Ces expirations peuvent être définis par rapport à la date d'accès (cas
préférable) ou à la date de dernière modification (pour des cas très
spécifiques) :

~~~~~~~ {.apache}
ExpiresActive On\
ExpiresDefault "access plus 1 month"
ExpiresByType text/html "access plus 1 month 15 days 2 hours"
ExpiresByType image/gif "modification plus 5 hours 3 minutes"
~~~~~~~

Pour les fichiers générés par Java, PHP ou Rails, il vous faudra définir
vous-même les entêtes HTTP. Par exemple pour PHP :

~~~~~~~ {.php}
<?php
$expires = 3600 * 24 * 365 * 10 ; // 10 ans
$format = 'D, d M Y H:i:s' ;
$to_browser = gmdate($format, time()+$expires) . ' GMT' ;
header("Expires: $to_browser"); 
header("Cache-Control: max-age=$expires") ;
~~~~~~~

Pour des fichiers HTML avec une expiration courte, comme une page
d'accueil, n'oubliez pas de gérer aussi les requêtes conditionnelles,
afin de permettre au navigateur de revalider son contenu et obtenir une
nouvelle expiration sans tout retélécharger.

Sur Lighttpd, les paramètres du modExpire sont similaires mais la syntaxe
du fichier de configuration laisse plus de latitude pour cibler les
contenus à mettre en cache :

~~~~~~~ {.lighttpd}
$HTTP["url"] =~ "^/images/" {
  expire.url = ( "" => "access plus 1 hours" )
}
~~~~~~~

### Cas particulier des connexions sécurisées (HTTPS)

Microsoft Internet Explorer a un comportement particulier quand les
ressources en cache viennent de connexions sécurisées par HTTPS. Le
navigateur impose d'avoir au moins établi un échange avec le serveur
dans la page courante avant de réutiliser des données de ce serveur dans
le cache. En pratique cela veut dire que le premier composant de chaque
domaine accédé en HTTPS sera toujours retéléchargé et jamais pris à
partir du cache, quelles que soient les entêtes que vous lui envoyez. Il
en va de même pour Mozilla Firefox.

En fait, on va même un peu plus loin puisque si vous demandez trois
images sur un domaine HTTPS, même si ces dernières sont en cache, ce sont
peut être les trois qui seront retéléchargées. Internet Explorer
télécharge la première, à dessein, mais risque de ne pas avoir encore la
réponse avant de tenter d'afficher la seconde image. Au lieu d'attendre,
il constate qu'il n'a toujours pas établi de connexion, et envoie une
seconde requête à la couche logicielle qui s'occupe du réseau.

### Versionnement des URLs et mise à jour des contenus

Avec une date d'expiration explicite, il n'y a aucun moyen pour
l'éditeur d'un site web de forcer une mise à jour d'un contenu mis en
cache par le navigateur. Si vous changez un logo ou faites des
corrections dans une feuille de style, certains visiteurs risquent de ne
jamais recevoir la nouvelle version.

Pour palier ce problème, il est possible de versionner les URL. Le cache
du navigateur est en effet lié à l'adresse de la ressource. Si on change
cette adresse, le navigateur considère qu'il s'agit d'une nouvelle
ressource. Il suffit alors de changer l'adresse des composants à chaque
modification.

#### Forme des adresses versionnées

Il suffit d'ajouter un jeton à l'adresse normale du fichier, et de
s'assurer que ce jeton sera différent à chaque modification. On peut
l'ajouter soit dans un paramètre à la fin de l'adresse
(monstyle.css?xxx), soit dans le nom du fichier lui même
(monstyle-xxx.css). La première forme a l'avantage de ne nécessiter
aucun changement dans le stockage ou la configuration du serveur web. La
seconde forme impose d'avoir un fichier physique par version, ou
d'utiliser une réécriture d'URL comme celle qui suit (exemple avec
mod_rewrite et Apache) mais permet de garder un historique des
versions :

~~~~~~~ {.apache .online}
RewriteRule ^(.*)-\d\.\d(\.[a-z]{1,5})$  $1$2
~~~~~~~

La seconde forme, avec un jeton directement intégré dans le nom du
fichier est très fortement préférable. En effet, HTTP propose par défaut
de désactiver les caches non explicites s'il y a des paramètres
(présence d'un point d'interrogation) dans l'adresse. Vous risqueriez de
ne pas utiliser au mieux certains caches mal configurés même si vous
avez une expiration explicite.

![Les adresses des fichiers javascript contiennent des numéros de version](img/chap03-les-adresses-des-fichiers-javascript-contiennent-des-numeros-de-version.png)

#### Forme et génération du jeton unique

Le jeton a pour seule contrainte d'être unique. Quand il est géré
manuellement ou via un processus de publication hors ligne, on utilise
généralement un simple numéro de version (monstyle-1.3.css). C'est ce
qu'utilisent la plupart des sites à fort trafic qui ont industrialisé
ces questions.

Il est aussi possible d'automatiser cette gestion en utilisant la date de
dernière modification du composant ou un hash de son contenu comme jeton.
C'est ce que fait le framework Ruby on Rails par défaut pour les fichiers
statiques. En PHP on pourrait utiliser la fonction suivante pour faire des
liens :

~~~~~~~ {.php}
<?php
function lien_avec_version($fichier) {
  $prefixe = "/var/www/htdocs" ; // suivant votre configuration
  $date = filemtime($prefixe.$fichier) ;
  return "$fichier?$date" ;
}
$url = lien_avec_version("/monimage.png") ;
echo " <img src='$url' alt='' /> ";
~~~~~~~

Si vous utilisez cette dernière méthode, préférez générer les jetons une
fois pour toute à la publication des pages. Si vous utilisez un code PHP
exécutable à chaque requête (comme dans l'exemple ci-avant) vous risquez
de surcharger inutilement vos serveurs.

#### Cas particuliers et favicon.ico

Certains fichiers ont une adresse web qui ne peut pas changer ou qu'il
serait préférable de ne pas changer. C'est par exemple le cas du
favicon, que certains navigateurs chercheront par défaut à l'adresse
`/favicon.ico`. Dans ce cas il n'est pas souhaitable de changer le nom du
fichier et donc pas possible de lui rajouter des paramètres pour
invalider le cache. Il faut donc éviter des expirations explicites trop
longues et proposer plutôt une expiration de quelques jours (une à deux
semaines par exemple).

**Recommandation** : définissez une expiration explicite de 2 à 15 jours
pour le favicon.ico afin d'en faciliter le cache sans empêcher d'en
changer le contenu à l'avenir.

Différencier les copies suivant les utilisateurs et les contextes
-----------------------------------------------------------------

Le bénéfice du cache HTTP est indéniable mais il pose d'autres
problèmes : si plusieurs personnes passent pas le même proxy, elles ne
pourront pas accéder à des données personnalisées. De même, les méthodes
vues jusqu'à présent empêchent la donnée de varier en fonction du
contexte (personne identifiée ou non, présence d'un panier, langue du
navigateur, etc.).

Pour nous aider, nous avons l'entête `Vary` et deux paramètres
supplémentaires à l'entête `Cache-Control`.

L'entête `Vary` permet d'informer le navigateur et les caches
intermédiaires du contexte de réutilisation de la page. En ajoutant
`Cookie` comme valeur, une donnée en cache ne sera réutilisée que pour les
futures requêtes avec la même valeur de cookie. En ajoutant
`Accept-Language` on pourra avoir un cache différent par langue des
navigateurs. On peut aussi trouver `Accept-Encoding` (qui permet de savoir
si le navigateur accepte ou pas la compression HTTP), `User-Agent` (pour
faire une page différente suivant les navigateurs) ou `Accept` (pour faire
un contenu qui peut être envoyé en plusieurs formats suivant ce que
supporte le navigateur). Ces différentes valeurs peuvent être cumulées
en les séparant par des virgules.

L'entête suivante permet de servir un cache différent suivant la langue
et le navigateur :

~~~~~~~ {.http .response .partial .online}
Vary: Accept-Language, User-Agent
~~~~~~~

Il est souvent plus simple de se passer totalement des caches partagés
(proxy) et de se contenter du cache des navigateurs. On s'assure alors
que chaque client a sa propre page et ses propres contenus, sans pour
autant s'empêcher de profiter du cache. Pour cela il suffit de rajouter
le paramètre `private` à l'entête `Cache-Control`.

~~~~~~~ {.http .response .partial .online}
Cache-Control: private, max-age=3600 
~~~~~~~

Le paramètre opposé, `public`, explicite que la donnée est publique,
partageable à tous et donc utilisable par un cache partagé. C'est public
qui est la valeur par défaut pour les requêtes de type GET. Pour les
requêtes de type POST ou en erreur 404 cela autorise le navigateur à
mettre le résultat en cache (ce qu'il n'aurait pas fait sinon) et donc à
ne pas retransmettre la donnée au serveur la prochaine fois. On se
contente habituellement de spécifier ce paramètre pour les requêtes de
type GET sans erreur, sinon le serveur risque de ne pas déclencher
certains traitements en provenance de formulaire (vu que le navigateur a
utilisé le cache).

~~~~~~~ {.http .response .partial .online}
Cache-Control: public, max-age=3600 
~~~~~~~

De manière générale, on choisit entre public et privé suivant que la
ressource contient des données propres à l'utilisateur courant ou si
elle peut être réutilisée par tout le monde. Dès que vous avez une
authentification ou des préférences utilisateurs, c'est probablement un
`Cache-Control:private` qu'il vous faut.


Cache applicatif HTML5
-----------------------

La spécification HTML5 a prévu des fonctionnalités avancées de cache
pour les applications hors-ligne. Il s'agit de pouvoir utiliser un site
web une fois déconnecté, par exemple pour lire du contenu pré-téléchargé
ou pour gérer des données comme des e-mails. Les images, illustrations,
et composants doivent alors être sauvegardés par l'appareil pour être
réutilisés sans être retéléchargés.

Un fichier texte, nommé manifeste, liste les différentes ressources qui
seront sauvegardées en cache suivant la syntaxe suivante : une première
ligne avec « CACHE MANIFEST », une adresse de ressource (relative ou
absolue) par ligne.

~~~~~~~ {.cachemanifest}
CACHE MANIFEST
 /style.css
 /yui.js
 /img/icones.png
 /img/bandeau.jpg
~~~~~~~

Il suffit ensuite de servir ce fichier avec le type mime
`text/cache-manifest` et d'en référencer l'adresse dans l'attribut
manifest de la balise `<html>` du document principal :

~~~~~~~ {.html}
<!DOCTYPE HTML>
<html manifest="/cache.manifest">
<head> ... </head>
<body>
...
</body>
</html>
~~~~~~~

À chaque accès, le navigateur télécharge le manifeste. S'il s'agit d'un
nouveau manifeste ou si le contenu a changé toutes les ressources sont
retéléchargées pour être mises à jour. Le cache applicatif HTML5 permet
en fait d'autres possibilités (liste blanche, fallback, API pour accéder
aux différents événement dans le navigateur, etc.) mais cela dépasse le
cadre de ce livre.

Il s'agit d'une alternative intéressante au cache HTTP avec expiration
explicite. Elle ajoute en effet une fonctionnalité nouvelle : l'auteur a
l'initiative de déclencher la mise à jour des ressources en cache. La
syntaxe est toutefois plus complexe, un fonctionnement moins souple et
son support est limité (seules les dernières versions des navigateurs
supportent cette nouveauté).

Exploiter au mieux les caches
-----------------------------

### Externaliser les ressources

De manière générale, il est préférable de ne pas mettre en cache les
pages web elles-même, uniquement leurs composants statiques. Elles
contiennent presque toutes une invite d’authentification utilisateur,
des publicités, des commentaires utilisateurs ou une section
d’actualités. Ces éléments doivent être réactualisés et, si la page était
mise en cache, ils resteraient bloqués à leur ancienne valeur.

Ainsi, afin de profiter au maximum du cache, on tente de séparer les
composants statiques du reste de la page. En externalisant les feuilles
de style et les javascript dans des fichiers externes, on permet que ces
codes soient mis en cache, selon les méthodes expliquées plus avant,
quand bien même la page HTML elle-même ne le serait pas.

Cette externalisation, où chaque déclaration javascript ou css est
retirée du code HTML, permet de réduire de façon importante la taille de
chaque page, et donc le temps de téléchargement pour l’utilisateur.
Chaque clic sur un lien va mener à une nouvelle page bien plus
rapidement.

En contre-partie, nous augmentons le nombre de requêtes HTTP à faire
lors du premier accès à votre site. Cela peut avoir un impact
significatif si votre site a une grande latence, ou si vos visiteurs ne
consultent qu’une à deux pages à chaque visite. Nous verrons cependant
dans les chapitres suivants qu’il est possible de regrouper les
différents fichiers javascript et CSS pour limiter cet effet négatif.

**Recommandation** : externalisez toutes les instructions CSS ou
javascript dans un fichier séparé de la page HTML. Vous pourrez alors
définir une expiration explicite importante sur ces éléments.

### Système de préchargement

Le préchargement est le second moyen pour optimiser au mieux les caches.
Il s’agit de télécharger un composant par avance pour la mettre en
cache. Quand le navigateur aura réellement besoin de cette donnée, il
pourra alors la récupérer instantanément à partir du cache.

Toute la difficulté est de trouver un équilibre entre le bénéfice du
préchargement et le risque de télécharger par avance des composants qui
ne seront pas utilisés ensuite. Un préchargement inutile est en effet
pénalisant pour l’utilisateur mais aussi pour vos serveurs.

Ainsi, même si votre page d’accueil n’a pas de commentaires, pourquoi ne
pas charger les icônes et codes javascript qui sont utilisés dans les
pages internes ? Les illustrations courantes, les javascripts et
feuilles de styles sont de très bon candidats au préchargement. Les
pages HTML elles-même offrent un ration utilité/risque un peu moins
intéressant.

**Recommandation** : si votre page courante est très différente des
pages suivantes probables, n’hésitez pas à précharger certains
composants afin d’accélérer la transition.

La seule contrainte forte du préchargement est de ne pas être
contre-productif, c’est à dire ne pas ralentir notre page d’accueil sous
prétexte d’accélérer les suivantes. Pour cela les ressources à
précharger sont initialisées en fin de document, après tout ce qui est
utilisé sur le moment.

Une formule assez simple pourrait être utilisée : on multiple le
pourcentage d'utilisateurs qui auront besoin de la ressource dans leurs
prochains accès par le surcoût que représente le téléchargement de cette
ressource si elle n'est pas préchargée. D'un autre côté on multiplie le
pourcentage d'utilisateurs qui n'auront pas besoin de la ressource à
court terme par le surcoût que représente le préchargement de cette
ressource. Si le premier chiffre est le plus grand alors il faut
précharger, sinon il vaut mieux s'abstenir.

En pratique, la question est beaucoup plus délicate car il est quasiment
impossible de déterminer le coût pour l'utilisateur d'un préchargement
inutile. Beaucoup d'utilisateurs ne ressentiront aucun désagrément,
alors que certains qui ont d'autres téléchargements parallèles verront
un ralentissement sans savoir à quoi l'attribuer.

La méthode utilisée pour le préchargement a aussi une influence dans la
décision. Ainsi comme c'est détaillé dans la suite, l'utilisation d'une
balise `<link rel="prefetch" ...>` présente moins de risques d'effet
négatif que l'utilisation d'un code javascript.

#### Prefetch et prerender

La méthode idéale est d’utiliser la balise `<link>` avec un attribut
`rel="prefetch"`. Le navigateur reconnaît alors cette balise et tente de
précharger la ressource dès qu’il est inactif. Le risque d’effet négatif
est très réduit car le navigateur vérifie qu’aucune activité réseau
n’est en cours sur aucun onglet avant de commencer. De plus il stoppe
ces préchargements dès qu’une requête normale est initiée, afin de ne
pas la pénaliser. Si cela peut impacter la bande passante d’autres
logiciels sur le même réseau, cela ne gênera jamais en aucune façon
votre session de navigation elle-même. Cette fonctionnalité est prévue
dans HTML5 mais n’est pour l’instant implémentée que dans les
navigateurs basés sur le moteur Gecko (dont Mozilla Firefox) et quelques
versions du navigateur Blackberry.

~~~~~~~ {.html .partial .online}
<link rel="prefetch" href="http://example.org/maressource">
~~~~~~~

Vous pouvez vérifier qu'une requête est réalisée par un navigateur
Mozilla dans le cadre des liens de préchargement grâce à la présence de
l'entête HTTP `X-Moz:prefetch`.

Le navigateur Chrome (mais uniquement lui) va même un peu plus loin en
permettant l'utilisation de l'attribut `rel="prerender"`. Contrairement à
prefetch le navigateur est incité à analyser la page préchargée pour en
télécharger les sous-composants et en commencer le rendu. L'effet est
similaire à celui d'une page téléchargée dans un onglet en tâche de
fond. Vous pouvez savoir si la page est chargée en tâche de fond ou en
avant-plan grâce à la propriété javascript
`document.webkitVisibilityState` qui retournera « visible », « hidden » ou
« prerender », suivant les cas.

~~~~~~~ {.html .partial .online}
<link rel="prerender" href="http://example.org/maressource">
~~~~~~~

#### Les méthodes manuelles

Il existe d’autres méthodes, mais toutes ont un même défaut majeur : il
n’est pas possible de faire un préchargement dépendant de l’activité
réseau du navigateur. Des préchargements trop importants peuvent
ralentir le chargement d’autres onglets ou mettre en attente des
ressources sur le même domaine. Retarder des requêtes souhaitées
maintenant par l’utilisateur afin d’accélérer une future requête
potentielle est rarement intéressant. Aussi, ces méthodes ne doivent pas
référencer plus de deux ressources par domaine (afin de ne pas trop
retarder les autres requêtes du même domaine) et uniquement, si elles ont
un bon rapport utilité/risque. De plus, afin de ne commencer qu’après le
chargement complet de la page courante, elles ne devraient être
initialisées qu'après l’événement onload de la page courante.

La plus simple de ces méthodes alternatives est d’utiliser des requêtes
Ajax. Elles ont l’avantage d’être asynchrones et de ne pas avoir
d’indicateur de chargement dans l’interface utilisateur. Un sablier ou
un indicateur équivalent donnerait en effet un très mauvais ressenti de
performance à l’utilisateur, même si le résultat objectif est
intéressant.

Il est aussi possible de charger des images en javascript avec `new
Image()`, de charger une bibliothèque de code javascript avec un simple
`<script>`, d'utiliser la balise `<object>`, ou même de charger des
éléments dans une iframe cachées. Ces trois méthodes peuvent toutefois
provoquer des indicateurs d’attente dans l’interface utilisateur, voire
bloquer l’interface entièrement (pendant l’interprétation de la
bibliothèque javascript par exemple).

#### DNS

Mozilla Firefox, Safari et Google Chrome, dans leurs dernières versions,
proposent aussi de précharger les résolutions DNS. Ils scannent alors
tous les liens de la page et font les requêtes DNS utiles quand
l’activité réseau le permet. Il n’y a rien à faire, mais il est possible
de demander explicitement le préchargement d’un nom de domaine avec la
syntaxe suivante :

~~~~~~~ {.html .partial .online}
<link rel="dns-prefetch" href="//example.org">
~~~~~~~

Par défaut ce préchargement n’est pas activé pour les liens sécurisé en
HTTPS. Il est possible de les demander explicitement avec la balise méta
suivante :

~~~~~~~ {.html .partial .online}
<meta http-equiv="x-dns-prefetch-control" value="on">
~~~~~~

Inversement, la valeur off permet de désactiver les préchargements DNS
(mais pourquoi le voudriez-vous ?).

Microsoft Internet Explorer 9 tente aussi de résoudre en avance les noms
de machines dans les liens en préchargement :

~~~~~~~ {.html .partial .online}
<link rel="dns-prefetch" href="http://example.org/index.html">
~~~~~~~

Internet Explorer 9 fait même mieux puisqu'il retient les domaines
utilisés lors des précédentes visites et tente d'en assurer la
résolution par avance, au cas où. Il le fait aussi par avance pour les
premières propositions de la barre d'adresse quand elle est déroulée.

### Mise en cache des requêtes Ajax

Les sites modernes font de plus en plus appel à des requêtes dites
« Ajax ». Il s’agit simplement de requêtes HTTP faites en javascript
pour actualiser des données précises sans avoir à recharger
l’intégralité de la page web et de ses composants.

C’est ainsi qu’est réalisée l’auto-complétion du champs de recherche sur
Google ou Yahoo! par exemple. Au fur et à mesure de la saisie le
navigateur réalise quelques requêtes HTTP pour demander au serveur une
liste de recherches probable pour compléter la saisie de l’utilisateur.

Si certaines de ces requêtes doivent être réactualisées à chaque
instant, d’autres peuvent tout à fait profiter des mécanismes de cache
expliquées précédemment. Ainsi l’auto-complétion du moteur de recherche
peut tout à fait être mise en cache pour plusieurs jours :

Si je commence par taper « conn » le navigateur va établir la liste des
recherches probables. Comme cela prend un peu de temps il est probable
que j’ai tapé une lettre de plus avant d’avoir la liste. Si j’en suis à
« connex » je peux me rendre compte qu’en anglais on dit « connection »
et pas « connexion ». Je reviens sur ma saisie.

N’est-il pas appréciable que le navigateur puisse me reproposer
instantanément la liste de termes qu’il avait téléchargé tout à l’heure
plutôt que de perdre du temps à faire une nouvelle requête ? C’est ce
qu’il se passe si la requête Ajax est mise en cache. Mieux, si je
reviens demain faire la même requête, ou une approchant, la liste sera
toujours en cache, avec une réactivité instantanée.

Les mécanismes techniques pour la mise en cache sont les mêmes que pour
n’importe quel autre composant. Les entêtes de cache peuvent être
ajoutées par le serveur web ou directement par votre langage de script
serveur.

Faites juste attention aux bibliothèques javascript qui activent par
défaut un mécanisme anti-cache (injection d’un paramètre aléatoire et
unique dans l’adresse). C’est toujours une fonctionnalité optionnelle
qu’on peut désactiver, pensez-y.

**Recommandation** : quand cela est possible, pensez à mettre aussi en
cache les requêtes Ajax, et éventuellement à désactiver le mécanisme
anti-cache de votre bibliothèque javascript.

### Mise en cache d’une page d’accueil ou d’une page événementielle à fort trafic

Sur des sites ou des pages à très fort trafic, il peut toutefois être
intéressant de profiter du cache pour les pages HTML elles-même, par
exemple la page d’accueil, les pages des catégories principales ou la
page qui fait l’actualité actuellement.

En leur permettant d’être mises en cache pour quelques minutes ou
quelques heures, on évite que l’utilisateur ait à recharger une page sur
laquelle il est probable qu’il reviendra plusieurs fois. Pour un site à
fort trafic ou un événementiel (imaginez le site d’un quotidien
d’actualité lors d’une manifestation ou d’une catastrophe), le gain sera
notable et appréciable.

Il s’agit de faire alors l’opération inverse de ce qui est préconisé
ordinairement. Au lieu d’externaliser toutes les données et ressources
statiques en laissant la page HTML dynamique hors du cache, nous allons
tenter d’externaliser toutes les composantes dynamiques de la page pour
pouvoir mettre le HTML lui-même dans le cache.

Parmi ces éléments dynamiques, on compte tout ce qui doit absolument être
mis à jour à chaque nouvel accès sur la page. Assez souvent il y a une
boite d’authentification utilisateur, des publicités, éventuellement un
panier sur un site marchand, ou des informations en temps réel sur un
site sportif ou d’actualité. Au lieu d’être inclus directement dans la
page HTML, on va réaliser du code javascript pour charger ces éléments
dans un second temps, par exemple avec Ajax. Attention toutefois à
proposer un lien ou un moyen d'accès alternatif à ses informations sans
utiliser javascript (un lien vers la page panier et vers la page
d'authentification suffisent probablement).

Notre page principale sera donc indépendante de tout élément temps réel
ou spécifique à un utilisateur. Elle pourra être mise en cache pour une
dizaine de minutes à quelques heures, suivant le contexte. Les éventuels
flux d’actualités ou commentaires utilisateurs ne seront pas rafraichis
pendant cette dizaine de minutes mais la fluidité de votre site en sera
améliorée.

**Recommandation** : en cas de besoin, définissez une expiration
explicite courte (d’une dizaine de minutes à une heure) pour les pages à
très fort trafic comme les pages d’accueil ou les pages événementielles.
Si une mise à jour plus fréquente est nécessaire pour certains
composants, il est possible de les charger dans un second temps par
Ajax.

Il s’agit majoritairement d’un compromis. Vous acceptez de charger plus
votre serveur (puisqu’il recevra une requête HTTP supplémentaire à
chaque accès, pour la partie Ajax) mais vous vous assurez que cette
éventuelle surcharge n’impacte pas le contenu principal, qui lui sera
toujours disponible rapidement. Pour peu que la partie statique et la
partie Ajax ne soient pas servies par les mêmes machines, vous pouvez
vous permettre d’être moins réactif sur la partie Ajax afin de
privilégier le service principal (navigation, contenu principal, etc.).

Si cette option n’est pas toujours bénéfique, pour des événements précis
il peut s’agir d’un gain notable en confort. Les proxy bénéficieront eux
aussi de cette architecture puisqu’il pourront servir votre contenu
principal même si votre site est en surcharge.

### Contenu dupliqué sur plusieurs URL

Les navigateurs et les serveurs proxy utilisent un système lié à l’URL
de chaque ressource pour gérer les caches. Si une ressource change
d’URL, alors c’est une nouvelle entrée dans le cache qui est utilisée.
C’est d’ailleurs sur ce principe que nous nous reposons pour versionner
les composants comment nous l’avons vu précédemment.

Cela implique toutefois qu’une ressource ne doit être accessible que par
une seule URL, sous peine de ne pas profiter au mieux du cache. Faites
donc attention à ne pas dupliquer vos URL. En particulier, une page
d’accueil doit être disponible sur « / » ou sur « /index.html », mais
pas les deux. Tous les liens doivent être uniformisés.

**Recommandation** : utilisez une et une seule adresse par ressource.
Faites particulièrement attention à l’uniformisation des adresses pour
les pages d’accueil et pages d’index (index.html, index.php, etc.).

Support HTTP et comportements par défaut
----------------------------------------

### Comportements possibles

HTTP est un protocole très souple. Les navigateurs sont finalement
responsables de ce qu’ils veulent faire. Quoi que vous demandiez, ils
peuvent techniquement ne pas en tenir compte, et leur comportement par
défaut est parfois lié à une heuristique différente suivant les éditeurs
et les versions.

Par défaut, c’est à dire sans précision dans les entêtes `Cache-Control`
ni `Expires`, le navigateur peut :

* ne rien stocker et retélécharger la ressource au prochain accès (cas
  le plus probable si vous n’envoyez ni entête `ETag` ni entête
  `Last-Modified`) ;
* stocker le contenu téléchargé et faire une revalidation avec une
  requête conditionnelle au prochain accès ;
* stocker le contenu et le servir de nouveau sans faire appel au
  serveur tout au long de la session de navigation (c’est à dire
  jusqu’à ce que l’utilisateur ferme son navigateur) ou pour une durée
  arbitraire.

Microsoft Internet Explorer 6 est connu, par exemple, pour employer
parfois cette troisième méthode de façon inadéquate. Il faut alors
forcer un nouveau téléchargement dans le navigateur avec la touche shift
lors de la demande de rafraichissement. Ceci peut être changé dans les
préférences Internet.

Mozilla Firefox utilise aussi cette troisième méthode dès qu'il
rencontre une entête `Last-modified`. Il met alors en cache la
ressource pour 10 % du temps depuis la dernière modification (un
composant mis à jour il y a dix jours sera mis en cache pour une journée
pleine). Pour éviter ce comportement il faut envoyer les paramètres
`no-store` et `no-cache` dans l'entête `Cache-Control`.

Pour Mozilla Firefox on peut constater ce fonctionnement avec Firebug en
dépliant les détails d'une requête et en regardant la section « cache ».

![Vérifier l'utilisation du cache par Mozilla Firefox](img/chap03-verifier-l-utilisation-du-cache-par-mozilla-firefox.png)

Les serveurs proxy, souvent dans les très grosses entreprises, sont
parfois configurés pour avoir eux aussi une politique de cache très
agressive et mettre en cache les pages pour quelques dizaines de minutes
ou quelques heures tant qu’ils n’ont pas d’information contraire
explicite. Ils ont toutefois tendance à ne pas mettre en cache les
contenus avec des paramètres dans l’URL (présence d’un point
d’interrogation dans l’adresse).

### Redirections et autres codes HTTP permanents

Comme nous l’avons vu, une réponse HTTP commence toujours par un code de
retour. Il est très généralement 200 (OK), 304 (le document n’a pas
changé), ou 404 (document inexistant à cette adresse).

Certains de ces codes de retour ont une sémantique qui précise l’aspect
temporaire ou permanent de cette réponse. C’est par exemple le cas des
redirections pour lesquelles on distingue trois codes distincts : 301,
302, et 307. Avec ces codes, le navigateur est informé que la ressource se trouve à une autre adresse.
Le navigateur va donc réaliser une nouvelle requête HTTP.

![Flux HTTP d'une redirection 302](img/chap03-flux-http-d-une-redirection-302.png)

Le code 301 marque une redirection permanente. Les navigateurs et proxy
sont encouragés à mettre en cache cette information. Les prochaines
requêtes réutiliseront ce résultat et ne feront plus d’aller-retour
inutile avec vos serveurs. C’est bénéfique pour vous comme pour vos
visiteurs.

Le code 307 est lui l’opposé, c’est la marque d’une redirection
temporaire. Les navigateurs et proxy sont explicitement informés qu’ils
doivent vérifier la présence de cette redirection à chaque nouvel accès,
quelle que soit leur configuration ou le contexte. Le code 302 est un
code à tout faire, à priori à considérer comme temporaire, donc à ne pas
mettre en cache non plus.

**Recommandation** : quand une redirection entre deux adresses est
permanente, utilisez le code HTTP 301 et non le code 302. Cette
redirection sera alors mise en cache par les visiteurs et les proxys.

D’autres codes HTTP ont une sémantique particulière. Ainsi un client
HTTP ne devrait jamais faire deux fois la même requête s’il obtient une
erreur 400 la première fois (comprenez : le résultat doit être mis en
cache). À l’inverse, le code 303 ne devrait jamais être mis en cache et
une nouvelle demande au serveur doit être réalisée à chaque fois que
nécessaire.

### Le cas particulier des rafraichissements

Les navigateurs ont prévu que l'utilisateur puisse outrepasser le cache.
C'est ce qui est fait quand vous demandez de réactualiser la page
courante. Dans ces cas là, le navigateur fera toujours une requête vers
le réseau pour mettre à jour son cache.

Ce comportement reste vrai pour les réactualisations automatiques.
Ainsi, la balise `<meta http-equiv=refresh>` ou l'entête HTTP Refresh
lancent aussi la nouvelle page sans passer par le cache. Ce peut être
une fonctionnalité intéressante à exploiter mais si vous souhaitez
simplement charger une nouvelle page après quelques secondes (par
exemple pour un interstitiel de publicité) il faut mieux employer un
code javascript qui utilise le document.location. Pour des questions de
compatibilité rien ne vous empêche cependant d'utiliser les deux : le
document.location en premier et la balise `<meta>` qui se déclenche une
seconde plus tard pour les utilisateurs sans javascript.

### Autres comportements par défaut

Dans les cas non couverts jusqu’à présent (aucune entête de cache
explicite, pas de code explicitement temporaire ou permanent), on
distingue alors trois groupes :

Le premier groupe correspond aux requêtes fait avec le verbe GET. Quand
le code de retour est 200, 203, 206, 300 ou 410, alors le navigateur et
le proxy sont encouragés à mettre la réponse en cache, suivant leurs
propres heuristiques. Le plus souvent, s’il y a une entête `ETag` ou
`Last-Modified` et pas de paramètres dans l’URL, alors la ressource risque
d’être mise en cache.

Le second groupe rassemble toutes les requêtes réalisées avec le verbe
POST. Le protocole HTTP indique qu’après un POST le cache associé à
cette adresse doit être expiré (ainsi que ceux des éventuelles adresses
dans les entêtes Location et `Content-Location` de la réponse). Cela
revient plus ou moins à dire qu’une requête de type POST ne doit jamais
être mise en cache.

**Recommandation** : pensez à utiliser des requêtes de types GET pour
envoyer vos formulaires quand le résultat peut être mis en cache (par
exemple pour un moteur de recherche). À l’inverse utilisez une requête 
de type POST quand cela change le contenu du serveur (par exemple 
l’envoi d’un nouveau commentaire) afin d’empêcher le cache. 
L’utilisation de l’entête Location peut aussi expirer le cache de la 
ressource associée (par exemple la page qui contient les commentaires 
en question).

Le troisième groupe regroupe tout ce qui n’est pas couvert par les deux
premiers (c’est à dire une requête de type GET avec un code de retour
autre que ceux listés). Ces ressources ne sont généralement pas mises en
cache, mais peuvent toutefois l’être. Ainsi certains navigateurs mettent
assez facilement en cache les erreurs 404 (page non trouvée). S’il y a
une entête `ETag` ou `Last-Modified`, alors il est probable que le
navigateur tente une revalidation avec le serveur au prochain accès afin
d’éviter un téléchargement inutile.

Dans tous ces trois cas, la présence d’une recommandation explicite via
`Cache-Control` ou `Expires` aura priorité sur le comportement par défaut.
Les navigateurs respectent assez bien ces instructions donc vous êtes
encouragés à proposer des entêtes explicites là où c’est pertinent.

**Recommandation** : précisez explicitement si vous souhaitez ou pas un
cache quand cela est pertinent afin d’aider le navigateur à gérer son
cache.

### Autres paramètres de Cache-Control

Précédemment nous avons vu les directives `max-age`, `s-maxage`, `public`,
`private`, `must-revalidate` et `proxy-revalidate` de l’entête `Cache-Control`.
Ce sont celles qui règlent la durée du cache ou qui imposent une
revalidation explicite.

Il existe toutefois deux autres paramètres :

* `no-cache`, contrairement à ce que laisse entendre son nom, indique
  que le contenu peut bien être mis en cache, mais doit toujours être
  revalidé avec le serveur avant d’être réutilisé ;
* `no-store`, lui, indiqué qu’aucune copie ne doit être stockée dans un
  cache ou dans un espace de stockage, que ce soit par un proxy ou par
  un navigateur.

#### Comportement de certains langages de script ou framework applicatifs

Certains mécanismes de sessions comme celui de PHP ajoutent
automatiquement une entête `Cache-Control` avec une directive `no-cache`.
C’est souvent souhaitable car, s’il y a une session, c’est qu’il y a des
informations privées et spécifique à un utilisateur, mais une directive
private peut parfois être plus adaptée (car elle n’interdit pas le cache
par le navigateur).

De plus, le mécanisme de session est alors souvent initialisé sur toutes
les pages, pas uniquement les pages privées. Dans ce cas, le cache sera
aussi désactivé sur les pages publiques qui n’ont aucune information
spécifique à l’utilisateur.

Si cela arrive, c’est à vous qu’il revient de ne pas initialiser le
mécanisme de session sur les pages ne comportant rien de spécifique, ou
de réécrire l’entête `Cache-Control`. Avec PHP cela se fait soit en
utilisant la fonction `session_cache_limiter()` et
`session_cache_expire()` avant l’appel à `session_start()`, soit en
réécrivant l’entête `Cache-Control` manuellement avec `header()` après
l’appel à `session_start()`.

**Recommandation** : vérifiez l’entête `Cache-Control` de vos pages
privées et de vos pages publiques si vous utilisez un système de session
dans votre applicatif serveur, et corrigez les si nécessaire.


Dans les navigateurs
--------------------

Les réglages de cache sont souvent accusés de tous les maux par 
les développeurs. Beaucoup de sites vous recommandent de modifier 
le cache ou de le désactiver. Vous l'avez peut être fait. Vos visiteurs, 
eux, ne toucheront pas aux réglages. Pensez donc à tout remettre 
aux valeurs par défaut, généralement sur « automatique » quand 
vous ne menez pas de test spécifique. 

Pour faire des analyses cache vide, commencez par vider le cache 
du navigateur, le fermer et le relancer, puis visitez le site 
souhaité. Alternativement vous pouvez recharger une page complète 
en demandant au navigateur de ne pas utiliser le cache. En général 
c'est en appuyant sur la touche « control » (« command » sous Mac) 
et/ou la touche « shift » quand vous lancez la réactualisation 
de la page. 

### Taille des caches

Une fois le site en production c'est le problème inverse qui se 
passe : les navigateurs réservent un espace disque bien trop 
petit pour leur cache. 

#### Navigateurs « de bureau »

Cet espace est de 50 Mo ou moins en général pour les anciens navigateurs, 
seul Chrome montait à un petit 80 Mo. On peut remplir son cache 
en une ou deux sessions de navigation. 

Les navigateurs récents ont fait un effort et Firefox utilise 
désormais une taille variable en fonction de l'espace disque 
disponible, jusqu'à 1 Go. Internet Explorer 9 occupe 1/256ème 
du disque, avec un maximum à 250 Mo. Chrome a désormais lui aussi 
une taille maximum du même ordre de grandeur : 250 à 330 Mo ont été 
reportés suivant les versions et l'espace disque disponible. 

À partir de là, pour ajouter des entrées, il faut effacer les plus 
anciennes. Comme il n'y a pas d'algorithme complexe pour déterminer 
quoi effacer, le cache se renouvelle très rapidement et l'efficacité 
décroit. Seul Internet Explorer 9 affecte pour l'instant des 
priorités aux différents objets dans le cache suivant leur rôle 
et leur usage. 

Si vous contrôlez le parc de votre société, augmenter la taille 
des caches de navigateurs améliorera sensiblement le confort 
de navigation de vos utilisateurs. 

#### Taille des caches des navigateurs mobiles

Sur les navigateurs mobiles la situation est plus complexe. 
Chaque appareil peut avoir sa propre configuration. Dans l'ensemble 
Android 2.x utilise seulement 4 à 8 Mo. Même les Blackberry font 
mieux avec 25 Mo. 

Safari mobile sur iOS 4.3 sait lui utiliser jusqu'à 100 Mo en fonction 
de la mémoire disponible mais il ne s'agit que de mémoire temporaire, 
pas de disque. Ce cache est perdu dès que Safari est déchargé de 
la mémoire. Une partie de ce cache peut aussi être supprimée si 
un autre processus réclame de la mémoire. Les objets effacés 
du cache à ce moment là ne seront pas forcément les plus anciens. 

Les navigateurs sur tablette sont un peu plus souples puisqu'ils 
acceptent de 20 Mo (Xoom sous Android 3.0, iPad 1) à 50 Mo (iPad 
2). Là aussi, les tablettes iOS (iPad) n'utilisent qu'une mémoire 
temporaire qui aura la même durée de vie que la session du navigateur. 

Il est important de noter que toutes ces tailles sont à entendre 
une fois les composants décompressés. On atteint donc très rapidement 
les 4 Mo du cache Android. 

**Recommandation**: pour des clients mobiles pensez que le 
cache se remplira très rapidement et ne sera finalement utilisable 
que pour la session de navigation en cours. 

Certaines anciennes versions iPhones ne pouvaient toutefois 
pas stocker en cache des composants de plus de 25 Ko, pour un total 
de 500 Ko. 

**Recommandation** : pour compatibilité avec les anciens iPhone, 
gardez au maximum des composants (CSS, javascript) de moins 
de 25 Ko une fois décompressés et limitez-vous à une vingtaine 
d'objets. 

### Mozilla Firefox

Dans Mozilla Firefox, les réglages de cache sont dans l'onglet 
« réseau » de la section « avancé » des préférences. Vous y trouverez 
un bouton pour vider le cache (retirer tout ce qui y est stocké) 
et un champ qui permet de définir la taille utilisée par Firefox 
pour cet usage. 

Pour les détails de configuration il faut aller à la page « about:config » 
et chercher les directives començant par « browser.cache ». Toutes ces 
directives sont décrites en anglais sur 
[http://kb.mozillazine.org/About:config_entries](http://kb.mozillazine.org/About:config_entries). 

Le contenu du cache lui-même peut être exploré en allant à la page 
« about:cache ». 

![Configuration du cache Mozilla Firefox](img/chap03-configuration-du-cache-mozilla-firefox.png)

Si vous installez la web developper toolbar de Firefox (et je 
vous recommande de le faire), il est aussi possible de désactiver 
temporairement ou vider le cache avec une simple option dans 
un menu déroulant. 

![Désactivation du cache dans la web developper toolbar de Firefox](img/chap03-desactivation-du-cache-dans-la-web-developper-toolbar-de-firefox.png)

L'extension Firebug possède également un menu pour désactiver le cache, 
dans l'onglet réseau. 

### Opera

Pour Opera, les préférences du cache se situent à l'onglet « avancé » 
dans la section « historique ». Pour désactiver le cache, il suffit 
de choisir l'item « désactiver » dans les listes déroulantes 
pour la taille des caches. 

![Configuration du cache Opera](img/chap03-configuration-du-cache-opera.png)

### Safari

Safari n'offre aucune interface pour modifier les informations 
de cache, mais vous pouvez vider le cache avec une option du menu 
principal. Sous Mac, les objets sont stockés dans ~/Library/Caches/Safari. 
Si vous souhaitez désactiver le cache complètement, une astuce 
peut être de supprimer ce répertoire et de le remplacer par un 
fichier vide. 

Vous avez aussi une option pour désactiver le cache temporairement 
dans les menus de développement si vous les avez activés dans les 
préférences avancées du navigateur. 

Chose rare, Safari a aussi un raccourci clavier pour vider le 
cache : `shift`, `command`, `E`. 

### Microsoft Internet Explorer

Pour Microsoft Internet Explorer, vous pouvez vider le cache 
en cliquant sur le bouton « Effacer... » de la section « historique 
de navigation » dans l'onglet « général » des options Internet. 

Dans les paramètres, assurez vous que la configuration reste 
sur la valeur par défaut, une vérification automatique. C'est 
aussi là que vous pourrez ouvrir le répertoire qui contient tous 
les objets mis en cache. 

![Configuration du cache Microsoft Internet Explorer 7](img/chap03-configuration-du-cache-microsoft-internet-explorer-7.png)

Il est important de noter que sélectionner « à chaque visite de cette 
page web » dans les préférences pour les fichiers temporaires 
ne suffit pas à désactiver complètement le cache. 

### Google Chrome

Il est possible de désactiver totalement le cache en passant 
deux paramètres en ligne de commande lors du lancement du navigateur 
(ou dans l'icône de raccourci) : `--disk-cache-size=1 –-media-cache-size=1`. 
La taille du cache utilisée est visualisable à l'adresse `chrome://net-internals/httpcache.stats`. 

Vider le cache se fait via les options du menu outil (représenté 
par la clef à molette dans le coin droit en haut) puis avec « effacer 
les données de navigation ». 

### Autres navigateurs ou versions plus récentes

Wikipedia maintient pour ses propres besoins une page qui référence 
les moyens de vider ou désactiver les caches des différents navigateurs. 
Les menus de ces logiciels changeant régulièrement, vous y trouverez 
les détails les plus à jour : <http://en.wikipedia.org/wiki/Bypass_your_cache>

À retenir
---------

* Le cache HTTP peut économiser plus de 80% du volume à télécharger 
  pour charger une page web ; 

* Il y aura toujours une proportion non négligeable de visites 
  avec un cache non initialisé, il ne faut pas les oublier ; 

* Désactiver les ETags ; 

* Ajouter une expiration explicite à tous les contenus statiques ; 

* Externalisez les CSS et javascript dans des fichiers séparés 
  du HTML ; 

* Préchargez le cache quand c'est pertinent ; 

* N'ayez qu'une seule adresse par contenu, évitez les duplications. 

Moins de requêtes HTTP
======================

Avec 65 requêtes HTTP en moyenne par page, souvent beaucoup plus, 
et des allers-retours réseaux qui constituent la majeure partie 
du temps de chargement, notre objectif principal reste de supprimer 
des requêtes HTTP. 

Les caches HTTP sont un excellent moyen de réduire ce nombre de 
requêtes HTTP, et donc d'améliorer le chargement de la page. 
Toutefois, comme nous l'avons vu auparavant, la moitié des utilisateurs 
ont de temps en temps une expérience sans cache et 20 % des pages 
sont quand même téléchargées entièrement. 

![Proportion d'utilisateurs ou pages vues avec un cache vide [^1]](img/chap04-proportion-dutilisateurs-ou-pages-vues-avec-un-cache-vide-1.gif)

  [^1]: © 2007- 2009  Reprinted with permission from Yahoo! Inc. YAHOO! and the YAHOO! logo are trademarks of Yahoo! Inc.

Ce chapitre s'attarde donc sur toutes les techniques supplémentaires 
qui nous permettent de diminuer encore le nombre de requêtes 
HTTP. 

Impact d'une requête HTTP
-------------------------

Comme nous l'avons vu dans le chapitre sur les premiers concepts, 
une requête HTTP ne dépend pas uniquement du poids de ce qui est 
téléchargé. En plus du fichier lui-même téléchargé, il faut faire 
un aller-retour réseau entre le navigateur, subir le délai de 
réaction du serveur web et prendre le temps d'envoyer la requête. 

Sur une bonne connexion, l'aller-retour réseau est souvent très 
réduit, quelques dizaines de milli-secondes tout au plus. Pour 
une requête simple sur un site qui fait un usage mesuré des cookies, 
l'envoi de la requête elle-même n'ajoute pas vraiment de délai. 

Sur une connexion un peu dégradée, le délai dû à l'aller-retour 
réseau peut facilement dépasser 100 ms. Quand on a 200 composants 
à télécharger (cas de www.lemonde.fr), multiplier 100 ms par 
200 commence à ne pas devenir négligeable. On obtiendrait près 
de 20 secondes uniquement à cause de ces délais, en plus des téléchargements 
eux-même.

Sur les sites modernes, avec beaucoup de composants, le nombre 
de requêtes HTTP devient un problème en soi. C'est d'autant plus 
frustrant que pendant ce temps là le navigateur ne fait rien, 
il attend. 

![Enchaînement de deux requêtes HTTP sur le réseau](img/chap04-enchainement-de-deux-requetes-http-sur-le-reseau.png)

Pour palier ce problème de latence les navigateurs téléchargent 
plusieurs composants en simultanés. Ainsi sur Microsoft Internet 
Explorer 6 qui a deux files de téléchargements simultanées, 
200 composants avec un aller-retour à 100 ms coûterait 10 
secondes. Sur Firefox 3.5 (6 téléchargements simultanés), on 
tombe à 3,5 secondes mais cela fait encore beaucoup.

Un des palliatifs utilisés est de tricher avec le navigateur 
en utilisant plusieurs domaines. Si un navigateur a deux files 
de téléchargement par domaine et qu'on utilise 3 domaines, cela 
fait six files de téléchargements. Cette technique sera étudiée 
un peu plus loin dans le livre. Pour l'instant notre but est de 
couper au plus haut, éviter de faire les requêtes HTTP quand cela 
est possible. 

Concaténation javascript et css
-------------------------------

Le premier pas à faire, qui offre le meilleur retour sur investissement, 
est d'abord de fusionner des contenus ensemble. 

Toujours pour l'exemple de www.lemonde.fr nous avons 40 fichiers 
javascript individuels dont 27 sur le domaine principal. Sur 
Microsoft Internet Explorer 6 avec une mauvaise connexion avec 
100 ms de latence, cela peut faire 1,5 secondes de perdues inutilement 
rien qu'avec ces 27 requêtes HTTP (une demie seconde avec Firefox 
3.5). 

En fusionnant ces différents fichiers javascript avant de les 
renvoyer au navigateur en un unique gros fichier qu'on téléchargera, on 
évite alors l'impact de 26 requêtes HTTP (il en faudra toujours 
une) et le délai que cela implique pour les mauvaises connexions 
ou les mauvais navigateurs. 

La problématique est similaire pour les feuilles de style. Ici 
nous avons 13 fichiers CSS dont 8 pour le domaine principal. En 
regroupant ces 13 fichiers en un unique gros fichier on économise 
12 requêtes. Chacune de ces requêtes impose au navigateur de 
se tourner les pouces pendant 30 à 100 ms (la latence réseau, le 
temps d'un aller-retour). Considérant que Microsoft Internet 
Explorer 7 utilises deux fils de téléchargement simultanés, 
c'est encore 6 fois 30 à 100 ms (donc de 200 ms à 600 ms au total) qui 
sont gagnés. 

Le gain est d'autant plus important qu'en réalité les fichiers 
javascript posent des problèmes aux navigateurs. C'est aussi 
vrai dans une moindre mesure pour les fichiers CSS. Ces fichiers 
ont tendance à les charger séquentiellement sans utiliser leurs 
possibilités de téléchargements parallèles : pendant ce temps 
rien d'autre ne se passe dans le navigateur. Si cela arrive, 27+12 
requêtes gagnées pour une latence de 100 ms c'est presque 4 secondes 
d'économisées lors du chargement de la page. 

Pour fusionner nos fichiers la procédure la plus évidente est 
encore la plus simple : faites des copier/coller dans un seul 
fichier que vous enregistrez sur le disque. C'est ce fichier 
qui sera à charger par le navigateur. 

**Recommandation** : Regroupez les feuilles de style apparaissant 
sur la même page en un seul fichier. Faites de même avec les codes 
javascript. Si possible, pensez à utiliser les attributs "defer" et "async" sur la balise script, 
si votre code est conçu pour être fonctionnel en asynchrone.
Limitez-vous si possible à un seul composant de 
chaque type sur une page. 

### Garder un développement souple

Avoir un seul fichier de taille importante n'est pas toujours 
idéal pour les développements. Il est appréciable de séparer 
chaque fonctionnalité javascript et chaque style dans un fichier 
distinct, manipulable rapidement. 

La précédente recommandation n'empêche pas un développement 
souple avec des fichiers multiples. Il suffit de séparer les 
environnements de développement et les environnements de production. 
Les premiers font référence aux fichiers individuels, comme 
habituellement. Les seconds font référence au fichier résultant, 
après fusion. 

La mise en œuvre manuelle de ce système ne prend pas un temps réellement 
important. Il suffit à chaque mise en production de réaliser 
un petit nombre de copier/coller et de changer les déclarations 
de style et de javascript dans les entêtes des gabarits HTML. 

Avec une petite aide, par exemple quelques lignes de PHP, l'application 
saura basculer toute seule entre le chargement de multiples 
fichiers et le chargement d'un fichier fusionné unique suivant 
qu'elle est en développement ou en production. Il suffira de 
penser à mettre à jour le fichier de fusion à chaque livraison 
en recette ou en production. Si cela apparaît contraignant et 
dangereux (puisque manuel), sachez que de nombreuses très grosses 
entreprises web fonctionnent ainsi. Parfois l'automatisation 
de tâches élémentaires et simples ne vaut pas le coût face aux 
contraintes que cela apporterait. 

#### Automatisation

Toutefois, mettre en place une procédure entièrement automatisée 
ne demande pas plus de quelques dizaines de lignes de script. 
Il faut alors référencer les fichiers javascript et les fichiers 
CSS dans un fichier de configuration. En développement, l'application 
relit dynamiquement cette configuration pour charger les composants 
un à un dans leur dernière version. 

![Mise en place d'une structure souple pour la fusion des fichiers css et javascript](img/chap04-mise-en-place-dune-structure-souple-pour-la-fusion-des-fichiers-css-et-javascript.png)

Une procédure automatisée relit cette même configuration pour 
recréer le fichier fusionné à partir des sources, par exemple 
lors de la livraison en environnement de test (mais ce peut être 
plus fréquent). Les environnements de test, recette et production 
n'ont plus qu'à faire référence à ce dernier. En général on profite 
de cette étape pour réaliser d'autres optimisations en même 
temps que la fusion, par exemple une minification des contenus 
(nous en parlerons plus loin dans ce livre). 

De nombreux composants PHP proposent d'automatiser ainsi le 
mécanisme de fusion (par exemple le plugin sfConbine pour le 
framework PHP Symfony : http://www.symfony-project.org/plugins/sfCombinePlugin). 

Une alternative intéressante est d'utiliser un module directement 
embarqué dans le serveur web. Ce dernier, qui ne prendra alors 
quasiment pas de ressources supplémentaires, se charge de faire 
la fusion dynamiquement, à la demande. On utilise une adresse 
web spécifique, à convenir, qui contient le nom de tous les fichiers. 
Le serveur web reconnaît cette adresse spécifique et s'occupe 
de fusionner à la demande tous les fichiers nécessaires avant 
de renvoyer le résultat, en faisant attention aux caches HTTP 
et aux dates de dernière modification des contenus. 

![Yahoo! concatène ses fichiers à la demande et les appelle par groupes](img/chap04-yahoo-concatene-ses-fichiers-a-la-demande-et-les-appelle-par-groupes.png)

Le mod_concat est un exemple d'implémentation de ce mécanisme 
pour le serveur web Apache. Une page avec une adresse qui finit 
par un double point d'interrogation et une liste de fichiers 
séparés par des virgules chargera en une seule requête le contenu 
des différents fichiers en renvoyant la date de modification 
du fichier le plus récent. Un module similaire existe sur lighttpd. 

Yahoo! utilise lui aussi un tel mécanisme qu'ils appellent « 
combo handler » pour ses serveurs de ressources statiques. L'adresse 
suivante permet ainsi de charger en une fois le code de base de 
la bibliothèque javascript YUI et le module de gestion des cookies : 
<http://yui.yahooapis.com/combo?3.3.0/build/yui/yui-min.js&3.3.0/build/cookie/cookie-min.js>

* Pour Apache 2 : <http://code.google.com/p/modconcat/>

* Pour Lighttpd : <http://code.google.com/p/lighttpd-mod-concat/> 

* Système Yahoo! : <http://www.stevesouders.com/blog/2008/07/17/yuis-combo-handler-cdn-service/> et <http://yuiblog.com/blog/2008/10/17/loading-yui/> 

### Fichier unique ou spécifique à la page

Lors de la fusion des fichiers en un seul, il faut réfléchir si 
l'on créé un fichier spécifique à la page en cours, ou si l'on créée 
un fichier central unique à toute l'application. Il s'agit finalement 
d'un compromis entre le temps de chargement de la première page 
et la réactivité des pages suivantes : 

Avec un gros fichier unique le visiteur télécharge forcément 
une partie non négligeable de code inutile, ralentissant le 
chargement de la première page. En échange ce fichier pourra 
être stocké dans le cache HTTP une fois pour toutes, les pages 
suivantes n'auront plus besoin d'aucun téléchargement sur 
le réseau. 

À l'inverse, avec des fichiers spécifiques à chaque page le visiteur 
devra initialiser un nouveau téléchargement à chaque nouveau 
modèle de page, sans pouvoir exploiter son cache HTTP même si 
les fichiers téléchargés contiennent des parties communes. 
Le premier accès ne sera pas inutilement lent mais chaque page 
subira un ralentissement. 

Il vous faut étudier quel est le pourcentage de réutilisation 
de vos composants sur les différentes pages de votre site. Si 
la réutilisation est forte et que vous pouvez vous le permettre, 
avoir un fichier central unique pour javascript et un fichier 
central unique pour la feuille de style vous permettra d'avoir 
un site très réactif. C'est le mode que vous devriez privilégier 
dans un premier temps. 

Dans une seconde étape, il est possible de spécialiser les pages 
qui diffèrent vraiment du reste du site. Il s'agit essentiellement 
des quelques pages très riches en animation ou qui ont besoin 
de composants additionnels très lourds. On peut par exemple 
citer les pages qui contiennent des éditeurs riches en ligne 
à base de javascript. Ces pages là peuvent se permettre de ne pas 
utiliser le fichier central mais leur version, spécifique. 

Éventuellement, s'il est possible d'individualiser quelques 
modèles de pages bien spécifiques et suffisamment distincts, 
qu'il couterait trop cher de regrouper avec un fichier central, 
il est possible de réaliser un fichier par modèle de page. Ce peut 
être par exemple un fichier pour la section boutique, un pour 
la section jeux, un pour les pages de contenu, un pour les pages 
de section, etc.

Il faut toutefois garder à l'esprit que cette dernière subdivision 
diminue l'efficacité du cache puisque plus rien n'est partagé 
entre deux pages de modèle différent. Gardez à l'esprit qu'éviter 
une requête HTTP vaut généralement mieux qu'un petit renflement 
du fichier central. Tout est histoire de compromis et c'est à 
vous de tracer la ligne jaune. 

* Page classique de contenu : site.css 

* Page d'accueil : accueil.css 

* Pages boutique : boutique.css 

* Pages jeux : jeux.css 

### Quand faire plusieurs fichiers

L'étape suivante est de dégager les pages d'arrivée des visiteurs. 
Il s'agit par exemple de la page d'accueil, des pages dont l'adresse 
est dictée à la radio, à la télévision, sur des plaquettes ou publicités, 
etc. Ces pages doivent être les plus optimisées et sont souvent 
très spécifiques. Sur ces pages, et sur ces pages uniquement, 
on s'autorise à ne pas charger le fichier central mais à charger 
uniquement un fichier spécifique à la page. Le système de cache 
HTTP se mettra en œuvre uniquement si le visiteur poursuit plus 
avant dans le site. 

Enfin, si cela devient nécessaire, il est possible de diviser 
les pages du site par modèles ou par section. Il s'agit d'optimiser 
le taux de réutilisation quand celui du fichier central est mauvais. 
On construit alors un fichier central qui contient ce qui est 
commun à l'essentiel des pages et on construit quelques fichiers 
additionnels suivant les types de pages. Ainsi une page chargera 
au plus deux feuilles de style : la feuille de style centrale avec 
ce qui est utilisable partout, et une feuille spécifique à son 
modèle (par exemple la feuille de style propre à la section « boutique 
en ligne »). 

Le calibrage de cette dernière étape est délicat, il faut éviter 
de créer trop de composants différents, et s'assurer que le fichier 
central ne devient pas trop pauvre. Rappelez-vous que la multiplication 
des requêtes HTTP coute bien plus cher qu'un petit ajout dans 
le fichier central, tout est question de compromis. Si vous avez 
des doutes, rien ne vaut un petit test en production sur des échantillons 
d'utilisateurs cibles. 

* Page classique de contenu : central.css 

* Page d'accueil : accueil.css 

* Pages boutique : central.css + boutique.css 

* Pages jeux : central.css + jeux.css 

* Page référencée dans une publicité : accueil-pub.css (si 
  central.css ne convient pas) 

* Page de configuration avec des composants évolués : central.css 
  + config.css + … 

En dehors des pages spécifiques comme celles des éditeurs riches 
en javascript, il faut considérer un maximum de deux feuilles 
de style par page, en visant au maximum la possibilité de ne charger 
que le fichier central si possible. Le plus souvent, pour les 
feuilles de style, il est envisageable d'avoir un seul fichier 
central pour quasiment toutes les pages du site, sans fichier 
secondaire. 

#### Spécificités des fichiers javascript

Si ce qui a été dit reste tout à fait valable pour les fichiers javascript, 
la problématique est un peu différente. Comme les fichiers javascript 
sont parfois lourds à télécharger, l'équilibre entre un cache 
efficace, un téléchargement initial léger et le nombre de requêtes 
HTTP sera différent de celui des feuilles de style. On tendra 
à accepter plus facilement d'avoir deux voire trois fichiers 
par page (mais pas plus, et uniquement si on en a vraiment besoin). 

Il est en effet plus fréquent d'avoir besoin d'un composant spécifique 
qui sera un peu gros pour une inclusion inconditionnelle dans 
toutes les pages. Suivant les besoins on peut même aller jusqu'à 
trois fichiers javascript : le fichier central du site, le fichier 
qui contient les composants propres à la section (boutique, 
actus, etc.), et le fichier qui contient les composants spécifique 
à la page ou au modèle de page. 

Il ne faut cependant pas oublier que cette multiplication a un 
coût sérieux et qu'avoir un seul fichier par page doit rester 
un objectif à tout moment. Tout ce qui a été dit au sujet de la fusion 
des feuilles de style reste valable pour les fichiers javascript. 

Enfin, n'hésitez pas à créer un dernier fichier javascript si 
celui-ci est chargé en fin de page HTML, juste avant la fermeture 
de la balise `<body>`. Ce code ne sera en effet chargé qu'à la fin 
de la page, après tout le contenu. L'effet négatif étant plus 
faible, tout ce qui est ainsi posé à la fin a bien moins d'influence. 
Vous verrez plus loin dans ce livre qu'insérer ainsi un fichier 
javascript en fin de page est de toutes façons une recommandation 
importante. On y transférera le maximum de code possible, diminuant 
ainsi les fichiers javascript chargés en haut de page dont nous 
parlions plus tôt. 

![Répartition et séparation des fichiers javascript dans la page](img/chap04-repartition-et-separation-des-fichiers-javascript-dans-la-page.png)

Images en sprites CSS
---------------------

Une fois les feuilles de styles et fichiers javascript concaténés 
il reste un composant principal qui occupe souvent plus de la 
moitié des requêtes HTTP : les images. Contrairement à ce que 
nous dit l'intuition, fusionner plusieurs images en un seul 
fichier est relativement courant. C'était même une pratique 
quasi systématique sur les jeux et autres applications locales 
quand nos micro-ordinateurs avaient moins de puissance et d'espace 
disque. 

Sur ces anciens jeux on parlait de sprites. Il s'agit d'un unique 
fichier image qui contient plusieurs icônes, ou plusieurs représentation 
d'un même composant (par exemple les vues sous différents angles 
d'un même objet). On trace une sorte de quadrillage virtuel dans 
une grande image et chaque case contient une icône, une vue de 
l'objet, ou une image quelconque. Pour afficher la bonne image 
il suffit d'en connaître ses coordonnées. 

![Exemple de quadrillage d'images](img/chap04-exemple-de-quadrillage-dimages.png)

Le web est un peu comme une ancienne machine où l'accès au disque 
est très lent. Chaque appel fichier prend un temps non négligeable 
et ralentit toute l'application. Les solutions mises en œuvre 
sont donc logiquement les mêmes. 

Sur le web vous trouverez de nombreux articles sur les « sprites 
CSS ». Il ne s'agit que de cela : combiner plusieurs images référencées 
par votre feuille de style en un seul fichier. 

### Gains de performance

Combiner dix images en un seul fichier (un « sprite ») permet d'éviter 
neuf requêtes HTTP. En considérant une latence de 50 ms c'est 
quasiment une demi-seconde qui est ainsi économisée. 

Le volume de données téléchargé ne change pas, ou pas beaucoup. 
L'image fusionnée a tendance à être de poids équivalent ou légèrement 
plus léger que la somme des dix images originales. Plus les images 
de départ sont petites et similaires et plus leur fusion a tendance 
à économiser aussi sur le poids à télécharger. 

Ajouté aux gains dus à la latence ou au poids des fichiers, le visiteur 
a aussi un meilleur ressenti du chargement de la page du fait qu'il 
ne voit pas toutes les images se charger une à une. Si les icônes 
similaires et fonctionnellement proches sont regroupées dans 
un même sprite, elles s'afficheront simultanément sur la page. 

![Comparaison du trafic réseau avec et sans sprite](img/chap04-comparaison-du-trafic-reseau-avec-et-sans-sprite.png)

Le contre-coup est le risque de faire télécharger dans le sprite 
des images qui ne sont pas utilisées par la page courante. Dans 
ce cas, l'augmentation de volume impactera négativement la performance. 
Toutefois, par expérience, si le regroupement est fait de manière 
cohérente le gain en nombre de requêtes HTTP compense largement 
l'augmentation du poids total. Même en regroupant un fichier 
avec la centaine d'icônes du site et que seules une vingtaine 
sont utilisées par page, il est probable que le résultat soit 
positif. 

Le sprite peut aussi être extrêmement intéressant pour les autres pages 
du site puisque le sprite sera alors en cache dans le navigateur 
et que les futures pages ne nécessiteront aucun téléchargement. 

**Recommandation** : Regroupez vos images de décoration liée 
par la feuille de style en quelques grosses images (sprites) 
et utilisez les règles CSS pour afficher uniquement la partie 
que vous souhaitez. 

### Mise en œuvre

La mise en œuvre du sprite lui-même est assez simple. Il suffit 
de copier les différentes images dans une plus grande et de les 
positionner sur une grille. À peu près tous les éditeurs d'images 
permettront de faire cette opération. La seule difficulté est 
de faire attention à respecter la grille pour faciliter ensuite 
les saisies dans la feuille de style. 

L'affichage d'une image spécifique se fait un peu comme la vision 
d'une photo au travers d'un masque. Au travers du masque on ne 
voit qu'une partie restreinte de la photo. Pour visualiser un 
autre détail on peut positionner la photo sous le masque afin 
que la partie souhaitée soit visible. 

#### Position du sprite

Le procédé est exactement le même avec CSS. Imaginons que nous 
manipulions un cadre de 10 pixels sur 10 pixels (la taille d'une 
icône), nous allons positionner l'image de fond dans la feuille 
de style à l'aide de l'instruction `background-position`. 
Il faudra le plus souvent faire correspondre le cadre avec une 
partie au milieu de l'image, et donc positionner le coin en haut 
à droite de l'image plusieurs pixels avant le coin en haut à droite 
du cadre visible dans la page. La conséquence est que le plus souvent 
les mesures sont négatives : 

Nous pourrions donc avoir un code CSS similaire à : 

~~~~~~~ {.css}
div#boite {
    background-image: url(sprite.png) ;
    background-repeat: no-repeat ;
    background-position: -20px -20px ;
    width: 10px ;
    height: 10px ;
}
~~~~~~~

![Exemple de quadrillage d'images](img/chap04-exemple-de-quadrillage-dimages.png)

#### Marges

Les choses sont assez simples dans l'exemple précédent parce 
que le cadre qui contient l'image de fond a la même taille que l'image 
à y positionner. Si le cadre est plus grand, en largeur ou en hauteur, 
il faudra placer des marges vides autour de l'icône à afficher 
pour que nous n'affichions pas aussi les images à droites et en 
bas de celle que nous avons choisi. 

Si la taille est inconnue en largeur, il faudra que l'icône à positionner 
soit dernière, première, seule sur sa ligne (suivant si l'icône 
à positionner doit être affichée à gauche, à droite, ou au milieu), 
ou du moins avec une marge suffisamment grande. La problématique 
est similaire en hauteur avec les colonnes. 

Il vous faut aussi penser à prévoir une marge suffisante pour 
les différents cas : Par exemple, si la taille de l'écran est moins grande
que chez vous votre cadre s'affichera peut être plus étroit, 
et donc plus long que chez vous. De même, si l'utilisateur a agrandi 
ses polices de caractères le texte prendra plus de place, en largeur 
ou en hauteur. Votre marge doit prendre en compte ces possibilités 
pour que jamais votre sprite n'affiche plus d'images qu'il n'est 
prévu. 

![Marges dans le quadrillage du sprite](img/chap04-marges-dans-le-quadrillage-du-sprite.png)

#### Répétition des images

La problématique des marges se fait d'autant plus visible quand 
une image doit être répétée horizontalement (en x) ou verticalement 
(en y). Dans ce cas il faut que le morceau d'image à afficher soit 
seule sur sa ligne (ou sur sa colonne) sans marge sur cette ligne. 

À cause de cette problématique, et parce que ces images sont généralement 
très étroites (ou très petites en hauteur), on sépare souvent 
ces images dans des sprites dédiés : un pour les images qui se répètent 
en hauteur, et un pour les images qui se répètent en largeur. Les 
images qui doivent se répéter sur les deux axes dans des dimensions 
inconnues ne sont pas intégrables dans des sprites. 

### Limitations

#### Marges et répétitions

Ces questions ont été traitées plus avant lors de la mise en œuvre. 

#### Volume et taille du sprite

Opera ne sait pas utiliser des positions de plus de 2042 pixels 
(négatifs ou positifs). Vous rencontrerez de toutes façons 
rarement cette limitation. Le regroupement en sprites doit 
être fait avec mesure. Si vous cumulez trop d'images dans vos 
sprites vous risquez d'avoir trop d'images inutiles pour la 
page en cours et d'augmenter inutilement le volume téléchargé 
par le navigateur. 

#### Taille mémoire

Nous avons dit que dans un sprite le poids de téléchargement des 
icônes inutiles est largement compensé par la réduction des 
requêtes HTTP. Toutefois les portions inutilisées du sprite 
ont un second effet : Pour afficher une image le navigateur la 
décompresse en mémoire. Là, un pixel utilise 4 octets, la compression 
jpeg ou png n'entre pas en compte. 

Pour un navigateur classique, tant qu'on n'exagère pas, cela 
n'a presque aucune conséquence. On évitera juste d'utiliser 
des sprites avec trop d'espace blanc inutile, ou plus de surface 
inutile que de surface utile. 

![Exemple de sprite à réserver à des usages spécifiques : organisation en diagonale](img/chap04-exemple-de-sprite-a-reserver-a-des-usages-specifiques--organisation-en-diagonale.png)

Pour les dispositifs mobiles (téléphones, tablettes) où la 
mémoire est très limitée, les parties inutilisées du sprite
peuvent, par contre, gêner la navigation. Sur ces plateformes il 
faudra faire attention à charger des petits sprites avec peu 
d'espace blanc et aucune image inutile. 

#### Images avec peu de couleurs

Ensuite vient une limitation liée aux couleurs. Si vous choisissez 
un format d'image limité en nombre de couleurs (GIF ou PNG8) vous 
risquez de dégrader la qualité globale de vos images (surtout 
les dégradés). En effet, là où chaque image avait sa propre liste 
de couleurs adaptée, dans un sprite toutes les images partageront 
le même index de couleurs. Si vous mettez ensemble une image qui 
avait 255 variantes de bleu avec une image qui avait 255 variantes 
de rouge, l'image résultante aura moitié moins de détails. Pour 
cela faites attention à ne mettre dans le même sprite que des images 
proches au niveau couleurs, ou passez l'image en PNG24. L'augmentation 
de volume sera normalement faible, et souvent compensée par 
le gain de poids du sprite. 

#### Accessibilité

On distingue généralement deux types d'usages pour les images 
sur une page web : les images de contenu et les images de décoration. 

Les images de contenu sont celles qui font partie intégrante 
de l'information transmise par la page. Leur disparition ferait 
perdre une donnée utile pour le visiteur et pas uniquement un 
enrichissement graphique ou une meilleure ergonomie. Parmi 
les images de contenu on retrouve entre autres les photos d'un 
article de presse, les diagrammes explicatifs, et les logos. 

Les images de décoration, elles, ne sont là que pour entourer 
le contenu. Elles font partie du graphisme de la page. Certaines 
sont là pour rendre le site agréable comme les différents fonds 
(de la page, du menu, etc.). D'autres sont indispensable à l'ergonomie 
ou à la compréhension du site comme les icônes, les traits de séparation, 
etc. 

En général on peut séparer les deux catégories en se posant les 
questions suivantes : L'image fait-elle perdre une information 
si je la retire ? Devrais-je en mettre une alternative texte si 
je retire l'aspect graphique du site ? L'image resterait-elle 
la même si je change totalement le graphisme du site ? 

Une image de contenu obtiendrait un « oui » à ces questions, une 
image de décoration aurait un « non ». Les premières devraient 
normalement être insérées via la balise HTML `<img>` et les secondes 
via un `background-image` en CSS. 

Les sprites sont réservés aux images de présentation. Un navigateur 
qui ne supporte pas CSS ou un robot qui se contente de lire le contenu 
n'aura aucun problème. Ces images sont là uniquement pour l'aspect 
graphique : elles ne manqueront pas si elles sont omises et elles 
n'ont aucune valeur prises hors contexte. 

À l'inverse, une image de contenu ne devrait pas être insérée 
via CSS et encore moins être mise en sprite. Omise (par exemple 
par un support partiel de CSS sur le navigateur) elle provoquera 
un manque d'information, et pris hors contexte le sprite n'a 
plus aucun sens (on ne sait pas quelle partie de l'image se réfère 
à quoi). 

### Avenir et nouvelles fonctionnalités des navigateurs

Les dernières avancées CSS des navigateurs laissent espérer 
qu'on pourra faire sauter rapidement les problèmes liés aux 
marges et aux répétitions. 

Le moteur Gecko de Mozilla (qui équipe entre autres le navigateur 
Firefox) contient déjà une valeur `-moz-image-rect` qui permet 
de déterminer quelle est la partie de l'image qui sera affichée 
: plus besoin de marges et plus aucun risque d'afficher plus que 
prévu en cas de répétition. 

Ainsi l'instruction suivante permet, avec Gecko, de n'afficher 
que l'icône de dix pixels aux coordonnées 50,60 par rapport au 
coin en haut à gauche (on donne l'adresse du sprite, puis les coins 
en haut à gauche et en bas à droite) : 

~~~~~~~ {.css .online}
background-image:-moz-image-rect('sprite.png',50px,60px,60px,70px); 
~~~~~~~

Le moteur webkit (sur lequel est basé Safari) contient lui un 
`-webkit-mask-image` qui, bien qu'il ne soit pas là pour ça, 
pourrait être détourné pour faire sauter les limitations de 
marge (mais pas celles de répétition). 

Le groupe de travail CSS 3 au W3C (l'organisme qui s'occupe de 
la standardisation du langage) réfléchit aussi aux futures 
syntaxes pour gérer efficacement les sprites. Pour l'instant 
plusieurs syntaxes ont été évoquées (`@sprite`, `sprite()`, 
`image-slice()`, etc.) mais rien n'est vraiment défini. 

Le mécanisme des sprites devenant de plus en plus répandu on peut 
toutefois imaginer que les navigateurs se mettent rapidement 
d'accord pour supporter une syntaxe similaire. D'ici un an au 
plus, si vous pouvez vous passer des anciens navigateurs, il 
est probable que les limitations de marge et de répétition ne 
soient plus qu'un lointain souvenir. 

### Les outils automatiques

De nombreux outils automatiques existent pour créer des sprites. 
La première catégorie regroupe tous ceux qui analysent une page 
web et construisent d'eux même les fichiers images en vous proposant 
les modifications de CSS à réaliser. Ces outils posent généralement 
plus de problèmes qu'ils n'en résolvent parce qu'ils ne savent 
pas gérer les limitations de marge. Ils ne savent pas non plus 
regrouper correctement les images fonctionnellement proches 
ou qui se retrouvent sur des pages différentes (mais qui ont un 
intérêt à partager le même sprite). Vous pouvez par exemple essayer 
[Sprite Me](http://spriteme.org/), 
et [Spritemapper](http://yostudios.github.com/Spritemapper/) 
pour cet usage. 

Une seconde catégorie d'outils est un peu plus évoluée. Elle 
se base sur vos propres préconisations via un pseudo-langage 
propriétaire dans la feuille de style. En face d'une image vous 
pouvez ajouter un commentaire puis donner le nom d'un sprite 
et éventuellement des options (sprite vertical, horizontal, 
marges, etc.). En développement la feuille de style est directement 
utilisable, sans sprites. Lors de la mise en recette l'outil 
analyse la feuille de style, créé les sprites suivant vos propres 
regroupements et vos options, puis modifie la feuille de style. 

Cela demande une discipline pour le développeur qui doit penser 
à ajouter du code en commentaire dans sa feuille de style, mais 
offre assez de souplesse. Du fait des options les problèmes de 
marges et de répétitions sont limités, ou au moins ils sont de 
la faute du développeur (et pas celle de l'outil). L'outil le 
plus connu à ce niveau est [Smart Sprites](https://github.com/carrotsearch/smartsprites). 

Enfin, vous trouverez aussi des scripts ou des pages web qui vous 
proposeront de construire un sprite et une liste de règles CSS 
à partir des options et de la liste d'images que vous leur donnerez. 
C'est ici à vous de faire l'intégration, seul le travail frustrant 
est automatisé (constitution de l'image et écriture des lignes 
CSS pour chaque image). On peut citer le [CSS sprite generator](http://spritegen.website-performance.org/) dans cette catégorie. 

![CSS sprite generator](img/chap04-css-sprite-generator.png)

Si la première catégorie d'outils est à éviter, tous ces outils 
partagent en plus un défaut : en créant automatiquement l'image 
de sprite vous n'aurez plus la main sur les réglages fins de l'image 
(gestion de la transparence, indexation intelligente des couleurs, 
etc.). Si vous souhaitez des images avec un poids le plus faible 
possible et une qualité sans défaut, seul un graphiste pourra 
vous produire l'image, à la main. Ces outils ne pourront vous 
proposer que ces images en pleines couleurs ou des images avec 
un choix de couleur peu performant. 

Si vous choisissez de faire le travail majoritairement à la main, 
l'outil [Sprite Cow](http://www.spritecow.com/) 
peut vous faciliter énormément le travail de détourage et de 
sélection des coordonnées à l'intérieur d'un sprite. Il vous 
permet de cliquer sur une icône d'un sprite et trouvera tout seul 
les contours de l'icône et les coordonnées CSS associées. 

![Sprite Cow](img/chap04-sprite-cow.png)

Données en ligne et téléchargements par lot
-------------------------------------------

Nous avons vu comment regrouper plusieurs composants externes 
en un seul, que nous parlions de feuille de style, javascript 
ou d'image. Si vous voulez aller plus loin il faudrait embarquer 
ces composants directement dans la page web, ou du moins les fusionner 
entre eux. 

### Externalisation des contenus

Quand nous parlions de cache dans le chapitre précédent j'avais 
posé comme recommandation d'insérer tous les codes css et javascript 
dans des fichiers externes pour qu'ils puissent utiliser le 
cache. Nous allons désormais modérer cette recommandation. 

Pour les pages d'accueil et pour les pages d'entrées du visiteur 
(par exemple la page nommée dans une publicité ou une plaquette),
la problématique est parfois un peu particulière. Ce sont les 
pages qui vont faire que le visiteur approfondira ses recherches 
ou pas, la vitesse est critique. Quelques dixièmes de secondes 
peuvent faire basculer votre taux de transformation. De plus, 
sur ces pages, le cache n'est pas le critère le plus important. 
Les internautes venant par définition d'un site tiers, et n'étant 
souvent pas des habitués, vos ressources ne seront pas encore 
en cache. 

Dans ce cas, le surcoût dû aux requêtes HTTP en plus n'est pas forcément 
compensé par le gain dû aux caches. On s'autorise alors à mettre 
le plus de contenus possible dans la page web elle-même, et à insérer 
directement les codes javascript et CSS au lieu de faire des fichiers 
externes. 

**Recommandation** : Pour les pages cibles (page d'accueil, 
publicité, plaquette) attirant essentiellement des nouveaux 
utilisateurs et suffisamment différentes du reste du site, 
envisagez d'intégrer directement les codes javascript et CSS 
dans le corps de la page HTML au lieu de fichiers externes. 

Ceci est une exception aux recommandations générales qui proposent 
l'externalisation des contenus pour profiter des caches HTTP. 

La question se pose aussi pour les images et les autres types de 
contenu. En effet, toute personne qui télécharge une image de 
fond déclarée en CSS devra avoir charger la feuille de style auparavant. 
Inversement, il existe une série d'images qui seront quasiment 
toujours chargés par les visiteurs qui chargeront la feuille 
de style. Ces images là pourraient avoir un avantage à être intégrées 
directement dans la feuille de style et à être téléchargées en 
un lot unique, ensemble. C'est ce qui est discuté ci-après. 

### Les méthodes de téléchargement par lots

Pouvoir intégrer les fichiers javascript et les feuilles de 
style directement à la page HTML est intéressant mais pas suffisant. 
Il peut être intéressant par exemple de regrouper différents 
types de contenus, ou de les proposer au téléchargement en file 
sans subir la latence entre chaque requête. C'est ce que proposent 
les trois solutions ci-dessous. 

* Insertion d'une contenu directement dans un lien via le protocole 
  « data: » 

* Insertion d'un contenu groupé avec un autre via le format mhtml 

* Insertion d'un contenu groupé avec un autre via une archive 
  jar 

Toutes sont limitées à un ou plusieurs navigateurs. Il faudra 
composer avec les différences et proposer des alternatives 
suivant le navigateur, ce qui vous imposera plus de travail. 
En général on propose une solution basé sur les deux premières 
méthodes à l'aide de commentaires HTML conditionnels (ils ne 
sont relus que par Microsoft Internet Explorer et permettent 
de gérer une version spécifique à ce navigateur). 

Toutes ont toutefois un avantage concret : Elles permettent 
de regrouper plusieurs fichiers de type différent tout en permettant 
de les télécharger en un seul lot et de mettre ce lot en cache. 

### Les liens en data:

Les liens en « data: » viennent de la RFC 2397 de 1998. Il s'agit 
tout simplement d'embarquer le contenu directement dans un 
lien plutôt que d'avoir son adresse sur le réseau. 

Au lieu d'initier un téléchargement, le navigateur lit les données 
embarquées dans le lien comme s'il s'agissait d'un fichier. 
Le lien peut représenter une image, un fichier texte, une feuille 
de style, etc. 

#### Fonctionnement

Le prototype de ces liens est le suivant : 

~~~~~~~ {.online}
data:[<mediatype>][;base64],<data> 
~~~~~~~

La première partie est fixe, il s'agit du protocole utilisé, 
`data:`. Vient ensuite le type du contenu, par exemple `image/png` 
pour une image ou `text/css` pour une feuille de style. Ce type 
de contenu peut embarquer un paramètre spécifiant codage caractère 
dans le type mime, par exemple `text/html;charset=utf-8`. 
Enfin on insère la donnée binaire ou texte elle-même, après une 
virgule. Par défaut, ce contenu est codé comme un lien, avec %xx où xx représente 
la valeur de l'octet à insérer (%20 pour un espace par exemple). 
Si le type mime contient le paramètre `base64` (les paramètres 
sont séparés par des points virgules) alors on utilise le codage 
base64. 

Exemple avec une image insérée dans le HTML : 

~~~~~~~ {.html .partial}
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUh
EugAAAAoAAAAKCAYAAACNMs+9AAAABGdBTUEAALGPC/xhBQAAAA
lwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9YGARc5KB0XV+IAA
AaddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72Ql
bgAAAF1JREFUGNO9zL0NglAAxPEfdLTs4BZM4DIO4C7OwQg2JoQ
9LE1exdlYvBBeZ7jqch9//q1uH4TLzw4d6+ErXMMcXuHWxId3KO
EtnnXXV6MJpcq2MLaI97CER3N0vr4MkhoXe0rZigAAAABJRU5Er
kJggg==" alt="Puce rouge" />
~~~~~~~

#### Utilisation

Cette fonctionnalité peut avoir un intérêt quand on souhaite 
avoir un seul fichier, sans composant externe (par exemple la 
diffusion d'un CV au format HTML). Nous l'utilisons ici pour 
son effet de bord : aucune requête HTTP n'est alors générée sur 
le réseau, l'image et son fichier source sont envoyées groupés. 

Si dans le cas du fichier HTML l'effet est peu utilisé, c'est très 
intéressant dans le cadre des feuilles de style. On sait que celui 
qui chargera la feuille de style chargera systématiquement 
juste après une série d'images de fond utilisées sur quasiment 
toutes les pages du site. Ces images ne sont jamais utilisées 
en dehors de la feuille de style. Si l'un est en cache, l'autre 
le sera aussi. Pourquoi les charger séparément avec plusieurs 
requêtes HTTP ? 

On embarque alors certaines séries d'images de fond (puces, 
dégradés, icônes) directement dans la feuille de style en utilisant 
ces liens en « data: » plutôt qu'en référençant de vrais fichiers 
sur le réseau. Si le poids de la feuille de style va augmenter fortement 
on aura toutefois fait moins de requêtes, et chargé plus rapidement 
la page. Le pari est que l'augmentation en volume total (30 % de 
plus pour les images intégrées ainsi) est compensé par le gain 
la diminution du nombre de requêtes. Pour des petites images 
(moins de 3 ko) c'est quasiment toujours le cas. 

S'il est possible de générer ces liens soit même, ils sont généralement 
créés par un scripts, par exemple par la fonction `base64_encode()` 
de PHP : 

~~~~~~~ {.php}
<?php
function lien_data($type, $fichier) {
   $data = file_gets_content( $fichier ) ;
   $base64 = base64_encode($data) ;
   return "data:$type;base64,$base64" ;
}
~~~~~~~

Google embarque aussi quelques images de contenu directement 
dans la page HTML : les vignettes de fichiers vidéos dans les résultats 
de recherche. Il s'agit d'images qui n'ont de sens que pour la 
requête courante (donc qui ne peuvent profiter du cache) et dont 
on veut éviter qu'elle apparaissent au fur et à mesure dans la 
page. 

![La vignette de vidéo est directement embarquée avec une adresse en data:](img/chap04-la-vignette-de-video-est-directement-embarquee-avec-une-adresse-en-data.png)

#### Limitations

Cette procédure demande toutefois quelques précautions. Tout 
d'abord il faut faire très attention à ne spécifier l'image que 
dans une seule règle CSS. Si besoin on peut construire un sélecteur 
complexe ou les mutualiser (plusieurs sélecteurs séparés par 
des virgules, pour un bloc de règles CSS). Si on répète plusieurs 
fois le lien c'est plusieurs fois qu'il faudra embarquer l'image 
dans la feuille de style et le poids total de cette dernière va 
exploser. 

Il faudra aussi faire attention à la compatibilité. Si quasiment 
tous les navigateurs supportent cette syntaxe dans leur dernière 
version, Microsoft Internet Explorer ne le permettait pas dans 
ses versions 6 et 7, qui sont encore assez répandues. Si vous utilisez 
un site web public, vous devrez peut être proposer des alternatives 
à ces deux versions. Une possibilité est de proposer une feuille 
de style additionnelle ou alternative spécifique à Internet 
Explorer 6 et 7 qui elle va utiliser des fichiers traditionnels, 
ou une autre méthode (par exemple les compositions mhtml vues 
juste après). Cela demande toutefois de maintenir deux versions 
de la même CSS, et donc des difficultés supplémentaires. 

C'est aussi pour cela qu'il faut autant que possible se limiter 
aux images de décoration insérées dans les feuilles de style. 
Utiliser les liens en data: pour des composants qui ont du sens 
par rapport au contenu (schéma illustratifs par exemple) risquerait 
de poser des problèmes à de nombreux lecteurs ou aux robots (indexation, 
aide à la compréhension, outils d'accessibilité, etc.). 

### Les fichiers composés mhtml

Les fichiers mhtml viennent du monde des emails (le nom vient 
de « mime html », mime venant lui-même de « multipurpose internet 
mail extensions », pour « extensions multifonctions pour e-mails 
internet). Ils y servent à insérer des images jointes au mail 
et y faire référence dans le corps du document (généralement 
du HTML). 

Ce concept, défini par la RFC 2557, a récemment été réutilisé 
par les équipes de performance web pour compenser le manque de 
liens en data: par Microsoft Internet Explorer 6 et 7. On le trouve 
aussi parfois sous le nom de MHT. Certains fichiers réalisés 
ainsi utilisent l'extension .mht. 

#### Fonctionnement

Le fichier mhtml doit être envoyé avec une entête `Content-Type` 
spécifique, `message/rfc822`. 

L'interne des fichiers est exactement le même que l'enveloppe 
d'un e-mail. Le contenu principal prend un type mime spécifique 
(`multipart/related`, qui est différent du type mime avec lequel 
est envoyé le fichier mhtml lui-même) et on utilise des séparateurs 
(« boundary ») pour diviser le message en plusieurs sous contenus. 
Chaque sous contenu a ensuite ses propres entêtes avec son type 
mime. 

Exemple de fichier MHTML (notez bien la présence d'entêtes et 
les différents blocs de contenu et la présence des deux tirets 
finaux) : 

~~~~~~~ {.email .mime}
From: eric.daspet@example.org
Subject: Exemple de mhtml
MIME-Version: 1.0
Content-Type: multipart/related ;
              boundary="==boundary-1" ;
              type="text/html"

<!doctype html>
<html>
<head>
<title>Test de puce rouge</title>
<link rel="stylesheet" href="mhtml:http://...fichier.html!st.css">
</head>
<body>
<p>Exemple de puce rouge : 
<img src="mhtml:http://......fichier.html!puce-rouge.png" alt="*">
</p>
</body>
</html>

--==boundary-1
Content-Type: text/css
Content-Location: st.css

img { border: 1px solid black ; }

--==boundary-1
Content-Type: image/png
Content-Location: puce-rouge.png
Content-Transfer-Encoding: base64

iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAABGdBTUEAALGP
C/xhBQAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9YGARc5KB0XV+IA
AAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAF1J
REFUGNO9zL0NglAAxPEfdLTs4BZM4DIO4C7OwQg2JoQ9LE1exdlYvBBeZ7jq
ch9//q1uH4TLzw4d6+ErXMMcXuHWxId3KOETnnXXV6MJpcq2MLaI97CER3N0
vr4MkhoXe0rZigAAAABJRU5ErkJggg==
--==boundary-1--
~~~~~~~

Pour référencer un contenu (identifié grâce à l'entête `Content-Location` 
dans le fichier mhtml) on utilise un lien de type suivant : 

~~~~~~~ {.oneline}
mhtml:adresse_du_fichier!identifiant_dans_le_fichier
~~~~~~~

Si tous les composants référencés et le code qui les référence 
sont dans le même fichier mhtml, il est aussi possible, voire 
préférable, de référencer tous les composants (même le principal) 
par leur URL réelle complète. Le navigateur sait alors résoudre 
les liens relatifs ou absolus classiques, et prendre le contenu 
dans le fichier mhtml s'il y est présent, ou aller le chercher 
sur le réseau sinon. 

#### Utilisation

Les cas d'utilisation des fichiers mhtml sont globalement les 
mêmes que les URLs en data:. Le système peut même sembler plus 
propre du fait qu'on sépare bien les différents contenus dans 
des blocs distincts. 

Comme désormais les dernières versions de Microsoft Internet 
Explorer supportent les adresses en data:, l'utilisation des 
fichiers mhtml devient anecdotique sur le web. 

#### Limitations

Les limitations sont elles aussi similaires aux URLs en data:. 
Ici toutefois seul Microsoft Internet Explorer et Opera depuis 
sa version 9 savent relire ces contenus. C'est d'ailleurs la 
seule alternative viable pour les premiers. 

Pour Mozilla Firefox, il est toutefois possible d'ajouter le 
support des mhtml via une extension. Les extensions « Mozilla 
Archive Format » (maf) ou « unMHT » permettent cela. Elles sont 
toutes les deux téléchargeables depuis le dépôt officiel des 
extensions Firefox. 

La faible compatibilité des fichiers mhtml les limite à des intranets 
où on contrôle les navigateurs web et leurs versions, ou comme 
alternative aux URL en data: pour les vieilles versions d'Internet 
Explorer. 

À l'aide de commentaires et de hacks CSS, certains ont réussi à 
faire interpréter un même fichier avec des liens en data: pour 
ceux qui le supportent, et des composants intégrés en mhtml pour 
les vieilles versions de Internet Explorer. Cette technique 
vous impose toutefois d'insérer deux fois en base64 le contenu 
de chaque image embarquée, et donc de doubler le poids de ces images 
pour le navigateur. Je préfère charger une feuille de style dédiée 
suivant le navigateur, par exemple à l'aide de commentaires 
conditionnels HTML. 

### Les archives jar

Les archives jar ne sont supportées que par les navigateurs basés 
sur Gecko (dont Mozilla Firefox). Il s'agit, comme pour les fichiers 
mhtml, d'embarquer plusieurs contenus en un fichier unique 
et de pouvoir les référencer individuellement par la suite. 

Ici, au lieu d'utiliser le format des pièces jointes e-mail, 
on réutilise le système des archives zip. Il suffit d'insérer 
tous vos fichiers dans une archive zip et de la distribuer avec 
le type `application/java-archive` ou `application/x-jar`. 
Généralement on utilise l'extension .jar plutôt que .zip, mais 
cela n'a aucune influence réelle. 

On peut ensuite référencer les documents avec une syntaxe quasiment 
identique à celle des fichiers mhtml : 

~~~~~~~ {.oneline}
jar:adresse_du_fichier_jar!adresse_dans_le_fichier_jar
~~~~~~~

Les utilisations et les limitations sont en tout point similaires 
à celles des fichiers mhtml. La seule différence est le support 
de Mozilla Firefox plutôt que Microsoft Internet Explorer et 
le fait que le fichier puisse être initialement compressé (mais 
HTTP peut compresser après coup les fichiers mhtml lors du transfert, 
donc ce n'est pas un critère différenciant). 

### Quand et quoi mettre « en ligne » ou par lot

Les données « en ligne » devraient être réservées aux pages d'accueil, 
aux pages d'arrivées spéciales (celles nommées dans une publicité, 
dans une plaquette). Dans ces cas l'idée est de convaincre l'utilisateur 
de continuer sur le site en lui donnant la meilleure expérience 
possible pour sa première page (sur laquelle on ne reviendra 
probablement pas), quitte à ralentir un peu le chargement de 
la seconde page (vu que le cache ne sera pas initialisé). Plus 
la page est spéciale, et uniquement utilisée comme page d'arrivée 
(et pas dans la suite de la navigation), plus la décision d'embarquer 
javascript, CSS, voire images et autres composants en ligne 
à du sens. 

Pour les téléchargements par lot via data:, mhtml ou jar les critères 
sont un peu plus souples. Il est en effet possible de profiter 
du cache quand les fichiers sont chargés en lot avec une feuille 
de style ou un javascript externe (et pas directement avec la 
page principale elle-même). Il n'y a que trois problématiques 
qui doivent alors guider le choix : 

* limiter la présence dans le lot de composants qui sont inutiles 
  pour la page en cours (et encore plus s'ils sont aussi potentiellement 
  inutiles dans les futures pages visitées) ; 

* ne pas tout mettre en un seul fichier mais faire en sorte que 
  la page ait au moins deux ou trois lots ou composants différents 
  de taille similaire (afin d'optimiser les capacité de téléchargement 
  parallèle des navigateurs) ; 

* ne pas charger un composant en lot avec un contenu principal 
  (CSS ou js) si ce composant est aussi utilisé ailleurs sans 
  ce contenu principal (sinon on va de toutes façons devoir le 
  recharger indépendamment et on l'aura téléchargé deux fois 
  au final). 



Globalement, si deux contenus sont toujours utilisés ensemble, 
les charger en un seul lot a presque toujours du sens. C'est par 
exemple le cas pour des images de fonds ou des icônes qui sont utilisées 
sur presque toutes les pages et référencées par la feuille de 
style. Dans ce cas les embarquer directement dans la feuille 
de style à l'aide de liens en data: ou mhtml est une bonne idée. 

Il reste que du fait de la complexité de mise en œuvre (devoir gérer 
mhtml et data: suivant le navigateur en proposant les deux) et 
du coût de maintenance associé, il faut vraiment restreindre 
ces solutions aux cas où le gain est assuré. 

**Recommandation** : Les images de décoration utiles sur la 
grande majorité des pages peuvent être intégrées directement 
dans la feuille de style à l'aide des procédés data: et mhtml. 

Pour servir le bon type de contenu à chaque navigateur, on peut,
par exemple, utiliser des commentaires conditionnels HTML (et 
donc des feuilles de style différentes). 

Repérer les requêtes HTTP inutiles
----------------------------------

Nous avons tenté de limiter les requêtes HTTP sur les contenus 
principaux, en regroupant les feuilles de style et les fichiers 
javascript entre eux, en regroupant les images en sprites, voire 
en proposant de télécharger les images directement avec la feuille 
de style. 

Il faut ensuite faire la chasse à toutes les requêtes HTTP qui 
pourraient être inutiles. On en identifie habituellement trois 
types : les redirections, les feuilles de style alternatives, 
et les images de gradients ou de coins arrondis. 

### Redirections

Les redirections sont les premiers candidats à l'élimination. On 
voit beaucoup de redirections sur les sites d'entreprise. La 
saisie de [http://example.org/](http://example.org/) nous 
renvoie vers [http://www.example.org/](http://www.example.org/) 
qui nous renvoie ensuite vers [http://www.example.org/index.html](http://www.example.org/) 
qui renvoie parfois à son tour vers [http://www.example.org/welcome/](http://www.example.org/welcome/). 
Si cet exemple fictif vous paraît un peu exagéré, sachez qu'il 
n'en est rien et qu'il n'est pas rare de voir quatre ou cinq redirections 
consécutives. 

Chaque redirection est un aller-retour entre le navigateur 
et le serveur. Si l'aller-retour est assez souvent rapide, quand 
on en enchaîne trois ou quatre, cela devient non négligeable. 
C'est d'autant plus vrai si la redirection concerne la page HTML 
elle-même puisque c'est tout l'affichage qui est mis en attente 
le temps de résoudre la redirection. 

![Flux des requêtes HTTP dans le cas d'une redirection](img/chap04-flux-des-requetes-http-dans-le-cas-dune-redirection.png)

La solution serait de supprimer les redirections, afficher 
directement le bon contenu lors du premier accès. Un compromis 
acceptable pourrait être de au moins faire en sorte que la première 
redirection nous emmène directement à la destination finale 
sans les étapes intermédiaire. 

**Recommandation** : Évitez les redirections, ou restreignez-les 
pour ne jamais les empiler les unes après les autres. 

S'il faut faire une redirection le problème peut être encore 
plus important si vous la réalisez à l'aide d'une balise `<meta>` 
en HTML ou pire, avec un code javascript` document.location`. 
Dans ces deux cas le navigateur tente de charger la page HTML complète 
avec images, javascript, feuilles de style. C'est parfois plusieurs 
dizaines de requêtes qui sont lancées avant de faire la redirection. 
C'est donc autant de temps perdu, inutilement. 

Si redirection il doit y avoir elle doit être faite directement 
en HTTP (on utilise un code de retour spécial, qui peut être 301, 
302 ou 307). Le navigateur et le serveur web se comprennent alors 
immédiatement sans besoin pour le premier de télécharger une 
lourde page HTML bourrée de composants. 

**Recommandation** : Les éventuelles redirections doivent 
être réalisées avec un code de retour HTTP 301, 302 ou 307 et pas 
à l'aide d'une balise HTML ou d'un code javascript. 

### Feuilles de style alternatives

Les pages web permettent de saisir plusieurs feuilles de style, 
certaines destinées à la présentation écran, d'autres à la présentation 
papier ou pour les appareils mobiles. On s'en sert par exemple 
pour masquer les menus de navigation à l'impression, ou pour 
donner une allure plus classique à une impression papier. Le 
navigateur se charge alors de prendre la feuille de style qui 
est adapté à son cas et d'ignorer les autres. 

Malheureusement, certains navigateurs téléchargent toutes 
ces feuilles de style, même quand ils ne s'en servent pas. Dès 
lors, il est important de ne proposer que les feuilles de style 
qui sont vraiment utiles et de ne pas les multiplier « au cas où 
». 

**Recommandation** : Ne proposer des feuilles de style pour 
des médias spécifiques qu'en cas de besoin. Vous éviterez d'ajouter 
des téléchargements inutiles pour certains navigateurs. 

Si plusieurs styles sont nécessaires, il est aussi recommandés 
de les regrouper dans le même fichier (impression et écran dans 
le même fichier) plutôt que de faire un fichier CSS par type d'application 
(impression et écran dans des fichiers séparés). On évite au 
navigateur de faire une requête supplémentaire inutilement 
vu qu'il va de toutes façons télécharger les deux. 

**Recommandation** : Insérer les styles destinés aux différents 
médias dans le même fichier CSS plutôt que dans des fichiers dédiés 
à chaque fois. 

Il est aussi possible de déclarer plusieurs styles pour la page. 
L'un sera le style par défaut, les autres seront des styles alternatifs, 
qui pourront être chargés sur demande de l'utilisateur. On s'en 
sert par exemple pour fournir des versions adaptées aux mal-voyants, 
ou pour proposer une version à contraste inversé (texte clair 
sur fond sombre). 

Là aussi certains navigateurs chargent tous ces styles par défaut 
bien qu'ils n'en utilisent qu'un seul à la fois. Ils le font pour 
pouvoir basculer rapidement de l'un à l'autre, mais du coup ils 
ralentissent le chargement de la page principale, ce qui est 
rarement une bonne idée. 

**Recommandation** : Ne spécifiez pas de style alternatif dans 
la page HTML. Si vous devez proposer plusieurs styles, reposez-vous 
sur un mécanisme à base de javascript ou via un fonctionnement 
applicatif côté serveur (le serveur se chargera d'envoyer la 
feuille de style adaptée et seulement celle là). 

### Bug CSS Android

Android, au moins jusqu'à sa version 2.3, hérite d'un [ancien 
bug du moteur Webkit](https://bugs.webkit.org/show_bug.cgi?id=24223). 
Ce dernier télécharge toutes les ressources liées par vos feuilles 
de style et applicables à votre contenu HTML. 

Normalement le navigateur gère les priorités et les redéfinitions 
dans les feuilles de style. Si j'ajoute une image de fond rouge 
à tous les paragraphes, un verte aux paragraphes d'introduction 
et une bleue quand l'écran fait moins de 600 pixels, le navigateur 
sait faire le tri et n'en télécharger qu'une seule : celle qu'il 
affichera réellement. Android n'a pas ce comportement astucieux 
et téléchargera les trois images quoi qu'il en soit. Le navigateur 
risque donc de télécharger beaucoup d'images inutiles, et de 
ralentir d'autant le chargement complet de la page. 

**Recommandation** : Ne vous reposez pas trop sur la cascade 
CSS. Faites en sorte que pour chaque balise HTML seule une image 
de fond soit applicable dans les feuilles de style. 

### Coins arrondis

Dans les mises en page riches, une partie non négligeable des 
images est souvent utilisée pour arrondir les angles des cadres. 
Les blocs rectangulaires purs et durs sont vus comme agressifs 
et « vieux » par certains graphistes. 

La technique la plus habituelle est d'utiliser une à neuf images 
pour les coins et les bordures, et un montage peu simple en html 
et CSS pour placer ces images dans les différents coins et bords. 
Le plus souvent ce sont quatre images qui sont utilisées, ou au 
moins une grande image (sprite) de taille extrêmement importante 
au cas où le bloc s'étende sur la page. Ceci est à multiplier par 
le nombre de blocs différents à arrondir : couleur, taille, épaisseur 
de bordure, etc. 

![Bloc avec des coins arrondis](img/chap04-bloc-avec-des-coins-arrondis.png)

Nous passer de ces coins arrondis peut améliorer sensiblement 
les performances, simplement grâce à la réduction de requêtes 
HTTP. C'est par exemple le choix qu'à fait Yahoo! Sport Europe. 
Si le choix a été difficile à prendre, après une phase d'acclimatation 
le design s'est trouvé tout aussi acceptable, mais avec de bien 
meilleures performances. 

Si votre graphiste est rétif à l'idée de ce genre de changement 
radical, il est heureusement possible d'envisager une autre 
solution : 

#### Utilisation de CSS 3

Le niveau 3 de la spécification CSS propose une amélioration 
que nous pouvons utiliser. Le module « Backgrounds and Borders 
» inclut en effet une propriété `border-radius` qui permet d'arrondir 
la bordure d'un bloc. Aucune image n'est alors mise en œuvre et 
le rendu sera tel qu'on l'attends : un bloc dont les bords sont 
arrondis. 

L'arrondi généré correspond à un quart d'ovale dont la distance 
entre le centre et les bords est donnée en paramètre. Il est donc 
possible de spécifier deux valeurs pour chaque coin (`border-{top|bottom}-{left|right}-radius`) 
séparées par un espace : la distance du centre de l'ovale par rapport 
au bord vertical puis la distance du centre par rapport au bord 
horizontal. Si on ne spécifie qu'une seule valeur elle servira 
pour les deux distances. 

![Bloc avec border-top-left-radius: 55pt 25pt](img/chap04-bloc-avec-border-top-left-radius.png)

Il est aussi possible de spécifier directement les quatre coins 
dans la directive border-radius. Il faut alors spécifier les 
distances avec les quatre bords verticaux séparés par des espaces, 
dans le même ordre que pour la propriété border, puis une barre 
oblique (_slash_) et les distances avec les quatre bords horizontaux. 

~~~~~~~ {.css}
border-radius: 55pt 55pt 55pt 55pt / 25pt 25pt 25pt 25pt ;
/* équivalent à : */
border-top-right-radius: 55pt 25pt ;
border-top-left-radius: 55pt 25pt ;
border-bottom-left-radius: 55pt 25pt ;
border-bottom-right-radius: 55pt 25pt ;
~~~~~~~

Tous les navigateurs récents acceptent ces directives CSS, 
y compris Microsoft Internet Explorer à partir de sa version 
9. Les anciens navigateurs Webkit (ce qui inclut à l'heure actuelle 
les dernières versions Iphone 4.2 et Android 2.3) nécessitent 
toutefois un préfixe `-webkit-`. Il vous suffit de préciser 
les deux versions, avec et sans préfixe, pour maximiser la compatibilité. 

~~~~~~~ {.css}
-webkit-border-radius: 55px ;
border-radius: 55px ;
~~~~~~~

**Recommandation** : Utilisez CSS 3 pour remplacer les blocs 
à coins arrondis ou formes arrondies créés à partir de fichiers 
images. 

#### Lâcher Microsoft Internet Explorer 6 à 8

Le problème restant avec la technique CSS 3 est celui des anciens 
navigateurs, et plus spécifiquement des Microsoft Internet 
Explorer 6, 7 et 8 qui ne supportent pas cette technique. 

Il y a deux solutions à envisager si ces navigateurs ne représentent 
pas votre cœur de cible. Heureusement pour nous, s'ils occupaient 
90 % du marché il y a encore peu, ce chiffre diminue chaque mois 
et tombera probablement en dessous de 50 % avant que vous ne lisiez 
ces lignes (il est d'environ 53 % lors de l'écriture). Quand Microsoft 
Internet Explorer 9 sortira, on peut facilement imaginer voir 
les parts de marché cumulées des versions 6, 7 et 8 tomber nettement 
plus bas (je voulais m'avancer à annoncer 30 % mais un relecteur 
m'a fait signe que je m'avançais un peu trop alors prenez ce nombre 
comme un pari). À l'heure où j'écris ces lignes Microsoft lui-même 
lance une opération de communication ayant pour but d'éliminer 
Internet Explorer 6 du marché et de provoquer une mise à jour. 

La première solution est celle de l'enrichissement progressif 
(ou de la dégradation acceptable, suivant le référentiel que 
vous choisissez). Il s'agit de n'appliquer les arrondis qu'aux 
navigateurs supportant CSS 3 et de considérer qu'il s'agit d'un 
enrichissement optionnel, d'un bonus. On parle souvent d'enrichissement 
progressif ou de dégradation élégante, il s'agit d'assumer 
de faire « mieux » pour certains navigateurs tant que les autres 
gardent un niveau de qualité acceptable. 

La seconde solution est d'utiliser les « hacks » CSS ou les commentaires 
conditionnels HTML pour revenir à l'ancienne méthode des coins 
arrondis à l'aide d'images pour ces vieilles versions de Microsoft 
Internet Explorer. Si cela vous paraît inacceptable de laisser 
ce défaut de performance à MSIE, dites vous bien que c'est ce que 
vous aviez jusqu'à maintenant, et qui ne vous posait pas de problème 
de conscience avant de lire ce chapitre. 

Les questions de performance sont toujours des compromis et 
une obligation de moyens. Vous pouvez faire mieux, mais si vous 
avez amélioré la situation sur ce point là pour 50 % des visiteurs 
c'est déjà pas mal non ? Les autres améliorations s'adresseront, 
elles, à tout le monde. Certaines même s'adresseront uniquement 
à Microsoft Internet Explorer, et vous aurez rétabli la balance 
sans rentrer dans des développements discutables. 

#### SVG et VML

Il nous reste encore deux solutions qui n'ont pas été abordées 
pour nos coins arrondis : utiliser des images vectorielles directement 
embarquées dans un code html ou javascript, ne nécessitant pas 
de multiples images tierces. 

Nous pouvons faire appel pour cela aux deux langages vectoriels 
utilisés sur le web : SVG et VML. Le premier, SVG, est supporté 
plus ou moins bien par tous les navigateurs récents mais souffre 
lui aussi d'un manque de support de Microsoft Internet Explorer 
6 à 8 (sauf plugin tiers à installer en plus). Il n'offre donc pas 
grand avantage par rapport à la solution en CSS 3. 

La seconde solution se base sur VML, qui est plus ou moins l'ancêtre 
de SVG tout en étant plus évolué sur de nombreux points. Ce format 
a été abandonné vers 1998 mais reste supporté et utilisé par Microsoft, 
entre autres dans son navigateur. Il est alors possible de l'utiliser 
pour créer nos coins arrondis dans Microsoft Internet Explorer, 
en parallèle de l'option CSS 3. 

Pour obtenir un bloc aux coins arrondis on utilise la balise VML 
`<roundrect>`. L'attribut `arcsize` permet de définir la profondeur 
de l'arrondi et l'attribut fillcolor la couleur de fond (mais 
des syntaxes existent pour mettre des images de fond ou des bordures 
et pas uniquement des couleurs pleines). 

Dans l'exemple suivant on commence par déclarer l'espace de 
nom VML (le préfixe utilisé par les balises VML) avant d'utiliser 
`<roundrect>` lui même. Ceux qui ont l'habitude d'utiliser 
XML doivent faire attention, la syntaxe pour déclarer l'espace 
de nom n'est pas une faute de frappe, y compris le préfixe « `xml:` 
» qui n'est lui-même pas déclaré (si cette phrase d'avertissement 
ne vous dit rien, ignorez là). 

~~~~~~~  {.xml .vml .partial}
<xml:namespace ns="urn:schemas-microsoft-com:vml" prefix="v" />
<v:roundrect arcsize=".02" fillcolor="#000">
Lorem ipsum dolor sit amet, consectetuer adipiscing
</v>
~~~~~~~

Pour que le tout fonctionne on ajoute quelques lignes à la feuille 
de style : 

~~~~~~~ {.css}
v\:roundrect {
    behavior:url(#default#VML) ;
}
~~~~~~~

Au final, si on souhaite utiliser la même balise pour tous les 
navigateurs, on peut finir avec un code similaire à celui ci : 

~~~~~~~ {.css}
v\:roundrect {
    color:#FFF ;
    display:block ;
    background-color:#000 ;
    -moz-border-radius:10px ;
    -webkit-border-radius: 10px ;
    -khtml-border-radius: 10px ;
    border-radius: 10px ;
    /* suite pour IE seulement : */
    behavior:url(#default#VML)
    /background-color:transparent ;
}
~~~~~~~

On notera l'obligation d'utiliser un « hack » CSS (la barre oblique 
inverse) pour retirer la couleur de fond dans Internet Explorer. 
Cette dernière est en effet alors précisée dans la balise VML. 
Pour Microsoft Internet Explorer 8 il faut aussi souvent ajouter 
les règles CSS `position:relative` et `z-index:0` à l'élément 
parent afin de faire fonctionner notre code VML. 

Deux projets existent afin d'améliorer la situation pour les 
intégrateurs web. Tous deux prennent en charge la création de 
l'élément VML et tout le code nécessaire pour le mettre en œuvre. 

La première s'appelle DD_Roundies. Elle vous propose de charger 
un fichier javascript spécifique avec un commentaire conditionnel 
HTML (pour ne pas l'exécuter dans les navigateurs qui n'en ont 
pas besoin) et de faire appel à la méthode `DD_Roundies.addRule()` 
pour chaque sélecteur CSS qui cible les images à coins arrondis. 

~~~~~~~ {.html .partial}
<!--[if lte IE 9]>
<script type="text/javascript" src="script/roundies.js">
</script><![endif]-->
DD_roundies.addRule('#menu li', '10px'); 
DD_roundies.addRule('h1', '20px') ;
~~~~~~~

La seconde méthode utilise un fichier HTC (l'équivalent des 
fichiers XBL, mais propre à Microsoft Internet Explorer). Ces 
fichiers permettent de rajouter des fonctionnements complexes 
à certaines balises HTML, en javascript, dans les feuilles de 
style. Le HTC s'occupe donc de tout le code VML pour peu qu'on l'inclue 
dans tous les sélecteurs CSS qui s'occupent des coins arrondis 
: 

~~~~~~~ {.css}
.arrondi { 
    -moz-border-radius: 10px; 
    -webkit-border-radius: 10px; 
    -khtml-border-radius: 10px; 
    border-radius: 10px; 
    behavior: url(border-radius.htc); 
}
~~~~~~~

Chaque méthode a ses propres avantages. La seconde est réputée 
fonctionner avec des cas plus complexes (images de fond, bordure, 
semi-transparence, etc.) mais ne gère que les cas où les quatre 
coins sont arrondis de la même façon et peut poser des problèmes 
de performance (je ne garantis donc pas que le remède n'est pas 
pire que le mal initial, c'est à vous de le vérifier). 

* DD_Roundies : [http://dillerdesign.com/experiment/DD_roundies/](http://dillerdesign.com/experiment/DD_roundies/) 

* HTC curved corners : [http://code.google.com/p/curved-corner/](http://code.google.com/p/curved-corner/) 

Si vous décidez d'utiliser une de ces deux méthodes, relisez 
le titre précédent « Lâcher Microsoft Internet Explorer 6 à 8 
» par acquis de conscience. Il est très probable que vous vous 
engagiez sur une solution qui ne couvre pas tous les cas d'utilisation, 
qui vous ajoute des contraintes de développement, et qui a, elle 
aussi, ses défauts de performance (htc pour l'une, javascript 
pour l'autre). À vous de faire vos choix, je ne vous donne que les 
armes, mais si vous vous tirez dans le pied à ce sujet, je dégage 
toute responsabilité. 

### Dégradés

Comme pour les coins arrondis, il est possible d'utiliser CSS 
3 pour remplacer les images de dégradés. Les dégradés sont alors 
créés directement par le navigateur sans fichier ou ressource 
externe, sans perte de performance. 

C'est important car pour créer un dégradé en image de fond il faut 
parfois créer une image de taille importante, qui sera lourde 
à charger ou lourde à afficher. Cela peut aussi avantageusement 
remplacer les images de fond pour les boutons et éliminer le besoin 
pour de nombreuses images (en fonction de la taille du bouton, 
de la couleur, etc.). 

La structure est complexe afin de gérer les différents navigateurs. 
En attendant que CSS 3 soit mieux implémenté vous pouvez générer 
tout le code nécessaire à l'aide d'un outil en ligne à l'adresse 
<http://www.display-inline.fr/projects/css-gradient/>. 

#### Utilisation de CSS 3

Des directives pour créer des dégradés ont été imaginées pour 
CSS 3. Il s'agit de spécifier une couleur de départ, une couleur 
de fin, éventuellement des couleurs intermédiaires, et une 
direction. À partir de là le navigateur est capable de créer un 
dégradé en image de fond, linéaire ou radial. 

Malheureusement la syntaxe n'a pas été figée immédiatement 
et il existe encore deux implémentations existantes. 

La syntaxe amenée à être pérennisée est supportée par Mozilla 
(Firefox), Safari à partir de la version 6, Chrome à partir de 
la version 10 et Opera à partir de la version 11.1. Une ancienne 
syntaxe propre à Webkit, plus complexe, est supportée par les 
versions plus anciennes de Chrome et Safari (y compris leurs 
versions iphone 4.2 et Android 2.3). Microsoft Internet Explorer 
prévoit un support pour sa version 10. 

La première syntaxe, pérenne, est la plus simple. Pour un dégradé 
linéaire on utilise `linear-gradient` avec une position de 
départ (horizontale puis verticale) et/ou un angle, puis au 
moins deux mentions de couleur (une couleur et optionnellement 
une mesure en pourcentage ou en pixels). Le dégradé suivant a 
une direction de 90 degrés, va du blanc au rouge puis du rouge au 
noir, le rouge se situant à 70% entre le départ et la fin du dégradé 
: 

~~~~~~~ {.oneline .css}
background-image: -moz-linear-gradient(90deg, white, red 70%, black);
~~~~~~~

Pour un dégradé radial on utilise `radial-gradient` avec une 
position de départ et/ou un angle, la forme (`elipse` ou `circle`) 
avec éventuellement la taille (`closest-side`, `closest-corner`, 
`farthest-side`, `farthest-corner`, `cover` ou `contain`), 
puis une suite de mentions de couleurs (à chaque fois une couleur 
et optionnellement une mesure). 

~~~~~~~ {.css}
background-image: -moz-radial-gradient(center center, 
                      circle closest-side #aedae5, #d8edf2 50px);
~~~~~~~

Les deux propriétés peuvent être préfixées par `repeating-` 
pour demander une répétition du gradient. 

Enfin, les navigateurs actuels demandent pour l'instant un 
préfixe pour interpréter ces syntaxes : `-moz-` pour Mozilla 
(utilisé dans les exemples), et `-o-` pour Opera et `-webkit-` 
pour Webkit. Dans le futur ces préfixes sont amenés à disparaître. 

#### Support des anciens navigateurs Wekit

Pour les anciens webkit (Safari et Chrome, y compris mobiles) 
c'est une `-webkit-linear-gradient` qui contrôle les deux 
types de dégradés.Les paramètres sont, dans l'ordre : 

* le mot clef `radial` ou `linear` respectivement pour des dégradés 
  radiaux ou linéaires ; 

* la position du début du dégradé avec d'abord la position horizontale 
  (mot clef `left`, `center`, `right`, ou une mesure par rapport 
  au bord) puis la position verticale (`top`, `center`, `left`, 
  ou une mesure) ; 

* uniquement dans le cas d'un dégradé radial : la taille du rayon 
  de début ; 

* la position de fin du dégradé, suivant la même syntaxe que le 
  début ; 

* uniquement dans le cas d'un dégradé radial : la taille du rayon 
  de fin ; 

* la couleur de début dans l'expression `from( )` ; 

* la couleur de fin dans l'expression `to( )` ; 

* d'optionnels points d'arrêts avec l'expression `color-stop( 
  )` qui contient deux arguments séparés par une virgule : une 
  mesure en pourcentage pour la position de la couleur intermédiaire 
  et le code couleur voulu. 

~~~~~~~ {.oneline .css}
background-image: -webkit-gradient( radial, left top, 10, left top, 450, from(#aedae5), to(white), color-stop(0.8, red) );
~~~~~~~

~~~~~~~ {.oneline .css}
background-image: -webkit-gradient(linear, left top, right top, from(#00f), to(#fff), color-stop(0.8, #fff));
~~~~~~~

![Dégradé radial](img/chap04-degrade-radial.png)

La syntaxe de Mozilla est un peu plus simple mais offre moins de 
possibilités pour les dégradés radiaux. La syntaxe Webkit permet 
par exemple de décentrer un dégradé radial. Cette dernière est 
aussi la seule à supporter actuellement les navigateurs mobiles 
Android et Iphone. 

Il n'est pas certain que cette ancienne syntaxe continue à être 
supportée pendant des années, aussi vous êtes encouragés à toujours 
aussi utiliser la syntaxe pérenne expliquée plus avant. Le navigateur 
utilisera celle qu'il reconnaitra (ou la dernière saisie s'il 
connait les deux). 

#### Support des anciens navigateurs Opera

Opera ne supporte les dégradés que depuis peu mais vous pouvez 
toutefois insérer une image SVG en image de fond. Elle sera plus 
petite et donc plus rapide à télécharger. Elle pourra aussi être 
utilisée dans plusieurs contexte sans avoir une image par taille 
nécessaire. 

#### Support des navigateurs Microsoft Internet Explorer

Pour l'instant (jusqu'à sa version 9 incluse) Internet Explorer 
ne supporte pas de syntaxe CSS pour générer des dégradés. Les 
seules solutions sont de placer une image de taille fixe ou d'utiliser 
une couleur pleine sans dégradé. 

#### Donner l'impression de dégradé pour les boutons

Une solution alternative pour les fonds des titres ou des boutons 
est de simuler un dégradé à l'aide d'aplats de couleur. L'objectif 
est de donner le ressenti d'un dégradé sans en avoir besoin. Le 
plus souvent, si l'objet est petit, il suffit d'avoir en fond 
une couleur clair sur la moitié supérieure et une couleur foncée 
sur la moitié inférieure. Il n'y a besoin d'aucun support particulier 
par le navigateur et d'aucun téléchargement. 

Exemples de boutons à l'aide de deux couleurs pleines, sans dégradé 

En comparant les versions avec et sans dégradé on aura certainement 
une impression négative sur la seconde mais le visiteur ne ressentira 
probablement pas de manque avec cette alternative. 

**Recommandation** : Quand cela est possible, remplacez les 
images de dégradés par des règles CSS 3, ou simulez les avec un 
jeu de deux couleurs. 

### Ombres portées

De la même manière que les coins arrondis et les dégradés, des 
améliorations CSS permettent désormais de réaliser des ombres 
avec quelques lignes de code sans nécessiter de faire télécharger 
des images dédiées par le navigateur. 

Le support des ombres est désormais complet par les navigateurs, 
mais n'existait pas sur Microsoft Internet Explorer sur les 
versions 6, 7 et 8. Sur ces anciennes versions il faudra compenser 
avec une image de fond ou accepter de ne pas afficher d'ombre. 

Il existe deux directives, `box-shadow` pour créer une ombre 
sur un bloc et `text-shadow` pour créer une ombre sous un texte. 
Elles prennent quatre valeurs : 

* un décalage horizontal pour faire en sorte que l'ombre se situe 
  sur un côté, 

* un décalage vertical pour faire en sorte que l'ombre se situe 
  en haut ou en bas, 

* une mesure pour l'entendue du flou autour de l'ombre, 

* la couleur de l'ombre. 

![Un bloc avec une ombre en bas à droite](img/chap04-un-bloc-avec-une-ombre-en-bas-a-droite.png)

Afin de s'assurer d'un support sur Safari (au moins jusqu'à la 
version 6 incluse) , Iphone (au moins jusqu'à la version 4.2 incluse) 
et Android (2.3 incluse), il faudra ajouter une seconde directive 
préfixée par `-webkit-` : `-webkit-text-shadow`. 

**Recommandation** : Quand cela est possible, remplacez les 
ombres portées réalisées avec des images par des règles CSS 3. 

Éviter les requêtes HTTP non souhaitées
---------------------------------------

Au delà de la réduction du volume de requêtes par regroupement 
ou par remplacement, de nombreuses requêtes HTTP sont présentes 
dans les pages par erreur. Ce sont ces requêtes qu'il nous reste 
à éliminer pour clôturer ce chapitre. 

### Duplications

Les ressources dupliquées sont les cas les plus courants de ces 
requêtes non souhaitées. On les retrouve fréquemment sur les 
sites gérés par des logiciels clefs en main avec beaucoup de modules 
ajoutés les uns aux autres. 

En étudiant la liste des ressources il arrive qu'on remarque 
qu'un même composant est chargé plusieurs fois. Ce peut être 
simplement deux balises <script> utilisant la même adresse 
ou un même composant chargé via des adresses proches (une qui 
contient un paramètre et l'autre non), par deux serveurs distincts 
(le chargement de jquery une fois par Google et une fois en local 
sur le site), ou dans des versions proches. 

Ces duplications imposent au navigateur de charger plusieurs fois le composant. Quand il s'agit de bibliothèques javascript de près de 100 Ko l'impact sur les performances est important. Pour traquer ces cas regardez la liste des ressources CSS et javascript téléchargées et traquez des noms similaires.

![Ces duplications imposent au navigateur de charger plusieurs fois le composant](img/chap04-ces-duplications-imposent-au-navigateur-de-charger.png)

Dans le cas d'images, cela peut être plusieurs images proches 
: couleurs proches, dégradés similaires, arrondis. Ces images 
peuvent généralement être regroupés avec un minimum de compromis 
avec le graphiste. Cela améliorera l'homogénéité du site, simplifiera 
les développements et améliorera les performances. 

**Recommandation** : Traquez et éliminez les composants chargés 
plusieurs fois par erreur et les images proches. 

### Manque de cache sur les réactualisations

Quand le navigateur tente de réactualiser une page sur requête 
de l'utilisateur, il ignore son éventuel cache et envoie des 
entêtes HTTP pour que les proxys intermédiaires ignorent eux 
aussi la version éventuellement en cache. Les objets liés sont 
eux aussi retéléchargés peu importe le statut du cache. 

Ce comportement est toutefois le même quand la réactualisation 
est automatique avec l'entête HTTP `Refresh`. Si l'entête est 
peu utilisée telle quelle, on la trouve parfois dans une balise 
`<meta>` pour faire des redirections. 

~~~~~~~ {.oneline}
<meta http-equiv=refresh content="0; url=http://new-page" />
~~~~~~~

Afin de ne pas mettre en échec les éléments déjà présents dans 
le cache, les redirections simples doivent à la place être réalisées 
par un code de retour HTTP (301, 302 ou 307 suivant les cas) et l'entête 
`Location`. En PHP par exemple cela peut être fait avec `header("Location: 
http://new-page/")`. 

S'il s'agit d'une actualisation différée alors un code javascript 
à l'aide de `document.location` et `window.setTimeout` peut 
remplacer la balise `<meta>`. 

~~~~~~~ {.oneline .javascript}
window.setTimeout( function() { document.location = "…" ; }, 1000) ;
~~~~~~~

**Recommandation** : Éliminez les entêtes « refresh » présente 
dans les entêtes HTTP ou dans les balises <meta>. Remplacez les 
par des redirections HTTP ou des compteurs javascript. 

### URL vides

Les navigateurs ont un comportement peu intuitif quand on spécifie 
une adresse vide, par exemple avec un attribut `src=""` ou `href=""`. 
Le navigateur interprète ces adresses comme des adresses relatives 
par rapport à la page courante. Une adresse relative vide référence 
alors tout simplement la page courante. 

En conséquence une image, un code javascript, une feuille de 
style, ou un composant quelconque avec une adresse vide référence 
tout simplement la page courante. Le navigateur se retrouve 
contraint de retélécharger la page HTML avant d'échouer à l'interpréter 
comme une image, du javascript ou comme une feuille de style. 

Ces adresses vides impliquent donc une requête HTTP, le téléchargement 
d'une page HTML qui fait parfois plus de 100 Ko puis une tentative 
d'interpréter cette page HTML comme si c'était un autre type 
de contenu. 

**Recommandation** : Traquez et éliminez les références à des 
URL vides. 

### Détection de jeu de caractères

Les navigateurs devraient recevoir le jeu de caractères à utiliser 
pour la page dans l'entête HTTP `Content-Type`. 

~~~~~~~ {.oneline .http .response .partial}
Content-Type: text/html;charset=UTF-8
~~~~~~~

Malheureusement certaines sites ne le spécifient pas, ou mal. 
Les navigateurs cherchent donc à détecter quel est le bon jeu de 
caractères. Pour les y aider les développeurs peuvent donc spécifier 
l'entête `Content-Type` dans les balises `<meta http-equiv>`. 

~~~~~~~ {.oneline .html .partial}
<meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
~~~~~~~

Lors de l'interprétation de la page HTML les navigateurs tentent 
de chercher ces balises `<meta>` dans les premiers milliers 
de caractères. Si votre serveur envoie la page en plusieurs bloc 
(l'entête arrivant immédiatement le temps que le corps de la 
page soit calculé) le navigateur pourrait ne pas en profiter 
puisqu'il attend la suite avant de réaliser le rendu. 

De plus, si le jeu de caractères trouvé n'est pas celui attendu 
le navigateur se doit de recommencer l'interprétation de la 
page. Ce faisant il perd un peu de temps mais certains navigateurs 
vont jusqu'à télécharger une seconde fois les composants qu'ils 
avaient commencer à télécharger. 

**Recommandation** : Précisez le bon jeu de caractères dans 
les entêtes HTTP et placez en plus le `<meta>` correspondant 
le plus haut possible dans votre page, si possible avant toute 
autre balise du `<head>`. 

### Adresses en erreur

Enfin, et bien que cela soit étonnant, de nombreux sites contiennent 
des liens ou des références vers des fichiers inexistants (javascript, 
CSS, images). Ces fichiers ont pu exister à un moment, et ne sont 
plus utilisés. Les références restent dans certains gabarits 
et provoquent des erreurs 404. 

Malheureusement ces références cassées ne se voient pas lors 
du test ou de la visite de la page si on ne fait pas attention. Seules 
les performances s'en ressortent puisqu'il faut télécharger 
la page d'erreur en lieu et place du composant initial, inutilement. 
Parfois ces pages d'erreur dépassent 100 Ko. 

**Recommandation** : Vérifiez la présence de références vers 
des composants ayant changé d'adresse ou ayant été supprimés 
(ou liés avec la mauvaise adresse) et corrigez les. 

Vous pouvez vérifier la présence ou l'absence de liens cassés 
avec webpagetest.org ou Firebug. Les pages d'erreur 404 apparaitront 
en rouge. 

Un des premiers composants référencés sur lemonde.fr est une 
erreur 404 

À retenir
---------

* Le nombre de requêtes HTTP est l'un des principaux facteurs 
  de ralentissement des pages web ; 

* Regroupez les composants javascript et les feuilles de style 
  en un seul fichier ; 

* Réalisez des sprites CSS pour charger plusieurs images en 
  une fois ; 

* Chargez les petites images présentes sur toutes les pages 
  par un système de téléchargement par lot comme les liens en 
  data: ; 

* Évitez les redirections HTTP ; 

* Profitez des nouvelles fonctionnalités CSS 3 pour remplacer 
  certaines images ; 

* Faites attention aux composants chargés inutilement plusieurs 
  fois.

Contenus plus petits
====================

Les deux chapitres précédents ont permis de travailler avec 
moins de requêtes HTTP. L'étape suivante est de diminuer le poids 
des composants ou des lots de composants qui restent à télécharger. 
Le poids moyen d'une page était de 500 Ko en 2009 mais la plupart 
des pages riches dépassent désormais facilement ce palier. 

Compression HTTP
----------------

La première solution pour diminuer le poids des composants à 
télécharger est presque magique. Elle permet d'obtenir des 
gains de l'ordre de 70 % sur le code HTML, CSS ou javascript et prend 
tout au plus quelques heures à mettre en place. 

### Gain en performance

Diviser par quatre le volume des contenus textuels c'est presque 
50 % de gagnés sur la totalité du site. Pour le visiteur le gain 
est immédiatement visible. Netflix a pu constater des améliorations 
de l'ordre de 25 % sur le temps de chargement complet de ses pages 
web après activation de la compression. 

Outre le gain de performance dû au temps de téléchargement, il 
y a aussi pour vous un gain financier immédiat, dû à la bande passante 
utilisée. Si vous divisez par deux le volume échangé par votre 
serveur, vous divisez aussi par deux les coûts associés. L'outil 
[http://www.port80software.com/tools/bandwidthcalchz.asp](http://www.port80software.com/tools/bandwidthcalchz.asp) 
vous permet de calculer immédiatement les sommes épargnées 
annuellement. 

Le gain en performance vient aussi avec son coût. La compression 
par le serveur web et la décompression par le navigateur nécessitent 
toutes les deux du temps processeur. Sur le navigateur on considère 
habituellement que le coût est nul. La décompression est une 
tâche très aisée et le gain dépasse très largement les quelques 
millisecondes d'occupation processeur. Côté serveur la compression 
n'est pas à coût nul, et prendra quelque chose comme 5 % du processeur. 
C'est cependant probablement négligeable au regard des gains. 

**Recommandation** : Activer la compression HTTP sur votre 
serveur web, au moins pour les contenus statiques à base de texte 
(CSS et javascript par exemple). 

### Fonctionnement

Le protocole HTTP qui assure les échanges entre le navigateur 
et le serveur web a déjà prévu la possibilité de compresser les 
contenus avant envoi. 

#### Algorithmes

Les algorithmes utilisés sont extrêmement proches du très courant 
zip que vous utilisez pour les documents de votre poste de travail. 
Il s'agit de gzip (RFC 1951 et 1952), deflate (RFC 1951 et 1950) 
et compress. En fait compress est quasiment inutilisé dans les 
échanges web et les deux autres sont à peu près équivalents, gzip 
n'est qu'une variation de deflate avec des méta-données. 

#### Auto-négociation

1. Le navigateur commence par envoyer une entête particulière 
   quand il envoie sa requête. Il s'agit de l'entête `Accept-Encoding`. 
   Le plus souvent la valeur est « gzip, deflate », qui signale 
   que le navigateur supporte ces deux algorithmes. 

~~~~~~~ {.http .request}
GET / HTTP/1.1
Accept: image/gif, image/jpeg, application/x-shockwave-flash, */*
Accept-Language: fr
UA-CPU: x86
Accept-Encoding: gzip, deflate
User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)
Host: www.yahoo.fr
Connection: Keep-Alive
~~~~~~~

2. Le serveur repère cette entête et peut choisir d'utiliser 
   un de ces deux algorithmes pour compresser le contenu juste 
   avant de l'envoyer. Si c'est le cas il ajoute alors l'entête 
   `Content-Encoding` pour informer le navigateur qu'il a 
   effectivement compressé les données et avec quel algorithme. 

~~~~~~~ {.http .response}
HTTP/1.1 200 OK
Date: Mon, 30 Aug 2010 16:21:10 GMT
Cache-Control: private
Vary: Accept-Encoding
Content-Type: text/html;charset=utf-8
Content-Encoding: gzip
Age: 0
Transfer-Encoding: chunked
Connection: keep-alive

<!DOCTYPE......
~~~~~~~

3. Enfin, le navigateur télécharge le contenu et le décompresse 
   avant de l'utiliser. 

Tout est géré par le navigateur et le serveur web. Si l'un des deux 
ne supporte pas ou ne veut pas de la compression alors il n'y aura 
pas mention de gzip ou de deflate dans les entêtes HTTP et le contenu 
sera envoyé décompressé. Le mécanisme ou sa présence est totalement 
transparent pour l'utilisateur. Seuls les temps de téléchargements 
réduits peuvent permettre de faire la différence. 

#### Interactions avec les caches

Dans l'exemple ci-avant le serveur a aussi ajouté `Accept-Encoding` 
dans l'entête `Vary`. Cela permet d'informer les différents 
que la réponse, compressée, n'est valable que pour les futures 
requêtes qui ont la même entête Accept-Encoding, donc qui ont 
le même support de compression. Si ce n'est pas le cas, la réponse 
ne devra pas être réutilisée (on risquerait d'envoyer une réponse 
compressée à un navigateur ne la supportant pas) et la requête 
doit être de nouveau réalisée. 

### Mise en place

La mise en place de la compression HTTP est généralement l'affaire 
de quelques lignes dans la configuration du serveur web. Quasiment 
tous ont un module intégré ou une extension pour gérer la compression : 

#### Apache 1.3

Apache 1.3 utilise le mod_gzip. Ce dernier n'est pas fourni avec 
le serveur web mais la plupart des distributions Linux le proposent 
dans les paquets disponibles. 

* [http://www.schroepl.net/projekte/mod_gzip/](http://www.schroepl.net/projekte/mod_gzip/) 

Pour une configuration minimale il suffit d'activer le module 
et d'ajouter quelques lignes dans la configuration Apache : 

~~~~~~~ {.apache .partial}
LoadModule gzip_module modules/mod_gzip.so
<IfModule mod_gzip.c>
  mod_gzip_on  Yes
  mod_gzip_item_include mime ^text/
  mod_gzip_send_vary On
</IfModule>
~~~~~~~

Par défaut les fichiers sont recompressés à chaque accès. En 
spécifiant un répertoire temporaire, on pourra mettre en cache 
les fichiers statiques compressés pour éviter de faire plusieurs 
fois le même travail : 

~~~~~~~ {.oneline .apache .partial}
mod_gzip_temp_dir /tmp
~~~~~~~

Il est aussi possible de pré-compresser les contenus statiques 
et de demander à mod_gzip de les utiliser au lieu de les compresser 
lui-même. Ici on utilise l'extension `.gz` : 

~~~~~~~ {.apache .partial}
mod_gzip_can_negotiate Yes
mod_gzip_static_suffix .gz
AddEncoding gzip .gz
mod_gzip_update_static Yes
~~~~~~~

Si le fichier compressé est plus vieux que le fichier original, 
c'est ce dernier qui est utilisé par Apache, pour éviter les erreurs 
de mises à jour. La dernière des quatre directives précédentes 
permet de mettre à jour les fichiers compressés automatiquement 
si nécessaire. 

Afin d'éviter d'occuper Apache inutilement on définit généralement 
une taille minimale au-deçà de laquelle il n'est pas utile de 
compresser. On peut aisément définir ce minimum à 1 Ko étant donné 
qu'un seul paquet TCP/IP. On définit la taille maximum des fichiers 
à compresser (pour ne pas tenter de compresser un fichier trop 
important en volume) et l'occupation maximum en mémoire (au-delà 
les fichiers sont compressés dans un répertoire temporaire) : 

~~~~~~~ {.apache .partial}
mod_gzip_minimum_file_size    1000
mod_gzip_maximum_file_size    500000
mod_gzip_maximum_inmem_size   600000
~~~~~~~

Enfin, on peut inclure ou exclure des contenus en fonction de 
leur nom de fichier, de leur type mime, ou du module responsable 
de leur gestion dans Apache. Le dernier paramètre de ces directives 
est toujours une expression rationnelle : 

~~~~~~~ {.apache .partial}
mod_gzip_item_include file \.html$
mod_gzip_item_include file \.css$
mod_gzip_item_include file \.js$
mod_gzip_item_include file \.xml$
mod_gzip_item_include file \.svg$
mod_gzip_item_exclude file \.mhtml$
mod_gzip_item_exclude handler    ^cgi-script$
mod_gzip_item_include mime ^text/
mod_gzip_item_include mime ^application/.*xml$
mod_gzip_item_exclude mime ^image/
~~~~~~~

#### Apache 2.x

Contrairement à Apache 1.3, Apache 2.x a un module officiel pour 
la gestion de la compression HTTP, le mod_deflate. Il n'est malheureusement 
pas activé par défaut et c'est à vous de le faire. 

* [http://httpd.apache.org/docs/2.0/mod/mod_deflate.html](http://httpd.apache.org/docs/2.0/mod/mod_deflate.html) 

Une fois le module chargé il suffit de déclarer le filtre « deflate » 
pour tous les types de fichiers souhaités : 

~~~~~~~ {.apache .partial}
AddOutputFilterByType DEFLATE text/html text/plain text/xml 
AddOutputFilterByType DEFLATE text/javascript application/javascript
AddOutputFilterByType DEFLATE text/css application/x-javascript
AddOutputFilterByType DEFLATE application/xml application/xhtml+xml
AddOutputFilterByType DEFLATE image/svg+xml application/rss+xml
~~~~~~~

Il est possible de choisir le niveau de compression (de 1 le plus 
faible à 9 le plus efficace) pour les contenus. Une compression 
faible sera suffisante et occupera bien moins le processeur. 
Il n'est pas recommandé d'essayer les niveaux les plus élevés 
: 

~~~~~~~ {.oneline .apache .partial}
DeflateCompressionLevel 3
~~~~~~~

#### Lighttpd

Pour Lighttpd c'est le module mod_compress qui s'occupe de la 
compression des contenus statiques. Un module mod_deflate 
existe aussi pour les versions les plus récentes de Lighttpd 
mais il ne permet pas le cache des contenus compressés. 

* [http://redmine.lighttpd.net/wiki/1/Docs:ModCompress](http://redmine.lighttpd.net/wiki/1/Docs:ModCompress) 

La liste des contenus à compresser est fixée par la directive 
`compress.filtype` : 

~~~~~~~ {.oneline .lighttpd .partial}
compress.filetype  = ("text/plain", "text/html")
~~~~~~~

Les contenus sont par défaut compressés à chaque requête. Pour 
éviter de recompresser des milliers de fois le même fichier statique 
Lighttpd nous permet de spécifier un répertoire où seront mis 
en cache les résultats de compression : 

~~~~~~~ {.oneline .lighttpd .partial}
compress.cache-dir = "/var/www/cache/"
~~~~~~~

Enfin, une dernière directive permet de désactiver la compression 
pour les fichiers trop volumineux. Le module contient une limite 
haute native de 128 Mo mais il est probablement une bonne idée 
de la réduire drastiquement : 

~~~~~~~ {.oneline .lighttpd .partial}
compress.max-filesize = 1000 # en Ko
~~~~~~~

#### IIS

Le module de compression est livré par défaut avec IIS. Il vous 
suffit de vous rendre dans l'outil de gestion IIS, dans l'onglet 
« services » des propriétés du site web. 

* [http://www.microsoft.com/technet/prodtechnol/WindowsServer2003/Library/IIS/25d2170b-09c0-45fd-8da4-898cf9a7d568.mspx](http://www.microsoft.com/technet/prodtechnol/WindowsServer2003/Library/IIS/25d2170b-09c0-45fd-8da4-898cf9a7d568.mspx) 

Il est aussi possible d'activer la compression via deux lignes 
de commande : 

~~~~~~~ {.oneline .wshell}
cscript adsutil.vbs set w3svc/filters/compression/parameters/HcDoStaticCompression true
~~~~~~~

~~~~~~~ {.oneline .wshell}
cscript adsutil.vbs set w3svc/filters/compression/parameters/HcDoDynamicCompression true
~~~~~~~

Si vous activez la compression des contenus statiques (ce qui 
est le minimum recommandé), vous pouvez spécifier un répertoire 
où seront stockés les fichiers compressés. Cela évitera de les 
recompresser si c'est déjà fait. L'adresse de ce répertoire 
est déterminée par la directive metabase `HcCompressionDirectory`. 

La liste des extensions des contenus à compresser est réglée 
par les directives metabase `HcFileExtensions` (contenus 
statiques) et `HcScriptFileExtensions` (contenus dynamiques). 
De même il est possible de régler séparément le niveau de compression 
pour les contenus statiques (`HcOnDemandCompLevel`, 10 par 
défaut) et dynamiques (`HcDynamicCompressionLevel`, 0 par 
défaut, valeur minimum). 

Attention ! Lorsque vous activez la compression les valeurs 
par défaut des entêtes de cache (paramètres metabase `HcExpiresHeader`, 
`HcCacheControlHeader`, et `HcSendCacheHeaders`) sont réglés 
pour désactiver tout cache par les proxy et les navigateurs. 

Pour faciliter, il existe des modules tierces qui permettent 
de gérer tout cela plus simplement. C'est par exemple le cas de 
IIS Accelerator : 

* [http://www.vigos.com/products/iis-accelerator/](http://www.vigos.com/products/iis-accelerator/) 

#### Nginx

Dans Nginx la compression est gérée par le module HttpGzip. On 
spécifie alors trois lignes, respectivement pour activer la 
compression, définir les types des contenus à compresser, et 
la taille minimale d'un fichier pour le prendre en compte : 

~~~~~~~ {.nginx .partial}
gzip             on;
gzip_types       text/plain application/xml text/css;
gzip_min_length  1000;
~~~~~~~

Par défaut la compression est faite à chaque requête. Le module 
HttpGzipStatic permet d'utiliser des versions précompressées 
sur disque. Dans ce cas Nginx recherche un fichier de même nom avec 
en plus le suffixe « .gz ». On active ce support avec une simple 
ligne de configuration : 

~~~~~~~ {.oneline .nginx .partial}
gzip_static      on;
~~~~~~~

#### Varnish
Depuis Varnish 3.0, la compression gzip est nativement supportée et activée par défaut.

Lorsqu'il reçoit un requête, Varnish vérifie si le client supporte gzip, d'après l'entête Accept-Encoding.
Si le client la supporte, il va forcer la valeur de Accept-Encoding à "gzip".

Lorsque Varnish va demander le contenu au backend, il forcera Accept-Encoding à "gzip", de sorte que si le backend renvoi du contenu gzipé, Varnish le gardera en cache sous cette forme compressé.
Si le backend renvoi du texte non compressé, Varnish le conservera non compressé.

Pour économiser de la place dans le cache, il est possible de contraindre Varnish à compresser la réponse du backend :

~~~~~~~
sub vcl_fetch {
    # si le contenu est de type texte (html, css, js, etc ...)
    if (beresp.http.content-type ~ "text") {
        # on force la compression avant la mise en cache
        set beresp.do_gzip = true;
    }
}
~~~~~~~

Note de la documentation : En règle générale, Varnish utilise peu le CPU, donc il peut être judicieux de lui déléguer la compression des contenus textes car il aura moins tendance à saturer le CPU que les autres serveurs web (notamment Apache).

Documentation officielle de Varnish sur la compression : [https://www.varnish-cache.org/docs/trunk/users-guide/compression.html](https://www.varnish-cache.org/docs/trunk/users-guide/compression.html)


#### Contenus dynamiques

Si Apache permet de compresser les contenus dynamiques, on se 
contente normalement de ne gérer que les contenus statiques. 
C'est alors à vous de gérer les contenus dynamiques (PHP, Ruby, 
Java, Python) en ajoutant un filtre là où c'est pertinent. Le 
faire manuellement vous permet de gérer le cache et son expiration 
précisément, ce que le serveur web ferait avec difficulté pour 
un contenu dynamique. 

Pour PHP il suffit de modifier le fichier de configuration php.ini 
et d'ajouter ou d'activer la directive `lib.output_compression`. 
Si on spécifie une valeur numérique, elle correspondra à la taille 
du tampon de sortie qui permet de compresser au fil de l'eau au 
lieu de gérer tout le contenu en mémoire (4 Ko par défaut). Il est 
aussi possible (mais moins recommandé) de l'activer au besoin 
lors de l'exécution à l'aide de `ob_start("ob_gzhandler")`. 

~~~~~~~ {.online .phpconf .ini}
zlib.output_compression = On 
~~~~~~~

Pour Django il faut simplement ajouter le module gzip dans le 
middleware : 

~~~~~~~ {.django .partial}
MIDDLEWARE_CLASSES = (
      'django.middleware.gzip.GZipMiddleware',
      …
)
~~~~~~~

#### Solutions tierces

Enfin, il est possible de désactiver totalement la compression 
HTTP sur vos serveurs applicatifs et dans les applications elles-même. 
C'est alors à un boitier qui se branche entre votre serveur web 
et Internet qu'est dévolue la tâche de réaliser la compression. 

Il existe de nombreux boitiers qui réalisent cette opération, 
et bien d'autres en même temps (gestion de la sécurité ssl, recompression 
des images, proxy cache, etc.). On peut par exemple citer ActivNetworks 
en France. 

Vous pouvez cependant faire vous-même un montage similaire 
avec serveur en proxy inverse qui ajoute un filtre de compression 
avant de renvoyer les résultats. Apache, Nginx, Varnish ou Squid avec 
le module ecap-gzip ([http://code.google.com/p/squid-ecap-gzip/](http://code.google.com/p/squid-ecap-gzip/)) 
conviennent parfaitement. 

### Quoi compresser

#### Formats de fichier

Les algorithmes de compression utilisés sont extrêmement performants 
pour les fichiers textes classiques. C'est le cas des fichiers 
HTML, CSS, javascript, XML, etc. Pour ces fichiers, même un niveau 
de compression minimum obtient de très bons résultats. 

Côté web on peut identifier les types mimes suivants : 

* text/plain, 

* text/html, 

* application/xhtml+xml, 

* text/css, 

* text/javascript, 

* application/x-javascript, 

* application/javascript, 

* application/json, 

* text/xml, 

* application/xml, 

* application/rdf+xml, 

* application/rss+xml, 

* application/atom+xml. 

On peut normalement sans trop de risques ajouter tous les types 
text/* et probablement tous les types application/*+xml. On 
devrait aussi pouvoir compresser efficacement les images svg 
(image/svg+xml) mais ces dernières peuvent déjà être compressées 
(fichiers en .svgz) sans que cela ne se voit sur le type mime. 

Les images (.bmp exclus mais on n'en trouve normalement pas sur 
le web), vidéos et musiques sont des formats déjà compressés 
avec des algorithmes dédiés. Tenter de les recompresser n'amènerait 
aucun gain et occuperait votre processeur pour rien. 

Les fichiers bureautiques récents sont déjà compressés avec 
un algorithme similaire à gzip. C'est le cas des fichiers OpenOffice 
(odt, ods, odp, odg...) et des fichiers Microsoft Office récents 
(xlsm, xlsx, docx, pptx). Ils sont à exclure de la compression 
HTTP. 

Les fichiers bureautiques plus anciens (xls, doc, ppt) et les 
PDF, eux, peuvent souvent diminuer après un passage par gzip. 
Le gain est toutefois bien plus faible que sur les fichiers textes 
aussi c'est à vous de vérifier si échanger quelques cycles processeur 
vaut le coup. 

Pour les polices de caractères, les polices eot, otf, et ttf peuvent 
souvent être compressées. Les woff le sont cependant déjà, et 
doivent donc être exclues pour ne pas ralentir inutilement vos 
serveurs. 

#### Types de contenu

En plus de la différenciation suivant le format du fichier, on 
fait habituellement une différence suivant le type de contenu, 
statique ou dynamique. Vous pouvez activer la compression des 
contenus statiques sans avoir de doute. Le cache du serveur web 
permettra de limiter la pénalité processeur tout en réduisant 
le volume de données échanger. C'est un des meilleurs retours 
sur investissement en terme de performance en raison du faible 
investissement humain nécessaire pour la mise en œuvre. 

Pour les contenus dynamiques, il s'agit de faire un compromis 
entre le processeur du serveur et le gain en volume parce que le 
résultat de la compression ne pourra pas être mis en cache. La 
plupart des sites à très fort trafic choisissent de désactiver 
la compression HTTP des contenus dynamiques, c'est à dire de 
la page HTML initiale et des éventuelles données JSON échangées 
à l'aide de requêtes AJAX. 

Si vous pouvez le permettre, tentez d'activer la compression 
des contenus dynamiques, mais ne luttez pas inutilement si vous 
dégradez les performances de votre serveur, il y a d'autres recommandations 
utiles dans ce livre. 

### Support des navigateurs

Tous les navigateurs récents supportent la compression HTTP 
avec gzip ou deflate, et ce depuis Microsoft Internet Explorer 
4 et Netscape 4. Seuls restent quelques mauvais navigateurs 
mobiles comme certains Blackberry. Ce n'est toutefois pas un 
problème grâce au mécanisme d'auto-négociation : Si un navigateur 
ne supporte pas la fonctionnalité il n'annonce pas le support 
de gzip ou de deflate dans les entêtes HTTP et le serveur web ne 
déclenchera pas la compression. 

Les seuls problèmes viennent des navigateurs qui annoncent 
supporter la compression mais qui ont des défauts dans leur gestion. 
C'était le cas pour les images dans Netscape 4 par exemple, mais 
comme il est de toutes façons conseillé de désactiver la compression 
HTTP des images, cela n'entraîne pas d'incompatibilités. Microsoft 
Internet Explorer 4 et 5 ont aussi un défaut qui désactive le cache 
quand ils reçoivent un contenu compressé mais vu la faible part 
de marché et que le visiteur obtient quand même la page correctement, 
c'est un défaut souvent jugé comme négligeable. 

#### Support non déclaré

La surprise vient en fait du nombre de visiteurs qui déclarent 
ne pas supporter la compression. Ils peuvent représenter jusqu'à 
15 % des visites uniques. Il s'agit en fait de deux catégories 
bien identifiées : 

* les utilisateurs avec des proxy, anti-virus, anti-malware, 
  outils de respect de la vie privée, et autres logiciels intermédiaires 
  mal configurés ; 

* les utilisateurs de Microsoft Internet Explorer 6 derrière 
  un proxy (un défaut fait qu'il ne déclare pas toujours le support 
  de la compression dans ce cas). 

Majoritairement il s'agit d'utilisateurs en entreprise, fortement 
bridés, avec Microsoft Internet Explorer 6, et un proxy d'entreprise 
trop agressif.

Il devrait être possible d'envoyer quand même du contenu compressé 
aux seconds, et probablement à la majorité des premiers, mais 
il n'y a malheureusement pas de solution efficace pour les détecter 
simplement dans le serveur web. 

Google a développé un test applicatif à base de cookie, javascript 
et iframe : Si le navigateur ne déclare pas supporter la compression 
HTTP et qu'aucun cookie au nom de « gzip » n'est présent, on insère 
ce cookie avec une valeur 0 et on ajoute une iframe spécifique. 
Cette iframe est envoyée compressée quelle que soit la déclaration 
du navigateur, et exclue de tout cache (`Cache-Control: no-cache`). 
Google a aussi remarqué qu'il était très important que l'adresse 
de la page présente l'extension « .html » pour certains proxys 
dans le cadre de ce test. Un javascript dans l'iframe modifie 
le cookie pour y mettre la valeur 1. Si la page compressée est correctement 
décodée le cookie passe donc à 1, sinon il reste à 0. Par la suite 
on peut activer la compression pour les navigateurs qui ont ce 
cookie à 1, même s'ils ne déclarent pas le support de la compression 
HTTP. 

D'autres considèrent que seuls les scripts réalisés à la main 
ne supportent pas toujours la compression mais que c'est toujours 
le cas pour les navigateurs réels et activent la compression 
pour les navigateurs connus (en se basant sur l'entête User-Agent), 
y compris quand ils n'en déclarent pas le support. Cette démarche 
est toutefois déconseillée car elle laissera forcément quelques 
visiteurs sur le bord de la route, même s'ils seront rares. 

Optimisation des images
-----------------------

La compression HTTP permet de diminuer le volume des données 
textuelles. Comme nous l'avons vu ça ne concerne pas les images, 
qui sont déjà compressées. Ces dernières représentent de 30 
% à 80 % du poids total d'une page web. La moindre optimisation 
impactera donc directement le temps de chargement et les performances 
du site. 

Heureusement tout n'est pas perdu. Nous allons voir que nous 
avons de multiples leviers pour optimiser le poids des images : 

### Format des images

Il existe quatre formats d'images courants sur le web : JPEG, 
SVG, GIF et PNG. 

Le jpeg est un format adapté pour les photos, avec des millions 
de couleurs différentes et des zones où on a toute une variation 
autour d'un même jeu de couleurs (pensez aux couleurs chair d'un 
visage). Lors de l'enregistrement d'un fichier jpeg le logiciel 
va supprimer certains détails peu importants dans le cadre d'une 
photo (il est peu important que le pixel soit exactement de la 
bonne couleur, tant que le ton, la saturation et la luminance 
sont proches). Si on utilise des réglages trop agressifs ou une 
image autre qu'une photo (par exemple un logo avec du texte) on 
verra très rapidement la dégradation en qualité. 

Le svg est un format un peu à part. Il n'enregistre pas la couleur 
de chaque pixel mais directement les formes (courbe, ligne, 
rectangle, cercle, etc.), sous forme XML. On parle de vecteurs, 
et d'image vectoriel. Il est impossible de représenter une photo 
ainsi mais en échange les logos ou les images créées à partir de 
formes et de courbes peuvent être représentés avec une grande 
fidélité quelle que soit la taille de l'image. On peut zoomer 
à l'infini sans jamais voir de pixelisation. Comme il s'agit 
d'un fichier qui contient du texte (du code XML), il est possible 
de le compresser avec zip. On lui donne alors généralement l'extension 
.svgz. 

Les formats gif et png sont deux formats concurrents ayant le 
même but. À l'inverse de jpeg le format ne dégrade en rien l'image, 
qui reste fidèle au pixel près à celle d'origine. En conséquence 
ils sont forcément bien plus volumineux que les jpeg pour représenter 
de simples photos. Leur domaine de prédilection ce sont les logos, 
les textes et les petits éléments d'interface graphique où il 
est important que les couleurs ou les délimitations soient fidèles. 
Ils arrivent aussi très bien à compresser les dégradés. 

#### Choisir le bon type d'image

Choisir le bon type d'image est un premier pas qui permet d'éviter 
des images inutilement volumineuses. 

![Exemples d'image à enregistrer en jpeg (à gauche) et en png (à droite)](img/chap05-exemples-dimage-a-enregistrer-en-jpeg-a-gauche-et-en-png-a-droite.png)

Tout d'abord, tout ce qui est dès le départ réalisé en mode vectoriel 
devrait rester dans le format svg. Quand cela est possible les 
graphiques statistiques (en barres, en camenbert, en courbes) 
sont aussi de très bons candidats au svg. 

Ensuite, les logos, les visuels contenant du texte à lire, et 
les petits éléments d'interface (souvent de gros aplats d'une 
seule couleur ou des dégradés, avec des couleurs qui doivent 
être exactes sous peine de provoquer un rendu disgracieux) devraient 
être en png. C'est aussi le seul format non vectoriel fiable sur 
le web pour avoir de la transparence. Il s'agit généralement 
de petites images, souvent avec peu de couleurs. 

Enfin, et il ne devrait rester plus que ça, tout ce qui est créé 
à partir de photos ou de dessins doit être réalisé à l'aide d'images 
jpeg. 

**Recommandation** : Vérifier que toutes les images créées 
à partir de formes vectorielles qui le peuvent sont sauvegardées 
en svg ; que les images de logo, contenant du texte ou des gros aplats 
de couleurs unie sont en png ; et que les photos et dessins sont 
en jpeg. 

**Notes** : Il est possible d'obtenir une transparence avec alpha 
sur des png 8 bits, ce qui permet de beaux dégradés sans passer par
le format 24 bits beaucoup lourd. Le second avantage est que IE6 
gère alors correctement (miracle) la transparence : les pixels ayant
un alpha de moins de 50% sont bien transparents et non bleus clairs.
Par contre, peu de logiciels de graphisme proposent cette option 
(ex: Fireworks, Pngquant, mais pas Photoshop CS6).

#### Gif ou png

Vous l'avez peut être remarqué, je n'ai rien recommandé pour 
gif. Gif et png sont des formats concurrents avec les mêmes objectifs. 
Gif est un peu plus ancien, limité à 256 couleurs, et a longtemps 
posé des problèmes de brevet. Png est plus récent, permet de se 
limiter à 256 couleurs mais aussi d'étendre à des millions de 
couleurs si l'on souhaite, et sait gérer une opacité variables 
sur chaque pixel (là où gif ne sait faire que de la couleur pleine 
ou du transparent total). 

Si je n'ai rien recommandé pour gif, c'est que png sait faire tout 
ce que sait faire gif, en propose encore plus, et obtient des fichiers 
moins volumineux. Le plus souvent sauvegarder en png un ancien 
fichier gif permet un gain non négligeable en poids (de l'ordre 
de 10 à 30 %). Il arrive parfois que ce soit l'inverse, mais dans 
ce cas la différence est de quelques octets et c'est souvent sur 
les toutes petites images où ça ne fait pas grande différence. 

Vous pouvez sans craintes passer tous vos gif en png et vous serez 
sûrs d'y gagner. Si vous souhaitez le faire en masse, l'outil 
imagemagick permet d'automatiser cela. 

**Recommandation** : Convertir toute les images gif non animées 
en fichiers png. 

En fait il reste un cas où gif peut être adapté, c'est pour les images 
animées. Originellement c'est quelque chose que png ne supporte 
pas, et donc où le format gif reste préconisé. Mozilla a toutefois 
proposé une évolution du png, le pnga, qui permet les animations 
(et qui reste compatible avec les anciens navigateurs, ces derniers 
afficheront simplement l'image non animée). 

### Recompression

Tout d'abord le plus simple pour gagner en poids sur une donnée 
compressée est de la recompresser, avec un meilleur algorithme 
ou des réglages plus agressifs. C'est ce que nous ferons en premier 
pour certaines de nos images. 

Si la solution paraît naïve il s'avère que les gains sont réels. 
Les outils de traitements d'image comme Adobe Photoshop utilisent 
en réalité des algorithmes ou des paramètres peu optimisés pour 
une diffusion web. En repassant derrière on peut gagner de 10 
à 50 % suivant les images. Comme les images représentent plus 
d'un tiers du poids total d'un site, une simple optimisation 
des images peut permettre de retirer 10 % du volume de téléchargement 
total. 

Cette possibilité de recompression est offerte avec les formats 
sans pertes, c'est à dire PNG, GIF et SVG. Sur ces formats l'images 
est exactement la même avant et après sa compression, au pixel 
près. Recompresser l'image se fait donc sans aucun compromis 
de qualité. On ne touchera par contre pas aux fichiers jpeg, qui 
eux subissent une dégradation à chaque compression, et donc 
qui subiraient une perte de qualité. 

#### Recompresser les png

Recompresser un png est l'histoire d'une ligne de commande. 
De nombreux outils existent pour automatiser cette tâche. Les 
plus connus sont pngcrush et optipng. Il existe aussi un service 
web, punnypng, qui permet d'optimiser ses images via une API. 
Ce dernier offre plus que la simple recompression mais impose 
d'être dépendant d'un service externe. 

* [http://pmt.sourceforge.net/pngcrush/](http://pmt.sourceforge.net/pngcrush/) 

* [http://optipng.sourceforge.net/](http://optipng.sourceforge.net/) 

* [http://www.punypng.com/](http://www.punypng.com/) 

Ces outils sont très intéressants car ils peuvent faire partie 
de votre processus de mise en production : Lorsque les fichiers 
sont packagés pour être mis en production un script passe toutes 
les images par pngcrush ou optipng pour les optimiser. 

Il n'est pas forcément utile d'utiliser les réglages les plus 
agressifs de ces outils. Les gains supplémentaires ne sont que 
de quelques octets et le temps nécessaire passe de quelque secondes 
à plusieurs minutes, voire dizaines de minutes. Le réglage par 
défaut obtient déjà de très bons résultats et offre le meilleur 
rapport efficacité/temps. 

**Recommandation** : Recompresser toutes les images png avec 
optipng. 

S'il s'agit de simplement vérifier l'état d'une page ou de faire 
des essais, vous pouvez aussi simplement utiliser les extensions 
Yslow (qui embarque l'outil en ligne smushit) et Google Page 
Speed. Les deux vous permettent d'analyser toutes les images 
d'une page de Firefox et de récupérer les images optimisées avec 
des statistiques sur les gains réalisés. 

* [http://developer.yahoo.com/yslow/](http://developer.yahoo.com/yslow/) 

* [http://code.google.com/intl/fr/speed/page-speed/](http://code.google.com/intl/fr/speed/page-speed/) 

Si seules les statistiques vous intéressent, web page test permet 
d'analyser une URL et vous liste les gains possibles sur chaque 
image, avec un cumul à la fin. 

* [http://www.webpagetest.org/](http://www.webpagetest.org/) 

Il peut être intéressant de noter ou Yslow, Page Speed, web page 
test, et même punnypng utilisent tous pngcrush ou optipng en 
interne. Seules les options et les traitements annexes changent. 
Les gains sont parfois différents mais généralement du même 
ordre de grandeur. Seul punnypng va parfois un peu plus loin suite 
aux différentes actions qu'il réalise en plus. 

#### Recompresser les svg

Les images svg se compressent et décompressent avec n'importe 
quel outil de gzip. Si elles ne sont pas compressées il suffit 
de les faire passer par gzip et d'en changer l'extension pour 
.svgz. Si elles sont déjà compressées vous pouver les décompresser 
pour relancer l'opération avec des paramètres plus agressifs. 

Si vous avez suivi les recommandations de compression HTTP précédentes, 
votre serveur web utilise de toutes façons gzip pour vos fichiers 
svg. Il n'est donc pas forcément important de réaliser ces recompressions 
à la main.

### Méta-données

Le troisième levier pour l'optimisation des images est celui 
de la suppression des méta-données. Les fichiers d'images contiennent 
en effet bien plus que les données graphiques. On y trouve des 
descriptions, le nom des logiciels utilisés, des informations 
sur les profils de couleurs du poste de travail d'origine, parfois 
des données de localisation, des informations sur l'appareil 
qui a servi à la prise de vue dans le cas d'une photo, des dates de 
modification, de création, et en cherchant bien on trouve même 
des photos avec une capture sonore embarquée. 

Tous les formats de fichiers sont ici concernés, même si les méta-données 
présentes dans chacun diffèrent. 

#### Png

Pour png les mêmes outils que précédemment font aussi le travail : 
pngcrush, optipng, et punnypng. Il n'y a donc pas besoin d'une 
opération supplémentaire si vous avez déjà exécuté l'un d'eux. 

#### Jpeg

Pour jpeg on peut utiliser jpegtran, jhead ou jpegoptim pour 
les supprimer. Aucun de ces outils ne touche à l'image elle-même, 
donc il n'y a aucun risque de dégradation de la qualité. 

* [http://sylvana.net/jpegcrop/jpegtran/](http://sylvana.net/jpegcrop/jpegtran/) 

* [http://www.sentex.net/~mwandel/jhead/](http://www.sentex.net/~mwandel/jhead/) 

* [http://freshmeat.net/projects/jpegoptim](http://freshmeat.net/projects/jpegoptim) 

**Recommandation** : Retirer les méta-données des images jpeg 
à l'aide de jpegtran. 

#### Copyright, licences

Lorsque le premier outil en ligne d'optimisation d'image a été 
publié, il y a eu une levée de boucliers contre la suppression 
des méta-données. Les associations d'auteurs et de photographes 
se sont plaints que ces opérations modifiaient les images et 
de plus retiraient potentiellement les mentions d'auteur, 
de copyright, ou de licence d'utilisation. 

Si vous cherchez à supprimer des données d'une image, assurez-vous 
en effet d'en avoir le droit, surtout si vous publiez ensuite 
cette image sur Internet. Vérifiez que vous en êtes l'auteur, 
ou que les conditions d'utilisations de l'image et le contexte 
légal vous en donnent le droit. Si l'image appartient à un tiers, 
il est probable que vous soyez obligés d'au moins laisser les 
mentions de copyright et le nom de l'auteur, peut être plus. 

### Qualité de l'image

Jusqu'à présent, en parlant format, compression ou méta-données, 
nous parlions de réduire le poids des images à qualité fixe. La 
qualité fixe est importante pour convaincre les graphistes 
et toute la chaîne décisionnelle qu'un site qui répond rapidement 
n'est pas forcément un site moche. La qualité graphique est aussi 
nécessaire, comme un moyen de porter l'information et d'accompagner 
le visiteur. 

Il est cependant aussi important de réfléchir à cette qualité 
graphique, de la discuter. Est-ce vraiment nécessaire que le 
bandeau de haut de page soit dans une si bonne définition ? Vaut-il 
mieux une superbe image qui mette une seconde à charger ou une 
image de moindre qualité qui soit instantanée ? 

Il n'y a certainement pas de réponse toute faite et je serai le 
dernier à vous demander de trancher systématiquement en faveur 
des performances. À l'inverse, si vous ne vous posez pas ces questions 
sérieusement et si la qualité graphique n'est jamais remise 
en question, alors vous avez un biais opposé, qui n'est pas meilleur. 

Il y a deux moyens d'action pour jouer sur la qualité d'une image 
web : la palette de couleurs, et la force de compression jpeg. 

#### Palette de couleurs

La palette de couleurs c'est simplement l'index de toutes les 
couleurs utilisées dans votre image. S'il y a seulement 256 couleurs, 
alors on peut coder chaque pixel sur un seul octet. Si on utilise 
toute l'étendue de la gamme informatique alors non seulement 
il en faut le triple mais en plus la faible répétition des couleurs 
permettra d'obtenir une bien moins bonne compression au final. 

C'est cette palette de couleurs qui fait que les images gif semblent 
toujours dégradées. Le format ne permet que 256 couleurs. Il 
faut donc modifier l'image et convertir certains pixels dans 
une couleur approchante. L'image finale a subit une détérioration 
avant sa sauvegarde. 

Quand vous construisez une image png, de combien de couleurs 
avez-vous besoin ? Une première étape est de vérifier que vous 
utilisez effectivement plus de 256 couleurs. Sinon, vous pouvez 
demander à votre logiciel d'édition d'images de réduire sa palette 
au nombre réel de couleurs. Étrangement, la plupart de ces logiciels 
ne font pas l'opération si vous ne le demandez pas. Sous Adobe 
Photoshop cela s'appelle indexer les couleurs (puisqu'on créé 
un index de toutes les couleurs utilisées). 

Ensuite la question est de savoir si on ne peut pas réduire encore 
les couleurs. Si deux couleurs de vert dans l'image sont très 
approchantes, peut-on utiliser le même aux deux endroits ? Le 
« enregistrer pour le web » d'Adobe Photoshop permet de tester 
plusieurs combinaisons et d'en comparer le résultat avant de 
choisir. Faites cependant très attention : L'oeil humain est 
très bon pour comparer, mais se laisse facilement tromper quand 
il n'a qu'un seul échantillon sous les yeux. En comparant une 
images en pleines couleurs et une image en 256 couleurs côte à 
côte, il est évident que la dégradation vous paraitra gênante, 
disgracieuse. Par la suite, même en retirant de la vue l'image 
en pleines couleurs vous verrez immédiatement tous les défauts 
de l'images en 256 couleurs. 

![Image en pleines couleurs (gauche) et réduite à 256 couleurs (droite)](img/chap05-image-en-pleines-couleurs-gauche-et-reduite-a-256-couleurs-droite.jpg)

Pourtant, faites l'expérience de mettre cette image dans vos 
pages, sans rien dire à personnes, vous verrez que pas grand monde 
ne s'en apercevra. Plus avant dans ce livre nous avions repéré 
une astuce similaire : Pour faire un bouton avec un dégradé, il 
suffit en fait de mettre une couleur claire sur la moitié haute 
et une plus foncée sur la moitié basse, et pas un réel dégradé. 
C'est ce que font beaucoup de sites web, et personne ne le remarque. 
Sachez-donc faire des compromis de qualité, ce n'est pas toujours 
négatif et cela peut vous apporter de réels gains de performance 
quand vous additionnez ceux de toutes les images. 

**Recommandation** : Réduisez à 256 couleurs ou moins la palette 
de la plupart de vos images PNG d'interface graphique. 

Adobe Photoshop, comme les autres logiciels d'édition d'image, 
vous permet de réduire le nombre de couleurs automatiquement. 
Il se charge lui même de remplacer les couleurs qui lui semblent 
proches. Cette automatisation n'est pas toujours souhaitable. 
Peut-être que le dégradé de vert est important, quand bien même 
les couleurs sont proches, et que les bleus peuvent être fusionnés 
quand bien même ils sont très différents. Les graphistes expérimentés 
choisissent eux même individuellement chaque couleur de la 
palette quand ils en ont le temps, leur permettant de faire ces 
choix cruciaux. 

Comme personne ne souhaite passer plusieurs heures à optimiser 
de simples images, je vous propose d'utiliser l'outil pngquant. 
Il sait réduire la palette en supprimant les détails inutiles 
ou invisibles à l'oeil. Le travail est souvent meilleur visuellement 
que ce que fait Photoshop. 

#### Qualité jpeg

Le second levier d'optimisation de qualité des images est le 
réglage qualité à l'enregistrement des fichiers jpeg. Comme 
on l'a vu le format jpeg est un format de compression qui dégrade 
l'image. Dans tous les logiciels d'édition d'image on vous propose 
à la sauvegarde un réglage de qualité qui va de 0 à 100% ou de 0 à 10. 
Il informe le logiciel du niveau de dégradations que l'on est 
prêt à accepter. 

Régler la qualité à 100 n'a virtuellement aucun intérêt pour 
le web. Jusqu'à 90 % aucune différence n'est généralement visible 
à l'oeil nu. Le plus souvent 80 % est un réglage par défaut qui permet 
d'avoir une image de très bonne qualité. Vous pouvez assez souvent 
descendre jusqu'à 70 % pour des illustrations sans que cela soit 
vraiment gênant. 

![Différence de qualité entre un jpeg 100 % (à gauche) et 75 % (à droite)](img/chap05-difference-de-qualite-entre-un-jpeg-100--a-gauche-et-75--a-droite.jpg)

La première étape est donc de vérifier quelle est la qualité de 
vos jpeg, et si baisser en qualité est acceptable ou pas. Le mieux 
reste de mener cette vérification et les discussions qui suivent 
directement avec vos graphistes afin que l'image soit crée dès 
le départ avec le bon niveau de qualité plutôt que de l'ouvrir 
pour la recompresser. Ce dernier procédé qui induirait forcément 
une perte de qualité supplémentaire inutile. 

Lors de la discussion autour de la qualité, comme précédemment, 
il ne faut pas perdre de vue qu'une différence de qualité flagrante 
et inacceptable lors d'une comparaison avec l'image originale 
ou sur le poste du graphiste pourra souvent passer inaperçue 
sur le web quand le visiteur n'a accès qu'à la version légère de 
l'image. Un test en situation réelle sans comparaison peut être 
intéressant en cas de débat. Vous devriez pouvoir ne monter au 
dessus de 80 % qu'exceptionnellement. 

Une fois le degré de qualité générale décidé il est possible d'aller 
plus loin. Le format jpeg autorise en effet des différence de 
qualité par zones dans l'image. Cela permet de s'assurer que 
l'objet principal de l'image (par exemple la pointe d'un stylo) 
soit parfait, et que le reste (stylo, fond) puisse être plus ou 
moins précis. On peut alors arriver à une qualité globale bien 
plus faible en gardant une dégradation invisible sur les zones 
qui accrochent l'oeil et qui contiennent le plus de détail. 

**Recommandation** : Réglez et diminuez la qualité de vos images 
jpeg à 80 ou 70 %, éventuellement en réservant une zone dans l'image 
avec une forte qualité pour quelques détails. 

L'outil JPEGmini ([http://jpegmini.com/](http://jpegmini.com/)) analyse l'image
source, détermine automatiquement les zones en leur appliquant le taux de
compression adéquat. Il existe en exécutables Windows et Mac, et est disponible
en service web.

Sous certaines versions d'Adobe Photoshop ce travail peut être 
réalisé directement par le graphiste à l'aide de calques. Lors 
de l'enregistrement « pour le web », l'outil de qualité permet 
de sélectionner un réglage différent par couche. Il suffit alors 
de copier le détail principal de l'image sur une couche à part, 
pour ensuite sélectionner une qualité de 90 % à cette couche et 
bien plus faible pour le reste de l'image. 

![Détourage d'une sélection jpeg pour en imposer la qualité[^1]](img/chap05-detourage-dune-selection-jpeg-pour-en-imposer-la-qualite1.jpg)

  [^1]: Image et captures reproduites avec l'autorisation de Christophe Andrieu ([http://stpo.fr/](http://stpo.fr/)) et Vincent Valentin ([http://www.htmlzengarden.com/](http://www.htmlzengarden.com/)) 

![Création d'une couche sous Photoshop à partir de la sélection](img/chap05-creation-dune-couche-sous-photoshop-a-partir-de-la-selection.jpg)

![Enregistrement « pour le web  » et gestion de la qualité par couche](img/chap05-enregistrement--pour-le-web---et-gestion-de-la-qualite-par-couche.jpg)

![Enregistrement en jpeg et définition de la qualité d'une couche ](img/chap05-enregistrement-en-jpeg-et-definition-de-la-qualite-dune-couche.jpg)

### Autres optimisations

Après avoir choisi le bon format, recompressé les png, retiré 
les méta-données, limité la palette au strict minimum, modifié 
avec pertinence la qualité des jpeg, il ne vous reste plus que 
quelques points à vérifier pour diminuer le poids de vos images : 

#### Taille de l'image adaptée

Tout d'abord, et même si la recommandation peut paraître superflue, 
veillez à ne pas diffuser une grande image qui sera ensuite réduite 
dans le navigateur à l'aide des propriétés HTML `width` et height 
ou de leurs équivalents CSS. 

Contrairement aux attentes ce cas se retrouve dans de nombreux 
sites. L'analyse de Google Page Speed peut vous aider à trouver 
automatiquement les cas problématiques dans vos pages. 

Le redimensionnement implique de télécharger une image plus 
volumineuse que nécessaire et en plus occupe le processeur avant 
l'affichage. Pour optimiser au mieux les performances, la taille 
native de l'image doit toujours coller exactement au pixel près 
à la taille réellement affichée dans le navigateur. Si vous avez 
besoin d'une vignette et d'une image grande taille, il est souvent 
préférable de livrer deux images plutôt que de réaliser le redimensionnement 
dans le navigateur. 

**Recommandation** : Vérifier que les images téléchargées 
sont utilisées directement dans leur taille réelle et non redimensionnées 
dans le navigateur. 

#### Pixels transparents

Le second point intéressant vient des travaux autour de l'outil 
PunnyPNG. Il s'agit de s'assurer que dans une image avec de la 
transparence, les pixels totalement transparents sont dans 
une couleur qui favorise une compression optimale. 

Ces pixels peuvent en effet, suivant le format d'enregistrement, 
garder une valeur de couleur additionnée à une valeur de transparence. 
Quand bien même la transparence serait totale, en interne une 
couleur est utilisée dans le fichier, et donc dans la compression. 
Si ces pixels utilisent de nombreuses couleurs différentes 
ils diminuent les possibilités de compression inutilement. 
Le plus simple est souvent de leur affecter une couleur unie, 
toujours la même. 

L'outil PunnyPNG fait cette opération, mais peut être réalisée 
à partir de n'importe quel éditeur d'images évolué. 

* PunnyPNG : [http://www.punypng.com/](http://www.punypng.com/) 

**Recommandation** : Utiliser PunnyPNG ou un outil équivalent 
pour s'assurer que les pixels transparents dans les PNG avec 
une couche d'opacité variable sont optimisés pour une meilleure 
compression. 

#### Animations gif

Nous avons vu précédemment que le format gif n'est conseillé 
que dans un seul cas : les images animées. Une image animée est 
constituée de multiples images qui s'affichent l'une après 
l'autre suivant une temporisation prédéfinie. 

L'outil gifsicle permet d'optimiser le poids de ces images animée 
en retirant de chaque sous-image les pixels déjà présents dans 
l'image précédente et qui n'ont pas bougé. Cette manipulation 
ne souffre d'aucun problème de compatibilité et peut être réalisée 
automatiquement par script lors de la mise en production d'une 
image. 

* Gifsicle : [http://www.lcdf.org/gifsicle/](http://www.lcdf.org/gifsicle/) 

**Recommandation** : Supprimer les pixels inutiles dans les 
images gif animées grâce à l'outil gifsicle. 

#### Espaces blancs

Enfin, et cette recommandation n'est qu'une question de bonne 
pratique, assurez-vous que vos images sont cadrées au plus juste. 
Enlevez toute bordure blanche, noire ou unie qui serait inutile. 
Ces bordures peuvent facilement être remplacées par des marges 
ou des bordures en CSS. Le poids total de vos images s'en trouvera 
allégé. 

**Recommandation** : Cadrer les images au plus juste en retirant 
les bordures unies et espaces blancs inutiles. 

### Cas particulier des favicon

Les favicon sont ces petites icônes visibles à gauche de la barre 
d'adresse dans le navigateur. Elles sont aussi visibles comme 
icône dans les favoris ou pour les rares utilisateurs qui font 
un glisser/déposer de la page vers leur bureau. 

Il s'agit habituellement de fichiers au format windows .ico, 
qui peuvent embarquer plusieurs version d'une même image à des 
résolutions différentes. Sur les navigateurs récents cela 
peut aussi être une image png ou gif, même si cette possibilité 
est rarement utilisée. 

Les navigateurs téléchargeront toujours le favicon, quoi que 
vous fassiez. Votre capacité d'intervention se limite donc 
à faire en sorte qu'il soit le plus petit possible. La différence 
entre un favicon de 300 ou de 500 octets est virtuellement nulle 
et il n'y a qu'une seule règle importante à respecter : viser une 
image qui fait au plus 1 Ko. Cette limite est due à TCP, pour éviter 
que le téléchargement ne nécessite plusieurs paquets réseaux. 

**Recommandation** : Supprimer les différentes résolutions 
de votre image favicon.ico pour tomber en dessous du kilo-octet. 
Seule la version 16x16 pixels est indispensable. 

Pour y arriver il suffit normalement de vérifier que votre favicon 
ne contient pas trop de versions différentes à des résolutions 
inutiles. Seule la version 16x16 pixels est nécessaire. Tout 
le reste est potentiellement superflu. Vous pouvez laisser 
quelques versions additionnelles mais seulement si ces versions 
ne vous font pas dépasser le kilo-octet. 

Minimisation des contenus texte
-------------------------------

Nous avons vu comment compresser les contenus textes avec HTTP 
au début de ce chapitre. Les résultats sont déjà importants. 
Il est toutefois possible d'aller encore plus loin avec une minimisation 
préalable. 

L'objectif de la minimisation est de supprimer les commentaires 
et les espaces blancs inutiles pour obtenir une version minimale 
des composants textes. On peut même envisager de remplacer certaines 
syntaxes par des syntaxes alternatives plus courtes. 

Les gains sont très variables, ils peuvent aller de 10 % à plus 
de 70 %. Tout dépend du style de codage et des optimisations mises 
en œuvre. Le résultat sera ensuite réduit une seconde fois par 
la compression HTTP comme nous l'avons vu plus avant dans ce livre. 

**Recommandation** : Minifier les fichiers javascript et CSS 
à la publication, par exemple avec YUI Compressor. 

### Javascript

Les trois outils les plus fréquents pour javascript sont Closure 
Compiler, YUI Compressor, et Dojo ShrinkSafe. Tous trois sont 
rattachés à une bibliothèque javascript de référence mais sont 
génériques et n'imposent pas d'utiliser la bibliothèque correspondante. 

* Closure Compiler : [http://code.google.com/intl/fr/closure/compiler/](http://code.google.com/intl/fr/closure/compiler/) 

* YUI Compressor : [http://developer.yahoo.com/yui/compressor/](http://developer.yahoo.com/yui/compressor/) 

* Dojo ShrinkSafe : [http://shrinksafe.dojotoolkit.org/](http://shrinksafe.dojotoolkit.org/) 

Ces outils permettent d'optimiser un fichier javascript en 
retirant les espaces blancs inutiles, les commentaires, et 
en ajoutant diverses optimisations. YUI fait par exemple des 
alias pour les noms de variable longs et fréquemment utilisés 
soient remplacés dans le code par des noms plus courts. 

Closure Compiler est parfois vu comme celui donnant les meilleurs 
résultats mais les différences sont assez négligeables. YUI 
Compressor a souvent ma préférence car c'est le seul à aussi minimiser 
les fichiers CSS. 

### CSS

Côté CSS il existe de nombreux outils en ligne. Les plus sûrs se 
contentent de retirer les espaces blancs et les commentaires. 
Les plus agressifs analysent tout le code pour aussi utiliser 
les propriétés courtes (fusionner les `background-image`, 
`background-position`, `background-repeat` et `background-color` 
dans la seule propriété `background` par exemple) mais peuvent 
parfois casser les astuces comme les « hacks CSS ». 

* Clean CSS : [http://www.cleancss.com/](http://www.cleancss.com/) ; agressif 

* CSS Compressor : [http://www.csscompressor.com/](http://www.csscompressor.com/) ; agressif 

* Lottery Post :[http://www.lotterypost.com/css-compress.aspx](http://www.lotterypost.com/css-compress.aspx) ; intermédiaire 

* YUI Compressor : [http://developer.yahoo.com/yui/compressor/](http://developer.yahoo.com/yui/compressor/) ; simple 

Si vous utilisez un outil marqué comme « agressif », pensez à en 
tester le résultat sur de nombreux navigateurs et sur l'intégralité 
de vos pages pour être certains qu'aucun dégât n'apparait. 

Il est aussi important de noter que YUI Compressor est le seul 
à permettre de réaliser la compression en local. Tous les autres 
outils sont basés sur des formulaires en ligne et peuvent poser 
des problèmes de confidentialité. 

### HTML

Des outils existent aussi pour compresser JSON, XML (et donc 
SVG) ou HTML. Ils sont toutefois beaucoup moins utilisés car 
ces formats sont le plus souvent gérés pour des ressources dynamiques 
et les compresseurs utilisés sont assez coûteux en processeur. 

En général la compression HTTP gzip suffit pour ces contenus. 
Il faut toutefois penser que chaque donnée a un coût, et éviter 
des commentaires inutilement longs ou nombreux dans les codes 
sources HTML. 

Poids des cookies
-----------------

Les cookies sont un problème de performance souvent invisible. 
Un cookie est envoyé par le navigateur pour chaque requête. En 
ajoutant un cookie de 500 octets sur les 150 composants d'une 
page c'est 75 Ko qui s'ajoutent au trafic réseau. Mais surtout 
ces 75 Ko s'ajoutent au trafic envoyé par le navigateur de l'internaute 
et pas au trafic reçu. Les connexions ADSL ayant des limites bien 
plus basses pour l'envoi (upload) que pour la réception (download), 
cette différence n'est pas négligeable. 

C'est d'autant plus gênant qu'un cookie vraiment trop volumineux 
(si le couple requête + cookie dépasse 1,5 Ko) pourrait imposer 
au navigateur d'envoyer sa requête en deux paquets TCP au lieu 
d'un seul, ce qui impose potentiellement un aller-retour serveur 
en plus. 

La première recommandation est d'éviter d'utiliser les cookies 
pour autre chose que de courts identifiants. Le contenu des 
sessions et des préférences peut être stocké côté serveur, ne 
laissant que le jeton de session dans le cookie. 

La seconde recommandation, plus importante, est de n'utiliser 
les cookies que sur les pages qui en ont besoin, c'est à dire habituellement 
les pages HTML et les requêtes Ajax. Le reste des composants, 
et principalement les fichiers javascript, css, images, n'ont 
pas besoin de recevoir les cookies. 

Nous n'avons pas moyen d'informer à l'avance le navigateur de 
l'utilité d'envoyer ou non les cookies. Par défaut ils sont envoyés 
pour tout le domaine pour lesquels ils ont été enregistrés initialement. 
Il ne reste donc qu'une option efficace : Séparer les contenus 
dynamiques (qui ont besoin des cookies) et les contenus statiques 
(qui n'ont pas besoin de cookies) sur des domaines différents, 
et ne pas envoyer de cookies sur ces derniers. 

Il est aussi possible d'utiliser des sous-domaines et non des 
domaines différents mais dans ce cas il faut bien penser à ce que 
les ressources dynamiques utilisent aussi un sous-domaine 
(par exemple « www.example.org » et non directement « example.org 
») et que les cookies sont restreints à ce sous-domaine (cela 
se règle dans les options d'envoi des cookies avec votre langage 
de programmation ou votre application). 

**Recommandation** : Séparer les contenus statiques sur un 
ou plusieurs domaines ou sous-domaines qui ne recevront ni n'enverront 
aucun cookie. 

Shopzilla, un boutique en ligne américaine importante, a noté 
une augmentation de ses revenus de 0,8 % après avoir retiré les 
cookies de ses composants statiques. Le travail nécessaire 
à la mise en œuvre se rentabilise donc assez rapidement. 

Pages d'erreur
--------------

Le dernier point de ce chapitre concerne les pages d'erreur. 
Tôt ou tard il arrive qu'un lien ou qu'une référence finisse par 
mener à une page d'erreur. Ce peut être une adresse mal écrite, 
une ressource qui a changé d'adresse, ou simplement un fichier 
qui a été supprimé volontairement mais dont on n'a pas retiré 
absolument toutes les références. Ces erreurs arrivent à tout 
le monde, y compris les plus gros sites comme Yahoo! ou Amazon. 

Le problème c'est que ces pages peuvent être assez lourdes. On 
y référence fréquemment un plan du site complet et dans ce cas 
leur poids varie de 25 Ko à 50 Ko (HTML uniquement). La page d'erreur 
de tf1.fr fait par exemple 30 Ko. 

**Recommandation** : Assurez-vous que les pages d'erreurs, 
et principalement la page d'erreur 404 soit minimale (HTML de 
5 Ko environ) 

Il arrive même que certains sites lancent une redirection vers 
la page d'accueil, ce qui fait un aller-retour serveur inutile 
et une page de 75 à 100 Ko à télécharger. La page d'orange.fr fait 
par exemple 

**Recommandation** : La page d'erreur ne doit pas mener à une 
redirection. 

Le navigateur qui a une référence cassée vers une image ou un javascript 
se retrouve donc à charger 25 à 100 Ko, ce qui forcément pénalise 
de façon importante les performances. 

Ces recommandations sont d'autant plus importantes que contrairement 
aux autres ressources, les erreurs HTTP ne sont pas mises en cache 
par le navigateur. La page d'erreur sera donc retéléchargée 
à chaque accès. Si l'erreur est dans une partie générique de la 
page HTML ou de la CSS, c'est le poids de toutes les pages vues qui 
est ainsi gonflé artificiellement. 

Pire, dans le cas du javascript le navigateur tente d'interpréter 
la page d'erreur en tant que code javascript même si le type mime 
est bien celui d'une page HTML. On perd alors aussi du temps à essayer 
d'interpréter le contenu de la page. 

### Cas particulier des favicon

Les images favicon sont ici encore un cas particulier. C'est 
un composant qu'on oublie assez facilement parce que c'est le 
navigateur qui va seul le chercher quand bien même on n'y fait 
pas référence. Télécharger une page d'erreur de 25 Ko en lieu 
et place du favicon serait une très mauvaise surprise. 

Un favicon correctement optimisé sera toujours plus petit que 
1 Ko, donc toujours plus petit que la page HTML d'erreur. Veillez 
donc à toujours avoir un fichier /favicon.ico, même s'il est 
perfectible du point de vue graphisme. Vous éviterez à certains 
navigateur d'avoir à télécharger une page d'erreur inutilement. 

**Recommandation** : Toujours prévoir un fichier favicon.ico 
à la racine du domaine. 

À retenir
---------

* Activer la compression HTTP des fichiers CSS et Javascript 

* Optimiser les images avec pngcrush ou équivalent 

* Faire des compromis de qualité pour diminuer le poids des images 

* Choisir le bon format pour chaque image (jpeg ou png, suivant 
  les cas) 

* Minimiser les Javascript et CSS à l'aide de YUI Compressor 
  ou équivalent 

* Faire des pages d'erreur petites (< 5 Ko) et sans redirection 

* Servir les ressources statiques via un domaine séparé, sans 
  cookies 

* S'assurer de la présence d'une icône à l'adresse /favicon.ico 

Parallélisation
===============

Nous l'avons vu dans le chapitre d'introduction aux premiers 
concepts, une session web a tendance à fortement sous-utiliser 
notre bande passante. Sur nos machines modernes, le processeur 
tourne lui aussi quasiment à vide pendant une bonne partie du 
chargement. 

La solution naturelle est de permettre au navigateur de paralléliser 
son travail, par exemple télécharger deux composants simultanément 
vu que la limitation est « par téléchargement » et non globale. 
On utilise alors mieux le processeur et la bande passante. 

On peut constater ce manque de parallélisation des navigateurs 
dans la représentation d'une session de navigation par webpagetest.org. 
Les deux courbes en bas représentent la bande passante et le processeur. 
Les lignes au dessus représentent des connexions à des serveurs 
web, et sur ces connexions les barres représentent l'activité 
réseau (les téléchargements, ou l'attente des téléchargements, 
suivant la couleur). 

![Téléchargements, occupation processeur et bande passante utilisée avec webpagetest](img/chap06-telechargements-occupation-processeur-et-bande-passante-utilisee-avec-webpagetest.png)

On repère tout de suite dans les courbes la sous-utilisation 
du processeur et de la bande passante. La lecture de l'échelle 
de la bande passante ajoute encore à la frustration quand on se 
rends compte que la vitesse la plus importante est de 51 Kb/s. 

Sur les lignes de téléchargement on peut voir que certaines connexions 
servent peu (les premières) et que les autres ont des « trous ». 
En permettant la parallélisation dès le début on gagnerait 2,5 
secondes au départ. En évitant le trou et en permettant l'utilisation 
soutenue de toutes les connexions, on finirait là aussi environ 
2,5 secondes plus tôt. 

Au final, avec plus de parallélisation on pourrait gagner presque 
5 secondes sur les 12 secondes qu'a duré le chargement de notre 
page exemple. Si on parallélisait encore plus en augmentant 
le nombre de connexions simultanées on pourrait même aller plus 
loin. Le processeur et la bande passante disponible nous permettraient 
de le faire, à nous de nous en charger. 

Plusieurs connexions TCP par domaine
------------------------------------

Le premier palliatif au manque de parallélisation est implémenté 
dans tous les navigateurs, et même proposé par la spécification 
du protocole HTTP 1.1 : Si la bande passante est sous-occupée 
à cause des aller-retours réseaux et de la gestion TCP, il suffit 
d'ouvrir plusieurs connexions parallèles pour compenser. 
Ouvrir plus de connexions permet d'optimiser la bande passante 
mais charge plus le processeur. 

Historiquement les navigateurs ouvraient nativement deux 
connexions en parallèle par site (même nom de domaine complet). Ils pouvaient 
ainsi télécharger deux images simultanément, et exploiter 
au mieux la bande passante. Depuis, les processeurs et les débits 
ont augmentés de façon impressionnante et deux connexions simultanées 
ne suffisent plus à couvrir même la moitié de notre bande passante 
ou à occuper significativement nos processeurs. 

Les navigateurs récents et les mobiles sont tous passés à six connexions persistantes 
simultanées par domaine, sauf Opera et Internet Explorer 10 qui sont passé à huit.


+-----------------------------+-------------------------------------+
| Navigateur                  | Connexions persistantes par domaine |
+=============================+=====================================+
| Firefox                     | 6                                   |
+-----------------------------+-------------------------------------+
| Chrome                      | 6                                   |
+-----------------------------+-------------------------------------+
| Safari Mac, iOS             | 6                                   |
+-----------------------------+-------------------------------------+
| Internet Explorer 6 et 7    | 2                                   |
+-----------------------------+-------------------------------------+
| Internet Explorer 8 et 9    | 6                                   |
+-----------------------------+-------------------------------------+
| Internet Explorer 10        | 8                                   |
+-----------------------------+-------------------------------------+
| Opera                       | 8                                   |
+-----------------------------+-------------------------------------+
| Opera Mobile                | 8                                   |
+-----------------------------+-------------------------------------+
| Android 2.3                 | 8                                   |
+-----------------------------+-------------------------------------+
| Android 4                   | 6                                   |
+-----------------------------+-------------------------------------+


### Serveurs HTTP 1.0

Tous les serveurs web ne gèrent pas les connexions persistantes. 
Les navigateurs s'adaptent et peuvent s'autoriser plus de connexions 
simultanées vers ces serveurs. Firefox passe alors à quinze 
connexions simultanées dans ce cas. 

Ces comportements se déclenchent en fait dès que le serveur répond 
à l'aide de la version 1.0 du protocole HTTP, quand bien même il 
utilise la fonctionnalité keep-alive (qui a quelques spécificités 
mais qui est similaire aux connexions persistantes de HTTP 1.1). 

L'exemple d'AOL est connu sur ce sujet : Afin d'améliorer la performance 
des navigateurs visitant son site, les serveurs AOL répondaient 
toujours en HTTP 1.0, et profitaient donc immédiatement de plus 
de connexions parallèles. 

### Serveurs proxys

Les proxys déclenchent eux aussi un comportement spécifique 
des navigateurs. Comme ces derniers ne se connectent pas directement 
aux serveurs web, les problématiques de charge réseau et de charge 
serveur sont différentes. 

Firefox passe alors à huit connexions simultanées par proxy, 
tous domaines confondus, Internet Explorer 9 à douze. Suivant 
le nombre de domaines que le navigateur essaye de joindre simultanément 
via le même proxy, ça peut se révéler bénéfique ou gênant. Généralement 
le bénéfice du proxy (cache partagé) compense toutefois les 
éventuelles dégradations dues au nombre de connexions simultanées. 

### Maximum de connexions

Les navigateurs maintiennent aussi un nombre total de connexions 
parallèles tous domaines confondus. Il est entre 30 et 35 pour 
tous les navigateurs récents, sauf Android 2.3 qui limite à 4. 

### Cas particulier d'Internet Explorer sur modem RTC et VPN

Il est intéressant de noter que les versions récentes d'Internet 
Explorer tentent de détecter les connexions par modem RTC et 
par VPN. Si une de ces connexions est trouvée, le navigateur est 
alors limité à deux connexions persistantes par domaine. 

Il est possible dans ce navigateur de connaître combien de connexions 
sont utilisées à l'aide de la propriété javascript `maxConnexionsPerServer` 
de l'objet `window`. 

Plusieurs domaines en parallèle
-------------------------------

Les équipes performance de Yahoo! avaient devancé cette évolution 
des navigateurs. En 2007, quand Microsoft Internet Explorer 
6 étaient encore majoritaire, avec deux connexions persistantes 
par domaine seulement, les équipes Yahoo! ont proposé de répartir 
les ressources sur plusieurs domaines. Si les données sont réparties 
sur trois domaines, cela fait mathématiquement trois fois plus 
de connexion simultanées. 

Chaque domaine supplémentaire a un coût : Il demande une requête 
DNS, et plusieurs nouvelles connexions TCP. Il ne serait donc 
pas positif de multiplier à l'excès le nombre de domaines sur 
lesquels sont répartis les composants d'une page. 

Des tests avaient été faits à l'époque, avec une page vide comportant 
20 images de 3,4 Ko. Le temps de chargement total de la page avait 
diminué en répartissant sur deux ou trois domaines, et avait 
augmenté quand le nombre de domaines avait dépassé quatre. Leur 
recommandation était donc de toujours répartir sur plusieurs 
domaines, mais jamais plus de quatre. 

### Nombre de domaines

Depuis, les navigateurs ont augmenté leur parallélisme, aussi 
l'impact d'avoir plusieurs domaines devrait se trouver diminué, 
et l'effet négatif pourra apparaître plus vite. 

De même, Yahoo! a fait un test adapté à ses sites, dont les noms 
de domaines sont quasiment toujours dans le cache. Ce n'est probablement 
pas vrai pour vous, et dans ce cas l'effet négatif arrive encore 
une fois plus rapidement. 

À l'inverse, il est probable que votre site ait plus de 20 composants, 
et dans ce cas l'impact d'une répartition sur plusieurs domaines 
sera plus fort. 

Il est sage de se contenter de deux domaines, surtout avec les 
avancées des navigateurs récents qui parallélisent déjà très 
bien, mais même la répartition sur deux domaines devient moins 
importante qu'avant sur un site léger. 

![Domaines utilisés par la page d'accueil Yahoo!, répartition en requêtes et en poids](img/chap06-domaines-utilises-par-la-page-daccueil-yahoo-repartition-en-requetes-et-en-poids.png)

Le nombre de domaines pertinent pour votre site devra faire l'objet 
de tests adaptés à votre cas, mais une bonne mesure peut être de 
compter le nombre de ressources, de diviser par 6 (nombre de connexions 
parallèles des navigateurs récents), puis encore par 5 ou 6 (en 
dessous de 5 requêtes que une même connexion, le coût d'ouverture 
d'une nouvelle connexion n'est pas forcément amorti). 

Cela nous donnerait le tableau suivant : 

+----------------------+-------------------------------+
| Nombre de composants | Nombre de domaines conseillés |
+======================+===============================+
| Moins de 30          | 1                             |
+----------------------+-------------------------------+
| Entre 30 et 70       | 2                             |
+----------------------+-------------------------------+
| Plus de 70           | 3                             |
+----------------------+-------------------------------+

Il convient toutefois de considérer que ce n'est qu'une première 
approximation qui vous permet ensuite de faire de vrais tests 
pour chaque valeur. 

Bien entendu, si votre site a encore beaucoup de vieux Internet 
Explorer 6 et 7, il faudra plus rapidement envisager deux ou trois 
domaines. 

À l'inverse, si votre site est sécurisé avec HTTPS, le coût d'une 
nouvelle connexion est prohibitif : N'ajoutez pas un domaine 
tant que vous avez moins de 8 à 10 requêtes par connexion. Si vous 
avez des visiteurs avec des navigateurs récents, cela veut dire 
que vous pourrez souvent rester avec un voire deux domaines. 

**Recommandation** : Sur un site standard, privilégiez la répartition 
de vos composants sur deux à trois domaines. 

### Mise en place

Ce que nous appelons domaine dans ce cadre c'est le nom complet 
de l'hôte, sous-domaine et port compris. Il est ainsi tout à fait 
possible de faire un www.example.org, www1.example.org et 
www2.example.org. 

Pour répartir vos contenus sur plusieurs domaines il est possible 
de réellement monter plusieurs domaines et de mettre les contenus 
sur l'un ou sur l'autre, suivant une règle précise. C'est indirectement 
ce qui est fait quand on utilise un réseau de diffusion de contenu 
(CDN, content delivery network). 

Plus simplement, il est possible de créer des domaines alias, 
c'est à dire pointant tous vers le même serveur, les mêmes fichiers, 
le même contexte. La seule différence sera le nom de machine utilisé 
dans l'adresse de la ressource. Côté serveur il suffit de créer 
plusieurs hôtes virtuels dans votre configuration Apache ou 
Nginx, avec la même configuration et la même destination. On 
pourra alors utiliser un domaine ou un autre, indifféremment, 
pour n'importe quelle ressource. 

Attention dans ce dernier cas d'utilisation : Si vous ne voulez 
pas annuler tout l'effet du cache HTTP (et vous ne le voulez pas, 
sinon relisez les chapitres précédents), il faut qu'une même 
ressource utilise toujours le même domaine ; ne créez pas des 
liens qui utilisent aléatoirement un domaine parmi une liste. 
Vous pouvez vous baser sur un hachage du nom de fichier, sur le 
type de fichier, sur sa source, sur sa fonction (illustration 
ou contenu), sur sa fréquence de mise à jour, ou sur n'importe 
quel critère tant qu'il ne permet pas de changer le domaine de 
l'URL tant que le fichier ne change pas. 

Pensez aussi à répartir relativement équitablement vos ressources 
sur les différents domaines, sinon vous perdrez un des intérêts 
de cette répartition. 

### Sécurité

Le fait d'utiliser des domaines tiers peut bloquer certaines 
fonctionnalités à cause des modèles de sécurité des navigateurs. 
Ainsi les contenus scriptables peuvent ne pas pouvoir interagir 
pleinement avec les contenus venant d'autres domaines, et les 
contenus protégés par des droits d'auteurs forts peuvent ne 
pas pouvoir être téléchargés sans entête HTTP particulière. 

Pour pouvoir télécharger une police de caractères sur un domaine 
tiers avec Firefox il faut que les fichiers de police envoient 
une entête HTTP `Access-Control-Allow-Origin` avec le nom 
de domaine de la page HTML comme valeur, ou `*` si l'accès est libre. 
Dans Apache cela peut se configurer comme suit : 

~~~~~~~ {.apache .partial}
<FilesMatch "\.(ttf|otf|woff)$">
    <IfModule mod_headers.c>
        Header set Access-Control-Allow-Origin "*"
    </IfModule>
</FilesMatch>
~~~~~~~

Pour que les animations Flash puissent réaliser des requêtes 
HTTP vers des domaines tiers il faudra que le domaine tiers comporte 
un fichier crossdomain.xml à la racine. Là aussi, il faut nommer 
les domaines source autorisés, ou * pour un accès libre (attention 
cela a des implications de sécurité, vous êtes encouragés à lister 
explicitement tous les domaines sources autorisés) : 

~~~~~~~ {.xml}
<?xml version="1.0"?>
<!DOCTYPE cross-domain-policy 
  SYSTEM "http://www.macromedia.com/xml/dtds/cross-domain-policy.dtd">
<cross-domain-policy> 
    <allow-access-from domain="*.example.org" />
</cross-domain-policy>
~~~~~~~

De plus, les codes javascript ne pourront pas explorer totalement 
les données venant de domaines tiers. Ainsi vous ne pourrez pas 
en javascript explorer le DOM d'une feuille de style venant d'un 
autre domaine. Vous aurez aussi des limitations pour les requêtes 
de type AJAX. 

Enfin, les feuilles de style CSS téléchargées depuis un domaine 
tiers ne pourront pas inclure de composants XBL. 

Toutes ces limitations de sécurités ne sont pas forcément limitantes, 
mais elles sont à garder à l'esprit quand vous choisirez les fichiers 
à laisser ou pas sur le domaine principal. 

Ressources bloquantes
---------------------

Nous avons augmenté le parallélisme au niveau réseau mais il 
nous reste un problème majeur : Le navigateur lui-même ne sait 
pas toujours gérer ce parallélisme. 

Certaines ressources bloquent historiquement l'activité 
réseau, le rendu, ou l'activité du navigateur. Au fur et à mesure 
des versions les navigateurs améliorent leur parallélisme 
et diminuent les goulots d'étranglement mais cela reste un problème 
pour les performances, au moins pour les navigateurs qui ne sont 
pas parfaitement à jour. 

On peut facilement constater le problème sous Microsoft Internet 
Explorer 6 en chargeant un long javascript suivi par quelques 
images : Pendant le chargement et l'exécution du javascript, 
aucune autre activité n'est réalisée par le navigateur. Au lieu 
d'être téléchargées en parallèle, les ressources sont chargées 
séquentiellement, doublant le temps nécessaire au chargement 
de la page. 

![Téléchargement de ressources en parallèle](img/chap06-telechargement-de-ressources-en-parallele.png)

Ce comportement est aussi celui de Internet Explorer 7 et d'Opera 
au moins jusqu'à sa version 11.5. Si un script apparaît, il faudra 
attendre son téléchargement et son exécution complète avant 
de faire quoi que ce soit d'autre. 

Firefox 3.5 et Internet Explorer 8 sont un peu plus avancés puisqu'il 
savent charger plusieurs scripts et feuilles de style en parallèle 
mais le téléchargement d'images en parallèle reste impossible. 
Sur ces navigateurs il faut attendre Internet Explorer 9 et Firefox 
3.6 pour cela (Chrome, Android, Safari mobile et Safari desktop 
savent tous le gérer dans leurs versions répandues). 

Seules les versions récentes des navigateurs (Chrome 11, Firefox 
3.6, Internet Explorer 9 et Safari 5.1) savent charger en parallèle 
les images quant elles apparaissent après une feuille de style 
suivie par un script « en ligne ». Les autres navigateurs (y compris 
toutes les versions d'Opera au moins jusqu'à la 11.5) attendent 
le téléchargement complet de la feuille de style pour exécuter 
le javascript et enfin commencer à télécharger les images. 

Pour l'instant (mars 2011) aucun navigateur ne sait correctement 
gérer le parallélisme en mixant des scripts et des iframes. 

Enfin, Firefox jusqu'à la version 3.6 ne savait pas charger en 
parallèle les scripts insérés à l'aide de `document.write`. 

+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Navigateur  | JS // JS | JS // CSS | JS // IMG | JS // IFRAME | CSS // CSS | CSS // inline JS | Async JS |
+=============+==========+===========+===========+==============+============+==================+==========+
| Android 2.3 | O        | O         | O         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Android 3.0 | O        | O         | O         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Chrome 11   | O        | O         | O         | N            | Y          | Y                | Y        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Firefox 3.0 | N        | N         | N         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Firefox 3.5 | O        | O         | N         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Firefox 3.6 | O        | O         | O         | N            | Y          | N                | Y        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Firefox 4.0 | O        | O         | O         | N            | Y          | Y                | Y        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| IE 6 et 7   | N        | N         | N         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| IE 8        | O        | O         | N         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| IE 9        | O        | O         | O         | N            | Y          | Y                | Y        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Iphone 4.3  | O        | O         | O         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Opera 11.5  | N        | N         | N         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+
| Safari 6    | O        | O         | O         | N            | Y          | N                | N        |
+-------------+----------+-----------+-----------+--------------+------------+------------------+----------+

[Browserscope](http://www.browserscope.org/?category=network) 
liste les fonctionnalités de chaque navigateur au regard de 
ces critères (et d'autres). Il est normalement tenu à jour assez 
rapidement après la sortie d'une nouvelle version. 

Positionner le code javascript
------------------------------

La première recommandation pour palier le problème de parallélisation 
des scripts est de faire attention à leur placement dans le corps 
de la page. Pour cela on distingue les scripts embarqués dans 
la page (« inline ») et les scripts externes. 

### Code javascript embarqué dans la page

Les codes embarqués, s'ils sont exécutables presque instantanément, 
devraient être placés le plus haut possible dans le `<head>`, 
avant les références vers les feuilles de style et les autres 
javascript. Placés en haut avant toute autre référence ils n'imposeront 
aucun téléchargement séquentiel avant de charger les composants 
de la page. 

Attention, si les scripts embarqués dans le `<head>` apparaissent 
après la déclaration des feuilles de style, ils risqueront d'empêcher 
la parallélisation sur une majorité de navigateurs (à l'heure 
actuelle, tous sauf Firefox 4 et Internet Explorer 9). 

À l'inverse, si ces scripts embarqués sont lents à exécuter, 
alors il faudra privilégier un positionnement tout en bas de 
la page, juste avant la fermeture du `</body>`. Ainsi, le contenu 
complet de la page sera accessible avant que l'effet négatif 
du script soit perçue. 

Les cas entre lent et instantané à exécuter dépendent du contexte, 
mais il est en général plus sûr de déléguer l'exécution à la fin 
de la page en cas de doute : Le haut de page est très sensible car 
tout délai laissera une page blanche ou la page précédente plus 
longtemps à la vue de l'utilisateur, provocant un ressenti très 
négatif. 

### Code javascript externe ou à exécution lente

Les scripts externes (dans des fichiers séparés) sont par nature 
lents. Dans le meilleur des cas, sur un site grand public, c'est 
de 50 millisecondes grand minimum pour un petit script avec une 
faible latence à plus d'une seconde pour un gros jquery monolithique. 

Pendant ce temps, Opera et Internet Explorer 7 bloquent toute 
activité. Pour ces navigateurs, un script qui met une seconde 
à se charger c'est une page dont le chargement est ralenti d'autant. 

Les autres navigateurs savent paralléliser les téléchargements 
suivants (images et autres fichiers javascript) mais le rendu 
s'arrête le temps que notre fichier soit téléchargé et exécuté. 

Pour éviter ces désagréments on conseille donc d'inclure l'essentiel 
des scripts externes en bas de page, juste avant la fermeture 
du `</body>`, avec le même objectif que pour les scripts lents 
embarqués directement dans la page. 

Placer les scripts en fin de page
---------------------------------

Quand on propose de déporter le javascript en fin de page la réaction 
est souvent une réaction de rejet. On n'imagine pas une page sans 
javascript et on craint de donner une mauvaise expérience à l'internaute. 

Pourtant, nous avons un fait essentiel qui soutient notre objectif 
: L'utilisateur cherche à utiliser en premier lieu le contenu 
textuel et graphique avant de tenter d'interagir avec les enrichissements 
javascript ou de profiter des animations. 

Ce que nous cherchons donc à faire ici n'est pas d'accélérer le 
chargement de la page, mais uniquement de faire arriver à l'écran 
le plus important le plus tôt possible : prioriser. En fait il 
se peut même que le chargement complet de la page soit légèrement 
plus long, mais nous considérons quand même cela comme un bénéfice 
si la page affichée à l'utilisateur est exploitable entre temps. 
C'est tout l'objectif de déporter le javascript en fin de page. 

**Recommandation** : Placez les codes javascripts externes 
ou lents en fin de page, juste avant la fermeture de la balise `</body>`. 

### Rendre la page fonctionnelle en attendant javascript

Malheureusement pour nous, les sites sont de plus en plus en dépendance 
forte avec javascript. Pour mettre en œuvre notre recommandation 
d'injecter le javascript uniquement à la fin de la page, il faut 
respecter une bonne pratique d'accessibilité : d'abord construire 
la page, utilisable sans javascript, et ajouter les enrichissements 
seulement ensuite. 

Il nous faudra toutefois éviter qu'un enrichissement dépendant 
de javascript ne soit utilisé avant qu'il ne soit fonctionnel. 
Trois options sont possibles : 

#### Cacher la fonctionnalité lors de l'attente

La première est de cacher la fonctionnalité tant qu'elle n'est 
pas complète. On utilisera cela pour les fonctionnalités annexes 
qui ne manqueront pas à l'utilisateur et dont l'apparition différée 
ne choquera pas l'utilisateur. Le cas parfait pour cette option 
est le carrousel : Par défaut seul le premier item du carrousel 
s'affiche. L'interface semble fonctionnelle, l'utilisateur 
se retrouve juste avec un seul visuel. 

Une fois le javascript disponible, c'est là que les autres visuels 
sont téléchargés en tâche de fond et que l'animation commence. 
Si c'est bien réalisé, l'utilisateur ne se rendra même pas compte 
que le chargement se fait en deux temps. Seul le fait que le premier 
visuel reste plus longtemps à l'écran la première fois permet 
de déceler l'astuce. 

![Carrousel avant le chargement complet des javascript](img/chap06-carrousel-avant-le-chargement-complet-des-javascript.png)

![Carrousel après le chargement complet des javascripts](img/chap06-carrousel-apres-le-chargement-complet-des-javascripts.png)

Le fait que les animations n'attirent pas immédiatement la vue 
mais seulement après un certain temps pourra même être vu comme 
une amélioration de l'expérience utilisateur. Outre les carrousels, 
ce fonctionnement est parfait pour tous les contenus qui ne manquent 
pas vraiment s'ils sont absents : les auto-complétions par exemple. 
On peut les utiliser aussi pour les contenus annexes et peu visibles 
dans les colonnes à droite ou à gauche du contenu principal. 

#### Attente lors de l'activation de la fonctionnalité

La seconde option est d'afficher tout ce qui sera utilisable 
à terme par l'utilisateur. C'est seulement quand on voudra cliquer 
sur un bouton ou agir avec cette interface qu'un visuel d'attente 
sera affiché, le temps de finir de télécharger tout le code nécessaire. 

On utilise ce mécanisme pour les animations et les interfaces 
au centre de l'écran ou qui sont essentielles à l'utilisateur, 
quand il n'est pas envisageable de les masquer par défaut ou de 
les faire apparaître à l'écran par la suite. 

C'est par exemple cette solution qu'on trouvera derrière les 
boutons « retirer le produit de la commande et recalculer le prix 
à payer » d'un récapitulatif de commande. Il est nécessaire pour 
l'utilisateur d'avoir immédiatement accès au bouton et d'en 
connaître la présence, par contre il est envisageable d'attendre 
un court instant une fois qu'on tente de faire une opération. 
Là aussi, il est probable que l'utilisateur ne se rende pas compte 
de l'astuce s'il n'utilise pas immédiatement le bouton. Nous 
avons donc tout à y gagner. 

Réaliser cet effet demande un investissement plus lourd que 
la méthode précédente puisqu'il faut exécuter un très court 
javascript en ligne dans le `<head>` (quelques lignes) afin 
d'activer ce comportement d'attente, puis détecter si un clic 
est en attente ou pas quand le javascript réel s'exécute en fin 
de page. 

#### Attente lors de l'affichage de la fonctionnalité

La troisième option c'est afficher directement un visuel d'attente 
à la place, à côté ou par dessus l'interface dont le javascript 
est en train de se télécharger. Ce peut être une image animée, 
une barre de chargement, ou simplement un bouton grisé et désactivé. 
Le visuel d'attente est ensuite supprimé quand le code javascript 
nécessaire est enfin entièrement chargé. 

![Interface vidéo où les commentaires et sous-titres sont en cours de chargement](img/chap06-interface-video-ou-les-commentaires-et-sous-titres-sont-en-cours-de-chargement.png)

Parce qu'elle montre à l'utilisateur que la page est encore en 
train de se charger, on réserve l'utilisation de cette option 
aux fonctionnalités qui constituent le contenu principal de 
la page : un player vidéo par exemple. 

#### Choisir comment vivre l'attente

Aucune de ces options n'est parfaite, et elles représentent 
toutes trois un compromis différent suivant le rôle et la place 
de la fonctionnalité javascript. Gardez toutefois à l'esprit 
que l'alternative c'est que votre page ne s'affiche pas du tout 
(page blanche ou attente sur la page précédente), ou à moitié 
(contenu haut et gauche, sans la barre de droite, le bas et plusieurs 
images) pendant ce temps. 

Quitte à attendre, offrez à vos utilisateurs le maximum, qu'ils 
puissent au moins avoir accès à une page visuellement propre 
et complète avec le reste des contenus et de la navigation. Son 
attente n'en sera que meilleure. 

### Prévoir l'arrivée des scripts : éviter les pages qui bougent

Afin de réaliser des pages qui fonctionne avec et sans javascript, 
il vous est conseillé d'utiliser un mécanisme d'enrichissement. 
Il s'agit de réaliser d'abord une page fonctionnelle sans javascript, 
puis de modifier le DOM avec javascript pour obtenir l'effet 
souhaité. Si les deux interfaces, avec et sans, diffèrent visuellement, 
l'effet peut être gênant pour l'utilisateur. C'est encore plus 
vrai si les scripts sont exécutés en fin de page puisque la page 
« sans javascript » sera donc affichée plus longtemps. 

Il est alors nécessaire de prévoir si la page exécutera ou non 
les scripts pour lui faire afficher directement la bonne mise 
en page. Pour cela on utilise généralement un script « en ligne 
», directement embarqué dans le `<head>` de la page HTML, qui 
se contente d'ajouter une classe nommée « js » à la balise racine 
`<html>`. 

~~~~~~~ {.oneline .html .partial}
<script>document.documentElement.className += " js";</script>
~~~~~~~

La feuille de style pourra alors prévoir les deux cas, avec ou 
sans javascript, et afficher immédiatement la mise en forme 
la plus proche possible de la mise en forme finale. 

Pour illustrer notre propos on peut prendre l'exemple d'une 
page avec des résultats sportifs. Sans javascript ces résultats 
sont présentés sous forme de plusieurs tableaux, un pour chaque 
poule, placés les uns en dessous des autres. 

![Résultats sportifs sans javascript](img/chap06-resultats-sportifs-sans-javascript.png)

Avec javascript nous souhaitons avoir une mise en page plus agréable 
sous forme d'onglets de façon à optimiser l'espace de la page. 

![Résultats sportifs avec javscript](img/chap06-resultats-sportifs-avec-javscript.png)

Il est possible de passer de l'un à l'autre avec uniquement des 
règles CSS. Les onglets supplémentaires (ici « résultats » et 
« à venir ») ne seront pas utilisables directement. Pour passer 
sur ces onglets il faudra attendre le chargement complet de Javascript 
mais l'utilisateur ne s'en apercevra probablement pas : Il s'agit 
d'un contenu annexe sur lequel l'utilisateur n'ira pas forcément 
cliquer en moins de quelques secondes. Si besoin on peut toutefois 
enregistrer un mécanisme d'attente comme décrit plus tôt. 

Par contre, il ne subira pas l'expérience désagréable qu'aurait 
été de voir tous les onglets dépliés pendant quelques secondes 
avant de les voir se replier sous ses yeux faisant totalement 
changer la forme générale de la page. 

Charger Javascript en asynchrone
--------------------------------

Plus que simplement déporter le javascript en bas de page, il 
existe aussi des techniques pour que les fichiers soient chargés 
de façon asynchrone, en parallèle des autres composants, sans 
bloquer le rendu de la page. 

Toutes ces techniques ont un coût, soit en complexité soit en 
compatibilité. Il vous est recommandé de répondre en premier 
à la recommandation précédente et de placer un maximum de javascript 
en bas de page. C'est particulièrement vrai si vous avez des fichiers 
de forte taille puisque même chargés en asynchrone ils continueront 
d'occuper vos files de téléchargement et d'empêcher le navigateur 
d'afficher le reste des composants. 

![Exemple de javascript bloquant sur covoiturage.fr](img/chap06-exemple-de-javascript-bloquant-sur-covoituragefr.png)

Sur l'exemple covoiturage.fr, retirer l'aspect bloquant du 
gros javascript (quatrième composant) permettrait d'avancer 
l'affichage de tous les autres composants de plus de deux secondes. 

### Fonctionnement natif des navigateurs

Outre le comportement natif des navigateurs récents qui savent 
paralléliser correctement le chargement des fichiers javascript, 
nous avons plusieurs techniques à notre disposition pour éviter 
de bloquer le rendu. 

#### Defer et async

Depuis longtemps Microsoft Internet Explorer accepte un attribut 
nommé « defer » sur les balises `<script>`. Il permet au navigateur 
d'initier le téléchargement mais lui demande d'en différer 
l'exécution à la fin de la page (dans l'ordre d'apparition). 
On évite alors l'effet bloquant tout en permettant au navigateur 
de quand même commencer le téléchargement au plus tôt. 

Cet attribut a longtemps été supporté uniquement par Microsoft 
Internet Explorer, mais il fait désormais partie des spécifications 
de HTML 5 et commence à être supporté ailleurs. 

~~~~~~~ {.html .partial .oneline}
<script src="/chemin/vers/fichier.js" defer></script> 
~~~~~~~

L'attribut « async » est lui tout nouveau, arrivé aussi avec HTML 
5. Le téléchargement du fichier est initié immédiatement et 
le script sera exécuté dès que possible (à la fin du téléchargement), 
mais le navigateur est informé qu'il doit continuer l'analyse 
et le rendu du reste de la page en attendant. Les scripts étant 
chargés de façon asynchrone, l'ordre d'exécution n'est plus 
assuré : Il dépend de l'ordre de fin de téléchargement, qui ne 
peut être prévu avec certitude. 

~~~~~~~ {.html .partial}
<script src="/chemin/vers/fichier1.js" defer></script>
<script src="/chemin/vers/fichier2.js" defer></script>
<!-- rien ne garantit que le script 1 s'exécutera toujours avant le script 2 -->
~~~~~~~

Dans les deux cas, il faudra porter la plus grande attention aux 
dépendances entre vos codes javascript : Si jQuery est chargé 
avec l'attribut async, rien ne garantit que jQuery sera bien 
initialisé quand vous tenterez de l'utiliser dans une autre 
balise `<script>` plus loin. 

Si les deux attributs sont tous les deux utilisés sur la même balise, 
les anciens Microsoft Internet Explorer utiliseront defer, 
les nouveaux navigateurs utiliseront async, et les autres agirons 
comme pour une balise de script classique. 

#### Insertion DOM

**@TODO**

#### document.write

**@TODO**

#### Préchargement sans exécution

**@TODO**

#### Ajax

**@TODO**

### Bibliothèques complètes

**@TODO**

Iframe et événement onload
--------------------------

**@TODO**


Cas des fournisseurs externes
-----------------------------

Nous avons vu dans ce chapitre qu'un chargement optimal des ressources passe
par un bon timing. Charger les bonnes ressources au bon moment permet d'avoir
ces ressources disponibles quand on en aura besoin sans bloquer le
téléchargement des autres ressources plus prioritaires. Le respect des bonnes
pratiques citées ci-dessus permet d'y arriver mais il n'est pas toujours
possible de les appliquer quand on dépend de fournisseurs externes. En
particulier, les régies publicitaires sont connues pour être de mauvais élèves
en la matière. Il n'est pas rare que les publicités soient chargées via un
JavaScript qui utilisera `document.write`.

Il existe une astuce pour améliorer grandement ce cas : l'utilisation d'une
iframe intégrée. En effet, HTML5 offre de nouveaux attributs sur les balises
iframe dont deux nous intéressent tout particulièrement : `seamless` et 
`srcdoc`. Leur utilisation combinée va nous permettre d'encapsuler la 
publicité dans une iframe, ce qui aura pour effet de permettre au navigateur 
de continuer à charger la page sans attendre que la publicité soit affichée.

L'attribut `srcdoc` permet d'injecter le contenu de l'iframe sans
passer par une ressource externe. On pourra ainsi éviter de télécharger une
ressource externe supplémentaire.

L'attribut `seamless`, lorsqu'il est appliqué à une iframe, permet à son contenu
de ce comporter comme un élément normal du DOM plutôt que comme un nouveau
document. Le contenu de l'iframe hérite ainsi des styles provenant du document
parent et, plus important, le JavaScript provenant de la régie publicitaire
continuera de détecter le bon domaine.

En pratique, si le chargement de la publicité se fait avec les lignes
suivantes :

~~~~~~~ {.html .partial}
<script type="text/javascript"><!--
  ad_width = 300;
  ad_height = 250;
//--></script>
<script type="text/javascript" src="http://www.example.com/ad/show_ads.js">
</script>
~~~~~~~

on pourra l'englober dans une iframe de la façon suivante :

~~~~~~~ {.html .partial}
<iframe src="about:blank" seamless srcdoc="
  <script type=text/javascript><!--
    ad_width = 300;
    ad_height = 250;
  //--></script>
  <script src=http://www.example.com/ad/show_ads.js></script>
">
</iframe>
~~~~~~~

Malheureusement, l'attribut `srcdoc` n'est pris en charge que par certains
navigateurs modernes. Il convient donc de prévoir un mécanisme par défaut
pour les autres navigateurs. Il consistera à ajouter ces quelques lignes juste
après l'iframe pour revenir à un `document.write` en l'absence de prise en
charge de `srcdoc` :

~~~~~~~ {.html .partial}
<script>
  function supports_srcdoc() {
    return 'srcdoc' in document.createElement('iframe');
  };

  // Support for pre-srcdoc browsers
  var iframes = document.getElementsByTagName("iframe");
  var iframe = iframes && iframes[iframes.length-1];
  var srcdoc = iframe && iframe.getAttribute('srcdoc');
  if (srcdoc && !supports_srcdoc()) {
    iframe.parentNode.removeChild(iframe);
    document.write(srcdoc);
  }
</script>
~~~~~~~

Réduire les espaces vides
=========================

CDN
---

**@TODO**

Keep-alive
----------

**@TODO**

Le pipelining HTTP
------------------

Le pipelining HTTP (qu'on pourrait traduire plus ou moins par 
enfilage HTTP) est une fonctionnalité prévue dans la version 
1.1 de HTTP. Bien que cette version soit celle utilisée par quasiment 
tous les serveurs web et tous les navigateurs, le pipelining 
lui-même est peu implémenté. 

Il s'agit simplement d'autoriser le navigateur à enchaîner 
les requêtes HTTP sur un même fil TCP sans attendre la réponse 
correspondante. On évite alors la perte de temps due à la latence 
entre chaque requête (le temps entre l'envoi du dernier octet 
de la réponse par le serveur et celui de la réception du dernier 
octet de la requête HTTP suivante). 

![Impact du pipelining HTTP](img/chap07-impact-du-pipelining-http.png)

Malheureusement tous les serveurs web et tous les proxy ne supportent 
pas bien ce mode d'accès et il est difficile ou impossible de le 
détecter à l'avance. Mozilla Firefox sait utiliser le pipelining 
mais il reste désactivé par défaut pour compatibilité (on peut 
l'activer manuellement dans la page about:config). Seul Opéra 
et Firefox Mobile l'activent par défaut, avec un mécanisme pour 
tenter d'éviter de l'utiliser sur des serveurs potentiellement 
incompatibles. Ils maintiennent une liste noire fixe des serveurs 
à éviter et une liste dynamique basée sur les comportements constatés 
par le navigateur par le passé. Des réflexions sont toutefois 
menées pour améliorer l'utilisation du pipelining à l'avenir. 

De plus, ce mécanisme peut aussi ralentir le chargement des pages :
le navigateur télécharge en réalité les composants sur plusieurs 
fils TCP en parallèle. Un peu comme les queues au supermarché, 
si on se contente de regarder le nombre de personnes en attente 
(le nombre de requêtes) par queue sans regarder le remplissage 
des chariots (la taille des composants à télécharger), on peut 
finalement attendre plus longtemps que prévu. Ici il n'est pas 
possible de déterminer à l'avance la taille des composants à 
télécharger (le remplissage des chariots) ou de changer de queue 
en cours de route. Ainsi, il est possible qu'un téléchargement 
soit en attente sur un fil TCP alors même que les autres fils TCP 
n'ont plus aucune activité. 

Support navigateur : http://www.blaze.io/technical/http-pipelining-request-distribution-algorithms/ 

Le protocole SPDY
-----------------

SPDY (à prononcer en anglais comme « speedy », « rapide » en anglais) 
est une évolution du protocole HTTP créée par Google. Cette couche 
intermédiaire entre HTTP et TCP offre plusieurs fonctionnalités 
et en particulier : 

* la possibilité d'utiliser un même fil TCP pour multiplexer 
  plusieurs requêtes/réponses simultanées, optimisant ainsi 
  la bande passante ; 

* la possibilité de conseiller le navigateur dès les entêtes 
  sur les ressources qu'il devrait charger sans attendre ; 

* la possibilité pour le serveur d'envoyer des contenus de sa 
  propre initiative s'il est extrêmement probable que le client 
  en aura besoin ; 

* la compression des entêtes HTTP. 

Il est possible d'avoir plus de détails sur le protocole SPDY 
sur les serveurs du projet Chromium (moteur du navigateur de 
Google Chrome) à l'adresse [http://www.chromium.org/spdy/](http://www.chromium.org/spdy/). 

Les trois premières fonctionnalités, indépendamment, permettent 
entre autres d'optimiser les échanges et de supprimer les effets 
de la latence réseau dans le téléchargement de plusieurs composants 
sur un même serveur. Ces mécanismes demandent toutefois une 
plus grande intelligence (et donc une plus grande complexité) 
et du serveur web et du navigateur web. 

Pour l'instant le protocole est à l'état de proposition et n'est 
implémenté que (partiellement) par les navigateurs Google Chrome et Firefox
et par quelques serveurs, dont ceux de Google, Wordpress et Twitter.
Un module pour le serveur web Apache est toutefois en développement sur
[http://code.google.com/p/mod-spdy/](http://code.google.com/p/mod-spdy/).

----

**@TODO**


Applicatif
==========

Nous avons vu au chapitre 6 qu'un chargement optimal des ressources passe par
un bon timing. Charger les bonnes ressources au bon moment permet d'avoir ces
ressources disponibles quand on en aura besoin sans bloquer le téléchargement
des autres ressources plus prioritaires.


**@TODO**
Temps de réponse des sites web
========================

Projet disponible sur <https://github.com/edas/webperf-book>

Auteur initial : Éric Daspet <http://eric.daspet.name/>

Mainteneurs actuels du projet : 

* vous ? 
*  
*  


Autres contributeurs :

* Pour l'enrichissement et la contribution à du contenu 
  original : [Bruno Michel](https://github.com/nono)

* Pour les relectures avant la mise sous forme communautaire :
  Nicolas Hoizey, Mathieu Pillard, Rudy Rigot, Anthony Ricaud,
  Vincent Voyer et certainement un ou deux dont je n'ai pas 
  retrouvé la trace dans mes archives

* Un gros merci à [Yoav Weiss](https://github.com/yoavweiss) 
  pour l'outil qui a permis la conversion du contenu dans le 
  format Pandoc

* En vrac pour des corrections et typos : 
  [Denis Roussel](https://github.com/KuiKui), 
  [Benjamin Dos Santos](https://github.com/bdossantos),
  [DirtyF](https://github.com/DirtyF), Tanguy Martin,
  [Tomhtml](https://github.com/TOMHTML), 
  [Franek](https://github.com/franek),
  [Mikael Randy](https://github.com/mikaelrandy),
  Agnès Haasser, [Pascal Borreli](https://github.com/pborreli)


Applicatif
==========

pas d'eval
----------

tableaux fixes
--------------

code propre
-----------

code light
----------

pas d'expression
----------------

pas d'iframes
-------------

pas de alphaimageloader
-----------------------

sélecteurs js
-------------

var en js
---------

DOM
---

Reflow
------

Pas de gif animé
----------------

Optimiser
=========

premier accès ou acces futurs
-----------------------------

Préchargement
-------------

Ressenti utilisateur
--------------------

Mettre en avant le contenu principal et non le temps total
----------------------------------------------------------

Site mobile
-----------

[http://www.silicon.fr/les-consommateurs-fuient-les-sites-web-trop-lents-41997.html](http://www.silicon.fr/les-consommateurs-fuient-les-sites-web-trop-lents-41997.html) 

> **la moitié** des personnes interrogées estime que les temps 
> de chargement sur un téléphone mobile doivent être identiques 
> à ceux mesurés sur un ordinateur

media queries pour mobile first [http://www.slideshare.net/bryanrieger/rethinking-the-mobile-web-by-yiibu](http://www.slideshare.net/bryanrieger/rethinking-the-mobile-web-by-yiibu) 

Modifier la congestion TCP
--------------------------

Mesurer la performance
======================

Quels outils, quelles mesures, comment, quand, etc.
---------------------------------------------------

Recommandations
===============

Résumé
------

Les règles Yahoo!
-----------------

Mise en oeuvre
==============

Processus de décision pour les sprites et optimisations
-------------------------------------------------------

Processus à la publication (minification, etc.)
-----------------------------------------------

Mesures et monitoring
---------------------

Industrialisation
-----------------

Définitions et concepts techniques
==================================

Visibilité
----------

Seo
---

sem
---

taux de rebond
--------------

abandon
-------

taux de transformation
----------------------

iframe
------

frame
-----

javascript
----------

css
---

feuille de style
----------------

Favicon.ico
-----------

Front-end
---------

back-end
--------

html
----

http
----

IP
--

TCP
---

Latence
-------

Pipelining
----------

Spdy
----

Xbl
---

htc
---

W3C
---

Outils utiles
=============

Navigateurs (note sur les versions)
-----------------------------------

### Firefox (note sur about:config)

### WebKit / Safari

### Opera

### Microsoft Internet Explorer

Analyse de trafic
-----------------

### Firebug

### IBM Page Detailer

### AOL Page Test

Mise en oeuvre d'une plateforme « web » 

### Google Page Speed

### Yslow

### Dynatrace Ajax

### Speedtracer

### Opera et Safari

### Analyse HTTP

### HttpFox

### TamperData

Proxy de débogage
-----------------

### Charles

### Fiddler

Optimisation des images
-----------------------

### Smushit

### Pngcrush

### Optipng

### Jpegtran

Serveur web
-----------

### Apache

### PHP

Automatisation des mesures
--------------------------

### Jiffy

### yslow

### page test

### Show slow

Références externes
===================

Liens et citations du livre
---------------------------

Documentations et ressources externes
-------------------------------------

Annexes
=======

Mesure des sites exemple
------------------------

Liste des sites web testés, avec l'identifiant webpagetest.org 
correspondant aux navigateurs Microsoft Internet Explorer 
7 et 8. Les tests ont été réalisés avec une bande passante de 1,5 
Mb/s à partir des États Unis. Vous pouvez visualiser les résultats 
à l'adresse [http://webpagetest.org/result/???/](http://webpagetest.org/result/???/) 
où l'identifiant remplace les points d'interrogation. 

+--------------------+-------------+-------------+
| Site               | IE7         | IE8         |
+====================+=============+=============+
| Google – Accueil   | 090404_1B2C | 090404_1B2D |
+--------------------+-------------+-------------+
| MSN France         | 090404_1B2E | 090404_1B2F |
+--------------------+-------------+-------------+
| Orange France      | 090404_1B2G | 090404_1B2H |
+--------------------+-------------+-------------+
| Free – Portail     | 090404_1B2J | 090404_1B2K |
+--------------------+-------------+-------------+
| Yahoo! France      | 090404_1B2M | 090404_1B2N |
+--------------------+-------------+-------------+
| Pages Jaunes       | 090404_1B2P | 090404_1B2Q |
+--------------------+-------------+-------------+
| L’internaute       | 090404_1B2R | 090404_1B2S |
+--------------------+-------------+-------------+
| eBay France        | 090404_1B2T | 090404_1B2V |
+--------------------+-------------+-------------+
| Wikipedia France   | 090404_1B2W | 090404_1B2X |
+--------------------+-------------+-------------+
| Youtube Français   | 090404_1B2Y | 090404_1B2Z |
+--------------------+-------------+-------------+
| Facebook France    | 090404_1B30 | 090404_1B31 |
+--------------------+-------------+-------------+
| Microsoft France   | 090404_1B32 | 090404_1B33 |
+--------------------+-------------+-------------+
| Mappy France       | 090404_1B34 | 090404_1B35 |
+--------------------+-------------+-------------+
| Price Minister     | 090404_1B36 | 090404_1B37 |
+--------------------+-------------+-------------+
| TF1                | 090404_1B38 | 090404_1B39 |
+--------------------+-------------+-------------+
| Dailymotion France | 090404_1B3A | 090404_1B3B |
+--------------------+-------------+-------------+
| Apple France       | 090404_1B3C | 090404_1B3D |
+--------------------+-------------+-------------+
| Comment ça marche  | 090404_1B3E | 090404_1B3F |
+--------------------+-------------+-------------+
| Au féminin         | 090404_1B3G | 090404_1B3H |
+--------------------+-------------+-------------+
| Cdiscount          | 090404_1B3J | 090404_1B3K |
+--------------------+-------------+-------------+
| La Redoute         | 090404_1B3M | 090404_1B3N |
+--------------------+-------------+-------------+
| voilà              | 090404_1B3P | 090404_1B3Q |
+--------------------+-------------+-------------+
| Amazon France      | 090404_1B3R | 090404_1B3S |
+--------------------+-------------+-------------+
| Skyrock blog       | 090404_1B3T | 090404_1B3V |
+--------------------+-------------+-------------+
| Over blog          | 090404_1B3W | 090404_1B3X |
+--------------------+-------------+-------------+
| Fnac               | 090404_1B3Y | 090404_1B3Z |
+--------------------+-------------+-------------+
| Doctissimo         | 090404_1B40 | 090404_1B41 |
+--------------------+-------------+-------------+
| SFR                | 090404_1B42 | 090404_1B43 |
+--------------------+-------------+-------------+
| 3 Suisses          | 090404_1B44 | 090404_1B45 |
+--------------------+-------------+-------------+
| Allo Ciné          | 090404_1B46 | 090404_1B47 |
+--------------------+-------------+-------------+
| Le Monde           | 090404_1B48 | 090404_1B49 |
+--------------------+-------------+-------------+
| Eurosport France   | 090404_1B4A | 090404_1B4B |
+--------------------+-------------+-------------+
| Yahoo! News France | 090404_1B4C | 090404_1B4D |
+--------------------+-------------+-------------+
| Google – Recherche | 090404_1B4E | 090404_1B4F |
+--------------------+-------------+-------------+

* Le livre : https://github.com/edas/webperf-book/
* Les tickets en cours : https://github.com/edas/webperf-book/issues 
* Quelques liens à pour fouiller : https://groups.diigo.com/group/web-performance 
* La liste de diffusion webperf française : https://groups.google.com/group/performance-web?hl=fr

Participer
==========

(re)Lire
--------

1. Se rendre sur https://github.com/edas/webperf-book/
2. Cliquer sur « content » dans le cadre principal
3. Choisir un des chapitres
4. La vue « raw » peut être moins trompeuse parfois

Faire une proposition, remonter une anomalie, …
-----------------------------------------------

Rendez-vous dans la rubrique « issues ». Vérifiez que ce que vous souhaitez remonter ne l’est pas déjà, et cliquez sur le bouton vers « new issue ». 

En marge de la description de votre ticket, pensez à cliquer sur tous les labels à gauches qui vous semblent s’appliquer à votre cas. Vous en aurez probablement au moins un jaune et un d’une autre couleur.

Réaliser une modification
-------------------------

N’hésitez jamais à modifier, rien n’est sacré, faites ce qui vous semble pertinent et utile. Si ça ne va pas, ça sera retouché par quelqu'un d'autre.

### Solution rapide et simple

À partir d’un chapitre en lecture, cliquez sur « edit » en haut à droite, modifiez, décrivez le changement effectué dans le formulaire en bas de façon à ce qu’il puisse être compris hors contexte, et validez votre envoi. Vous devez être authentifié sur github pour cela.

### Solution complète (via GIT)

1. Faire un fork sur votre espace github
2. Faire une branche par groupe d’ajout/modification
3. Modifier et envoyer vos modifications sur la branche via git
4. Envoyer une pull-request
5. Lire les retours s’il y en a
6. Réaliser les corrections et améliorations, les envoyer
7. Retour à l’étape 5

Tant que la pull-request n’est pas acceptée, tout ce que vous ajouterez à la branche viendra automatiquement s’ajouter à la même pull-request (ce qui permet d’améliorer, corriger, compléter). Donc : Une branche par pull-request.

Contenu : La forme
==================

Format
------

Les fichiers sont des fichiers texte, codage UTF-8, avec la syntaxe Markdown et les extensions Pandoc. Github offre un apperçu avec sa propre extension de Markdown, qui ne comprend pas toutes les subtilités de Pandoc. Entre autres : les listes recommencent toujours à partir de 1, les tableaux ne sont pas compris et les images peuvent ne pas s’afficher et générer un lien cassé.

- Détails : http://johnmacfarlane.net/pandoc/README.html#pandocs-markdown 

Structure
---------

Les chapitres sont dans le répertoire « content », les images dans « content/img ». Tous les fichiers commencent par « chapXX- » où XX est le numéro du chapitre concerné. 
Les outils sont dans « tools », n’hésitez pas à en ajouter.

Images
------

Pensez à réaliser des images avec la meilleure qualité possible, et en très grand format. Elles seront recoupées plus tard lors de la réalisation du fichier final mais cela permet d’envisager une publication papier (qui a besoin de haute qualité).

En toutes occasions, préférez les formats vectoriels si vous le pouvez (par exemple pour les graphiques). Dans le cas contraire, la plupart des images (captures d’écran, graphiques, schéma, et tout ce qui contient du texte à lire) devraient utiliser le format PNG.

Contenu : Le fond
=================

Français - anglais
------------------

En toute occasion, préférez le terme français quand il existe. S’il vous semble peu usité, vous pouvez mettre la traduction anglaise entre parenthèses lors de la première occurrence.

Utiliser le français encourage à s’impliquer et comprendre les concepts sous-jacents, par rapport à des utilisations un peu « formule magique » de termes anglais.

Ton, approche personnelle
-------------------------

Le ton de la discussion, non formelle mais avec vouvoiement est préféré. J’ai anciennement beaucoup utilisé le « je » étant donné que le projet a démarré comme un projet personnel. Vous pouvez rendre plus neutres les tournures ou utiliser le « nous ».

Sources
-------

Précisez toujours la source des affirmations, particulièrement les affirmations chiffrées. Signalez tous les liens qui vous semblent manquer.

Cette oeuvre, création, site ou texte est sous licence Creative Commons  Attribution -  Partage dans les Mêmes Conditions 4.0 International. Pour accéder à une copie de cette licence, merci de vous rendre à l'adresse suivante http://creativecommons.org/licenses/by-sa/4.0/deed.fr ou envoyez un courrier à Creative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.


Creative Commons Attribution-ShareAlike 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.

Section 1 – Definitions.

Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.
Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.
BY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.
Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.
Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.
Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.
License Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.
Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.
Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.
Licensor means the individual(s) or entity(ies) granting rights under this Public License.
Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.
Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.
You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.
Section 2 – Scope.

License grant.
Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:
reproduce and Share the Licensed Material, in whole or in part; and
produce, reproduce, and Share Adapted Material.
Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.
Term. The term of this Public License is specified in Section 6(a).
Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.
Downstream recipients.
Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.
Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.
No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.
No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).
Other rights.

Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.
Patent and trademark rights are not licensed under this Public License.
To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.
Section 3 – License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the following conditions.

Attribution.

If You Share the Licensed Material (including in modified form), You must:

retain the following if it is supplied by the Licensor with the Licensed Material:
identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);
a copyright notice;
a notice that refers to this Public License;
a notice that refers to the disclaimer of warranties;
a URI or hyperlink to the Licensed Material to the extent reasonably practicable;
indicate if You modified the Licensed Material and retain an indication of any previous modifications; and
indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.
You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.
If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.
ShareAlike.
In addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.

The Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.
You must include the text of, or the URI or hyperlink to, the Adapter's License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.
You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter's License You apply.
Section 4 – Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:

for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;
if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and
You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.
For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.
Section 5 – Disclaimer of Warranties and Limitation of Liability.

 Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.
 To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.
The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.
Section 6 – Term and Termination.

This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.
Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:

automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or
upon express reinstatement by the Licensor.
For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.
For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.
Sections 1, 5, 6, 7, and 8 survive termination of this Public License.
Section 7 – Other Terms and Conditions.

The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.
Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.
Section 8 – Interpretation.

For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.
To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.
No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.
Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.

J’ai commencé à écrire un livre technique sur les temps de réponses des sites web il y a maintenant quelques années. Il est temps de faire le deuil de la forme du projet initial. 

Suite à des changements professionnels et personnels successifs j’ai eu de moins en moins de temps à y accorder et la rédaction plafonne irrémédiablement à 50%. Le contenu vieillit en regard des capacités actuelles des navigateurs et en regard de l’état de l’art. Voir mourir ce contenu dans lequel j’ai tant investi me semble trop difficile.

J’ai tenté d’y investir un peu plus de temps, de me faire accompagner par un auteur secondaire, de co-écrire, ou même de donner ce contenu à un auteur qui pourrait prendre la suite mais à chaque fois il est évident que le travail est énorme et que peu de gens sont à la fois suffisamment experts et avec assez de temps libre.

J'ai donc décidé de relancer la chose sous la forme d'un projet collaboratif. Pour l'instant la licence et les conditions sont rédigés dans l'unique objectif de compléter le contenu et le mettre à jour. Des conditions plus libres seront probablement données à partir de là.

Vous trouverez plus de renseignements complémentaires et les futures actualités sur <http://n.survol.fr/avec/webperf-book>, et en particulier dans deux billets : [Appel aux bonnes volontés](http://n.survol.fr/n/livre-webperf-appel-aux-bonnes-volontes) et [Licence](http://n.survol.fr/n/livre-en-redaction-communautaire-licence). Vous êtes invités à réagir directement sur ces billets et les suivants.

-- [Éric] (http://eric.daspet.name/)


Format d'édition
---------------------
Les contenus avec l'extension "md" sont codés dans le format [Markdown][], avec les extensions proposées par l'outil [Pandoc][]. Les contenus avec l'extension "yml" sont codés dans le format [Yaml][].

  [Markdown]: http://daringfireball.net/projects/markdown/
  [Pandoc]:  http://johnmacfarlane.net/pandoc/
  [Yaml]: http://www.yaml.org/
  
Licence et droits d'utilisation
------------------------------------

Ces contenus, sauf précision contraire (par exemple pour certaines illustrations appartenant à des tiers) sont distribués sous licence Creative Commons avec les mention Attribution Partage à l'identique. Le contenu exact de la licence est disponible dans le fichier license.md à la racine de ce projet.

