This is a simple script that adds events to Ganglia. We have it installed on
all hosts and whenever we do something we'll log it e.g.

ganglialog set sysctl xxx = 1000

This will show up on a graph for that host. 

Keep you messages short and avoid using special characters. I'd stick with
-,.= 

2011-10-25 Drew Stephens <drew@dinomite.net>
Copyright 2011, Clearspring, Inc.

This is free software; you can redistribute it and/or modify it under
the same terms as the Perl 5 programming language system itself.

USAGE

This pulls the XML file from gmetad (localhost:8651 by default) and pumps
it into Graphite (localhost:2023 the carbon-aggregator port by default).

I run it as a minutely cronjob:

    * * * * * root cd /home/web/gang2graph; perl gang2graph.pl

With 160 hosts and 38,000 metrics the elapsed time is less than 3 seconds
on a 4100 series AMD Opteron, but it is hobbled by Graphite's disk writing.
For this reason, gang2graph support specify a set of hosts to send to
Graphite, ignoring the rest.  Simply set the $host variable in gang2graph.pl
to a set of hosts to copy and only those will be copied.  The checking is a
regex, so subsets of hostnames works.  To copy metrics from this set of hosts:

    apb00.clearspring.com
    apb01.clearspring.com
    apw00.clearspring.com
    apw01.clearspring.com

but exclude apm*, adm*, etc. I use this $hosts:

    ['apb', 'apw']

Feel free to email me with questions.

go.ganglia
==========

gmetric: http://godoc.org/github.com/ganglia/ganglia_contrib/ganglia-go/gmetric

gmon: http://godoc.org/github.com/ganglia/ganglia_contrib/ganglia-go/gmon

See ../README
For the debian package, ganglia_logtailer_helper.pyc and tailnostate.py are 
stored in /usr/share/ganglia-logtailer, along with all base plugins that ship 
with ganglia-logtailer.  Plugins you create should be put in 
/usr/local/share/ganglia-logtailer/.  

Imported from

https://bitbucket.org/maplebed/ganglia-logtailer

##################
##
##  ganglia-logtailer
##
##  ganglia-logtailer is a python script that will tail any log file,
##  crunch the data collected, and report summary data to ganglia.
##
##  This directory contains the script ganglia-logtailer, its required
##  helper classes, and two example classes, one generic and one for
##  parsing Apache logs.
##
##  Copyright Linden Lab, 2008
##  License to modify and redistribute granted under the GPL v2 or later
##
##################


0. Table of Contents
1. Overview
2. Installation
3. License

1. Overview

Many metrics associated with ganglia and gmetric plugins are rather easy to
collect; you poll the relevant application for a value and report it.  Examples
are asking MySQL for the number of questions and calculating queries per
second, or asking iostat for the percentage disk I/O currently being used.
However, there are a large number of applications out there that don't support
being queried for interesting data, but do provide a log file which, when
properly parsed, yields the interesting data we desire.  An example of the
latter category is Apache, which does not furnish any interface for measuring
queries per second, yet has a log file allowing you to count how many queries
come in over a specific time period.

ganglia-logtailer is designed to make it easy to parse any log file, pull out
the information you desire, and plug it into ganglia to make pretty graphs.

ganglia-logtailer is comprised of three parts:
* ganglia-logtailer - the python script doing the real work
* ganglia_logtailer_helper.py and tailnostate.py - supporting python classes
* ApacheLogtailer.py, DummyLogtailer.py, etc. - user modifiable, log-file specific classes

You must modify DummyLogtailer.py or write new FooLogtailer.py classes for
each of the log files you wish to parse.  As every log file has a specific
format, you must write the regular expression to properly pick out interesting
information for your specific application.  As each application has different
ways of expressing what might be interesting metrics to graph, you must write
the functions to collect the information present in each line of the log file.

DummyLogtailer.py is an example file; it does nothing other than count the
number of lines present in the log file and report lines per second.  However,
it does have extensive comments explaining the purpose of each of the functions
present in the file.  I would recommend reading that file and modifying it to
do more interesting things.

ApacheLogtailer.py is a fully functional class parsing an Apache log.  At
Linden Lab, we use a custom log format (and include the very interesting %D -
time to execute the query in microseconds), so the regular expression included
there will probably have to be changed for your environment.
ApacheLogtailer.py defines and returns the number of Apache requests per
second, also broken out by return code (200, 300, 400, 500), and the average,
maximum, and 90th percentile run time of all queries caught during the sample
period.

The rest of the *Logtailer.py classes present are customized for different
types of logs (postfix, bind, etc.)

ganglia-logtailer can be invoked in two different modes, either as a daemon
(which tells it to run as a persistent process) or invoked from cron on a
regular basis.  I recommend using daemon mode for testing, but invoking it from
cron every 1-5 minutes for deploy.  I make this recommendation because (aside
from minimizing the number of running daemons), there are no start scripts to
invoke daemon mode on system boot, and there is no facility to relaunch the
process if it were to crash or raise an exception.

ganglia-logtailer will log certain bits of information to
/var/log/ganglia/ganglia_logtailer in case of error.  Log level is variable by
modfying ganglia-logtailer and editing the following line:
logger.setLevel(logging.INFO) Look up the 'logging' python module for valid
values.  (logging.DEBUG is a good bet)

2. Installation

ganglia-logtailer depends on the 'logtail' package
(http://packages.debian.org/etch/logtail) when run in cron mode.

i.   Copy ganglia-logtailer to /usr/local/bin/ (or wherever you store
       unpackaged binaries)
ii.  Copy ganglia_logtailer_helper.py and tailnostate.py to
       /usr/local/share/ganglia-logtailer (or somewhere in your python search 
       path)
iii. Copy ApacheLogtailer.py and DummyLogtailer.py to /usr/local/share/ganglia-logtailer
       (or somewhere in your python search path)

Create the directory ganglia-logtailer uses to store state:
/var/lib/ganglia-logtailer/

Test the installation by invoking ganglia-logtailer with the DummyLogtailer
class on a log file:
# ganglia-logtailer --classname DummyLogtailer --log_file /var/log/daemon.log --mode daemon
wait 30s to 1m, then check and see whether your new metric is present in
ganglia.

If all goes well, try out one of the real modules:
# ganglia-logtailer --classname PostfixLogtailer --log_file /var/log/mail.log --mode daemon

If that works as well, deploy!  Add the following to
/etc/cron.d/ganglia-logtailer
* * * * * root /usr/local/bin/ganglia-logtailer --classname PostfixLogtailer --log_file /var/log/mail.log --mode cron

3. License

These scripts are all released under the GPL v2 or later.  For a full
description of the licence, please visit http://www.gnu.org/licenses/gpl.txt




Ganglia ramdisk is a collection of a cronjob and a start script that help you
set up the gmetad rrds repository on a ramdisk intsead of using physical disk.
This helps as your cluster grows. Missing is grub.conf, which should be
modified to specify the size of the ramdisk you wish to create. Also missing is
switching /var/lib/ganglia/rrds (or wherever your default storage is) to a
symlink and pointing it to the ramdisk.


This script will look for any gmetad.conf files in /etc/gmetad. It will then grab the xml_port, and attempt to connect to each port.
It will grab the first 512 bytes of data. If it retrieves no data, or cannot connect it will then restart gmetad (with -r flag)

Things to do:

- Have it restart only the instance that is down
-Have it verify xml tag in the data to verify it is xml

Email any questions to jim.greene@gmail.com

This is a modified version of embedded gmetric written in Python. It has been modified from

http://code.google.com/p/embeddedgmetric/

Main changes are

  - Support for Ganglia 3.1
  - Ability to specify metric group which gets grouped in the Ganglia Web UI. 


Latest version can also be used inside your own Python code with optionally
reading system's gmond.conf e.g.

========= CODE ==========

from gmetric import get_gmetrics

# Parse the config file to get all gmetric channels
gmetrics = get_gmetrics('/etc/ganglia/gmond.conf')
spoof = "%s:%s" % (hostname, hostname)

Then you can do thins like these

    for gmetric in gmetrics:
        gmetric.send('uptime', uptime,
                     TYPE='float', UNITS='seconds', SPOOF=spoof,
                     GROUP='mydaemon', DMAX=20)
        gmetric.send('some_count', upstreamCount,
                     TYPE='float', SPOOF=spoof, UNITS='count',
                     GROUP='mydaemon', DMAX=3600)


Acknowledgements:

Thanks to Ilya Grigorik for posting Ruby Gmetric

https://github.com/igrigorik/gmetric

It was much easier to figure out proper XDR packet packing in his code than looking through the C code

GMOND-DEBUG
===========

BUILDING
--------

```
. source.env
for i in gems/cache/*.gem; do gem install $i; done
```

RUNNING
-------

```
. source.env
./bin/gmond-debug
```


OLD DOCUMENTATION
-----------------

### What is this project about:

There is not one monitoring project to rule them all:

Ganglia, Graphite, Collectd, Opentsdb, ... they all have their specific unique functionality and their associate unique storage.

Instead of trying to create one central storage, we want to send the different metric information, to each monitoring solution for their optimized function.

This project's code will:

- listen into the gmond UDP protocol 
- optionally poll existing gmond's and put the message on to a 0mq (pub/sub).

From there, other subscribers can pull the information into graphite, collectd, opentsdb etc..

We have deliberately chosen not to go for peer to peer communication, but for a bus/queue oriented system.

It currently doesn't do more than put things on the queue, the next step is to write subscribers for the other monitoring systems.

And maybe , just maybe,  this will evolve into a swiss-army knife of monitoring/metrics conversion ....

### Thanks!

A big thanks to Vladimir Vuksan (@vvuksan) for helping me out with the original proof of concept!

### Requirements:
#### Centos

    # yum install libxml2-devel
    # yum install libxslt-devel
    # yum install zeromq-devel
    # yum install uuid-devel
    # yum install json-c-devel

### Configuring gmond

Just add another udp send channel for your existing gmond's

    udp_send_channel {
        host = 127.0.0.1
        port = 1234
    }

### Running it:

    gmond-zmq - A gmond UDP receiver that pushes things to a 0mq Pub/Sub

    Usage: gmond-zmq [-p port] [-P file] [-d] [-k]
    gmond-zmq --help

      -p, --port PORT           Specify port
    (default: 1234)
      -P, --pid FILE            save PID in FILE when using -d option.
    (default: /var/run/gmond-zmq.pid)
      -d, --daemon              Daemonize mode
      -k, --kill [PORT]         Kill specified running daemons - leave blank to kill all.
      -u, --user USER           User to run as
      -G, --group GROUP         Group to run as
      --gmond-host [HOST]   hostname/ip address of the gmond to poll
      --gmond-port [PORT]   tcp port of the gmond to poll
      --gmond-interval [seconds]
    interval to poll the gmond, 0 = disable (default)
      --zmq-port [PORT]     tcp port of the zmq publisher
      --zmq-host [HOST]     hostname/ip address of the zmq publisher
      -v, --verbose             more verbose output
      -t, --test-zmq            Starts a test zmq subscriber
      -?, --help                Display this usage information.

### Message examples

    {"timestamp":1324639623,"payload":{"name":"machine_type","val":"x86_64","slope":"zero","dmax":"0","tn":"809","units":"","type":"string","tmax":"1200","hostname":"localhost"},"id":"f6412a10-0f86-012f-0bdb-080027701f72","context":"METRIC","source":"GMOND"}
    {"timestamp":1324639623,"payload":{"name":"proc_total","val":"105","slope":"both","dmax":"0","tn":"89","units":" ","type":"uint32","tmax":"950","hostname":"localhost"},"id":"f6415250-0f86-012f-0bdc-080027701f72","context":"METRIC","source":"GMOND"}
    {"timestamp":1324639623,"payload":{"name":"cpu_num","val":"1","slope":"zero","dmax":"0","tn":"809","units":"CPUs","type":"uint16","tmax":"1200","hostname":"localhost"},"id":"f6417410-0f86-012f-0bdd-080027701f72","context":"METRIC","source":"GMOND"}
    {"timestamp":1324639623,"payload":{"name":"cpu_speed","val":"2800","slope":"zero","dmax":"0","tn":"809","units":"MHz","type":"uint32","tmax":"1200","hostname":"localhost"},"id":"f64186c0-0f86-012f-0bdf-080027701f72","context":"METRIC","source":"GMOND"}
    {"timestamp":1324639623,"payload":{"name":"pkts_out","val":"3.27","slope":"both","dmax":"0","tn":"49","units":"packets/sec","type":"float","tmax":"300","hostname":"localhost"},"id":"f641aa00-0f86-012f-0be0-080027701f72","context":"METRIC","source":"GMOND"}
    {"timestamp":1324639623,"payload":{"name":"swap_free","val":"741752","slope":"both","dmax":"0","tn":"89","units":"KB","type":"float","tmax":"180","hostname":"localhost"},"id":"f641c720-0f86-012f-0be1-080027701f72","context":"METRIC","source":"GMOND"}

### Some inspiration:

- [The Ganglia XDR protocol](https://github.com/fastly/ganglia/blob/master/lib/gm_protocol.x)
- [Gmetric library - ruby lib to send ganglia metrics](https://github.com/igrigorik/gmetric/blob/master/lib/gmetric.rb)
- [Gmond Source code](https://github.com/ganglia/monitor-core/blob/master/gmond/gmond.c#L1211)
- [Gmetric Python code](https://github.com/ganglia/ganglia_contrib/blob/master/gmetric-python/gmetric.py#L107)
- [Vladimir Vuksan sample Python Gmond Listener code](https://gist.github.com/1377993)
- [My initial sample Gmond listener code](https://gist.github.com/1376525)
- [Ruby XDR gem](http://rubyforge.org/projects/ruby-xdr/)

More details on Ganglia Graphite integration please read.

"http://vuksan.com/blog/2010/09/29/integrating-graphite-with-ganglia/":http://vuksan.com/blog/2010/09/29/integrating-graphite-with-ganglia/

====================
check_ganglia_metric
====================

`check_ganglia_metric <http://pypi.python.org/pypi/check_ganglia_metric/>`_ is
a Nagios plugin that allows you to trigger alerts on any Ganglia metric.

Ganglia statistics for nagios

This script should be called from cron once per minute. It queries nagios for statistics about its process (eg number of checks, average check delay, number of warning / critical problems, etc.) and reports them to ganglia every 15 seconds.

Note that this is not a script to check ganglia metrics from nagios; for that go to https://github.com/ganglia/monitor-core/wiki/Integrating-Ganglia-with-Nagios


This is the official repository for hosting all other user-contributed tools and programs.

To have your programs added here, please fork the repository, create separate sub-directories for each program 
and submit a pull request.

If you have any questions, you could reach us at:

ganglia-developers@lists.sourceforge.net

(subscription required: http://lists.sourceforge.net/lists/listinfo/ganglia-developers)

Alternatively, you could join our IRC channel on irc.freenode.net #ganglia and
ping one of the developers.

Thank you for your contribution!

  -- Ganglia Development Team

