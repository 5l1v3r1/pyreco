__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Hebel documentation build configuration file, created by
# sphinx-quickstart on Mon Nov 25 19:20:29 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import mock
import sys, os

MOCK_MODULES = ['hebel.pycuda_ops', 'hebel.pycuda_ops.linalg',
                'hebel.pycuda_ops.cuda', 'hebel.pycuda_ops.cudart',
                'hebel.pycuda_ops.elementwise', 'hebel.pycuda_ops.matrix',
                'hebel.pycuda_ops.reductions', 'hebel.pycuda_ops.softmax',
                'hebel.pycuda_ops.cublas', 'hebel.pycuda_ops.cudadrv', 'skdata',
                'skdata.mnist', 'skdata.mnist.view', 'pycuda', 'pycuda.autoinit',
                'pycuda.compiler', 'pycuda.cumath', 'pycuda.driver',
                'pycuda.elementwise', 'pycuda.gpuarray', 'numpy']

for mod_name in MOCK_MODULES:
    sys.modules[mod_name] = mock.Mock()

sys.path = ['../'] + sys.path

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo', 'sphinx.ext.coverage', 'sphinx.ext.mathjax']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Hebel'
copyright = u'2013, Hannes Bretschneider'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
from .hebel.version import version
# The full version, including alpha/beta/rc tags.
from .hebel.version import release

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Hebeldoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Hebel.tex', u'Hebel Documentation',
   u'Hannes Bretschneider', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'hebel', u'Hebel Documentation',
     [u'Hannes Bretschneider'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Hebel', u'Hebel Documentation',
   u'Hannes Bretschneider', 'Hebel', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = mnist_neural_net_deep_script
#!/usr/bin/env python

import hebel
from hebel.models import NeuralNet
from hebel.optimizers import SGD
from hebel.parameter_updaters import MomentumUpdate
from hebel.data_providers import MNISTDataProvider
from hebel.monitors import ProgressMonitor
from hebel.schedulers import exponential_scheduler, linear_scheduler_up

hebel.init(random_seed=0)

# Initialize data providers
train_data = MNISTDataProvider('train', batch_size=100)
validation_data = MNISTDataProvider('val')
test_data = MNISTDataProvider('test')

D = train_data.D                        # Dimensionality of inputs 
K = 10                                  # Number of classes

# Create model object
model = NeuralNet(n_in=train_data.D, n_out=K,
                  layers=[2000, 2000, 2000, 500],
                  activation_function='relu',
                  dropout=True, input_dropout=0.2)

# Create optimizer object
progress_monitor = ProgressMonitor(
    experiment_name='mnist',
    save_model_path='examples/mnist',
    save_interval=5,
    output_to_log=True)

optimizer = SGD(model, MomentumUpdate, train_data, validation_data, progress_monitor,
                learning_rate_schedule=exponential_scheduler(5., .995),
                momentum_schedule=linear_scheduler_up(.1, .9, 100))

# Run model
optimizer.run(50)

# Evaulate error on test set
test_error = model.test_error(test_data)
print "Error on test set: %.3f" % test_error

########NEW FILE########
__FILENAME__ = neural_net_regression_example
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

"""Example of neural net with a linear regression output layer, using
the Boston dataset.

"""

def main():
    import numpy as np
    import pycuda.autoinit
    from pycuda import gpuarray
    from skdata import toy
    from hebel.data_providers import BatchDataProvider
    from hebel.models import NeuralNetRegression
    from hebel.optimizers import SGD
    from hebel.parameter_updaters import SimpleSGDUpdate
    from hebel.monitors import SimpleProgressMonitor
    from hebel.schedulers import exponential_scheduler

    # Get data
    data_cpu, targets_cpu = toy.Boston().regression_task()
    data = gpuarray.to_gpu(data_cpu.astype(np.float32))
    targets = gpuarray.to_gpu(targets_cpu.astype(np.float32))
    data_provider = BatchDataProvider(data, targets)

    # Create model object
    model = NeuralNetRegression(n_in=data_cpu.shape[1], n_out=targets_cpu.shape[1],
                                layers=[100], activation_function='relu')
    
    # Create optimizer object
    optimizer = SGD(model, SimpleSGDUpdate, data_provider, data_provider,
                    learning_rate_schedule=exponential_scheduler(.1, .9999),
                    early_stopping=True)
    optimizer.run(3000)
    
if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = config
# Copyright (c) 2008--2011, Theano Development Team, Hannes Bretschneider
# All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:

#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in the
#       documentation and/or other materials provided with the distribution.
#     * Neither the name of Theano nor the names of its contributors may be
#       used to endorse or promote products derived from this software without
#       specific prior written permission.

# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ''AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS

"""Parse YAML configuration files that describe experiment setups.
Heavily copied from pylearn2
"""

import re, yaml, os, uuid
from .utils.call_check import checked_call
from .utils import serial
from .utils.string_utils import match
import warnings
from itertools import izip

is_initialized = False
root = os.path.curdir

def run_from_config(yaml_src):
    config = load(yaml_src)
    optimizer = config['optimizer']
    run_conf = config['run_conf']
    run_conf['yaml_config'] = yaml_src
    run_conf['task_id'] = str(uuid.uuid4())
    optimizer.run(**run_conf)

    if config.has_key('test_dataset'):
        test_data = config['test_dataset']['test_data']
        model = optimizer.model
        progress_monitor = optimizer.progress_monitor

        test_error = model.test_error(test_data, average=True)
        progress_monitor.test_error = test_error

def load(stream, overrides=None, **kwargs):
    """
    Loads a YAML configuration from a string or file-like object.

    Parameters
    ----------
    stream : str or object
        Either a string containing valid YAML or a file-like object
        supporting the .read() interface.
    overrides : dict, optional
        A dictionary containing overrides to apply. The location of
        the override is specified in the key as a dot-delimited path
        to the desired parameter, e.g. "model.corruptor.corruption_level".

    Returns
    -------
    graph : dict or object
        The dictionary or object (if the top-level element specified an
        Python object to instantiate).

    Notes
    -----
    Other keyword arguments are passed on to `yaml.load`.
    """

    global is_initialized
    if not is_initialized:
        initialize()

    if isinstance(stream, basestring):
        string = stream
    else:
        string = '\n'.join(stream.readlines())

    # processed_string = preprocess(string)

    proxy_graph = yaml.load(string, **kwargs)

    from . import init
    init_dict = proxy_graph.get('init', {})
    init(**init_dict)
    
    if overrides is not None:
        handle_overrides(proxy_graph, overrides)
    return instantiate_all(proxy_graph)


def load_path(path, overrides=None, **kwargs):
    """
    Convenience function for loading a YAML configuration from a file.

    Parameters
    ----------
    path : str
        The path to the file to load on disk.
    overrides : dict, optional
        A dictionary containing overrides to apply. The location of
        the override is specified in the key as a dot-delimited path
        to the desired parameter, e.g. "model.corruptor.corruption_level".

    Returns
    -------
    graph : dict or object
        The dictionary or object (if the top-level element specified an
        Python object to instantiate).

    Notes
    -----
    Other keyword arguments are passed on to `yaml.load`.
    """
    f = open(path, 'r')
    content = ''.join(f.readlines())
    f.close()

    if not isinstance(content, str):
        raise AssertionError("Expected content to be of type str but it is "+str(type(content)))

    return load(content, **kwargs)


def handle_overrides(graph, overrides):
    """
    Handle any overrides for this model configuration.

    Parameters
    ----------
    graph : dict or object
        A dictionary (or an ObjectProxy) containing the object graph
        loaded from a YAML file.
    overrides : dict
        A dictionary containing overrides to apply. The location of
        the override is specified in the key as a dot-delimited path
        to the desired parameter, e.g. "model.corruptor.corruption_level".
    """
    for key in overrides:
        levels = key.split('.')
        part = graph
        for lvl in levels[:-1]:
            try:
                part = part[lvl]
            except KeyError:
                raise KeyError("'%s' override failed at '%s'", (key, lvl))
        try:
            part[levels[-1]] = overrides[key]
        except KeyError:
            raise KeyError("'%s' override failed at '%s'", (key, levels[-1]))


def instantiate_all(graph):
    """
    Instantiate all ObjectProxy objects in a nested hierarchy.

    Parameters
    ----------
    graph : dict or object
        A dictionary (or an ObjectProxy) containing the object graph
        loaded from a YAML file.

    Returns
    -------
    graph : dict or object
        The dictionary or object resulting after the recursive instantiation.
    """

    def should_instantiate(obj):
        classes = [ObjectProxy, dict, list]
        return True in [isinstance(obj, cls) for cls in classes]

    if not isinstance(graph, list):
        for key in graph:
            if should_instantiate(graph[key]):
                graph[key] = instantiate_all(graph[key])
        if hasattr(graph, 'keys'):
            for key in graph.keys():
                if should_instantiate(key):
                    new_key = instantiate_all(key)
                    graph[new_key] = graph[key]
                    del graph[key]

    if isinstance(graph, ObjectProxy):
        graph = graph.instantiate()

    if isinstance(graph, list):
        for i, elem in enumerate(graph):
            if should_instantiate(elem):
                graph[i] = instantiate_all(elem)

    return graph


class ObjectProxy(object):
    """
    Class used to delay instantiation of objects so that overrides can be
    applied.
    """
    def __init__(self, cls, kwds, yaml_src):
        """

        """
        self.cls = cls
        self.kwds = kwds
        self.yaml_src = yaml_src
        self.instance = None

    def __setitem__(self, key, value):
        self.kwds[key] = value

    def __getitem__(self, key):
        return self.kwds[key]

    def __iter__(self):
        return self.kwds.__iter__()

    def keys(self):
        return list(self.kwds)

    def instantiate(self):
        """
        Instantiate this object with the supplied parameters in `self.kwds`,
        or if already instantiated, return the cached instance.
        """
        if self.instance is None:
            self.instance = checked_call(self.cls, self.kwds)
        #endif
        try:
            self.instance.yaml_src = self.yaml_src
        except AttributeError:
            pass
        return self.instance


def try_to_import(tag_suffix):
    components = tag_suffix.split('.')
    modulename = '.'.join(components[:-1])
    try:
        exec('import %s' % modulename)
    except ImportError, e:
        # We know it's an ImportError, but is it an ImportError related to
        # this path,
        # or did the module we're importing have an unrelated ImportError?
        # and yes, this test can still have false positives, feel free to
        # improve it
        pieces = modulename.split('.')
        str_e = str(e)
        found = True in [piece.find(str(e)) != -1 for piece in pieces]

        if found:
            # The yaml file is probably to blame.
            # Report the problem with the full module path from the YAML
            # file
            raise ImportError("Could not import %s; ImportError was %s" %
                              (modulename, str_e))
        else:

            pcomponents = components[:-1]
            assert len(pcomponents) >= 1
            j = 1
            while j <= len(pcomponents):
                modulename = '.'.join(pcomponents[:j])
                try:
                    exec('import %s' % modulename)
                except:
                    base_msg = 'Could not import %s' % modulename
                    if j > 1:
                        modulename = '.'.join(pcomponents[:j-1])
                        base_msg += ' but could import %s' % modulename
                    raise ImportError(base_msg + '. Original exception: '+str(e))
                j += 1



    try:
        obj = eval(tag_suffix)
    except AttributeError, e:
        try:
            # Try to figure out what the wrong field name was
            # If we fail to do it, just fall back to giving the usual
            # attribute error
            pieces = tag_suffix.split('.')
            module = '.'.join(pieces[:-1])
            field = pieces[-1]
            candidates = dir(eval(module))

            msg = ('Could not evaluate %s. ' % tag_suffix) + \
            'Did you mean ' + match(field, candidates) +'? '+ \
            'Original error was '+str(e)

        except:
            warnings.warn("Attempt to decipher AttributeError failed")
            raise AttributeError( ('Could not evaluate %s. ' % tag_suffix) +
                'Original error was '+str(e))
        raise AttributeError( msg )
    return obj


def multi_constructor(loader, tag_suffix, node):
    """
    Constructor function passed to PyYAML telling it how to construct
    objects from argument descriptions. See PyYAML documentation for
    details on the call signature.
    """
    yaml_src = yaml.serialize(node)
    mapping = loader.construct_mapping(node)
    if '.' not in tag_suffix:
        classname = tag_suffix
        rval = ObjectProxy(classname, mapping, yaml_src)
    else:
        classname = try_to_import(tag_suffix)
        rval = ObjectProxy(classname, mapping, yaml_src)

    return rval


def multi_constructor_pkl(loader, tag_suffix, node):
    """
    Constructor function passed to PyYAML telling it how to load
    objects from paths to .pkl files. See PyYAML documentation for
    details on the call signature.
    """

    mapping = loader.construct_yaml_str(node)
    if tag_suffix != "" and tag_suffix != u"":
        raise AssertionError('Expected tag_suffix to be "" but it is "'+tag_suffix+'"')

    rval = ObjectProxy(None, {}, yaml.serialize(node))
    rval.instance = serial.load(mapping)

    return rval


def multi_constructor_import(loader, tag_suffix, node):
    yaml_src = yaml.serialize(node)
    mapping = loader.construct_mapping(node)
    if '.' not in tag_suffix:
        raise yaml.YAMLError("import tag suffix contains no '.'")
    else:
        rval = try_to_import(tag_suffix)
    return rval

def multi_constructor_include(load, tag_suffix, node):
    global root

    old_root = root

    filename = os.path.join(root, loader.construct_scalar(node))
    root = os.path.split(filename)[0]
    data = yaml.load(open(filename), 'r')

    root = old_root
    return data

def initialize():
    """
    Initialize the configuration system by installing YAML handlers.
    Automatically done on first call to load() specified in this file.
    """
    global is_initialized
    # Add the custom multi-constructor
    yaml.add_multi_constructor('!obj:', multi_constructor)
    yaml.add_multi_constructor('!pkl:', multi_constructor_pkl)
    yaml.add_multi_constructor('!import:', multi_constructor_import)
    yaml.add_multi_constructor('!include:', multi_constructor_include)

    def import_constructor(loader, node):
        value = loader.construct_scalar(node)
        return try_to_import(value)

    yaml.add_constructor('!import', import_constructor)
    yaml.add_implicit_resolver(
        '!import',
        re.compile(r'(?:[a-zA-Z_][\w_]+\.)+[a-zA-Z_][\w_]+')
    )
    is_initialized = True

########NEW FILE########
__FILENAME__ = cross_validation
from .utils.math import ceil_div
import numpy as np
import os
from hebel.optimizers import SGD

class CrossValidation(object):
    def __init__(self, config, data):

        self.n_folds = config['n_folds']
        self.n_data = config['n_data']
        self.validation_share = config['validation_share']

        self.fold_size = ceil_div(self.n_data, self.n_folds)
        self.N_train_validate = self.n_data - self.fold_size
        self.N_train = int(np.ceil((1. - self.validation_share) * self.N_train_validate))

        self.models_cv = []
        self.progress_monitors_cv = []
        self.fold_idx = []

        self.fold_stats = []

        self.train_error = {
            'training_error': [],
            'validation_error': []
        }

        self.predictions = None
        self.config = config
        self.data = data

        np.random.seed(config.get('numpy_seed'))

    def run_fold(self, k):
        fold_range = (k*self.fold_size, min((k+1)*self.fold_size, self.n_data))
        test_idx = np.arange(fold_range[0], fold_range[1], dtype=np.int32)

        train_validate_idx = np.random.permutation(
            np.r_[np.arange(0, fold_range[0], dtype=np.int32),
                  np.arange(fold_range[1], self.n_data, dtype=np.int32)])
        train_idx = train_validate_idx[:self.N_train]
        validate_idx = train_validate_idx[self.N_train:]

        self.fold_idx.append({
            'test_idx': test_idx,
            'train_idx': train_idx,
            'validate_idx': validate_idx
        })

        dp_train = self.make_data_provider(train_idx, self.config['batch_size'])
        dp_validate = self.make_data_provider(validate_idx, self.config['batch_size'])
        dp_test = self.make_data_provider(test_idx, test_idx.shape[0])

        model = self.make_model()
        self.models_cv.append(model)

        progress_monitor = self.make_progress_monitor(k)
        self.progress_monitors_cv.append(progress_monitor)

        learning_rate_schedule = self.config['learning_rate_fct'](**self.config['learning_rate_params'])

        momentum_schedule = self.config['momentum_schedule_fct'](**self.config['momentum_schedule_params']) \
                            if 'momentum_schedule_fct' in self.config else None
        
        optimizer = SGD(model, self.config['parameter_updater'], dp_train, dp_validate,
                        progress_monitor,
                        learning_rate_schedule=learning_rate_schedule,
                        momentum_schedule=momentum_schedule,
                        early_stopping=self.config.get('early_stopping', True))

        optimizer.run(self.config['epochs'], yaml_config=self.config['yaml_config'])

        stats = self.get_stats(dp_train, dp_test, model)
        self.fold_stats.append(stats)

        predictions_fold = model.feed_forward(dp_test.data).get()
        self.predictions = np.r_[self.predictions, predictions_fold] \
                           if self.predictions is not None else predictions_fold

        self.make_figures(model, progress_monitor, k)
        
        self.train_error['training_error'].append(progress_monitor.train_error)
        self.train_error['validation_error'].append(progress_monitor.validation_error)

    def run(self):
        for k in range(self.n_folds):
            self.run_fold(k)

    def make_data_provider(self, idx, batch_size):
        raise NotImplementedError

    def make_model(self):
        raise NotImplementedError

    def make_progress_monitor(self, fold):
        raise NotImplementedError

    def get_stats_func(self, dp_train, dp_test, model):
        return {}

    def make_figures(self, model, progress_monitor, fold):
        pass

    def post_run(self):
        pass
########NEW FILE########
__FILENAME__ = data_providers
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

""" 
All data consumed by Hebel models must be provided in the form of
``DataProvider`` objects. ``DataProviders`` are classes that provide
iterators which return batches for training. By writing custom
``DataProviders```, this creates a lot of flexibility about where data
can come from and enables any sort of pre-processing on the data. For
example, a user could write a ``DataProvider`` that receives data from
the internet or through a pipe from a different process. Or, when
working with text data, a user may define a custom ``DataProvider`` to
perform tokenization and stemming on the text before returning it.

A ``DataProvider`` is defined by subclassing the
:class:`hebel.data_provider.DataProvider` class and must implement at
a minimum the special methods ``__iter__`` and ``next``.
"""

import numpy as np
from pycuda import gpuarray

class DataProvider(object):
    """ This is the abstract base class for ``DataProvider``
    objects. Subclass this class to implement a custom design. At a
    minimum you must provide implementations of the ``next`` method.
    """
    
    def __init__(self, data, targets, batch_size):
        self.data = data
        self.targets = targets

        self.N = data.shape[0]

        self.i = 0
        self.batch_size = batch_size

    @property
    def batch_size(self):
        return self._batch_size

    @batch_size.setter
    def batch_size(self, value):
        self._batch_size = value
        self._make_batches()

    def _make_batches(self):
        self.data_batches = tuple(
            self.data[i:i+self.batch_size]
            for i in range(0, self.N, self.batch_size)
        )
        self.targets_batches = tuple(
            self.targets[i:i+self.batch_size]
            for i in range(0, self.N, self.batch_size)
        )
        self.n_batches = len(self.data_batches)

    def __getitem__(self, batch_idx):
        raise NotImplementedError

    def __iter__(self):
        self.i = 0
        return self

    def next(self):
        raise NotImplementedError

    @property
    def shape(self):
        return self.data.shape


class MiniBatchDataProvider(DataProvider):
    """ This is the standard ``DataProvider`` for mini-batch learning
    with stochastic gradient descent.

    Input and target data may either be provided as ``numpy.array``
    objects, or as ``pycuda.GPUArray`` objects. The latter is
    preferred if the data can fit on GPU memory and will be much
    faster, as the data won't have to be transferred to the GPU for
    every minibatch. If the data is provided as a ``numpy.array``,
    then every minibatch is automatically converted to to a
    ``pycuda.GPUArray`` and transferred to the GPU.

    :param data: Input data.
    :param targets: Target data.
    :param batch_size: The size of mini-batches.
    """
    
    def __getitem__(self, batch_idx):
        # return self.data[batch_idx*self.batch_size:(batch_idx+1)*self.batch_size]
        return self.data_batches[batch_idx], self.targets_batches[batch_idx]

    def next(self):
        if self.i >= self.n_batches:
            self.i = 0
            raise StopIteration

        minibatch_data  = self.data_batches[self.i]
        minibatch_targets = self.targets_batches[self.i]

        self.i += 1

        if not isinstance(minibatch_data, gpuarray.GPUArray):
            minibatch_data = gpuarray.to_gpu(minibatch_data)

        if not isinstance(minibatch_targets, gpuarray.GPUArray):
            minibatch_targets = gpuarray.to_gpu(minibatch_targets)

        return minibatch_data, minibatch_targets


class MultiTaskDataProvider(DataProvider):
    """ ``DataProvider`` for multi-task learning that uses the same
    training data for multiple targets.

    This ``DataProvider`` is similar to the
    :class:`hebel.data_provider.MiniBatchDataProvider`, except that it
    has not one but multiple targets.

    :param data: Input data.
    :param targets: Multiple targets as a list or tuple.
    :param batch_size: The size of mini-batches.

    **See also:**

    :class:`hebel.models.MultitaskNeuralNet`, :class:`hebel.layers.MultitaskTopLayer`
    
    """
    
    def __init__(self, data, targets, batch_size=None):
        if isinstance(targets, (list, tuple)):
            assert all([targets[0].shape[0] == t.shape[0] for t in targets])
        if isinstance(data, (list, tuple)):
            assert all([type(targets[0]) == type(t) for t in targets])
        self.data = data

        if not isinstance(targets, gpuarray.GPUArray):
            targets = gpuarray.to_gpu(targets)
        self.targets = targets

        try:
            self.N = data.shape[0]
        except AttributeError:
            self.N = data[0].shape[0]

        self.i = 0
        if batch_size is not None:
            self.batch_size = batch_size
        else:
            self.batch_size = self.N

    def _make_batches(self):
        if not isinstance(self.data, (list, tuple)):
            self.data_batches = tuple(
                self.data[i:i+self.batch_size]
                for i in range(0, self.N, self.batch_size)
            )
        else:
            self.data_batches = \
                tuple(tuple(d[i:i+self.batch_size]
                            for d in self.data)
                      for i in range(0, self.N, self.batch_size))

        if not isinstance(self.targets, (list, tuple)):
            self.target_batches = tuple(
                self.targets[i:i+self.batch_size]
                for i in range(0, self.N, self.batch_size)
            )
        else:
            self.target_batches = \
                tuple(tuple(tuple(t[i:i+self.batch_size]
                                  for t in self.targets))
                      for i in range(0, self.N, self.batch_size)
            )
        self.n_batches = len(self.data_batches)

    def __getitem__(self, batch_idx):
        return self.data_batches[batch_idx], self.target_batches[batch_idx]

    def next(self):
        if self.i >= self.n_batches:
            self.i = 0
            raise StopIteration

        minibatch_data = self.data_batches[self.i]
        minibatch_targets = self.target_batches[self.i]

        self.i += 1
        return minibatch_data, minibatch_targets


class BatchDataProvider(MiniBatchDataProvider):
    """``DataProvider`` for batch learning. Always returns the full data set.

    :param data: Input data.
    :param targets: Target data.

    **See also:**

    :class:`hebel.data_providers.MiniBatchDataProvider`

    """
    def __init__(self, data, targets):
        self.data = data
        self.targets = targets
        self.N = data.shape[0]
        self.i = 0
        self.batch_size = self.N

    def __getitem__(self, batch_idx):
        if batch_idx == 0:
            return self.data, self.targets
        else:
            raise ValueError("batch_idx out of bounds")

    def next(self):
        if self.i >= self.N:
            self.i = 0
            raise StopIteration

        self.i += self.N
        return self.data, self.targets

class DummyDataProvider(DataProvider):
    """A dummy ``DataProvider`` that does not store any data and
    always returns ``None``.
    """
    
    def __init__(self, *args, **kwargs):
        pass

    def __getitem__(self, batch_idx):
        return None, None

    def next(self):
        return None, None

class MNISTDataProvider(MiniBatchDataProvider):
    """``DataProvider`` that automatically provides data from the
    `MNIST <http://yann.lecun.com/exdb/mnist/>`_ data set of
    hand-written digits.

    Depends on the `skdata <http://jaberg.github.io/skdata/>`_ package.

    :param array: {'train', 'val', 'test'}
        Whether to use the official training, validation, or test data split of MNIST.
    :param batch_size: The size of mini-batches.
    """

    try:
        from skdata.mnist.view import OfficialVectorClassification
    except ImportError:
        from skdata.mnist.views import OfficialVectorClassification
    mnist = OfficialVectorClassification()

    def __init__(self, array, batch_size=None):

        self.train_idx = self.mnist.fit_idxs
        self.val_idx = self.mnist.val_idxs
        self.test_idx = self.mnist.tst_idxs

        self.N_train = self.train_idx.shape[0]
        self.N_val = self.val_idx.shape[0]
        self.N_test = self.test_idx.shape[0]
        self.D = self.mnist.all_vectors.shape[1]

        if array == 'train':
            self.data = gpuarray.to_gpu(self.mnist.all_vectors[self.train_idx]
                                   .astype(np.float32) / 255.)
            targets = self.mnist.all_labels[self.train_idx]
            labels_soft = np.zeros((self.N_train, 10), dtype=np.float32)
            labels_soft[range(self.N_train), targets] = 1.
            self.targets = gpuarray.to_gpu(labels_soft)
            self.N = self.N_train
        elif array == 'val':
            self.data = gpuarray.to_gpu(self.mnist.all_vectors[self.val_idx]
                                   .astype(np.float32) / 255.)
            self.N = self.N_val
            targets = self.mnist.all_labels[self.val_idx]
            labels_soft = np.zeros((self.N_train, 10), dtype=np.float32)
            labels_soft[range(self.N_val), targets] = 1.
            self.targets = gpuarray.to_gpu(labels_soft)
        elif array == 'test':
            self.data = gpuarray.to_gpu(self.mnist.all_vectors[self.test_idx]
                                   .astype(np.float32) / 255.)
            targets = self.mnist.all_labels[self.test_idx]
            labels_soft = np.zeros((self.N_test, 10), dtype=np.float32)
            labels_soft[range(self.N_test), targets] = 1.
            self.targets = gpuarray.to_gpu(labels_soft)
            self.N = self.N_test
        else:
            raise ValueError('Unknown partition "%s"' % array)

        self.batch_size = batch_size if batch_size is not None else self.N
        self.i = 0
        self._make_batches()

    # def __getitem__(self, batch_idx):
    #     if self.batch_size is None:
    #         if batch_idx == 0:
    #             return self.data, self.targets
    #         else:
    #             raise ValueError("batch_idx out of bounds")
    #     else:
    #         minibatch_data = self.data[batch_idx*self.batch_size:(batch_idx+1)*self.batch_size]
    #         minibatch_targets = self.targets[batch_idx*self.batch_size:(batch_idx+1)*self.batch_size]
    #         return minibatch_data, minibatch_targets

    # def next(self):
    #     if self.i >= self.N:
    #         self.i = 0
    #         raise StopIteration

    #     if self.batch_size is None:
    #         self.i += self.N
    #         return self.data, self.targets
    #     else:
    #         minibatch_data = self.data[self.i:self.i+self.batch_size]
    #         minibatch_targets = self.targets[self.i:self.i+self.batch_size]
    #         self.i += self.batch_size
    #         return minibatch_data, minibatch_targets

########NEW FILE########
__FILENAME__ = dummy_layer
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from .hidden_layer import HiddenLayer


class DummyLayer(HiddenLayer):
    """ This class has no hidden units and simply passes through its
    input
    """

    lr_multiplier = []
    n_parameters = 0
    l1_penalty_weight = 0.
    l2_penalty_weight = 0.
    dropout = False

    def __init__(self, n_in):
        self.n_in = n_in
        self.n_units = n_in

    @property
    def parameters(self):
        return []

    @parameters.setter
    def parameters(self, value):
        pass

    def update_parameters(self, values, stream=None):
        pass

    @property
    def l1_penalty(self):
        return 0.

    @property
    def l2_penalty(self):
        return 0.

    def feed_forward(self, input_data, prediction=False):
        assert input_data.shape[1] == self.n_in
        return (input_data,)

    def backprop(self, input_data, df_output, cache=None):
        return tuple(), df_output

########NEW FILE########
__FILENAME__ = hidden_layer
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
import cPickle
from itertools import izip
from pycuda import gpuarray
from pycuda.gpuarray import GPUArray
from math import sqrt
from .. import sampler
from ..pycuda_ops import eps
from ..pycuda_ops import linalg
from ..pycuda_ops.elementwise import sigmoid, df_sigmoid, \
     tanh, df_tanh, relu, df_relu, linear, df_linear, \
     sample_dropout_mask, apply_dropout_mask, sign, mult_matrix
from ..pycuda_ops.matrix import add_vec_to_mat
from ..pycuda_ops.reductions import matrix_sum_out_axis


class HiddenLayer(object):
    r"""A fully connected hidden layer.

    The ``HiddenLayer`` class represents a fully connected hidden
    layer that can use a multitude of activation functions and supports
    dropout, L1, and L2 regularization.

    **Parameters:**

    n_in : integer
        Number of input units.

    n_out : integer
        Number of hidden units.

    activation_function : {``sigmoid``, ``tanh``, ``relu``, ``linear``}, optional
        Which activation function to use. Default is sigmoid.

    dropout : bool
        Whether the layer should use dropout (with dropout probability 0.5)

    parameters : array_like of ``GPUArray``
        Parameters used to initialize the layer. If this is omitted,
        then the weights are initalized randomly using *Bengio's rule*
        (uniform distribution with scale :math:`4 \cdot \sqrt{6 /
        (\mathtt{n\_in} + \mathtt{n\_out})}` if using sigmoid
        activations and :math:`\sqrt{6 / (\mathtt{n\_in} +
        \mathtt{n\_out})}` if using tanh, relu, or linear activations)
        and the biases are initialized to zero. If ``parameters`` is
        given, then is must be in the form ``[weights, biases]``,
        where the shape of weights is ``(n_in, n_out)`` and the shape
        of ``biases`` is ``(n_out,)``. Both weights and biases must be
        ``GPUArray``.
    
    weights_scale : float, optional
        If ``parameters`` is omitted, then this factor is used as
        scale for initializing the weights instead of *Bengio's rule*.

    l1_penalty_weight : float, optional
        Weight used for L1 regularization of the weights.

    l2_penalty_weight : float, optional
       Weight used for L2 regularization of the weights.

    lr_multiplier : float, optional
        If this parameter is omitted, then the learning rate for the
        layer is scaled by :math:`2 / \sqrt{\mathtt{n\_in}}`. You may
        specify a different factor here.

    **Examples**::

        # Use the simple initializer and initialize with random weights
        hidden_layer = HiddenLayer(500, 10000)

        # Sample weights yourself, specify an L1 penalty, and don't
        # use learning rate scaling
        import numpy as np
        from pycuda import gpuarray

        n_in = 500
        n_out = 1000
        weights = gpuarray.to_gpu(.01 * np.random.randn(n_in, n_out))
        biases = gpuarray.to_gpu(np.zeros((n_out,)))
        hidden_layer = HiddenLayer(n_in, n_out,
                                   parameters=(weights, biases),
                                   l1_penalty_weight=.1,
                                   lr_multiplier=1.)

    """
    n_parameters = 2
    W = None
    b = None

    def __init__(self, n_in, n_units,
                 activation_function='sigmoid',
                 dropout=False,
                 parameters=None,
                 weights_scale=None,
                 l1_penalty_weight=0.,
                 l2_penalty_weight=0.,
                 lr_multiplier=None):

        self._set_activation_fct(activation_function)

        if weights_scale is None:
            self._set_weights_scale(activation_function, n_in, n_units)
        else:
            self.weights_scale = weights_scale

        if parameters is not None:
            if isinstance(parameters, basestring):
                self.parameters = cPickle.loads(open(parameters))
            else:
                self.W, self.b = parameters
        else:
            self.W = self.weights_scale * \
                     sampler.gen_uniform((n_in, n_units),
                                         dtype=np.float32) \
              - .5 * self.weights_scale

            self.b = gpuarray.zeros((n_units,), dtype=np.float32)

        assert self.W.shape == (n_in, n_units)
        assert self.b.shape == (n_units,)

        self.n_in = n_in
        self.n_units = n_units

        self.lr_multiplier = lr_multiplier if lr_multiplier is not None else \
            2 * [1. / np.sqrt(self.n_in, dtype=np.float32)]

        self.l1_penalty_weight = l1_penalty_weight
        self.l2_penalty_weight = l2_penalty_weight

        self.dropout = dropout

        self.persistent_temp_objects_config = (
            ('activations', ('batch_size', self.n_units), np.float32),
            ('dropout_prob_array', ('batch_size', self.n_units), np.float32),
            ('dropout_mask', ('batch_size', self.n_units), np.int8),
            ('df_W', self.W.shape, self.W.dtype),
            ('df_b', self.b.shape, self.b.dtype),
            ('df_input', ('batch_size', self.n_in), np.float32),
            ('df_activations', ('batch_size', self.n_units), np.float32),
            ('delta', ('batch_size', self.n_units), np.float32)
        )

    def preallocate_temp_objects(self, batch_size):
        from ..data_providers import DataProvider

        if isinstance(batch_size, DataProvider):
            batch_size = batch_size.batch_size
        self._batch_size = batch_size

        if hasattr(self, 'persistent_temp_objects_config'):
            self.persistent_temp_objects = {}
            for tmp_obj_name, tmp_obj_shape, tmp_obj_dtype \
              in self.persistent_temp_objects_config:
                tmp_obj_shape = tuple(s if not s == 'batch_size' else batch_size
                                      for s in tmp_obj_shape)
                tmp_obj = gpuarray.empty(tmp_obj_shape, tmp_obj_dtype)
                self.persistent_temp_objects[tmp_obj_name] = tmp_obj

    def get_temp_object(self, name, shape, dtype):
        if hasattr(self, "persistent_temp_objects") and \
          name in self.persistent_temp_objects:
            tmp_obj = self.persistent_temp_objects[name]
            if tmp_obj.shape == shape and tmp_obj.dtype == dtype:
                return tmp_obj
        return gpuarray.empty(shape, dtype)

    @property
    def parameters(self):
        """Return a tuple ``(weights, biases)``"""
        return (self.W, self.b)

    @parameters.setter
    def parameters(self, value):
        """Update the parameters. ``value`` must have the shape
        ``(weights, biases)``"""
        self.W = value[0] if isinstance(value[0], GPUArray) else \
          gpuarray.to_gpu(value[0])
        self.b = value[1] if isinstance(value[0], GPUArray) else \
          gpuarray.to_gpu(value[1])

    def update_parameters(self, values, stream=None):
        assert len(values) == self.n_parameters

        for (param, (gparam, mult)) \
            in izip((self.W, self.b), values):
            param._axpbyz(1., gparam, mult, param,
                          stream=stream)

    @property
    def architecture(self):
        """Returns a dictionary describing the architecture of the layer."""
        arch = {'class': self.__class__,
                'n_in': self.n_in,
                'n_units': self.n_units,
                'activation_function': self.activation_function
                if hasattr(self, 'activation_function') else None}
        return arch

    @staticmethod
    def _resolve_activation_fct(activation_function):
        if activation_function == 'sigmoid':
            f = sigmoid
            df = df_sigmoid
        elif activation_function == 'tanh':
            f = tanh
            df = df_tanh
        elif activation_function == 'relu':
            f = relu
            df = df_relu
        elif activation_function == 'linear':
            f = linear
            df = df_linear
        else:
            raise ValueError

        return f, df

    def _set_activation_fct(self, activation_function):
        self.activation_function = activation_function
        self.f, self.df = self._resolve_activation_fct(activation_function)

    def _set_weights_scale(self, activation_function, n_in, n_units):
        if activation_function in ('tanh', 'relu', 'linear'):
            self.weights_scale = sqrt(6. / (n_in + n_units))
        elif activation_function == 'sigmoid':
            self.weights_scale = 4 * sqrt(6. / (n_in + n_units))
        else:
            raise ValueError

    @property
    def l1_penalty(self):
        return float(self.l1_penalty_weight) * gpuarray.sum(abs(self.W)).get()

    @property
    def l2_penalty(self):
        return float(self.l2_penalty_weight) * .5 * \
            gpuarray.sum(self.W ** 2.).get()

    def feed_forward(self, input_data, prediction=False):
        """Propagate forward through the layer

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        **Returns:**
        
        activations : ``GPUArray``
            The activations of the hidden units.
        """

        activations = self.get_temp_object('activations',
            (input_data.shape[0], self.n_units), input_data.dtype)
        linalg.dot(input_data, self.W, target=activations)
        activations = add_vec_to_mat(activations, self.b, inplace=True)

        self.f(activations)

        if self.dropout and prediction:
            activations *= .5

        if self.dropout and not prediction:
            dropout_mask = self.get_temp_object('dropout_mask', activations.shape,
                                                np.int8)
            dropout_prob_array = self.get_temp_object('dropout_prob_array',
                                                      activations.shape,
                                                      activations.dtype)
            sample_dropout_mask(activations, dropout_mask=dropout_mask,
                                dropout_prob_array=dropout_prob_array)
            return activations, dropout_mask

        return (activations,)

    def backprop(self, input_data, df_output, cache=None):
        """ Backpropagate through the hidden layer

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        df_output : ``GPUArray``
            Gradients with respect to the activations of this layer
            (received from the layer above).

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        **Returns:**

        gradients : tuple of ``GPUArray``
            Gradients with respect to the weights and biases in the
            form ``(df_weights, df_biases)``.

        df_input : ``GPUArray``
            Gradients with respect to the input.
        """

        # Get cache if it wasn't provided
        if cache is None:
            cache = self.feed_forward(input_data,
                                      prediction=False)

        if len(cache) == 2:
            activations, dropout_mask = cache
        else:
            activations = cache[0]

        # Multiply the binary mask with the incoming gradients
        if self.dropout and dropout_mask is not None:
            apply_dropout_mask(df_output, dropout_mask)

        # Get temporary objects
        df_W = self.get_temp_object('df_W', self.W.shape, self.W.dtype)
        df_b = self.get_temp_object('df_b', self.b.shape, self.b.dtype)
        df_input = self.get_temp_object('df_input',
                input_data.shape, input_data.dtype)
        df_activations = self.get_temp_object('df_activations',
                activations.shape, activations.dtype)
        delta = self.get_temp_object('delta',
                activations.shape, activations.dtype)
        

        # Get gradient wrt activation function
        self.df(activations, df_activations)
        mult_matrix(df_activations, df_output, delta)

        # Gradient wrt weights
        linalg.dot(input_data, delta, transa='T', target=df_W)
        # Gradient wrt bias
        matrix_sum_out_axis(delta, 0, target=df_b)
        # Gradient wrt inputs
        linalg.dot(delta, self.W, transb='T', target=df_input)

        # L1 weight decay
        if self.l1_penalty_weight:
            df_W += self.l1_penalty_weight * sign(self.W)

        # L2 weight decay
        if self.l2_penalty_weight:
            df_W += self.l2_penalty_weight * self.W

        return (df_W, df_b), df_input

    def __getstate__(self):
        result = self.__dict__.copy()
        try:
            del result['persistent_temp_objects']
        except KeyError:
            pass
        return result

    def __setstate__(self, dict):
        self.__dict__ = dict
        self.preallocate_temp_objects(self._batch_size)

########NEW FILE########
__FILENAME__ = input_dropout
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
import cPickle
from pycuda import gpuarray
from .dummy_layer import DummyLayer
from ..pycuda_ops.elementwise import sample_dropout_mask, \
    apply_dropout_mask
from ..pycuda_ops.matrix import add_vec_to_mat
from ..pycuda_ops.reductions import matrix_sum_out_axis

class InputDropout(DummyLayer):
    r"""This layer performs dropout on the input data.

    It does not have any learnable parameters of its own. It should be
    used as the first layer and will perform dropout with any dropout
    probability on the incoming data.

    **Parameters:**

    n_in : integer
        Number of input units.

    dropout_probability : float in [0, 1]
        Probability of dropping out each unit.

    compute_input_gradients : Bool
        Whether to compute the gradients with respect to the input
        data. This only necessary if you're training a model where the
        input itself is learned.

    """

    def __init__(self, n_in, dropout_probability=.2,
                 compute_input_gradients=False):
        self.n_in = n_in
        self.n_units = n_in
        
        assert dropout_probability >= 0. and \
            dropout_probability <= 1.
        self.dropout_probability = dropout_probability
        self.compute_input_gradients = compute_input_gradients

        self.persistent_temp_objects_config = (
            ('dropout_input', ('batch_size', self.n_units), np.float32),
            ('dropout_prob_array', ('batch_size', self.n_units), np.float32),
            ('dropout_mask', ('batch_size', self.n_units), np.int8)
        )

    def feed_forward(self, input_data, prediction=False):
        """Propagate forward through the layer

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to perform dropout on.

        prediction : bool, optional
            Whether to use prediction model. If true, then the data is
            scaled by ``1 - dropout_probability`` uses dropout.

        **Returns:**
        
        dropout_data : ``GPUArray``
            The data after performing dropout.
        """

        assert input_data.shape[1] == self.n_in

        if not prediction:
            dropout_input = self.get_temp_object('dropout_input',
                input_data.shape, input_data.dtype)
            dropout_prob_array = self.get_temp_object('dropout_prob_array',
                input_data.shape, input_data.dtype)
            dropout_mask = self.get_temp_object('dropout_mask',
                input_data.shape, np.int8)
            sample_dropout_mask(input_data,
                self.dropout_probability, target=dropout_input,
                dropout_prob_array=dropout_prob_array, dropout_mask=dropout_mask)
            return dropout_input, dropout_mask
        else:
            return (input_data * (1 - self.dropout_probability),)

    def backprop(self, input_data, df_output, cache=None):
        """ Backpropagate through the hidden layer

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to perform dropout on.

        df_output : ``GPUArray``
            Gradients with respect to the output of this layer
            (received from the layer above).

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        **Returns:**

        gradients : empty tuple
            Gradients are empty since this layer has no parameters.

        df_input : ``GPUArray``
            Gradients with respect to the input.
        """

        if self.compute_input_gradients:            
            apply_dropout_mask(df_output, dropout_mask)

        return tuple(), df_output

########NEW FILE########
__FILENAME__ = linear_regression_layer
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
from pycuda import gpuarray, cumath
from math import sqrt
from .. import sampler
from .softmax_layer import SoftmaxLayer
from ..pycuda_ops.elementwise import sign, nan_to_zeros
from ..pycuda_ops.reductions import matrix_sum_out_axis
from ..pycuda_ops.matrix import add_vec_to_mat
from ..pycuda_ops import linalg


class LinearRegressionLayer(SoftmaxLayer):
    r"""Linear regression layer with linear outputs and squared loss error function.

        **Parameters:**
    
    n_in : integer
        Number of input units.

    n_out : integer
        Number of output units (classes).

    parameters : array_like of ``GPUArray``
        Parameters used to initialize the layer. If this is omitted,
        then the weights are initalized randomly using *Bengio's rule*
        (uniform distribution with scale :math:`4 \cdot \sqrt{6 /
        (\mathtt{n\_in} + \mathtt{n\_out})}`) and the biases are
        initialized to zero. If ``parameters`` is given, then is must
        be in the form ``[weights, biases]``, where the shape of
        weights is ``(n_in, n_out)`` and the shape of ``biases`` is
        ``(n_out,)``. Both weights and biases must be ``GPUArray``.
    
    weights_scale : float, optional
        If ``parameters`` is omitted, then this factor is used as
        scale for initializing the weights instead of *Bengio's rule*.

    l1_penalty_weight : float, optional
        Weight used for L1 regularization of the weights.

    l2_penalty_weight : float, optional
       Weight used for L2 regularization of the weights.

    lr_multiplier : float, optional
        If this parameter is omitted, then the learning rate for the
        layer is scaled by :math:`2 / \sqrt{\mathtt{n\_in}}`. You may
        specify a different factor here.

    test_error_fct : {``class_error``, ``kl_error``, ``cross_entropy_error``}, optional
        Which error function to use on the test set. Default is
        ``class_error`` for classification error. Other choices are
        ``kl_error``, the Kullback-Leibler divergence, or
        ``cross_entropy_error``.

    **See also:**

    :class:`hebel.models.NeuralNetRegression`,
    :class:`hebel.models.NeuralNet`,
    :class:`hebel.layers.LogisticLayer`

    """

    
    n_parameters = 2
    
    def __init__(self, n_in, n_out,
                 parameters=None,
                 weights_scale=None,
                 l1_penalty_weight=0.,
                 l2_penalty_weight=0.,
                 lr_multiplier=None):

        # Initialize weight using Bengio's rule
        self.weights_scale = 4 * sqrt(6. / (n_in + n_out)) \
                             if weights_scale is None \
                                else weights_scale

        if parameters is not None:
            self.W, self.b = parameters
        else:
            self.W = self.weights_scale * \
                     sampler.gen_uniform((n_in, n_out), dtype=np.float32) \
                     - .5 * self.weights_scale

            self.b = gpuarray.zeros((n_out,), dtype=np.float32)

        self.n_in = n_in
        self.n_out = n_out

        self.l1_penalty_weight = l1_penalty_weight
        self.l2_penalty_weight = l2_penalty_weight

        self.lr_multiplier = 2 * [1. / np.sqrt(n_in, dtype=np.float32)] \
          if lr_multiplier is None else lr_multiplier

        self.persistent_temp_objects_config = (
            ('activations', ('batch_size', self.n_out), np.float32),
            ('df_W', self.W.shape, self.W.dtype),
            ('df_b', self.b.shape, self.b.dtype),
            ('df_input', ('batch_size', self.n_in), np.float32),
            ('delta', ('batch_size', self.n_out), np.float32)
        )

    def feed_forward(self, input_data, prediction=False):
        """Propagate forward through the layer.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        **Returns:**
        
        activations : ``GPUArray``
            The activations of the output units.
        """

        activations = self.get_temp_object('activations',
            (input_data.shape[0], self.n_out), input_data.dtype)
        
        linalg.dot(input_data, self.W, target=activations)
        activations = add_vec_to_mat(activations, self.b, inplace=True)

        return activations

    def test_error(self, input_data, targets, average=True,
                   cache=None, prediction=True):
        """Compute the test error function given some data and targets.

        Uses the error function defined in
        :class:`SoftmaxLayer.test_error_fct`, which may be different
        from the cross-entropy error function used for
        training'. Alternatively, the other test error functions may
        be called directly.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute the test error function for.

        targets : ``GPUArray``
            The target values of the units.

        average : bool
            Whether to divide the value of the error function by the
            number of data points given.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        **Returns:**
        test_error : float
        """

        return self.squared_loss(input_data, targets, average,
                                 cache, prediction)

    def squared_loss(self, input_data, targets, average=True,
                     cache=None, prediction=False):
        if cache is not None:
            activations = cache
        else:
            activations  = \
                self.feed_forward(input_data, prediction=prediction)

        loss = gpuarray.sum(
            matrix_sum_out_axis((targets - activations) ** 2, 1))

        if average: loss = loss.mean()
        return float(loss.get())
    train_error = squared_loss

########NEW FILE########
__FILENAME__ = logistic_layer
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
import cPickle
from pycuda import gpuarray
from pycuda import cumath
from math import sqrt
from .. import sampler
from .top_layer import TopLayer
from ..pycuda_ops import eps, linalg
from ..pycuda_ops.elementwise import sign, nan_to_zeros, substract_matrix, sigmoid
from ..pycuda_ops.reductions import matrix_sum_out_axis
from ..pycuda_ops.matrix import add_vec_to_mat
from ..pycuda_ops.softmax import cross_entropy_logistic


class LogisticLayer(TopLayer):
    r""" A logistic classification layer for two classes, using
    cross-entropy loss function and sigmoid activations.

    **Parameters:**
    
    n_in : integer
        Number of input units.

    parameters : array_like of ``GPUArray``
        Parameters used to initialize the layer. If this is omitted,
        then the weights are initalized randomly using *Bengio's rule*
        (uniform distribution with scale :math:`4 \cdot \sqrt{6 /
        (\mathtt{n\_in} + \mathtt{n\_out})}`) and the biases are
        initialized to zero. If ``parameters`` is given, then is must
        be in the form ``[weights, biases]``, where the shape of
        weights is ``(n_in, n_out)`` and the shape of ``biases`` is
        ``(n_out,)``. Both weights and biases must be ``GPUArray``.
    
    weights_scale : float, optional
        If ``parameters`` is omitted, then this factor is used as
        scale for initializing the weights instead of *Bengio's rule*.

    l1_penalty_weight : float, optional
        Weight used for L1 regularization of the weights.

    l2_penalty_weight : float, optional
       Weight used for L2 regularization of the weights.

    lr_multiplier : float, optional
        If this parameter is omitted, then the learning rate for the
        layer is scaled by :math:`2 / \sqrt{\mathtt{n\_in}}`. You may
        specify a different factor here.

    test_error_fct : {``class_error``, ``kl_error``, ``cross_entropy_error``}, optional
        Which error function to use on the test set. Default is
        ``class_error`` for classification error. Other choices are
        ``kl_error``, the Kullback-Leibler divergence, or
        ``cross_entropy_error``.

    **See also:**

    :class:`hebel.layers.SoftmaxLayer`,
    :class:`hebel.models.NeuralNet`,
    :class:`hebel.models.NeuralNetRegression`,
    :class:`hebel.layers.LinearRegressionLayer`

    **Examples**::

        # Use the simple initializer and initialize with random weights
        logistic_layer = LogisticLayer(1000)

        # Sample weights yourself, specify an L1 penalty, and don't
        # use learning rate scaling
        import numpy as np
        from pycuda import gpuarray

        n_in = 1000
        weights = gpuarray.to_gpu(.01 * np.random.randn(n_in, 1))
        biases = gpuarray.to_gpu(np.zeros((1,)))
        softmax_layer = SoftmaxLayer(n_in,
                                     parameters=(weights, biases),
                                     l1_penalty_weight=.1,
                                     lr_multiplier=1.)
    """

    n_parameters = 2
    n_out = 1

    def __init__(self, n_in,
                 parameters=None,
                 weights_scale=None,
                 l1_penalty_weight=0., l2_penalty_weight=0.,
                 lr_multiplier=None,
                 test_error_fct='class_error'):

        # Initialize weight using Bengio's rule
        self.weights_scale = 4 * sqrt(6. / (n_in + 1)) \
                             if weights_scale is None \
                                else weights_scale

        if parameters is not None:
            self.W, self.b = parameters
        else:
            self.W = self.weights_scale * \
                     sampler.gen_uniform((n_in, 1), dtype=np.float32) \
                     - .5 * self.weights_scale

            self.b = gpuarray.zeros((1,), dtype=np.float32)

        self.n_in = n_in

        self.test_error_fct = test_error_fct

        self.l1_penalty_weight = l1_penalty_weight
        self.l2_penalty_weight = l2_penalty_weight

        self.lr_multiplier = 2 * [1. / np.sqrt(n_in, dtype=np.float32)] \
          if lr_multiplier is None else lr_multiplier

        self.persistent_temp_objects_config = (
            ('activations', ('batch_size', 1), np.float32),
            ('df_W', self.W.shape, np.float32),
            ('df_b', self.b.shape, np.float32),
            ('df_input', ('batch_size', self.n_in), np.float32),
            ('delta', ('batch_size', 1), np.float32)
        )

    @property
    def architecture(self):
        return {'class': self.__class__,
                'n_in': self.n_in,
                'n_out': 1}

    def feed_forward(self, input_data, prediction=False):
        """Propagate forward through the layer.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        **Returns:**
        
        activations : ``GPUArray``
            The activations of the output units.
        """

        activations = self.get_temp_object('activations',
            (input_data.shape[0], 1), input_data.dtype)
        linalg.dot(input_data, self.W, target=activations)
        activations = add_vec_to_mat(activations, self.b, inplace=True)

        sigmoid(activations)

        return activations

    def backprop(self, input_data, targets,
                 cache=None):
        """ Backpropagate through the logistic layer.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        targets : ``GPUArray``
            The target values of the units.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        **Returns:**

        gradients : tuple of ``GPUArray``
            Gradients with respect to the weights and biases in the
            form ``(df_weights, df_biases)``.

        df_input : ``GPUArray``
            Gradients with respect to the input.
        """

        if cache is not None:
            activations = cache
        else:
            activations = self.feed_forward(input_data, prediction=False)

        # Get temporary objects
        df_W = self.get_temp_object('df_W', self.W.shape, self.W.dtype)
        df_b = self.get_temp_object('df_b', self.b.shape, self.b.dtype)
        df_input = self.get_temp_object('df_input',
                input_data.shape, input_data.dtype)
        delta = self.get_temp_object('delta',
                activations.shape, activations.dtype)

        substract_matrix(activations, targets, delta)
        nan_to_zeros(delta, delta)

        # Gradient wrt weights
        linalg.dot(input_data, delta, transa='T', target=df_W)
        # Gradient wrt bias
        matrix_sum_out_axis(delta, 0, target=df_b)

        # Gradient wrt input
        linalg.dot(delta, self.W, transb='T', target=df_input)

        # L1 penalty
        if self.l1_penalty_weight:
            df_W += self.l1_penalty_weight * sign(self.W)

        # L2 penalty
        if self.l2_penalty_weight:
            df_W += self.l2_penalty_weight * self.W

        return (df_W, df_b), df_input

    def test_error(self, input_data, targets, average=True,
                   cache=None, prediction=True):
        """Compute the test error function given some data and targets.

        Uses the error function defined in
        :class:`SoftmaxLayer.test_error_fct`, which may be different
        from the cross-entropy error function used for
        training'. Alternatively, the other test error functions may
        be called directly.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute the test error function for.

        targets : ``GPUArray``
            The target values of the units.

        average : bool
            Whether to divide the value of the error function by the
            number of data points given.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        **Returns:**
        test_error : float
        """    
        if self.test_error_fct == 'class_error':
            test_error = self.class_error
        elif self.test_error_fct == 'cross_entropy_error':
            test_error = self.cross_entropy_error
        else:
            raise ValueError('unknown test error function "%s"'
                             % self.test_error_fct)

        return test_error(input_data, targets, average,
                          cache, prediction)

    def cross_entropy_error(self, input_data, targets, average=True,
                            cache=None, prediction=False):
        """ Return the cross entropy error
        """

        if cache is not None:
            activations = cache
        else:
            activations = \
              self.feed_forward(input_data, prediction=prediction)

        loss = cross_entropy_logistic(activations, targets)

        if average: loss /= targets.shape[0]
        return loss
        
    train_error = cross_entropy_error

    def class_error(self, input_data, targets, average=True,
                    cache=None, prediction=False):
        """ Return the classification error rate
        """

        if cache is not None:
            activations = cache
        else:
            activations = \
              self.feed_forward(input_data, prediction=prediction)

        targets = targets.get()
        class_error = np.sum((activations.get() >= .5) != (targets >= .5))

        if average: class_error = float(class_error) / targets.shape[0]
        return class_error

########NEW FILE########
__FILENAME__ = multitask_top_layer
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
from itertools import izip
from pycuda import gpuarray
from .top_layer import TopLayer
from .softmax_layer import SoftmaxLayer


class MultitaskTopLayer(TopLayer):
    """Top layer for performing multi-task training.

    This is a top layer that enables multi-task training, which
    can be thought of as training multiple models on the same data
    and sharing weights in all but the final layer. A
    ``MultitaskTopLayer`` has multiple layers as children that are
    subclasses of :class:`hebel.layers.TopLayer`. During the
    forward pass, the input from the previous layer is passed on
    to all tasks and during backpropagation, the gradients are
    added together from the different tasks (with different
    weights if necessary).

    There are two ways of initializing ``MultitaskTopLayer``:

    1. By supplying ``n_in``, ``n_out``, and optionally
       ``n_tasks``, which will initialize all tasks with
       :class:`hebel.layers.LogisticLayer`. If ``n_tasks`` is
       given, ``n_out`` must be an integer and ``n_tasks`` identical
       tasks will be created. If ``n_out`` is an ``array_like``, then
       as many tasks will be created as there are elements in
       ``n_out`` and ``n_tasks`` will be ignored.

    2. If ``tasks`` is supplied, then it must be an ``array_like``
       of objects derived from :class:`hebel.layers.TopLayer`, one
       object for each class. In this case ``n_in``, ``n_out``, and
       ``n_tasks`` will be ignored. The user must make sure that all
       tasks have their ``n_in`` member variable set to the same
       value.

    **Parameters:**

    n_in : integer, optional
        Number of input units. Is ignored, when ``tasks`` is supplied.

    n_out : integer or array_like, optional
        Number of output units. May be an integer (all tasks get
        the same number of units; ``n_tasks`` must be given), or
        ``array_like`` (create as many tasks as elements in
        ``n_out`` with different sizes; ``n_tasks is ignored). Is
        always ignored when ``tasks`` is supplied.

    test_error_fct : string, optional
        See :class:`hebel.layers.LogisticLayer` for
        options. Ignored when ``tasks`` is supplied.
        
    l1_penalty_weight : float or list/tuple of floats, optional
        Weight(s) for L1 regularization. Ignored when ``tasks`` is
        supplied. 
        
    l2_penalty_weight : float or list/tuple of floats, optional
        Weight(s)for L2 regularization. Ignored when ``tasks`` is
        supplied. 

    tasks : list/tuple of :class:`hebel.layers.TopLayer` objects, optional
        Tasks for multitask learning. Overrides ``n_in``,
        ``n_out``, ``test_error_fct``, ``l1_penalty_weight``,
        ``l2_penalty_weight``, ``n_tasks``, and ``lr_multiplier``.

    task_weights : list/tuple of floats, optional
        Weights to use when adding the gradients from the
        different tasks. Default is ``1./self.n_tasks``. The
        weights don't need to necessarily add up to one.

    n_tasks : integer, optional
        Number of tasks. Ignored if ``n_out`` is a list, or
        ``tasks`` is supplied.

    lr_multiplier : float or list/tuple of floats
        A task dependant multiplier for the learning rate. If this
        is ignored, then the tasks default is used. It is ignored
        when ``tasks`` is supplied.

    **See also:**
    :class:`hebel.layers.TopLayer`,
    :class:`hebel.layers.LogisticLayer`

    **Examples**::

        # Simple form of the constructor
        # Creating five tasks with same number of classes
        multitask_layer = MultitaskTopLayer(n_in=1000, n_out=10, n_tasks=5)

        # Extended form of the constructor
        # Initializing every task independently

        n_in = 1000              # n_in must be the same for all tasks
        tasks = (
            SoftmaxLayer(n_in, 10, l1_penalty_weight=.1),
            SoftmaxLayer(n_in, 15, l2_penalty_weight=.2),
            SoftmaxLayer(n_in, 10),
            SoftmaxLayer(n_in, 10),
            SoftmaxLayer(n_in, 20)
        )
        task_weights = [1./5, 1./10, 1./10, 2./5, 1./5]
        multitask_layer = MultitaskTopLayer(tasks=tasks,
                                            task_weights=task_weights)
    """

    def __init__(self, n_in=None, n_out=None,
                 test_error_fct='class_error',
                 l1_penalty_weight=0., l2_penalty_weight=0.,
                 tasks=None, task_weights=None, n_tasks=None,
                 lr_multiplier=None):

        if tasks is None and (n_in is None or n_out is None):
            raise ValueError('Either `tasks` or `n_in` and `n_out` ' +
                             'must be provided')

        if not tasks:
            self.n_in = n_in
            self.n_out = n_out if n_tasks is None else n_tasks * [n_out]
            # Number of output tasks
            self.n_tasks = n_tasks if n_tasks is not None else len(n_out)
            self.tasks = []

            if not isinstance(test_error_fct, (list, tuple)):
                test_error_fct = self.n_tasks * [test_error_fct]
            if not isinstance(l1_penalty_weight, (list, tuple)):
                l1_penalty_weight = self.n_tasks * [l1_penalty_weight]
            if not isinstance(l2_penalty_weight, (list, tuple)):
                l2_penalty_weight = self.n_tasks * [l2_penalty_weight]

            for (n_out_task, test_error_task, l1_task, l2_task) in \
              zip(self.n_out, test_error_fct,
                  l1_penalty_weight, l2_penalty_weight):
                self.tasks.append(SoftmaxLayer(n_in=n_in,
                                                n_out=n_out_task,
                                                l1_penalty_weight=l1_task,
                                                l2_penalty_weight=l2_task,
                                                test_error_fct=test_error_task,
                                                lr_multiplier=lr_multiplier))

        else:
            self.tasks = tasks
            assert all([self.tasks[0].n_in == t.n_in for t in tasks])

            self.n_in = self.tasks[0].n_in
            self.n_out = [t.n_out for t in self.tasks]
            self.n_tasks = len(self.tasks)

        if task_weights is not None:
            self.task_weights = task_weights
        else:
            self.task_weights = self.n_tasks * [1. / self.n_tasks]

        self.l1_penalty_weight = l1_penalty_weight
        self.l2_penalty_weight = l2_penalty_weight

        self.n_parameters = sum(task.n_parameters for task in self.tasks)
        self.lr_multiplier = [lr for task in self.tasks
                              for lr in task.lr_multiplier]

    def preallocate_temp_objects(self, batch_size):
        for task in self.tasks:
            if hasattr(task, 'preallocate_temp_objects'):
                task.preallocate_temp_objects(batch_size)

    @property
    def parameters(self):
        """Return a list where each element contains the parameters for a task.
        """
        parameters = []
        for task in self.tasks:
            parameters.extend(task.parameters)
        return parameters

    @parameters.setter
    def parameters(self, value):
        """Update the parameters.

        ``value`` must be a list/tuple of length
        ``MultitaskTopLayer.n_tasks``, each element of which must have
        the correct number of parameters for the task.
        """

        assert len(value) == self.n_parameters
        i = 0
        for task in self.tasks:
            task.parameters = value[i:i + task.n_parameters]
            i += task.n_parameters

    def update_parameters(self, value):
        assert len(value) == self.n_parameters
        i = 0
        for task in self.tasks:
            task.update_parameters(value[i:i + task.n_parameters])
            i += task.n_parameters

    @property
    def architecture(self):
        """Returns a dictionary describing the architecture of the layer."""
        return [task.architecture for task in self.tasks]

    @property
    def l1_penalty(self):
        """Compute the L1 penalty for all tasks."""
        return sum([task.l1_penalty for task in self.tasks])

    @property
    def l2_penalty(self):
        """Compute the L2 penalty for all tasks."""
        return sum([task.l2_penalty for task in self.tasks])

    def feed_forward(self, input_data, prediction=False):
        """Call ``feed_forward`` for each task and combine the activations.

        Passes ``input_data`` to all tasks and returns the activations
        as a list.
    
        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the task
            uses dropout.

        **Returns:**
        
        activations : list of ``GPUArray``
            The activations of the output units, one element for each task.
        """

        activations = []

        for task in self.tasks:
            activations_task = task.feed_forward(input_data, prediction)
            activations.append(activations_task)

        return activations

    def backprop(self, input_data, targets, cache=None):
        """Compute gradients for each task and combine the results.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        targets : ``GPUArray``
            The target values of the units.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        **Returns:**

        gradients : list
            Gradients with respect to the weights and biases for each task

        df_input : ``GPUArray``
            Gradients with respect to the input, obtained by adding
            the gradients with respect to the inputs from each task,
            weighted by ``MultitaskTopLayer.task_weights``.
        """

        df_input = gpuarray.zeros_like(input_data)

        if cache is None: cache = self.n_tasks * [None]

        gradients = []
        for targets_task, cache_task, task, task_weight  in \
          izip(targets, cache, self.tasks, self.task_weights):
            gradients_task, df_input_task = \
              task.backprop(input_data, targets_task,
                            cache_task)

            df_input = df_input.mul_add(1., df_input_task, task_weight)

            gradients.extend(gradients_task)

        return gradients, df_input

    def test_error(self, input_data, targets, average=True,
                   cache=None, prediction=False,
                   sum_errors=True):
        """Compute the error function on a test data set.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute the test error function for.

        targets : ``GPUArray``
            The target values of the units.

        average : bool
            Whether to divide the value of the error function by the
            number of data points given.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        sum_errors : bool, optional
            Whether to add up the errors from the different tasks. If
            this option is chosen, the user must make sure that all
            tasks use the same test error function.

        **Returns:**
        
        test_error : float or list
            Returns a float when ``sum_errors == True`` and a list
            with the individual errors otherwise.
        """

        test_error = []
        if cache is None:
            cache = self.n_tasks * [None]
        for targets_task, cache_task, task in \
            izip(targets, cache, self.tasks):
            test_error.append(task.test_error(input_data, targets_task,
                                              average, cache_task,
                                              prediction))

        if sum_errors:
            return sum(test_error)
        else:
            return np.array(test_error)

    def cross_entropy_error(self, input_data, targets, average=True,
                            cache=None, prediction=False,
                            sum_errors=True):
        """ Computes the cross-entropy error for all tasks.
        """

        loss = []
        if cache is None:
            cache = self.n_tasks * [None]

        for targets_task, cache_task, task in \
            izip(targets, cache, self.tasks):
            loss.append(task.cross_entropy_error(
                input_data, targets_task, average=average,
                cache=cache_task,
                prediction=prediction))

        if sum_errors:
            return sum(loss)
        else:
            return loss

    train_error = cross_entropy_error

########NEW FILE########
__FILENAME__ = softmax_layer
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
import cPickle
from pycuda import gpuarray
from pycuda import cumath
from math import sqrt
from .. import sampler
from .top_layer import TopLayer
from ..pycuda_ops import eps, linalg
from ..pycuda_ops.elementwise import sign, nan_to_zeros, substract_matrix
from ..pycuda_ops.reductions import matrix_sum_out_axis
from ..pycuda_ops.matrix import add_vec_to_mat
from ..pycuda_ops.softmax import softmax, cross_entropy


class SoftmaxLayer(TopLayer):
    r""" A multiclass classification layer, using
    cross-entropy loss function and softmax activations.

    **Parameters:**
    
    n_in : integer
        Number of input units.

    n_out : integer
        Number of output units (classes).

    parameters : array_like of ``GPUArray``
        Parameters used to initialize the layer. If this is omitted,
        then the weights are initalized randomly using *Bengio's rule*
        (uniform distribution with scale :math:`4 \cdot \sqrt{6 /
        (\mathtt{n\_in} + \mathtt{n\_out})}`) and the biases are
        initialized to zero. If ``parameters`` is given, then is must
        be in the form ``[weights, biases]``, where the shape of
        weights is ``(n_in, n_out)`` and the shape of ``biases`` is
        ``(n_out,)``. Both weights and biases must be ``GPUArray``.
    
    weights_scale : float, optional
        If ``parameters`` is omitted, then this factor is used as
        scale for initializing the weights instead of *Bengio's rule*.

    l1_penalty_weight : float, optional
        Weight used for L1 regularization of the weights.

    l2_penalty_weight : float, optional
       Weight used for L2 regularization of the weights.

    lr_multiplier : float, optional
        If this parameter is omitted, then the learning rate for the
        layer is scaled by :math:`2 / \sqrt{\mathtt{n\_in}}`. You may
        specify a different factor here.

    test_error_fct : {``class_error``, ``kl_error``, ``cross_entropy_error``}, optional
        Which error function to use on the test set. Default is
        ``class_error`` for classification error. Other choices are
        ``kl_error``, the Kullback-Leibler divergence, or
        ``cross_entropy_error``.

    **See also:**

    :class:`hebel.layers.LogisticLayer`,
    :class:`hebel.models.NeuralNet`,
    :class:`hebel.models.NeuralNetRegression`,
    :class:`hebel.layers.LinearRegressionLayer`

    **Examples**::

        # Use the simple initializer and initialize with random weights
        softmax_layer = SoftmaxLayer(1000, 10)

        # Sample weights yourself, specify an L1 penalty, and don't
        # use learning rate scaling
        import numpy as np
        from pycuda import gpuarray

        n_in = 1000
        n_out = 10
        weights = gpuarray.to_gpu(.01 * np.random.randn(n_in, n_out))
        biases = gpuarray.to_gpu(np.zeros((n_out,)))
        softmax_layer = SoftmaxLayer(n_in, n_out,
                                       parameters=(weights, biases),
                                       l1_penalty_weight=.1,
                                       lr_multiplier=1.)
    """

    n_parameters = 2

    def __init__(self, n_in, n_out,
                 parameters=None,
                 weights_scale=None,
                 l1_penalty_weight=0., l2_penalty_weight=0.,
                 lr_multiplier=None,
                 test_error_fct='class_error'):

        # Initialize weight using Bengio's rule
        self.weights_scale = 4 * sqrt(6. / (n_in + n_out)) \
                             if weights_scale is None \
                                else weights_scale

        if parameters is not None:
            self.W, self.b = parameters
        else:
            self.W = self.weights_scale * \
                     sampler.gen_uniform((n_in, n_out), dtype=np.float32) \
                     - .5 * self.weights_scale

            self.b = gpuarray.zeros((n_out,), dtype=np.float32)

        self.n_in = n_in
        self.n_out = n_out

        self.test_error_fct = test_error_fct

        self.l1_penalty_weight = l1_penalty_weight
        self.l2_penalty_weight = l2_penalty_weight

        self.lr_multiplier = 2 * [1. / np.sqrt(n_in, dtype=np.float32)] \
          if lr_multiplier is None else lr_multiplier

        self.persistent_temp_objects_config = (
            ('activations', ('batch_size', self.n_out), np.float32),
            ('lin_activations', ('batch_size', self.n_out), np.float32),            
            ('df_W', self.W.shape, np.float32),
            ('df_b', self.b.shape, np.float32),
            ('df_input', ('batch_size', self.n_in), np.float32),
            ('delta', ('batch_size', self.n_out), np.float32)
        )

    @property
    def architecture(self):
        return {'class': self.__class__,
                'n_in': self.n_in,
                'n_out': self.n_out}

    def feed_forward(self, input_data, prediction=False):
        """Propagate forward through the layer.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        **Returns:**
        
        activations : ``GPUArray``
            The activations of the output units.
        """

        lin_activations = self.get_temp_object('lin_activations',
            (input_data.shape[0], self.n_out), input_data.dtype)
        activations = self.get_temp_object('activations',
            (input_data.shape[0], self.n_out), input_data.dtype)
        linalg.dot(input_data, self.W, target=lin_activations)
        lin_activations = add_vec_to_mat(lin_activations, self.b, inplace=True)
        softmax(lin_activations, activations)

        return activations

    def backprop(self, input_data, targets,
                 cache=None):
        """ Backpropagate through the logistic layer.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute activations for.

        targets : ``GPUArray``
            The target values of the units.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        **Returns:**

        gradients : tuple of ``GPUArray``
            Gradients with respect to the weights and biases in the
            form ``(df_weights, df_biases)``.

        df_input : ``GPUArray``
            Gradients with respect to the input.
        """

        if cache is not None:
            activations = cache
        else:
            activations = self.feed_forward(input_data, prediction=False)

        # Get temporary objects
        df_W = self.get_temp_object('df_W', self.W.shape, self.W.dtype)
        df_b = self.get_temp_object('df_b', self.b.shape, self.b.dtype)
        df_input = self.get_temp_object('df_input',
                input_data.shape, input_data.dtype)
        delta = self.get_temp_object('delta',
                activations.shape, activations.dtype)

        substract_matrix(activations, targets, delta)
        nan_to_zeros(delta, delta)

        # Gradient wrt weights
        linalg.dot(input_data, delta, transa='T', target=df_W)
        # Gradient wrt bias
        matrix_sum_out_axis(delta, 0, target=df_b)

        # Gradient wrt input
        linalg.dot(delta, self.W, transb='T', target=df_input)

        # L1 penalty
        if self.l1_penalty_weight:
            df_W += self.l1_penalty_weight * sign(self.W)

        # L2 penalty
        if self.l2_penalty_weight:
            df_W += self.l2_penalty_weight * self.W

        return (df_W, df_b), df_input

    def test_error(self, input_data, targets, average=True,
                   cache=None, prediction=True):
        """Compute the test error function given some data and targets.

        Uses the error function defined in
        :class:`SoftmaxLayer.test_error_fct`, which may be different
        from the cross-entropy error function used for
        training'. Alternatively, the other test error functions may
        be called directly.

        **Parameters:**

        input_data : ``GPUArray``
            Inpute data to compute the test error function for.

        targets : ``GPUArray``
            The target values of the units.

        average : bool
            Whether to divide the value of the error function by the
            number of data points given.

        cache : list of ``GPUArray``
            Cache obtained from forward pass. If the cache is
            provided, then the activations are not recalculated.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved if the layers
            uses dropout.

        **Returns:**
        test_error : float
        """    
        if self.test_error_fct == 'class_error':
            test_error = self.class_error
        elif self.test_error_fct == 'kl_error':
            test_error = self.kl_error
        elif self.test_error_fct == 'cross_entropy_error':
            test_error = self.cross_entropy_error
        else:
            raise ValueError('unknown test error function "%s"'
                             % self.test_error_fct)

        return test_error(input_data, targets, average,
                          cache, prediction)

    def cross_entropy_error(self, input_data, targets, average=True,
                            cache=None, prediction=False):
        """ Return the cross entropy error
        """

        if cache is not None:
            activations = cache
        else:
            activations = \
              self.feed_forward(input_data, prediction=prediction)

        loss = cross_entropy(activations, targets)

        if average: loss /= targets.shape[0]
        return loss
        
    train_error = cross_entropy_error

    def class_error(self, input_data, targets, average=True,
                    cache=None, prediction=False):
        """ Return the classification error rate
        """

        if cache is not None:
            activations = cache
        else:
            activations = \
              self.feed_forward(input_data, prediction=prediction)

        targets = targets.get().argmax(1)
        class_error = np.sum(activations.get().argmax(1) != targets)

        if average: class_error = float(class_error) / targets.shape[0]
        return class_error

    def kl_error(self, input_data, targets, average=True,
                 cache=None, prediction=True):
        """ The KL divergence error
        """

        if cache is not None:
            activations = cache
        else:
            activations = \
              self.feed_forward(input_data, prediction=prediction)

        targets_non_nan = gpuarray.empty_like(targets)
        nan_to_zeros(targets, targets_non_nan)
        kl_error = gpuarray.sum(targets_non_nan *
                                (cumath.log(targets_non_nan + eps) -
                                 cumath.log(activations + eps)))
        if average:
            kl_error /= targets.shape[0]
        return float(kl_error.get())

########NEW FILE########
__FILENAME__ = top_layer
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from .hidden_layer import HiddenLayer


class TopLayer(HiddenLayer):
    """Abstract base class for a top-level layer."""
    
    n_tasks = 1

########NEW FILE########
__FILENAME__ = logistic_regression
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from .neural_net import NeuralNet


class LogisticRegression(NeuralNet):
    """ A logistic regression model

    """

    def __init__(self, n_in, n_out, test_error_fct='class_error'):
        super(LogisticRegression, self).\
            __init__(n_in, n_out, [],
                     test_error_fct=test_error_fct)

########NEW FILE########
__FILENAME__ = model
class Model(object):
    """ Abstract base-class for a Hebel model
    """

    def __init__(self):
        raise NotImplentedError

    @property
    def parameters(self):
        raise NotImplentedError

    @parameters.setter
    def parameters(self, value):
        raise NotImplentedError

    def update_parameters(self, value):
        raise NotImplentedError

    def evaluate(self, input_data, targets,
                 return_cache=False, prediction=True):
        """ Evaluate the loss function without computing gradients
        """

        raise NotImplentedError

    def training_pass(self, input_data, targets):
        """ Perform a full forward and backward pass through the model
        """

        raise NotImplentedError

    def test_error(self, input_data, targets, average=True, cache=None):
        """ Evaulate performance on a test set

        """
        raise NotImplentedError

    def feed_forward(self, input_data, return_cache=False, prediction=True):
        """ Get predictions from the model
        """

        raise NotImplentedError

########NEW FILE########
__FILENAME__ = multitask_neural_net
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from .neural_net import NeuralNet
from ..layers import MultitaskTopLayer

class MultitaskNeuralNet(NeuralNet):
    TopLayerClass = MultitaskTopLayer

########NEW FILE########
__FILENAME__ = neural_net
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
from hashlib import md5
from ..layers import HiddenLayer, TopLayer, SoftmaxLayer, LogisticLayer, InputDropout
from .model import Model


class NeuralNet(Model):
    """ A neural network for classification using the cross-entropy
    loss function.

    **Parameters:**

    layers : array_like
        An array of either integers or instances of
        :class:`hebel.models.HiddenLayer` objects. If integers are
        given, they represent the number of hidden units in each layer
        and new ``HiddenLayer`` objects will be created. If
        ``HiddenLayer`` instances are given, the user must make sure
        that each ``HiddenLayer`` has ``n_in`` set to the preceding
        layer's ``n_units``. If ``HiddenLayer`` instances are passed,
        then ``activation_function``, ``dropout``, ``n_in``,
        ``l1_penalty_weight``, and ``l2_penalty_weight`` are ignored.

    top_layer : :class:`hebel.models.TopLayer` instance, optional
        If ``top_layer`` is given, then it is used for the output
        layer, otherwise, a ``LogisticLayer`` instance is created.

    activation_function : {'sigmoid', 'tanh', 'relu', or 'linear'}, optional
        The activation function to be used in the hidden layers.

    dropout : bool, optional
        Whether to use dropout regularization

    input_dropout : float, in ``[0, 1]``
        Dropout probability for the input (default 0.0).

    n_in : integer, optional
        The dimensionality of the input. Must be given, if the first
        hidden layer is not passed as a
        :class:`hebel.models.HiddenLayer` instance.

    n_out : integer, optional
        The number of classes to predict from. Must be given, if a
        :class:`hebel.models.HiddenLayer` instance is not given in
        ``top_layer``.

    l1_penalty_weight : float, optional
        Weight for L1 regularization

    l2_penalty_weight : float, optional
        Weight for L2 regularization

    kwargs : optional
        Any additional arguments are passed on to ``top_layer``

    **See also:**
    
    :class:`hebel.models.LogisticRegression`,
    :class:`hebel.models.NeuralNetRegression`,
    :class:`hebel.models.MultitaskNeuralNet`

    **Examples**::

        # Simple form
        model = NeuralNet(layers=[1000, 1000],
                          activation_function='relu',
                          dropout=True,
                          n_in=784, n_out=10,
                          l1_penalty_weight=.1)

        # Extended form, initializing with ``HiddenLayer`` and ``TopLayer`` objects
        hidden_layers = [HiddenLayer(784, 1000, 'relu', dropout=True,
                                     l1_penalty_weight=.2),
                         HiddenLayer(1000, 1000, 'relu', dropout=True,
                                     l1_penalty_weight=.1)]
        softmax_layer = LogisticLayer(1000, 10, l1_penalty_weight=.1)

        model = NeuralNet(hidden_layers, softmax_layer)
    """

    TopLayerClass = SoftmaxLayer

    def __init__(self, layers, top_layer=None, activation_function='sigmoid',
                 dropout=False, input_dropout=0., n_in=None, n_out=None,
                 l1_penalty_weight=0., l2_penalty_weight=0.,
                 **kwargs):
        self.n_layers = len(layers)
        if n_out == 1 and self.TopLayerClass == SoftmaxLayer:
            self.TopLayerClass = LogisticLayer

        if l1_penalty_weight is not None and \
           not np.isscalar(l1_penalty_weight) and \
           len(l1_penalty_weight) != (self.n_layers + 1):
            raise ValueError("l1_penalty_weight must be a scalar "
                             "or have length %d",
                             self.n_layers + 1)

        if l2_penalty_weight is not None and \
           not np.isscalar(l2_penalty_weight) and \
           len(l2_penalty_weight) != (self.n_layers + 1):
            raise ValueError("l2_penalty_weight must be a scalar "
                             "or have length %d",
                             self.n_layers + 1)

        if np.isscalar(l1_penalty_weight):
            self.l1_penalty_weight_hidden = self.n_layers * [l1_penalty_weight]
            self.l1_penalty_weight_output = l1_penalty_weight
        else:
            self.l1_penalty_weight_hidden = l1_penalty_weight[:-1]
            self.l1_penalty_weight_output = l1_penalty_weight[-1]

        if np.isscalar(l2_penalty_weight):
            self.l2_penalty_weight_hidden = self.n_layers * [l2_penalty_weight]
            self.l2_penalty_weight_output = l2_penalty_weight
        else:
            self.l2_penalty_weight_hidden = l2_penalty_weight[:-1]
            self.l2_penalty_weight_output = l2_penalty_weight[-1]

        if type(dropout) is not list:
            if self.n_layers:
                dropout = self.n_layers * [dropout]
            else:
                dropout = [False]

        self.hidden_layers = []

        self.input_dropout = input_dropout
        if input_dropout:
            self.hidden_layers.append(InputDropout(n_in, input_dropout))

        for i, hidden_layer in enumerate(layers):
            if isinstance(hidden_layer, HiddenLayer):
                self.hidden_layers.append(hidden_layer)
            elif isinstance(hidden_layer, int):
                n_in_hidden = self.hidden_layers[-1].n_units if self.hidden_layers else n_in
                self.hidden_layers.append(
                    HiddenLayer(
                        n_in_hidden, hidden_layer,
                        activation_function,
                        dropout=dropout[i],
                        l1_penalty_weight=self.l1_penalty_weight_hidden[i],
                        l2_penalty_weight=self.l2_penalty_weight_hidden[i]))

        self.n_units_hidden = [hl.n_units for hl in self.hidden_layers]

        if top_layer is None:
            assert issubclass(self.TopLayerClass, TopLayer)
            n_in_top_layer = self.n_units_hidden[-1] \
                             if self.n_units_hidden else n_in
            self.top_layer = self.TopLayerClass(
                n_in_top_layer, n_out,
                l1_penalty_weight=self.l1_penalty_weight_output,
                l2_penalty_weight=self.l2_penalty_weight_output,
                **kwargs)
        else:
            self.top_layer = top_layer

        self.n_in = self.hidden_layers[0].n_in if self.hidden_layers else n_in
        self.n_out = self.top_layer.n_out 

        self.n_parameters = sum(hl.n_parameters
                                for hl in self.hidden_layers) + \
                                    self.top_layer.n_parameters

        self.lr_multiplier = [lr for hl in
                              self.hidden_layers + [self.top_layer]
                              for lr in hl.lr_multiplier]
    
    def preallocate_temp_objects(self, data_provider):
        for hl in self.hidden_layers:
            if hasattr(hl, 'preallocate_temp_objects'):
                hl.preallocate_temp_objects(data_provider)
        if hasattr(self.top_layer, 'preallocate_temp_objects'):
            self.top_layer.preallocate_temp_objects(data_provider)
    
    @property
    def parameters(self):
        """ A property that returns all of the model's parameters. """
        parameters = []
        for hl in self.hidden_layers:
            parameters.extend(hl.parameters)
        parameters.extend(self.top_layer.parameters)
        return parameters

    @parameters.setter
    def parameters(self, value):
        """ Used to set all of the model's parameters to new values.

        **Parameters:**

        value : array_like
            New values for the model parameters. Must be of length
            ``self.n_parameters``.
        """
    
        if len(value) != self.n_parameters:
            raise ValueError("Incorrect length of parameter vector. "
                             "Model has %d parameters, but got %d" %
                             (self.n_parameters, len(value)))

        i = 0
        for hl in self.hidden_layers:
            hl.parameters = value[i:i + hl.n_parameters]
            i += hl.n_parameters

        self.top_layer.parameters = value[-self.top_layer.n_parameters:]

    def update_parameters(self, value):
        assert len(value) == self.n_parameters

        i = 0
        for hl in self.hidden_layers:
            hl.update_parameters(value[i:i + hl.n_parameters])
            i += hl.n_parameters

        self.top_layer.update_parameters(value[-self.top_layer.n_parameters:])

    def checksum(self):
        """ Returns an MD5 digest of the model.

        This can be used to easily identify whether two models have the
        same architecture.
        """
        
        m = md5()
        for hl in self.hidden_layers:
            m.update(str(hl.architecture))
        m.update(str(self.top_layer.architecture))
        return m.hexdigest()

    def evaluate(self, input_data, targets,
                 return_cache=False, prediction=True):
        """ Evaluate the loss function without computing gradients.

        **Parameters:**

        input_data : GPUArray
            Data to evaluate

        targets: GPUArray
            Targets

        return_cache : bool, optional
            Whether to return intermediary variables from the
            computation and the hidden activations.

        prediction : bool, optional
            Whether to use prediction model. Only relevant when using
            dropout. If true, then weights are halved in layers that
            use dropout.

        **Returns:**

        loss : float
            The value of the loss function.

        hidden_cache : list, only returned if ``return_cache == True``
            Cache as returned by :meth:`hebel.models.NeuralNet.feed_forward`.

        activations : list, only returned if ``return_cache == True``
            Hidden activations as returned by
            :meth:`hebel.models.NeuralNet.feed_forward`.
        """

        # Forward pass
        activations, hidden_cache = self.feed_forward(
            input_data, return_cache=True, prediction=prediction)

        loss = self.top_layer.train_error(None,
            targets, average=False, cache=activations,
            prediction=prediction)

        for hl in self.hidden_layers:
            if hl.l1_penalty_weight: loss += hl.l1_penalty
            if hl.l2_penalty_weight: loss += hl.l2_penalty

        if self.top_layer.l1_penalty_weight: loss += self.top_layer.l1_penalty
        if self.top_layer.l2_penalty_weight: loss += self.top_layer.l2_penalty

        if not return_cache:
            return loss
        else:
            return loss, hidden_cache, activations

    def training_pass(self, input_data, targets):
        """ Perform a full forward and backward pass through the model.

        **Parameters:**

        input_data : GPUArray
            Data to train the model with.

        targets : GPUArray
            Training targets.

        **Returns:**

        loss : float
            Value of loss function as evaluated on the data and targets.

        gradients : list of GPUArray
            Gradients obtained from backpropagation in the backward pass.
        """

        # Forward pass
        loss, hidden_cache, logistic_cache = self.evaluate(
            input_data, targets, return_cache=True, prediction=False)

        # Backpropagation
        if self.hidden_layers:
            hidden_activations = hidden_cache[-1][0]
        else:
            hidden_activations = input_data

        df_top_layer = \
          self.top_layer.backprop(hidden_activations, targets,
                                  cache=logistic_cache)
        gradients = list(df_top_layer[0][::-1])
        df_hidden = df_top_layer[1]

        if self.hidden_layers:
            hidden_inputs = [input_data] + [c[0] for c in hidden_cache[:-1]]            
            for hl, hc, hi in \
                zip(self.hidden_layers[::-1], hidden_cache[::-1],
                    hidden_inputs[::-1]):
                g, df_hidden = hl.backprop(hi, df_hidden, cache=hc)
                gradients.extend(g[::-1])

        gradients.reverse()

        return loss, gradients

    def test_error(self, test_data, average=True):
        """ Evaulate performance on a test set.

        **Parameters:**

        test_data : :class:``hebel.data_provider.DataProvider``
            A ``DataProvider`` instance to evaluate on the model.

        average : bool, optional
            Whether to divide the loss function by the number of
            examples in the test data set.

        **Returns:**

        test_error : float
        """

        test_error = 0.
        for batch_data, batch_targets in test_data:
            _, hidden_cache, logistic_cache = \
              self.evaluate(batch_data, batch_targets,
                            return_cache=True,
                            prediction=True)

            if self.hidden_layers:
                hidden_activations = hidden_cache[-1]
            else:
                hidden_activations = batch_data

            test_error += self.top_layer.test_error(hidden_activations,
                                                    batch_targets, average=False,
                                                    cache=logistic_cache,
                                                    prediction=True)

        if average: test_error /= float(test_data.N)

        return test_error

    def feed_forward(self, input_data, return_cache=False, prediction=True):
        """ Run data forward through the model.

        **Parameters:**

        input_data : GPUArray
            Data to run through the model.

        return_cache : bool, optional
            Whether to return the intermediary results.

        prediction : bool, optional
            Whether to run in prediction mode. Only relevant when
            using dropout. If true, weights are halved. If false, then
            half of hidden units are randomly dropped and the dropout
            mask is returned in case ``return_cache==True``.

        **Returns:**
        
        prediction : GPUArray
            Predictions from the model.

        cache : list of GPUArray, only returned if ``return_cache == True``
            Results of intermediary computations.    
        """

        hidden_cache = None     # Create variable in case there are no hidden layers
        if self.hidden_layers:
            # Forward pass
            hidden_cache = []
            for i in range(len(self.hidden_layers)):
                hidden_activations = hidden_cache[i - 1][0] if i else input_data
                # Use dropout predict if previous layer has dropout
                hidden_cache.append(self.hidden_layers[i]
                                    .feed_forward(hidden_activations,
                                                  prediction=prediction))

            hidden_activations = hidden_cache[-1][0]

        else:
            hidden_activations = input_data

        # Use dropout_predict if last hidden layer has dropout
        activations = \
          self.top_layer.feed_forward(hidden_activations,
                                      prediction=False)

        if return_cache:
            return activations, hidden_cache
        return activations

########NEW FILE########
__FILENAME__ = neural_net_regression
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from .neural_net import NeuralNet
from ..layers import LinearRegressionLayer

class NeuralNetRegression(NeuralNet):
    """A neural network for regression using the squared error loss
    function.

    This class exists for convenience. The same results can be
    achieved by creating a :class:`hebel.models.NeuralNet` instance
    and passing a :class:`hebel.layers.LinearRegressionLayer` instance
    as the ``top_layer`` argument.

    **Parameters:**

    layers : array_like
        An array of either integers or instances of
        :class:`hebel.models.HiddenLayer` objects. If integers are
        given, they represent the number of hidden units in each layer
        and new ``HiddenLayer`` objects will be created. If
        ``HiddenLayer`` instances are given, the user must make sure
        that each ``HiddenLayer`` has ``n_in`` set to the preceding
        layer's ``n_units``. If ``HiddenLayer`` instances are passed,
        then ``activation_function``, ``dropout``, ``n_in``,
        ``l1_penalty_weight``, and ``l2_penalty_weight`` are ignored.

    top_layer : :class:`hebel.models.TopLayer` instance, optional
        If ``top_layer`` is given, then it is used for the output
        layer, otherwise, a ``LinearRegressionLayer`` instance is created.

    activation_function : {'sigmoid', 'tanh', 'relu', or 'linear'}, optional
        The activation function to be used in the hidden layers.

    dropout : bool, optional
        Whether to use dropout regularization

    n_in : integer, optional
        The dimensionality of the input. Must be given, if the first
        hidden layer is not passed as a
        :class:`hebel.models.HiddenLayer` instance.

    n_out : integer, optional
        The number of classes to predict from. Must be given, if a
        :class:`hebel.models.HiddenLayer` instance is not given in
        ``top_layer``.

    l1_penalty_weight : float, optional
        Weight for L1 regularization

    l2_penalty_weight : float, optional
        Weight for L2 regularization

    kwargs : optional
        Any additional arguments are passed on to ``top_layer``

    **See also:**
    
    :class:`hebel.models.NeuralNet`,
    :class:`hebel.models.MultitaskNeuralNet`,
    :class:`hebel.layers.LinearRegressionLayer`

    """
    TopLayerClass = LinearRegressionLayer

########NEW FILE########
__FILENAME__ = monitors
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

""" Implements monitors that report on the progress of training, such
as error rates and parameters. Currently, we just have
SimpleProgressMonitor, which simply prints the current error to the
shell.

"""

import numpy as np
import time, cPickle, os, sys
from datetime import datetime

class ProgressMonitor(object):
    def __init__(self, experiment_name=None, save_model_path=None,
                 save_interval=None, output_to_log=False, 
                 model=None):

        self.experiment_name = experiment_name
        self.save_model_path = save_model_path
        self.save_interval = save_interval
        self.output_to_log = output_to_log
        self.model = model

        self.train_error = []
        self.validation_error = []
        self.avg_epoch_t = None
        self._time = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')

        self.epochs = 0

        self.makedir()

    def print_(self, obj):
        if self.log is not None:
            self.log.write(str(obj) + '\n')
        print obj
        sys.stdout.flush()

    @property
    def yaml_config(self):
        return self._yaml_config

    @yaml_config.setter
    def yaml_config(self, yaml_config):
        if yaml_config is not None:
            self._yaml_config = yaml_config
            yaml_path = os.path.join(self.save_path, 'yaml_config.yml')
            f = open(yaml_path, 'w')
            f.write(self._yaml_config)
            self._yaml_config = yaml_config

    @property
    def test_error(self):
        return self._test_error

    @test_error.setter
    def test_error(self, test_error):
        self._test_error = test_error
        self.print_("Test error: %.4f" % test_error)
        f = open(os.path.join(self.save_path, "test_error"), 'w')
        f.write('%.5f\n' % test_error)

    def makedir(self):
        experiment_dir_name = '_'.join((
            self.experiment_name,
            datetime.now().strftime('%Y-%m-%dT%H-%M-%S')))

        path = os.path.join(self.save_model_path,
                            experiment_dir_name)
        if not os.path.exists(path):
            os.makedirs(path)
        self.save_path = path

        if self.output_to_log:
            self.log = open(os.path.join(self.save_path, 'output.log'), 'w', 1)
            # sys.stdout = self.log
            # sys.stderr = self.log

    def start_training(self):
        self.start_time = datetime.now()

    def report(self, epoch, train_error, validation_error=None,
               new_best=None, epoch_t=None):
        # Print logs
        self.train_error.append((epoch, train_error))
        if validation_error is not None:
            self.validation_error.append((epoch, validation_error))
        self.print_error(epoch, train_error, validation_error, new_best)

        if epoch_t is not None:
            self.avg_epoch_t = ((epoch - 1) * \
                                self.avg_epoch_t + epoch_t) / epoch \
                                if self.avg_epoch_t is not None else epoch_t

        # Pickle model
        if self.save_interval is not None:
            if not epoch % self.save_interval:
                filename = 'model_%s_epoch%04d.pkl' % (
                  self.experiment_name,
                  epoch)
                path = os.path.join(self.save_path, filename)
                cPickle.dump(self.model, open(path, 'wb'))
        elif new_best is not None and new_best:
            filename = 'model_%s_current_best.pkl' % self.experiment_name
            path = os.path.join(self.save_path, filename)
            cPickle.dump(self.model, open(path, 'wb'))

    def print_error(self, epoch, train_error, validation_error=None, new_best=None):
        if validation_error is not None:
            report_str = 'Epoch %d, Validation error: %.5g, Train Loss: %.3f' % \
              (epoch, validation_error, train_error)
            if new_best is not None and new_best:
                report_str += ' (*)'
        else:
            report_str = 'Epoch %d, Train Loss: %.3f' % \
              (epoch, train_error)
        self.print_(report_str)

    def avg_weight(self):
        self.print_("\nAvg weights:")

        i = 0
        for param in self.model.parameters:
            if len(param.shape) != 2: continue
            param_cpu = np.abs(param.get())
            mean_weight = param_cpu.mean()
            std_weight = param_cpu.std()
            self.print_('Layer %d: %.4f [%.4f]' % (i, mean_weight, std_weight))
            
            i += 1

    def finish_training(self):
        # Print logs
        end_time = datetime.now()
        self.train_time = end_time - self.start_time
        self.print_("Runtime: %dm %ds" % (self.train_time.total_seconds() // 60,
                                    self.train_time.total_seconds() % 60))
        self.print_("Avg. time per epoch %.2fs" % self.avg_epoch_t)

        # Pickle model
        filename = 'model_%s_final.pkl' % self.experiment_name
        path = os.path.join(self.save_path, filename)
        self.print_("Saving model to %s" % path)
        cPickle.dump(self.model, open(path, 'wb'))
        if self.save_interval is None:
            os.remove(os.path.join(
                self.save_path, 'model_%s_current_best.pkl' % self.experiment_name))

    def __del__(self):
        if self.output_to_log:
            self.log.close()


class SimpleProgressMonitor(object):
    def __init__(self, model=None):
        self.model = model

        self.train_error = []
        self.validation_error = []
        self.avg_epoch_t = None
        self._time = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')

    def start_training(self):
        self.start_time = datetime.now()

    def report(self, epoch, train_error, validation_error=None,
               new_best=None, epoch_t=None):
        self.train_error.append((epoch, train_error))
        if validation_error is not None:
            self.validation_error.append((epoch, validation_error))

        # Print logs
        self.print_error(epoch, train_error, validation_error)

        if epoch_t is not None:
            self.avg_epoch_t = ((epoch - 1) * \
                                self.avg_epoch_t + epoch_t) / epoch \
                                if self.avg_epoch_t is not None else epoch_t
        sys.stdout.flush()

    def print_error(self, epoch, train_error, validation_error=None):
        if validation_error is not None:
            print 'Epoch %d, Validation error: %.5g, Train Loss: %.3f' % \
              (epoch, validation_error, train_error)
        else:
            print 'Epoch %d, Train Loss: %.3f' % \
              (epoch, train_error)

    def avg_weight(self):
        print "\nAvg weights:"

        i = 0
        for param in self.model.parameters:
            if len(param.shape) != 2: continue
            param_cpu = np.abs(param.get())
            mean_weight = param_cpu.mean()
            std_weight = param_cpu.std()
            print 'Layer %d: %.4f [%.4f]' % (i, mean_weight, std_weight)
            i += 1
        sys.stdout.flush()

    def finish_training(self):
        # Print logs
        end_time = datetime.now()
        self.train_time = end_time - self.start_time
        print "Runtime: %dm %ds" % (self.train_time.total_seconds() // 60,
                                    self.train_time.total_seconds() % 60)
        print "Avg. time per epoch %.2fs" % self.avg_epoch_t
        sys.stdout.flush()
########NEW FILE########
__FILENAME__ = optimizers
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

""" Implements optimization algorithms to train the models. The single
algorithm we have in online stochastic gradient descent (SGD).

"""

import numpy as np
import time, cPickle, os, inspect
from .pycuda_ops.matrix import vector_normalize
from .schedulers import constant_scheduler
from .monitors import SimpleProgressMonitor


class EarlyStoppingModule(object):
    def __init__(self, model):
        self.model = model
        self.best_validation_loss = np.inf

    def update(self, epoch, validation_loss):
        if validation_loss < self.best_validation_loss:
            print '* ',
            self.best_validation_loss = validation_loss
            self.best_params = [p.copy() for p in self.model.parameters]
            assert self.best_params[0] is not self.model.parameters[0]
            self.best_epoch = epoch
            return True
        return False

    def finish(self):
        self.model.parameters = self.best_params
        print "Optimization complete. " \
            "Best validation error of %.5g obtained in self.epoch %d" % \
            (self.best_validation_loss, self.best_epoch)


class SGD(object):
    @property
    def best_validation_loss(self):
        return self.early_stopping_module.best_validation_loss

    def __init__(self,
                 model, parameter_updater,
                 train_data,
                 validation_data=None,
                 progress_monitor=None,
                 learning_rate_schedule=constant_scheduler(.1),
                 momentum_schedule=None,
                 early_stopping=True):

        """ Stochastic gradient descent
        """

        ### Initialization

        self.model = model

        ### Training data
        self.train_data = train_data

        ### Validation data
        self.validation_data = validation_data

        ### Data size
        self.N_train = self.train_data.N

        if validation_data is not None:
            self.N_validation = self.validation_data.N

        ### Learning rate schedule
        self.learning_parameter_iterators = [learning_rate_schedule]

        ### Momentum, rmsprop, etc

        self.parameter_updater = parameter_updater(self.model)

        if momentum_schedule is not None:
            self.learning_parameter_iterators.append(momentum_schedule)

        if progress_monitor is None:
            self.progress_monitor = SimpleProgressMonitor(model=self.model)
        else:
            self.progress_monitor = progress_monitor

        if self.progress_monitor.model is None:
            self.progress_monitor.model = self.model

        self.early_stopping_module = EarlyStoppingModule(self.model) \
                                     if early_stopping else None

        self.model.preallocate_temp_objects(self.train_data)

    def run(self, iterations=200, validation_interval=5,
            yaml_config=None,
            task_id=None):
        # Initialize variables
        self.epoch = 0
        done_looping = False

        self.progress_monitor.start_training()

        self.progress_monitor.task_id = task_id
        self.progress_monitor.yaml_config = yaml_config

        # Main loop
        for self.epoch in range(self.epoch, self.epoch + iterations):
            learning_parameters = map(lambda lp: lp.next(),
                                      self.learning_parameter_iterators)
            if done_looping: break

            try:
                t = time.time()

                # Train on mini-batches
                train_loss = 0.

                for batch_idx, (batch_data, batch_targets) in \
                  enumerate(self.train_data):
                    batch_size = self.train_data.batch_size

                    self.parameter_updater.pre_gradient_update()

                    batch_loss, gradients = \
                        self.model.training_pass(batch_data, batch_targets)
                    train_loss += batch_loss
                    self.parameter_updater\
                      .post_gradient_update(gradients, batch_size,
                                            learning_parameters)

                # Evaluate on validation data
                if self.validation_data is not None and \
                   not self.epoch % validation_interval:
                    validation_loss_rate = self.model.test_error(
                        self.validation_data)
                    # validation_loss = 0.
                    # for batch_idx, (batch_data, batch_targets) in \
                    #   enumerate(self.validation_data):

                    #     validation_loss += self.model.test_error(batch_data,
                    #                                              batch_targets,
                    #                                              average=False)

                    # validation_loss_rate = \
                    #     validation_loss / float(self.N_validation)

                    new_best = self.early_stopping_module.update(
                        self.epoch, validation_loss_rate) \
                        if self.early_stopping_module is not None else None

                    epoch_t = time.time() - t

                    self.progress_monitor.report(self.epoch, train_loss,
                                                 validation_loss_rate,
                                                 new_best,
                                                 epoch_t=epoch_t)
                else:
                    epoch_t = time.time() - t
                    self.progress_monitor.report(self.epoch, train_loss,
                                                 epoch_t=epoch_t)

            except KeyboardInterrupt:
                print "Keyboard interrupt. Stopping training and cleaning up."
                done_looping = True

        if self.early_stopping_module is not None:
            self.early_stopping_module.finish()

        self.progress_monitor.finish_training()

    def norm_v_norm(self):
        if self.max_vec_norm:
            for w in self.model.parameters:
                if len(w.shape) == 2:
                    vector_normalize(w, self.max_vec_norm)

########NEW FILE########
__FILENAME__ = parameter_updaters
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

""" Implements different variants of updating the parameters in SGD,
such as momentum and Nesterov momentum.

"""

from pycuda import gpuarray
from itertools import izip


class ParameterUpdater(object):
    def __init__(self, model):
        self.model = model

    def pre_gradient_update(self, stream=None):
        pass

    def post_gradient_update(self, gradients, stream=None):
        pass


class SimpleSGDUpdate(ParameterUpdater):
    def post_gradient_update(self, gradients, batch_size,
                             learning_parameters,
                             stream=None):
        learning_rate = learning_parameters[0]

        multiplier = [-lr_mult * learning_rate / batch_size for lr_mult in
                      self.model.lr_multiplier]
        update = zip(gradients, multiplier)
        self.model.update_parameters(update)


class MomentumUpdate(ParameterUpdater):
    def __init__(self, model):
        self.model = model
        self.velocity = [gpuarray.zeros_like(p)
                         for p in self.model.parameters]

    def post_gradient_update(self, gradients, batch_size,
                             learning_parameters, stream=None):
        learning_rate, momentum = learning_parameters

        updates = []
        for gparam, vparam, lr_multiplier in \
            izip(gradients, self.velocity, self.model.lr_multiplier):
            vparam._axpbyz(momentum,
                           gparam, -learning_rate * lr_multiplier / batch_size,
                           vparam, stream=stream)
            updates.append((vparam, 1.))
        self.model.update_parameters(updates)


class NesterovMomentumUpdate(MomentumUpdate):
    def pre_gradient_update(self):
        """ First step of Nesterov momentum method:
        take step in direction of accumulated gradient
        """

        updates = zip(self.velocity, self.model.n_parameters * [1.])
        self.model.update_parameters(updates)

    def post_gradient_update(self, gradients, batch_size,
                             learning_parameters, stream=None):
        """ Second step of Nesterov momentum method:
        take step in direction of new gradient and update velocity
        """

        learning_rate, momentum = learning_parameters

        updates = []
        for param, gparam, vparam, lr_multiplier in \
          izip(self.model.parameters, gradients,
              self.velocity, self.model.lr_multiplier):

            updates.append(
                (gparam, -learning_rate * lr_multiplier / batch_size))
            # param -= learning_rate*lr_multiplier/batch_size*gparam
            # param._axpbyz(1., gparam, -learning_rate*lr_multiplier/batch_size,
            #               param, stream=stream)
            # vparam = momentum*vparam \
            #    - learning_rate*lr_multiplier/batch_size*gparam
            vparam._axpbyz(momentum, gparam, -learning_rate*lr_multiplier/batch_size,
                           vparam, stream=stream)
        self.model.update_parameters(updates)

########NEW FILE########
__FILENAME__ = cublas
# This file is taken from scikits.cuda (https://github.com/lebedov/scikits.cuda)
# Copyright (c) 2009-2013, Lev Givon. All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:

# Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
# Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
# Neither the name of Lev Givon nor the names of any contributors may
# be used to endorse or promote products derived from this software
# without specific prior written permission.  THIS SOFTWARE IS
# PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
# OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
# SUCH DAMAGE.

#!/usr/bin/env python

"""
Python interface to CUBLAS functions.

Note: this module does not explicitly depend on PyCUDA.
"""

import re
import sys
import warnings
import ctypes
import ctypes.util
import atexit
import numpy as np

from string import Template

import cuda
import utils


if sys.platform == 'linux2':
    _libcublas_libname_list = ['libcublas.so', 'libcublas.so.4', 'libcublas.so.5']
elif sys.platform == 'darwin':
    _libcublas_libname_list = ['libcublas.dylib']
elif sys.platform == 'win32':
    _libcublas_libname_list = ['cublas64_60.dll', 'cublas32_60.dll', 
                               'cublas64_55.dll', 'cublas32_55.dll', 
                               'cublas64_50.dll', 'cublas32_50.dll']
else:
    raise RuntimeError('unsupported platform')

# Print understandable error message when library cannot be found:
_libcublas = None
for _libcublas_libname in _libcublas_libname_list:
    try:
        _libcublas = ctypes.cdll.LoadLibrary(_libcublas_libname)
    except OSError:
        pass
    else:
        break
if _libcublas == None:
    raise OSError('cublas library not found')

# Generic CUBLAS error:
class cublasError(Exception):
    """CUBLAS error"""
    pass

# Exceptions corresponding to different CUBLAS errors:
class cublasNotInitialized(cublasError):
    """CUBLAS library not initialized."""
    pass

class cublasAllocFailed(cublasError):
    """Resource allocation failed."""
    pass

class cublasInvalidValue(cublasError):
    """Unsupported numerical value was passed to function."""
    pass

class cublasArchMismatch(cublasError):
    """Function requires an architectural feature absent from the device."""
    pass

class cublasMappingError(cublasError):
    """Access to GPU memory space failed."""
    pass

class cublasExecutionFailed(cublasError):
    """GPU program failed to execute."""
    pass

class cublasInternalError(cublasError):
    """An internal CUBLAS operation failed."""
    pass

cublasExceptions = {
    0x1: cublasNotInitialized,
    0x3: cublasAllocFailed,
    0x7: cublasInvalidValue,
    0x8: cublasArchMismatch,
    0xb: cublasMappingError,
    0xd: cublasExecutionFailed,
    0xe: cublasInternalError,
    }

_CUBLAS_OP = {
    0: 0,   # CUBLAS_OP_N
    'n': 0, 
    'N': 0,
    1: 1,   # CUBLAS_OP_T
    't': 1, 
    'T': 1,
    2: 2,   # CUBLAS_OP_C
    'c': 2, 
    'C': 2,
    }

_CUBLAS_FILL_MODE = {
    0: 0,   # CUBLAS_FILL_MODE_LOWER
    'l': 0, 
    'L': 0,
    1: 1,   # CUBLAS_FILL_MODE_UPPER
    'u': 1, 
    'U': 1,
    }

_CUBLAS_DIAG = {
    0: 0,   # CUBLAS_DIAG_NON_UNIT,
    'n': 0, 
    'N': 0,
    1: 1,   # CUBLAS_DIAG_UNIT
    'u': 1, 
    'U': 1,
    }

_CUBLAS_SIDE_MODE = {
    0: 0,   # CUBLAS_SIDE_LEFT
    'l': 0,
    'L': 0, 
    1: 1,   # CUBLAS_SIDE_RIGHT
    'r': 1,
    'r': 1  
    }

def cublasCheckStatus(status):
    """
    Raise CUBLAS exception
    
    Raise an exception corresponding to the specified CUBLAS error
    code.
    
    Parameters
    ----------
    status : int
        CUBLAS error code.

    See Also
    --------
    cublasExceptions

    """
    
    if status != 0:
        try:
            raise cublasExceptions[status]
        except KeyError:
            raise cublasError

# Helper functions:

_libcublas.cublasCreate_v2.restype = int
_libcublas.cublasCreate_v2.argtypes = [ctypes.c_void_p]
def cublasCreate():
    """
    Initialize CUBLAS.

    Initializes CUBLAS and creates a handle to a structure holding
    the CUBLAS library context.

    Returns
    -------
    handle : int
        CUBLAS context.
            
    """

    handle = ctypes.c_int()
    status = _libcublas.cublasCreate_v2(ctypes.byref(handle))
    cublasCheckStatus(status)
    return handle.value    

_libcublas.cublasDestroy_v2.restype = int
_libcublas.cublasDestroy_v2.argtypes = [ctypes.c_int]
def cublasDestroy(handle):
    """
    Release CUBLAS resources.

    Releases hardware resources used by CUBLAS.

    Parameters
    ----------
    handle : int
        CUBLAS context.
        
    """

    status = _libcublas.cublasDestroy_v2(ctypes.c_int(handle))
    cublasCheckStatus(status)

_libcublas.cublasGetVersion_v2.restype = int
_libcublas.cublasGetVersion_v2.argtypes = [ctypes.c_int,
                                           ctypes.c_void_p]
def cublasGetVersion(handle):
    """
    Get CUBLAS version.

    Returns version number of installed CUBLAS libraries.

    Parameters
    ----------
    handle : int
        CUBLAS context.

    Returns
    -------
    version : int
        CUBLAS version.

    """
    
    version = ctypes.c_int()
    status = _libcublas.cublasGetVersion_v2(handle, ctypes.byref(version))
    cublasCheckStatus(status)
    return version.value

_libcublas.cublasSetStream_v2.restype = int
_libcublas.cublasSetStream_v2.argtypes = [ctypes.c_int,
                                          ctypes.c_int]
def cublasSetStream(handle, id):
    """
    Set current CUBLAS library stream.
    
    Parameters
    ----------
    handle : id
        CUBLAS context.
    id : int
        Stream ID.

    """

    status = _libcublas.cublasSetStream_v2(handle, id)
    cublasCheckStatus(status)

_libcublas.cublasGetStream_v2.restype = int
_libcublas.cublasGetStream_v2.argtypes = [ctypes.c_int,
                                          ctypes.c_void_p]
def cublasGetStream(handle):
    """
    Set current CUBLAS library stream.

    Parameters
    ----------
    handle : int
        CUBLAS context.
  
    Returns
    -------
    id : int
        Stream ID.
  
    """
    
    id = ctypes.c_int()
    status = _libcublas.cublasGetStream_v2(handle, ctypes.byref(id))
    cublasCheckStatus(status)
    return id.value

try:
    _libcublas.cublasGetCurrentCtx.restype = int
except AttributeError:
    def cublasGetCurrentCtx():
        raise NotImplementedError(
            'cublasGetCurrentCtx() not found; CULA CUBLAS library probably\n'
            'precedes NVIDIA CUBLAS library in library search path')
else:
    def cublasGetCurrentCtx():
        """
        Get current CUBLAS context.
        
        Returns the current context used by CUBLAS.
        
        Returns
        -------
        handle : int
            CUBLAS context.
        
        """

        return _libcublas.cublasGetCurrentCtx()
    
### BLAS Level 1 Functions ###

# ISAMAX, IDAMAX, ICAMAX, IZAMAX
I_AMAX_doc = Template(
"""
    Index of maximum magnitude element.

    Finds the smallest index of the maximum magnitude element of a
    ${precision} ${real} vector.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vector.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input vector.
    incx : int
        Storage spacing between elements of `x`.

    Returns
    -------
    idx : int
        Index of maximum magnitude element.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data} 
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> h = cublasCreate()
    >>> m = ${func}(h, x_gpu.size, x_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(m, np.argmax(np.abs(x)))
    True
    
    Notes
    -----
    This function returns a 0-based index.
    
""")

_libcublas.cublasIsamax_v2.restype = int
_libcublas.cublasIsamax_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIsamax(handle, n, x, incx):
    result = ctypes.c_int()    
    status = \
           _libcublas.cublasIsamax_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1

cublasIsamax.__doc__ = \
                     I_AMAX_doc.substitute(precision='single-precision',
                                           real='real',
                                           data='np.random.rand(5).astype(np.float32)',
                                           func='cublasIsamax')

_libcublas.cublasIdamax_v2.restype = int
_libcublas.cublasIdamax_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIdamax(handle, n, x, incx):
    result = ctypes.c_int()
    status = \
           _libcublas.cublasIdamax_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1

cublasIdamax.__doc__ = \
                     I_AMAX_doc.substitute(precision='double-precision',
                                           real='real',
                                           data='np.random.rand(5).astype(np.float64)',
                                           func='cublasIdamax')

_libcublas.cublasIcamax_v2.restype = int
_libcublas.cublasIcamax_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIcamax(handle, n, x, incx):
    result = ctypes.c_int()
    status = \
           _libcublas.cublasIcamax_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1

cublasIcamax.__doc__ = \
                     I_AMAX_doc.substitute(precision='single precision',
                                           real='complex',
                                           data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                           func='cublasIcamax')

_libcublas.cublasIzamax_v2.restype = int
_libcublas.cublasIzamax_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIzamax(handle, n, x, incx):
    result = ctypes.c_int()
    status = \
           _libcublas.cublasIzamax_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1
    
cublasIzamax.__doc__ = \
                     I_AMAX_doc.substitute(precision='double precision',
                                           real='complex',
                                           data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                           func='cublasIzamax')

# ISAMIN, IDAMIN, ICAMIN, IZAMIN
I_AMIN_doc = Template(
"""
    Index of minimum magnitude element (${precision} ${real}).

    Finds the smallest index of the minimum magnitude element of a
    ${precision} ${real} vector.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vector.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input vector.
    incx : int
        Storage spacing between elements of `x`.

    Returns
    -------
    idx : int
        Index of minimum magnitude element.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> h = cublasCreate()
    >>> m = ${func}(h, x_gpu.size, x_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(m, np.argmin(x))
    True

    Notes
    -----
    This function returns a 0-based index.

    """
)

_libcublas.cublasIsamin_v2.restype = int
_libcublas.cublasIsamin_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIsamin(handle, n, x, incx):
    result = ctypes.c_int()
    status = \
           _libcublas.cublasIsamin_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1

cublasIsamin.__doc__ = \
                     I_AMIN_doc.substitute(precision='single-precision',
                                           real='real',
                                           data='np.random.rand(5).astype(np.float32)',
                                           func='cublasIsamin')

_libcublas.cublasIdamin_v2.restype = int
_libcublas.cublasIdamin_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIdamin(handle, n, x, incx):
    result = ctypes.c_int()
    status = \
           _libcublas.cublasIdamin_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1

cublasIdamin.__doc__ = \
                     I_AMIN_doc.substitute(precision='double-precision',
                                           real='real',
                                           data='np.random.rand(5).astype(np.float64)',
                                           func='cublasIdamin')

_libcublas.cublasIcamin_v2.restype = int
_libcublas.cublasIcamin_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIcamin(handle, n, x, incx):
    result = ctypes.c_int()
    status = \
           _libcublas.cublasIcamin_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1

cublasIcamin.__doc__ = \
                     I_AMIN_doc.substitute(precision='single-precision',
                                           real='complex',
                                           data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                           func='cublasIcamin')

_libcublas.cublasIzamin_v2.restype = int
_libcublas.cublasIzamin_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasIzamin(handle, n, x, incx):
    result = ctypes.c_int()
    status = \
           _libcublas.cublasIzamin_v2(handle,
                                      n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return result.value-1

cublasIzamin.__doc__ = \
                     I_AMIN_doc.substitute(precision='double-precision',
                                           real='complex',
                                           data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                           func='cublasIzamin')

# SASUM, DASUM, SCASUM, DZASUM
_ASUM_doc = Template(                    
"""
    Sum of absolute values of ${precision} ${real} vector.

    Computes the sum of the absolute values of the elements of a
    ${precision} ${real} vector.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vector.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input vector.
    incx : int
        Storage spacing between elements of `x`.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> h = cublasCreate()
    >>> s = ${func}(h, x_gpu.size, x_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(s, np.sum(np.abs(x)))
    True

    Returns
    -------
    s : ${ret_type}
        Sum of absolute values.
        
    """
)

_libcublas.cublasSasum_v2.restype = int
_libcublas.cublasSasum_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasSasum(handle, n, x, incx):
    result = ctypes.c_float()
    status = _libcublas.cublasSasum_v2(handle,
                                       n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float32(result.value)

cublasSasum.__doc__ = \
                    _ASUM_doc.substitute(precision='single-precision',
                                         real='real',
                                         data='np.random.rand(5).astype(np.float32)',
                                         func='cublasSasum',
                                         ret_type='numpy.float32')

_libcublas.cublasDasum_v2.restype = int
_libcublas.cublasDasum_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasDasum(handle, n, x, incx):
    result = ctypes.c_double()
    status = _libcublas.cublasDasum_v2(handle,
                                       n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float64(result.value)

cublasDasum.__doc__ = \
                    _ASUM_doc.substitute(precision='double-precision',
                                         real='real',
                                         data='np.random.rand(5).astype(np.float64)',
                                         func='cublasDasum',
                                         ret_type='numpy.float64')

_libcublas.cublasScasum_v2.restype = int
_libcublas.cublasScasum_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasScasum(handle, n, x, incx):
    result = ctypes.c_float()
    status = _libcublas.cublasScasum_v2(handle,
                                        n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float32(result.value)
    
cublasScasum.__doc__ = \
                     _ASUM_doc.substitute(precision='single-precision',
                                          real='complex',
                                          data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                          func='cublasScasum',
                                          ret_type='numpy.float32')

_libcublas.cublasDzasum_v2.restype = int
_libcublas.cublasDzasum_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasDzasum(handle, n, x, incx):
    result = ctypes.c_double()
    status = _libcublas.cublasDzasum_v2(handle,
                                        n, int(x), incx, ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float64(result.value)

cublasDzasum.__doc__ = \
                     _ASUM_doc.substitute(precision='double-precision',
                                          real='complex',
                                          data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                          func='cublasDzasum',
                                          ret_type='numpy.float64')

# SAXPY, DAXPY, CAXPY, ZAXPY
_AXPY_doc = Template(
"""
    Vector addition (${precision} ${real}).

    Computes the sum of a ${precision} ${real} vector scaled by a
    ${precision} ${real} scalar and another ${precision} ${real} vector.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    alpha : ${type}
        Scalar.
    x : ctypes.c_void_p
        Pointer to single-precision input vector.
    incx : int
        Storage spacing between elements of `x`.
    y : ctypes.c_void_p
        Pointer to single-precision input/output vector.
    incy : int
        Storage spacing between elements of `y`.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> alpha = ${alpha} 
    >>> x = ${data}
    >>> y = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> y_gpu = gpuarray.to_gpu(y)
    >>> h = cublasCreate()
    >>> ${func}(h, x_gpu.size, alpha, x_gpu.gpudata, 1, y_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(y_gpu.get(), alpha*x+y)
    True

    Notes
    -----
    Both `x` and `y` must contain `n` elements.
    
    """
)

_libcublas.cublasSaxpy_v2.restype = int
_libcublas.cublasSaxpy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSaxpy(handle, n, alpha, x, incx, y, incy):
    status = _libcublas.cublasSaxpy_v2(handle,
                                       n, ctypes.byref(ctypes.c_float(alpha)),
                                       int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasSaxpy.__doc__ = \
                    _AXPY_doc.substitute(precision='single-precision',
                                         real='real',
                                         type='numpy.float32',
                                         alpha='np.float32(np.random.rand())',
                                         data='np.random.rand(5).astype(np.float32)',
                                         func='cublasSaxpy')

_libcublas.cublasDaxpy_v2.restype = int
_libcublas.cublasDaxpy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDaxpy(handle, n, alpha, x, incx, y, incy):
    status = _libcublas.cublasDaxpy_v2(handle,
                                       n, ctypes.byref(ctypes.c_double(alpha)),
                                       int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasDaxpy.__doc__ = \
                    _AXPY_doc.substitute(precision='double-precision',
                                         real='real',
                                         type='numpy.float64',
                                         alpha='np.float64(np.random.rand())',
                                         data='np.random.rand(5).astype(np.float64)',
                                         func='cublasDaxpy')

_libcublas.cublasCaxpy_v2.restype = int
_libcublas.cublasCaxpy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCaxpy(handle, n, alpha, x, incx, y, incy):
    status = _libcublas.cublasCaxpy_v2(handle, n,
                                       ctypes.byref(cuda.cuFloatComplex(alpha.real, alpha.imag)),
                                       int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasCaxpy.__doc__ = \
                    _AXPY_doc.substitute(precision='single-precision',
                                         real='complex',
                                         type='numpy.complex64',
                                         alpha='(np.random.rand()+1j*np.random.rand()).astype(np.complex64)',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',             
                                         func='cublasCaxpy')

_libcublas.cublasZaxpy_v2.restype = int
_libcublas.cublasZaxpy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZaxpy(handle, n, alpha, x, incx, y, incy):
    status = _libcublas.cublasZaxpy_v2(handle, n,
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real, alpha.imag)),
                                       int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasZaxpy.__doc__ = \
                    _AXPY_doc.substitute(precision='double-precision',
                                         real='complex',
                                         type='numpy.complex128',
                                         alpha='(np.random.rand()+1j*np.random.rand()).astype(np.complex128)',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',             
                                         func='cublasZaxpy')

# SCOPY, DCOPY, CCOPY, ZCOPY
_COPY_doc = Template(
"""
    Vector copy (${precision} ${real})

    Copies a ${precision} ${real} vector to another ${precision} ${real}
    vector.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input vector.
    incx : int
        Storage spacing between elements of `x`.
    y : ctypes.c_void_p
        Pointer to ${precision} ${real} output vector.
    incy : int
        Storage spacing between elements of `y`.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> y_gpu = gpuarray.zeros_like(x_gpu)
    >>> h = cublasCreate()
    >>> ${func}(h, x_gpu.size, x_gpu.gpudata, 1, y_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(y_gpu.get(), x_gpu.get())
    True
    
    Notes
    -----
    Both `x` and `y` must contain `n` elements.

""")

_libcublas.cublasScopy_v2.restype = int
_libcublas.cublasScopy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasScopy(handle, n, x, incx, y, incy):
    status = _libcublas.cublasScopy_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)
                
cublasScopy.__doc__ = \
                    _COPY_doc.substitute(precision='single-precision',
                                         real='real',
                                         data='np.random.rand(5).astype(np.float32)',
                                         func='cublasScopy')

_libcublas.cublasDcopy_v2.restype = int
_libcublas.cublasDcopy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDcopy(handle, n, x, incx, y, incy):
    status = _libcublas.cublasDcopy_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)
                
cublasDcopy.__doc__ = \
                    _COPY_doc.substitute(precision='double-precision',
                                         real='real',
                                         data='np.random.rand(5).astype(np.float64)',
                                         func='cublasDcopy')

_libcublas.cublasCcopy_v2.restype = int
_libcublas.cublasCcopy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCcopy(handle, n, x, incx, y, incy):
    status = _libcublas.cublasCcopy_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)
                
cublasCcopy.__doc__ = \
                    _COPY_doc.substitute(precision='single-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+np.random.rand(5).astype(np.complex64)',
                                         func='cublasCcopy')

_libcublas.cublasZcopy_v2.restype = int
_libcublas.cublasZcopy_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZcopy(handle, n, x, incx, y, incy):
    status = _libcublas.cublasZcopy_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)
                
cublasZcopy.__doc__ = \
                    _COPY_doc.substitute(precision='double-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+np.random.rand(5).astype(np.complex128)',
                                         func='cublasZcopy')

# SDOT, DDOT, CDOTU, CDOTC, ZDOTU, ZDOTC
_DOT_doc = Template(
"""
    Vector dot product (${precision} ${real})

    Computes the dot product of two ${precision} ${real} vectors.
    cublasCdotc and cublasZdotc use the conjugate of the first vector
    when computing the dot product.
    
    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input vector.
    incx : int
        Storage spacing between elements of `x`.
    y : ctypes.c_void_p
        Pointer to ${precision} ${real} input/output vector.
    incy : int
        Storage spacing between elements of `y`.

    Returns
    -------
    d : ${ret_type}
        Dot product of `x` and `y`.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data}
    >>> y = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> y_gpu = gpuarray.to_gpu(y)
    >>> h = cublasCreate()
    >>> d = ${func}(x_gpu.size, x_gpu.gpudata, 1, y_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> ${check} 
    True

    Notes
    -----
    Both `x` and `y` must contain `n` elements.
    
""")

_libcublas.cublasSdot_v2.restype = int
_libcublas.cublasSdot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p]
def cublasSdot(handle, n, x, incx, y, incy):
    result = ctypes.c_float()
    status = _libcublas.cublasSdot_v2(handle, n,
                                      int(x), incx, int(y), incy,
                                      ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float32(result.value)

cublasSdot.__doc__ = _DOT_doc.substitute(precision='single-precision',
                                         real='real',
                                         data='np.float32(np.random.rand(5))',
                                         ret_type='np.float32',
                                         func='cublasSdot',
                                         check='np.allclose(d, np.dot(x, y))')

_libcublas.cublasDdot_v2.restype = int
_libcublas.cublasDdot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p]
def cublasDdot(handle, n, x, incx, y, incy):
    result = ctypes.c_double()
    status = _libcublas.cublasDdot_v2(handle, n,
                                      int(x), incx, int(y), incy,
                                      ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float64(result.value)

cublasDdot.__doc__ = _DOT_doc.substitute(precision='double-precision',
                                         real='real',
                                         data='np.float64(np.random.rand(5))',
                                         ret_type='np.float64',
                                         func='cublasDdot',
                                         check='np.allclose(d, np.dot(x, y))')

_libcublas.cublasCdotu_v2.restype = int
_libcublas.cublasCdotu_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasCdotu(handle, n, x, incx, y, incy):
    result = cuda.cuFloatComplex()
    status = _libcublas.cublasCdotu_v2(handle, n,
                                       int(x), incx, int(y), incy,
                                       ctypes.byref(result))
    cublasCheckStatus(status)
    return np.complex64(result.value)

cublasCdotu.__doc__ = _DOT_doc.substitute(precision='single-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                         ret_type='np.complex64',
                                         func='cublasCdotu',
                                         check='np.allclose(d, np.dot(x, y))')

_libcublas.cublasCdotc_v2.restype = int
_libcublas.cublasCdotc_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasCdotc(handle, n, x, incx, y, incy):
    result = cuda.cuFloatComplex()
    status = _libcublas.cublasCdotc_v2(handle, n,
                                       int(x), incx, int(y), incy,
                                       ctypes.byref(result))
    cublasCheckStatus(status)
    return np.complex64(result.value)

cublasCdotc.__doc__ = _DOT_doc.substitute(precision='single-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                         ret_type='np.complex64',
                                         func='cublasCdotc',
                                         check='np.allclose(d, np.dot(np.conj(x), y))')

_libcublas.cublasZdotu_v2.restype = int
_libcublas.cublasZdotu_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasZdotu(handle, n, x, incx, y, incy):
    result = cuda.cuDoubleComplex()
    status = _libcublas.cublasZdotu_v2(handle, n,
                                       int(x), incx, int(y), incy,
                                       ctypes.byref(result))
    cublasCheckStatus(status)
    return np.complex128(result.value)

cublasZdotu.__doc__ = _DOT_doc.substitute(precision='double-precision',
                                          real='complex',
                                          data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                          ret_type='np.complex128',
                                          func='cublasZdotu',
                                          check='np.allclose(d, np.dot(x, y))')

_libcublas.cublasZdotc_v2.restype = int
_libcublas.cublasZdotc_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasZdotc(handle, n, x, incx, y, incy):
    result = cuda.cuDoubleComplex()
    status = _libcublas.cublasZdotc_v2(handle, n,
                                       int(x), incx, int(y), incy,
                                       ctypes.byref(result))
    cublasCheckStatus(status)
    return np.complex128(result.value)

cublasZdotc.__doc__ = _DOT_doc.substitute(precision='double-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                         ret_type='np.complex128',
                                         func='cublasZdotc',
                                         check='np.allclose(d, np.dot(np.conj(x), y))')

# SNRM2, DNRM2, SCNRM2, DZNRM2
_NRM2_doc = Template(
"""
    Euclidean norm (2-norm) of real vector.

    Computes the Euclidean norm of a ${precision} ${real} vector.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input vector.
    incx : int
        Storage spacing between elements of `x`.

    Returns
    -------
    nrm : ${ret_type}
        Euclidean norm of `x`.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> h = cublasCreate()
    >>> nrm = ${func}(h, x.size, x_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(nrm, np.linalg.norm(x))
    True
    
""")

_libcublas.cublasSnrm2_v2.restype = int
_libcublas.cublasSnrm2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasSnrm2(handle, n, x, incx):
    result = ctypes.c_float()
    status = _libcublas.cublasSnrm2_v2(handle,
                                       n, int(x), incx,
                                       ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float32(result.value)
    
cublasSnrm2.__doc__ = \
                    _NRM2_doc.substitute(precision='single-precision',
                                         real='real',
                                         data='np.float32(np.random.rand(5))',
                                         ret_type = 'numpy.float32',
                                         func='cublasSnrm2')

_libcublas.cublasDnrm2_v2.restype = int
_libcublas.cublasDnrm2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasDnrm2(handle, n, x, incx):
    result = ctypes.c_double()
    status = _libcublas.cublasDnrm2_v2(handle,
                                       n, int(x), incx,
                                       ctypes.byref(result))
    cublasCheckStatus(status)
    return np.float64(result.value)
    
cublasDnrm2.__doc__ = \
                    _NRM2_doc.substitute(precision='double-precision',
                                         real='real',
                                         data='np.float64(np.random.rand(5))',
                                         ret_type = 'numpy.float64',
                                         func='cublasDnrm2')

_libcublas.cublasScnrm2_v2.restype = int
_libcublas.cublasScnrm2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasScnrm2(handle, n, x, incx):
    result = cuda.cuFloatComplex()
    status = _libcublas.cublasScnrm2_v2(handle,
                                        n, int(x), incx,
                                        ctypes.byref(result))
    cublasCheckStatus(status)
    return np.complex64(result.value)
    
cublasScnrm2.__doc__ = \
                    _NRM2_doc.substitute(precision='single-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                         ret_type = 'numpy.complex64',
                                         func='cublasScnrm2')

_libcublas.cublasDznrm2_v2.restype = int
_libcublas.cublasDznrm2_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p]
def cublasDznrm2(handle, n, x, incx):
    result = cuda.cuDoubleComplex()
    status = _libcublas.cublasDznrm2_v2(handle,
                                        n, int(x), incx,
                                        ctypes.byref(result))
    cublasCheckStatus(status)
    return np.complex128(result.value)
    
cublasDznrm2.__doc__ = \
                    _NRM2_doc.substitute(precision='double-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                         ret_type = 'numpy.complex128',
                                         func='cublasDznrm2')


# SROT, DROT, CROT, CSROT, ZROT, ZDROT
_ROT_doc = Template(
"""
    Apply a ${real} rotation to ${real} vectors (${precision})

    Multiplies the ${precision} matrix `[[c, s], [-s, c]]`
    with the 2 x `n` ${precision} matrix `[[x.T], [y.T]]`.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input/output vector.
    incx : int
        Storage spacing between elements of `x`.
    y : ctypes.c_void_p
        Pointer to ${precision} ${real} input/output vector.
    incy : int
        Storage spacing between elements of `y`.
    c : ${c_type}
        Element of rotation matrix.
    s : ${s_type}
        Element of rotation matrix.

    Notes
    -----
    Both `x` and `y` must contain `n` elements.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> s = ${s_val}; c = ${c_val};
    >>> x = ${data}
    >>> y = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> y_gpu = gpuarray.to_gpu(y)
    >>> h = cublasCreate()
    >>> ${func}(h, x.size, x_gpu.gpudata, 1, y_gpu.gpudata, 1, c, s)
    >>> cublasDestroy(h)
    >>> np.allclose(x_gpu.get(), c*x+s*y)
    True
    >>> np.allclose(y_gpu.get(), -s*x+c*y)
    True
    
""")

_libcublas.cublasSrot_v2.restype = int
_libcublas.cublasSrot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p]
def cublasSrot(handle, n, x, incx, y, incy, c, s):
    status = _libcublas.cublasSrot_v2(handle,
                                      n, int(x), incx,
                                      int(y), incy,
                                      ctypes.byref(ctypes.c_float(c)),
                                      ctypes.byref(ctypes.c_float(s)))

    cublasCheckStatus(status)
        
cublasSrot.__doc__ = _ROT_doc.substitute(precision='single-precision',
                                         real='real',
                                         c_type='numpy.float32',
                                         s_type='numpy.float32',
                                         c_val='np.float32(np.random.rand())',
                                         s_val='np.float32(np.random.rand())',
                                         data='np.random.rand(5).astype(np.float32)',
                                         func='cublasSrot')

_libcublas.cublasDrot_v2.restype = int
_libcublas.cublasDrot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p]
def cublasDrot(handle, n, x, incx, y, incy, c, s):
    status = _libcublas.cublasDrot_v2(handle,
                                      n, int(x),
                                      incx, int(y), incy,
                                      ctypes.byref(ctypes.c_double(c)),
                                      ctypes.byref(ctypes.c_double(s)))
    cublasCheckStatus(status)
        
cublasDrot.__doc__ = _ROT_doc.substitute(precision='double-precision',
                                         real='real',
                                         c_type='numpy.float64',
                                         s_type='numpy.float64',
                                         c_val='np.float64(np.random.rand())',
                                         s_val='np.float64(np.random.rand())',
                                         data='np.random.rand(5).astype(np.float64)',
                                         func='cublasDrot')

_libcublas.cublasCrot_v2.restype = int
_libcublas.cublasCrot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p]
def cublasCrot(handle, n, x, incx, y, incy, c, s):
    status = _libcublas.cublasCrot_v2(handle,
                                      n, int(x),
                                      incx, int(y), incy,
                                      ctypes.byref(ctypes.c_float(c)),
                                      ctypes.byref(cuda.cuFloatComplex(s.real,
                                                                       s.imag)))
    cublasCheckStatus(status)
        
cublasCrot.__doc__ = _ROT_doc.substitute(precision='single-precision',
                                         real='complex',
                                         c_type='numpy.float32',
                                         s_type='numpy.complex64',
                                         c_val='np.float32(np.random.rand())',
                                         s_val='np.complex64(np.random.rand()+1j*np.random.rand())',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                         func='cublasCrot')

_libcublas.cublasCsrot_v2.restype = int
_libcublas.cublasCsrot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p]
def cublasCsrot(handle, n, x, incx, y, incy, c, s):
    status = _libcublas.cublasCsrot_v2(handle,
                                       n, int(x),
                                       incx, int(y), incy,
                                       c, s)
    cublasCheckStatus(status)
        
cublasCsrot.__doc__ = _ROT_doc.substitute(precision='single-precision',
                                          real='complex',
                                          c_type='numpy.float32',
                                          s_type='numpy.float32',
                                          c_val='np.float32(np.random.rand())',
                                          s_val='np.float32(np.random.rand())',
                                          data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                          func='cublasCsrot')

_libcublas.cublasZrot_v2.restype = int
_libcublas.cublasZrot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p]
def cublasZrot(handle, n, x, incx, y, incy, c, s):
    status = _libcublas.cublasZrot_v2(handle,
                                      n, int(x),
                                      incx, int(y), incy,
                                      c,
                                      ctypes.byref(cuda.cuDoubleComplex(s.real, s.imag)))
    cublasCheckStatus(status)
        
cublasZrot.__doc__ = _ROT_doc.substitute(precision='double-precision',
                                         real='complex',
                                         c_type='numpy.float64',
                                         s_type='numpy.complex128',
                                         c_val='np.float64(np.random.rand())',
                                         s_val='np.complex128(np.random.rand()+1j*np.random.rand())',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                         func='cublasZrot')

_libcublas.cublasZdrot_v2.restype = int
_libcublas.cublasZdrot_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p]
def cublasZdrot(handle, n, x, incx, y, incy, c, s):
    status = _libcublas.cublasZdrot_v2(handle,
                                       n, int(x),
                                       incx, int(y), incy,
                                       c, s)
    cublasCheckStatus(status)
        
cublasZdrot.__doc__ = _ROT_doc.substitute(precision='double-precision',
                                          real='complex',
                                          c_type='numpy.float64',
                                          s_type='numpy.float64',
                                          c_val='np.float64(np.random.rand())',
                                          s_val='np.float64(np.random.rand())',
                                          data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                          func='cublasZdrot')


# SROTG, DROTG, CROTG, ZROTG
_ROTG_doc = Template(
"""
    Construct a ${precision} ${real} Givens rotation matrix.

    Constructs the ${precision} ${real} Givens rotation matrix
    `G = [[c, s], [-s, c]]` such that
    `dot(G, [[a], [b]] == [[r], [0]]`, where
    `c**2+s**2 == 1` and `r == a**2+b**2` for real numbers and
    `c**2+(conj(s)*s) == 1` and `r ==
    (a/abs(a))*sqrt(abs(a)**2+abs(b)**2)` for `a != 0` and `r == b`
    for `a == 0`.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    a, b : ${type}
        Entries of vector whose second entry should be zeroed
        out by the rotation.

    Returns
    -------
    r : ${type}
        Defined above.
    c : ${c_type}
        Cosine component of rotation matrix.
    s : ${s_type}
        Sine component of rotation matrix.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> a = ${a_val}
    >>> b = ${b_val}
    >>> h = cublasCreate()
    >>> r, c, s = ${func}(h, a, b)
    >>> cublasDestroy(h)
    >>> np.allclose(np.dot(np.array([[c, s], [-np.conj(s), c]]), np.array([[a], [b]])), np.array([[r], [0.0]]))
    True

""")

_libcublas.cublasSrotg_v2.restype = int
_libcublas.cublasSrotg_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p]
def cublasSrotg(handle, a, b):
    _a = ctypes.c_float(a)
    _b = ctypes.c_float(b)
    _c = ctypes.c_float()
    _s = ctypes.c_float()
    status = _libcublas.cublasSrotg_v2(handle,
                                       ctypes.byref(_a), ctypes.byref(_b),
                                       ctypes.byref(_c), ctypes.byref(_s))
    cublasCheckStatus(status)
    return np.float32(_a.value), np.float32(_c.value), np.float32(_s.value)
                                  
cublasSrotg.__doc__ = \
                    _ROTG_doc.substitute(precision='single-precision',
                                         real='real',
                                         type='numpy.float32',
                                         c_type='numpy.float32',
                                         s_type='numpy.float32',
                                         a_val='np.float32(np.random.rand())',
                                         b_val='np.float32(np.random.rand())',
                                         func='cublasSrotg')

_libcublas.cublasDrotg_v2.restype = int
_libcublas.cublasDrotg_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p]
def cublasDrotg(handle, a, b):
    _a = ctypes.c_double(a)
    _b = ctypes.c_double(b)
    _c = ctypes.c_double()
    _s = ctypes.c_double()
    status = _libcublas.cublasDrotg_v2(handle,
                                       ctypes.byref(_a), ctypes.byref(_b),
                                       ctypes.byref(_c), ctypes.byref(_s))
    cublasCheckStatus(status)
    return np.float64(_a.value), np.float64(_c.value), np.float64(_s.value)
                                  
cublasDrotg.__doc__ = \
                    _ROTG_doc.substitute(precision='double-precision',
                                         real='real',
                                         type='numpy.float64',
                                         c_type='numpy.float64',
                                         s_type='numpy.float64',
                                         a_val='np.float64(np.random.rand())',
                                         b_val='np.float64(np.random.rand())',
                                         func='cublasDrotg')

_libcublas.cublasCrotg_v2.restype = int
_libcublas.cublasCrotg_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p]
def cublasCrotg(handle, a, b):
    _a = cuda.cuFloatComplex(a.real, a.imag)
    _b = cuda.cuFloatComplex(b.real, b.imag)
    _c = ctypes.c_float()
    _s = cuda.cuFloatComplex()
    status = _libcublas.cublasCrotg_v2(handle,
                                       ctypes.byref(_a), _b,
                                       ctypes.byref(_c), ctypes.byref(_s))
    cublasCheckStatus(status)
    return np.complex64(_a.value), np.float32(_c.value), np.complex64(_s.value)
                                  
cublasCrotg.__doc__ = \
                    _ROTG_doc.substitute(precision='single-precision',
                                         real='complex',
                                         type='numpy.complex64',
                                         c_type='numpy.float32',
                                         s_type='numpy.complex64',
                                         a_val='np.complex64(np.random.rand()+1j*np.random.rand())',
                                         b_val='np.complex64(np.random.rand()+1j*np.random.rand())',
                                         func='cublasCrotg')

_libcublas.cublasZrotg_v2.restype = int
_libcublas.cublasZrotg_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p]
def cublasZrotg(handle, a, b):
    _a = cuda.cuDoubleComplex(a.real, a.imag)
    _b = cuda.cuDoubleComplex(b.real, b.imag)
    _c = ctypes.c_double()
    _s = cuda.cuDoubleComplex()
    status = _libcublas.cublasZrotg_v2(handle,
                                       ctypes.byref(_a), _b,
                                       ctypes.byref(_c), ctypes.byref(_s))
    cublasCheckStatus(status)
    return np.complex128(_a.value), np.float64(_c.value), np.complex128(_s.value)
                                  
cublasZrotg.__doc__ = \
                    _ROTG_doc.substitute(precision='double-precision',
                                         real='complex',
                                         type='numpy.complex128',
                                         c_type='numpy.float64',
                                         s_type='numpy.complex128',
                                         a_val='np.complex128(np.random.rand()+1j*np.random.rand())',
                                         b_val='np.complex128(np.random.rand()+1j*np.random.rand())',
                                         func='cublasZrotg')

# SROTM, DROTM (need to add example)
_ROTM_doc = Template(        
"""
    Apply a ${precision} real modified Givens rotation.

    Applies the ${precision} real modified Givens rotation matrix `h`
    to the 2 x `n` matrix `[[x.T], [y.T]]`.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    x : ctypes.c_void_p
        Pointer to ${precision} real input/output vector.
    incx : int
        Storage spacing between elements of `x`.
    y : ctypes.c_void_p
        Pointer to ${precision} real input/output vector.
    incy : int
        Storage spacing between elements of `y`.
    sparam : numpy.ndarray
        sparam[0] contains the `flag` described below;
        sparam[1:5] contains the values `[h00, h10, h01, h11]`
        that determine the rotation matrix `h`.

    Notes
    -----
    The rotation matrix may assume the following values:

    for `flag` == -1.0, `h` == `[[h00, h01], [h10, h11]]`
    for `flag` == 0.0,  `h` == `[[1.0, h01], [h10, 1.0]]`
    for `flag` == 1.0,  `h` == `[[h00, 1.0], [-1.0, h11]]`
    for `flag` == -2.0, `h` == `[[1.0, 0.0], [0.0, 1.0]]`

    Both `x` and `y` must contain `n` elements.
    
""")

_libcublas.cublasSrotm_v2.restype = int
_libcublas.cublasSrotm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasSrotm(handle, n, x, incx, y, incy, sparam):
    status = _libcublas.cublasSrotm_v2(handle,
                                       n, int(x), incx, int(y),
                                       incy, int(sparam.ctypes.data))
    cublasCheckStatus(status)

cublasSrotm.__doc__ = \
                    _ROTM_doc.substitute(precision='single-precision')

_libcublas.cublasDrotm_v2.restype = int
_libcublas.cublasDrotm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasDrotm(handle, n, x, incx, y, incy, sparam):
    status = _libcublas.cublasDrotm_v2(handle,
                                       n, int(x), incx, int(y),
                                       incy, int(sparam.ctypes.data))
    cublasCheckStatus(status)

cublasDrotm.__doc__ = \
                    _ROTM_doc.substitute(precision='double-precision')
                                        
# SROTMG, DROTMG (need to add example)
_ROTMG_doc = Template( 
"""
    Construct a ${precision} real modified Givens rotation matrix.

    Constructs the ${precision} real modified Givens rotation matrix
    `h = [[h11, h12], [h21, h22]]` that zeros out the second entry of
    the vector `[[sqrt(d1)*x1], [sqrt(d2)*x2]]`.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    d1 : ${type}
        ${precision} real value.
    d2 : ${type}
        ${precision} real value.
    x1 : ${type}
        ${precision} real value.
    x2 : ${type}
        ${precision} real value.

    Returns
    -------
    sparam : numpy.ndarray
        sparam[0] contains the `flag` described below;
        sparam[1:5] contains the values `[h00, h10, h01, h11]`
        that determine the rotation matrix `h`.
        
    Notes
    -----
    The rotation matrix may assume the following values:

    for `flag` == -1.0, `h` == `[[h00, h01], [h10, h11]]`
    for `flag` == 0.0,  `h` == `[[1.0, h01], [h10, 1.0]]`
    for `flag` == 1.0,  `h` == `[[h00, 1.0], [-1.0, h11]]`
    for `flag` == -2.0, `h` == `[[1.0, 0.0], [0.0, 1.0]]`

""")

_libcublas.cublasSrotmg_v2.restype = int
_libcublas.cublasSrotmg_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p]
def cublasSrotmg(handle, d1, d2, x1, y1):
    _d1 = ctypes.c_float(d1)
    _d2 = ctypes.c_float(d2)
    _x1 = ctypes.c_float(x1)
    _y1 = ctypes.c_float(y1)
    sparam = np.empty(5, np.float32)

    status = _libcublas.cublasSrotmg_v2(handle,
                                        ctypes.byref(_d1), ctypes.byref(_d2),
                                        ctypes.byref(_x1), ctypes.byref(_y1),
                                        int(sparam.ctypes.data))
    cublasCheckStatus(status)        
    return sparam

cublasSrotmg.__doc__ = \
                     _ROTMG_doc.substitute(precision='single-precision',
                                           type='numpy.float32')

_libcublas.cublasDrotmg_v2.restype = int
_libcublas.cublasDrotmg_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p]
def cublasDrotmg(handle, d1, d2, x1, y1):
    _d1 = ctypes.c_double(d1)
    _d2 = ctypes.c_double(d2)
    _x1 = ctypes.c_double(x1)
    _y1 = ctypes.c_double(y1)
    sparam = np.empty(5, np.float64)

    status = _libcublas.cublasDrotmg_v2(handle,
                                        ctypes.byref(_d1), ctypes.byref(_d2),
                                        ctypes.byref(_x1), ctypes.byref(_y1),
                                        int(sparam.ctypes.data))
    cublasCheckStatus(status)        
    return sparam

cublasDrotmg.__doc__ = \
                     _ROTMG_doc.substitute(precision='double-precision',
                                           type='numpy.float64')

# SSCAL, DSCAL, CSCAL, CSCAL, CSSCAL, ZSCAL, ZDSCAL
_SCAL_doc = Template(
"""
    Scale a ${precision} ${real} vector by a ${precision} ${a_real} scalar.

    Replaces a ${precision} ${real} vector `x` with
    `alpha * x`.
    
    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    alpha : ${a_type}
        Scalar multiplier.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input/output vector.
    incx : int
        Storage spacing between elements of `x`.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> alpha = ${alpha}
    >>> h = cublasCreate()
    >>> ${func}(h, x.size, alpha, x_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(x_gpu.get(), alpha*x)
    True
    
""")

_libcublas.cublasSscal_v2.restype = int
_libcublas.cublasSscal_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSscal(handle, n, alpha, x, incx):
    status = _libcublas.cublasSscal_v2(handle, n,
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(x), incx)
    cublasCheckStatus(status)
        
cublasSscal.__doc__ = \
                    _SCAL_doc.substitute(precision='single-precision',
                                         real='real',
                                         a_real='real',
                                         a_type='numpy.float32',
                                         alpha='np.float32(np.random.rand())',
                                         data='np.random.rand(5).astype(np.float32)',
                                         func='cublasSscal')

_libcublas.cublasDscal_v2.restype = int
_libcublas.cublasDscal_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDscal(handle, n, alpha, x, incx):
    status = _libcublas.cublasDscal_v2(handle, n,
                                       ctypes.byref(ctypes.c_double(alpha)),
                                       int(x), incx)
    cublasCheckStatus(status)
        
cublasDscal.__doc__ = \
                    _SCAL_doc.substitute(precision='double-precision',
                                         real='real',
                                         a_real='real',
                                         a_type='numpy.float64',
                                         alpha='np.float64(np.random.rand())',
                                         data='np.random.rand(5).astype(np.float64)',
                                         func='cublasDscal')

_libcublas.cublasCscal_v2.restype = int
_libcublas.cublasCscal_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCscal(handle, n, alpha, x, incx):
    status = _libcublas.cublasCscal_v2(handle, n,
                                       ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)),
                                       int(x), incx)
    cublasCheckStatus(status)
        
cublasCscal.__doc__ = \
                    _SCAL_doc.substitute(precision='single-precision',
                                         real='complex',
                                         a_real='complex',
                                         a_type='numpy.complex64',
                                         alpha='np.complex64(np.random.rand()+1j*np.random.rand())',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                         func='cublasCscal')

_libcublas.cublasCsscal_v2.restype = int
_libcublas.cublasCsscal_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCsscal(handle, n, alpha, x, incx):
    status = _libcublas.cublasCsscal_v2(handle, n,
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(x), incx)
    cublasCheckStatus(status)
        
cublasCsscal.__doc__ = \
                    _SCAL_doc.substitute(precision='single-precision',
                                         real='complex',
                                         a_real='real',
                                         a_type='numpy.float32',
                                         alpha='np.float32(np.random.rand())',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                         func='cublasCsscal')

_libcublas.cublasZscal_v2.restype = int
_libcublas.cublasZscal_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZscal(handle, n, alpha, x, incx):
    status = _libcublas.cublasZscal_v2(handle, n,
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                         alpha.imag)),
                                       int(x), incx)
    cublasCheckStatus(status)
        
cublasZscal.__doc__ = \
                    _SCAL_doc.substitute(precision='double-precision',
                                         real='complex',
                                         a_real='complex',
                                         a_type='numpy.complex128',
                                         alpha='np.complex128(np.random.rand()+1j*np.random.rand())',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                         func='cublasZscal')

_libcublas.cublasZdscal_v2.restype = int
_libcublas.cublasZdscal_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZdscal(handle, n, alpha, x, incx):
    status = _libcublas.cublasZdscal_v2(handle, n,
                                       ctypes.byref(ctypes.c_double(alpha)),
                                       int(x), incx)
    cublasCheckStatus(status)
        
cublasZdscal.__doc__ = \
                    _SCAL_doc.substitute(precision='double-precision',
                                         real='complex',
                                         a_real='real',
                                         a_type='numpy.float64',
                                         alpha='np.float64(np.random.rand())',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                         func='cublasZdscal')

# SSWAP, DSWAP, CSWAP, ZSWAP
_SWAP_doc = Template(
"""
    Swap ${precision} ${real} vectors.

    Swaps the contents of one ${precision} ${real} vector with those
    of another ${precision} ${real} vector.

    Parameters
    ----------
    handle : int
        CUBLAS context.
    n : int
        Number of elements in input vectors.
    x : ctypes.c_void_p
        Pointer to ${precision} ${real} input/output vector.
    incx : int
        Storage spacing between elements of `x`.
    y : ctypes.c_void_p
        Pointer to ${precision} ${real} input/output vector.
    incy : int
        Storage spacing between elements of `y`.

    Examples
    --------
    >>> import pycuda.autoinit
    >>> import pycuda.gpuarray as gpuarray
    >>> import numpy as np
    >>> x = ${data}
    >>> y = ${data}
    >>> x_gpu = gpuarray.to_gpu(x)
    >>> y_gpu = gpuarray.to_gpu(y)
    >>> h = cublasCreate() 
    >>> ${func}(x.size, x_gpu.gpudata, 1, y_gpu.gpudata, 1)
    >>> cublasDestroy(h)
    >>> np.allclose(x_gpu.get(), y)
    True
    >>> np.allclose(y_gpu.get(), x)
    True

    Notes
    -----
    Both `x` and `y` must contain `n` elements.

""")

_libcublas.cublasSswap_v2.restype = int
_libcublas.cublasSswap_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSswap(handle, n, x, incx, y, incy):
    status = _libcublas.cublasSswap_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasSswap.__doc__ = \
                    _SWAP_doc.substitute(precision='single-precision',
                                         real='real',
                                         data='np.random.rand(5).astype(np.float32)',
                                         func='cublasSswap')

_libcublas.cublasDswap_v2.restype = int
_libcublas.cublasDswap_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]    
def cublasDswap(handle, n, x, incx, y, incy):
    status = _libcublas.cublasDswap_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasDswap.__doc__ = \
                    _SWAP_doc.substitute(precision='double-precision',
                                         real='real',
                                         data='np.random.rand(5).astype(np.float64)',
                                         func='cublasDswap')

_libcublas.cublasCswap_v2.restype = int
_libcublas.cublasCswap_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCswap(handle, n, x, incx, y, incy):
    status = _libcublas.cublasCswap_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasCswap.__doc__ = \
                    _SWAP_doc.substitute(precision='single-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex64)',
                                         func='cublasCswap')

_libcublas.cublasZswap_v2.restype = int
_libcublas.cublasZswap_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZswap(handle, n, x, incx, y, incy):
    status = _libcublas.cublasZswap_v2(handle,
                                       n, int(x), incx, int(y), incy)
    cublasCheckStatus(status)

cublasZswap.__doc__ = \
                    _SWAP_doc.substitute(precision='double-precision',
                                         real='complex',
                                         data='(np.random.rand(5)+1j*np.random.rand(5)).astype(np.complex128)',
                                         func='cublasZswap')

### BLAS Level 2 Functions ###

# SGBMV, DGVMV, CGBMV, ZGBMV 
_libcublas.cublasSgbmv_v2.restype = int
_libcublas.cublasSgbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_char,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSgbmv(handle, trans, m, n, kl, ku, alpha, A, lda,
                x, incx, beta, y, incy):
    """
    Matrix-vector product for real general banded matrix.

    """

    status = _libcublas.cublasSgbmv_v2(handle,
                                       trans, m, n, kl, ku,
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda,
                                       int(x), incx,
                                       ctypes.byref(ctypes.c_float(beta)),
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasDgbmv_v2.restype = int
_libcublas.cublasDgbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_char,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDgbmv(handle, trans, m, n, kl, ku, alpha, A, lda, 
                x, incx, beta, y, incy):
    """
    Matrix-vector product for real general banded matrix.

    """

    status = _libcublas.cublasDgbmv_v2(handle,
                                       trans, m, n, kl, ku,
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(ctypes.c_float(beta)),
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasCgbmv_v2.restype = int
_libcublas.cublasCgbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_char,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCgbmv(handle, trans, m, n, kl, ku, alpha, A, lda,
                x, incx, beta, y, incy):
    """
    Matrix-vector product for complex general banded matrix.

    """

    status = _libcublas.cublasCgbmv_v2(handle,
                                       trans, m, n, kl, ku,
                                       ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasZgbmv_v2.restype = int
_libcublas.cublasZgbmv_v2.argtypes = [ctypes.c_char,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZgbmv(handle, trans, m, n, kl, ku, alpha, A, lda, 
                x, incx, beta, y, incy):
    """
    Matrix-vector product for complex general banded matrix.

    """

    status = _libcublas.cublasZgbmv_v2(handle,
                                       trans, m, n, kl, ku,
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                         alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                         beta.imag)),
                              int(y), incy)
    cublasCheckStatus(status)
    
# SGEMV, DGEMV, CGEMV, ZGEMV # XXX need to adjust
# _GEMV_doc = Template( 
# """
#     Matrix-vector product for ${precision} ${real} general matrix.

#     Computes the product `alpha*op(A)*x+beta*y`, where `op(A)` == `A`
#     or `op(A)` == `A.T`, and stores it in `y`.

#     Parameters
#     ----------
#     trans : char
#         If `upper(trans)` in `['T', 'C']`, assume that `A` is
#         transposed.
#     m : int
#         Number of rows in `A`.
#     n : int
#         Number of columns in `A`.
#     alpha : ${a_type}
#         `A` is multiplied by this quantity. 
#     A : ctypes.c_void_p
#         Pointer to ${precision} matrix. The matrix has
#         shape `(lda, n)` if `upper(trans)` == 'N', `(lda, m)`
#         otherwise.
#     lda : int
#         Leading dimension of `A`.
#     X : ctypes.c_void_p
#         Pointer to ${precision} array of length at least
#         `(1+(n-1)*abs(incx))` if `upper(trans) == 'N',
#         `(1+(m+1)*abs(incx))` otherwise.
#     incx : int
#         Spacing between elements of `x`. Must be nonzero.
#     beta : ${a_type}
#         `y` is multiplied by this quantity. If zero, `y` is ignored.
#     y : ctypes.c_void_p
#         Pointer to ${precision} array of length at least
#         `(1+(m+1)*abs(incy))` if `upper(trans)` == `N`,
#         `(1+(n+1)*abs(incy))` otherwise.
#     incy : int
#         Spacing between elements of `y`. Must be nonzero.

#     Examples
#     --------
#     >>> import pycuda.autoinit
#     >>> import pycuda.gpuarray as gpuarray
#     >>> import numpy as np
#     >>> a = np.random.rand(2, 3).astype(np.float32)
#     >>> x = np.random.rand(3, 1).astype(np.float32)
#     >>> a_gpu = gpuarray.to_gpu(a.T.copy())
#     >>> x_gpu = gpuarray.to_gpu(x)
#     >>> y_gpu = gpuarray.empty((2, 1), np.float32)
#     >>> alpha = np.float32(1.0)
#     >>> beta = np.float32(0)
#     >>> h = cublasCreate()
#     >>> ${func}(h, 'n', 2, 3, alpha, a_gpu.gpudata, 2, x_gpu.gpudata, 1, beta, y_gpu.gpudata, 1)
#     >>> cublasDestroy(h)
#     >>> np.allclose(y_gpu.get(), np.dot(a, x))
#     True

# """
    
_libcublas.cublasSgemv_v2.restype = int
_libcublas.cublasSgemv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSgemv(handle, trans, m, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for real general matrix.

    """

    status = _libcublas.cublasSgemv_v2(handle,
                                       _CUBLAS_OP[trans], m, n,
                                       ctypes.byref(ctypes.c_float(alpha)), int(A), lda,
                                       int(x), incx,
                                       ctypes.byref(ctypes.c_float(beta)), int(y), incy) 
    cublasCheckStatus(status)
        
_libcublas.cublasDgemv_v2.restype = int
_libcublas.cublasDgemv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDgemv(handle, trans, m, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for real general matrix.

    """

    status = _libcublas.cublasDgemv_v2(handle,
                                       _CUBLAS_OP[trans], m, n,
                                       ctypes.byref(ctypes.c_double(alpha)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(ctypes.c_double(beta)),
                                       int(y), incy)
    cublasCheckStatus(status)
    
_libcublas.cublasCgemv_v2.restype = int
_libcublas.cublasCgemv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCgemv(handle, trans, m, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for complex general matrix.

    """

    status = _libcublas.cublasCgemv_v2(handle,
                                       _CUBLAS_OP[trans], m, n,
                                       ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)
    
_libcublas.cublasZgemv_v2.restype = int
_libcublas.cublasZgemv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,        
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZgemv(handle, trans, m, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for complex general matrix.

    """

    status = _libcublas.cublasZgemv_v2(handle,
                                       _CUBLAS_OP[trans], m, n,
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                         alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                         beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

# SGER, DGER, CGERU, CGERC, ZGERU, ZGERC
_libcublas.cublasSger_v2.restype = int
_libcublas.cublasSger_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int]
def cublasSger(handle, m, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-1 operation on real general matrix.

    """
    
    status = _libcublas.cublasSger_v2(handle,
                                      m, n,
                                      ctypes.byref(ctypes.c_float(alpha)),
                                      int(x), incx,
                                      int(y), incy, int(A), lda)
    cublasCheckStatus(status)

_libcublas.cublasDger_v2.restype = int
_libcublas.cublasDger_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int]
def cublasDger(handle, m, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-1 operation on real general matrix.

    """
    
    status = _libcublas.cublasDger_v2(handle,
                                      m, n,
                                      ctypes.byref(ctypes.c_double(alpha)),
                                      int(x), incx,
                                      int(y), incy, int(A), lda)
    cublasCheckStatus(status)
    
_libcublas.cublasCgerc_v2.restype = int
_libcublas.cublasCgerc_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCgerc(handle, m, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-1 operation on complex general matrix.

    """

    status = _libcublas.cublasCgerc_v2(handle,
                                       m, n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                            alpha.imag)),
                                       int(x), incx, int(y), incy, int(A), lda)
    cublasCheckStatus(status)
    
_libcublas.cublasCgeru_v2.restype = int
_libcublas.cublasCgeru_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCgeru(handle, m, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-1 operation on complex general matrix.

    """

    status = _libcublas.cublasCgeru_v2(handle,
                                       m, n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                              alpha.imag)),
                                       int(x), incx, int(y), incy, int(A), lda)
    cublasCheckStatus(status)
    
_libcublas.cublasZgerc_v2.restype = int
_libcublas.cublasZgerc_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZgerc(handle, m, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-1 operation on complex general matrix.

    """

    status = _libcublas.cublasZgerc_v2(handle,
                                       m, n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                               alpha.imag)),
                                       int(x), incx, int(y), incy, int(A), lda)
    cublasCheckStatus(status)

_libcublas.cublasZgeru_v2.restype = int
_libcublas.cublasZgeru_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZgeru(handle, m, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-1 operation on complex general matrix.

    """

    status = _libcublas.cublasZgeru_v2(handle,
                                       m, n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                               alpha.imag)),
                                       int(x), incx, int(y), incy, int(A), lda)
    cublasCheckStatus(status)

# SSBMV, DSBMV 
_libcublas.cublasSsbmv_v2.restype = int
_libcublas.cublasSsbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]

def cublasSsbmv(handle, uplo, n, k, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for real symmetric-banded matrix.

    """

    status = _libcublas.cublasSsbmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], n, k,
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(ctypes.c_float(beta)),
                                       int(y), incy)
    cublasCheckStatus(status)
        
_libcublas.cublasDsbmv_v2.restype = int
_libcublas.cublasDsbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDsbmv(handle, uplo, n, k, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for real symmetric-banded matrix.

    """

    status = _libcublas.cublasDsbmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], n, k,
                                       ctypes.byref(ctypes.c_double(alpha)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(ctypes.c_double(beta)),
                                       int(y), incy)
    cublasCheckStatus(status)
        
# SSPMV, DSPMV
_libcublas.cublasSspmv_v2.restype = int
_libcublas.cublasSspmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSspmv(handle, uplo, n, alpha, AP, x, incx, beta, y, incy):
    """
    Matrix-vector product for real symmetric-packed matrix.

    """

    status = _libcublas.cublasSspmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n,
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       ctypes.byref(ctypes.c_float(AP)),
                                       int(x),
                                       incx,
                                       ctypes.byref(ctypes.c_float(beta)),
                                       int(y),
                                       incy)
    cublasCheckStatus(status)
        
_libcublas.cublasDspmv_v2.restype = int
_libcublas.cublasDspmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDspmv(handle, uplo, n, alpha, AP, x, incx, beta, y, incy):
    """
    Matrix-vector product for real symmetric-packed matrix.

    """

    status = _libcublas.cublasDspmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n,
                                       ctypes.byref(ctypes.c_double(alpha)),
                                       ctypes.byref(ctypes.c_double(AP)),
                                       int(x),
                                       incx,
                                       ctypes.byref(ctypes.c_double(beta)),
                                       int(y),
                                       incy)
    cublasCheckStatus(status)

# SSPR, DSPR
_libcublas.cublasSspr_v2.restype = int
_libcublas.cublasSspr_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p]
def cublasSspr(handle, uplo, n, alpha, x, incx, AP):
    """
    Rank-1 operation on real symmetric-packed matrix.

    """
    
    status = _libcublas.cublasSspr_v2(handle, 
                                      _CUBLAS_FILL_MODE[uplo], n,                                       
                                      ctypes.byref(ctypes.c_float(alpha)), 
                                      int(x), incx, int(AP))                                      
    cublasCheckStatus(status)


_libcublas.cublasDspr_v2.restype = int
_libcublas.cublasDspr_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p]
def cublasDspr(handle, uplo, n, alpha, x, incx, AP):
    """
    Rank-1 operation on real symmetric-packed matrix.

    """

    status = _libcublas.cublasDspr_v2(handle, 
                                      _CUBLAS_FILL_MODE[uplo], n,                                       
                                      ctypes.byref(ctypes.c_double(alpha)), 
                                      int(x), incx, int(AP))                                           
    cublasCheckStatus(status)

# SSPR2, DSPR2
_libcublas.cublasSspr2_v2.restype = int
_libcublas.cublasSspr2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasSspr2(handle, uplo, n, alpha, x, incx, y, incy, AP):
    """
    Rank-2 operation on real symmetric-packed matrix.

    """

    status = _libcublas.cublasSspr2_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], n, 
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(x), incx, int(y), incy, int(AP))    
                                                                              
    cublasCheckStatus(status)

_libcublas.cublasDspr2_v2.restype = int
_libcublas.cublasDspr2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasDspr2(handle, uplo, n, alpha, x, incx, y, incy, AP):
    """
    Rank-2 operation on real symmetric-packed matrix.

    """

    status = _libcublas.cublasDspr2_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], n, 
                                       ctypes.byref(ctypes.c_double(alpha)), 
                                       int(x), incx, int(y), incy, int(AP))
    cublasCheckStatus(status)

# SSYMV, DSYMV, CSYMV, ZSYMV
_libcublas.cublasSsymv_v2.restype = int
_libcublas.cublasSsymv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSsymv(handle, uplo, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for real symmetric matrix.
    
    """
    
    status = _libcublas.cublasSsymv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], n, 
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(ctypes.c_float(beta)), 
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasDsymv_v2.restype = int
_libcublas.cublasDsymv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDsymv(handle, uplo, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for real symmetric matrix.
    
    """

    status = _libcublas.cublasDsymv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], n, 
                                       ctypes.byref(ctypes.c_double(alpha)), 
                                       int(A), lda, int(x), incx, 
                                       ctypes.byref(ctypes.c_double(beta)), 
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasCsymv_v2.restype = int
_libcublas.cublasCsymv_v2.argtypes = [ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int]
def cublasCsymv(handle, uplo, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for complex symmetric matrix.

    """

    status = _libcublas.cublasCsymv_v2(handle, 
                                        _CUBLAS_FILL_MODE[uplo], n, 
                                        ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)), 
                                        int(A), lda, int(x), incx, 
                                        ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)), 
                                        int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasZsymv_v2.restype = int
_libcublas.cublasZsymv_v2.argtypes = [ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int]
def cublasZsymv(handle, uplo, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for complex symmetric matrix.

    """

    status = _libcublas.cublasZsymv_v2(handle, 
                                        _CUBLAS_FILL_MODE[uplo], n, 
                                        ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                            alpha.imag)), 
                                        int(A), lda, int(x), incx, 
                                        ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                            beta.imag)), 
                                        int(y), incy)
    cublasCheckStatus(status)

# SSYR, DSYR, CSYR, ZSYR
_libcublas.cublasSsyr_v2.restype = int
_libcublas.cublasSsyr_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int]
def cublasSsyr(handle, uplo, n, alpha, x, incx, A, lda): 
    """
    Rank-1 operation on real symmetric matrix.

    """
   
    status = _libcublas.cublasSsyr_v2(handle,
                                      _CUBLAS_FILL_MODE[uplo], n, 
                                      ctypes.byref(ctypes.c_float(alpha)),
                                      int(x), incx, int(A), lda)
    cublasCheckStatus(status)

_libcublas.cublasDsyr_v2.restype = int
_libcublas.cublasDsyr_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int]
def cublasDsyr(handle, uplo, n, alpha, x, incx, A, lda):
    """
    Rank-1 operation on real symmetric matrix.

    """

    status = _libcublas.cublasDsyr_v2(handle,
                                      _CUBLAS_FILL_MODE[uplo], n, 
                                      ctypes.byref(ctypes.c_double(alpha)), 
                                      int(x), incx, int(A), lda)
    cublasCheckStatus(status)

_libcublas.cublasCsyr_v2.restype = int
_libcublas.cublasCsyr_v2.argtypes = [ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int]
def cublasCsyr(handle, uplo, n, alpha, x, incx, A, lda):
    """
    Rank-1 operation on complex symmetric matrix.

    """

    status = _libcublas.cublasCsyr_v2(handle,
                                        _CUBLAS_FILL_MODE[uplo], n, 
                                        ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)),
                                        int(x), incx, int(A), lda)
    cublasCheckStatus(status)

_libcublas.cublasZsyr_v2.restype = int
_libcublas.cublasZsyr_v2.argtypes = [ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int]
def cublasZsyr(handle, uplo, n, alpha, x, incx, A, lda):
    """
    Rank-1 operation on complex symmetric matrix.

    """

    status = _libcublas.cublasZsyr_v2(handle,
                                        _CUBLAS_FILL_MODE[uplo], n, 
                                        ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                        alpha.imag)),
                                        int(x), incx, int(A), lda)
    cublasCheckStatus(status)

# SSYR2, DSYR2, CSYR2, ZSYR2
_libcublas.cublasSsyr2_v2.restype = int
_libcublas.cublasSsyr2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSsyr2(handle, uplo, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-2 operation on real symmetric matrix.

    """

    status = _libcublas.cublasSsyr2_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], n, 
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(x), incx, int(y), incy,
                                       int(A), lda)
    cublasCheckStatus(status)

_libcublas.cublasDsyr2_v2.restype = int
_libcublas.cublasDsyr2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                   
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDsyr2(handle, uplo, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-2 operation on real symmetric matrix.

    """

    status = _libcublas.cublasDsyr2_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], n, 
                                       ctypes.byref(ctypes.c_double(alpha)), 
                                       int(x), incx, int(y), incy, 
                                       int(A), lda)                                       
    cublasCheckStatus(status)

_libcublas.cublasCsyr2_v2.restype = int
_libcublas.cublasCsyr2_v2.argtypes = [ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_int,                                   
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int]
def cublasCsyr2(handle, uplo, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-2 operation on complex symmetric matrix.

    """

    status = _libcublas.cublasCsyr2_v2(handle, 
                                        _CUBLAS_FILL_MODE[uplo], n, 
                                        ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)), 
                                        int(x), incx, int(y), incy, 
                                        int(A), lda)                                       
    cublasCheckStatus(status)
 
_libcublas.cublasZsyr2_v2.restype = int
_libcublas.cublasZsyr2_v2.argtypes = [ctypes.c_int,
                                        ctypes.c_int,
                                        ctypes.c_int,                                   
                                        ctypes.c_void_p,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int,
                                        ctypes.c_void_p,
                                        ctypes.c_int]
def cublasZsyr2(handle, uplo, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-2 operation on complex symmetric matrix.

    """

    status = _libcublas.cublasZsyr2_v2(handle, 
                                        _CUBLAS_FILL_MODE[uplo], n, 
                                        ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                            alpha.imag)), 
                                        int(x), incx, int(y), incy, 
                                        int(A), lda)                                       
    cublasCheckStatus(status)

# STBMV, DTBMV, CTBMV, ZTBMV
_libcublas.cublasStbmv_v2.restype = int
_libcublas.cublasStbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasStbmv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Matrix-vector product for real triangular-banded matrix.

    """
    
    status = _libcublas.cublasStbmv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasDtbmv_v2.restype = int
_libcublas.cublasDtbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtbmv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Matrix-vector product for real triangular-banded matrix.

    """

    status = _libcublas.cublasDtbmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)                                       
    cublasCheckStatus(status)

_libcublas.cublasCtbmv_v2.restype = int
_libcublas.cublasCtbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtbmv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Matrix-vector product for complex triangular-banded matrix.

    """
    
    status = _libcublas.cublasCtbmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)                           
    cublasCheckStatus(status)

_libcublas.cublasZtbmv_v2.restype = int
_libcublas.cublasZtbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtbmv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Matrix-vector product for complex triangular-banded matrix.

    """
    
    status = _libcublas.cublasZtbmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

# STBSV, DTBSV, CTBSV, ZTBSV
_libcublas.cublasStbsv_v2.restype = int
_libcublas.cublasStbsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasStbsv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Solve real triangular-banded system with one right-hand side.

    """
    
    status = _libcublas.cublasStbsv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)                                       
    cublasCheckStatus(status)

_libcublas.cublasDtbsv_v2.restype = int
_libcublas.cublasDtbsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtbsv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Solve real triangular-banded system with one right-hand side.

    """

    status = _libcublas.cublasDtbsv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)                           
    cublasCheckStatus(status)

_libcublas.cublasCtbsv_v2.restype = int
_libcublas.cublasCtbsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtbsv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Solve complex triangular-banded system with one right-hand side.

    """
    
    status = _libcublas.cublasCtbsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)                                       
    cublasCheckStatus(status)

_libcublas.cublasZtbsv_v2.restype = int
_libcublas.cublasZtbsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtbsv(handle, uplo, trans, diag, n, k, A, lda, x, incx):
    """
    Solve complex triangular-banded system with one right-hand side.

    """
    
    status = _libcublas.cublasZtbsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, k, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

# STPMV, DTPMV, CTPMV, ZTPMV
_libcublas.cublasStpmv_v2.restype = int
_libcublas.cublasStpmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasStpmv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Matrix-vector product for real triangular-packed matrix.

    """
    
    status = _libcublas.cublasStpmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasCtpmv_v2.restype = int
_libcublas.cublasCtpmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtpmv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Matrix-vector product for complex triangular-packed matrix.

    """
    
    status = _libcublas.cublasCtpmv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasDtpmv_v2.restype = int
_libcublas.cublasDtpmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtpmv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Matrix-vector product for real triangular-packed matrix.

    """

    status = _libcublas.cublasDtpmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasZtpmv_v2.restype = int
_libcublas.cublasZtpmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtpmv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Matrix-vector product for complex triangular-packed matrix.

    """
    
    status = _libcublas.cublasZtpmv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)

# STPSV, DTPSV, CTPSV, ZTPSV
_libcublas.cublasStpsv_v2.restype = int
_libcublas.cublasStpsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasStpsv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Solve real triangular-packed system with one right-hand side.

    """
    
    status = _libcublas.cublasStpsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)


_libcublas.cublasDtpsv_v2.restype = int
_libcublas.cublasDtpsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtpsv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Solve real triangular-packed system with one right-hand side.

    """

    status = _libcublas.cublasDtpsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasCtpsv_v2.restype = int
_libcublas.cublasCtpsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtpsv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Solve complex triangular-packed system with one right-hand side.
    
    """
    
    status = _libcublas.cublasCtpsv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasZtpsv_v2.restype = int
_libcublas.cublasZtpsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtpsv(handle, uplo, trans, diag, n, AP, x, incx):
    """
    Solve complex triangular-packed system with one right-hand size.

    """
    
    status = _libcublas.cublasZtpsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(AP), int(x), incx)
    cublasCheckStatus(status)

# STRMV, DTRMV, CTRMV, ZTRMV
_libcublas.cublasStrmv_v2.restype = int
_libcublas.cublasStrmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasStrmv(handle, uplo, trans, diag, n, A, lda, x, inx):
    """
    Matrix-vector product for real triangular matrix.

    """
    
    status = _libcublas.cublasStrmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), inx)                                       
    cublasCheckStatus(status)

_libcublas.cublasCtrmv_v2.restype = int
_libcublas.cublasCtrmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtrmv(handle, uplo, trans, diag, n, A, lda, x, incx):
    """
    Matrix-vector product for complex triangular matrix.

    """
    
    status = _libcublas.cublasCtrmv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasDtrmv_v2.restype = int
_libcublas.cublasDtrmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtrmv(handle, uplo, trans, diag, n, A, lda, x, inx):
    """
    Matrix-vector product for real triangular matrix.

    """

    status = _libcublas.cublasDtrmv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), inx)
    cublasCheckStatus(status)

_libcublas.cublasZtrmv_v2.restype = int
_libcublas.cublasZtrmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtrmv(handle, uplo, trans, diag, n, A, lda, x, incx):
    """
    Matrix-vector product for complex triangular matrix.

    """
    
    status = _libcublas.cublasZtrmv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

# STRSV, DTRSV, CTRSV, ZTRSV
_libcublas.cublasStrsv_v2.restype = int
_libcublas.cublasStrsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasStrsv(handle, uplo, trans, diag, n, A, lda, x, incx):
    """
    Solve real triangular system with one right-hand side.

    """
    
    status = _libcublas.cublasStrsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), incx)                                       
    cublasCheckStatus(status)

_libcublas.cublasDtrsv_v2.restype = int
_libcublas.cublasDtrsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtrsv(handle, uplo, trans, diag, n, A, lda, x, incx):
    """
    Solve real triangular system with one right-hand side.

    """

    status = _libcublas.cublasDtrsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasCtrsv_v2.restype = int
_libcublas.cublasCtrsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtrsv(handle, uplo, trans, diag, n, A, lda, x, incx):
    """
    Solve complex triangular system with one right-hand side.

    """
    
    status = _libcublas.cublasCtrsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

_libcublas.cublasZtrsv_v2.restype = int
_libcublas.cublasZtrsv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtrsv(handle, uplo, trans, diag, n, A, lda, x, incx):
    """
    Solve complex triangular system with one right-hand side.

    """
    
    status = _libcublas.cublasZtrsv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       n, int(A), lda, int(x), incx)
    cublasCheckStatus(status)

# CHEMV, ZHEMV
_libcublas.cublasChemv_v2.restype = int
_libcublas.cublasChemv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasChemv(handle, uplo, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix vector product for Hermitian matrix.
    
    """
    
    status = _libcublas.cublasChemv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                           alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasZhemv_v2.restype = int
_libcublas.cublasZhemv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                       
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZhemv(handle, uplo, n, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for Hermitian matrix.

    """
    
    status = _libcublas.cublasZhemv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                            alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real, 
                                                                         beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

# CHBMV, ZHBMV
_libcublas.cublasChbmv_v2.restype = int
_libcublas.cublasChbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasChbmv(handle, uplo, n, k, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for Hermitian-banded matrix.

    """
    
    status = _libcublas.cublasChbmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, k,
                                       ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasZhbmv_v2.restype = int
_libcublas.cublasZhbmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZhbmv(handle, uplo, n, k, alpha, A, lda, x, incx, beta, y, incy):
    """
    Matrix-vector product for Hermitian banded matrix.

    """
    
    status = _libcublas.cublasZhbmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, k,
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                         alpha.imag)),
                                       int(A), lda, int(x), incx,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real, 
                                                                         beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

# CHPMV, ZHPMV
_libcublas.cublasChpmv_v2.restype = int
_libcublas.cublasChpmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasChpmv(handle, uplo, n, alpha, AP, x, incx, beta, y, incy):
    """
    Matrix-vector product for Hermitian-packed matrix.

    """
    
    status = _libcublas.cublasChpmv_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                           alpha.imag)),
                                       int(AP), int(x), incx,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

_libcublas.cublasZhpmv_v2.restype = int
_libcublas.cublasZhpmv_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZhpmv(handle, uplo, n, alpha, AP, x, incx, beta, y, incy):
    """
    Matrix-vector product for Hermitian-packed matrix.

    """
    
    status = _libcublas.cublasZhpmv_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                            alpha.imag)),
                                       int(AP), int(x), incx,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real, 
                                                                         beta.imag)),
                                       int(y), incy)
    cublasCheckStatus(status)

# CHER, ZHER
_libcublas.cublasCher_v2.restype = int
_libcublas.cublasCher_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int]
def cublasCher(handle, uplo, n, alpha, x, incx, A, lda):
    """
    Rank-1 operation on Hermitian matrix.

    """

    status = _libcublas.cublasCher_v2(handle, 
                                      _CUBLAS_FILL_MODE[uplo], 
                                      n, alpha, int(x), incx, int(A), lda)
    cublasCheckStatus(status)

_libcublas.cublasZher_v2.restype = int
_libcublas.cublasZher_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,                                     
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p,
                                     ctypes.c_int]
def cublasZher(handle, uplo, n, alpha, x, incx, A, lda):
    """
    Rank-1 operation on Hermitian matrix.

    """
    
    status = _libcublas.cublasZher_v2(handle, 
                                      _CUBLAS_FILL_MODE[uplo], 
                                      n, alpha, int(x), incx, int(A), lda)
    cublasCheckStatus(status)


# CHER2, ZHER2
_libcublas.cublasCher2_v2.restype = int
_libcublas.cublasCher2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCher2(handle, uplo, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-2 operation on Hermitian matrix.


    """
    
    status = _libcublas.cublasCher2_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                           alpha.imag)),
                                       int(x), incx, int(y), incy, int(A), lda)                           
    cublasCheckStatus(status)

_libcublas.cublasZher2_v2.restype = int
_libcublas.cublasZher2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZher2(handle, uplo, n, alpha, x, incx, y, incy, A, lda):
    """
    Rank-2 operation on Hermitian matrix.

    """
    
    status = _libcublas.cublasZher2_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                            alpha.imag)),
                                       int(x), incx, int(y), incy, int(A), lda)
    cublasCheckStatus(status)

# CHPR, ZHPR
_libcublas.cublasChpr_v2.restype = int
_libcublas.cublasChpr_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,                                     
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p]
def cublasChpr(handle, uplo, n, alpha, x, incx, AP):
    """
    Rank-1 operation on Hermitian-packed matrix.
    
    """
    
    status = _libcublas.cublasChpr_v2(handle, 
                                      _CUBLAS_FILL_MODE[uplo], 
                                      n, ctypes.byref(ctypes.c_float(alpha)),
                                      int(x), incx, int(AP))
    cublasCheckStatus(status)

_libcublas.cublasZhpr_v2.restype = int
_libcublas.cublasZhpr_v2.argtypes = [ctypes.c_int,
                                     ctypes.c_int,
                                     ctypes.c_int,                                     
                                     ctypes.c_void_p,
                                     ctypes.c_void_p,
                                     ctypes.c_int,
                                     ctypes.c_void_p]
def cublasZhpr(handle, uplo, n, alpha, x, incx, AP):
    """
    Rank-1 operation on Hermitian-packed matrix.

    """
    
    status = _libcublas.cublasZhpr_v2(handle,
                                      _CUBLAS_FILL_MODE[uplo], 
                                      n, ctypes.byref(ctypes.c_double(alpha)),
                                      int(x), incx, int(AP))
    cublasCheckStatus(status)

# CHPR2, ZHPR2
_libcublas.cublasChpr2.restype = int
_libcublas.cublasChpr2.argtypes = [ctypes.c_int,
                                   ctypes.c_int,
                                   ctypes.c_int,                                   
                                   ctypes.c_void_p,
                                   ctypes.c_void_p,
                                   ctypes.c_int,
                                   ctypes.c_void_p,
                                   ctypes.c_int,
                                   ctypes.c_void_p]
def cublasChpr2(handle, uplo, n, alpha, x, inx, y, incy, AP):
    """
    Rank-2 operation on Hermitian-packed matrix.
    
    """

    status = _libcublas.cublasChpr2_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                           alpha.imag)),
                                       int(x), incx, int(y), incy, int(AP))
    cublasCheckStatus(status)

_libcublas.cublasZhpr2_v2.restype = int
_libcublas.cublasZhpr2_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p]
def cublasZhpr2(handle, uplo, n, alpha, x, inx, y, incy, AP):
    """
    Rank-2 operation on Hermitian-packed matrix.

    """
    
    status = _libcublas.cublasZhpr2_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,  
                                                                            alpha.imag)),
                                       int(x), incx, int(y), incy, int(AP))
    cublasCheckStatus(status)

# SGEMM, CGEMM, DGEMM, ZGEMM
_libcublas.cublasSgemm_v2.restype = int
_libcublas.cublasSgemm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSgemm(handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for real general matrix.

    """

    status = _libcublas.cublasSgemm_v2(handle,
                                       _CUBLAS_OP[transa],
                                       _CUBLAS_OP[transb], m, n, k, 
                                       ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, int(B), ldb,
                                       ctypes.byref(ctypes.c_float(beta)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasCgemm_v2.restype = int
_libcublas.cublasCgemm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCgemm(handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for complex general matrix.

    """

    status = _libcublas.cublasCgemm_v2(handle,
                                       _CUBLAS_OP[transa],
                                       _CUBLAS_OP[transb], m, n, k, 
                                       ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)),
                                       int(A), lda, int(B), ldb,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasDgemm_v2.restype = int
_libcublas.cublasDgemm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDgemm(handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for real general matrix.

    """

    status = _libcublas.cublasDgemm_v2(handle,
                                       _CUBLAS_OP[transa],
                                       _CUBLAS_OP[transb], m, n, k, 
                                       ctypes.byref(ctypes.c_double(alpha)),
                                       int(A), lda, int(B), ldb,
                                       ctypes.byref(ctypes.c_double(beta)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasZgemm_v2.restype = int
_libcublas.cublasZgemm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZgemm(handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for complex general matrix.

    """

    status = _libcublas.cublasZgemm_v2(handle,
                                       _CUBLAS_OP[transa],
                                       _CUBLAS_OP[transb], m, n, k, 
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                         alpha.imag)),
                                       int(A), lda, int(B), ldb,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                         beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)
    
# SSYMM, DSYMM, CSYMM, ZSYMM
_libcublas.cublasSsymm_v2.restype = int
_libcublas.cublasSsymm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSsymm(handle, side, uplo, m, n, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for symmetric matrix.

    """
    
    status = _libcublas.cublasSsymm_v2(handle,
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       m, n, ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, int(B), ldb, 
                                       ctypes.byref(ctypes.c_float(beta)), 
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasDsymm_v2.restype = int
_libcublas.cublasDsymm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]

def cublasDsymm(handle, side, uplo, m, n, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for real symmetric matrix.

    """
    
    status = _libcublas.cublasDsymm_v2(handle,
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo],
                                       m, n, ctypes.byref(ctypes.c_double(alpha)),
                                       int(A), lda, int(B), ldb, 
                                       ctypes.byref(ctypes.c_double(beta)), 
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasCsymm_v2.restype = int
_libcublas.cublasCsymm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCsymm(handle, side, uplo, m, n, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for complex symmetric matrix.

    """
    
    status = _libcublas.cublasCsymm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       m, n, ctypes.byref(cuda.cuFloatComplex(alpha.real,                   
                                                                              alpha.imag)),
                                       int(A), lda, int(B), ldb,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real, 
                                                                        beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasZsymm_v2.restype = int
_libcublas.cublasZsymm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZsymm(handle, side, uplo, m, n, alpha, A, lda, B, ldb, beta, C, ldc):                
    """
    Matrix-matrix product for complex symmetric matrix.

    """
    
    status = _libcublas.cublasZsymm_v2(handle,
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], m, n,
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                         alpha.imag)),
                                       int(A), lda, int(B), ldb,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                         beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

# SSYRK, DSYRK, CSYRK, ZSYRK
_libcublas.cublasSsyrk_v2.restype = int
_libcublas.cublasSsyrk_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasSsyrk(handle, uplo, trans, n, k, alpha, A, lda, beta, C, ldc):
    """
    Rank-k operation on real symmetric matrix.

    """
    
    status = _libcublas.cublasSsyrk_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       n, k, ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, 
                                       ctypes.byref(ctypes.c_float(beta)), 
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasDsyrk_v2.restype = int
_libcublas.cublasDsyrk_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDsyrk(handle, uplo, trans, n, k, alpha, A, lda, beta, C, ldc):
    """
    Rank-k operation on real symmetric matrix.

    """
    
    status = _libcublas.cublasDsyrk_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       n, k, ctypes.byref(cuda.cuFloatComplex(alpha.real,     
                                                                              alpha.imag)),
                                       int(A), lda, 
                                       ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                        beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasCsyrk_v2.restype = int
_libcublas.cublasCsyrk_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCsyrk(handle, uplo, trans, n, k, alpha, A, lda, beta, C, ldc):
    """
    Rank-k operation on complex symmetric matrix.

    """
    
    status = _libcublas.cublasCsyrk_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       n, k, ctypes.byref(cuda.cuFloatComplex(alpha.real,       
                                                                              alpha.imag)),
                                       int(A), lda,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real, 
                                                                        beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasZsyrk_v2.restype = int
_libcublas.cublasZsyrk_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZsyrk(handle, uplo, trans, n, k, alpha, A, lda, beta, C, ldc):
    """
    Rank-k operation on complex symmetric matrix.

    """
    
    status = _libcublas.cublasZsyrk_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       n, k, ctypes.byref(cuda.cuDoubleComplex(alpha.real,    
                                                                               alpha.imag)),
                                       int(A), lda,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                         beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

# SSYR2K, DSYR2K, CSYR2K, ZSYR2K
_libcublas.cublasSsyr2k_v2.restype = int
_libcublas.cublasSsyr2k_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,                                       
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int]
def cublasSsyr2k(handle, uplo, trans, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Rank-2k operation on real symmetric matrix.

    """
    
    status = _libcublas.cublasSsyr2k_v2(handle,
                                        _CUBLAS_FILL_MODE[uplo], 
                                        _CUBLAS_OP[trans], 
                                        n, k, ctypes.byref(ctypes.c_float(alpha)),
                                        int(A), lda, int(B), ldb, 
                                        ctypes.byref(ctypes.c_float(beta)), 
                                        int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasDsyr2k_v2.restype = int
_libcublas.cublasDsyr2k_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,                                       
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int]
def cublasDsyr2k(handle, uplo, trans, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Rank-2k operation on real symmetric matrix.

    """

    status = _libcublas.cublasDsyr2k_v2(handle, 
                                        _CUBLAS_FILL_MODE[uplo], 
                                        _CUBLAS_OP[trans], 
                                        n, k, ctypes.byref(ctypes.c_double(alpha)),
                                        int(A), lda, int(B), ldb, 
                                        ctypes.byref(ctypes.c_double(beta)), 
                                        int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasCsyr2k_v2.restype = int
_libcublas.cublasCsyr2k_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,                                       
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int]
def cublasCsyr2k(handle, uplo, trans, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Rank-2k operation on complex symmetric matrix.

    """

    status = _libcublas.cublasCsyr2k_v2(handle,
                                        _CUBLAS_FILL_MODE[uplo], 
                                        _CUBLAS_OP[trans], 
                                        n, k, ctypes.byref(cuda.cuFloatComplex(alpha.real,                
                                                                               alpha.imag)),
                                        int(A), lda, int(B), ldb,
                                        ctypes.byref(cuda.cuFloatComplex(beta.real, 
                                                                         beta.imag)),
                                        int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasZsyr2k_v2.restype = int
_libcublas.cublasZsyr2k_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,                                       
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int]
def cublasZsyr2k(handle, uplo, trans, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Rank-2k operation on complex symmetric matrix.
    
    """
    
    status = _libcublas.cublasZsyr2k_v2(handle,
                                        _CUBLAS_FILL_MODE[uplo], 
                                        _CUBLAS_OP[trans], 
                                        n, k, ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                                alpha.imag)),
                                        int(A), lda, int(B), ldb,
                                        ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                          beta.imag)),
                                        int(C), ldc)
    cublasCheckStatus(status)

# STRMM, DTRMM, CTRMM, ZTRMM
_libcublas.cublasStrmm_v2.restype = int
_libcublas.cublasStrmm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,                                      
                                      ctypes.c_int]
def cublasStrmm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb, C, ldc):
    """
    Matrix-matrix product for real triangular matrix.

    """
    
    status = _libcublas.cublasStrmm_v2(handle,
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, int(B), ldb, int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasDtrmm_v2.restype = int
_libcublas.cublasDtrmm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                       
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtrmm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb, C, ldc):                
    """
    Matrix-matrix product for real triangular matrix.

    """
    
    status = _libcublas.cublasDtrmm_v2(handle,
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(ctypes.c_double(alpha)),
                                       int(A), lda, int(B), ldb, int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasCtrmm_v2.restype = int
_libcublas.cublasCtrmm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                       
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtrmm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb, C, ldc):    
    """
    Matrix-matrix product for complex triangular matrix.

    """
    
    status = _libcublas.cublasCtrmm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                              alpha.imag)),
                                       int(A), lda, int(B), ldb)
    cublasCheckStatus(status)

_libcublas.cublasZtrmm_v2.restype = int
_libcublas.cublasZtrmm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtrmm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb, C, ldc):
    """
    Matrix-matrix product for complex triangular matrix.

    """
    
    status = _libcublas.cublasZtrmm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,     
                                                                               alpha.imag)),
                                       int(A), lda, int(B), ldb, int(C), ldc)
    cublasCheckStatus(status)

# STRSM, DTRSM, CTRSM, ZTRSM
_libcublas.cublasStrsm_v2.restype = int
_libcublas.cublasStrsm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasStrsm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb):
    """
    Solve a real triangular system with multiple right-hand sides.

    """
    
    status = _libcublas.cublasStrsm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, int(B), ldb)
    cublasCheckStatus(status)

_libcublas.cublasDtrsm_v2.restype = int
_libcublas.cublasDtrsm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasDtrsm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb):
    """
    Solve a real triangular system with multiple right-hand sides.

    """
    
    status = _libcublas.cublasDtrsm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(ctypes.c_double(alpha)),
                                       int(A), lda, int(B), ldb)
    cublasCheckStatus(status)

_libcublas.cublasCtrsm_v2.restype = int
_libcublas.cublasCtrsm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCtrsm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb):
    """
    Solve a complex triangular system with multiple right-hand sides.

    """
    
    status = _libcublas.cublasCtrsm_v2(handle,
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                              alpha.imag)),
                                       int(A), lda, int(B), ldb)
    cublasCheckStatus(status)

_libcublas.cublasZtrsm_v2.restype = int
_libcublas.cublasZtrsm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZtrsm(handle, side, uplo, transa, diag, m, n, alpha, A, lda, B, ldb):
    """
    Solve complex triangular system with multiple right-hand sides.

    """
    
    status = _libcublas.cublasZtrsm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       _CUBLAS_DIAG[diag], 
                                       m, n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,                    
                                                                               alpha.imag)),
                                       int(A), lda, int(B), ldb)
    cublasCheckStatus(status)

# CHEMM, ZHEMM
_libcublas.cublasChemm_v2.restype = int
_libcublas.cublasChemm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasChemm(handle, side, uplo, m, n, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Matrix-matrix product for complex Hermitian matrix.

    """
    
    status = _libcublas.cublasChemm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], m, n,
                                       ctypes.byref(cuda.cuFloatComplex(alpha.real,
                                                                        alpha.imag)),
                                       int(A), lda, int(B), ldb,
                                       ctypes.byref(cuda.cuFloatComplex(beta.real, 
                                                                        beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasZhemm_v2.restype = int
_libcublas.cublasZhemm_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZhemm(handle, side, uplo, m, n, alpha, A, lda, B, ldb, beta, C, ldc):                
    """
    Matrix-matrix product for Hermitian matrix.

    """
    
    status = _libcublas.cublasZhemm_v2(handle, 
                                       _CUBLAS_SIDE_MODE[side], 
                                       _CUBLAS_FILL_MODE[uplo], m, n,                                       
                                       ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                         alpha.imag)),
                                                                         int(A), lda, int(B), ldb,
                                       ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                         beta.imag)),
                                       int(C), ldc)
    cublasCheckStatus(status)

# CHERK, ZHERK
_libcublas.cublasCherk_v2.restype = int
_libcublas.cublasCherk_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasCherk(handle, uplo, trans, n, k, alpha, A, lda, beta, C, ldc):
    """
    Rank-k operation on Hermitian matrix.

    """
    
    status = _libcublas.cublasCherk_v2(handle, 
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans], 
                                       n, k, ctypes.byref(ctypes.c_float(alpha)),
                                       int(A), lda, 
                                       ctypes.byref(ctypes.c_float(beta)),
                                       int(C), ldc)
    cublasCheckStatus(status)

_libcublas.cublasZherk_v2.restype = int
_libcublas.cublasZherk_v2.argtypes = [ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,
                                      ctypes.c_int,                                      
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int,
                                      ctypes.c_void_p,
                                      ctypes.c_void_p,
                                      ctypes.c_int]
def cublasZherk(handle, uplo, trans, n, k, alpha, A, lda, beta, C, ldc):
    """
    Rank-k operation on Hermitian matrix.

    """
    
    status = _libcublas.cublasZherk_v2(handle,
                                       _CUBLAS_FILL_MODE[uplo], 
                                       _CUBLAS_OP[trans],
                                       n, k, ctypes.byref(ctypes.c_double(alpha)),
                                       int(A), lda, 
                                       ctypes.byref(ctypes.c_double(beta)),
                                       int(C), ldc)
    cublasCheckStatus(status)

# CHER2K, ZHER2K
_libcublas.cublasCher2k_v2.restype = int
_libcublas.cublasCher2k_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_float,
                                       ctypes.c_void_p,
                                       ctypes.c_int]
def cublasCher2k(handle, uplo, trans, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Rank-2k operation on Hermitian matrix.

    """
    
    status = _libcublas.cublasCher2k_v2(handle, 
                                        _CUBLAS_FILL_MODE[uplo], 
                                        _CUBLAS_OP[trans], 
                                        n, k, ctypes.byref(cuda.cuFloatComplex(alpha.real,                 
                                                                               alpha.imag)),
                                        int(A), lda, int(B), ldb, 
                                        ctypes.byref(cuda.cuFloatComplex(beta.real,
                                                                         beta.imag)),
                                        int(C), ldc)
    cublasCheckStatus(status)
        
_libcublas.cublasZher2k_v2.restype = int
_libcublas.cublasZher2k_v2.argtypes = [ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,
                                       ctypes.c_int,                                       
                                       ctypes.c_void_p,                                       
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_int,
                                       ctypes.c_void_p,
                                       ctypes.c_void_p,
                                       ctypes.c_int]
def cublasZher2k(handle, uplo, trans, n, k, alpha, A, lda, B, ldb, beta, C, ldc):
    """
    Rank-2k operation on Hermitian matrix.

    """

    status = _libcublas.cublasZher2k_v2(handle,
                                        _CUBLAS_FILL_MODE[uplo], 
                                        _CUBLAS_OP[trans], 
                                        n, k, ctypes.byref(cuda.cuDoubleComplex(alpha.real,
                                                                                alpha.imag)),
                                        int(A), lda, int(B), ldb,
                                        ctypes.byref(cuda.cuDoubleComplex(beta.real,
                                                                          beta.imag)), 
                                        int(C), ldc)
    cublasCheckStatus(status)

### BLAS-like extension routines ###

# SDGMM, DDGMM, CDGMM, ZDGMM
_libcublas.cublasSdgmm.restype = int
_libcublas.cublasSdgmm.argtypes = [ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int]

def cublasSdgmm(handle, mode, m, n, A, lda, x, incx, C, ldc):
    """
    Matrix-diagonal matrix product for real general matrix.
        
    """

    status = _libcublas.cublasSdgmm(handle,
                                    _CUBLAS_SIDE[mode],
                                    m, n, 
                                    int(A), lda, 
                                    int(x), incx,
                                    int(C), ldc)
    cublasCheckStatus(status)
  
_libcublas.cublasDdgmm.restype = int
_libcublas.cublasDdgmm.argtypes = [ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int]

def cublasDdgmm(handle, mode, m, n, A, lda, x, incx, C, ldc):
    """
    Matrix-diagonal matrix product for real general matrix.
        
    """

    status = _libcublas.cublasDdgmm(handle,
                                    _CUBLAS_SIDE[mode],
                                    m, n, 
                                    int(A), lda, 
                                    int(x), incx,
                                    int(C), ldc)
    cublasCheckStatus(status)
    
_libcublas.cublasCdgmm.restype = int
_libcublas.cublasCdgmm.argtypes = [ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int]

def cublasCdgmm(mode, m, n, A, lda, x, incx, C, ldc):
    """
    Matrix-diagonal matrix product for complex general matrix.
        
    """

    status = _libcublas.cublasCdgmm(handle,
                                    _CUBLAS_SIDE[mode],
                                    m, n, 
                                    int(A), lda, 
                                    int(x), incx,
                                    int(C), ldc)
    cublasCheckStatus(status)
      
_libcublas.cublasZdgmm.restype = int
_libcublas.cublasZdgmm.argtypes = [ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int,
                                    ctypes.c_void_p,
                                    ctypes.c_int]

def cublasZdgmm(mode, m, n, A, lda, x, incx, C, ldc):
    """
    Matrix-diagonal matrix product for complex general matrix.
        
    """

    status = _libcublas.cublasZdgmm(handle,
                                    _CUBLAS_SIDE[mode],
                                    m, n, 
                                    int(A), lda, 
                                    int(x), incx,
                                    int(C), ldc)
    cublasCheckStatus(status)        

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = cuda
# This file is taken from scikits.cuda (https://github.com/lebedov/scikits.cuda)
# Copyright (c) 2009-2013, Lev Givon. All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:

# Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
# Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
# Neither the name of Lev Givon nor the names of any contributors may
# be used to endorse or promote products derived from this software
# without specific prior written permission.  THIS SOFTWARE IS
# PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
# OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
# SUCH DAMAGE.

#!/usr/bin/env python

"""
Python interface to CUDA functions.
"""

from cudart import *
from cudadrv import *


########NEW FILE########
__FILENAME__ = cudadrv
# This file is taken from scikits.cuda (https://github.com/lebedov/scikits.cuda)
# Copyright (c) 2009-2013, Lev Givon. All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:

# Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
# Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
# Neither the name of Lev Givon nor the names of any contributors may
# be used to endorse or promote products derived from this software
# without specific prior written permission.  THIS SOFTWARE IS
# PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
# OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
# SUCH DAMAGE.

#!/usr/bin/env python

"""
Python interface to CUDA driver functions.
"""

import sys, ctypes

# Load CUDA driver library:
if sys.platform == 'linux2':
    _libcuda_libname_list = ['libcuda.so', 'libcuda.so.3', 'libcuda.so.4']
elif sys.platform == 'darwin':
    _libcuda_libname_list = ['libcuda.dylib']
elif sys.platform == 'win32':
    _libcuda_libname_list = ['nvcuda.dll']
else:
    raise RuntimeError('unsupported platform')

# Print understandable error message when library cannot be found:
_libcuda = None
for _libcuda_libname in _libcuda_libname_list:
    try:
        _libcuda = ctypes.cdll.LoadLibrary(_libcuda_libname)
    except OSError:
        pass
    else:
        break
if _libcuda == None:
    raise OSError('CUDA driver library not found')


# Exceptions corresponding to various CUDA driver errors:

class CUDA_ERROR(Exception):
    """CUDA error."""
    pass

class CUDA_ERROR_INVALID_VALUE(CUDA_ERROR):
    pass

class CUDA_ERROR_OUT_OF_MEMORY(CUDA_ERROR):
    pass

class CUDA_ERROR_NOT_INITIALIZED(CUDA_ERROR):
    pass

class CUDA_ERROR_DEINITIALIZED(CUDA_ERROR):
    pass

class CUDA_ERROR_PROFILER_DISABLED(CUDA_ERROR):
    pass

class CUDA_ERROR_PROFILER_NOT_INITIALIZED(CUDA_ERROR):
    pass

class CUDA_ERROR_PROFILER_ALREADY_STARTED(CUDA_ERROR):
    pass

class CUDA_ERROR_PROFILER_ALREADY_STOPPED(CUDA_ERROR):
    pass

class CUDA_ERROR_NO_DEVICE(CUDA_ERROR):
    pass

class CUDA_ERROR_INVALID_DEVICE(CUDA_ERROR):
    pass

class CUDA_ERROR_INVALID_IMAGE(CUDA_ERROR):
    pass

class CUDA_ERROR_INVALID_CONTEXT(CUDA_ERROR):
    pass

class CUDA_ERROR_CONTEXT_ALREADY_CURRENT(CUDA_ERROR):
    pass

class CUDA_ERROR_MAP_FAILED(CUDA_ERROR):
    pass

class CUDA_ERROR_UNMAP_FAILED(CUDA_ERROR):
    pass

class CUDA_ERROR_ARRAY_IS_MAPPED(CUDA_ERROR):
    pass

class CUDA_ERROR_ALREADY_MAPPED(CUDA_ERROR):
    pass

class CUDA_ERROR_NO_BINARY_FOR_GPU(CUDA_ERROR):
    pass

class CUDA_ERROR_ALREADY_ACQUIRED(CUDA_ERROR):
    pass

class CUDA_ERROR_NOT_MAPPED(CUDA_ERROR):
    pass

class CUDA_ERROR_NOT_MAPPED_AS_ARRAY(CUDA_ERROR):
    pass

class CUDA_ERROR_NOT_MAPPED_AS_POINTER(CUDA_ERROR):
    pass

class CUDA_ERROR_ECC_UNCORRECTABLE(CUDA_ERROR):
    pass

class CUDA_ERROR_UNSUPPORTED_LIMIT(CUDA_ERROR):
    pass

class CUDA_ERROR_CONTEXT_ALREADY_IN_USE(CUDA_ERROR):
    pass

class CUDA_ERROR_INVALID_SOURCE(CUDA_ERROR):
    pass

class CUDA_ERROR_FILE_NOT_FOUND(CUDA_ERROR):
    pass

class CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND(CUDA_ERROR):
    pass

class CUDA_ERROR_SHARED_OBJECT_INIT_FAILED(CUDA_ERROR):
    pass

class CUDA_ERROR_OPERATING_SYSTEM(CUDA_ERROR):
    pass

class CUDA_ERROR_INVALID_HANDLE(CUDA_ERROR):
    pass

class CUDA_ERROR_NOT_FOUND(CUDA_ERROR):
    pass

class CUDA_ERROR_NOT_READY(CUDA_ERROR):
    pass


CUDA_EXCEPTIONS = {
    1: CUDA_ERROR_INVALID_VALUE,
    2: CUDA_ERROR_OUT_OF_MEMORY,
    3: CUDA_ERROR_NOT_INITIALIZED,
    4: CUDA_ERROR_DEINITIALIZED,
    5: CUDA_ERROR_PROFILER_DISABLED,
    6: CUDA_ERROR_PROFILER_NOT_INITIALIZED,
    7: CUDA_ERROR_PROFILER_ALREADY_STARTED,
    8: CUDA_ERROR_PROFILER_ALREADY_STOPPED,
    100: CUDA_ERROR_NO_DEVICE,
    101: CUDA_ERROR_INVALID_DEVICE,
    200: CUDA_ERROR_INVALID_IMAGE,
    201: CUDA_ERROR_INVALID_CONTEXT,
    202: CUDA_ERROR_CONTEXT_ALREADY_CURRENT,
    205: CUDA_ERROR_MAP_FAILED,
    206: CUDA_ERROR_UNMAP_FAILED,
    207: CUDA_ERROR_ARRAY_IS_MAPPED,
    208: CUDA_ERROR_ALREADY_MAPPED,
    209: CUDA_ERROR_NO_BINARY_FOR_GPU,
    210: CUDA_ERROR_ALREADY_ACQUIRED,
    211: CUDA_ERROR_NOT_MAPPED,
    212: CUDA_ERROR_NOT_MAPPED_AS_ARRAY,
    213: CUDA_ERROR_NOT_MAPPED_AS_POINTER,
    214: CUDA_ERROR_ECC_UNCORRECTABLE,
    215: CUDA_ERROR_UNSUPPORTED_LIMIT,
    216: CUDA_ERROR_CONTEXT_ALREADY_IN_USE,
    300: CUDA_ERROR_INVALID_SOURCE,
    301: CUDA_ERROR_FILE_NOT_FOUND,
    302: CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND,
    303: CUDA_ERROR_SHARED_OBJECT_INIT_FAILED,
    304: CUDA_ERROR_OPERATING_SYSTEM,
    400: CUDA_ERROR_INVALID_HANDLE,
    500: CUDA_ERROR_NOT_FOUND,
    600: CUDA_ERROR_NOT_READY,
    }

def cuCheckStatus(status):
    """
    Raise CUDA exception.

    Raise an exception corresponding to the specified CUDA driver
    error code.

    Parameters
    ----------
    status : int
        CUDA driver error code.

    See Also
    --------
    CUDA_EXCEPTIONS

    """

    if status != 0:
        try:
            raise CUDA_EXCEPTIONS[status]
        except KeyError:
            raise CUDA_ERROR

        
CU_POINTER_ATTRIBUTE_CONTEXT = 1
CU_POINTER_ATTRIBUTE_MEMORY_TYPE = 2 
CU_POINTER_ATTRIBUTE_DEVICE_POINTER = 3
CU_POINTER_ATTRIBUTE_HOST_POINTER = 4

_libcuda.cuPointerGetAttribute.restype = int
_libcuda.cuPointerGetAttribute.argtypes = [ctypes.c_void_p,
                                           ctypes.c_int,
                                           ctypes.c_uint]
def cuPointerGetAttribute(attribute, ptr):
    data = ctypes.c_void_p()
    status = _libcuda.cuPointerGetAttribute(data, attribute, ptr)
    cuCheckStatus(status)
    return data

########NEW FILE########
__FILENAME__ = cudart
# This file is taken from scikits.cuda (https://github.com/lebedov/scikits.cuda)
# Copyright (c) 2009-2013, Lev Givon. All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:

# Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
# Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
# Neither the name of Lev Givon nor the names of any contributors may
# be used to endorse or promote products derived from this software
# without specific prior written permission.  THIS SOFTWARE IS
# PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
# OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
# SUCH DAMAGE.

#!/usr/bin/env python

"""
Python interface to CUDA runtime functions.
"""

import atexit, ctypes, sys, warnings
import numpy as np

# Load CUDA runtime library:
if sys.platform == 'linux2':
    _libcudart_libname_list = ['libcudart.so', 'libcudart.so.3', 'libcudart.so.4']
elif sys.platform == 'darwin':
    _libcudart_libname_list = ['libcudart.dylib']
elif sys.platform == 'win32':
    _libcudart_libname_list = ['cudart64_60.dll', 'cudart32_60.dll', 
                               'cudart64_55.dll', 'cudart32_55.dll', 
                               'cudart64_50.dll', 'cudart32_50.dll']
else:
    raise RuntimeError('unsupported platform')

# Print understandable error message when library cannot be found:
_libcudart = None
for _libcudart_libname in _libcudart_libname_list:
    try:
        _libcudart = ctypes.cdll.LoadLibrary(_libcudart_libname)
    except OSError:
        pass
    else:
        break
if _libcudart == None:
    raise OSError('CUDA runtime library not found')

# Code adapted from PARRET:
def POINTER(obj):
    """
    Create ctypes pointer to object.

    Notes
    -----
    This function converts None to a real NULL pointer because of bug
    in how ctypes handles None on 64-bit platforms.

    """

    p = ctypes.POINTER(obj)
    if not isinstance(p.from_param, classmethod):
        def from_param(cls, x):
            if x is None:
                return cls()
            else:
                return x
        p.from_param = classmethod(from_param)

    return p

# Classes corresponding to CUDA vector structures:
class float2(ctypes.Structure):
    _fields_ = [
        ('x', ctypes.c_float),
        ('y', ctypes.c_float)
        ]

class cuFloatComplex(float2):
    @property
    def value(self):
        return complex(self.x, self.y)

class double2(ctypes.Structure):
    _fields_ = [
        ('x', ctypes.c_double),
        ('y', ctypes.c_double)
        ]

class cuDoubleComplex(double2):
    @property
    def value(self):
        return complex(self.x, self.y)

def gpuarray_ptr(g):
    """
    Return ctypes pointer to data in GPUAarray object.

    """

    addr = int(g.gpudata)
    if g.dtype == np.int8:
        return ctypes.cast(addr, POINTER(ctypes.c_byte))
    if g.dtype == np.uint8:
        return ctypes.cast(addr, POINTER(ctypes.c_ubyte))
    if g.dtype == np.int16:
        return ctypes.cast(addr, POINTER(ctypes.c_short))
    if g.dtype == np.uint16:
        return ctypes.cast(addr, POINTER(ctypes.c_ushort))
    if g.dtype == np.int32:
        return ctypes.cast(addr, POINTER(ctypes.c_int))
    if g.dtype == np.uint32:
        return ctypes.cast(addr, POINTER(ctypes.c_uint))
    if g.dtype == np.int64:
        return ctypes.cast(addr, POINTER(ctypes.c_long))
    if g.dtype == np.uint64:
        return ctypes.cast(addr, POINTER(ctypes.c_ulong))
    if g.dtype == np.float32:
        return ctypes.cast(addr, POINTER(ctypes.c_float))
    elif g.dtype == np.float64:
        return ctypes.cast(addr, POINTER(ctypes.c_double))
    elif g.dtype == np.complex64:
        return ctypes.cast(addr, POINTER(cuFloatComplex))
    elif g.dtype == np.complex128:
        return ctypes.cast(addr, POINTER(cuDoubleComplex))
    else:
        raise ValueError('unrecognized type')

_libcudart.cudaGetErrorString.restype = ctypes.c_char_p
_libcudart.cudaGetErrorString.argtypes = [ctypes.c_int]
def cudaGetErrorString(e):
    """
    Retrieve CUDA error string.

    Return the string associated with the specified CUDA error status
    code.

    Parameters
    ----------
    e : int
        Error number.

    Returns
    -------
    s : str
        Error string.

    """

    return _libcudart.cudaGetErrorString(e)

# Generic CUDA error:
class cudaError(Exception):
    """CUDA error."""
    pass

# Exceptions corresponding to various CUDA runtime errors:
class cudaErrorMissingConfiguration(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(1)
    pass

class cudaErrorMemoryAllocation(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(2)
    pass

class cudaErrorInitializationError(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(3)
    pass

class cudaErrorLaunchFailure(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(4)
    pass

class cudaErrorPriorLaunchFailure(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(5)
    pass

class cudaErrorLaunchTimeout(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(6)
    pass

class cudaErrorLaunchOutOfResources(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(7)
    pass

class cudaErrorInvalidDeviceFunction(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(8)
    pass

class cudaErrorInvalidConfiguration(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(9)
    pass

class cudaErrorInvalidDevice(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(10)
    pass

class cudaErrorInvalidValue(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(11)
    pass

class cudaErrorInvalidPitchValue(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(12)
    pass

class cudaErrorInvalidSymbol(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(13)
    pass

class cudaErrorMapBufferObjectFailed(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(14)
    pass

class cudaErrorUnmapBufferObjectFailed(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(15)
    pass

class cudaErrorInvalidHostPointer(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(16)
    pass

class cudaErrorInvalidDevicePointer(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(17)
    pass

class cudaErrorInvalidTexture(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(18)
    pass

class cudaErrorInvalidTextureBinding(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(19)
    pass

class cudaErrorInvalidChannelDescriptor(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(20)
    pass

class cudaErrorInvalidMemcpyDirection(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(21)
    pass

class cudaErrorTextureFetchFailed(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(23)
    pass

class cudaErrorTextureNotBound(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(24)
    pass

class cudaErrorSynchronizationError(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(25)
    pass

class cudaErrorInvalidFilterSetting(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(26)
    pass

class cudaErrorInvalidNormSetting(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(27)
    pass

class cudaErrorMixedDeviceExecution(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(28)
    pass

class cudaErrorCudartUnloading(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(29)
    pass

class cudaErrorUnknown(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(30)
    pass

class cudaErrorNotYetImplemented(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(31)
    pass

class cudaErrorMemoryValueTooLarge(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(32)
    pass

class cudaErrorInvalidResourceHandle(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(33)
    pass

class cudaErrorNotReady(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(34)
    pass

class cudaErrorInsufficientDriver(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(35)
    pass

class cudaErrorSetOnActiveProcess(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(36)
    pass

class cudaErrorInvalidSurface(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(37)
    pass

class cudaErrorNoDevice(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(38)
    pass

class cudaErrorECCUncorrectable(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(39)
    pass

class cudaErrorSharedObjectSymbolNotFound(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(40)
    pass

class cudaErrorSharedObjectInitFailed(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(41)
    pass

class cudaErrorUnsupportedLimit(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(42)
    pass

class cudaErrorDuplicateVariableName(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(43)
    pass

class cudaErrorDuplicateTextureName(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(44)
    pass

class cudaErrorDuplicateSurfaceName(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(45)
    pass

class cudaErrorDevicesUnavailable(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(46)
    pass

class cudaErrorInvalidKernelImage(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(47)
    pass

class cudaErrorNoKernelImageForDevice(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(48)
    pass

class cudaErrorIncompatibleDriverContext(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(49)
    pass

class cudaErrorPeerAccessAlreadyEnabled(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(50)
    pass

class cudaErrorPeerAccessNotEnabled(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(51)
    pass

class cudaErrorDeviceAlreadyInUse(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(54)
    pass

class cudaErrorProfilerDisabled(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(55)
    pass

class cudaErrorProfilerNotInitialized(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(56)
    pass

class cudaErrorProfilerAlreadyStarted(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(57)
    pass

class cudaErrorProfilerAlreadyStopped(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(58)
    pass

class cudaErrorStartupFailure(cudaError):
    __doc__ = _libcudart.cudaGetErrorString(127)
    pass

cudaExceptions = {
    1: cudaErrorMissingConfiguration,
    2: cudaErrorMemoryAllocation,
    3: cudaErrorInitializationError,
    4: cudaErrorLaunchFailure,
    5: cudaErrorPriorLaunchFailure,
    6: cudaErrorLaunchTimeout,
    7: cudaErrorLaunchOutOfResources,
    8: cudaErrorInvalidDeviceFunction,
    9: cudaErrorInvalidConfiguration,
    10: cudaErrorInvalidDevice,
    11: cudaErrorInvalidValue,
    12: cudaErrorInvalidPitchValue,
    13: cudaErrorInvalidSymbol,
    14: cudaErrorMapBufferObjectFailed,
    15: cudaErrorUnmapBufferObjectFailed,
    16: cudaErrorInvalidHostPointer,
    17: cudaErrorInvalidDevicePointer,
    18: cudaErrorInvalidTexture,
    19: cudaErrorInvalidTextureBinding,
    20: cudaErrorInvalidChannelDescriptor,
    21: cudaErrorInvalidMemcpyDirection,
    22: cudaError,
    23: cudaErrorTextureFetchFailed,
    24: cudaErrorTextureNotBound,
    25: cudaErrorSynchronizationError,
    26: cudaErrorInvalidFilterSetting,
    27: cudaErrorInvalidNormSetting,
    28: cudaErrorMixedDeviceExecution,
    29: cudaErrorCudartUnloading,
    30: cudaErrorUnknown,
    31: cudaErrorNotYetImplemented,
    32: cudaErrorMemoryValueTooLarge,
    33: cudaErrorInvalidResourceHandle,
    34: cudaErrorNotReady,
    35: cudaErrorInsufficientDriver,
    36: cudaErrorSetOnActiveProcess,
    37: cudaErrorInvalidSurface,
    38: cudaErrorNoDevice,
    39: cudaErrorECCUncorrectable,
    40: cudaErrorSharedObjectSymbolNotFound,
    41: cudaErrorSharedObjectInitFailed,
    42: cudaErrorUnsupportedLimit,
    43: cudaErrorDuplicateVariableName,
    44: cudaErrorDuplicateTextureName,
    45: cudaErrorDuplicateSurfaceName,
    46: cudaErrorDevicesUnavailable,
    47: cudaErrorInvalidKernelImage,
    48: cudaErrorNoKernelImageForDevice,
    49: cudaErrorIncompatibleDriverContext,
    50: cudaErrorPeerAccessAlreadyEnabled,
    51: cudaErrorPeerAccessNotEnabled,
    52: cudaError,
    53: cudaError,
    54: cudaErrorDeviceAlreadyInUse,
    55: cudaErrorProfilerDisabled,
    56: cudaErrorProfilerNotInitialized,
    57: cudaErrorProfilerAlreadyStarted,
    58: cudaErrorProfilerAlreadyStopped,
    127: cudaErrorStartupFailure
    }

def cudaCheckStatus(status):
    """
    Raise CUDA exception.

    Raise an exception corresponding to the specified CUDA runtime error
    code.

    Parameters
    ----------
    status : int
        CUDA runtime error code.

    See Also
    --------
    cudaExceptions

    """

    if status != 0:
        try:
            raise cudaExceptions[status]
        except KeyError:
            raise cudaError

# Memory allocation functions (adapted from pystream):
_libcudart.cudaMalloc.restype = int
_libcudart.cudaMalloc.argtypes = [ctypes.POINTER(ctypes.c_void_p),
                                  ctypes.c_size_t]
def cudaMalloc(count, ctype=None):
    """
    Allocate device memory.

    Allocate memory on the device associated with the current active
    context.

    Parameters
    ----------
    count : int
        Number of bytes of memory to allocate
    ctype : _ctypes.SimpleType, optional
        ctypes type to cast returned pointer.

    Returns
    -------
    ptr : ctypes pointer
        Pointer to allocated device memory.

    """

    ptr = ctypes.c_void_p()
    status = _libcudart.cudaMalloc(ctypes.byref(ptr), count)
    cudaCheckStatus(status)
    if ctype != None:
        ptr = ctypes.cast(ptr, ctypes.POINTER(ctype))
    return ptr

_libcudart.cudaFree.restype = int
_libcudart.cudaFree.argtypes = [ctypes.c_void_p]
def cudaFree(ptr):
    """
    Free device memory.

    Free allocated memory on the device associated with the current active
    context.

    Parameters
    ----------
    ptr : ctypes pointer
        Pointer to allocated device memory.

    """

    status = _libcudart.cudaFree(ptr)
    cudaCheckStatus(status)

_libcudart.cudaMallocPitch.restype = int
_libcudart.cudaMallocPitch.argtypes = [ctypes.POINTER(ctypes.c_void_p),
                                       ctypes.POINTER(ctypes.c_size_t),
                                       ctypes.c_size_t, ctypes.c_size_t]
def cudaMallocPitch(pitch, rows, cols, elesize):
    """
    Allocate pitched device memory.

    Allocate pitched memory on the device associated with the current active
    context.

    Parameters
    ----------
    pitch : int
        Pitch for allocation.
    rows : int
        Requested pitched allocation height.
    cols : int
        Requested pitched allocation width.
    elesize : int
        Size of memory element.

    Returns
    -------
    ptr : ctypes pointer
        Pointer to allocated device memory.

    """

    ptr = ctypes.c_void_p()
    status = _libcudart.cudaMallocPitch(ctypes.byref(ptr),
                                        ctypes.c_size_t(pitch), cols*elesize,
                                        rows)
    cudaCheckStatus(status)
    return ptr, pitch

# Memory copy modes:
cudaMemcpyHostToHost = 0
cudaMemcpyHostToDevice = 1
cudaMemcpyDeviceToHost = 2
cudaMemcpyDeviceToDevice = 3
cudaMemcpyDefault = 4

_libcudart.cudaMemcpy.restype = int
_libcudart.cudaMemcpy.argtypes = [ctypes.c_void_p, ctypes.c_void_p,
                                  ctypes.c_size_t, ctypes.c_int]
def cudaMemcpy_htod(dst, src, count):
    """
    Copy memory from host to device.

    Copy data from host memory to device memory.

    Parameters
    ----------
    dst : ctypes pointer
        Device memory pointer.
    src : ctypes pointer
        Host memory pointer.
    count : int
        Number of bytes to copy.

    """

    status = _libcudart.cudaMemcpy(dst, src,
                                   ctypes.c_size_t(count),
                                   cudaMemcpyHostToDevice)
    cudaCheckStatus(status)

def cudaMemcpy_dtoh(dst, src, count):
    """
    Copy memory from device to host.

    Copy data from device memory to host memory.

    Parameters
    ----------
    dst : ctypes pointer
        Host memory pointer.
    src : ctypes pointer
        Device memory pointer.
    count : int
        Number of bytes to copy.

    """

    status = _libcudart.cudaMemcpy(dst, src,
                                   ctypes.c_size_t(count),
                                   cudaMemcpyDeviceToHost)
    cudaCheckStatus(status)

_libcudart.cudaMemGetInfo.restype = int
_libcudart.cudaMemGetInfo.argtypes = [ctypes.c_void_p,
                                      ctypes.c_void_p]
def cudaMemGetInfo():
    """
    Return the amount of free and total device memory.

    Returns
    -------
    free : long
        Free memory in bytes.
    total : long
        Total memory in bytes.

    """

    free = ctypes.c_size_t()
    total = ctypes.c_size_t()
    status = _libcudart.cudaMemGetInfo(ctypes.byref(free),
                                       ctypes.byref(total))
    cudaCheckStatus(status)
    return free.value, total.value

_libcudart.cudaSetDevice.restype = int
_libcudart.cudaSetDevice.argtypes = [ctypes.c_int]
def cudaSetDevice(dev):
    """
    Set current CUDA device.

    Select a device to use for subsequent CUDA operations.

    Parameters
    ----------
    dev : int
        Device number.

    """

    status = _libcudart.cudaSetDevice(dev)
    cudaCheckStatus(status)

_libcudart.cudaGetDevice.restype = int
_libcudart.cudaGetDevice.argtypes = [ctypes.POINTER(ctypes.c_int)]
def cudaGetDevice():
    """
    Get current CUDA device.

    Return the identifying number of the device currently used to
    process CUDA operations.

    Returns
    -------
    dev : int
        Device number.

    """

    dev = ctypes.c_int()
    status = _libcudart.cudaGetDevice(ctypes.byref(dev))
    cudaCheckStatus(status)
    return dev.value

_libcudart.cudaDriverGetVersion.restype = int
_libcudart.cudaDriverGetVersion.argtypes = [ctypes.POINTER(ctypes.c_int)]
def cudaDriverGetVersion():
    """
    Get installed CUDA driver version.

    Return the version of the installed CUDA driver as an integer. If
    no driver is detected, 0 is returned.

    Returns
    -------
    version : int
        Driver version.

    """

    version = ctypes.c_int()
    status = _libcudart.cudaDriverGetVersion(ctypes.byref(version))
    cudaCheckStatus(status)
    return version.value

# Memory types:
cudaMemoryTypeHost = 1
cudaMemoryTypeDevice = 2

class cudaPointerAttributes(ctypes.Structure):
    _fields_ = [
        ('memoryType', ctypes.c_int),
        ('device', ctypes.c_int),
        ('devicePointer', ctypes.c_void_p),
        ('hostPointer', ctypes.c_void_p)
        ]

_libcudart.cudaPointerGetAttributes.restype = int
_libcudart.cudaPointerGetAttributes.argtypes = [ctypes.c_void_p,
                                                ctypes.c_void_p]
def cudaPointerGetAttributes(ptr):
    """
    Get memory pointer attributes.

    Returns attributes of the specified pointer.

    Parameters
    ----------
    ptr : ctypes pointer
        Memory pointer to examine.

    Returns
    -------
    memory_type : int
        Memory type; 1 indicates host memory, 2 indicates device
        memory.
    device : int
        Number of device associated with pointer.

    Notes
    -----
    This function only works with CUDA 4.0 and later.

    """

    attributes = cudaPointerAttributes()
    status = \
        _libcudart.cudaPointerGetAttributes(ctypes.byref(attributes), ptr)
    cudaCheckStatus(status)
    return attributes.memoryType, attributes.device


########NEW FILE########
__FILENAME__ = elementwise
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
from pycuda import gpuarray
from pycuda.elementwise import ElementwiseKernel
from .. import sampler
from .matrix import extract_columns, insert_columns

class Kernel(object):
    """ Defers creation of the ElementwiseKernels until the first
    runtime and automatically selects kernels for double and float.
    """

    def __init__(self, name, signature_float, code_float, 
                 signature_double, code_double):
        self.name = name
        self.kernel_float = ElementwiseKernel(signature_float, code_float, name)
        self.kernel_double = ElementwiseKernel(signature_double, code_double, name)

    def __call__(self, *args, **kwargs):
        if args[0].dtype == np.float32:
            self.kernel_float(*args, **kwargs)
        elif args[0].dtype == np.float64:
            self.kernel_double(*args, **kwargs)
        else:
            raise ValueError("Unknown datatype, must be np.float32 or np.float64")

    def get_kernel(self, dtype):
        if dtype == np.float32 or dtype == 'float':
            return self.kernel_float
        elif dtype == np.float64 or dtype == 'double':
            return self.kernel_double
        else:
            raise ValueError("Unknown datatype, must be np.float32 or np.float64")

all_kernels = None
exp_func = None
log_func = None
def init():
    from pycuda import elementwise
    
    global all_kernels
    global exp_func
    global log_func

    all_kernels_code = {
        'sign': {
            'float':  ("float *mat, float *target",
                       "target[i] = (mat[i] > 0.) - (mat[i] < 0);"),
            'double': ("double *mat, double *target",
                       "target[i] = (mat[i] > 0.) - (mat[i] < 0);")
        },

        'sigmoid': {
            'float':  ("float *mat",
                       "mat[i] = 1. / (1. + __expf(-mat[i]))",),
            'double': ("double *mat",
                       "mat[i] = 1. / (1. + exp(-mat[i]))")
        },

        'df_sigmoid': {
            'float': ("float *mat, float *target",
                      """const float f = mat[i];
                      target[i] = f * (1 - f);
                      """),
            'double': ("double *mat, double *target",
                       """const double f = mat[i];
                       target[i] = f * (1 - f);
                       """)
        },

        'tanh_inplace': {
            'float':  ("float *mat",
                       "mat[i] = tanhf(mat[i]);"),
            'double': ("double *mat",
                       "mat[i] = tanh(mat[i]);")
        },

        'df_tanh': {
            'float': ("float *mat, float *target",
                      """float f = mat[i];
                      target[i] = 1 - pow(f, 2);"""),
            'double': ("double *mat, double *target",
                       """double f = mat[i];
                       target[i] = 1 - pow(f, 2);""")
        },

        'relu': {
            'float':  ("float *mat",
                       "if (mat[i] < 0.) mat[i] = 0.",),
            'double': ("double *mat",
                       "if (mat[i] < 0.) mat[i] = 0.")
        },

        'df_relu': {
            'float':  ("float *mat, float *target",
                       "if (mat[i] <= 0.)\n  target[i] = 0.;\nelse\n  target[i] = 1.;"),
            'double': ("double *mat, double *target",
                       "if (mat[i] <= 0.)\n  target[i] = 0.;\nelse\n  target[i] = 1.;")
        },

        'sample_dropout_mask': {
            'float':  ("float *mat, float *target, char *dropout_mask, "
                       "float *dropout_prob_array, float dropout_probability",
                       """if (dropout_prob_array[i] <= dropout_probability) {
                            dropout_mask[i] = 0.;
                            target[i] = 0.;
                          } else {
                            dropout_mask[i] = 1.;
                            if (target != mat)
                                target[i] = mat[i];
                          }
                        """),
            'double':  ("double *mat, double *targets, char *dropout_mask, "
                        "double *dropout_prob_array float dropout_probability",
                        """if (dropout_prob_array[i] <= dropout_probability) {
                            dropout_mask[i] = 0.;
                            target[i] = 0.;
                          } else {
                            dropout_mask[i] = 1.;
                            if (target != mat)                    
                                target[i] = mat[i];
                          }
                        """)
        },

        'apply_dropout_mask': {
            'float':    ("float *mat, char *mask",
                         "if (mask[i] == 0.) mat[i] = 0;"),
            'double':   ("double *mat, char *mask",
                         "if (mask[i] == 0.) mat[i] = 0;"),
        },

        'nan_to_zeros': {
            'float':    ("float *mat, float *target",
                         "target[i] = isnan(mat[i]) ? 0. : mat[i];"),
            'double':   ("double *mat, double *target",
                         "target[i] = isnan(mat[i]) ? 0. : mat[i];")
        },

        'mult_matrix': {
            'float': ("const float *a, const float *b, float *c",
                      "c[i] = a[i] * b[i];"),
            'double': ("const double *b, const double *b, double *c",
                       "c[i] = a[i] * b[i];")

        },
        'substract_matrix': {
            'float': ("const float *a, const float *b, float *c",
                      "c[i] = a[i] - b[i];"),
            'double': ("const double *a, const double *b, double *c",
                       "c[i] = a[i] - b[i];")
        }
    }

    all_kernels = {
        name: Kernel(name, 
                     val['float'][0], val['float'][1],
                     val['double'][0], val['double'][1])
        for name, val in all_kernels_code.iteritems()
    }

    exp_func = elementwise.get_unary_func_kernel('expf', np.float32)
    log_func = elementwise.get_unary_func_kernel('logf', np.float32)

def sign(x, target=None):
    assert x.flags.c_contiguous
    if target is None:
        target = gpuarray.GPUArray(x.shape, dtype=x.dtype)
    assert target.shape == x.shape
    assert target.dtype == x.dtype
    assert target.flags.c_contiguous
    all_kernels['sign'](x, target)
    return target

def sigmoid(x):
    assert x.flags.c_contiguous
    all_kernels['sigmoid'](x)

def df_sigmoid(f, target=None):
    assert f.flags.c_contiguous
    if target is None:
        target = gpuarray.empty_like(f)
    all_kernels['df_sigmoid'](f, target)
    return target

def tanh(x):
    assert x.flags.c_contiguous
    all_kernels['tanh_inplace'](x)

def df_tanh(f, target=None):
    assert f.flags.c_contiguous
    if target is None:
        target = gpuarray.empty_like(f)
    all_kernels['df_tanh'](f, target)
    return target

def relu(x):
    assert x.flags.c_contiguous
    all_kernels['relu'](x)

def df_relu(x, target=None):
    assert x.flags.c_contiguous
    if target is None:
        target = gpuarray.empty_like(x)        
    all_kernels['df_relu'](x, target)
    return target

def linear(x):
    pass

def df_linear(x):
    return x

def sample_dropout_mask(x, dropout_probability=.5, columns=None, stream=None, target=None,
                        dropout_mask=None, dropout_prob_array=None):
    """ Samples a dropout mask and applies it in place"""

    assert x.flags.c_contiguous

    if columns is not None:
        assert len(columns) == 2
        x_tmp = x
        x = extract_columns(x, columns[0], columns[1])

    shape = x.shape

    if dropout_prob_array is None:
        dropout_prob_array = gpuarray.empty(shape, x.dtype)
    sampler.fill_uniform(dropout_prob_array, stream)

    if dropout_mask is None:
        dropout_mask = gpuarray.empty(shape, np.int8)

    if target is None: target = x
    
    all_kernels['sample_dropout_mask'](
        x, target, dropout_mask, dropout_prob_array,
        np.float32(dropout_probability))

    if columns is not None:
        insert_columns(x, x_tmp, columns[0])

    return dropout_mask

def apply_dropout_mask(x, mask, columns=None, stream=None):
    assert x.flags.c_contiguous

    if columns is not None:
        assert len(columns) == 2
        x_tmp = x
        x = extract_columns(x, columns[0], columns[1])

    assert x.shape == mask.shape
    shape = x.shape

    all_kernels['apply_dropout_mask'](x, mask)

    if columns is not None:
        insert_columns(x, x_tmp, columns[0])

def nan_to_zeros(x, target=None):
    assert x.flags.c_contiguous
    if target is None:
        target = gpuarray.empty_like(x)
    assert target.flags.c_contiguous
    all_kernels['nan_to_zeros'](x, target)
    return target

def mult_matrix(a, b, target=None):
    assert a.shape == b.shape
    if target is None:
        target = gpuarray.empty_like(a)

    all_kernels['mult_matrix'](a, b, target)
    return target

def substract_matrix(a, b, target=None):
    assert a.shape == b.shape
    if target is None:
        target = gpuarray.empty_like(a)

    all_kernels['substract_matrix'](a, b, target)
    return target

########NEW FILE########
__FILENAME__ = linalg
# This file is modified from scikits.cuda (https://github.com/lebedov/scikits.cuda)
# Copyright (c) 2009-2013, Lev Givon. All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:

# Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
# Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
# Neither the name of Lev Givon nor the names of any contributors may
# be used to endorse or promote products derived from this software
# without specific prior written permission.  THIS SOFTWARE IS
# PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
# OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
# SUCH DAMAGE.

from string import lower
import pycuda.gpuarray as gpuarray
import numpy as np
from . import cublas

def init():
    global _global_cublas_handle
    _global_cublas_handle = cublas.cublasCreate()

def dot(x_gpu, y_gpu, transa='N', transb='N', handle=None, target=None):
    """
    Dot product of two arrays.

    For 1D arrays, this function computes the inner product. For 2D
    arrays of shapes `(m, k)` and `(k, n)`, it computes the matrix
    product; the result has shape `(m, n)`.

    Parameters
    ----------
    x_gpu : pycuda.gpuarray.GPUArray
        Input array.
    y_gpu : pycuda.gpuarray.GPUArray
        Input array.
    transa : char
        If 'T', compute the product of the transpose of `x_gpu`.
        If 'C', compute the product of the Hermitian of `x_gpu`.
    transb : char
        If 'T', compute the product of the transpose of `y_gpu`.
        If 'C', compute the product of the Hermitian of `y_gpu`.
    handle : int
        CUBLAS context. If no context is specified, the default handle from
        `scikits.cuda.misc._global_cublas_handle` is used.

    Returns
    -------
    c_gpu : pycuda.gpuarray.GPUArray, float{32,64}, or complex{64,128}
        Inner product of `x_gpu` and `y_gpu`. When the inputs are 1D
        arrays, the result will be returned as a scalar.

    Notes
    -----
    The input matrices must all contain elements of the same data type.

    Examples
    --------
    >>> import pycuda.gpuarray as gpuarray
    >>> import pycuda.autoinit
    >>> import numpy as np
    >>> import linalg
    >>> import misc
    >>> linalg.init()
    >>> a = np.asarray(np.random.rand(4, 2), np.float32)
    >>> b = np.asarray(np.random.rand(2, 2), np.float32)
    >>> a_gpu = gpuarray.to_gpu(a)
    >>> b_gpu = gpuarray.to_gpu(b)
    >>> c_gpu = linalg.dot(a_gpu, b_gpu)
    >>> np.allclose(np.dot(a, b), c_gpu.get())
    True
    >>> d = np.asarray(np.random.rand(5), np.float32)
    >>> e = np.asarray(np.random.rand(5), np.float32)
    >>> d_gpu = gpuarray.to_gpu(d)
    >>> e_gpu = gpuarray.to_gpu(e)
    >>> f = linalg.dot(d_gpu, e_gpu)
    >>> np.allclose(np.dot(d, e), f)
    True

    """

    if handle is None:
        handle = _global_cublas_handle

    if len(x_gpu.shape) == 1 and len(y_gpu.shape) == 1:

        if x_gpu.size != y_gpu.size:
            raise ValueError('arrays must be of same length')

        # Compute inner product for 1D arrays:
        if (x_gpu.dtype == np.complex64 and y_gpu.dtype == np.complex64):
            cublas_func = cublas.cublasCdotu
        elif (x_gpu.dtype == np.float32 and y_gpu.dtype == np.float32):
            cublas_func = cublas.cublasSdot
        elif (x_gpu.dtype == np.complex128 and y_gpu.dtype == np.complex128):
            cublas_func = cublas.cublasZdotu
        elif (x_gpu.dtype == np.float64 and y_gpu.dtype == np.float64):
            cublas_func = cublas.cublasDdot
        else:
            raise ValueError('unsupported combination of input types')

        return cublas_func(handle, x_gpu.size, x_gpu.gpudata, 1,
                           y_gpu.gpudata, 1)
    else:

        # Get the shapes of the arguments (accounting for the
        # possibility that one of them may only have one dimension):
        x_shape = x_gpu.shape
        y_shape = y_gpu.shape
        if len(x_shape) == 1:
            x_shape = (1, x_shape[0])
        if len(y_shape) == 1:
            y_shape = (1, y_shape[0])

        # Perform matrix multiplication for 2D arrays:
        if (x_gpu.dtype == np.complex64 and y_gpu.dtype == np.complex64):
            cublas_func = cublas.cublasCgemm
            alpha = np.complex64(1.0)
            beta = np.complex64(0.0)
        elif (x_gpu.dtype == np.float32 and y_gpu.dtype == np.float32):
            cublas_func = cublas.cublasSgemm
            alpha = np.float32(1.0)
            beta = np.float32(0.0)
        elif (x_gpu.dtype == np.complex128 and y_gpu.dtype == np.complex128):
            cublas_func = cublas.cublasZgemm
            alpha = np.complex128(1.0)
            beta = np.complex128(0.0)
        elif (x_gpu.dtype == np.float64 and y_gpu.dtype == np.float64):
            cublas_func = cublas.cublasDgemm
            alpha = np.float64(1.0)
            beta = np.float64(0.0)
        else:
            raise ValueError('unsupported combination of input types')

        transa = lower(transa)
        transb = lower(transb)

        if transb in ['t', 'c']:
            m, k = y_shape
        elif transb in ['n']:
            k, m = y_shape
        else:
            raise ValueError('invalid value for transb')

        if transa in ['t', 'c']:
            l, n = x_shape
        elif transa in ['n']:
            n, l = x_shape
        else:
            raise ValueError('invalid value for transa')

        if l != k:
            raise ValueError('objects are not aligned')

        if transb == 'n':
            lda = max(1, m)
        else:
            lda = max(1, k)

        if transa == 'n':
            ldb = max(1, k)
        else:
            ldb = max(1, n)

        ldc = max(1, m)

        # Note that the desired shape of the output matrix is the transpose
        # of what CUBLAS assumes:

        if target is None:
            target = gpuarray.empty((n, ldc), x_gpu.dtype)
        
        cublas_func(handle, transb, transa, m, n, k, alpha, y_gpu.gpudata,
                    lda, x_gpu.gpudata, ldb, beta, target.gpudata, ldc)

        return target

########NEW FILE########
__FILENAME__ = matrix
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
from pycuda import driver as drv
from pycuda import gpuarray
from hebel.utils.math import ceil_div

add_row_vec_kernel = None
add_col_vec_kernel = None
vector_normalize_kernel = None
_compilation_constants = {
    'add_vec_block_size': 16
}
def init():
    from pycuda.compiler import SourceModule
    
    global add_row_vec_kernel
    global add_col_vec_kernel
    global vector_normalize_kernel

    code = """
    #include <stdint.h>
    __global__ void addRowVecToMat(const float *mat,
                                   const float *vec,
                                   float *target,
                                   const int32_t n,
                                   const int32_t m,
                                   const int substract)
    {
      const int tx = threadIdx.x;
      const int ty = threadIdx.y;
      const int tidx = blockIdx.x * blockDim.x + threadIdx.x;
      const int tidy = blockIdx.y * blockDim.y + threadIdx.y;

      __shared__ float shared_vec[%(add_vec_block_size)d];

      if ((tx == 0) & (tidy < m))
          shared_vec[ty] = vec[tidy];
      __syncthreads();

      if ((tidy < m) & (tidx < n))
      {
          if (substract)
              target[tidx*m+tidy] = mat[tidx*m+tidy] - shared_vec[ty];
          else
              target[tidx*m+tidy] = mat[tidx*m+tidy] + shared_vec[ty];      
      }
    }

    __global__ void addColVecToMat(const float *mat,
                                   const float *vec,
                                   float *target,
                                   const int32_t n,
                                   const int32_t m,
                                   const int substract)
    {
      const int tx = threadIdx.x;
      const int ty = threadIdx.y;
      const int tidx = blockIdx.x * blockDim.x + threadIdx.x;
      const int tidy = blockIdx.y * blockDim.y + threadIdx.y;

      __shared__ float shared_vec[%(add_vec_block_size)d];

      if ((ty == 0) & (tidx < n))
          shared_vec[tx] = vec[tidx];
      __syncthreads();

      if ((tidy < m) & (tidx < n))
      {
          if (substract)
              target[tidx*m+tidy] = mat[tidx*m+tidy] - shared_vec[tx];
          else
              target[tidx*m+tidy] = mat[tidx*m+tidy] + shared_vec[tx];      
      }
    }

    __global__ void kVectorNormalize(float* mat,
                                     float max_vec_norm,
                                     unsigned int width,
                                     unsigned int height) {

        __shared__ float sum_shared[32];
        __shared__ float vec_norm;
        float sum = 0;

        for (unsigned int i = threadIdx.x; i < height; i += 32)
            sum += powf(mat[blockIdx.x + i * width], 2);

        sum_shared[threadIdx.x] = sum;

        __syncthreads();

        if (threadIdx.x == 0) {
            sum = 0;

            for (unsigned int i = 0; i < 32; i++)
                sum += sum_shared[i];

            vec_norm = sqrtf(sum);
        }
        __syncthreads();

        for (unsigned int i = threadIdx.x; i < height; i += 32) {
            if (vec_norm > max_vec_norm)
                mat[blockIdx.x + i * width] /= (vec_norm / max_vec_norm);
        }
    }
    """ % _compilation_constants

    mod = SourceModule(code)
    add_row_vec_kernel = mod.get_function('addRowVecToMat')
    add_col_vec_kernel = mod.get_function('addColVecToMat')
    vector_normalize_kernel = mod.get_function("kVectorNormalize")

def add_vec_to_mat(mat, vec, axis=None, inplace=False,
                   target=None, substract=False):
    """ Add a vector to a matrix
    """

    assert mat.flags.c_contiguous

    if axis is None:
        if vec.shape[0] == mat.shape[0]:
            axis = 0
        elif vec.shape[0] == mat.shape[1]:
            axis = 1
        else:
            raise ValueError('Vector length must be equal '
                             'to one side of the matrix')

    n, m = mat.shape

    block = (_compilation_constants['add_vec_block_size'],
             _compilation_constants['add_vec_block_size'], 1)
    gridx = ceil_div(n, block[0])
    gridy = ceil_div(m, block[1])
    grid = (gridx, gridy, 1)

    if inplace:
        target = mat
    elif target is None:
            target = gpuarray.empty_like(mat)

    if axis == 0:
        assert vec.shape[0] == mat.shape[0]
        add_col_vec_kernel(mat, vec, target, np.uint32(n), np.uint32(m),
                           np.int32(substract), block=block, grid=grid)
    elif axis == 1:
        assert vec.shape[0] == mat.shape[1]
        add_row_vec_kernel(mat, vec, target, np.uint32(n), np.uint32(m),
                           np.int32(substract), block=block, grid=grid)
    return target


def vector_normalize(mat, max_vec_norm=1.):
    """ Normalize each column vector in mat to length
    max_vec_norm if it is longer than max_vec_norm
    """
    assert mat.flags.c_contiguous
    n, m = mat.shape

    vector_normalize_kernel(mat, np.float32(max_vec_norm),
                            np.int32(m), np.int32(n),
                            block=(32,1,1), grid=(m,1,1))


def extract_columns(mat, start=0, stop=None, target=None):
    dtype = mat.dtype
    itemsize = np.dtype(dtype).itemsize
    N, M = mat.shape
    if stop is None:
        stop = M
    m = stop - start

    assert mat.flags.c_contiguous
    assert start >= 0 and start <= M and stop >= 0 and \
        stop <= M and stop > start

    if target is None:
        target = gpuarray.empty((N, m), dtype)

    copy = drv.Memcpy2D()
    copy.set_src_device(mat.gpudata)
    copy.src_x_in_bytes = start * itemsize
    copy.set_dst_device(target.gpudata)
    copy.src_pitch = M * itemsize
    copy.dst_pitch = copy.width_in_bytes = m * itemsize
    copy.height = N
    copy(aligned=True)

    return target


def insert_columns(src, dst, offset):
    dtype = src.dtype
    itemsize = np.dtype(dtype).itemsize
    h_src, w_src = src.shape
    h_dst, w_dst = dst.shape

    assert dst.dtype == dtype
    assert h_src == h_dst
    assert w_dst >= offset + w_src

    copy = drv.Memcpy2D()
    copy.set_src_device(src.gpudata)
    copy.set_dst_device(dst.gpudata)
    copy.dst_x_in_bytes = offset * itemsize
    copy.src_pitch = copy.width_in_bytes = w_src * itemsize
    copy.dst_pitch = w_dst * itemsize
    copy.height = h_src
    copy(aligned=True)

########NEW FILE########
__FILENAME__ = reductions
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import numpy as np
from pycuda import gpuarray
from . import linalg

max_column = None
max_row = None
def init():
    from pycuda.compiler import SourceModule

    global max_column
    global max_row

    code = """
#include "float.h"

__global__ void kMaxColumnwise(float* mat,
                               float* target,
                               unsigned int width,
                               unsigned int height) {
    __shared__ float max_vals[32];
    float cur_max = -FLT_MAX;
    float val = 0;

    for (unsigned int i = threadIdx.x; i < height; i += 32) {
        val = mat[blockIdx.x + i * width];

        if (val > cur_max)
            cur_max = val;
    }

    max_vals[threadIdx.x] = cur_max;

    __syncthreads();

    if (threadIdx.x == 0) {
        cur_max = -FLT_MAX;

        for (unsigned int i = 0; i < 32; i++)
            if (max_vals[i] > cur_max)
                cur_max = max_vals[i];

        target[blockIdx.x] = cur_max;
    }
    // __syncthreads();
}

__global__ void kMaxRowwise(float* mat,
                            float* target,
                            unsigned int width,
                            unsigned int height) {
    __shared__ float max_vals[32];
    float cur_max = -FLT_MAX;
    float val = 0;

    for (unsigned int i = threadIdx.x; i < width; i += 32) {
        val = mat[blockIdx.x * width + i];

        if (val > cur_max)
            cur_max = val;
    }

    max_vals[threadIdx.x] = cur_max;

    __syncthreads();

    if (threadIdx.x == 0) {
        cur_max = -FLT_MAX;

        for (unsigned int i = 0; i < 32; i++)
            if (max_vals[i] > cur_max)
                cur_max = max_vals[i];

        target[blockIdx.x] = cur_max;
    }
    // __syncthreads();
}
"""

    mod = SourceModule(code)
    max_column = mod.get_function("kMaxColumnwise")
    max_row = mod.get_function("kMaxRowwise")


def max_by_axis(mat, axis=0):
    assert mat.flags.c_contiguous
    assert axis in (0, 1)

    n, m = mat.shape

    if axis == 0:
        target = gpuarray.empty(m, dtype=np.float32)
        max_column(mat, target, np.int32(m), np.int32(n),
                   block=(32, 1, 1), grid=(m, 1, 1))

    elif axis == 1:
        target = gpuarray.empty(n, dtype=np.float32)
        max_row(mat, target, np.int32(m), np.int32(n),
                block=(32, 1, 1), grid=(n, 1, 1))

    return target


def _matrix_sum_out_axis_wrapper():
    one_vector_cache = {}

    def f(mat, axis=0, cache_one_vector=True, target=None):
        assert mat.flags.c_contiguous
        N, M = mat.shape

        if axis == 0:
            vec_shape = (N, 1)
            try:
                ones = one_vector_cache[vec_shape]
            except KeyError:
                ones = gpuarray.empty(vec_shape, dtype=mat.dtype).fill(1.)
                if cache_one_vector: one_vector_cache[vec_shape] = ones

            if target is None:
                target = gpuarray.empty((M,), mat.dtype)

            # if len(target.shape) == 1:
                # target = target.reshape((target.shape[0], 1))
                # target.shape = (target.shape[0], 1)
            assert target.shape == (M,)
            linalg.dot(mat, ones, transa='T', target=target)
        elif axis == 1:
            vec_shape = (M, 1)
            try:
                ones = one_vector_cache[vec_shape]
            except KeyError:
                ones = gpuarray.empty((M, 1), dtype=mat.dtype).fill(1.)
                if cache_one_vector: one_vector_cache[vec_shape] = ones

            if target is None:
                target = gpuarray.empty((N,), mat.dtype)

            # if len(target.shape) == 1:
            #     target = target.reshape((target.shape[0], 1))
            assert target.shape == (N,)
            linalg.dot(mat, ones, target=target)
        else:
            raise ValueError('axis must be 0 or 1')

        # target.shape = (target.shape[0], 1)
        return target
    return f
matrix_sum_out_axis = _matrix_sum_out_axis_wrapper()

########NEW FILE########
__FILENAME__ = softmax
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from . import eps
from .reductions import max_by_axis
from .matrix import add_vec_to_mat
from .reductions import matrix_sum_out_axis
from .elementwise import nan_to_zeros
from pycuda import cumath, gpuarray
import numpy as np

exp_func = None
log_func = None
def init():
    global exp_func
    global log_func
    _temp = __import__('hebel.pycuda_ops.elementwise', fromlist=['exp_func', 'log_func'])
    exp_func = _temp.exp_func
    log_func = _temp.log_func

def logsumexp(mat, tmp=None):
    max_dim = max_by_axis(mat, 1)
    if tmp is None:
        tmp = gpuarray.empty_like(mat)
    add_vec_to_mat(mat, max_dim, 0, target=tmp, substract=True)

    exp_func.prepared_async_call(tmp._grid, tmp._block, None,
                                 tmp.gpudata, tmp.gpudata, tmp.mem_size)
    
    # tmp = cumath.exp(tmp)
    tmp = matrix_sum_out_axis(tmp, 1)
    # tmp = cumath.log(tmp)
    log_func.prepared_async_call(tmp._grid, tmp._block, None,
                                 tmp.gpudata, tmp.gpudata, tmp.mem_size)
    max_dim += tmp
    return max_dim

def softmax(mat, tmp=None):
    if tmp is None:
        tmp = gpuarray.empty_like(mat)
    L = logsumexp(mat, tmp)
    add_vec_to_mat(mat, L, target=tmp, substract=True)
    exp_func.prepared_async_call(tmp._grid, tmp._block, None,
                                 tmp.gpudata, tmp.gpudata,
                                 tmp.mem_size)
    return tmp

def cross_entropy(x, y):
    loss = y * cumath.log(x + eps)
    nan_to_zeros(loss, loss)
    loss = -gpuarray.sum(loss)
    return float(loss.get())

def cross_entropy_logistic(x, y):
    loss = y * cumath.log(x + eps) + (1. - y) * cumath.log(1. - x + eps)
    loss = -gpuarray.sum(loss)
    return float(loss.get())
########NEW FILE########
__FILENAME__ = utils
# This file is taken from scikits.cuda (https://github.com/lebedov/scikits.cuda)
# Copyright (c) 2009-2013, Lev Givon. All rights reserved.

# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:

# Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
# Redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution.
# Neither the name of Lev Givon nor the names of any contributors may
# be used to endorse or promote products derived from this software
# without specific prior written permission.  THIS SOFTWARE IS
# PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
# OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
# SUCH DAMAGE.

#!/usr/bin/env python

"""
Utility functions.
"""

import ctypes
import re
import subprocess

try:
    import elftools
except ImportError:
    import re

    def get_soname(filename):
        """
        Retrieve SONAME of shared library.

        Parameters
        ----------
        filename : str
            Full path to shared library.

        Returns
        -------
        soname : str
            SONAME of shared library.

        Notes
        -----
        This function uses the `objdump` system command.
        
        """
        
        try:
            p = subprocess.Popen(['objdump', '-p', filename],
                                 stdout=subprocess.PIPE)
            out = p.communicate()[0]
        except:
            raise RuntimeError('error executing objdump')
        else:
            result = re.search('^\s+SONAME\s+(.+)$',out,re.MULTILINE)
            if result:
                return result.group(1)
            else:

                # No SONAME found:
                return ''

else:
    import ctypes
    import elftools.elf.elffile as elffile
    import elftools.construct.macros as macros
    import elftools.elf.structs as structs

    def get_soname(filename):
        """
        Retrieve SONAME of shared library.

        Parameters
        ----------
        filename : str
            Full path to shared library.

        Returns
        -------
        soname : str
            SONAME of shared library.

        Notes
        -----
        This function uses the pyelftools [ELF] package.

        References
        ----------
        .. [ELF] http://pypi.python.org/pypi/pyelftools
        
        """

        stream = open(filename, 'rb')
        f = elffile.ELFFile(stream)
        dynamic = f.get_section_by_name('.dynamic')
        dynstr = f.get_section_by_name('.dynstr')

        # Handle libraries built for different machine architectures:         
        if f.header['e_machine'] == 'EM_X86_64':
            st = structs.Struct('Elf64_Dyn',
                                macros.ULInt64('d_tag'),
                                macros.ULInt64('d_val'))
        elif f.header['e_machine'] == 'EM_386':
            st = structs.Struct('Elf32_Dyn',
                                macros.ULInt32('d_tag'),
                                macros.ULInt32('d_val'))
        else:
            raise RuntimeError('unsupported machine architecture')

        entsize = dynamic['sh_entsize']
        for k in xrange(dynamic['sh_size']/entsize):
            result = st.parse(dynamic.data()[k*entsize:(k+1)*entsize])

            # The following value for the SONAME tag is specified in elf.h:  
            if result.d_tag == 14:
                return dynstr.get_string(result.d_val)

        # No SONAME found:
        return ''

########NEW FILE########
__FILENAME__ = schedulers
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

""" A bunch of different schedulers to scale learning
parameters. These are used e.g. to slowly reduce the learning rate
during training or scale momentum up and down during the early and
late phases of training.
"""


def constant_scheduler(value):
    while True:
        yield value


def exponential_scheduler(init_value, decay):
    """ Decreases exponentially """

    value = init_value
    while True:
        yield value
        value *= decay


def linear_scheduler_up(init_value, target_value, duration):
    """ Increases linearly and then stays flat """

    value = init_value
    t = 0
    while True:
        yield value
        t += 1
        if t < duration:
            value = init_value + t * (target_value - init_value) / duration
        else:
            value = target_value


def linear_scheduler_up_down(init_value, target_value, final_value,
                             duration_up, t_decrease, duration_down):
    """ Increases linearly to target_value, stays at target_value until
    t_decrease and then decreases linearly
    """

    value = init_value
    t = 0

    while True:
        yield value
        t += 1
        if t < duration_up:
            value = init_value + t * (target_value - init_value) / \
                    float(duration_up)
        elif t > t_decrease:
            value = target_value - (t - t_decrease) * \
                    (target_value - final_value) / \
                    float(duration_down)
        else:
            value = target_value

########NEW FILE########
__FILENAME__ = call_check
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

"""
Utility functions for checking passed arguments against call signature
of a function or class constructor.
"""
import functools
import inspect
import types
from .string_utils import match

def check_call_arguments(to_call, kwargs):
    """
    Check the call signature against a dictionary of proposed arguments,
    raising an informative exception in the case of mismatch.

    Parameters
    ----------
    to_call : class or callable
        Function or class to examine (in the case of classes, the
        constructor call signature is analyzed)
    kwargs : dict
        Dictionary mapping parameter names (including positional
        arguments) to proposed values.
    """
    if 'self' in kwargs.keys():
        raise TypeError("Your dictionary includes an entry for 'self', "
                        "which is just asking for trouble")

    orig_to_call = getattr(to_call, '__name__', str(to_call))
    if not isinstance(to_call, types.FunctionType):
        if hasattr(to_call, '__init__'):
            to_call = to_call.__init__
        elif hasattr(to_call, '__call__'):
            to_call = to_call.__call__

    args, varargs, keywords, defaults = inspect.getargspec(to_call)

    if any(not isinstance(arg, str) for arg in args):
        raise TypeError('%s uses argument unpacking, which is deprecated and '
                        'unsupported by this pylearn2' % orig_to_call)

    if varargs is not None:
        raise TypeError('%s has a variable length argument list, but '
                        'this is not supported by config resolution' %
                        orig_to_call)

    if keywords is None:
        bad_keywords = [arg_name for arg_name in kwargs.keys()
                        if arg_name not in args]

        if len(bad_keywords) > 0:
            bad = ', '.join(bad_keywords)
            args = [ arg for arg in args if arg != 'self' ]
            if len(args) == 0:
                matched_str = '(It does not support any keywords, actually)'
            else:
                matched = [ match(keyword, args) for keyword in bad_keywords ]
                matched_str = 'Did you mean %s?' % (', '.join(matched))
            raise TypeError('%s does not support the following '
                            'keywords: %s. %s' %
                            (orig_to_call, bad, matched_str))

    if defaults is None:
        num_defaults = 0
    else:
        num_defaults = len(defaults)

    required = args[:len(args) - num_defaults]
    missing = [arg for arg in required if arg not in kwargs]

    if len(missing) > 0:
        #iff the im_self (or __self__) field is present, this is a
        # bound method, which has 'self' listed as an argument, but
        # which should not be supplied by kwargs
        is_bound = hasattr(to_call, 'im_self') or hasattr(to_call, '__self__')
        if len(missing) > 1 or missing[0] != 'self' or not is_bound:
            if 'self' in missing:
                missing.remove('self')
            missing = ', '.join([str(m) for m in missing])
            raise TypeError('%s did not get these expected '
                            'arguments: %s' % (orig_to_call, missing))

def checked_call(to_call, kwargs):
    """
    Attempt calling a function or instantiating a class with a given set of
    arguments, raising a more helpful exception in the case of argument
    mismatch.

    Parameters
    ----------
    to_call : class or callable
        Function or class to examine (in the case of classes, the
        constructor call signature is analyzed)
    kwargs : dict
        Dictionary mapping parameter names (including positional
        arguments) to proposed values.
    """
    try:
        return to_call(**kwargs)
    except TypeError:
        check_call_arguments(to_call, kwargs)
        raise

def sensible_argument_errors(func):
    @functools.wraps(func)
    def wrapped_func(*args, **kwargs):
        try:
            func(*args, **kwargs)
        except TypeError:
            argnames, varargs, keywords, defaults = inspect.getargspec(func)
            posargs = dict(zip(argnames, args))
            bad_keywords = []
            for keyword in kwargs:
                if keyword not in argnames:
                    bad_keywords.append(keyword)

            if len(bad_keywords) > 0:
                bad = ', '.join(bad_keywords)
                raise TypeError('%s() does not support the following '
                                'keywords: %s' % (str(func.func_name), bad))
            allargsgot = set(list(kwargs.keys()) + list(posargs.keys()))
            numrequired = len(argnames) - len(defaults)
            diff = list(set(argnames[:numrequired]) - allargsgot)
            if len(diff) > 0:
                raise TypeError('%s() did not get required args: %s' %
                                (str(func.func_name), ', '.join(diff)))
            raise
    return wrapped_func


########NEW FILE########
__FILENAME__ = environ
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

#Utilities for working with environment variables
import os

def putenv(key, value):
    #this makes the change visible to other parts of the code
    #in this same process
    os.environ[key] = value
    # this makes it available to any subprocesses we launch
    os.putenv(key, value)


########NEW FILE########
__FILENAME__ = exc
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

__author__ = "Ian Goodfellow"
"""
Exceptions related to datasets
"""

class EnvironmentVariableError(Exception):
    """ An exception raised when a required environment variable is not defined """

    def __init__(self, *args):
        super(EnvironmentVariableError,self).__init__(*args)

class NoDataPathError(EnvironmentVariableError):
    """
    Exception raised when PYLEARN2_DATA_PATH is required but has not been
    defined.
    """
    def __init__(self):
        super(NoDataPathError, self).__init__(data_path_essay)

data_path_essay = """\
You need to define your PYLEARN2_DATA_PATH environment variable. If you are
using a computer at LISA, this should be set to /data/lisa/data.
"""

class NotInstalledError(Exception):
    """
    Exception raised when a dataset appears not to be installed.
    This is different from an individual file missing within a dataset,
    the file not loading correctly, etc.
    This exception is used to make unit tests skip testing of datasets
    that haven't been installed.
    We do want the unit test to run and crash if the dataset is installed
    incorrectly.
    """

########NEW FILE########
__FILENAME__ = math
ceil_div = lambda x, y: int((x + y - 1) / y)
########NEW FILE########
__FILENAME__ = plotting
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from pycuda import gpuarray
import numpy as np
from math import ceil

def show_filters(W, img_dims, columns=10, normalize=True, **kwargs):
    import matplotlib.pyplot as plt
    if isinstance(W, gpuarray.GPUArray): W = W.get()

    D, N = W.shape

    if normalize:
        W = W - W.min() #[np.newaxis,:]
        W = W / W.max() #[np.newaxis,:]

    rows = int(ceil(N / columns))
        
    fig = plt.figure(1, **kwargs)
    plt.subplots_adjust(left=0., right=.51, wspace=.1, hspace=.01)    

    filters = np.rollaxis(W.reshape(img_dims + (N,)), 2)
    filters = np.vstack([np.hstack(filters[i:i+columns]) for i in range(0, N, columns)])
    plt.axis('off')
    plt.imshow(filters, cmap=plt.cm.gray, interpolation='nearest', figure=fig)

########NEW FILE########
__FILENAME__ = serial
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import cPickle
import pickle
import numpy as np
import os
import time
import warnings
import sys
from .string_utils import preprocess
from cPickle import BadPickleGet
io = None
hdf_reader = None
import struct
from . import environ
from .string_utils import match
import shutil

def raise_cannot_open(path):
    pieces = path.split('/')
    for i in xrange(1,len(pieces)+1):
        so_far = '/'.join(pieces[0:i])
        if not os.path.exists(so_far):
            if i == 1:
                if so_far == '':
                    continue
                raise IOError('Cannot open '+path+' ('+so_far+' does not exist)')
            parent = '/'.join(pieces[0:i-1])
            bad = pieces[i-1]

            if not os.path.isdir(parent):
                raise IOError("Cannot open "+path+" because "+parent+" is not a directory.")

            candidates = os.listdir(parent)

            if len(candidates) == 0:
                raise IOError("Cannot open "+path+" because "+parent+" is empty.")

            if len(candidates) > 100:
                # Don't attempt to guess the right name if the directory is huge
                raise IOError("Cannot open "+path+" but can open "+parent+".")

            if os.path.islink(path):
                raise IOError(path + " appears to be a symlink to a non-existent file")
            raise IOError("Cannot open "+path+" but can open "+parent+". Did you mean "+match(bad,candidates)+" instead of "+bad+"?")
        # end if
    # end for
    assert False

def load(filepath, recurse_depth=0, retry = True):

    try:
        import joblib
        joblib_available = True
    except ImportError:
        joblib_available = False
    if recurse_depth == 0:
        filepath = preprocess(filepath)

    if filepath.endswith('.npy'):
        return np.load(filepath)

    if filepath.endswith('.mat'):
        global io
        if io is None:
            import scipy.io
            io = scipy.io
        try:
            return io.loadmat(filepath)
        except NotImplementedError, nei:
            if str(nei).find('HDF reader') != -1:
                global hdf_reader
                if hdf_reader is None:
                    import h5py
                    hdf_reader = h5py
                return hdf_reader.File(filepath)
            else:
                raise
        #this code should never be reached
        assert False

    def exponential_backoff():
        if recurse_depth > 9:
            print ('Max number of tries exceeded while trying to open ' +
                   filepath)
            print 'attempting to open via reading string'
            f = open(filepath, 'rb')
            lines = f.readlines()
            f.close()
            content = ''.join(lines)
            return cPickle.loads(content)
        else:
            nsec = 0.5 * (2.0 ** float(recurse_depth))
            print "Waiting " + str(nsec) + " seconds and trying again"
            time.sleep(nsec)
            return load(filepath, recurse_depth + 1, retry)

    try:
        if not joblib_available:
            with open(filepath, 'rb') as f:
                obj = cPickle.load(f)
        else:
            try:
                obj = joblib.load(filepath)
            except Exception, e:
                if os.path.exists(filepath) and not os.path.isdir(filepath):
                    raise
                raise_cannot_open(filepath)


    except BadPickleGet, e:
        print ('Failed to open ' + str(filepath) +
               ' due to BadPickleGet with exception string ' + str(e))

        if not retry:
            raise
        obj =  exponential_backoff()
    except EOFError, e:

        print ('Failed to open ' + str(filepath) +
               ' due to EOFError with exception string ' + str(e))

        if not retry:
            raise
        obj =  exponential_backoff()
    except ValueError, e:
        print ('Failed to open ' + str(filepath) +
               ' due to ValueError with string ' + str(e))

        if not retry:
            raise
        obj =  exponential_backoff()
    except Exception, e:
        #assert False
        exc_str = str(e)
        if len(exc_str) > 0:
            import pdb
            tb = pdb.traceback.format_exc()
            raise Exception("Couldn't open '" + str(filepath) +
                            "' due to: " + str(type(e)) + ', ' + str(e) +
                            ". Orig traceback:\n" + tb)
        else:
            print ("Couldn't open '" + str(filepath) +
                   "' and exception has no string. Opening it again outside "
                   "the try/catch so you can see whatever error it prints on "
                   "its own.")
            f = open(filepath, 'rb')
            obj = cPickle.load(f)
            f.close()

    #if the object has no yaml_src, we give it one that just says it
    #came from this file. could cause trouble if you save obj again
    #to a different location
    if not hasattr(obj,'yaml_src'):
        try:
            obj.yaml_src = '!pkl: "'+os.path.abspath(filepath)+'"'
        except:
            pass

    return obj


def save(filepath, obj, on_overwrite = 'ignore'):
    """
    Serialize `object` to a file denoted by `filepath`.

    Parameters
    ----------
    filepath : str
        A filename. If the suffix is `.joblib` and joblib can be
        imported, `joblib.dump` is used in place of the regular
        pickling mechanisms; this results in much faster saves by
        saving arrays as separate .npy files on disk. If the file
        suffix is `.npy` than `numpy.save` is attempted on `obj`.
        Otherwise, (c)pickle is used.

    obj : object
        A Python object to be serialized.

    on_overwrite: A string specifying what to do if the file already
                exists.
                ignore: just overwrite it
                backup: make a copy of the file (<filepath>.bak) and
                        delete it when done saving the new copy.
                        this allows recovery of the old version of
                        the file if saving the new one fails
    """


    filepath = preprocess(filepath)

    if os.path.exists(filepath):
        if on_overwrite == 'backup':
            backup = filepath + '.bak'
            shutil.move(filepath, backup)
            save(filepath, obj)
            try:
                os.remove(backup)
            except Exception, e:
                warnings.warn("Got an error while traing to remove "+backup+":"+str(e))
            return
        else:
            assert on_overwrite == 'ignore'


    try:
        _save(filepath, obj)
    except RuntimeError, e:
        """ Sometimes for large theano graphs, pickle/cPickle exceed the
            maximum recursion depth. This seems to me like a fundamental
            design flaw in pickle/cPickle. The workaround I employ here
            is the one recommended to someone who had a similar problem
            on stackexchange:

            http://stackoverflow.com/questions/2134706/hitting-maximum-recursion-depth-using-pythons-pickle-cpickle

            Obviously this does not scale and could cause a crash
            but I don't see another solution short of writing our
            own implementation of pickle.
        """
        if str(e).find('recursion') != -1:
            warnings.warn('pylearn2.utils.save encountered the following '
                          'error: ' + str(e) +
                          '\nAttempting to resolve this error by calling ' +
                          'sys.setrecusionlimit and retrying')
            old_limit = sys.getrecursionlimit()
            try:
                sys.setrecursionlimit(50000)
                _save(filepath, obj)
            finally:
                sys.setrecursionlimit(old_limit)


def get_pickle_protocol():
    """
    Allow configuration of the pickle protocol on a per-machine basis.
    This way, if you use multiple platforms with different versions of
    pickle, you can configure each of them to use the highest protocol
    supported by all of the machines that you want to be able to
    communicate.
    """
    try:
        protocol_str = os.environ['PYLEARN2_PICKLE_PROTOCOL']
    except KeyError:
        # If not defined, we default to 0 because this is the default
        # protocol used by cPickle.dump (and because it results in
        # maximum portability)
        protocol_str = '0'
    if protocol_str == 'pickle.HIGHEST_PROTOCOL':
        return pickle.HIGHEST_PROTOCOL
    return int(protocol_str)

def _save(filepath, obj):
    try:
        import joblib
        joblib_available = True
    except ImportError:
        joblib_available = False
    if filepath.endswith('.npy'):
        np.save(filepath, obj)
        return
    # This is dumb
    # assert filepath.endswith('.pkl')
    save_dir = os.path.dirname(filepath)
    # Handle current working directory case.
    if save_dir == '':
        save_dir = '.'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    if os.path.exists(save_dir) and not os.path.isdir(save_dir):
        raise IOError("save path %s exists, not a directory" % save_dir)
    elif not os.access(save_dir, os.W_OK):
        raise IOError("permission error creating %s" % filepath)
    try:
        if joblib_available and filepath.endswith('.joblib'):
            joblib.dump(obj, filepath)
        else:
            if filepath.endswith('.joblib'):
                warnings.warn('Warning: .joblib suffix specified but joblib '
                              'unavailable. Using ordinary pickle.')
            with open(filepath, 'wb') as filehandle:
                cPickle.dump(obj, filehandle, get_pickle_protocol())
    except Exception, e:
        # TODO: logging, or warning
        print "cPickle has failed to write an object to " + filepath
        if str(e).find('maximum recursion depth exceeded') != -1:
            raise
        try:
            # TODO: logging, or warning
            print 'retrying with pickle'
            with open(filepath, "wb") as f:
                pickle.dump(obj, f)
        except Exception, e2:
            if str(e) == '' and str(e2) == '':
                # TODO: logging, or warning
                print (
                    'neither cPickle nor pickle could write to %s' % filepath
                )
                print (
                    'moreover, neither of them raised an exception that '
                    'can be converted to a string'
                )
                print (
                    'now re-attempting to write with cPickle outside the '
                    'try/catch loop so you can see if it prints anything '
                    'when it dies'
                )
                with open(filepath, 'wb') as f:
                    cPickle.dump(obj, f, get_pickle_protocol())
                print ('Somehow or other, the file write worked once '
                       'we quit using the try/catch.')
            else:
                if str(e2) == 'env':
                    raise

                import pdb
                tb = pdb.traceback.format_exc()
                raise IOError(str(obj) +
                              ' could not be written to '+
                              str(filepath) +
                              ' by cPickle due to ' + str(e) +
                              ' nor by pickle due to ' + str(e2) +
                              '. \nTraceback '+ tb)
        print ('Warning: ' + str(filepath) +
               ' was written by pickle instead of cPickle, due to '
               + str(e) +
               ' (perhaps your object is really big?)')


def clone_via_serialize(obj):
    s = cPickle.dumps(obj, get_pickle_protocol())
    return cPickle.loads(s)


def to_string(obj):
    return cPickle.dumps(obj, get_pickle_protocol())

def from_string(s):
    return cPickle.loads(s)


def mkdir(filepath):
    """
    Make a directory. Should succeed even if it needs to make more than one
    directory and nest subdirectories to do so. Raises an error if the
    directory can't be made. Does not raise an error if the directory
    already exists.
    """
    try:
        os.makedirs(filepath)
    except:
        if not os.path.isdir(filepath):
            raise

def read_int( fin, n = 1):
    if n == 1:
        s = fin.read(4)
        if len(s) != 4:
            raise ValueError('fin did not contain 4 bytes')
        return struct.unpack('i', s)[0]
    else:
        rval = []
        for i in xrange(n):
            rval.append(read_int(fin))
        return rval

#dictionary to convert lush binary matrix magic numbers
#to dtypes
lush_magic = {
            507333717 : 'uint8',
            507333716 : 'int32',
            507333713 : 'float32',
            507333715 : 'float64'
        }

def read_bin_lush_matrix(filepath):
    f = open(filepath,'rb')
    try:
        magic = read_int(f)
    except ValueError:
        raise ValueError("Couldn't read magic number")
    ndim = read_int(f)

    if ndim == 0:
        shape = ()
    else:
        shape = read_int(f, max(3, ndim))

    total_elems = 1
    for dim in shape:
        total_elems *= dim

    try:
        dtype = lush_magic[magic]
    except KeyError:
        raise ValueError('Unrecognized lush magic number '+str(magic))

    rval = np.fromfile(file = f, dtype = dtype, count = total_elems)

    excess = f.read(-1)

    if excess != '':
        raise ValueError(str(len(excess))+' extra bytes found at end of file.'
                ' This indicates  mismatch between header and content')

    rval = rval.reshape(*shape)

    f.close()

    return rval

def load_train_file(config_file_path):
    """Loads and parses a yaml file for a Train object.
    Publishes the relevant training environment variables"""
    from pylearn2.config import yaml_parse

    suffix_to_strip = '.yaml'

    # publish environment variables related to file name
    if config_file_path.endswith(suffix_to_strip):
        config_file_full_stem = config_file_path[0:-len(suffix_to_strip)]
    else:
        config_file_full_stem = config_file_path

    for varname in ["PYLEARN2_TRAIN_FILE_NAME", #this one is deprecated
            "PYLEARN2_TRAIN_FILE_FULL_STEM"]: #this is the new, accepted name
        environ.putenv(varname, config_file_full_stem)

    directory = config_file_path.split('/')[:-1]
    directory = '/'.join(directory)
    if directory != '':
        directory += '/'
    environ.putenv("PYLEARN2_TRAIN_DIR", directory)
    environ.putenv("PYLEARN2_TRAIN_BASE_NAME", config_file_path.split('/')[-1] )
    environ.putenv("PYLEARN2_TRAIN_FILE_STEM", config_file_full_stem.split('/')[-1] )

    return yaml_parse.load_path(config_file_path)

########NEW FILE########
__FILENAME__ = string_utils
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

""" Utilities for modifying strings"""

import os
import warnings
import re
import functools

from .exc import EnvironmentVariableError, NoDataPathError

def preprocess(string):
    """
    Preprocesses a string, by replacing ${VARNAME} with
    os.environ['VARNAME']

    Parameters
    ----------
    string: the str object to preprocess

    Returns
    -------
    the preprocessed string
    """

    split = string.split('${')

    rval = [split[0]]

    for candidate in split[1:]:
        subsplit = candidate.split('}')

        if len(subsplit) < 2:
            raise ValueError('Open ${ not followed by } before ' \
                    + 'end of string or next ${ in "' \
                    + string + '"')

        varname = subsplit[0]

        if varname == 'PYLEARN2_TRAIN_FILE_NAME':
            warnings.warn("PYLEARN2_TRAIN_FILE_NAME is deprecated and may be "
                    "removed from the library on or after Oct 22, 2013. Switch"
                    " to PYLEARN2_TRAIN_FILE_FULL_STEM")

        try:
            val = os.environ[varname]
        except KeyError:
            if varname == 'PYLEARN2_DATA_PATH':
                raise NoDataPathError()
            if varname == 'PYLEARN2_VIEWER_COMMAND':
                raise EnvironmentVariableError(environment_variable_essay)

            raise ValueError('Unrecognized environment variable "' + varname
                    + '". Did you mean ' + match(varname, os.environ.keys())
                    + '?')

        rval.append(val)

        rval.append('}'.join(subsplit[1:]))

    rval = ''.join(rval)

    return rval




def find_number(s):
    """ s is a string
        returns None if there are no numbers in the string
        otherwise returns the range of characters occupied by the first
        number in the string """

    r = re.search('-?\d+[.e]?\d*',s)
    if r is not None:
        return r.span(0)
    return None

def tokenize_by_number(s):
    """ splits a string into a list of tokens
        each is either a string containing no numbers
        or a float """

    r = find_number(s)

    if r == None:
        return [ s ]
    else:
        tokens = []
        if r[0] > 0:
            tokens.append(s[0:r[0]])
        tokens.append( float(s[r[0]:r[1]]) )
        if r[1] < len(s):
            tokens.extend(tokenize_by_number(s[r[1]:]))
        return tokens
    assert False #line should be unreached


def number_aware_alphabetical_cmp(str1, str2):
    """ cmp function for sorting a list of strings by alphabetical order, but with
        numbers sorted numerically.

        i.e., foo1, foo2, foo10, foo11
        instead of foo1, foo10
    """

    def flatten_tokens(tokens):
        l = []
        for token in tokens:
            if isinstance(token, str):
                for char in token:
                    l.append(char)
            else:
                assert isinstance(token, float)
                l.append(token)
        return l

    seq1 = flatten_tokens(tokenize_by_number(str1))
    seq2 = flatten_tokens(tokenize_by_number(str2))

    l = min(len(seq1),len(seq2))

    i = 0

    while i < l:
        if seq1[i] < seq2[i]:
            return -1
        elif seq1[i] > seq2[i]:
            return 1
        i += 1

    if len(seq1) < len(seq2):
        return -1
    elif len(seq1) > len(seq2):
        return 1

    return 0

def match(wrong, candidates):
    """
        wrong: a mispelling
        candidates: a set of correct words

        returns a guess of which candidate is the right one

        This should be used with a small number of candidates and a high potential
        edit distance.
        ie, use it to correct a wrong filename in a directory, wrong class name
        in a module, etc. Don't use it to correct small typos of freeform natural
        language words.
    """

    assert len(candidates) > 0

    # Current implementation tries all candidates and outputs the one
    # with the min score
    # Could try to do something smarter

    def score(w1,w2):
        # Current implementation returns negative dot product of
        # the two words mapped into a feature space by mapping phi
        # w -> [ phi(w1), .1 phi(first letter of w), .1 phi(last letter of w) ]
        # Could try to do something smarter

        w1 = w1.lower()
        w2 = w2.lower()

        def phi(w):
            # Current feature mapping is to the vector of counts of
            # all letters and two-letter sequences
            # Could try to do something smarter
            rval = {}

            for i in xrange(len(w)):
                l = w[i]
                rval[l] = rval.get(l,0.) + 1.
                if i < len(w)-1:
                    b = w[i:i+2]
                    rval[b] = rval.get(b,0.) + 1.

            return rval

        d1 = phi(w1)
        d2 = phi(w2)

        def mul(d1, d2):
            rval = 0

            for key in set(d1).union(d2):
                rval += d1.get(key,0) * d2.get(key,0)

            return rval

        tot_score = mul(phi(w1),phi(w2)) / float(len(w1)*len(w2)) + \
            0.1 * mul(phi(w1[0:1]), phi(w2[0:1])) + \
            0.1 * mul(phi(w1[-1:]), phi(w2[-1:]))

        return  tot_score

    scored_candidates = [ (-score(wrong, candidate), candidate)
            for candidate in candidates ]

    scored_candidates.sort()

    return scored_candidates[0][1]

def censor_non_alphanum(s):
    """
    Returns s with all non-alphanumeric characters replaced with *
    """

    def censor(ch):
        if (ch >= 'A' and ch <= 'z') or (ch >= '0' and ch <= '9'):
            return ch
        return '*'

    return ''.join([censor(ch) for ch in s])

environment_variable_essay = """
PYLEARN2_VIEWER_COMMAND not defined. PLEASE READ THE FOLLOWING MESSAGE CAREFULLY
TO SET UP THIS ENVIRONMENT VARIABLE:

pylearn2 uses an external program to display images. Because different systems have different
image programs available, pylearn2 requires the user to specify what image viewer program to
use.

You need to choose an image viewer program that pylearn2 should use. Then tell pylearn2 to use
that image viewer program by defining your PYLEARN2_VIEWER_COMMAND environment variable.

You need to choose PYLEARN_VIEWER_COMMAND such that running

${PYLEARN2_VIEWER_COMMAND} image.png

in a command prompt on your machine will do the following:
    -open an image viewer in a new process.
    -not return until you have closed the image.

Acceptable commands include:
    gwenview
    eog --new-instance

This is assuming that you have gwenview or a version of eog that supports --new-instance
installed on your machine. If you don't, install one of those, or figure out a command
that has the above properties that is available from your setup.

On most linux setups, you can define your environment variable by adding this line to your
~/.bashrc file:

export PYLEARN2_VIEWER_COMMAND="eog --new-instance"

*** YOU MUST INCLUDE THE WORD "export". DO NOT JUST ASSIGN TO THE ENVIRONMENT VARIABLE ***
If you do not include the word "export", the environment variable will be set in your
bash shell, but will not be visible to processes that you launch from it, like the python
interpreter.

Don't forget that changes from your .bashrc file won't apply until you run

source ~/.bashrc

or open a new terminal window. If you're seeing this from an ipython notebook
you'll need to restart the ipython notebook, or maybe modify os.environ from
an ipython cell.
"""

########NEW FILE########
__FILENAME__ = version
version = '0.03-dev'
release = '0.02'
########NEW FILE########
__FILENAME__ = hebel_test
# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import hebel
hebel.init(0)

import unittest
import random
import numpy as np
from pycuda import gpuarray
from pycuda.curandom import rand as curand
from hebel import sampler
from hebel.models import NeuralNet, NeuralNetRegression
from hebel.optimizers import SGD
from hebel.parameter_updaters import SimpleSGDUpdate, \
    MomentumUpdate, NesterovMomentumUpdate
from hebel.data_providers import MNISTDataProvider, BatchDataProvider
from hebel.monitors import SimpleProgressMonitor
from hebel.schedulers import exponential_scheduler, linear_scheduler_up, \
    constant_scheduler
from hebel.pycuda_ops.matrix import extract_columns, insert_columns
from hebel.pycuda_ops.elementwise import sample_dropout_mask


class TestNeuralNetMNIST(unittest.TestCase):
    def setUp(self):
        self.train_data = MNISTDataProvider('train', 100)
        self.test_data = MNISTDataProvider('test')
        self.D = self.train_data.D
        self.n_out = 10

    def test_relu(self):
        model = NeuralNet(n_in=self.D, n_out=self.n_out,
                          layers=[1000], activation_function='relu',
                          dropout=True)
        optimizer = SGD(model, SimpleSGDUpdate, self.train_data,
                        self.test_data,
                        learning_rate_schedule=exponential_scheduler(1., .99),
                        progress_monitor=SimpleProgressMonitor())
        optimizer.run(20)
        self.assertLess(optimizer.progress_monitor.train_error[-1][1],
                        optimizer.progress_monitor.train_error[0][1])
        del model, optimizer

    def test_momentum(self):
        model = NeuralNet(n_in=self.D, n_out=self.n_out,
                          layers=[1000], activation_function='relu',
                          dropout=True)
        optimizer = SGD(model, MomentumUpdate, self.train_data,
                        self.test_data,
                        learning_rate_schedule=exponential_scheduler(1., .99),
                        momentum_schedule=linear_scheduler_up(.5, .9, 5),
                        progress_monitor=SimpleProgressMonitor())
        optimizer.run(20)
        self.assertLess(optimizer.progress_monitor.train_error[-1][1],
                        optimizer.progress_monitor.train_error[0][1])
        del model, optimizer

    def test_nesterov_momentum(self):
        model = NeuralNet(n_in=self.D, n_out=self.n_out,
                          layers=[100], activation_function='relu',
                          dropout=True)
        optimizer = SGD(model, NesterovMomentumUpdate, self.train_data,
                        self.test_data,
                        learning_rate_schedule=exponential_scheduler(1., .99),
                        momentum_schedule=linear_scheduler_up(.5, .9, 5),
                        progress_monitor=SimpleProgressMonitor())
        optimizer.run(20)
        self.assertLess(optimizer.progress_monitor.train_error[-1][1],
                        optimizer.progress_monitor.train_error[0][1])
        del model, optimizer


class TestColumnSlicing(unittest.TestCase):
    def test_extract_columns(self):
        for _ in range(20):
            dtype = random.choice((np.float32, np.float64))
            N = np.random.randint(100, 1000)
            M = np.random.randint(100, 1000)
            a = np.random.randint(0, M)
            b = np.random.randint(a + 1, M)
            m = b - a
            assert m > 0

            X = curand((N, M), dtype)
            Y = extract_columns(X, a, b)

            self.assertTrue(np.all(X.get()[:, a:b] == Y.get()))

    def test_insert_columns(self):
        for _ in range(20):
            dtype = random.choice((np.float32, np.float64))
            N = np.random.randint(100, 1000)
            M = np.random.randint(100, 1000)
            m = np.random.randint(1, M)
            offset = np.random.randint(0, M - m)

            X = curand((N, M), dtype)
            Y = curand((N, m), dtype)
            insert_columns(Y, X, offset)

            self.assertTrue(np.all(X.get()[:, offset:offset+m] == Y.get()))


class TestSampleDropoutMask(unittest.TestCase):
    TOL = 1e-3

    def test_sample_dropout_mask(self):
        for _ in range(20):
            height = 1000
            width = 10000
            dropout_prob = np.random.rand()
            X = sampler.gen_uniform((height, width), np.float32)
            dropout_mask = sample_dropout_mask(X, dropout_prob)
            dropout_rate = 1. - dropout_mask.get().mean()

            self.assertLess(np.abs(dropout_prob - dropout_rate), self.TOL)
            self.assertTrue(np.all((X.get() != 0.) == dropout_mask.get()))

    def test_sample_dropout_mask_columns(self):
        for _ in range(20):
            height = 10000
            width = 10000
            dropout_prob = np.random.rand()
            X = sampler.gen_uniform((height, width), np.float32)

            start = np.random.randint(0, width - 1000)
            end = start + 1000
            columns = (start, end)

            dropout_mask = sample_dropout_mask(X, dropout_prob, columns)
            dropout_rate = 1. - dropout_mask.get().mean()

            self.assertEqual(dropout_mask.shape, (X.shape[0], end - start))
            self.assertLess(np.abs(dropout_prob - dropout_rate),
                            self.TOL)
            self.assertTrue(np.all((X.get()[:, start:end] != 0.)
                                   == dropout_mask.get()))

class TestNeuralNetRegression(unittest.TestCase):
    def test_neural_net_regression(self):
        for _ in range(20):
            N = 10000    # Number of data points
            D = 100      # Dimensionality of exogenous data
            P = 50       # Dimensionality of endogenous data

            W_true = 10 * np.random.rand(D, P) - 5
            b_true = 100 * np.random.rand(P) - 50

            X = np.random.randn(N, D)
            Y = np.dot(X, W_true) + b_true[np.newaxis, :] + np.random.randn(N, P)        

            W_lstsq = np.linalg.lstsq(np.c_[np.ones((N, 1)), X], Y)[0]
            b_lstsq = W_lstsq[0]
            W_lstsq = W_lstsq[1:]

            data_provider = BatchDataProvider(gpuarray.to_gpu(X.astype(np.float32)),
                                              gpuarray.to_gpu(Y.astype(np.float32)))

            model = NeuralNetRegression([], n_in=D, n_out=P)
            optimizer = SGD(model, SimpleSGDUpdate, 
                            data_provider, data_provider,
                            learning_rate_schedule=constant_scheduler(10.),
                            early_stopping=True)
            optimizer.run(100)

            self.assertLess(np.abs(W_lstsq - model.top_layer.W.get()).max(),
                            1e-5)
        
if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = train_model

# Copyright (C) 2013  Hannes Bretschneider

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from hebel.config import run_from_config

description = """ Run this script with a yaml configuration file as input.
E.g.:

python train_model.py examples/mnist_neural_net_deep.yml

"""

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('config_file')
    args = parser.parse_args()

    if not args.config_file.endswith('.yml') and not args.config_file.endswith('.yaml'):
        args.config_file = args.config_file + '.yml'

    yaml_src = ''.join(open(args.config_file).readlines())

    run_from_config(yaml_src)

########NEW FILE########
