__FILENAME__ = celeryconfig
BROKER_HOST = "localhost"
BROKER_PORT = 5672
# BROKER_USER = "myuser"
# BROKER_PASSWORD = "mypassword"
# BROKER_VHOST = "myvhost"

# CELERY_RESULT_BACKEND = "database"
# CELERY_RESULT_DBURI = "postgresql:///openlibrary"
CELERY_RESULT_BACKEND = "couchdb"
CELERY_RESULT_DBURI = "http://localhost:5984/celery"


CELERY_IMPORTS = ("openlibrary.tasks",)

# These two files need to be separately mentioned since the tasks will
# run in the celery workers
OL_CONFIG = "conf/openlibrary.yml"

# Repeating tasks
from datetime import timedelta


from celery.backends import BACKEND_ALIASES
BACKEND_ALIASES['couchdb'] = "openlibrary.core.celery_couchdb.CouchDBBackend"

CELERY_ALWAYS_EAGER = True

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get('assignee'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [int(doc.get('_id').replace("case-","")),
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get('created'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get('creator_name'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get('history',[{}])[-1].get("at","") 
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [len(doc.get('history',[{}]))
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get('status'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = reduce
_count

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get('subject'),
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get("status"),
               doc.get('assignee'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get("status"),
               int(doc.get('_id').replace("case-","")),
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get("status"),
               doc.get('created'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get("status"),
               doc.get('creator_name'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get("status"),
               doc.get('history',[{}])[-1].get("at","") 
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get("status"),
               len(doc.get('history',[{}]))
               ], 1

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get('status'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = reduce
_count

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "case":
        yield [doc.get("status"),
               doc.get('subject'), 
               doc.get('history',[{}])[-1].get("at","")
               ], 1

########NEW FILE########
__FILENAME__ = map
def _map(doc):
    if not doc['_id'].startswith("/books/"):
        return

    counts = {}

    for k, count in doc['loans'].items():
        yyyy, mm = k.split("-")

        # store overall, per-year and per-month counts
        counts[""] = counts.get("", 0) + count
        counts[yyyy] = counts.get(yyyy, 0) + count
        counts[k] = counts.get(k, 0) + count

    for k, v in counts.items():
        yield [k, v], 1

########NEW FILE########
__FILENAME__ = reduce
_sum
########NEW FILE########
__FILENAME__ = map
def _map(doc):
    if not doc['_id'].startswith("loans/"):
        return

    import re, datetime
    def parse_datetime(datestring):
        """Parses from isoformat.
        Is there any way to do this in stdlib?
        """
        tokens = re.split('-|T|:|\.| ', datestring)
        return datetime.datetime(*map(int, tokens))
        
    if doc.get('status') == 'completed':
        t_start = parse_datetime(doc['t_start'])
        t_end = parse_datetime(doc['t_end'])
        delta = t_end - t_start
        duration = delta.days * 24 * 60 + delta.seconds / 60 # duration in minutes
        
        yield [t_end.year, t_end.month, t_end.day], duration
########NEW FILE########
__FILENAME__ = reduce
def _reduce(keys, values, rereduce=False):
    if rereduce:
        sum = 0.0
        freq = {}
        count = 0
        for d in values:
            sum += d['avg'] * d['count']
            for k, v in d['freq'].items():
                freq[k] = freq.get(k, 0) + v
            count += d['count']
        return {"avg": sum/count, "freq": freq, "count": count}
    else:
        sum = 0.0
        freq = {}
        for value in values:
            sum += value
            freq[value/60] = freq.get(value/60, 0) + 1
        return {"avg": sum/len(values), "freq": freq, "count": len(values)}
########NEW FILE########
__FILENAME__ = map
def _map(doc):
    lib = doc.get("library")
    status = doc.get("status")
    if lib:
        yield [lib, status], 1

########NEW FILE########
__FILENAME__ = reduce
_sum

########NEW FILE########
__FILENAME__ = map
def _map(doc):
    if not doc['_id'].startswith("loans/"):
        return

    import re, datetime
    def parse_datetime(datestring):
        """Parses from isoformat.
        Is there any way to do this in stdlib?
        """
        tokens = re.split('-|T|:|\.| ', datestring)
        return datetime.datetime(*map(int, tokens))

    if 't_start' in doc:
        t = parse_datetime(doc['t_start'])        
        yield ["", t.year, t.month, t.day], {doc['resource_type']: 1, "total": 1}
        if "library" in doc:
            yield [doc["library"], t.year, t.month, t.day], {doc['resource_type']: 1, "total": 1}

########NEW FILE########
__FILENAME__ = reduce
def _reduce(keys, values, rereduce):
    from collections import defaultdict
    d = defaultdict(lambda: 0)
    
    for v in values:
        for k, count in v.items():
            d[k] = d[k] + count
    
    return d
########NEW FILE########
__FILENAME__ = map
def _map(doc):
    if not doc['_id'].startswith("/people/"):
        return

    counts = {}

    for k, count in doc['loans'].items():
        yyyy, mm = k.split("-")

        # store overall, per-year and per-month counts
        counts[""] = counts.get("", 0) + count
        counts[yyyy] = counts.get(yyyy, 0) + count
        counts[k] = counts.get(k, 0) + count

    for k, v in counts.items():
        yield [k, v], 1

########NEW FILE########
__FILENAME__ = reduce
_sum
########NEW FILE########
__FILENAME__ = map
def map(doc):
    ctx = doc.get("context")
    if isinstance(ctx, dict):
        for i in ctx.get("keys",[]):
            yield [i,doc.get("started_at","")], None

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if doc.get("type","") == "task":
        yield [doc["command"], doc["finished_at"]], 1
        yield [None, doc['finished_at']], 1


########NEW FILE########
__FILENAME__ = map
def fun(doc):
    if "ocaid" in doc:
        yield doc["_id"], None 

########NEW FILE########
__FILENAME__ = reduce
_count

########NEW FILE########
__FILENAME__ = map
def fun(doc):
    if "covers" in doc:
        yield doc["_id"], None

########NEW FILE########
__FILENAME__ = reduce
_count

########NEW FILE########
__FILENAME__ = map
def map(doc):
    last_modified = doc.get("last_modified", {}).get("value", "")
    covers = doc.get("covers") or [-1]
    
    cover_value = {"cover": covers[0], 'last_modified': last_modified}
    
    d = {}
    d.update(doc.get("identifiers", {}))
    
    d.update({
        "isbn": doc.get("isbn_10", [])  + doc.get("isbn_13", []),
        "lccn": doc.get("lccn", []),
        "oclc": doc.get("oclc_numbers", []),
    })
    
    d['isbn'] = [isbn.upper().replace("-", "") for isbn in d['isbn']]
    
    d['ia'] = [s[len("ia:"):] for s in doc.get("source_records", []) if s.startswith("ia:")]
    if 'ocaid' in doc and doc['ocaid'] not in d['ia']:
        d['ia'].append(doc['ocaid'])

    d['olid'] = [doc['key'].split("/")[-1]]

    for name, values in d.items():
        for v in values:
            yield [name, v], cover_value

########NEW FILE########
__FILENAME__ = map
def map(doc):
    t = doc.get("dirty")
    if t:
        yield t, doc.get("works")

########NEW FILE########
__FILENAME__ = map
def map(doc):
    if 'works' in doc:
        yield doc['works'], None
########NEW FILE########
__FILENAME__ = map
import re
def map(doc, re_subject=re.compile("[, _]+")):
    def get_subject_key(prefix, subject):
        if isinstance(subject, basestring):
            key = prefix + re_subject.sub("_", subject.lower()).strip("_")
            return key

    type = doc.get('type', {}).get('key')
    if type == "/type/work":
        work = doc
        editions = work.get('editions', [])
        ebooks = sum(1 for e in editions if 'ocaid' in e)
    
        dates = [d['last_modified']['value'] 
                    for d in [work] + editions
                    if 'last_modified' in d]
        last_modified = max(dates or [""])
        
        subjects = {
            "subjects": work.get("subjects"),
            "people": work.get("subject_people"),
            "places": work.get("subject_places"),
            "times": work.get("subject_times")
        }
        # strip off empty values
        subjects = dict((k, v) for k, v in subjects.items() if v)
        
        counts = [1, len(editions), ebooks, last_modified, subjects]

        # work
        yield doc['key'], counts

        # editions
        for e in doc.get("editions", []):
            yield e['key'], [1, 1, int('ocaid' in e), e.get("last_modified", {}).get('value', ''), subjects]

        # authors
        for a in doc.get('authors', []):
            if 'author' in a:
                yield a['author']['key'], counts

        prefixes = {
            "subjects": "subject:",
            "people": "person:",
            "places": "place:",
            "times": "time:"
        }
        for name, values in subjects.items():
            prefix = prefixes[name]
            for s in values:
                key = get_subject_key(prefix, s)
                if key:
                    yield key, counts[:-1] + [{name: [s]}] 
del re

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# supervisor documentation build configuration file
#
# This file is execfile()d with the current directory set to its containing
# dir.
#
# The contents of this file are pickled, so don't put values in the
# namespace that aren't pickleable (module imports are okay, they're
# removed automatically).
#
# All configuration values have a default value; values that are commented
# out serve to show the default value.

import sys, os
from datetime import date

# If your extensions are in another directory, add it here. If the
# directory is relative to the documentation root, use os.path.abspath to
# make it absolute, like shown here.
#sys.path.append(os.path.abspath('some/directory'))

parent = os.path.dirname(os.path.dirname(__file__))
sys.path.append(parent)
wd = os.getcwd()

# General configuration
# ---------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['.templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General substitutions.
project = 'openlibrary'
year = date.today().year
copyright = '2006-%d, Internet Archive' % year

# The default replacements for |version| and |release|, also used in various
# other places throughout the built documents.
#
# The short X.Y version.
version = '2.0'
# The full version, including alpha/beta/rc tags.
release = version

# There are two options for replacing |today|: either, you set today to
# some non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directories, that shouldn't be
# searched for source files.
#exclude_dirs = []

# The reST default role (used for this markup: `text`) to use for all
# documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'


# Options for HTML output
# -----------------------

# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
#html_style = 'repoze.css'

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as
# html_title.
#html_short_title = None

# The name of an image file (within the static path) to place at the top of
# the sidebar.
html_logo = '../static/images/logo_OL-lg.png'

# The name of an image file (within the static path) to use as favicon of
# the docs.  This file should be a Windows icon file (.ico) being 16x16 or
# 32x32 pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets)
# here, relative to this directory. They are copied after the builtin
# static files, so a file named "default.css" will overwrite the builtin
# "default.css".
#html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page
# bottom, using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, the reST sources are included in the HTML build as
# _sources/<name>.
#html_copy_source = True

# If true, an OpenSearch description file will be output, and all pages
# will contain a <link> tag referring to it.  The value of this option must
# be the base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'openlibrary'


# Options for LaTeX output
# ------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, document class [howto/manual]).
#latex_documents = [
#  ('index', 'supervisor.tex', 'supervisor Documentation',
#   'Supervisor Developers', 'manual'),
#]

# The name of an image file (relative to this directory) to place at the
# top of the title page.
#latex_logo = '.static/logo_hi.gif'

# For "manual" documents, if this is true, then toplevel headings are
# parts, not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

autodoc_default_flags = ['members', 'undoc-members', 'show-inheritance']

# OL themes

html_theme_options = {
    'linkcolor': '#036DAA',
    'visitedlinkcolor': '#003366',

    'sidebarbgcolor': '#E2DCC5', 
    'sidebarlinkcolor': '#036DAA',
    'sidebartextcolor': 'black',

    'relbarbgcolor': '#E2DCC5',
    'relbartextcolor': 'black',
    'relbarlinkcolor': '#036DAA',

    'footerbgcolor': '#CBC4A7',
    'footertextcolor': 'black',

    'headbgcolor': 'white',
    'headtextcolor': '#20435C',
    'headlinkcolor': '#036DAA',
}
########NEW FILE########
__FILENAME__ = model
"""

"""
import datetime
import hmac
import random
import uuid

import web

from infogami import config
from infogami.utils.view import render_template
from infogami.infobase.client import ClientException
from openlibrary.core import helpers as h
from openlibrary.core import support


def sendmail(to, msg, cc=None):
    cc = cc or []
    if config.get('dummy_sendmail'):
        message = ('' +
            'To: ' + to + '\n' +
            'From:' + config.from_address + '\n' +
            'Subject:' + msg.subject + '\n' +
            '\n' +
            web.safestr(msg))

        print >> web.debug, "sending email", message
    else:
        web.sendmail(config.from_address, to, subject=msg.subject.strip(), message=web.safestr(msg), cc=cc)

def verify_hash(secret_key, text, hash):
    """Verifies if the hash is generated
    """
    salt = hash.split('$', 1)[0]
    return generate_hash(secret_key, text, salt) == hash

def generate_hash(secret_key, text, salt=None):
    salt = salt or hmac.HMAC(secret_key, str(random.random())).hexdigest()[:5]
    hash = hmac.HMAC(secret_key, salt + web.utf8(text)).hexdigest()
    return '%s$%s' % (salt, hash)

def get_secret_key():
    return config.infobase['secret_key']

def generate_uuid():
    return str(uuid.uuid4()).replace("-", "")

def send_verification_email(username, email):
    """Sends account verification email.
    """
    key = "account/%s/verify" % username

    doc = create_link_doc(key, username, email)
    web.ctx.site.store[key] = doc

    link = web.ctx.home + "/account/verify/" + doc['code']
    msg = render_template("email/account/verify", username=username, email=email, password=None, link=link)
    sendmail(email, msg)

def create_link_doc(key, username, email):
    """Creates doc required for generating verification link email.

    The doc contains username, email and a generated code.
    """
    code = generate_uuid()

    now = datetime.datetime.utcnow()
    expires = now + datetime.timedelta(days=14)

    return {
        "_key": key,
        "_rev": None,
        "type": "account-link",
        "username": username,
        "email": email,
        "code": code,
        "created_on": now.isoformat(),
        "expires_on": expires.isoformat()
    }



class Link(web.storage):
    def get_expiration_time(self):
        d = self['expires_on'].split(".")[0]
        return datetime.datetime.strptime(d, "%Y-%m-%dT%H:%M:%S")

    def get_creation_time(self):
        d = self['created_on'].split(".")[0]
        return datetime.datetime.strptime(d, "%Y-%m-%dT%H:%M:%S")

    def delete(self):
        del web.ctx.site.store[self['_key']]



class Account(web.storage):
    @property
    def username(self):
        return self._key.split("/")[-1]
        
    def get_edit_count(self):
        user = self.get_user()
        return user and user.get_edit_count() or 0
        
    @property
    def registered_on(self):
        """Returns the registration time."""
        t = self.get("created_on")
        return t and h.parse_datetime(t)
        
    @property
    def activated_on(self):
        user = self.get_user()
        return user and user.created
        
    @property
    def displayname(self):
        key = "/people/" + self.username
        doc = web.ctx.site.get(key)
        if doc:
            return doc.displayname or self.username
        elif "data" in self:
            return self.data.get("displayname") or self.username
        else:
            return self.username
            
    def creation_time(self):
        d = self['created_on'].split(".")[0]
        return datetime.datetime.strptime(d, "%Y-%m-%dT%H:%M:%S")
        
    def get_cases(self):
        """Returns all support cases filed by this user.
        """
        email = self.email
        username = self.username
        
        # XXX-Anand: very inefficient. Optimize it later.
        cases = support.Support().get_all_cases()
        cases = [c for c in cases if c.creator_email == email or c.creator_username == username]
        return cases

    def verify_password(self, password):
        return verify_hash(get_secret_key(), password, self.enc_password)
        
    def update_password(self, new_password):
        web.ctx.site.update_account(self.username, password=new_password)
    
    def update_email(self, email):
        web.ctx.site.update_account(self.username, email=email)
        
    def send_verification_email(self):
        send_verification_email(self.username, self.email)

    def activate(self):
        web.ctx.site.activate_account(username=self.username)
        
    def block(self):
        """Blocks this account."""
        web.ctx.site.update_account(self.username, status="blocked")

    def unblock(self):
        """Unblocks this account."""
        web.ctx.site.update_account(self.username, status="active")

    def is_blocked(self):
        """Tests if this account is blocked."""
        return self.status == "blocked"

    def login(self, password):
        """Tries to login with the given password and returns the status.
        
        The return value can be one of the following:

            * ok
            * account_not_vefified
            * account_not_found
            * account_incorrect_password
            * account_blocked

        If the login is successful, the `last_login` time is updated.
        """
        if self.is_blocked():
            return "account_blocked"
        try:
            web.ctx.site.login(self.username, password)
        except ClientException, e:
            code = e.get_data().get("code")
            return code
        else:
            self['last_login'] = datetime.datetime.utcnow().isoformat()
            self._save()
            return "ok"
            
    def _save(self):
        """Saves this account in store.
        """
        web.ctx.site.store[self._key] = self

    @property
    def last_login(self):
        """Returns the last_login time of the user, if available.

        The `last_login` will not be available for accounts, who haven't
        been logged in after this feature is added.
        """
        t = self.get("last_login")
        return t and h.parse_datetime(t)
    
    def get_user(self):
        key = "/people/" + self.username
        doc = web.ctx.site.get(key)
        return doc

    def get_creation_info(self):
        key = "/people/" + self.username
        doc = web.ctx.site.get(key)
        return doc.get_creation_info()

    def get_activation_link(self):
        key = "account/%s/verify"%self.username
        doc = web.ctx.site.store.get(key)
        if doc:
            return Link(doc)
        else:
            return False
    
    def get_password_reset_link(self):
        key = "account/%s/password"%self.username
        doc = web.ctx.site.store.get(key)
        if doc:
            return Link(doc)
        else:
            return False

    def get_links(self):
        """Returns all the verification links present in the database.
        """
        return web.ctx.site.store.values(type="account-link", name="username", value=self.username)

    def get_tags(self):
        """Returns list of tags that this user has.
        """
        return self.get("tags", [])

    def has_tag(self, tag):
        return tag in self.get_tags()

    def add_tag(self, tag):
        tags = self.get_tags()
        if tag not in tags:
            tags.append(tag)
        self['tags'] = tags
        self._save()

    def remove_tag(self, tag):
        tags = self.get_tags()
        if tag in tags:
            tags.remove(tag)
        self['tags'] = tags
        self._save()

    def set_bot_flag(self, flag):
        """Enables/disables the bot flag.
        """
        self.bot = flag
        self._save()
########NEW FILE########
__FILENAME__ = actions
"""Custom OL Actions.
"""
import infogami
import sys

@infogami.action 
def runmain(modulename, *args):
    print "run_main", modulename, sys.argv
    mod = __import__(modulename, globals(), locals(), modulename.split("."))
    mod.main(*args)

########NEW FILE########
__FILENAME__ = code
import os
import web

from infogami.utils.view import render_template
from openlibrary.core import admin

import utils

app = web.auto_application()
app.add_processor(utils.admin_processor)
app.notfound = utils.notfound

class home(app.page):
    path = "/admin/?"
    
    def GET(self):
        stats = admin.get_stats()
        return render_template("admin/index", stats)
        
class static(app.page):
    path = "(/(?:images|js|css)/.*)"
    
    def GET(self, path):
        raise web.seeother("/static/upstream" + path)

def setup():
    # load templates from this package so that they are available via render_template
    from infogami.utils import template
    template.load_templates(os.path.dirname(__file__))
########NEW FILE########
__FILENAME__ = numbers
"""
List of functions that return various numbers which are stored in the
admin database by the stats module.

All functions prefixed with `admin_range__` will be run for each day and the
result will be stored as the part after it. e.g. the result of
admin_range__foo will be stored under the key `foo`.

All functions prefixed with `admin_delta__` will be run for the current
day and the result will be stored as the part after it. e.g. the
result of `admin_delta__foo` will be stored under the key `foo`.

All functions prefixed with `admin_total__` will be run for the current
day and the result will be stored as `total_<key>`. e.g. the result of
`admin_total__foo` will be stored under the key `total__foo`.

Functions with names other than the these will not be called from the
main harness. They can be utility functions.

"""
import os
import time
import urllib
import logging
import tempfile
import datetime
import calendar
import functools

import web
import couchdb

class InvalidType(TypeError): pass
class NoStats(TypeError): pass

sqlitefile = None

# Utility functions
def query_single_thing(db, typ, start, end):
    "Query the counts a single type from the things table"
    q1 = "SELECT id as id from thing where key=$typ"
    typ = '/type/%s'%typ
    result = db.query(q1, vars=locals())
    try:
        kid = result[0].id 
    except IndexError:
        raise InvalidType("No id for type '/type/%s in the datbase"%typ)
    q2 = "select count(*) as count from thing where type=%d and created >= '%s' and created < '%s'"% (kid, start, end)
    result = db.query(q2)
    count = result[0].count
    return count


def single_thing_skeleton(**kargs):
    """Returns number of things of `type` added between `start` and `end`.

    `type` is partially applied for admin__[work, edition, user, author, list].
    """
    try:
        typ   = kargs['type']
        start = kargs['start'].strftime("%Y-%m-%d")
        end   = kargs['end'].strftime("%Y-%m-%d %H:%M:%S")
        db    = kargs['thingdb']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_range__%s"%(k, typ))
    return query_single_thing(db, typ, start, end)
    

# Public functions that are used by stats.py
def admin_range__human_edits(**kargs):
    """Calculates the number of edits between the `start` and `end`
    parameters done by humans. `thingdb` is the database.
    """
    try:
        start = kargs['start'].strftime("%Y-%m-%d")
        end   = kargs['end'].strftime("%Y-%m-%d %H:%M:%S")
        db    = kargs['thingdb']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_range__human_edits"%k)
    q1 = "SELECT count(*) AS count FROM transaction WHERE created >= '%s' and created < '%s'"% (start, end)
    result = db.query(q1)
    total_edits = result[0].count
    q1 = "SELECT count(DISTINCT t.id) AS count FROM transaction t, version v WHERE v.transaction_id=t.id AND t.created >= '%s' and t.created < '%s' AND t.author_id IN (SELECT thing_id FROM account WHERE bot = 't')"% (start, end)
    result = db.query(q1)
    bot_edits = result[0].count
    return total_edits - bot_edits

def admin_range__bot_edits(**kargs):
    """Calculates the number of edits between the `start` and `end`
    parameters done by bots. `thingdb` is the database.
    """
    try:
        start = kargs['start'].strftime("%Y-%m-%d")
        end   = kargs['end'].strftime("%Y-%m-%d %H:%M:%S")
        db    = kargs['thingdb']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_range__bot_edits"%k)
    q1 = "SELECT count(*) AS count FROM transaction t, version v WHERE v.transaction_id=t.id AND t.created >= '%s' and t.created < '%s' AND t.author_id IN (SELECT thing_id FROM account WHERE bot = 't')"% (start, end)
    result = db.query(q1)
    count = result[0].count
    return count
    

def admin_range__covers(**kargs):
    "Queries the number of covers added between `start` and `end`"
    try:
        start = kargs['start'].strftime("%Y-%m-%d")
        end   = kargs['end'].strftime("%Y-%m-%d %H:%M:%S")
        db    = kargs['coverdb']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_range__covers"%k)
    q1 = "SELECT count(*) as count from cover where created>= '%s' and created < '%s'"% (start, end)
    result = db.query(q1)
    count = result[0].count
    return count


admin_range__works    = functools.partial(single_thing_skeleton, type="work")
admin_range__editions = functools.partial(single_thing_skeleton, type="edition")
admin_range__users    = functools.partial(single_thing_skeleton, type="user")
admin_range__authors  = functools.partial(single_thing_skeleton, type="author")
admin_range__lists    = functools.partial(single_thing_skeleton, type="list")
admin_range__members  = functools.partial(single_thing_skeleton, type="user")

def admin_range__visitors(**kargs):
    "Finds number of unique IPs to visit the OL website."
    try:
        date = kargs['start']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_range__visitors"%k)
    global sqlitefile
    if not sqlitefile:
        sqlitefile = tempfile.mktemp(prefix="sqlite-")
        url = "http://www.archive.org/download/stats/numUniqueIPsOL.sqlite"
        logging.debug("  Downloading '%s'", url)
        sqlite_contents = urllib.urlopen(url).read()
        f = open(sqlitefile, "w")
        f.write(sqlite_contents)
        f.close()
    db = web.database(dbn="sqlite", db = sqlitefile)
    d = date.replace(hour = 0, minute = 0, second = 0, microsecond = 0)
    key = calendar.timegm(d.timetuple())
    q = "SELECT value AS count FROM data WHERE timestamp = %d"%key
    result = list(db.query(q))
    if result:
        return result[0].count
    else:
        logging.debug("  No statistics obtained for %s (%d)", date, key)
        raise NoStats("No record for %s"%date)
    
def admin_range__loans(**kargs):
    """Finds the number of loans on a given day.

    Loan info is written to infobase write log. Grepping through the log file gives us the counts.
    
    WARNING: This script must be run on the node that has infobase logs.
    """
    try:
        db         = kargs['thingdb']
        start      = kargs['start']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_range__loans"%k)

    q = "select count(*) as count from stats where type='loan' and created::date=$date"
    date = "%04d/%02d/%02d" % (start.year, start.month, start.day)
    result = db.query(q, vars={"date": date})
    return result[0].count

def admin_total__authors(**kargs):
    try:
        db    = kargs['seeds_db']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__authors"%k)    
    off1 = db.view("_all_docs", startkey="/authors",   limit=0, stale="ok").offset
    off2 = db.view("_all_docs", startkey="/authors/Z", limit=0, stale="ok").offset
    total_authors = off2 - off1
    return total_authors


def admin_total__subjects(**kargs):
    try:
        db    = kargs['seeds_db']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__subjects"%k)
    rows = db.view("_all_docs", startkey="a", stale="ok", limit = 0)
    total_subjects = rows.total_rows - rows.offset
    return total_subjects


def admin_total__lists(**kargs):
    try:
        db    = kargs['thingdb']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__lists"%k)    
    # Computing total number of lists
    q1 = "SELECT id as id from thing where key='/type/list'"
    result = db.query(q1)
    try:
        kid = result[0].id 
    except IndexError:
        raise InvalidType("No id for type '/type/list' in the database")
    q2 = "select count(*) as count from thing where type=%d"% kid
    result = db.query(q2)
    total_lists = result[0].count
    return total_lists


def admin_total__covers(**kargs):
    try:
        db    = kargs['editions_db']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__covers"%k)    
    total_covers = db.view("admin/editions_with_covers", stale="ok").rows[0].value
    return total_covers


def admin_total__works(**kargs):
    try:
        db    = kargs['works_db']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__works"%k)    
    total_works = db.info()["doc_count"]
    return total_works


def admin_total__editions(**kargs):
    try:
        db    = kargs['editions_db']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__editions"%k)
    total_editions = db.info()["doc_count"]
    return total_editions


def admin_total__ebooks(**kargs):
    try:
        db    = kargs['editions_db']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__ebooks"%k)
    total_ebooks = db.view("admin/ebooks", stale="ok").rows[0].value
    return total_ebooks

def admin_total__members(**kargs):
    try:
        db    = kargs['thingdb']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_total__members"%k)
    q1 = "SELECT id as id from thing where key='/type/user'"
    result = db.query(q1)
    try:
        kid = result[0].id 
    except IndexError:
        raise InvalidType("No id for type '/type/user in the datbase")
    q2 = "select count(*) as count from thing where type=%d"% kid
    result = db.query(q2)
    count = result[0].count
    return count
    

def admin_delta__ebooks(**kargs):
    try:
        editions_db = kargs['editions_db']
        admin_db    = kargs['admin_db']
        yesterday   = kargs['start']
        today       = kargs['end']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_delta__ebooks"%k)
    current_total = editions_db.view("admin/ebooks", stale="ok").rows[0].value
    yesterdays_key = yesterday.strftime("counts-%Y-%m-%d")
    try:
        last_total = admin_db[yesterdays_key]["total_ebooks"]
        logging.debug("Yesterdays count for total_ebook %s", last_total)
    except (couchdb.http.ResourceNotFound, KeyError):
        logging.warn("No total_ebook found for %s. Using 0", yesterdays_key)
        last_total = 0
    current_count = current_total - last_total
    return current_count

def admin_delta__subjects(**kargs):
    try:
        editions_db = kargs['editions_db']
        admin_db    = kargs['admin_db']
        seeds_db    = kargs['seeds_db']
        yesterday   = kargs['start']
        today       = kargs['end']
    except KeyError, k:
        raise TypeError("%s is a required argument for admin_delta__subjects"%k)
    rows = seeds_db.view("_all_docs", startkey="a", stale="ok", limit=0)
    current_total = rows.total_rows - rows.offset
    yesterdays_key = yesterday.strftime("counts-%Y-%m-%d")
    try:
        last_total = admin_db[yesterdays_key]["total_subjects"]
        logging.debug("Yesterdays count for total_subject %s", last_total)
    except (couchdb.http.ResourceNotFound, KeyError):
        logging.warn("No total_subject found for %s. Using 0", yesterdays_key)
        last_total = 0
    current_count = current_total - last_total
    return current_count

########NEW FILE########
__FILENAME__ = stats
"""
Script to read out data from thingdb and put it in couch so that it
can be queried by the /admin pages on openlibrary
"""


import os
import logging
import datetime

import web
import yaml
import couchdb

import numbers


web.config.debug = False

class InvalidType(TypeError): pass

def connect_to_pg(config_file):
    """Connects to the postgres database specified in the dictionary
    `config`. Needs a top level key `db_parameters` and under that
    `database` (or `db`) at the least. If `user` and `host` are
    provided, they're used as well."""
    f = open(config_file)
    config = yaml.load(f)
    f.close()
    conf = {}
    conf["db"] = config["db_parameters"].get("database") or config["db_parameters"].get("db")
    if not conf['db']:
        raise KeyError("database/db")
    host = config["db_parameters"].get("host")
    user = config["db_parameters"].get("user") or config["db_parameters"].get("username")
    if host:
        conf["host"] = host
    if user:
        conf["user"] = user
    logging.debug(" Postgres Database : %(db)s"%conf)
    return web.database(dbn="postgres",**conf)


def connect_to_couch(config_file):
    "Connects to the couch databases"
    f = open(config_file)
    config = yaml.load(f)
    f.close()
    admin_db = config["admin"]["counts_db"]
    editions_db = config["lists"]["editions_db"]
    works_db = config["lists"]["works_db"]
    seeds_db = config["lists"]["seeds_db"]
    logging.debug(" Admin Database is %s", admin_db)
    logging.debug(" Editions Database is %s", editions_db)
    logging.debug(" Works Database is %s", works_db)
    logging.debug(" Seeds Database is %s", seeds_db)
    return couchdb.Database(admin_db), couchdb.Database(editions_db), couchdb.Database(works_db), couchdb.Database(seeds_db)

def get_config_info(infobase_config):
    """Parses the config file(s) to get back all the necessary pieces of data.

    Add extra parameters here and change the point of calling.
    """
    f = open(infobase_config)
    config = yaml.load(f)
    f.close()
    logroot = config.get("writelog")
    return logroot
    
def store_data(db, data, date):
    uid = "counts-%s"%date
    logging.debug(" Updating admin_db for %s - %s", uid, data)
    try:
        vals = db[uid]
        vals.update(data)
    except couchdb.http.ResourceNotFound:
        vals = data
        db[uid] = vals
    db.save(vals)

    # start storing data in store as well, so that we can phase out couch
    doc = web.ctx.site.store.get(uid) or {}
    doc.update(data)
    doc['type'] = 'admin-stats'
    web.ctx.site.store[uid] = doc

def run_gathering_functions(infobase_db, coverstore_db, seeds_db, editions_db, works_db, admin_db,
                            start, end, logroot, prefix, key_prefix = None):
    """Runs all the data gathering functions with the given prefix
    inside the numbers module"""
    funcs = [x for x in dir(numbers) if x.startswith(prefix)]
    d = {}
    for i in funcs:
        fn = getattr(numbers, i)
        key = i.replace(prefix,"")
        if key_prefix:
            key = "%s_%s"% (key_prefix, key)
        try:
            ret = fn(thingdb     = infobase_db,
                     coverdb     = coverstore_db,
                     seeds_db    = seeds_db,
                     editions_db = editions_db,
                     works_db    = works_db,
                     admin_db    = admin_db,
                     logroot     = logroot,
                     start       = start,
                     end         = end)
            logging.info("  %s - %s", i, ret)
            d[key] = ret
        except numbers.NoStats:
            logging.warning("  %s - No statistics available", i)
        except Exception, k:
            logging.warning("  Failed with %s", k)
    return d

def setup_ol_config(openlibrary_config_file):
    """Setup OL configuration.

    Required for storing counts in store.
    """
    import infogami
    from infogami import config
    from infogami.utils import delegate

    config.plugin_path += ['openlibrary.plugins']
    config.site = "openlibrary.org"

    infogami.load_config(openlibrary_config_file)
    infogami.config.infobase_parameters = dict(type="ol")

    if config.get("infobase_config_file"):
        dir = os.path.dirname(openlibrary_config_file)
        path = os.path.join(dir, config.infobase_config_file)
        config.infobase = yaml.safe_load(open(path).read())

    infogami._setup()

def main(infobase_config, openlibrary_config, coverstore_config, ndays = 1):
    logging.basicConfig(level=logging.DEBUG, format = "%(levelname)-8s : %(filename)-12s:%(lineno)4d : %(message)s")
    logging.info("Parsing config file")
    try:
        infobase_db = connect_to_pg(infobase_config)
        coverstore_db = connect_to_pg(coverstore_config)
        admin_db, editions_db, works_db, seeds_db = connect_to_couch(openlibrary_config)
        logroot = get_config_info(infobase_config)
    except KeyError,k:
        logging.critical("Config file section '%s' missing", k.args[0])
        return -1

    setup_ol_config(openlibrary_config)

    # Gather delta and total counts
    # Total counts are simply computed and updated for the current day
    # Delta counts are computed by subtracting the current total from yesterday's total
    today = datetime.datetime.now()
    yesterday = today - datetime.timedelta(days = 1)
    data = {}
    logging.info("Gathering total data")
    data.update(run_gathering_functions(infobase_db, coverstore_db, seeds_db, editions_db, works_db, admin_db,
                                        yesterday, today, logroot,
                                        prefix = "admin_total__", key_prefix = "total"))
    logging.info("Gathering data using difference between totals")
    data.update(run_gathering_functions(infobase_db, coverstore_db, seeds_db, editions_db, works_db, admin_db,
                                        yesterday, today, logroot,
                                        prefix = "admin_delta__"))
    store_data(admin_db, data, today.strftime("%Y-%m-%d"))
    # Now gather data which can be queried based on date ranges
    # The queries will be from the beginning of today till right now
    # The data will be stored as the counts of the current day.
    end = datetime.datetime.now() # Right now
    start = datetime.datetime(hour = 0, minute = 0, second = 0, day = end.day, month = end.month, year = end.year) # Beginning of the day
    logging.info("Gathering range data")
    data = {}
    for i in range(int(ndays)):
        logging.info(" %s to %s", start, end)
        data.update(run_gathering_functions(infobase_db, coverstore_db, seeds_db, editions_db, works_db, admin_db,
                                            start, end, logroot,
                                            prefix = "admin_range__"))
        store_data(admin_db, data, start.strftime("%Y-%m-%d"))
        end = start
        start = end - datetime.timedelta(days = 1)
    if numbers.sqlitefile:
        logging.info("Removing sqlite file used for ipstats")
        os.unlink(numbers.sqlitefile)
    return 0

########NEW FILE########
__FILENAME__ = utils
"""utils for admin application.
"""
import web

from infogami.utils import delegate, features
from infogami.utils.view import render_template

def admin_processor(handler):
    """web.py application processor for enabling infogami and verifying admin permissions.
    """
    delegate.initialize_context()
    delegate.context.features = []
    features.loadhook()
    
    # required to give a special look and feel in site template
    delegate.context.setdefault('bodyid', 'admin')
    delegate.context.setdefault('usergroup', 'admin')

    page = handler()
    return render_template("site", page)
    
def notfound():
    msg = render_template(
            "site", 
            render_template("notfound", web.ctx.path, create=False))
    return web.notfound(msg)
########NEW FILE########
__FILENAME__ = api
r"""Open Library API Client.

Sample Usage::

    ol = OpenLibrary("http://0.0.0.0:8080")
    ol.login('joe', 'secret')
    
    page = ol.get("/sandbox")
    print page["body"]

    page["body"] += "\n\nTest from API"
    ol.save("/sandbox", page, "test from api")
"""

__version__ = "0.1"
__author__ = "Anand Chitipothu <anandology@gmail.com>"


import os
import re
import datetime
from ConfigParser import ConfigParser
import urllib, urllib2
import simplejson
import web
import logging

logger = logging.getLogger("openlibrary.api")

class OLError(Exception):
    def __init__(self, http_error):
        self.headers = http_error.headers
        msg = http_error.msg + ": " + http_error.read()
        Exception.__init__(self, msg)


class OpenLibrary:
    def __init__(self, base_url="https://openlibrary.org"):
        self.base_url = base_url.rstrip('/')
        self.cookie = None

    def _request(self, path, method='GET', data=None, headers=None):
        logger.info("%s %s", method, path)
        url = self.base_url + path
        headers = headers or {}
        if self.cookie:
            headers['Cookie'] = self.cookie
        
        try:
            req = urllib2.Request(url, data, headers)
            req.get_method = lambda: method
            return urllib2.urlopen(req)
        except urllib2.HTTPError, e:
            raise OLError(e)

    def autologin(self, section=None):
        """Login to Open Library with credentials taken from ~/.olrc file.

        The ~/.olrc file must be in ini format (format readable by
        ConfigParser module) and there should be a section with the
        server name. A sample configuration file may look like this::

            [openlibrary.org]
            username = joe
            password = secret

            [0.0.0.0:8080]
            username = joe
            password = joe123
            
        Optionally section name can be passed as argument to force using a different section name.
        """
        config = ConfigParser()
        config.read(os.path.expanduser('~/.olrc'))
        
        section = section or self.base_url.replace('http://', '')

        if not config.has_section(section):
            raise Exception("No section found with name %s in ~/.olrc" % repr(section))

        username = config.get(section, 'username')
        password = config.get(section, 'password')
        return self.login(username, password)

    def login(self, username, password):
        """Login to Open Library with given credentials.
        """
        headers = {'Content-Type': 'application/json'}  
        try:      
            data = simplejson.dumps(dict(username=username, password=password))
            response = self._request('/account/login', method='POST', data=data, headers=headers)
        except urllib2.HTTPError, e:
            response = e
        
        if 'Set-Cookie' in response.headers:
            cookies = response.headers['Set-Cookie'].split(',')
            self.cookie =  ';'.join([c.split(';')[0] for c in cookies])

    def get(self, key, v=None):
        data = self._request(key + '.json' + ('?v=%d' % v if v else '')).read()
        return unmarshal(simplejson.loads(data))
        
    def get_many(self, keys):
        """Get multiple documents in a single request as a dictionary.
        """
        if len(keys) > 500:
            # get in chunks of 500 to avoid crossing the URL length limit.
            d = {}
            for chunk in web.group(keys, 100):
                d.update(self._get_many(chunk))
            return d
        else:
            return self._get_many(keys)
        
    def _get_many(self, keys):
        response = self._request("/api/get_many?" + urllib.urlencode({"keys": simplejson.dumps(keys)}))
        return simplejson.loads(response.read())['result']

    def save(self, key, data, comment=None):
        headers = {'Content-Type': 'application/json'}
        data = marshal(data)
        if comment:
            headers['Opt'] = '"http://openlibrary.org/dev/docs/api"; ns=42'
            headers['42-comment'] = comment
        data = simplejson.dumps(data)
        return self._request(key, method="PUT", data=data, headers=headers).read()
        
    def _call_write(self, name, query, comment, action):
        headers = {'Content-Type': 'application/json'}
        query = marshal(query)

        # use HTTP Extension Framework to add custom headers. see RFC 2774 for more details.
        if comment or action:
            headers['Opt'] = '"http://openlibrary.org/dev/docs/api"; ns=42'
        if comment:
            headers['42-comment'] = comment
        if action:
            headers['42-action'] = action

        response = self._request('/api/' + name, method="POST", data=simplejson.dumps(query), headers=headers)
        return simplejson.loads(response.read())

    def save_many(self, query, comment=None, action=None):
        return self._call_write('save_many', query, comment, action)
        
    def write(self, query, comment="", action=""):
        """Internal write API."""
        return self._call_write('write', query, comment, action)

    def new(self, query, comment=None, action=None):
        return self._call_write('new', query, comment, action)

    def query(self, q=None, **kw):
        """Query Open Library.

        Open Library always limits the result to 1000 items due to
        performance issues. Pass limit=False to fetch all matching
        results by making multiple requests to the server. Please note
        the an iterator is returned insted of list when limit=False is
        passed.::

            >>> ol.query({'type': '/type/type', 'limit': 2}) #doctest: +SKIP
            [{'key': '/type/property'}, {'key': '/type/type'}]

            >>> ol.query(type='/type/type', limit=2) #doctest: +SKIP
            [{'key': '/type/property'}, {'key': '/type/type'}]
        """
        q = dict(q or {})
        q.update(kw)
        q = marshal(q)
        def unlimited_query(q):
            q['limit'] = 1000
            q.setdefault('offset', 0)
            q.setdefault('sort', 'key')

            while True:
                result = self.query(q)
                for r in result:
                    yield r
                if len(result) < 1000:
                    break
                q['offset'] += len(result)

        if 'limit' in q and q['limit'] == False:
            return unlimited_query(q)
        else:
            q = simplejson.dumps(q)
            response = self._request("/query.json?" + urllib.urlencode(dict(query=q)))
            return unmarshal(simplejson.loads(response.read()))


def marshal(data):
    """Serializes the specified data in the format required by OL.::

        >>> marshal(datetime.datetime(2009, 1, 2, 3, 4, 5, 6789))
        {'type': '/type/datetime', 'value': '2009-01-02T03:04:05.006789'}
    """
    if isinstance(data, list):
        return [marshal(d) for d in data]
    elif isinstance(data, dict):
        return dict((k, marshal(v)) for k, v in data.iteritems())
    elif isinstance(data, datetime.datetime):
        return {"type": "/type/datetime", "value": data.isoformat()}
    elif isinstance(data, Text):
        return {"type": "/type/text", "value": unicode(data)}
    elif isinstance(data, Reference):
        return {"key": unicode(data)}
    else:
        return data


def unmarshal(d):
    u"""Converts OL serialized objects to python.::

        >>> unmarshal({"type": "/type/text", "value": "hello, world"})
        <text: u'hello, world'>
        >>> unmarshal({"type": "/type/datetime", "value": "2009-01-02T03:04:05.006789"})
        datetime.datetime(2009, 1, 2, 3, 4, 5, 6789)
    """
    if isinstance(d, list):
        return [unmarshal(v) for v in d]
    elif isinstance(d, dict):
        if 'key' in d and len(d) == 1:
            return Reference(d['key'])
        elif 'value' in d and 'type' in d:
            if d['type'] == '/type/text':
                return Text(d['value'])
            elif d['type'] == '/type/datetime':
                return parse_datetime(d['value'])
            else:
                return d['value']
        else:
            return dict([(k, unmarshal(v)) for k, v in d.iteritems()])
    else:
        return d


def parse_datetime(value):
    """Parses ISO datetime formatted string.::

        >>> parse_datetime("2009-01-02T03:04:05.006789")
        datetime.datetime(2009, 1, 2, 3, 4, 5, 6789)
    """
    if isinstance(value, datetime.datetime):
        return value
    else:
        tokens = re.split('-|T|:|\.| ', value)
        return datetime.datetime(*map(int, tokens))


class Text(unicode):
    def __repr__(self):
        return "<text: %s>" % unicode.__repr__(self)


class Reference(unicode):
    def __repr__(self):
        return "<ref: %s>" % unicode.__repr__(self)

########NEW FILE########
__FILENAME__ = app
"""Utilities to build the app.
"""
from infogami.utils import app as _app
from infogami.utils.view import render, public
from infogami.utils.macro import macro

class view(_app.page):
    """A view is a class that defines how a page or a set of pages
    identified by a regualar expression are rendered.

    Here is a sample view::

        from openlibrary import app

        class hello(app.view):
            path = "/hello/(.*)"

            def GET(self, name):
                return app.render_template("hello", name)
    """
    # In infogami, the class with this functionality is called page. 
    # We are redefining with a slightly different terminology to make
    # things more readable.
    pass

# view is just a base class. 
# Defining a class extending from _app.page auto-registers it inside infogami.
# Undoing that.
del _app.pages['/view']    

class subview(_app.view):
    """Subviews are views that work an object in the database.

    Each subview URL will have two parts, the prefix identifying the key
    of the document in the database to work on and the suffix iden identifying
    the action.

    For example, the in the subview with URL "/works/OL123W/foo/identifiers", 
    "identifiers" is the action and "/works/OL123W" is the key of the document.
    The middle part "foo" is added by a middleware to make the URLs readable 
    and not that is transparant to this.

    Here is a sample subview:

        class work_identifiers(delegate.view):
            suffix = "identifiers"
            types = ["/type/edition"]
    """
    # In infogami, the class with this functionality is called a view. 
    # We are redefining with a slightly different terminology to make
    # things more readable.

    # Tell infogami not to consider this as a view class
    suffix = None
    types = None

@macro
@public
def render_template(name, *a, **kw):
    if "." in name:
        name = name.rsplit(".", 1)[0]
    return render[name](*a, **kw)

########NEW FILE########
__FILENAME__ = load_book
import web, re, os
from openlibrary.catalog.utils import flip_name, author_dates_match, key_int, error_mail

def east_in_by_statement(rec, author):
    if 'by_statement' not in rec:
        return False
    if 'authors' not in rec:
        return False
    name = author['name']
    flipped = flip_name(name)
    name = name.replace('.', '')
    name = name.replace(', ', '')
    if name == flipped.replace('.', ''):
        return False
    return rec['by_statement'].find(name) != -1

def do_flip(author):
    # given an author name flip it in place
    if 'personal_name' not in author:
        return
    if author['personal_name'] != author['name']:
        return
    first_comma = author['name'].find(', ')
    if first_comma == -1:
        return
    # e.g: Harper, John Murdoch, 1845-
    if author['name'].find(',', first_comma + 1) != -1:
        return
    if author['name'].find('i.e.') != -1:
        return
    if author['name'].find('i. e.') != -1:
        return
    name = flip_name(author['name'])
    author['name'] = name
    author['personal_name'] = name

def find_author(name, send_mail=True):
    def walk_redirects(obj, seen):
        seen.add(obj['key'])
        while obj['type']['key'] == '/type/redirect':
            assert obj['location'] != obj['key']
            obj = web.ctx.site.get(obj['location'])
            seen.add(obj['key'])
        return obj

    q = {'type': '/type/author', 'name': name} # FIXME should have no limit
    reply = list(web.ctx.site.things(q))
    authors = [web.ctx.site.get(k) for k in reply]
    if any(a.type.key != '/type/author' for a in authors):
        seen = set()
        authors = [walk_redirects(a, seen) for a in authors if a['key'] not in seen]
    return authors

def pick_from_matches(author, match): # no DB calls in this function
    maybe = []
    if 'birth_date' in author and 'death_date' in author:
        maybe = [m for m in match if 'birth_date' in m and 'death_date' in m]
    elif 'date' in author:
        maybe = [m for m in match if 'date' in m]
    if not maybe:
        maybe = match
    if len(maybe) == 1:
        return maybe[0]
    return min(maybe, key=key_int)

def find_entity(author): # no direct DB calls
    name = author['name']
    things = find_author(name)
    et = author.get('entity_type')
    if et and et != 'person':
        if not things:
            return None
        db_entity = things[0]
        assert db_entity['type']['key'] == '/type/author'
        return db_entity
    if ', ' in name:
        things += find_author(flip_name(name))
    match = []
    seen = set()
    for a in things:
        key = a['key']
        if key in seen:
            continue
        seen.add(key)
        orig_key = key
        assert a.type.key == '/type/author'
        if 'birth_date' in author and 'birth_date' not in a:
            continue
        if 'birth_date' not in author and 'birth_date' in a:
            continue
        if not author_dates_match(author, a):
            continue
        match.append(a)
    if not match:
        return None
    if len(match) == 1:
        return match[0]
    try:
        return pick_from_matches(author, match)
    except ValueError:
        print 'author:', author
        print 'match:', match
        raise

def import_author(author, eastern=False):
    existing = find_entity(author)
    if existing:
        assert existing.type.key == '/type/author'
        for k in 'last_modified', 'id', 'revision', 'created':
            if existing.k:
                del existing.k
        new = existing
        if 'death_date' in author and 'death_date' not in existing:
            new['death_date'] = author['death_date']
        return new
    if not eastern:
        do_flip(author)
    a = { 'type': { 'key': '/type/author' } }
    for f in 'name', 'title', 'personal_name', 'birth_date', 'death_date', 'date':
        if f in author:
            a[f] = author[f]
    return a

class InvalidLanguage(Exception):
    def __init__(self, code):
        self.code = code
    def __str__(self):
        return "invalid language code: '%s'" % self.code

type_map = { 'description': 'text', 'notes': 'text', 'number_of_pages': 'int' }

def build_query(rec):
    book = {
        'type': { 'key': '/type/edition'},
    }


    for k, v in rec.iteritems():
        if k == 'authors':
            if v and v[0]:
                book[k] = []
                for author in v:
                    east = east_in_by_statement(rec, author)
                    book[k].append(import_author(author, eastern=east))
            continue
        if k == 'languages':
            langs = []
            for l in v:
                if web.ctx.site.get('/languages/' + l) is None:
                    raise InvalidLanguage(l)
            book[k] = [{'key': '/languages/' + l} for l in v]
            continue
        if k in type_map:
            t = '/type/' + type_map[k]
            if isinstance(v, list):
                book[k] = [{'type': t, 'value': i} for i in v]
            else:
                book[k] = {'type': t, 'value': v}
        else:
            book[k] = v

    return book

########NEW FILE########
__FILENAME__ = merge
from openlibrary.catalog.merge.merge_marc import build_marc, attempt_merge
from pprint import pprint
import web

threshold = 875

def db_name(a):
    date = None
    if a.birth_date or a.death_date:
        date = a.get('birth_date', '') + '-' + a.get('death_date', '')
    elif a.date:
        #assert not a.birth_date and not a.death_date 
        date = a.date
    return ' '.join([a['name'], date]) if date else a['name']

def undelete_author(a):
    a = web.ctx.site.get(a.key, revision=a.revision-1)
    author_type = a.type.key
    assert author_type == '/type/author'
    web.ctx.site.save(a.dict(), comment='undelete author')
    return web.ctx.site.get(a.key)

def try_merge(e1, edition_key, existing):
    thing_type = existing.type.key
    if thing_type == '/type/delete':
        return False
    assert thing_type == '/type/edition'

    rec2 = {}
    rec2['full_title'] = existing.title
    if existing.subtitle:
        rec2['full_title'] += ' ' + existing.subtitle
    for f in 'isbn', 'isbn_10', 'isbn_13', 'lccn', 'publish_country', 'publishers', 'publish_date':
        if existing.get(f):
            rec2[f] = existing[f]
    if existing.authors:
        rec2['authors'] = []
        for a in existing.authors:
            author_type = a.type.key
            while author_type == '/type/delete' or author_type == '/type/redirect':
                if author_type == '/type/delete':
                    a = undelete_author(a)
                    author_type = a.type.key
                    continue
                if author_type == '/type/redirect':
                    a = web.ctx.site.get(a.location)
                    author_type = a.type.key
                    continue
            assert author_type == '/type/author'
            assert a['name']
            rec2['authors'].append({'name': a['name'], 'db_name': db_name(a)})

    e2 = build_marc(rec2)
    return attempt_merge(e1, e2, threshold, debug=False)

########NEW FILE########
__FILENAME__ = test_add_book
from load_book import build_query, InvalidLanguage
from . import load, RequiredField, build_pool, add_db_name
import py.test
from openlibrary.catalog.merge.merge_marc import build_marc
from openlibrary.catalog.marc.parse import read_edition
from openlibrary.catalog.marc.marc_binary import MarcBinary
from merge import try_merge
from copy import deepcopy
from urllib import urlopen
from collections import defaultdict
from pprint import pprint

def add_languages(mock_site):
    languages = [
        ('eng', 'English'),
        ('spa', 'Spanish'),
        ('fre', 'French'),
        ('yid', 'Yiddish'),
    ]
    for code, name in languages:
        mock_site.save({
            'key': '/languages/' + code,
            'name': name,
            'type': {'key': '/type/language'},
        })

def test_build_query(mock_site):
    add_languages(mock_site)
    rec = {
        'title': 'magic',
        'languages': ['eng', 'fre'],
        'authors': [{}],
        'description': 'test',
    }
    q = build_query(rec)
    assert q['title'] == 'magic'
    assert q['description'] == { 'type': '/type/text', 'value': 'test' }
    assert q['type'] == {'key': '/type/edition'}
    assert q['languages'] == [{'key': '/languages/eng'}, {'key': '/languages/fre'}]

    py.test.raises(InvalidLanguage, build_query, {'languages': ['wtf']})

def test_load(mock_site):
    add_languages(mock_site)
    rec = {'ocaid': 'test item'}
    py.test.raises(RequiredField, load, {'ocaid': 'test_item'})

    rec = {
        'ocaid': 'test_item',
        'source_records': ['ia:test_item'],
        'title': 'Test item',
        'languages': ['eng'],
    }
    reply = load(rec)
    assert reply['success'] == True

    assert reply['edition']['status'] == 'created'
    e = mock_site.get(reply['edition']['key'])
    assert e.type.key == '/type/edition'
    assert e.title == 'Test item'
    assert e.ocaid == 'test_item'
    assert e.source_records == ['ia:test_item']
    l = e.languages
    assert len(l) == 1 and l[0].key == '/languages/eng'

    assert reply['work']['status'] == 'created'
    w = mock_site.get(reply['work']['key'])
    assert w.title == 'Test item'
    assert w.type.key == '/type/work'

    rec = {
        'ocaid': 'test_item',
        'title': 'Test item',
        'subjects': ['Protected DAISY', 'In library'],
        'source_records': 'ia:test_item',
    }
    reply = load(rec)
    assert reply['success'] == True
    w = mock_site.get(reply['work']['key'])
    assert w.title == 'Test item'
    assert w.subjects == ['Protected DAISY', 'In library']

    rec = {
        'ocaid': 'test_item',
        'title': 'Test item',
        'authors': [{'name': 'John Doe'}],
        'source_records': 'ia:test_item',
    }
    reply = load(rec)
    assert reply['success'] == True
    w = mock_site.get(reply['work']['key'])
    if 'authors' in reply:
        assert reply['authors'][0]['status'] == 'created'
        assert reply['authors'][0]['name'] == 'John Doe'
        akey1 = reply['authors'][0]['key']
        a = mock_site.get(akey1)
        assert w.authors
        assert a.type.key == '/type/author'

    rec = {
        'ocaid': 'test_item',
        'title': 'Test item',
        'authors': [{'name': 'Doe, John', 'entity_type': 'person'}],
        'source_records': 'ia:test_item',
    }
    reply = load(rec)
    assert reply['success'] == True
    if 'authors' in reply:
        assert reply['authors'][0]['status'] == 'modified'
        akey2 = reply['authors'][0]['key']
        assert akey1 == akey2

    rec = {
        'ocaid': 'test_item2',
        'title': 'Test item',
        'authors': [{'name': 'James Smith'}],
        'source_records': 'ia:test_item2',
    }
    reply = load(rec)
    assert reply['authors'][0]['status'] == 'created'
    assert reply['work']['status'] == 'created'
    assert reply['edition']['status'] == 'created'
    w = mock_site.get(reply['work']['key'])
    e = mock_site.get(reply['edition']['key'])
    assert e.ocaid == 'test_item2'
    assert len(w.authors) == 1
    assert len(e.authors) == 1

#def test_author_matching(mock_site):

def test_duplicate_ia_book(mock_site):
    add_languages(mock_site)

    rec = {
        'ocaid': 'test_item',
        'source_records': ['ia:test_item'],
        'title': 'Test item',
        'languages': ['eng'],
    }
    reply = load(rec)
    assert reply['success'] == True

    assert reply['edition']['status'] == 'created'
    e = mock_site.get(reply['edition']['key'])
    assert e.type.key == '/type/edition'
    assert e.source_records == ['ia:test_item']

    rec = {
        'ocaid': 'test_item',
        'source_records': ['ia:test_item'],
        'title': 'Different item',
        'languages': ['fre'],
    }

    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'matched'

def test_from_marc_3(mock_site):
    add_languages(mock_site)
    ia = 'treatiseonhistor00dixo'
    data = open('test_data/' + ia + '_meta.mrc').read()
    assert len(data) == int(data[:5])
    rec = read_edition(MarcBinary(data))
    rec['source_records'] = ['ia:' + ia]
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'created'
    e = mock_site.get(reply['edition']['key'])
    assert e.type.key == '/type/edition'

def test_from_marc_2(mock_site):
    add_languages(mock_site)
    ia = 'roadstogreatness00gall'
    
    data = open('test_data/' + ia + '_meta.mrc').read()
    assert len(data) == int(data[:5])
    rec = read_edition(MarcBinary(data))
    rec['source_records'] = ['ia:' + ia]
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'created'
    e = mock_site.get(reply['edition']['key'])
    assert e.type.key == '/type/edition'

    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'matched'

def test_from_marc(mock_site):
    from openlibrary.catalog.marc.marc_binary import MarcBinary
    from openlibrary.catalog.marc.parse import read_edition

    add_languages(mock_site)
    data = open('test_data/flatlandromanceo00abbouoft_meta.mrc').read()
    assert len(data) == int(data[:5])
    rec = read_edition(MarcBinary(data))
    reply = load(rec)
    assert reply['success'] == True
    akey1 = reply['authors'][0]['key']
    a = mock_site.get(akey1)
    assert a.type.key == '/type/author'
    assert a.name == 'Edwin Abbott Abbott'
    assert a.birth_date == '1838'
    assert a.death_date == '1926'

def test_build_pool(mock_site):
    assert build_pool({'title': 'test'}) == {'title': []}
    etype = '/type/edition'
    ekey = mock_site.new_key(etype)
    e = {
        'title': 'test',
        'type': {'key': etype},
        'lccn': ['123'],
        'oclc_numbers': ['456'],
        'key': ekey,
    }

    mock_site.save(e)
    pool = build_pool(e)
    assert pool == {
        'lccn': ['/books/OL1M'],
        'oclc_numbers': ['/books/OL1M'],
        'title': ['/books/OL1M'],
    }

    pool = build_pool({'lccn': ['234'], 'oclc_numbers': ['456'], 'title': 'test',})
    assert pool == { 'oclc_numbers': ['/books/OL1M'], 'title': ['/books/OL1M'], }

def test_try_merge(mock_site):
    rec = {
        'title': 'Test item',
        'lccn': ['123'],
        'authors': [{'name': 'Smith, John', 'birth_date': '1980'}],
        'source_records': ['ia:test_item'],
    }
    reply = load(rec)
    ekey = reply['edition']['key']
    e = mock_site.get(ekey)

    rec['full_title'] = rec['title']
    if rec.get('subtitle'):
        rec['full_title'] += ' ' + rec['subtitle']
    e1 = build_marc(rec)
    add_db_name(e1)

    assert try_merge(e1, ekey, e)

def test_load_multiple(mock_site):
    rec = {
        'title': 'Test item',
        'lccn': ['123'],
        'source_records': ['ia:test_item'],
        'authors': [{'name': 'Smith, John', 'birth_date': '1980'}],
    }
    reply = load(rec)
    assert reply['success'] == True
    ekey1 = reply['edition']['key']

    reply = load(rec)
    assert reply['success'] == True
    ekey2 = reply['edition']['key']
    assert ekey1 == ekey2

    reply = load({'title': 'Test item', 'source_records': ['ia:test_item2'], 'lccn': ['456']})
    assert reply['success'] == True
    ekey3 = reply['edition']['key']
    assert ekey3 != ekey1

    reply = load(rec)
    assert reply['success'] == True
    ekey4 = reply['edition']['key']

    assert ekey1 == ekey2 == ekey4

def test_add_db_name():
    authors = [
        {'name': 'Smith, John' },
        {'name': 'Smith, John', 'date': '1950' },
        {   'name': 'Smith, John',
            'birth_date': '1895',
            'death_date': '1964' },
    ]
    orig = deepcopy(authors)
    add_db_name({'authors': authors})
    orig[0]['db_name'] = orig[0]['name']
    orig[1]['db_name'] = orig[1]['name'] + ' 1950'
    orig[2]['db_name'] = orig[2]['name'] + ' 1895-1964'
    assert authors == orig

    rec = {}
    add_db_name(rec)
    assert rec == {}

def test_from_marc(mock_site):
    add_languages(mock_site)
    ia = 'coursepuremath00hardrich'
    marc = MarcBinary(open('test_data/' + ia + '_meta.mrc').read())
    rec = read_edition(marc)
    rec['source_records'] = ['ia:' + ia]
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'created'
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'matched'

    ia = 'flatlandromanceo00abbouoft'
    marc = MarcBinary(open('test_data/' + ia + '_meta.mrc').read())

    rec = read_edition(marc)
    rec['source_records'] = ['ia:' + ia]
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'created'
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'matched'

def test_real_example(mock_site):
    add_languages(mock_site)

    src = 'v38.i37.records.utf8:16478504:1254'
    marc = MarcBinary(open('test_data/' + src).read())
    rec = read_edition(marc)
    rec['source_records'] = ['marc:' + src]
    reply = load(rec)
    assert reply['success'] == True
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'matched'

    src = 'v39.i28.records.utf8:5362776:1764'
    marc = MarcBinary(open('test_data/' + src).read())
    rec = read_edition(marc)
    rec['source_records'] = ['marc:' + src]
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'modified'

def test_missing_ocaid(mock_site):
    add_languages(mock_site)
    ia = 'descendantsofhug00cham'
    src = ia + '_meta.mrc'
    marc = MarcBinary(open('test_data/' + src).read())
    rec = read_edition(marc)
    rec['source_records'] = ['marc:testdata.mrc']
    reply = load(rec)
    assert reply['success'] == True
    rec['source_records'] = ['ia:' + ia]
    rec['ocaid'] = ia
    reply = load(rec)
    assert reply['success'] == True
    e = mock_site.get(reply['edition']['key'])
    assert e.ocaid == ia
    assert 'ia:' + ia in e.source_records

def test_extra_author(mock_site):
    add_languages(mock_site)

    mock_site.save({
        "name": "Hubert Howe Bancroft",
        "death_date": "1918.",
        "alternate_names": ["HUBERT HOWE BANCROFT", "Hubert Howe Bandcroft"], 
        "key": "/authors/OL563100A", 
        "birth_date": "1832", 
        "personal_name": "Hubert Howe Bancroft", 
        "type": {"key": "/type/author"}, 
    })

    mock_site.save({
        "title": "The works of Hubert Howe Bancroft",
        "covers": [6060295, 5551343],
        "first_sentence": {"type": "/type/text", "value": "When it first became known to Europe that a new continent had been discovered, the wise men, philosophers, and especially the learned ecclesiastics, were sorely perplexed to account for such a discovery."},
        "subject_places": ["Alaska", "America", "Arizona", "British Columbia", "California", "Canadian Northwest", "Central America", "Colorado", "Idaho", "Mexico", "Montana", "Nevada", "New Mexico", "Northwest Coast of North America", "Northwest boundary of the United States", "Oregon", "Pacific States", "Texas", "United States", "Utah", "Washington (State)", "West (U.S.)", "Wyoming"], 
        "excerpts": [{"excerpt": "When it first became known to Europe that a new continent had been discovered, the wise men, philosophers, and especially the learned ecclesiastics, were sorely perplexed to account for such a discovery."}], 
        "first_publish_date": "1882", 
        "key": "/works/OL3421434W",
        "authors": [{"type": {"key": "/type/author_role"}, "author": {"key": "/authors/OL563100A"}}],
        "subject_times": ["1540-1810", "1810-1821", "1821-1861", "1821-1951", "1846-1850", "1850-1950", "1859-", "1859-1950", "1867-1910", "1867-1959", "1871-1903", "Civil War, 1861-1865", "Conquest, 1519-1540", "European intervention, 1861-1867", "Spanish colony, 1540-1810", "To 1519", "To 1821", "To 1846", "To 1859", "To 1867", "To 1871", "To 1889", "To 1912", "Wars of Independence, 1810-1821"],
        "type": {"key": "/type/work"},
        "subjects": ["Antiquities", "Archaeology", "Autobiography", "Bibliography", "California Civil War, 1861-1865", "Comparative Literature", "Comparative civilization", "Courts", "Description and travel", "Discovery and exploration", "Early accounts to 1600", "English essays", "Ethnology", "Foreign relations", "Gold discoveries", "Historians", "History", "Indians", "Indians of Central America", "Indians of Mexico", "Indians of North America", "Languages", "Law", "Mayas", "Mexican War, 1846-1848", "Nahuas", "Nahuatl language", "Oregon question", "Political aspects of Law", "Politics and government", "Religion and mythology", "Religions", "Social life and customs", "Spanish", "Vigilance committees", "Writing", "Zamorano 80", "Accessible book", "Protected DAISY"]
    })

    ia = 'workshuberthowe00racegoog'
    src = ia + '_meta.mrc'
    marc = MarcBinary(open('test_data/' + src).read())
    rec = read_edition(marc)
    rec['source_records'] = ['ia:' + ia]

    reply = load(rec)
    assert reply['success'] == True

    w = mock_site.get(reply['work']['key'])

    reply = load(rec)
    assert reply['success'] == True
    w = mock_site.get(reply['work']['key'])
    #assert len(w['authors']) == 1


def test_missing_source_records(mock_site):
    add_languages(mock_site)

    mock_site.save({
        'key': '/authors/OL592898A',
        'name': 'Michael Robert Marrus',
        'personal_name': 'Michael Robert Marrus',
        'type': { 'key': '/type/author' }
    })

    mock_site.save({
        'authors': [{'author': '/authors/OL592898A', 'type': { 'key': '/type/author_role' }}],
        'key': '/works/OL16029710W',
        'subjects': ['Nuremberg Trial of Major German War Criminals, Nuremberg, Germany, 1945-1946', 'Protected DAISY', 'Lending library'],
        'title': 'The Nuremberg war crimes trial, 1945-46',
        'type': { 'key': '/type/work' },
    })

    mock_site.save({
        "number_of_pages": 276, 
        "subtitle": "a documentary history", 
        "series": ["The Bedford series in history and culture"], 
        "covers": [6649715, 3865334, 173632], 
        "lc_classifications": ["D804.G42 N87 1997"], 
        "ocaid": "nurembergwarcrim00marr", 
        "contributions": ["Marrus, Michael Robert."], 
        "uri_descriptions": ["Book review (H-Net)"], 
        "title": "The Nuremberg war crimes trial, 1945-46", 
        "languages": [{"key": "/languages/eng"}], 
        "subjects": ["Nuremberg Trial of Major German War Criminals, Nuremberg, Germany, 1945-1946"],
        "publish_country": "mau", "by_statement": "[compiled by] Michael R. Marrus.", 
        "type": {"key": "/type/edition"}, 
        "uris": ["http://www.h-net.org/review/hrev-a0a6c9-aa"],
        "publishers": ["Bedford Books"], 
        "ia_box_id": ["IA127618"], 
        "key": "/books/OL1023483M", 
        "authors": [{"key": "/authors/OL592898A"}], 
        "publish_places": ["Boston"], 
        "pagination": "xi, 276 p. :", 
        "lccn": ["96086777"], 
        "notes": {"type": "/type/text", "value": "Includes bibliographical references (p. 262-268) and index."}, 
        "identifiers": {"goodreads": ["326638"], "librarything": ["1114474"]}, 
        "url": ["http://www.h-net.org/review/hrev-a0a6c9-aa"], 
        "isbn_10": ["031216386X", "0312136919"], 
        "publish_date": "1997", 
        "works": [{"key": "/works/OL16029710W"}]
    })

    ia = 'nurembergwarcrim1997marr'
    src = ia + '_meta.mrc'
    marc = MarcBinary(open('test_data/' + src).read())
    rec = read_edition(marc)
    rec['source_records'] = ['ia:' + ia]

    reply = load(rec)
    assert reply['success'] == True
    e = mock_site.get(reply['edition']['key'])
    assert 'source_records' in e

def test_no_extra_author(mock_site):
    add_languages(mock_site)

    author = {
        "name": "Paul Michael Boothe",
        "key": "/authors/OL2894448A",
        "type": {"key": "/type/author"},
    }
    mock_site.save(author)

    work = {
        "title": "A Separate Pension Plan for Alberta", 
        "covers": [1644794], 
        "key": "/works/OL8611498W",
        "authors": [{"type": "/type/author_role", "author": {"key": "/authors/OL2894448A"}}],
        "type": {"key": "/type/work"}, 
    }
    mock_site.save(work)

    edition = {
        "number_of_pages": 90,
        "subtitle": "Analysis and Discussion (Western Studies in Economic Policy, No. 5)",
        "weight": "6.2 ounces",
        "covers": [1644794],
        "latest_revision": 6,
        "title": "A Separate Pension Plan for Alberta",
        "languages": [{"key": "/languages/eng"}],
        "subjects": ["Economics", "Alberta", "Political Science / State & Local Government", "Government policy", "Old age pensions", "Pensions", "Social security"], 
        "type": {"key": "/type/edition"},
        "physical_dimensions": "9 x 6 x 0.2 inches",
        "publishers": ["The University of Alberta Press"],
        "physical_format": "Paperback",
        "key": "/books/OL8211505M",
        "authors": [{"key": "/authors/OL2894448A"}],
        "identifiers": {"goodreads": ["4340973"], "librarything": ["5580522"]},
        "isbn_13": ["9780888643513"],
        "isbn_10": ["0888643519"],
        "publish_date": "May 1, 2000",
        "works": [{"key": "/works/OL8611498W"}]
    }
    mock_site.save(edition)

    src = 'v39.i34.records.utf8:186503:1413'
    marc = MarcBinary(open('test_data/' + src).read())
    rec = read_edition(marc)
    rec['source_records'] = ['marc:' + src]

    #pprint(rec)

    reply = load(rec)
    assert reply['success'] == True

    a = mock_site.get(reply['authors'][0]['key'])
    pprint(a.dict())

    if 'authors' in reply:
        assert reply['authors'][0]['key'] == author['key']
    assert reply['edition']['key'] == edition['key']
    assert reply['work']['key'] == work['key']

    e = mock_site.get(reply['edition']['key'])
    w = mock_site.get(reply['work']['key'])
    assert 'source_records' in e
    assert len(e['authors']) == 1
    assert len(w['authors']) == 1

def test_don_quixote(mock_site):
    return
    dq = [u'lifeexploitsofin01cerv', u'cu31924096224518',
        u'elingeniosedcrit04cerv', u'ingeniousgentlem01cervuoft',
        u'historyofingenio01cerv', u'lifeexploitsofin02cerviala',
        u'elingeniosohidal03cervuoft', u'nybc209000', u'elingeniosohidal11cerv',
        u'elingeniosohidal01cervuoft', u'elingeniosoh01cerv',
        u'donquixotedelama00cerviala', u'1896elingeniosohid02cerv',
        u'ingeniousgentlem04cervuoft', u'cu31924027656978', u'histoiredeladmir01cerv',
        u'donquijotedelama04cerv', u'cu31924027657075', u'donquixotedelama03cervuoft',
        u'aventurasdedonqu00cerv', u'p1elingeniosohid03cerv',
        u'geshikhefundonik01cervuoft', u'historyofvalorou02cerviala',
        u'ingeniousgentlem01cerv', u'donquixotedelama01cervuoft',
        u'ingeniousgentlem0195cerv', u'firstpartofdelig00cervuoft',
        u'p4elingeniosohid02cerv', u'donquijote00cervuoft', u'cu31924008863924',
        u'c2elingeniosohid02cerv', u'historyofvalorou03cerviala',
        u'historyofingenio01cerviala', u'historyadventure00cerv',
        u'elingeniosohidal00cerv', u'lifeexploitsofin01cervuoft',
        u'p2elingeniosohid05cerv', u'nybc203136', u'elingeniosohidal00cervuoft',
        u'donquixotedelama02cervuoft', u'lingnieuxcheva00cerv',
        u'ingeniousgentlem03cerv', u'vidayhechosdeli00siscgoog',
        u'lifeandexploits01jarvgoog', u'elingeniosohida00puiggoog',
        u'elingeniosohida00navagoog', u'donquichottedel02florgoog',
        u'historydonquixo00cogoog', u'vidayhechosdeli01siscgoog',
        u'elingeniosohida28saavgoog', u'historyvalorous00brangoog',
        u'elingeniosohida01goog', u'historyandadven00unkngoog',
        u'historyvalorous01goog', u'ingeniousgentle11saavgoog',
        u'elingeniosohida10saavgoog', u'adventuresdonqu00jarvgoog',
        u'historydonquixo04saavgoog', u'lingnieuxcheval00rouxgoog',
        u'elingeniosohida19saavgoog', u'historyingeniou00lalagoog',
        u'elingeniosohida00ormsgoog', u'historyandadven01smolgoog',
        u'elingeniosohida27saavgoog', u'elingeniosohida21saavgoog',
        u'historyingeniou00mottgoog', u'historyingeniou03unkngoog',
        u'lifeandexploits00jarvgoog', u'ingeniousgentle00conggoog',
        u'elingeniosohida00quixgoog', u'elingeniosohida01saavgoog',
        u'donquixotedelam02saavgoog', u'adventuresdonqu00gilbgoog',
        u'historyingeniou02saavgoog', u'donquixotedelam03saavgoog',
        u'elingeniosohida00ochogoog', u'historyingeniou08mottgoog',
        u'lifeandexploits01saavgoog', u'firstpartdeligh00shelgoog',
        u'elingeniosohida00castgoog', u'elingeniosohida01castgoog',
        u'adventofdonquixo00cerv', u'portablecervante00cerv',
        u'firstpartofdelig14cerv', u'donquixotemanofl00cerv',
        u'firstpartofdelig00cerv']

    add_languages(mock_site)
    edition_status_counts = defaultdict(int)
    work_status_counts = defaultdict(int)
    author_status_counts = defaultdict(int)
    for num, ia in enumerate(dq):
        marc_url = 'http://archive.org/download/%s/%s_meta.mrc' % (ia, ia)
        data = urlopen(marc_url).read()
        if '<title>Internet Archive: Page Not Found</title>' in data:
            continue
        marc = MarcBinary(data)
        rec = read_edition(marc)
        rec['source_records'] = ['ia:' + ia]
        reply = load(rec)
        q = {
            'type': '/type/work',
            'authors.author': '/authors/OL1A',
        }
        work_keys = list(mock_site.things(q))
        assert work_keys
        
        assert reply['success'] == True

def test_same_twice(mock_site):
    add_languages(mock_site)
    rec = {
            'source_records': ['ia:test_item'],
            "publishers": ["Ten Speed Press"], "pagination": "20 p.", "description": "A macabre mash-up of the children's classic Pat the Bunny and the present-day zombie phenomenon, with the tactile features of the original book revoltingly re-imagined for an adult audience.", "title": "Pat The Zombie", "isbn_13": ["9781607740360"], "languages": ["eng"], "isbn_10": ["1607740362"], "authors": [{"entity_type": "person", "name": "Aaron Ximm", "personal_name": "Aaron Ximm"}], "contributions": ["Kaveh Soofi (Illustrator)"]}
    reply = load(rec)
    assert reply['success'] == True
    assert reply['edition']['status'] == 'created'
    assert reply['work']['status'] == 'created'
    reply = load(rec)
    print reply
    assert reply['success'] == True
    assert reply['edition']['status'] != 'created'
    assert reply['work']['status'] != 'created'


########NEW FILE########
__FILENAME__ = add_covers
from urllib2 import urlopen
import simplejson

base = 'http://ia331526.us.archive.org:7001/openlibrary.org/log/'

out = open('edition_and_isbn', 'w')
offset = '2009-06-01:0'
while not offset.startswith('2010-03-17:'):
    url = base + offset
    ret = simplejson.load(urlopen(url))
    offset, data = ret['offset'], ret['data']
    print offset, len(data)
    for i in data:
        action = i.pop('action')
        key = i['data'].pop('key', None)
        if action == 'new_account':
            continue
        author = i['data'].get('author', None) if 'data' in i else None
        if author != '/user/ImportBot':
            continue
        assert action in ('save_many', 'save')
        if action == 'save' and key.startswith('/b/'):
            e = i['data']['query']
            if e:
                isbn = e.get('isbn_10', None)
                if isbn:
                    print >> out, (key, isbn)
        elif action == 'save_many':
            for e in i['data']['query']:
                if e['type'] == '/type/edition' and e['key'].startswith('/b/'):
                    isbn = e.get('isbn_10', None)
                    if isbn:
                        print >> out, (e['key'], isbn)
out.close()

########NEW FILE########
__FILENAME__ = amazon_to_arc
import socket

#url = "http://www.amazon.com/dp/1847195881"
#asin = "1847195881"

def get(sock, host, url):
    send = 'GET %s HTTP/1.1\r\nHost: %s\r\nAccept-Encoding: identity\r\n\r\n' % (url, host)
    sock.sendall(send)

    fp = sock.makefile('rb', 0)

    line = fp.readline()
    print 'status:', `line`

    state = 'header'
    for line in fp:
        if line == '\r\n':
            break
        print 'header', `line`

    while True:
        chunk_size = int(fp.readline(),16)
        print chunk_size
        if chunk_size == 0:
            break
        print len(fp.read(chunk_size))
        print `fp.read(2)`
    line = fp.readline()
    print `line`
    fp.close()

host = 'openlibrary.org'
host = 'www.amazon.com'
sock = socket.create_connection((host, 80))

url = 'http://openlibrary.org/type/work'
url = "http://www.amazon.com/dp/1847195881"
get(sock, host, url)

url = 'http://openlibrary.org/type/edition'
url = "http://www.amazon.com/dp/0393062287"
get(sock, host, url)

########NEW FILE########
__FILENAME__ = arc_index
import os

arc_dir = '/2/edward/amazon/arc'

def read_arc(filename):
    f = open(arc_dir + '/' + filename)
    idx = open(arc_dir + '/' + filename + '.idx', 'w')
    while True:
        pos = f.tell()
        line = f.readline()
        if line == '':
            break
        print >> idx, pos
        size = int(line[:-1].split(' ')[4])
        f.read(size)
        line = f.readline()
    f.close()
    idx.close()

for filename in (i for i in os.listdir(arc_dir) if i.endswith('.arc')):
    print filename
    read_arc(filename)

########NEW FILE########
__FILENAME__ = arc_view
import web, os
from StringIO import StringIO

arc_dir = '/2/edward/amazon/arc'
urls = (
    '/', 'index',
    '/(\d+\.arc)', 'arc_view',
    '/(\d+\.arc)/(\d+)', 'page_view',
)
app = web.application(urls, globals(), autoreload=True)

class arc_view:
    def GET(self, filename):
        ret = '<html><body>'
        ret += '<a href="/">back to index</a><br>'
        ret += '<h1>%s</h1>' % filename
        idx = open(arc_dir + '/' + filename + '.idx')
        arc = open(arc_dir + '/' + filename)
        for pos in idx:
            arc.seek(int(pos))
            line = arc.readline()[:-1].split(' ')
            ret += '<a href="%s/%d">from ARC</a> OR <a href="%s">original</a> %s <br>' % (filename, int(pos), line[0], line[0])
        idx.close()

        ret += '</body></html>'
        return ret

class page_view:
    def GET(self, filename, offset):
        arc = open(arc_dir + '/' + filename)
        arc.seek(int(offset))
        size = int(arc.readline().split(' ')[4])
        f = StringIO(arc.read(size))
        f.readline()
        ret = ''
        while True:
            line=f.readline()
            if line == '\r\n':
                break
        while True:
            line = f.readline()
            chunk_size = int(line, 16)
            if chunk_size == 0:
                break
            buf = f.read(chunk_size)
            ret += buf
            f.readline()
        return ret

class index:        
    def GET(self):
        ret = '<html><body><ul>'
        for filename in os.listdir(arc_dir):
            if not filename.endswith('.arc'):
                continue
            f = open(arc_dir + '/' + filename)
            line = f.readline()
            f.close()
            ret += '<li><a href="/%s">%s</a> - %s' % (filename, filename, line)
        ret += '</body></html>'
        return ret

if __name__ == "__main__":
    app.run()

########NEW FILE########
__FILENAME__ = crawl
from lxml.html import parse, tostring, fromstring
import re, sys, os, socket
from urllib import unquote
from urllib2 import urlopen
from time import sleep
from os.path import exists
from datetime import date, timedelta, datetime
import codecs

# scrap Amazon for book and author data

re_expect_end = re.compile('</div>\n</body>\n</html>[ \n]*$')

# publisher = Big Idea Books & Just Me Music
re_personalized = re.compile('Personalized for (.*) \((Boy|Girl)\)', re.I)

def percent(a, b):
    return float(a * 100.0) / b

class PersonalizedBooks(Exception):
    pass

page_size = 12
max_pages = 100
max_results = page_size * max_pages

# http://www.amazon.com/s/qid=1265761735/ref=sr_nr_n_0/177-5112913-4864616?ie=UTF8&rs=1000&bbn=1000&rnid=1000&rh=i%3Astripbooks%2Cp_n%5Ffeature%5Fbrowse-bin%3A618083011%2Cp%5Fn%5Fdate%3A20090101%2Cn%3A%211000%2Cn%3A1
re_product_title = re.compile('/dp/([^/]*)')
re_result_count = re.compile('Showing (?:[\d,]+ - [\d,]+ of )?([\d,]+) Result')
#re_rh_n = re.compile('rh=n%3A(\d+)%2C')
re_rh_n = re.compile('%2Cn%3A(\d+)')
re_facet_count = re.compile(u'^\xa0\(([\d,]+)\)$')
u'\xa0(8)'

base_url = "http://www.amazon.com/s?ie=UTF8&rh="
rh = 'i:stripbooks,p_n_feature_browse-bin:618083011,p_n_date:'

out_dir = '/0/amazon'
arc_dir = '/0/amazon/arc'

# 4 = Children's Books, 28 = Teens
re_child_book_param = re.compile(',n:(4|28)(?:&page=\d+)?$')

def now():
    return datetime.utcnow().replace(microsecond=0)

max_size = 1024 * 1024 * 1024 * 10 # 10 GB
ip = '207.241.229.141'
content_type_hdr = 'Content-Type: '
re_charset_header = re.compile('; charset=(.+)\r\n')
version_block = '1 0 Open Library\nURL IP-address Archive-date Content-type Archive-length\n'

class Scraper:
    def __init__(self, recording=True):
        self.host = 'www.amazon.com'
        self.sock = socket.create_connection((self.host, 80))
        self.recording = recording
        self.cur_arc = None

    def add_to_arc(self, url, start, content_type, reply):
        d = start.strftime('%Y%m%d%H%M%S')
        if self.cur_arc is None or os.stat(arc_dir + self.cur_arc).st_size > max_size:
            self.cur_arc = now().strftime('%Y%m%d%H%M%S') + '.arc'
            assert not exists(arc_dir + self.cur_arc)
            out = open(arc_dir + self.cur_arc, 'w')
            out.write(' '.join(['filespec://' + self.cur_arc, ip, d, 'text/plain', str(len(version_block))]) + '\n')
            out.write(version_block)
        else:
            out = open(arc_dir + self.cur_arc, 'a')
        out.write('\n' + ' '.join([url, ip, d, content_type, str(len(reply))]) + '\n')
        out.write(reply)
        out.close()

    def get(self, url):
        start = now()
        send = 'GET %s HTTP/1.1\r\nHost: %s\r\nUser-Agent: Mozilla/5.0\r\nAccept-Encoding: identity\r\n\r\n' % (url, self.host)
        self.sock.sendall(send)

        fp = self.sock.makefile('rb', 0)
        recv_buf = ''

        line = fp.readline()
        if not line.startswith('HTTP/1.1 200'):
            print 'status:', `line`
        recv_buf += line

        body = ''
        content_type = None
        charset = None
        for line in fp: # read headers
            recv_buf += line
            if line.lower().startswith('transfer-encoding'):
                assert line == 'Transfer-Encoding: chunked\r\n'
            if line == '\r\n':
                break
            if line.lower().startswith('content-type'):
                assert line.startswith(content_type_hdr)
                assert line[-2:] == '\r\n'
                content_type = line[len(content_type_hdr):line.find(';') if ';' in line else -2]
                if 'charset' in line.lower():
                    m = re_charset_header.search(line)
                    charset = m.group(1)

        while True:
            line = fp.readline()
            recv_buf += line
            chunk_size = int(line, 16)
            if chunk_size == 0:
                break
            chunk = fp.read(chunk_size)
            recv_buf += chunk
            body += chunk
            assert chunk_size == len(chunk)
            recv_buf += fp.read(2)
        line = fp.readline()
        recv_buf += line
        fp.close()
        if self.recording:
            self.add_to_arc(url, start, content_type, recv_buf)
        return body.decode(charset) if charset else body

scraper = Scraper(recording=True)

def get_url(params):
    url = base_url + params
    page = scraper.get(url)
    return fromstring(page)

def get_total(root):
    if root.find(".//h1[@id='noResultsTitle']") is not None:
        return 0
    result_count = root.find(".//td[@class='resultCount']").text
    m = re_result_count.match(result_count)
    return int(m.group(1).replace(',', ''))

def read_books(params, root):
    # sometimes there is no link, bug at Amazaon
    # either skip it, or reload the page
    for i in range(5):
        book_links = [e.find('.//a[@href]') for e in root.find_class('dataColumn')]
        if all(a is not None for a in book_links):
            break
        sleep(2)
        print 'retry:', params
        root = get_url(params)
    if re_child_book_param.search(params) and all(re_personalized.search(span.text) for span in root.find_class('srTitle')):
        raise PersonalizedBooks
    return [re_product_title.search(a.attrib['href']).group(1) for a in book_links if a is not None and a.text]

def get_cats(root):
    cats = []
    for div in root.find_class('narrowItemHeading'):
        if div.text != 'Department':
            continue
        container = div.getparent()
        assert container.tag == 'td' and container.attrib['class'] == 'refinementContainer'
        break

    table = container.find('table')
    for e in table.iterfind(".//div[@class='refinement']"):
        a = e[0]
        assert a.tag == 'a'
        span1 = a[0]
        assert span1.tag == 'span' and span1.attrib['class'] == 'refinementLink'
        span2 = a[1]
        assert span2.tag == 'span' and span2.attrib['class'] == 'narrowValue'
        href = a.attrib['href']
        m1 = re_rh_n.search(href)
        if not m1:
            print 'no match:'
            print `href`
        m2 = re_facet_count.search(span2.text)
        cats.append((int(m1.group(1)), span1.text, int(m2.group(1).replace(',',''))))
        
    return cats

    for e in container.find('table').find_class('refinementLink'):
        a = e.getparent()
        assert a.tag == 'a'
        cat = { 'url': a.attrib['href'], 'title': e.text }
        href = a.attrib['href']
        m = re_rh_n.search(href)
        cats.append((int(m.group(1)), e.text))

def read_page(params):
    # read search results page
    root = get_url(params)
    total = get_total(root)
    if total == 0:
        print 'no results found'
        return total, set(), []
    grand_total = total
    pages = (total / page_size) + 1
    print 'total:', total, 'pages:', pages

    cats = get_cats(root)
    print 'cats 1'
    for a, b, c in cats:
        print "%8d %-30s %8d" % (a, b, c)
    #return grand_total, [], cats

    books = set()

    books.update(read_books(params, root))
    for page in range(2, min((pages, 100))+1):
        params_with_page = params + "&page=%d" % page
        books.update(read_books(params_with_page, get_url(params_with_page)))
        print page, len(books)

    print len(books)

    cats = get_cats(root)
    print 'cats 2'
    for a, b, c in cats:
        print "%8d %30s %8d" % (a, b, c)
    print 'cat total:', sum(i[2] for i in cats)
    if total > max_results:
        for n, title, count in cats:
            print `n, title, count`
            params_with_cat = params + ",n:" + str(n)
            root = get_url(params_with_cat)
            cat_total = get_total(root)
            pages = (cat_total / page_size) + 1
            print 'cat_total:', total, 'pages:', total / page_size
            if cat_total > max_results:
                print 'cat_total (%d) > max results (%d)' % (total, max_results)
    #        assert cat_total <= max_results
            try:
                books.update(read_books(params_with_cat, root))
            except PersonalizedBooks:
                print 'WARNING: Personalized Books'
                continue
            for page in range(2, min((pages, 100)) + 1):
                params_with_page = params_with_cat + "&page=%d" % page
                try:
                    books.update(read_books(params_with_page, get_url(params_with_page)))
                except PersonalizedBooks:
                    print 'WARNING: Personalized Books'
                    break
                print `n, title, page, cat_total / page_size, len(books), "%.1f%%" % percent(len(books), grand_total)`
            print

    return total, books, cats

def write_books(books):
    i = 0
    error_count = 0

    for asin in books:
        i+= 1
        for attempt in range(5):
            try:
                #page = urlopen('http://amazon.com/dp/' + asin).read()
                page = scraper.get('http://www.amazon.com/dp/' + asin)
                if re_expect_end.search(page):
                    break
                print 'bad page ending'
                print `page[-60:]`
                error_count += 1
                if error_count == 50:
                    print 'too many bad endings'
                    print 'http://amazon.com/dp/' + asin
                    sys.exit(0)
            except:
                pass
            print 'retry'
            sleep(5)

if __name__ == '__main__':

    one_day = timedelta(days=1)
    cur = date(2009, 1, 1) # start date
    cur = date(2009, 11, 11) # start date
    #cur = date(2009, 12, 25)
    while True:
        print cur
        total, books, cats = read_page(rh + cur.strftime("%Y%m%d"))
        open(out_dir + '/total.' + str(cur), 'w').write(str(total) + "\n")

        out = open(out_dir + "/cats." + str(cur), 'w')
        for i in cats:
            print >> out, i
        out.close()
        print len(books)
        write_books(books)
        cur += one_day

########NEW FILE########
__FILENAME__ = crawl_top_books
from openlibrary.catalog.amazon.crawl import read_page, write_books, get_url, get_cats

def get_serp():
    params = 'i:stripbooks,n:!1000,p_n_feature_browse-bin:618083011'

    #crawled = set(i[:-1] for i in open('/2/edward/amazon/crawled'))

    total, books, cats = read_page(params)
    print 'total:', total, 'number of books:', len(books), 'number of cats:', len(cats)

#get_serp()

params = 'i:stripbooks,n:9988'
root = get_url(params)
cats = get_cats(root)

for a, b, c in cats:
    print "%8d %-30s %8d" % (a, b, c)

#books = [i[:-1] for i in open('/2/edward/amazon/best_sellers2')]
#write_books(books)

########NEW FILE########
__FILENAME__ = extract_amazon_fields
# find fields in amazon data that don't appear in MARC data, extract and store in shelve

import shelve

seg_file = '/home/edward/ol/amazon/seg/22'

match = set(eval(line)[0] \
    for line \
    in open('/home/edward/ol/merge/amazon_marc/amazon_lc_map'))

# fields that MARC is missing:
# binding
# subject
# category
# series
# series_num
# edition
# dimensions
# first_sentence
# sip []
# cap []
# shipping_weight

fields = [ 'binding', 'subject', 'category', 'series', 'series_num', 'edition',\
    'dimensions', 'first_sentence', 'sip', 'cap', 'shipping_weight' ]

d = shelve.open('amazon_fields.shelve', protocol=-1, writeback=True)

for line in open(seg_file):
    isbn, item = eval(line)
    if isbn not in match:
        continue
    d[isbn] = dict([(f, item[f]) for f in fields if f in item])
d.close

########NEW FILE########
__FILENAME__ = get_other_editions
from catalog.read_rc import read_rc
import web, urllib2, sys, os.path
from time import time

rc = read_rc()
web.config.db_parameters = dict(dbn='postgres', db=rc['db'], user=rc['user'], pw=rc['pw'], host=rc['host'])
web.config.db_printing = False
web.load()
dir = sys.argv[1]

chunk = 10
t0 = time()
isbn_iter = web.query('select value from edition_str where key_id=30')
for i, row in enumerate(isbn_iter):
    isbn = row.value
    dest = dir + '/' + isbn
    if os.path.exists(dest):
        continue
    if len(isbn) != 10:
        continue
    url = 'http://www.amazon.com/dp/other-editions/' + isbn
    try:
        page = urllib2.urlopen(url).read()
    except urllib2.HTTPError, error:
        if error.code != 404:
            raise
        page = ''
    open(dest, 'w').write(page)
    if i % chunk == 0:
        t1 = time() - t0
        rec_per_sec = float(i) / float(t1)
        print "%s %s %.2f rec/sec" % (url, isbn, rec_per_sec)

########NEW FILE########
__FILENAME__ = import
import sys,re, os
from parse import read_edition
from lxml.html import fromstring
import catalog.importer.pool as pool
from catalog.importer.db_read import get_mc, withKey
import catalog.merge.amazon as amazon_merge
from catalog.get_ia import get_from_local, get_ia
from catalog.merge.merge_marc import build_marc
import catalog.marc.fast_parse as fast_parse
import urllib2

re_amazon = re.compile('^([A-Z0-9]{10}),(\d+):(.*)$', re.S)

re_normalize = re.compile('[^\w ]')
re_whitespace = re.compile('\s+')
re_title_parens = re.compile('^(.+) \([^)]+?\)$')

re_meta_marc = re.compile('([^/]+)_(meta|marc)\.(mrc|xml)')
# marc:marc_ithaca_college/ic_marc.mrc:224977427:1064

threshold = 875

def normalize_str(s):
    s = re_normalize.sub('', s.strip())
    s = re_whitespace.sub(' ', s)
    return str(s.lower())

# isbn, short title
def build_index_fields(asin, edition):
    title = edition['title']
    if 'subtitle' in edition:
        title += ' ' + edition['subtitle']

    def norm(s):
        return normalize_str(s)[:25].rstrip()

    titles = set([norm(title)])
    m = re_title_parens.match(title)
    if m:
        titles.add(norm(m.group(1)))

    isbn = set([asin])
    for field in 'asin', 'isbn_10', 'isbn_13':
        if field in edition:
            isbn.add(edition[field].replace('-', ''))
    return {'title': list(titles), 'isbn': list(isbn)}

def read_amazon_file(f):
    while True:
        buf = f.read(1024)
        if not buf:
            break
        m = re_amazon.match(buf)
        (asin, page_len, page) = m.groups()
        page += f.read(int(page_len) - len(page))
        try:
            edition = read_edition(fromstring(page))
        except:
            print 'bad record:', asin
            raise
        if not edition:
            continue
        yield asin, edition

def follow_redirects(key):
    keys = []
    thing = None
    while not thing or thing['type']['key'] == '/type/redirect':
        keys.append(key)
        thing = withKey(key)
        assert thing
        if thing['type']['key'] == '/type/redirect':
            print 'following redirect %s => %s' % (key, thing['location'])
            key = thing['location']
    return (keys, thing)

def ia_match(a, ia):
    try:
        loc, rec = get_ia(ia)
    except urllib2.HTTPError:
        return False
    if rec is None or 'full_title' not in rec:
        return False
    try:
        e1 = build_marc(rec)
    except TypeError:
        print rec
        raise
    return amazon_merge.attempt_merge(a, e1, threshold, debug=False)

def marc_match(a, loc):
    assert loc
    rec = fast_parse.read_edition(get_from_local(loc))
    e1 = build_marc(rec)
    #print 'amazon:', a
    return amazon_merge.attempt_merge(a, e1, threshold, debug=False)

def source_records_match(a, thing):
    marc = 'marc:'
    amazon = 'amazon:'
    ia = 'ia:'
    match = False
    for src in thing['source_records']:
        if not src.startswith('marc:marc_ithaca_college/ic'):
            m = re_meta_marc.search(src)
            if m:
                src = 'ia:' + m.group(1)
        if src.startswith(marc):
            if marc_match(a, src[len(marc):]):
                match = True
                break
        elif src.startswith(ia):
            if src == 'ia:ic':
                print thing['source_records']
            if ia_match(a, src[len(ia):]):
                match = True
                break
        else:
            assert src.startswith(amazon)
            continue
    return match


def try_merge(edition, ekey, thing):
    thing_type = thing['type']['key']
    if 'isbn_10' not in edition:
        print edition
    asin = edition.get('isbn_10', None) or edition['asin']
    if 'authors' in edition:
        authors = [i['name'] for i in edition['authors']]
    else:
        authors = []
    a = amazon_merge.build_amazon(edition, authors)
    assert isinstance(asin, basestring)
    assert thing_type == '/type/edition'
    #print edition['asin'], ekey
    if 'source_records' in thing:
        if 'amazon:' + asin in thing['source_records']:
            return True
        return source_records_match(a, thing)

    #print 'no source records'
    mc = get_mc(ekey)
    #print 'mc:', mc
    if mc == 'amazon:' + asin:
        return True
    if not mc:
        return False
    data = get_from_local(mc)
    e1 = build_marc(fast_parse.read_edition(data))
    return amazon_merge.attempt_merge(a, e1, threshold, debug=False)

def import_file(filename):
    for asin, edition in read_amazon_file(open(filename)):
        index_fields = build_index_fields(asin, edition)
        found = pool.build(index_fields)
        if 'title' not in found:
            print found
            print asin
            print edition
            print index_fields
            print

        if not found['title'] and not found['isbn']:
            #print 'no pool load book:', asin
            # TODO load book
            continue
        #print asin, found
        #print `edition['title'], edition.get('subtitle', None), edition.get('flags', None), edition.get('binding', None)`
        if 'sims' in edition:
            del edition['sims']
        #print edition
        #print

        seen = set()
        for k, v in found.iteritems():
            for ekey in v:
                if ekey in seen:
                    continue
                keys, thing = follow_redirects(ekey)
                seen.update(keys)
                assert thing
                try:
                    m = try_merge(edition, ekey, thing)
                except:
                    print asin
                    print edition
                    print ekey
                    print found
                    raise

# import_file(sys.argv[1])

d = sys.argv[1]
for f in os.listdir(d):
    if not f.startswith('amazon.'):
        continue
    print f
    if '2009-02' in f:
        continue
    import_file(d + "/" + f)

########NEW FILE########
__FILENAME__ = list_done
from lxml.html import fromstring, tostring
from openlibrary.catalog.utils.arc import read_arc, read_body
import re, os, sys

arc_dir = '/2/edward/amazon/arc'
total = 0
srtitle = 0
producttitle = 0

re_book_url = re.compile('^http://www.amazon.com/[^/]+/dp/([0-9A-Z]{10})/')
re_result_count = re.compile('^Showing ([,0-9]+) - ([,0-9]+) of ([,0-9]+) Results$')

bad_serp = 0

out = open('/2/edward/amazon/crawled2', 'w')

for filename in (i for i in os.listdir(arc_dir) if i.endswith('.arc')):
    print filename, total, srtitle, producttitle
    for url, wire in read_arc(arc_dir +'/' + filename):
        if url.startswith('file'):
            continue
        if not url.startswith('http://www.amazon.com/s?'):
            continue
        body = read_body(wire)
        doc = fromstring(body)
        found = []
        try:
            doc.get_element_by_id('noResultsTitle')
#            print 'no results:', url
            continue
        except KeyError:
            pass
        rc = doc.find_class('resultCount')
        if rc:
            m = re_result_count.match(rc[0].text)
            if m:
                (a, b, c) = map(lambda i: int(i.replace(',','')), m.groups())
                if a == c + 1 and b == c:
#                    print 'result count:', rc[0].text
#                    print 'empty page'
                    continue
        for e in doc.find_class('fastTrackList'):
            if e.text == 'This item is currently not available.':
                print e.text
                
        for pt in doc.find_class('productTitle'):
            assert pt.tag == 'div'
            assert pt[0].tag == 'a'
            href = pt[0].attrib['href']
            m = re_book_url.match(href)
            found.append(m.group(1))
            total += 1
            producttitle += 1

        for e in doc.find_class('srTitle'):
            td = e.getparent().getparent()
            assert td.tag == 'td'
            assert td[0].tag == 'a'
            href = td[0].attrib['href']
            m = re_book_url.match(href)
            found.append(m.group(1))
            total += 1
            srtitle += 1

        if len(found) == 0:
            print url
            bad_serp += 1
            open('bad_serp%d.html' % bad_serp, 'w').write(body)
        for asin in found:
            print >> out, asin
out.close()

########NEW FILE########
__FILENAME__ = load_merge
from time import time
from pprint import pprint
from catalog.marc.MARC21 import MARC21Record
from catalog.marc.parse import pick_first_date
import urllib2

entity_fields = ('name', 'birth_date', 'death_date', 'date')

def find_entity(site, entity):
    entity = dict((k, entity[k]) for k in entity_fields if k in entity)
    print entity
    things = site.things(entity)
    if not things:
        print "person not found"
        return

    print "found", len(things), "match"
    for key in things:
        db_entity = site.withKey(key, lazy=False)._get_data()
        pprint(db_entity)
        for field in entity_fields:
            if field in entity:
                assert field in db_entity
            else:
                assert field not in db_entity

def get_from_archive(locator):
    (file, offset, length) = locator.split (":")
    offset = int (offset)
    length = int (length)

    r0, r1 = offset, offset+length-1
    url = 'http://www.archive.org/download/%s'% file

    assert 0 < length < 100000

    ureq = urllib2.Request(url, None, {'Range':'bytes=%d-%d'% (r0, r1)},)
    result = urllib2.urlopen(ureq).read(100000)
    rec = MARC21Record(result)
    return rec

def contrib(r):
    contribs = []
    for f in r.get_fields('700'):
        print f.subfield_sequence
        contrib = {}
        if 'a' not in f.contents and 'c' not in f.contents:
            continue # should at least be a name or title
        name = " ".join([j.strip(' /,;:') for i, j in f.subfield_sequence if i in 'abc'])
        if 'd' in f.contents:
            contrib = pick_first_date(f.contents['d'])
            contrib['db_name'] = ' '.join([name] + f.contents['d'])
        else:
            contrib['db_name'] = name
        contrib['name'] = name
        contrib['entity_type'] = 'person'
        subfields = [
            ('a', 'personal_name'),
            ('b', 'numeration'),
            ('c', 'title')
        ]
        for subfield, field_name in subfields:
            if subfield in f.contents:
                contrib[field_name] = ' '.join([x.strip(' /,;:') for x in f.contents[subfield]])
        if 'q' in f.contents:
            contrib['fuller_name'] = ' '.join(f.contents['q'])
        contribs.append(contrib)

    for f in r.get_fields('710'):
        print f.subfield_sequence
        contrib = {
            'entity_type': 'org',
            'name': " ".join([j.strip(' /,;:') for i, j in f.subfield_sequence if i in 'ab'])
        }
        contrib['db_name'] = contrib['name']
        contribs.append(contrib)

    for f in r.get_fields('711'):
        print f.subfield_sequence
        contrib = {
            'entity_type': 'event',
            'name': " ".join([j.strip(' /,;:') for i, j in f.subfield_sequence if i in 'acdn'])
        }
        contrib['db_name'] = contrib['name']
        contribs.append(contrib)
    return contribs

def load(site, filename):
    for line in open(filename):
        isbn, lc_src, amazon = eval(line)
        versions = site.versions({'machine_comment': lc_src})
        assert len(versions) == 1
        thing = site.withID(versions[0]['thing_id'])
        
        if 'authors' not in amazon:
            continue
        author_count = 0
        for name, role in amazon['authors']:
            if role != 'Author':
                continue
            author_count+=1
            if author_count > 1:
                break
        if author_count < 2:
            continue

        print lc_src
        print 'amazon:', amazon['authors']


        try:
            print 'LC authors:', [x.name for x in thing.authors]
        except AttributeError:
            print 'no authors in LC'
        lc_contrib = []
        try:
            lc_contrib = thing.contributions
            print 'LC contributions:', lc_contrib
        except AttributeError:
            print 'no contributions in LC'
        if lc_contrib:
            r = get_from_archive(lc_src)
            contrib_detail = contrib(r)
            assert len(lc_contrib) == len(contrib_detail)
            for c, detail in zip(lc_contrib, contrib_detail):
                print c,
                find_entity(site, detail)
        print
        continue
        print "LC"
        pprint (thing._get_data())
        print "Amazon"
        pprint (amazon)
        print
    #    for x in web.query("select thing_id from version where machine_comment=" + web.sqlquote(lc)):
    #        t = site.withID(x.thing_id)
    #        print t.title



########NEW FILE########
__FILENAME__ = other_editions
import re, os.path, urllib2
from BeautifulSoup import BeautifulSoup

# http://amazon.com/other-editions/dp/0312153325 has:
# http://www.amazon.com/gp/product/0312247869
re_link = re.compile('^http://www\.amazon\.com/(?:(.*)/dp|gp/product)/(\d{9}[\dX]|B[A-Z0-9]+)$')

desc_skip = set(['(Bargain Price)', '(Kindle Book)'])

def read_bucket_table(f):
    html = ''
    bucket = False
    table = False
    for line in f:
        if line[:-1] == '<div class="bucket">':
            bucket = True
            continue
        if bucket and line[:-1] == '   <table border="0" cellpadding="2" cellspacing="0">':
            table = True
        if table:
            html += line
            if line[:-1] == '   </table>':
                break
    return html

def parse_html(html):
    soup = BeautifulSoup(html)
    for tr in soup('tr')[2:]:
        td = tr('td')
        assert len(td) == 3
        td0 = td[0]
        assert td0['class'] == 'small'
        assert len(td0) == 3
        (nl, link, desc) = td0
        assert nl == '\n'
        href = link['href']
        if href.startswith("http://www.amazon.com:80/gp/redirect.html"):
            # audio book, skip for now
            continue
        m = re_link.match(link['href'])
        yield str(m.group(2)), desc.strip()

def get_from_amazon(isbn):
    url = 'http://www.amazon.com/dp/other-editions/' + isbn
    try:
        return urllib2.urlopen(url).read()
    except urllib2.HTTPError, error:
        if error.code != 404:
            raise
        return ''

def find_others(isbn, dir):
    filename = dir + "/" + isbn
    if len(isbn) != 10:
        return []
    if not os.path.exists(filename):
        open(filename, 'w').write(get_from_amazon(isbn))
    html = read_bucket_table(open(dir + "/" + isbn))
    if not html:
        return []
    l = [i for i in parse_html(html) if not i[0].startswith('B') and i[1] not in desc_skip]
    return l

########NEW FILE########
__FILENAME__ = parse
from lxml.html import parse, tostring
import re, os, sys, web
from warnings import warn
from math import floor
from pprint import pprint
import htmlentitydefs

class BrokenTitle(Exception):
    pass

class IncompletePage(Exception):
    pass

class MissingAuthor(Exception):
    pass

role_re = re.compile("^ \(([^)]+)\)")

#: sample: ' [Paperback, Large Print]'

re_title = re.compile("""
    (?:\ \[([A-Za-z, ]+)\])? # flags
    (?:\(\ ([^()]+|[^()]*\(.*\)[^()]*)\))?
    """, re.MULTILINE | re.X)

re_split_title = re.compile(r'''^
    (.+?(?:\ \(.+\))?)
    (?::\ (\ *[^:]+))?$
''', re.X)

re_missing_author = re.compile('\n\n(~  )?\(([A-Za-z, ]+)\), ')

re_list_price = re.compile('^\$([\d,]+)\.(\d\d)$')
re_amazon_price = re.compile('^\$([\d,]+)\.(\d\d)$')
# '$0.04\n      \n    '
re_you_save = re.compile('^\$([\d,]+)\.(\d\d)\s*\((\d+)%\)\s*$')

re_pages = re.compile('^\s*(\d+)(?:\.0)? pages\s*$')
re_sales_rank = re.compile('^ #([0-9,]+) in Books')
re_html_in_title = re.compile('</?(i|em|br)>', re.I)

def unescape(text):
    def fixup(m):
        text = m.group(0)
        if text[:2] == "&#":
            # character reference
            try:
                if text[:3] == "&#x":
                    return unichr(int(text[3:-1], 16))
                else:
                    return unichr(int(text[2:-1]))
            except ValueError:
                pass
        else:
            # named entity
            try:
                text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])
            except KeyError:
                pass
        return text # leave as is
    return re.sub("&#?\w+;", fixup, text)

def to_dict(k, v):
    return {k: v} if v else None

def read_authors(by_span):
    authors = []
    if re_missing_author.match(by_span.text):
        raise MissingAuthor
    try:
        assert by_span.text in ('\n\n', '\n\n~ ')
    except:
        print `by_span.text`
        raise
    expect_end = False
    for e in by_span:
        if expect_end:
            assert e.tag in ('br', 'span')
            break
        assert e.tag == 'a'
        if e.tail.endswith('\n\n'):
            expect_end = True
        else:
            assert e.tail.endswith(', ')
        m = role_re.match(e.tail)
        if m:
            authors.append({ 'name': e.text, 'role': m.group(1), 'href': e.attrib['href'] })
        else:
            authors.append({ 'name': e.text, 'href': e.attrib['href'] })
    return authors

def get_title_and_authors(doc, title_from_html):
    try:
        prodImage = doc.get_element_by_id('prodImage')
    except KeyError:
        raise IncompletePage
    full_title = unescape(prodImage.attrib['alt']) # double quoted
    full_title = re_html_in_title.sub('', full_title).replace('&apos;', "'")

    m = re_split_title.match(full_title)
    (title, subtitle) = m.groups()
    # maybe need to descape title
    title_id = doc.get_element_by_id('btAsinTitle')
    assert title_id.tag == 'span'
    assert title_id.getparent().tag == 'h1'
    assert title_id.getparent().attrib['class'] == 'parseasinTitle'
    buying_div = title_id.getparent().getparent()
    assert buying_div.tag == 'div'
    assert buying_div.attrib['class'] == 'buying'
    by_span = buying_div[1]
    assert by_span.tag == 'span'

    book = {
        'full_title': full_title,
        'title': title,
        'has_cover_img': "no-image-avail" not in prodImage.attrib['src']
    }

    authors = []
    if len(by_span) and by_span[0].tag == 'a':
        #print len(by_span), [e.tag for e in by_span]
        book['authors'] = read_authors(by_span)
    title_text = title_id.text_content()
    if not title_text.startswith(full_title):
        print 'alt:', `prodImage.attrib['alt']`
        print 'title mistmach:', `full_title`, '!=', `title_text`
        title_text = title_from_html.decode('latin-1')
        print 'title_text:', `title_text`
        print 'full_title:', `full_title`
    if not title_text.startswith(full_title):
        print 'alt:', `prodImage.attrib['alt']`
        print 'title mistmach:', `full_title`, '!=', `title_text`
        raise BrokenTitle
    if full_title != title_text:
        btAsinTitle = title_text[len(full_title):]
        m = re_title.match(btAsinTitle)
        if not m:
            print 'title:', `btAsinTitle`
        (flag, binding) = m.groups()
        if binding is not None:
            book['binding'] = binding
        if flag:
            book['flag'] = flag
    if subtitle:
        book['subtitle'] = subtitle

    return book

def dollars_and_cents(dollars, cents):
    # input: dollars and cents as strings
    # output: value in cents as an int
    return int(dollars.replace(',', '')) * 100 + int(cents)

def read_price_block(doc):
    price_block = doc.get_element_by_id('priceBlock', None)
    book = {}
    if price_block is None:
        return
    assert price_block.tag == 'div' and price_block.attrib['class'] == 'buying'
    table = price_block[0]
    assert table.tag == 'table' and table.attrib['class'] == 'product'
    for tr in table:
        assert tr.tag == 'tr' and len(tr) == 2
        assert all(td.tag == 'td' for td in tr)
        heading = tr[0].text
        value = tr[1].text_content()

        if heading == 'List Price:':
            m = re_list_price.match(value)
            list_price = dollars_and_cents(m.group(1), m.group(2))
            book["list_price"] = list_price
        elif heading == "Price:":
            b = tr[1][0]
            assert b.tag == 'b' and b.attrib['class'] == 'priceLarge'
            m = re_amazon_price.match(b.text)
            amazon_price = dollars_and_cents(m.group(1), m.group(2))
            book["amazon_price"] = amazon_price
        elif heading == 'You Save:':
            continue # don't need to check
            # fails for 057124954X: '$0.04\n      \n    '
            m = re_you_save.match(value)
            you_save = dollars_and_cents(m.group(1), m.group(2))
            assert list_price - amazon_price == you_save
            assert floor(float(you_save * 100) / list_price + 0.5) == int(m.group(3))
        elif heading == 'Value Priced at:':
            continue # skip
            m = re_amazon_price.match(value)
            book["value_priced_at"] = dollars_and_cents(m.group(1), m.group(2))
        elif heading == 'Import List Price:':
            pass

    return book

def find_avail_span(doc):
    for div in doc.find_class('buying'):
        if div.tag != 'div' or not len(div):
            continue
        if div[0].tag == 'span':
            span = div[0]
        elif div[0].tag == 'br' and div[1].tag == 'b' and div[2].tag == 'span':
            span = div[2]
        else:
            continue
        if span.attrib['class'].startswith('avail'):
            return span

def read_avail(doc):
    traffic_signals = set(['Red', 'Orange', 'Green'])
    span = find_avail_span(doc)
    color = span.attrib['class'][5:]
    assert color in traffic_signals
    gift_wrap = span.getnext().getnext().tail
    book = {
        'avail_color': color,
        'amazon_availability': span.text,
        'gift_wrap': bool(gift_wrap) and 'Gift-wrap available' in gift_wrap
    }
    return book

def read_other_editions(doc):
    oe = doc.get_element_by_id('oeTable', None)
    if oe is None:
        return
    assert oe.tag == 'table' and oe.attrib['class'] == 'otherEditions'
    assert len(oe) == 2 and len(oe[0]) == 2 and len(oe[1]) == 2
    assert oe[0][0][0].tag == 'a'
    oe = oe[0][0][1]
    assert oe.tag == 'table'
    other_editions = []
    for tr in oe[1:]:
        assert tr.tag == 'tr'
        if 'bgcolor' in tr.attrib:
            assert tr.attrib['bgcolor'] == '#ffffff'
        else:
            assert tr[0].attrib['id'] == 'oeShowMore'
            break
        assert tr[0].attrib['class'] == 'tiny'
        a = tr[0][0]
        assert a.tag == 'a'
        row = [a.attrib['href'][-10:], a.text, a.tail.strip()]
        other_editions.append(row)
    return {'other_editions': other_editions }

def read_sims(doc):
    sims = doc.find_class('sims-faceouts')
    if len(sims) == 0:
        return
    assert len(sims) == 1
    sims = sims[0]
    assert sims.tag == 'table'
    found = []
    if sims[0].tag == 'tbody':
        tr = sims[0][0]
    else:
        assert sims[0].tag == 'tr'
        tr = sims[0]
    for td in tr:
        assert td.tag == 'td'
        a = td[1][0]
        assert a.tag == 'a'
        found.append({'asin': a.attrib['href'][-10:], 'title': a.text})
    return to_dict('sims', found)

def find_product_details_ul(doc):
    a = doc.get_element_by_id('productDetails', None)
    if a is None:
        return
    try:
        assert a.tag == 'a' and a.attrib['name'] == 'productDetails'
    except:
        print tostring(a)
        raise
    hr = a.getnext()
    assert hr.tag == 'hr' and hr.attrib['class'] == 'bucketDivider'
    table = hr.getnext()
    td = table[0][0]
    assert td.tag == 'td' and td.attrib['class'] == 'bucket'
    h2 = td[0]
    assert h2.tag == 'h2' and h2.text == 'Product Details'
    div = td[1]
    assert div.tag == 'div' and div.attrib['class'] == 'content'
    ul = div[0]
    if div[0].tag == 'table':
        ul = div[1]
    assert ul.tag == 'ul'
    assert ul[-1].tag == 'div' and ul[-2].tag == 'p'
    return ul

def read_li(li):
    assert li.tag == 'li'
    b = li[0]
    assert b.tag == 'b'
    return b

re_series = re.compile('^<li>(?:This is item <b>(\d+)</b> in|This item is part of) <a href="?/gp/series/(\d+).*?><b>The <i>(.+?)</i> Series</b></a>\.</li>')

def read_series(doc):
    ul = doc.find_class('linkBullets')
    if len(ul) == 0:
        return
    assert len(ul) == 1
    ul = ul[0]
    if len(ul) == 0:
        return
    li = ul[0]
    assert li.tag == 'li'
    (series_num, series_id, series) = re_series.match(tostring(li)).groups()
    found = {}
    if series_num:
        found["series_num"] = int(series_num)
    found["series"] = series
    found["series_id"] = series_id
    return found

def read_product_details(doc):
    ul = find_product_details_ul(doc)
    if ul is None:
        return

    headings = {
        'Publisher': 'publisher',
        'Language': 'language',
        'ISBN-10': 'isbn_10',
        'ISBN-13': 'isbn_13',
        'ASIN': 'asin',
        'Product Dimensions': 'dimensions',
        'Shipping Weight': 'shipping_weight',
    }

    found = {}
    ul_start = 0
    if 'Reading level' in ul[0][0].text:
        ul_start = 1
        li = ul[0]
        b = read_li(li)
        found['reading_level'] = b.tail.strip()

    li = ul[ul_start]
    b = read_li(li)
    (binding, pages) = (b.text, b.tail)
    if binding[-1] == ':':
        binding = binding[:-1]
    found['binding'] = binding
    if pages:
        m = re_pages.match(pages)
        if m:
            found['number_of_pages'] = int(m.group(1))
        else:
            warn("can't parse number_of_pages: " + pages)

    seen_average_customer_review = False
    for li in ul[ul_start + 1:-2 if ul[-3].tag != 'br' else -3]:
#        if li.tag == 'p' and len(li) == 0:
#            continue
        b = read_li(li)
        h = b.text.strip(': \n')
        if h in ('Also Available in', 'In-Print Editions'):
            break
        if seen_average_customer_review:
            break
        if h == 'Amazon.com Sales Rank':
            m = re_sales_rank.match(b.tail)
            found['sales_rank'] = int(m.group(1).replace(",", ""))
            break
        if h in ('Shipping Information', 'Note', 'Shipping'):
            continue
        if h == 'Average Customer Review':
            seen_average_customer_review = True
            continue
        if h == 'Shipping Weight':
            found['shipping_weight'] = b.tail.strip('( ')
            continue
        heading = headings[h]
        found[heading] = b.tail.strip()
    return found

re_pub_date = re.compile("^(.*) \((.*\d{4})\)$")
re_pub_edition = re.compile("^(.*); (.*)$")

def parse_publisher(edition):
    if 'publisher' in edition:
        m = re_pub_date.match(edition["publisher"])
        if m:
            edition["publisher"] = m.group(1)
            edition["publish_date"] = m.group(2)
        m = re_pub_edition.match(edition["publisher"])
        if m:
            edition["publisher"] = m.group(1)
            edition["edition"] = m.group(2)

re_latest_blog_posts = re.compile('\s*(.*?) latest blog posts')
re_plog_link = re.compile('^/gp/blog/([A-Z0-9]+)$')

def read_plog(doc):
    div = doc.get_element_by_id('plog', None)
    if div is None:
        return
    assert div.tag == 'div' and div.attrib['class'] == 'plog'
    table = div[1]
    b = table[0][0][0]
    assert b.tag == 'b' and b.attrib['class'] == 'h1'
    m = re_latest_blog_posts.match(b.text)
    name = m.group(1)
    found = {}
    if name.endswith("'s"):
        found["plog_name"] = name[:-2]
    else:
        assert name.endswith("s'")
        found["plog_name"] = name[:-1]
    div = table[2][1][0]
    found["plog_img"] = div[0].attrib['src'].replace(".T.", ".L.")
    ul = div[-1]
    assert ul.tag == 'ul' and ul.attrib['class'] == 'profileLink'
    li = ul[0]
    assert li.tag == 'li' and li.attrib['class'] == 'carat'
    assert li[0].tag == 'a'

    href = li[0].attrib['href']
    m = re_plog_link.match(href)
    found["plog_id"] = m.group(1)

    return found

re_cite = {
    'citing': re.compile('\nThis book cites (\d+) \nbook(?:s)?:'),
    'cited': re.compile('\n(\d+) \nbook(?:s)? \ncites? this book:')
}
    
def read_citing(doc):
    div = doc.get_element_by_id('bookCitations', None)
    found = {}
    if div is None:
        return
    content = div[0][2]
    assert content.tag == 'div' and content.attrib['class'] == 'content'
    a = content[0]
    assert a.tag == 'a'
    b = content[1]
    name = a.attrib['name']
    assert name in ('citing', 'cited')
    found[name] = b.text
    if len(content) > 7:
        print len(content)
        for num, i in enumerate(content):
            print num, i.tag, i.attrib
        a = content[8]
        assert a.tag == 'a'
        b = content[9]
        assert a.attrib['name'] == 'cited'
        found['cited'] = b.text
    for k, v in found.items():
        m = re_cite[k].match(v)
        found[k] = int(m.group(1))
    return found

def find_inside_this_book(doc):
    for b in doc.find_class('h1'):
        if b.text == 'Inside This Book':
            assert b.tag == 'b'
            return b.getparent()
    return None

def read_first_sentence(inside):
    if len(inside) == 4:
        assert inside[2].tag == 'span'
        assert inside[2].attrib['class'] == 'tiny'
        assert inside[2][0].tail.strip() == 'Browse and search another edition of this book.'
        div = inside[3]
    else:
        assert len(inside) == 3
        div = inside[2]
    assert div.tag == 'div' and div.attrib['class'] == 'content'
    if div[0].tag in ('a', 'b'):
        assert div[0].text != 'First Sentence:'
        return
    assert div[0].tag == 'strong'
    assert div[0].text == 'First Sentence:'
    assert div[1].tag == 'br'
    return div[1].tail.strip(u"\n \xa0")

def find_bucket(doc, text):
    for div in doc.find_class('bucket'):
        h2 = div[0]
        if h2.tag == 'h2' and h2.text == text:
            return div
    return None

# New & Used Textbooks

def read_subject(doc):
    div = find_bucket(doc, 'Look for Similar Items by Subject')
    if div is None:
        return
    assert div.tag == 'div'
    form = div[1][0]
    assert form.tag == 'form'
    input = form[0]
    assert input.tag == 'input' and input.attrib['type'] == 'hidden' \
        and input.attrib['name'] == 'index' \
        and input.attrib['value'] == 'books'
    found = []
    for input in form[3:-4:3]:
        a = input.getnext()
        assert a.tag == 'a'
        found_text = a.text if len(a) == 0 else a[0].text
        assert found_text is not None
        found.append(found_text)
    return to_dict('subjects', found)

def read_category(doc):
    div = find_bucket(doc, 'Look for Similar Items by Category')
    if div is None:
        return
    assert div.tag == 'div'
    ul = div[1][0]
    assert ul.tag == 'ul'
    found = []
    for li in ul:
        assert all(a.tail == ' > ' for a in li[:-1])
        cat = [a.text for a in li]
        if cat[-1] == 'All Titles':
            cat.pop()
        found.append(tuple(cat))
#        if 'Series' in cat:
#            edition["series2"] = cat
    # maybe strip 'Books' from start of category
    found = [i[1:] if i[0] == 'Books' else i for i in found]
    return to_dict('category', found)

def read_tags(doc):
    table = doc.find_class('tag-cols')
    if len(table) == 0:
        return
    assert len(table) == 1
    table = table[0]
    assert len(table) == 1
    tr = table[0]

def read_edition(doc, title_from_html=None):
    edition = {}
    book = get_title_and_authors(doc, title_from_html)
    edition.update(book)

    ret = read_price_block(doc)
    if ret:
        edition.update(ret)
    inside = find_inside_this_book(doc)
    if inside is not None:
        sentence = read_first_sentence(inside)
        if sentence:
            edition['first_sentence'] = sentence
    func = [
        #read_citing,
        read_plog,
        read_series,
        #read_avail,
        read_product_details,
        read_other_editions,
        #read_sims, # not needed now
        read_subject,
        read_category,
    ]
    for f in func:
        ret = f(doc)
        if ret:
            edition.update(ret)
    parse_publisher(edition)
    if 'isbn_10' not in edition and 'asin' not in edition:
        return None
    return edition

# ['subtitle', 'binding', 'shipping_weight', 'category', 'first_sentence',  'title', 'full_title', 'authors', 'dimensions', 'publisher', 'language', 'number_of_pages', 'isbn_13', 'isbn_10', 'publish_date']
def edition_to_ol(edition):
    ol = {}
    fields = ['title', 'subtitle', 'publish_date', 'number_of_pages', 'first_sentence']
    for f in fields:
        if f in edition:
            ol[f] = edition[f]
    if 'isbn_10' in edition:
        ol['isbn_10'] = [edition['isbn_10']]
    if 'isbn_13' in edition:
        ol['isbn_13'] = [edition['isbn_13'].replace('-','')]
    if 'category' in edition:
        ol['subjects'] = edition['category']
    if 'binding' in edition:
        ol['physical_format'] = edition['binding']
    if 'dimensions' in edition:
        ol['physical_dimensions'] = edition['dimensions']
    if 'shipping_weight' in edition:
        ol['weight'] = edition['shipping_weight']
    if 'authors' in edition:
        ol['authors'] = [a for a in edition['authors'] if a['name'] != 'n/a']
    if 'publisher' in edition:
        ol['publishers'] = [edition['publisher']]
    else:
        print 'publisher missing'

    for k, v in ol.iteritems():
        if isinstance(v, basestring) and v[-1] == '(':
            pprint(edition)
            print 'ends with "(":', `k, v`
            sys.exit(0)

    return ol

if __name__ == '__main__':
    #for dir in ('/2008/sample/', 'pages/'):
    page_dir = sys.argv[1]
    for filename in os.listdir(page_dir):
        #if '1435438671' not in filename:
        #    continue
        if filename.endswith('.swp'):
            continue
        edition = {}
        doc = parse(page_dir + '/' + filename).getroot()
        assert doc is not None
        edition = read_edition(doc)
        ol = edition_to_ol(edition)
        pprint (ol)

########NEW FILE########
__FILENAME__ = read_serp
from lxml.html import fromstring
from openlibrary.catalog.utils.arc import read_arc, read_body
import os, re

arc_dir = '/2/edward/amazon/arc'

re_book_url = re.compile('^http://www.amazon.com/[^/]+/dp/([0-9A-Z]{10})/')
re_result_count = re.compile('^Showing ([,0-9]+) - ([,0-9]+) of ([,0-9]+) Results$')
re_title = re.compile('<title>Amazon.com: (.*?)(:?, Page \d+)?</title>')
crawled = set(i[:-1] for i in open('/2/edward/amazon/crawled'))

# /2/edward/amazon/arc/20100311*.arc

def find_pt(doc):
    found = []
    for pt in doc.find_class('productTitle'):
        assert pt.tag == 'div'
        assert pt[0].tag == 'a'
        href = pt[0].attrib['href']
        m = re_book_url.match(href)
        print m.group(1)
        found.append(m.group(1))
    return found

def find_srtitle(doc):
    found = []
    for e in doc.find_class('srTitle'):
        td = e.getparent().getparent()
        assert td.tag == 'td'
        assert td[0].tag == 'a'
        href = td[0].attrib['href']
        m = re_book_url.match(href)
        found.append(m.group(1))
    return found

found_books = set()

prev = ''
#out = open('/2/edward/amazon/best_sellers2', 'w')
for filename in (i for i in os.listdir(arc_dir) if i.endswith('.arc')):
    if not filename.startswith('20100412'):
        continue
    for url, wire in read_arc(arc_dir +'/' + filename):
        #print filename, url
        if url.startswith('file'):
            continue
        if not url.startswith('http://www.amazon.com/s?'):
            continue
        body = read_body(wire)
        m = re_title.search(body)
        if m.group(1) != prev:
            print m.group(1)
            prev = m.group(1)
        continue
        doc = fromstring(body)
        try:
            doc.get_element_by_id('noResultsTitle')
            continue
        except KeyError:
            pass
        rc = doc.find_class('resultCount')
        if rc:
            m = re_result_count.match(rc[0].text)
            if m:
                (a, b, c) = map(lambda i: int(i.replace(',','')), m.groups())
                if a == c + 1 and b == c:
                    continue
        for e in doc.find_class('fastTrackList'):
            if e.text == 'This item is currently not available.':
                print e.text

        assert len(find_pt(doc)) == 0
        serp_found = find_srtitle(doc)
        for asin in serp_found:
            if asin in crawled:
                continue
            if asin not in found_books:
                print >> out, asin
        found_books.update(serp_found)
        print len(serp_found), len(found_books), filename, url

#out.close()

########NEW FILE########
__FILENAME__ = upload
from catalog.read_rc import read_rc
import httplib, web, time, sys
from datetime import date, timedelta

rc = read_rc()
accesskey = rc['s3_accesskey']
secret = rc['s3_secret']

db = web.database(dbn='mysql', host=rc['ia_db_host'], user=rc['ia_db_user'], \
        passwd=rc['ia_db_pass'], db='archive')
db.printing = False

crawl_dir = '/1/edward/amazon/crawl'
collection = 'ol_data'
mediatype = 'data'

con = httplib.HTTPConnection('s3.us.archive.org')
con.connect()

def wait_for_upload(ia):
    while True:
        rows = list(db.select('catalog', where='identifier = $ia', vars={'ia': ia}))
        if len(rows) == 0:
            return
        print "\r", len(rows), 'tasks still running',
        time.sleep(5)
    print '\ndone'

no_bucket_error = '<Code>NoSuchBucket</Code>'
internal_error = '<Code>InternalError</Code>'

def put_file(con, ia, filename, headers):
    print 'uploading %s' % filename
    headers['authorization'] = "LOW " + accesskey + ':' + secret
    url = 'http://s3.us.archive.org/' + ia + '/' + filename
    print url
    data = open(crawl_dir + '/' + filename).read()
    for attempt in range(5):
        con.request('PUT', url, data, headers)
        res = con.getresponse()
        body = res.read()
        if '<Error>' not in body:
            return
        print 'error'
        print body
        if no_bucket_error not in body and internal_error not in body:
            sys.exit(0)
        print 'retry'
        time.sleep(5)
    print 'too many failed attempts'

def create_item(con, ia, cur_date):
    headers = {
        'x-amz-auto-make-bucket': 1,
        'x-archive-meta01-collection': collection,
        'x-archive-meta-mediatype': mediatype,
        'x-archive-meta-language': 'eng',
        'x-archive-meta-title': 'Amazon crawl ' + cur_date,
        'x-archive-meta-description': 'Crawl of Amazon. Books published on ' + cur_date + '.',
        'x-archive-meta-year': cur_date[:4],
        'x-archive-meta-date': cur_date.replace('-', ''),
    }

    filename =  'index.' + cur_date
    put_file(con, ia, filename, headers)

def upload_index(con, cur_date):
    ia = 'amazon_crawl.' + cur_date 

    create_item(con, ia, cur_date)
    wait_for_upload(ia)
    time.sleep(5)

    put_file(con, ia, 'amazon.' + cur_date, {})
    put_file(con, ia, 'cats.' + cur_date, {})
    put_file(con, ia, 'list.' + cur_date, {})

one_day = timedelta(days=1)
cur = date(2009, 4, 26) # start from
while True:
    print cur
    upload_index(con, str(cur))
    cur -= one_day

con.close()

########NEW FILE########
__FILENAME__ = upload_arc
from openlibrary.catalog.read_rc import read_rc
import httplib, web, time, sys, os

rc = read_rc()
accesskey = rc['s3_accesskey']
secret = rc['s3_secret']
#arc_dir = '/2/edward/amazon/arc'
arc_dir = '/0/amazon'

no_bucket_error = '<Code>NoSuchBucket</Code>'
internal_error = '<Code>InternalError</Code>'

done = [
    '20100210013733.arc',
    '20100210015013.arc',
    '20100210020316.arc',
    '20100210021445.arc',
    '20100210022726.arc',
    '20100210024019.arc',
    '20100210025249.arc',
    '20100210030609.arc',
    '20100210031752.arc',
    '20100210033024.arc',
    '20100210034255.arc',
    '20100210035501.arc',
    '20100210040904.arc',
    '20100210042130.arc',
    '20100210043351.arc',
    '20100210044553.arc',
    '20100210051017.arc',
    '20100210052258.arc',
    '20100210053601.arc',
    '20100210194700.arc',
    '20100210201110.arc',
    '20100212000643.arc',
    '20100212001705.arc',
    '20100212002656.arc',
    '20100212004512.arc',
    '20100212010934.arc',
    '20100212013415.arc',
    '20100212015925.arc',
    '20100212022248.arc',
    '20100212024600.arc',
    '20100212030916.arc',
    '20100212033221.arc',
    '20100212035616.arc',
    '20100212042043.arc',
    '20100212044622.arc',
    '20100212051112.arc',
    '20100212053604.arc',
    '20100212060140.arc',
    '20100212062647.arc',
    '20100212065128.arc',
    '20100212165731.arc',
    '20100212184748.arc',
    '20100212184807.arc',
    '20100212184822.arc',
    '20100212190147.arc',
    '20100212192404.arc',
    '20100212194513.arc',
    '20100212200700.arc',
    '20100212202810.arc',
    '20100212204852.arc',
    '20100212210951.arc',
    '20100212213032.arc',
    '20100212215107.arc'
]
    
def put_file(con, ia, filename, headers):
    print 'uploading %s' % filename
    headers['authorization'] = "LOW " + accesskey + ':' + secret
    url = 'http://s3.us.archive.org/' + ia + '/' + filename
    print url
    data = open(arc_dir + '/' + filename).read()
    for attempt in range(5):
        con.request('PUT', url, data, headers)
        res = con.getresponse()
        body = res.read()
        if '<Error>' not in body:
            return
        print 'error'
        print body
        if no_bucket_error not in body and internal_error not in body:
            sys.exit(0)
        print 'retry'
        time.sleep(5)
    print 'too many failed attempts'

ia = 'amazon_book_crawl'
for filename in os.listdir(arc_dir):
    if filename in done:
        continue
    if not filename.endswith('.arc'):
        continue
    print filename
    con = httplib.HTTPConnection('s3.us.archive.org')
    con.connect()
    put_file(con, ia, filename, {})
    con.close()


########NEW FILE########
__FILENAME__ = list_titles
titles = {}
with_title = {}

for line in open("/1/pharos/edward/titles"):
    try:
        loc, fields = eval(line)
    except SyntaxError:
        break
    except ValueError:
        continue
    t = [b for a, b in fields if a == 'c']
    if len(t) != 1:
        continue
    fields = tuple((a, b.strip('.') if a=='d' else b) for a, b in fields)
    title = t[0].strip(' ,.').lower()
    titles[title] = titles.get(title, 0) + 1
    with_title.setdefault(title, {})
    with_title[title][fields] = with_title[title].get(fields, 0) + 1

for k, v in sorted(((a, b) for a, b in titles.items() if b > 10), reverse=True, key=lambda x: x[1]):
    print `k`, v
    for a, b in sorted(((a, b) for a, b in with_title[k].items() if b > 5), reverse=True, key=lambda x: x[1])[0:30]:
        print '  ', a, b
    print

########NEW FILE########
__FILENAME__ = marc
from catalog.infostore import get_site
from catalog.marc.db.web_marc_db import search_query
from catalog.get_ia import get_data
from catalog.marc.fast_parse import get_all_subfields, get_tag_lines, get_first_tag, get_subfields
import sys
site = get_site()

name = sys.argv[1] # example: 'Leonardo da Vinci'
author_keys = site.things({'type': '/type/author', 'name': name})
print len(author_keys), 'authors found'

edition_keys = set()
for ak in author_keys:
    edition_keys.update(site.things({'type': '/type/edition', 'authors': ak}))
print len(edition_keys), 'editions found'

locs = set()
for ek in edition_keys:
    e = site.withKey(ek)
    for i in e.isbn_10 if e.isbn_10 else []:
        locs.update(search_query('isbn', i))
    for i in e.lccn if e.lccn else []:
        locs.update(search_query('lccn', i))
    for i in e.oclc_numbers if e.oclc_numbers else []:
        locs.update(search_query('oclc', i))
print len(locs), 'MARC records found'

def ldv(line):
    for s in ('1452', '1519', 'eonard', 'inci'):
        if line.find(s) != -1:
            return True
    return False
    
for loc in locs:
#    print loc
    data = get_data(loc)
    if not data:
        print "couldn't get"
        continue
    line = get_first_tag(data, set(['100', '110', '111']))
    if line and ldv(line):
        print list(get_all_subfields(line))

    line = get_first_tag(data, set(['700', '710', '711']))
    if line and ldv(line):
        print list(get_all_subfields(line))

########NEW FILE########
__FILENAME__ = merge
# -*- coding: utf-8 -*-
from openlibrary.catalog.importer.db_read import withKey, get_things, get_mc
from openlibrary.catalog.read_rc import read_rc
from openlibrary.catalog.utils import key_int, match_with_bad_chars, pick_best_author, remove_trailing_number_dot
from unicodedata import normalize
import web, re, sys, codecs, urllib
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, unmarshal, Reference
from openlibrary.catalog.utils.edit import fix_edition
from openlibrary.catalog.utils.query import query_iter

def urlread(url):
    return urllib.urlopen(url).read()

def norm(s):
    return normalize('NFC', s)

def copy_fields(from_author, to_author, name):
    new_fields = { 'name': name, 'personal_name': name }
    for k, v in from_author.iteritems():
        if k in ('name', 'personal_name', 'key', 'last_modified', 'type', 'id', 'revision'):
            continue
        if k in to_author:
            assert v == to_author[k]
        else:
            new_fields[k] = v
    return new_fields

def test_copy_fields():
    f = {'name': 'Sheila K. McCullagh', 'personal_name': 'Sheila K. McCullagh', 'last_modified': {'type': '/type/datetime', 'value': '2008-08-30 20:40:41.784992'}, 'key': '/a/OL4340365A', 'birth_date': '1920', 'type': {'key': '/type/author'}, 'id': 18087251, 'revision': 1}
    t = {'name': 'Sheila K. McCullagh', 'last_modified': {'type': '/type/datetime', 'value': '2008-04-29 13:35:46.87638'}, 'key': '/a/OL2622088A', 'type': {'key': '/type/author'}, 'id': 9890186, 'revision': 1}

    assert copy_fields(f, t, 'Sheila K. McCullagh') == {'birth_date': '1920', 'name': 'Sheila K. McCullagh', 'personal_name': 'Sheila K. McCullagh'}


def update_author(key, new):
    q = { 'key': key, }
    for k, v in new.iteritems():
        q[k] = { 'connect': 'update', 'value': v }
    print ol.write(q, comment='merge author')

def update_edition(ol, e, old, new, debug=False):
    key = e['key']
    if debug:
        print 'key:', key
        print 'old:', old
        print 'new:', new
    fix_edition(key, e, ol)
    authors = []
    if debug:
        print 'current authors:', e['authors']
    for cur in e['authors']:
        cur = cur['key']
        if debug:
            print old, cur in old
        a = new if cur in old else cur
        if debug:
            print cur, '->', a
        if a not in authors:
            authors.append(a)
    if debug:
        print 'authors:', authors
    e['authors'] = [{'key': a} for a in authors]

    try:
        ret = ol.save(key, e, 'merge authors')
    except:
        if debug:
            print e
        raise
    if debug:
        print ret

    update = []
    for wkey in e.get('works', []):
        need_update = False
        print 'work:', wkey
        w = ol.get(wkey)
        for a in w['authors']:
            if a['author'] in old:
                a['author'] = Reference(new)
                need_update = True
        if need_update:
            update.append(w)

    if update:
        ret = ol.save_many(update, 'merge authors')

def switch_author(ol, old, new, other, debug=False):
    q = { 'authors': old, 'type': '/type/edition', }
    for e in query_iter(q):
        if debug:
            print 'switch author:', e['key']
        print e
        e = ol.get(e['key'])
        update_edition(ol, e, other, new, debug)

def make_redirect(ol, old, new):
    r = {'type': {'key': '/type/redirect'}, 'location': new}
    ol.save(old, r, 'merge authors, replace with redirect')

re_number_dot = re.compile('\d{2,}[- ]*(\.+)$')

def do_normalize(author_key, best_key, authors):
    #print "do_normalize(%s, %s, %s)" % (author_key, best_key, authors)
    need_update = False
    a = ol.get(author_key)
    if author_key == best_key:
        for k, v in a.items():
            if 'date' in k:
                m = re_number_dot.search(v)
                if m:
                    need_update = True
                    v = v[:-len(m.group(1))]
            if not isinstance(v, unicode):
                continue
            norm_v = norm(v)
            if v == norm_v:
                continue
            a[k] = norm_v
            need_update = True
    else:
        best = ol.get(best_key)
        author_keys = set(k for k in a.keys() + best.keys() if k not in ('key', 'last_modified', 'type', 'id', 'revision'))
        for k in author_keys:
            if k not in best:
                v = a[k]
                if not isinstance(v, unicode):
                    continue
                norm_v = norm(v)
                if v == norm_v:
                    continue
                a[k] = norm_v
                need_update = True
                continue
            v = best[k]
            if 'date' in k:
                v = remove_trailing_number_dot(v)
            if isinstance(v, unicode):
                v = norm(v)
            if k not in a or v != a[k]:
                a[k] = v
                need_update = True
    if not need_update:
        return
    #print 'save(%s, %s)' % (author_key, `a`)
    ol.save(author_key, a, 'merge authors')

def has_image(key):
    url = 'http://covers.openlibrary.org/a/query?olid=' + key[3:]
    ret = urlread(url).strip()
    return ret != '[]'

def merge_authors(ol, keys, debug=False):
#    print 'merge author %s:"%s" and %s:"%s"' % (author['key'], author['name'], merge_with['key'], merge_with['name'])
#    print 'becomes: "%s"' % `new_name`
    authors = [a for a in (withKey(k) for k in keys) if a['type']['key'] != '/type/redirect']
    not_redirect = set(a['key'] for a in authors)
    if debug:
        for a in authors:
            print a

    assert all(a['type']['key'] == '/type/author' for a in authors)
    name1 = authors[0]['name']
    for a in authors:
        print `a['key'], a['name']`
    assert all(match_with_bad_chars(a['name'], name1) for a in authors[1:])

    best_key = pick_best_author(authors)['key']

    imgs = [a['key'] for a in authors if a['key'] != '/a/OL2688880A' and has_image(a['key'])]
    if len(imgs) == 1:
        new_key = imgs[0]
    else:
        new_key = "/a/OL%dA" % min(key_int(a) for a in authors)
        # Molière and O. J. O. Ferreira
        if len(imgs) != 0:
            print 'imgs:', imgs
            return # skip
        if not (imgs == [u'/a/OL21848A', u'/a/OL4280680A'] \
                or imgs == [u'/a/OL325189A', u'/a/OL266422A'] \
                or imgs == [u'/a/OL5160945A', u'/a/OL5776228A']):
            print imgs
            assert len(imgs) == 0

    print new_key
    print best_key

    do_normalize(new_key, best_key, authors)
    old_keys = set(k for k in keys if k != new_key) 
    print 'old keys:', old_keys

    for old in old_keys:
        # /b/OL21291659M
        switch_author(ol, old, new_key, old_keys, debug=True)
        if old in not_redirect:
            make_redirect(ol, old, new_key)
        q = { 'authors': old, 'type': '/type/edition', }
        if list(get_things(q)) != []:
            switch_author(ol, old, new_key, old_keys, debug=True)
        #l = list(query_iter(q))
        #print old, l
        #assert l == []
    
if __name__ == '__main__':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

    rc = read_rc()
    ol = OpenLibrary("http://openlibrary.org")
    ol.login('EdwardBot', rc['EdwardBot']) 
    assert len(sys.argv) > 2
    merge_authors(ol, sys.argv[1:])

########NEW FILE########
__FILENAME__ = new
from catalog.olwrite import Infogami
from catalog.read_rc import read_rc
import sys

rc = read_rc()
infogami = Infogami(rc['infogami'])
infogami.login('EdwardBot', rc['EdwardBot'])

name = sys.argv[1]

q = {
    'create': 'unless_exists',
    'name': name,
    'personal_name': name
    'entity_type': 'person',
    'key': infogami.new_key('/type/author'),
    'type': '/type/author',
}

print infogami.write(q, comment='create author')

########NEW FILE########
__FILENAME__ = noble
# coding=utf-8
from catalog.get_ia import read_marc_file
from catalog.read_rc import read_rc
from time import time
from catalog.marc.fast_parse import index_fields, get_tag_lines, get_first_tag, get_all_subfields
import web, os, os.path, re, sys

titles = [ "Accolade", "Adi", "Aetheling", "Aga Khan", "Ajaw", "Ali'i", 
        "Allamah", "Altgrave", "Ammaveedu", "Anji", "Ryūkyū", "Archtreasurer", 
        "Aryamehr", "Atabeg", "Ban", "Baron", "Batonishvili", "Begum", "Bey", 
        "Boier", "Boyar", "Bulou", "Burgmann", "Buring Khan", "Caliph", 
        "Castellan", "Chakravatin", "Comte", "Conde", "Count",
        "Count palatine", "Countess", "Crown prince", "Daula", 
        "Despot", "Doge", "Dowager", "Duchess of Rothesay", "Duke", "Earl", 
        "Edler", "Elector", "Elteber", "Emir", "Emperor", "Emperor-elect", 
        "Erbherr", "Feudal baron", "Fils de France", "Fraujaz", "Fürst",
        "Grand duke", "Grand prince", "Grand Župan", "Grandee", "Haty-a", 
        "Hersir", "Hidalgo", "Highness", "Hold", "Hteik Tin", "Ichirgu-boil", 
        "Infante", "Jang", "Jarl", "Jonkheer", "Junker", "Kavkhan", "Khagan", 
        "Khagan Bek", "Khan", "Khanum", "Khatun", "Knight", "Knyaz",
        "Kodaw-gyi", "Kralj", "Lady", "Lamido", "Landgrave", "Lendmann", 
        "Lord", "Madame Royale", "Magnate", "Maha Uparaja", 
        "Maha Uparaja Anaudrapa Ainshe Min", "Maharaja", "Maharajadhiraja", 
        "Maharana", "Maharao", "Maharaol", "Malik", "Margrave", "Marquess", 
        "Marquis de Bauffremont", "Marquise", "Mepe-Mepeta", "Mesne lord", 
        "Mian", "Min Ye", "Min-nyi Min-tha", "Mir", "Mirza", "Monsieur", "Mormaer", "Morza", "Mwami", "Naib", "Nawab", "Nayak", "Negus", "Nobile", "Obalumo", "Orangun", "Aftab", "Ottoman", "Padishah", "Paigah", "Hyderabad", "Paladin", "Palaiyakkarar", "Palatine", "Panapillai Amma", "Paramount Ruler", "Pasha", "Patricianship", "Pharaoh", "Piast dynasty", "Prescriptive barony", "Prince", "Prince du Sang", "Prince-Bishop", "Princely Highness", "Princeps", "Princess", "Principalía", "Privy chamber", "Rai", "Raja", "Rajah Muda of Sarawak", "Rajus", "Rana", "Rao Raja", "Ratu", "Ridder", "Ro", "Roko", "Sado Min", "Sahib", "Samanta", "Sawai Maharaja", "Shah", "Shahzada", "Shamkhal", "Shanyu", "Shwe Kodaw-gyi", "Shwe Kodaw-gyi Awratha", "Shwe Kodaw-gyi Rajaputra", "Sidi", "Sir", "Sultan", "Sunan", "Susuhunan", "Szlachta", "Tenant-in-chief", "Thakur", "Thampi", "Tsar", "Tsarevitch", "Tu'i", "Ueekata", "Uparaja", "Uparat", "Viceroy", "Victory", "Vidame", "Viscount", "Vizier", "Wazirzada", "Yang di-Pertuan Besar", "Zamindar", "Zeman", "Župa"]

rc = read_rc()
web.config.db_parameters = dict(dbn='postgres', db='ol_merge', user=rc['user'], pw=rc['pw'], host=rc['host'])
web.config.db_printing = False
web.load()

def sources():
    return ((i.id, i.archive_id, i.name) for i in web.select('marc_source'))

def process_record(pos, loc, data):
    for tag in '100', '700':
        line = get_first_tag(data, set([tag]))
        if line:
            fields = list(get_all_subfields(line))
            if any(k == 'c' for k, v in fields):
                print (loc, fields)

def files(ia):
    endings = ['.mrc', '.marc', '.out', '.dat', '.records.utf8']
    def good(filename):
        return any(filename.endswith(e) for e in endings)

    dir = rc['marc_path'] + ia
    dir_len = len(dir) + 1
    files = []
    for dirpath, dirnames, filenames in os.walk(dir):
        files.extend(dirpath + "/" + f for f in sorted(filenames))
    return [(i[dir_len:], os.path.getsize(i)) for i in files if good(i)]

rec_no = 0

for source_id, ia, name in sources():
    for part, size in files(ia):
        full_part = ia + "/" + part
        filename = rc['marc_path'] + full_part
        assert os.path.exists(filename)
        f = open(filename)
        for pos, loc, data in read_marc_file(full_part, f):
            rec_no +=1
            process_record(pos, loc, data)

########NEW FILE########
__FILENAME__ = rename
#!/usr/bin/python

import web, re, sys, codecs
from pprint import pprint

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

web.load()

from infogami.infobase.infobase import Infobase
import infogami.infobase.writequery as writequery
site = Infobase().get_site('openlibrary.org')

re_marc_name = re.compile('^(.*), (.*)$')
re_end_dot = re.compile('[^ ][^ ]\.$', re.UNICODE)
re_odd_dot = re.compile('[^ ][^ ]\. ', re.UNICODE)
re_initial_then_dot = re.compile(r'\b[A-Z]\.')

def find_by_statements(author_key):
    q = {
        'authors': author_key,
        'type': '/type/edition',
    }
    by = []
    for key in site.things(q):
        try:
            by.append(site.withKey(key).by_statement.value)
        except AttributeError:
            pass
    return by

def east_in_by_statement(name, flipped, by_statements):
    assert name.find(', ') != -1
    name = name.replace('.', '')
    name = name.replace(', ', ' ')
    if name == flipped.replace('.', ''):
        return False
    for by in by_statements:
        if by.find(name) != -1:
            return True
    return False

def get_type_id(type):
    w = "key='" + type + "' and site_id=1"
    return web.select('thing', what='id', where=w)[0].id

author_type_id = get_type_id('/type/author')

def get_thing(id):
    sql = "select key, value from datum where thing_id=%d and end_revision=2147483647 and key != 'type'" % id
    iter = web.query(sql)
    thing = {}
    for row in iter:
        thing[row.key] = row.value
    return thing

def get_author_by_name(name):
    sql = "select id from thing, datum where thing.type=$type and thing.id=thing_id and datum.key='name' and datum.value=$name and datum.datatype=2 and datum.end_revision=2147483647"
    iter = web.query(sql, vars={'name': name, 'type': author_type_id})
    return [row.id for row in iter]

def flip_name(name):
    # strip end dots like this: "Smith, John." but not like this: "Smith, J."
    m = re_end_dot.search(name)
    if m:
        name = name[:-1]

    m = re_marc_name.match(name)
    return m.group(2) + ' ' + m.group(1)

def pick_name(a, b, flipped):
    if re_initial_then_dot.search(a):
        return flipped
    else:
        return b

east_list = [line[:-1].lower() for line in open("east")]
east = frozenset(east_list + [flip_name(i) for i in east_list])

def author_dates_match(a, b):
    for k in ['birth_date', 'death_date', 'date']:
        if k in a and k in b and a[k] != b[k]:
            return False
    return True

def get_other_authors(name):
    other = get_author_by_name(name)
    if name.find('.') != -1:
        name = name.replace('.', '')
        other.extend(get_author_by_name(name))
    return other

def key_int(rec):
    return int(web.numify(rec['key']))

def switch_author(old, new):
    q = { 'authors': old['key'], 'type': '/type/edition', }
    for key in site.things(q):
        edition = site.withKey(key)
        authors = []
        for author in edition.authors:
            if author.key == old['key']:
                author_key = new['key']
            else:
                author_key = author.key
            authors.append({ 'key': author_key })

        q = {
            'key': key,
            'authors': { 'connect': 'update_list', 'value': authors }
        }
#        pprint(q)
        site.write(q, comment='fix author name')

def make_redirect(old, new):
    q = {
        'key': old['key'],
        'location': {'connect': 'update', 'value': new['key'] },
        'type': {'connect': 'update', 'value': '/type/redirect' },
    }
    for k in old.iterkeys():
        if k != 'key':
            q[str(k)] = { 'connect': 'update', 'value': None }
#    pprint(q)
    print site.write(q, comment='replace with redirect')

def copy_fields(from_author, to_author, name):
    new_fields = { 'name': name, 'personal_name': name }
    for k, v in from_author.iteritems():
        if k in ('name', 'key'):
            continue
        if k in author:
            assert v == to_author[k]
        else:
            new_fields[k] = v
    return new_fields

def update_author(key, new):
    q = { 'key': key, }
    for k, v in new.iteritems():
        q[k] = { 'connect': 'update', 'value': v }
#    pprint(q)
    print site.write(q, comment='fix author name')

def merge_authors(author, merge_with, name):
    print 'merge author %s:"%s" and %s:"%s"' % (author['key'], author['name'], merge_with['key'], merge_with['name'])
    new_name = pick_name(author['name'], merge_with['name'], name)
    print 'becomes: "%s"' % new_name
    if key_int(author) < key_int(merge_with):
        new_key = author['key']
        print "copy fields from merge_with to", new_key
        new = copy_fields(merge_with, author, new_name)
        update_author(new_key, new)
        switch_author(merge_with, author)
#        print "delete merge_with"
        make_redirect(merge_with, author)
    else:
        new_key = merge_with['key']
        print "copy fields from author to", new_key
        new = copy_fields(merge_with, author, new_name)
        update_author(new_key, new)
        switch_author(author, merge_with)
#        print "delete author"
        make_redirect(author, merge_with)
    print

print 'running query'
# limit for test runs
for thing_row in web.select('thing', what='id, key', where='type='+`author_type_id`, limit=10000):
    id = thing_row.id
    author = get_thing(id)

    if 'personal_name' not in author \
            or author['personal_name'] != author['name']:
        continue
    if author['name'].find(', ') == -1:
        continue
    if author['name'].lower().replace('.', '') in east:
        continue

    key = author['key']
    name = flip_name(author['name'])
    other = get_other_authors(name)
    if len(other) == 0 and not re_odd_dot.search(author['name']):
        by_statements = find_by_statements(author['key'])
        print author['name'], "by:", ', '.join('"%s"' % i for i in by_statements)
        if east_in_by_statement(author['name'], name, by_statements):
            print "east in by statement"
            print
            continue
        print "rename %s to %s" % (`author['name']`, `name`)
        q = {
            'key': key,
            'name': { 'connect': 'update', 'value': name},
            'personal_name': { 'connect': 'update', 'value': name},
        }
        print `q`
        continue

    if len(other) != 1:
#        print "other length:", other
        continue
    # don't merge authors when more than one like "Smith, John"
    if len(get_author_by_name(author['name'])) > 1:
#        print "found more authors with same name"
        continue

    merge_with = get_thing(other[0])
    try:
        if not author_dates_match(author, merge_with):
            print "date mismatch"
            continue
    except KeyError:
        pprint(author)
        pprint(merge_with)
        raise
    by_statements = find_by_statements(author['key'])
    print author['name'], "by:", ', '.join('"%s"' % i for i in by_statements)
    if east_in_by_statement(author['name'], name, by_statements):
        print "east in by statement"
        print
        continue
    merge_authors(author, merge_with, name)

########NEW FILE########
__FILENAME__ = utils
import re

re_marc_name = re.compile('^(.*), (.*)$')
re_initial_then_dot = re.compile(r'\b[A-Z]\.')

def flip_name(name):
    m = re_marc_name.match(name)
    return m.group(2) + ' ' + m.group(1)

def pick_name(a, b):
    if re_initial_then_dot.search(a):
        return flip_name(a)
    else:
        return b

def east_in_by_statement(name, by_statements):
    assert name.find(', ') != -1
    name = name.replace('.', '')
    flipped = flip_name(name)
    name = name.replace(', ', ' ')
    if name == flipped:
        return False
    for by in by_statements:
        if by.find(name) != -1:
            return True
    return False

def test_merge():
    data = [
        (u'Hood, Christopher', u'Christopher Hood', u'Christopher Hood'),
        (u'Pawsey, Margaret M.', u'Margaret M Pawsey', u'Margaret M. Pawsey'),
        (u'Elchardus, M.', u'M Elchardus', u'M. Elchardus'),
        (u'Hayes, Mike.', u'Mike Hayes', u'Mike Hayes'),
        (u'Krause, Rainer.', u'Rainer Krause', u'Rainer Krause'),
        (u'Hoffmann, Manfred.', u'Manfred Hoffmann', u'Manfred Hoffmann'),
        (u'Masson, Veneta.', u'Veneta Masson', u'Veneta Masson'),
        (u'Baker, Ernest.', u'Ernest Baker', u'Ernest Baker'),
        (u'Hooper, James.', u'James Hooper', u'James Hooper'),
        (u'Bront\xeb, Charlotte', u'Charlotte Bront\xeb', u'Charlotte Bront\xeb'),
        (u'Nichols, Francis Henry', u'Francis Henry Nichols', u'Francis Henry Nichols'),
        (u'Becker, Bernd', u'Bernd Becker', u'Bernd Becker'),
        (u'Sadleir, Richard.', u'Richard Sadleir', u'Richard Sadleir'),
    ]
    for a, b, want in data:
        assert pick_name(a, b) == want

    assert east_in_by_statement("Wang, Qi", ["Wang Qi."])
    assert not east_in_by_statement("Walker, Charles L.",\
            ["edited by A. Karl Larson and Katharine Miles Larson."])
    assert not east_in_by_statement("Luoma, Gary A.", ["Gary A. Luoma"])
    assert not east_in_by_statement("Tan, Tan", ["Tan Tan zhu.", "Tan Tan zhu.", "Tan Tan ; [cha tu Li Ruguang ; ze ren bian ji Wang Zhengxiang]."])

########NEW FILE########
__FILENAME__ = web_merge
import web
from catalog.db_read import withKey
from pprint import pformat

urls = (
    '/', 'index'
)

base = 'http://openlibrary.org'

class index:
    def GET(self):
        web.header('Content-Type','text/html; charset=utf-8', unique=True)
        input = web.input()
        print "<html>\n<head><title>Author merge</title></head><body>"
        print "<h1>Author merge</h1>"
        print '<form name="main" method="get">'
        print '<table>'
        print '<tr><td>Authors</td>'
        author = {}
        for field in ('a', 'b'):
            print '<td>'
            if field in input:
                key = input[field]
                if key.startswith(base):
                    key = key[len(base):]
                author[field] = withKey(key)
                print '<input type="text" name="%s" value="%s">' % (field, key)
            else:
                print '<input type="text" name="%s">' % field
            print '</td>'
        print '<td><input type="submit" value="Load"></td>'
        print '</tr>'
        if 'a' in author and 'b' in author:
            a = author['a']
            b = author['b']
            keys = [withKey(prop['key'])['name'] for prop in withKey('/type/author')['properties']]
            for k in keys:
                if k in a or k in b:
                    print '<tr><td>%s</td><td>%s</td><td>%s</td></tr>' % \
                            (k, a.get(k, ''), b.get(k, ''))
        print '</table>'
        print "</body></html>"

web.webapi.internalerror = web.debugerror

if __name__ == "__main__": web.run(urls, globals(), web.reloader)

########NEW FILE########
__FILENAME__ = web_merge2
import web, re
from urllib2 import urlopen
import simplejson as json
from pprint import pformat

from catalog.utils.query import query_iter

urls = (
    '/', 'index'
)
app = web.application(urls, globals())

re_year = re.compile('^(\d+)[,.*]+$')

def result_table(data, birth, death, order):
    html = ' %d results' % len(data)
    l = []
    def clean(i, default, field):
        if field not in i:
            return default
        if i[field] is None:
            return ''
        m = re_year.match(i[field])
        return m.group(1) if m else i[field]
 
    data = [
        {
            'key': i['key'],
            'name': i['name'],
            'birth': clean(i, birth, 'birth_date'),
            'death': clean(i, death, 'death_date'),
        } for i in data]
               
    base_url = web.htmlquote("?birth=%s&death=%s&order=" % (web.urlquote(birth), web.urlquote(death)))
    html += '<tr>'
    html += '<th><a href="' + base_url + 'name">Name</a></th>'
    if birth:
        html += '<th>birth</th>'
    else:
        html += '<th><a href="' + base_url + 'birth">birth</a></th>'
    if death:
        html += '<th>death</th>'
    else:
        html += '<th><a href="' + base_url + 'death">death</a></th>'
    html += '</tr>'
    if order:
        data = sorted(data, key=lambda i:i[order])
    for i in data:
        html += '<tr><td><a href="http://openlibrary.org%s">%s</td><td>%s</td><td>%s</td><tr>' % (i['key'], web.htmlquote(i['name']), i['birth'], i['death'])
    return '<table>' + html + '</table>'

def get_all(url):
    all = []
    offset = 0
    limit = 500
    while True:
        ret = json.load(urlopen(url + "&limit=%d&offset=%d" % (limit, offset)))
        if not ret:
            return all
        all += ret
        if len(all) >= 1000:
            return all
        offset += limit

class index:        
    def GET(self):
        input = web.input()
        birth = input.get('birth', '').strip()
        death = input.get('death', '').strip()
        order = input.get('order', '').strip()
        if order not in ('', 'name', 'birth', 'death'):
            order = ''
        html = '''
<html>
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Merge author</title>
<style>
body { font-family: arial,helvetica,san-serif; }
th { text-align: left; }
</style>
</head>
<body>
'''
        html += '<form method="get">\n'
        html += 'Birth: <input type="text" size="7" name="birth" value="%s">\n' % web.htmlquote(birth)
        html += 'Death: <input type="text" size="7" name="death" value="%s">\n' % web.htmlquote(death)
        html += '<input type="submit" value="Search">\n</form>'

        if birth or death:
            url = 'http://openlibrary.org/query.json?type=/type/author&birth_date=%s&death_date=%s&name=' % (web.urlquote(birth), web.urlquote(death))
            data = get_all(url)
            html += result_table(data, birth, death, order)
        return html + '</body>\n</html>'

if __name__ == "__main__":
    app.run()

########NEW FILE########
__FILENAME__ = crawl
import re
from urllib2 import urlopen
from os.path import exists

# crawl catalogue.nla.gov.au

re_th = re.compile('^<th nowrap align="RIGHT" valign="TOP">(\d{3})</th>$', re.I)
re_td = re.compile('^<td(?: VALIGN="TOP")?>(.*)</td>$')
re_span = re.compile('<span class="subfield"><strong>\|(.|&(?:gt|lt|amp);)</strong>(.*?)</span>')

trans = dict(lt='<', gt='>', amp='&')

def read_row(tag, row):
    assert len(row) == 3
    if tag[0:2] == '00':
        assert all(i == '' for i in row[0:1])
        return (tag, row[2])
    else:
        end = 0
        subfields = []
        while end != len(row[2]):
            m = re_span.match(row[2], end)
            end = m.end()
            (k, v) = m.groups()
            if len(k) != 1:
                k = trans[k[1:-1]]
            subfields.append((k, v))
        assert all(len(i) == 1 for i in row[0:1])
        return (tag, row[0], row[1], subfields)

def extract_marc(f):
    expect = 'table'
    col = 0
    row = []
    lines = []
    for line in f: # state machine
        if expect == 'table':
            if '<table border="0" class="librarianview">' in line:
                expect = 'tr'
            continue
        if expect == 'tr':
            if line.startswith('</table>'):
                break
            assert line.startswith('<tr>')
            expect = 'th'
            continue
        if expect == 'th':
            m = re_th.match(line)
            assert m
            tag = m.group(1)
            expect = 'td'
            continue
        if expect == 'td':
            if line.startswith('</tr>'):
                lines.append(read_row(tag, row))
                tag = None
                row = []
                expect = 'tr'
                continue
            if line == '<td>\n':
                expect = 'span'
                continue
            m = re_td.match(line)
            row.append(m.group(1))
            continue
        if expect == 'span':
            row.append(line[:-1])
            expect = '/td'
            continue
        if expect == '/td':
            assert line == '</td>\n'
            expect = 'td'
            continue
    return lines

i = 1
while 1:
    i+=1
    filename = 'marc/%d' % i
    if exists(filename):
        continue
    print i, 
    url = 'http://catalogue.nla.gov.au/Record/%d/Details' % i
    web_input = None
    for attempt in range(5):
        try:
            web_input = urlopen(url)
            break
        except:
            pass
    if not web_input:
        break

    out = open('marc/%d' % i, 'w')
    try:
        marc = extract_marc(web_input)
    except:
        print url
        raise
    print len(marc)
    for line in marc:
        print >> out, line
    out.close()
    #sleep(0.5)

########NEW FILE########
__FILENAME__ = authors
from catalog.infostore import get_site
from catalog.read_rc import read_rc
import web, sys, codecs, os.path, re
from pprint import pprint
from catalog.olwrite import Infogami
site = get_site()

import psycopg2
rc = read_rc()
infogami = Infogami(rc['infogami'])
infogami.login('EdwardBot', rc['EdwardBot'])

re_marc_name = re.compile('^(.*), (.*)$')
re_end_dot = re.compile('[^ ][^ ]\.$', re.UNICODE)

out = open('author_replace3', 'w')

# find books with matching ISBN and fix them to use better author record

def flip_name(name):
    # strip end dots like this: "Smith, John." but not like this: "Smith, J."
    m = re_end_dot.search(name)
    if m:
        name = name[:-1]

    m = re_marc_name.match(name)
    return m.group(2) + ' ' + m.group(1)

conn = psycopg2.connect("dbname='%s' user='%s' host='%s' password='%s'" \
        % ('ol_merge', rc['user'], rc['host'], rc['pw']));
cur = conn.cursor()

author_fields = ('key', 'name', 'title', 'birth_date', 'death_date', 'personal_name')

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
for line in open('dups'):
    isbn, num = eval(line)
    if isbn < '0273314165':
        continue
    cur.execute('select key from isbn where value=%(v)s', {'v':isbn})
    found = []
    names = {}
    for i in cur.fetchall():
        key = i[0]
        e = site.withKey(key)
        author_list = e.authors or []
        authors = [dict(((k, v) for k, v in a._get_data().items() if k in author_fields)) for a in author_list if a]
        for a in authors:
            if 'name' not in a:
                continue
            name = a['name']
            if name.find(', ') != -1:
                name = flip_name(name)
            a2 = a.copy()
            a2['edition'] = key
            names.setdefault(name, []).append(a2)
        found.append((key, authors))
    if len([1 for k, a in found if a]) < 2:
        continue
    if not any(any('birth_date' in j or 'death_date' in j for j in i[1]) for i in found):
        continue
    names = dict((k, v) for k, v in names.iteritems() if len(set(i['key'] for i in v)) > 1)
    if not names:
        continue
    author_replace = {}
    for name, authors in names.items():
        seen = set()
#        print 'birth:', [a['birth_date'].strip('.') for a in authors if 'birth_date' in a]
#        print 'death:', [a['death_date'].strip('.') for a in authors if 'death_date' in a]
        with_dates = None
        no_dates = []
        for a in authors:
            if a['key'] in seen:
                continue
            seen.add(a['key'])
            if 'birth_date' in a or 'death_date' in a:
                if with_dates:
                    with_dates = None
                    break
                with_dates = a['key']
                continue
            no_dates.append(a['key'])
        if with_dates and no_dates:
            for i in no_dates:
                assert i not in author_replace
                author_replace[i] = with_dates
    if not author_replace:
        continue
    print isbn, author_replace
#    pprint(names)
    for key, authors in found:
        replace = [a['key'] for a in authors if a['key'] in author_replace]
        if len(replace) == 0:
            continue
#        print len(replace), key, [a['key'] for a in authors]
        new_authors = []
        this = {}
        for a in authors:
            akey = a['key']
            if akey in author_replace:
                this[akey] = author_replace[akey]
                akey = author_replace[akey]
            if akey not in new_authors:
                new_authors.append(akey)
        q = {
            'key': key,
            'authors': { 'connect': 'update_list', 'value': new_authors }
        }
        print >> out, (key, this)
#    for k in author_replace.keys():
#        print k, len(site.things({'type': '/type/edition', 'authors': k}))
            
#    pprint(found)
#    for name, v in names.items():
#        print name
#        for edition, author in v:
#            print author, site.things({'type': '/type/edition', 'authors': author})
#    print
out.close()

########NEW FILE########
__FILENAME__ = find
import web, sys, codecs, os.path
from catalog.read_rc import read_rc
import psycopg2
from catalog.infostore import get_site
from catalog.merge.merge_marc import attempt_merge, build_marc
import catalog.marc.fast_parse as fast_parse

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

# need to use multiple databases
# use psycopg2 to until open library is upgraded to web 3.0

rc = read_rc()
threshold = 875

conn = psycopg2.connect("dbname='%s' user='%s' host='%s' password='%s'" \
        % ('ol_merge', rc['user'], rc['host'], rc['pw']));
cur1 = conn.cursor()
cur2 = conn.cursor()

site = get_site()

marc_path = '/2/pharos/marc/'

def get_marc(loc):
    try:
        filename, p, l = loc.split(':')
    except ValueError:
        return None
    if not os.path.exists(marc_path + filename):
        return None
    f = open(marc_path + filename)
    f.seek(int(p))
    buf = f.read(int(l))
    f.close()
    rec = fast_parse.read_edition(buf)
    if rec:
        return build_marc(rec)

for line in open('dups'):
    v, num = eval(line)
    cur2.execute('select key from isbn where value=%(v)s', {'v':v})
    editions = []
    for i in cur2.fetchall():
        key = i[0]
        t = site.withKey(key)
        mc =  site.versions({'key': key})[0].machine_comment
        editions.append({'key': key, 'title': t.title, 'loc': mc})
    if len(editions) != 2:
        continue
    if any(not i['loc'] or i['loc'].startswith('amazon:') for i in editions):
        continue
    e1 = get_marc(editions[0]['loc'])
    if not e1:
        continue
    e2 = get_marc(editions[1]['loc'])
    if not e2:
        continue

#    print v, [i['title'] for i in editions]
#    print e1
#    print e2
    match = attempt_merge(e1, e2, threshold, debug=False)
    if match:
        print tuple([v] + [i['key'] for i in editions])

sys.exit(0)
cur1.execute('select value, count(*) as num from isbn group by value having count(*) > 1')
for i in cur1.fetchall():
    print i
    cur2.execute('select key from isbn where value=%(v)s', {'v':i[0]})
    print cur2.fetchall()

########NEW FILE########
__FILENAME__ = find_dups
#!/usr/bin/python

from openlibrary.api import OpenLibrary
from subprocess import Popen, PIPE
import MySQLdb

ia_db_host = 'dbmeta.us.archive.org'
ia_db_user = 'archive'
ia_db_pass = Popen(["/opt/.petabox/dbserver"], stdout=PIPE).communicate()[0]

ol = OpenLibrary('http://openlibrary.org/')

local_db = MySQLdb.connect(db='merge_editions')
local_cur = conn.cursor()

archive_db = MySQLdb.connect(host=ia_db_host, user=ia_db_user, \
        passwd=ia_db_pass, db='archive')
archive_cur = conn.cursor()

fields = ['identifier', 'updated', 'collection']
sql_fields = ', '.join(fields)

archive_cur.execute("select " + sql_fields + \
    " from metadata" + \
    " where scanner is not null and mediatype='texts'" + \
        " and (not curatestate='dark' or curatestate is null)" + \
        " and collection is not null and boxid is not null and identifier not like 'zdanh_test%' and scandate is not null " + \
        " order by updated")

for num, (ia, updated, collection) in enumerate(cur.fetchall()):
    if 'lending' not in collection and 'inlibrary' not in collection:
        continue
    q = {'type': '/type/edition', 'ocaid': ia}
    editions = set(str(i) for i in ol.query(q))
    q = {'type': '/type/edition', 'source_records': 'ia:' + ia}
    editions.update(str(i) for i in ol.query(q))
    if len(editions) > 1:
        print (ia, list(editions))
        local_cur.execute('replace into merge (ia, editions) values (%s, %s)', [ia, ' '.join(editions)])

########NEW FILE########
__FILENAME__ = find_easy
import MySQLdb, datetime, re, sys
sys.path.append('/1/src/openlibrary')
from openlibrary.api import OpenLibrary, Reference
from collections import defaultdict
from pprint import pprint

re_edition_key = re.compile('^/books/OL(\d+)M$')
re_nonword = re.compile(r'\W', re.U)
re_edition = re.compile(' ed edition$')

ol = OpenLibrary('http://openlibrary.org/')

conn = MySQLdb.connect(db='merge_editions')
cur = conn.cursor()

skip = 'guineapigscomple00elwa'
skip = None
total = 5601
cur.execute("select ia, editions, done, unmerge_count from merge where unmerge_count != 0") # and ia='hantayo00hillrich'")
unmerge_field_counts = defaultdict(int)
num = 0
for ia, ekeys, done, unmerge_count in cur.fetchall():
#    if unmerge_count == 0:
#        continue
    num += 1
    if num % 100 == 0:
        print '%d/%d %.2f%%' % (num, total, ((float(num) * 100) / total)), ia
    if skip:
        if skip == ia:
            skip = None
        continue
    ekeys = ['/books/OL%dM' % x for x in sorted(int(re_edition_key.match(ekey).group(1)) for ekey in ekeys.split(' '))]
    min_ekey = ekeys[0]
    
    if len(ekeys) > 3:
        print ia, ekeys
    editions = [ol.get(ekey) for ekey in ekeys]
    all_keys = set()
    for e in editions:
        for k in 'classifications', 'identifiers', 'table_of_contents':
            if k in e and not e[k]:
                del e[k]
    for e in editions:
        all_keys.update(e.keys())
    for k in 'latest_revision', 'revision', 'created', 'last_modified', 'key', 'type', 'genres':
        if k in all_keys:
            all_keys.remove(k)

    for k in all_keys.copy():
        if k.startswith('subject'):
            all_keys.remove(k)

    for e in editions: # resolve redirects
        if 'authors' not in e:
            continue
        new_authors = []
        for akey in e['authors']:
            a = ol.get(akey)
            if a['type'] == Reference('/type/redirect'):
                akey = Reference(a['location'])
            else:
                assert a['type'] == Reference('/type/author')
            new_authors.append(akey)
        e['authors'] = new_authors

    merged = {}
    k = 'publish_date'
    publish_dates = set(e[k] for e in editions if k in e and len(e[k]) != 4)

    k = 'pagination'
    all_pagination = set(e[k].strip(':.') for e in editions if e.get(k))

    one_item_lists = {}
    for k in 'lc_classifications', 'publishers', 'contributions', 'series', 'authors':
        one_item_lists[k] = set(e[k][0].strip('.') for e in editions if e.get(k) and len(set(e[k])) == 1)

    for k in 'source_records', 'ia_box_id':
        merged[k] = []
        for e in editions:
            for sr in e.get(k, []):
                if sr not in merged[k]:
                    merged[k].append(sr)

    for k in ['other_titles', 'isbn_10', 'series', 'oclc_numbers', 'publishers']:
        if k not in all_keys:
            continue
        merged[k] = []
        for e in editions:
            for sr in e.get(k, []):
                if sr not in merged[k]:
                    merged[k].append(sr)

    k = 'ocaid'
    for e in editions:
        if e.get(k) and 'ia:' + e[k] not in merged['source_records']:
            merged['source_records'].append(e[k])

    k = 'identifiers'
    if k in all_keys:
        merged[k] = {}
        for e in editions:
            if k not in e:
                continue
            for a, b in e[k].items():
                for c in b:
                    if c in merged[k].setdefault(a, []):
                        continue
                    merged[k][a].append(c)

    any_publish_country = False
    k = 'publish_country'
    if k in all_keys:
        for e in editions:
            if e.get(k) and not e[k].strip().startswith('xx'):
                any_publish_country = True

    skip_fields = set(['source_records', 'ia_box_id', 'identifiers', 'ocaid', 'other_titles', 'series', 'isbn_10'])
    for k in all_keys:
        if k in skip_fields:
            continue

        uniq = defaultdict(list)
        for num, e in enumerate(editions):
            if e.get(k):
                if k == 'publish_date' and len(e[k]) == 4 and e[k].isdigit and any(e[k] in pd for pd in publish_dates):
                    continue
                if k == 'pagination' and any(len(i) > len(e[k].strip('.:')) and e[k].strip('.:') in i for i in all_pagination):
                    continue
                if k in one_item_lists and len(set(e.get(k, []))) == 1 and any(len(i) > len(e[k][0].strip('.')) and e[k][0].strip('.') in i for i in one_item_lists[k]):
                    continue
                if k == 'publish_country' and any_publish_country and e.get(k, '').strip().startswith('xx'):
                    continue
                if k == 'edition_name' and e[k].endswith(' ed edition'):
                    e[k] = e[k][:-len(' edition')]
                uniq[re_nonword.sub('', `e[k]`.lower())].append(num)

        if len(uniq) == 1:
            merged[k] = uniq.keys()[0]
            merged[k] = editions[uniq.values()[0][0]][k]
            continue

        if k == 'covers':
            assert all(isinstance(e[k], list) for e in editions if k in e)
            covers = set()
            for e in editions:
                if k in e:
                    covers.update(c for c in e[k] if c != -1)
            merged['covers'] = sorted(covers)
            continue

        if k == 'notes':
            merged['notes'] = ''
            for e in editions:
                if e.get('notes'):
                    merged['notes'] += e['notes'] + '\n'
            continue

        if k == 'ocaid':
            for e in editions:
                if e.get('ocaid'):
                    if e['ocaid'].endswith('goog'):
                        print e['key'], e['ocaid'], ia
                    merged['ocaid'] = e['ocaid']
                    break
            assert merged['ocaid']
            continue

        if k =='authors':
            min_author = set(min((e.get('authors', []) for e in editions), key=len))
            if all(min_author <= set(e.get('authors', [])) for e in editions):
                merged[k] = max((e.get('authors', []) for e in editions), key=len)
                continue
        merged[k] = None
    unmerged = len([1 for v in merged.values() if v is None])
    if unmerged == 1:
        assert len([k for k, v in merged.items() if v is None]) == 1
        for k, v in merged.items():
            if v is None:
                if k == 'series':
                    print ia, [e[k] for e in editions if e.get(k)]
                unmerge_field_counts[k] += 1
    #print 'unmerged count', unmerged, ia, ekeys
    cur.execute('update merge set unmerge_count=%s where ia=%s', [unmerged, ia])

print dict(unmerge_field_counts)
print unmerge_field_counts.items()
for k,v in sorted(unmerge_field_counts.items(), key=lambda i:i[1]):
    print '%30s: %d' % (k, v)

########NEW FILE########
__FILENAME__ = merge
#!/usr/bin/python

import MySQLdb, datetime, re, sys
sys.path.append('/1/src/openlibrary')
from openlibrary.api import OpenLibrary, Reference
from flask import Flask, render_template, request, flash, redirect, url_for, g
from collections import defaultdict
app = Flask(__name__)

re_edition_key = re.compile('^/books/OL(\d+)M$')

ol = OpenLibrary('http://openlibrary.org/')
ol.login('EdwardBot', 'As1Wae9b')

@app.before_request
def before_request():
    g.db = MySQLdb.connect(db='merge_editions')

@app.after_request
def after_request(r):
    g.db.close()
    return r

re_nonword = re.compile(r'\W', re.U)

rows = 200

app.secret_key = 'rt9%s#)5kid$!u*5_@*$f2f_%jq++nl3@d%=7f%v4&78^m4p7c'

@app.route("/")
def index():
    page = int(request.args.get('page', 1))
    cur = g.db.cursor()
    cur.execute('select count(*) from merge where done is null')
    total = cur.fetchone()[0]
    cur.execute('select count(*) from merge where done is null and unmerge_count = 0')
    easy = cur.fetchone()[0]
    cur.execute('select ia, editions, unmerge_count from merge where done is null limit %s offset %s', [rows, (page-1) * rows])
    reply = cur.fetchall()
    return render_template('index.html', merge_list=reply, total=total, rows=rows, page=page, easy=easy )

def run_merge(ia):
    cur = g.db.cursor()
    cur.execute('select editions from merge where ia=%s', ia)
    [ekeys] = cur.fetchone()
    ekeys = ['/books/OL%dM' % x for x in sorted(int(re_edition_key.match(ekey).group(1)) for ekey in ekeys.split(' '))]
    min_ekey = ekeys[0]

    editions = [ol.get(ekey) for ekey in ekeys]
    editions_by_key = dict((e['key'][7:], e) for e in editions)
    merged = build_merged(editions)

    missing = []
    for k, v in merged.items():
        if v is not None:
            continue
        use_ekey = request.form.get(k)
        if use_ekey is None:
            missing.append(k)
            continue
        merged[k] = editions_by_key[use_ekey][k]
    if missing:
        flash('please select: ' + ', '.join(missing))
        return redirect(url_for('merge', ia=ia))

    master = ol.get(min_ekey)
    for k, v in merged.items():
        master[k] = v

    updates = []
    updates.append(master)
    for ekey in ekeys:
        if ekey == min_ekey:
            continue
        ol_redirect = {
            'type': Reference('/type/redirect'),
            'location': min_ekey,
            'key': ekey,
        }
        updates.append(ol_redirect)
    #print len(updates), min_ekey
    try:
        ol.save_many(updates, 'merge lending editions')
    except:
        #for i in updates:
        #    print i
        raise
    cur.execute('update merge set done=now() where ia=%s', [ia])

    flash(ia + ' merged')
    return redirect(url_for('index'))

def build_merged(editions):
    all_keys = set()

    for e in editions:
        for k in 'classifications', 'identifiers':
            if k in e and not e[k]:
                del e[k]

    for e in editions:
        all_keys.update(e.keys())

    for k in 'latest_revision', 'revision', 'created', 'last_modified', 'key', 'type', 'genres':
        if k in all_keys:
            all_keys.remove(k)

    for k in all_keys.copy():
        if k.startswith('subject'):
            all_keys.remove(k)

    merged = {}
    k = 'publish_date'
    publish_dates = set(e[k] for e in editions if k in e and len(e[k]) != 4)

    k = 'pagination'
    all_pagination = set(e[k] for e in editions if e.get(k))

    one_item_lists = {}
    for k in 'lc_classifications', 'publishers', 'contributions', 'series':
        one_item_lists[k] = set(e[k][0].strip('.') for e in editions if e.get(k) and len(set(e[k])) == 1)

    for k in 'source_records', 'ia_box_id':
        merged[k] = []
        for e in editions:
            for sr in e.get(k, []):
                if sr not in merged[k]:
                    merged[k].append(sr)

    for k in ['other_titles', 'isbn_10', 'series']:
        if k not in all_keys:
            continue
        merged[k] = []
        for e in editions:
            for sr in e.get(k, []):
                if sr not in merged[k]:
                    merged[k].append(sr)


    k = 'ocaid'
    for e in editions:
        if e.get(k) and 'ia:' + e[k] not in merged['source_records']:
            merged['source_records'].append(e[k])

    k = 'identifiers'
    if k in all_keys:
        merged[k] = {}
        for e in editions:
            if k not in e:
                continue
            for a, b in e[k].items():
                for c in b:
                    if c in merged[k].setdefault(a, []):
                        continue
                    merged[k][a].append(c)

    any_publish_country = False
    k = 'publish_country'
    if k in all_keys:
        for e in editions:
            if e.get(k) and not e[k].strip().startswith('xx'):
                any_publish_country = True

    for k in all_keys:
        if k in ('source_records', 'ia_box_id', 'identifiers'):
            continue

        uniq = defaultdict(list)
        for num, e in enumerate(editions):
            if e.get(k):
                if k == 'publish_date' and len(e[k]) == 4 and e[k].isdigit and any(e[k] in pd for pd in publish_dates):
                    continue
                if k == 'pagination' and any(len(i) > len(e[k]) and e[k] in i for i in all_pagination):
                    continue
                if k in one_item_lists and len(set(e.get(k, []))) == 1 and any(len(i) > len(e[k][0].strip('.')) and e[k][0].strip('.') in i for i in one_item_lists[k]):
                    continue
                if k == 'publish_country' and any_publish_country and e.get(k, '').strip().startswith('xx'):
                    continue
                if k == 'edition_name' and e[k].endswith(' ed edition'):
                    e[k] = e[k][:-len(' edition')]
                uniq[re_nonword.sub('', `e[k]`.lower())].append(num)

        if len(uniq) == 1:
            #merged[k] = uniq.keys()[0]
            merged[k] = editions[uniq.values()[0][0]][k]
            continue

        if k == 'covers':
            assert all(isinstance(e[k], list) for e in editions if k in e)
            covers = set()
            for e in editions:
                if k in e:
                    covers.update(c for c in e[k] if c != -1)
            merged['covers'] = sorted(covers)
            continue

        if k == 'notes':
            merged['notes'] = ''
            for e in editions:
                if e.get('notes'):
                    merged['notes'] += e['notes'] + '\n'
            continue

        if k == 'ocaid':
            for e in editions:
                if e.get('ocaid'):
                    #assert not e['ocaid'].endswith('goog')
                    merged['ocaid'] = e['ocaid']
                    break
            assert merged['ocaid']
            continue
        merged[k] = None

    return merged

@app.route("/merge/<ia>", methods=['GET', 'POST'])
def merge(ia):
    if request.method == 'POST':
        return run_merge(ia)

    cur = g.db.cursor()
    cur.execute('select ia, editions, done from merge where ia = %s', [ia])
    ia, ekeys, done = cur.fetchone()
    ekeys = ['/books/OL%dM' % x for x in sorted(int(re_edition_key.match(ekey).group(1)) for ekey in ekeys.split(' '))]
    min_ekey = ekeys[0]

    editions = [ol.get(ekey) for ekey in ekeys]

    merged = build_merged(editions)
    all_keys = merged.keys()

    wkeys = set()
    works = []
    if False: 
        for e in editions:
            for wkey in e.get('works', []):
                if wkey not in wkeys:
                    w = ol.get(wkey)
                    works.append(w)
                    q = {'type':'/type/edition', 'works':wkey, 'limit': 1000}
                    work_editions = ol.query(q)
                    w['number_of_editions'] = len(work_editions)
                    wkeys.add(wkey)

    return render_template('merge.html',
            ia=ia,
            editions=editions,
            keys=sorted(all_keys),
            merged = merged,
            ekeys=ekeys,
            works=works,
            master=min_ekey)

if __name__ == "__main__":
    app.run(host='0.0.0.0', debug=True)

########NEW FILE########
__FILENAME__ = merge_works
import MySQLdb, datetime, re, sys
sys.path.append('/1/src/openlibrary')
from openlibrary.api import OpenLibrary, Reference
from pprint import pprint

conn = MySQLdb.connect(db='merge_editions')
cur = conn.cursor()

re_edition_key = re.compile('^/books/OL(\d+)M$')
re_work_key = re.compile('^/works/OL(\d+)W$')
ol = OpenLibrary('http://openlibrary.org/')
ol.login('EdwardBot', 'As1Wae9b')

re_iso_date = re.compile('^(\d{4})-\d\d-\d\d$')
re_end_year = re.compile('(\d{4})$')

def get_publish_year(d):
    if not d:
        return
    m = re_iso_date.match(d)
    if m:
        return int(m.group(1))
    m = re_end_year.match(d)
    if m:
        return int(m.group(1))

{'lc_classifications': ['PZ7.H558 Ru'], 'dewey_number': ['[E]']}
def merge_works(works):
    master = works.pop(0)
    master_first_publish_year = get_publish_year(master.get('first_publish_date'))
    subtitles = sorted((w['subtitle'] for w in works if w.get('subtitle')), key=lambda s: len(s))
    if subtitles and len(subtitles[-1]) > len(master.get('subtitle', '')):
        master['subtitle'] = subtitles[-1]
    updates = []
    for w in works:
        wkey = w.pop('key')
        q = {'type': '/type/edition', 'works': wkey}
        for ekey in ol.query(q):
            e = ol.get(ekey)
            assert len(e['works']) == 1 and e['works'][0] == wkey
            e['works'] = [Reference(master['key'])]
            updates.append(e)
        assert w['type'] != Reference('/type/redirect')
        updates.append({
            'key': wkey,
            'type': Reference('/type/redirect'),
            'location': master['key'],
        })
        for f in 'covers', 'subjects', 'subject_places', 'subject_people', 'subject_times', 'lc_classifications', 'dewey_number':
            if not w.get(f):
                continue
            assert not isinstance(w[f], basestring)
            for i in w[f]:
                if i not in master.setdefault(f, []):
                    master[f].append(i)

        if w.get('first_sentence') and not master.get('first_sentence'):
            master['first_sentence'] = w['first_sentence']
        if w.get('first_publish_date'):
            if not master.get('first_publish_date'):
                master['first_publish_date'] = w['first_publish_date']
            else:
                publish_year = get_publish_year(w['first_publish_date'])
                if publish_year < master_first_publish_year:
                    master['first_publish_date'] = w['first_publish_date']
                    master_first_publish_year = publish_year

        for excerpt in w.get('exceprts', []):
            master.setdefault('exceprts', []).append(excerpt)

        for f in 'title', 'subtitle', 'created', 'last_modified', 'latest_revision', 'revision', 'number_of_editions', 'type', 'first_sentence', 'authors', 'first_publish_date', 'excerpts', 'covers', 'subjects', 'subject_places', 'subject_people', 'subject_times', 'lc_classifications', 'dewey_number':
            try:
                del w[f]
            except KeyError:
                pass

        print w
        assert not w
    updates.append(master)
    print len(updates), [(doc['key'], doc['type']) for doc in updates]
    # update master
    # update editions to point at master
    # replace works with redirects
    print ol.save_many(updates, 'merge works')

skip = 'seventeenagainst00voig'
skip = 'inlineskatingbas00sava'
skip = 'elephantatwaldor00mira'
skip = 'sybasesqlserverp00paul'
skip = 'karmadunl00dunl'
skip = 'norbychronicles00asim'
skip = 'elizabethbarrett00fors'
skip = None
updates = []
cur.execute('select ia, editions, done, unmerge_count from merge')
for ia, ekeys, done, unmerge_count in cur.fetchall():
    if skip:
        if ia == skip:
            skip = None
        else:
            continue
    ekeys = ['/books/OL%dM' % x for x in sorted(int(re_edition_key.match(ekey).group(1)) for ekey in ekeys.split(' '))]
    editions = [ol.get(ekey) for ekey in ekeys]

    if any('authors' not in e or 'works' not in e for e in editions):
        continue
    author0 = editions[0]['authors'][0]
    work0 = editions[0]['works'][0]
    try:
        if not all(author0 == e['authors'][0] for e in editions[1:]):
            continue
    except:
        print 'editions:', [e['key'] for e in editions]
        raise
    if all(work0 == e['works'][0] for e in editions[1:]):
        continue
    wkeys = []
    for e in editions:
        for wkey in e['works']:
            if wkey not in wkeys:
                wkeys.append(wkey)

    works = []
    for wkey in wkeys:
        w = ol.get(wkey)
        q = {'type': '/type/edition', 'works': wkey, 'limit': 1000}
        w['number_of_editions'] = len(ol.query(q))
        works.append(w)
    title0 = works[0]['title'].lower()
    if not all(w['title'].lower() == title0 for w in works[1:]):
        continue
    print ia, ekeys
    print '  works:', wkeys
    def work_key_int(wkey):
        return int(re_work_key.match(wkey).group(1))
    works = sorted(works, cmp=lambda a,b:-cmp(a['number_of_editions'],b['number_of_editions']) or cmp(work_key_int(a['key']), work_key_int(b['key'])))
    print '  titles:', [(w['title'], w['number_of_editions']) for w in works]
    print author0
    #print [w['authors'][0]['author'] for w in works]
    assert all(author0 == w['authors'][0]['author'] for w in works)
    merge_works(works)
    print


########NEW FILE########
__FILENAME__ = run_merge
import MySQLdb, datetime, re, sys
from openlibrary.api import OpenLibrary, Reference
from collections import defaultdict

re_edition_key = re.compile('^/books/OL(\d+)M$')
re_nonword = re.compile(r'\W', re.U)

conn = MySQLdb.connect(db='merge_editions')
cur = conn.cursor()
cur2 = conn.cursor()

ol = OpenLibrary('http://openlibrary.org/')
ol.login('EdwardBot', 'As1Wae9b')

cur.execute('select ia, editions, done from merge where done is null and unmerge_count=0')
for ia, ekeys, done in cur.fetchall():
    updates = []
    ekeys = ['/books/OL%dM' % x for x in sorted(int(re_edition_key.match(ekey).group(1)) for ekey in ekeys.split(' '))]
    print (ia, ekeys)
    min_ekey = ekeys[0]
    editions = [ol.get(ekey) for ekey in ekeys]
    master = editions[0]

    for e in editions:
        for k in 'classifications', 'identifiers', 'table_of_contents':
            if k in e and not e[k]:
                del e[k]

    all_keys = set()
    for e in editions:
        all_keys.update(k for k, v in e.items() if v)
    for k in 'latest_revision', 'revision', 'created', 'last_modified', 'key', 'type', 'genres':
        if k in all_keys:
            all_keys.remove(k)

    for k in all_keys.copy():
        if k.startswith('subject'):
            all_keys.remove(k)

    for e in editions: # resolve redirects
        if 'authors' not in e:
            continue
        new_authors = []
        for akey in e['authors']:
            a = ol.get(akey)
            if a['type'] == Reference('/type/redirect'):
                akey = Reference(a['location'])
            else:
                assert a['type'] == Reference('/type/author')
            new_authors.append(akey)
        e['authors'] = new_authors

    k = 'publish_date'
    publish_dates = set(e[k] for e in editions if k in e and len(e[k]) != 4)

    k = 'pagination'
    all_pagination = set(e[k].strip(':.') for e in editions if e.get(k))

    one_item_lists = {}
    for k in 'lc_classifications', 'publishers', 'contributions', 'series':
        one_item_lists[k] = set(e[k][0].strip('.') for e in editions if e.get(k) and len(set(e[k])) == 1)


    master.setdefault('source_records', [])
    for k in 'source_records', 'ia_box_id', 'other_titles','isbn_10','series':
        for e in editions[1:]:
            if not e.get(k):
                continue
            for i in e[k]:
                if i not in master.setdefault(k, []):
                    master[k].append(i)

    k = 'ocaid'
    for e in editions[1:]:
        if e.get(k) and 'ia:' + e[k] not in master['source_records']:
            master['source_records'].append(e[k])

    k = 'identifiers'
    if any(k in e for e in editions):
        master.setdefault(k, {})
        for e in editions[1:]:
            if k not in e:
                continue
            for a, b in e[k].items():
                for c in b:
                    if c in master[k].setdefault(a, []):
                        continue
                    master[k][a].append(c)

    any_publish_country = False
    k = 'publish_country'
    if k in all_keys:
        for e in editions:
            if e.get(k) and not e[k].strip().startswith('xx'):
                any_publish_country = True

    no_merge = False
    skip_fields = set(['source_records', 'ia_box_id', 'identifiers', 'ocaid', 'other_titles', 'series', 'isbn_10'])
    for k in all_keys:
        if k in skip_fields:
            continue

        uniq = defaultdict(list)
        for num, e in enumerate(editions):
            if e.get(k):
                if k == 'publish_date' and len(e[k]) == 4 and e[k].isdigit and any(e[k] in pd for pd in publish_dates):
                    continue
                if k == 'pagination' and any(len(i) > len(e[k].strip('.:')) and e[k].strip('.:') in i for i in all_pagination):
                    continue
                if k in one_item_lists and len(set(e.get(k, []))) == 1 and any(len(i) > len(e[k][0].strip('.')) and e[k][0].strip('.') in i for i in one_item_lists[k]):
                    continue
                if k == 'publish_country' and any_publish_country and e.get(k, '').strip().startswith('xx'):
                    continue
                if k == 'edition_name' and e[k].endswith(' ed edition'):
                    e[k] = e[k][:-len(' edition')]
                uniq[re_nonword.sub('', `e[k]`.lower())].append(num)

        if len(uniq) == 0:
            continue
        if len(uniq) == 1:
            master[k] = editions[uniq.values()[0][0]][k]
            continue

        if k == 'covers':
            assert all(isinstance(e[k], list) for e in editions if k in e)
            covers = set()
            for e in editions:
                if k in e:
                    covers.update(c for c in e[k] if c != -1)
            master['covers'] = sorted(covers)
            continue

        if k == 'notes':
            master['notes'] = ''
            for e in editions:
                if e.get('notes'):
                    master['notes'] += e['notes'] + '\n'
            continue

        if k == 'ocaid':
            for e in editions:
                if e.get('ocaid'):
                    if e['ocaid'].endswith('goog'):
                        print e['key'], e['ocaid'], ia
                    master['ocaid'] = e['ocaid']
                    break
            assert master['ocaid']
            continue

        if k == 'authors':
            min_author = set(min((e.get('authors', []) for e in editions), key=len))
            if all(min_author <= set(e.get('authors', [])) for e in editions):
                master[k] = max((e.get('authors', []) for e in editions), key=len)
                continue

        print 'unmerged field:', k
        print [e.get(k) for e in editions]
        no_merge = True
    if no_merge:
        continue
    if 'location' in master and isinstance(master['location'], basestring) and master['location'].startswith('/books/'):
        del master['location']
    updates.append(master)
    for e in editions[1:]:
        redirect = {
            'type': Reference('/type/redirect'),
            'location': min_ekey,
            'key': e['key'],
        }
        updates.append(redirect)
    print len(updates), min_ekey
    try:
        print ol.save_many(updates, 'merge lending editions')
    except:
        for i in updates:
            print i
        raise
    cur2.execute('update merge set done=now() where ia=%s', [ia])

########NEW FILE########
__FILENAME__ = get_ia
from openlibrary.catalog.marc import fast_parse, read_xml, is_display_marc
from openlibrary.catalog.utils import error_mail
from lxml import etree
import xml.parsers.expat
import urllib2, os.path, socket
from openlibrary.catalog.read_rc import read_rc
from time import sleep
from openlibrary.utils.ia import find_item

base = "http://www.archive.org/download/"

rc = read_rc()

class NoMARCXML:
    pass

def urlopen_keep_trying(url):
    for i in range(3):
        try:
            f = urllib2.urlopen(url)
        except urllib2.HTTPError, error:
            if error.code in (403, 404):
                #print "404 for '%s'" % url
                raise
            else:
                print 'error:', error.code, error.msg
            pass
        except urllib2.URLError:
            pass
        else:
            return f
        print url, "failed"
        sleep(2)
        print "trying again"

def bad_ia_xml(ia):
    if ia == 'revistadoinstit01paulgoog':
        return False
    # need to handle 404s:
    # http://www.archive.org/details/index1858mary
    loc = ia + "/" + ia + "_marc.xml"
    return '<!--' in urlopen_keep_trying(base + loc).read()

def get_marc_ia_data(ia, host=None, path=None):
    ia = ia.strip() # 'cyclopdiaofedu00kidd '
    ending = 'meta.mrc'
    if host and path:
        url = 'http://%s%s/%s_%s' % (host, path, ia, ending)
    else:
        url = 'http://www.archive.org/download/' + ia + '/' + ia + '_' + ending
    f = urlopen_keep_trying(url)
    return f.read() if f else None

def get_marc_ia(ia):
    ia = ia.strip() # 'cyclopdiaofedu00kidd '
    url = base + ia + "/" + ia + "_meta.mrc"
    data = urlopen_keep_trying(url).read()
    length = int(data[0:5])
    if len(data) != length:
        data = data.decode('utf-8').encode('raw_unicode_escape')
    assert len(data) == length

    assert 'Internet Archive: Error' not in data
    print 'leader:', data[:24]
    return data
    return fast_parse.read_edition(data, accept_electronic = True)

def get_ia(ia):
    ia = ia.strip() # 'cyclopdiaofedu00kidd '
    # read MARC record of scanned book from archive.org
    # try the XML first because it has better character encoding
    # if there is a problem with the XML switch to the binary MARC
    xml_file = ia + "_marc.xml"
    loc = ia + "/" + xml_file
    try:
        print base + loc
        f = urlopen_keep_trying(base + loc)
    except urllib2.HTTPError, error:
        if error.code == 404:
            raise NoMARCXML
        else:
            print 'error:', error.code, error.msg
            raise
    assert f
    if f:
        try:
            return read_xml.read_edition(f)
        except read_xml.BadXML:
            pass
        except xml.parsers.expat.ExpatError:
            #print 'IA:', `ia`
            #print 'XML parse error:', base + loc
            pass
    print base + loc
    if '<title>Internet Archive: Page Not Found</title>' in urllib2.urlopen(base + loc).read(200):
        raise NoMARCXML
    url = base + ia + "/" + ia + "_meta.mrc"
    print url
    try:
        f = urlopen_keep_trying(url)
    except urllib2.URLError:
        pass
    if not f:
        return None
    data = f.read()
    length = data[0:5]
    loc = ia + "/" + ia + "_meta.mrc:0:" + length
    if len(data) == 0:
        print 'zero length MARC for', url
        return None
    if 'Internet Archive: Error' in data:
        print 'internet archive error for', url
        return None
    if data.startswith('<html>\n<head>'):
        print 'internet archive error for', url
        return None
    try:
        return fast_parse.read_edition(data, accept_electronic = True)
    except (ValueError, AssertionError, fast_parse.BadDictionary):
        print `data`
        raise

def files(archive_id):
    url = base + archive_id + "/" + archive_id + "_files.xml"
    for i in range(5):
        try:
            tree = etree.parse(urlopen_keep_trying(url))
            break
        except xml.parsers.expat.ExpatError:
            sleep(2)
    try:
        tree = etree.parse(urlopen_keep_trying(url))
    except:
        print "error reading", url
        raise
    assert tree
    for i in tree.getroot():
        assert i.tag == 'file'
        name = i.attrib['name']
        print 'name:', name
        if name == 'wfm_bk_marc' or name.endswith('.mrc') or name.endswith('.marc') or name.endswith('.out') or name.endswith('.dat') or name.endswith('.records.utf8'):
            size = i.find('size')
            if size is not None:
                yield name, int(size.text)
            else:
                yield name, None

def get_data(loc):
    try:
        filename, p, l = loc.split(':')
    except ValueError:
        return None
    marc_path = rc.get('marc_path')
    if not marc_path:
        return None
    if not os.path.exists(marc_path + '/' + filename):
        return None
    f = open(rc['marc_path'] + '/' + filename)
    f.seek(int(p))
    buf = f.read(int(l))
    f.close()
    return buf

def get_from_archive(locator):
    if locator.startswith('marc:'):
        locator = locator[5:]
    filename, offset, length = locator.split (":")
    offset = int (offset)
    length = int (length)

    ia, rest = filename.split('/', 1)

    for attempt in range(5):
        try:
            host, path = find_item(ia)
            break
        except socket.timeout:
            if attempt == 4:
                raise
            print 'retry, attempt', attempt

    r0, r1 = offset, offset+length-1
    url = 'http://' + host + path + '/' + rest 

    assert 0 < length < 100000

    ureq = urllib2.Request(url, None, {'Range':'bytes=%d-%d'% (r0, r1)},)

    f = None
    for i in range(3):
        try:
            f = urllib2.urlopen(ureq)
        except urllib2.HTTPError, error:
            if error.code == 416:
                raise
            elif error.code == 404:
                print "404 for '%s'" % url
                raise
            else:
                print url
                print 'error:', error.code, error.msg
        except urllib2.URLError:
            pass
    if f:
        return f.read(100000)
    else:
        print locator, url, 'failed'

def get_from_local(locator):
    try:
        file, offset, length = locator.split(':')
    except:
        print 'locator:', `locator`
        raise
    f = open(rc['marc_path'] + '/' + file)
    f.seek(int(offset))
    buf = f.read(int(length))
    f.close()
    return buf

def read_marc_file(part, f, pos=0):
    try:
        for data, int_length in fast_parse.read_file(f):
            loc = "marc:%s:%d:%d" % (part, pos, int_length)
            pos += int_length
            yield (pos, loc, data)
    except ValueError:
        print f
        raise

def marc_formats(ia, host=None, path=None):
    files = {
        ia + '_marc.xml': 'xml',
        ia + '_meta.mrc': 'bin',
    }
    has = { 'xml': False, 'bin': False }
    ending = 'files.xml'
    if host and path:
        url = 'http://%s%s/%s_%s' % (host, path, ia, ending)
    else:
        url = 'http://www.archive.org/download/' + ia + '/' + ia + '_' + ending
    for attempt in range(10):
        f = urlopen_keep_trying(url)
        if f is not None:
            break
        sleep(10)
    if f is None:
        msg_from = 'load_scribe@archive.org'
        msg_to = ['edward@archive.org']
        subject = "error reading %s_files.xml" % ia
        msg = url
        error_mail(msg_from, msg_to, subject, msg)
        return has
    data = f.read()
    try:
        root = etree.fromstring(data)
    except:
        print 'bad:', `data`
        return has
    for e in root:
        name = e.attrib['name']
        if name in files:
            has[files[name]] = True
        if all(has.values()):
            break
    return has

def test_get_ia():
    ia = "poeticalworksoft00grayiala"
    expect = {
        'publisher': ['Printed by C. Whittingham for T. N. Longman and O. Rees [etc]'],
        'number_of_pages': 223,
        'full_title': 'The poetical works of Thomas Gray with some account of his life and writings ; the whole carefully revised and illustrated by notes ; to which are annexed, Poems addressed to, and in memory of Mr. Gray ; several of which were never before collected.',
        'publish_date': '1800',
        'publish_country': 'enk',
        'authors': [
            {'db_name': 'Gray, Thomas 1716-1771.', 'name': 'Gray, Thomas'}
        ],
        'oclc': ['5047966']
    }
    assert get_ia(ia) == expect


########NEW FILE########
__FILENAME__ = check_for_marc
from openlibrary.utils.ia import find_item
from time import sleep
import httplib
import socket

def head(host, path, ia):
    conn = httplib.HTTPConnection(host)
    conn.request("HEAD", path + "/" + ia + "_marc.xml")
    return conn.getresponse()

bad_machine = set()
out = open('has_marc', 'w')
no = open('no_marc', 'w')
later = open('later', 'w')
for line in open('to_load'):
    ia = line[:-1]
    if line.startswith('('):
        print >> no, ia
        continue
    (host, path) = find_item(ia)
    if not host:
        print >> no, ia
        continue
    if host in bad_machine:
        print >> later, ia
        continue
#    print "http://" + host + path + "/" + ia + "_marc.xml"
    try:
        r1 = head(host, path, ia)
    except socket.error:
        print 'socket error'
        print "http://" + host + path + "/" + ia + "_marc.xml"
        print 'try later'
        bad_machine.add(ia)
        print >> later, ia
        continue
        print 'retry in 2 seconds'

    if r1.status in (403, 404):
        print >> no, ia
        continue
    if r1.status != 200:
        print ia, host, path
        print r1.status, r1.reason
        print "http://" + host + path + "/" + ia + "_marc.xml"
    assert r1.status == 200

    print ia
    print >> out, ia
out.close()
later.close()
no.close()

########NEW FILE########
__FILENAME__ = count_archive_books
from catalog.read_rc import read_rc
import web, sys
rc = read_rc()
web.config.db_parameters = dict(dbn='mysql', db='archive', user=rc['ia_db_user'], pw=rc['ia_db_pass'], host=rc['ia_db_host'])
web.load()

row = list(web.select('metadata', what='count(*) as num', where="scanner = 'google' and mediatype='texts' and noindex is null"))
print 'Image PDFs:', row[0].num

row = list(web.select('metadata', what='count(*) as num', where="scanner != 'google' and noindex is null and mediatype='texts'"))
print 'Scanned books:', row[0].num

sys.exit(0)

for row in web.select('metadata', scanner='google'):
    print row.identifier

########NEW FILE########
__FILENAME__ = extract_paragraphs
from xml.etree.cElementTree import iterparse, tostring, Element
import sys, re

ns = '{http://www.abbyy.com/FineReader_xml/FineReader6-schema-v1.xml}'
page_tag = ns + 'page'

re_par_end_dot = re.compile(r'\.\W*$')

class PageBreak (object):
    def __init__(self, page_num):
        self.page_num = page_num

def read_text_line(line):
    text = ''
    for fmt in line:
        for c in fmt:
            text += c.text
    return text

def par_text(lines):
    cur = ''
    for line_num, line in enumerate(lines):
        first_char = line[0][0]
        if first_char.attrib['wordStart'] == 'false' or first_char.attrib['wordFromDictionary'] == 'false' and cur.endswith('- '):
            cur = cur[:-2]
        for fmt in line:
            cur += ''.join(c.text for c in fmt)
        if line_num + 1 != len(lines):
            cur += ' '
    return cur

def line_end_dot(line):
    return bool(re_par_end_dot.search(read_text_line(line)))

def par_unfinished(last_line, page_w):
    last_line_len = sum(len(fmt) for fmt in last_line)
    if last_line_len < 15 or line_end_dot(last_line):
        return False
    last_line_last_char = last_line[-1][-1]
    r = float(last_line_last_char.attrib['r'])
    return r / page_w > 0.75

def col_unfinished(last_line):
    return sum(len(fmt) for fmt in last_line) > 14 and not line_end_dot(last_line)

def par_iter(ia):
    f = open(ia + '_abbyy')
    incomplete_par = None
    end_column_par = None
    skipped_par = []
    #for page_num, (eve, page) in enumerate(iterparse(f, tag=page_tag)):
    for page_num, (eve, page) in enumerate(iterparse(f)):
        if page.tag != page_tag:
            continue
        if incomplete_par is None:
            yield [PageBreak(page_num)]

        page_w = float(page.attrib['width'])
        assert page.tag == page_tag

        for block_num, block in enumerate(page):
            if block.attrib['blockType'] != 'Text':
                continue
            region, text = block
            for par_num, par in enumerate(text):
                if len(par) == 0 or len(par[0]) == 0 or len(par[0][0]) == 0:
                    continue
                last_line = par[-1]
                if end_column_par is not None:
                    if line_end_dot(last_line) and int(par[0].attrib['t']) < int(end_column_par[0].attrib['b']):
                        print 'end column par'
                        yield list(end_column_par) + list(par)
                        end_column_par = None
                        continue
                    else:
                        yield list(end_column_par)
                    end_column_par = None

                if incomplete_par is not None:
                    if line_end_dot(last_line):
                        yield list(incomplete_par) + [PageBreak(page_num)] + list(par)
                        for p in skipped_par:
                            yield list(p)
                        incomplete_par = None
                        skipped_par = []
                    else:
                        skipped_par.append(par)
                elif par_num + 1 == len(text) and block_num + 1 == len(page) and par_unfinished(last_line, page_w):
                        incomplete_par = par
                elif par_num + 1 == len(text) and block_num + 1 != len(page) and col_unfinished(last_line):
                        end_column_par = par
                else:
                    yield list(par)

        page.clear()

for lines in par_iter(sys.argv[1]):
    lines = [l for l in lines if not isinstance(l, PageBreak)]
    text = par_text(lines)
    print text.encode('utf-8')
    print

########NEW FILE########
__FILENAME__ = get_loaded
from catalog.read_rc import read_rc
import web, sys
rc = read_rc()
web.config.db_parameters = dict(dbn='postgres', db=rc['db'], user=rc['user'], pw=rc['pw'], host=rc['host'])
web.load()

iter = web.select('version', what='machine_comment', where="machine_comment like 'ia:%%'")

for row in iter:
    print row.machine_comment[3:]

########NEW FILE########
__FILENAME__ = parse_abbyy
from lxml.etree import iterparse, tostring
import re

ns = '{http://www.abbyy.com/FineReader_xml/FineReader6-schema-v1.xml}'
page_tag = ns + 'page'
block_tag = ns + 'block'
region_tag = ns + 'region'
text_tag = ns + 'text'
rect_tag = ns + 'rect'
par_tag = ns + 'par'
line_tag = ns + 'line'
formatting_tag = ns + 'formatting'
charParams_tag = ns + 'charParams'

re_page_num = re.compile(r'^\[?\d+\]?$')

def abbyy_to_par(f, debug=False):
    prev = ''
    page_count = 0
    for event, element in iterparse(f):
        if element.tag == page_tag:
            page_count+= 1
            if debug:
                print 'page', page_count
            page_break = True
            for block in element:
                assert block.tag == block_tag
                if block.attrib['blockType'] in ('Picture', 'Table'):
                    continue
                assert block.attrib['blockType'] == 'Text'
                assert len(block) in (1, 2)
                region = block[0]
                assert region.tag == region_tag
                text = []
                if len(block) == 2:
                    e_text = block[1]
                    assert e_text.tag == text_tag
                if debug:
                    print 'block', block.attrib
                first_line_in_block = True
                for par in e_text:
                    assert par.tag == par_tag
                    text = ''
                    for line in par:
                        assert line.tag == line_tag
                        for formatting in line:
                            assert formatting.tag == formatting_tag
                            cur = ''.join(e.text for e in formatting)
                            if first_line_in_block:
                                first_line_in_block = False
                                if re_page_num.match(cur.strip()):
                                    if debug:
                                        print 'page number:', cur
                                    continue
                            if formatting[0].attrib['wordStart'] == 'true' and text and text[-1] != ' ':
                                text += ' '
                            if cur != ' ' and formatting[0].attrib['wordStart'] == 'false' and text and text[-1] == '-':
                                text = text[:-1] + cur
                            else:
                                text += cur
                            for charParams in formatting:
                                assert charParams.tag == charParams_tag
                    if text == '':
                        continue
                    if page_break:
                        if prev and text[0].islower():
                            if prev[-1] == '-':
                                prev = prev[:-1] + text
                            else:
                                prev += ' ' + text
                            continue
                        page_break = False
                    if prev:
                        yield prev
                    prev = text

            element.clear()
    if prev:
        yield prev

if __name__ == '__main__':
    import sys
    for i in abbyy_to_par(sys.stdin, debug=False):
        print i.encode('utf-8')

########NEW FILE########
__FILENAME__ = scanned_identifiers
from catalog.read_rc import read_rc
import web, sys
rc = read_rc()
web.config.db_parameters = dict(dbn='mysql', db='archive', user=rc['ia_db_user'], pw=rc['ia_db_pass'], host=rc['ia_db_host'])
web.load()

iter = web.select('metadata', where="scanner != 'google' and noindex is null and mediatype='texts'")

for row in iter:
    print row.identifier

########NEW FILE########
__FILENAME__ = scan_img
import httplib
import xml.etree.ElementTree as et
import xml.parsers.expat, socket # for exceptions
import urllib, re
from openlibrary.catalog.get_ia import urlopen_keep_trying
from openlibrary.utils.ia import find_item

re_remove_xmlns = re.compile(' xmlns="[^"]+"')

def parse_scandata_xml(xml):
    xml = re_remove_xmlns.sub('', xml)
    tree = et.fromstring(xml)
    leaf = None
    leafNum = None
    cover = None
    title = None
    for e in tree.find('pageData'):
        assert e.tag == 'page'
        leaf = int(e.attrib['leafNum'])
        if leaf > 25: # enough
            break
        page_type = e.findtext('pageType')
        if page_type == 'Cover':
            cover = leaf
        elif page_type == 'Title Page' or page_type == 'Title':
            title = leaf
            break
    return (cover, title)

def zip_test(ia_host, ia_path, ia, zip_type):
    conn = httplib.HTTPConnection(ia_host)
    conn.request('HEAD', ia_path + "/" + ia + "_" + zip_type + ".zip")
    r1 = conn.getresponse()
    try:
        assert r1.status in (200, 403, 404)
    except AssertionError:
        print r1.status, r1.reason
        raise
    return r1.status

def find_title_leaf_et(ia_host, ia_path, scandata):
    return parse_scandata_xml(scandata)

def find_title(item_id):
    (ia_host, ia_path) = find_item(item_id)

    if not ia_host:
        return
    url = 'http://' + ia_host + ia_path + "/" + item_id + "_scandata.xml"
    scandata = None
    try:
        scandata = urlopen_keep_trying(url).read()
    except:
        pass
    if not scandata or '<book>' not in scandata[:100]:
        url = "http://" + ia_host + "/zipview.php?zip=" + ia_path + "/scandata.zip&file=scandata.xml"
        scandata = urlopen_keep_trying(url).read()
    if not scandata or '<book>' not in scandata:
        return

    zip_type = 'tif' if item_id.endswith('goog') else 'jp2'
    try:
        status = zip_test(ia_host, ia_path, item_id, zip_type)
    except socket.error:
        #print 'socket error:', ia_host
        bad_hosts.add(ia_host)
        return
    if status in (403, 404):
        #print zip_type, ' not found:', item_id
        return

    (cover, title) = parse_scandata_xml(scandata)
    return title

def find_img(item_id):
    (ia_host, ia_path) = find_item(item_id)

    if not ia_host:
        print 'no host', item_id, ia_host
        return
    url = 'http://' + ia_host + ia_path + "/" + item_id + "_scandata.xml"
    scandata = None
    try:
        scandata = urlopen_keep_trying(url).read()
    except:
        pass
    if not scandata or '<book>' not in scandata[:100]:
        url = "http://" + ia_host + "/zipview.php?zip=" + ia_path + "/scandata.zip&file=scandata.xml"
        scandata = urlopen_keep_trying(url).read()
    if not scandata or '<book>' not in scandata:
        return {}

    zip_type = 'tif' if item_id.endswith('goog') else 'jp2'
    try:
        status = zip_test(ia_host, ia_path, item_id, zip_type)
    except socket.error:
        print 'socket error:', ia_host
        bad_hosts.add(ia_host)
        return
    if status in (403, 404):
        print zip_type, ' not found:', item_id
        return

    (cover, title) = parse_scandata_xml(scandata)
    return {
        'item_id': item_id,
        'ia_host': ia_host, 
        'ia_path': ia_path,
        'cover': cover,
        'title': title
    }

def test_find_img():
    flatland ='flatlandromanceo00abbouoft'
    ret = find_img(flatland)
    assert ret['item_id'] == 'flatlandromanceo00abbouoft'
    assert ret['cover'] == 1 
    assert ret['title'] == 7

def test_find_img2():
    item_id = 'cu31924000331631'
    ret = find_img(item_id)
    assert ret['item_id'] == item_id
    assert ret['cover'] is None
    assert ret['title'] == 0

def test_no_full_text():
    item_id = 'histoirepopulair02cabeuoft'
    print find_img(item_id)

########NEW FILE########
__FILENAME__ = to_load
scanned = set(i[:-1] for i in open('scanned'))
loaded = set(i[:-1] for i in open('loaded'))

to_load = scanned - loaded
for i in sorted(to_load):
    print i

########NEW FILE########
__FILENAME__ = add_source_records
#!/usr/bin/python2.5
from time import time, sleep
import catalog.marc.fast_parse as fast_parse
import web, sys, codecs, re
import catalog.importer.pool as pool
from catalog.utils.query import query_iter
from catalog.importer.merge import try_merge
from catalog.marc.new_parser import read_edition
from catalog.importer.load import build_query
from catalog.get_ia import files, read_marc_file
from catalog.merge.merge_marc import build_marc
from catalog.importer.db_read import get_mc, withKey
from openlibrary.api import OpenLibrary

from catalog.read_rc import read_rc

rc = read_rc()

marc_index = web.database(dbn='postgres', db='marc_index')
marc_index.printing = False

db_amazon = web.database(dbn='postgres', db='amazon')
db_amazon.printing = False

ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

t0 = time()
t_prev = time()
rec_no = 0
chunk = 50
load_count = 0

archive_id = sys.argv[1]

def percent(a, b):
    return float(a * 100.0) / b

def progress(archive_id, rec_no, start_pos, pos):
    global t_prev, load_count
    cur_time = time()
    t = cur_time - t_prev
    t_prev = cur_time
    t1 = cur_time - t0
    rec_per_sec = chunk / t
    bytes_per_sec_total = (pos - start_pos) / t1

    q = {
        'chunk': chunk,
        'rec_no': rec_no,
        't': t,
        't1': t1,
        'part': part,
        'pos': pos,
        'load_count': load_count,
        'time': cur_time,
        'bytes_per_sec_total': bytes_per_sec_total,
    }
    pool.post_progress(archive_id, q)

def is_loaded(loc):
    assert loc.startswith('marc:')
    vars = {'loc': loc[5:]}
    db_iter = marc_index.query('select * from machine_comment where v=$loc', vars)
    if list(db_iter):
        return True
    iter = query_iter({'type': '/type/edition', 'source_records': loc})
    return bool(list(iter))

re_meta_mrc = re.compile('^([^/]*)_meta.mrc:0:\d+$')

def amazon_source_records(asin):
    iter = db_amazon.select('amazon', where='asin = $asin', vars={'asin':asin})
    return ["amazon:%s:%s:%d:%d" % (asin, r.seg, r.start, r.length) for r in iter]

def fix_toc(e):
    toc = e.get('table_of_contents')
    if not toc:
        return
    if isinstance(toc[0], dict) and toc[0]['type'] == '/type/toc_item':
        return
    return [{'title': unicode(i), 'type': '/type/toc_item'} for i in toc if i != u'']

re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon)\.$')

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

def add_source_records(key, new, thing):
    sr = None
    e = ol.get(key)
    if 'source_records' in e:
        if new in e['source_records']:
            return
        e['source_records'].append(new)
    else:
        existing = get_mc(key)
        amazon = 'amazon:'
        if existing.startswith(amazon):
            sr = amazon_source_records(existing[len(amazon):]) or [existing]
        else:
            m = re_meta_mrc.match(existing)
            sr = ['marc:' + existing if not m else 'ia:' + m.group(1)]
        assert new not in sr
        e['source_records'] = sr + [new]

    # fix other bits of the record as well
    new_toc = fix_toc(e)
    if new_toc:
        e['table_of_contents'] = new_toc
    if e.get('subjects', None) and any(has_dot(s) for s in e['subjects']):
        subjects = [s[:-1] if has_dot(s) else s for s in e['subjects']]
        e['subjects'] = subjects
    print ol.save(key, e, 'found a matching MARC record')
    if new_toc:
        new_edition = ol.get(key)
        # [{u'type': <ref: u'/type/toc_item'>}, ...]
        assert 'title' in new_edition['table_of_contents'][0]

def load_part(archive_id, part, start_pos=0):
    print 'load_part:', archive_id, part
    global rec_no, t_prev, load_count
    full_part = archive_id + "/" + part
    f = open(rc['marc_path'] + "/" + full_part)
    if start_pos:
        f.seek(start_pos)
    for pos, loc, data in read_marc_file(full_part, f, pos=start_pos):
        rec_no += 1
        if rec_no % chunk == 0:
            progress(archive_id, rec_no, start_pos, pos)

        if is_loaded(loc):
            continue
        want = ['001', '003', '010', '020', '035', '245']
        try:
            index_fields = fast_parse.index_fields(data, want)
        except KeyError:
            print loc
            print fast_parse.get_tag_lines(data, ['245'])
            raise
        except AssertionError:
            print loc
            raise
        if not index_fields or 'title' not in index_fields:
            continue

        edition_pool = pool.build(index_fields)

        if not edition_pool:
            continue

        rec = fast_parse.read_edition(data)
        e1 = build_marc(rec)

        match = False
        seen = set()
        for k, v in edition_pool.iteritems():
            for edition_key in v:
                if edition_key in seen:
                    continue
                seen.add(edition_key)
                thing = withKey(edition_key)
                assert thing
                if try_merge(e1, edition_key, thing):
                    add_source_records(edition_key, loc, thing)
                    match = True

        if not match:
            yield loc, data

start = pool.get_start(archive_id)
go = 'part' not in start

print archive_id

for part, size in files(archive_id):
    print part, size
    load_part(archive_id, part)

print "finished"

########NEW FILE########
__FILENAME__ = db_read
import web
import simplejson as json
from urllib import urlopen, urlencode
from time import sleep
from openlibrary.catalog.read_rc import read_rc

staging = False

db = web.database(dbn='postgres', db='marc_index', host='ol-db')
db.printing = False

def find_author(name): # unused
    iter = web.query('select key from thing, author_str where thing_id = id and key_id = 1 and value = $name', {'name': name })
    return [row.key for row in iter]

def read_from_url(url):
    data = None
    for i in range(20):
        try:
            data = urlopen(url).read()
            if data:
                break
            print 'data == None'
        except IOError:
            print 'IOError'
            print url
        sleep(10)
    if not data:
        return None
    ret = json.loads(data)
    if ret['status'] == 'fail' and ret['message'].startswith('Not Found: '):
        return None
    if ret['status'] != 'ok':
        print ret
    assert ret['status'] == 'ok'
    return ret['result']

def set_staging(v):
    global staging
    staging = v

def api_url():
    return "http://openlibrary.org%s/api/" % (':8080' if staging else '')
    
def api_versions(): return api_url() + "versions?"
def api_things(): return api_url() + "things?"
def api_get(): return api_url() + "get?key="

def get_versions(q): # unused
    url = api_versions() + urlencode({'query': json.dumps(q)})
    return read_from_url(url)

def get_things(q):
    url = api_things() + urlencode({'query': json.dumps(q)})
    return read_from_url(url)

def get_mc(key):
    found = list(db.query('select v from machine_comment where k=$key', {'key': key}))
    return found[0].v if found else None

def withKey(key):
    def process(key):
        return read_from_url(api_get() + key)

    for attempt in range(5):
        try:
            return process(key)
        except ValueError:
            pass
        sleep(10)
    return process(key)

########NEW FILE########
__FILENAME__ = from_scribe
import MySQLdb
from catalog.read_rc import read_rc
import catalog.marc.fast_parse as fast_parse
import catalog.marc.parse_xml as parse_xml
from time import time
from lang import add_lang
from olwrite import Infogami
from load import build_query
from merge import try_merge
from db_read import get_things
from catalog.get_ia import get_ia, urlopen_keep_trying
from catalog.merge.merge_marc import build_marc
import pool, sys, urllib2

archive_url = "http://archive.org/download/"

rc = read_rc()

infogami = Infogami('pharosdb.us.archive.org:7070')
infogami.login('ImportBot', rc['ImportBot'])

conn = MySQLdb.connect(host=rc['ia_db_host'], user=rc['ia_db_user'], \
        passwd=rc['ia_db_pass'], db='archive')
cur = conn.cursor()

#collection = sys.argv[1]

#print 'loading from collection: %s' % collection

def read_short_title(title):
    return str(fast_parse.normalize_str(title)[:25])

def make_index_fields(rec):
    fields = {}
    for k, v in rec.iteritems():
        if k in ('lccn', 'oclc', 'isbn'):
            fields[k] = v
            continue
        if k == 'full_title':
            fields['title'] = [read_short_title(v)]
    return fields

def write_edition(loc, edition):
    add_lang(edition)
    q = build_query(loc, edition)

    for a in (i for i in q.get('authors', []) if 'key' not in i):
        a['key'] = infogami.new_key('/type/author')

    key = infogami.new_key('/type/edition')
    last_key = key
    q['key'] = key
    ret = infogami.write(q, comment='initial import', machine_comment=loc)
    assert ret['status'] == 'ok'
    print ret
    pool.update(key, q)

def load():
    global rec_no, t_prev
    skipping = False
    #for ia in ['nybc200715']:
    #cur.execute("select identifier from metadata where collection=%(c)s", {'c': collection})
    cur.execute("select identifier from metadata where scanner is not null and scanner != 'google' and noindex is null and mediatype='texts' and curatestate='approved'") # order by curatedate")
    for ia, in cur.fetchall():
        rec_no += 1
        if rec_no % chunk == 0:
            t = time() - t_prev
            t_prev = time()
            t1 = time() - t0
            rec_per_sec = chunk / t
            rec_per_sec_total = rec_no / t1
            remaining = total - rec_no
            sec = remaining / rec_per_sec_total
            print "%8d current: %9.3f overall: %9.3f" % (rec_no, rec_per_sec, rec_per_sec_total),
            hours = sec / 3600
            print "%6.3f hours" % hours

        print ia
        if get_things({'type': '/type/edition', 'ocaid': ia}):
            print 'already loaded'
            continue
        try:
            loc, rec = get_ia(ia)
        except (KeyboardInterrupt, NameError):
            raise
        except urllib2.HTTPError:
            continue
        if loc is None:
            continue
        print loc, rec

        if not loc.endswith('.xml'):
            print "not XML"
            continue
        if 'full_title' not in rec:
            print "full_title missing"
            continue
        index_fields = make_index_fields(rec)
        if not index_fields:
            print "no index_fields"
            continue

        edition_pool = pool.build(index_fields)
        print edition_pool

        if not edition_pool:
            yield loc, ia
            continue

        e1 = build_marc(rec)

        match = False
        for k, v in edition_pool.iteritems():
            if k == 'title' and len(v) > 50:
                continue
            for edition_key in v:
                if try_merge(e1, edition_key.replace('\/', '/')):
                    match = True
                    break
            if match:
                break
        if not match:
            yield loc, ia

t0 = time()
t_prev = time()
chunk = 50
last_key = None
load_count = 0
rec_no = 0
total = 100000

for loc, ia in load():
    print "load", loc, ia
    url = archive_url + loc
    f = urlopen_keep_trying(url)
    try:
        edition = parse_xml.parse(f)
    except AssertionError:
        continue
    except parse_xml.BadSubtag:
        continue
    except KeyError:
        continue
    if 'title' not in edition:
        continue
    edition['ocaid'] = ia
    write_edition("ia:" + ia, edition)


print "finished"

########NEW FILE########
__FILENAME__ = import_imagepdf
import web,  sys
from catalog.utils.query import query, withKey
from catalog.read_rc import read_rc
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, unmarshal

rc = read_rc()
ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

db = web.database(dbn='mysql', host=rc['ia_db_host'], user=rc['ia_db_user'], \
        passwd=rc['ia_db_pass'], db='archive')
db.printing = False

iter = db.query("select identifier from metadata where noindex is null and mediatype='texts' and scanner='google'")

for i in iter:
    ia = i.identifier
    print ia
    if query({'type': '/type/edition', 'ocaid': ia}):
        print 'already loaded'
        continue
    if query({'type': '/type/edition', 'source_records': 'ia:' + ia}):
        print 'already loaded'
        continue

########NEW FILE########
__FILENAME__ = import_marc
#!/usr/bin/python2.5
from time import time, sleep
import openlibrary.catalog.marc.fast_parse as fast_parse
from openlibrary.catalog.marc.marc_binary import MarcBinary
import web, sys, codecs, re, urllib2, httplib
from openlibrary.catalog.importer import pool
import simplejson as json
from openlibrary.catalog.utils.query import query_iter
from openlibrary.catalog.importer.merge import try_merge
from openlibrary.catalog.marc.parse import read_edition
from openlibrary.catalog.importer.load import build_query, east_in_by_statement, import_author
from openlibrary.catalog.works.find_work_for_edition import find_matching_work
#from openlibrary.catalog.works.find_works import find_title_redirects, find_works, get_books, books_query, update_works
from openlibrary.catalog.get_ia import files, read_marc_file
from openlibrary.catalog.merge.merge_marc import build_marc
from openlibrary.catalog.importer.db_read import get_mc, withKey
from openlibrary.catalog.marc.marc_subject import subjects_for_work
from openlibrary.api import OpenLibrary, unmarshal

from openlibrary.catalog.read_rc import read_rc

rc = read_rc()

marc_index = web.database(dbn='postgres', db='marc_index')
marc_index.printing = True

db_amazon = web.database(dbn='postgres', db='amazon')
db_amazon.printing = False

ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

t0 = time()
t_prev = time()
rec_no = 0
chunk = 50
load_count = 0

re_edition_key = re.compile('^/(?:books|b)/(OL\d+M)$')

archive_id = sys.argv[1]

def get_with_retry(key):
    for i in range(3):
        try:
            return ol.get(key)
        except urllib2.HTTPError, error:
            if error.code != 500:
                raise
        print 'retry save'
        sleep(10)
    return ol.get(key)

def save_with_retry(key, data, comment):
    for i in range(3):
        try:
            return ol.save(key, data, comment)
        except urllib2.HTTPError, error:
            if error.code != 500:
                raise
        print 'retry save'
        sleep(10)

# urllib2.HTTPError: HTTP Error 500: Internal Server Error

def percent(a, b):
    return float(a * 100.0) / b

def progress(archive_id, rec_no, start_pos, pos):
    global t_prev, load_count
    cur_time = time()
    t = cur_time - t_prev
    t_prev = cur_time
    t1 = cur_time - t0
    rec_per_sec = chunk / t
    bytes_per_sec_total = (pos - start_pos) / t1

    q = {
        'chunk': chunk,
        'rec_no': rec_no,
        't': t,
        't1': t1,
        'part': part,
        'pos': pos,
        'load_count': load_count,
        'time': cur_time,
        'bytes_per_sec_total': bytes_per_sec_total,
    }
    pool.post_progress(archive_id, q)

def is_loaded(loc):
    assert loc.startswith('marc:')
    vars = {'loc': loc[5:]}
    db_iter = marc_index.query('select * from machine_comment where v=$loc', vars)
    if list(db_iter):
        return True
    iter = query_iter({'type': '/type/edition', 'source_records': loc})
    return bool(list(iter))

re_meta_mrc = re.compile('^([^/]*)_meta.mrc:0:\d+$')

def amazon_source_records(asin):
    iter = db_amazon.select('amazon', where='asin = $asin', vars={'asin':asin})
    return ["amazon:%s:%s:%d:%d" % (asin, r.seg, r.start, r.length) for r in iter]

def fix_toc(e):
    toc = e.get('table_of_contents', None)
    if not toc:
        return
    if isinstance(toc[0], dict) and toc[0]['type'] == '/type/toc_item':
        return
    return [{'title': unicode(i), 'type': '/type/toc_item'} for i in toc if i != u'']

re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon)\.$')

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

def author_from_data(loc, data):
    edition = read_edition(data)
    assert 'authors' in edition
    east = east_in_by_statement(edition)
    assert len(edition['authors']) == 1
    print `edition['authors'][0]`
    a = import_author(edition['authors'][0], eastern=east)
    if 'key' in a:
        return {'key': a['key']}
    ret = ol.new(a, comment='new author')
    print 'ret:', ret
    assert isinstance(ret, basestring)
    return {'key': ret}

def undelete_author(a):
    key = a['key']
    assert a['type'] == '/type/delete'
    url = 'http://openlibrary.org' + key + '.json?v=' + str(a['revision'] - 1)
    prev = unmarshal(json.load(urllib2.urlopen(url)))
    assert prev['type'] == '/type/author'
    save_with_retry(key, prev, 'undelete author')

def undelete_authors(authors):
    for a in authors:
        if a['type'] == '/type/delete':
            undelete_author(a)
        else:
            print a
            assert a['type'] == '/type/author'

def add_source_records(key, new, thing, data):
    sr = None
    e = get_with_retry(key)
    if 'source_records' in e:
        if new in e['source_records']:
            return
        e['source_records'].append(new)
    else:
        existing = get_mc(key)
        amazon = 'amazon:'
        if existing.startswith('ia:'):
            sr = [existing]
        elif existing.startswith(amazon):
            sr = amazon_source_records(existing[len(amazon):]) or [existing]
        else:
            m = re_meta_mrc.match(existing)
            sr = ['marc:' + existing if not m else 'ia:' + m.group(1)]
        assert new not in sr
        e['source_records'] = sr + [new]

    # fix other bits of the record as well
    new_toc = fix_toc(e)
    if new_toc:
        e['table_of_contents'] = new_toc
    if e.get('subjects', None) and any(has_dot(s) for s in e['subjects']):
        subjects = [s[:-1] if has_dot(s) else s for s in e['subjects']]
        e['subjects'] = subjects
    if 'authors' in e:
        if any(a=='None' for a in e['authors']):
            assert len(e['authors']) == 1
            new_author = author_from_data(new, data)
            e['authors'] = [new_author]
        else:
            print e['authors']
            authors = [get_with_retry(akey) for akey in e['authors']]
            while any(a['type'] == '/type/redirect' for a in authors):
                print 'following redirects'
                authors = [ol.get(a['location']) if a['type'] == '/type/redirect' else a for a in authors]
            e['authors'] = [{'key': a['key']} for a in authors]
            undelete_authors(authors)
    try:
        print save_with_retry(key, e, 'found a matching MARC record')
    except:
        print e
        raise
    if new_toc:
        new_edition = ol.get(key)
        # [{u'type': <ref: u'/type/toc_item'>}, ...]
        assert 'title' in new_edition['table_of_contents'][0]

def load_part(archive_id, part, start_pos=0):
    print 'load_part:', archive_id, part
    global rec_no, t_prev, load_count
    full_part = archive_id + "/" + part
    f = open(rc['marc_path'] + "/" + full_part)
    if start_pos:
        f.seek(start_pos)
    for pos, loc, data in read_marc_file(full_part, f, pos=start_pos):
        rec_no += 1
        if rec_no % chunk == 0:
            progress(archive_id, rec_no, start_pos, pos)

        if is_loaded(loc):
            continue
        want = ['001', '003', '010', '020', '035', '245']
        try:
            index_fields = fast_parse.index_fields(data, want)
        except KeyError:
            print loc
            print fast_parse.get_tag_lines(data, ['245'])
            raise
        except AssertionError:
            print loc
            raise
        except fast_parse.NotBook:
            continue
        if not index_fields or 'title' not in index_fields:
            continue

        print loc
        edition_pool = pool.build(index_fields)

        if not edition_pool:
            yield loc, data
            continue

        rec = fast_parse.read_edition(data)
        e1 = build_marc(rec)

        match = False
        seen = set()
        for k, v in edition_pool.iteritems():
            for edition_key in v:
                if edition_key in seen:
                    continue
                thing = None
                while not thing or thing['type']['key'] == '/type/redirect':
                    seen.add(edition_key)
                    thing = withKey(edition_key)
                    assert thing
                    if thing['type']['key'] == '/type/redirect':
                        print 'following redirect %s => %s' % (edition_key, thing['location'])
                        edition_key = thing['location']
                if try_merge(e1, edition_key, thing):
                    add_source_records(edition_key, loc, thing, data)
                    match = True
                    break
            if match:
                break

        if not match:
            yield loc, data

start = pool.get_start(archive_id)
go = 'part' not in start

print archive_id

def write(q): # unused
    if 0:
        for i in range(10):
            try:
                return ol.new(q, comment='initial import')
            except (KeyboardInterrupt, NameError):
                raise
            except:
                pass
            sleep(30)
    try:
        return ol.new(q, comment='initial import')
    except:
        print q
        raise

def write_edition(loc, edition):
    q = build_query(loc, edition)
    authors = []
    for a in q.get('authors', []):
        if 'key' in a:
            authors.append({'key': a['key']})
        else:
            try:
                ret = ol.new(a, comment='new author')
            except:
                print a
                raise
            print 'ret:', ret
            assert isinstance(ret, basestring)
#            assert ret['status'] == 'ok'
#            assert 'created' in ret and len(ret['created']) == 1
            authors.append({'key': ret})
    q['source_records'] = [loc]
    if authors:
        q['authors'] = authors

    wkey = None
    subjects = subjects_for_work(rec)
    if 'authors' in q:
        wkey = find_matching_work(q)
    if wkey:
        w = ol.get(wkey)
        need_update = False
        for k, subject_list in subjects.items():
            for s in subject_list:
                if s not in w.get(k, []):
                    w.setdefault(k, []).append(s)
                    need_update = True
        if need_update:
            ol.save(wkey, w, 'add subjects from new record')
    else:
        w = {
            'type': '/type/work',
            'title': q['title'],
        }
        if 'authors' in q:
            w['authors'] = [{'type':'/type/author_role', 'author': akey} for akey in q['authors']]
        w.update(subjects)

        wkey = ol.new(w, comment='initial import')
    q['works'] = [{'key': wkey}]

    for attempt in range(5):
        if attempt > 0:
            print 'retrying'
        try:
            ret = ol.new(q, comment='initial import')
        except httplib.BadStatusLine:
            sleep(10)
            continue
        except: # httplib.BadStatusLine
            print q
            raise
        break
    print 'ret:', ret
    assert isinstance(ret, basestring)
    key = '/b/' + re_edition_key.match(ret).group(1)
#    assert ret['status'] == 'ok'
#    assert 'created' in ret
#    editions = [i for i in ret['created'] if i.startswith('/b/OL')]
#    assert len(editions) == 1
    # get key from return
    pool.update(key, q)

    return

    for a in authors:
        akey = a['key']
        title_redirects = find_title_redirects(akey)
        works = find_works(akey, get_books(akey, books_query(akey)), existing=title_redirects)
        works = list(works)
        updated = update_works(akey, works, do_updates=True)

for part, size in files(archive_id):
#for part, size in marc_loc_updates:
    print part, size
    if not go:
        if part == start['part']:
            go = True
            print "starting %s at %d" % (part, start['pos'])
            part_iter = load_part(archive_id, part, start_pos=start['pos'])
        else:
            continue
    else:
        part_iter = load_part(archive_id, part)

    for loc, data in part_iter:
        #if loc == 'marc_binghamton_univ/bgm_openlib_final_10-15.mrc:265680068:4538':
        #    continue
        assert len(data) == int(data[:5])
        rec = MarcBinary(data)
        edition = read_edition(rec)
        if edition['title'] == 'See.':
            print 'See.', edition
            continue
        if edition['title'] == 'See also.':
            print 'See also.', edition
            continue
        load_count += 1
        if load_count % 100 == 0:
            print "load count", load_count
        write_edition(loc, edition)
        sleep(2)

print "finished"

########NEW FILE########
__FILENAME__ = import_server
#!/usr/local/bin/python2.5
import web, dbhash, sys
import simplejson as json
from openlibrary.catalog.load import add_keys
from copy import deepcopy
from openlibrary.catalog.merge.index import *
from urllib import urlopen, urlencode

path = '/1/edward/marc_index/'
#dbm_fields = ('lccn', 'oclc', 'isbn', 'title')
#dbm = dict((i, dbhash.open(path + i + '.dbm', flag='w')) for i in dbm_fields)

store_db = dbhash.open(path + "store.dbm", flag='w')

urls = (
#    '/', 'index',
    '/store/(.*)', 'store',
    '/keys', 'keys',
)

app = web.application(urls, globals())

def build_pool(index_fields): # unused
    pool = {}
    for field in dbm_fields:
        if not field in index_fields:
            continue
        for v in index_fields[field]:
            if field == 'isbn' and len(v) < 10:
                continue
            try:
                v = str(v)
            except UnicodeEncodeError:
                print index_fields
                print `field, v`
                print v
                raise
            if v not in dbm[field]:
                continue
            pool.setdefault(field, set()).update(dbm[field][v].split(' '))
    return dict((k, sorted(v)) for k, v in pool.iteritems())

def add_to_indexes(record, dbm): # unused
    if 'title' not in record or record['title'] is None:
        return
    if 'subtitle' in record and record['subtitle'] is not None:
        title = record['title'] + ' ' + record['subtitle']
    else:
        title = record['title']
    st = str(short_title(title))
#    if st in dbm['title'] and id in dbm['title'][st].split(' '):
#        return # already done
    add_to_index(dbm['title'], st, record['key'])
    if 'title_prefix' in record and record['title_prefix'] is not None:
        title2 = short_title(record['title_prefix'] + title)
        add_to_index(dbm['title'], title2, record['key'])

    fields = [
        ('lccn', 'lccn', clean_lccn),
        ('oclc_numbers', 'oclc', None),
        ('isbn_10', 'isbn', None),
        ('isbn_13', 'isbn', None),
    ]
    for a, b, clean in fields:
        if a not in record:
            continue
        for v in record[a]:
            if not v:
                continue
            if clean:
                v = clean(v)
            add_to_index(dbm[b], v, record['key'])

class index: # unused
    def GET(self):
        web.header('Content-Type','application/json; charset=utf-8', unique=True)
        q = web.input()
        fields = dict((f, q[f].split('_')) for f in dbm_fields if f in q)
        pool = build_pool(fields)
        print cjson.encode({'fields': fields, 'pool': pool})
    def POST(self):
        q = cjson.decode(web.data())
        add_to_indexes(q, dbm)
        print 'success',
        
class store:
    def GET(self, key):
        web.header('Content-Type','application/json; charset=utf-8', unique=True)
        key = str(key)
        if key in store_db:
            return store_db[key]
        else:
            error = {'error': key + " not found", 'keys': store_db.keys() }
            return json.dumps(error)
    def POST(self, key):
        key = str(key)
        store_db[key] = web.data()
        return "saved"

class keys:
    def GET(self):
        web.header('Content-Type','application/json; charset=utf-8', unique=True)
        return json.dumps(store_db.keys())

if __name__ == '__main__':
    try:
        app.run()
    except:
        print "closing dbm files"
#        for v in dbm.itervalues():
#            v.close()
        store_db.close()
        print "closed"
        raise

########NEW FILE########
__FILENAME__ = lang
from db_read import get_things

def get_langs():
    lang = []
    offset = 0
    while True:
        i = get_things({'type': '/type/language', 'limit': 100, 'offset': offset})
        lang += i
        if len(i) != 100:
            break
        offset += 100
    return set(lang)

languages = get_langs()

def add_lang(edition):
    if 'languages' not in edition:
        return
    lang_key = edition['languages'][0]['key']
    if lang_key in ('/l/   ', '/l/|||'):
        del edition['languages']
    elif lang_key not in languages:
        del edition['languages']

########NEW FILE########
__FILENAME__ = load
import web, re, os
from db_read import withKey
from openlibrary.catalog.utils import flip_name, author_dates_match, key_int, error_mail
from openlibrary.catalog.utils.query import query_iter
from pprint import pprint
from openlibrary.catalog.read_rc import read_rc
from openlibrary.api import OpenLibrary

rc = read_rc()
ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

password = open(os.path.expanduser('~/.openlibrary_db_password')).read()
if password.endswith('\n'):
    password = password[:-1]
db_error = web.database(dbn='postgres', db='ol_errors', host='localhost', user='openlibrary', pw=password)

def walk_redirects(obj, seen):
    # called from find_author
    # uses ol client API
    seen.add(obj['key'])
    while obj['type'] == '/type/redirect':
        assert obj['location'] != obj['key']
        obj = ol.get(obj['location'])
        seen.add(obj['key'])
    return obj

def find_author(name, send_mail=True):
    q = {'type': '/type/author', 'name': name, 'limit': 0}
    reply = list(ol.query(q))
    authors = [ol.get(k) for k in reply]
    if any(a['type'] != '/type/author' for a in authors):
        subject = 'author query redirect: ' + `q['name']`
        body = 'Error: author query result should not contain redirects\n\n'
        body += 'query: ' + `q` + '\n\nresult\n'
        if send_mail:
            result = ''
            for a in authors:
                if a['type'] == '/type/redirect':
                    result += a['key'] + ' redirects to ' + a['location'] + '\n'
                elif a['type'] == '/type/delete':
                    result += a['key'] + ' is deleted ' + '\n'
                elif a['type'] == '/type/author':
                    result += a['key'] + ' is an author: ' + a['name'] + '\n'
                else:
                    result += a['key'] + 'has bad type' + a + '\n'
            body += result
            addr = 'edward@archive.org'
            #error_mail(addr, [addr], subject, body)
            db_error.insert('errors', query=name, result=result, t=web.SQLLiteral("now()"))
        seen = set()
        authors = [walk_redirects(a, seen) for a in authors if a['key'] not in seen]
    return authors

def do_flip(author):
    # given an author name flip it in place
    if 'personal_name' not in author:
        return
    if author['personal_name'] != author['name']:
        return
    first_comma = author['name'].find(', ')
    if first_comma == -1:
        return
    # e.g: Harper, John Murdoch, 1845-
    if author['name'].find(',', first_comma + 1) != -1:
        return
    if author['name'].find('i.e.') != -1:
        return
    if author['name'].find('i. e.') != -1:
        return
    name = flip_name(author['name'])
    author['name'] = name
    author['personal_name'] = name

def pick_from_matches(author, match):
    maybe = []
    if 'birth_date' in author and 'death_date' in author:
        maybe = [m for m in match if 'birth_date' in m and 'death_date' in m]
    elif 'date' in author:
        maybe = [m for m in match if 'date' in m]
    if not maybe:
        maybe = match
    if len(maybe) == 1:
        return maybe[0]
    return min(maybe, key=key_int)

def find_entity(author):
    name = author['name']
    things = find_author(name)
    if author['entity_type'] != 'person':
        if not things:
            return None
        db_entity = things[0]
#        if db_entity['type']['key'] == '/type/redirect':
#            db_entity = withKey(db_entity['location'])
        assert db_entity['type'] == '/type/author'
        return db_entity
    if ', ' in name:
        things += find_author(flip_name(name))
    match = []
    seen = set()
    for a in things:
        key = a['key']
        if key in seen:
            continue
        seen.add(key)
        orig_key = key
        assert a['type'] == '/type/author'
        if 'birth_date' in author and 'birth_date' not in a:
            continue
        if 'birth_date' not in author and 'birth_date' in a:
            continue
        if not author_dates_match(author, a):
            continue
        match.append(a)
    if not match:
        return None
    if len(match) == 1:
        return match[0]
    try:
        return pick_from_matches(author, match)
    except ValueError:
        print 'author:', author
        print 'match:', match
        raise

def import_author(author, eastern=False):
    existing = find_entity(author)
    if existing:
        assert existing['type'] == '/type/author'
        for k in 'last_modified', 'id', 'revision', 'created':
            if k in existing:
                del existing[k]
        new = existing
        if 'death_date' in author and 'death_date' not in existing:
            new['death_date'] = {
                'connect': 'update',
                'value': author['death_date'],
            }
        return new
    else:
        if not eastern:
            do_flip(author)
        a = {
#            'create': 'unless_exists',
            'type': { 'key': '/type/author' },
            'name': author['name']
        }
        for f in 'title', 'personal_name', 'enumeration', 'birth_date', 'death_date', 'date':
            if f in author:
                a[f] = author[f]
        return a

type_map = {
    'description': 'text',
    'notes': 'text',
    'number_of_pages': 'int',
}

def east_in_by_statement(rec):
    if 'by_statement' not in rec:
        return False
    if 'authors' not in rec:
        return False
    name = rec['authors'][0]['name']
    flipped = flip_name(name)
    name = name.replace('.', '')
    name = name.replace(', ', '')
    if name == flipped.replace('.', ''):
        return False
    return rec['by_statement'].find(name) != -1

def check_if_loaded(loc): # unused
    return bool(get_versions({'machine_comment': loc}))

# {'publishers': [u'D. Obradovi'], 'pagination': u'204p.', 'source_records': [u'ia:zadovoljstvauivo00lubb'], 'title': u'Zadovoljstva u ivotu', 'series': [u'Srpska knjiecna zadruga.  [Izdanja] 133'], 'number_of_pages': {'type': '/type/int', 'value': 204}, 'languages': [{'key': '/languages/ser'}], 'lc_classifications': [u'BJ1571 A819 1910'], 'publish_date': '1910', 'authors': [{'key': '/authors/OL162549A'}], 'ocaid': u'zadovoljstvauivo00lubb', 'publish_places': [u'Beograd'], 'type': {'key': '/type/edition'}}

def build_query(loc, rec):
    if 'table_of_contents' in rec:
        assert not isinstance(rec['table_of_contents'][0], list)
    book = {
        'type': { 'key': '/type/edition'},
    }

    try:
        east = east_in_by_statement(rec)
    except:
        pprint(rec)
        raise
    if east:
        print rec

    langs = rec.get('languages', [])
    print langs
    if any(l['key'] == '/languages/zxx' for l in langs):
        print 'zxx found in langs'
        rec['languages'] = [l for l in langs if l['key'] != '/languages/zxx']
        print 'fixed:', langs

    for l in rec.get('languages', []):
        print l
        if l['key'] == '/languages/ser':
            l['key'] = '/languages/srp'
        if l['key'] in ('/languages/end', '/languages/enk', '/languages/ent', '/languages/enb'):
            l['key'] = '/languages/eng'
        if l['key'] == '/languages/emg':
            l['key'] = '/languages/eng'
        if l['key'] == '/languages/cro':
            l['key'] = '/languages/chu'
        if l['key'] == '/languages/jap':
            l['key'] = '/languages/jpn'
        if l['key'] == '/languages/fra':
            l['key'] = '/languages/fre'
        if l['key'] == '/languages/ila':
            l['key'] = '/languages/ita'
        if l['key'] == '/languages/gwr':
            l['key'] = '/languages/ger'
        if l['key'] == '/languages/fr ':
            l['key'] = '/languages/fre'
        if l['key'] == '/languages/it ':
            l['key'] = '/languages/ita'
        if l['key'] == '/languages/fle': # flemish -> dutch
            l['key'] = '/languages/dut'
        assert withKey(l['key'])

    for k, v in rec.iteritems():
        if k == 'authors':
            book[k] = [import_author(v[0], eastern=east)]
            continue
        if k in type_map:
            t = '/type/' + type_map[k]
            if isinstance(v, list):
                book[k] = [{'type': t, 'value': i} for i in v]
            else:
                book[k] = {'type': t, 'value': v}
        else:
            book[k] = v

    assert 'title' in book
    return book

########NEW FILE########
__FILENAME__ = load_cornell
import web, re, httplib, sys, urllib2
import simplejson as json
import openlibrary.catalog.importer.pool as pool
from openlibrary.catalog.merge.merge_marc import build_marc
from openlibrary.catalog.read_rc import read_rc
from openlibrary.catalog.importer.load import build_query, east_in_by_statement, import_author
from openlibrary.catalog.utils.query import query, withKey
from openlibrary.catalog.importer.merge import try_merge
from openlibrary.catalog.importer.lang import add_lang
from openlibrary.catalog.importer.update import add_source_records
from openlibrary.catalog.get_ia import get_ia, urlopen_keep_trying, NoMARCXML
from openlibrary.catalog.importer.db_read import get_mc
import openlibrary.catalog.marc.parse_xml as parse_xml
from time import time, sleep
import openlibrary.catalog.marc.fast_parse as fast_parse
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, unmarshal

rc = read_rc()
ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

db_amazon = web.database(dbn='postgres', db='amazon')
db_amazon.printing = False

db = web.database(dbn='mysql', host=rc['ia_db_host'], user=rc['ia_db_user'], \
        passwd=rc['ia_db_pass'], db='archive')
db.printing = False

start = '2009-10-11 22:04:57'
fh_log = open('/1/edward/logs/load_scribe', 'a')

t0 = time()
t_prev = time()
rec_no = 0
chunk = 50
load_count = 0

def read_short_title(title):
    return str(fast_parse.normalize_str(title)[:25])

def make_index_fields(rec):
    fields = {}
    for k, v in rec.iteritems():
        if k in ('lccn', 'oclc', 'isbn'):
            fields[k] = v
            continue
        if k == 'full_title':
            fields['title'] = [read_short_title(v)]
    return fields

archive_url = "http://archive.org/download/"

def load(loc, ia):
    print "load", loc, ia
    url = archive_url + loc
    f = urlopen_keep_trying(url)
    try:
        edition = parse_xml.parse(f)
    except AssertionError:
        return
    except parse_xml.BadSubtag:
        return
    except KeyError:
        return
    if 'title' not in edition:
        return
    edition['ocaid'] = ia
    write_edition("ia:" + ia, edition)

def write_edition(loc, edition):
    add_lang(edition)
    q = build_query(loc, edition)
    authors = []
    for a in q.get('authors', []):
        if 'key' in a:
            authors.append({'key': a['key']})
        else:
            try:
                ret = ol.new(a, comment='new author')
            except:
                print a
                raise
            print 'ret:', ret
            assert isinstance(ret, basestring)
            authors.append({'key': ret})
    q['source_records'] = [loc]
    if authors:
        q['authors'] = authors

    for attempt in range(50):
        if attempt > 0:
            print 'retrying'
        try:
            ret = ol.new(q, comment='initial import')
        except httplib.BadStatusLine:
            sleep(30)
            continue
        except: # httplib.BadStatusLine
            print q
            raise
        break
    print 'ret:', ret
    assert isinstance(ret, basestring)
    key = ret
    pool.update(key, q)

def write_log(ia, when, msg):
    #print >> fh_log, (ia, when, msg)
    print (ia, when, msg)
    fh_log.flush()

#iter = db.query("select identifier, updated from metadata where scanner is not null and noindex is null and mediatype='texts' and (curatestate='approved' or curatestate is null) and scandate is not null and updated > $start order by updated", {'start': start})
iter = db.query("select identifier, updated from metadata where contributor='Cornell University Library' and scanner is not null and noindex is null and mediatype='texts' and (curatestate='approved' or curatestate is null) and scandate is not null order by updated", {'start': start})
t_start = time()
for row in iter:
    ia = row.identifier
    print `ia`, row.updated
    when = str(row.updated)
    if query({'type': '/type/edition', 'ocaid': ia}):
        print 'already loaded'
        continue
    if query({'type': '/type/edition', 'source_records': 'ia:' + ia}):
        print 'already loaded'
        continue
    try:
        loc, rec = get_ia(ia)
    except (KeyboardInterrupt, NameError):
        raise
    except NoMARCXML:
        write_log(ia, when, "no MARCXML")
        continue
    except urllib2.HTTPError as error:
        write_log(ia, when, "error: HTTPError: " + str(error))
        continue
    if loc is None:
        write_log(ia, when, "error: no loc ")
    if rec is None:
        write_log(ia, when, "error: no rec")
        continue
    print loc, rec

    if not loc.endswith('.xml'):
        print "not XML"
        write_log(ia, when, "error: not XML")
        continue
    if 'full_title' not in rec:
        print "full_title missing"
        write_log(ia, when, "error: full_title missing")
        continue
    index_fields = make_index_fields(rec)
    if not index_fields:
        print "no index_fields"
        write_log(ia, when, "error: no index fields")
        continue

    edition_pool = pool.build(index_fields)

    if not edition_pool:
        load(loc, ia)
        write_log(ia, when, "loaded")
        continue

    e1 = build_marc(rec)

    match = False
    seen = set()
    for k, v in edition_pool.iteritems():
        for edition_key in v:
            if edition_key in seen:
                continue
            thing = None
            while not thing or thing['type']['key'] == '/type/redirect':
                seen.add(edition_key)
                thing = withKey(edition_key)
                assert thing
                if thing['type']['key'] == '/type/redirect':
                    print 'following redirect %s => %s' % (edition_key, thing['location'])
                    edition_key = thing['location']
            if try_merge(e1, edition_key, thing):
                add_source_records(edition_key, ia)
                write_log(ia, when, "found match: " + edition_key)
                match = True
                break
        if match:
            break

    if not match:
        load(loc, ia)
        write_log(ia, when, "loaded")

########NEW FILE########
__FILENAME__ = load_scribe
import web, re, httplib, sys, urllib2, threading
import simplejson as json
from lxml import etree
import openlibrary.catalog.importer.pool as pool
from openlibrary.catalog.marc.marc_xml import read_marc_file, MarcXml, BlankTag, BadSubtag
from openlibrary.catalog.marc.marc_binary import MarcBinary
from openlibrary.catalog.merge.merge_marc import build_marc
from openlibrary.catalog.importer.load import build_query, east_in_by_statement
from openlibrary.catalog.utils import error_mail
from openlibrary.catalog.utils.query import query, withKey
from openlibrary.catalog.importer.merge import try_merge
from openlibrary.catalog.importer.update import add_source_records
from openlibrary.catalog.get_ia import get_ia, urlopen_keep_trying, NoMARCXML, bad_ia_xml, marc_formats, get_marc_ia, get_marc_ia_data
from openlibrary.catalog.title_page_img.load import add_cover_image
from openlibrary.solr.update_work import update_work, solr_update
#from openlibrary.catalog.works.find_works import find_title_redirects, find_works, get_books, books_query, update_works
from openlibrary.catalog.works.find_work_for_edition import find_matching_work
from openlibrary.catalog.marc import fast_parse, is_display_marc
from openlibrary.catalog.marc.parse import read_edition, NoTitle
from openlibrary.catalog.marc.marc_subject import subjects_for_work
from openlibrary.utils.ia import find_item
from openlibrary import config
from time import time, sleep
from openlibrary.api import OpenLibrary
from pprint import pprint
from subprocess import Popen, PIPE
import argparse

parser = argparse.ArgumentParser(description='scribe loader')
parser.add_argument('--skip_hide_books', action='store_true')
parser.add_argument('--item_id')
parser.add_argument('--config', default='openlibrary.yml')
args = parser.parse_args()

config_file = args.config
config.load(config_file)
import_bot_password = config.runtime_config['load_scribe']['import_bot_password']
# '/1/var/log/openlibrary/load_scribe'
load_scribe_log = config.runtime_config['load_scribe']['log']

ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', import_bot_password)

password = Popen(["/opt/.petabox/dbserver"], stdout=PIPE).communicate()[0]
db = web.database(dbn='mysql', host='dbmeta.us.archive.org', user='archive', \
        passwd=password, db='archive')
db.printing = False

re_census = re.compile('^\d+(st|nd|rd|th)census')

re_edition_key = re.compile('^/(?:books|b)/(OL\d+M)$')

def read_short_title(title):
    return str(fast_parse.normalize_str(title)[:25])

def make_index_fields(rec):
    fields = {}
    for k, v in rec.iteritems():
        if k in ('lccn', 'oclc', 'isbn'):
            fields[k] = v
            continue
        if k == 'full_title':
            fields['title'] = [read_short_title(v)]
    return fields

def load_binary(ia, host, path):
    url = 'http://' + host + path + '/' + ia + '_meta.mrc'
    print url
    f = urlopen_keep_trying(url)
    data = f.read()
    assert '<title>Internet Archive: Page Not Found</title>' not in data[:200]
    if len(data) != int(data[:5]):
        data = data.decode('utf-8').encode('raw_unicode_escape')
    assert len(data) == int(data[:5])
    return MarcBinary(data)

def load_xml(ia, host, path):
    url = 'http://' + host + path + '/' + ia + '_marc.xml'
    print url
    f = urlopen_keep_trying(url)
    root = etree.parse(f).getroot()
    if root.tag == '{http://www.loc.gov/MARC21/slim}collection':
        root = root[0]
    return MarcXml(root)
    edition = read_edition(rec)
    assert 'title' in edition
    return edition

def load(ia, use_binary=False):
    print "load", ia
    if not use_binary:
        try:
            rec = load_xml(ia, host, path)
            edition = read_edition(rec)
        except BadSubtag:
            use_binary = True
        except BlankTag:
            use_binary = True
    if use_binary:
        rec = load_binary(ia, host, path)
        edition = read_edition(rec)
    pprint(edition)
    assert 'title' in edition

    edition['ocaid'] = ia
    write_edition(ia, edition, rec)

def write_edition(ia, edition, rec):
    loc = 'ia:' + ia
    if ia == 'munkai00apor':
        edition['languages'] = [
            {'key': '/languages/lat'},
            {'key': '/languages/hun'},
        ]
    elif ia == 'fzfasp00helt':
        edition['languages'] = [{'key': '/languages/hun'}]
    elif ia == 'coursetconfren00sema':
        edition['languages'] = [{'key': '/languages/fre'}]
    elif ia == 'repertoiredepein02rein':
        edition['languages'] = [{'key': '/languages/fre'}]
    elif ia == 'repertoiredepein01rein':
        edition['languages'] = [{'key': '/languages/fre'}]
    elif ia == 'cihm_39338':
        edition['languages'] = [{'key': '/languages/ger'}]
    elif ia == 'ofilhoprdigodr00mano':
        edition['languages'] = [{'key': '/languages/por'}]
    elif ia == 'nekaroronneteyer00hill':
        edition['languages'] = [{'key': '/languages/moh'}]
    elif ia == 'adventuresofamer00kouw':
        edition['languages'] = [{'key': '/languages/eng'}]
    elif ia == 'goldentreasury00wrig':
        edition['languages'] = [{'key': '/languages/grc'}]
    elif ia == 'dasrmischepriv00rein':
        edition['languages'] = [{'key': '/languages/ger'}]
    elif ia == 'lespritdelaligu02anqu':
        edition['languages'] = [{'key': '/languages/fre'}]
    elif ia == 'derelephantenord00berl':
        del edition['languages']
    q = build_query(loc, edition)
    authors = []
    for a in q.get('authors', []):
        if 'key' in a:
            authors.append({'key': a['key']})
        else:
            try:
                ret = ol.new(a, comment='new author')
            except:
                print a
                raise
            print 'ret:', ret
            assert isinstance(ret, basestring)
            authors.append({'key': ret})
    q['source_records'] = [loc]
    if authors:
        q['authors'] = authors

    wkey = None
    subjects = subjects_for_work(rec)
    subjects.setdefault('subjects', []).append('Accessible book')

    if 'printdisabled' in collections:
        subjects['subjects'].append('Protected DAISY')
    elif 'lendinglibrary' in collections:
        subjects['subjects'] += ['Protected DAISY', 'Lending library']
    elif 'inlibrary' in collections:
        subjects['subjects'] += ['Protected DAISY', 'In library']

    if 'authors' in q:
        wkey = find_matching_work(q)
    if wkey:
        w = ol.get(wkey)
        need_update = False
        for k, subject_list in subjects.items():
            for s in subject_list:
                if s not in w.get(k, []):
                    w.setdefault(k, []).append(s)
                    need_update = True
        if need_update:
            ol.save(wkey, w, 'add subjects from new record')
    else:
        w = {
            'type': '/type/work',
            'title': q['title'],
        }
        if 'authors' in q:
            w['authors'] = [{'type':'/type/author_role', 'author': akey} for akey in q['authors']]
        w.update(subjects)

        wkey = ol.new(w, comment='initial import')

    q['works'] = [{'key': wkey}]
    for attempt in range(50):
        if attempt > 0:
            print 'retrying'
        try:
            pprint(q)
            ret = ol.new(q, comment='initial import')
        except httplib.BadStatusLine:
            sleep(30)
            continue
        except: # httplib.BadStatusLine
            print q
            raise
        break
    print 'ret:', ret
    assert isinstance(ret, basestring)
    key = '/b/' + re_edition_key.match(ret).group(1)
    pool.update(key, q)

    print 'add_cover_image'
    t = threading.Thread(target=add_cover_image, args=(ret, ia))
    t.start()
    return

    print 'run work finder'


    # too slow
    for a in authors:
        akey = a['key']
        title_redirects = find_title_redirects(akey)
        works = find_works(akey, get_books(akey, books_query(akey)), existing=title_redirects)
        works = list(works)
        updated = update_works(akey, works, do_updates=True)

fh_log = None

def write_log(ia, when, msg):
    print >> fh_log, (ia, when, msg)
    fh_log.flush()

hide_state_file = config.runtime_config['state_dir'] + '/load_scribe_hide'
ignore_noindex = set(['printdisabled', 'lendinglibrary', 'inlibrary'])

def hide_books(start):
    hide_start = open(hide_state_file).readline()[:-1]
    print 'hide start:', hide_start

    mend = []
    fix_works = set()
    db_iter = db.query("select identifier, collection, updated from metadata where (noindex is not null or curatestate='dark') and mediatype='texts' and scandate is not null and updated > $start", {'start': hide_start})
    last_updated = None
    for row in db_iter:
        ia = row.identifier
        if row.collection:
            collections = set(i.lower().strip() for i in row.collection.split(';'))
            if ignore_noindex & collections:
                continue
        print `ia`, row.updated
        for eq in query({'type': '/type/edition', 'ocaid': ia}):
            print eq['key']
            e = ol.get(eq['key'])
            if 'ocaid' not in e:
                continue
            if 'works' in e:
                fix_works.update(e['works'])
            print e['key'], `e.get('title', None)`
            del e['ocaid']
            mend.append(e)
        last_updated = row.updated
    print 'removing links from %d editions' % len(mend)
    if not mend:
        return
    print ol.save_many(mend, 'remove link')
    requests = []
    for wkey in fix_works:
        requests += update_work(withKey(wkey))
    if fix_works:
        solr_update(requests + ['<commit/>'], debug=True)
    print >> open(hide_state_file, 'w'), last_updated

def load_error_mail(ia, marc_display, subject):
    msg_from = 'load_scribe@archive.org'
    msg_to = ['edward@archive.org']
    subject += ': ' + ia
    msg = 'http://www.archive.org/details/%s\n' % ia
    msg += 'http://www.archive.org/download/%s\n'
    msg += '\n' + bad_binary
    error_mail(msg_from, msg_to, subject, msg)

def error_marc_403(ia):
    msg_from = 'load_scribe@archive.org'
    msg_to = ['edward@archive.org']
    msg = 'http://www.archive.org/details/' + ia
    subject = 'MARC 403: ' + ia
    error_mail(msg_from, msg_to, subject, msg)

def bad_marc_alert(bad_marc):
    assert bad_marc
    msg_from = 'load_scribe@archive.org'
    msg_to = ['edward@archive.org']
    subject = '%d bad MARC' % len(bad_marc)
    msg = '\n'.join((
        'http://www.archive.org/details/%s\n' +
        'http://www.archive.org/download/%s\n\n' +
        '%s\n\n') % (ia, ia, `data`) for ia, data in bad_marc)
    error_mail(msg_from, msg_to, subject, msg)

if __name__ == '__main__':
    fh_log = open(load_scribe_log, 'a')

    open(config.runtime_config['state_dir'] + '/load_scribe.pid', 'w').write(os.getpid())
    start = open(state_file).readline()[:-1]
    bad_marc_last_sent = time()
    bad_marc = []

    while True:

        if args.item_id:
            db_iter = db.query("select identifier, contributor, updated, noindex, collection, format from metadata where scanner is not null and mediatype='texts' and (not curatestate='dark' or curatestate is null) and scandate is not null and format is not null and identifier=$item_id", {'item_id': args.item_id})
        else:
            print 'start:', start
            db_iter = db.query("select identifier, contributor, updated, noindex, collection, format from metadata where scanner is not null and mediatype='texts' and (not curatestate='dark' or curatestate is null) and scandate is not null and format is not null and updated between $start and date_add($start, interval 2 day) order by updated", {'start': start})
        t_start = time()
        for row in db_iter:
            if len(bad_marc) > 10 or (bad_marc and time() - bad_marc_last_sent > (4 * 60 * 60)):
                bad_marc_alert(bad_marc)
                bad_marc = []
                bad_marc_last_sent = time()

            ia = row.identifier
            host, path = find_item(ia)
            if 'pdf' not in row.format.lower():
                continue # scancenter and billing staff often use format like "%pdf%" as a proxy for having derived
            if row.contributor == 'Allen County Public Library Genealogy Center':
                print 'skipping Allen County Public Library Genealogy Center'
                continue
            if row.collection:
                collections = set(i.lower().strip() for i in row.collection.split(';'))
            else:
                collections = set()
            if row.noindex:
                if not row.collection:
                    continue
                collections = set(i.lower().strip() for i in row.collection.split(';'))
                if not ignore_noindex & collections:
                    continue
            if ia.startswith('annualreportspri'):
                print 'skipping:', ia
                continue
            if 'shenzhentest' in collections:
                continue

            if any('census' in c for c in collections):
                print 'skipping census'
                continue

            if re_census.match(ia) or ia.startswith('populationschedu') or ia.startswith('michigancensus') or 'census00reel' in ia or ia.startswith('populationsc1880'):
                print 'ia:', ia
                print 'collections:', list(collections)
                print 'census not marked correctly'
                continue
            assert 'passportapplicat' not in ia and 'passengerlistsof' not in ia
            if 'passportapplicat' in ia:
                print 'skip passport applications for now:', ia
                continue
            if 'passengerlistsof' in ia:
                print 'skip passenger lists', ia
                continue
            print `ia`, row.updated
            when = str(row.updated)
            if query({'type': '/type/edition', 'ocaid': ia}):
                print 'already loaded'
                continue
            if query({'type': '/type/edition', 'source_records': 'ia:' + ia}):
                print 'already loaded'
                continue

            try:
                formats = marc_formats(ia, host, path)
            except urllib2.HTTPError as error:
                write_log(ia, when, "error: HTTPError: " + str(error))
                continue
            use_binary = False
            bad_binary = None
            print formats
            rec = {}
            if formats['bin']:
                print 'binary'
                use_binary = True
                try:
                    marc_data = get_marc_ia_data(ia, host, path)
                except urllib2.HTTPError as error:
                    if error.code == 403:
                        error_marc_403(ia)
                        continue
                    raise
                if marc_data == '':
                    bad_binary = 'MARC binary empty string'
                if not bad_binary and is_display_marc(marc_data):
                    use_binary = False
                    bad_binary = marc_data
                    bad_marc.append((ia, marc_data))
                if not bad_binary:
                    try:
                        length = int(marc_data[0:5])
                    except ValueError:
                        bad_binary = "MARC doesn't start with number"
                if not bad_binary and len(marc_data) != length:
                    try:
                        marc_marc_data = marc_data.decode('utf-8').encode('raw_unicode_escape')
                    except:
                        bad_binary = "double UTF-8 decode error"
                if not bad_binary and len(marc_data) != length:
                    bad_binary = 'MARC length mismatch: %d != %d' % (len(marc_data), length)
                if not bad_binary and 'Internet Archive: Error' in marc_data:
                    bad_binary = 'Internet Archive: Error'
                if not bad_binary:
                    if str(marc_data)[6:8] != 'am': # only want books
                        print 'not a book!'
                        continue
                    try:
                        rec = fast_parse.read_edition(marc_data, accept_electronic = True)
                    except:
                        bad_binary = "MARC parse error"
            if bad_binary and not formats['xml']:
                load_error_mail(ia, bad_binary, 'bad MARC binary, no MARC XML')
                continue
            if not use_binary and formats['xml']:
                if bad_ia_xml(ia) and bad_binary:
                    load_error_mail(ia, bad_binary, 'bad MARC binary, bad MARC XML')
                    continue
                try:
                    rec = get_ia(ia)
                except (KeyboardInterrupt, NameError):
                    raise
                except NoMARCXML:
                    write_log(ia, when, "no MARCXML")
                    continue
                except urllib2.HTTPError as error:
                    write_log(ia, when, "error: HTTPError: " + str(error))
                    continue
            if not use_binary and not formats['xml']:
                print 'skipping, no MARC'
                continue

            if not rec:
                write_log(ia, when, "error: no rec")
                continue
            if 'physical_format' in rec:
                format = rec['physical_format'].lower()
                if format.startswith('[graphic') or format.startswith('[cartograph'):
                    continue
            print rec

            if 'full_title' not in rec:
                print "full_title missing"
                write_log(ia, when, "error: full_title missing")
                continue
            index_fields = make_index_fields(rec)
            if not index_fields:
                print "no index_fields"
                write_log(ia, when, "error: no index fields")
                continue

            edition_pool = pool.build(index_fields)

            if not edition_pool:
                load(ia, use_binary=use_binary)
                write_log(ia, when, "loaded")
                continue

            e1 = build_marc(rec)

            match = False
            seen = set()
            for k, v in edition_pool.iteritems():
                for edition_key in v:
                    if edition_key in seen:
                        continue
                    thing = None
                    found = True
                    while not thing or thing['type']['key'] == '/type/redirect':
                        seen.add(edition_key)
                        thing = withKey(edition_key)
                        assert thing
                        if 'type' not in thing:
                            print thing
                        if thing.get('error') == 'notfound':
                            found = False
                            break
                        if thing['type']['key'] == '/type/redirect':
                            print 'following redirect %s => %s' % (edition_key, thing['location'])
                            edition_key = thing['location']
                    if not found:
                        continue
                    if try_merge(e1, edition_key, thing):
                        add_source_records(edition_key, ia)
                        write_log(ia, when, "found match: " + edition_key)
                        match = True
                        break
                if match:
                    break

            if not match:
                try:
                    load(ia, use_binary=use_binary)
                except:
                    print 'bad item:', ia
                    raise
                write_log(ia, when, "loaded")
            print >> open(state_file, 'w'), row.updated
        start = row.updated
        secs = time() - t_start
        mins = secs / 60
        print "finished %d took mins" % mins
        if args.item_id:
            break
        if not args.skip_hide_books:
            hide_books(start)
        print >> open(state_file, 'w'), start
        if mins < 30:
            print 'waiting'
            sleep(60 * 30 - secs)

########NEW FILE########
__FILENAME__ = merge
from openlibrary.catalog.merge.merge_marc import *
from openlibrary.catalog.read_rc import read_rc
import openlibrary.catalog.merge.amazon as amazon
from openlibrary.catalog.get_ia import *
from openlibrary.catalog.importer.db_read import withKey, get_mc
from openlibrary.api import OpenLibrary, Reference
import openlibrary.catalog.marc.fast_parse as fast_parse
import xml.parsers.expat
import web, sys
from time import sleep

rc = read_rc()

ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

ia_db = web.database(dbn='mysql', db='archive', user=rc['ia_db_user'], pw=rc['ia_db_pass'], host=rc['ia_db_host'])
ia_db.printing = False

re_meta_marc = re.compile('([^/]+)_(meta|marc)\.(mrc|xml)')

threshold = 875
amazon.set_isbn_match(225)

def try_amazon(thing):
    if 'isbn_10' not in thing:
        return None
    if 'authors' in thing:
        authors = []
        for a in thing['authors']:
            # this is a hack
            # the type of thing['authors'] should all be the same type
            if isinstance(a, dict):
                akey = a['key']
            else:
                assert isinstance(a, basestring)
                akey = a
            author_thing = withKey(akey)
            if 'name' in author_thing:
                authors.append(author_thing['name'])
    else:
        authors = []
    return amazon.build_amazon(thing, authors)

def is_dark_or_bad(ia):
    vars = { 'ia': ia }
    db_iter = None
    for attempt in range(5):
        try:
            db_iter = ia_db.query('select curatestate from metadata where identifier=$ia', vars)
            break
        except:
            print 'retry, attempt', attempt
            sleep(10)
    if db_iter is None:
        return False
    rows = list(db_iter)
    if len(rows) == 0:
        return True
    assert len(rows) == 1
    return rows[0].curatestate == 'dark'

def marc_match(e1, loc):
    print 'loc:', loc
    rec = fast_parse.read_edition(get_from_archive(loc))
    print 'rec:', rec
    try:
        e2 = build_marc(rec)
    except TypeError:
        print rec
        raise
    return attempt_merge(e1, e2, threshold, debug=False)

def ia_match(e1, ia):
    try:
        rec = get_ia(ia)
    except NoMARCXML:
        return False
    except urllib2.HTTPError:
        return False
    if rec is None or 'full_title' not in rec:
        return False
    try:
        e2 = build_marc(rec)
    except TypeError:
        print rec
        raise
    return attempt_merge(e1, e2, threshold, debug=False)

def amazon_match(e1, thing):
    try:
        a = try_amazon(thing)
    except IndexError:
        print thing['key']
        raise
    except AttributeError:
        return False
    if not a:
        return False
    try:
        return amazon.attempt_merge(a, e1, threshold, debug=False)
    except:
        print a
        print e1
        print thing['key']
        raise

def fix_source_records(key, thing):
    if 'source_records' not in thing:
        return False
    src_rec = thing['source_records']
    marc_ia = 'marc:ia:'
    if not any(i.startswith(marc_ia) for i in src_rec):
        return False
    e = ol.get(key)
    new = [i[5:] if i.startswith(marc_ia) else i for i in e['source_records']]
    e['source_records'] = new
    print e['ocaid']
    print e['source_records']
    assert 'ocaid' in e and ('ia:' + e['ocaid'] in e['source_records'])
    print 'fix source records'
    print ol.save(key, e, 'fix bad source records')
    return True

def source_records_match(e1, thing):
    marc = 'marc:'
    amazon = 'amazon:'
    ia = 'ia:'
    match = False
    for src in thing['source_records']:
        if src == 'marc:initial import':
            continue
        # hippocrates01hippuoft/hippocrates01hippuoft_marc.xml
        m = re_meta_marc.search(src)
        if m:
            src = 'ia:' + m.group(1)
        if src.startswith(marc):
            if marc_match(e1, src[len(marc):]):
                match = True
                break
        elif src.startswith(ia):
            if ia_match(e1, src[len(ia):]):
                match = True
                break
        else:
            assert src.startswith(amazon)
            if amazon_match(e1, thing):
                match = True
                break
    return match

def try_merge(e1, edition_key, thing):
    thing_type = thing['type']
    if thing_type != Reference('/type/edition'):
        print thing['key'], 'is', str(thing['type'])
    if thing_type == Reference('/type/delete'):
        return False
    assert thing_type == Reference('/type/edition')

    if 'source_records' in thing:
        if fix_source_records(edition_key, thing):
            thing = withKey(edition_key) # reload
        return source_records_match(e1, thing)

    ia = thing.get('ocaid', None)
    print edition_key
    mc = get_mc(edition_key)
    print mc
    if mc:
        if mc.startswith('ia:'):
            ia = mc[3:]
        elif mc.endswith('.xml') or mc.endswith('.mrc'):
            ia = mc[:mc.find('/')]
        if '_meta.mrc:' in mc:
            print thing
            if 'ocaid' not in thing:
                return False
            ia = thing['ocaid']
    rec2 = None
    if ia:
        if is_dark_or_bad(ia):
            return False
        try:
            rec2 = get_ia(ia)
        except xml.parsers.expat.ExpatError:
            return False
        except NoMARCXML:
            print 'no MARCXML'
            pass
        except urllib2.HTTPError, error:
            print error.code
            assert error.code in (404, 403)
        if not rec2:
            return True
    if not rec2:
        if not mc:
            mc = get_mc(thing['key'])
        if not mc or mc == 'initial import':
            return False
        if mc.startswith('amazon:'):
            try:
                a = try_amazon(thing)
            except IndexError:
                print thing['key']
                raise
            except AttributeError:
                return False
            if not a:
                return False
            try:
                return amazon.attempt_merge(a, e1, threshold, debug=False)
            except:
                print a
                print e1
                print thing['key']
                raise
        print 'mc:', mc
        try:
            assert not mc.startswith('ia:')
            data = get_from_archive(mc)
            if not data:
                return True
            rec2 = fast_parse.read_edition(data)
        except (fast_parse.SoundRecording, IndexError, AssertionError):
            print mc
            print edition_key
            return False
        except:
            print mc
            print edition_key
            raise
    if not rec2:
        return False
    try:
        e2 = build_marc(rec2)
    except TypeError:
        print rec2
        raise
    return attempt_merge(e1, e2, threshold, debug=False)

########NEW FILE########
__FILENAME__ = olwrite
import web
from infogami.infobase import client
import simplejson
import sys

web.ctx.ip = '127.0.0.1'

class Infogami:
    def __init__(self, host, sitename='openlibrary.org'):
        self.conn = client.connect(type='remote', base_url=host)
        self.sitename = sitename

    def _request(self, path, method, data):
        out = self.conn.request(self.sitename, path, method, data)
        out = simplejson.loads(out)
        if out['status'] == 'fail':
            raise Exception(out['message'])
        return out

    def login(self, username, password):
        return self._request('/account/login', 'POST', dict(username=username, password=password))

    def write(self, query, comment='', machine_comment=None):
        query = simplejson.dumps(query)
        return self._request('/write', 'POST', dict(query=query, comment=comment, machine_comment=machine_comment))

    def new_key(self, type):
        return self._request('/new_key', 'GET', dict(type=type))['result']

def add_to_database(infogami, q, loc):
# sample return
#    {'status': 'ok', 'result': {'updated': [], 'created': ['/b/OL13489313M']}}

    for a in (i for i in q.get('authors', []) if 'key' not in i):
        a['key'] = infogami.new_key('/type/author')

    q['key'] = infogami.new_key('/type/edition')
    ret = infogami.write(q, comment='initial import', machine_comment=loc)
    assert ret['status'] == 'ok'
    keys = [ i for i in ret['result']['created'] if i.startswith('/b/')]
    try:
        assert len(keys) == 1 or keys[0] == q['key']
    except AssertionError:
        print q
        print ret
        print keys
        raise
    return q['key']

########NEW FILE########
__FILENAME__ = pool
from urllib2 import urlopen, Request
import simplejson as json
import web
from openlibrary.catalog.merge.merge_index import add_to_indexes

# need to use multiple databases
# use psycopg2 until open library is upgraded to web 3.0

import psycopg2
from openlibrary.catalog.read_rc import read_rc
conn = psycopg2.connect(database='marc_index', host='ol-db')
cur = conn.cursor()

pool_url = 'http://0.0.0.0:9020/'

db_fields = ('isbn', 'title', 'oclc', 'lccn')

def build(index_fields):
    pool = {}
    for field in db_fields:
        if not field in index_fields:
            continue
        for v in index_fields[field]:
            if field == 'isbn' and len(v) < 10:
                continue
            cur.execute('select k from ' + field + ' where v=%(v)s', {'v': v})
            pool.setdefault(field, set()).update(i[0] for i in cur.fetchall())
    return dict((k, sorted(v)) for k, v in pool.iteritems())

def update(key, q):
    seen = set()
    for field, value in add_to_indexes(q):
        vars = {'key': key, 'value': value }
        if (field, value) in seen:
            print (key, field, value)
            print seen
            print q
        if (field, value) in seen:
            continue
        seen.add((field, value))
        cur.execute('insert into ' + field + ' (k, v) values (%(key)s, %(value)s)', vars)

def post_progress(archive_id, q):
    url = pool_url + "store/" + archive_id
    req = Request(url, json.dumps(q))
    urlopen(req).read()

def get_start(archive_id):
    url = pool_url + "store/" + archive_id
    data = urlopen(url).read()
    return json.loads(data)

########NEW FILE########
__FILENAME__ = scribe
#!/usr/bin/python
from subprocess import Popen, PIPE
from openlibrary.utils.ia import find_item, FindItemError
from openlibrary.api import OpenLibrary
from openlibrary.catalog.read_rc import read_rc
from openlibrary.catalog.get_ia import marc_formats, get_marc_ia_data
from openlibrary.catalog.marc import is_display_marc
from time import sleep, time
from pprint import pprint

import MySQLdb
import re, urllib2, httplib, json, codecs, socket, sys

ol = OpenLibrary('http://openlibrary.org/')

rc = read_rc()

base_url = 'http://openlibrary.org'

ia_db_host = 'dbmeta.us.archive.org'
ia_db_user = 'archive'
ia_db_pass = Popen(["/opt/.petabox/dbserver"], stdout=PIPE).communicate()[0]

re_census = re.compile('^\d+(st|nd|rd|th)census')

fields = ['identifier', 'contributor', 'updated', 'noindex', 'collection', 'format', 'boxid']
sql_fields = ', '.join(fields)

scanned_start = open('scanned_start').readline()[:-1]

ignore_noindex = set(['printdisabled', 'lendinglibrary', 'inlibrary'])

def login(h1):
    body = json.dumps({'username': 'ImportBot', 'password': rc['ImportBot']})
    headers = {'Content-Type': 'application/json'}  
    h1.request('POST', base_url + '/account/login', body, headers)
    print base_url + '/account/login'
    res = h1.getresponse()

    print res.read()
    print 'status:', res.status
    assert res.status == 200
    cookies = res.getheader('set-cookie').split(',')
    cookie =  ';'.join([c.split(';')[0] for c in cookies])
    return cookie

h1 = httplib.HTTPConnection('openlibrary.org')
h1.set_debuglevel(1)
cookie = login(h1)
h1.close()

bad = open('bad_import', 'a')
bad_lang = open('bad_lang', 'a')
logfile = open('log', 'a')
logfile.flush()

class BadImport (Exception):
    pass

class BadLang (Exception):
    pass

import_api_url = base_url + '/api/import'
def post_to_import_api(ia, marc_data, contenttype, subjects = [], boxid = None, scanned=True):
    print "POST to /api/import:", (ia, len(marc_data))

    cover_url = 'http://www.archive.org/download/' + ia + '/page/' + ia + '_preview.jpg'

    headers = {
        'Content-type': contenttype,
        'Cookie': cookie,
        'x-archive-meta-source-record': 'ia:' + ia,
    }
    if scanned:
        headers['x-archive-meta-cover'] = cover_url
        headers['x-archive-meta-ocaid'] = ia
    else:
        headers['x-archive-meta-ia-loaded-id'] = ia

    for num, s in enumerate(subjects):
        headers['x-archive-meta%02d-subject' % (num + 1)] = s

    if boxid:
        headers['x-archive-meta-ia-box-id'] = boxid

    print import_api_url
    h1 = httplib.HTTPConnection('openlibrary.org')
    #h1.set_debuglevel(1)
    h1.request('POST', import_api_url, marc_data, headers)
    try:
        res = h1.getresponse()
    except httplib.BadStatusLine:
        raise BadImport
    body = res.read()
    if res.status != 200:
        raise BadImport
    else:
        try:
            reply = json.loads(body)
        except ValueError:
            print 'not JSON:', `body`
            raise BadImport
    assert res.status == 200
    print >> logfile, reply
    print reply
    if not reply['success'] and reply['error'].startswith('invalid language code:'):
        raise BadLang
    assert reply['success']
    h1.close()

def check_marc_data(marc_data):
    if marc_data == '':
        return 'MARC binary empty string'
    if is_display_marc(marc_data):
        return 'display MARC'
    try:
        length = int(marc_data[0:5])
    except ValueError:
        return "MARC doesn't start with number"
    double_encode = False
    if len(marc_data) != length:
        try:
            marc_data = marc_data.decode('utf-8').encode('raw_unicode_escape')
            double_encode = True
        except:
            return "double UTF-8 decode error"
    if len(marc_data) != length:
        return 'MARC length mismatch: %d != %d' % (len(marc_data), length)
    if str(marc_data)[6:8] != 'am': # only want books
        return 'not a book!'
    if double_encode:
        return 'double encoded'
    return None

def load_book(ia, collections, boxid, scanned=True):
    if ia.startswith('annualreportspri'):
        print 'skipping:', ia
        return
    if 'shenzhentest' in collections:
        return

    if any('census' in c for c in collections):
        print 'skipping census'
        return

    if re_census.match(ia) or ia.startswith('populationschedu') or ia.startswith('michigancensus') or 'census00reel' in ia or ia.startswith('populationsc1880'):
        print 'ia:', ia
        print 'collections:', list(collections)
        print 'census not marked correctly'
        return
    try:
        host, path = find_item(ia)
    except socket.timeout:
        print 'socket timeout:', ia
        return
    except FindItemError:
        print 'find item error:', ia
    bad_binary = None
    try:
        formats = marc_formats(ia, host, path)
    except urllib2.HTTPError as error:
        return

    if formats['bin']: # binary MARC
        marc_data = get_marc_ia_data(ia, host, path)
        assert isinstance(marc_data, str)
        marc_error = check_marc_data(marc_data)
        if marc_error == 'double encode':
            marc_data = marc_data.decode('utf-8').encode('raw_unicode_escape')
            marc_error = None
        if marc_error:
            return
        contenttype = 'application/marc'
    elif formats['xml']: # MARC XML
        return # waiting for Raj to fox MARC XML loader
        marc_data = urllib2.urlopen('http://' + host + path + '/' + ia + '_meta.xml').read()
        contenttype = 'text/xml'
    else:
        return
    subjects = []
    if scanned:
        if 'lendinglibrary' in collections:
            subjects += ['Protected DAISY', 'Lending library']
        elif 'inlibrary' in collections:
            subjects += ['Protected DAISY', 'In library']
        elif 'printdisabled' in collections:
            subjects.append('Protected DAISY')

    if not boxid:
        boxid = None
    try:
        post_to_import_api(ia, marc_data, contenttype, subjects, boxid, scanned=scanned)
    except BadImport:
        print >> bad, ia
        bad.flush()
    except BadLang:
        print >> bad_lang, ia
        bad_lang.flush()

if __name__ == '__main__':
    skip = 'troubleshootingm00bige'
    skip = None
    while True:
        loaded_start = open('loaded_start').readline()[:-1]
        print loaded_start

        conn = MySQLdb.connect(host=ia_db_host, user=ia_db_user, \
                passwd=ia_db_pass, db='archive')

        cur = conn.cursor()
        cur.execute("select " + sql_fields + \
            " from metadata" + \
            " where identifier not like '%%test%%' and mediatype='texts'" + \
                " and (not curatestate='dark' or curatestate is null)" + \
                " and (collection like 'inlibrary%%' or collection like 'lendinglibrary%%' or (scanner is not null and scancenter is not null and scandate is null))" + \
                " and updated > %s" + \
                " order by updated", [loaded_start])
        t_start = time()

        for ia, contributor, updated, noindex, collection, ia_format, boxid in cur.fetchall():
            print updated, ia
            if contributor == 'Allen County Public Library Genealogy Center':
                print 'skipping Allen County Public Library Genealogy Center'
                continue
            collections = set()
            if collection:
                collections = set(i.lower().strip() for i in collection.split(';'))

            q = {'type': '/type/edition', 'ocaid': ia}
            if ol.query(q):
                continue
            q = {'type': '/type/edition', 'ia_loaded_id': ia}
            if ol.query(q):
                continue
            load_book(ia, collections, boxid, scanned=False)
            print >> open('loaded_start', 'w'), updated
        cur.close()

        scanned_start = open('scanned_start').readline()[:-1]
        print scanned_start
        cur = conn.cursor()
        cur.execute("select " + sql_fields + \
            " from metadata" + \
            " where mediatype='texts'" + \
                " and (not curatestate='dark' or curatestate is null)" + \
                " and format is not null " + \
                " and (collection like 'inlibrary%%' or collection like 'lendinglibrary%%' or scandate is not null)" + \
                " and updated > %s" + \
                " order by updated", [scanned_start])
        t_start = time()

        for ia, contributor, updated, noindex, collection, ia_format, boxid in cur.fetchall():
            print updated, ia
            if skip:
                if ia == skip:
                    skip = None
                continue
            if ia == 'treatiseonhistor00dixo':
                continue
            if ia == 'derheiligejohann00nean': # language is 'ge ' should be 'ger'
                continue
            if ia == 'lenseignementetl00kuhn': # language is ' fr' should be 'fre'
                continue
            if ia == 'recherchesetnote00gali': # language is 'efr' should be 'fre'
                continue
            if ia == 'placenamesinstra00macd': # language is 'd  '
                continue
            if ia == 'conaantnoanynjia00walk': # language is 'max':
                continue
            if 'pdf' not in ia_format.lower():
                continue # scancenter and billing staff often use format like "%pdf%" as a proxy for having derived

            collections = set()
            if noindex:
                if not collection:
                    continue
                collections = set(i.lower().strip() for i in collection.split(';'))
                if not ignore_noindex & collections:
                    continue
            if 'inlibrary' not in collections and contributor == 'Allen County Public Library Genealogy Center':
                print 'skipping Allen County Public Library Genealogy Center'
                continue

            load_book(ia, collections, boxid, scanned=True)
            print >> open('scanned_start', 'w'), updated

        cur.close()
        secs = time() - t_start
        mins = secs / 60
        print "finished %d took mins" % mins
        if mins < 30:
            print 'waiting'
            sleep(60 * 30 - secs)

########NEW FILE########
__FILENAME__ = status
import web, urllib2
import simplejson as json
from pprint import pformat
from time import time
from catalog.read_rc import read_rc

urls = (
    '/', 'index'
)

app = web.application(urls, globals())

base_url = "http://0.0.0.0:9020/"
rc = read_rc()

done = ['marc_western_washington_univ', 'marc_miami_univ_ohio', 'bcl_marc', 'marc_oregon_summit_records', 'CollingswoodLibraryMarcDump10-27-2008', 'hollis_marc', 'marc_laurentian', 'marc_ithaca_college', 'marc_cca']

def read_book_count():
    lines = list(open(rc['book_count']))
    t0, count0 = [int(i) for i in lines[0].split()]
    t, count = [int(i) for i in lines[-1].split()]
    t_delta = time() - t0
    count_delta = count - count0
    rec_per_sec = float(count_delta) / t_delta
    return rec_per_sec, count

files = eval(open('files').read())

def server_read(path):
    return json.load(urllib2.urlopen(base_url + path))

def progress(archive, part, pos):
    total = 0
    pass_cur = False
    for f, size in files[archive]:
        cur = archive + '/' + f
        if size is None:
            return (None, None)
        size = int(size)
        if cur == part or part == f:
            pass_cur = True
        if not pass_cur:
            pos += size
        total += size
    assert pass_cur
    return (pos, total)

class index:
    def GET(self): # yes, this is a bit of a mess
        web.header('Content-Type','text/html; charset=utf-8', unique=True)
        web.header('Refresh','60', unique=True)
        page = ''
        page += "<html>\n<head><title>Import status</title>"
        page += "<style>th { vertical-align: bottom; text-align: left }</style>"
        page += "</head><body>"
        page += "<h1>Import status</h1>"
#        page += '<p style="padding: 5px; background: lightblue; border: black 1px solid; font-size:125%; font-weight: bold">MARC import is paused during the OCA conference</p>'
        page += "<b>Done:</b>"
        page += ', '.join('<a href="http://archive.org/details/%s">%s</a>' % (ia, ia) for ia in done) + '<br>'
        page += "<table>"
        page += "<tr><th>Archive ID</th><th>input<br>(rec/sec)</th>"
        page += "<th>no match<br>(%)</th>"
        page += "<th>load<br>(rec/sec)</th>"
#        page += "<th>last update<br>(secs)</th><th>running<br>(hours)</th>"
        page += "<th>progress</th>"
        page += "<th>remaining<br>(hours)</th>"
        page += "<th>remaining<br>(records)</th>"
        page += "</tr>"
        cur_time = time()
        total_recs = 0
        total_t = 0
        total_load = 0
        total_rec_per_sec = 0
        total_load_per_sec = 0
        total_future_load = 0
        for k in server_read('keys'):
            if k.endswith('2'):
                continue
            if k in done:
                continue

            broken = False
            q = server_read('store/' + k)
            t1 = cur_time - q['time']
            rec_no = q['rec_no']
            chunk = q['chunk']
            load_count = q['load_count']
            rec_per_sec = rec_no / q['t1']
            load_per_sec = load_count / q['t1']
            if k in done:
                page += '<tr bgcolor="#00ff00">'
                broken = True
            elif t1 > 600:
                broken = True
                page += '<tr bgcolor="red">'
            elif t1 > 120:
                broken = True
                page += '<tr bgcolor="yellow">'
            else:
                page += '<tr bgcolor="#00ff00">'
                total_rec_per_sec += rec_per_sec
                total_load_per_sec += load_per_sec
                total_recs += rec_no
                total_load += load_count
                total_t += q['t1']
            page += '<td><a href="http://archive.org/details/%s">%s</a></td>' % (k.rstrip('2'), k)
#            page += '<td><a href="http://openlibrary.org/show-marc/%s">current record</a></td>' % q['cur']
#            if 'last_key' in q and q['last_key']:
#                last_key = q['last_key']
#                page += '<td><a href="http://openlibrary.org%s">%s</a></td>' % (last_key, last_key[3:])
#            else:
#                page += '<td>No key</td>'
            if k in done:
                for i in range(5):
                    page += '<td></td>'
                page += '<td align="right">100.0%</td>'
            else:
                page += '<td align="right">%.2f</td>' % rec_per_sec
                no_match = float(q['load_count']) / q['rec_no']
                page += '<td align="right">%.2f%%</td>' % (no_match * 100)
                page += '<td align="right">%.2f</td>' % load_per_sec
                hours = q['t1'] / 3600.0
                page += '<td align="right">%.2f</td>' % hours
                (pos, total) = progress(k, q['part'], q['pos'])
                if pos:
                    page += '<td align="right">%.2f%%</td>' % (float(pos * 100) / total)
                else:
                    page += '<td align="right">n/a</td>'
                if 'bytes_per_sec_total' in q and total is not None and pos:
                    remaining_bytes = total - pos
                    sec = remaining_bytes / q['bytes_per_sec_total']
                    hours = sec / 3600
                    days = hours / 24
                    page += '<td align="right">%.2f</td>' % hours
                    total_bytes = q['bytes_per_sec_total'] * q['t1']
                    avg_bytes = total_bytes / q['rec_no']
                    future_load = ((remaining_bytes / avg_bytes) * no_match) 
                    total_future_load += future_load
                    page += '<td align="right">%s</td>' % web.commify(int(future_load))
                else:
                    page += '<td></td>'

            page += '</tr>'
        page += '<tr><td>Total</td><td align="right">%.2f</td>' % total_rec_per_sec
        if total_recs:
            page += '<td align="right">%.2f%%</td>' % (float(total_load * 100.0) / total_recs)
        else:
            page += '<td align="right"></td>' 
        page += '<td align="right">%.2f</td>' % total_load_per_sec
        page += '<td></td>' * 3 + '<td align="right">%s</td>' % web.commify(int(total_future_load))
        page += '</tr></table>'
#        page += "<table>"
#        page += '<tr><td align="right">loading:</td><td align="right">%.1f</td><td>rec/hour</td></tr>' % (total_load_per_sec * (60 * 60))
#        page += '<tr><td align="right">loading:</td><td align="right">%.1f</td><td>rec/day</td></tr>' % (total_load_per_sec * (60 * 60 * 24))
#        if total_load_per_sec:
#            page += '<tr><td>one million records takes:</td><td align="right">%.1f</td><td>hours</td></tr>' % ((1000000 / total_load_per_sec) / (60 * 60))
#            page += '<tr><td>one million records takes:</td><td align="right">%.1f</td><td>days</td></tr>' % ((1000000 / total_load_per_sec) / (60 * 60 * 24))
#        page += "</table>"
        rec_per_sec, count = read_book_count()
        page += "Total records per second: %.2f<br>" % rec_per_sec
        day = web.commify(int(rec_per_sec * (60 * 60 * 24)))
        page += "Total records per day: %s<br>" % day

        page += "Books in Open Library: " + web.commify(count) + "<br>"
        page += '</body>\n<html>'
        return page

if __name__ == '__main__':
    app.run()

########NEW FILE########
__FILENAME__ = update
import re, web, sys
import simplejson as json
from urllib2 import urlopen, URLError
from openlibrary.catalog.read_rc import read_rc
from openlibrary.catalog.importer.db_read import get_mc
from time import sleep
from openlibrary.catalog.title_page_img.load import add_cover_image
from openlibrary.api import OpenLibrary, unmarshal, marshal
from pprint import pprint

rc = read_rc()
ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

re_meta_mrc = re.compile('^([^/]*)_meta.mrc:0:\d+$')

def make_redirect(old, new, msg='replace with redirect'):
    r = {'type': {'key': '/type/redirect'}, 'location': new}
    ol.save(old, r, msg)

def fix_toc(e):
    toc = e.get('table_of_contents', None)
    if not toc:
        return
    print e['key']
    pprint(toc)
    # http://openlibrary.org/books/OL789133M - /type/toc_item missing from table_of_contents
    if isinstance(toc[0], dict) and ('pagenum' in toc[0] or toc[0]['type'] == '/type/toc_item'):
        return
    return [{'title': unicode(i), 'type': '/type/toc_item'} for i in toc if i != u'']

re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon)\.$')

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

def undelete_author(a):
    key = a['key']
    assert a['type'] == '/type/delete'
    url = 'http://openlibrary.org' + key + '.json?v=' + str(a['revision'] - 1)
    prev = unmarshal(json.load(urlopen(url)))
    assert prev['type'] == '/type/author'
    ol.save(key, prev, 'undelete author')

def undelete_authors(authors):
    for a in authors:
        if a['type'] == '/type/delete':
            undelete_author(a)
        else:
            print a
            assert a['type'] == '/type/author'

re_edition_key = re.compile('^/b(?:ooks)?/(OL\d+M)')

def add_source_records(key, ia, v=None):
    new = 'ia:' + ia
    sr = None
    m = re_edition_key.match(key)
    old_style_key = '/b/' + m.group(1)
    key = '/books/' + m.group(1)
    e = ol.get(key, v=v)
    need_update = False
    if 'ocaid' not in e:
        need_update = True
        e['ocaid'] = ia
    if 'source_records' in e:
        if new in e['source_records'] and not need_update:
            return
        e['source_records'].append(new)
    else:
        existing = get_mc(old_style_key)
        print 'get_mc(%s) == %s' % (old_style_key, existing)
        if existing is None:
            sr = []
        elif existing.startswith('ia:') or existing.startswith('amazon:'):
            sr = [existing]
        else:
            m = re_meta_mrc.match(existing)
            sr = ['marc:' + existing if not m else 'ia:' + m.group(1)]
        print 'ocaid:', e['ocaid']
        if 'ocaid' in e and 'ia:' + e['ocaid'] not in sr:
            sr.append('ia:' + e['ocaid'])
        print 'sr:', sr
        print 'ocaid:', e['ocaid']
        if new not in sr:
            e['source_records'] = sr + [new]
        else:
            e['source_records'] = sr
        assert 'source_records' in e

    # fix other bits of the record as well
    new_toc = fix_toc(e)
    if new_toc:
        e['table_of_contents'] = new_toc
    if e.get('subjects', None) and any(has_dot(s) for s in e['subjects']):
        subjects = [s[:-1] if has_dot(s) else s for s in e['subjects']]
        e['subjects'] = subjects
    if 'authors' in e:
        assert not any(a=='None' for a in e['authors'])
        print e['authors']
        authors = [ol.get(akey) for akey in e['authors']]
        authors = [ol.get(a['location']) if a['type'] == '/type/redirect' else a \
                for a in authors]
        for a in authors:
            if a['type'] == '/type/redirect':
                print 'double redirect on:', e['key']
        e['authors'] = [{'key': a['key']} for a in authors]
        undelete_authors(authors)
    print 'saving', key
    assert 'source_records' in e
    print ol.save(key, e, 'found a matching MARC record')
    add_cover_image(key, ia)

def ocaid_and_source_records(key, ocaid, source_records):
    e = ol.get(key)
    need_update = False
    e['ocaid'] = ocaid
    e['source_records'] = source_records

    # fix other bits of the record as well
    new_toc = fix_toc(e)
    if new_toc:
        e['table_of_contents'] = new_toc
    if e.get('subjects', None) and any(has_dot(s) for s in e['subjects']):
        subjects = [s[:-1] if has_dot(s) else s for s in e['subjects']]
        e['subjects'] = subjects
    if 'authors' in e:
        assert not any(a=='None' for a in e['authors'])
        print e['authors']
        authors = [ol.get(akey) for akey in e['authors']]
        authors = [ol.get(a['location']) if a['type'] == '/type/redirect' else a \
                for a in authors]
        e['authors'] = [{'key': a['key']} for a in authors]
        undelete_authors(authors)
    try:
        print ol.save(key, e, 'merge scanned books')
    except:
        print e
        raise
    if new_toc:
        new_edition = ol.get(key)
        # [{u'type': <ref: u'/type/toc_item'>}, ...]
        assert 'title' in new_edition['table_of_contents'][0]

########NEW FILE########
__FILENAME__ = improve_data
from catalog.infostore import get_site
import sys, codecs

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

site = get_site()

def get_authors_by_name(name):
    return site.things({'name': name, 'type': '/type/author'})

def get_books_by_author(key):
    return site.things({'authors': key, 'type': '/type/edition'})

for author_key in get_authors_by_name(sys.argv[1]):
    print author_keys
    book_keys = get_books_by_author(author_key)
    for key in book_keys:
        t = site.get(key)
        print key, t.title
        print '  ', t.isbn_10

########NEW FILE########
__FILENAME__ = web_ui
from catalog.infostore import get_site
import web
from catalog.marc.db.web_marc_db import search_query, show_locs

site = get_site()

def get_authors_by_name(name):
    return site.things({'name': name, 'type': '/type/author'})

def get_books_by_author(key):
    return site.things({'authors': key, 'type': '/type/edition'})

def author_search(author_key):
    book_keys = get_books_by_author(author_key)
    for key in book_keys:
        t = site.get(key)
        print key, t.title, '<br>'
        print '&nbsp;&nbsp;', t.isbn_10, '<br>'
        locs = []
        for i in t.isbn_10 if t.isbn_10 else []:
            for l in search_query('isbn', i):
                if l not in locs:
                    locs.append(l)
        for i in t.lccn if t.lccn else []:
            for l in search_query('lccn', i):
                if l not in locs:
                    locs.append(l)
        show_locs(locs, None)

urls = (
    '/', 'index'
)

class index():
    def GET(self):
        web.header('Content-Type','text/html; charset=utf-8', unique=True)
        input = web.input()
        author_key = input.get('author', None)
        print "<html>\n<head>\n<title>Author fixer</title>"
        print '''
<style>
th { text-align: left }
td { padding: 5px; background: #eee }
</style>'''

        print '</head><body><a name="top">'
        print '<form name="main" method="get">'
        if author_key:
            print '<input type="text" name="author" value="%s">' % web.htmlquote(author_key)
        else:
            print '<input type="text" name="author">'
        print '<input type="submit" value="find">'
        print '</form>'
        if author_key:
            author_search(author_key)
        print "</body></html>"


if __name__ == "__main__":
    web.run(urls, globals(), web.reloader)

########NEW FILE########
__FILENAME__ = infostore
import web
web.config.db_printing = False
from infogami.infobase import cache
import infogami
from read_rc import read_rc

def get_infobase(rc):
    from infogami.infobase import infobase, dbstore
    import web
    web.config.db_parameters = infogami.config.db_parameters
    web.config.db_printing = False

    schema = dbstore.Schema()
    schema.add_table_group('type', '/type/type')
    schema.add_table_group('type', '/type/property')
    schema.add_table_group('type', '/type/backreference')
    schema.add_table_group('user', '/type/user')
    schema.add_table_group('user', '/type/usergroup')
    schema.add_table_group('user', '/type/permission')

    schema.add_table_group('edition', '/type/edition')
    schema.add_table_group('author', '/type/author')
    schema.add_table_group('scan', '/type/scan_location')
    schema.add_table_group('scan', '/type/scan_record')

    schema.add_seq('/type/edition', '/b/OL%dM')
    schema.add_seq('/type/author', '/a/OL%dA')

    store = dbstore.DBStore(schema)
    secret_key = rc['secret_key']
    return infobase.Infobase(store, secret_key)

def get_site(staging=False):
    if 'site' in web.ctx:
        return web.ctx.site
    rc = read_rc()

    param = dict((k, rc['staging_' + k if staging else k]) for k in ('db', 'user', 'pw', 'host'))
    print param
    param['dbn'] = 'postgres'

    infogami.config.db_parameters = param
    infogami.config.db_printing = False

    ib = get_infobase(rc)
    web.ctx.site = ib.get('openlibrary.org')
    cache.loadhook()
    return web.ctx.site

########NEW FILE########
__FILENAME__ = iter_scan_records
from catalog.read_rc import read_rc
from catalog.infostore import get_site
from catalog.get_ia import get_from_archive
from catalog.marc.fast_parse import get_all_tag_lines
import os

site = get_site()

marc_path = '/2/pharos/marc/'

def get_data(loc):
    try:
        filename, p, l = loc.split(':')
    except ValueError:
        return None
    if not os.path.exists(marc_path + filename):
        return None
    f = open(marc_path + filename)
    f.seek(int(p))
    buf = f.read(int(l))
    f.close()
    return buf


def edition_marc(key):
    mc = list(set(v.machine_comment for v in site.versions({'key': key })))
    return [loc for loc in mc if loc]
    
key_start = len('/scan_record')
for key in site.things({'type': '/type/scan_record'}):
    assert key.startswith('/scan_record/b/')
    edition_key = key[key_start:]
    for loc in edition_marc(edition_key):
        data = get_data(loc)
        if not data or data.find('icrof') == -1:
            continue
        print "http://openlibrary.org" + edition_key
        print "http://openlibrary.org/show-marc/" + loc
        for tag, tag_line in get_all_tag_lines(data):
            if tag_line.find('icrof') == -1:
                continue
            print tag + ":", tag_line[2:-1].replace('\x1f', ' $')
        print

########NEW FILE########
__FILENAME__ = lang
import sys
from StringIO import StringIO
import time, re

def cftime():
	t,m = divmod(time.time(), 1.0)
	return re.sub(r':\d\d ', r'\1.%02d ',
		      (time.ctime(t), int(100*m)))
	
def warn (msg):
	sys.stderr.write ("%s\n" % msg)

def die (msg):
	raise Exception (msg)

def lines_positions (input):
	done = False
	while not done:
		line = StringIO ()
		pos = input.tell ()
		while True:
			ch = input.read (1)
			if len (ch) == 0:
				done = True
				break
			elif ch == '\n':
				break
			else:
				line.write (ch)
		yield (line.getvalue (), pos)

class Box:
	def __init__ (self):
		self.empty = True
	def set (self, val):
		self.value = val
		self.empty = False
	def get (self):
		if self.empty:
			raise Exception ("get: box is empty")
		else:
			return self.value

def memoized (f):
        box = Box ()
        def get ():     
		if box.empty:
			box.set (f ())
                return box.get ()
        return get

########NEW FILE########
__FILENAME__ = update
from urllib2 import urlopen
from lxml.html import parse
from openlibrary.catalog.read_rc import read_rc
import os, sys, httplib, subprocess
from time import sleep

# httplib.HTTPConnection.debuglevel = 1 

# http://home.us.archive.org/~samuel/abouts3.txt

rc = read_rc()
accesskey = rc['s3_accesskey']
secret = rc['s3_secret']

def put_file(con, ia, filename):
    print 'uploading %s' % filename
    headers = {
        'authorization': "LOW " + accesskey + ':' + secret,
        'x-archive-queue-derive': 0,
    }
    url = 'http://s3.us.archive.org/' + ia + '/' + filename
    print url
    data = open(d + '/' + filename).read()
    for attempt in range(5):
        con.request('PUT', url, data, headers)
        try:
            res = con.getresponse()
        except httplib.BadStatusLine as bad:
            print 'bad status line:', bad.line
            raise
        body = res.read()
        if '<Error>' not in body:
            return
        print 'error'
        print body
        if no_bucket_error not in body and internal_error not in body:
            sys.exit(0)
        print 'retry'
        time.sleep(5)
    print 'too many failed attempts'

subprocess.call(["/usr/bin/perl", "get.pl"])

d = '/1/edward/lc_updates'
item_id = 'marc_loc_updates'
url = 'http://www.archive.org/download/' + item_id
existing = frozenset(l[2] for l in parse(url).getroot().iterlinks())

to_upload = set(os.listdir(d)) - existing

for f in to_upload:
    con = httplib.HTTPConnection('s3.us.archive.org')
    con.connect()
    put_file(con, item_id, f)
    con.close()
    sleep(10)

########NEW FILE########
__FILENAME__ = load
type_map = {
    'description': 'text',
    'notes': 'text',
    'number_of_pages': 'int',
    'url': 'uri',
}

def get_author_num(web):
    # find largest author key
    rows = web.query("select key from thing where site_id=1 and key LIKE '/a/OL%%A' order by id desc limit 10")
    return max(int(web.numify(i.key)) for i in rows)

def get_edition_num(web):
    # find largest edition key
    rows = web.query("select key from thing where site_id=1 and key LIKE '/b/OL%%M' order by id desc limit 10")
    return max(int(web.numify(i.key)) for i in rows)

def add_keys(web, edition):
    # add author and edition keys to a new edition
    if 'authors' in edition:
        for a in edition['authors']:
            a.setdefault('key', '/a/OL%dA' % (get_author_num(web) + 1))
    edition['key'] = '/b/OL%dM' % (get_edition_num(web) + 1)

def build_query(loc, rec):
    if 'title' not in rec:
        print 'missing title:', loc
        return
    if 'edition_name' in rec:
        assert rec['edition_name']
    assert 'source_record_loc' not in rec

    book = {
        'create': 'unless_exists',
        'type': { 'key': '/type/edition'},
    }

    east = east_in_by_statement(rec)
    if east:
        print rec

    for k, v in rec.iteritems():
        if k == 'authors':
            book[k] = [import_author(v[0], eastern=east)]
            continue
        if k in type_map:
            t = '/type/' + type_map[k]
            if isinstance(v, list):
                book[k] = [{'type': t, 'value': i} for i in v]
            else:
                book[k] = {'type': t, 'value': v}
        else:
            book[k] = v

    if 'title' not in book:
        pprint(rec)
        pprint(book)
    assert 'title' in book
    if 'publish_country' in book:
        assert book['publish_country'] not in ('   ', '|||')
    if 'publish_date' in book:
        assert book['publish_date'] != '||||'
    if 'languages' in book:
        lang_key = book['languages'][0]['key']
        if lang_key in ('/l/   ', '/l/|||'):
            del book['languages']
        elif not site.things({'key': lang_key, 'type': '/type/language'}):
            print lang_key, "not found for", loc
            del book['languages']
    return book


########NEW FILE########
__FILENAME__ = all
from openlibrary.catalog.get_ia import read_marc_file
from openlibrary.catalog.read_rc import read_rc
import web, os.path

# iterate through every MARC record on disk

rc = read_rc()

def files(ia):
    endings = ['.mrc', '.marc', '.out', '.dat', '.records.utf8']
    def good(filename):
        return any(filename.endswith(e) for e in endings)

    dir = rc['marc_path'] + '/' + ia
    dir_len = len(dir) + 1
    files = []
    for dirpath, dirnames, filenames in os.walk(dir):
        files.extend(dirpath + "/" + f for f in sorted(filenames))
    return [(i[dir_len:], os.path.getsize(i)) for i in files if good(i)]

def iter_marc(sources):
    rec_no = 0
    for ia in sources:
        for part, size in files(ia):
            full_part = ia + "/" + part
            filename = rc['marc_path'] + full_part
            assert os.path.exists(filename)
            f = open(filename)
            for pos, loc, data in read_marc_file(full_part, f):
                rec_no +=1
                yield rec_no, pos, loc, data

########NEW FILE########
__FILENAME__ = build_db
from sources import sources
from catalog.marc.fast_parse import index_fields, read_file
from catalog.get_ia import files
from catalog.read_rc import read_rc
from time import time
import os, web

rc = read_rc()

web.config.db_parameters = dict(dbn='postgres', db='marc_records', user=rc['user'], pw=rc['pw'], host=rc['host'])
web.config.db_printing = False
web.load()

def progress_update(rec_no, t):
    remaining = total - rec_no
    rec_per_sec = chunk / t
    mins = (float((t/chunk) * remaining) / 60)
    print "%d %.3f rec/sec" % (rec_no, rec_per_sec),
    if mins > 1440:
        print "%.3f days left" % (mins / 1440)
    elif mins > 60:
        print "%.3f hours left" % (mins / 60)
    else:
        print "%.3f minutes left" % mins

fields = ['isbn', 'oclc', 'lccn', 'call_number', 'title']

def process_record(file_id, pos, length, data):
    rec = index_fields(data, ['001', '010', '020', '035', '245'], check_author = False)
    if not rec:
        return
    extra = dict((f, rec[f][0]) for f in ('title', 'lccn', 'call_number') if f in rec)
    rec_id = web.insert('rec', marc_file = file_id, pos=pos, len=length, **extra)
    for f in (f for f in ('isbn', 'oclc') if f in rec):
        for v in rec[f]:
            web.insert(f, seqname=False, rec=rec_id, value=v)

t_prev = time()
rec_no = 0
chunk = 1000
total = 32856039

for ia, name in sources():
    print ia, name
    for part, size in files(ia):
        file_id = web.insert('files', ia=ia, part=part)
        print part, size
        full_part = ia + "/" + part
        filename = rc['marc_path'] + full_part
        if not os.path.exists(filename):
            continue
        pos = 0
        for data, length in read_file(open(filename)):
            pos += length
            rec_no +=1
            if rec_no % chunk == 0:
                t = time() - t_prev
                progress_update(rec_no, t)
                t_prev = time()
            process_record(file_id, pos, length, data)

print rec_no

########NEW FILE########
__FILENAME__ = build_record
from fast_parse import *
from warnings import warn
from openlibrary.catalog.utils import pick_first_date
import re

re_question = re.compile('^\?+$')
re_lccn = re.compile('(...\d+).*')
re_letters = re.compile('[A-Za-z]')
re_isbn = re.compile('([^ ()]+[\dX])(?: \((?:v\. (\d+)(?: : )?)?(.*)\))?')
re_oclc = re.compile ('^\(OCoLC\).*?0*(\d+)')
re_int = re.compile ('\d{2,}')
re_number_dot = re.compile('\d{3,}\.$')

re_translation1 = re.compile(r'^(.{,6})\btranslation of\b', re.I)
re_translation2 = re.compile(r'^([\'"]?).*?\btranslation of\b[ :,;]*(.*)\1', re.I)

# no monograph should be longer than 50,000 pages
max_number_of_pages = 50000

want = [
    '001',
    '008', # publish date, country and language
    '010', # lccn
    '020', # isbn
    '035', # oclc
    '050', # lc classification
    '082', # dewey
    '100', '110', '111', # authors TODO
    '130', '240', # work title
    '245', # title
    '250', # edition
    '260', # publisher
    '300', # pagination
    '440', '490', '830' # series
    ] + map(str, range(500,590)) + [ # notes + toc + description
    '600', '610', '630', '650', '651', # subjects + genre
    '700', '710', '711', # contributions
    '246', '730', '740', # other titles
    '852', # location
    '856' # URL
]

re_series = re.compile('^(.*) series$', re.I)

def read_lccn(fields):
    if '010' not in fields:
        return {}

    found = []
    for line in fields['010']:
        for k, v in get_subfields(line, ['a']):
            lccn = v.strip()
            if re_question.match(lccn):
                continue
            m = re_lccn.search(lccn)
            if not m:
                continue
            lccn = re_letters.sub('', m.group(1)).strip()
            if lccn:
                found.append(lccn)

    return {'lccn': found}

def read_isbn(fields):
    if '020' not in fields:
        return {}

    found = []
    for line in fields['020']:
        if line.find('\x1f') != -1:
            for k, v in get_subfields(line, ['a', 'z']):
                m = re_isbn.match(v)
                if m:
                    found.append(m.group(1))
        else:
            m = re_isbn.match(line[3:-1])
            if m:
                found.append(m.group(1))
    ret = {}
    for i in found:
        i = i.replace('-', '')
        if len(i) == 13:
            ret.setdefault('isbn_13', []).append(i)
        else:
            ret.setdefault('isbn_10', []).append(i)

    return ret

def read_oclc(fields):
    if '035' not in fields:
        return {}

    found = []
    for line in fields['035']:
        for v in get_subfield_values(line, ['a']):
            m = re_oclc.match(v)
            if not m:
                continue
            oclc = m.group(1)
            if oclc not in found:
                found.append(oclc)
    return {'oclc_number': found } if found else {}

def read_author_person(line):
    author = {}
    contents = get_contents(line, ['a', 'b', 'c', 'd'])
    if 'a' not in contents and 'c' not in contents:
        return None # should at least be a name or title
    name = [v.strip(' /,;:') for v in get_subfield_values(line, ['a', 'b', 'c'])]
    if 'd' in contents:
        author = pick_first_date(contents['d'])
        if 'death_date' in author and author['death_date']:
            death_date = author['death_date']
            if re_number_dot.search(death_date):
                author['death_date'] = death_date[:-1]

    author['name'] = ' '.join(name)
    author['entity_type'] = 'person'
    subfields = [
        ('a', 'personal_name'),
        ('b', 'numeration'),
        ('c', 'title')
    ]
    for subfield, field_name in subfields:
        if subfield in contents:
            author[field_name] = ' '.join([x.strip(' /,;:') for x in contents[subfield]])
    if 'q' in contents:
        author['fuller_name'] = ' '.join(contents['q'])
    return author

def read_authors(fields):
    found = []
    author = [tag for tag in fields if tag in ('100', '110', '111')]
    if len(author) == 0:
        return {}
    if len(author) != 1:
        for tag in ('100', '110', '111'):
            print tag, fields[tag]
    assert len(author) == 1
    if '100' in fields:
        line = fields['100'][0]
        author = read_author_person(line)
    if '110' in fields:
        line = fields['110'][0]
        name = [v.strip(' /,;:') for v in get_subfield_values(line, ['a', 'b'])]
        author = { 'entity_type': 'org', 'name': ' '.join(name) }
    if '111' in fields:
        line = fields['111'][0]
        name = [v.strip(' /,;:') for v in get_subfield_values(line, ['a', 'c', 'd', 'n'])]
        author = { 'entity_type': 'event', 'name': ' '.join(name) }

    return {'authors': [author]} if author else {}

def read_title(fields):
    if '245' not in fields:
        return {}

#   example MARC record with multiple titles:
#   http://openlibrary.org/show-marc/marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:299505697:862
#   assert len(fields['245']) == 1
    line = fields['245'][0]
    contents = get_contents(line, ['a', 'b', 'c', 'h'])

    edition = {}
    title = None
#   MARC record with 245a missing:
#   http://openlibrary.org/show-marc/marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:516779055:1304
    if 'a' in contents:
        title = ' '.join(x.strip(' /,;:') for x in contents['a'])
    elif 'b' in contents: # handle broken records
        title = contents['b'][0].strip(' /,;:')
        del contents['b'][0]
    edition['title'] = title
    if 'b' in contents and contents['b']:
        edition["subtitle"] = ' : '.join([x.strip(' /,;:') for x in contents['b']])
    if 'c' in contents:
        edition["by_statement"] = ' '.join(contents['c'])
    if 'h' in contents:
        edition["physical_format"] = ' '.join(contents['h'])
    return edition

def read_lc_classification(fields):
    if '050' not in fields:
        return {}

    found = []
    for line in fields['050']:
        contents = get_contents(line, ['a', 'b'])
        if 'b' in contents:
            b = ' '.join(contents['b'])
            if 'a' in contents:
                found += [' '.join([a, b]) for a in contents['a']]
            else:
                found += [b]
        # http://openlibrary.org/show-marc/marc_university_of_toronto/uoft.marc:671135731:596
        elif 'a' in contents:
            found += contents['a']
    if found:
        return {'lc_classifications': [i.strip() for i in found]}
    else:
        return {}

def read_dewey(fields):
    if '082' not in fields:
        return {}
    found = []
    for line in fields['082']:
        found += get_subfield_values(line, ['a'])
    return {'dewey_decimal_class': found }

def join_subfield_values(line, subfields):
    return ' '.join(get_subfield_values(line, subfields))

def read_work_titles(fields):
    found = []
    if '240' in fields:
        for line in fields['240']:
            title = join_subfield_values(line, ['a', 'm', 'n', 'p', 'r'])
            if title not in found:
                found.append(title)

    if '130' in fields:
        for line in fields['130']:
            title = ' '.join(get_lower_subfields(line))
            if title not in found:
                found.append(title)

    return { 'work_titles': found } if found else {}

def read_edition_name(fields):
    if '250' not in fields:
        return {}
    found = []
    for line in fields['250']:
        found += [v for k, v in get_all_subfields(line)]
    return {'edition_name': ' '.join(found)}

def read_publisher(fields):
    if '260' not in fields:
        return {}
    publisher = []
    publish_place = []
    for line in fields['260']:
        contents = get_contents(line, ['a', 'b'])
        if 'b' in contents:
            publisher += [x.strip(" /,;:") for x in contents['b']]
        if 'a' in contents:
            publish_place += [x.strip(" /.,;:") for x in contents['a']]
    edition = {}
    if publisher:
        edition["publishers"] = publisher
    if publish_place:
        edition["publish_places"] = publish_place
    return edition

def read_pagination(fields):
    if '300' not in fields:
        return {}

    pagination = []
    edition = {}
    for line in fields['300']:
        pagination += get_subfield_values(line, ['a'])
    if pagination:
        edition["pagination"] = ' '.join(pagination)
        num = [] # http://openlibrary.org/show-marc/marc_university_of_toronto/uoft.marc:2617696:825
        for x in pagination:
            num += [ int(i) for i in re_int.findall(x.replace(',',''))]
            num += [ int(i) for i in re_int.findall(x) ]
        valid = [i for i in num if i < max_number_of_pages]
        if valid:
            edition["number_of_pages"] = max(valid)
    return edition

def read_series(fields):
    found = []
    for tag in ('440', '490', '830'):
        if tag not in fields:
            continue
        for line in fields[tag]:
            this = []
            for k, v in get_subfields(line, ['a', 'v']):
                if k == 'v' and v:
                    this.append(v)
                    continue
                v = v.rstrip('.,; ')
                m = re_series.match(v)
                if m:
                    v = m.group(1)
                if v:
                    this.append(v)
            if this:
                s = ' -- '.join(this)
                if s not in found:
                    found.append(s)
    return {'series': found} if found else {}
                
def read_contributions(fields):
    want = [
        ('700', 'abcde'),
        ('710', 'ab'),
        ('711', 'acdn'),
    ]

    found = []
    for tag, subfields in want:
        if tag not in fields:
            continue
        for line in fields[tag]:
            found.append(join_subfield_values(line, subfields))
    return { 'contributions': found } if found else {}

def remove_duplicates(seq):
    u = []
    for x in seq:
        if x not in u:
            u.append(x)
    return u

def read_subjects(fields):
    want = [
        ('600', 'abcd'),
        ('610', 'ab'),
        ('630', 'acdegnpqst'),
        ('650', 'a'),
        ('651', 'a'),
    ]

    found = []
    subdivision = ['v', 'x', 'y', 'z']

    for tag, subfields in want:
        if tag not in fields:
            continue
        for line in fields[tag]:
            a = get_subfield_values(line, subdivision)
            b = " -- ".join(get_subfield_values(line, subfields) + a)
            found.append(b)
    
    return {'subjects': found} if found else {}
    
def read_genres(fields):
    found = []
    for tag in '600', '650', '651':
        if tag not in fields:
            continue
        for line in fields[tag]:
            found += get_subfield_values(line, ['v'])
    return { 'genres': remove_duplicates(found) } if found else {}

def read_translation(fields):
    tag = '500'
    if tag not in fields:
        return {}
    for line in fields[tag]:
        for value in get_subfield_values(line, ['a']):
            value = value.strip()
            if not re_translation1.match(value):
                continue
            if value.startswith("Translation of the author's thesis"):
                continue # not interested
            m = re_translation2.match(value)
            return { 'translation_of': m.group(2) }
    return {}

def read_notes(fields):
    found = []
    for tag in range(500,590):
        if tag in (505, 520) or str(tag) not in fields:
            continue
        tag = str(tag)
        for line in fields[tag]:
            try:
                x = get_lower_subfields(line)
            except IndexError:
                print `line`
                raise
            if x:
                found.append(' '.join(x))
    return {'notes': '\n\n'.join(found)} if found else {}

def read_toc(fields):
    if '505' not in fields:
        return {}

    toc = []
    for line in fields['505']:
        toc_line = []
        for k, v in get_all_subfields(line):
            if k == 'a':
                toc_split = [i.strip() for i in v.split('--')]
                if any(len(i) > 2048 for i in toc_split):
                    toc_split = [i.strip() for i in v.split(' - ')]
                # http://openlibrary.org/show-marc/marc_miami_univ_ohio/allbibs0036.out:3918815:7321
                if any(len(i) > 2048 for i in toc_split):
                    toc_split = [i.strip() for i in v.split('; ')]
                # FIXME:
                # http://openlibrary.org/show-marc/marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:938969487:3862
                if any(len(i) > 2048 for i in toc_split):
                    toc_split = [i.strip() for i in v.split(' / ')]
                assert isinstance(toc_split, list)
                toc.extend(toc_split)
                continue
            if k == 't':
                if toc_line:
                    toc.append(' -- '.join(toc_line))
                if (len(v) > 2048):
                    toc_line = [i.strip() for i in v.strip('/').split('--')]
                continue
            toc_line.append(v.strip(' -'))
        if toc_line:
            toc.append('-- '.join(toc_line))
    if not toc:
        return {}
    found = []
    for i in toc:
        if len(i) > 2048:
            i = i.split('  ')
            found.extend(i)
        else:
            found.append(i)
    return { 'table_of_contents': found }

def read_description(fields):
    if '520' not in fields:
        return {}
    found = []
    wrap = False
    for line in fields['520']:
        this = get_subfield_values(line, ['a'])
        if len(this) != 1:
#            print `fields['520']`
#            print `line`
            print len(this)
        assert len(this) == 1
        found += this
        if line[-3:-1] == '++':
            wrap = True
        else:
            wrap = False
    return {'description': "\n\n".join(found) } if found else {}

def read_other_titles(fields):
    found = []
    
    if '246' in fields:
        for line in fields['246']:
            title = join_subfield_values(line, ['a'])
            if title not in found:
                found.append(title)

    if '730' in fields:
        for line in fields['730']:
            title = ' '.join(get_lower_subfields(line))
            if title not in found:
                found.append(title)

    if '740' in fields:
        for line in fields['740']:
            title = join_subfield_values(line, ['a', 'p', 'n'])
            if title not in found:
                found.append(title)

    return {"other_titles": found} if found else {}

def read_location(fields):
    if '852' not in fields:
        return {}
    found = []
    for line in fields['852']:
        found += [v for v in get_subfield_values(line, ['a']) if v]
    return { 'location': found } if found else {}

def read_url(fields):
    if '856' not in fields:
        return {}
    found = []
    for line in fields['856']:
        found += get_subfield_values(line, ['u'])
    return { 'uri': found } if found else {}

def build_record(data):
    fields = {}
    for tag, line in handle_wrapped_lines(get_tag_lines(data, want)):
        fields.setdefault(tag, []).append(line)

    edition = {}
    if len(fields['008']) != 1:
        warn("There should be a single '008' field.")
        return {}
    f = fields['008'][0]
    publish_date = str(f)[7:11]
    if publish_date.isdigit():
        edition["publish_date"] = publish_date
    publish_country = str(f)[15:18]
    if publish_country not in ('|||', '   '):
        edition["publish_country"] = publish_country
    lang = str(f)[35:38]
    if lang not in ('   ', '|||'):
        edition["languages"] = [{ 'key': '/l/' + lang }]
    edition.update(read_lccn(fields))
    edition.update(read_isbn(fields))
    edition.update(read_oclc(fields))
    edition.update(read_lc_classification(fields))
    edition.update(read_dewey(fields))
    edition.update(read_authors(fields))
    edition.update(read_title(fields))
    edition.update(read_genres(fields))
    edition.update(read_subjects(fields))
    edition.update(read_pagination(fields))
    edition.update(read_series(fields))
    edition.update(read_work_titles(fields))
    edition.update(read_other_titles(fields))
    edition.update(read_edition_name(fields))
    edition.update(read_publisher(fields))
    edition.update(read_contributions(fields))
    edition.update(read_location(fields))
    edition.update(read_url(fields))
    edition.update(read_toc(fields))
    edition.update(read_notes(fields))
    edition.update(read_translation(fields))
    edition.update(read_description(fields))
    return edition

def test_candide_by_voltaire():
    bpl = open('test_data/bpl_0486266893').read()
    lc = open('test_data/lc_1416500308').read()

    fields = {}
    want = [ '41', '490', '830' ]
    for tag, line in handle_wrapped_lines(get_tag_lines(lc, want)):
        fields.setdefault(tag, []).append(line)
    assert read_series(fields) == {'series': [u'Enriched classics']}

    fields = {}
    want = [ '41', '490', '830' ]
    for tag, line in handle_wrapped_lines(get_tag_lines(bpl, want)):
        fields.setdefault(tag, []).append(line)
    assert read_series(fields) == {'series': [u'Dover thrift editions']}

########NEW FILE########
__FILENAME__ = cmdline
#!/usr/bin/python2.5
from openlibrary.catalog.marc.fast_parse import *
from openlibrary.catalog.get_ia import get_from_archive
import sys, codecs, re

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

re_subtag = re.compile('\x1f[^\x1b]')

def fmt_subfields(line, is_marc8=False):
    def bold(s):
        return ''.join(c + "\b" + c for c in s)
    assert line[-1] == '\x1e'

    encode = {
        'k': lambda s: bold('$%s' % s),
        'v': lambda s: translate(s, leader_says_marc8=marc8),
    }
    return ''.join(encode[k](v) for k, v in split_line(line[2:-1]))
    pos = 0
    prev = None
    subfields = []
    for m in re_subtag.finditer(line[2:-1]):
        if prev is None:
            prev = m.start()
            continue
        subfields.append(line[prev+3:m.start()+2])
        prev = m.start()
    subfields.append(line[prev+3:-1])

    return ''.join(' ' + bold('$' + i[0]) + ' ' + (translate(i[1:], leader_says_marc8=leader_says_marc8) if i else '') for i in subfields)

def show_book(data):
    is_marc8 = data[9] == ' '
    print 'leader:', data[:24]
    for tag, line in get_all_tag_lines(data):
        print tag, `line`
        continue
        if tag.startswith('00'):
            print tag, line[:-1]
        else:
            print tag, line[0:2], fmt_subfields(line, is_marc8=is_marc8)

if __name__ == '__main__':
    source = sys.argv[1]
    if ':' in source:
        data = get_from_archive(source)
    else:
        data = open(source).read()
    show_book(data)

########NEW FILE########
__FILENAME__ = by_author
#!/usr/bin/python2.5
import web, re
from catalog.utils.query import query_iter, withKey
from web_marc_db import search_query, marc_data, esc
from catalog.marc.fast_parse import get_all_subfields, get_tag_lines, get_first_tag, get_subfields
from catalog.utils import pick_first_date, flip_name, author_dates_match
from collections import defaultdict

urls = (
    '/', 'index'
)

def read_line(line, name):
    if not line or '\x1fd' not in line:
        return
    subfields = tuple((k, v.strip(' /,;:')) for k, v in get_subfields(line, 'abcd'))
    marc_name = ' '.join(v for k, v in subfields if k in 'abc')
    flipped = flip_name(marc_name)
    if marc_name != name and flipped != name:
        return
    d = pick_first_date(v for k, v in subfields if k in 'abcd')
    dates = tuple(d.get(k, None) for k in ['birth_date', 'death_date', 'date'])
    return (marc_name, flipped, dates)

def data_from_marc(locs, name):
    lines = defaultdict(list)
    for loc in locs:
        data = marc_data(loc)
        line = read_line(get_first_tag(data, set(['100'])), name)
        if line:
            lines[line].append(loc)
        for tag, line in get_tag_lines(data, set(['700'])):
            line = read_line(line, name)
            if line:
                lines[line].append(loc)
    return lines

def author_search(name):
    q = {
        'type':'/type/author',
        'name': name,
        'birth_date': None,
        'death_date': None,
        'dates': None
    }
    return [a for a in query_iter(q) if a.get('birth_date', None) or a.get('death_date', None) or a.get('dates', None)]

def search(author, name):
    book_fields = ('title_prefix', 'title');
    q = { 'type': '/type/edition', 'authors': author, 'title_prefix': None, 'title': None, 'isbn_10': None}
    found = list(query_iter(q))
    db_author = ''
    names = set([name])
    t = ''
    books = []
    for e in found:
        locs = set()
        for i in e['isbn_10'] or []:
            locs.update(search_query('isbn', i))
        if not locs:
            books.append((e['key'], (e['title_prefix'] or '') + e['title'], None, []))
            continue
        found = data_from_marc(locs, name)
        if len(found) != 1:
            locs = []
            for i in found.values():
                locs.append(i)
            books.append((e['key'], (e['title_prefix'] or '') + e['title'], None, locs))
            continue
        marc_author = found.keys()[0]
        locs = found.values()[0]
        names.update(marc_author[0:2])
        books.append((e['key'], (e['title_prefix'] or '') + e['title'], marc_author, locs))

    authors = []
    names2 = set()
    for n in names:
        if ', ' in n:
            continue
        i = n.rfind(' ')
        names2.add("%s, %s" % (n[i+1:], n[:i]))
    names.update(names2)

    for n in names:
        for a in author_search(n):
            authors.append(a)

    for a in authors:
        q = {
            'type': '/type/edition',
            'authors': a['key'],
            'title_prefix': None,
            'title': None,
            'isbn_10': None
        }
        a['editions'] = list(query_iter(q))

    author_map = {}

    for key, title, a, locs in books:
        t += '<tr><td><a href="http://openlibrary.org' + key + '">' + web.htmlquote(title) + '</a>'
        t += '<br>' + ', '.join('<a href="http://openlibrary.org/show-marc/%s">%s</a>' % (i, i) for i in locs) + '</td>'
#        t += '<td>' + web.htmlquote(`a[2]`) + '</td>'
        if a:
            if a[2] not in author_map:
                dates = {'birth_date': a[2][0], 'death_date': a[2][1], 'dates': a[2][2]}
                db_match = [db for db in authors if author_dates_match(dates, db)]
                author_map[a[2]] = db_match[0] if len(db_match) == 1 else None

            match = author_map[a[2]]
            if match:
                t += '<td><a href="http://openlibrary.org%s">%s-%s</a></td>' % (match['key'], match['birth_date'] or '', match['death_date'] or '')
            else:
                t += '<td>%s-%s (no match)</td>' % (dates['birth_date'] or '', dates['death_date'] or '')
        t += '</tr>\n'

    ret = ''
    if authors:
        ret += '<ul>'
        for a in authors:
            ret += '<li><a href="http://openlibrary.org%s">%s</a> (%s-%s) %d editions' % (a['key'], web.htmlquote(name), a['birth_date'] or '', a['death_date'] or '', len(a['editions']))
        ret += '</ul>'

    return ret + '<table>' + t + '</table>'

class index():
    def GET(self):
        web.header('Content-Type','text/html; charset=utf-8', unique=True)
        input = web.input()
        title = 'Tidy author'
        if 'author' in input and input.author:
            author = input.author
            name = withKey(author)['name']
            q_name = web.htmlquote(name)
            title = q_name + ' - Tidy author'
        else:
            author = None
        ret = "<html>\n<head>\n<title>%s</title>" % title

        ret += '''
<style>
th { text-align: left }
td { padding: 5px; background: #eee }
</style>'''

        ret += '</head><body><a name="top">'
        ret += '<form name="main" method="get"><table><tr><td align="right">Author</td><td>'
        if author:
            ret += '<input type="text" name="author" value="%s">' % web.htmlquote(author)
        else:
            ret += '<input type="text" name="author">'
        ret += '</td>'
        ret += '<td><input type="submit" value="find"></td></tr>'
        ret += '</table>'
        ret += '</form>'
        if author: 
            ret += 'Author: <a href="http://openlibrary.org%s">%s</a><br>' % (author, name)
            ret += search(author, name)
        ret += "</body></html>"
        return ret

app = web.application(urls, globals())

if __name__ == "__main__":
    app.run()

########NEW FILE########
__FILENAME__ = db_index
from catalog.get_ia import read_marc_file
from time import time
from catalog.marc.fast_parse import index_fields, get_tag_lines
import os, os.path, re
from catalog.marc.all import all_files
from catalog.read_rc import read_rc

rc = read_rc()

fields = ['title', 'oclc', 'isbn', 'lccn']

out = dict((i, open(i, 'a')) for i in fields)
rec_id = 0
db_rec = open('recs', 'a')
db_file = open('files', 'a')
file_id = 0

re_escape = re.compile(r'[\n\r\t\0\\]')
trans = { '\n': '\\n', '\r': '\\r', '\t': '\\t', '\\': '\\\\', '\0': '', }

def esc_group(m):
    return trans[m.group(0)]
def esc(str): return re_escape.sub(esc_group, str)

def add_to_index(fh, value, key):
    if not value:
        return
    try:
        value = str(value)
    except UnicodeEncodeError:
        return
    print >> fh, "\t".join([key, esc(value)])

def process_record(pos, loc, data, file_id):
    global rec_id
    want = [
#        '006', # Material Characteristics
        '010', # LCCN
        '020', # ISBN
        '035', # OCLC
#        '130', '240', # work title
        '245', # title
#        '246', '730', '740' # other titles
    ]
    try:
        rec = index_fields(data, want, check_author = False)
    except:
        print loc
        raise
    if not rec:
        return
    field_size = { 'isbn': 16, 'oclc': 16, 'title': 25, 'lccn': 16 }
    if 'isbn' in rec:
        rec['isbn'] = [i for i in rec['isbn'] if len(i) <= 16]
    if 'oclc' in rec:
        rec['oclc'] = [i for i in rec['oclc'] if len(i) <= 16]
    if 'lccn' in rec:
        rec['lccn'] = [i for i in rec['lccn'] if len(i) <= 16]
    for k, v in rec.iteritems():
        if 'isbn' != k and any(len(i) > field_size[k] for i in v):
            print loc
            print rec
            assert False
    rec_id += 1
    (f, p, l) = loc[5:].split(':')
    print >> db_rec, '\t'.join([str(rec_id), str(file_id), p, l])

    for k, v in rec.iteritems():
        if not v:
            continue
        for i in v:
            add_to_index(out[k], i, str(rec_id)) 

def progress_update(rec_no, t):
    remaining = total - rec_no
    rec_per_sec = chunk / t
    mins = (float((t/chunk) * remaining) / 60)
    print "%d %.3f rec/sec" % (rec_no, rec_per_sec),
    if mins > 1440:
        print "%.3f days left" % (mins / 1440)
    elif mins > 60:
        print "%.3f hours left" % (mins / 60)
    else:
        print "%.3f minutes left" % mins

t_prev = time()
rec_no = 0
chunk = 10000
total = 32856039


for name, part, size in all_files():
    f = open(name)
    print part
    file_id += 1
    print file_id, part, size
    print >> db_file, '\t'.join([str(file_id), part])
    filename = rc['marc_path'] + '/' + part
    if not os.path.exists(filename):
        print filename, 'missing'
    #    continue
    assert os.path.exists(filename)
    f = open(filename)
    for pos, loc, data in read_marc_file(part, f):
        rec_no +=1
        if rec_no % chunk == 0:
            t = time() - t_prev
            progress_update(rec_no, t)
            t_prev = time()
        process_record(pos, loc, data, file_id)

db_file.close()
db_rec.close()

print "closing files"
for v in out.values():
    v.close()
print "finished"

########NEW FILE########
__FILENAME__ = find_bad_isbn
#from catalog.get_ia import *
from catalog.get_ia import read_marc_file
from catalog.read_rc import read_rc
#from sources import sources
from time import time
from catalog.marc.fast_parse import index_fields, get_tag_lines
import web, os, os.path, re

# build an index of ISBN to MARC records

rc = read_rc()
web.config.db_parameters = dict(dbn='postgres', db='ol_merge', user=rc['user'], pw=rc['pw'], host=rc['host'])
web.config.db_printing = False
web.load()

def process_record(pos, loc, data):
    global rec_id
    want = [
#        '006', # Material Characteristics
#        '010', # LCCN
        '020', # ISBN
#        '035', # OCLC
#        '130', '240', # work title
#        '245', # title
#        '246', '730', '740' # other titles
    ]
    rec = index_fields(data, want, check_author = False)
    field_size = { 'isbn': 16, 'oclc': 16, 'title': 25, 'lccn': 16 }
    if not rec or 'isbn' not in rec:
        return
    for isbn in rec['isbn']:
        if ';' in isbn:
            print loc
            print rec
        assert ';' not in isbn
    too_long = any(len(i) > 16 for i in rec['isbn'])
    if not too_long:
        return
    print loc
    print rec
    assert not too_long
    
    for a, length in field_size:
        if a not in rec:
            continue
        too_long = any(len(i) > size for i in rec[a])
        if not too_long:
            continue
        print loc
        print rec
        assert too_long
#    rec = list(get_tag_lines(data, want))
    return

def progress_update(rec_no, t):
    remaining = total - rec_no
    rec_per_sec = chunk / t
    mins = (float((t/chunk) * remaining) / 60)
    print "%d %.3f rec/sec" % (rec_no, rec_per_sec),
    if mins > 1440:
        print "%.3f days left" % (mins / 1440)
    elif mins > 60:
        print "%.3f hours left" % (mins / 60)
    else:
        print "%.3f minutes left" % mins

t_prev = time()
rec_no = 0
chunk = 1000
total = 32856039

def sources():
    return ((i.id, i.archive_id, i.name) for i in web.select('marc_source'))

def files(ia):
    dir = rc['marc_path'] + ia
    dir_len = len(dir) + 1
    files = []
    for dirpath, dirnames, filenames in os.walk(dir):
        files.extend(dirpath + "/" + f for f in sorted(filenames))
    return [(i[dir_len:], os.path.getsize(i)) for i in files]

for source_id, ia, name in sources():
    print
    print source_id, ia, name
    for part, size in files(ia):
        print ia, part, size
        full_part = ia + "/" + part
        filename = rc['marc_path'] + full_part
        if not os.path.exists(filename):
            print filename, 'missing'
        #    continue
        assert os.path.exists(filename)
        f = open(filename)
        for pos, loc, data in read_marc_file(full_part, f):
            rec_no +=1
            if rec_no % chunk == 0:
                t = time() - t_prev
                progress_update(rec_no, t)
                t_prev = time()
            process_record(pos, loc, data)


########NEW FILE########
__FILENAME__ = isbn_index
from catalog.get_ia import *
from catalog.read_rc import read_rc
from catalog.marc.sources import sources
from time import time
from catalog.marc.fast_parse import index_fields
import dbhash

# build an index of ISBN to MARC records

rc = read_rc()
#db = dbhash.open(rc['index_path'] + 'isbn_to_marc.dbm', 'w')
oclc_db = dbhash.open(rc['index_path'] + 'oclc.dbm', 'w')
title_db = dbhash.open(rc['index_path'] + 'title.dbm', 'w')

def add_to_db(db, isbn, loc):
    if isbn in db:
        db[isbn] += ' ' + loc
    else:
        db[isbn] = loc

def process_record(pos, loc, data):
    rec = index_fields(data, ['010'])
    if not rec:
        return
    for isbn in rec.get('oclc', []):
        try:
            add_to_db(oclc_db, str(isbn), loc)
        except (KeyboardInterrupt, NameError):
            raise
        except:
            pass
    for isbn in rec.get('title', []):
        try:
            add_to_db(title_db, str(isbn), loc)
        except (KeyboardInterrupt, NameError):
            raise
        except:
            pass

def progress_update(rec_no, t):
    remaining = total - rec_no
    rec_per_sec = chunk / t
    mins = (float((t/chunk) * remaining) / 60)
    print "isbn %d %.3f rec/sec" % (rec_no, rec_per_sec),
    if mins > 1440:
        print "%.3f days left" % (mins / 1440)
    elif mins > 60:
        print "%.3f hours left" % (mins / 60)
    else:
        print "%.3f minutes left" % mins

t_prev = time()
rec_no = 0
chunk = 1000
total = 32856039

for ia, name in sources():
    print ia, name
    for part, size in files(ia):
        print part, size
        full_part = ia + "/" + part
        filename = rc['marc_path'] + full_part
        if not os.path.exists(filename):
            continue
        f = open(filename)
        for pos, loc, data in read_marc_file(full_part, f):
            rec_no +=1
            if rec_no % chunk == 0:
                t = time() - t_prev
                progress_update(rec_no, t)
                t_prev = time()
            process_record(pos, loc, data)

title_db.close()
oclc_db.close()

########NEW FILE########
__FILENAME__ = lookup
import dbhash, sys
from catalog.read_rc import read_rc

rc = read_rc()
db = dbhash.open(rc['index_path'] + 'isbn_to_marc.dbm', 'r')
isbn = sys.argv[1]
if isbn in db:
    print db[isbn]
else:
    print isbn, 'not found'

########NEW FILE########
__FILENAME__ = web_author
import web, dbhash, re
from catalog.infostore import get_site
from catalog.get_ia import get_data
from catalog.read_rc import read_rc
from catalog.marc.build_record import build_record
from catalog.marc.fast_parse import get_all_subfields, get_tag_lines, get_first_tag, get_subfields

trans = {'&':'amp','<':'lt','>':'gt'}
re_html_replace = re.compile('([&<>])')

def esc(s):
    if not isinstance(s, basestring):
        return s
    return re_html_replace.sub(lambda m: "&%s;" % trans[m.group(1)], s.encode('utf8')).replace('\n', '<br>')

data_cache = {}
def marc_data(loc):
    if loc not in data_cache:
        data_cache[loc] = get_data(loc)
    return data_cache[loc]

def marc_authors(data):
    line = get_first_tag(data, set(['100', '110', '111']))
    return ''.join("<b>$%s</b>%s" % (esc(k), esc(v)) for k, v in get_all_subfields(line)) if line else None

def marc_publisher(data):
    line = get_first_tag(data, set(['260']))
    return ''.join("<b>$%s</b>%s" % (esc(k), esc(v)) for k, v in get_all_subfields(line)) if line else None

def marc_title(data):
    line = get_first_tag(data, set(['245']))
    return ''.join("<b>$%s</b>%s" % (esc(k), esc(v)) for k, v in get_subfields(line, set(['a', 'b', 'c']))) if line else None

def marc_by_statement(data):
    line = get_first_tag(data, set(['245']))
    return ''.join("<b>$%s</b>%s" % (esc(k), esc(v)) for k, v in get_subfields(line, set(['c']))) if line else None



rc = read_rc()

urls = (
    '/', 'index'
)

site = get_site()

db_isbn = dbhash.open(rc['index_path'] + 'isbn_to_marc.dbm', 'r')

def marc_table(l):
    print '<table>'
    print '<tr><td colspan="2">', l, "</td></tr>"
    data = marc_data(l)
    print '<tr><td>MARC author</td><td>', marc_authors(data), '</td></tr>'
    print '<tr><td>MARC by statement</td><td>', marc_by_statement(data), '</td></tr>'
    print '</table>'


class index:
    def GET(self):
        web.header('Content-Type','text/html; charset=utf-8', unique=True)
        key = web.input().author
        thing = site.get(key)
        title = ' - '.join([thing.name, key, 'Split author'])
        print "<html>\n<head>\n<title>%s</title>" % title
        print '''
<style>
th { text-align: left }
td { padding: 5px; background: #eee; vertical-align: top }
</style>'''

        print '</head><body><a name="top">'
        print thing.name, '<p>'
        for k in site.things({'type': '/type/edition', 'authors': key}):
            t = site.get(k)
            print '<a href="http://openlibrary.org%s">%s</a></td>' % (k, t.title)
            if t.isbn_10:
                isbn = str(t.isbn_10[0])
                locs = db_isbn[isbn].split(' ') if isbn in db_isbn else []
                print '(ISBN: <a href="http://wiki-beta.us.archive.org:8081/?isbn=%s">%s</a> <a href="http://amazon.com/dp/%s">Amazon</a>)' % (isbn, isbn, isbn)
            else:
                isbn = None
                locs = []
            if locs:
                for l in locs:
                    marc_table(l)
            print '<p>'
        print '<body><html>'

if __name__ == "__main__":
    web.run(urls, globals(), web.reloader)


########NEW FILE########
__FILENAME__ = web_marc_db
# lookup MARC records and show details on the web
from catalog.read_rc import read_rc
from catalog.get_ia import get_data
from catalog.marc.build_record import build_record
from catalog.marc.fast_parse import get_all_subfields, get_tag_lines, get_first_tag, get_subfields
from pprint import pprint
import re, sys, os.path, web
#from catalog.amazon.other_editions import find_others
from catalog.utils import strip_count

db = web.database(dbn='postgres', db='marc_lookup')
db.printing = False

trans = {'&':'amp','<':'lt','>':'gt'}
re_html_replace = re.compile('([&<>])')

def marc_authors(data):
    line = get_first_tag(data, set(['100', '110', '111']))
    return ''.join("<b>$%s</b>%s" % (esc(k), esc(v)) for k, v in get_all_subfields(line)) if line else None

def marc_publisher(data):
    line = get_first_tag(data, set(['260']))
    return ''.join("<b>$%s</b>%s" % (esc(k), esc(v)) for k, v in get_all_subfield)

def marc_title(data):
    line = get_first_tag(data, set(['245']))
    return ''.join("<b>$%s</b>%s" % (esc(k), esc(v)) for k, v in get_subfields(line, set(['a', 'b']))) if line else None

def find_isbn_file():
    for p in sys.path:
        f = p + "/catalog/isbn"
        if os.path.exists(f):
            return f

#isbn_file = find_isbn_file()
#isbn_count = os.path.getsize(isbn_file) / 11

def list_to_html(l):
    def blue(s):
        return ' <span style="color:blue; font-weight:bold">%s</span> ' % s
    return blue('[') + blue('|').join(l) + blue(']')

def esc(s):
    if not isinstance(s, basestring):
        return s
    return re_html_replace.sub(lambda m: "&%s;" % trans[m.group(1)], s.encode('utf8')).replace('\n', '<br>')

data_cache = {}
def marc_data(loc):
    if loc not in data_cache:
        data_cache[loc] = get_data(loc)
    return data_cache[loc]

def counts_html(v):
    count = {}
    lens = [len(i) for i, loc in v if i and isinstance(i, basestring)]
    sep = '<br>' if lens and max(lens) > 20 else ' '
    for i, loc in v:
        count.setdefault(i, []).append(loc)
    s = sorted(count.iteritems(), cmp=lambda x,y: cmp(len(y[1]), len(x[1]) ))
    s = strip_count(s)
    return sep.join('<b>%d</b>: <span title="%s">%s</span>' % (len(loc), `loc`, value if value else '<em>empty</em>') for value, loc in s)

def list_works(this_isbn):
    works = find_others(this_isbn, rc['amazon_other_editions'])
    print '<a name="work">'
    print '<h2>Other editions of the same work</h2>'
    if not works:
        print 'no work found'
        return
    print '<table>'
    print '<tr><th>ISBN</th><th>Amazon edition</th><th></th><th>MARC titles</th></tr>'
    for isbn, note in works:
        if note.lower().find('audio') != -1:
            continue
        locs = db_isbn[isbn].split(' ') if isbn in db_isbn else []
#        titles = [read_full_title(get_first_tag(marc_data(i), set(['245'])), accept_sound = True) for i in locs]
        titles = [(marc_title(marc_data(i)), i) for i in locs]
        num = len(locs)
        #print '<tr><td><a href="/?isbn=%s">%s</a></td><td>%s</td><td>%d</td><td>%s</td></tr>' % (isbn, isbn, note, len(locs), list_to_html(titles))
        print '<tr><td><a href="/?isbn=%s">%s</a></td><td>%s</td><td>%d</td><td>' % (isbn, isbn, note, len(locs))
        print counts_html(titles)
        print '</td></tr>'
    print '</table>'

def most_freq_isbn(input):
    counts = {}
    for a in input:
        if a:
            for b in a[0]:
                counts[b] = counts.get(b, 0) + 1
    return max(counts, key=counts.get)

def show_locs(locs, isbn):
    recs = [(loc, build_record(marc_data(loc))) for loc in locs]
    keys = set()
    ret = "records found from %d libraries<br>" % len(recs)
    ret += '<ul>'
    for loc, rec in recs:
        s = loc[:loc.find('/')]
        ret += '<li><a href="http://openlibrary.org/show-marc/%s">%s</a>' % (loc, s)
        keys.update([k for k in rec.keys()])
        for f in 'uri':
            if f in rec:
                del rec[f]
    ret += '</ul>'
    keys -= set(['uri'])
    ret += '<table>'
    first_key = True
    first = []
    for f in ['isbn_10', 'title', 'subtitle', 'by_statement', 'authors', 'contributions']:
        if f in keys:
            first += [f]
            keys -= set([f])
    for k in first + list(keys):
        v = [(rec.get(k, None), loc) for loc, rec in recs]
#        if k == 'isbn_10' and not isbn and v:
#            isbn = most_freq_isbn(v)
        if k == 'languages':
            v = [([ i['key'][3:] for i in l ] if l else None, loc) for l, loc in v]
        if all(i is None or (isinstance(i, list) and len(i) == 1) for i, loc in v):
            v = [ (i[0] if i else None, loc) for i, loc in v]

        ret += '<tr><th valign="top">%s</th><td>' % k
        if any(isinstance(i, list) or isinstance(i, dict) for i, loc in v):
            if k == 'authors': # easiest to switch to raw MARC display
                v = [(marc_authors(marc_data(loc)), loc) for i, loc in v ]
            elif k == 'isbn_10':
                v = [ (list_to_html(sorted(i)), loc) if i else (None, loc) for i, loc in v]
            else:
                v = [ (list_to_html(i), loc) if i else (None, loc) for i, loc in v]
        else:
            v = [ (esc(i), loc) for i, loc in v]
#        print `[i[0] for i in v]`, '<br>'
        ret += counts_html(v)
        if isbn and first_key:
            ret += '<td valign="top" rowspan="%d"><img src="http://covers.openlibrary.org/b/isbn/%s-L.jpg">' % (len(first) + len(keys), isbn)
            first_key = False
        ret += '</td></tr>'
    ret += '</table>'
    return ret

def search_query(field, value):
    sql = 'select part, pos, len ' \
        + 'from files, recs, ' + field \
        + ' where ' + field + '.rec=recs.id and recs.marc_file=files.id and value=$v'
    iter = db.query(sql, {'v': value})
    return [':'.join([i.part.strip(), str(i.pos), str(i.len)]) for i in iter]

########NEW FILE########
__FILENAME__ = web_ui
#!/usr/bin/python2.5
import random
import web
from web_marc_db import search_query, show_locs

# too slow
#def random_isbn():
#    sql = "select value from isbn order by random() limit 1"
#    return list(web.query(sql))[0].value

isbn_file = '../../isbn'
isbn_count = 9093242

def random_isbn():
    f = open(isbn_file)
    while 1:
        f.seek(random.randrange(isbn_count) * 11)
        isbn = f.read(10)
        break
        found = list(web.select('isbn', where='value=$v', vars={'v':isbn}))
        if found > 1:
            break
    f.close()
    return isbn

def search(field, value):
    locs = search_query(field, value)
    print locs
    if locs:
        return show_locs(locs, value if field == 'isbn' else None)
    else:
        return value + ' not found'

urls = (
    '/random', 'rand',
    '/', 'index'
)

class rand():
    def GET(self):
        isbn = random_isbn()
        web.redirect('/?isbn=' + isbn)

class index():
    def GET(self):
        web.header('Content-Type','text/html; charset=utf-8', unique=True)
        input = web.input()
        lccn = None
        oclc = None
        isbn = None
        title = 'MARC lookup'
        if 'isbn' in input and input.isbn:
            isbn = input.isbn
            if isbn == 'random':
                isbn = random_isbn()
            title = 'MARC lookup: isbn=' + isbn
        if 'lccn' in input and input.lccn:
            lccn = input.lccn
            title = 'MARC lookup: lccn=' + lccn
        if 'oclc' in input and input.oclc:
            oclc = input.oclc
            title = 'MARC lookup: oclc=' + oclc
        ret = "<html>\n<head>\n<title>%s</title>" % title
        ret += '''
<style>
th { text-align: left }
td { padding: 5px; background: #eee }
</style>'''

        ret += '</head><body><a name="top">'
        ret += '<form name="main" method="get"><table><tr><td align="right">ISBN</td><td>'
        if isbn:
            ret += '<input type="text" name="isbn" value="%s">' % web.htmlquote(isbn)
        else:
            ret += '<input type="text" name="isbn">'
        ret += ' or <a href="/random">random</a><br>'
        ret += '</td></tr><tr><td align="right">LCCN</td><td>'
        if lccn:
            ret += '<input type="text" name="lccn" value="%s">' % web.htmlquote(lccn)
        else:
            ret += '<input type="text" name="lccn">'
        ret += '</td></tr><tr><td align="right">OCLC</td><td>'
        if oclc:
            ret += '<input type="text" name="oclc" value="%s">' % web.htmlquote(oclc)
        else:
            ret += '<input type="text" name="oclc">'
        ret += '</td></tr>'
        ret += '<tr><td></td><td><input type="submit" value="find"></td></tr>'
        ret += '</table>'
        ret += '</form>'
        if isbn:
            ret += search('isbn', isbn)
        elif lccn:
            search('lccn', lccn)
        elif oclc:
            search('oclc', oclc)
        ret += "</body></html>"
        return ret

app = web.application(urls, globals())

if __name__ == "__main__":
    app.run()

########NEW FILE########
__FILENAME__ = download
#!/usr/bin/python
# downloader so Karen doesn't need to download entire MARC files
import web
import web.form as form
import urllib2
from pprint import pprint

urls = (
    '/', 'index',
    '/(bpl\d+\.mrc):(\d+):(\d+)', 'get',
)

files = (
    ('bpl101.mrc', 50000),
    ('bpl102.mrc', 50000),
    ('bpl103.mrc', 50000),
    ('bpl104.mrc', 50000),
    ('bpl105.mrc', 50000),
    ('bpl106.mrc', 50000),
    ('bpl107.mrc', 50000),
    ('bpl108.mrc', 50000),
    ('bpl109.mrc', 50000),
    ('bpl110.mrc', 50000),
    ('bpl111.mrc', 50000),
    ('bpl112.mrc', 50000),
    ('bpl113.mrc', 50000),
    ('bpl114.mrc', 50000),
    ('bpl115.mrc', 50000),
    ('bpl116.mrc', 50000),
    ('bpl117.mrc', 50000),
    ('bpl118.mrc', 49997),
    ('bpl119.mrc', 50000),
    ('bpl120.mrc', 50000),
    ('bpl121.mrc', 50000),
    ('bpl122.mrc', 49999),
    ('bpl123.mrc', 50000),
    ('bpl124.mrc', 50000),
    ('bpl125.mrc', 50000),
    ('bpl126.mrc', 50000),
    ('bpl127.mrc', 50000),
    ('bpl128.mrc', 49999),
    ('bpl129.mrc', 49999),
    ('bpl130.mrc', 50000),
    ('bpl131.mrc', 49999),
    ('bpl132.mrc', 50000),
    ('bpl133.mrc', 50000),
    ('bpl134.mrc', 49999),
    ('bpl135.mrc', 50000),
    ('bpl136.mrc', 50000),
    ('bpl137.mrc', 49999),
    ('bpl138.mrc', 50000),
    ('bpl139.mrc', 50000),
    ('bpl140.mrc', 50000),
    ('bpl141.mrc', 50000),
    ('bpl142.mrc', 50000),
    ('bpl143.mrc', 50000),
    ('bpl144.mrc', 50000),
    ('bpl145.mrc', 50000),
    ('bpl146.mrc', 50000),
    ('bpl147.mrc', 41036),
)

myform = form.Form( 
    form.Dropdown('file', [(i, "%s - %d records" % (i, j)) for i, j in files]),
    form.Textbox("start", 
        form.notnull,
        form.regexp('\d+', 'Must be a digit'),
        form.Validator('Must be less than 50000', lambda x:int(x)>50000)),
    form.Textbox("count", 
        form.notnull,
        form.regexp('\d+', 'Must be a digit'),
        form.Validator('Must be less than 50000', lambda x:int(x)>50000)))

def start_and_len(file, start, count):
    f = urllib2.urlopen("http://archive.org/download/bpl_marc/" + file)
    pos = 0
    num = 0
    start_pos = None
    while num < start + count:
        data = f.read(5)
        if data == '':
            break
        rec_len = int(data)
        f.read(rec_len-5)
        pos+=rec_len
        num+=1
        if num == start:
            start_pos = pos

    f.close()
    return (start_pos, pos - start_pos)

class index:
    def GET(self):
        this_form = myform()
        this_form.fill()
        print '<form name="main" method="get">'
        if not this_form.valid:
            print '<p class="error">Try again:</p>'
        print this_form.render()
        print '<input type="submit"></form>'
        if this_form['start'].value:
            file = this_form['file'].value
            (offset, length) = start_and_len(file, int(this_form['start'].value), int(this_form['count'].value))
            print "%.1fKB" % (float(length) / 1024.0)
            url = "http://wiki-beta.us.archive.org:9090/%s:%d:%d" % (file, offset, length)
            print '<a href="%s">download</a>' % url

class get:
    def GET(self, file, offset, length):
        offset = int(offset)
        length = int(length)
        web.header("Content-Type","application/octet-stream")
        r0, r1 = offset, offset+length-1
        url = "http://archive.org/download/bpl_marc/" + file
        ureq = urllib2.Request(url, None, {'Range':'bytes=%d-%d'% (r0, r1)},)
        f = urllib2.urlopen(ureq)
        while 1:
            buf = f.read(1024)
            if not buf:
                break
            web.output(buf)
        f.close()

web.webapi.internalerror = web.debugerror

if __name__ == "__main__": web.run(urls, globals(), web.reloader)

########NEW FILE########
__FILENAME__ = fast_parse
# fast parse for merge

import re
from pymarc import MARC8ToUnicode
import mnemonics
from unicodedata import normalize
from openlibrary.catalog.utils import tidy_isbn

re_real_book = re.compile('(pbk|hardcover|alk[^a-z]paper|cloth)', re.I)

def translate(bytes_in, leader_says_marc8=False):
    marc8 = MARC8ToUnicode(quiet=True)
    try:
        if leader_says_marc8:
            data = marc8.translate(mnemonics.read(bytes_in))
        else:
            data = bytes_in.decode('utf-8')
        return normalize('NFC', data)
    except:
        print 'translate error for:', `bytes_in`
        print 'marc8:', leader_says_marc8
        raise

re_question = re.compile('^\?+$')
re_lccn = re.compile('(...\d+).*')
re_letters_and_bad = re.compile('[A-Za-z\x80-\xff]')
re_int = re.compile ('\d{2,}')
re_isbn = re.compile('([^ ()]+[\dX])(?: \((?:v\. (\d+)(?: : )?)?(.*)\))?')
re_oclc = re.compile ('^\(OCoLC\).*?0*(\d+)')

re_normalize = re.compile('[^\w ]')
re_whitespace = re.compile('\s+')

def normalize_str(s):
    s = re_normalize.sub('', s.strip())
    s = re_whitespace.sub(' ', s)
    return str(s.lower())

# no monograph should be longer than 50,000 pages
max_number_of_pages = 50000

class InvalidMarcFile(Exception):
    pass

def read_file(f):
    buf = None
    while 1:
        if buf:
            length = buf[:5]
            try:
                int_length = int(length)
            except:
                print `buf`
                raise
        else:
            length = f.read(5)
            buf = length
        if length == "":
            break
        if not length.isdigit():
            print 'not a digit:', `length`
            raise InvalidMarcFile
        int_length = int(length)
        data = buf + f.read(int_length - len(buf))
        buf = None
        if not data.endswith("\x1e\x1d"):
            # skip bad record, should warn somehow
            end_index = data.rfind('\x1e\x1d')
            if end_index != -1:
                end = end_index + 2
                yield (data[:end], end)
                buf = data[end:]
                continue
        if data.find('\x1d') == -1:
            data += f.read(40)
            int_length = data.find('\x1d') + 1
            assert int_length
            buf = data[int_length:]
            data = data[:int_length]
        assert data.endswith("\x1e\x1d")
        if len(data) < int_length:
            break
        yield (data, int_length)

def read_author_person(line, is_marc8):
    name = []
    name_and_date = []
    for k, v in get_subfields(line, ['a', 'b', 'c', 'd'], is_marc8):
        if k != 'd':
            v = v.strip(' /,;:')
            name.append(v)
        name_and_date.append(v)
    if not name:
        return []

    return [{ 'db_name': u' '.join(name_and_date), 'name': u' '.join(name), }]

# exceptions:
class SoundRecording(Exception):
    pass

class NotBook(Exception):
    pass

class BadDictionary(Exception):
    pass

def read_full_title(line, accept_sound = False, is_marc8=False):
    if not accept_sound and v.lower().startswith("[sound"):
        raise SoundRecording
    if v.lower().startswith("[graphic") or v.lower().startswith("[cartographic"):
        raise NotBook
    title = [v.strip(' /,;:') for k, v in get_subfields(line, ['a', 'b'], is_marc8)]
    return ' '.join([t for t in title if t])

def read_short_title(line, is_marc8):
    title = normalize_str(read_full_title(line, is_marc8))[:25].rstrip()
    if title:
        return [title]
    else:
        return []

def read_title_and_subtitle(data, is_marc8): # not currently used
    line = get_first_tag(data, set(['245']))
    contents = get_contents(line, ['a', 'b', 'c', 'h'], is_marc8)

    title = None
    if 'a' in contents:
        title = ' '.join(x.strip(' /,;:') for x in contents['a'])
    elif 'b' in contents:
        title = contents['b'][0].strip(' /,;:')
        del contents['b'][0]
    subtitle = None
    if 'b' in contents and contents['b']:
        subtitle = ' : '.join([x.strip(' /,;:') for x in contents['b']])
    return (title, subtitle)

def get_raw_subfields(line, want):
    # no translate
    want = set(want)
    #assert line[2] == '\x1f'
    for i in line[3:-1].split('\x1f'):
        if i and i[0] in want:
            yield i[0], i[1:]

def get_all_subfields(line, is_marc8=False):
    for i in line[3:-1].split('\x1f'):
        if i:
            j = translate(i, is_marc8)
            yield j[0], j[1:]

def get_subfields(line, want, is_marc8=False):
    want = set(want)
    #assert line[2] == '\x1f'
    for i in line[3:-1].split('\x1f'):
        if i and i[0] in want:
            yield i[0], translate(i[1:], is_marc8)

def read_directory(data):
    dir_end = data.find('\x1e')
    if dir_end == -1:
        raise BadDictionary
    directory = data[24:dir_end]
    if len(directory) % 12 != 0:
        print 'directory is the wrong size'
        # directory is the wrong size
        # sometimes the leader includes some utf-8 by mistake
        directory = data[:dir_end].decode('utf-8')[24:]
        if len(directory) % 12 != 0:
            print len(directory) / 12
            raise BadDictionary
    iter_dir = (directory[i*12:(i+1)*12] for i in range(len(directory) / 12))
    return dir_end, iter_dir

def get_tag_line(data, line):
    length = int(line[3:7])
    offset = int(line[7:12])

    # handle off-by-one errors in MARC records
    try:
        if data[offset] != '\x1e':
            offset += data[offset:].find('\x1e')
        last = offset+length
        if data[last] != '\x1e':
            length += data[last:].find('\x1e')
    except IndexError:
        pass

    tag_line = data[offset + 1:offset + length + 1]
    if not line[0:2] == '00':
        # marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:636441290:1277
        if tag_line[1:8] == '{llig}\x1f':
            tag_line = tag_line[0] + u'\uFE20' + tag_line[7:]
    return tag_line

re_dates = re.compile('^\(?(\d+-\d*|\d*-\d+)\)?$')

def get_person_content(line, is_marc8):
    contents = {}
    for k, v in get_subfields(line, ['a', 'b', 'c', 'd', 'q'], is_marc8):
        if k != 'd' and re_dates.match(v): # wrong subtag
            k = 'd'
        contents.setdefault(k, []).append(v)
    return contents

def get_contents(line, want):
    contents = {}
    for k, v in get_subfields(line, want, is_marc8):
        contents.setdefault(k, []).append(v)
    return contents

def get_lower_subfields(line, is_marc8):
    if len(line) < 4: 
        return [] # http://openlibrary.org/show-marc/marc_university_of_toronto/uoft.marc:2479215:693
    return [translate(i[1:], is_marc8) for i in line[3:-1].split('\x1f') if i and i[0].islower()]

def get_subfield_values(line, want, is_marc8):
    return [v for k, v in get_subfields(line, want, is_marc8)]

def get_all_tag_lines(data):
    dir_end, iter_dir = read_directory(data)
    data = data[dir_end:]
    for line in iter_dir:
        yield (line[:3], get_tag_line(data, line))
    #return [(line[:3], get_tag_line(data, line)) for line in iter_dir]

def get_first_tag(data, want): # return first line of wanted tag
    dir_end, iter_dir = read_directory(data)
    data = data[dir_end:]
    for line in iter_dir:
        if line[:3] in want:
            return get_tag_line(data, line)

def get_tag_lines(data, want):
    want = set(want)
    dir_end, iter_dir = read_directory(data)
    data = data[dir_end:]
    return [(line[:3], get_tag_line(data, line)) for line in iter_dir if line[:3] in want]

def read_control_number(line, is_marc8):
    assert line[-1] == '\x1e'
    return [line[:-1]]

def read_lccn(line, is_marc8):
    found = []
    for k, v in get_raw_subfields(line, ['a']):
        lccn = v.strip()
        if re_question.match(lccn):
            continue
        m = re_lccn.search(lccn)
        if not m:
            continue
        # remove letters and bad chars
        lccn = re_letters_and_bad.sub('', m.group(1)).strip()
        if lccn:
            found.append(lccn)
    return found

def read_isbn(line, is_marc8):
    found = []
    if line.find('\x1f') != -1:
        for k, v in get_raw_subfields(line, ['a', 'z']):
            m = re_isbn.match(v)
            if m:
                found.append(m.group(1))
    else:
        m = re_isbn.match(line[3:-1])
        if m:
            found = [m.group(1)]
    return map(str, tidy_isbn(found))

def read_oclc(line, is_marc8):
    found = []
    for k, v in get_raw_subfields(line, ['a']):
        m = re_oclc.match(v)
        if m:
            found.append(m.group(1))
    return found

def read_publisher(line, is_marc8):
    return [i for i in (v.strip(' /,;:') for k, v in get_subfields(line, ['b'], is_marc8)) if i]

def read_author_org(line, is_marc8):
    name = " ".join(v.strip(' /,;:') for k, v in get_subfields(line, ['a', 'b'], is_marc8))
    return [{ 'name': name, 'db_name': name, }]

def read_author_event(line, is_marc8):
    name = " ".join(v.strip(' /,;:') for k, v in get_subfields(line, ['a', 'b', 'd', 'n'], is_marc8))
    return [{ 'name': name, 'db_name': name, }]

def add_oclc(edition):
    if 'control_numer' not in edition:
        return
    oclc = edition['control_number'][0]
    assert oclc.isdigit()
    edition.setdefault('oclc', []).append(oclc)

def index_fields(data, want, check_author = True):
    if str(data)[6:8] != 'am': # only want books
        return None
    is_marc8 = data[9] != 'a'
    edition = {}
    # ['006', '010', '020', '035', '245']
    author = {
        '100': 'person',
        '110': 'org',
        '111': 'even',
    }


    if check_author:
        want += author.keys()
    fields = get_tag_lines(data, ['006', '008', '260'] + want)
    read_tag = {
        '001': (read_control_number, 'control_number'),
        '010': (read_lccn, 'lccn'),
        '020': (read_isbn, 'isbn'),
        '035': (read_oclc, 'oclc'),
        '245': (read_short_title, 'title'),
    }

    seen_008 = False
    oclc_001 = False
    is_real_book = False

    tag_006_says_electric = False
    for tag, line in fields:
        if tag == '003': # control number identifier
            if line.lower().startswith('ocolc'):
                oclc_001 = True
            continue
        if tag == '006':
            if line[0] == 'm': # don't want electronic resources
                tag_006_says_electric = True
            continue
        if tag == '008':
            if seen_008: # dup
                return None
            seen_008 = True
            continue
        if tag == '020' and re_real_book.search(line):
            is_real_book = True
        if tag == '260': 
            if line.find('\x1fh[sound') != -1: # sound recording
                return None
            continue

        if tag in author:
            if 'author' in edition:
                return None
            else:
                edition['author'] = author[tag]
            continue
        assert tag in read_tag
        proc, key = read_tag[tag]
        try:
            found = proc(line, is_marc8)
        except SoundRecording:
            return None
        if found:
            edition.setdefault(key, []).extend(found)
    if oclc_001:
        add_oclc(edition)
    if 'control_number' in edition:
        del edition['control_number']
    if not seen_008:
        return None
#    if 'title' not in edition:
#        return None
    if tag_006_says_electric and not is_real_book:
        return None
    return edition

def read_edition(data, accept_electronic = False):
    is_marc8 = data[9] != 'a'
    edition = {}
    want = ['001', '003', '006', '008', '010', '020', '035', \
            '100', '110', '111', '700', '710', '711', '245', '260', '300']
    fields = get_tag_lines(data, want)
    read_tag = [
        ('001', read_control_number, 'control_number'),
        ('010', read_lccn, 'lccn'),
        ('020', read_isbn, 'isbn'),
        ('035', read_oclc, 'oclc'),
        ('100', read_author_person, 'authors'),
        ('110', read_author_org, 'authors'),
        ('111', read_author_event, 'authors'),
        ('700', read_author_person, 'contribs'),
        ('710', read_author_org, 'contribs'),
        ('711', read_author_event, 'contribs'),
        ('260', read_publisher, 'publishers'),
    ]

    oclc_001 = False
    tag_006_says_electric = False
    is_real_book = False
    for tag, line in fields:
        if tag == '003': # control number identifier
            if line.lower().startswith('ocolc'):
                oclc_001 = True
            continue
        if tag == '006':
            if line[0] == 'm':
                tag_006_says_electric = True
            continue
        if tag == '008': # not interested in '19uu' for merge
            #assert len(line) == 41 usually
            if line[7:11].isdigit(): 
                edition['publish_date'] = line[7:11]
            edition['publish_country'] = line[15:18]
            continue
        if tag == '020' and re_real_book.search(line):
            is_real_book = True
        for t, proc, key in read_tag:
            if t != tag:
                continue
            found = proc(line, is_marc8=is_marc8)
            if found:
                edition.setdefault(key, []).extend(found)
            break
        if tag == '245':
            edition['full_title'] = read_full_title(line, is_marc8=is_marc8)
            continue
        if tag == '300':
            for k, v in get_subfields(line, ['a'], is_marc8):
                num = [ int(i) for i in re_int.findall(v) ]
                num = [i for i in num if i < max_number_of_pages]
                if not num:
                    continue
                max_page_num = max(num)
                if 'number_of_pages' not in edition \
                        or max_page_num > edition['number_of_pages']:
                    edition['number_of_pages'] = max_page_num
    if oclc_001:
        add_oclc(edition)
    if 'control_number' in edition:
        del edition['control_number']
    if not accept_electronic and tag_006_says_electric and not is_real_book:
        print 'electronic resources'
        return None

    return edition

def handle_wrapped_lines(iter):
    cur_lines = []
    cur_tag = None
    maybe_wrap = False
    for t, l in iter:
        if len(l) > 500 and l.endswith('++\x1e'):
            assert not cur_tag or cur_tag == t
            cur_tag = t
            cur_lines.append(l)
            continue
        if cur_lines:
            yield cur_tag, cur_lines[0][:-3] + ''.join(i[2:-3] for i in cur_lines[1:]) + l[2:]
            cur_tag = None
            cur_lines = []
            continue
        yield t, l
    assert not cur_lines

def split_line(s):
    # TODO: document this function
    pos = -1
    marks = []
    while 1:
        pos = s.find('\x1f', pos + 1)
        if pos == -1:
            break
        if s[pos+1] != '\x1b':
            marks.append(pos)
    if not marks:
        return [('v', s)]

    ret = []
    if s[:marks[0]]:
        ret.append(('v', s[:marks[0]]))
    for i in range(len(marks)):
        m = marks[i]
        ret.append(('k', s[m+1:m+2]))
        if len(marks) == i+1:
            if s[m+2:]:
                ret.append(('v', s[m+2:]))
        else:
            if s[m+2:marks[i+1]]:
                ret.append(('v', s[m+2:marks[i+1]]))
    return ret

def test_wrapped_lines():
    data = open('test_data/wrapped_lines').read()
    ret = list(handle_wrapped_lines(get_tag_lines(data, ['520'])))
    assert len(ret) == 2
    a, b = ret
    assert a[0] == '520' and b[0] == '520'
    assert len(a[1]) == 2295
    assert len(b[1]) == 248

def test_translate():
    assert translate('Vieira, Claudio Bara\xe2una,') == u'Vieira, Claudio Bara\xfana,'

def test_read_oclc():
    from pprint import pprint
    for f in ('oregon_27194315',):
        data = open('test_data/' + f).read()
        i = index_fields(data, ['001', '003', '010', '020', '035', '245'])
        assert 'oclc' in i
        e = read_edition(data)
        assert 'oclc' in e

def test_record():
    assert read_edition(open('test_data/lc_0444897283').read())

def test_empty():
    assert read_edition('') == {}

def bad_marc_line():
    line = '0 \x1f\xe2aEtude objective des ph\xe2enom\xe1enes neuro-psychiques;\x1e'
    assert list(get_all_subfields(line)) == [(u'\xe1', u'Etude objective des ph\xe9nom\xe8nes neuro-psychiques;')]

def test_index_fields():
    data = open('test_data/ithaca_college_75002321').read()
    lccn = index_fields(data, ['010'])['lccn'][0]
    assert lccn == '75002321'

def test_ia_charset():
    data = open('test_data/histoirereligieu05cr_meta.mrc').read()
    line = list(get_tag_lines(data, set(['100'])))[0][1]
    a = list(get_all_subfields(line, ia_bad_charset=True))[0][1]
    expect = u'Cr\xe9tineau-Joly, J.'
    assert a == expect

########NEW FILE########
__FILENAME__ = find_translation
#!/usr/bin/python2.5
from catalog.marc.fast_parse import *
import sys, codecs

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

for data, length in read_file(open(sys.argv[1])):
    line = get_first_tag(data, set(['500']))
    if not line:
        continue
    subtag, value = get_all_subfields(line).next()
    if subtag != 'a':
        continue
    if value.startswith("Translation of the author's thesis"):
        continue
    start = value.lower().find('translation of')
    if start == -1 or start > 6:
        continue
    print value

########NEW FILE########
__FILENAME__ = get_subjects
from collections import defaultdict
import re
from openlibrary.catalog.utils import remove_trailing_dot, flip_name

re_flip_name = re.compile('^(.+), ([A-Z].+)$')

# 'Rhodes, Dan (Fictitious character)'
re_fictitious_character = re.compile('^(.+), (.+)( \(.* character\))$')
re_etc = re.compile('^(.+?)[, .]+etc[, .]?$', re.I)
re_comma = re.compile('^([A-Z])([A-Za-z ]+?) *, ([A-Z][A-Z a-z]+)$')

re_place_comma = re.compile('^(.+), (.+)$')
re_paren = re.compile('[()]')
def flip_place(s):
    s = remove_trailing_dot(s)
    # Whitechapel (London, England)
    # East End (London, England)
    # Whitechapel (Londres, Inglaterra)
    if re_paren.search(s):
        return s
    m = re_place_comma.match(s)
    return m.group(2) + ' ' + m.group(1) if m else s

def flip_subject(s):
    m = re_comma.match(s)
    if m:
        return m.group(3) + ' ' + m.group(1).lower()+m.group(2)
    else:
        return s

def tidy_subject(s):
    s = s.strip()
    if len(s) < 2:
        print 'short subject:', `s`
    else:
        s = s[0].upper() + s[1:]
    m = re_etc.search(s)
    if m:
        return m.group(1)
    s = remove_trailing_dot(s)
    m = re_fictitious_character.match(s)
    if m:
        return m.group(2) + ' ' + m.group(1) + m.group(3)
    m = re_comma.match(s)
    if m:
        return m.group(3) + ' ' + m.group(1) + m.group(2)
    return s

def four_types(i):
    want = set(['subject', 'time', 'place', 'person'])
    ret = dict((k, i[k]) for k in want if k in i)
    for j in (j for j in i.keys() if j not in want):
        for k, v in i[j].items():
            if 'subject' in ret:
                ret['subject'][k] = ret['subject'].get(k, 0) + v
            else:
                ret['subject'] = {k: v}
    return ret

re_aspects = re.compile(' [Aa]spects$')
def find_aspects(f):
    cur = [(i, j) for i, j in f.get_subfields('ax')]
    if len(cur) < 2 or cur[0][0] != 'a' or cur[1][0] != 'x':
        return
    a, x = cur[0][1], cur[1][1]
    x = x.strip('. ')
    a = a.strip('. ')
    if not re_aspects.search(x):
        return
    if a == 'Body, Human':
        a = 'the Human body'
    return x + ' of ' + flip_subject(a)

subject_fields = set(['600', '610', '611', '630', '648', '650', '651', '662'])

def read_subjects(rec):
    subjects = defaultdict(lambda: defaultdict(int))
    for tag, field in rec.read_fields(subject_fields):
        f = rec.decode_field(field)
        aspects = find_aspects(f)

        if tag == '600': # people
            name_and_date = []
            for k, v in f.get_subfields(['a', 'b', 'c', 'd']):
                v = '(' + v.strip('.() ') + ')' if k == 'd' else v.strip(' /,;:')
                if k == 'a':
                    m = re_flip_name.match(v)
                    if m:
                        v = flip_name(v)
                name_and_date.append(v)
            name = remove_trailing_dot(' '.join(name_and_date)).strip()
            if name != '':
                subjects['person'][name] += 1
        elif tag == '610': # org
            v = ' '.join(f.get_subfield_values('abcd'))
            v = v.strip()
            if v:
                v = remove_trailing_dot(v).strip()
            if v:
                v = tidy_subject(v)
            if v:
                subjects['org'][v] += 1

            for v in f.get_subfield_values('a'):
                v = v.strip()
                if v:
                    v = remove_trailing_dot(v).strip()
                if v:
                    v = tidy_subject(v)
                if v:
                    subjects['org'][v] += 1
        elif tag == '611': # event
            v = ' '.join(j.strip() for i, j in f.get_all_subfields() if i not in 'vxyz')
            if v:
                v = v.strip()
            v = tidy_subject(v)
            if v:
                subjects['event'][v] += 1
        elif tag == '630': # work
            for v in f.get_subfield_values(['a']):
                v = v.strip()
                if v:
                    v = remove_trailing_dot(v).strip()
                if v:
                    v = tidy_subject(v)
                if v:
                    subjects['work'][v] += 1
        elif tag == '650': # topical
            for v in f.get_subfield_values(['a']):
                if v:
                    v = v.strip()
                v = tidy_subject(v)
                if v:
                    subjects['subject'][v] += 1
        elif tag == '651': # geo
            for v in f.get_subfield_values(['a']):
                if v:
                    subjects['place'][flip_place(v).strip()] += 1

        for v in f.get_subfield_values(['y']):
            v = v.strip()
            if v:
                subjects['time'][remove_trailing_dot(v).strip()] += 1
        for v in f.get_subfield_values(['v']):
            v = v.strip()
            if v:
                v = remove_trailing_dot(v).strip()
            v = tidy_subject(v)
            if v:
                subjects['subject'][v] += 1
        for v in f.get_subfield_values(['z']):
            v = v.strip()
            if v:
                subjects['place'][flip_place(v).strip()] += 1
        for v in f.get_subfield_values(['x']):
            v = v.strip()
            if not v:
                continue
            if aspects and re_aspects.search(v):
                continue
            v = tidy_subject(v)
            if v:
                subjects['subject'][v] += 1

    return dict((k, dict(v)) for k, v in subjects.items())

def subjects_for_work(rec):
    field_map = {
        'subject': 'subjects',
        'place': 'subject_places',
        'time': 'subject_times',
        'person': 'subject_people',
    }

    subjects = four_types(read_subjects(rec))

    return dict((field_map[k], v.keys()) for k, v in subjects.items())



########NEW FILE########
__FILENAME__ = has_001
#!/usr/bin/python2.5
from catalog.marc.fast_parse import *
from catalog.read_rc import read_rc
from catalog.get_ia import files
from sources import sources
import sys, os

rc = read_rc()
read_count = 10000

show_bad_records = False

for ia, name in sources(): # find which sources include '001' tag
    has_001 = 0
    rec_no = 0
    for part, size in files(ia):
        filename = rc['marc_path'] + ia + "/" + part
        if not os.path.exists(filename):
            continue
        for data, length in read_file(open(filename)):
            if rec_no == read_count:
                break
            rec_no += 1
            if list(get_tag_lines(data, ['001'])):
                has_001 += 1
            elif show_bad_records:
                print data[:24]
                for tag, line in get_all_tag_lines(data):
                    if tag.startswith('00'):
                        print tag, line[:-1]
                    else:
                        print tag, list(get_all_subfields(line))
        if rec_no == read_count:
            break
    print "%5d %s %s" % (has_001, ia, name)
    continue

########NEW FILE########
__FILENAME__ = html
from openlibrary.catalog.marc.fast_parse import get_all_tag_lines, translate, split_line
import re

trans = {'&':'&amp;','<':'&lt;','>':'&gt;','\n':'<br>', '\x1b': '<b>[esc]</b>'}
re_html_replace = re.compile('([&<>\n\x1b])')

def esc(s):
    return re_html_replace.sub(lambda m: trans[m.group(1)], s)

def esc_sp(s):
    return esc(s).replace(' ', '&nbsp;')

class html_record():
    def __init__(self, data):
        assert len(data) == int(data[:5])
        self.data = data
        self.leader = data[:24]
        self.is_marc8 = data[9] != 'a'
    def html(self):
        return '<br>\n'.join(self.html_line(t, l) for t, l in get_all_tag_lines(self.data))

    def html_subfields(self, line):
        assert line[-1] == '\x1e'
        encode = {
            'k': lambda s: '<b>$%s</b>' % esc(translate(s, self.is_marc8)),
            'v': lambda s: esc(translate(s, self.is_marc8)),
        }
        return ''.join(encode[k](v) for k, v in split_line(line[2:-1]))

    def html_line(self, tag, line):
        if tag.startswith('00'):
            s = esc_sp(line[:-1])
        else:
            s = esc_sp(line[0:2]) + ' ' + self.html_subfields(line)
        return u'<large>' + tag + u'</large> <code>' + s + u'</code>'

def test_html_subfields():
    samples = [
        ('  \x1fa0123456789\x1e', '<b>$a</b>0123456789'),
        ('  end of wrapped\x1e', 'end of wrapped'),
        ('  \x1fa<whatever>\x1e', '<b>$a</b>&lt;whatever&gt;'),
    ]
    for input, output in samples:
        assert html_subfields(input) == output

def test_html_line():
    samples = [
        ('020', '  \x1fa0123456789\x1e', '&nbsp;&nbsp; <b>$a</b>0123456789'),
        ('520', '  end of wrapped\x1e', '&nbsp;&nbsp; end of wrapped'),
        ('245', '10\x1faDbu ma la \xca\xbejug pa\xca\xbei kar t\xcc\xa3i\xcc\x84k :\x1fbDwags-brgyud grub pa\xca\xbei s\xcc\x81in\xcc\x87 rta /\x1fcKarma-pa Mi-bskyod-rdo-rje.\x1e', u'10 <b>$a</b>Dbu ma la \u02bejug pa\u02bei kar \u1e6d\u012bk :<b>$b</b>Dwags-brgyud grub pa\u02bei \u015bi\u1e45 rta /<b>$c</b>Karma-pa Mi-bskyod-rdo-rje.'),
    ]

    for tag, input, output in samples:
        expect = '<large>%s</large> <code>%s</code>' % (tag, output)
        assert html_line(tag, input) == expect

########NEW FILE########
__FILENAME__ = lang
from catalog.infostore import get_site
import web

site = get_site()
lang = set(site.things({'type': '/type/language'}))

def add_lang(edition):
    if 'languages' not in edition:
        return
    key = edition['languages'][0]['key']
    if key in ('/l/   ', '/l/|||'):
        del edition['languages']
    elif key not in lang:
        del edition['languages']

########NEW FILE########
__FILENAME__ = load
#!/usr/bin/python

from catalog.marc.parse import parser
from catalog.merge.names import flip_marc_name
from time import time
import codecs, shelve, re, sys
import catalog.marc.MARC21
import psycopg2

skip_keys = set(['key', 'comment', 'type', 'machine_comment', 'create', 'authors'])

conn = psycopg2.connect("dbname='infobase_data' user='edward' password=''");
cur = conn.cursor()

def sql_get_val (sql):
    cur.execute(sql)
    rows = cur.fetchall()
    return rows[0][0]

def sql_do (sql):
    cur.execute(sql)

thing_id = sql_get_val("select max(id) from thing")
version_id = sql_get_val("select max(id) from version")

edition_type = str(sql_get_val("select id from thing where key='/type/edition'"))
author_type = str(sql_get_val("select id from thing where key='/type/author'"))
author_key = {}

re_escape = re.compile(r'[\n\r\t\0\\]')
trans = { '\n': '\\n', '\r': '\\r', '\t': '\\t', '\\': '\\\\', '\0': '', }

def esc_group(m):
    return trans[m.group(0)]

def esc(str): return re_escape.sub(esc_group, str)

type_map = {
    'description': 'text',
    'notes': 'text',
    'number_of_pages': 'int',
    'url': 'uri',
}

type_map2 = { 'text': '3', 'int': '6', 'uri': '4' }

pharos = "/1/pharos/"
files = [ "marc_records_scriblio_net/part%02d.dat" % x for x in range(1,30) ]
files.reverse()
edition_num = 0
t0 = time()
total = 7022999
books = []
progress = open('progress', 'w')

author_num = 0

author_cache = {}

def import_author(author):
    global author_num
    name = unicode(author["db_name"].replace("\0", ""))
    if name in author_cache:
        return { 'key': author_cache[name] }
    else:
        author_num+=1
        key = '/a/OL%dA' % author_num
        a = {
            'create': 'unless_exists',
            'comment': 'initial import',
            'type': { 'key': '/type/author' },
            'key': '/a/OL%dA' % author_num,
            'name': author['name']
        }
        for f in 'title', 'personal_name', 'enumeration', 'birth_date', 'death_date':
            if f in author:
                a[f] = author[f]
        author_cache[name] = key
        return a

def write_edition(edition_data, edition_num):
    global books
    book_key = '/b/OL%dM' % edition_num
    book = {
        'create': 'unless_exists',
        'comment': 'initial import',
        'machine_comment': edition_data['source_record_loc'],
        'type': { 'key': '/type/edition' },
        'key': book_key
    }
    for k, v in edition_data.iteritems():
        if k == 'edition' and v == '':
            continue
        if k == 'source_record_loc' or k.startswith('language'):
            continue
        if k == 'authors':
            book[k] = [import_author(v[0])]
            continue
        if k in type_map:
            t = '/type/' + type_map[k]
            if isinstance(v, list):
                book[k] = [{'type': t, 'value': i} for i in v]
            else:
                book[k] = {'type': t, 'value': v}
        else:
            book[k] = v
    books.append(book)
    if edition_num % 1000 == 0:
        t1 = time() - t0
        remaining = total - edition_num
        print "%9d books loaded. running: %.1f hours. %.1f rec/sec. %.1f hours left: %s %s" % (edition_num, t1/3600, edition_num/t1, ((t1/edition_num) * remaining) / 3600, edition_data['source_record_loc'], `edition_data['title']`)
        progress.write(`edition_num, int(t1), edition_data['source_record_loc'], edition_data['title']` + "\n")
        progress.flush()
    if edition_num % 10000 == 0:
        bulk_load(books)
        books = []

bad_data_file = open('bad_data', 'w')
def bad_data(record_source_loc):
    bad_data_file.write(record_source_loc + "\n")

def write_datum(datum, thing_id, k, v, num):
    if isinstance(v, dict):
        data_type = type_map2[v['type'][6:]]
        v = v['value']
    else:
        data_type = '2' # string
    datum.write("\t".join((str(thing_id), '1', '\N', k, esc(unicode(v)), data_type, num)) + "\n")

def bulk_load(books):
    global thing_id, version_id
    thing = codecs.open("/var/tmp/thing", "w", "utf-8")
    version = codecs.open("/var/tmp/version", "w", "utf-8")
    datum = codecs.open("/var/tmp/datum", "w", "utf-8")

    for edition in books:
        thing_id += 1
        version_id += 1
        thing.write("\t".join((str(thing_id), '1', edition['key'], '1', '\N', '\N', '\N')) + "\n");
        version.write("\t".join((str(version_id), str(thing_id), '1', '\N', '\N', edition['comment'], edition['machine_comment'], '\N')) + "\n");
        datum.write("\t".join((str(thing_id), '1', '\N', 'type', edition_type, '0', '\N')) + "\n")

        for k, v in edition.iteritems():
            if k in skip_keys:
                continue
            if isinstance(v, list):
                for a, b in enumerate(v):
                    write_datum(datum, thing_id, k, b, str(a))
            else:
                write_datum(datum, thing_id, k, v, '\N')

        if 'authors' in edition:
            edition_thing_id = thing_id
            num = 0
            for author in edition['authors']:
                if len(author) == 1:
                    datum.write("\t".join((str(edition_thing_id), '1', '\N', 'authors', str(author_key[author['key']]), '0', str(num))) + "\n")
                    num+=1
                    continue
                thing_id += 1
                author_key[author['key']] = thing_id
                version_id += 1
                datum.write("\t".join((str(edition_thing_id), '1', '\N', 'authors', str(thing_id), '0', str(num))) + "\n")
                num += 1
                thing.write("\t".join((str(thing_id), '1', author['key'], '1', '\N', '\N', '\N')) + "\n");
                version.write("\t".join((str(version_id), str(thing_id), '1', '\N', '\N', author['comment'], '\N', '\N')) + "\n");
                datum.write("\t".join((str(thing_id), '1', '\N', 'type', author_type, '0', '\N')) + "\n")
                for k, v in author.iteritems():
                    if k in skip_keys:
                        continue
                    datum.write("\t".join((str(thing_id), '1', '\N', k, esc(unicode(v)), '2', '\N')) + "\n")


    thing.close()
    version.close()
    datum.close()
    print "loading batch of editions into database"
    sql_do("copy thing from '/var/tmp/thing'")
    sql_do("copy version from '/var/tmp/version'")
    sql_do("copy datum from '/var/tmp/datum'")
    conn.commit()

    print "batch of editions loaded"

edition_num = 0
for file_locator in files:
    input = open(pharos + file_locator)
    try:
        for edition_data in parser(file_locator, input, bad_data):
            edition_num+=1
            write_edition(edition_data, edition_num)
    except catalog.marc.MARC21.MARC21Exn:
        pass
    input.close()

bad_data_file.close()
progress.close()
t1 = time() - t0
print "total run time: %.1f" % (t1/3600)

########NEW FILE########
__FILENAME__ = lookup
import web
import dbhash

dbm = dbhash.open("/2/pharos/imagepdfs/oclc_to_marc.dbm", "r")

urls = (
    '/', 'index'
)

class index:
    def GET(self):
        web.header('Content-Type','text/html; charset=utf-8', unique=True)

        input = web.input()
        if 'oclc' in input:
            html_oclc = web.htmlquote(input.oclc)
            print "<html>\n<head><title>OCLC to MARC: %s</title><body>" % html_oclc
        else:
            print "<html>\n<head><title>OCLC to MARC</title><body>"
        print '<form method="get">'
        print 'OCLC:'
        if 'oclc' in input:
            print '<input type="text" name="oclc" value="%s">' % html_oclc
        else:
            print '<input type="text" name="oclc">'
        print '<input type="submit" value="Find MARC">'
        print '</form>'

        if 'oclc' in input:
            print 'Searching for OCLC: %s<p>' % html_oclc
            if input.oclc in dbm:
                loc = dbm[input.oclc]
                print '<ul>'
                for l in loc.split(' '):
                    print '<li><a href="http://openlibrary.org/show-marc/%s">%s</a>' % (l, l)
                print '</ul>'
            else:
                print html_oclc, 'not found'

web.webapi.internalerror = web.debugerror

if __name__ == '__main__':
    web.run(urls, globals(), web.reloader)

########NEW FILE########
__FILENAME__ = marc_base
import re

re_isbn = re.compile('([^ ()]+[\dX])(?: \((?:v\. (\d+)(?: : )?)?(.*)\))?')
# handle ISBN like: 1402563884c$26.95
re_isbn_and_price = re.compile('^([-\d]+X?)c\$[\d.]+$')

class MarcBase(object):
    def read_isbn(self, f):
        found = []
        for k, v in f.get_subfields(['a', 'z']):
            m = re_isbn_and_price.match(v)
            if not m:
                m = re_isbn.match(v)
            if not m:
                continue
            found.append(m.group(1))
        return found

    def build_fields(self, want):
        self.fields = {}
        want = set(want)
        for tag, line in self.read_fields(want):
            self.fields.setdefault(tag, []).append(line)

    def get_fields(self, tag):
        return [self.decode_field(i) for i in self.fields.get(tag, [])]

########NEW FILE########
__FILENAME__ = marc_binary
from openlibrary.catalog.marc.fast_parse import get_tag_lines, handle_wrapped_lines, get_all_tag_lines
from openlibrary.catalog.marc import fast_parse
from marc_base import MarcBase
from unicodedata import normalize
from pymarc import MARC8ToUnicode
from openlibrary.catalog.marc import mnemonics

marc8 = MARC8ToUnicode(quiet=True)


def norm(s):
    return normalize('NFC', unicode(s))

class BinaryDataField():
    def __init__(self, rec, line):
        self.rec = rec
        if line:
            while line[-2] == '\x1e': # ia:engineercorpsofhe00sher
                line = line[:-1]
        self.line = line

    def translate(self, data):
        utf8 = self.rec.leader()[9] == 'a'
        if utf8:
            try:
                data = data.decode('utf-8')
            except:
                utf8 = False
        if not utf8:
            data = mnemonics.read(data)
            data = marc8.translate(data)
        data = normalize('NFC', data)
        return data

    def ind1(self):
        return self.line[0]

    def ind2(self):
        return self.line[1]

    def remove_brackets(self):
        line = self.line
        if line[4] == '[' and line[-2] == ']':
            self.line = line[0:4] + line[5:-2] + line[-1]

    def get_subfields(self, want):
        want = set(want)
        for i in self.line[3:-1].split('\x1f'):
            if i and i[0] in want:
                yield i[0], self.translate(i[1:])

    def get_contents(self, want):
        contents = {}
        for k, v in self.get_subfields(want):
            if v:
                contents.setdefault(k, []).append(v)
        return contents

    def get_subfield_values(self, want):
        return [v for k, v in self.get_subfields(want)]

    def get_all_subfields(self):
        return fast_parse.get_all_subfields(self.line, self.rec.leader()[9] != 'a')

    def get_all_subfields(self):
        for i in self.line[3:-1].split('\x1f'):
            if i:
                j = self.translate(i)
                yield j[0], j[1:]

    def get_lower_subfields(self):
        for k, v in self.get_all_subfields():
            if k.islower():
                yield v

#class BinaryIaDataField(BinaryDataField):
#    def translate(self, data):
#        return fast_parse.translate(data, bad_ia_charset=True)

class MarcBinary(MarcBase):
    def __init__(self, data):
        assert len(data) and isinstance(data, basestring)
        self.data = data

    def leader(self):
        return self.data[:24]

    def all_fields(self):
        marc8 = self.leader()[9] != 'a'
        for tag, line in handle_wrapped_lines(get_all_tag_lines(self.data)):
            if tag.startswith('00'):
                # marc_upei/marc-for-openlibrary-bigset.mrc:78997353:588
                if tag == '008' and line == '':
                    continue
                assert line[-1] == '\x1e'
                yield tag, line[:-1]
            else:
                yield tag, BinaryDataField(self, line)

    def read_fields(self, want):
        want = set(want)
        marc8 = self.leader()[9] != 'a'
        #for tag, line in handle_wrapped_lines(get_tag_lines(self.data, want)):
        for tag, line in handle_wrapped_lines(get_tag_lines(self.data, want)):
            if tag not in want:
                continue
            if tag.startswith('00'):
                # marc_upei/marc-for-openlibrary-bigset.mrc:78997353:588
                if tag == '008' and line == '':
                    continue
                assert line[-1] == '\x1e'
                yield tag, line[:-1]
            else:
                yield tag, BinaryDataField(self, line)

    def decode_field(self, field):
        return field # noop on MARC binary

    def read_isbn(self, f):
        if '\x1f' in f.line:
            return super(MarcBinary, self).read_isbn(f)
        else:
            m = re_isbn.match(line[3:-1])
            if m:
                return [m.group(1)]
        return []

########NEW FILE########
__FILENAME__ = marc_subject
from openlibrary.catalog.utils import remove_trailing_dot, remove_trailing_number_dot, flip_name
import re
from collections import defaultdict
from openlibrary.catalog.get_ia import get_from_archive, marc_formats, urlopen_keep_trying
from openlibrary.catalog.marc.marc_binary import MarcBinary
from openlibrary.catalog.importer.db_read import get_mc
from openlibrary.catalog.marc.marc_xml import BadSubtag, BlankTag
from openlibrary.catalog.marc.marc_xml import read_marc_file, MarcXml, BlankTag, BadSubtag
from lxml import etree

subject_fields = set(['600', '610', '611', '630', '648', '650', '651', '662'])

re_flip_name = re.compile('^(.+), ([A-Z].+)$')

# 'Rhodes, Dan (Fictitious character)'
re_fictitious_character = re.compile('^(.+), (.+)( \(.* character\))$')
re_etc = re.compile('^(.+?)[, .]+etc[, .]?$', re.I)
re_comma = re.compile('^([A-Z])([A-Za-z ]+?) *, ([A-Z][A-Z a-z]+)$')

re_place_comma = re.compile('^(.+), (.+)$')
re_paren = re.compile('[()]')
def flip_place(s):
    s = remove_trailing_dot(s)
    # Whitechapel (London, England)
    # East End (London, England)
    # Whitechapel (Londres, Inglaterra)
    if re_paren.search(s):
        return s
    m = re_place_comma.match(s)
    return m.group(2) + ' ' + m.group(1) if m else s

def flip_subject(s):
    m = re_comma.match(s)
    if m:
        return m.group(3) + ' ' + m.group(1).lower()+m.group(2)
    else:
        return s

def four_types(i):
    want = set(['subject', 'time', 'place', 'person'])
    ret = dict((k, i[k]) for k in want if k in i)
    for j in (j for j in i.keys() if j not in want):
        for k, v in i[j].items():
            if 'subject' in ret:
                ret['subject'][k] = ret['subject'].get(k, 0) + v
            else:
                ret['subject'] = {k: v}
    return ret

archive_url = "http://archive.org/download/"

def bad_marc_alert(ia):
    from pprint import pformat
    msg_from = 'load_scribe@archive.org'
    msg_to = 'edward@archive.org'
    msg = '''\
From: %s
To: %s
Subject: bad MARC: %s

bad MARC: %s

''' % (msg_from, msg_to, ia, ia)

    import smtplib
    server = smtplib.SMTP('mail.archive.org')
    server.sendmail(msg_from, [msg_to], msg)
    server.quit()

def load_binary(ia):
    url = archive_url + ia + '/' + ia + '_meta.mrc'
    f = urlopen_keep_trying(url)
    data = f.read()
    assert '<title>Internet Archive: Page Not Found</title>' not in data[:200]
    if len(data) != int(data[:5]):
        data = data.decode('utf-8').encode('raw_unicode_escape')
    if len(data) != int(data[:5]):
        bad_marc_alert(ia)
        return
    return MarcBinary(data)

def load_xml(ia):
    url = archive_url + ia + '/' + ia + '_marc.xml'
    f = urlopen_keep_trying(url)
    root = etree.parse(f).getroot()
    if root.tag == '{http://www.loc.gov/MARC21/slim}collection':
        root = root[0]
    return MarcXml(root)

def subjects_for_work(rec):
    field_map = {
        'subject': 'subjects',
        'place': 'subject_places',
        'time': 'subject_times',
        'person': 'subject_people',
    }

    subjects = four_types(read_subjects(rec))

    return dict((field_map[k], v.keys()) for k, v in subjects.items())

re_edition_key = re.compile(r'^/(?:b|books)/(OL\d+M)$')

def get_subjects_from_ia(ia):
    formats = marc_formats(ia)
    if not any(formats.values()):
        return {}
    rec = None
    if formats['bin']:
        rec = load_binary(ia) 
    if not rec:
        assert formats['xml']
        rec = load_xml(ia)
    return read_subjects(rec)

def bad_source_record(e, sr):
    from pprint import pformat
    import smtplib
    msg_from = 'marc_subject@archive.org'
    msg_to = 'edward@archive.org'
    msg = '''\
From: %s
To: %s
Subject: bad source record: %s

Bad source record: %s

%s
''' % (msg_from, msg_to, e['key'], sr, pformat(e))

    server = smtplib.SMTP('mail.archive.org')
    server.sendmail(msg_from, [msg_to], msg)
    server.quit()

re_ia_marc = re.compile('^(?:.*/)?([^/]+)_(marc\.xml|meta\.mrc)(:0:\d+)?$')
def get_work_subjects(w, do_get_mc=True):
    found = set()
    for e in w['editions']:
        sr = e.get('source_records', [])
        if sr:
            for i in sr:
                if i.endswith('initial import'):
                    bad_source_record(e, i)
                    continue
                if i.startswith('ia:') or i.startswith('marc:'):
                    found.add(i)
                    continue
        else:
            mc = None
            if do_get_mc:
                m = re_edition_key.match(e['key'])
                mc = get_mc('/b/' + m.group(1))
            if mc:
                if mc.endswith('initial import'):
                    bad_source_record(e, mc)
                    continue
                if not mc.startswith('amazon:') and not re_ia_marc.match(mc):
                    found.add('marc:' + mc)
    subjects = []
    for sr in found:
        if sr.startswith('marc:ia:'):
            subjects.append(get_subjects_from_ia(sr[8:]))
        elif sr.startswith('marc:'):
            loc = sr[5:]
            data = get_from_archive(loc)
            rec = MarcBinary(data)
            try:
                subjects.append(read_subjects(rec))
            except:
                print 'bad MARC:', loc
                print 'data:', `data`
                raise
        else:
            assert sr.startswith('ia:')
            subjects.append(get_subjects_from_ia(sr[3:]))
    return combine_subjects(subjects)

def tidy_subject(s):
    s = s.strip()
    if len(s) < 2:
        print 'short subject:', `s`
    else:
        s = s[0].upper() + s[1:]
    m = re_etc.search(s)
    if m:
        return m.group(1)
    s = remove_trailing_dot(s)
    m = re_fictitious_character.match(s)
    if m:
        return m.group(2) + ' ' + m.group(1) + m.group(3)
    m = re_comma.match(s)
    if m:
        return m.group(3) + ' ' + m.group(1) + m.group(2)
    return s

re_aspects = re.compile(' [Aa]spects$')
def find_aspects(f):
    cur = [(i, j) for i, j in f.get_subfields('ax')]
    if len(cur) < 2 or cur[0][0] != 'a' or cur[1][0] != 'x':
        return
    a, x = cur[0][1], cur[1][1]
    x = x.strip('. ')
    a = a.strip('. ')
    if not re_aspects.search(x):
        return
    if a == 'Body, Human':
        a = 'the Human body'
    return x + ' of ' + flip_subject(a)

def read_subjects(rec):
    subjects = defaultdict(lambda: defaultdict(int))
    for tag, field in rec.read_fields(subject_fields):
        f = rec.decode_field(field)
        aspects = find_aspects(f)

        if tag == '600': # people
            name_and_date = []
            for k, v in f.get_subfields(['a', 'b', 'c', 'd']):
                v = '(' + v.strip('.() ') + ')' if k == 'd' else v.strip(' /,;:')
                if k == 'a':
                    m = re_flip_name.match(v)
                    if m:
                        v = flip_name(v)
                name_and_date.append(v)
            name = remove_trailing_dot(' '.join(name_and_date)).strip()
            if name != '':
                subjects['person'][name] += 1
        elif tag == '610': # org
            v = ' '.join(f.get_subfield_values('abcd'))
            v = v.strip()
            if v:
                v = remove_trailing_dot(v).strip()
            if v:
                v = tidy_subject(v)
            if v:
                subjects['org'][v] += 1

            for v in f.get_subfield_values('a'):
                v = v.strip()
                if v:
                    v = remove_trailing_dot(v).strip()
                if v:
                    v = tidy_subject(v)
                if v:
                    subjects['org'][v] += 1
        elif tag == '611': # event
            v = ' '.join(j.strip() for i, j in f.get_all_subfields() if i not in 'vxyz')
            if v:
                v = v.strip()
            v = tidy_subject(v)
            if v:
                subjects['event'][v] += 1
        elif tag == '630': # work
            for v in f.get_subfield_values(['a']):
                v = v.strip()
                if v:
                    v = remove_trailing_dot(v).strip()
                if v:
                    v = tidy_subject(v)
                if v:
                    subjects['work'][v] += 1
        elif tag == '650': # topical
            for v in f.get_subfield_values(['a']):
                if v:
                    v = v.strip()
                v = tidy_subject(v)
                if v:
                    subjects['subject'][v] += 1
        elif tag == '651': # geo
            for v in f.get_subfield_values(['a']):
                if v:
                    subjects['place'][flip_place(v).strip()] += 1

        for v in f.get_subfield_values(['y']):
            v = v.strip()
            if v:
                subjects['time'][remove_trailing_dot(v).strip()] += 1
        for v in f.get_subfield_values(['v']):
            v = v.strip()
            if v:
                v = remove_trailing_dot(v).strip()
            v = tidy_subject(v)
            if v:
                subjects['subject'][v] += 1
        for v in f.get_subfield_values(['z']):
            v = v.strip()
            if v:
                subjects['place'][flip_place(v).strip()] += 1
        for v in f.get_subfield_values(['x']):
            v = v.strip()
            if not v:
                continue
            if aspects and re_aspects.search(v):
                continue
            v = tidy_subject(v)
            if v:
                subjects['subject'][v] += 1

    return dict((k, dict(v)) for k, v in subjects.items())

def combine_subjects(subjects):
    all_subjects = defaultdict(lambda: defaultdict(int))
    for a in subjects:
        for b, c in a.items():
            for d, e in c.items():
                all_subjects[b][d] += e

    return dict((k, dict(v)) for k, v in all_subjects.items())

########NEW FILE########
__FILENAME__ = marc_xml
from lxml import etree
from marc_base import MarcBase
from unicodedata import normalize

data_tag = '{http://www.loc.gov/MARC21/slim}datafield'
control_tag = '{http://www.loc.gov/MARC21/slim}controlfield'
subfield_tag = '{http://www.loc.gov/MARC21/slim}subfield'
leader_tag = '{http://www.loc.gov/MARC21/slim}leader'
record_tag = '{http://www.loc.gov/MARC21/slim}record'

def read_marc_file(f):
    for event, elem in etree.iterparse(f, tag=record_tag):
        yield MarcXml(elem)
        elem.clear()

class BlankTag:
    pass

class BadSubtag:
    pass

def norm(s):
    return normalize('NFC', unicode(s.replace(u'\xa0', ' ')))

def get_text(e):
    return norm(e.text) if e.text else u''

class DataField:
    def __init__(self, element):
        assert element.tag == data_tag
        self.element = element

    def remove_brackets(self):
        f = self.element[0]
        l = self.element[-1]
        if f.text and l.text and f.text.startswith('[') and l.text.endswith(']'):
            f.text = f.text[1:]
            l.text = l.text[:-1]

    def ind1(self):
        return self.element.attrib['ind1']
    def ind2(self):
        return self.element.attrib['ind2']

    def read_subfields(self):
        for i in self.element:
            assert i.tag == subfield_tag
            k = i.attrib['code']
            if k == '':
                raise BadSubtag
            yield k, i

    def get_lower_subfields(self):
        for k, v in self.read_subfields():
            if k.islower():
                yield get_text(v)

    def get_all_subfields(self):
        for k, v in self.read_subfields():
            yield k, get_text(v)

    def get_subfields(self, want):
        want = set(want)
        for k, v in self.read_subfields():
            if k not in want:
                continue
            yield k, get_text(v)

    def get_subfield_values(self, want):
        return [v for k, v in self.get_subfields(want)]

    def get_contents(self, want):
        contents = {}
        for k, v in self.get_subfields(want):
            if v:
                contents.setdefault(k, []).append(v)
        return contents

class MarcXml(MarcBase):
    def __init__(self, record):
        assert record.tag == record_tag
        self.record = record

    def leader(self):
        leader_element = self.record[0]
        if not isinstance(leader_element.tag, str):
            leader_element = self.record[1]
        assert leader_element.tag == leader_tag
        return get_text(leader_element)

    def all_fields(self):
        for i in self.record:
            if i.tag != data_tag and i.tag != control_tag:
                continue
            if i.attrib['tag'] == '':
                raise BlankTag
            yield i.attrib['tag'], i

    def read_fields(self, want):
        want = set(want)

        # http://www.archive.org/download/abridgedacademy00levegoog/abridgedacademy00levegoog_marc.xml

        non_digit = False
        for i in self.record:
            if i.tag != data_tag and i.tag != control_tag:
                continue
            tag = i.attrib['tag']
            if tag == '':
                raise BlankTag
            if tag == 'FMT':
                continue
            if not tag.isdigit():
                non_digit = True
            else:
                if tag[0] != '9' and non_digit:
                    raise BadSubtag

            if i.attrib['tag'] not in want:
                continue
            yield i.attrib['tag'], i

    def decode_field(self, field):
        if field.tag == control_tag:
            return get_text(field)
        if field.tag == data_tag:
            return DataField(field)

########NEW FILE########
__FILENAME__ = mnemonics
# read MARC mnemonics
# result is in MARC8 and still needs to be converted to Unicode

import re

re_brace = re.compile('(\{.+?\})', re.U)

mapping = {
 '{00}': '\x00',
 '{01}': '\x01',
 '{02}': '\x02',
 '{03}': '\x03',
 '{04}': '\x04',
 '{05}': '\x05',
 '{06}': '\x06',
 '{07}': '\x07',
 '{08}': '\x08',
 '{09}': '\t',
 '{0A}': '\n',
 '{0B}': '\x0b',
 '{0C}': '\x0c',
 '{0D}': '\r',
 '{0E}': '\x0e',
 '{0F}': '\x0f',
 '{0}': '0',
 '{10}': '\x10',
 '{11}': '\x11',
 '{12}': '\x12',
 '{13}': '\x13',
 '{14}': '\x14',
 '{15}': '\x15',
 '{16}': '\x16',
 '{17}': '\x17',
 '{18}': '\x18',
 '{19}': '\x19',
 '{1A}': '\x1a',
 '{1B}': '\x1b',
 '{1C}': '\x1c',
 '{1D}': '\x1d',
 '{1E}': '\x1e',
 '{1F}': '\x1f',
 '{1}': '1',
 '{20}': ' ',
 '{21}': '!',
 '{22}': '"',
 '{23}': '#',
 '{24}': '$',
 '{25}': '%',
 '{26}': '&',
 '{27}': "'",
 '{28}': '(',
 '{29}': ')',
 '{2A}': '*',
 '{2B}': '+',
 '{2C}': ',',
 '{2D}': '-',
 '{2E}': '.',
 '{2F}': '/',
 '{2}': '2',
 '{30}': '0',
 '{31}': '1',
 '{32}': '2',
 '{33}': '3',
 '{34}': '4',
 '{35}': '5',
 '{36}': '6',
 '{37}': '7',
 '{38}': '8',
 '{39}': '9',
 '{3A}': ':',
 '{3B}': ';',
 '{3C}': '<',
 '{3D}': '=',
 '{3E}': '>',
 '{3F}': '?',
 '{3}': '3',
 '{40}': '@',
 '{41}': 'A',
 '{42}': 'B',
 '{43}': 'C',
 '{44}': 'D',
 '{45}': 'E',
 '{46}': 'F',
 '{47}': 'G',
 '{48}': 'H',
 '{49}': 'I',
 '{4A}': 'J',
 '{4B}': 'K',
 '{4C}': 'L',
 '{4D}': 'M',
 '{4E}': 'N',
 '{4F}': 'O',
 '{4}': '4',
 '{50}': 'P',
 '{51}': 'Q',
 '{52}': 'R',
 '{53}': 'S',
 '{54}': 'T',
 '{55}': 'U',
 '{56}': 'V',
 '{57}': 'W',
 '{58}': 'X',
 '{59}': 'Y',
 '{5A}': 'Z',
 '{5B}': '[',
 '{5C}': '\\',
 '{5D}': ']',
 '{5E}': '^',
 '{5F}': '_',
 '{5}': '5',
 '{60}': '`',
 '{61}': 'a',
 '{62}': 'b',
 '{63}': 'c',
 '{64}': 'd',
 '{65}': 'e',
 '{66}': 'f',
 '{67}': 'g',
 '{68}': 'h',
 '{69}': 'i',
 '{6A}': 'j',
 '{6B}': 'k',
 '{6C}': 'l',
 '{6D}': 'm',
 '{6E}': 'n',
 '{6F}': 'o',
 '{6}': '6',
 '{70}': 'p',
 '{71}': 'q',
 '{72}': 'r',
 '{73}': 's',
 '{74}': 't',
 '{75}': 'u',
 '{76}': 'v',
 '{77}': 'w',
 '{78}': 'x',
 '{79}': 'y',
 '{7A}': 'z',
 '{7B}': '{',
 '{7C}': '|',
 '{7D}': '}',
 '{7E}': '~',
 '{7F}': '\x7f',
 '{7}': '7',
 '{80}': '\x80',
 '{81}': '\x81',
 '{82}': '\x82',
 '{83}': '\x83',
 '{84}': '\x84',
 '{85}': '\x85',
 '{86}': '\x86',
 '{87}': '\x87',
 '{88}': '\x88',
 '{89}': '\x89',
 '{8A}': '\x8a',
 '{8B}': '\x8b',
 '{8C}': '\x8c',
 '{8D}': '\x8d',
 '{8E}': '\x8e',
 '{8F}': '\x8f',
 '{8}': '8',
 '{90}': '\x90',
 '{91}': '\x91',
 '{92}': '\x92',
 '{93}': '\x93',
 '{94}': '\x94',
 '{95}': '\x95',
 '{96}': '\x96',
 '{97}': '\x97',
 '{98}': '\x98',
 '{99}': '\x99',
 '{9A}': '\x9a',
 '{9B}': '\x9b',
 '{9C}': '\x9c',
 '{9D}': '\x9d',
 '{9E}': '\x9e',
 '{9F}': '\x9f',
 '{9}': '9',
 '{A0}': '\xa0',
 '{A1}': '\xa1',
 '{A2}': '\xa2',
 '{A3}': '\xa3',
 '{A4}': '\xa4',
 '{A5}': '\xa5',
 '{A6}': '\xa6',
 '{A7}': '\xa7',
 '{A8}': '\xa8',
 '{A9}': '\xa9',
 '{AA}': '\xaa',
 '{AB}': '\xab',
 '{AC}': '\xac',
 '{AD}': '\xad',
 '{AElig}': '\xa5',
 '{AE}': '\xae',
 '{AF}': '\xaf',
 '{Aacute}': '\xe2A',
 '{Abreve}': '\xe6A',
 '{Acirc}': '\xe3A',
 '{Acy}': 'A',
 '{Agrave}': '\xe1A',
 '{Aogon}': '\xf1A',
 '{Aring}': '\xeaA',
 '{Atilde}': '\xe4A',
 '{Auml}': '\xe8A',
 '{A}': 'A',
 '{B0}': '\xb0',
 '{B1}': '\xb1',
 '{B2}': '\xb2',
 '{B3}': '\xb3',
 '{B4}': '\xb4',
 '{B5}': '\xb5',
 '{B6}': '\xb6',
 '{B7}': '\xb7',
 '{B8}': '\xb8',
 '{B9}': '\xb9',
 '{BA}': '\xba',
 '{BB}': '\xbb',
 '{BC}': '\xbc',
 '{BD}': '\xbd',
 '{BE}': '\xbe',
 '{BF}': '\xbf',
 '{Bcy}': 'B',
 '{B}': 'B',
 '{C0}': '\xc0',
 '{C1}': '\xc1',
 '{C2}': '\xc2',
 '{C3}': '\xc3',
 '{C4}': '\xc4',
 '{C5}': '\xc5',
 '{C6}': '\xc6',
 '{C7}': '\xc7',
 '{C8}': '\xc8',
 '{C9}': '\xc9',
 '{CA}': '\xca',
 '{CB}': '\xcb',
 '{CC}': '\xcc',
 '{CD}': '\xcd',
 '{CE}': '\xce',
 '{CF}': '\xcf',
 '{CHcy}': 'Ch',
 '{Cacute}': '\xe2C',
 '{Ccaron}': '\xe9C',
 '{Ccedil}': '\xf0C',
 '{C}': 'C',
 '{D0}': '\xd0',
 '{D1}': '\xd1',
 '{D2}': '\xd2',
 '{D3}': '\xd3',
 '{D4}': '\xd4',
 '{D5}': '\xd5',
 '{D6}': '\xd6',
 '{D7}': '\xd7',
 '{D8}': '\xd8',
 '{D9}': '\xd9',
 '{DA}': '\xda',
 '{DB}': '\xdb',
 '{DC}': '\xdc',
 '{DD}': '\xdd',
 '{DE}': '\xde',
 '{DF}': '\xdf',
 '{DJEcy}': '\xa3',
 '{DZEcy}': 'Dz',
 '{DZHEcy}': 'D\xe9z',
 '{Dagger}': '|',
 '{Dcaron}': '\xe9D',
 '{Dcy}': 'D',
 '{Dstrok}': '\xa3',
 '{D}': 'D',
 '{E0}': '\xe0',
 '{E1}': '\xe1',
 '{E2}': '\xe2',
 '{E3}': '\xe3',
 '{E4}': '\xe4',
 '{E5}': '\xe5',
 '{E6}': '\xe6',
 '{E7}': '\xe7',
 '{E8}': '\xe8',
 '{E9}': '\xe9',
 '{EA}': '\xea',
 '{EB}': '\xeb',
 '{EC}': '\xec',
 '{ED}': '\xed',
 '{EE}': '\xee',
 '{EF}': '\xef',
 '{ETH}': '\xa3',
 '{Eacute}': '\xe2E',
 '{Ecaron}': '\xe9E',
 '{Ecirc}': '\xe3E',
 '{Ecy}': '\xe7E',
 '{Egrave}': '\xe1E',
 '{Ehookr}': '\xf1E',
 '{Eogon}': '\xf1E',
 '{Euml}': '\xe8E',
 '{E}': 'E',
 '{F0}': '\xf0',
 '{F1}': '\xf1',
 '{F2}': '\xf2',
 '{F3}': '\xf3',
 '{F4}': '\xf4',
 '{F5}': '\xf5',
 '{F6}': '\xf6',
 '{F7}': '\xf7',
 '{F8}': '\xf8',
 '{F9}': '\xf9',
 '{FA}': '\xfa',
 '{FB}': '\xfb',
 '{FC}': '\xfc',
 '{FD}': '\xfd',
 '{FE}': '\xfe',
 '{FF}': '\xff',
 '{Fcy}': 'F',
 '{F}': 'F',
 '{GEcy}': 'G',
 '{GHcy}': 'G',
 '{GJEcy}': '\xe2G',
 '{Gcy}': 'G',
 '{G}': 'G',
 '{HARDcy}': '\xb7',
 '{Hcy}': 'H',
 '{H}': 'H',
 '{IEcy}': '\xebI\xecE',
 '{IJlig}': 'IJ',
 '{IOcy}': '\xebI\xecO',
 '{IYcy}': 'Y',
 '{Iacute}': '\xe2I',
 '{Icaron}': '\xe9I',
 '{Icirc}': '\xe3I',
 '{Icy}': 'I',
 '{Idot}': '\xe7I',
 '{Igrave}': '\xe1I',
 '{Iumlcy}': '\xe8I',
 '{Iuml}': '\xe8I',
 '{I}': 'I',
 '{JEcy}': 'J',
 '{JIcy}': '\xe8I',
 '{Jcy}': '\xe6I',
 '{J}': 'J',
 '{KHcy}': 'Kh',
 '{KJEcy}': '\xe2K',
 '{Kcy}': 'K',
 '{K}': 'K',
 '{LJEcy}': 'Lj',
 '{Lacute}': '\xe2L',
 '{Lcy}': 'L',
 '{Lstrok}': '\xa1',
 '{L}': 'L',
 '{Mcy}': 'M',
 '{M}': 'M',
 '{NJEcy}': 'Nj',
 '{Nacute}': '\xe2N',
 '{Ncaron}': '\xe9N',
 '{Ncy}': 'N',
 '{No}': 'No.',
 '{Ntilde}': '\xb4N',
 '{N}': 'N',
 '{OElig}': '\xa6',
 '{Oacute}': '\xe2O',
 '{Ocirc}': '\xe3O',
 '{Ocy}': 'O',
 '{Odblac}': '\xeeO',
 '{Ograve}': '\xe1O',
 '{Ohorn}': '\xac',
 '{Ostrok}': '\xa2',
 '{Otilde}': '\xe4O',
 '{Ouml}': '\xe8O',
 '{O}': 'O',
 '{Pcy}': 'P',
 '{P}': 'P',
 '{Q}': 'Q',
 '{Racute}': '\xe2R',
 '{Rcaron}': '\xe9R',
 '{Rcy}': 'R',
 '{R}': 'R',
 '{SHCHcy}': 'Shch',
 '{SHcy}': 'Sh',
 '{SOFTcy}': '\xa7',
 '{Sacute}': '\xe2S',
 '{Scommab}': '\xf7S',
 '{Scy}': 'S',
 '{S}': 'S',
 '{THORN}': '\xa4',
 '{TSHEcy}': '\xe2C',
 '{TScy}': '\xebT\xecS',
 '{Tcaron}': '\xe9T',
 '{Tcommab}': '\xf7T',
 '{Tcy}': 'T',
 '{T}': 'T',
 '{Uacute}': '\xe2U',
 '{Ubrevecy}': '\xe6U',
 '{Ucirc}': '\xe3U',
 '{Ucy}': 'U',
 '{Udblac}': '\xeeU',
 '{Ugrave}': '\xe1U',
 '{Uhorn}': '\xad',
 '{Uring}': '\xeaU',
 '{Uuml}': '\xe8U',
 '{U}': 'U',
 '{Vcy}': 'V',
 '{V}': 'V',
 '{W}': 'W',
 '{X}': 'X',
 '{YAcy}': '\xebI\xecA',
 '{YEcy}': 'E',
 '{YIcy}': 'I',
 '{YUcy}': '\xebI\xecU',
 '{Yacute}': '\xe2Y',
 '{Ycy}': 'Y',
 '{Y}': 'Y',
 '{ZHcy}': 'Zh',
 '{ZHuacy}': '\xebZ\xech',
 '{Zacute}': '\xe2Z',
 '{Zcy}': 'Z',
 '{Zdot}': '\xe7Z',
 '{Z}': 'Z',
 '{aacute}': '\xe2a',
 '{abreve}': '\xe6a',
 '{acirc}': '\xe3a',
 '{acute}': '\xe2',
 '{acy}': 'a',
 '{aelig}': '\xb5',
 '{agrave}': '\xe1a',
 '{agr}': 'b',
 '{alif}': '\xae',
 '{amp}': '&',
 '{aogon}': '\xf1a',
 '{apos}': "'",
 '{arab}': '(3',
 '{aring}': '\xeaa',
 '{ast}': '*',
 '{asuper}': 'a',
 '{atilde}': '\xe4a',
 '{auml}': '\xe8a',
 '{ayn}': '\xb0',
 '{a}': 'a',
 '{bcy}': 'b',
 '{bgr}': 'c',
 '{breveb}': '\xf9',
 '{breve}': '\xe6',
 '{brvbar}': '|',
 '{bsol}': '\\',
 '{bull}': '*',
 '{b}': 'b',
 '{cacute}': '\xe2c',
 '{candra}': '\xef',
 '{caron}': '\xe9',
 '{ccaron}': '\xe9c',
 '{ccedil}': '\xf0c',
 '{cedil}': '\xf0',
 '{cent}': 'c',
 '{chcy}': 'ch',
 '{circb}': '\xf4',
 '{circ}': '\xe3',
 '{cjk}': '$1',
 '{colon}': ':',
 '{commaa}': '\xfe',
 '{commab}': '\xf7',
 '{commat}': '@',
 '{comma}': ',',
 '{copy}': '\xc3',
 '{curren}': '*',
 '{cyril}': '(N',
 '{c}': 'c',
 '{dagger}': '|',
 '{dblac}': '\xee',
 '{dbldotb}': '\xf3',
 '{dblunder}': '\xf5',
 '{dcaron}': '\xe9d',
 '{dcy}': 'd',
 '{deg}': '\xc0',
 '{diaer}': '\xe8',
 '{divide}': '/',
 '{djecy}': '\xb3',
 '{dollar}': '$',
 '{dotb}': '\xf2',
 '{dot}': '\xe7',
 '{dstrok}': '\xb3',
 '{dzecy}': 'dz',
 '{dzhecy}': 'd\xe9z',
 '{d}': 'd',
 '{eacute}': '\xe2e',
 '{ea}': '\xea',
 '{ecaron}': '\xe9e',
 '{ecirc}': '\xe3e',
 '{ecy}': '\xe7e',
 '{egrave}': '\xe1e',
 '{ehookr}': '\xf1e',
 '{eogon}': '\xf1e',
 '{equals}': '=',
 '{esc}': '\x1b',
 '{eth}': '\xba',
 '{euml}': '\xe8e',
 '{excl}': '!',
 '{e}': 'e',
 '{fcy}': 'f',
 '{flat}': '\xa9',
 '{fnof}': 'f',
 '{frac12}': '1/2',
 '{frac14}': '1/4',
 '{frac34}': '3/4',
 '{f}': 'f',
 '{gcy}': 'g',
 '{gecy}': 'g',
 '{ggr}': 'g',
 '{ghcy}': 'g',
 '{gjecy}': '\xe2g',
 '{grave}': '\xe1',
 '{greek}': 'g',
 '{gs}': '\x1d',
 '{gt}': '>',
 '{g}': 'g',
 '{hardcy}': '\xb7',
 '{hardsign}': '\xb7',
 '{hcy}': 'h',
 '{hebrew}': '(2',
 '{hellip}': '...',
 '{hooka}': '\xe0',
 '{hookl}': '\xf7',
 '{hookr}': '\xf1',
 '{hyphen}': '-',
 '{h}': 'h',
 '{iacute}': '\xe2i',
 '{icaron}': '\xe9i',
 '{icirc}': '\xe3i',
 '{icy}': 'i',
 '{iecy}': '\xebi\xece',
 '{iexcl}': '\xc6',
 '{igrave}': '\xe1i',
 '{ijlig}': 'ij',
 '{inodot}': '\xb8',
 '{iocy}': '\xebi\xeco',
 '{iquest}': '\xc5',
 '{iumlcy}': '\xe8i',
 '{iuml}': '\xe8i',
 '{iycy}': 'y',
 '{i}': 'i',
 '{jcy}': '\xe6i',
 '{jecy}': 'j',
 '{jicy}': '\xe8i',
 '{joiner}': '\x8d',
 '{j}': 'j',
 '{kcy}': 'k',
 '{khcy}': 'kh',
 '{kjecy}': '\xe2k',
 '{k}': 'k',
 '{lacute}': '\xe2l',
 '{laquo}': '"',
 '{latin}': '(B',
 '{lcub}': '{',
 '{lcy}': 'l',
 '{ldbltil}': '\xfa',
 '{ldquo}': '"',
 '{ljecy}': 'lj',
 '{llig}': '\xeb',
 '{lpar}': '(',
 '{lsqb}': '[',
 '{lsquor}': "'",
 '{lsquo}': "'",
 '{lstrok}': '\xb1',
 '{lt}': '<',
 '{l}': 'l',
 '{macr}': '\xe5',
 '{mcy}': 'm',
 '{mdash}': '--',
 '{middot}': '\xa8',
 '{mlPrime}': '\xb7',
 '{mllhring}': '\xb0',
 '{mlprime}': '\xa7',
 '{mlrhring}': '\xae',
 '{m}': 'm',
 '{nacute}': '\xe2n',
 '{ncaron}': '\xe9n',
 '{ncy}': 'n',
 '{ndash}': '--',
 '{njecy}': 'nj',
 '{nonjoin}': '\x8e',
 '{ntilde}': '\xb4n',
 '{num}': '#',
 '{n}': 'n',
 '{oacute}': '\xe2o',
 '{ocirc}': '\xe3o',
 '{ocy}': 'o',
 '{odblac}': '\xeeo',
 '{oelig}': '\xb6',
 '{ogon}': '\xf1',
 '{ograve}': '\xe1o',
 '{ohorn}': '\xbc',
 '{ordf}': 'a',
 '{ordm}': 'o',
 '{ostrok}': '\xb2',
 '{osuper}': 'o',
 '{otilde}': '\xe4o',
 '{ouml}': '\xe8o',
 '{o}': 'o',
 '{para}': '|',
 '{pcy}': 'p',
 '{percnt}': '%',
 '{period}': '.',
 '{phono}': '\xc2',
 '{pipe}': '|',
 '{plusmn}': '\xab',
 '{plus}': '+',
 '{pound}': '\xb9',
 '{p}': 'p',
 '{quest}': '?',
 '{quot}': '"',
 '{q}': 'q',
 '{racute}': '\xe2r',
 '{raquo}': '"',
 '{rcaron}': '\xe9r',
 '{rcedil}': '\xf8',
 '{rcommaa}': '\xed',
 '{rcub}': '}',
 '{rcy}': 'r',
 '{rdbltil}': '\xfb',
 '{rdquofh}': '"',
 '{rdquor}': '"',
 '{reg}': '\xaa',
 '{ringb}': '\xf4',
 '{ring}': '\xea',
 '{rlig}': '\xec',
 '{rpar}': ')',
 '{rsqb}': ']',
 '{rsquor}': "'",
 '{rsquo}': "'",
 '{rs}': '\x1e',
 '{r}': 'r',
 '{sacute}': '\xe2s',
 '{scommab}': '\xf7s',
 '{scriptl}': '\xc1',
 '{scy}': 's',
 '{sect}': '|',
 '{semi}': ';',
 '{sharp}': '\xc4',
 '{shchcy}': 'shch',
 '{shcy}': 'sh',
 '{shy}': '-',
 '{softcy}': '\xa7',
 '{softsign}': '\xa7',
 '{sol}': '/',
 '{space}': ' ',
 '{spcirc}': '^',
 '{spgrave}': '`',
 '{sptilde}': '~',
 '{spundscr}': '_',
 '{squf}': '|',
 '{sub}': 'b',
 '{sup1}': '\x1bp1\x1bs',
 '{sup2}': '\x1bp2\x1bs',
 '{sup3}': '\x1bp3\x1bs',
 '{super}': 'p',
 '{szlig}': 'ss',
 '{s}': 's',
 '{tcaron}': '\xe9t',
 '{tcommab}': '\xf7t',
 '{tcy}': 't',
 '{thorn}': '\xb4',
 '{tilde}': '\xe4',
 '{times}': 'x',
 '{trade}': '(Tm)',
 '{tscy}': '\xebt\xecs',
 '{tshecy}': '\xe2c',
 '{t}': 't',
 '{uacute}': '\xe2u',
 '{ubrevecy}': '\xe6u',
 '{ucirc}': '\xe3u',
 '{ucy}': 'u',
 '{udblac}': '\xeeu',
 '{ugrave}': '\xe1u',
 '{uhorn}': '\xbd',
 '{uml}': '\xe8',
 '{under}': '\xf6',
 '{uring}': '\xeau',
 '{us}': '\x1f',
 '{uuml}': '\xe8u',
 '{u}': 'u',
 '{vcy}': 'v',
 '{verbar}': '|',
 '{vlineb}': '\xf2',
 '{v}': 'v',
 '{w}': 'w',
 '{x}': 'x',
 '{yacute}': '\xe2y',
 '{yacy}': '\xebi\xeca',
 '{ycy}': 'y',
 '{yecy}': 'e',
 '{yen}': 'Y',
 '{yicy}': 'i',
 '{yucy}': '\xebi\xecu',
 '{y}': 'y',
 '{zacute}': '\xe2z',
 '{zcy}': 'z',
 '{zdot}': '\xe7z',
 '{zhcy}': 'zh',
 '{zhuacy}': '\xebz\xech',
 '{z}': 'z'}

def load_table(filename):
    mapping = {}
    for line in (i.split(",") for i in open(filename) if i.startswith("{")):
        key = line[0]
        value = ""
        for d in line[2].strip().split(" "):
            assert len(d) == 4
            assert d[3] == 'd'
            value += chr(int(d[0:3]))

        mapping[key] = value
    return mapping

def test_read():
    input = 'Tha{mllhring}{macr}alib{macr}i, {mllhring}Abd al-Malik ibn Mu{dotb}hammad,'
    output = 'Tha\xb0\xe5alib\xe5i, \xb0Abd al-Malik ibn Mu\xf2hammad,'
    assert read(input) == output
    input = 'El Ing.{eniero} Federico E. Capurro y el nacimiento de la profesi\xe2on bibliotecaria en el Uruguay.'
    assert read(input) == input

def read(input):
    return re_brace.sub(lambda x: mapping.get(x.group(1), x.group(1)), input)

########NEW FILE########
__FILENAME__ = parse
import re
from openlibrary.catalog.utils import pick_first_date, tidy_isbn, flip_name, remove_trailing_dot, remove_trailing_number_dot
from get_subjects import subjects_for_work
from collections import defaultdict

re_question = re.compile('^\?+$')
re_lccn = re.compile('(...\d+).*')
re_letters = re.compile('[A-Za-z]')
re_oclc = re.compile('^\(OCoLC\).*?0*(\d+)')
re_ocolc = re.compile('^ocolc *$', re.I)
re_ocn_or_ocm = re.compile('^oc[nm]0*(\d+) *$')
re_int = re.compile ('\d{2,}')
re_number_dot = re.compile('\d{3,}\.$')
re_bracket_field = re.compile('^\s*(\[.*\])\.?\s*$')
foc = '[from old catalog]'

def strip_foc(s):
    return s[:-len(foc)].rstrip() if s.endswith(foc) else s

class NoTitle(Exception):
    pass

class SeeAlsoAsTitle(Exception):
    pass

want = [
    '001',
    '003', # for OCLC
    '008', # publish date, country and language
    '010', # lccn
    '020', # isbn
    '035', # oclc
    '050', # lc classification
    '082', # dewey
    '100', '110', '111', # authors TODO
    '130', '240', # work title
    '245', # title
    '250', # edition
    '260', # publisher
    '300', # pagination
    '440', '490', '830' # series
    ] + [str(i) for i in range(500,595)] + [ # notes + toc + description
    #'600', '610', '611', '630', '648', '650', '651', '662', # subjects
    '700', '710', '711', '720', # contributions
    '246', '730', '740', # other titles
    '852', # location
    '856'] # URL

class BadMARC(Exception):
    pass

class SeaAlsoAsTitle(Exception):
    pass

def read_lccn(rec):
    fields = rec.get_fields('010')
    if not fields:
        return

    found = []
    for f in fields:
        for k, v in f.get_subfields(['a']):
            lccn = v.strip()
            if re_question.match(lccn):
                continue
            m = re_lccn.search(lccn)
            if not m:
                continue
            lccn = re_letters.sub('', m.group(1)).strip()
            if lccn:
                found.append(lccn)

    return found

def remove_duplicates(seq):
    u = []
    for x in seq:
        if x not in u:
            u.append(x)
    return u

def read_oclc(rec):
    found = []
    tag_001 = rec.get_fields('001')
    tag_003 = rec.get_fields('003')
    if tag_001 and tag_003 and re_ocolc.match(tag_003[0]):
        oclc = tag_001[0]
        m = re_ocn_or_ocm.match(oclc)
        if m:
            oclc = m.group(1)
        if oclc.isdigit():
            found.append(oclc)

    for f in rec.get_fields('035'):
        for k, v in f.get_subfields(['a']):
            m = re_oclc.match(v)
            if not m:
                m = re_ocn_or_ocm.match(v)
                if m and not m.group(1).isdigit():
                    m = None
            if m:
                oclc = m.group(1)
                if oclc not in found:
                    found.append(oclc)
    return remove_duplicates(found)

def read_lc_classification(rec):
    fields = rec.get_fields('050')
    if not fields:
        return

    found = []
    for f in fields:
        contents = f.get_contents(['a', 'b'])
        if 'b' in contents:
            b = ' '.join(contents['b'])
            if 'a' in contents:
                found += [' '.join([a, b]) for a in contents['a']]
            else:
                found += [b]
        # http://openlibrary.org/show-marc/marc_university_of_toronto/uoft.marc:671135731:596
        elif 'a' in contents:
            found += contents['a']
    return found

def read_isbn(rec):
    fields = rec.get_fields('020')
    if not fields:
        return

    found = []
    for f in fields:
        isbn = rec.read_isbn(f)
        if isbn:
            found += isbn
    ret = {}
    seen = set()

    for i in tidy_isbn(found):
        if i in seen: # avoid dups
            continue
        seen.add(i)
        if len(i) == 13:
            ret.setdefault('isbn_13', []).append(i)
        elif len(i) <= 16:
            ret.setdefault('isbn_10', []).append(i)
    return ret

def read_dewey(rec):
    fields = rec.get_fields('082')
    if not fields:
        return
    found = []
    for f in fields:
        found += f.get_subfield_values(['a'])
    return found

def read_work_titles(rec):
    found = []
    tag_240 = rec.get_fields('240')
    if tag_240:
        for f in tag_240:
            title = f.get_subfield_values(['a', 'm', 'n', 'p', 'r'])
            found.append(remove_trailing_dot(' '.join(title).strip(',')))

    tag_130 = rec.get_fields('130')
    if tag_130:
        for f in tag_130:
            title = ' '.join(v for k, v in f.get_all_subfields() if k.islower() and k != 'n')
            found.append(remove_trailing_dot(title.strip(',')))

    return remove_duplicates(found)

def read_title(rec):
    fields = rec.get_fields('245')
    if not fields:
        fields = rec.get_fields('740')
    if not fields:
        raise NoTitle

#   example MARC record with multiple titles:
#   http://openlibrary.org/show-marc/marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:299505697:862
    contents = fields[0].get_contents(['a', 'b', 'c', 'h', 'p'])

    b_and_p = [i for i in fields[0].get_subfield_values(['b', 'p']) if i]

    ret = {}
    title = None

#   MARC record with 245a missing:
#   http://openlibrary.org/show-marc/marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:516779055:1304
    if 'a' in contents:
        title = ' '.join(x.strip(' /,;:') for x in contents['a'])
    elif b_and_p:
        title = b_and_p.pop(0).strip(' /,;:')
# talis_openlibrary_contribution/talis-openlibrary-contribution.mrc:183427199:255
    if title in ('See.', 'See also.'):
        raise SeeAlsoAsTitle
# talis_openlibrary_contribution/talis-openlibrary-contribution.mrc:5654086:483
# scrapbooksofmoun03tupp
    if title is None:
        subfields = list(fields[0].get_all_subfields())
        title = ' '.join(v for k, v in subfields)
        if not title: # ia:scrapbooksofmoun03tupp
            raise NoTitle
    ret['title'] = remove_trailing_dot(title)
    if b_and_p:
        ret["subtitle"] = ' : '.join(remove_trailing_dot(x.strip(' /,;:')) for x in b_and_p)
    if 'c' in contents:
        ret["by_statement"] = remove_trailing_dot(' '.join(contents['c']))
    if 'h' in contents:
        h = ' '.join(contents['h']).strip(' ')
        m = re_bracket_field.match(h)
        if m:
            h = m.group(1)
        assert h
        ret["physical_format"] = h
    return ret

def read_edition_name(rec):
    fields = rec.get_fields('250')
    if not fields:
        return
    found = []
    for f in fields:
        f.remove_brackets()
        found += [v for k, v in f.get_all_subfields()]
    return ' '.join(found)

lang_map = {
    'ser': 'srp', # http://www.archive.org/details/zadovoljstvauivo00lubb
    'end': 'eng',
    'enk': 'eng',
    'ent': 'eng',
    'cro': 'chu',
    'jap': 'jpn',
    'fra': 'fre',
    'gwr': 'ger',
    'sze': 'slo',
    'fr ': 'fre',
    'fle': 'dut', # flemish -> dutch
    'it ': 'ita',
}

def read_languages(rec):
    fields = rec.get_fields('041')
    if not fields:
        return
    found = []
    for f in fields:
        found += [i.lower() for i in f.get_subfield_values('a') if i and len(i) == 3]
    return [lang_map.get(i, i) for i in found if i != 'zxx']

def read_pub_date(rec):
    fields = rec.get_fields('260')
    if not fields:
        return
    found = []
    for f in fields:
        f.remove_brackets()
        found += [i for i in f.get_subfield_values('c') if i]
    return remove_trailing_number_dot(found[0]) if found else None

def read_publisher(rec):
    fields = rec.get_fields('260')
    if not fields:
        return
    publisher = []
    publish_places = []
    for f in fields:
        f.remove_brackets()
        contents = f.get_contents(['a', 'b'])
        if 'b' in contents:
            publisher += [x.strip(" /,;:") for x in contents['b']]
        if 'a' in contents:
            publish_places += [x.strip(" /.,;:") for x in contents['a'] if x]
    edition = {}
    if publisher:
        edition["publishers"] = publisher
    if len(publish_places) and publish_places[0]:
        edition["publish_places"] = publish_places
    return edition

def read_author_person(f):
    f.remove_brackets()
    author = {}
    contents = f.get_contents(['a', 'b', 'c', 'd', 'e'])
    if 'a' not in contents and 'c' not in contents:
        return # should at least be a name or title
    name = [v.strip(' /,;:') for v in f.get_subfield_values(['a', 'b', 'c'])]
    if 'd' in contents:
        author = pick_first_date(strip_foc(d).strip(',') for d in contents['d'])
        if 'death_date' in author and author['death_date']:
            death_date = author['death_date']
            if re_number_dot.search(death_date):
                author['death_date'] = death_date[:-1]

    author['name'] = ' '.join(name)
    author['entity_type'] = 'person'
    subfields = [
        ('a', 'personal_name'),
        ('b', 'numeration'),
        ('c', 'title'),
        ('e', 'role')
    ]
    for subfield, field_name in subfields:
        if subfield in contents:
            author[field_name] = remove_trailing_dot(' '.join([x.strip(' /,;:') for x in contents[subfield]]))
    if 'q' in contents:
        author['fuller_name'] = ' '.join(contents['q'])
    for f in 'name', 'personal_name':
        if f in author:
            author[f] = remove_trailing_dot(strip_foc(author[f]))
    return author

# 1. if authors in 100, 110, 111 use them
# 2. if first contrib is 710 or 711 use it
# 3. if 

def person_last_name(f):
    v = list(f.get_subfield_values('a'))[0]
    return v[:v.find(', ')] if ', ' in v else v

def last_name_in_245c(rec, person):
    fields = rec.get_fields('245')
    if not fields:
        return
    last_name = person_last_name(person).lower()
    return any(any(last_name in v.lower() for v in f.get_subfield_values(['c'])) for f in fields)

def read_authors(rec):
    count = 0
    fields_100 = rec.get_fields('100')
    fields_110 = rec.get_fields('110')
    fields_111 = rec.get_fields('111')
    count = len(fields_100) + len(fields_110) + len(fields_111)
    if count == 0:
        return
    # talis_openlibrary_contribution/talis-openlibrary-contribution.mrc:11601515:773 has two authors:
    # 100 1  $aDowling, James Walter Frederick.
    # 111 2  $aConference on Civil Engineering Problems Overseas.

    found = [f for f in (read_author_person(f) for f in fields_100) if f]
    for f in fields_110:
        f.remove_brackets()
        name = [v.strip(' /,;:') for v in f.get_subfield_values(['a', 'b'])]
        found.append({ 'entity_type': 'org', 'name': remove_trailing_dot(' '.join(name))})
    for f in fields_111:
        f.remove_brackets()
        name = [v.strip(' /,;:') for v in f.get_subfield_values(['a', 'c', 'd', 'n'])]
        found.append({ 'entity_type': 'event', 'name': remove_trailing_dot(' '.join(name))})
    if found:
        return found

# no monograph should be longer than 50,000 pages
max_number_of_pages = 50000

def read_pagination(rec):
    fields = rec.get_fields('300')
    if not fields:
        return

    pagination = []
    edition = {}
    for f in fields:
        pagination += f.get_subfield_values(['a'])
    if pagination:
        edition["pagination"] = ' '.join(pagination)
        num = [] # http://openlibrary.org/show-marc/marc_university_of_toronto/uoft.marc:2617696:825
        for x in pagination:
            num += [ int(i) for i in re_int.findall(x.replace(',',''))]
            num += [ int(i) for i in re_int.findall(x) ]
        valid = [i for i in num if i < max_number_of_pages]
        if valid:
            edition["number_of_pages"] = max(valid)
    return edition

def read_series(rec):
    found = []
    for tag in ('440', '490', '830'):
        fields = rec.get_fields(tag)
        if not fields:
            continue
        for f in fields:
            this = []
            for k, v in f.get_subfields(['a', 'v']):
                if k == 'v' and v:
                    this.append(v)
                    continue
                v = v.rstrip('.,; ')
                if v:
                    this.append(v)
            if this:
                found += [' -- '.join(this)]
    return found

def read_notes(rec):
    found = []
    for tag in range(500,595):
        if tag in (505, 520):
            continue
        fields = rec.get_fields(str(tag))
        if not fields:
            continue
        for f in fields:
            x = f.get_lower_subfields()
            if x:
                found.append(' '.join(x).strip(' '))
    if found:
        return '\n\n'.join(found)

def read_description(rec):
    fields = rec.get_fields('520')
    if not fields:
        return
    found = []
    for f in fields:
        this = [i for i in f.get_subfield_values(['a']) if i]
        #if len(this) != 1:
        #    print f.get_all_subfields()
        # multiple 'a' subfields
        # marc_loc_updates/v37.i47.records.utf8:5325207:1062
        # 520: $aManpower policy;$aNusa Tenggara Barat Province
        found += this
    if found:
        return "\n\n".join(found).strip(' ')

def read_url(rec):
    found = []
    for f in rec.get_fields('856'):
        contents = f.get_contents(['3', 'u'])
        if not contents.get('u', []):
            #print `f.ind1(), f.ind2()`, list(f.get_all_subfields())
            continue
        if '3' not in contents:
            found += [{ 'url': u.strip(' ') } for u in contents['u']]
            continue
        assert len(contents['3']) == 1
        title = contents['3'][0].strip(' ')
        found += [{ 'url': u.strip(' '), 'title': title  } for u in contents['u']]

    return found

def read_other_titles(rec):
    return [' '.join(f.get_subfield_values(['a'])) for f in rec.get_fields('246')] \
        + [' '.join(f.get_lower_subfields()) for f in rec.get_fields('730')] \
        + [' '.join(f.get_subfield_values(['a', 'p', 'n'])) for f in rec.get_fields('740')]

def read_location(rec):
    fields = rec.get_fields('852')
    if not fields:
        return
    found = []
    for f in fields:
        found += [v for v in f.get_subfield_values(['a']) if v]
    return found

def read_contributions(rec):
    want = dict((
        ('700', 'abcdeq'),
        ('710', 'ab'),
        ('711', 'acdn'),
        ('720', 'a'),
    ))

    ret = {}
    skip_authors = set()
    for tag in ('100', '110', '111'):
        fields = rec.get_fields(tag)
        for f in fields:
            skip_authors.add(tuple(f.get_all_subfields()))
    
    if not skip_authors:
        for tag, f in rec.read_fields(['700', '710', '711', '720']):
            f = rec.decode_field(f)
            if tag in ('700', '720'):
                if 'authors' not in ret or last_name_in_245c(rec, f):
                    ret.setdefault('authors', []).append(read_author_person(f))
                    skip_authors.add(tuple(f.get_subfields(want[tag])))
                continue
            elif 'authors' in ret:
                break
            if tag == '710':
                name = [v.strip(' /,;:') for v in f.get_subfield_values(want[tag])]
                ret['authors'] = [{ 'entity_type': 'org', 'name': remove_trailing_dot(' '.join(name))}]
                skip_authors.add(tuple(f.get_subfields(want[tag])))
                break
            if tag == '711':
                name = [v.strip(' /,;:') for v in f.get_subfield_values(want[tag])]
                ret['authors'] = [{ 'entity_type': 'event', 'name': remove_trailing_dot(' '.join(name))}]
                skip_authors.add(tuple(f.get_subfields(want[tag])))
                break

    for tag, f in rec.read_fields(['700', '710', '711', '720']): 
        sub = want[tag]
        cur = tuple(rec.decode_field(f).get_subfields(sub))
        if tuple(cur) in skip_authors:
            continue
        name = remove_trailing_dot(' '.join(strip_foc(i[1]) for i in cur).strip(','))
        ret.setdefault('contributions', []).append(name) # need to add flip_name

    return ret

def read_toc(rec):
    fields = rec.get_fields('505')

    toc = []
    for f in fields:
        toc_line = []
        for k, v in f.get_all_subfields():
            if k == 'a':
                toc_split = [i.strip() for i in v.split('--')]
                if any(len(i) > 2048 for i in toc_split):
                    toc_split = [i.strip() for i in v.split(' - ')]
                # http://openlibrary.org/show-marc/marc_miami_univ_ohio/allbibs0036.out:3918815:7321
                if any(len(i) > 2048 for i in toc_split):
                    toc_split = [i.strip() for i in v.split('; ')]
                # FIXME:
                # http://openlibrary.org/show-marc/marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:938969487:3862
                if any(len(i) > 2048 for i in toc_split):
                    toc_split = [i.strip() for i in v.split(' / ')]
                assert isinstance(toc_split, list)
                toc.extend(toc_split)
                continue
            if k == 't':
                if toc_line:
                    toc.append(' -- '.join(toc_line))
                if (len(v) > 2048):
                    toc_line = [i.strip() for i in v.strip('/').split('--')]
                else:
                    toc_line = [v.strip('/')]
                continue
            toc_line.append(v.strip(' -'))
        if toc_line:
            toc.append('-- '.join(toc_line))
    found = []
    for i in toc:
        if len(i) > 2048:
            i = i.split('  ')
            found.extend(i)
        else:
            found.append(i)
    return [{'title': i, 'type': '/type/toc_item'} for i in found]

def update_edition(rec, edition, func, field):
    v = func(rec)
    if v:
        edition[field] = v

re_bad_char = re.compile(u'[\xa0\xf6]')

def read_edition(rec):
    handle_missing_008=True
    rec.build_fields(want)
    edition = {}
    tag_008 = rec.get_fields('008')
    if len(tag_008) == 0:
        if not handle_missing_008:
            raise BadMARC("single '008' field required")
    if len(tag_008) > 1:
        len_40 = [f for f in tag_008 if len(f) == 40]
        if len_40:
            tag_008 = len_40
        tag_008 = [min(tag_008, key=lambda f:f.count(' '))]
    if len(tag_008) == 1:
        #assert len(tag_008[0]) == 40
        f = re_bad_char.sub(' ', tag_008[0])
        if not f:
            raise BadMARC("'008' field must not be blank")
        publish_date = str(f)[7:11]

        if publish_date.isdigit() and publish_date != '0000':
            edition["publish_date"] = publish_date
        if str(f)[6] == 't':
            edition["copyright_date"] = str(f)[11:15]
        publish_country = str(f)[15:18]
        if publish_country not in ('|||', '   ', '\x01\x01\x01', '???'):
            edition["publish_country"] = publish_country
        lang = str(f)[35:38]
        if lang not in ('   ', '|||', '', '???', 'zxx'):
            # diebrokeradical400poll
            if str(f)[34:37].lower() == 'eng':
                lang = 'eng'
            else:
                lang = lang.lower()
            edition['languages'] = [lang_map.get(lang, lang)]
    else:
        assert handle_missing_008
        update_edition(rec, edition, read_languages, 'languages')
        update_edition(rec, edition, read_pub_date, 'publish_date')

    update_edition(rec, edition, read_lccn, 'lccn')
    update_edition(rec, edition, read_authors, 'authors')
    update_edition(rec, edition, read_oclc, 'oclc_numbers')
    update_edition(rec, edition, read_lc_classification, 'lc_classifications')
    update_edition(rec, edition, read_dewey, 'dewey_decimal_class')
    update_edition(rec, edition, read_work_titles, 'work_titles')
    update_edition(rec, edition, read_other_titles, 'other_titles')
    update_edition(rec, edition, read_edition_name, 'edition_name')
    update_edition(rec, edition, read_series, 'series')
    update_edition(rec, edition, read_notes, 'notes')
    update_edition(rec, edition, read_description, 'description')
    update_edition(rec, edition, read_location, 'location')
    update_edition(rec, edition, read_toc, 'table_of_contents')
    update_edition(rec, edition, read_url, 'links')

    edition.update(read_contributions(rec))
    edition.update(subjects_for_work(rec))

    try:
        edition.update(read_title(rec))
    except NoTitle:
        if 'work_titles' in edition:
            assert len(edition['work_titles']) == 1
            edition['title'] = edition['work_titles'][0]
            del edition['work_titles']
        else:
            raise

    for func in (read_publisher, read_isbn, read_pagination):
        v = func(rec)
        if v:
            edition.update(v)

    return edition

if __name__ == '__main__':
    import sys
    loc = sys.argv[1]

########NEW FILE########
__FILENAME__ = parse_xml
from lxml import etree
import xml.parsers.expat
from parse import read_edition
from unicodedata import normalize

slim = '{http://www.loc.gov/MARC21/slim}'
leader_tag = slim + 'leader'
data_tag = slim + 'datafield'
control_tag = slim + 'controlfield'
subfield_tag = slim + 'subfield'
collection_tag = slim + 'collection'
record_tag = slim + 'record'

def norm(s):
    return normalize('NFC', unicode(s))

class BadSubtag:
    pass

class MultipleTitles:
    pass

class MultipleWorkTitles:
    pass

class datafield:
    def __init__(self, element):
        assert element.tag == data_tag
        self.contents = {}
        self.subfield_sequence = []
        self.indicator1 = element.attrib['ind1']
        self.indicator2 = element.attrib['ind2']
        for i in element:
            assert i.tag == subfield_tag
            text = norm(i.text) if i.text else u''
            if i.attrib['code'] == '':
                raise BadSubtag
            self.contents.setdefault(i.attrib['code'], []).append(text)
            self.subfield_sequence.append((i.attrib['code'], text))

class xml_rec:
    def __init__(self, f):
        self.root = etree.parse(f).getroot()
        if self.root.tag == collection_tag:
            assert self.root[0].tag == record_tag
            self.root = self.root[0]
        self.dataFields = {}
        self.has_blank_tag = False
        for i in self.root:
            if i.tag == data_tag or i.tag == control_tag:
                if i.attrib['tag'] == '':
                    self.has_blank_tag = True
                else:
                    self.dataFields.setdefault(i.attrib['tag'], []).append(i)

    def leader(self):
        leader = self.root[0]
        assert leader.tag == leader_tag
        return norm(leader.text)

    def fields(self):
        return self.dataFields.keys()

    def get_field(self, tag, default=None):
        if tag not in self.dataFields:
            return default
        if tag == '245' and len(self.dataFields[tag]) > 1:
            raise MultipleTitles
        if tag == '240' and len(self.dataFields[tag]) > 1:
            raise MultipleWorkTitles
        if tag != '006':
            assert len(self.dataFields[tag]) == 1
        element = self.dataFields[tag][0]
        if element.tag == control_tag:
            return norm(element.text) if element.text else u''
        if element.tag == data_tag:
            return datafield(element)
        return default

    def get_fields(self, tag):
        if tag not in self.dataFields:
            return []
        if self.dataFields[tag][0].tag == control_tag:
            return [norm(i.text) if i.text else u'' for i in self.dataFields[tag]]
        if self.dataFields[tag][0].tag == data_tag:
            return [datafield(i) for i in self.dataFields[tag]]
        return []

def parse(f):
    rec = xml_rec(f)
    edition = {}
    if rec.has_blank_tag:
        print 'has blank tag'
    if rec.has_blank_tag or not read_edition(rec, edition):
        return {}
    return edition

########NEW FILE########
__FILENAME__ = read_from_archive
import urllib2
import xml.etree.ElementTree as et
from MARC21 import MARC21Record
from MARC21Exn import MARC21Exn
from pprint import pprint
from time import sleep

archive_url = "http://archive.org/download/"

def urlopen_keep_trying(url):
    while 1:
        print url
        try:
            return urllib2.urlopen(url)
        except urllib2.URLError:
            pass
        sleep(5)

class FileWrapper(file):
    def __init__(self, part):
        self.fh = urlopen_keep_trying(archive_url + part)
        self.pos = 0
    def read(self, size):
        self.pos += size
        return self.fh.read(size)
    def tell(self):
        return self.pos
    def close(self):
        return self.fh.close()

def files(archive_id):
    url = archive_url + archive_id + "/" + archive_id + "_files.xml"
    tree = et.parse(urlopen_keep_trying(url))
    for i in tree.getroot():
        assert i.tag == 'file'
        name = i.attrib['name']
        if name.endswith('.mrc') or name.endswith('.marc') or name.endswith('.out'):
            yield archive_id + "/" + name

def bad_data(i):
    pass

def read_marc_file(part, f, pos = 0):
    buf = None
    while 1:
        if buf:
            length = buf[:5]
            int_length = int(length)
        else:
            length = f.read(5)
            buf = length
        if length == "":
            break
        assert length.isdigit()
        int_length = int(length)
        if 0 and part == 'marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc':
            if pos == 295782261:
                int_length+=1
            elif pos == 299825918:
                int_length+=5
        data = buf + f.read(int_length - len(buf))
        buf = None
        if data.find('\x1d') == -1:
            data += f.read(40)
            int_length = data.find('\x1d') + 1
            print `data[-40:]`
            assert int_length
            buf = data[int_length:]
            data = data[:int_length]
        assert data.endswith("\x1e\x1d")
        if len(data) < int_length:
            yield (loc, 'bad')
            break
        loc = "%s:%d:%d" % (part, pos, int_length)
        pos += int_length
        if str(data)[6:8] != 'am':
            yield (loc, 'not_book')
            continue
        try:
            rec = MARC21Record(data)
        except IndexError:
            rec = 'bad'
        except TypeError:
            rec = 'bad'
        except ValueError:
            rec = 'bad'
        except EOFError:
            print "EOF"
            print `data`
            rec = 'bad'
        except MARC21Exn:
            rec = 'bad'
        yield (loc, rec)

#archive = [ "unc_catalog_marc", "marc_oregon_summit_records",
#    "marc_university_of_toronto", "marc_miami_univ_ohio",
#archive = [ "marc_western_washington_univ", "marc_boston_college" ]
archive = [ "marc_western_washington_univ" ]
#archive = [ "marc_boston_college" ]
archive = [ "bcl_marc" ]

# marc_university_of_toronto/uoft.marc:2568231948:707
# marc_university_of_toronto/uoft.marc:2571412614:886

# marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:286075909:955

def check():
    part = 'marc_boston_college/bc_openlibrary.mrc'
    pos = 2147477123
    part = 'marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc'
    pos = 295780665
    pos = 395466242
    pos = 495359719
    pos = 778742194
    pos = 875901953
#    pos = 299825182
#    pos = 298636444
#    pos = 299562947
#    pos = 299702849
#    pos = 303085246
    part = 'marc_records_scriblio_net/part10.dat'
    pos = 99557594
#    ureq = urllib2.Request(archive_url + part, None, {'Range':'bytes=%d-%d'% (pos, pos+1000000000)} )
#    f = urllib2.urlopen(ureq)
    f = urllib2.urlopen(archive_url + part)

    i = 0
    total = 0
    bad_record = []
    not_interesting = 0
    for loc, rec in read_marc_file(part, f, pos):
        total+=1
        if rec == 'not_book':
            not_interesting += 1
            continue
        if rec == 'bad':
            print 'bad:', loc
            bad_record.append(loc)
            continue
    #    if str(rec.leader)[6:8] != 'am':
    #        not_interesting += 1
    #        continue
        i+=1
        if i % 1000 == 0:
            print i, loc

#check()

for archive_id in archive:
    i = 0
    total = 0
    bad_record = []
    not_interesting = 0
    print archive_id
    for part in files(archive_id):
        print part
        f = urlopen_keep_trying(archive_url + part)
        for loc, rec in read_marc_file(part, f):
            total+=1
            if rec == 'not_book':
                not_interesting += 1
                continue
            if rec == 'bad':
                print 'bad:', loc
                bad_record.append(loc)
                continue
            i+=1
            if i % 1000 == 0:
                print i, loc
    for loc in bad_record:
        print loc
    print archive_id, total, i, not_interesting, len(bad_record)

########NEW FILE########
__FILENAME__ = read_toc
from catalog.marc.fast_parse import get_all_subfields
import re

samples = [
    "00\x1faDi 1 juan.Guo se tian xiang /Wu Jingsuo bian.Ba duan jin /Xingshizhushi bian ji --di 2 juan.Wu sheng xi ;Shi er lou /Li Yu --di 3 juan.Jin xiang ting /Su'anzhuren bian.\x1ftFen zhuang lou /Zhuxishanren --\x1fgdi 4 juan.Wu se shi /Bilian'gezhuren.Ba dong tian /Wuseshizhuren.Wu feng yin /Chichi dao ren bian zhu --di 5 juan.Shuang feng qi yuan /Xueqiaozhuren zi ding.Jin shi yuan.Qing meng tuo /Anyangjiumin --di 6 juan.Wu mei yuan.Xiu qiu yuan.Yuan yang ying /Qiaoyunshanren bian --di 7 juan.Mei ren shu /Xu Zhen.Wan hua lou /Li Yutang --di 8 juan.Bei shi yan yi /Du Gang.Kong kong huan /Wugangzhuren bian ci.Chun qiu pei --di 9 juan.Qian Qi guo zhi /Wumenxiaoke.Hou Qi guo zhi /Yanshuisanren.Qiao shi yan yi /Lu Yingyang --di 10 juan.Liaohai dan zhong lu /Lu Renlong.Tian bao tu.Jin xiu yi --di 11 juan.Shi mei tu.Huan xi yuan jia /Xihuyuyinzhuren.Feng liu he shang.Liang jiao hun /Tianhuazangzhuren --di 12 juan.Ge lian hua ying.Qi lou chong meng /Wang Lanzhi.\x1e",
    '00\x1ftManierismus als Artistik : systematische Aspekte einer \xe8asthetischen Kategorie / R\xe8udiger Zymner -- "Stil" und "Manier" in der Alltagskultur / Kaspar Maase -- Die Wortfamilie von it. "Maniera" zwischen Literatur, bildender Kunst und Psychologie / Margarete Lindemann -- Der Manierismus : zur Problematik einer kunsthistorischen Erfindung / Horst Bredekamp -- Inszenierte K\xe8unstlichkeit : Musik als manieristisches Dispositiv / Hermann Danuser -- Manierismus als Stilbegriff in der Architekturgeschichte / Hermann Hipp -- "Raffael ohne H\xe8ande," oder, Das Kunstwerk zwischen Sch\xe8opfung und Fabrikation : Konzepte der "maniera" bei Vasari und seinen Zeitgenossen / Ursula Link-Heer -- "Sprezzatura" : Pontormos Portraits und das h\xe8ofische Ideal des Manierismus / Axel Christoph Gampp -- Maniera and the grotesque / Maria Fabricius Hansen -- Neulateinisches Figurengedicht und manieristische Poetik : zum "Poematum liber" (1573) des Richard Willis / Ulrich Ernst -- Manierismus als Selbstbehauptung, Jean Paul / Wolfgang Braungart --  Artistische Erkenntnis : (Sprach-)Alchimie und Manierismus in der Romantik / Axel Dunker -- "Als lebeten sie" / Holk Cruse.\x1e',
]

re_gt = re.compile('^(gt)+$')
re_gtr = re.compile('^(gtr)+$')
re_at = re.compile('^at+$')
re_end_num = re.compile('\d[]. ]*$')
for line in open('test_data/marc_toc'):
    (loc, line) = eval(line)
    #print loc
    subfields = list(get_all_subfields(line))
    if subfields[0][0] == '6':
        subfields.pop(0)
    subtags = ''.join(k for k, v in subfields)
    if re_at.match(subtags):
        a = subfields[0][1]
        m = re_end_num.search(a)
        print bool(m), `a`
        continue

        if not m:
            for k, v in subfields:
                print k, `v`
        assert m
    continue
    if re_gtr.match(subtags):
        continue
        for i in range(len(subfields)/3):
            g = subfields[i * 3][1]
            t = subfields[i * 3 + 1][1].strip('- /')
            r = subfields[i * 3 + 2][1].strip('- ')
            print `g, t, r`
        print
        continue
    if re_gt.match(subtags):
        continue
        for i in range(len(subfields)/2):
            g = subfields[i * 2][1]
            t = subfields[i * 2 + 1][1].strip('- /')
            print `g, t`
        print
        continue
    print subtags
    for k, v in subfields:
        print k, `v`
    print

########NEW FILE########
__FILENAME__ = read_xml
import xml.etree.ElementTree as et
import xml.parsers.expat
from pprint import pprint
import re, sys, codecs
from time import sleep
from unicodedata import normalize

re_question = re.compile('^\?+$')
re_lccn = re.compile('(...\d+).*')
re_letters = re.compile('[A-Za-z]')
re_int = re.compile ('\d{2,}')
re_isbn = re.compile('([^ ()]+[\dX])(?: \((?:v\. (\d+)(?: : )?)?(.*)\))?')
re_oclc = re.compile ('^\(OCoLC\).*?0*(\d+)')

# no monograph should be longer than 50,000 pages
max_number_of_pages = 50000

slim = '{http://www.loc.gov/MARC21/slim}'
leader_tag = slim + 'leader'
data_tag = slim + 'datafield'
control_tag = slim + 'controlfield'
subfield_tag = slim + 'subfield'
collection_tag = slim + 'collection'
record_tag = slim + 'record'

class BadXML:
    pass

def read_author_person(line):
    name = []
    name_and_date = []
    for k, v in get_subfields(line, ['a', 'b', 'c', 'd']):
        if k != 'd':
            v = v.strip(' /,;:')
            name.append(v)
        name_and_date.append(v)
    if not name:
        return []

    return [{ 'db_name': ' '.join(name_and_date), 'name': ' '.join(name), }]

def read_full_title(line):
    title = [v.strip(' /,;:') for k, v in get_subfields(line, ['a', 'b'])]
    return ' '.join([t for t in title if t])

def read_lccn(line):
    found = []
    for k, v in get_subfields(line, ['a']):
        lccn = v.strip()
        if re_question.match(lccn):
            continue
        m = re_lccn.search(lccn)
        if not m:
            continue
        lccn = re_letters.sub('', m.group(1)).strip()
        if lccn:
            found.append(lccn)
    return found

def read_isbn(line):
    found = []
    for k, v in get_subfields(line, ['a', 'z']):
        m = re_isbn.match(v)
        if m:
            found.append(m.group(1))
    return [i.replace('-', '') for i in found]

def read_oclc(line):
    found = []
    for k, v in get_subfields(line, ['a']):
        m = re_oclc.match(v)
        if m:
            found.append(m.group(1))
    return found

def read_publisher(line):
    return [v.strip(' /,;:') for k, v in get_subfields(line, ['b'])]

def read_author_org(line):
    name = " ".join(v.strip(' /,;:') for k, v in get_subfields(line, ['a', 'b']))
    return [{ 'name': name, 'db_name': name, }]

def read_author_event(line):
    name = " ".join(v.strip(' /,;:') for k, v in get_subfields(line, ['a', 'b', 'd', 'n']))
    return [{ 'name': name, 'db_name': name, }]

def get_tag_lines(f, want):
    tree = et.parse(f)
    root = tree.getroot()
    if root.tag == collection_tag:
        root = root[0]
    assert root.tag == record_tag
    fields = []
    for i in root:
        if i.tag == leader_tag and i.text[6:8] != 'am': # only want books:
            return []
        if i.tag != data_tag and i.tag != control_tag:
            continue
        tag = i.attrib['tag']
        if tag == '':
            raise BadXML
        if tag not in want:
            continue
        if i.tag == control_tag:
            fields.append((tag, i.text))
            continue
        field = {
            'ind1': i.attrib['ind1'],
            'ind2': i.attrib['ind2'],
            'seq': [],
        }
        for k in i:
            assert k.tag == subfield_tag
            if k.attrib['code'] == '':
                raise BadXML
            field['seq'].append((k.attrib['code'], k.text if k.text else ''))
        fields.append((tag, field))
    return fields

def get_subfields(line, want):
    want = set(want)
    return (i for i in line['seq'] if i[0] in want)

def read_edition(f):
    edition = {}
    want = ['008', '010', '020', '035', \
            '100', '110', '111', '700', '710', '711', '245', '260', '300']
    fields = get_tag_lines(f, want)
    read_tag = [
        ('010', read_lccn, 'lccn'),
        ('020', read_isbn, 'isbn'),
        ('035', read_oclc, 'oclc'),
        ('100', read_author_person, 'authors'),
        ('110', read_author_org, 'authors'),
        ('111', read_author_event, 'authors'),
        ('700', read_author_person, 'contribs'),
        ('710', read_author_org, 'contribs'),
        ('711', read_author_event, 'contribs'),
        ('260', read_publisher, 'publisher'),
    ]

    for tag, line in fields:
        # http://openlibrary.org/b/OL7074573M
        # if tag == '006':
        #    if line[0] == 'm':
        #        return None
        #    continue
        if tag == '008':
            publish_date = unicode(line)[7:11]
            if publish_date.isdigit():
                edition["publish_date"] = publish_date
            publish_country = unicode(line)[15:18]
            if publish_country not in ('|||', '   '):
                edition["publish_country"] = publish_country
            continue
        for t, proc, key in read_tag:
            if t != tag:
                continue
            found = proc(line)
            if found:
                edition.setdefault(key, []).extend(found)
            break
        if tag == '245':
            edition['full_title'] = read_full_title(line)
            continue
        if tag == '300':
            for k, v in get_subfields(line, ['a']):
                num = [ int(i) for i in re_int.findall(v) ]
                num = [i for i in num if i < max_number_of_pages]
                if not num:
                    continue
                max_page_num = max(num)
                if 'number_of_pages' not in edition \
                        or max_page_num > edition['number_of_pages']:
                    edition['number_of_pages'] = max_page_num
    return edition

def test_parse():
    expect = {
        'publisher': ['Univ. Press'],
        'number_of_pages': 128,
        'full_title': 'Human efficiency and levels of intelligence.', 
        'publish_date': '1920',
        'publish_country': 'nju',
        'authors': [{
            'db_name': 'Goddard, Henry Herbert.',
            'name': 'Goddard, Henry Herbert.'
        }],
    }

    assert read_edition("humanefficiencyl00godduoft") == expect

def test_xml():
    f = open('test_data/nybc200247_marc.xml')
    lines = get_tag_lines(f, ['245'])
    for tag, line in lines:
        title = list(get_subfields(line, ['a']))[0][1]
        print title
        print normalize('NFC', title)
#    print read_edition(open('test_data/nybc200247_marc.xml'))['full_title']

def test_yale_xml():
    f = open('test_data/39002054008678.yale.edu_marc.xml')
    read_edition(f)
    f.close()

#test_yale_xml()

########NEW FILE########
__FILENAME__ = show-marc
#!/usr/bin/python2.5

import web
import urllib2
from time import time
import re

from MARC21 import MARC21Record, MARC21HtmlPrint, MARC21Exn

class show_marc:
    def GET(self, record, offset, length):
        web.header('Content-Type', 'text/html; charset=utf-8')

        file = locator = record
        offset = int(offset)
        length = int(length)

        print "record_locator: <code>%s</code><p/><hr>" % locator

        r0, r1 = offset, offset+length-1
        url = 'http://www.archive.org/download/%s'% file

        assert 0 < length < 100000

        t0 = time()
        ureq = urllib2.Request(url,
                               None,
                               {'Range':'bytes=%d-%d'% (r0, r1)},
                               )

        
        result = urllib2.urlopen(ureq).read(100000)
        # print 'urllib2 got %d bytes (%.3f sec):<p/>'% (len(result), time()-t0)

        rec = None
        try:
            rec = MARC21Record(result)
        except (ValueError,MARC21Exn), e:
            print 'Invalid MARC data %s<p/>'% str(e)

        if rec:
            print '<b>LEADER:</b> <code>%s</code><br/>'% result[:24]
            MARC21HtmlPrint(rec)

        print '<hr><p/>'

        print """In order to retrieve the exact bytes of this MARC record, perform a
        HTTP GET on the url <blockquote><b>%s</b></blockquote> and include the HTTP header"""% url
        print "<pre>&nbsp;&nbsp;&nbsp; Range: bytes=%d-%d</pre>"% (r0, r1)
        print """You will need to do this with a special purpose web client
           such as <a href=http://curl.haxx.se>curl</a>, not a browser.
           The curl command you'd use is:
           <blockquote>
           <code>curl -L -r %d-%d '%s'</code>"""% (r0, r1, url)

if __name__ == "__main__":
    urls = (
        "/show-marc/(.*):(\d+):(\d+)", "show_marc"
    )
    web.run(urls, globals())

########NEW FILE########
__FILENAME__ = show_records
#!/usr/bin/python2.5
from openlibrary.catalog.marc.fast_parse import *
import sys
from collections import defaultdict

# read a MARC binary showing one record at a time

field_counts = defaultdict(int)

for data, length in read_file(open(sys.argv[1])):
    print data[:24]
    is_marc8 = data[9] != 'a'
    for tag, line in get_all_tag_lines(data):
        if tag.startswith('00'):
            print tag, line[:-1]
        else:
            print tag, list(get_all_subfields(line, is_marc8))
        field_counts[tag] += 1
    print
    print dict(field_counts)
    print

########NEW FILE########
__FILENAME__ = simple
#!/usr/bin/python
from fast_parse import *
from marc_binary import MarcBinary
from pprint import pprint
import parse
#from parse import read_edition, SeeAlsoAsTitle, NoTitle
import sys, codecs, re
from getopt import getopt

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

re_subtag = re.compile('\x1f(.)([^\x1f]*)')

def fmt_subfields(line):
    def bold(s):
        return ''.join(c + "\b" + c for c in s)
    assert line[-1] == '\x1e'
    return ''.join(bold("$" + m.group(1)) + translate(m.group(2)) for m in re_subtag.finditer(line[2:-1]))

show_non_books = False
verbose = False
show_pos = False
build_rec = False
show_field = None
show_leader = True
show_leader = False

opts, files = getopt(sys.argv[1:], 'vl', ['verbose', 'show-pos', 'show-non-books', 'build-record', 'show-field='])

for o, a in opts:
    if o == '--show-pos':
        show_pos = True
    elif o == '--show-non-books':
        show_non_books = True
    elif o in ('-v', '--verbose'):
        verbose = True
    elif o == '--build-record':
        build_rec = True
    elif o == '--show-field':
        show_field = a
    elif o == '-l':
        show_leader = True

# simple parse of a MARC binary, just counts types of items

def show_book(data):
    print 'leader:', data[:24]
    for tag, line in get_all_tag_lines(data):
        if tag.startswith('00'):
            print tag, line[:-1]
        else:
            print tag, line[0:2], fmt_subfields(line)

total, bad_dict, sound_rec, not_book, book = 0, 0, 0, 0, 0
f = open(files[0])
next = 0
for data, length in read_file(f):
    pos = next
    next += length
    total += 1
    if show_field:
        get_first_tag(data, set([show_field]))
    if show_leader:
        print data[:24]
    if show_pos:
        print pos
    if verbose:
        show_book(data)
        print
    if build_rec:
        marc_rec = MarcBinary(data)
        edition_marc_bin = parse.read_edition(marc_rec)
        pprint(edition_marc_bin)
        print
    try:
        rec = read_edition(data)
    except SoundRecording:
        sound_rec += 1
        continue
    except BadDictionary:
        bad_dict += 1
        continue
    except NotBook:
        if show_non_books:
            show_book(data)
            print
        not_book += 1
    else:
        book += 1
f.close()

print "total records:", total
print "sound recordings:", sound_rec
print "records with bad dictionary:", bad_dict
print "not books:", not_book
print "books:", book

########NEW FILE########
__FILENAME__ = simple_html
#!/usr/bin/python2.5
from catalog.marc.fast_parse import *
from html import as_html
from build_record import build_record
import sys, re

trans = {'&':'&amp;','<':'&lt;','>':'&gt;','\n':'<br>'}
re_html_replace = re.compile('([&<>\n])')

def esc(s):
    return re_html_replace.sub(lambda m: trans[m.group(1)], s.encode('utf8'))

fields = [
    ('title', 'Title'),
    ('subtitle', 'Subtitle'),
    ('by_statement', 'By statement'),
    ('authors', 'Authors'),
    ('contributions', 'Contributions'),
    ('edition_name', 'Edition'),
    ('publish_date', 'Publish date'),
    ('publishers', 'Publishers'),
    ('publish_places', 'Publish places'),
    ('publish_country', 'Publish country'),
    ('work_titles', 'Work titles'),
    ('other_titles', 'Other titles'),
    ('pagination', 'Pagination'),
    ('number_of_pages', 'Number of pages'),
    ('lc_classifications', 'LC classifications'),
    ('dewey_decimal_class', 'Dewey decimal class'),
    ('oclc_number', 'OCLC number'),
    ('lccn', 'LCCN'),
    ('series', 'Series'),
    ('genres', 'Genres'),
    ('languages', 'Languages'),
    ('subjects', 'Subjects'),
    ('description', 'Description'),
    ('notes', 'Notes'),
    ('uri', 'URL'),
    ('table_of_contents', 'Table of contents'),
]

re_end_dot = re.compile('[^ ][^ ]\.$', re.UNICODE)
re_marc_name = re.compile('^(.*), (.*)$')
re_year = re.compile(r'\b(\d{4})\b')

def flip_name(name):
    # strip end dots like this: "Smith, John." but not like this: "Smith, J."
    m = re_end_dot.search(name)
    if m:
        name = name[:-1]

    if name.find(', ') == -1:
        return name
    m = re_marc_name.match(name)
    return m.group(2) + ' ' + m.group(1)

def html_author(a):
    extra = ''
    et = a['entity_type']
    if et == 'person':
        a['name'] = flip_name(a['name'])
        if 'date' in a:
            extra = ", " + a['date']
        elif 'birth_date' in a or 'death_date' in a:
            extra = ", " + a.get('birth_date', '') + ' - ' + a.get('death_date', '')
    elif et == 'org':
        et = 'organization'
    return '<b>' + et + ':</b> ' + esc(a['name'] + extra)

def output_record_as_html(rec):
    rows = []
    rec.setdefault('number_of_pages', None)
    for k, label in fields:
        if k not in rec:
#            rows.append('<tr><th>%s</th><td>%s</td></tr>\n' % (label, '<em>empty</em>'))
            continue
        if k == 'authors':
            v = '<br>\n'.join(html_author(a) for a in rec[k])
        elif k == 'languages':
            v = ','.join(esc(i['key'][3:]) for i in rec[k])
        elif isinstance(rec[k], list):
            v = '<br>\n'.join(esc(i) for i in rec[k])
        elif rec[k] is None:
            v = '<em>empty</em>'
        else:
            v = esc(unicode(rec[k]))
        rows.append('<tr><th>%s</th><td>%s</td></tr>\n' % (label, v))

    return '<table>' + ''.join(rows) + '</table>'

style = """<style>
td,th { padding: 3px; }
tr { background: #eee; }
th { text-align: left; vertical-align: top; }
</style>"""

dir = sys.argv[2]

index = open(dir + "/index.html", "w")
print >> index, "<html>\n<head><title>MARC records</title>" + style + "</head>\n<body>\n<ul>"

rec_no = 0
for data, length in read_file(open(sys.argv[1])):
    rec_no += 1
    rec = build_record(data)
    title = rec['title'] 
    filename = dir + "/" + str(rec_no) + ".html"
    f = open(filename, 'w')
    print >> f, "<html>\n<head><title>" + title + "</title>" + style + "</head>\n<body>"
    print >> f, '<a href="index.html">Back to index</a><br>'
    print >> f, output_record_as_html(rec)
    print >> f, "<h2>MARC record</h2>"
    print >> f, as_html(data)
    print >> f, '<br>\n<a href="index.html">Back to index</a>'
    print >> f, "</body></html>"
    print >> index, '<li><a href="%d.html">%s</a>' % (rec_no, title)

print >> index, "</ul>\n</body></html>"

########NEW FILE########
__FILENAME__ = sources
import sys, os

def find_sources():
    for p in sys.path:
        f = p + "/catalog/marc/sources"
        if os.path.exists(f):
            return f

def sources():
    return [tuple(i[:-1].split('\t')) for i in open(find_sources())]

########NEW FILE########
__FILENAME__ = test_marc
#!/usr/bin/python

import unittest
from parse import normalize_isbn, biggest_decimal, compile_marc_spec

class MockField:
    def __init__(self, subfields):
        self.subfield_sequence = subfields
        self.contents = {}
        for k, v in subfields:
            self.contents.setdefault(k, []).append(v)
    def get_elts(self, i):
        if i in self.contents:
            return self.contents[i]
        else:
            return []
class MockRecord:
    def __init__(self, subfields):
        self.field = MockField(subfields)
    def get_fields(self, tag):
        return [self.field]

class TestMarcParse(unittest.TestCase):
    def test_normalize_isbn(self):
        data = [
            ('0300067003 (cloth : alk. paper)', '0300067003'),
            ('0197263771 (cased)', '0197263771'),
            ('8831789589 (pbk.)', '8831789589'),
            ('9788831789585 (pbk.)', '9788831789585'),
            ('1402051891 (hd.bd.)', '1402051891'),
            ('9061791308', '9061791308'),
            ('9788831789530', '9788831789530'),
            ('8831789538', '8831789538'),
            ('97883178953X ', '97883178953X'),
            ('0-14-118250-4', '0141182504'),
            ('0321434250 (textbook)', '0321434250'),
        ]

        for (input, expect) in data:
            output = normalize_isbn(input)
            self.assertEqual(expect.lower(), output.lower())

    def test_biggest_decimal(self):
        data = [
            ("xx, 1065 , [57] p. :", '1065'),
            ("193 p., 31 p. of plates", '193'),
        ]
        for (input, expect) in data:
            output = biggest_decimal(input)
            self.assertEqual(output, expect)

    def xtest_subject_order(self):
        gen = compile_marc_spec('650:a--x--v--y--z')

        data = [
            ([  ('a', 'Authors, American'),
                ('y', '19th century'),
                ('x', 'Biography.')],
                'Authors, American -- 19th century -- Biography.'),
            ([  ('a', 'Western stories'),
                ('x', 'History and criticism.')],
                'Western stories -- History and criticism.'),
            ([  ('a', 'United States'),
                ('x', 'History'),
                ('y', 'Revolution, 1775-1783'),
                ('x', 'Influence.')],
                'United States -- History -- Revolution, 1775-1783 -- Influence.'
            ),
            ([  ('a', 'West Indies, British'),
                ('x', 'History'),
                ('y', '18th century.')],
                'West Indies, British -- History -- 18th century.'),
            ([  ('a', 'Great Britain'),
                ('x', 'Relations'),
                ('z', 'West Indies, British.')],
                'Great Britain -- Relations -- West Indies, British.'),
            ([  ('a', 'West Indies, British'),
                ('x', 'Relations'),
                ('z', 'Great Britain.')],
                'West Indies, British -- Relations -- Great Britain.')
        ]
        for (input, expect) in data:
            output = [i for i in gen(MockRecord(input))]
            self.assertEqual([expect], output)

    def test_title(self):
        gen = compile_marc_spec('245:ab clean_name')
        data = [
            ([  ('a', 'Railroad construction.'),
                ('b', 'Theory and practice.  A textbook for the use of students in colleges and technical schools.'),
                ('b', 'By Walter Loring Webb.')],
                'Railroad construction. Theory and practice.  A textbook for the use of students in colleges and technical schools. By Walter Loring Webb.')
        ]

        for (input, expect) in data:
            output = [i for i in gen(MockRecord(input))]
            self.assertEqual([expect], output)
    def test_by_statement(self):
        gen = compile_marc_spec('245:c')
        data = [
            ([  ('a', u'Trois contes de No\u0308el'),
                ('c', u'[par] Madame Georges Renard,'),
                ('c', u'edited by F. Th. Meylan ...')],
                '[par] Madame Georges Renard, edited by F. Th. Meylan ...')
        ]
        for (input, expect) in data:
            output = [i for i in gen(MockRecord(input))]
            self.assertEqual([expect], output)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_parse
#!/usr/bin/python
import unittest

from parse import read_edition, SeeAlsoAsTitle, NoTitle
from marc_binary import MarcBinary
from marc_xml import MarcXml, BadSubtag, BlankTag
from pprint import pprint, pformat
from urllib import urlopen
from lxml import etree
import os
import simplejson

record_tag = '{http://www.loc.gov/MARC21/slim}record'

xml_samples = ['39002054008678.yale.edu', 'flatlandromanceo00abbouoft',
    'nybc200247', 'secretcodeofsucc00stjo', 'warofrebellionco1473unit',
    'zweibchersatir01horauoft', 'onquietcomedyint00brid', '00schlgoog',
    '0descriptionofta1682unit', '1733mmoiresdel00vill', '13dipolarcycload00burk',
    'bijouorannualofl1828cole', 'soilsurveyrepor00statgoog', 'diebrokeradical400poll', 'cu31924091184469',
    'engineercorpsofh00sher',
    ]

bin_samples = ['bpl_0486266893', 'flatlandromanceo00abbouoft_meta.mrc',
    'histoirereligieu05cr_meta.mrc', 'ithaca_college_75002321', 'lc_0444897283',
    'lc_1416500308', 'ocm00400866', 'secretcodeofsucc00stjo_meta.mrc',
    'uoft_4351105_1626', 'warofrebellionco1473unit_meta.mrc', 'wrapped_lines',
    'wwu_51323556', 'zweibchersatir01horauoft_meta.mrc', 'talis_two_authors.mrc',
    'talis_no_title.mrc', 'talis_740.mrc', 'talis_245p.mrc', 'talis_856.mrc',
    'talis_multi_work_tiles.mrc', 'talis_empty_245.mrc', 'ithaca_two_856u.mrc',
    'collingswood_bad_008.mrc', 'collingswood_520aa.mrc', 'upei_broken_008.mrc',
    'upei_short_008.mrc', 'diebrokeradical400poll_meta.mrc', 'cu31924091184469_meta.mrc',
    'engineercorpsofh00sher_meta.mrc', 'henrywardbeecher00robauoft_meta.mrc', 'thewilliamsrecord_vol29b_meta.mrc' ] # 'lc00adam_meta.mrc' ]

class TestParse(unittest.TestCase):
    def test_xml(self):
        for i in xml_samples:
            try:
                expect_filename = 'test_data/xml_expect/' + i + '_marc.xml'
                path = 'test_data/xml_input/' + i + '_marc.xml'
                element = etree.parse(open(path)).getroot()
                if element.tag != record_tag and element[0].tag == record_tag:
                    element = element[0]
                rec = MarcXml(element)
                edition_marc_xml = read_edition(rec)
                assert edition_marc_xml
    #            if i.startswith('engin'):
    #                pprint(edition_marc_xml)
    #                assert False
                j = {}
                if os.path.exists(expect_filename):
                    j = simplejson.load(open(expect_filename))
                    if not j:
                        print expect_filename
                    assert j
                if not j:
                    simplejson.dump(edition_marc_xml, open(expect_filename, 'w'), indent=2)
                    continue
                self.assertEqual(sorted(edition_marc_xml.keys()), sorted(j.keys()))
                for k in edition_marc_xml.keys():
                    print `i, k, edition_marc_xml[k]`
                    self.assertEqual(edition_marc_xml[k], j[k])
                self.assertEqual(edition_marc_xml, j)
            except:
                print 'bad marc:', i
                raise

    def test_binary(self):
        for i in bin_samples:
            try:
                expect_filename = 'test_data/bin_expect/' + i
                data = open('test_data/bin_input/' + i).read()
                if len(data) != int(data[:5]):
                    data = data.decode('utf-8').encode('raw_unicode_escape')
                assert len(data) == int(data[:5])
                rec = MarcBinary(data)
                edition_marc_bin = read_edition(rec)
                assert edition_marc_bin
    #            if i.startswith('engin'):
    #                pprint(edition_marc_bin)
    #                assert False
                j = {}
                if os.path.exists(expect_filename):
                    j = simplejson.load(open(expect_filename))
                    if not j:
                        print expect_filename
                    assert j
                if not j:
                    simplejson.dump(edition_marc_bin, open(expect_filename, 'w'), indent=2)
                    continue
                self.assertEqual(sorted(edition_marc_bin.keys()), sorted(j.keys()))
                for k in edition_marc_bin.keys():
                    if isinstance(j[k], list):
                        for item1, item2 in zip(edition_marc_bin[k], j[k]):
                            #print (i, k, item1)
                            self.assertEqual(item1, item2)

                    self.assertEqual(edition_marc_bin[k], j[k])
                self.assertEqual(edition_marc_bin, j)
            except:
                print 'bad marc:', i
                raise

        i = 'talis_see_also.mrc'
        f = open('test_data/bin_input/' + i)
        rec = MarcBinary(f.read())
        self.assertRaises(SeeAlsoAsTitle, read_edition, rec)

        i = 'talis_no_title2.mrc'
        f = open('test_data/bin_input/' + i)
        rec = MarcBinary(f.read())
        self.assertRaises(NoTitle, read_edition, rec)

########NEW FILE########
__FILENAME__ = test_work_subject
from marc_xml import MarcXml, BadSubtag, BlankTag
from lxml import etree
from marc_binary import MarcBinary
from marc_subject import read_subjects, tidy_subject, four_types
from collections import defaultdict

xml_samples = [
    ('bijouorannualofl1828cole', {}),
    ('flatlandromanceo00abbouoft', {}),
    ('lesabndioeinas00sche', {}),
    ('onquietcomedyint00brid', {}),
    ('zweibchersatir01horauoft', {}),

    ('00schlgoog',
        {'subject': {u'Jewish law': 1}}),
    ('0descriptionofta1682unit', {
        'place': {u'United States': 1},
        'subject': {u"Decedents' estates": 1, u'Taxation': 1, u'S. 1983 97th Congress': 1, u'S. 2479 97th Congress': 1}
    }),
    ('13dipolarcycload00burk',
        {'subject': {u'Allene': 1, u'Ring formation (Chemistry)': 1, u'Trimethylenemethane': 1}}),
    ('1733mmoiresdel00vill',
        {'place': {u'Spain': 1}, 'subject': {u'Courts and court life': 1, u'History': 1}}),
    ('39002054008678.yale.edu',
        {'place': {u'Ontario': 2}, 'subject': {u'Description and travel': 1, u'History': 1}}),
    ('abhandlungender01ggoog',
        {'place': {u'Lusatia': 1, u'Germany': 1}, 'subject': {u'Natural history': 2, u'Periodicals': 1}}),
    ('nybc200247',
        {'person': {u'Simon Dubnow (1860-1941)': 1}, 'subject': {u'Philosophy': 1, u'Jews': 1, u'History': 1}}),
    ('scrapbooksofmoun03tupp', {
        'person': {u'William Vaughn Tupper (1835-1898)': 1},
        'subject': {u'Photographs': 4, u'Sources': 1, u'Description and travel': 2, u'Travel': 1, u'History': 1, u'Travel photography': 1},
        'place': {u'Europe': 3, u'Egypt': 2},
        'time': {u'19th century': 1}}),
    ('secretcodeofsucc00stjo',
        {'subject': {u'Success in business': 1}}),
    ('warofrebellionco1473unit', {
        'time': {u'Civil War, 1861-1865': 2},
        'place': {u'United States': 2, u'Confederate States of America': 1},
        'subject': {u'Sources': 2, u'Regimental histories': 1, u'History': 3}}),
]

bin_samples = [
    ('bpl_0486266893', {}),
    ('flatlandromanceo00abbouoft_meta.mrc', {}),
    ('lc_1416500308', {}),
    ('talis_245p.mrc', {}),
    ('talis_740.mrc', {}),
    ('talis_empty_245.mrc', {}),
    ('talis_multi_work_tiles.mrc', {}),
    ('talis_no_title2.mrc', {}),
    ('talis_no_title.mrc', {}),
    ('talis_see_also.mrc', {}),
    ('talis_two_authors.mrc', {}),
    ('zweibchersatir01horauoft_meta.mrc', {}),

    ('1733mmoiresdel00vill_meta.mrc',
        {'place': {u'Spain': 1}, 'subject': {u'Courts and court life': 1, u'History': 1}}),
    ('collingswood_520aa.mrc',
        {'subject': {u'Learning disabilities': 1, u'People with disabilities': 1, u'Talking books': 1, u'Juvenile literature': 1, u'Juvenile fiction': 3, u'Friendship': 1}}),
    ('collingswood_bad_008.mrc',
        {'subject': {u'War games': 1, u'Battles': 1}}),
    ('histoirereligieu05cr_meta.mrc',
        {'org': {u'Jesuits': 4}, 'subject': {u'Influence': 1, u'History': 1}}),
    ('ithaca_college_75002321', {
        'place': {u'New Jersey': 3},
        'subject': {u'Congresses': 3, u'Negative income tax': 1, u'Guaranteed annual income': 1, u'Labor supply': 1}}),
    ('ithaca_two_856u.mrc', {
        'place': {u'Great Britain': 2},
        'subject': {u'Statistics': 1, u'Periodicals': 2}}),
    ('lc_0444897283',
        {'subject': {u'Shipyards': 1, u'Shipbuilding': 1, u'Data processing': 2, u'Congresses': 3, u'Naval architecture': 1, u'Automation': 1}}),
    ('ocm00400866',
        {'subject': {u'School songbooks': 1, u'Choruses (Mixed voices) with piano': 1}}),
    ('scrapbooksofmoun03tupp_meta.mrc', {
        'person': {u'William Vaughn Tupper (1835-1898)': 1},
        'subject': {u'Photographs': 4, u'Sources': 1, u'Description and travel': 2, u'Travel': 1, u'History': 1, u'Travel photography': 1},
        'place': {u'Europe': 3, u'Egypt': 2},
        'time': {u'19th century': 1}}),
    ('secretcodeofsucc00stjo_meta.mrc',
        {'subject': {u'Success in business': 1}}),
    ('talis_856.mrc',
        {'subject': {u'Politics and government': 1, u'Jewish-Arab relations': 1, u'Middle East': 1, u'Arab-Israeli conflict': 1}, 'time': {u'1945-': 1}}),
    ('uoft_4351105_1626',
        {'subject': {u'Aesthetics': 1, u'History and criticism': 1}}),
    ('upei_broken_008.mrc',
        {'place': {u'West Africa': 1}, 'subject': {u'Social life and customs': 1}}),
    ('upei_short_008.mrc',
        {'place': {u'Charlottetown (P.E.I.)': 1, u'Prince Edward Island': 1}, 'subject': {u'Social conditions': 1, u'Economic conditions': 1, u'Guidebooks': 1, u'Description and travel': 2}}),
    ('warofrebellionco1473unit_meta.mrc',
        {'time': {u'Civil War, 1861-1865': 2}, 'place': {u'United States': 2, u'Confederate States of America': 1}, 'subject': {u'Sources': 2, u'Regimental histories': 1, u'History': 3}}),
    ('wrapped_lines',
        {'org': {u'United States': 1, u'United States. Congress. House. Committee on Foreign Affairs': 1}, 'place': {u'United States': 1}, 'subject': {u'Foreign relations': 1}}),
    ('wwu_51323556',
        {'subject': {u'Statistical methods': 1, u'Spatial analysis (Statistics)': 1, u'Population geography': 1}}),
]

record_tag = '{http://www.loc.gov/MARC21/slim}record'

class TestSubjects:
    def _test_subjects(self, rec, expect):
        print read_subjects(rec)
        assert read_subjects(rec) == expect

    def test_subjects_xml(self):
        for item, expect in xml_samples:
            #print item
            filename = 'test_data/' + item + '_marc.xml'
            element = etree.parse(filename).getroot()
            if element.tag != record_tag and element[0].tag == record_tag:
                element = element[0]
            rec = MarcXml(element)
            yield self._test_subjects, rec, expect

    def test_subjects_bin(self):
        for item, expect in bin_samples:
            filename = 'test_data/' + item

            data = open(filename).read()
            if len(data) != int(data[:5]):
                data = data.decode('utf-8').encode('raw_unicode_escape')
            rec = MarcBinary(data)
            yield self._test_subjects, rec, expect

subjects = []
for item, expect in xml_samples:
    filename = 'test_data/' + item + '_marc.xml'
    element = etree.parse(filename).getroot()
    if element.tag != record_tag and element[0].tag == record_tag:
        element = element[0]
    rec = MarcXml(element)
    subjects.append(read_subjects(rec))

for item, expect in bin_samples:
    filename = 'test_data/' + item

    data = open(filename).read()
    if len(data) != int(data[:5]):
        data = data.decode('utf-8').encode('raw_unicode_escape')
    rec = MarcBinary(data)
    subjects.append(read_subjects(rec))

all_subjects = defaultdict(lambda: defaultdict(int))
for a in subjects:
    for b, c in a.items():
        for d, e in c.items():
            all_subjects[b][d] += e

print four_types(dict((k, dict(v)) for k, v in all_subjects.items()))

########NEW FILE########
__FILENAME__ = xml_to_html
from marc_xml import MarcXml
from lxml import etree
import re

trans = {'&':'&amp;','<':'&lt;','>':'&gt;','\n':'<br>'}
re_html_replace = re.compile('([&<>\n])')

def esc(s):
    return re_html_replace.sub(lambda m: trans[m.group(1)], s)

def esc_sp(s):
    return esc(s).replace(' ', '&nbsp;')

def html_subfields(line):
    return ''.join("<b>$%s</b>%s" % (k, esc(v)) for k, v in line.get_all_subfields())

def html_line(tag, field):
    if tag.startswith('00'):
        s = esc_sp(field)
    else:
        s = esc_sp(field.ind1() + field.ind2()) + ' ' + html_subfields(field)
    return u'<large>' + tag + u'</large> <code>' + s + u'</code>'

class html_record():
    def __init__(self, data):
        root = etree.fromstring(data)
        if root.tag == '{http://www.loc.gov/MARC21/slim}collection':
            root = root[0]
        rec = MarcXml(root)
        self.rec = rec
        self.leader = rec.leader()

    def html(self):
        rec = self.rec
        lines = (html_line(t, rec.decode_field(f)) for t, f in rec.all_fields())
        return '<br>\n'.join(lines)

if __name__ == '__main__':
    samples = '''1893manualofharm00jadauoft 39002054008678.yale.edu flatlandromanceo00abbouoft nybc200247 onquietcomedyint00brid secretcodeofsucc00stjo warofrebellionco1473unit zweibchersatir01horauoft cu31924091184469'''.split()

    for filename in samples:
        print 'test_data/xml_input/' + filename + '_marc.xml'
        data = open('test_data/xml_input/' + filename + '_marc.xml').read()
        if data == '':
            continue
        rec = html_record(data)
        print rec.leader
        print rec.html()
        print

########NEW FILE########
__FILENAME__ = amazon
import re
from names import match_name
from normalize import normalize

re_year = re.compile('(\d{4})$')
re_amazon_title_paren = re.compile('^(.*) \([^)]+?\)$')
re_and_of_space = re.compile(' and | of | ')

isbn_match = 85

def set_isbn_match(score):
    isbn_match = score

def amazon_year(date):
    m = re_year.search(date)
    try:
        assert m
        year = m.group(1)
    except:
        print date
        raise
    return year

def build_amazon(edition, authors):
    amazon = build_titles(full_title(edition))

    amazon['isbn'] = edition['isbn_10']
    if 'publish_date' in edition:
        try:
            amazon['publish_date'] = amazon_year(edition['publish_date'])
        except:
            print edition['isbn_10'], edition['publish_date']
            raise
    if authors:
        amazon['authors'] = authors
    else:
        amazon['authors'] = []
    if 'number_of_pages' in edition:
        amazon['number_of_pages'] = edition['number_of_pages']
    if 'publishers' in edition:
        amazon['publishers'] = edition['publishers']

    return amazon


def build_titles(title):
    normalized_title = normalize(title).lower()
    titles = [ title, normalized_title ];
    if title.find(' & ') != -1:
        t = title.replace(" & ", " and ")
        titles.append(t)
        titles.append(normalize(t))
    t2 = []
    for t in titles:
        if t.lower().startswith('the '):
            t2.append(t[4:])
        elif t.lower().startswith('a '):
            t2.append(t[2:])
    titles += t2

    if re_amazon_title_paren.match(title):
        t2 = []
        for t in titles:
            m = re_amazon_title_paren.match(t)
            if m:
                t2.append(m.group(1))
                t2.append(normalize(m.group(1)))
        titles += t2

    return {
        'full_title':       title,
        'normalized_title': normalized_title,
        'titles':           titles,
        'short_title':      normalized_title[:25],
    }

def within(a, b, distance):
    return abs(a-b) <= distance

def compare_date(e1, e2):
    if 'publish_date' not in e1 or 'publish_date' not in e2:
        return ('publish_date', 'value missing', 0)
    if e1['publish_date'] == e2['publish_date']:
        return ('publish_date', 'exact match', 200)
    try:
        e1_pub = int(e1['publish_date'])
        e2_pub = int(e2['publish_date'])
        if within(e1_pub, e2_pub, 1):
            return ('publish_date', 'within 1 year', 100)
        elif within(e1_pub, e2_pub, 2):
            return ('publish_date', '+/-2 years', -25)
        else:
            return ('publish_date', 'mismatch', -250)
    except ValueError, TypeError:
        return ('publish_date', 'mismatch', -250)

def compare_isbn10(e1, e2):
    if len(e1['isbn']) == 0 or len(e2['isbn']) == 0:
        return ('isbn', 'missing', 0)
    for i in e1['isbn']:
        for j in e2['isbn']:
            if i == j:
                return ('isbn', 'match', isbn_match)

    return ('ISBN', 'mismatch', -225)

def level1_merge(e1, e2):
    score = []
    if e1['short_title'] == e2['short_title']:
        score.append(('short_title', 'match', 450))
    else:
        score.append(('short_title', 'mismatch', 0))

    score.append(compare_date(e1, e2))
    score.append(compare_isbn10(e1, e2))
    return score

def compare_authors(amazon, marc):
    if len(amazon['authors']) == 0 and 'authors' not in marc:
        return ('authors', 'no authors', 75)
    if len(amazon['authors']) == 0:
        return ('authors', 'field missing from one record', -25)

    for name in amazon['authors']:
        if 'authors' in marc and match_name(name, marc['authors'][0]['name']):
            return ('authors', 'exact match', 125)
        if 'by_statement' in marc and marc['by_statement'].find(name) != -1:
            return ('authors', 'exact match', 125)
    if 'authors' not in marc:
        return ('authors', 'field missing from one record', -25)

    max_score = 0
    for a in amazon['authors']:
        percent, ordered = keyword_match(a[0], marc['authors'][0]['name'])
        if percent > 0.50:
            score = percent * 80
            if ordered:
                score += 10
            if score > max_score:
                max_score = score
    if max_score:
        return ('authors', 'keyword match', max_score)
    else:
        return ('authors', 'mismatch', -200)

def title_replace_amp(amazon):
    return normalize(amazon['full-title'].replace(" & ", " and ")).lower()

def substr_match(a, b):
    return a.find(b) != -1 or b.find(a) != -1

def keyword_match(in1, in2):
    s1, s2 = [i.split() for i in in1, in2]
    s1_set = set(s1)
    s2_set = set(s2)
    match = s1_set & s2_set
    if len(s1) == 0 and len(s2) == 0:
        return 0, True
    ordered = [x for x in s1 if x in match] == [x for x in s2 if x in match]
    return float(len(match)) / max(len(s1), len(s2)), ordered

def strip_and_compare(t1, t2):
    t1 = re_and_of_space.sub('', t1).lower()
    t2 = re_and_of_space.sub('', t2).lower()
    return t1 == t2

def compare_title(amazon, marc):
    amazon_title = amazon['normalized_title'].lower()
    marc_title = normalize(marc['full_title']).lower()
    short = False
    if len(amazon_title) < 9 or len(marc_title) < 9:
        short = True

    if not short:
        for a in amazon['titles']:
            for m in marc['titles']:
                if a.lower() == m.lower():
                    return ('full_title', 'exact match', 600)
                if strip_and_compare(a, m):
                    return ('full_title', 'exact match', 600)

        for a in amazon['titles']:
            for m in marc['titles']:
                if substr_match(a.lower(), m.lower()):
                    return ('full_title', 'contained within other title', 350)

    max_score = 0
    for a in amazon['titles']:
        for m in marc['titles']:
            percent, ordered = keyword_match(a, m)
            score = percent * 450
            if ordered:
                score += 50
            if score and score > max_score:
                max_score = score
    if max_score:
        return ('full_title', 'keyword match', max_score)
    elif short:
        return ('full_title', 'shorter than 9 characters', 0)
    else:
        return ('full_title', 'mismatch', -600)

def compare_number_of_pages(amazon, marc):
    if 'number_of_pages' not in amazon or 'number_of_pages' not in marc:
        return
    amazon_pages = amazon['number_of_pages']
    marc_pages = marc['number_of_pages']
    if amazon_pages == marc_pages:
        if amazon_pages > 10:
            return ('number_of_pages', 'match exactly and > 10', 100)
        else:
            return ('number_of_pages', 'match exactly and < 10', 50)
    elif within(amazon_pages, marc_pages, 10):
        if amazon_pages > 10 and marc_pages > 10:
            return ('number_of_pages', 'match within 10 and both are > 10', 50)
        else:
            return ('number_of_pages', 'match within 10 and either are < 10', 20)
    else:
        return ('number_of_pages', 'non-match (by more than 10)', -225)

def short_part_publisher_match(p1, p2):
    pub1 = p1.split()
    pub2 = p2.split()
    if len(pub1) == 1 or len(pub2) == 1:
        return False
    for i, j in zip(pub1, pub2):
        if not substr_match(i, j):
            return False
    return True

re_press = re.compile(' press$')

def compare_publisher(amazon, marc):
    if 'publishers' not in amazon or 'publishers' not in marc:
        return ('publishers', 'either missing', 0)

    assert 'publishers' in amazon and 'publishers' in marc
    for amazon_pub in amazon['publishers']:
        norm_amazon = normalize(amazon_pub)
        for marc_pub in marc['publishers']:
            norm_marc = normalize(marc_pub)
            if norm_amazon == norm_marc:
                return ('publishers', 'match', 100)
#            if re_press.sub('', norm_amazon) == re_press.sub('', norm_marc):
#                return ('publishers', 'match', 100)
            if substr_match(norm_amazon, norm_marc):
                return ('publishers', 'occur within the other', 100)
            if substr_match(norm_amazon.replace(' ', ''), norm_marc.replace(' ', '')):
                return ('publishers', 'occur within the other', 100)
            if short_part_publisher_match(norm_amazon, norm_marc):
                return ('publishers', 'match', 100)
    return ('publishers', 'mismatch', -25)

def level2_merge(amazon, marc):
    score = []
    score.append(compare_date(amazon, marc))
    score.append(compare_isbn10(amazon, marc))
    score.append(compare_title(amazon, marc))
    page_score = compare_number_of_pages(amazon, marc)
    if page_score:
        score.append(page_score)

    score.append(compare_publisher(amazon, marc))
    score.append(compare_authors(amazon, marc))

    return score

def full_title(edition):
    title = edition['title']
    if 'subtitle' in edition:
        title += ' ' + edition['subtitle']
    return title

def test_full_title():
    assert full_title({ 'title': "Hamlet"}) == "Hamlet"
    edition = {
        'title': 'Flatland',
        'subtitle': 'A Romance of Many Dimensions',
    }
    assert full_title(edition) == "Flatland A Romance of Many Dimensions"

def test_merge_titles():
    marc = {
        'title_with_subtitles': 'Spytime : the undoing of James Jesus Angleton : a novel',
        'title': 'Spytime',
        'full_title': 'Spytime : the undoing of James Jesus Angleton : a novel',
    }
    amazon = {
        'subtitle': 'The Undoing oF James Jesus Angleton',
        'title': 'Spytime',
    }

    amazon = build_titles(unicode(full_title(amazon)))
    marc = build_titles(marc['title_with_subtitles'])
    assert amazon['short_title'] == marc['short_title']
    assert compare_title(amazon, marc) == ('full-title', 'containted within other title', 350)

def test_merge_titles2():
    amazon = {'title': u'Sea Birds Britain Ireland'}
    marc = {
        'title_with_subtitles': u'seabirds of Britain and Ireland',
        'title': u'seabirds of Britain and Ireland', 
        'full_title': u'The seabirds of Britain and Ireland',
    }
    amazon = build_titles(unicode(full_title(amazon)))
    marc = build_titles(marc['title_with_subtitles'])
    assert compare_title(amazon, marc) == ('full-title', 'exact match', 600)

def attempt_merge(amazon, marc, threshold, debug = False):
    l1 = level1_merge(amazon, marc)
    total = sum(i[2] for i in l1)
    if debug:
        print total, l1
    if total >= threshold:
        return True
    l2 = level2_merge(amazon, marc)
    total = sum(i[2] for i in l2)
    if debug:
        print total, l2
    return total >= threshold

def test_merge():
    amazon = {'publishers': [u'Collins'], 'isbn_10': ['0002167360'], 'number_of_pages': 120, 'short_title': u'souvenirs', 'normalized_title': u'souvenirs', 'full_title': u'Souvenirs', 'titles': [u'Souvenirs', u'souvenirs'], 'publish_date': u'1975', 'authors': [(u'David Hamilton', u'Photographer')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002167360'], 'short_title': u'souvenirs', 'normalized_title': u'souvenirs', 'full_title': u'Souvenirs', 'titles': [u'Souvenirs', u'souvenirs'], 'publish_date': '1978', 'authors': [{'birth_date': u'1933', 'db_name': u'Hamilton, David 1933-', 'entity_type': 'person', 'name': u'Hamilton, David', 'personal_name': u'Hamilton, David'}], 'source_record_loc': 'marc_records_scriblio_net/part11.dat:155728070:617', 'number_of_pages': 120}

    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_merge2():
    amazon = {'publishers': [u'Collins'], 'isbn_10': ['0002167530'], 'number_of_pages': 287, 'short_title': u'sea birds britain ireland', 'normalized_title': u'sea birds britain ireland', 'full_title': u'Sea Birds Britain Ireland', 'titles': [u'Sea Birds Britain Ireland', u'sea birds britain ireland'], 'publish_date': u'1975', 'authors': [(u'Stanley Cramp', u'Author')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002167530'], 'short_title': u'seabirds of britain and i', 'normalized_title': u'seabirds of britain and ireland', 'full_title': u'seabirds of Britain and Ireland', 'titles': [u'seabirds of Britain and Ireland', u'seabirds of britain and ireland'], 'publish_date': '1974', 'authors': [{'db_name': u'Cramp, Stanley.', 'entity_type': 'person', 'name': u'Cramp, Stanley.', 'personal_name': u'Cramp, Stanley.'}], 'source_record_loc': 'marc_records_scriblio_net/part08.dat:61449973:855'}

    threshold = 735
    #assert attempt_merge(amazon, marc, threshold)

def test_merge3():
    amazon = {'publishers': [u'Intl Specialized Book Service Inc'], 'isbn_10': ['0002169770'], 'number_of_pages': 207, 'short_title': u'women of the north', 'normalized_title': u'women of the north', 'full_title': u'Women of the North', 'titles': [u'Women of the North', u'women of the north'], 'publish_date': u'1985', 'authors': [(u'Jane Wordsworth', u'Author')]}
    marc = {'publisher': [u'Collins', u'Exclusive distributor ISBS'], 'isbn_10': [u'0002169770'], 'short_title': u'women of the north', 'normalized_title': u'women of the north', 'full_title': u'Women of the North', 'titles': [u'Women of the North', u'women of the north'], 'publish_date': '1981', 'number_of_pages': 207, 'authors': [{'db_name': u'Wordsworth, Jane.', 'entity_type': 'person', 'name': u'Wordsworth, Jane.', 'personal_name': u'Wordsworth, Jane.'}], 'source_record_loc': 'marc_records_scriblio_net/part17.dat:110989084:798'}

    threshold = 735
#    assert attempt_merge(amazon, marc, threshold)

def test_merge4():
    amazon = {'publishers': [u'HarperCollins Publishers Ltd'], 'isbn_10': ['0002173433'], 'number_of_pages': 128, 'short_title': u'd day to victory', 'normalized_title': u'd day to victory', 'full_title': u'D-Day to Victory', 'titles': [u'D-Day to Victory', u'd day to victory'], 'publish_date': u'1984', 'authors': [(u'Wynfod Vaughan-Thomas', u'Editor, Introduction')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002173433'], 'short_title': u'great front pages  d day ', 'normalized_title': u'great front pages  d day to victory 1944 1945', 'full_title': u'Great front pages : D-Day to victory 1944-1945', 'titles': [u'Great front pages : D-Day to victory 1944-1945', u'great front pages  dday to victory 1944 1945'], 'publish_date': '1984', 'number_of_pages': 128, 'by_statement': 'introduced by Wynford Vaughan-Thomas.', 'source_record_loc': 'marc_records_scriblio_net/part17.dat:102360356:983'}

    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_merge5():
    amazon = {'publishers': [u'HarperCollins Publishers (Australia) Pty Ltd'], 'isbn_10': ['0002174049'], 'number_of_pages': 120, 'short_title': u'netherlandish and german ', 'normalized_title': u'netherlandish and german paintings national gallery schools of painting', 'full_title': u'Netherlandish and German Paintings (National Gallery Schools of Painting)', 'titles': [u'Netherlandish and German Paintings (National Gallery Schools of Painting)', u'netherlandish and german paintings national gallery schools of painting', u'Netherlandish and German Paintings', u'netherlandish and german paintings'], 'publish_date': u'1985', 'authors': [(u'Alistair Smith', u'Author')]}
    marc = {'publisher': [u'National Gallery in association with W. Collins'], 'isbn_10': [u'0002174049'], 'short_title': u'early netherlandish and g', 'normalized_title': u'early netherlandish and german paintings', 'full_title': u'Early Netherlandish and German paintings', 'titles': [u'Early Netherlandish and German paintings', u'early netherlandish and german paintings'], 'publish_date': '1985', 'authors': [{'db_name': u'National Gallery (Great Britain)', 'name': u'National Gallery (Great Britain)', 'entity_type': 'org'}], 'number_of_pages': 116, 'by_statement': 'Alistair Smith.', 'source_record_loc': 'marc_records_scriblio_net/part17.dat:170029527:1210'}
    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_compare_authors():
    amazon = {'authors': [(u'Alistair Smith', u'Author')]}
    marc = {'authors': [{'db_name': u'National Gallery (Great Britain)', 'name': u'National Gallery (Great Britain)', 'entity_type': 'org'}], 'by_statement': 'Alistair Smith.'}
    assert compare_authors(amazon, marc) == ('authors', 'exact match', 125)

def test_merge6():
    amazon = {'publishers': [u'Fount'], 'isbn_10': ['0002176157'], 'number_of_pages': 224, 'short_title': u'basil hume', 'normalized_title': u'basil hume', 'full_title': u'Basil Hume', 'titles': [u'Basil Hume', u'basil hume'], 'publish_date': u'1986', 'authors': [(u'Tony Castle', u'Editor')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002176157'], 'short_title': u'basil hume  a portrait', 'normalized_title': u'basil hume  a portrait', 'full_title': u'Basil Hume : a portrait', 'titles': [u'Basil Hume : a portrait', u'basil hume  a portrait'], 'number_of_pages': 158, 'publish_date': '1986', 'by_statement': 'edited by Tony Castle.', 'source_record_loc': 'marc_records_scriblio_net/part19.dat:39883132:951'}
    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_merge7():
    amazon = {'publishers': [u'HarperCollins Publishers Ltd'], 'isbn_10': ['0002176319'], 'number_of_pages': 256, 'short_title': u'pucklers progress', 'normalized_title': u'pucklers progress', 'full_title': u"Puckler's Progress", 'titles': [u"Puckler's Progress", u'pucklers progress'], 'publish_date': u'1987', 'authors': [(u'Flora Brennan', u'Editor')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002176319'], 'short_title': u'pucklers progress  the ad', 'normalized_title': u'pucklers progress  the adventures of prince puckler muskau in england wales and ireland as told in letters to his former wife 1826 9', 'full_title': u"Puckler's progress : the adventures of Prince Pu\u0308ckler-Muskau in England, Wales, and Ireland as told in letters to his former wife, 1826-9", 'titles': [u"Puckler's progress : the adventures of Prince Pu\u0308ckler-Muskau in England, Wales, and Ireland as told in letters to his former wife, 1826-9", u'pucklers progress  the adventures of prince puckler muskau in england wales and ireland as told in letters to his former wife 1826 9'], 'publish_date': '1987', 'authors': [{'name': u'Pu\u0308ckler-Muskau, Hermann Furst von', 'title': u'Furst von', 'death_date': u'1871.', 'db_name': u'Pu\u0308ckler-Muskau, Hermann Furst von 1785-1871.', 'birth_date': u'1785', 'personal_name': u'Pu\u0308ckler-Muskau, Hermann', 'entity_type': 'person'}], 'number_of_pages': 254, 'by_statement': 'translated by Flora Brennan.', 'source_record_loc': 'marc_records_scriblio_net/part19.dat:148554594:1050'}
    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_compare_publisher():
    amazon = { 'publishers': ['foo'] }
    amazon2 = { 'publishers': ['bar'] }
    marc = { 'publishers': ['foo'] }
    marc2 = { 'publishers': ['foo', 'bar'] }
    assert compare_publisher({}, {}) == ('publisher', 'either missing', 0)
    assert compare_publisher(amazon, {}) == ('publisher', 'either missing', 0)
    assert compare_publisher({}, marc) == ('publisher', 'either missing', 0)
    assert compare_publisher(amazon, marc) == ('publisher', 'match', 100)
    assert compare_publisher(amazon2, marc) == ('publisher', 'mismatch', -25)
    assert compare_publisher(amazon2, marc2) == ('publisher', 'match', 100)

def test_merge8():
    amazon = {'publishers': [u'Shambhala'], 'isbn': [u'1590301390'], 'number_of_pages': 144, 'short_title': u'the spiritual teaching of', 'normalized_title': u'the spiritual teaching of ramana maharshi', 'full_title': u'The Spiritual Teaching of Ramana Maharshi', 'titles': [u'The Spiritual Teaching of Ramana Maharshi', u'the spiritualteaching of ramana maharshi', u'Spiritual Teaching of Ramana Maharshi', u'spiritual teaching of ramana maharshi'], 'publish_date': u'2004', 'authors': [u'Ramana Maharshi.']}
    marc = {'isbn': [], 'number_of_pages': 180, 'short_title': 'the spiritual teaching of', 'normalized_title': 'the spiritual teaching of mary of the incarnation', 'full_title': 'The spiritual teaching of Mary of the Incarnation', 'titles': ['The spiritual teaching of Mary of the Incarnation', 'the spiritual teaching of mary of the incarnation', 'spiritual teaching of Mary of the Incarnation', 'spiritual teaching of mary of the incarnation'], 'publish_date': '1963', 'publish_country': 'nyu', 'authors': [{'db_name': 'Jett\xc3\xa9, Fernand.', 'name': 'Jett\xc3\xa9, Fernand.'}]}
    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

#test_merge8()

########NEW FILE########
__FILENAME__ = build_db
#!/usr/bin/python2.5
# converts from text files containing MARC tags to text versions of merge pools

import re, anydbm
from time import time
from collections import defaultdict

grand_total = 31388611

archive = [
    ('marc_western_washington_univ', 'Western Washington University', 737442),
    ('marc_records_scriblio_net', 'Library of Congress', 7025345),
    ('marc_university_of_toronto', 'University of Toronto', 5585777),
    ('marc_miami_univ_ohio', 'Miami University', 2029148),
    ('bcl_marc', 'Boston College', 1983155),
    ('bpl_marc', 'Boston Public Library', 2165372),
    ('unc_catalog_marc', 'University of North Carolina at Chapel Hill', 3738594),
    ('marc_oregon_summit_records', 'Oregon Summit', 3275046),
    ('talis_openlibrary_contribution', 'Talis', 4848732),
]

path = '/0/pharos/edward/db/'

re_isbn = re.compile('([^ ()]+[\dX])(?: \((?:v\. (\d+)(?: : )?)?(.*)\))?')
re_question = re.compile('^\?+$')
re_lccn = re.compile('(...\d+).*')
re_oclc = re.compile ('^\(OCoLC\).*?0*(\d+)', re.IGNORECASE)

re_normalize = re.compile('[^\w ]')
re_whitespace = re.compile('\s+')

def normalize(s):
    s = re_normalize.sub('', s.strip())
    s = re_whitespace.sub(' ', s)
    return s.lower()

def add_to_map(d, k, loc):
    d[k].append(loc)

def add_title_to_map(title, loc):
    title = str(normalize(title)[:25])
    add_to_map(title_map, title, loc)

def add_title(prefix_len, subtags):
    title_and_subtitle = []
    title = []
    for k, v in subtags:
        if k not in ('a', 'b'):
            continue
        v = v.strip(' /,;:')
        title_and_subtitle.append(v)
        if k == 'a':
            title.append(v)
    
    titles = [' '.join(title)]
    if title != title_and_subtitle:
        titles.append(' '.join(title_and_subtitle))
    if prefix_len and prefix_len != '0':
        try:
            prefix_len = int(prefix_len)
            titles += [t[prefix_len:] for t in titles]
        except ValueError:
            pass
    return titles

loc_num = 0

def write_map(archive_id, name, d):
    f = open('d/' + archive_id + '_' + name, 'w')
    for k, v in d.iteritems():
        f.write(k + '\t' + ' '.join([str(i) for i in v]) + '\n')
    f.close()

def add_record(edition):
    global loc_num
    loc_str, tags = edition
    loc_num+=1
    f_loc.write(str(loc_num) + ' ' + loc_str + '\n')
    loc = loc_num
    for tag, ind, subtags in tags:
        if tag == '010':
            for k, v in subtags:
                lccn = v.strip()
                if re_question.match(lccn):
                    continue
                m = re_lccn.search(lccn)
                if m:
                    add_to_map(lccn_map, m.group(1), loc)
            continue
        if tag == '020':
            for k, v in subtags:
                m = re_isbn.match(v)
                if m:
                    add_to_map(isbn_map, m.group(1), loc)
            continue
        if tag == '035':
            for k, v in subtags:
                m = re_oclc.match(v)
                if m:
                    add_to_map(oclc_map, m.group(1), loc)
            continue
        if tag == '245':
            for t in add_title(ind[1], subtags):
                add_title_to_map(t, loc)
            continue

overall = 0
t0_overall = time()
f_loc = open('d/loc_map', 'w')
for archive_id, name, total in sorted(archive, key=lambda x: x[2]):
    t0 = time()
    i = 0

    isbn_map = defaultdict(list)
    lccn_map = defaultdict(list)
    title_map = defaultdict(list)
    oclc_map = defaultdict(list)
    print archive_id
    for line in open(archive_id):
        rec = eval(line)
        add_record(rec)
        i+=1
        overall+=1
        if i % 10000 == 0:
            t1 = time() - t0
            t1_overall = time() - t0_overall
            remaining = total - i
            remaining2 = grand_total - overall
            print "%8d %6.2f%% %5.3f rec/sec %.3f minutes left" % (i, (float(i) * 100) / total, i/t1, float((t1/i) * remaining) / 60),
            print "overall: %6.2f%% %.3f minutes left" % ((float(overall) * 100) / grand_total, float((t1_overall/overall) * remaining2) / 60)

    print archive_id
    write_map(archive_id, 'isbn', isbn_map)
    write_map(archive_id, 'lccn', lccn_map)
    write_map(archive_id, 'title', title_map)
    write_map(archive_id, 'oclc', oclc_map)

f_loc.close()
print 'end'

########NEW FILE########
__FILENAME__ = index
from normalize import normalize
from time import time
import re

def add_to_index(dbm, key, edition_key):
    if not key:
        return
    try:
        key = str(key)
    except UnicodeEncodeError:
        return
    if key in dbm:
        dbm[key] += ' ' + edition_key
    else:
        dbm[key] = edition_key

def short_title(s):
    return normalize(s)[:25]

re_letters = re.compile('[A-Za-z]')

def clean_lccn(lccn):
    return re_letters.sub('', lccn).strip()

re_isbn = re.compile('([-0-9X]{10,})')

def clean_isbn(isbn):
    m = re_isbn.search(isbn)
    if m:
        return m.group(1).replace('-', '')

def record_to_dbm(record, dbm):
    def callback(field, value, key):
        add_to_index(dbm[field], value, key)
    read_record(record, callback)

def read_record(record, callback):
    if 'title' not in record or record['title'] is None:
        return
    if 'subtitle' in record and record['subtitle'] is not None:
        title = record['title'] + ' ' + record['subtitle']
    else:
        title = record['title']
    key = record['key']
    callback('title', short_title(title), key)
    if 'title_prefix' in record and record['title_prefix'] is not None:
        title2 = short_title(record['title_prefix'] + title)
        callback('title', title2, key)

    fields = [
        ('lccn', 'lccn', clean_lccn),
        ('oclc_numbers', 'oclc', None),
        ('isbn_10', 'isbn', clean_isbn),
        ('isbn_13', 'isbn', None),
    ]
    for a, b, clean in fields:
        if a not in record:
            continue
        for v in record[a]:
            if not v:
                continue
            if clean:
                v = clean(v)
                if not v:
                    continue
            callback(b, v, key)

def test_read_record():
    def empty_dbm():
        return dict((i, {}) for i in ('lccn', 'oclc', 'isbn', 'title'))

    dbm = empty_dbm()

    line = '{"title_prefix": null, "subtitle": null, "description": null, "language": null, "title": "Metamagical Themas", "by_statement": null, "notes": null, "language_code": null, "id": 9888119, "edition_name": null, "publish_date": null, "key": "/b/OL7254007M", "authors": [{"key": "/a/OL2621476A"}], "ocaid": null, "type": "/type/edition", "coverimage": null}'
    line = line.replace('null', 'None')
    record = eval(line)
    read_record(record, dbm)
    assert dbm == { 'lccn': {}, 'isbn': {}, 'oclc': {}, 'title': {'metamagical themas': '9888119'} }

    record = {"pagination": "8, 304 p.", "description": "Test", "title": "Kabita\u0304.", "lccn": ["sa 64009056"], "notes": "Bibliographical footnotes.\r\nIn Oriya.", "number_of_pages": 304, "languages": [{"key": "/l/ori"}], "authors": [{"key": "/a/OL1A"}], "lc_classifications": ["PK2579.R255 K3"], "publish_date": "1962", "publish_country": "ii ", "key": "/b/OL1M", "language_code": "304", "coverimage": "/static/images/book.trans.gif", "oclc_numbers": ["31249133"], "type": "/type/edition", "id": 96}
    dbm = empty_dbm()
    read_record(record, dbm)
    assert dbm == {'lccn': {'64009056': '96'}, 'isbn': {}, 'oclc': {'31249133': '96'}, 'title': {'kabitau0304': '96'}}

########NEW FILE########
__FILENAME__ = load_from_json
# build a merge database from JSON dump

import simplejson, re
from normalize import normalize
from time import time

re_escape = re.compile(r'[\n\r\t\0\\]')
trans = { '\n': '\\n', '\r': '\\r', '\t': '\\t', '\\': '\\\\', '\0': '', }

def esc_group(m):
    return trans[m.group(0)]
def esc(str): return re_escape.sub(esc_group, str)

def add_to_index(fh, value, key):
    if not value:
        return
    try:
        value = str(value)
    except UnicodeEncodeError:
        return
    print >> fh, "\t".join([key, esc(value)])

def short_title(s):
    return normalize(s)[:25]

re_letters = re.compile('[A-Za-z]')
re_dash_or_space = re.compile('[- ]')

def clean_lccn(lccn):
    return re_letters.sub('', lccn).strip()

def clean_isbn(isbn):
    return re_dash_or_space.sub('', isbn)

def load_record(record, f):
    if 'title' not in record or record['title'] is None:
        return
    if 'subtitle' in record and record['subtitle'] is not None:
        title = record['title'] + ' ' + record['subtitle']
    else:
        title = record['title']
    key = record['key']
    add_to_index(f['title'], short_title(title), key)
    if 'title_prefix' in record and record['title_prefix'] is not None:
        title2 = short_title(record['title_prefix'] + title)
        add_to_index(f['title'], title2, key)

    fields = [
        ('lccn', 'lccn', clean_lccn),
        ('oclc_numbers', 'oclc', None),
        ('isbn_10', 'isbn', clean_isbn),
        ('isbn_13', 'isbn', clean_isbn),
    ]
    for a, b, clean in fields:
        if a not in record:
            continue
        for v in record[a]:
            if not v or b=='isbn' and len(v) < 10:
                continue
            if clean:
                v = clean(v)
            add_to_index(f[b], v, key)

total = 29107946 # FIXME

path = '/1/edward/index/'
index_fields = ('lccn', 'oclc', 'isbn', 'title')
files = dict((i, open(path + i, 'w')) for i in index_fields)

rec_no = 0
chunk = 10000
t0 = time()
t_prev = time()

filename = '/1/anand/bsddb/json.txt'
for line in open(filename):
    rec_no += 1

    if rec_no % chunk == 0:
        t = time() - t_prev
        t_prev = time()
        t1 = time() - t0
        rec_per_sec = chunk / t
        rec_per_sec_total = rec_no / t1
        remaining = total - rec_no
        sec = remaining / rec_per_sec_total
        print "%d current: %.3f overall: %.3f" % \
            (rec_no, rec_per_sec, rec_per_sec_total),
        mins = sec / 60
        print "%.3f minutes left" % mins

    # split line
    key, type, json_data = line.split('\t')
    if type != '/type/edition':
        continue
    try:
        rec = simplejson.loads(json_data)
        load_record(rec, files)
    except:
        print 'record number:', rec_no
        print line
        raise

print rec_no
print "closing files"
for v in files.values():
    v.close()
print "finished"

########NEW FILE########
__FILENAME__ = merge
import re
from names import match_name
from normalize import normalize

re_year = re.compile('(\d{4})$')
re_amazon_title_paren = re.compile('^(.*) \([^)]+?\)$')
re_and_of_space = re.compile(' and | of | ')

isbn_match = 85

def set_isbn_match(score):
    isbn_match = score

def amazon_year(date):
    m = re_year.search(date)
    try:
        assert m
        year = m.group(1)
    except:
        print date
        raise
    return year

def build_amazon(edition, author):
    amazon = merge.build_titles(full_title(edition))

    amazon['isbn'] = editon['isbn_10']
    if 'publish_date' in edition:
        amazon['publish_date'] = merge.amazon_year(edition['publish_date'])
    if authors:
        amazon['authors'] = authors
    else:
        amazon['authors'] = []
    if 'number_of_pages' in edition:
        amazon['number_of_pages'] = edition['number_of_pages']
    if 'publishers' in edition:
        assert len(edition['publishers']) == 1
        amazon['publisher'] = edition['publishers'][0]

    return amazon


def build_titles(title):
    normalized_title = normalize(title).lower()
    titles = [ title, normalized_title ];
    if title.find(' & ') != -1:
        t = title.replace(" & ", " and ")
        titles.append(t)
        titles.append(normalize(t))
    t2 = []
    for t in titles:
        if t.lower().startswith('the '):
            t2.append(t[4:])
        elif t.lower().startswith('a '):
            t2.append(t[2:])
    titles += t2

    if re_amazon_title_paren.match(title):
        t2 = []
        for t in titles:
            m = re_amazon_title_paren.match(t)
            if m:
                t2.append(m.group(1))
                t2.append(normalize(m.group(1)))
        titles += t2

    return {
        'full_title':       title,
        'normalized_title': normalized_title,
        'titles':           titles,
        'short_title':      normalized_title[:25],
    }

def within(a, b, distance):
    return abs(a-b) <= distance

def compare_date(e1, e2):
    if 'publish_date' not in e1 or 'publish_date' not in e2:
        return ('date', 'value missing', 0)
    if e1['publish_date'] == e2['publish_date']:
        return ('date', 'exact match', 200)
    try:
        e1_pub = int(e1['publish_date'])
        e2_pub = int(e2['publish_date'])
        if within(e1_pub, e2_pub, 1):
            return ('date', 'within 1 year', 100)
        elif within(e1_pub, e2_pub, 2):
            return ('date', '+/-2 years', -25)
        else:
            return ('date', 'mismatch', -250)
    except ValueError, TypeError:
        return ('date', 'mismatch', -250)

def compare_isbn10(e1, e2):
    if len(e1['isbn']) == 0 or len(e2['isbn']) == 0:
        return ('ISBN', 'missing', 0)
    for i in e1['isbn']:
        for j in e2['isbn']:
            if i == j:
                return ('ISBN', 'match', isbn_match)

    return ('ISBN', 'mismatch', -225)

def level1_merge(e1, e2):
    score = []
    if e1['short_title'] == e2['short_title']:
        score.append(('short-title', 'match', 450))
    else:
        score.append(('short-title', 'mismatch', 0))

    score.append(compare_date(e1, e2))
    score.append(compare_isbn10(e1, e2))
    return score

def compare_authors(amazon, marc):
    if len(amazon['authors']) == 0 and 'authors' not in marc:
        return ('main', 'no authors', 75)
    if len(amazon['authors']) == 0:
        return ('main', 'field missing from one record', -25)

    for name in amazon['authors']:
        if 'authors' in marc and match_name(name, marc['authors'][0]['name']):
            return ('main', 'exact match', 125)
        if 'by_statement' in marc and marc['by_statement'].find(name) != -1:
            return ('main', 'exact match', 125)
    if 'authors' not in marc:
        return ('main', 'field missing from one record', -25)

    max_score = 0
    for a in amazon['authors']:
        percent, ordered = keyword_match(a[0], marc['authors'][0]['name'])
        if percent > 0.50:
            score = percent * 80
            if ordered:
                score += 10
            if score > max_score:
                max_score = score
    if max_score:
        return ('main', 'keyword match', max_score)
    else:
        return ('main', 'mismatch', -200)

def title_replace_amp(amazon):
    return normalize(amazon['full-title'].replace(" & ", " and ")).lower()

def substr_match(a, b):
    return a.find(b) != -1 or b.find(a) != -1

def keyword_match(in1, in2):
    s1, s2 = [i.split() for i in in1, in2]
    s1_set = set(s1)
    s2_set = set(s2)
    match = s1_set & s2_set
    if len(s1) == 0 and len(s2) == 0:
        return 0, True
    ordered = [x for x in s1 if x in match] == [x for x in s2 if x in match]
    return float(len(match)) / max(len(s1), len(s2)), ordered

def strip_and_compare(t1, t2):
    t1 = re_and_of_space.sub('', t1).lower()
    t2 = re_and_of_space.sub('', t2).lower()
    return t1 == t2

def compare_title(amazon, marc):
    amazon_title = amazon['normalized_title'].lower()
    marc_title = normalize(marc['full_title']).lower()
    short = False
    if len(amazon_title) < 9 or len(marc_title) < 9:
        short = True

    if not short:
        for a in amazon['titles']:
            for m in marc['titles']:
                if a.lower() == m.lower():
                    return ('full-title', 'exact match', 600)
                if strip_and_compare(a, m):
                    return ('full-title', 'exact match', 600)

        for a in amazon['titles']:
            for m in marc['titles']:
                if substr_match(a.lower(), m.lower()):
                    return ('full-title', 'containted within other title', 350)

    max_score = 0
    for a in amazon['titles']:
        for m in marc['titles']:
            percent, ordered = keyword_match(a, m)
            score = percent * 450
            if ordered:
                score += 50
            if score and score > max_score:
                max_score = score
    if max_score:
        return ('full-title', 'keyword match', max_score)
    elif short:
        return ('full-title', 'shorter than 9 characters', 0)
    else:
        return ('full-title', 'mismatch', -600)

def compare_number_of_pages(amazon, marc):
    if 'number_of_pages' not in amazon or 'number_of_pages' not in marc:
        return
    amazon_pages = amazon['number_of_pages']
    marc_pages = marc['number_of_pages']
    if amazon_pages == marc_pages:
        if amazon_pages > 10:
            return ('pagination', 'match exactly and > 10', 100)
        else:
            return ('pagination', 'match exactly and < 10', 50)
    elif within(amazon_pages, marc_pages, 10):
        if amazon_pages > 10 and marc_pages > 10:
            return ('pagination', 'match within 10 and both are > 10', 50)
        else:
            return ('pagination', 'match within 10 and either are < 10', 20)
    else:
        return ('pagination', 'non-match (by more than 10)', -225)

def short_part_publisher_match(p1, p2):
    pub1 = p1.split()
    pub2 = p2.split()
    if len(pub1) == 1 or len(pub2) == 1:
        return False
    for i, j in zip(pub1, pub2):
        if not substr_match(i, j):
            return False
    return True

def compare_publisher(amazon, marc):
    if 'publisher' in amazon and 'publishers' in marc:
        amazon_pub = amazon['publisher']
        norm_amazon = normalize(amazon_pub)
        for marc_pub in marc['publishers']:
            norm_marc = normalize(marc_pub)
            if norm_amazon == norm_marc:
                return ('publisher', 'match', 100)
            elif substr_match(norm_amazon, norm_marc):
                return ('publisher', 'occur within the other', 100)
            elif substr_match(norm_amazon.replace(' ', ''), norm_marc.replace(' ', '')):
                return ('publisher', 'occur within the other', 100)
            elif short_part_publisher_match(norm_amazon, norm_marc):
                return ('publisher', 'match', 100)
        return ('publisher', 'mismatch', -25)

    if 'publisher' not in amazon or 'publishers' not in marc:
        return ('publisher', 'either missing', 0)

def level2_merge(amazon, marc):
#    print 'MARC', marc
    score = []
    score.append(compare_date(amazon, marc))
    score.append(compare_isbn10(amazon, marc))
    score.append(compare_title(amazon, marc))
    page_score = compare_number_of_pages(amazon, marc)
    if page_score:
        score.append(page_score)

    score.append(compare_publisher(amazon, marc))
    score.append(compare_authors(amazon, marc))

    return score

def full_title(edition):
    title = edition['title']
    if 'subtitle' in edition:
        title += ' ' + edition['subtitle']
    return title

def test_full_title():
    assert full_title({ 'title': "Hamlet"}) == "Hamlet"
    edition = {
        'title': 'Flatland',
        'subtitle': 'A Romance of Many Dimensions',
    }
    assert full_title(edition) == "Flatland A Romance of Many Dimensions"

def test_merge_titles():
    marc = {
        'title_with_subtitles': 'Spytime : the undoing of James Jesus Angleton : a novel',
        'title': 'Spytime',
        'full_title': 'Spytime : the undoing of James Jesus Angleton : a novel',
    }
    amazon = {
        'subtitle': 'The Undoing oF James Jesus Angleton',
        'title': 'Spytime',
    }

    amazon = build_titles(unicode(full_title(amazon)))
    marc = build_titles(marc['title_with_subtitles'])
    assert amazon['short_title'] == marc['short_title']
    assert compare_title(amazon, marc) == ('full-title', 'containted within other title', 350)

def test_merge_titles2():
    amazon = {'title': u'Sea Birds Britain Ireland'}
    marc = {
        'title_with_subtitles': u'seabirds of Britain and Ireland',
        'title': u'seabirds of Britain and Ireland', 
        'full_title': u'The seabirds of Britain and Ireland',
    }
    amazon = build_titles(unicode(full_title(amazon)))
    marc = build_titles(marc['title_with_subtitles'])
    assert compare_title(amazon, marc) == ('full-title', 'exact match', 600)

def attempt_merge(amazon, marc, threshold):
    l1 = level1_merge(amazon, marc)
    total = sum(i[2] for i in l1)
    if total >= threshold:
        return True
    l2 = level2_merge(amazon, marc)
    total = sum(i[2] for i in l2)
    return total >= threshold

def test_merge():
    amazon = {'publisher': u'Collins', 'isbn_10': ['0002167360'], 'number_of_pages': 120, 'short_title': u'souvenirs', 'normalized_title': u'souvenirs', 'full_title': u'Souvenirs', 'titles': [u'Souvenirs', u'souvenirs'], 'publish_date': u'1975', 'authors': [(u'David Hamilton', u'Photographer')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002167360'], 'short_title': u'souvenirs', 'normalized_title': u'souvenirs', 'full_title': u'Souvenirs', 'titles': [u'Souvenirs', u'souvenirs'], 'publish_date': '1978', 'authors': [{'birth_date': u'1933', 'db_name': u'Hamilton, David 1933-', 'entity_type': 'person', 'name': u'Hamilton, David', 'personal_name': u'Hamilton, David'}], 'source_record_loc': 'marc_records_scriblio_net/part11.dat:155728070:617', 'number_of_pages': 120}

    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_merge2():
    amazon = {'publisher': u'Collins', 'isbn_10': ['0002167530'], 'number_of_pages': 287, 'short_title': u'sea birds britain ireland', 'normalized_title': u'sea birds britain ireland', 'full_title': u'Sea Birds Britain Ireland', 'titles': [u'Sea Birds Britain Ireland', u'sea birds britain ireland'], 'publish_date': u'1975', 'authors': [(u'Stanley Cramp', u'Author')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002167530'], 'short_title': u'seabirds of britain and i', 'normalized_title': u'seabirds of britain and ireland', 'full_title': u'seabirds of Britain and Ireland', 'titles': [u'seabirds of Britain and Ireland', u'seabirds of britain and ireland'], 'publish_date': '1974', 'authors': [{'db_name': u'Cramp, Stanley.', 'entity_type': 'person', 'name': u'Cramp, Stanley.', 'personal_name': u'Cramp, Stanley.'}], 'source_record_loc': 'marc_records_scriblio_net/part08.dat:61449973:855'}

    threshold = 735
    #assert attempt_merge(amazon, marc, threshold)

def test_merge3():
    amazon = {'publisher': u'Intl Specialized Book Service Inc', 'isbn_10': ['0002169770'], 'number_of_pages': 207, 'short_title': u'women of the north', 'normalized_title': u'women of the north', 'full_title': u'Women of the North', 'titles': [u'Women of the North', u'women of the north'], 'publish_date': u'1985', 'authors': [(u'Jane Wordsworth', u'Author')]}
    marc = {'publisher': [u'Collins', u'Exclusive distributor ISBS'], 'isbn_10': [u'0002169770'], 'short_title': u'women of the north', 'normalized_title': u'women of the north', 'full_title': u'Women of the North', 'titles': [u'Women of the North', u'women of the north'], 'publish_date': '1981', 'number_of_pages': 207, 'authors': [{'db_name': u'Wordsworth, Jane.', 'entity_type': 'person', 'name': u'Wordsworth, Jane.', 'personal_name': u'Wordsworth, Jane.'}], 'source_record_loc': 'marc_records_scriblio_net/part17.dat:110989084:798'}

    threshold = 735
#    assert attempt_merge(amazon, marc, threshold)

def test_merge4():
    amazon = {'publisher': u'HarperCollins Publishers Ltd', 'isbn_10': ['0002173433'], 'number_of_pages': 128, 'short_title': u'd day to victory', 'normalized_title': u'd day to victory', 'full_title': u'D-Day to Victory', 'titles': [u'D-Day to Victory', u'd day to victory'], 'publish_date': u'1984', 'authors': [(u'Wynfod Vaughan-Thomas', u'Editor, Introduction')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002173433'], 'short_title': u'great front pages  d day ', 'normalized_title': u'great front pages  d day to victory 1944 1945', 'full_title': u'Great front pages : D-Day to victory 1944-1945', 'titles': [u'Great front pages : D-Day to victory 1944-1945', u'great front pages  dday to victory 1944 1945'], 'publish_date': '1984', 'number_of_pages': 128, 'by_statement': 'introduced by Wynford Vaughan-Thomas.', 'source_record_loc': 'marc_records_scriblio_net/part17.dat:102360356:983'}

    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_merge5():
    amazon = {'publisher': u'HarperCollins Publishers (Australia) Pty Ltd', 'isbn_10': ['0002174049'], 'number_of_pages': 120, 'short_title': u'netherlandish and german ', 'normalized_title': u'netherlandish and german paintings national gallery schools of painting', 'full_title': u'Netherlandish and German Paintings (National Gallery Schools of Painting)', 'titles': [u'Netherlandish and German Paintings (National Gallery Schools of Painting)', u'netherlandish and german paintings national gallery schools of painting', u'Netherlandish and German Paintings', u'netherlandish and german paintings'], 'publish_date': u'1985', 'authors': [(u'Alistair Smith', u'Author')]}
    marc = {'publisher': [u'National Gallery in association with W. Collins'], 'isbn_10': [u'0002174049'], 'short_title': u'early netherlandish and g', 'normalized_title': u'early netherlandish and german paintings', 'full_title': u'Early Netherlandish and German paintings', 'titles': [u'Early Netherlandish and German paintings', u'early netherlandish and german paintings'], 'publish_date': '1985', 'authors': [{'db_name': u'National Gallery (Great Britain)', 'name': u'National Gallery (Great Britain)', 'entity_type': 'org'}], 'number_of_pages': 116, 'by_statement': 'Alistair Smith.', 'source_record_loc': 'marc_records_scriblio_net/part17.dat:170029527:1210'}
    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_compare_authors():
    amazon = {'authors': [(u'Alistair Smith', u'Author')]}
    marc = {'authors': [{'db_name': u'National Gallery (Great Britain)', 'name': u'National Gallery (Great Britain)', 'entity_type': 'org'}], 'by_statement': 'Alistair Smith.'}
    assert compare_authors(amazon, marc) == ('main', 'exact match', 125)

def test_merge6():
    amazon = {'publisher': u'Fount', 'isbn_10': ['0002176157'], 'number_of_pages': 224, 'short_title': u'basil hume', 'normalized_title': u'basil hume', 'full_title': u'Basil Hume', 'titles': [u'Basil Hume', u'basil hume'], 'publish_date': u'1986', 'authors': [(u'Tony Castle', u'Editor')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002176157'], 'short_title': u'basil hume  a portrait', 'normalized_title': u'basil hume  a portrait', 'full_title': u'Basil Hume : a portrait', 'titles': [u'Basil Hume : a portrait', u'basil hume  a portrait'], 'number_of_pages': 158, 'publish_date': '1986', 'by_statement': 'edited by Tony Castle.', 'source_record_loc': 'marc_records_scriblio_net/part19.dat:39883132:951'}
    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_merge7():
    amazon = {'publisher': u'HarperCollins Publishers Ltd', 'isbn_10': ['0002176319'], 'number_of_pages': 256, 'short_title': u'pucklers progress', 'normalized_title': u'pucklers progress', 'full_title': u"Puckler's Progress", 'titles': [u"Puckler's Progress", u'pucklers progress'], 'publish_date': u'1987', 'authors': [(u'Flora Brennan', u'Editor')]}
    marc = {'publisher': [u'Collins'], 'isbn_10': [u'0002176319'], 'short_title': u'pucklers progress  the ad', 'normalized_title': u'pucklers progress  the adventures of prince puckler muskau in england wales and ireland as told in letters to his former wife 1826 9', 'full_title': u"Puckler's progress : the adventures of Prince Pu\u0308ckler-Muskau in England, Wales, and Ireland as told in letters to his former wife, 1826-9", 'titles': [u"Puckler's progress : the adventures of Prince Pu\u0308ckler-Muskau in England, Wales, and Ireland as told in letters to his former wife, 1826-9", u'pucklers progress  the adventures of prince puckler muskau in england wales and ireland as told in letters to his former wife 1826 9'], 'publish_date': '1987', 'authors': [{'name': u'Pu\u0308ckler-Muskau, Hermann Furst von', 'title': u'Furst von', 'death_date': u'1871.', 'db_name': u'Pu\u0308ckler-Muskau, Hermann Furst von 1785-1871.', 'birth_date': u'1785', 'personal_name': u'Pu\u0308ckler-Muskau, Hermann', 'entity_type': 'person'}], 'number_of_pages': 254, 'by_statement': 'translated by Flora Brennan.', 'source_record_loc': 'marc_records_scriblio_net/part19.dat:148554594:1050'}
    threshold = 735
    assert attempt_merge(amazon, marc, threshold)

def test_compare_publisher():
    amazon = { 'publisher': 'foo' }
    amazon2 = { 'publisher': 'bar' }
    marc = { 'publishers': ['foo'] }
    marc2 = { 'publishers': ['foo', 'bar'] }
    assert compare_publisher({}, {}) == ('publisher', 'either missing', 0)
    assert compare_publisher(amazon, {}) == ('publisher', 'either missing', 0)
    assert compare_publisher({}, marc) == ('publisher', 'either missing', 0)
    assert compare_publisher(amazon, marc) == ('publisher', 'match', 100)
    assert compare_publisher(amazon2, marc) == ('publisher', 'mismatch', -25)
    assert compare_publisher(amazon2, marc2) == ('publisher', 'match', 100)

########NEW FILE########
__FILENAME__ = bot
#!/usr/bin/python

import sys
import web
import sys, codecs
from catalog.utils.query import query_iter, set_staging, withKey

from catalog.merge.merge_marc import *
from catalog.utils.query import get_mc, withKey
import catalog.merge.amazon as merge_amazon
import catalog.merge.merge_marc as merge_marc
from catalog.merge.merge_bot.merge import amazon_and_marc, get_record
from pprint import pformat

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
#set_staging(True)

urls = (
    '/', 'index'
)
app = web.application(urls, globals())

threshold = 875

#def text_box(k, input): return k

def list_to_html(l):
    def blue(s):
        return ' <span style="color:blue; font-weight:bold">%s</span> ' % s
    return blue('[') + blue('|').join(l) + blue(']')

def as_html(x):
    return list_to_html(x) if isinstance(x, list) else x
 
class index:        
    def head(self, title):
        style = '''
body { font-family: Arial,Helvectica,Sans-serif }
th { text-align: left; }
td { background: #eee; }
'''
        return '<html>\n<head>\n<title>' + title + '</title>\n' + \
            '<style>' + style + '</style>\n</head>\n<body>'

    def tail(self):
        return '</body>\n</html>\n'

    def text_box(self, k):
        if self.input[k]:
            v = web.htmlquote(self.input[k])
            return '<input type="text" name="%s" value="%s">' % (k, v)
        else:
            return '<input type="text" name="%s">' % k

    def form(self):
        return '<form>' \
            + 'ISBN: ' + self.text_box('isbn') \
            + ' or OCLC: ' + self.text_box('oclc') \
            + ' <input type="submit" value="search">' \
            + '</form>'

    def field_table(self, input, rec_amazon, rec_marc):
        yield '<table>'
        yield '''<tr>
<th>Field</th>
<th>match</th>
<th>score</th>
<th>Amazon</th>
<th>MARC</th>
</tr>'''
        total = 0
        for field, match, score in input:
            yield '<tr>'
            yield '<td>%s</td>' % field
            yield '<td>%s</td>' % web.htmlquote(match)
            yield '<td>%s</td>' % score
            yield '<td>%s</td>' % as_html(rec_amazon.get(field, None))
#            if field == 'number_of_pages':
#                yield '<td>%s</td>' % (web.htmlquote(rec_marc['pagination']) if 'pagination' in rec_marc else '<i>pagination missing</i>')
            if field == 'authors':
                authors = rec_marc.get(field, [])
                yield '<td>%s</td>' % list_to_html(web.htmlquote(a['name']) for a in authors)
            else:
                yield '<td>%s</td>' % as_html(rec_marc.get(field, None))
            yield '</tr>'
            total += score
        yield '</table>'
        yield 'threshold %d, total: %d, ' % (threshold, total)
        yield (('match' if total >= threshold else 'no match') + '<br>')

    def marc_compare(self, editions):
        key1 = editions[0]['key']
        mc1 = get_mc(key1)
        rec1 = get_record(key1, mc1)
        key2 = editions[1]['key']
        mc2 = get_mc(key2)
        rec2 = get_record(key2, mc2)

        yield '<h2>Level 1</h2>'
        l1 = merge_marc.level1_merge(rec1, rec2)
        for i in self.field_table(l1, rec1, rec2):
            yield i

        yield '<h2>Level 2</h2>'
        l2 = merge_marc.level2_merge(rec1, rec2)
        for i in self.field_table(l2, rec1, rec2):
            yield i

    def amazon_compare(self, editions):
        key1 = editions[0]['key']
        key2 = editions[1]['key']
        try:
            (rec_amazon, rec_marc) = amazon_and_marc(key1, key2)
        except AssertionError:
            yield 'must be one amazon and one marc edition'
            return
        yield '<h2>Level 1</h2>'
        l1 = merge_amazon.level1_merge(rec_amazon, rec_marc)
        for i in self.field_table(l1, rec_amazon, rec_marc):
            yield i

        yield '<h2>Level 2</h2>'
        l2 = merge_amazon.level2_merge(rec_amazon, rec_marc)
        for i in self.field_table(l2, rec_amazon, rec_marc):
            yield i

    def search(self, editions):
        yield str(len(editions)) + ' editions found<p>'
        yield '<table>'
        yield '<tr><th>Key</th><th>OCLC</th><th>ISBN</th><th>title</th><th>subtitle</th></tr>'
        for e in editions:
            url = 'http://openlibrary.org' + e['key']
            title = web.htmlquote(e['title']) if e['title'] else 'no title'
            yield '<tr><td><a href="%s">%s</a></td>' % (url, e['key'])
            yield '<td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>' % (e['oclc_numbers'], e['isbn_10'], title, (web.htmlquote(e['subtitle']) if e.get('subtitle', None) else '<i>no subtitle</i>'))
        yield '</table><p>'

        if len(editions) == 2:
            yield '2 editions found, lets compare them<br>'
            for i in self.marc_compare(editions):
                yield i

    def isbn_search(self, v):
        q = {'type': '/type/edition', 'isbn_10': v, 'title': None, 'subtitle': None}
        editions = []
        for e in query_iter(q):
            e['isbn_10'] = v
            editions.append(e)
        yield 'searching for ISBN ' + web.htmlquote(v) + ': '
        for i in self.search(editions):
            yield i

    def oclc_search(self, v):
        q = {'type': '/type/edition', 'oclc_numbers': v, 'title': None, 'subtitle': None, 'isbn_10': None}
        editions = []
        print q
        for e in query_iter(q):
            e['oclc_numbers'] = v
            editions.append(e)
        yield 'searching for OCLC ' + web.htmlquote(v) + ': '
        for i in self.search(editions):
            yield i

    def title_search(self, v):
        q = {'type': '/type/edition', 'isbn_10': None, 'title': v}
        editions = []
        for e in query_iter(q):
            e['title'] = v
            editions.append(e)
        yield 'searcing for title "' + web.htmlquote(v) + '": '
        for i in self.search(editions):
            yield i

    def GET(self):
        #self.input = web.input(ol=None, isbn=None, title=None)
        self.input = web.input(isbn=None, oclc=None)
        ret = self.head('Merge debug')
#        ret += web.htmlquote(`dict(self.input)`)
        for i in self.form():
            ret += i
        if self.input.isbn:
            isbn = self.input.isbn
            for i in self.isbn_search(isbn):
                ret += i
        elif self.input.oclc:
            oclc = self.input.oclc
            for i in self.oclc_search(oclc):
                ret += i
#        elif self.input.title:
#            title = self.input.title
#            for i in self.title_search(title):
#                ret += i
        ret += '</body>\n</html>\n'
        return ret

if __name__ == "__main__":
    app.run()


########NEW FILE########
__FILENAME__ = merge
#!/usr/bin/python2.5 

import catalog.merge.merge_marc as marc
import catalog.merge.amazon as amazon
from catalog.utils.query import get_mc, withKey

from catalog.get_ia import get_from_archive
import catalog.marc.fast_parse as fast_parse

def try_amazon(key):
    thing = withKey(key)
    if 'isbn_10' not in thing:
        return None
    if 'authors' in thing:
        authors = []
        for a in thing['authors']:
            author_thing = withKey(a['key'])
            if 'name' in author_thing:
                authors.append(author_thing['name'])
    else:
        authors = []
    return amazon.build_amazon(thing, authors)

def get_record(key, mc):
    data = get_from_archive(mc)
    try:
        rec = fast_parse.read_edition(data)
    except (fast_parse.SoundRecording, IndexError, AssertionError):
        print mc
        print key
        return False
    try:
        return marc.build_marc(rec)
    except TypeError:
        print rec
        raise

def attempt_merge(a, m, threshold, debug = False):
    l1 = amazon.level1_merge(a, m)
    total = sum(i[2] for i in l1)
    if debug:
        print total, l1
    if total >= threshold:
        return True
    l2 = amazon.level2_merge(a, m)
    total = sum(i[2] for i in l2)
    if debug:
        print total, l2
    return total >= threshold

sample_amazon = {'publishers': ['New Riders Press'], 'isbn': ['0321525655'], 'number_of_pages': 240, 'short_title': 'presentation zen simple i', 'normalized_title': 'presentation zen simple ideas on presentation design and delivery voices that matter', 'full_title': 'Presentation Zen Simple Ideas on Presentation Design and Delivery (Voices That Matter)', 'titles': ['Presentation Zen Simple Ideas on Presentation Design and Delivery (Voices That Matter)', 'presentation zen simple ideas on presentation design and delivery voices that matter', 'Presentation Zen Simple Ideas on Presentation Design and Delivery', 'presentation zen simple ideas on presentation design and delivery'], 'publish_date': '2007', 'authors': ['Garr Reynolds']}
sample_marc = {'publishers': [u'New Riders'], 'isbn': ['9780321525659', '0321525655'], 'lccn': ['2008297172'], 'number_of_pages': 229, 'short_title': u'presentation zen simple i', 'normalized_title': u'presentation zen simple ideas on presentation design and delivery', 'full_title': u'Presentation zen simple ideas on presentation design and delivery', 'titles': [u'Presentation zen simple ideas on presentation design and delivery', u'presentation zen simple ideas on presentation design and delivery'], 'publish_date': '2008', 'publish_country': 'cau', 'authors': [{'db_name': u'Reynolds, Garr.', 'name': u'Reynolds, Garr.'}]}

def amazon_and_marc(key1, key2):
    if all(k in ('/b/OL9621221M', '/b/OL20749803M') for k in (key1, key2)):
        return sample_amazon, sample_marc
    mc1 = get_mc(key1)
    mc2 = get_mc(key2)
    if mc1.startswith('amazon:'):
        assert not mc2.startswith('amazon:')
        rec_amazon = try_amazon(key1)
        rec_marc = get_record(key2, mc2)
    else:
        assert mc2.startswith('amazon:')
        rec_amazon = try_amazon(key2)
        rec_marc = get_record(key1, mc1)
    return rec_amazon, rec_marc

def marc_and_marc(key1, key2):
    mc1 = get_mc(key1)
    rec1 = get_record(key1, mc1)
    mc2 = get_mc(key2)
    rec2 = get_record(key2, mc2)
    return rec1, rec2

if __name__ == '__main__':
    key1 = '/b/OL9621221M' # amazon
    key2 = '/b/OL20749803M'
    rec_amazon, rec_marc = amazon_and_marc(key1, key2)
    threshold = 875
    print attempt_merge(rec_amazon, rec_marc, threshold, debug=True)

########NEW FILE########
__FILENAME__ = merge_index
# build a merge database from JSON dump

import re
from normalize import normalize

def short_title(s):
    return normalize(s)[:25]

re_letters = re.compile('[A-Za-z]')

def clean_lccn(lccn):
    return re_letters.sub('', lccn).strip()

re_isbn = re.compile('([-0-9X]{10,})')

def clean_isbn(isbn):
    m = re_isbn.search(isbn)
    if m:
        return m.group(1).replace('-', '')



def add_to_indexes(record):
    if 'title' not in record or record['title'] is None:
        return
    if 'subtitle' in record and record['subtitle'] is not None:
        title = record['title'] + ' ' + record['subtitle']
    else:
        title = record['title']
    title1 = short_title(title)
    yield 'title', title1
    if 'title_prefix' in record and record['title_prefix'] is not None:
        title2 = short_title(record['title_prefix'] + title)
        if title1 != title2:
            yield 'title', title2

    fields = [
        ('lccn', 'lccn', clean_lccn),
        ('oclc_numbers', 'oclc', None),
        ('isbn_10', 'isbn', clean_isbn),
        ('isbn_13', 'isbn', None),
    ]
    for a, b, clean in fields:
        if a not in record:
            continue
        for v in record[a]:
            if not v or b=='isbn' and len(v) < 10:
                continue
            if clean:
                v = clean(v)
                if not v:
                    continue
            yield b, v

########NEW FILE########
__FILENAME__ = merge_marc
import re
from names import match_name
from normalize import normalize

# fields needed for merge process:
# title_prefix, title, subtitle, isbn, publish_country, lccn, publishers, publish_date, number_of_pages, authors

re_year = re.compile('(\d{4})$')
re_amazon_title_paren = re.compile('^(.*) \([^)]+?\)$')

isbn_match = 85

def set_isbn_match(score):
    isbn_match = score

def build_titles(title):
    normalized_title = normalize(title).lower()
    titles = [ title, normalized_title ];
    if title.find(' & ') != -1:
        t = title.replace(" & ", " and ")
        titles.append(t)
        titles.append(normalize(t))
    t2 = []
    for t in titles:
        if t.lower().startswith('the '):
            t2.append(t[4:])
        elif t.lower().startswith('a '):
            t2.append(t[2:])
    titles += t2

    if re_amazon_title_paren.match(title):
        t2 = []
        for t in titles:
            m = re_amazon_title_paren.match(t)
            if m:
                t2.append(m.group(1))
                t2.append(normalize(m.group(1)))
        titles += t2

    return {
        'full_title':       title,
        'normalized_title': normalized_title,
        'titles':           titles,
        'short_title':      normalized_title[:25],
    }

def within(a, b, distance):
    return abs(a-b) <= distance

def compare_country(e1, e2):
    field = 'publish_country'
    if field not in e1 or field not in e2:
        return (field, 'value missing', 0)
    if e1[field] == e2[field]:
        return (field, 'match', 40)
    # West Berlin (wb) == Germany (gw)
    if e1[field] in ('gw ', 'wb ') and e2[field] in ('gw ', 'wb '):
        return (field, 'match', 40)
    return (field, 'mismatch', -205)

def compare_lccn(e1, e2):
    field = 'lccn'
    if field not in e1 or field not in e2:
        return (field, 'value missing', 0)
    if e1[field] == e2[field]:
        return (field, 'match', 200)
    return (field, 'mismatch', -320)

def compare_date(e1, e2):
    if 'publish_date' not in e1 or 'publish_date' not in e2:
        return ('date', 'value missing', 0)
    if e1['publish_date'] == e2['publish_date']:
        return ('date', 'exact match', 200)
    try:
        e1_pub = int(e1['publish_date'])
        e2_pub = int(e2['publish_date'])
        if within(e1_pub, e2_pub, 2):
            return ('date', '+/-2 years', -25)
        else:
            return ('date', 'mismatch', -250)
    except ValueError, TypeError:
        return ('date', 'mismatch', -250)

def compare_isbn10(e1, e2):
    if len(e1['isbn']) == 0 or len(e2['isbn']) == 0:
        return ('ISBN', 'missing', 0)
    for i in e1['isbn']:
        for j in e2['isbn']:
            if i == j:
                return ('ISBN', 'match', isbn_match)

    return ('ISBN', 'mismatch', -225)

# 450 + 200 + 85 + 200

def level1_merge(e1, e2):
    score = []
    if e1['short_title'] == e2['short_title']:
        score.append(('short-title', 'match', 450))
    else:
        score.append(('short-title', 'mismatch', 0))

    score.append(compare_lccn(e1, e2))
    score.append(compare_date(e1, e2))
    score.append(compare_isbn10(e1, e2))
    return score

def compare_author_fields(e1_authors, e2_authors):
    for i in e1_authors:
        for j in e2_authors:
            if normalize(i['db_name']) == normalize(j['db_name']):
                return True
            if normalize(i['name']).strip('.') == normalize(j['name']).strip('.'):
                return True
    return False

def compare_author_keywords(e1_authors, e2_authors):
    max_score = 0
    for i in e1_authors:
        for j in e2_authors:
            percent, ordered = keyword_match(i['name'], j['name'])
            if percent > 0.50:
                score = percent * 80
                if ordered:
                    score += 10
                if score > max_score:
                    max_score = score
    if max_score:
        return ('authors', 'keyword match', max_score)
    else:
        return ('authors', 'mismatch', -200)

def compare_authors(e1, e2):
    if 'authors' in e1 and 'authors' in e2:
        if compare_author_fields(e1['authors'], e2['authors']):
            return ('authors', 'exact match', 125)
    if 'authors' in e1 and 'contribs' in e2 and \
            compare_author_fields(e1['authors'], e2['contribs']):
        return ('authors', 'exact match', 125)
    if 'contribs' in e1 and 'authors' in e2 and \
            compare_author_fields(e1['contribs'], e2['authors']):
        return ('authors', 'exact match', 125)
    if 'authors' in e1 and 'authors' in e2:
        return compare_author_keywords(e1['authors'], e2['authors'])

    if 'authors' not in e1 and 'authors' not in e2:
        if 'contribs' in e1 and 'contribs' in e2 and \
                compare_author_fields(e1['contribs'], e2['contribs']):
            return ('authors', 'exact match', 125)
        return ('authors', 'no authors', 75)
    return ('authors', 'field missing from one record', -25)


def title_replace_amp(amazon):
    return normalize(amazon['full-title'].replace(" & ", " and ")).lower()

def substr_match(a, b):
    return a.find(b) != -1 or b.find(a) != -1

def keyword_match(in1, in2):
    s1, s2 = [i.split() for i in in1, in2]
    s1_set = set(s1)
    s2_set = set(s2)
    match = s1_set & s2_set
    if len(s1) == 0 and len(s2) == 0:
        return 0, True
    ordered = [x for x in s1 if x in match] == [x for x in s2 if x in match]
    return float(len(match)) / max(len(s1), len(s2)), ordered

def compare_title(amazon, marc):
    amazon_title = amazon['normalized_title'].lower()
    marc_title = normalize(marc['full_title']).lower()
    short = False
    if len(amazon_title) < 9 or len(marc_title) < 9:
        short = True

    if not short:
        for a in amazon['titles']:
            for m in marc['titles']:
                if a == m:
                    return ('full-title', 'exact match', 600)

        for a in amazon['titles']:
            for m in marc['titles']:
                if substr_match(a, m):
                    return ('full-title', 'containted within other title', 350)

    max_score = 0
    for a in amazon['titles']:
        for m in marc['titles']:
            percent, ordered = keyword_match(a, m)
            score = percent * 450
            if ordered:
                score += 50
            if score and score > max_score:
                max_score = score
    if max_score:
        return ('full-title', 'keyword match', max_score)
    elif short:
        return ('full-title', 'shorter than 9 characters', 0)
    else:
        return ('full-title', 'mismatch', -600)

def compare_number_of_pages(amazon, marc):
    if 'number_of_pages' not in amazon or 'number_of_pages' not in marc:
        return
    amazon_pages = amazon['number_of_pages']
    marc_pages = marc['number_of_pages']
    if amazon_pages == marc_pages:
        if amazon_pages > 10:
            return ('pagination', 'match exactly and > 10', 100)
        else:
            return ('pagination', 'match exactly and < 10', 50)
    elif within(amazon_pages, marc_pages, 10):
        if amazon_pages > 10 and marc_pages > 10:
            return ('pagination', 'match within 10 and both are > 10', 50)
        else:
            return ('pagination', 'match within 10 and either are < 10', 20)
    else:
        return ('pagination', 'non-match (by more than 10)', -225)

def short_part_publisher_match(p1, p2):
    pub1 = p1.split()
    pub2 = p2.split()
    if len(pub1) == 1 or len(pub2) == 1:
        return False
    for i, j in zip(pub1, pub2):
        if not substr_match(i, j):
            return False
    return True

def compare_publisher(e1, e2):
    if 'publishers' in e1 and 'publishers' in e2:
        for e1_pub in e1['publishers']:
            e1_norm = normalize(e1_pub)
            for e2_pub in e2['publishers']:
                e2_norm = normalize(e2_pub)
                if e1_norm == e2_norm:
                    return ('publisher', 'match', 100)
                elif substr_match(e1_norm, e2_norm):
                    return ('publisher', 'occur within the other', 100)
                elif substr_match(e1_norm.replace(' ', ''), e2_norm.replace(' ', '')):
                    return ('publisher', 'occur within the other', 100)
                elif short_part_publisher_match(e1_norm, e2_norm):
                    return ('publisher', 'match', 100)
        return ('publisher', 'mismatch', -25)

    if 'publishers' not in e1 or 'publishers' not in e2:
        return ('publisher', 'either missing', 0)

def level2_merge(e1, e2):
    score = []
    score.append(compare_date(e1, e2))
    score.append(compare_country(e1, e2))
    score.append(compare_isbn10(e1, e2))
    score.append(compare_title(e1, e2))
    score.append(compare_lccn(e1, e2))
    page_score = compare_number_of_pages(e1, e2)
    if page_score:
        score.append(page_score)

    score.append(compare_publisher(e1, e2))
    score.append(compare_authors(e1, e2))

    return score

def build_marc(edition):
    marc = build_titles(edition['full_title'])
    marc['isbn'] = []
    for f in 'isbn', 'isbn_10', 'isbn_13':
        marc['isbn'].extend(edition.get(f, []))
    if 'publish_country' in edition \
            and edition['publish_country'] not in ('   ', '|||'):
        marc['publish_country'] = edition['publish_country']
    for f in 'lccn', 'publishers', 'publish_date', 'number_of_pages', 'authors', 'contribs':
        if f in edition:
            marc[f] = edition[f]
    return marc

def attempt_merge(e1, e2, threshold, debug = False):
    l1 = level1_merge(e1, e2)
    total = sum(i[2] for i in l1)
    if debug:
        print total, l1
    if total >= threshold:
        return True
    l2 = level2_merge(e1, e2)
    total = sum(i[2] for i in l2)
    if debug:
        print total, l2
    return total >= threshold

def test_merge():
    bpl = {'authors': [{'birth_date': u'1897',
                      'db_name': u'Green, Constance McLaughlin 1897-',
                      'entity_type': 'person',
                      'name': u'Green, Constance McLaughlin',
                      'personal_name': u'Green, Constance McLaughlin'}],
         'full_title': u'Eli Whitney and the birth of American technology',
         'isbn': [u'188674632X'],
         'normalized_title': u'eli whitney and the birth of american technology',
         'number_of_pages': 215,
         'publish_date': '1956',
         'publishers': [u'HarperCollins', u'[distributed by Talman Pub.]'],
         'short_title': u'eli whitney and the birth',
         'source_record_loc': 'bpl101.mrc:0:1226',
         'titles': [u'Eli Whitney and the birth of American technology',
                    u'eli whitney and the birth of american technology']}
    lc = {'authors': [{'birth_date': u'1897',
                     'db_name': u'Green, Constance McLaughlin 1897-',
                     'entity_type': 'person',
                     'name': u'Green, Constance McLaughlin',
                     'personal_name': u'Green, Constance McLaughlin'}],
        'full_title': u'Eli Whitney and the birth of American technology.',
        'isbn': [],
        'normalized_title': u'eli whitney and the birth of american technology',
        'number_of_pages': 215,
        'publish_date': '1956',
        'publishers': ['Little, Brown'],
        'short_title': u'eli whitney and the birth',
        'source_record_loc': 'marc_records_scriblio_net/part04.dat:119539872:591',
        'titles': [u'Eli Whitney and the birth of American technology.',
                   u'eli whitney and the birth of american technology']}

    assert compare_authors(bpl, lc) == ('authors', 'exact match', 125)
    threshold = 735
    assert attempt_merge(bpl, lc, threshold)

def test_author_contrib():
    rec1 = {'authors': [{'db_name': u'Bruner, Jerome S.', 'name': u'Bruner, Jerome S.'}],
    'full_title': u'Contemporary approaches to cognition a symposium held at the University of Colorado.',
    'number_of_pages': 210,
    'publish_country': 'xxu',
    'publish_date': '1957',
    'publishers': [u'Harvard U.P']}

    rec2 = {'authors': [{'db_name': u'University of Colorado (Boulder campus). Dept. of Psychology.',
                'name': u'University of Colorado (Boulder campus). Dept. of Psychology.'}],
    'contribs': [{'db_name': u'Bruner, Jerome S.', 'name': u'Bruner, Jerome S.'}],
    'full_title': u'Contemporary approaches to cognition a symposium held at the University of Colorado',
    'lccn': ['57012963'],
    'number_of_pages': 210,
    'publish_country': 'mau',
    'publish_date': '1957',
    'publishers': [u'Harvard University Press']}

    e1 = build_marc(rec1)
    e2 = build_marc(rec2)

    assert compare_authors(e1, e2) == ('authors', 'exact match', 125)
    threshold = 875
    assert attempt_merge(e1, e2, threshold)

test_author_contrib()

########NEW FILE########
__FILENAME__ = names
import re
from normalize import normalize

re_split_parts = re.compile('(.*?[. ]+)')
re_marc_name = re.compile('^(.*), (.*)$')
re_amazon_space_name = re.compile('^(.+?[^ ]) +([A-Z][a-z]?)$')

verbose = False

titles = frozenset([normalize(x) for x in 'Mrs', 'Sir', 'pseud', 'Lady', 'Baron', 'lawyer', 'Lord', 'actress', 'Dame', 'Mr', 'Viscount', 'professeur', 'Graf', 'Dr', 'Countess', 'Ministerialrat', 'Oberamtsrat', 'Rechtsanwalt'])

# marquis de

def flip_name(name):
    m = re_marc_name.match(name)
    if not m:
        return None
    return m.group(2) + ' ' + m.group(1)

def match_seq(parts1, parts2):
    if len(parts1) == len(parts2):
        return False
    if len(parts1) > len(parts2):
        longer, shorter = parts1, parts2
    else:
        longer, shorter = parts2, parts1

    i = 0
    for j in shorter:
        while not compare_part(j, longer[i]):
            i+=1
            if i >= len(longer):
                return False
    return True

def compare_part(p1, p2):
    p1 = normalize(p1)
    p2 = normalize(p2)
    return p1.startswith(p2) or p2.startswith(p1)

def compare_parts(parts1, parts2):
    if len(parts1) != len(parts2):
        return False
    for i, j in zip(parts1, parts2):
        if not compare_part(i, j):
            return False
    return True

def split_parts(s):
    parts = []
    m = re_split_parts.match(s)
    if not m:
        return [s.strip()]
    while m:
        end = m.end()
        parts.append(m.group(1).strip())
        m = re_split_parts.match(s, end)
    if end != len(s):
        parts.append(s[end:].strip())
    return parts

def amazon_title(amazon_first_parts, marc_first_parts):
    if normalize(amazon_first_parts[0]) not in titles:
        return False
    if compare_parts(marc_first_parts, amazon_first_parts[1:]):
        if verbose:
            print "match with Amazon title"
        return True
    if match_seq(marc_first_parts, amazon_first_parts[1:]):
        if verbose:
            print "partial match, with Amazon title"
        return True
    return False

def marc_title(amazon_first_parts, marc_first_parts):
#            print 'title found: ', marc_first_parts[-1]
    if normalize(marc_first_parts[-1]) not in titles:
        return False
    if compare_parts(marc_first_parts[:-1], amazon_first_parts):
        if verbose:
            print "match with MARC end title"
        return True
    if normalize(amazon_first_parts[0]) in titles:
        if compare_parts(marc_first_parts[:-1], amazon_first_parts[1:]):
            if verbose:
                print "match, both with titles"
            return True
        if match_seq(marc_first_parts[:-1], amazon_first_parts[1:]):
            if verbose:
                print "partial match, both with titles"
            return True
    if match_seq(marc_first_parts[:-1], amazon_first_parts):
        if verbose:
            print "partial match with MARC end title"
        return True
    if match_seq(marc_first_parts, amazon_first_parts):
        if verbose:
            print "partial match with MARC end title"
    return False

# use for person, org and event because the LC data says "Berkovitch, Israel." is an org

def remove_trailing_dot(s):
    s = s.strip()
    if len(s) < 3 or not s.endswith('.') or s[-3] == ' ' or s[-3] == '.':
        return s
    return s[:-1]

def flip_marc_name(marc):
    m = re_marc_name.match(marc)
    if not m:
        return remove_trailing_dot(marc)
    first_parts = split_parts(m.group(2))
    if normalize(first_parts[-1]) not in titles: 
        # example: Eccles, David Eccles Viscount
        return remove_trailing_dot(m.group(2)) + ' ' + m.group(1)
    if len(first_parts) > 2 and normalize(first_parts[-2]) == normalize(m.group(1)):
        return u' '.join(first_parts[0:-1])
    return u' '.join(first_parts[:-1] + [m.group(1)])

def match_marc_name(marc1, marc2, last_name_only_ok):
    m1_normalized = normalize(marc1)
    m2_normalized = normalize(marc2)
    if m1_normalized == m2_normalized:
        return True
    m1 = re_marc_name.match(marc1)
    m2 = re_marc_name.match(marc2)
    if not m1:
        if m2 and marc1_normalized == normalize(m2.group(1)):
            return last_name_only_ok
        else:
            return False
    if not m2:
        if marc2_normalized == normalize(m1.group(1)):
            return last_name_only_ok
        else:
            return False
    if marc1_normalized == normalize(m2.group(2) + ' ' + m2.group(1)) or marc2_normalized == normalize(m1.group(2) + ' ' + m1.group(1)):
        return True
    if not (m1.group(1).endswith(' ' + m2.group(1)) or m1.endswith('.' + m2.group(1)) or \
            m2.group(1).endswith(' ' + m1.group(1)) or m2.endswith('.' + m1.group(1))):
        return False # Last name mismatch
    marc1_first_parts = split_parts(m1.group(2))
    marc2_first_parts = split_parts(m2.group(2))
    if compare_parts(marc1_first_parts, marc2_first_parts):
        return True
    if match_seq(marc1_first_parts, marc2_first_parts):
        return True
    if marc_title(marc1_first_parts, marc2_first_parts):
        return True
    if marc_title(marc2_first_parts, marc1_first_parts):
        return True
    if amazon_title(marc1_first_parts, marc2_first_parts):
        return True
    if amazon_title(marc2_first_parts, marc1_first_parts):
        return True
    return False

# try different combinations looking for a match
def match_name2(name1, name2):
    if name1 == name2:
        return True
    n1_normalized = normalize(name1)
    n2_normalized = normalize(name2)
    if n1_normalized == n2_normalized:
        return True
    n1_parts = split_parts(name1)
    n2_parts = split_parts(name2)
    if compare_parts(n1_parts, n2_parts):
        return True
    if match_seq(n1_parts, n2_parts):
        return True
    if marc_title(n1_parts, n2_parts):
        return True
    if marc_title(n2_parts, n1_parts):
        return True
    if amazon_title(n1_parts, n2_parts):
        return True
    if amazon_title(n2_parts, n1_parts):
        return True
    return False

def match_surname(surname, name):
    if name.endswith(' ' + surname) or name.endswith('.' + surname):
        return True
    surname = surname.replace(' ', '')
    if name.endswith(' ' + surname) or name.endswith('.' + surname):
        return True
    return False

def amazon_spaced_name(amazon, marc):
    len_amazon = len(amazon)
    if len_amazon != 30 and len_amazon != 31:
        return False
    m = re_amazon_space_name.search(amazon)
    if not m:
        return False
    amazon_surname = m.group(1)
    if normalize(amazon_surname) == normalize(marc):
        return True
    amazon_initals = m.group(2)
    m = re_marc_name.match(marc)
    if not m:
        return False
    marc_surname = m.group(1)
    if normalize(amazon_surname) != normalize(marc_surname):
        return False
    marc_first_parts = split_parts(m.group(2))
    amazon_first_parts = [x for x in amazon_initals]
    if compare_parts(marc_first_parts, amazon_first_parts):
        return True
    if match_seq(amazon_first_parts, marc_first_parts):
        return True
    return False

def match_name(amazon, marc, last_name_only_ok=True):
    if amazon_spaced_name(amazon, marc):
        return True
    amazon_normalized = normalize(amazon)
    amazon_normalized_no_space = normalize(amazon).replace(' ', '')
    marc_normalized = normalize(marc)
    # catches events and organizations
    if amazon_normalized == marc_normalized:
        if verbose:
            print 'normalized names match'
        return True
    if amazon_normalized_no_space == marc_normalized.replace(' ', ''):
        if verbose:
            print 'normalized, spaces removed, names match'
        return True
    # split MARC name
    m = re_marc_name.match(marc)
    if not m:
        return False
    surname = m.group(1)
    surname_no_space = surname.replace(' ', '')
    if amazon_normalized == normalize(surname) \
            or amazon_normalized_no_space == normalize(surname_no_space):
        if verbose:
            print 'Amazon only has a last name, it matches MARC'
        return last_name_only_ok
    if amazon_normalized == normalize(m.group(2) + ' ' + surname):
        if verbose:
            print 'match'
        return True
    if amazon_normalized_no_space == normalize(m.group(2) + surname).replace(' ', ''):
        if verbose:
            print 'match when spaces removed'
        return True
    if not match_surname(surname, amazon):
        if verbose:
            print 'Last name mismatch'
        return False
    marc_first_parts = split_parts(m.group(2))
    amazon_first_parts = split_parts(amazon[0:-(len(m.group(1))+1)])
    if compare_parts(marc_first_parts, amazon_first_parts):
        if verbose:
            print "match"
        return True
    if marc_title(amazon_first_parts, marc_first_parts):
        return True
    if amazon_title(amazon_first_parts, marc_first_parts):
        return True
    if match_seq(amazon_first_parts, marc_first_parts):
        if verbose:
            print "partial match"
        return True
    if verbose:
        print "no match"
    return False

def match_not_just_surname(amazon, marc):
    return match_name(amazon, marc, last_name_only_ok=False)

########NEW FILE########
__FILENAME__ = name_tests
#!/usr/bin/python
# coding: utf-8

# run using py.tests:
# py.test name_tests.py

import names

samples = [
    ("John Smith", "Smith, John"),
    ("Diane DiPrima", "Di Prima, Diane."),
    ("Buckley, William F.", "William F. Buckley Jr."),
    ("Duong Thu Huong", u"Dương, Thu Hương."),
#    ("Deanne Spears", "Milan Spears, Deanne."),
#    ("Victor Erofeyev", "Erofeev, V. V."),
#    ("Courcy Catherine De", "De Courcy, Catherine."),
#    ("Andy Mayer", "Mayer, Andrew"),
#    ("Neelam Saran Gour", "Gour, Neelum Saran"),
#    ("Bankim Chandra Chatterjee", "Chatterji, Bankim Chandra "),
#    ("Mrs. Humphrey Ward", "Ward, Humphry Mrs."),
#    ("William F. Buckley Jr.", "Buckley, William F."),
#    ("John of the Cross Saint", "Saint John of the Cross"),
#    ('Louis Philippe', 'Louis Philippe King of the French'),
#    ('Gregory of Tours', 'Gregory Saint, Bishop of Tours'),
#    ('Marjorie Allen', 'Allen, Marjory Gill Allen, Baroness'),
    ('Shewbridge                   Ea', 'Shewbridge, Edythe.'),
    ('Maitland-Jones               Jf', 'Maitland-Jones, J. F.'),
    ('Quine                        Wv', 'Quine, W. V.'),
    ('Auden                        Wh', 'Auden, W. H.'),
    ('Evans                        Cm', 'Evans, Charles M.'),
    ('Buckwalter                   L', 'Buckwalter, Len.'),
    ('Bozic                        Sm', 'Bozic, S. M.'),
    ('Lawrence                     Dh', 'Lawrence, D. H.'),
    ('De Grazia                    T', 'De Grazia'),
    ('Purcell                      R', 'Purcell, Rosamond Wolff.'),
    ('Warring                      Rh', 'Warring, R. H.'),
]

def test_names():
    for amazon, marc in samples:
        yield check, amazon, marc

def test_flip_name():
    assert names.flip_name("Smith, John") == "John Smith"
    assert names.flip_name("John Smith") == None

def test_compare_part():
    assert names.compare_part("J", "John")
    assert names.compare_part("John", "J")
    assert not names.compare_part("John", "Jack")
    assert names.compare_part("j", "John")
    assert not names.compare_part("X", "John")

def test_remove_trailing_dot():
#    print names.remove_trailing_dot("Jr.")
#    assert names.remove_trailing_dot("Jr.") == "Jr."
    assert names.remove_trailing_dot("John Smith.") == "John Smith"

def check(i, j):
    assert names.match_name(i, j)

########NEW FILE########
__FILENAME__ = normalize
import re, unicodedata 

#re_brace = re.compile('{[^{}]+?}')
re_normalize = re.compile('[^[:alpha:] ]', re.I)
re_whitespace = re.compile('[-\s,;.]+')

def normalize(s):
    if isinstance(s, unicode):
        s = unicodedata.normalize('NFC', s.replace(u'\u0142', u'l'))
    s = s.replace(' & ', ' and ')
    # remove {mlrhring} and friends
    # see http://www.loc.gov/marc/mnemonics.html
    # s = re_brace.sub('', s)
    s = re_whitespace.sub(' ', s.lower())
    s = re_normalize.sub('', s.strip())
    return s

#def test_normalize():
#    a = "The La{dotb}t{macr}a{mlrhring}if al-ma{mllhring}{macr}arif of Tha{mllhring} {macr}alibi. The book of curious and entertaining information"
#    b = u"The La\xf2t\xe5a\xaeif al-ma\xb0\xe5arif of Tha\xb0 \xe5alibi. The book of curious and entertaining information"
#    assert normalize(a) == normalize(b)
#
#    a = "Tha{mllhring}{macr}alib{macr}i, {mllhring}Abd al-Malik ibn Mu{dotb}hammad 961 or 2-1037 or 8."
#    b = u"Tha\xb0\xe5alib\xe5i, \xb0Abd al-Malik ibn Mu\xf2hammad 961 or 2-1037 or 8."
#    assert normalize(a) == normalize(b)


########NEW FILE########
__FILENAME__ = parse
import sys
from cStringIO import StringIO
from xml.parsers.expat import error as xml_error
from elementtree import ElementTree
from types import *
from lang import *

def input_items (input):
	def buf2elt (buf):
		buf.seek (0, 0)
		elt = None
		try:
			et = ElementTree.parse (buf)
			elt = et.getroot ()
		except xml_error, e:
			elt = None
			warn ("ignoring XML error: %s" % e)
		buf.close ()
		return elt

	buf = None
	bufpos = None
	for (line, linepos) in lines_positions (input):
		if line.startswith('<?xml '):
			if buf is not None:
				yield (buf2elt (buf), bufpos)
			buf = StringIO ()
			bufpos = None
		else:
			if buf: # this lets us start anywhere and pick up the next record
				if bufpos is None:
					bufpos = linepos
				buf.write (line)
	if buf is not None:
		yield (buf2elt (buf), bufpos)

def setval (x, v, k):
	x[k] = v

def addval (x, v, k, translate=lambda x: x):
	v = translate (v)
	vv = x.get (k)
	if vv:
		vv.append (v)
	else:
		x[k] = [v]

def concval (x, v, k, sep=" "):
	vv = x.get (k)
	if vv:
		x[k] = vv + sep + v
	else:
		x[k] = v

def thingify_with (field):
	return lambda v: { field: v }

element_dispatch = {
	'title': (setval, 'title'),
	'creator': (addval, 'authors', thingify_with ('name')),
	'subject': (addval, 'subject'),
	'description': (concval, 'description', "; "),
	'publisher': (setval, 'publisher'),
	'date': (setval, 'publish_date'),
	# if can be a language_code, enter that and also provide language, else store as language
	'language': (setval, 'language'),
	'sponsor': (setval, 'scan_sponsor'),
	'contributor': (setval, 'scan_contributor'),
	'identifier': (setval, 'oca_identifier')
	}

ignored = {}

def parse_item (r):
	global ignored
	e = {}
	for field in r:
		text = field.text
		if text is None: continue
		tag = field.tag
		action = element_dispatch.get (tag)
		if action:
			f = action[0]
			args = action[1:]
			v = encode_val (text)
			f (e, v, *args)
		else:
			count = ignored.get (tag) or 0
			ignored[tag] = count + 1
	return e

limit = 1000
def test_input (input):
	n = 0
	global ignored
	ignored = {}
	for (r,pos) in input_items (input):
		# if limit and n == limit: break
		if r is None: continue
		o = parse_item (r)
		print o
		n += 1
		if n % 100 == 0:
			warn ("...... read %d records" % n)
	warn ("ignored:")
	for (tag,count) in ignored.iteritems ():
		warn ("\t%d\t%s" % (count, tag))
	warn ("done.  read %d records" % n)

def parser (input):
	for (r,pos) in input_items (input):
		if r is None: continue
		d = parse_item (r)
		d["source_record_pos"] = pos
		yield d

def encode_val (v):
	if isinstance (v, StringType):
		return v
	elif isinstance (v, UnicodeType):
		return v.encode ('utf8')
	else:
		die ("couldn't encode value: %s" % repr (v))

# parse_input (sys.stdin)

########NEW FILE########
__FILENAME__ = olwrite
import web
from infogami.infobase import client
import simplejson
import sys

web.ctx.ip = '127.0.0.1'

class Infogami:
    def __init__(self, host, sitename='openlibrary.org'):
        self.conn = client.connect(type='remote', base_url=host)
        self.sitename = sitename

    def _request(self, path, method, data):
        out = self.conn.request(self.sitename, path, method, data)
        out = simplejson.loads(out)
        if out['status'] == 'fail':
            raise Exception(out['message'])
        return out

    def login(self, username, password):
        return self._request('/account/login', 'POST', dict(username=username, password=password))

    def write(self, query, comment='', machine_comment=None):
        query = simplejson.dumps(query)
        return self._request('/write', 'POST', dict(query=query, comment=comment, machine_comment=machine_comment))

    def new_key(self, type):
        return self._request('/new_key', 'GET', dict(type=type))['result']

def add_to_database(infogami, q, loc):
# sample return
#    {'status': 'ok', 'result': {'updated': [], 'created': ['/b/OL13489313M']}}

    for a in (i for i in q.get('authors', []) if 'key' not in i):
        a['key'] = infogami.new_key('/type/author')

    q['key'] = infogami.new_key('/type/edition')
    ret = infogami.write(q, comment='initial import', machine_comment=loc)
    assert ret['status'] == 'ok'
    keys = [ i for i in ret['result']['created'] if i.startswith('/b/')]
    try:
        assert len(keys) == 1 or keys[0] == q['key']
    except AssertionError:
        print q
        print ret
        print keys
        raise
    return q['key']

########NEW FILE########
__FILENAME__ = onix-import
import web
import infogami.tdb as tdb
from infogami.tdb import NotFound, Things, LazyThing
from items import *
from onix import parser
import sys
import unicodedata
import re
import os
from lang import *
from types import *

source_name = None
source_path = None
edition_prefix = None
author_prefix = None

edition_records = set ([])
item_names = {}
#edition_names = set ([])
#author_names = {}

def setup ():
	def getvar (name, required=True):
		val = os.getenv (name)
		if required and val is None:
			raise Exception ("found no environment variable %s" % name)
		return val
	dbname = getvar ("PHAROS_DBNAME")
	dbuser = getvar ("PHAROS_DBUSER")
	dbpass = getvar ("PHAROS_DBPASS")
	web.config.db_parameters = dict(dbn='postgres', db=dbname, user=dbuser, pw=dbpass)
	web.db._hasPooling = False
	web.config.db_printing = False
	web.load()
	tdb.setup()
	logfile = getvar ("PHAROS_LOGFILE", False)
	if logfile:
		tdb.logger.set_logfile (open (logfile, "a"))
		sys.stderr.write ("logging to %s\n" % logfile)

	global source_name, source_path
	source_dir = getvar ("PHAROS_SOURCE_DIR")
	source_name = sys.argv[1]
	source_path = "%s/%s" % (source_dir, source_name)

	global edition_prefix, author_prefix
	edition_prefix = getvar ("PHAROS_EDITION_PREFIX", False) or ""
	author_prefix = getvar ("PHAROS_AUTHOR_PREFIX", False) or ""

	setup_names ()

def setup_names ():
	global item_names, edition_records, source_name

	warn ("walking the length and breadth of the database ...")
	author_type = Author.type ()
	edition_type = Edition.type ()
	walked = 0
	parent_id = site_object().id
	for r in web.query ("SELECT id,name FROM thing WHERE parent_id = $parent_id", vars=locals()):
		item_names[r.name] = r.id
	
	for r in web.query ("SELECT d1.value FROM datum AS d1, datum AS d2 WHERE d1.version_id=d2.version_id AND d1.key='source_record_lineno' AND d2.key='source_name' AND d2.value=$source_name", { 'source_name': source_name }):
		edition_records.add (int (r.value))

	warn ("noted %d items" % len (item_names))
	if len (edition_records) > 0:
		warn ("already have %d records from this source; they will be ignored" % len (edition_records))

def import_file (input):
	n = 0
	for x in parser (input):
		n += 1
		import_item (x)
		if n % 100 == 0:
			sys.stderr.write ("." * 30 + " read %d records\n" % n)
	sys.stderr.write ("\nread %d records\n" % n)

skipped = 0
imported = 0

def import_author (x):
	name = author_prefix + name_string (x["name"])
	a = None

	global item_names
	aid = item_names.get (name, None)
	if aid:
		a = LazyThing (aid)
		# warn ("---------------------------> already author %s" % name)
	else:
		a = Author (name, d=massage_dict (x))
		a.save ()
		item_names[name] = a.id
		# warn ("AUTHOR %s" % name)
	return a

def import_item (x):
	global skipped, imported

	global edition_records
	lineno = x["source_record_lineno"]
	if lineno in edition_records:
		skipped += 1
		if skipped % 100 == 0:
			warn ("skipped %d" % skipped)
		return

	# import the authors
	authors = map (import_author, x.get ("authors") or [])
	if x.get ("authors"):
		del x["authors"]

	# find a unique name for the edition
	global item_names
	name = None
	for n in edition_name_choices (x):
		nn = edition_prefix + n
		if nn not in item_names:
			name = nn
			break

	if not name:
		raise Exception ("couldn't find a unique name for %s" % x)

	e = Edition (name, d=massage_dict (x))
	global source_name
	e.source_name = source_name
	e.authors = authors
	e.save ()
	item_names[name] = e.id
	edition_records.add (e.source_record_lineno)
	imported += 1
	if imported % 100 == 0:
		warn ("imported %d" % imported)

	# sys.stderr.write ("EDITION %s\n" % name)

ignore_title_words = ['a', 'the']
tsep = '_'

def edition_name_choices (x):
	# use up to 25 chars of title, including last word
	title = name_safe (x['title'])
	title_words = [ w for w in title.split() if w.lower() not in ignore_title_words ]
	if len (title_words) == 0:
		raise Exception ("no usable title chars")
	ttail = title_words.pop (-1)
	tlen = len (ttail)
	name = ""
	nlen = 1 + tlen
	if title_words:
		name = title_words.pop (0)
		nlen = len (name) + 1 + tlen
		while title_words:
			w = title_words.pop (0)
			wlen = len (w)
			if nlen + 1 + wlen < 25:
				name += "_" + w
				nlen += 1 + wlen
	if name:
		name += "_"
	name += ttail
	name = name[0:30]
	yield name

	ed_number = x.get ('edition_number')
	if ed_number:
		name = tsep.join ([name, name_string (ed_number)])
		yield name

	ed_type = x.get ('edition_type')
	if ed_type:
		name = tsep.join ([name, name_string (ed_type)])
		yield name

	ed = x.get ('edition')
	if ed:
		name = tsep.join ([name, name_string (ed)])
		yield name

	format = x.get ('physical_format')
	if format:
		name = tsep.join ([name, name_string (format)])
		yield name

	nlen = len (name)
	n = 0
	while True:
		name = name[:nlen] + tsep + "%d" % n
		yield name
		n += 1

	return

re_name_safe = re.compile (r'[^a-zA-Z0-9]')
def name_safe (s):
	s = asciify (s)
	s = s.replace ("'", "")
	return re.sub (re_name_safe, ' ', s)

def name_string (s):
	s = name_safe (s)
	words = s.split ()
	return '_'.join (words)

def asciify (s):
	return unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore')

def massage_value (v):
	if (isinstance (v, UnicodeType)):
		return v.encode ('utf8')
	elif (isinstance (v, ListType)):
		return map (massage_value, v)
	else:
		return v

def massage_dict (d):
	dd = {}
	for (k, v) in d.iteritems ():
		dd[k] = massage_value (v)
	return dd

if __name__ == "__main__":
	setup()
	sys.stderr.write ("--> setup finished\n")
	import_file (open (source_path, "r"))
	sys.stderr.write ("--> import finished\n")

########NEW FILE########
__FILENAME__ = onix
# wrapper code for easier handling of ONIX files:
#
# OnixHandler -- a sax ContentHandler that produces a stream of ONIX "product" data in xmltramp objects
#
# OnixProduct -- a wrapper for the objects produced by OnixHandler, providing human-friendly field access
# (mostly just providing a dictionary interface where long ("reference") names can be used even when the
# data is encoded with opaque ("short") names.)

from xml.sax.handler import *
from catalog.onix.sax_utils import *
from catalog.onix import xmltramp

repo_path = os.getenv ("PHAROS_REPO")
codelists_path = "%s/%s" % (repo_path, "catalog/onix/ONIX_BookProduct_CodeLists.xsd")
ref_dtd_path = "%s/%s" % (repo_path, "catalog/onix/ONIX_BookProduct_Release2.1_reference.xsd")

# for testing, also set URL_CACHE_DIR; see bottom.

onix_codelists = None
onix_shortnames = None

def init ():
	f = open (codelists_path, "r")
	onix_codelists = parse_codelists (f)
	f.close ()
	f = open (ref_dtd_path, "r")
	onix_shortnames = parse_shortnames (f)
	f.close ()

class OnixProduct:
	# N.B.: this only works when using the "short" names of elements.
	# we should check that the document uses the short DTD, and if not,
	# use the reference names to access field values.

	def __init__ (self, p):
		self.p = p

	@staticmethod
	def reify_child (v):
		if len (v._dir) == 1 and isinstance (v._dir[0], StringTypes):
			return v._dir[0]
		else:
			return OnixProduct (v)

	def __getitem__ (self, n):
		slicing = False
		if isinstance (n, SliceType):
			slicing = True
			reference_name = n.start
		else:
			reference_name = n
		name = OnixProduct.get_shortname (reference_name) # or reference_name.lower ()
		values = self.p[name:]
		if slicing:
			return map (OnixProduct.reify_child, values)
		else:
			if len (values) == 0:
				raise KeyError ("no value for %s (%s)" % (reference_name, name))
			elif len (values) > 1:
				raise Exception ("more than one value for %s (%s)" % (reference_name, name))
			return OnixProduct.reify_child (values[0])

	def get (self, n):
		try:
			return self.__getitem__ (n)
		except KeyError:
			return None

	def getLineNumber (self):
		return self.p.getLineNumber ()

	def __unicode__ (self):
		return self.p.__unicode__ ()

	def __str__ (self):
		return self.__unicode__ ()

	def pi_type_name (code):
		return onix_codelists["List5"][code][0]

	@staticmethod
	def contributor_role (code):
		return onix_codelists["List17"][code][0]

	@staticmethod
	def get_shortname (reference_name):
		try:
			return onix_shortnames[reference_name]
		except KeyError:
			raise Exception ("unknown reference name: %s" % reference_name)

class OnixHandler (ContentHandler):

	def __init__ (self, parser, receiver):
		self.parser = parser
		self.receiver = receiver
		self.subhandler = None
		ContentHandler.__init__ (self)

	def startElementNS (self, name, qname, attrs):
		if self.subhandler:
			self.subhandler.startElementNS (name, qname, attrs)
			self.subdepth += 1
		else:
			(uri, localname) = name
			if localname == "product":
				self.subhandler = xmltramp.Seeder (self.parser)
				self.subhandler.startElementNS (name, qname, attrs)
				self.subdepth = 1

	def endElementNS (self, name, qname):
		if self.subhandler:
			self.subhandler.endElementNS (name, qname)
			self.subdepth -= 1
			if self.subdepth == 0:
				self.receiver (self.subhandler.result)
				self.subhandler = None

	def characters (self, content):
		if self.subhandler:
			self.subhandler.characters (content)

def parse_shortnames (input):
	def schema (name, attrs):
		def element (name, attrs):
			def typespec (name, attrs):
				def attribute (name, attrs):
					if (attrs.getValueByQName ('name') == "shortname"):
						shortname = attrs.getValueByQName ('fixed')
						return CollectorValue (shortname)
					else:
						return CollectorNone ()
				return NodeCollector ({ 'attribute': attribute, collector_any: typespec })
			elt_name = attrs.getValueByQName ('name')
			return NamedCollector (elt_name, { collector_any: typespec })
		return DictCollector ({ 'element': element })
	return collector_parse (input, { 'schema': schema })

def parse_codelists (input):
	def schema (name, attrs):
		def simpleType (name, attrs):
			def restriction (name, attrs):
				def enumeration (name, attrs):
					def annotation (name, attrs):
						def documentation (name, attrs):
							return TextCollector ()
						return ListCollector ({ 'documentation': documentation })
					return NamedCollector (attrs.getValueByQName (u'value'), { 'annotation': annotation })
				return DictCollector ({ 'enumeration': enumeration })
			return NamedCollector (attrs.getValueByQName (u'name'), { 'restriction': restriction })
		return DictCollector ({ 'simpleType': simpleType })
	return collector_parse (input, { 'schema': schema })

init ()

### testing

from xml.sax.saxutils import prepare_input_source

class TestErrorHandler:
	def error (self, exn):
		raise exn
	def fatalError (self, exn):
		raise exn
	def warning (self, exn):
		sys.stderr.write ("warning: %s\n" % exn.getMessage)

def produce_items (input, produce):
	source = prepare_input_source (input)

	parser = xml.sax.make_parser ()
	parser.setFeature (xml.sax.handler.feature_namespaces, 1)
	parser.setContentHandler (OnixHandler (parser, process_item))
	url_cache_dir = os.getenv ("URL_CACHE_DIR")
	if url_cache_dir:
		sys.stderr.write ("using url cache in %s\n" % url_cache_dir)
		parser.setEntityResolver (CachingEntityResolver (parser, url_cache_dir))
	else:
		sys.stderr.write ("no url_cache_dir; XML resources will always be loaded from network\n")
	parser.setErrorHandler (TestErrorHandler ())
	parser.parse (source)

def process_item (i):
	print OnixProduct (i)

if __name__ == "__main__":
	from sys import stdin
	print "Reading ONIX data from standard input ..."
	produce_items (stdin, process_item)

########NEW FILE########
__FILENAME__ = parse
# provides a parser from ONIX files to Open Library items

import re
import sys
import os
from types import *
from lang import *

import xml.sax
from xml.sax.handler import *
from xml.sax.saxutils import prepare_input_source

from thread_utils import AsyncChannel, threaded_generator
from onix import OnixProduct, OnixHandler, onix_codelists

def parser (input):
	# returns a generator that produces dicts representing Open Library items

	def produce_items (produce):
		source = prepare_input_source (input)

		parser = xml.sax.make_parser ()
		parser.setFeature (xml.sax.handler.feature_namespaces, 1)
		parser.setContentHandler (OnixHandler (parser, process_product))
		url_cache_dir = os.getenv ("URL_CACHE_DIR")
		if url_cache_dir:
			sys.stderr.write ("using url cache in %s\n" % url_cache_dir)
			parser.setEntityResolver (CachingEntityResolver (parser, url_cache_dir))
		else:
			sys.stderr.write ("no url_cache_dir; XML resources will always be loaded from network\n")
		parser.setErrorHandler (MyErrorHandler ())
		parser.parse (source)

	return threaded_generator (produce_items, 50)

def process_product (p):
	op = OnixProduct (p)	# the incoming record
	o = {}			# the Open Library item we're producing

	# record id
	o['source_record_lineno'] = p.getLineNumber ()

	# title, subtitle
	tt = [ t for t in op["Title":] if t["TitleType"] == '01' ]
	if len (tt) > 1:
		raise Exception ("more than one distinctive title")
	elif len(tt) == 0:
		raise Exception ("no distinctive title")
	t = tt[0]
	prefix = t.get ("TitlePrefix")
	if prefix:
		prefix = prefix.strip ()
		o['title_prefix_len'] = len (prefix) + 1  # prefix plus space
		o['title'] = prefix + " " + t["TitleWithoutPrefix"].strip ()
	else:
		title = t.get ("TitleText")
		if title:
			o['title'] = title
	subtitle = t.get ("Subtitle")
	if subtitle:
		o['subtitle'] = subtitle

	# id codes (ISBN, etc.)
	for pi in op["ProductIdentifier":]:
		pi_type = pi["ProductIDType"]
		pi_val = pi["IDValue"]
		if pi_type != '01':
			type_name = str (OnixProduct.pi_type_name (pi_type)).replace ("-", "_")
			o[type_name] = pi_val

	# author, contributors
	for c in op["Contributor":]:
		role_codes = c["ContributorRole":]
		role_codes.sort ()
		role_code = role_codes[0]

		name = person_name (c)
		if not name:
			warn ("=====> no name for contributor at line %d" % c.getLineNumber ())
			continue

		if role_code != 'A01':
			role = OnixProduct.contributor_role (role_code)
			add_val (o, "contributions", role + ": " + name)
			continue

		author = {}
		author["name"] = name
		add_val (o, "authors", author)

		# iname = c.get ("PersonNameInverted")
		# if iname:
		# 	author["inverted_name"] = iname
		# 	# XXX else construct inverted name from name parts

		pnis = c["PersonNameIdentifier":]
		if len (pnis) > 0:
			warn ("got PersonNameIdentifier(s): %s" % pnis[0]["IDValue"])

		# other_names = c["Name":]
		# XX: for pseudonyms, etc. ... should stash this somewhere

		for pdate in c["PersonDate":]:
			role = pdate["PersonDateRole"]
			# fmt = None
			# fmt_code = pdate.get ("DateFormat")
			# if fmt_code:
			# 	fmt = onix_codelists["List55"][fmt_code]
			date = pdate["Date"]
			if role == "007": author["birth_date"] = date
			elif role == "008": author["death_date"] = date
			else: die ("bad date role: %s" % role)

		bio = c.get ("BiographicalNote")
		if bio:
			author["bio"] = bio

		# website
		# country
		# region

	contrib = op.get ("ContributorStatement")
	if not o.get ("authors"):
		# XXX: shouldn't do this: the ContributorStatement could have anything in it
		# ... but this is the only way to get author names for one of the catalogs
		if contrib:
			author = {}
			author["name"] = re_by.sub ('', contrib)
			add_val (o, "authors", author)

	# edition
	ed_type = op.get ("EditionTypeCode")
	if ed_type:
		o["edition_type"] = self.codelists["List21"][ed_type][0]
	ed_number = op.get ("EditionNumber")
	if ed_number:
		ed_vers_num = op.get ("EditionVersionNumber")
		if ed_vers_num:
			ed_number += "-" + ed_vers_num
		o["edition_number"] = ed_number
	edition = op.get ("EditionStatement")
	if edition:
		o["edition"] = edition

	# format
	format = op.get ("ProductFormDescription")
	if format:
		o["physical_format"] = format
	npages = op.get ("NumberOfPages")
	if npages:
		o["number_of_pages"] = npages
	nillus = op.get ("NumberOfIllustrations")
	if nillus:
		o["number_of_illustrations"] = nillus
	ill_note = op.get ("IllustrationsNote")
	if ill_note:
		add_val (o, "notes", ill_note)
	# see also <illustrations> composite

	# dimensions

	# language
	# (see also <language> composite)
	lang_code = op.get ("LanguageOfText")
	if lang_code:
		o["language_code"] = lang_code
		o["language"] = self.codelists["List74"][lang_code][0]

	# subject
	bisac = op.get ("BASICMainSubject")
	if bisac:
		add_val (o, "BISAC_subject_categories", bisac)
	for subject in op["Subject":]:
		scheme = subject.get ("SubjectSchemeIdentifier")
		if scheme and scheme == "10":
			code = subject.get ("SubjectCode")
			if code:
				add_val (o, "BISAC_subject_categories", code)

	# description
	for text in op["OtherText":]:
		# type = text["TextTypeCode"]
		format = text["TextFormat"]
		if format not in ("00", "02", "07"): # ASCII, HTML, Basic ASCII
			raise Exception ("unsupported description format: %s" % self.codelists["List34"][format][0])
		if o.get ("description"):
			o["description"] += "\n" + text["Text"]
		else:
			o["description"] = text["Text"]
	if not o.get ("description"):
		descr = op.get ("MainDescription")
		if descr:
			o["description"] = descr

	self.receiver (o)

	# publisher
	for pub in op["Publisher":]:
		role = pub.get ("PublishingRole")
		if role is None or role == "01":
			name = pub.get ("PublisherName")
			if name:
				o["publisher"] = name
			break
	if not o.get ("publisher"):
		pub = op.get ("PublisherName")
		if pub:
			o["publisher"] = pub

	# imprint
	imprint = op.get ("Imprint")
	if imprint:
		name = imprint.get ("ImprintName")
		if name:
			o["imprint"] = name
	if not o.get ("imprint"):
		imprint = op.get ("ImprintName")
		if imprint:
			o["imprint"] = imprint

	# publish_status
	pstat = op.get ("PublishingStatus")
	if pstat and pstat != "??":
		status = self.codelists["List64"][pstat][0]
		pstatnote = op.get ("PublishingStatusNote")
		if pstatnote:
			stats += ": " + pstatnote
		o["publish_status"] = status

	# publish_date
	pdate = op.get ("PublicationDate")
	if pdate:
		o["publish_date"] = pdate # YYYY[MM[DD]]
		# XXX: need to convert

class MyErrorHandler:
	def error (self, exn):
		raise exn
	def fatalError (self, exn):
		raise exn
	def warning (self, exn):
		sys.stderr.write ("warning: %s\n" % exn.getMessage)

name_parts = ["TitlesBeforeNames", "NamesBeforeKey", "PrefixToKey", "KeyNames", "NamesAfterKey", "SuffixToKey"]
def person_name (x):
	global name_parts
	name = x.get ("PersonName")
	if not name:
		parts = [ p for p in map (lambda p: x.get (p), name_parts) if p ]
		name = " ".join (parts)
	if not name:
		iname = x.get ("PersonNameInverted")
		if iname:
			# XXX this often works, but is not reliable;
			# shouldn't really mess with unstructured names
			m = re_iname.match (iname)
			if m:
				name = m.group (2) + " " + m.group (1)
			else:
				name = iname
	if not name:
		name = x.get ("CorporateName")
	return name

def elt_get (e, tag, reference_name):
       ee = e.get (tag) or e.get (reference_name.lower ())
       if ee:
               return unicode (ee)
       else:
               return None

re_by = re.compile ("^\s*by\s+", re.IGNORECASE)
re_iname = re.compile ("^(.*),\s*(.*)$")

def add_val (o, key, val):
	if val is not None:
		o.setdefault (key, []).append (val)


########NEW FILE########
__FILENAME__ = sax_utils
import os
from types import *
import urlparse
from urlcache import URLCache
import xml.sax
from xml.sax.handler import *
import sys

class CachingEntityResolver (EntityResolver):
	def __init__ (self, parser, dir):
		self.parser = parser
		if not os.path.isdir (dir):
			raise Exception ("CachingEntityResolver: no such directory: %s" % dir)
		self.cache = URLCache (dir)

	def resolveEntity (self, pubid, sysid):
		parser_sysid = self.parser.getSystemId ()
		src = None
		if sysid.startswith ("http:"):
			src = self.resolveURL (sysid)
		elif isinstance (parser_sysid, StringTypes) and parser_sysid.startswith ("http:"):
			src = self.resolveURL (sysid, parser_sysid)
		if not src:
			src = EntityResolver.resolveEntity (self, p, s)
		return src

	def resolveURL (self, sysid, base = ""):
		url = urlparse.urljoin (base, sysid)
		source = xml.sax.xmlreader.InputSource (url)
		f = self.cache.get (url)
		source.setByteStream (f)
		return source

def collector_parse (input, dispatch):
	parser = xml.sax.make_parser ()
	parser.setFeature (xml.sax.handler.feature_namespaces, 1)
	handler = CollectorHandler (parser, dispatch)
	# parser.setContentHandler (handler)	# CollectorHandler sets ContentHandler
	parser.parse (input)
	return handler.get_value ()

class CollectorHandler:
	def __init__ (self, parser, base):
		self.parser = parser
		base_collector = None
		if isinstance (base, Collector):
			base_collector = base
		else:
			base_collector = NodeCollector (base)
		self.collectors = [base_collector]
		base_collector.start (None, self)
		self.set_handler ()

	def get_value (self):
		if len (self.collectors) == 1:
			return self.collectors[0].finish ()
		else:
			raise Exception ("CollectorHandler.get_value(): collection not finished")

	def top_collector (self):
		if not len (self.collectors):
			return None
		else:
			return self.collectors[-1]

	def push_collector (self, collector):
		self.collectors.append (collector)
		self.set_handler ()

	def pop_collector (self):
		self.collectors.pop ()
		self.set_handler ()

	def set_handler (self):
		self.parser.setContentHandler (self.top_collector ())

class Collector (ContentHandler):
	def start (self, parent, handler):
		self.parent = parent
		self.handler = handler
	def end (self):
		self.handler.pop_collector ()
		self.handler = None
		value = self.finish ()
		if not isinstance (value, CollectorNoneValue):
			self.parent.collect (value)
		self.parent = None
	def finish (self):
		pass
	def endElementNS (self, name, qname):
		self.end ()

class TextCollector (Collector):
	def __init__ (self):
		self.value = None
	def characters (self, content):
		self.value = content
	def finish (self):
		return self.value

class NodeCollector (Collector):
	def __init__ (self, collector_table, strict=False):
		self.collector_table = collector_table
		self.strict = strict
		self.ignoring = 0
		self.value = collector_none
	def startElementNS (self, name, qname, attrs):
		if self.ignoring:
			self.ignoring += 1
		else:
			(uri, localname) = name
			c_maker = self.collector_table.get (localname) or self.collector_table.get (collector_any)
			if c_maker:
				c = c_maker (name, attrs)
				c.start (self, self.handler)
				self.handler.push_collector (c)
			else:
				if self.strict:
					raise Exception ("no handler for element '%s'; handlers: %s" % (localname, self.collector_table.keys ()))
				else:
					self.ignoring += 1
	def endElementNS (self, name, qname):
		if self.ignoring:
			self.ignoring -= 1
		else:
			self.end ()
	def collect (self, value):
		self.value = value
	def finish (self):
		return self.value

class NamedCollector (NodeCollector):
	def __init__ (self, name, collector_table):
		NodeCollector.__init__ (self, collector_table)
		self.name = name
	def finish (self):
		if self.value is collector_none:
			return collector_none
		else:
			return (self.name, self.value)

class ListCollector (NodeCollector):
	def __init__ (self, collector_table):
		NodeCollector.__init__ (self, collector_table)
		self.values = []
	def collect (self, value):
		self.values.append (value)
	def finish (self):
		return self.values

class DictCollector (NodeCollector):
	def __init__ (self, collector_table):
		NodeCollector.__init__ (self, collector_table)
		self.values = {}
	def collect (self, key_value):
		(key, value) = key_value
		if self.values.get (key):
			raise Exception ("dictionary key '%s' is already mapped" % key)
		else:
			self.values[key] = value
	def finish (self):
		return self.values

class CollectorValue (NodeCollector):
	def __init__ (self, val):
		NodeCollector.__init__ (self, {}, strict=False)
		self.collect (val)

class CollectorNoneValue: pass
collector_none = CollectorNoneValue ()
def CollectorNone ():
	return CollectorValue (collector_none)

class CollectorAnyElement: pass
collector_any = CollectorAnyElement ()

########NEW FILE########
__FILENAME__ = thread_utils
# 2007 dbg for the Internet Archive

import sys
from threading import Thread, Lock, Condition

class AsyncChannel:
	# yes, i believe this is just Queue ... i was new to python and couldn't find it

	def __init__ (self, buffer_size=1):
		self.buffer = []
		self.max_items = buffer_size
		self.lock = Lock ()
		self.not_empty = Condition (self.lock)
		self.not_full = Condition (self.lock)

	def get (self):
		self.lock.acquire ()
		while len (self.buffer) == 0:
			self.not_empty.wait ()
		val = self.buffer.pop (0)
		self.not_full.notifyAll ()
		self.lock.release ()
		return val

	def put (self, val):
		self.lock.acquire ()
		while len (self.buffer) == self.max_items:
			self.not_full.wait ()
		self.buffer.append (val)
		self.not_empty.notifyAll ()
		self.lock.release ()

class ForeignException:

	def __init__ (self, exc_type, exc_value, exc_traceback):
		self.exc_type = exc_type
		self.exc_value = exc_value
		self.exc_traceback = exc_traceback

	def re_raise (self):
		raise self.exc_type, self.exc_value, self.exc_traceback

def ForeignException_extract ():
	(exc_type, exc_value, exc_traceback) = sys.exc_info()
	return ForeignException (exc_type, exc_value, exc_traceback)

def threaded_generator (producer, buffer_size=1):
	# the producer function will be invoked with a single argument, a "produce" function.
	# the producer may pass an object to this "produce" function any number of times before
	# returning.  the values thus passed will, in turn, be produced by the generator which
	# is the return value of threaded_generator().
	#
	# this provides a sort of coroutine facility, because python's generators can't do that:
	# they can only yield values from the bottom of the call stack.  sometimes you need to
	# keep control context between producing values.

	t = None
	chan = AsyncChannel (buffer_size)

	def produce (val):
		chan.put (val)

	def main ():
		try:
			producer (produce)
			chan.put (StopIteration ())
		except:
			chan.put (ForeignException_extract ())

	def generator ():
		while True:
			v = chan.get ()
			if isinstance (v, StopIteration):
				break
			if isinstance (v, ForeignException):
				v.re_raise ()
			else:
				yield v

	t = Thread (target=main)
	t.setDaemon (True)
	t.start ()
	return generator ()

########NEW FILE########
__FILENAME__ = urlcache
import sys
import os
import time
import shutil
import urllib
import traceback
from fcntl import *

class URLCache:
	def __init__ (self, dir):
		self.dir = dir

	def get_entries (self):
		entries = {}
		index_file = self.dir + "/index"
		next = 0
		index = open (index_file, "a")	# create index file if it doesn't exist
		index.close ()
		index = open (index_file, "r+")
		flock (index, LOCK_EX)

		for url in index:
			entries[url.rstrip ()] = next
			next += 1
		return (entries, next, index)

	def get (self, url):
		url = url.strip ()
		(entries, next, index) = self.get_entries ()
		id = entries.get (url)
		if id is None:
			# with index locked, add an entry for this url and
			# open a locked, temporary file to load its data
			index.seek (0, 2)
			index.write ("%s\n" % url)
			data_file = self.dir + "/" + str (next)
			tmp_data_file = data_file + "-fetching"
			tmp_data = open (tmp_data_file, "w")
			flock (tmp_data, LOCK_EX)
			index.close ()

			# having released the lock on the index, suck data
			# into the temporary file
			sys.stderr.write ("URLCache: fetching %s\n" % url)
			net_data = urllib.urlopen (url)
			shutil.copyfileobj (net_data, tmp_data)
			tmp_data.flush ()
			os.link (tmp_data_file, data_file)  # the fetch is good: attach it
			tmp_data.close ()		    # drop lock on temporary file
			os.unlink (tmp_data_file)
			id = next

		else:
			# there is already an entry for this url, so release the lock on the index
			index.close ()

		data_file = self.dir + "/" + str (id)
		if os.path.exists (data_file):
			return open (data_file, "r")
		else:
			# wait for fetch to finish
			tmp_data_file = data_file + "-fetching"
			sys.stderr.write ("URLCache: waiting for %s\n" % data_file)
			try:
				try:
					tmp_data = open (tmp_data_file)
					flock (tmp_data, LOCK_SH)
					tmp_data.close ()
				except OSError, e:
					pass
				return open (data_file, "r")
			except Exception, exn:
				# in case this happens, just blow away your cache
				raise Exception ("URLCache: sorry, corrupted state for url '%s': %s" % (url, str (exn)))

########NEW FILE########
__FILENAME__ = xmltramp
"""xmltramp: Make XML documents easily accessible."""

__version__ = "2.17"
__author__ = "Aaron Swartz"
__credits__ = "Many thanks to pjz, bitsko, and DanC."
__copyright__ = "(C) 2003-2006 Aaron Swartz. GNU GPL 2."

if not hasattr(__builtins__, 'True'): True, False = 1, 0
def isstr(f): return isinstance(f, type('')) or isinstance(f, type(u''))
def islst(f): return isinstance(f, type(())) or isinstance(f, type([]))

empty = {'http://www.w3.org/1999/xhtml': ['img', 'br', 'hr', 'meta', 'link', 'base', 'param', 'input', 'col', 'area']}

def quote(x, elt=True):
	if elt and '<' in x and len(x) > 24 and x.find(']]>') == -1: return "<![CDATA["+x+"]]>"
	else: x = x.replace('&', '&amp;').replace('<', '&lt;').replace(']]>', ']]&gt;')
	if not elt: x = x.replace('"', '&quot;')
	return x

class Element:
	def __init__(self, name, attrs=None, children=None, prefixes=None, line=None):
		if islst(name) and name[0] == None: name = name[1]
		if attrs:
			na = {}
			for k in attrs.keys():
				if islst(k) and k[0] == None: na[k[1]] = attrs[k]
				else: na[k] = attrs[k]
			attrs = na
		
		self._name = name
		self._attrs = attrs or {}
		self._dir = children or []
		
		prefixes = prefixes or {}
		self._prefixes = dict(zip(prefixes.values(), prefixes.keys()))
		
		if prefixes: self._dNS = prefixes.get(None, None)
		else: self._dNS = None

		self._line = line
	
	def __repr__(self, recursive=0, multiline=0, inprefixes=None):
		def qname(name, inprefixes): 
			if islst(name):
				if inprefixes[name[0]] is not None:
					return inprefixes[name[0]]+':'+name[1]
				else:
					return name[1]
			else:
				return name
		
		def arep(a, inprefixes, addns=1):
			out = ''

			for p in self._prefixes.keys():
				if not p in inprefixes.keys():
					if addns: out += ' xmlns'
					if addns and self._prefixes[p]: out += ':'+self._prefixes[p]
					if addns: out += '="'+quote(p, False)+'"'
					inprefixes[p] = self._prefixes[p]
			
			for k in a.keys():
				out += ' ' + qname(k, inprefixes)+ '="' + quote(a[k], False) + '"'
			
			return out
		
		inprefixes = inprefixes or {u'http://www.w3.org/XML/1998/namespace':'xml'}
		
		# need to call first to set inprefixes:
		attributes = arep(self._attrs, inprefixes, recursive) 
		out = '<' + qname(self._name, inprefixes)  + attributes 
		
		if not self._dir and (self._name[0] in empty.keys() 
		  and self._name[1] in empty[self._name[0]]):
			out += ' />'
			return out
		
		out += '>'

		if recursive:
			content = 0
			for x in self._dir: 
				if isinstance(x, Element): content = 1
				
			pad = '\n' + ('\t' * recursive)
			for x in self._dir:
				if multiline and content: out +=  pad 
				if isstr(x): out += quote(x)
				elif isinstance(x, Element):
					out += x.__repr__(recursive+1, multiline, inprefixes.copy())
				else:
					raise TypeError, "I wasn't expecting "+`x`+"."
			if multiline and content: out += '\n' + ('\t' * (recursive-1))
		else:
			if self._dir: out += '...'
		
		out += '</'+qname(self._name, inprefixes)+'>'
			
		return out
	
	def __unicode__(self):
		text = ''
		for x in self._dir:
			text += unicode(x)
		return ' '.join(text.split())
		
	def __str__(self):
		return self.__unicode__().encode('utf-8')
	
	def __getattr__(self, n):
		if n[0] == '_': raise AttributeError, "Use foo['"+n+"'] to access the child element."
		if self._dNS: n = (self._dNS, n)
		for x in self._dir:
			if isinstance(x, Element) and x._name == n: return x
		raise AttributeError, 'No child element named %s' % repr(n)
		
	def __hasattr__(self, n):
		for x in self._dir:
			if isinstance(x, Element) and x._name == n: return True
		return False
		
 	def __setattr__(self, n, v):
		if n[0] == '_': self.__dict__[n] = v
		else: self[n] = v
 

	def __getitem__(self, n):
		if isinstance(n, type(0)): # d[1] == d._dir[1]
			return self._dir[n]
		elif isinstance(n, slice(0).__class__):
			# numerical slices
			if isinstance(n.start, type(0)): return self._dir[n.start:n.stop]
			
			# d['foo':] == all <foo>s
			n = n.start
			if self._dNS and not islst(n): n = (self._dNS, n)
			out = []
			for x in self._dir:
				if isinstance(x, Element) and x._name == n: out.append(x) 
			return out
		else: # d['foo'] == first <foo>
			if self._dNS and not islst(n): n = (self._dNS, n)
			for x in self._dir:
				if isinstance(x, Element) and x._name == n: return x
			raise KeyError
	
	def __setitem__(self, n, v):
		if isinstance(n, type(0)): # d[1]
			self._dir[n] = v
		elif isinstance(n, slice(0).__class__):
			# d['foo':] adds a new foo
			n = n.start
			if self._dNS and not islst(n): n = (self._dNS, n)

			nv = Element(n)
			self._dir.append(nv)
			
		else: # d["foo"] replaces first <foo> and dels rest
			if self._dNS and not islst(n): n = (self._dNS, n)

			nv = Element(n); nv._dir.append(v)
			replaced = False

			todel = []
			for i in range(len(self)):
				if self[i]._name == n:
					if replaced:
						todel.append(i)
					else:
						self[i] = nv
						replaced = True
			if not replaced: self._dir.append(nv)
			for i in todel: del self[i]

	def __delitem__(self, n):
		if isinstance(n, type(0)): del self._dir[n]
		elif isinstance(n, slice(0).__class__):
			# delete all <foo>s
			n = n.start
			if self._dNS and not islst(n): n = (self._dNS, n)
			
			for i in range(len(self)):
				if self[i]._name == n: del self[i]
		else:
			# delete first foo
			for i in range(len(self)):
				if self[i]._name == n: del self[i]
				break
	
	def __call__(self, *_pos, **_set): 
		if _set:
			for k in _set.keys(): self._attrs[k] = _set[k]
		if len(_pos) > 1:
			for i in range(0, len(_pos), 2):
				self._attrs[_pos[i]] = _pos[i+1]
		if len(_pos) == 1 is not None:
			return self._attrs[_pos[0]]
		if len(_pos) == 0:
			return self._attrs

	def __len__(self): return len(self._dir)

	def get(self, n):
		try:
			return self.__getitem__(n)
		except KeyError:
			return None

	def getLineNumber (self):
		return self._line

class Namespace:
	def __init__(self, uri): self.__uri = uri
	def __getattr__(self, n): return (self.__uri, n)
	def __getitem__(self, n): return (self.__uri, n)

from xml.sax.handler import EntityResolver, DTDHandler, ContentHandler, ErrorHandler

class Seeder(EntityResolver, DTDHandler, ContentHandler, ErrorHandler):
	def __init__(self, parser=None):
		if parser:
			self.getLineNumber = lambda: parser.getLineNumber ()
		else:
			self.getLineNumber = lambda: None
		self.stack = []
		self.ch = ''
		self.prefixes = {}
		ContentHandler.__init__(self)
		
	def startPrefixMapping(self, prefix, uri):
		if not self.prefixes.has_key(prefix): self.prefixes[prefix] = []
		self.prefixes[prefix].append(uri)
	def endPrefixMapping(self, prefix):
		self.prefixes[prefix].pop()
	
	def startElementNS(self, name, qname, attrs):
		ch = self.ch; self.ch = ''	
		if ch and not ch.isspace(): self.stack[-1]._dir.append(ch)

		attrs = dict(attrs)
		newprefixes = {}
		for k in self.prefixes.keys(): newprefixes[k] = self.prefixes[k][-1]
		
		self.stack.append(Element(name, attrs, prefixes=newprefixes.copy(), line=self.getLineNumber ()))
	
	def characters(self, ch):
		self.ch += ch
	
	def endElementNS(self, name, qname):
		ch = self.ch; self.ch = ''
		if ch and not ch.isspace(): self.stack[-1]._dir.append(ch)
	
		element = self.stack.pop()
		if self.stack:
			self.stack[-1]._dir.append(element)
		else:
			self.result = element

from xml.sax import make_parser
from xml.sax.handler import feature_namespaces

def seed(fileobj):
	seeder = Seeder()
	parser = make_parser()
	parser.setFeature(feature_namespaces, 1)
	parser.setContentHandler(seeder)
	parser.parse(fileobj)
	return seeder.result

def parse(text):
	from StringIO import StringIO
	return seed(StringIO(text))

def load(url): 
	import urllib
	return seed(urllib.urlopen(url))

def unittest():
	parse('<doc>a<baz>f<b>o</b>ob<b>a</b>r</baz>a</doc>').__repr__(1,1) == \
	  '<doc>\n\ta<baz>\n\t\tf<b>o</b>ob<b>a</b>r\n\t</baz>a\n</doc>'
	
	assert str(parse("<doc />")) == ""
	assert str(parse("<doc>I <b>love</b> you.</doc>")) == "I love you."
	assert parse("<doc>\nmom\nwow\n</doc>")[0].strip() == "mom\nwow"
	assert str(parse('<bing>  <bang> <bong>center</bong> </bang>  </bing>')) == "center"
	assert str(parse('<doc>\xcf\x80</doc>')) == '\xcf\x80'
	
	d = Element('foo', attrs={'foo':'bar'}, children=['hit with a', Element('bar'), Element('bar')])
	
	try: 
		d._doesnotexist
		raise "ExpectedError", "but found success. Damn."
	except AttributeError: pass
	assert d.bar._name == 'bar'
	try:
		d.doesnotexist
		raise "ExpectedError", "but found success. Damn."
	except AttributeError: pass
	
	assert hasattr(d, 'bar') == True
	
	assert d('foo') == 'bar'
	d(silly='yes')
	assert d('silly') == 'yes'
	assert d() == d._attrs
	
	assert d[0] == 'hit with a'
	d[0] = 'ice cream'
	assert d[0] == 'ice cream'
	del d[0]
	assert d[0]._name == "bar"
	assert len(d[:]) == len(d._dir)
	assert len(d[1:]) == len(d._dir) - 1
	assert len(d['bar':]) == 2
	d['bar':] = 'baz'
	assert len(d['bar':]) == 3
	assert d['bar']._name == 'bar'
	
	d = Element('foo')
	
	doc = Namespace("http://example.org/bar")
	bbc = Namespace("http://example.org/bbc")
	dc = Namespace("http://purl.org/dc/elements/1.1/")
	d = parse("""<doc version="2.7182818284590451"
	  xmlns="http://example.org/bar" 
	  xmlns:dc="http://purl.org/dc/elements/1.1/"
	  xmlns:bbc="http://example.org/bbc">
		<author>John Polk and John Palfrey</author>
		<dc:creator>John Polk</dc:creator>
		<dc:creator>John Palfrey</dc:creator>
		<bbc:show bbc:station="4">Buffy</bbc:show>
	</doc>""")

	assert repr(d) == '<doc version="2.7182818284590451">...</doc>'
	assert d.__repr__(1) == '<doc xmlns:bbc="http://example.org/bbc" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns="http://example.org/bar" version="2.7182818284590451"><author>John Polk and John Palfrey</author><dc:creator>John Polk</dc:creator><dc:creator>John Palfrey</dc:creator><bbc:show bbc:station="4">Buffy</bbc:show></doc>'
	assert d.__repr__(1,1) == '<doc xmlns:bbc="http://example.org/bbc" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns="http://example.org/bar" version="2.7182818284590451">\n\t<author>John Polk and John Palfrey</author>\n\t<dc:creator>John Polk</dc:creator>\n\t<dc:creator>John Palfrey</dc:creator>\n\t<bbc:show bbc:station="4">Buffy</bbc:show>\n</doc>'

	assert repr(parse("<doc xml:lang='en' />")) == '<doc xml:lang="en"></doc>'

	assert str(d.author) == str(d['author']) == "John Polk and John Palfrey"
	assert d.author._name == doc.author
	assert str(d[dc.creator]) == "John Polk"
	assert d[dc.creator]._name == dc.creator
	assert str(d[dc.creator:][1]) == "John Palfrey"
	d[dc.creator] = "Me!!!"
	assert str(d[dc.creator]) == "Me!!!"
	assert len(d[dc.creator:]) == 1
	d[dc.creator:] = "You!!!"
	assert len(d[dc.creator:]) == 2
	
	assert d[bbc.show](bbc.station) == "4"
	d[bbc.show](bbc.station, "5")
	assert d[bbc.show](bbc.station) == "5"

	e = Element('e')
	e.c = '<img src="foo">'
	assert e.__repr__(1) == '<e><c>&lt;img src="foo"></c></e>'
	e.c = '2 > 4'
	assert e.__repr__(1) == '<e><c>2 > 4</c></e>'
	e.c = 'CDATA sections are <em>closed</em> with ]]>.'
	assert e.__repr__(1) == '<e><c>CDATA sections are &lt;em>closed&lt;/em> with ]]&gt;.</c></e>'
	e.c = parse('<div xmlns="http://www.w3.org/1999/xhtml">i<br /><span></span>love<br />you</div>')
	assert e.__repr__(1) == '<e><c><div xmlns="http://www.w3.org/1999/xhtml">i<br /><span></span>love<br />you</div></c></e>'	
	
	e = Element('e')
	e('c', 'that "sucks"')
	assert e.__repr__(1) == '<e c="that &quot;sucks&quot;"></e>'

	
	assert quote("]]>") == "]]&gt;"
	assert quote('< dkdkdsd dkd sksdksdfsd fsdfdsf]]> kfdfkg >') == '&lt; dkdkdsd dkd sksdksdfsd fsdfdsf]]&gt; kfdfkg >'
	
	assert parse('<x a="&lt;"></x>').__repr__(1) == '<x a="&lt;"></x>'
	assert parse('<a xmlns="http://a"><b xmlns="http://b"/></a>').__repr__(1) == '<a xmlns="http://a"><b xmlns="http://b"></b></a>'
	
if __name__ == '__main__': unittest()

########NEW FILE########
__FILENAME__ = build_object
from openlibrary.catalog.utils import flip_name, pick_first_date


def build_person_object(p, marc_alt):
    ab = [(k, v.strip(' /,;:')) for k, v in p if k in 'ab']

    has_b = any(k=='b' for k, v in p)

    orig_name = ' '.join(v if k == 'a' else v for k, v in ab)
    c = ' '.join(v for k, v in p if k == 'c')
    name = flip_name(orig_name)
    if name[0].isdigit():
        name = orig_name
    else:
        of_count = c.count('of ')
    #    if of_count == 1 and not has_b and 'of the ' not in c:
    #        if c.startswith('King')
    #
    #        if c.startswith('Queen'):
    #        name += ' ' + c[c.find('of '):]
    #
        if of_count == 1 and 'of the ' not in c and 'Emperor of ' not in c:
            name += ' ' + c[c.find('of '):]
        elif ' ' not in name and of_count > 1:
            name += ', ' + c
        elif c.endswith(' of') or c.endswith(' de') and any(k == 'a' and ', ' in v for k, v in p):
            name = ' '.join(v for k, v in ab)
            c += ' ' + name[:name.find(', ')]
            name = name[name.find(', ') + 2:] + ', ' + c

    person = {}
    d = [v for k, v in p if k =='d']
    if d:
        person = pick_first_date(d)
    person['name'] = name
    person['sort'] = orig_name

    if any(k=='b' for k, v in p):
        person['enumeration'] = ' '.join(v for k, v in p if k == 'b')

    if c:
        person['title'] = c
    person['marc'] = [p] + list(marc_alt)

    return person

def test_consort():
    line = (('a', u'Elizabeth'), ('c', u'Empress, consort of Franz Joseph, Emperor of Austria'))
    p = build_person_object(marc, [])
    p['name'] == u'Empress Elizabeth, consort of Franz Joseph, Emperor of Austria',

    line = (('a', u'Mary'), ('c', u'Queen, Consort of George V, King of Great Britain'), ('d', u'1867-1953'))
    p = build_person_object(marc, [])
    p['name'] == u'Queen Mary, Consort of George V, King of Great Britain'

def test_king_no_number():
    marc = (('a', u'Henry'), ('b', u'IV'), ('c', u'King of England'), ('d', u'1367-1413'))
    p = build_person_object(marc, [])
    assert p['name'] == u'Henry IV of England'

    marc = (('a', u'John'), ('c', u'King of England'), ('d', u'1167-1216'))
    p = build_person_object(marc, [])
    assert p['name'] == 'King John of England'


########NEW FILE########
__FILENAME__ = from_works
from openlibrary.catalog.utils.query import query_iter, set_staging, get_mc
from openlibrary.catalog.get_ia import get_data
from openlibrary.catalog.marc.fast_parse import get_tag_lines, get_all_subfields, get_subfields

from pprint import pprint
from identify_people import read_people
from build_object import build_person_object
import sys
from collections import defaultdict

set_staging(True)

def work_and_marc():
    i = 0
    skip = True
    for w in query_iter({'type': '/type/work', 'title': None}):
        if skip:
            if w['key'] == '/w/OL56814W':
                skip = False
            else:
                continue
        marc = set()
        q = {'type': '/type/edition', 'works': w['key'], 'title': None, 'source_records': None}
        for e in query_iter(q):
            if e.get('source_records', []):
                marc.update(i[5:] for i in e['source_records'] if i.startswith('marc:'))
            mc = get_mc(e['key'])
            if mc and not mc.startswith('ia:') and not mc.startswith('amazon:'):
                marc.add(mc)
        if marc:
            yield w, marc
        else:
            print 'no marc:', w


def read_works():
    i = 0
    pages = {}
    page_marc = {}

    for work, marc in work_and_marc():
        lines = []
        for loc in marc:
            data = get_data(loc)
            if not data:
                continue
            found = [v for k, v in get_tag_lines(data, set(['600']))]
            if found:
                lines.append((loc, found))
        if not lines:
            continue
        work['lines'] = lines
        i += 1
        print i, work['key'], work['title']

        try:
            people, marc_alt = read_people(j[1] for j in lines)
        except AssertionError:
            print work['lines']
            continue
        except KeyError:
            print work['lines']
            continue

        marc_alt_reverse = defaultdict(set)
        for k, v in marc_alt.items():
            marc_alt_reverse[v].add(k)

        w = ol.get(work['key'])
        w['subject_people'] = []
        for p, num in people.iteritems():
            print '  %2d %s' % (num, ' '.join("%s: %s" % (k, v) for k, v in p))
            print '     ', p
            if p in page_marc:
                w['subject_people'].append({'key': '/subjects/people/' + page_marc[p]})
                continue
            obj = build_person_object(p, marc_alt_reverse.get(p, []))
            key = obj['name'].replace(' ', '_')
            full_key = '/subjects/people/' + key
            w['subject_people'].append({'key': full_key})

            if key in pages:
                print key
                pages[key]['marc'].append(p)
                continue

            for m in obj['marc']:
                page_marc[m] = key

            pages[key] = obj
            obj_for_db = obj.copy()
            del obj_for_db['marc']
            obj_for_db['key'] = full_key
            obj_for_db['type'] = '/type/person'
            print ol.save(full_key.encode('utf-8'), obj_for_db, 'create a new person page')

        print w
        print ol.save(w['key'], w, 'add links to people that this work is about')

def from_sample():
    i = 0
    pages = {}
    page_marc = {}
    for line in open('work_and_marc5'):
        i += 1
        w = eval(line)
#        print i, w['key'], w['title']
#        print w['lines']
        try:
            people, marc_alt = read_people(j[1] for j in w['lines'])
        except AssertionError:
            print [j[1] for j in w['lines']]
            raise
        marc_alt_reverse = defaultdict(set)
        for k, v in marc_alt.items():
            marc_alt_reverse[v].add(k)

        for p, num in people.iteritems():
            if p in page_marc:
                continue
            obj = build_person_object(p, marc_alt_reverse.get(p, []))
            key = obj['name'].replace(' ', '_')
            for m in obj['marc']:
                page_marc[m] = key
            if key in pages:
#                print key
#                print p
#                for m in pages[key]['marc']:
#                    print m
#                print
                pages[key]['marc'].append(p)
            else:
                pages[key] = obj
#                pprint(obj)
#                continue
                if obj['name'][1].isdigit():
                    print [j[1] for j in w['lines']]
                    pprint(obj)
#                assert not obj['name'][1].isdigit()

from_sample()
#read_works()

########NEW FILE########
__FILENAME__ = identify_people
from openlibrary.catalog.marc.cmdline import fmt_subfields
from openlibrary.catalog.marc.fast_parse import get_subfields, get_all_subfields
from openlibrary.catalog.utils import remove_trailing_dot, remove_trailing_number_dot, match_with_bad_chars, pick_first_date
import openlibrary.catalog.utils.authority as authority
from openlibrary.catalog.merge.normalize import normalize
from collections import defaultdict
from pprint import pprint
import re

def strip_death(date):
    return date[:date.rfind('-')+1]

def test_strip_death():
    assert strip_death("1850-1910") == "1850-"

re_dates = re.compile(' (\d{2,4}-(?:\d{2,4})?)$')
re_dates_in_field = re.compile('^(\d{2,4})-((?:\d{2,4})?)$')
re_dates_in_field_bc = re.compile('^(\d{2,4}) B\.C\.-((?:\d{2,4}) B\.C\.?)$')

def test_transpose_date():
    assert transpose_date(u'1452') == u'1425'

def transpose_date(date):
    return date[:-2] + date[-1] + date[-2]

def is_date_transposed(d1, d2):
    m1 = re_dates_in_field.match(d1)
    m2 = re_dates_in_field.match(d2)
    if not m1 and not m2:
        m1 = re_dates_in_field_bc.match(d1)
        m2 = re_dates_in_field_bc.match(d2)
    if not m1 or not m2:
        return False

    if m1.group(1) == m2.group(1):
        death1 = m1.group(2)
        death2 = m2.group(2)
        if not death1 or not death2:
            return False
        return transpose_date(death1) == death2
    if m1.group(2) == m2.group(2):
        birth1 = m1.group(1)
        birth2 = m2.group(1)
        return transpose_date(birth1) == birth2
    return False

def test_is_date_transposed():
    assert is_date_transposed(u'1452-1485', u'1425-1485')

def dates_not_close(d1, d2):
    m1 = re_dates_in_field.match(d1)
    if not m1:
        return False
    m2 = re_dates_in_field.match(d2)
    if not m2:
        return False

    birth1 = int(m1.group(1))
    birth2 = int(m2.group(1))
    if abs(birth1 - birth2) >= 10:
        return True

    if not m1.group(2) or not m2.group(2):
        return False

    death1 = int(m1.group(2))
    death2 = int(m2.group(2))
    return abs(death1 - death2) >= 10

def test_dates_not_close():
    assert dates_not_close('1825-1899', '1804-1849') 
    assert not dates_not_close(u'1907-2003', u'1909-')
    assert not dates_not_close('1825-1899', '1826-1898') 

def combinations(iterable, r):
    # combinations('ABCD', 2) --> AB AC AD BC BD CD
    # combinations(range(4), 3) --> 012 013 023 123
    pool = tuple(iterable)
    n = len(pool)
    if r > n:
        return
    indices = range(r)
    yield tuple(pool[i] for i in indices)
    while True:
        for i in reversed(range(r)):
            if indices[i] != i + n - r:
                break
        else:
            return
        indices[i] += 1
        for j in range(i+1, r):
            indices[j] = indices[j-1] + 1
        yield tuple(pool[i] for i in indices)

def tidy_subfield(v):
    return remove_trailing_dot(v.strip(' /,;:'))

re_bc_date = re.compile('(\d+)B\.C\.')

def clean_subfield(k, v):
    if k in 'abc':
        v = tidy_subfield(v)
    elif k == 'd':
        v = remove_trailing_number_dot(v.strip(' ,'))
        v = re_bc_date.sub(lambda m: m.group(1) + " B.C.", v)
    return (k, v)

def has_subtag(a, subfields):
    return any(k==a for k, v in subfields)

def question_date(p1, p2):
    marc_date1 = tuple(v for k, v in p1 if k =='d')
    if not marc_date1:
        return
    marc_date2 = tuple(v for k, v in p2 if k =='d')
    if not marc_date1 or not marc_date2 or marc_date1 == marc_date2:
        return

    assert len(marc_date1) == 1 and len(marc_date2) == 1

    name1 = tuple((k, v) for k, v in p1 if k !='d')
    name2 = tuple((k, v) for k, v in p2 if k !='d')
    if name1 != name2:
        return

    marc_date1 = marc_date1[0]
    question1 = '?' in marc_date1

    marc_date2 = marc_date2[0]
    question2 = '?' in marc_date2

    if (not question1 and not question2) or (question1 and question2):
        return # xor

    if marc_date1.replace('?', '') != marc_date2.replace('?', ''):
        return
    return 1 if question1 else 2

def get_marc_date(p):
    marc_date = tuple(v for k, v in p if k =='d')
    if not marc_date:
        return
    assert len(marc_date) == 1
    return marc_date[0].strip()

def build_by_name(found):
    by_name = defaultdict(set)
    for p in found:
        if has_subtag('d', p):
            without_date = tuple(i for i in p if i[0] != 'd')
            by_name[without_date].add(p)

    return by_name

def build_name_and_birth(found):
    # one author missing death date
    name_and_birth = defaultdict(set)
    for p in found:
        d = get_marc_date(p)
        if not d or d[-1] == '-' or '-' not in d:
            continue
        without_death = tuple((k, (v if k!='d' else strip_death(v))) for k, v in p)
#        assert without_death not in name_and_birth
        name_and_birth[without_death].add(p)
    return name_and_birth

def authority_lookup(to_check, found, marc_alt):
    found_matches = False
    for person_key, match in to_check.items():
        if len(match) == 1:
            continue
        if len(match) == 2:
            d1, d2 = [get_marc_date(p) for p in match]
            if dates_not_close(d1, d2) and not is_date_transposed(d1, d2):
                continue

        name = ' '.join(v.strip() for k, v in person_key if k != 'd')
        search_results = authority.search(name)
        match_dates = dict((get_marc_date(p), p) for p in match)
        norm_name = normalize(name)
        authority_match = None
        for i in search_results:
            if i['type'] != 'personal name' or i['a'] == 'reference':
                continue
            if norm_name not in normalize(i['heading']):
                continue
            for d, p in match_dates.items():
                if i['heading'].endswith(d):
                    if authority_match: # more than one match
                        print 'dups:', match_dates.items()
                        authority_match = None
                        break
                    authority_match = p
        if authority_match:
            for p in match:
                if p == authority_match:
                    continue
                found[authority_match] += found.pop(p)
                marc_alt[p] = authority_match
                found_matches = True
    return found_matches

def subtag_should_be_c(found, marc_alt):
    merge = []
    for p1, p2 in combinations(found, 2):
        if len(p1) != len(p2):
            continue

        subtag1 = [k for k, v in p1 if k in 'abcdq']
        subtag2 = [k for k, v in p2 if k in 'abcdq']

        if subtag1 == subtag2:
            continue

        def no_question_if_d(p):
            return [v.replace('?', '') if k == 'd' else tidy_subfield(v) for k, v in p]
        if no_question_if_d(p1) != no_question_if_d(p2):
            continue

        for i in range(len(subtag1)):
            if subtag1[i] == subtag2[i]:
                continue
            assert len(p1[i][1]) >= 5

            if subtag1[i] == 'c':
                assert subtag2[i] in 'bq'
                merge.append((p1, p2))
            else:
                assert subtag1[i] in 'bq' and subtag2[i] == 'c'
                merge.append((p2, p1))
            break

    for a, b in merge:
        if b not in found:
            continue
        found[a] += found.pop(b)
        marc_alt[b] = a

def merge_question_date(found, marc_alt):
    merge = []
    for p1, p2 in combinations(found, 2):
        primary = question_date(p1, p2)
        if primary is None:
            continue
        if primary == 1:
            merge.append((p1, p2))
        else:
            assert primary == 2
            merge.append((p2, p1))

    for a, b in merge:
        found[a] += found.pop(b)
        marc_alt[b] = a

re_bad_marc = re.compile(' ?\$ ?[a-z] ')
def remove_bad_marc_subtag(s):
    s = re_bad_marc.sub(' ', s)
    return s

def test_remove_bad_marc_subtag():
    assert remove_bad_marc_subtag('John, $ c King of England') == 'John, King of England'


def just_abcdq(p):
    return tuple((k, v) for k, v in p if k in 'abcdq')

def similar_dates(found, marc_alt):
    # 1516 == d. 1516
    merge = []
    for p1, p2 in combinations(found, 2):
        subtag1 = [k for k, v in p1]
        subtag2 = [k for k, v in p2]
        if subtag1 != subtag2:
            continue
        if [(k, v) for k, v in p1 if k != 'd'] != [(k, v) for k, v in p2 if k != 'd']:
            continue
        d1 = [v for k, v in p1 if k == 'd']
        d2 = [v for k, v in p2 if k == 'd']
        if d1 == d2:
            continue
        assert len(d1) == 1 and len(d2) == 1
        d1, d2 = d1[0], d2[0]
        if d1 == 'd. ' + d2:
            merge.append((p1, p2))
            continue
        if d2 == 'd. ' + d1:
            merge.append((p2, p1))
            continue

    for a, b in merge:
        if b not in found:
            continue
        found[a] += found.pop(b)
        marc_alt[b] = a

re_simple_date = re.compile('^(\d+)-(\d+)?\.?$')

def fix_bad_subtags(found, marc_alt):
    just_values = defaultdict(lambda:defaultdict(int))
    for p, num in found.items():
        just_values[tuple(v.strip(',') for k, v in p)][p] += num

    for a, b in just_values.items():
        if len(b) == 1:
            continue
        b = sorted(b.items(), key=lambda i:i[1])
        if b[-1][1] == b[-2][1]:
            continue
        new = b.pop()[0]
        for i, j in b:
            found[new] += found.pop(i)
            marc_alt[i] = new

def wrong_subtag_on_date(found, marc_alt):
    for p in found.keys():
        found_simple_date = False
        for k, v in p:
            if k != 'd' and re_simple_date.match(v):
                found_simple_date = True
                break
        if not found_simple_date:
            continue
        new = tuple((('d' if k !='d' and re_simple_date.match(v) else k), v) for k, v in p)
        if new in found:
            found[new] += found.pop(p)
            marc_alt[p] = new

def missing_subtag(found, marc_alt):
    merge = defaultdict(set)
    for p1, p2 in combinations(found, 2):
        subtag1 = [k for k, v in p1 if k in 'abcdq']
        subtag2 = [k for k, v in p2 if k in 'abcdq']

        if subtag1 == subtag2:
            continue

        name1 = ' '.join(v.strip() for k, v in p1)
        name2 = ' '.join(v.strip() for k, v in p2)

        if not match_with_bad_chars(name1, name2) \
                and normalize(name1) != normalize(name2) \
                and normalize(remove_bad_marc_subtag(name1)) != normalize(remove_bad_marc_subtag(name2)) \
                and normalize(name1.lower().replace(' the', '')) != normalize(name2.lower().replace(' the', '')):
            continue

        if len(subtag1) > len(subtag2):
            merge[p2].add(just_abcdq(p1))
        else:
            merge[p1].add(just_abcdq(p2))

    def flat_len(p):
        return len(' '.join(v for k, v in p))

    for old, new in merge.items():
        by_size = sorted((flat_len(p), p) for p in new)
        if len(by_size) > 1:
            assert by_size[-1][0] > by_size[-2][0]
        new_marc = by_size[-1][1]

        found[new_marc] += found.pop(old)
        marc_alt[old] = new_marc

def date_field_missing(p):
    if has_subtag('d', p):
        return p
    assert has_subtag('a', p)
    for k, v in p:
        if k == 'a':
            a = v
            break
    m = re_dates.search(a)
    if not m:
        return p
    d = m.group(1)
    a = tidy_subfield(a[:m.start()])
    new = []
    for k, v in p:
        if k == 'a' and a:
            new.append(('a', a))
            a = None
            continue
        if k not in ('b', 'c') and d:
            new.append(('d', d))
            d = None
        new.append((k, v))
    if d:
        new.append(('d', d))
    return tuple(new)

def bad_char_name_match(found, marc_alt):
    merge = []
    for p1, p2 in combinations(found, 2):
        if p1 == p2:
            continue
        if get_marc_date(p1) != get_marc_date(p2):
            continue
        p1, p2 = sorted([p1, p2], key=lambda i:found[i])
        if found[p1] != found[p2]:
            name1 = ' '.join(v for k, v in p1 if k in 'abc')
            name2 = ' '.join(v for k, v in p2 if k in 'abc')
            if match_with_bad_chars(name1, name2):
                found[p2] += found.pop(p1)
                marc_alt[p1] = p2

    for a, b in merge:
        if b not in found:
            continue
        found[a] += found.pop(b)
        marc_alt[b] = a

def check_for_dup_a(p):
    for a1, a2 in combinations((v for k, v in p if k == 'a'), 2):
        assert a1 != a2

def read_people(people):
    found = defaultdict(int)
    marc_alt = {}
    people = list(people)

    for lines in people:
        for line in lines:
            p = tuple(clean_subfield(k, v) for k, v in get_all_subfields(line))
            #check_for_dup_a(p)
            found[date_field_missing(p)]+=1

    for p in found.keys():
        c = None
        for k, v in p:
            if k == 'c':
                c = v
                break
        if not c or c.lower() != 'family':
            continue
        new = tuple((k, v + ' family' if k == 'a' else v) for k, v in p if k != 'c') 
        if new in found:
            found[new] += found.pop(p)
            marc_alt[p] = new

    fix_bad_subtags(found, marc_alt)

    wrong_subtag_on_date(found, marc_alt)

    try:
        missing_subtag(found, marc_alt)
    except AssertionError:
        print people
        raise

    found_name = defaultdict(int)
    for p, num in found.items():
        found_name[just_abcdq(p)] += num
    found = found_name

    assert found

    if len(found) == 1:
        return dict(found), marc_alt

    #for func in subtag_should_be_c, merge_question_date:
    #for func in subtag_should_be_c, merge_question_date, missing_subtag, bad_char_name_match:
    for func in subtag_should_be_c, merge_question_date, bad_char_name_match, similar_dates:
        func(found, marc_alt)

        if len(found) == 1:
            return dict(found), marc_alt

    assert found

    # one author missing death date
    name_and_birth = build_name_and_birth(found)

    assert found

    try:
        if authority_lookup(name_and_birth, found, marc_alt):
            if len(found) == 1:
                return dict(found), marc_alt

            name_and_birth = build_name_and_birth(found)
    except AssertionError:
        print people
        raise

    assert found

    for p, num in found.items():
        if p not in name_and_birth:
            continue
        assert len(name_and_birth[p]) == 1
        new_name = list(name_and_birth[p])[0]
        found[new_name] += found.pop(p)
        marc_alt[p] = new_name

    assert found

    if len(found) == 1:
        return dict(found), marc_alt

    # match up authors with the same name
    # where one has dates and the other doesn't
    by_name = build_by_name(found)

    try:
        if authority_lookup(by_name, found, marc_alt):
            if len(found) == 1:
                return dict(found), marc_alt
            by_name = build_by_name(found) # rebuild
    except AssertionError:
        print people
        raise

    for p, num in found.items():
        if p not in by_name:
            continue
        if len(by_name[p]) != 1:
            for i in by_name[p]:
                print i
            print people
        assert len(by_name[p]) == 1
        new_name = list(by_name[p])[0]
        found[new_name] += found.pop(p)
        marc_alt[p] = new_name
    assert found

    if len(found) == 1:
        return dict(found), marc_alt

    by_date = defaultdict(set)
    for p in found:
        if not has_subtag('d', p):
            continue
        d = tuple(v for k, v in p if k=='d')
        by_date[d].add(p)
#    for k, v in by_date.iteritems():
#        print len(v), k, v

    return dict(found), marc_alt

def read_files():
    read_file('work_and_marc2')
    read_file('work_and_marc3')

def read_file(filename):
    for file_line in open(filename):
        w = eval(file_line)
        if len(w['lines']) == 1:
            continue
        lines = [i[1] for i in w['lines']]
        print w['key'], w['title']
        print lines
        people, marc_alt = read_people(lines)
#        for p, num in people.iteritems():
#            if any(k=='d' for k, v in people):
#                continue
        for p, num in people.iteritems():
            print '  %2d %s' % (num, ' '.join("%s: %s" % (k, v) for k, v in p))
            print '     ', p
        print
#read_file()

def test_accents():
    lines = [
        ['00\x1faB\xe5adar\xe5aya\xf2na.\x1ftBrahmas\xe5utra.\x1e'], 
        ['00\x1faB\xe5adar\xe5aya\xf2na.\x1ftBrahmas\xe5utra.\x1e'], 
        ['00\x1faB\xe5adar\xe5aya\xf2na.\x1ftBrahmas\xe5utra.\x1e'], 
        ['00\x1faB\xe5adar\xe5aya\xf2na.\x1ftBrahmas\xe5utra.\x1e'], 
        ['00\x1faB\xe5adar\xe5aya\xf2na.\x1ftBrahmas\xe5utra.\x1e'], 
        ['00\x1faB\xe5adar\xe5ayana.\x1ftBrahmas\xe5utra.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {(('a', u'B\u0101dar\u0101ya\u1e47a'),): 6}
    assert b == { (('a', u'B\u0101dar\u0101yana'),): (('a', u'B\u0101dar\u0101ya\u1e47a'),)}

def test_same_name_one_date_missing():
    lines = [
        ['10\x1faAbedin, Zainul\x1fxCriticism and interpretation.\x1e'], 
        ['10\x1faAbedin, Zainul,\x1fd1914-1976\x1fxCriticism and interpretation.\x1e'],

        ['10\x1faAbedin, Zainul\x1fxCriticism and interpretation.\x1e'], 
        ['10\x1faAbedin, Zainul,\x1fd1914-1976\x1fxCriticism and interpretation.\x1e']
    ]
    a, b = read_people(lines)

    assert a == {(('a', u'Abedin, Zainul'), ('d', u'1914-1976')): 4}
    assert b == {(('a', u'Abedin, Zainul'),): (('a', u'Abedin, Zainul'), ('d', u'1914-1976'))}

def test_matching_name_missing_death():
    lines = [
        ['10\x1faFrisch, Max,\x1fd1911-1991\x1e'],
        ['10\x1faFrisch, Max,\x1fd1911-\x1e'],
        ['10\x1faFrisch, Max,\x1fd1911-\x1e']
    ]
    a, b = read_people(lines)
    assert a == {(('a', u'Frisch, Max'), ('d', u'1911-1991')): 3}
    assert b == {(('a', u'Frisch, Max'), ('d', u'1911-')): (('a', u'Frisch, Max'), ('d', u'1911-1991'))}

def test_matching_dates():
    lines = [
        ['00\x1faMichelangelo Buonarroti,\x1fd1475-1564.\x1e'],
        ['00\x1faMichelangelo Buonarroti,\x1fd1475-1564.\x1e'],
        ['16\x1faBuonarroti, Michel Angelo,\x1fd1475-1564.\x1e']
    ]
    a, b = read_people(lines)

def test_harold_osman_kelly():
    lines = [
        ['10\x1faKelly, Harold Osman,\x1fd1884-1955.\x1e'],
        ['10\x1faKelly, Harold Osman,\x1fd1884-1956.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {(('a', u'Kelly, Harold Osman'), ('d', u'1884-1955')): 2}
    assert b == {(('a', u'Kelly, Harold Osman'), ('d', u'1884-1956')): (('a', u'Kelly, Harold Osman'), ('d', u'1884-1955'))}

def test_question_date():
    lines = [
        ['10\x1faBurke, Edmund,\x1fd1729?-1797.\x1ftReflections on the revolution in France.\x1e', '10\x1faCalonne,\x1fcM. de\x1fq(Charles Alexandre de),\x1fd1734-1802.\x1e'],
        ['10\x1faBurke, Edmund,\x1fd1729-1797.\x1ftReflections on the Revolution in France.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        (('a', u'Burke, Edmund'), ('d', u'1729?-1797')): 2,
        (('a', u'Calonne'), ('c', u'M. de'), ('q', u'(Charles Alexandre de),'), ('d', u'1734-1802')): 1
    }

    assert b == {
        (('a', u'Burke, Edmund'), ('d', u'1729-1797')): (('a', u'Burke, Edmund'), ('d', u'1729?-1797'))
    }


def test_pope_sixtus():
    lines = [
        ['00\x1faSixtus\x1fbV,\x1fcPope,\x1fd1521-1590.\x1e'], 
        ['04\x1faSixtus\x1fbV,\x1fcPope.\x1e'],
        ['00\x1faSixtus\x1fbV,\x1fcPope,\x1fd1520-1590.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        ((u'a', u'Sixtus'), (u'b', u'V'), (u'c', u'Pope'), (u'd', u'1520-1590')): 3
    }

    assert b == {
        (('a', u'Sixtus'), ('b', u'V'), ('c', u'Pope')): (('a', u'Sixtus'), ('b', u'V'), ('c', u'Pope'), ('d', u'1520-1590')),
        (('a', u'Sixtus'), ('b', u'V'), ('c', u'Pope'), ('d', u'1521-1590')): (('a', u'Sixtus'), ('b', u'V'), ('c', u'Pope'), ('d', u'1520-1590'))
    }

def test_william_the_conqueror():
    lines = [
        ['00\x1faWilliam\x1fbI,\x1fcKing of England,\x1fd1027 or 8-1087.\x1e'], ['04\x1faWilliam\x1fbI,\x1fcKing of England,\x1fd1027?-1087.\x1e'],
        ['00\x1faWilliam\x1fbI,\x1fcKing of England,\x1fd1027 or 8-1087.\x1e'], ['00\x1faWilliam\x1fbI,\x1fcKing of England,\x1fd1027 or 8-1087\x1e'],
        ['00\x1faWilliam\x1fbI,\x1fcKing of England,\x1fd1027 or 8-1087.\x1e'], ['00\x1faWilliam\x1fbI,\x1fcKing of England,\x1fd1027 or 8-1087.\x1e']
    ]
    a, b = read_people(lines)

    assert a == {(('a', u'William'), ('b', u'I'), ('c', u'King of England'), ('d', u'1027 or 8-1087')): 6}
    assert b == {(('a', u'William'), ('b', u'I'), ('c', u'King of England'), ('d', u'1027?-1087')): (('a', u'William'), ('b', u'I'), ('c', u'King of England'), ('d', u'1027 or 8-1087'))}

def test_missing_d():
    lines = [
        [' 0\x1faDickens, Charles, 1812-1870\x1fxManuscripts\x1fxFacsimiles.\x1e'],
        ['10\x1faDickens, Charles,\x1fd1812-1870\x1fxManuscripts\x1fxFacsimiles.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {(('a', u'Dickens, Charles'), ('d', u'1812-1870')): 2}
    #assert b == {(('a', u'Dickens, Charles, 1812-1870'),): (('a', u'Dickens, Charles'), ('d', u'1812-1870'))}
    assert b == {}

def test_missing_c():
    return # skip for now
    lines = [
        ['00\x1faMuhammad Quli Qutb Shah,\x1fcSultan of Golkunda,\x1fd1565-1612.\x1e'],
        ['00\x1faMuhammad Quli Qutb Shah,\x1fcSultan of Golkunda,\x1fd1565-1612.\x1e'],
        ['10\x1faMuhammad Quli Qutb Shah, Sultan of Golconda,\x1fd1565-1612\x1e']
    ]
    a, b = read_people(lines)
    assert a == {(('a', u'Muhammad Quli Qutb Shah'), ('c', u'Sultan of Golkunda'), ('d', u'1565-1612')): 3}

def test_same_len_subtag():
    lines = [
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1e', '10\x1faShakespeare, William,\x1fd1564-1616\x1fxStage history\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e', '10\x1faShakespeare, William,\x1fd1564-1616\x1fxStage history.\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fxDrama.\x1e', '10\x1faShakespeare, William,\x1fd1564-1616\x1fxStage history.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        (('a', u'John'), ('c', u'King of England'), ('d', u'1167-1216')): 3,
        (('a', u'Shakespeare, William'), ('d', u'1564-1616')): 3
    }

def test_king_john():
    lines = [
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama\x1e', '10\x1faKean, Charles John,\x1fd1811?-1868\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167?-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama.\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fvDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167?-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167?-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167?-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167?-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167?-1216\x1fvDrama.\x1e', '00\x1faHenry\x1fbVIII,\x1fcKing of England,\x1fd1491-1547\x1fvDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama.\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['14\x1faShakespeare, William,\x1fd1564-1616.\x1ftKing John.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama.\x1e', '10\x1faShakespeare, William,\x1fd1564-1616.\x1ftKing John.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fvDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama\x1e', '00\x1faHenry\x1fbVIII,\x1fcKing of England,\x1fd1491-1547\x1fxDrama\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167?-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e', '10\x1faShakespeare, William,\x1fd1564-1616.\x1fxKing John\x1fxProblems, exercises, etc.\x1e', '01\x1faJohn,\x1fcKing of England,\x1fd1167-1216\x1fxDrama.\x1e'],
        ['00\x1faJohn, $ c King of England,\x1fd1167-1216\x1fxDrama\x1e'],
        ['00\x1faJohn, $ c King of England,\x1fd1167-1216\x1fxDrama\x1e'],
        ['00\x1faJohn\x1fbKing of England,\x1fd1167-1216\x1fxDrama.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        (('a', u'Shakespeare, William'), ('d', u'1564-1616')): 3,
        (('a', u'Kean, Charles John'), ('d', u'1811?-1868')): 1,
        (('a', u'John'), ('c', u'King of England'), ('d', u'1167?-1216')): 35,
        (('a', u'Henry'), ('b', u'VIII'),('c', u'King of England'), ('d', u'1491-1547')): 2
    }

def test_non_ascii():
    lines = [
        ['00\x1faA\xe2soka,\x1fcKing of Magadha,\x1fdfl. 259 B.C.\x1e'],
        ['00\x1faA{acute}soka,\x1fcKing of Magadha\x1fdfl. 259 B.C.\x1e'],
        ['00\x1faAsoka,\x1fcKing of Magadha,\x1fdfl. 259 B.C..\x1e', '30\x1faMaurya family.\x1e'],
        ['04\x1faAs\xcc\x81oka,\x1fcKing of Magadha,\x1fdca. 274-232 B.C.\x1e'],
        ['00\x1faA\xe2soka,\x1fcKing of Magadha,\x1fdfl. 259 B.C.\x1e', '30\x1faMaurya dynasty.\x1e'],
        ['04\x1faAsoka,\x1fcKing of Magadha,\x1fdca. 274-232 B.C.\x1e'],
        ['00\x1faA\xe2soka,\x1fcKing of Magadha,\x1fdfl. 259 B.C\x1e', '30\x1faMaurya dynasty\x1e'],
        ['00\x1faAs\xcc\x81oka,\x1fcKing of Magadha,\x1fdfl. 259 B.C.\x1e', '30\x1faMaurya family.\x1e']
    ]
    a, b = read_people(lines)
    print a

def test_q_should_be_c():
    lines = [
        ['10\x1faLafayette, Marie Joseph Paul Yves Roch Gilbert Du Motier,\x1fcmarquis de,\x1fd1757-1834\x1fxTravel\x1fzNew York (State)\x1fzNew York.\x1e'],
        ['10\x1faLafayette, Marie Joseph Paul Yves Roch Gilbert Du Motier,\x1fcmarquis de,\x1fd1757-1834\x1fxTravel\x1fzNew York (State)\x1fzNew York.\x1e'],
        ['10\x1faLafayette, Marie Joseph Paul Yves Roch Gilbert Du Motier,\x1fqmarquis de,\x1fd1757-1834.\x1e']
    ]
    a, b = read_people(lines)

def test_date_in_a():
    lines = [
        ['10\x1faMachiavelli, Niccol\xe1o,\x1fd1469-1527\x1fxFiction.\x1e', '10\x1faBorgia, Cesare,\x1fd1476?-1507\x1fxFiction.\x1e'],
        [' 0\x1faBorgia, Cesare, 1476?-1507\x1fxFiction.\x1e', ' 0\x1faMachiavelli, Niccolo, 1469-1527\x1fxFiction.\x1e'],
        ['10\x1faMachiavelli, Niccol\xe1o,\x1fd1469-1527\x1fxFiction.\x1e', '10\x1faBorgia, Cesare,\x1fd1476?-1507\x1fxFiction.\x1e'],
        ['10\x1faMachiavelli, Niccol\xe1o,\x1fd1469-1527\x1fxFiction.\x1e', '10\x1faBorgia, Cesare,\x1fd1476?-1507\x1fxFiction.\x1e'], ['10\x1faMachiavelli, Niccol\xe1o,\x1fd1469-1527\x1fxFiction.\x1e', '10\x1faBorgia, Cesare,\x1fd1476?-1507\x1fxFiction.\x1e'], ['10\x1faMachiavelli, Niccol\xe1o,\x1fd1469-1527\x1fxFiction\x1e', '10\x1faBorgia, Cesare,\x1fd1476?-1507\x1fxFiction\x1e'],
        ['10\x1faMachiavelli, Niccol\xe1o,\x1fd1469-1527\x1fxFiction.\x1e', '10\x1faBorgia, Cesare,\x1fd1476?-1507\x1fxFiction.\x1e']
    ]
    a, b = read_people(lines)
    print a
    assert a == {(('a', u'Borgia, Cesare'), ('d', u'1476?-1507')): 7, (('a', u'Machiavelli, Niccol\xf2'), ('d', u'1469-1527')): 7}

def test_king_asoka():
    return
    lines = [
        ['00\x1faA\xe2soka,\x1fcKing of Magadha,\x1fdfl. 259 B.C.\x1e'],
        ['00\x1faA{acute}soka,\x1fcKing of Magadha\x1fdfl. 259 B.C.\x1e'],
        ['00\x1faAsoka,\x1fcKing of Magadha,\x1fdfl. 259 B.C..\x1e', '30\x1faMaurya family.\x1e'],
        ['04\x1faAs\xcc\x81oka,\x1fcKing of Magadha,\x1fdca. 274-232 B.C.\x1e'],
        ['00\x1faA\xe2soka,\x1fcKing of Magadha,\x1fdfl. 259 B.C.\x1e', '30\x1faMaurya dynasty.\x1e'],
        ['04\x1faAsoka,\x1fcKing of Magadha,\x1fdca. 274-232 B.C.\x1e'],
        ['00\x1faA\xe2soka,\x1fcKing of Magadha,\x1fdfl. 259 B.C\x1e', '30\x1faMaurya dynasty\x1e'],
        ['00\x1faAs\xcc\x81oka,\x1fcKing of Magadha,\x1fdfl. 259 B.C.\x1e', '30\x1faMaurya family.\x1e']
    ]
    a, b = read_people(lines)
    print a
    # (('a', u'Asoka'), ('c', u'King of Magadha'), ('d', u'fl. 259 B.C..')): 1
    assert a == {
        (('a', u'A\u015boka'), ('c', u'King of Magadha'), ('d', u'fl. 259 B.C.')): 7,
        (('a', u'Maurya dynasty'),): 2,
        (('a', u'Maurya family'),): 2,
        (('a', u'Asoka'), ('c', u'King of Magadha'), ('d', u'ca. 274-232 B.C.')): 1
    }

def test_name_lookup():
    lines = [
        ['10\x1faBellini, Giovanni,\x1fd1516.\x1e'],
        ['10\x1faBellini, Giovanni,\x1fdd. 1516\x1e']
    ]
    a, b = read_people(lines)
    assert a == {(('a', 'Bellini, Giovanni'), ('d', 'd. 1516')): 2}
    assert b == {((u'a', u'Bellini, Giovanni'), (u'd', u'1516')): ((u'a', u'Bellini, Giovanni'), (u'd', u'd. 1516'))}

def test_cleopatra():
    return
    lines = [
        ['00\x1faCleopatra,\x1fcQueen of Egypt,\x1fdd. 30 B.C\x1fxFiction.\x1e'],
        ['00\x1faCleopatra,\x1fcQueen of Egypt,\x1fdd. 30 B.C.\x1fxFiction\x1e'],
        [' 0\x1faCleopatra, Queen of Egypt, d. 30 B.C.\x1fxFiction.\x1e'],
        ['00\x1faCleopatra,\x1fcQueen of Egypt,\x1fdd. 30 B.C.\x1fxFiction\x1e'],
        ['00\x1faCleopatra,\x1fcqueen of Egypt,\x1fdd. B.C. 30\x1fxFiction\x1e'],
        ['00\x1faCleopatra,\x1fcQueen of Egypt,\x1fdd. 30 B.C.\x1fxFiction\x1e'],
        ['00\x1faCleopatra,\x1fcQueen of Egypt,\x1fdd. 30 B.C.\x1fvFiction.\x1e'],
        ['00\x1faCleopatra,\x1fcQueen of Egypt,\x1fdd. 30 B.C.\x1fxFiction.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        (('a', u'Cleopatra'), ('c', u'Queen of Egypt'), ('d', u'd. 30 B.C.')): 8,
    }

def test_date_field_missing():
    lines = [[' 0\x1faMoore, Henry Spencer, 1898-\x1e']]
    a, b = read_people(lines)
    assert a == {
        (('a', u'Moore, Henry Spencer'), ('d', u'1898-')): 1
    }
    assert b == {}

def test_numbers_in_name():
    lines = [
        [' 0\x1faFielding, Henry, 1707-1754.  The history of the adventures of Joseph Andrews.\x1e'],
        ['14\x1faFielding, Henry,\x1fd1707-1754.\x1ftJoseph Andrews.\x1e'],
        ['10\x1faFielding, Henry,\x1fd1707-1754.\x1ftHistory of the adventures of Joseph Andrews.\x1e'],
        ['14\x1faFielding, Henry,\x1fd1707-1754.\x1ftJoseph Andrews.\x1e'],
        ['10\x1faFielding, Henry,\x1fd1707-1754.\x1ftHistory of the adventures of Joseph Andrews.\x1e'],
        ['10\x1faFielding, Henry,\x1fd1707-1754.\x1ftHistory of the adventures of Joseph Andrews.\x1e'],
        ['10\x1faFielding, Henry,\x1fd1707-1754.\x1ftHistory of the adventures of Joseph Andrews\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        (('a', u'Fielding, Henry'), ('d', u'1707-1754')): 7
    }

def test_caesar():
    lines = [
        ['10\x1faCaesar, Julius.\x1e'],
        ['14\x1faCaesar, Julius,\x1fd100 B.C.-44B.C.\x1e'],
        ['14\x1faCaesar, Julius,\x1fd100 B.C.-44 B.C.\x1e'],
        ['10\x1faCaesar, Julius\x1e'],
        ['14\x1faCaesar, Julius,\x1fd100 B.C.-44 B.C.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {(('a', 'Caesar, Julius'), ('d', '100 B.C.-44 B.C.')): 5}

def test_salvador_dali():
    lines = [
        ['14\x1faDali\xcc\x81, Salvador,\x1fd1904-1989\x1fvCatalogs.\x1e'],
        ['10\x1faDali, Salvador,\x1fd1904-\x1e'],
        ['10\x1faDal\xe2i, Salvador,\x1fd1904-\x1e'],
        ['10\x1faDal\xe2i, Salvador,\x1fd1904-\x1e'],
        ['10\x1faDal\xe2i, Salvador,\x1fd1904-\x1fxCatalogs.\x1e', '10\x1faMorse, Albert Reynolds,\x1fd1914-\x1fxArt collections\x1fxCatalogs.\x1e'],
        ['10\x1faDal\xe2i, Salvador\x1fy1904-\x1e'],
        ['14\x1faDali\xcc\x81, Salvador,\x1fd1904- \x1fvexhibitions.\x1e'],
        ['14\x1faDali\xcc\x81, Salvador,\x1fd1904- \x1fvexhibitions.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        ((u'a', u'Dal\xed, Salvador'), (u'd', u'1904-1989')): 8,
        ((u'a', u'Morse, Albert Reynolds'), (u'd', u'1914-')): 1
    }

def test_date_in_y():
    lines = [
        ['10\x1faShakespeare, William,\x1fd1564-1616\x1fxStage history\x1fy1800-1950.\x1e'],
        ['10\x1faShakespeare, William,\x1fd1564-1616\x1fxStage history\x1fy1800-\x1e'],
        ['10\x1faShakespeare, William,\x1fd1564-1616.\x1e'],
        ['10\x1faShakespeare, William,\x1fd1564-1616\x1fxDramatic production\x1e']
    ]
    a, b = read_people(lines)
    assert a == {((u'a', u'Shakespeare, William'), (u'd', u'1564-1616')): 4}

def test_subtags_swapped():
    lines = [
        ['20\x1faCompton-Burnett, I.\x1fq(Ivy),\x1fd1884-1969.\x1e'],
        ['10\x1faCompton-Burnett, I.\x1fd(Ivy),\x1fq1884-1969\x1fxCriticism and interpretation.\x1e'],
        ['20\x1faCompton-Burnett, I.\x1fq(Ivy),\x1fd1884-1969.\x1e'],
        ['14\x1faCompton-Burnett, I.\x1fq(Ivy),\x1fd1884-1969.\x1e'],
        ['20\x1faCompton-Burnett, I.\x1fq(Ivy),\x1fd1884-1969\x1fxCriticism and interpretation.\x1e'],
        ['20\x1faCompton-Burnett, I.\x1fq(Ivy),\x1fd1884-1969\x1fxCriticism and interpretation.\x1e'],
        ['20\x1faCompton-Burnett, I.\x1fq(Ivy),\x1fd1884-1969\x1fxCriticism and interpretation.\x1e'],
        ['14\x1faCompton-Burnett, I.\x1fq(Ivy),\x1fd1884-1969.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {((u'a', u'Compton-Burnett, I.'), (u'q', u'(Ivy),'), (u'd', u'1884-1969')): 8}

def test():
    lines = [
        [
            '10\x1faWashington, George,\x1fd1732-1799\x1fxFamily\x1fvJuvenile fiction.\x1e',
            '10\x1faJudge, Oney\x1fvJuvenile fiction.\x1e',
            '11\x1faWashington, George,\x1fd1732-1799\x1fxFamily\x1fxFiction.\x1e',
            '11\x1faJudge, Oney\x1fvFiction.\x1e'
        ], [
            '10\x1faJudge, Oney\x1fvJuvenile fiction.\x1e',
            '10\x1faWashington, George,\x1fd1732-1799\x1fxFamily\x1fvJuvenile fiction.\x1e',
            '11\x1faJudge, Oney\x1fvFiction.\x1e',
            '11\x1faWashington, George,\x1fc1732-1799\x1fxFamily\x1fvFiction.\x1e'
        ]
    ]

    a, b = read_people(lines)
    assert a == {((u'a', u'Judge, Oney'),): 4, ((u'a', u'Washington, George'), (u'd', u'1732-1799')): 4}

#    lines = [
#        [' 0\x1faHadrian, Emperor of Rome, 76-138\x1fxFiction.\x1e'],
#        ['00\x1faHadrianus,\x1fcEmperor of Rome,\x1fd76-138\x1fxFiction.\x1e']
#    ]
#    lines = [[' 0\x1faGreene, Graham, 1904- .  The basement room. 1971.\x1e']]

#    lines = [
#        [' 0\x1faGyllembourg-Ehrensvard, Thomasine Christine Buntzen, 1773-1856. To tidsaldre.\x1e'],
#        ['10\x1faGyllembourg, Thomasine,\x1fd1773-1856.\x1ftTo tidsaldre.\x1e'],
#        ['14\x1faGyllembourg-Ehrensva\xcc\x88rd, Thomasine.\x1ftTo tidsaldre.\x1e']
#    ]

#    lines = [['10\x1faClifford, Henry de Clifford, 14th lord,\x1fd1455?-1523\x1fxFiction.\x1e',]]

def test_same_name_different_dates():
    lines = [
        ['10\x1faStrauss, Johann,\x1fd1825-1899.\x1e', '10\x1faStrauss, Johann,\x1fd1804-1849.\x1e'], 
        ['10\x1faStrauss, Johann,\x1fd1825-1899.\x1e', '10\x1faStrauss, Johann,\x1fd1804-1849.\x1e'], 
        ['10\x1faStrauss, Johann,\x1fd1825-1899.\x1e', '10\x1faStrauss, Johann,\x1fd1804-1849.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        ((u'a', u'Strauss, Johann'), (u'd', u'1804-1849')): 3,
        ((u'a', u'Strauss, Johann'), (u'd', u'1825-1899')): 3
    }

def test_king_richard_iii():
    lines = [
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fvDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1425-1485\x1fvDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fvDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fvDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fvDrama.\x1e'],
        ['10\x1faShakespeare, William,\x1fd1564-1616.\x1ftKing Richard III.\x1e', '00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fvDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fxDrama\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fvDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fxDrama\x1e'],
        ['04\x1faRichard\x1fbIII,\x1fcKing of England\x1fxDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fxDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fxDrama\x1e'],
        ['10\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fxDrama.\x1e'],
        ['00\x1faRichard\x1fbIII,\x1fcKing of England,\x1fd1452-1485\x1fvDrama.\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        ((u'a', u'Richard'), (u'b', u'III'), (u'c', u'King of England'), (u'd', u'1452-1485')): 14,
        ((u'a', u'Shakespeare, William'), (u'd', u'1564-1616')): 1
    }

def test_churchill_family():
    lines = [
        ['30\x1faChurchill family\x1e'],
        ['30\x1faChurchill family.\x1e'],
        ['34\x1faChurchill,\x1fcfamily.\x1e'],
        ['30\x1faChurchill family\x1e'],
        ['34\x1faChurchill,\x1fcFamily.\x1e']
    ]
    a, b = read_people(lines)
    assert a == { (('a', 'Churchill family'),): 5}

def test_william_thompson():
    lines = [
        ['10\x1faHodgskin, Thomas,\x1fd1787-1869.\x1e', '10\x1faThompson, William,\x1fd1785?-1833.\x1e'],
        ['10\x1faHodgskin, Thomas,\x1fd1787-1869.\x1e', '10\x1faThompson, William,\x1fd1775-1833.\x1e'],
        ['10\x1faHodgskin, Thomas,\x1fd1787-1869.\x1e', '10\x1faThompson, William,\x1fd1775-1833.\x1e']
    ]
    a, b = read_people(lines)
    assert a == { # better if we could merge the William Thompson subjects
        ((u'a', u'Hodgskin, Thomas'), (u'd', u'1787-1869')): 3,
        ((u'a', u'Thompson, William'), (u'd', u'1775-1833')): 2,
        ((u'a', u'Thompson, William'), (u'd', u'1785?-1833')): 1
    }

def test_marcus_porcius():
    lines = [
        ['10\x1faCato, Marcus Porcius,\x1fd95-46 B.C.\x1fxDrama.\x1e'],
        ['10\x1faCato, Marcus Porcius,\x1fd95-46 B.C.\x1fxDrama\x1e'],
        ['10\x1faCato, Marcus Porcius,\x1fd95-46 B.C.\x1fxDrama.\x1e'],
        ['10\x1faCato, Marcus Porcius,\x1fd95-46 B.C.\x1fxDrama.\x1e'],
        ['10\x1faCato, Marcus Porcius,\x1fd95-46 B.C.\x1fvDrama.\x1e'],
        ['10\x1faCato, Marcus Porcius,\x1fd234-149 B.C.\x1fvDrama.\x1e'],
        ['10\x1faCato, Marcus Porcius,\x1fd95-46 B.C.\x1fxDrama\x1fxEarly works to 1800\x1e']
    ]
    a, b = read_people(lines)
    assert a == {
        ((u'a', u'Cato, Marcus Porcius'), (u'd', u'234-149 B.C.')): 1,
        ((u'a', u'Cato, Marcus Porcius'), (u'd', u'95-46 B.C.')): 6
    }

########NEW FILE########
__FILENAME__ = read_rc
import os.path

# ~/.olrc looks like this:
# 
# db=''
# user=''
# pw= ''
# host = ''
# secret_key = ''
 
def read_rc():
    rc_file = os.path.expanduser('~/.olrc')
    if not os.path.exists(rc_file):
        return {}
    f = open(rc_file)
    return eval('dict(' + ', '.join(i for i in f if i) + ')')

########NEW FILE########
__FILENAME__ = add_source_records
import os, re, sys, codecs
from openlibrary.catalog.read_rc import read_rc
from openlibrary.catalog.importer.db_read import get_mc

sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, unmarshal, marshal

rc = read_rc()
ol = OpenLibrary("http://dev.openlibrary.org")
ol.login('EdwardBot', rc['EdwardBot']) 

test_dir = '/home/edward/ol/test_data'

re_edition = re.compile('^/b/OL\d+M$')

re_meta_mrc = re.compile('^([^/]*)_meta.mrc:0:\d+$')

#out = open('source_records', 'w')
for f in os.listdir(test_dir):
    key = f.replace('_', '/')
    if not re_edition.match(key):
        continue
    print key
    continue
    mc = get_mc(key)
    print key, mc
    if not mc:
        continue
    e = ol.get(key)
    if e.get('source_records', []):
        continue
    if mc.startswith('ia:') or mc.startswith('amazon:'):
        sr = mc
    else:
        m = re_meta_mrc.match(mc)
        sr = 'marc:' + mc if not m else 'ia:' + m.group(1)
    e['source_records'] = [sr]
    print >> out, (key, sr)
    print ol.save(key, e, 'add source record')
#out.close()

########NEW FILE########
__FILENAME__ = count_41
import web, os.path
from catalog.get_ia import read_marc_file
from catalog.read_rc import read_rc
from catalog.marc.fast_parse import get_first_tag, get_all_subfields
from catalog.utils.query import query_iter

marc_index = web.database(dbn='postgres', db='marc_index')
marc_index.printing = False

rc = read_rc()

def get_keys(loc):
    assert loc.startswith('marc:')
    vars = {'loc': loc[5:]}
    db_iter = marc_index.query('select k from machine_comment where v=$loc', vars)
    mc = list(db_iter)
    if mc:
        return [r.k for r in mc]
    iter = query_iter({'type': '/type/edition', 'source_records': loc})
    return [e['key'] for e in iter]

def files():
    endings = ['.mrc', '.marc', '.out', '.dat', '.records.utf8']
    def good(filename):
        return any(filename.endswith(e) for e in endings)

    dir = rc['marc_path']
    dir_len = len(dir) + 1
    for dirpath, dirnames, filenames in os.walk(dir):
        for f in sorted(f for f in filenames if good(f)):
            name = dirpath + "/" + f
            yield name, name[dir_len:], os.path.getsize(name)

def percent(a, b):
    return "%.2f%%" % (float(a * 100.0) / b)

chunk = 10000

books = 0
has_041 = 0
has_a = 0
has_h = 0
has_2 = 0
i2 = 0
i1_0 = 0
i1_1 = 0
for name, part, size in files():
    f = open(name)
    print part
    for pos, loc, data in read_marc_file(part, f):
        if str(data)[6:8] != 'am': # only want books
            continue
        books += 1
        line = get_first_tag(data, set(['041']))
        if not line:
            continue
        has_041 += 1
        if line[0] == '0':
            i1_0 += 1
        if line[0] == '1':
            i1_1 += 1
        subfields = list(get_all_subfields(line))
        print loc
        keys = get_keys(loc)
        print keys, line[0:2], subfields
        continue
        if line[1] != ' ':
            i2 += 1
            print 'i2:', line[0:2], subfields
        if '\x1fa' in line:
            has_a +=1
        else:
            print 'no a:', line[0:2], subfields
        if '\x1fh' in line:
            has_h +=1
        if '\x1f2' in line:
            has_2 +=1
            print 'has 2:', line[0:2], subfields
        if has_041 % chunk == 0:
            print books, percent(has_041, books), percent(i1_0, has_041), \
                percent(i1_1, has_041), i2, percent(has_a, has_041), \
                percent(has_h, has_041), has_2
#        print total, line[0:2], list(get_all_subfields(line))

########NEW FILE########
__FILENAME__ = get_651
from catalog.importer.db_read import get_mc, withKey
from catalog.get_ia import get_from_local
from catalog.marc.fast_parse import get_tag_lines, get_all_subfields
import sys, web
import simplejson as json

def get_src(key):
    e = withKey(key)
    if 'source_records' in e:
        return e['source_records']
    src = get_mc(key)
    if src:
        return [src]

def get_651(key):
    found = []
    for src in get_src(key):
        data = get_from_local(src)
        for tag, line in get_tag_lines(data, ['651']):
            found.append(list(get_all_subfields(line)))
    return found

urls = (
    '^(/b/OL\d+M)$', 'lookup'
)
app = web.application(urls, globals())

class lookup:        
    def GET(self, key):
        return json.dumps(get_651(key))

if __name__ == "__main__":
    app.run()

########NEW FILE########
__FILENAME__ = remove_subject_period
from catalog.utils.query import query_iter, set_staging, withKey
import sys, codecs, re
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, Reference
from catalog.read_rc import read_rc
rc = read_rc()

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
set_staging(True)

ol = OpenLibrary("http://dev.openlibrary.org")
ol.login('EdwardBot', rc['EdwardBot'])

re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon)\.$')

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

q = { 'type': '/type/edition', 'table_of_contents': None, 'subjects': None }
queue = []
count = 0
for e in query_iter(q):
    if not e.get('subjects', None) or not any(has_dot(s) for s in e['subjects']):
        continue
    subjects = [s[:-1] if has_dot(s) else s for s in e['subjects']]
    q = {
        'key': e['key'],
        'subjects': {'connect': 'update_list', 'value': subjects },
    }
    # need to fix table_of_contents to pass validation
    toc = e['table_of_contents']
    if toc and (isinstance(toc[0], basestring) or toc[0]['type'] == '/type/text'):
        if isinstance(toc[0], basestring):
            assert all(isinstance(i, basestring) for i in toc)
            new_toc = [{'title': i, 'type': '/type/toc_item'} for i in toc]
        else:
            assert all(i['type'] == '/type/text' for i in toc)
            new_toc = [{'title': i['value'], 'type': '/type/toc_item'} for i in toc]
        q['table_of_contents'] = {'connect': 'update_list', 'value': new_toc }
    queue.append(q)
    count += 1
    if len(queue) == 100:
        print count, 'writing to db'
        print ol.write(queue, "remove trailing period from subjects")
        queue = []

print ol.write(queue, "remove trailing period from subjects")

########NEW FILE########
__FILENAME__ = work_author_role
import sys, codecs
from openlibrary.catalog.utils.query import query_iter, set_staging, query
from openlibrary.api import OpenLibrary, Reference
from openlibrary.catalog.read_rc import read_rc
from time import sleep

set_staging(True)
rc = read_rc()

ol = OpenLibrary("http://dev.openlibrary.org")
ol.login('EdwardBot', rc['EdwardBot'])
sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

work_q = {
    'type': '/type/work',
    'authors': None,
    'title': None,
}

queue = []

for w in query_iter(work_q):
    if not w.get('authors'):
        print 'no authors'
        continue
    if any(isinstance(a, dict) and 'author' in a for a in w['authors']):
        continue
    print len(queue), w['key'], w['title'] # , ol.get(w['authors'][0]['key'])['name']
    full = ol.get(w['key'])
    authors = full['authors']
    assert all(isinstance(a, Reference) for a in authors)
    full['authors'] = [{'author':a} for a in authors]
    queue.append(full)
    if len(queue) > 1000:
        print 'saving'
        print ol.save_many(queue, 'update format of authors in works to provide roles')
        queue = []
        print 'two second pause'
        sleep(2)
    continue
    work_e = {
        'type': '/type/edition',
        'works': w['key'],
        'by_statement': None,
    }
    for e in query_iter(work_e):
        by = e['by_statement']
        if by:
            print '  ', e['key'], by

print 'saving'
print ol.save_many(queue, 'update format of authors in works to provide roles')


########NEW FILE########
__FILENAME__ = solr
#!/usr/bin/python

from time import sleep, time
import urllib, web, subprocess, sys
from pprint import pprint
from catalog.read_rc import read_rc

rc = read_rc()

def solr_query(q, start=0, rows=None, sort_by="publicdate desc"):
    q += " AND NOT collection:test_collection AND NOT collection:opensource AND NOT collection:microfilm"
#    q += " AND NOT collection:test_collection AND collection:gutenberg"
    url = rc['solr_url'] + "?q=%s;%s&wt=json&start=%d" % (urllib.quote(q), urllib.quote_plus(sort_by), start)
    if rows:
        url += "&rows=%d" % rows
    ret = eval(urllib.urlopen(url).read())
    return ret['response']

def get_books(**args):
    ret = solr_query("mediatype:texts AND format:scandata", **args)
    #ret = solr_query("mediatype:texts", **args)
    return [d['identifier'] for d in ret['docs']]

if __name__ == '__main__':
    rows = 1000
    out = open(sys.argv[1], 'w')
    for i in range(20):
        print i
        books = list(get_books(rows=rows, start=i * rows))
        if not books:
            break
        for b in books:
            print >> out, b
    out.close()

    print "finished"

########NEW FILE########
__FILENAME__ = isbn_and_author_date
# read Talis, find books with ISBN and author date, add date to author

from catalog.read_rc import read_rc
from catalog.marc.fast_parse import *
from catalog.infostore import get_site
from catalog.merge.names import match_name
from catalog.marc.build_record import read_author_person

import re

site = get_site()

re_author_date_subfield = re.compile('\x1f[az]')
re_isbn_subfield = re.compile('\x1f[az]')

rc = read_rc()
filename = rc['marc_path'] + 'talis_openlibrary_contribution/talis-openlibrary-contribution.mrc'

seen = set()

def build_fields(data):
    fields = {}
    for tag, line in get_tag_lines(data, ['020', '100']):
        if tag in fields:
            return {}
        fields[tag] = line
    if '020' not in fields or '100' not in fields:
        return {}
    if fields['100'].find('\x1fd') == -1:
        return {}
    if not re_isbn_subfield.search(fields['020']):
        return {}
    return fields

def find_authors(isbn_list, name):
    edition_keys = []
    for isbn in isbn_list:
        edition_keys.extend(site.things({'type': '/type/edition', 'isbn_10': isbn}))
    authors = set()
    for k in edition_keys:
        t = site.withKey(k)
        if t.authors:
            authors.update(t.authors)
    for a in authors:
        if not match_name(a.name, name, last_name_only_ok=False):
            continue
        books = site.things({'type': '/type/edition', 'authors': a.key})
        print `a.key, a.name, a.birth_date, a.death_date, len(books)`
    print

for data, length in read_file(open(filename)):
    fields = build_fields(data)
    if not fields:
        continue
    isbn_list = read_isbn(fields['020'])
    if not isbn_list:
        continue

    if any(isbn in seen for isbn in isbn_list):
        continue
    seen.update(isbn_list)
    person = read_author_person(fields['100'])
    print list(get_all_subfields(fields['100']))
    print person
    print isbn_list
    find_authors(isbn_list, person['personal_name'])
#        fields.append(tag, list(get_all_subfields(line)))

########NEW FILE########
__FILENAME__ = load
from openlibrary.catalog.read_rc import read_rc
import urllib, httplib, json

rc = read_rc()

def add_cover_image(ekey, ia):
    h1 = httplib.HTTPConnection('openlibrary.org')
    body = json.dumps(dict(username='ImportBot', password=rc['ImportBot']))
    headers = {'Content-Type': 'application/json'}  
    h1.request('POST', 'http://openlibrary.org/account/login', body, headers)

    res = h1.getresponse()

    res.read()
    assert res.status == 200
    cookies = res.getheader('set-cookie').split(',')
    cookie =  ';'.join([c.split(';')[0] for c in cookies])

    cover_url = 'http://www.archive.org/download/' + ia + '/page/' + ia + '_preview.jpg'
    body = urllib.urlencode({"url": cover_url})
    assert ekey.startswith('/books/')
    add_cover_url = 'http://openlibrary.org' + ekey + '/add-cover.json'
    headers = {'Content-type': 'application/x-www-form-urlencoded', 'Cookie': cookie}
    h1.request('POST', add_cover_url, body, headers)
    res = h1.getresponse()
    res.read()
    return

########NEW FILE########
__FILENAME__ = replace_cover_with_title
from openlibrary.utils.ia import find_item
from openlibrary.catalog.read_rc import read_rc
from openlibrary.catalog.utils.query import query, withKey, has_cover
from subprocess import Popen, PIPE
import web, re, urllib, sys
import xml.etree.ElementTree as et
import xml.parsers.expat, socket # for exceptions
import httplib
from time import sleep

re_single_cover = re.compile('^\[(\d+)\]$')
re_remove_xmlns = re.compile(' xmlns="[^"]+"')

fh_log = open('/1/edward/logs/covers2', 'a')

def write_log(ol, ia, url):
    print >> fh_log, (ol, ia, url)
    fh_log.flush()

def parse_scandata_xml(f):
    xml = f.read()
    xml = re_remove_xmlns.sub('', xml)
    #tree = et.parse(f)
    tree = et.fromstring(xml)
    leaf = None
    leafNum = None
    cover = None
    title = None
    for e in tree.find('pageData'):
        assert e.tag == 'page'
        leaf = int(e.attrib['leafNum'])
        if leaf > 25: # enough
            break
        page_type = e.findtext('pageType')
        if page_type == 'Cover':
            cover = leaf
        elif page_type == 'Title Page' or page_type == 'Title':
            title = leaf
            break
    return (cover, title)

def find_title_leaf_et(ia_host, ia_path, url):
    f = urllib.urlopen(url)
    try:
        return parse_scandata_xml(f)
    except xml.parsers.expat.ExpatError:
        print url
        return (None, None)

def jp2_zip_test(ia_host, ia_path, ia):
    conn = httplib.HTTPConnection(ia_host)
    conn.request('HEAD', ia_path + "/" + ia + "_jp2.zip")
    r1 = conn.getresponse()
    try:
        assert r1.status in (200, 403, 404)
    except AssertionError:
        print r1.status, r1.reason
        raise
    return r1.status

def scandata_url(ia_host, ia_path, item_id):
    conn = httplib.HTTPConnection(ia_host)
    conn.request('HEAD', ia_path + "/scandata.zip")
    r = conn.getresponse()
    try:
        assert r.status in (200, 403, 404)
    except AssertionError:
        print r.status, r.reason
        raise
    if r.status == 200:
        None
    conn = httplib.HTTPConnection(ia_host)
    path = ia_path + "/" + item_id + "_scandata.xml"
    conn.request('HEAD', path)
    r = conn.getresponse()
    try:
        assert r.status in (200, 403, 404)
    except AssertionError:
        print ia_host, path
        print r.status, r.reason
        raise
    return 'http://' + ia_host + path if r.status == 200 else None

def scandata_zip_test(ia_host, ia_path):
    conn = httplib.HTTPConnection(ia_host)
    conn.request('HEAD', ia_path + "/scandata.zip")
    r1 = conn.getresponse()
    try:
        assert r1.status in (200, 403, 404)
    except AssertionError:
        print r1.status, r1.reason
        raise
    return r1.status



def urlread(url):
    return urllib.urlopen(url).read()

def post_cover(ol, source_url):
    param = urllib.urlencode({'olid': ol[3:], 'source_url': source_url})
    headers = {"Content-type": "application/x-www-form-urlencoded"}
    conn = httplib.HTTPConnection("covers.openlibrary.org", timeout=20)
    conn.request("POST", "/b/upload", param, headers)
    r1 = conn.getresponse()
    print r1.status, r1.reason
    if r1.status not in (200, 303, 500):
        open('upload.html', 'w').write(r1.read())
        print r1.getheaders()
        print r1.msg
        sys.exit()
    conn.close()

def post(ol, ia, ia_host, ia_path, cover, title):
    use_cover = False
    if title is None:
        if cover is None:
            return
        use_cover = True
#    http://covers.openlibrary.org/b/query?olid=OL7232120M
    if False and not use_cover:
        data = urlread('http://openlibrary.org/query.json?key=/b/OL7232119M&publish_date=')
        try:
            ret = eval(data)
        except:
            print `data`
        pub_date = ret[0]['publish_date']
        use_cover = pub_date.isdigit() and int(pub_date) > 1955
    leaf = cover if use_cover else title
    source_url = "http://%s/GnuBook/GnuBookImages.php?zip=%s/%s_jp2.zip&file=%s_jp2/%s_%04d.jp2" % (ia_host, ia_path, ia, ia, ia, leaf)
#    print leaf, source_url
    query = 'http://covers.openlibrary.org/b/query?olid=' + ol[3:]
    #print query
    if use_cover:
        print 'use_cover',
    print 'http://openlibrary.org' + ol
    for attempt in range(5):
        if attempt > 0:
            print 'trying again (%d)' % attempt
        try:
            ret = urlread(query).strip()
        except IOError:
            continue
        print ret
        if not re_single_cover.match(ret):
            print "unexpected reply: '%s'" % ret
            break
        try:
            write_log(ol, ia, source_url)
            post_cover(ol, source_url)
        except socket.timeout:
            print 'socket timeout'
            break
        except httplib.BadStatusLine:
            print 'bad status line'
            continue
        break

bad_hosts = set()

def find_img(item_id):
    e = query({'type':'/type/edition', 'source_records':'ia:' + item_id})
    if len(e) != 1:
        print 'no source_records:', e
        e = query({'type':'/type/edition', 'ocaid': item_id})
        if len(e) != 1:
            print 'no ocaid:', e
            return
    ol = e[0]['key']
    (ia_host, ia_path) = find_item(item_id)

    if not ia_host:
        print 'no host', item_id, ia_host
        return
    if ia_host in bad_hosts:
        print 'bad_host'
    try:
        url = scandata_url(ia_host, ia_path, item_id)
        if not url:
            return
    except socket.error:
        print 'socket error:', ia_host
        bad_hosts.add(ia_host)
        return

    try:
        status = jp2_zip_test(ia_host, ia_path, item_id)
    except socket.error:
        print 'socket error:', ia_host
        bad_hosts.add(ia_host)
        return
    if status in (403, 404):
        print 'jp2 not found:', (ol, item_id)
        return

    try:
        (cover, title) = find_title_leaf_et(ia_host, ia_path, url)
    except (KeyboardInterrupt, SystemExit, NameError):
        raise
    if not cover or not title:
        return
#    except:
#        print 'skip error:', ol, item_id, ia_host, ia_path
#        return
    print (ol, item_id, ia_host, ia_path, cover, title)
    post(ol, item_id, ia_host, ia_path, cover, title)

def has_cover_retry(key):
    for attempt in range(5):
        try:
            return has_cover(key)
        except KeyboardInterrupt:
            raise
        except:
            pass
        sleep(2)

skip = True
skip = False
for line in open('/1/edward/jsondump/2009-07-29/has_ocaid'):
    key = line[:-1]
    if key == '/b/OL6539962M': # the end
        break
    if skip:
        if key == '/b/OL6539962M':
            skip = False
        else:
            continue
    if not has_cover_retry(key):
        print 'no cover'
        continue
    print key
    e = withKey(key)
    if not e.get('ocaid', None):
        print 'no ocaid'
        continue
    find_img(e['ocaid'].strip())

fh_log.close()

print 'finished'

########NEW FILE########
__FILENAME__ = parse
import re, sys
import xml.etree.ElementTree as et
from pprint import pprint

def parse_catrecord(catrecord):
    record = {}
    re_bad_tag = re.compile(r'(<[^>]*?\s[^>]*?>)')
    re_white = re.compile(r'\s')
    catrecord = re_bad_tag.sub(lambda m: re_white.sub('', m.group(1)), catrecord)
    tree = et.fromstring(catrecord)
    record = {}
    for e in tree:
        f = e.tag.lower()
        if e.tag == 'AUTHORS':
            assert f not in record
            record[f] = [(a.tag.lower(), a.text) for a in e]
            continue
        if e.tag == 'SEGMENT':
            d = dict([(a.tag.lower(), a.text) for a in e])
            record.setdefault(f, []).append(d)
            continue
        elif e.tag in ('SUBJ', 'COLL', 'ALTTI', 'SERIES'):
            record.setdefault(f, []).append(e.text)
            continue
        assert len(e) == 0
        assert f not in record
        record[f] = e.text
    return record

def parse_file():
    cur = ''
    expect = 'start'
    i = 0
    re_call = re.compile('^<CALL>(.*)</CALL>\r\n$')
    re_itemid = re.compile('^<ITEMID>(.*)</ITEMID>\r\n$')
    for line in open(sys.argv[1]):
        i+=1
        assert expect != 'end_file'
        if expect == 'start':
            assert line == '<LIBRARY>Department of Treasury\r\n'
            expect = 'start_catrecord'
            continue
        if expect == 'start_catrecord':
            if line == '</CATRECORD>\r\n':
                print "skipping duplicate CATRECORD"
                continue
            assert line == '<CATRECORD>\r\n'
            cur += line
            expect = 'end_catrecord'
            continue
        if expect == 'end_catrecord': 
            if line.startswith('</CATRECORD>'):
                cur += '</CATRECORD>'
                yield parse_catrecord(cur)

                cur = ''
                if line == '</CATRECORD></LIBRARY>\r\n':
                    expect = 'end_file'
                else:
                    assert line == '</CATRECORD>\r\n'
                    expect = 'start_catrecord'
                continue
            else:
                cur += line

    assert expect == 'end_file'

for rec in parse_file():
    pprint(rec)

########NEW FILE########
__FILENAME__ = update_count
from olwrite import Infogami, add_to_database
import web, dbhash
from read_rc import read_rc
import cjson, re, sys
from time import time

def commify(n):
    """
Add commas to an integer `n`.
 
>>> commify(1)
'1'
>>> commify(123)
'123'
>>> commify(1234)
'1,234'
>>> commify(1234567890)
'1,234,567,890'
>>> commify(None)
>>>
"""
    if n is None: return None
    r = []
    for i, c in enumerate(reversed(str(n))):
        if i and (not (i % 3)):
            r.insert(0, ',')
        r.insert(0, c)
    return ''.join(r)

def count_books():
    rows = list(web.query("select count(*) as num from thing where type=52"))
    return rows[0].num

def count_fulltext():
    rows = list(web.query("select count(*) as num from edition_str where key_id=40"))
    return commify(rows[0].num)

def get_macro():
    rows = list(web.query("select data from data, thing where thing_id=thing.id and key='/macros/BookCount' and revision=latest_revision"))
    return cjson.decode(rows[0].data)['macro']['value']

rc = read_rc()
web.config.db_parameters = dict(dbn='postgres', db=rc['db'], user=rc['user'], pw=rc['pw'], host=rc['host'])
web.config.db_printing = False
web.ctx.ip = '127.0.0.1'
web.load()

book_count = count_books()
open('/home/edward/book_count', 'a').write("%d %d\n" % (time(), book_count))

infogami = Infogami(rc['infogami'])
infogami.login('edward', rc['edward'])

macro = get_macro()
re_books = re.compile(r'books = "<strong>[\d,]+</strong>"')
books = commify(book_count)
macro = re_books.sub('books = "<strong>' + books + '</strong>"', macro)

# full text count is disabled so that the number stays about 1 million
# fulltext = count_fulltext()
# re_fulltext = re.compile(r'fulltext = "<strong>[\d,]+</strong>"')
# macro = re_fulltext.sub('fulltext = "<strong>' + fulltext + '</strong>"', macro)

q = {
    'key': '/macros/BookCount',
    'macro': { 'connect': 'update', 'type': '/type/text', 'value': macro }
}
infogami.write(q, comment='update book count')

########NEW FILE########
__FILENAME__ = authority
from mechanize import Browser
import re, os.path
from openlibrary.catalog.read_rc import read_rc

rc = read_rc()

start = "http://authorities.loc.gov/cgi-bin/Pwebrecon.cgi?DB=local&PAGE=First"
def get_table_rows(fh):
    cur = ''
    expect = 'thesauri'
    for line in fh:
        if expect == 'thesauri':
            if line == '<TH><a href="/help/thesauri.htm">Type of Heading</a></TH>\n':
                expect = 'headings_close_tr'
                continue
        if expect == 'headings_close_tr':
            assert line == '</TR>\n'
            expect = 'tr'
            continue
        if expect == 'tr':
            assert line == '<TR>\n'
            expect = 'center'
            continue
        if expect == 'center':
            if line == '<TR>\n':
                yield cur.decode('utf-8')
                cur = ''
            elif line == '</CENTER>\n':
                yield cur.decode('utf-8')
                break
            else:
                cur += line
            continue

re_row = re.compile('^<TD ALIGN=RIGHT VALIGN=TOP >\n<A HREF="/cgi-bin/Pwebrecon\.cgi\?(.*)"\n><IMG SRC="/images/([^.]+)\.gif" BORDER=0 ALT="(?:[^"]+)"></A>\n(\d+)\n</TD>\n<TD ALIGN=RIGHT>\n(\d+)\n</TD>\n<TD ALIGN=LEFT>\n(.+)\n</TD>\n<TD ALIGN=LEFT>\n(.+)\n</TD>\n$')
re_no_link = re.compile('^<TD ALIGN=RIGHT VALIGN=TOP >\n</A>\n\d+\n</TD>\n<TD ALIGN=RIGHT>')

def read_serp(fh):
    cur_row = 0
    for row in get_table_rows(fh):
        cur_row += 1
        if re_no_link.match(row):
            continue
        m = re_row.match(row)
        if not m:
            print row
        (param, a, row_num, bib_records, heading, type_of_heading) = m.groups()
        assert str(cur_row) == row_num
        yield {
            'a': a,
            'bib_records': bib_records,
            'heading': heading,
            'type': type_of_heading
        }

def search(arg):
    assert '/' not in arg # because we use it in a filename
    cache = rc['authority_cache']
    filename = cache + '/' + arg
    if os.path.exists(filename):
        return [eval(i) for i in open(filename)]
    br = Browser()
    br.set_handle_robots(False)
    br.open(start)
    br.select_form(name="querybox")
    br['Search_Arg'] = arg.encode('utf-8')
    br['Search_Code'] = ['NHED_']
    res = br.submit()
    found = list(read_serp(res))
    br.close()
    out = open(filename, 'w')
    for i in found:
        print >> out, i
    out.close()
    return found

def test_harold_osman_kelly():
    arg = 'Kelly, Harold Osman'
    found = search(arg)
    assert found[0]['heading'] == 'Kelly, Harold Osman, 1884-1955'

def test_jesus():
    arg = 'Jesus Christ'
    found = search(arg)
    assert found[0]['heading'] == 'Jesus Christ'

def test_pope_sixtus():
    arg = 'Sixtus V Pope'
    found = search(arg)
    assert found[0]['heading'] == 'Sixtus V, Pope, 1520-1590'

def test_william_the_conqueror():
    arg = 'William I King of England'
    found = search(arg)
    assert found[0]['heading'] == 'William I, King of England, 1027 or 8-1087'

def test_non_ascii_result():
    arg = 'Asoka King of Magadha'
    found = search(arg)
    assert found[0]['heading'] == u'As\u0301oka, King of Magadha, fl. 259 B.C.'

def test_non_ascii_param():
    arg = u'A\u015boka King of Magadha'
    found = search(arg)
    assert found[0]['heading'] == u'As\u0301oka, King of Magadha, fl. 259 B.C.'

########NEW FILE########
__FILENAME__ = del
from catalog.infostore import get_site
from catalog.olwrite import Infogami
from catalog.read_rc import read_rc

rc = read_rc()
infogami = Infogami(rc['infogami'])

site = get_site()

# throwaway bit of code for deleting bad scan records
# BPL can't scan microtext

keys = site.things({'type': '/type/scan_record', 'locations': '/scanning_center/MBMBN/BPL1MI', 'scan_status': 'NOT_SCANNED'})
while keys:
    for key in keys:
        sr = site.withKey(key)
        print key
        q = {
            'key': key,
            'type': { 'connect': 'update', 'value': '/type/delete' },
        }
        ret = infogami.write(q, comment="can't scan microtext")
        assert ret['status'] == 'ok'
    keys = site.things({'type': '/type/scan_record', 'locations': '/scanning_center/MBMBN/BPL1MI', 'scan_status': 'NOT_SCANNED'})

########NEW FILE########
__FILENAME__ = edit
import re, web
from openlibrary.catalog.importer.db_read import get_mc
from openlibrary.api import unmarshal
from time import sleep

re_meta_mrc = re.compile('([^/]+)_(meta|marc).(mrc|xml)')
re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon)\.$')

db_amazon = web.database(dbn='postgres', db='amazon')
db_amazon.printing = False

def query_with_retry(ol, q):
    for attempt in range(50):
        try:
            return ol.query(q)
        except:
            sleep(5)
            print 'retry attempt', attempt

def get_with_retry(ol, k):
    for attempt in range(50):
        try:
            return ol.get(k)
        except:
            sleep(5)
            print 'retry attempt', attempt

def amazon_source_records(asin):
    iter = db_amazon.select('amazon', where='asin = $asin', vars={'asin':asin})
    return ["amazon:%s:%s:%d:%d" % (asin, r.seg, r.start, r.length) for r in iter]

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

def fix_toc(e):
    toc = e.get('table_of_contents', None)
    if not toc:
        return
    if isinstance(toc[0], dict) and toc[0]['type'] == '/type/toc_item':
        if len(toc) == 1 and 'title' not in toc[0]:
            del e['table_of_contents'] # remove empty toc
        return
    new_toc = [{'title': unicode(i), 'type': '/type/toc_item'} for i in toc if i != u'']
    e['table_of_contents'] = new_toc

def fix_subject(e):
    if e.get('subjects', None) and any(has_dot(s) for s in e['subjects']):
        subjects = [s[:-1] if has_dot(s) else s for s in e['subjects']]
        e['subjects'] = subjects

def undelete_author(a, ol):
    key = a['key']
    assert a['type'] == '/type/delete'
    url = 'http://openlibrary.org' + key + '.json?v=' + str(a['revision'] - 1)
    prev = unmarshal(json.load(urllib2.urlopen(url)))
    assert prev['type'] == '/type/author'
    ol.save(key, prev, 'undelete author')

def undelete_authors(authors, ol):
    for a in authors:
        if a['type'] == '/type/delete':
            undelete_author(a, ol)
        else:
            assert a['type'] == '/type/author'

def fix_authors(e, ol):
    if 'authors' not in e:
        return
    authors = [get_with_retry(ol, akey) for akey in e['authors']]
    while any(a['type'] == '/type/redirect' for a in authors):
        print 'following redirects'
        authors = [get_with_retry(ol, a['location']) if a['type'] == '/type/redirect' else a for a in authors]
    e['authors'] = [{'key': a['key']} for a in authors]
    undelete_authors(authors, ol)

def fix_edition(key, e, ol):
    existing = get_mc(key)
    if 'source_records' not in e and existing:
        amazon = 'amazon:'
        if existing.startswith('ia:'):
            sr = [existing]
        elif existing.startswith(amazon):
            sr = amazon_source_records(existing[len(amazon):]) or [existing]
        else:
            print 'existing:', existing
            m = re_meta_mrc.search(existing)
            sr = ['marc:' + existing if not m else 'ia:' + m.group(1)]
        e['source_records'] = sr
    if 'ocaid' in e:
        ia = 'ia:' + e['ocaid']
        if 'source_records' not in e:
            e['source_records'] = [ia]
        elif ia not in e['source_records']:
            e['source_records'].append(ia)

    fix_toc(e)
    fix_subject(e)
    fix_authors(e, ol)
    return e

########NEW FILE########
__FILENAME__ = query
import urllib, urllib2
import web
import simplejson as json
from time import sleep
import sys

query_host = 'openlibrary.org'

def urlopen(url, data=None):
    version = "%s.%s.%s" % sys.version_info[:3]
    user_agent = 'Mozilla/5.0 (openlibrary; %s) Python/%s' % (__name__, version)
    headers = {
        'User-Agent': user_agent
    }
    req = urllib2.Request(url, data, headers)
    return urllib2.urlopen(req)

def jsonload(url):
    return json.load(urlopen(url))

def urlread(url):
    return urlopen(url).read()

def set_query_host(host):
    global query_host
    query_host = host

def has_cover(key):
    url = 'http://covers.openlibrary.org/' + key[1] + '/query?olid=' + key[3:]
    return urlread(url).strip() != '[]'

def has_cover_retry(key):
    for attempt in range(5):
        try:
            return has_cover(key)
        except KeyboardInterrupt:
            raise
        except:
            pass
        sleep(2)

def base_url():
    return "http://" + query_host

def query_url():
    return base_url() + "/query.json?query="

def get_all_ia():
    print 'c'
    q = {'source_records~': 'ia:*', 'type': '/type/edition'}
    limit = 10
    q['limit'] = limit
    q['offset'] = 0

    while True:
        url = base_url() + "/api/things?query=" + web.urlquote(json.dumps(q))
        ret = jsonload(url)['result']
        for i in ret:
            yield i
        if not ret:
            return
        q['offset'] += limit

def query(q):
    url = query_url() + urllib.quote(json.dumps(q))
    ret = None
    for i in range(20):
        try:
            ret = urlread(url)
            while ret.startswith('canceling statement due to statement timeout'):
                ret = urlread(url)
            if not ret:
                print 'ret == None'
        except IOError:
            pass
        if ret:
            try:
                data = json.loads(ret)
                if isinstance(data, dict):
                    if 'error' in data:
                        print 'error:'
                        print ret
                    assert 'error' not in data
                return data
            except:
                print ret
                print url
        sleep(20)

def query_iter(q, limit=500, offset=0):
    q['limit'] = limit
    q['offset'] = offset
    while True:
        ret = query(q)
        if not ret:
            return
        for i in ret:
            yield i
        # We haven't got as many we have requested. No point making one more request
        if len(ret) < limit:
            break
        q['offset'] += limit

def get_editions_with_covers_by_author(author, count):
    q = {'type': '/type/edition', 'title_prefix': None, 'subtitle': None, 'title': None, 'authors': author}
    with_covers = []
    for e in query_iter(q, limit=count):
        if not has_cover(e['key']):
            continue
        with_covers.append(e)
        if len(with_covers) == count:
            return with_covers
    return with_covers

def version_iter(q, limit=500, offset=0):
    q['limit'] = limit
    q['offset'] = offset
    while True:
        url = base_url() + '/version'
        v = jsonload(url)
        if not v:
            return
        for i in query(q):
            yield i
        q['offset'] += limit

def withKey(key):
    url = base_url() + key + '.json'
    for i in range(20):
        try:
            return jsonload(url)
        except:
            pass
        print 'retry:', i
        print url

def get_marc_src(e):
    mc = get_mc(e['key'])
    if mc:
        yield mc
    if not e.get('source_records', []):
        return
    for src in e['source_records']:
        if src.startswith('marc:') and src != 'marc:' + mc:
            yield src[5:]

def get_mc(key): # get machine comment
    v = jsonload(base_url() + key + '.json?m=history')

    comments = [i['machine_comment'] for i in v if i.get('machine_comment', None) and ':' in i['machine_comment']]
    if len(comments) == 0:
        return None
    if len(set(comments)) != 1:
        print key
        print comments
    assert len(set(comments)) == 1
    if comments[0] == 'initial import':
        return None
    return comments[0]

########NEW FILE########
__FILENAME__ = find_ol_authors
from catalog.utils import pick_first_date
import web, re, sys, codecs
sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

re_marc_name = re.compile('^(.*), (.*)$')
re_end_dot = re.compile('[^ ][^ ]\.$', re.UNICODE)

def flip_name(name):
    # strip end dots like this: "Smith, John." but not like this: "Smith, J."
    m = re_end_dot.search(name)
    if m:
        name = name[:-1]

    m = re_marc_name.match(name)
    return m.group(2) + ' ' + m.group(1)

for wikipedia, marc in (eval(i) for i in open("matches4")):
    dates = pick_first_date(v for k, v in marc if k == 'd')
    name = ' '.join(v for k, v in marc if k in 'abc')
    print name
    if ', ' in name:
        print flip_name(name)
    print dates


########NEW FILE########
__FILENAME__ = lookup
import web, re, codecs, sys
from time import time
from catalog.marc.fast_parse import get_subfields, get_all_subfields, get_subfield_values
from catalog.utils import pick_first_date
from unicodedata import normalize
from pprint import pprint

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

# bad cats:
# ... animal births
# ... animal deaths
# ... peoples

db = web.database(dbn='postgres', db='wiki_people')
db.printing = False

re_comma = re.compile(', *')

re_marc_name = re.compile('^(.*), (.*)$')

def flip_name(name):
    m = re_marc_name.match(name)
    if m:
        return m.group(2) + ' ' + m.group(1)
    return name

re_title_of = re.compile('^(.*) (of .*)$')

re_digit = re.compile('\d+')
re_decade = re.compile('^(\d+)s$')
re_bc_date = re.compile('^(.*) B\.C\.?$')
re_cent = re.compile('^(?:fl\.? ?)?(\d+)[a-z]{0,2}\.? cent\.$')
# fl. 13th cent/14th cent.
re_cent_range = re.compile('^(?:fl\.? ?)?(\d+)[a-z]{0,2}\.?(?: cent)?[-/](\d+)[a-z]{0,2}\.? cent\.$')
re_century = re.compile('^(\d+)[a-z][a-z] century$')

def decade_match(a, start, ca):
    end = start + 10
    if ca:
        start -= 9
        end += 9
    if a.isdigit():
        return start <= int(a) < end
    return any((start <= int(c) < end) for c in re_digit.findall(a))

def year_approx_match(a, b): 
    approx_century_match = False
    if a.startswith('ca. '):
        ca = True
        a = a[4:]
        range = 15
    else:
        ca = False
        range = 9
    if a == b:
        return True
    if a.replace('.', '') == b:
        return True # ca. 440 B.C.
    if a.endswith(' cent.') and b.endswith(' century') and b.startswith(a[:-1]):
        return True

    bc = False
    if b.endswith(' BC'):
        m = re_bc_date.match(a)
        if m:
            a = m.group(1)
            b = b[:-3]
            bc = True
    if approx_century_match and a.isdigit() and b.endswith(' century'):
        a = int(a)
        m = re_century.match(b)
        assert m
        cent = int(m.group(1))
        start = cent - 1 if not bc else cent
        end = cent if not bc else cent + 1
        if start * 100 <= a < end * 100:
            return True

    if b.isdigit():
        b = int(b)
        if a.isdigit() and (bc or b < 1850) and abs(int(a) - b) <= range:
            return True
        if approx_century_match and a.endswith(' cent.'):
            m = re_cent.match(a)
            if m:
                cent = int(m.group(1))
                start = cent - 1 if not bc else cent
                end = cent if not bc else cent + 1
                if start * 100 <= b < end * 100:
                    return True
        for c in re_digit.findall(a):
            c = int(c)
            if c == b:
                return True
            if (bc or b < 1850) and abs(c - b) <= range:
                return True
        return False
    m = re_decade.match(b)
    if not m:
        return False
    start = int(m.group(1))
    return decade_match(a, start, ca)

def test_year_approx_match():
    assert not year_approx_match('1939', '1940')
    assert year_approx_match('582', '6th century')
    assert year_approx_match('13th cent.', '1240')
    assert year_approx_match('ca. 360 B.C.', '365 BC')
    assert year_approx_match('1889', '1890')
    assert year_approx_match('1883?', '1882')
    assert year_approx_match('1328?', '1320s')
    assert year_approx_match('11th cent.', '11th century')
    assert not year_approx_match('1330', '1320s')
    assert not year_approx_match('245 B.C.', '3rd century BC')

#test_year_approx_match()

# fl. 13th cent/14th cent.
def cent_range(c):
    m = re_cent_range.match(c)
    if m:
        a, b = int(m.group(1)), int(m.group(2))
        assert b == a + 1
        return ((a-1) * 100, b * 100)
    m = re_cent.match(c)
    assert m
    a = int(m.group(1))
    return ((a-1) * 100, a * 100)

re_fl = re.compile('^fl\.? ?(\d+)\.?$')

def get_birth_and_death(cats):
    birth = None
    death = None
    for c in cats:
        if c.endswith(' births'):
            birth = c[:-7]
            continue
        elif c.endswith(' deaths'):
            death = c[:-7]
            continue
    return birth, death

re_century_writers_cat = re.compile('(\d+)[a-z]{2}-century.* writers')

def date_match(dates, cats):
    match_found = False
    if len(dates) == 1 and 'date' in dates:
        marc = dates['date']
        if marc.startswith('fl.'):
            m = re_fl.match(marc)
            if m:
                birth, death = get_birth_and_death(cats)
                if birth and death and birth.isdigit() and death.isdigit():
                    return int(birth) < int(m.group(1)) < int(death)
        if marc.endswith(' cent.'):
            m = re_cent.match(marc)
            if m:
                cent = marc[:-6] + '-century'
                if any(c.endswith(' writers') and cent in c for c in cats):
                    return True
            m = re_cent_range.match(marc)
            if m:
                if any(cm.group(1) in m.groups() for cm in (re_century_writers_cat.match(c) for c in cats) if cm):
                    return True

            try:
                (a, b) = cent_range(marc)
            except:
                print marc
                raise
            for c in cats:
                for f in (' births', ' deaths'):
                    if not c.endswith(f):
                        continue
                    date = c[:-len(f)]
                    if date.isdigit():
                        if a < int(date) < b:
                            match_found = True
                        else:
                            return False
                    else:
                        if year_approx_match(marc, date):
                            match_found = True
                        else:
                            return False

        return match_found

    for f in ['birth', 'death']:
        if f + '_date' not in dates:
            continue
        marc = dates[f + '_date']
        this_cats = [i[:-(len(f)+2)] for i in cats if i.endswith(' %ss' % f)]
        if not this_cats:
            continue
        m = any(year_approx_match(marc, i) for i in this_cats)
        if m:
            match_found = True
        else:
            return False
    return match_found

def norm_name(n):
    return re_comma.sub(' ', n).lower()

# example: "Ibn Daud, Abraham ben David," -> "Ibn Daud"
re_name_comma = re.compile('^([^, ]+ [^, ]+)?, [^ ]')

def name_lookup(fields):
    def join_fields(fields, want):
        return ' '.join(v for k, v in fields if k in want)

    fields = [(k, v.lower()) for k, v in fields]

    if not any(k == 'd' for k, v in fields):
        return []
    ab = [v for k, v in fields if k in 'ab']
    name = ' '.join(ab)
    flipped = flip_name(name)
    names = set([name, flipped])

    a = join_fields(fields, 'a')
    m = re_name_comma.match(a)
    if m:
        names.add(m.group(1))

    #names = set([flipped])
    if any(k == 'c' for k, v in fields):
        name = join_fields(fields, 'abc')
        names.update([name, flip_name(name)])
        title = [v for k, v in fields if k in 'c'] 
        names.update([' '.join(title + ab), ' '.join(title + [flipped])])
        title = ' '.join(title)
        names.update(["%s (%s)" % (name, title), "%s (%s)" % (flipped, title)])
        sp = title.find(' ')
        if sp != -1:
            m = re_title_of.search(title)
            if m:
                role, of_place = m.groups()
                names.update([' '.join(ab + [of_place]), ' '.join([flipped, of_place])])
                names.update([' '.join([role] + ab + [of_place]), ' '.join([role, flipped, of_place])])

            t = title[:sp]
            names.update([' '.join([t] + ab), ' '.join([t, flipped])])
        if 'of st. ' in title: # for "Richard of St. Victor"
            names.update([i.replace('of st.', 'of saint') for i in names])

    found = []
    for n in set(re_comma.sub(' ', n) for n in names):
        iter = db.query("select title, cats, name, persondata from names, people where people.id = names.person_id and name=$n", {'n':n})
        x = [(i.title, eval(i.cats), i.name, i.persondata) for i in iter if not i.title.startswith('Personal life of ')]
        found += x
    return found

noble_or_clergy = ['King', 'Queen', 'Prince', 'Princess', 'Duke', 'Archduke', 'Baron', 'Pope', 'Antipope', 'Bishop', 'Archbishop']
re_noble_or_clergy = re.compile('(' + '|'.join( noble_or_clergy ) + ')')

def strip_brackets(line):
    if line[4] == '[' and line[-2] == ']':
        return line[0:4] + line[5:-2] + line[-1]
    else:
        return line

def fmt_line(fields):
    def bold(s):
        return ''.join(i + '\b' + i for i in s)
    def norm(s):
        return normalize('NFC', s)
    return ''.join(bold("$" + k) + norm(v) for k, v in fields)

def pick_from_match(match):
    l = [(norm_name(k), v) for k, v in match.items()]
    good = [(k, v) for k, v in l if any(k == m for m in v['match_name'])]
    if len(good) == 1:
        return dict(good)
    exact_date = [(k, v) for k, v in l if v['exact_dates']]
    if len(exact_date) == 1:
        return dict(exact_date)
    if len(exact_date) > 1 and len(good) > 1:
        exact_date = [(k, v) for k, v in good if v['exact_dates']]
        if len(exact_date) == 1:
            return dict(exact_date)
    return match

def more_than_one_match(match):
    return [("http://en.wikipedia.org/wiki/" + name.replace(' ', '_'), i) for name, i in match.items()]

def test_date_match():
    # $aAngelico,$cfra,$dca. 1400-l455.
    dates = {'birth_date': u'ca. 1400', 'death_date': u'1455'}
    cats = [u'1395 births', u'1455 deaths']
    assert date_match(dates, cats)

    # $aAndocides,$dca. 440-ca. 390 B.C.
    dates = {'birth_date': u'ca. 440 B.C.', 'death_date': u'ca. 390 B.C.'}
    cats = [u'440 BC births', u'390 BC deaths', u'Ancient Athenians']
    assert date_match(dates, cats)

    # $aAlexander,$cof Hales,$dca. 1185-1245.
    dates = {'birth_date': u'ca. 1185', 'death_date': u'1245'}
    cats = [u'13th century philosophers', u'1245 deaths', u'Roman Catholic philosophers', u'English theologians', u'Franciscans', u'Scholastic philosophers', u'People from Gloucestershire']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1922'}
    cats = [u'1830 births', u'1876 deaths']
    assert not date_match(dates, cats)

    dates = {'birth_date': u'1889', 'death_date': u'1947'}
    cats = [u'1890 births', u'1947 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1889', 'death_date': u'1947'}
    cats = [u'1890 births', u'1947 deaths']
    assert date_match(dates, cats)

    dates = {}
    cats = [u'1890 births', u'1947 deaths']
    assert not date_match(dates, cats)

    dates = {'birth_date': u'1883?', 'death_date': u'1963'}
    cats = [u'1882 births', u'1963 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1328?', 'death_date': u'1369'}
    cats = [u'Karaite rabbis', u'1320s births', u'1369 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'ca. 1110', 'death_date': u'ca. 1180'}
    cats = [u'1120s births', u'1198 deaths']
    assert date_match(dates, cats)

    # $aAbu Nuwas,$dca. 756-ca. 810.  # Abu Nuwas
    dates = {'birth_date': u'ca. 756', 'death_date': u'ca. 810'}
    cats = [u'750 births', u'810 deaths']
    assert date_match(dates, cats)

date_cats = (' births', ' deaths', 'century writers', 'century Latin writers', 'century women writers', 'century French writers') # time for an regexp

def exact_date_match(dates, cats):
    if 'date' in dates or not all(i in dates for i in ('birth_date', 'death_date')):
        return False
    if any('ca.' in i for i in dates.values()):
        return False
    birth, death = get_birth_and_death(cats)
    return dates['birth_date'] == birth and dates['death_date'] == death

def look_for_match(found, dates, verbose):
    match = {}
    for name, cats, match_name, pd in found:
        found_name_match = norm_name(name) == match_name
        #seen.add(name)
        if not any(any(cat.endswith(i) for i in date_cats) for cat in cats):
            if False and not found_name_match:
                print 'name match, but no date cats'
                print name, cats, match_name
                print dates
                print
            continue
        exact_dm = exact_date_match(dates, cats)
        dm = exact_dm or date_match(dates, cats)
        if not dm and found_name_match:
            if 'death_date' in dates:
                death = dates['death_date']
                if death + ' deaths' in cats:
                    dm = True
            elif 'birth_date' in dates:
                birth = dates['birth_date']
                if birth.isdigit():
                    assert birth + ' births' not in cats
        if dm:
            if name in match:
                match[name]['match_name'].append(match_name)
            else:
                match[name] = {'cats': cats, 'exact_dates': exact_dm, 'match_name': [match_name]}
        if not verbose:
            continue
        print (name, match_name)
        print "cats =", cats
        print ('match' if dm else 'no match')
        for field in ['birth', 'death']:
            print field + 's:', [i[:-(len(field)+2)] for i in cats if i.endswith(' %ss' % field)],
        print
    if verbose:
        print '---'
    return match

def test_lookup():
    line = '00\x1faEgeria,\x1fd4th/5th cent.\x1e' # count=3
    wiki = 'Egeria (pilgrim)'
    print fmt_line(get_subfields(line, 'abcd'))
    fields = tuple((k, v.strip(' /,;:')) for k, v in get_subfields(line, 'abcd'))
    print fields
    found = name_lookup(fields)
    print found
    dates = pick_first_date(v for k, v in fields if k == 'd')
    assert dates.items()[0] != ('date', '')
    print dates
    print
    print look_for_match(found, dates, True)

#test_lookup()

def test_lookup2():
    line = '00\x1faRichard,\x1fcof St. Victor,\x1fdd. 1173.\x1e'
    print fmt_line(get_subfields(line, 'abcd'))
    fields = tuple((k, v.strip(' /,;:')) for k, v in get_subfields(line, 'abcd'))
    print fields
    found = name_lookup(fields)
    dates = pick_first_date(v for k, v in fields if k == 'd')
    assert dates.items()[0] != ('date', '')
    print dates
    print
    match = look_for_match(found, dates, False)
    pprint(match)
    print
    match = pick_from_match(match)
    pprint(match)

def test_lookup3():
    line = '00\x1faJohn,\x1fcof Paris,\x1fd1240?-1306.\x1e'
    print fmt_line(get_subfields(line, 'abcd'))
    fields = tuple((k, v.strip(' /,;:')) for k, v in get_subfields(line, 'abcd'))
    print fields
    found = name_lookup(fields)
#    print [i for i in found if 'Paris' in i[0]]
#    found = [(u'John of Paris', [u'Christian philosophers', u'Dominicans', u'Roman Catholic theologians', u'13th-century Latin writers', u'1255 births', u'1306 deaths'], u'john of paris', None)]
    dates = pick_first_date(v for k, v in fields if k == 'd')
    match = look_for_match(found, dates, False)
    match = pick_from_match(match)
    pprint(match)

def test_lookup4():
    fields = (('a', 'Forbes, George'), ('d', '1849-1936.'))
    found = name_lookup(fields)
    dates = pick_first_date(v for k, v in fields if k == 'd')
    match = look_for_match(found, dates, False)
    for k, v in match.iteritems():
        print k, v
    match = pick_from_match(match)
    pprint(match)

#test_lookup4()

def db_marc_lookup():
    verbose = False
    articles = set()
    count = 0
    count_with_date = 0
    t0 = time()
    match_count = 0
    total = 3596802
    prev_fields = None
    fh = open('matches', 'w')
    bad = codecs.open('more_than_one_match', 'w', 'utf8')
    for line in open('/1/edward/wikipedia/marc_authors2'):
        count+=1
#        (author_count, line) = eval(line)
        (line, author_count) = eval(line)
#        line = strip_brackets(line)
        if count % 5000 == 0:
            t1 = time() - t0
            rec_per_sec = count / t1
            time_left = (total - count) / rec_per_sec
            #print fmt_line(get_subfields(line, 'abcd'))
#            print list(get_subfields(line, 'abcd'))
            print line
            print count, count_with_date, match_count, "%.2f%% %.2f mins left" % (float(match_count * 100.0) / float(count_with_date), time_left / 60)
        fields = tuple((k, v.strip(' /,;:')) for k, v in line)
        if prev_fields == fields:
            continue
        prev_fields = fields
        dates = pick_first_date(v for k, v in fields if k == 'd')
        if dates.items()[0] == ('date', ''):
            continue
        count_with_date += 1
        if verbose:
            print line
            print dates
        is_noble_or_clergy = any(k =='c' and re_noble_or_clergy.search(v) for k, v in fields)
        found = name_lookup(fields)
        if not found:
            continue
            if is_noble_or_clergy:
                print 'noble or clergy not found:', line
                print
            continue
        match = look_for_match(found, dates, verbose)

        if not match:
            continue
            if is_noble_or_clergy:
                print 'noble or clergy not found:'
                print fmt_line(line)
                print found
                print
            continue
        match_count+=1
#        articles.add(match.keys()[0])
        if len(match) != 1:
            match = pick_from_match(match)
        if len(match) != 1:
            print >> bad, "\n" + fmt_line(line)
            for i in more_than_one_match(match):
                print >> bad, i
        else:
            #print (list(get_subfields(line, 'abcd')), match.keys()[0])
            cats = match.values()[0]['cats']
            exact = match.values()[0]['exact_dates']
            dc = [i for i in cats if any(i.endswith(j) for j in date_cats)]
            print >> fh, (match.keys()[0], fields, author_count, dc, exact, 'Living people' in cats)
    print match_count
    fh.close()

if __name__ == '__main__':
    db_marc_lookup()

########NEW FILE########
__FILENAME__ = process
# coding=utf8
import bz2, codecs, sys, re
import simplejson as json
from catalog.marc.fast_parse import get_subfields, get_all_subfields, get_subfield_values
from unicodedata import normalize
import MySQLdb
from catalog.utils import pick_first_date
from time import time

re_marc_name = re.compile('^(.*), (.*)$')

def norm(s):
    return normalize('NFC', s)

def get_conn():
    return MySQLdb.connect(passwd='', user='', use_unicode=True, charset='utf8', db='wiki_people')

def get_cursor():
    return get_conn().cursor()

sys.stdout = codecs.getwriter('utf8')(sys.stdout)
re_skip = re.compile('^(History|Demograph(ics|y)|Lists?) of')

def list_names():
    for line in bz2.BZ2File('people.bz2'):
        cur = json.loads(line.decode('utf8'))
        title = cur['title']
        if re_skip.match(title):
            continue
        print title

def redirects():
    titles = set([line[:-1] for line in codecs.open('people_names', 'r', 'utf8')])

    for line in bz2.BZ2File('redirects.bz2'):
        (f, t) = json.loads(line.decode('utf8'))
        t = t.replace('_', ' ')
        if t in titles:
            print (f, t)

def redirect_dict():
    redirects = {}
    for line in open('people_redirects'):
        (f, t) = eval(line)
        t = t.replace('_', ' ')
        redirects.setdefault(t, []).append(f)
    print redirects

def add_redirects():
    redirects = eval(open('redirect_dict').read())
    for line in bz2.BZ2File('people.bz2'):
        cur = json.loads(line.decode('utf8'))
        title = cur['title']
        if re_skip.match(title):
            continue
        if title in redirects:
            cur['redirects'] = redirects[title]
        print cur

#add_redirects()
#redirect_dict()

re_syntax = re.compile(r'(.*?)(\||{{|}}|\[\[|\]\])', re.DOTALL)
re_html_comment = re.compile('<!-- .* -->')
re_space_or_underscore = re.compile('[ _]')
re_infobox_template = re.compile('^infobox[_ ]books?(?:\s*<!--.*-->)?\s*', re.I)
re_persondata = re.compile('^Persondata\s*', re.I)

re_line = re.compile('^\s*\|\s*([A-Z ]+?)\s*=\s*(.*?)\s*$')
def parse_template2(s):
    fields = {}
    for l in s.split('\n'):
        m = re_line.match(l)
        if not m:
            continue
        name, value = m.groups()
        fields[name.strip()] = value
    return fields

def parse_template(s, expected_name):
    template_depth = 1
    link_depth = 0
    pos = 2
    buf = ''

    data = []
    while template_depth > 0:
        m = re_syntax.match(s[pos:])

        pos = pos+m.end()
        buf += m.group(1)
        if m.group(2) == '{{':
            buf += m.group(2)
            template_depth += 1
            continue

        if m.group(2) == '[[':
            buf += m.group(2)
            link_depth += 1
            continue

        if template_depth == 1 and link_depth == 0:
            data.append(buf)
            buf = ''
        elif m.group(2) == '|':
            buf += '|'
        if m.group(2) == '}}':
            buf += m.group(2)
            template_depth -= 1
            continue
        if m.group(2) == ']]':
            buf += m.group(2)
            if link_depth > 0:
                link_depth -= 1
            continue
        assert m.group(2) == '|'
    if buf != '}}':
        return parse_template2(s)
    assert buf == '}}'

    template_name = data.pop(0)
    try:
        assert template_name.lstrip().lower().startswith(expected_name.lower())
        #assert re_persondata.match(infobox_template)
        #assert re_infobox_template.match(infobox_template)
    except AssertionError:
        print template_name
        raise

    fields = {}
    for line in data:
        line = line.strip();
        if line == '' or ((line.startswith('<!--') or line.startswith('< --')) and line.endswith('-->')) or line == 'PLEASE SEE [[WP:PDATA]]!':
            continue
        if '=' in line:
            name, value = line.split('=', 1)
        else:
            m = re_missing_equals.match(line)
            if not m:
                return parse_template2(s)
            name, value = m.groups()
        fields[name.strip()] = value.strip()
    return fields

re_missing_equals = re.compile('^([A-Z ]+) (.+)$')

def parse_pd(pd):
    lines = pd.split('\n')
    print `lines[-1]`
    assert lines[-1] == '}}'

def read_person_data():
    expect = set([u'DATE OF DEATH', u'NAME', u'SHORT DESCRIPTION', u'ALTERNATIVE NAMES', u'PLACE OF BIRTH', u'DATE OF BIRTH', u'PLACE OF DEATH'])
    for line in open('people'):
        cur = eval(line)
        if 'persondata' not in cur:
            continue
        title = cur['title']
        if title == 'Murray Bookchin':
            continue
#        print 'title:', title
        pd = cur['persondata']
        k = set(parse_template(pd, 'persondata').keys())
        if k > expect:
            print title
            print k

def iter_people():
    return (eval(line) for line in open('people'))

def date_cats():
    re_date_cat = re.compile('^(.*\d.*) (birth|death)s$')
    cats = {'birth': {}, 'death':{}}
    for cur in iter_people():
        title = cur['title']
        #print [cat for cat in cur['cats'] if cat.endswith('births') or cat.endswith('deaths')]
        for cat in cur['cats']:
            m = re_date_cat.match(cat)
            if not m:
                continue
            cats[m.group(2)].setdefault(m.group(1), set()).add(title)
#        print 'birth:', [(i[0], len(i[1])) for i in sorted(cats['birth'].items(), reverse = True, key = lambda i: len(i[1]))[:5]]
#        print 'death:', [(i[0], len(i[1])) for i in sorted(cats['death'].items(), reverse = True, key = lambda i: len(i[1]))[:5]]
    print cats

#read_person_data()
#date_cats()

def fmt_line(fields):
    def bold(s):
        return ''.join(i + '\b' + i for i in s)
    return ''.join(bold("$" + k) + norm(v) for k, v in fields)
    
def strip_brackets(line):
    if line[4] == '[' and line[-2] == ']':
        return line[0:4] + line[5:-2] + line[-1]
    else:
        return line

def read_marc():
    for line in bz2.BZ2File('marc_authors.bz2'):
        line = eval(line)
        if '[Sound recording]' in line:
            continue
        line = strip_brackets(line)
        #print expr_in_utf8(get_all_subfields(line))
        print fmt_line(get_subfields(line, 'abcd'))

#read_marc()

#   528,859 wikipedia
# 3,596,802 MARC


def get_names(cur):
    titles = [cur['title']] + cur.get('redirects', [])
    if 'persondata' in cur:
        pd = parse_template(cur['persondata'], 'persondata')
        if 'NAME' in pd and pd['NAME']:
            titles.append(pd['NAME'])
        if 'ALTERNATIVE NAMES' in pd:
            alt = pd['ALTERNATIVE NAMES']
            if len(alt) > 100 and ',' in alt and ';' not in alt:
                alt = alt.split(',')
            else:
                alt = alt.split(';')
            titles += [j for j in (i.strip() for i in alt) if j]
    return set(i.lower() for i in titles)

def read_people():
    from collections import defaultdict
#    wiki = []
#    title_lookup = defaultdict(list)
    maximum = 0
    for cur in iter_people():
#        wiki.append(cur)
        titles = [cur['title']] + cur.get('redirects', [])
        if 'persondata' in cur:
            pd = parse_template(cur['persondata'], 'persondata')
            if 'NAME' in pd and pd['NAME']:
                titles.append(pd['NAME'])
            if 'ALTERNATIVE NAMES' in pd:
                alt = pd['ALTERNATIVE NAMES']
                if len(alt) > 100 and ',' in alt and ';' not in alt:
                    alt = alt.split(',')
                else:
                    alt = alt.split(';')
                titles += [j for j in (i.strip() for i in alt) if j]
        cur_max = max(len(i) for i in titles)
        if cur_max > maximum:
            maximum = cur_max
            print maximum
            print cur['title']
            print titles
#        for t in set(titles):
#            title_lookup[t].append(cur)

# filter names: Robert Bob Adam Hincmar Anselm

# Personal life of Marcus Tullius Cicero

def load_db():
    c = get_cursor()
    c.execute('truncate people')
    c.execute('truncate names')
    c.execute('truncate redirects')
    for person in iter_people():
#        print person
        c.execute('insert into people (title, len, infobox, defaultsort, persondata, cats) values (%s, %s, %s, %s, %s, %s)', (person['title'], person['len'], person.get('infobox', None), person.get('defaultsort', None), person.get('persondata', None), `person.get('cats', [])`))
        id = conn.insert_id()
        c.executemany('insert ignore into names (person_id, name) values (%s, %s)', [(id, n) for n in get_names(person)])
        if 'redirects' in person:
            redirects = set(r.lower() for r in person['redirects'])
            c.executemany('insert ignore into redirects (person_id, redirect) values (%s, %s)', [(id, r) for r in redirects])

re_lifetime = re.compile('\{\{lifetime\| *(\d+s?(?: BC)?|missing|unknown|\d\d?[a-z][a-z] century)? *(?:\| *(\d+s?(?: BC)?|living|unknown|missing|\d\d?[a-z][a-z] century)? *)?(?:\|([^|]*))?\}\}', re.I)

def load_lifetime():
    c = get_cursor()
    for person in iter_people():
        if 'lifetime' not in person:
            continue
        m = re_lifetime.match(person['lifetime'])
        if not m:
            continue
        (birth, death, defaultsort) = m.groups()
        cats = person.get('cats', [])
#        print "select id from people where title='%s'" % person['title']
        c.execute("select id from people where title=%s", (person['title'],))
        (id,) = c.fetchone()
        update_cats = False
        if birth and birth.lower() not in ('missing', 'unknown'):
            new_cat = birth + " births"
            if new_cat not in cats:
                cats.append(new_cat)
                update_cats = True
        if death and death.lower() not in ('missing', 'unknown', 'living'):
            new_cat = death + " deaths"
            if new_cat not in cats:
                cats.append(new_cat)
                update_cats = True
        if update_cats:
            print person['title']
#            print 'update people set cats=%s where id=%s' % (`cats`, id)
            c.execute('update people set cats=%s where id=%s', (`cats`, id))
        if defaultsort:
            add_to_names(c, id, defaultsort)

re_defaultsort = re.compile('^{{defaultsort(?:key)?[;:|]\n?(.*)\n?}}$', re.I)

re_comma = re.compile(', *')
re_comma_and_space = re.compile('[, ]+')

def add_to_names(c, id, name):
    name = re_comma.sub(' ', name).lower().strip()
    c.execute('insert ignore into names (person_id, name) values (%s, %s)', (id, name))

def add_default_sort():
    c = get_cursor()
    c.execute("select id, title, defaultsort from people where defaultsort is not null")
    for id, title, ds in c.fetchall():
#        print id, ds
        if title == 'Omar Gooding':
            ds = '{{DEFAULTSORT:Gooding, Omar}}'
        m = re_defaultsort.match(ds)
        if not m:
            print "http://en.wikipedia.org/wiki/" + title.replace(' ', '_')
            print ds
        if m.group(1):
            add_to_names(c, id, m.group(1))

re_br_or_semicolon = re.compile('(?:</?br ?/?>|;)')
re_strip = re.compile("(?:\([^)]*\)|<!--.*?-->|\[\[.*?\]\]|'''?)")
re_strip2 = re.compile('(?:<.*?>|\{\{.*?\}\})')

def add_names_from_infobox():
    c = get_cursor()
    c.execute("select id, title, infobox from people where infobox is not null")
    for id, title, infobox in c.fetchall():
        try:
            infobox = parse_template(infobox, 'infobox')
        except AttributeError:
            continue
        for field in 'name', 'full name':
            if field not in infobox or not infobox[field]:
                continue
            v = re_strip.sub('', infobox[field])
#            v = infobox[field]
            v = [i for i in (re_strip2.sub('', i).strip(' ,:') for i in re_br_or_semicolon.split(v)) if i]
            for i in v:
                i = re_comma_and_space.sub(' ', i)
                if title != i:
                    add_to_names(c, id, i)
#            print title, ':', field, ':', infobox[field], v

def strip_commas_from_names():
    c = get_cursor()
    c.execute("select person_id, name from names where name like '%,%'")
    for id, name in c.fetchall():
        new = re_comma.sub(' ', name)
        if new == ' ' or new == name:
            print (id, name, new)
        assert new != ' ' and new != name
        c.execute("update ignore names set name=%s where person_id=%s and name=%s", (new, id, name))

#read_people()

#load_db()

def flip_name(name):
    m = re_marc_name.match(name)
    if m:
        return m.group(2) + ' ' + m.group(1)
    return name

re_digit = re.compile('\d+')
re_decade = re.compile('^(\d+)s$')
re_bc_date = re.compile('^(.*) B\.C\.?$')
re_cent = re.compile('^(\d+)[a-z][a-z] cent\.$')
re_century = re.compile('^(\d+)[a-z][a-z] century$')

def decade_match(a, start):
    end = start + 10
    if a.isdigit():
        return start <= int(a) < end
    return any((start <= int(c) < end) for c in re_digit.findall(a))

def year_approx_match(a, b): 
    approx_century_match = False
    if a.startswith('ca. '):
        ca = True
        a = a[4:]
        range = 20
    else:
        ca = False
        range = 9
    if a == b:
        return True
    if a.replace('.', '') == b:
        return True # ca. 440 B.C.
    if a.endswith(' cent.') and b.endswith(' century') and b.startswith(a[:-1]):
        return True

    bc = False
    if b.endswith(' BC'):
        m = re_bc_date.match(a)
        if m:
            a = m.group(1)
            b = b[:-3]
            bc = True
    if approx_century_match and a.isdigit() and b.endswith(' century'):
        a = int(a)
        m = re_century.match(b)
        assert m
        cent = int(m.group(1))
        start = cent - 1 if not bc else cent
        end = cent if not bc else cent + 1
        #print cent, start, a, end
        if start * 100 <= a < end * 100:
            return True

    if b.isdigit():
        b = int(b)
        if a.isdigit() and (bc or b < 1850) and abs(int(a) - b) <= range:
            return True
        if approx_century_match and a.endswith(' cent.'):
            m = re_cent.match(a)
            if m:
                cent = int(m.group(1))
                start = cent - 1 if not bc else cent
                end = cent if not bc else cent + 1
                if start * 100 <= b < end * 100:
                    return True
        for c in re_digit.findall(a):
            c = int(c)
            if c == b:
                return True
            if (bc or b < 1850) and abs(c - b) <= range:
                return True
        return False
    m = re_decade.match(b)
    if not m:
        return False
    start = int(m.group(1))
    return decade_match(a, start)

def test_year_approx_match():
    assert not year_approx_match('1939', '1940')
    assert year_approx_match('582', '6th century')
    assert year_approx_match('13th cent.', '1240')
    assert year_approx_match('ca. 360 B.C.', '365 BC')
    assert year_approx_match('1889', '1890')
    assert year_approx_match('1883?', '1882')
    assert year_approx_match('1328?', '1320s')
    assert year_approx_match('11th cent.', '11th century')
    assert not year_approx_match('1330', '1320s')
    assert not year_approx_match('245 B.C.', '3rd century BC')

def date_match(dates, cats):
    match_found = False
    for f in ['birth', 'death']:
        if f + '_date' not in dates:
            continue
        marc = dates[f + '_date']
        this_cats = [i[:-(len(f)+2)] for i in cats if i.endswith(' %ss' % f)]
        if not this_cats:
            continue
        m = any(year_approx_match(marc, i) for i in this_cats)
        #print m, marc, this_cats
        if m:
            match_found = True
        else:
            return False
    return match_found

def test_date_match():
    # $aAngelico,$cfra,$dca. 1400-l455.
    dates = {'birth_date': u'ca. 1400', 'death_date': u'1455'}
    cats = [u'1395 births', u'1455 deaths']
    assert date_match(dates, cats)

    # $aAndocides,$dca. 440-ca. 390 B.C.
    dates = {'birth_date': u'ca. 440 B.C.', 'death_date': u'ca. 390 B.C.'}
    cats = [u'440 BC births', u'390 BC deaths', u'Ancient Athenians']
    assert date_match(dates, cats)

    # $aAlexander,$cof Hales,$dca. 1185-1245.
    dates = {'birth_date': u'ca. 1185', 'death_date': u'1245'}
    cats = [u'13th century philosophers', u'1245 deaths', u'Roman Catholic philosophers', u'English theologians', u'Franciscans', u'Scholastic philosophers', u'People from Gloucestershire']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1922'}
    cats = [u'1830 births', u'1876 deaths']
    assert not date_match(dates, cats)

    dates = {'birth_date': u'1889', 'death_date': u'1947'}
    cats = [u'1890 births', u'1947 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1889', 'death_date': u'1947'}
    cats = [u'1890 births', u'1947 deaths']
    assert date_match(dates, cats)

    dates = {}
    cats = [u'1890 births', u'1947 deaths']
    assert not date_match(dates, cats)

    dates = {'birth_date': u'1883?', 'death_date': u'1963'}
    cats = [u'1882 births', u'1963 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1328?', 'death_date': u'1369'}
    cats = [u'Karaite rabbis', u'1320s births', u'1369 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'ca. 1110', 'death_date': u'ca. 1180'}
    cats = [u'1120s births', u'1198 deaths']
    assert date_match(dates, cats)

    # $aAbu Nuwas,$dca. 756-ca. 810.  # Abu Nuwas
    dates = {'birth_date': u'ca. 756', 'death_date': u'ca. 810'}
    cats = [u'750 births', u'810 deaths']
    assert date_match(dates, cats)

re_title_of = re.compile('^(.*) (of .*)$')

def name_lookup(c, fields):
    def join_fields(fields, want):
        return ' '.join(v for k, v in fields if k in want)
    if not any(k == 'd' for k, v in fields):
        return []
    ab = [v for k, v in fields if k in 'ab']
    name = ' '.join(ab)
    flipped = flip_name(name)
    names = set([name, flipped])
    #names = set([flipped])
    if any(k == 'c' for k, v in fields):
        name = join_fields(fields, 'abc')
        names.update([name, flip_name(name)])
        title = [v for k, v in fields if k in 'c'] 
        names.update([' '.join(title + ab), ' '.join(title + [flipped])])
        title = ' '.join(title)
        names.update(["%s (%s)" % (name, title), "%s (%s)" % (flipped, title)])
        sp = title.find(' ')
        if sp != -1:
            m = re_title_of.search(title)
            if m:
                role, of_place = m.groups()
                names.update([' '.join(ab + [of_place]), ' '.join([flipped, of_place])])
                names.update([' '.join([role] + ab + [of_place]), ' '.join([role, flipped, of_place])])

            t = title[:sp]
            names.update([' '.join([t] + ab), ' '.join([t, flipped])])

    found = []
    for n in set(re_comma.sub(' ', n) for n in names):
        c.execute("select title, cats, name, persondata from names, people where people.id = names.person_id and name=%s", (n,))
        found += c.fetchall()
    return found

# $aAleksandr Mikhaĭlovich,$cGrand Duke of Russia,$d1866-1933.
# == Grand Duke Alexander Mikhailovich of Russia

def pick_from_match(match):
    good = [(name, (cats, match_name)) for name, (cats, match_name) in match.items() if name.lower() == match_name]
    if len(good) == 1:
        return dict(good)
    return match

def more_than_one_match(match):
    for name, (cats, match_name) in match.items():
        print name, cats, match_name
        print "http://en.wikipedia.org/wiki/" + name.replace(' ', '_')
    print

#$aSmith, William,$d1769-1839
#William Smith (geologist) [u'English geologists', u'Canal engineers', u'People from Oxfordshire', u'Somerset coalfield', u'1769 births', u'1839 deaths', u'People from Scarborough, North Yorkshire', u'Wollaston Medal winners'] william smith
#http://en.wikipedia.org/wiki/William_Smith_(geologist)
#William Smith (South Carolina senator) [u'1762 births', u'1840 deaths', u'United States Senators from South Carolina', u'Democratic Party (United States) vice presidential nominees', u'South Carolina lawyers'] william smith
#http://en.wikipedia.org/wiki/William_Smith_(South_Carolina_senator)

noble_or_clergy = ['King', 'Queen', 'Prince', 'Princess', 'Duke', 'Archduke', 'Baron', 'Pope', 'Antipope', 'Bishop', 'Archbishop']
re_noble_or_clergy = re.compile('(' + '|'.join( noble_or_clergy ) + ')')

def db_marc_lookup():
    verbose = False
    c = get_cursor()
    articles = set()
    count = 0
    count_with_date = 0
    t0 = time()
    match_count = 0
    total = 3596802
    prev_fields = None
    fh = open('matches3', 'w')
    for line in bz2.BZ2File('marc_authors.bz2'):
        count+=1
        line = eval(line)
        line = strip_brackets(line)
        if count % 5000 == 0:
            t1 = time() - t0
            rec_per_sec = count / t1
            time_left = (total - count) / rec_per_sec
            print fmt_line(get_subfields(line, 'abcd'))
            print count, count_with_date, match_count, "%.2f%% %.2f mins left" % (float(match_count * 100.0) / float(count_with_date), time_left / 60)
        fields = tuple((k, v.strip(' /,;:')) for k, v in get_subfields(line, 'abcd'))
        if prev_fields == fields:
            continue
        prev_fields = fields
        dates = pick_first_date(v for k, v in fields if k == 'd')
        if dates.items()[0] == ('date', ''):
            continue
        count_with_date += 1
        if verbose:
            print fmt_line(get_subfields(line, 'abcd'))
            print dates
        is_noble_or_clergy = any(re_noble_or_clergy.search(v) \
            for v in get_subfield_values(line, 'c'))
        found = name_lookup(c, fields)
        if not found:
            continue
            if is_noble_or_clergy:
                print 'noble or clergy not found:'
                print fmt_line(get_subfields(line, 'abcd'))
                print
            continue
        match = {}
        seen = set()
        for name, cats, match_name, pd in found:
            if name in seen:
                continue
            seen.add(name)
            cats = eval(cats)
            if not any(cat.endswith(' births') or cat.endswith(' deaths') for cat in cats):
                continue
            dm = date_match(dates, cats)
            if dm:
                match[name] = (cats, match_name)
            if not verbose:
                continue
            print (name, match_name)
            print "cats =", cats
            print ('match' if dm else 'no match')
            for field in ['birth', 'death']:
                print field + 's:', [i[:-(len(field)+2)] for i in cats if i.endswith(' %ss' % field)],
            print
        if verbose:
            print '---'

        if not match:
            continue
            if is_noble_or_clergy:
                print 'noble or clergy not found:'
                print fmt_line(get_subfields(line, 'abcd'))
                print found
                print
            continue
        match_count+=1
#        articles.add(match.keys()[0])
        if len(match) != 1:
            match = pick_from_match(match)
        if len(match) != 1:
            print count, match_count
            print fmt_line(get_subfields(line, 'abcd'))
            more_than_one_match(match)
        else:
            #print (list(get_subfields(line, 'abcd')), match.keys()[0])
            print >> fh, (match.keys()[0], fields)
        continue
#        print len(articles), match[0][0], fmt_line(get_subfields(line, 'abcd'))
        assert len(match) == 1
    print match_count
    fh.close()

#add_names_from_infobox()
#test_year_approx_match()
db_marc_lookup()
#test_date_match()
#add_default_sort()
#strip_commas_from_names()

#load_lifetime()

########NEW FILE########
__FILENAME__ = read
import sys, codecs, re
from catalog.marc.fast_parse import translate
sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

prev = None
cur_marc = []

trans = {'&':'&amp;','<':'&lt;','>':'&gt;','\n':'<br>'}
re_html_replace = re.compile('([&<>\n])')

def esc(s):
    return re_html_replace.sub(lambda m: trans[m.group(1)], s)

def esc_sp(s):
    return esc(s).replace(' ', '&nbsp;')

print '<html>\n<head><title>Authors</title></head>\n<body>'

print '87 authors with 10 or more variants in MARC records<br>'

def html_subfields(marc):
    return ''.join('<b>' + k + '</b>' + esc(translate(v)) for k, v in marc)

for line in open("matches4"):
    wiki, marc = eval(line)
    if prev and prev != wiki:
        if len(cur_marc) > 9:
            print '<h2><a href="http://en.wikipedia.org/wiki/%s">%s</a></h2>' % (prev.replace(" ", "_"), prev)
            print "%d variants in MARC records<br>" % len(cur_marc)
            print "<ul>", ''.join("<li>%s</li>\n" % html_subfields(li) for li in cur_marc), "</ul>"
#            for i in cur_marc:
#                print '  ', i
        cur_marc = []
    cur_marc.append(marc)
    prev = wiki

print '</body>\n</html>'

########NEW FILE########
__FILENAME__ = tidy
import bz2, codecs, sys, re
import simplejson as json
from catalog.marc.fast_parse import get_subfields, get_all_subfields, get_subfield_values
from unicodedata import normalize
import MySQLdb
from catalog.utils import pick_first_date
from time import time

re_marc_name = re.compile('^(.*), (.*)$')

def norm(s):
    return normalize('NFC', s)

def get_conn():
    return MySQLdb.connect(passwd='', user='', use_unicode=True, charset='utf8', db='wiki_people')

def get_cursor():
    return get_conn().cursor()



sys.stdout = codecs.getwriter('utf8')(sys.stdout)
re_skip = re.compile('^(History|Demograph(ics|y)|Lists?) of')

def names():

    for line in bz2.BZ2File('people.bz2'):
        cur = json.loads(line.decode('utf8'))
        title = cur['title']
        if re_skip.match(title):
            continue
        print title

def redirects():
    titles = set([line[:-1] for line in codecs.open('people_names', 'r', 'utf8')])

    for line in bz2.BZ2File('redirects.bz2'):
        (f, t) = json.loads(line.decode('utf8'))
        t = t.replace('_', ' ')
        if t in titles:
            print (f, t)

def redirect_dict():
    redirects = {}
    for line in open('people_redirects'):
        (f, t) = eval(line)
        t = t.replace('_', ' ')
        redirects.setdefault(t, []).append(f)
    print redirects

def add_redirects():
    redirects = eval(open('redirect_dict').read())
    for line in bz2.BZ2File('people.bz2'):
        cur = json.loads(line.decode('utf8'))
        title = cur['title']
        if re_skip.match(title):
            continue
        if title in redirects:
            cur['redirects'] = redirects[title]
        print cur

#add_redirects()
#redirect_dict()

re_syntax = re.compile(r'(.*?)(\||{{|}}|\[\[|\]\])', re.DOTALL)
re_html_comment = re.compile('<!-- .* -->')
re_space_or_underscore = re.compile('[ _]')
re_infobox_template = re.compile('^infobox[_ ]books?(?:\s*<!--.*-->)?\s*', re.I)
re_persondata = re.compile('^Persondata\s*', re.I)

re_line = re.compile('^\s*\|\s*([A-Z ]+?)\s*=\s*(.*?)\s*$')
def parse_template2(s):
    fields = {}
    for l in s.split('\n'):
        m = re_line.match(l)
        if not m:
            continue
        name, value = m.groups()
        fields[name.strip()] = value
    return fields

def parse_template(s):
    template_depth = 1
    link_depth = 0
    pos = 2
    buf = ''

    data = []
    while template_depth > 0:
        m = re_syntax.match(s[pos:])

        pos = pos+m.end()
        buf += m.group(1)
        if m.group(2) == '{{':
            buf += m.group(2)
            template_depth += 1
            continue

        if m.group(2) == '[[':
            buf += m.group(2)
            link_depth += 1
            continue

        if template_depth == 1 and link_depth == 0:
            data.append(buf)
            buf = ''
        if m.group(2) == '}}':
            buf += m.group(2)
            template_depth -= 1
            continue
        if m.group(2) == ']]':
            buf += m.group(2)
            if link_depth > 0:
                link_depth -= 1
            continue
        assert m.group(2) == '|'
    if buf != '}}':
        return parse_template2(s)
    assert buf == '}}'

    infobox_template = data.pop(0)
    try:
        assert re_persondata.match(infobox_template)
        #assert re_infobox_template.match(infobox_template)
    except AssertionError:
        print infobox_template
        raise

    fields = {}
    for line in data:
        line = line.strip();
        if line == '' or ((line.startswith('<!--') or line.startswith('< --')) and line.endswith('-->')) or line == 'PLEASE SEE [[WP:PDATA]]!':
            continue
        if '=' in line:
            name, value = line.split('=', 1)
        else:
            m = re_missing_equals.match(line)
            if not m:
                return parse_template2(s)
            name, value = m.groups()
        fields[name.strip()] = value.strip()
    return fields

re_missing_equals = re.compile('^([A-Z ]+) (.+)$')

def parse_pd(pd):
    lines = pd.split('\n')
    print `lines[-1]`
    assert lines[-1] == '}}'

def read_person_data():
    expect = set([u'DATE OF DEATH', u'NAME', u'SHORT DESCRIPTION', u'ALTERNATIVE NAMES', u'PLACE OF BIRTH', u'DATE OF BIRTH', u'PLACE OF DEATH'])
    for line in open('people'):
        cur = eval(line)
        if 'persondata' not in cur:
            continue
        title = cur['title']
        if title == 'Murray Bookchin':
            continue
#        print 'title:', title
        pd = cur['persondata']
        k = set(parse_template(pd).keys())
        if k > expect:
            print title
            print k

def iter_people():
    return (eval(line) for line in open('people'))

def date_cats():
    re_date_cat = re.compile('^(.*\d.*) (birth|death)s$')
    cats = {'birth': {}, 'death':{}}
    for cur in iter_people():
        title = cur['title']
        #print [cat for cat in cur['cats'] if cat.endswith('births') or cat.endswith('deaths')]
        for cat in cur['cats']:
            m = re_date_cat.match(cat)
            if not m:
                continue
            cats[m.group(2)].setdefault(m.group(1), set()).add(title)
#        print 'birth:', [(i[0], len(i[1])) for i in sorted(cats['birth'].items(), reverse = True, key = lambda i: len(i[1]))[:5]]
#        print 'death:', [(i[0], len(i[1])) for i in sorted(cats['death'].items(), reverse = True, key = lambda i: len(i[1]))[:5]]
    print cats

#read_person_data()
#date_cats()

def fmt_line(fields):
    def bold(s):
        return ''.join(i + '\b' + i for i in s)
    return ''.join(bold("$" + k) + norm(v) for k, v in fields)
    
def strip_brackets(line):
    if line[4] == '[' and line[-2] == ']':
        return line[0:4] + line[5:-2] + line[-1]
    else:
        return line

def read_marc():
    for line in bz2.BZ2File('marc_authors.bz2'):
        line = eval(line)
        if '[Sound recording]' in line:
            continue
        line = strip_brackets(line)
        #print expr_in_utf8(get_all_subfields(line))
        print fmt_line(get_subfields(line, 'abcd'))

#read_marc()

#   528,859 wikipedia
# 3,596,802 MARC


def get_names(cur):
    titles = [cur['title']] + cur.get('redirects', [])
    if 'persondata' in cur:
        pd = parse_template(cur['persondata'])
        if 'NAME' in pd and pd['NAME']:
            titles.append(pd['NAME'])
        if 'ALTERNATIVE NAMES' in pd:
            alt = pd['ALTERNATIVE NAMES']
            if len(alt) > 100 and ',' in alt and ';' not in alt:
                alt = alt.split(',')
            else:
                alt = alt.split(';')
            titles += [j for j in (i.strip() for i in alt) if j]
    return set(i.lower() for i in titles)

def read_people():
    from collections import defaultdict
#    wiki = []
#    title_lookup = defaultdict(list)
    maximum = 0
    for cur in iter_people():
#        wiki.append(cur)
        titles = [cur['title']] + cur.get('redirects', [])
        if 'persondata' in cur:
            pd = parse_template(cur['persondata'])
            if 'NAME' in pd and pd['NAME']:
                titles.append(pd['NAME'])
            if 'ALTERNATIVE NAMES' in pd:
                alt = pd['ALTERNATIVE NAMES']
                if len(alt) > 100 and ',' in alt and ';' not in alt:
                    alt = alt.split(',')
                else:
                    alt = alt.split(';')
                titles += [j for j in (i.strip() for i in alt) if j]
        cur_max = max(len(i) for i in titles)
        if cur_max > maximum:
            maximum = cur_max
            print maximum
            print cur['title']
            print titles
#        for t in set(titles):
#            title_lookup[t].append(cur)

def load_db():
    c = get_cursor()
    c.execute('truncate people')
    c.execute('truncate names')
    c.execute('truncate redirects')
    for person in iter_people():
#        print person
        c.execute('insert into people (title, len, infobox, defaultsort, persondata, cats) values (%s, %s, %s, %s, %s, %s)', (person['title'], person['len'], person.get('infobox', None), person.get('defaultsort', None), person.get('persondata', None), `person.get('cats', [])`))
        id = conn.insert_id()
        c.executemany('insert ignore into names (person_id, name) values (%s, %s)', [(id, n) for n in get_names(person)])
        if 'redirects' in person:
            redirects = set(r.lower() for r in person['redirects'])
            c.executemany('insert ignore into redirects (person_id, redirect) values (%s, %s)', [(id, r) for r in redirects])
        
#        print 'insert into 

#read_people()

#load_db()

def flip_name(name):
    m = re_marc_name.match(name)
    if m:
        return m.group(2) + ' ' + m.group(1)
    return name

re_digit = re.compile('\d+')
re_decade = re.compile('^(\d+)s$')
re_bc_date = re.compile('^(.*) B\.C\.?$')
re_cent = re.compile('^(\d+)[a-z][a-z] cent\.$')
re_century = re.compile('^(\d+)[a-z][a-z] century$')

def decade_match(a, start):
    end = start + 10
    if a.isdigit():
        return start <= int(a) < end
    return any((start <= int(c) < end) for c in re_digit.findall(a))

def year_approx_match(a, b): 
    approx_century_match = False
    if a.startswith('ca. '):
        ca = True
        a = a[4:]
        range = 20
    else:
        ca = False
        range = 9
    if a == b:
        return True
    if a.replace('.', '') == b:
        return True # ca. 440 B.C.
    if a.endswith(' cent.') and b.endswith(' century') and b.startswith(a[:-1]):
        return True

    bc = False
    if b.endswith(' BC'):
        m = re_bc_date.match(a)
        if m:
            a = m.group(1)
            b = b[:-3]
            bc = True
    if approx_century_match and a.isdigit() and b.endswith(' century'):
        a = int(a)
        m = re_century.match(b)
        assert m
        cent = int(m.group(1))
        start = cent - 1 if not bc else cent
        end = cent if not bc else cent + 1
        #print cent, start, a, end
        if start * 100 <= a < end * 100:
            return True

    if b.isdigit():
        b = int(b)
        if a.isdigit() and (bc or b < 1900) and abs(int(a) - b) <= range:
            return True
        if approx_century_match and a.endswith(' cent.'):
            m = re_cent.match(a)
            if m:
                cent = int(m.group(1))
                start = cent - 1 if not bc else cent
                end = cent if not bc else cent + 1
                if start * 100 <= b < end * 100:
                    return True
        for c in re_digit.findall(a):
            c = int(c)
            if c == b:
                return True
            if (bc or b < 1900) and abs(c - b) <= range:
                return True
        return False
    m = re_decade.match(b)
    if not m:
        return False
    start = int(m.group(1))
    return decade_match(a, start)

def test_year_approx_match():
    assert not year_approx_match('1939', '1940')
    assert year_approx_match('582', '6th century')
    assert year_approx_match('13th cent.', '1240')
    assert year_approx_match('ca. 360 B.C.', '365 BC')
    assert year_approx_match('1889', '1890')
    assert year_approx_match('1883?', '1882')
    assert year_approx_match('1328?', '1320s')
    assert year_approx_match('11th cent.', '11th century')
    assert not year_approx_match('1330', '1320s')
    assert not year_approx_match('245 B.C.', '3rd century BC')

def date_match(dates, cats):
    match_found = False
    for f in ['birth', 'death']:
        if f + '_date' not in dates:
            continue
        marc = dates[f + '_date']
        this_cats = [i[:-(len(f)+2)] for i in cats if i.endswith(' %ss' % f)]
        if not this_cats:
            continue
        m = any(year_approx_match(marc, i) for i in this_cats)
        #print m, marc, this_cats
        if m:
            match_found = True
        else:
            return False
    return match_found

def test_date_match():
    # $aAngelico,$cfra,$dca. 1400-l455.
    dates = {'birth_date': u'ca. 1400', 'death_date': u'1455'}
    cats = [u'1395 births', u'1455 deaths']
    assert date_match(dates, cats)

    # $aAndocides,$dca. 440-ca. 390 B.C.
    dates = {'birth_date': u'ca. 440 B.C.', 'death_date': u'ca. 390 B.C.'}
    cats = [u'440 BC births', u'390 BC deaths', u'Ancient Athenians']
    assert date_match(dates, cats)

    # $aAlexander,$cof Hales,$dca. 1185-1245.
    dates = {'birth_date': u'ca. 1185', 'death_date': u'1245'}
    cats = [u'13th century philosophers', u'1245 deaths', u'Roman Catholic philosophers', u'English theologians', u'Franciscans', u'Scholastic philosophers', u'People from Gloucestershire']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1922'}
    cats = [u'1830 births', u'1876 deaths']
    assert not date_match(dates, cats)

    dates = {'birth_date': u'1889', 'death_date': u'1947'}
    cats = [u'1890 births', u'1947 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1889', 'death_date': u'1947'}
    cats = [u'1890 births', u'1947 deaths']
    assert date_match(dates, cats)

    dates = {}
    cats = [u'1890 births', u'1947 deaths']
    assert not date_match(dates, cats)

    dates = {'birth_date': u'1883?', 'death_date': u'1963'}
    cats = [u'1882 births', u'1963 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'1328?', 'death_date': u'1369'}
    cats = [u'Karaite rabbis', u'1320s births', u'1369 deaths']
    assert date_match(dates, cats)

    dates = {'birth_date': u'ca. 1110', 'death_date': u'ca. 1180'}
    cats = [u'1120s births', u'1198 deaths']
    assert date_match(dates, cats)

    # $aAbu Nuwas,$dca. 756-ca. 810.  # Abu Nuwas
    dates = {'birth_date': u'ca. 756', 'death_date': u'ca. 810'}
    cats = [u'750 births', u'810 deaths']
    assert date_match(dates, cats)

re_title_of = re.compile(' (of .*)$')

def name_lookup(c, fields):
    def join_fields(fields, want):
        return ' '.join(v for k, v in fields if k in want)
    if not any(k == 'd' for k, v in fields):
        return []
    ab = [v for k, v in fields if k in 'ab']
    name = ' '.join(ab)
    flipped = flip_name(name)
    #names.update([name, flipped])
    names = set([flipped])
    if any(k == 'c' for k, v in fields):
        name = join_fields(fields, 'abc')
        names.update([name, flip_name(name)])
        title = [v for k, v in fields if k in 'c'] 
        names.update([' '.join(title + ab), ' '.join(title + [flipped])])

        title = ' '.join(title)
        sp = title.find(' ')
        if sp != -1:
            m = re_title_of.search(title)
            if m:
                t = m.group(1)
                names.update([' '.join(ab + [t]), ' '.join([flipped, t])])

            t = title[:sp]
            names.update([' '.join([t] + ab), ' '.join([t, flipped])])

    found = []
    names.update(n.replace(',', '') for n in names.copy() if ',' in n)
    for n in names:
        c.execute("select title, cats, name, persondata from names, people where people.id = names.person_id and name=%s", (n,))
        found += c.fetchall()
    return found

def db_marc_lookup():
    c = get_cursor()
    articles = set()
    count = 0
    t0 = time()
    match_count = 0
    total = 3596802
    for line in bz2.BZ2File('marc_authors.bz2'):
        count+=1
        if count % 1000 == 0:
            t1 = time() - t0
            rec_per_sec = count / t1
            time_left = (total - count) / rec_per_sec
            print count, match_count, "%.2f%% %.2f mins left" % ((match_count * 100) / count, time_left / 60)
        line = eval(line)
        line = strip_brackets(line)
        fields = [(k, v.strip(' /,;:')) for k, v in get_subfields(line, 'abcd')]
        dates = pick_first_date(v for k, v in fields if k == 'd')
        if dates.items()[0] == ('date', ''):
            continue
        found = name_lookup(c, fields)
        if not found:
            continue
        match = {}
        seen = set()
#        print fmt_line(get_subfields(line, 'abcd'))
#        print dates
        for name, cats, match_name, pd in found:
            if name in seen:
                continue
            seen.add(name)
            cats = eval(cats)
            if not any(cat.endswith(' births') or cat.endswith(' deaths') for cat in cats):
                continue
            dm = date_match(dates, cats)
            if dm:
                match[name] = (cats, match_name)
            continue
            print (name, match_name)
            print "cats =", cats
            print ('match' if dm else 'no match')
            for field in ['birth', 'death']:
                print field + 's:', [i[:-(len(field)+2)] for i in cats if i.endswith(' %ss' % field)],
            print
#        print '---'

        if not match:
            continue
        match_count+=1
#        articles.add(match.keys()[0])
        if len(match) != 1:
            print count, match_count
            print fmt_line(get_subfields(line, 'abcd'))
            for name, (cats, match_name) in match.items():
                print name, cats, match_name
                print "http://en.wikipedia.org/wiki/" + name.replace(' ', '_')
            print
        continue
#        print len(articles), match[0][0], fmt_line(get_subfields(line, 'abcd'))
        assert len(match) == 1
    print match_count

#test_year_approx_match()
#db_marc_lookup()
#test_date_match()

########NEW FILE########
__FILENAME__ = uniq
print len(set(eval(l)[1] for l in open('matches2')))

########NEW FILE########
__FILENAME__ = add_fields_to_works
#!/usr/local/bin/python2.5
import sys, urllib, re, codecs
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary
import simplejson as json
from collections import defaultdict
from catalog.read_rc import read_rc
from catalog.utils.query import query, query_iter, set_staging, base_url
from catalog.utils import mk_norm, get_title

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
set_staging(True)

rc = read_rc()

ol = OpenLibrary(base_url())
ol.login('EdwardBot', rc['EdwardBot'])

re_year = re.compile('(\d{3,})$')

queue = []

def iter_works(fields):
    q = { 'type':'/type/work', 'key': None }
    for f in fields: q[f] = None
    return query_iter(q)

def dates():
    f = 'first_publish_date'
    for w in iter_works([f, 'title']):
        if f in w:
            continue
        q = { 'type':'/type/edition', 'works': w['key'], 'publish_date': None }
        years = defaultdict(list)
        for e in query_iter(q):
            date = e.get('publish_date', None)
            if not date or date == '0000':
                continue
            m = re_year.match(date)
            if not m:
                continue
            year = int(m.group(1))
            years[year].append(e['key'])
        if not years:
            continue
        first = min(years.keys())
        assert first != 0
        print w['key'], `w['title']`, first
        q = {
            'key': w['key'],
            f: { 'connect': 'update', 'value': str(first)}
        }
        queue.append(q)
        if len(queue) == 200:
            print ol.write(queue, comment='add first publish date')
            queue = []
    print ol.write(queue, comment='add first publish date')

def lang():
    f = 'original_languages'
    queue = []
    for w in iter_works([f, 'title']):
        if f in w and w[f]:
            continue
        q = {
            'type':'/type/edition',
            'works': w['key'],
            'languages': None,
            'title': None,
            'title_prefix': None
        }
        editions = [e for e in query_iter(q) if e['languages']]
        title = mk_norm(w['title'])
        if not editions or any(len(e['languages']) != 1 for e in editions):
            continue
        lang = [e['languages'][0]['key'] for e in editions if mk_norm(get_title(e)) == title]
        if len(lang) < 2:
            continue
        first = lang[0]
        if any(l != first for l in lang):
            continue
        print w['key'], `w['title']`, first, len(lang)
        q = {
            'key': w['key'],
            f: { 'connect': 'update_list', 'value': [first]}
        }
        queue.append(q)
        if len(queue) == 200:
            print ol.write(queue, comment='add original language')
            queue = []
    print ol.write(queue, comment='add original language')

def toc_items(toc_list):
    return [{'title': item, 'type': '/type/toc_item'} for item in toc_list] 

def add_fields():
    comment = 'add fields to works'
    queue = []
    seen = set()
    fields = ['genres', 'first_sentence', 'dewey_number', \
            'lc_classifications', 'publish_date'] #, 'table_of_contents']
    for w in iter_works(fields + ['title']):
        if w['key'] in seen or all(w.get(f, None) for f in fields):
            continue
        seen.add(w['key'])
        q = { 'type':'/type/edition', 'works': w['key']}
        for f in fields: q[f] = None
        editions = list(query_iter(q))

        found = {}

        for f in fields:
            if not w.get(f, None):
                if f == 'publish_date':
                    years = defaultdict(list)
                    for e in editions:
                        date = e.get(f, None)
                        if not date or date == '0000':
                            continue
                        m = re_year.match(date)
                        if not m:
                            continue
                        year = int(m.group(1))
                        years[year].append(e['key'])
                    if years:
                        found[f] = str(min(years.keys()))
                    continue
                if f == 'genres':
                    found_list = [[g.strip('.') for g in e[f]] for e in editions \
                        if e.get(f, None) and not any('ranslation' in i for i in e[f])]
                if f == 'table_of_contents':
                    found_list = []
                    for e in query_iter(q):
                        if not e.get(f, None):
                            continue
                        toc = e[f]
                        print e['key'], toc
                        print e
                        print
                        if isinstance(toc[0], basestring):
                            found_list.append(toc_items(toc))
                        else:
                            assert isinstance(toc[0], dict)
                            if toc[0]['type'] == '/type/text':
                                found_list.append(toc_items([i['value'] for i in toc]))
                            else:
                                assert toc[0]['type']['key'] == '/type/toc_item'
                                found_list.append(toc)
                else:
                    found_list = [e[f] for e in query_iter(q) if e.get(f, None)]
                if found_list:
                    first = found_list[0]
                    if all(i == first for i in found_list):
                        found[f] = first

        if not found:
            continue

        print len(queue) + 1, w['key'], len(editions), w['title']
        print found

        q = { 'key': w['key'], }
        for f in fields:
            if not f in found:
                continue
            if f == 'publish_date':
                q['first_publish_date'] = { 'connect': 'update', 'value': found[f]}
            elif f == 'first_sentence':
                q[f] = { 'connect': 'update', 'value': found[f]}
            else:
                q[f] = { 'connect': 'update_list', 'value': found[f]}
        queue.append(q)
        if len(queue) == 200:
            print ol.write(queue, comment=comment)
            queue = []
    print ol.write(queue, comment=comment)

add_fields()

########NEW FILE########
__FILENAME__ = by_author
#!/usr/local/bin/python2.5
import re, sys, codecs, web
from openlibrary.catalog.get_ia import get_from_archive
from openlibrary.catalog.marc.fast_parse import get_subfield_values, get_first_tag, get_tag_lines, get_subfields
from openlibrary.catalog.utils.query import query_iter, set_staging, query
from openlibrary.catalog.utils import mk_norm
from openlibrary.catalog.read_rc import read_rc
from collections import defaultdict
from pprint import pprint, pformat
from catalog.utils.edit import fix_edition
import urllib
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, Reference
import olapi

rc = read_rc()

ol = OpenLibrary("http://dev.openlibrary.org")
ol.login('EdwardBot', rc['EdwardBot'])

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon|etc)\.$')

base_url = "http://dev.openlibrary.org"
query_url = base_url + "/query.json?query="

work_num = 184076

set_staging(True)

def withKey(key):
    url = base_url + key + ".json"
    return urllib.urlopen(url).read()

def find_new_work_key():
    global work_num
    while 1:
        key = "/w/OL%dW" % work_num
        ret = withKey(key)
        if ret.startswith("Not Found:"):
            return work_num
        work_num += 1

def next_work_key():
    global work_num
    key = "/w/OL%dW" % work_num
    ret = withKey(key)
    while not ret.startswith("Not Found:"):
        work_num += 1
        key = "/w/OL%dW" % work_num
        ret = withKey(key)
    work_num += 1
    return key

# sample title: The Dollar Hen (Illustrated Edition) (Dodo Press)
re_parens = re.compile('^(.*?)(?: \(.+ (?:Edition|Press)\))+$')

def top_rev_wt(d):
    d_sorted = sorted(d.keys(), cmp=lambda i, j: cmp(d[j], d[i]) or cmp(len(j), len(i)))
    return d_sorted[0]

def books_query(akey): # live version
    q = {
        'type':'/type/edition',
        'authors': akey,
        '*': None
    }
    return query_iter(q)

def freq_dict_top(d):
    return sorted(d.keys(), reverse=True, key=lambda i:d[i])[0]


def get_work_title(e):
    if e['key'] not in marc:
        assert not e.get('work_titles', [])
        return
#    assert e.get('work_titles', [])
    data = marc[e['key']][1]
    line = get_first_tag(data, set(['240']))
    if not line:
        assert not e.get('work_titles', [])
        return
    return ' '.join(get_subfield_values(line, ['a'])).strip('. ')

def get_books(akey):
    for e in books_query(akey):
        if not e.get('title', None):
            continue
        if len(e.get('authors', [])) != 1:
            continue
#        if 'works' in e:
#            continue
        if 'title_prefix' in e and e['title_prefix']:
            prefix = e['title_prefix']
            if prefix[-1] != ' ':
                prefix += ' '
            title = prefix + e['title']
        else:
            title = e['title']

        title = title.strip(' ')
        if has_dot(title):
            title = title[:-1]
        if title.strip('. ') in ['Publications', 'Works', 'Report', \
                'Letters', 'Calendar', 'Bulletin', 'Plays', 'Sermons', 'Correspondence']:
            continue

        m = re_parens.match(title)
        if m:
            title = m.group(1)

        n = mk_norm(title)

        book = {
            'title': title,
            'norm_title': n,
            'key': e['key'],
        }

        if 'languages' in e:
            book['lang'] = [l['key'][3:] for l in e['languages']]

        if e.get('table_of_contents', None):
            if isinstance(e['table_of_contents'][0], basestring):
                book['table_of_contents'] = e['table_of_contents']
            else:
                assert isinstance(e['table_of_contents'][0], dict)
                if e['table_of_contents'][0]['type'] == '/type/text':
                    book['table_of_contents'] = [i['value'] for i in e['table_of_contents']]

        wt = get_work_title(e)
        if not wt:
            yield book
            continue
        if wt in ('Works', 'Selections'):
            yield book
            continue
        n_wt = mk_norm(wt)
        book['work_title'] = wt
        book['norm_wt'] = n_wt
        yield book

def build_work_title_map(equiv, norm_titles):
    # map of book titles to work titles
    title_to_work_title = defaultdict(set)
    for (norm_title, norm_wt), v in equiv.items():
        if v != 1:
            title_to_work_title[norm_title].add(norm_wt)

    title_map = {}
    for title, v in title_to_work_title.items():
        if len(v) == 1:
            title_map[title] = list(v)[0]
            continue
        most_common_title = max(v, key=lambda i:norm_titles[i])
        if title != most_common_title:
            title_map[title] = most_common_title
        for i in v:
            if i != most_common_title:
                title_map[i] = most_common_title
    return title_map

def find_works(akey):
    equiv = defaultdict(int) # title and work title pairs
    norm_titles = defaultdict(int) # frequency of titles
    books_by_key = {}
    books = []
    rev_wt = defaultdict(lambda: defaultdict(int))

    for book in get_books(akey):
        if 'norm_wt' in book:
            pair = (book['norm_title'], book['norm_wt'])
            equiv[pair] += 1
            rev_wt[book['norm_wt']][book['work_title']] +=1
        norm_titles[book['norm_title']] += 1
        books_by_key[book['key']] = book
        books.append(book)

    title_map = build_work_title_map(equiv, norm_titles)

    works = defaultdict(lambda: defaultdict(list))
    work_titles = defaultdict(list)
    for b in books:
        if 'eng' not in b.get('lang', []) and 'norm_wt' in b:
            work_titles[b['norm_wt']].append(b['key'])
            continue
        n = b['norm_title']
        title = b['title']
        if n in title_map:
            n = title_map[n]
            title = top_rev_wt(rev_wt[n])
        works[n][title].append(b['key'])

    works = sorted([(sum(map(len, w.values() + [work_titles[n]])), n, w) for n, w in works.items()])

    for work_count, norm, w in works:
        if work_count < 2:
            continue
        first = sorted(w.items(), reverse=True, key=lambda i:len(i[1]))[0][0]
        titles = defaultdict(int)
        for key_list in w.values():
            for ekey in key_list:
                b = books_by_key[ekey]
                title = b['title']
                titles[title] += 1
        keys = work_titles[norm]
        for values in w.values():
            keys += values
        assert work_count == len(keys)
        title = max(titles.keys(), key=lambda i:titles[i])
        toc = [(k, books_by_key[k].get('table_of_contents', None)) for k in keys]
        yield {'title': first, 'editions': keys, 'toc': dict((k, v) for k, v in toc if v)}

def print_works(works):
    for w in works:
        print len(w['editions']), w['title']

def toc_items(toc_list):
    return [{'title': unicode(item), 'type': Reference('/type/toc_item')} for item in toc_list] 

def add_works(akey, works):
    queue = []
    for w in works:
        w['key'] = next_work_key()
        q = {
            'authors': [akey],
            'create': 'unless_exists',
            'type': '/type/work',
            'key': w['key'],
            'title': w['title']
        }
        #queue.append(q)
        print ol.write(q, comment='create work')
        for ekey in w['editions']:
            e = ol.get(ekey)
            fix_edition(ekey, e, ol)
            e['works'] = [Reference(w['key'])]
            try:
                ol.save(ekey, e, 'found a work')
            except olapi.OLError:
                print ekey
                print e
                raise

def by_authors():
    find_new_work_key()

    skipping = False
    skipping = True
    q = { 'type':'/type/author', 'name': None, 'works': None }
    for a in query_iter(q, offset=215000):
        akey = a['key']
        if skipping:
            print 'skipping:', akey, a['name']
            if akey == '/a/OL218496A':
                skipping = False
            continue

        q = {
            'type':'/type/work',
            'authors': akey,
        }
        if query(q):
            print akey, `a['name']`, 'has works'
            continue

    #    print akey, a['name']
        found = find_works(akey)
        works = [i for i in found if len(i['editions']) > 2]
        if works:
            #open('found/' + akey[3:], 'w').write(`works`)
            print akey, `a['name']`
            #pprint(works)
            #print_works(works)
            add_works(akey, works)
            print

by_authors()
sys.exit(0)
akey = '/a/OL27695A'
akey = '/a/OL2527041A'
akey = '/a/OL17005A'
akey = '/a/OL117645A'
print akey
works = list(find_works(akey))
pprint(works)

########NEW FILE########
__FILENAME__ = find
import web, re, sys
from catalog.read_rc import read_rc
from catalog.infostore import get_site
#from catalog.db_read import get_things, withKey
from pprint import pprint
from catalog.amazon.other_editions import find_others

rc = read_rc()

re_translation_of = re.compile('^Translation of\b[: ]*([^\n]*?)\.?$', re.I | re.M)

site = get_site()

def isbn_link(i):
    return '<a href="http://wiki-beta.us.archive.org:8081/?isbn=%s">%s</a> (<a href="http://amazon.com/dp/%s">Amazon.com</a>)' % (i, i, i)

def ol_link(key):
    return '<a href="http://openlibrary.org%s">%s</a></td>' % (key, key)

def search(title, author):
    q = { 'type': '/type/author', 'name': author }
    print q
    authors = site.things(q)
    print authors
    seen = set()
    pool = set()
#    for a in authors:
#        q = { 'type': '/type/edition', 'authors': a, 'title': title }
#        pool.update(site.things(q))
    found_titles = {}
    found_isbn = {}
    author_keys = ','.join("'%s'" % a for a in authors)

    print author_keys
    iter = web.query("select id, key from thing where thing.id in (select thing_id from edition_ref, thing where edition_ref.key_id=11 and edition_ref.value = thing.id and thing.key in (" + author_keys + "))")
    key_to_id = {}
    id_to_key = {}
    for row in iter:
        print row
        key_to_id[row.key] = row.id
        id_to_key[row.id] = row.key


    iter = web.query("select thing_id, edition_str.value as title from edition_str where key_id=3 and thing_id in (select thing_id from edition_ref, thing where edition_ref.key_id=11 and edition_ref.value = thing.id and thing.key in (" + author_keys + "))")
    id_to_title = {}
    title_to_key = {}
    for row in iter:
        print row
        t = row.title.lower().strip('.')
        id_to_title[row.thing_id] = row.title
        title_to_key.setdefault(t, []).append(id_to_key[row.thing_id])

    if title.lower() not in title_to_key:
        print 'title not found'
        return

    pool = set(title_to_key[title.lower()])

    editions = []
    while pool:
        key = pool.pop()
        print key
        seen.add(key)
        e = site.withKey(key)
        translation_of = None
        if e.notes:
            m = re_translation_of.search(e.notes)
            if m:
                translation_of = m.group(1).lower()
                pool.update(k for k in title_to_key[translation_of] if k not in seen)
                found_titles.setdefault(translation_of, []).append(key)
        if e.isbn_10:
            for i in e.isbn_10:
                found_isbn.setdefault(i, []).append(key)
            join_isbn = ', '.join(map(isbn_link, e.isbn_10))
        else:
            join_isbn = ''
        rec = {
            'key': key,
            'publish_date': e.publish_date,
            'publishers': ', '.join(p.encode('utf-8') for p in (e.publishers or [])),
            'isbn': join_isbn,
        }
        editions.append(rec)

        if e.work_titles:
            for t in e.work_titles:
                t=t.strip('.')
                pool.update(k for k in title_to_key.get(t.lower(), []) if k not in seen)
                found_titles.setdefault(t, []).append(key)
        if e.other_titles:
            for t in e.other_titles:
                t=t.strip('.')
                pool.update(k for k in title_to_key.get(t.lower(), []) if k not in seen)
                found_titles.setdefault(t, []).append(key)

    print '<table>'
    for e in sorted(editions, key=lambda e: e['publish_date'] and e['publish_date'][-4:]):
        print '<tr>'
        print '<td>', ol_link(e['key'])
        print '<td>', e['publish_date'], '</td><td>', e['publishers'], '</td>'
        print '<td>', e['isbn'], '</td>'
        print '</tr>'
    print '</table>'

    if found_titles:
        print '<h2>Other titles</h2>'
        print '<ul>'
        for k, v in found_titles.iteritems():
            if k == title:
                continue
            print '<li><a href="/?title=%s&author=%s">%s</a>' % (k, author, k),
            print 'from', ', '.join(ol_link(i) for i in v)
        print '</ul>'

    extra_isbn = {}
    for k, v in found_isbn.iteritems():
        for isbn, note in find_others(k, rc['amazon_other_editions']):
            if note.lower().find('audio') != -1:
                continue
            if isbn not in found_isbn:
                extra_isbn.setdefault(isbn, []).extend(v)

    if extra_isbn:
        print '<h2>Other ISBN</h2>'
        print '<ul>'
        for k in sorted(extra_isbn):
            print '<li>', isbn_link(k),
            print 'from', ', '.join(ol_link(i) for i in extra_isbn[k])
        print '</ul>'

title = 'Journey to the centre of the earth'
author = 'Jules Verne'
search(title, author)

########NEW FILE########
__FILENAME__ = find_other_editions
#!/usr/local/bin/python2.5
import sys, codecs
from catalog.merge.names import match_name
from catalog.utils import fmt_author, get_title, mk_norm
from catalog.utils.query import query_iter, set_staging, withKey

# find duplicate authors and other editions of works

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
set_staging(True)

def other_editions(title, wkey, work_author):
    # look for other editions with the same title
    wakey = work_author['key']
    q = { 'type': '/type/edition', 'title': title }
    for k in 'works', 'title_prefix', 'key', 'authors':
        q[k] = None
    found = []
    for e in query_iter(q):
        if not e.get('authors', None):
            continue
        if e.get('works', None) and any(i['key'] == wkey for i in e['works']):
            continue
        if any(i['key'] == wakey for i in e['authors']):
            continue
        for akey in (a['key'] for a in e.get('authors', [])):
            a = withKey(akey)
            name = a.get('name', '')
            if match_name(name, work_author['name'], last_name_only_ok=True):
                yield (e, a)

q = { 'type':'/type/work' }
for k in 'key', 'title', 'authors':
    q[k] = None

for w in query_iter(q):
    wkey = w['key']
    titles = set([w['title']])
    q = { 'type': '/type/edition', 'works': wkey }
    for k in 'title', 'title_prefix', 'key', 'authors':
        q[k] = None

    wakey = w['authors'][0]['key']
    work_author = withKey(wakey)

    for e in query_iter(q):
        if not e.get('title', None): 
            continue
        titles.update([get_title(e), e['title']])

    found = []
    for title in titles:
        found += list(other_editions(title, wkey, work_author))

    if not found:
        continue
    print w
    print titles
    print wakey + ':', fmt_author(work_author)
    for e, a in found:
        print '  ', a['key'] + ": ", fmt_author(a)
        print '  ', e
    print

########NEW FILE########
__FILENAME__ = find_works
#!/usr/bin/python

# find works and create pages on production

import re, sys, web, urllib2
from openlibrary.solr.update_work import update_work, solr_update, update_author
from openlibrary.catalog.get_ia import get_from_archive, get_data
from openlibrary.catalog.marc.fast_parse import get_subfield_values, get_first_tag, get_tag_lines, get_subfields, BadDictionary
from openlibrary.catalog.utils.query import query_iter, withKey
from openlibrary.catalog.utils import mk_norm
from openlibrary.catalog.read_rc import read_rc
from collections import defaultdict
from pprint import pprint, pformat
from openlibrary.catalog.utils.edit import fix_edition
from openlibrary.catalog.importer.db_read import get_mc
from urllib import urlopen
from openlibrary.api import OpenLibrary
from lxml import etree
from time import sleep, time, strftime
from openlibrary.catalog.marc.marc_subject import get_work_subjects, four_types
import simplejson as json

ol = OpenLibrary("http://openlibrary.org")

re_skip = re.compile(r'\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon|etc)\.$')
re_work_key = re.compile('^/works/OL(\d+)W$')
re_lang_key = re.compile('^/(?:l|languages)/([a-z]{3})$')
re_author_key = re.compile('^/(?:a|authors)/(OL\d+A)$')

re_ia_marc = re.compile('^(?:.*/)?([^/]+)_(marc\.xml|meta\.mrc)(:0:\d+)?$')

ns = '{http://www.loc.gov/MARC21/slim}'
ns_leader = ns + 'leader'
ns_data = ns + 'datafield'

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

def get_with_retry(k):
    for attempt in range(50):
        try:
            return ol.get(k)
        except:
            pass
        print 'retry'
        sleep(5)
    return ol.get()

#set_staging(True)

# sample title: The Dollar Hen (Illustrated Edition) (Dodo Press)
re_parens = re.compile('^(.*?)(?: \(.+ (?:Edition|Press|Print|Plays|Collection|Publication|Novels|Mysteries|Book Series|Classics Library|Classics|Books)\))+$', re.I)

def top_rev_wt(d):
    d_sorted = sorted(d.keys(), cmp=lambda i, j: cmp(d[j], d[i]) or cmp(len(j), len(i)))
    return d_sorted[0]

def books_query(akey): # live version
    q = {
        'type':'/type/edition',
        'authors': akey,
        'source_records': None,
        'title': None,
        'work_title': None,
        'table_of_contents': None,
        'languages': None,
        'title_prefix': None,
        'subtitle': None,
    }
    return query_iter(q)

def freq_dict_top(d):
    return sorted(d.keys(), reverse=True, key=lambda i:d[i])[0]

def get_marc_src(e, mc):
    if mc and mc.startswith('amazon:'):
        mc = None
    if mc and mc.startswith('ia:'):
        yield 'ia', mc[3:]
    elif mc:
        m = re_ia_marc.match(mc)
        if m:
            yield 'ia', m.group(1)
        else:
            yield 'marc', mc
    source_records = e.get('source_records', [])
    if not source_records:
        return
    for src in source_records:
        if src.startswith('ia:'):
            if not mc or src != mc:
                yield 'ia', src[3:]
            continue
        if src.startswith('marc:'):
            if not mc or src != 'marc:' + mc:
                yield 'marc', src[5:]
            continue

def get_ia_work_title(ia):
    # FIXME: rewrite to use MARC binary
    url = 'http://www.archive.org/download/' + ia + '/' + ia + '_marc.xml'
    try:
        root = etree.parse(urlopen(url)).getroot()
    except KeyboardInterrupt:
        raise
    except:
        return
    e = root.find(ns_data + "[@tag='240']")
    if e is None:
        return
    wt = ' '.join(s.text for s in e if s.attrib['code'] == 'a' and s.text)
    return wt

def get_work_title(e, mc):
    # use first work title we find in source MARC records
    wt = None
    for src_type, src in get_marc_src(e, mc):
        if src_type == 'ia':
            wt = get_ia_work_title(src)
            if wt:
                wt = wt.strip('. ')
            if wt:
                break
            continue
        assert src_type == 'marc'
        data = None
        try:
            data = get_data(src)
        except ValueError:
            print 'bad record source:', src
            print 'http://openlibrary.org' + e['key']
            continue
        except urllib2.HTTPError, error:
            print 'HTTP error:', error.code, error.msg
            print e['key']
        if not data:
            continue
        is_marc8 = data[9] != 'a'
        try:
            line = get_first_tag(data, set(['240']))
        except BadDictionary:
            print 'bad dictionary:', src
            print 'http://openlibrary.org' + e['key']
            continue
        if line:
            wt = ' '.join(get_subfield_values(line, ['a'], is_marc8)).strip('. ')
            break
    if wt:
        return wt
    for f in 'work_titles', 'work_title':
        e_wt = e.get(f, [])
        if e_wt:
            assert isinstance(e_wt, list)
            return e_wt[0].strip('. ')

# don't use any of these as work titles
bad_titles = ['Publications', 'Works. English', 'Missal', 'Works', 'Report', \
    'Letters', 'Calendar', 'Bulletin', 'Plays', 'Sermons', 'Correspondence', \
    'Bill', 'Bills', 'Selections', 'Selected works', 'Selected works. English', \
    'The Novels', 'Laws, etc']

def get_books(akey, query, do_get_mc=True):
    for e in query:
        try:
            if not e.get('title', None):
                continue
        except:
            print e
#        if len(e.get('authors', [])) != 1:
#            continue
        if 'title_prefix' in e and e['title_prefix']:
            prefix = e['title_prefix']
            if prefix[-1] != ' ':
                prefix += ' '
            title = prefix + e['title']
        else:
            title = e['title']

        title = title.strip(' ')
        if has_dot(title):
            title = title[:-1]

        m = re_parens.match(title)
        if m:
            title = m.group(1)

        n = mk_norm(title)

        book = {
            'title': title,
            'norm_title': n,
            'key': e['key'],
        }

        lang = e.get('languages', [])
        if lang:
            book['lang'] = [re_lang_key.match(l['key']).group(1) for l in lang]

        if e.get('table_of_contents', None):
            if isinstance(e['table_of_contents'][0], basestring):
                book['table_of_contents'] = e['table_of_contents']
            else:
                assert isinstance(e['table_of_contents'][0], dict)
                if e['table_of_contents'][0].get('type', None) == '/type/text':
                    book['table_of_contents'] = [i['value'] for i in e['table_of_contents']]
        if 'subtitle' in e:
            book['subtitle'] = e['subtitle']

        if 'source_records' in e:
            book['source_records'] = e['source_records']

        mc = get_mc(e['key']) if do_get_mc else None
        wt = get_work_title(e, mc)
        if not wt:
            yield book
            continue
        if wt in bad_titles:
            yield book
            continue
        n_wt = mk_norm(wt)
        book['work_title'] = wt
        book['norm_wt'] = n_wt
        yield book

def build_work_title_map(equiv, norm_titles):
    # map of normalized book titles to normalized work titles
    if not equiv:
        return {}
    title_to_work_title = defaultdict(set)
    for (norm_title, norm_wt), v in equiv.items():
        if v != 1:
            title_to_work_title[norm_title].add(norm_wt)

    title_map = {}
    for norm_title, work_titles in title_to_work_title.items():
        if len(work_titles) == 1:
            title_map[norm_title] = list(work_titles)[0]
            continue
        most_common_title = max(work_titles, key=lambda i:norm_titles[i])
        if norm_title != most_common_title:
            title_map[norm_title] = most_common_title
        for work_title in work_titles:
            if work_title != most_common_title:
                title_map[work_title] = most_common_title
    return title_map

def get_first_version(key):
    url = 'http://openlibrary.org' + key + '.json?v=1'
    try:
        return json.load(urlopen(url))
    except:
        print url
        raise

def get_existing_works(akey):
    q = {
        'type':'/type/work',
        'authors': {'author': {'key': akey}},
        'limit': 0,
    }
    seen = set()
    for wkey in ol.query(q):
        if wkey in seen:
            continue # skip dups
        if wkey.startswith('DUP'):
            continue
        try:
            w = get_with_retry(wkey)
        except:
            print wkey
            raise
        if w['type'] in ('/type/redirect', '/type/delete'):
            continue
        if w['type'] != '/type/work':
            print 'infobase error, should only return works'
            print q
            print w['key']
        assert w['type'] == '/type/work'
        yield w

def find_title_redirects(akey):
    title_redirects = {}
    for w in get_existing_works(akey):
        try:
            norm_wt = mk_norm(w['title'])
        except:
            print w['key']
            raise
        q = {'type':'/type/redirect', 'location': str(w['key']), 'limit': 0}
        try:
            query_iter = ol.query(q)
        except:
            print q
            raise
        for r in map(get_first_version, query_iter):
            redirect_history = json.load(urlopen('http://openlibrary.org%s.json?m=history' % r['key']))
            if any(v['author'].endswith('/WorkBot') and v['comment'] == "merge works" for v in redirect_history):
                continue
            #print 'redirect:', r
            if mk_norm(r['title']) == norm_wt:
                continue
            if r['title'] in title_redirects:
                assert title_redirects[r['title']] == w['title']
            #print 'redirect:', r['key'], r['title'], 'work:', w['key'], w['title']
            title_redirects[r['title']] = w['title']
    return title_redirects

def find_works2(book_iter):
    var = {}
    var['equiv'] = defaultdict(int) # normalized title and work title pairs
    var['norm_titles'] = defaultdict(int) # frequency of titles
    var['books_by_key'] = {}
    var['books'] = []
    # normalized work title to regular title
    var['rev_wt'] = defaultdict(lambda: defaultdict(int))

    for book in book_iter:
        if 'norm_wt' in book:
            pair = (book['norm_title'], book['norm_wt'])
            var['equiv'][pair] += 1
            var['rev_wt'][book['norm_wt']][book['work_title']] +=1
        var['norm_titles'][book['norm_title']] += 1 # used to build title_map
        var['books_by_key'][book['key']] = book
        var['books'].append(book)

    return var

def find_works3(var, existing={}):
    title_map = build_work_title_map(var['equiv'], var['norm_titles'])

    for a, b in existing.items():
        norm_a = mk_norm(a)
        norm_b = mk_norm(b)
        var['rev_wt'][norm_b][norm_a] +=1
        title_map[norm_a] = norm_b

    var['works'] = defaultdict(lambda: defaultdict(list))
    var['work_titles'] = defaultdict(list)
    for b in var['books']:
        if 'eng' not in b.get('lang', []) and 'norm_wt' in b:
            var['work_titles'][b['norm_wt']].append(b['key'])
        n = b['norm_title']
        title = b['title']
        if n in title_map:
            n = title_map[n]
            title = top_rev_wt(var['rev_wt'][n])
        var['works'][n][title].append(b['key'])

def find_work_sort(var):
    def sum_len(n, w):
        # example n: 'magic'
        # example w: {'magic': ['/books/OL1M', ... '/books/OL4M']}
        # example work_titles: {'magic': ['/books/OL1M', '/books/OL3M']}
        return sum(len(i) for i in w.values() + [var['work_titles'][n]])
    return sorted([(sum_len(n, w), n, w) for n, w in var['works'].items()])

def find_works(book_iter, existing={}, do_get_mc=True):

    var = find_works2(book_iter)
    find_works3(var, existing)

    works = find_work_sort(var)

    for work_count, norm, w in works:
        first = sorted(w.items(), reverse=True, key=lambda i:len(i[1]))[0][0]
        titles = defaultdict(int)
        for key_list in w.values():
            for ekey in key_list:
                b = var['books_by_key'][ekey]
                title = b['title']
                titles[title] += 1
        keys = var['work_titles'][norm]
        for values in w.values():
            keys += values
        assert work_count == len(keys)
        title = max(titles.keys(), key=lambda i:titles[i])
        toc_iter = ((k, var['books_by_key'][k].get('table_of_contents', None)) for k in keys)
        toc = dict((k, v) for k, v in toc_iter if v)
        # sometimes keys contains duplicates
        editions = [var['books_by_key'][k] for k in set(keys)]
        subtitles = defaultdict(lambda: defaultdict(int))
        edition_count = 0
        with_subtitle_count = 0
        for e in editions:
            edition_count += 1
            subtitle = e.get('subtitle') or ''
            if subtitle != '':
                with_subtitle_count += 1
            norm_subtitle = mk_norm(subtitle)
            if norm_subtitle != norm:
                subtitles[norm_subtitle][subtitle] += 1
        use_subtitle = None
        for k, v in subtitles.iteritems():
            lc_k = k.strip(' .').lower()
            if lc_k in ('', 'roman') or 'edition' in lc_k:
                continue
            num = sum(v.values())
            overall = float(num) / float(edition_count)
            ratio = float(num) / float(with_subtitle_count)
            if overall > 0.2 and ratio > 0.5:
                use_subtitle = freq_dict_top(v)
        w = {'title': first, 'editions': editions}
        if use_subtitle:
            w['subtitle'] = use_subtitle
        if toc:
            w['toc'] = toc
        try:
            subjects = four_types(get_work_subjects(w, do_get_mc=do_get_mc))
        except:
            print w
            raise
        if subjects:
            w['subjects'] = subjects
        yield w

def print_works(works):
    for w in works:
        print len(w['editions']), w['title']
        print '   ', [e['key'] for e in w['editions']]
        print '   ', w.get('subtitle', None)
        print '   ', w.get('subjects', None)


def books_from_cache():
    for line in open('book_cache'):
        yield eval(line)

def add_subjects_to_work(subjects, w):
    mapping = {
        'subject': 'subjects',
        'place': 'subject_places',
        'time': 'subject_times',
        'person': 'subject_people',
    }
    for k, v in subjects.items():
        k = mapping[k]
        subjects = [i[0] for i in sorted(v.items(), key=lambda i:i[1], reverse=True) if i != '']
        existing_subjects = set(w.get(k, []))
        w.setdefault(k, []).extend(s for s in subjects if s not in existing_subjects)
        if w.get(k):
            w[k] = [unicode(i) for i in w[k]]
        try:
            assert all(i != '' and not i.endswith(' ') for i in w[k])
        except AssertionError:
            print 'subjects end with space'
            print w
            print subjects
            raise

def add_detail_to_work(i, j):
    if 'subtitle' in i:
        j['subtitle'] = i['subtitle']
    if 'subjects' in i:
        add_subjects_to_work(i['subjects'], j)

def fix_up_authors(w, akey, editions):
    print 'looking for author:', akey
    #print (w, akey, editions)
    seen_akey = False
    need_save = False
    for a in w.get('authors', []):
        print 'work:', w['key']
        obj = withKey(a['author']['key'])
        if obj['type']['key'] == '/type/redirect':
            a['author']['key'] = obj['location']
            print obj['key'], 'redirects to', obj['location']
            #a['author']['key'] = '/authors/' + re_author_key.match(a['author']['key']).group(1)
            assert a['author']['key'].startswith('/authors/')
            obj = withKey(a['author']['key'])
            assert obj['type']['key'] == '/type/author'
            need_save = True
        if akey == a['author']['key']:
            seen_akey = True
    if seen_akey:
        if need_save:
            print 'need save:', a
        return need_save
    try:
        ekey = editions[0]['key']
    except:
        print 'editions:', editions
        raise
    #print 'author %s missing. copying from first edition %s' % (akey, ekey)
    #print 'before:'
    for a in w.get('authors', []):
        print a
    e = withKey(ekey)
    #print e
    if not e.get('authors', None):
        print 'no authors in edition'
        return
    print 'authors from first edition', e['authors']
    w['authors'] = [{'type':'/type/author_role', 'author':a} for a in e['authors']]
    #print 'after:'
    #for a in w['authors']:
    #    print a
    return True

def new_work(akey, w, do_updates, fh_log):
    ol_work = {
        'title': w['title'],
        'type': '/type/work',
        'authors': [{'type':'/type/author_role', 'author': akey}],
    }
    add_detail_to_work(w, ol_work)
    print >> fh_log, ol_work
    if do_updates:
        for attempt in range(5):
            try:
                wkey = ol.new(ol_work, comment='work found')
                break
            except:
                if attempt == 4:
                    raise
                print 'retrying: %d attempt' % attempt
        print >> fh_log, 'new work:', wkey, `w['title']`
    else:
        print >> fh_log, 'new work:', `w['title']`
    update = []
    for e in w['editions']:
        try:
            e = ol.get(e['key'])
        except:
            print 'edition:', e['key']
            raise
        if do_updates:
            e['works'] = [{'key': wkey}]
        assert e['type'] == '/type/edition'
        update.append(e)
    if do_updates:
        try:
            print >> fh_log, ol.save_many(update, "add editions to new work")
        except:
            pprint(update)
            # raise # ignore errors
        return [wkey]
    return []

def fix_toc(e):
    toc = e.get('table_of_contents')
    if not toc:
        return
    try:
        if isinstance(toc[0], dict) and toc[0]['type'] == '/type/toc_item':
            return
    except:
        print 'toc'
        print toc
        print `toc`
    return [{'title': unicode(i), 'type': '/type/toc_item'} for i in toc if i != u'']

def update_work_with_best_match(akey, w, work_to_edition, do_updates, fh_log):
    work_updated = []
    best = w['best_match']['key']
    update = []
    subjects_from_existing_works = defaultdict(set)
    for wkey in w['existing_works'].iterkeys():
        if wkey == best:
            continue
        existing = get_with_retry(wkey)
        for k in 'subjects', 'subject_places', 'subject_times', 'subject_people':
            if existing.get(k):
                subjects_from_existing_works[k].update(existing[k])

        update.append({'type': '/type/redirect', 'location': best, 'key': wkey})
        work_updated.append(wkey)

    for wkey in w['existing_works'].iterkeys():
        editions = set(work_to_edition[wkey])
        editions.update(e['key'] for e in w['editions'])
        for ekey in editions:
            e = get_with_retry(ekey)
            e['works'] = [{'key': best}]
            authors = []
            for akey in e['authors']:
                a = get_with_retry(akey)
                if a['type'] == '/type/redirect':
                    m = re_author_key.match(a['location'])
                    akey = '/authors/' + m.group(1)
                authors.append({'key': str(akey)})
            e['authors'] = authors
            new_toc = fix_toc(e)
            if new_toc:
                e['table_of_contents'] = new_toc
            update.append(e)

    cur_work = w['best_match']
    need_save = fix_up_authors(cur_work, akey, w['editions']) 
    if any(subjects_from_existing_works.values()):
        need_save = True
    if need_save or cur_work['title'] != w['title'] \
            or ('subtitle' in w and 'subtitle' not in cur_work) \
            or ('subjects' in w and 'subjects' not in cur_work):
        if cur_work['title'] != w['title']:
            print 'update work title:', best, `cur_work['title']`, '->', `w['title']`
        existing_work = get_with_retry(best)
        if existing_work['type'] != '/type/work':
            pprint(existing_work)
        assert existing_work['type'] == '/type/work'
        existing_work['title'] = w['title']
        for k, v in subjects_from_existing_works.items():
            existing_subjects = set(existing_work.get(k, []))
            existing_work.setdefault(k, []).extend(s for s in v if s not in existing_subjects)
        add_detail_to_work(w, existing_work)
        for a in existing_work.get('authors', []):
            obj = withKey(a['author'])
            if obj['type']['key'] != '/type/redirect':
                continue
            new_akey = obj['location']
            a['author'] = {'key': new_akey}
            assert new_akey.startswith('/authors/')
            obj = withKey(new_akey)
            assert obj['type']['key'] == '/type/author'
        print >> fh_log, 'existing:', existing_work
        print >> fh_log, 'subtitle:', `existing_work['subtitle']` if 'subtitle' in existing_work else 'n/a'
        update.append(existing_work)
        work_updated.append(best)
    if do_updates:
        try:
            print >> fh_log, ol.save_many(update, 'merge works')
        except:
            for page in update:
                print page
            raise
    return work_updated

def update_works(akey, works, do_updates=False):
    # we can now look up all works by an author   
    if do_updates:
        rc = read_rc()
        ol.login('WorkBot', rc['WorkBot']) 
    assert do_updates

    fh_log = open('/1/var/log/openlibrary/work_finder/' + strftime('%F_%T'), 'w')
    works = list(works)
    print >> fh_log, akey
    print >> fh_log, 'works:'
    pprint(works, fh_log)

    while True: # until redirects repaired
        q = {'type':'/type/edition', 'authors': akey, 'works': None}
        work_to_edition = defaultdict(set)
        edition_to_work = defaultdict(set)
        for e in query_iter(q):
            if not isinstance(e, dict):
                continue
            if e.get('works', None):
                for w in e['works']:
                    work_to_edition[w['key']].add(e['key'])
                    edition_to_work[e['key']].add(w['key'])

        work_by_key = {}
        fix_redirects = []
        for k, editions in work_to_edition.items():
            w = withKey(k)
            if w['type']['key'] == '/type/redirect':
                wkey = w['location']
                print >> fh_log, 'redirect found', w['key'], '->', wkey, editions
                assert re_work_key.match(wkey)
                for ekey in editions:
                    e = get_with_retry(ekey)
                    e['works'] = [{'key': wkey}]
                    fix_redirects.append(e)
                continue
            work_by_key[k] = w
        if not fix_redirects:
            print >> fh_log, 'no redirects left'
            break
        print >> fh_log, 'save redirects'
        try:
            ol.save_many(fix_redirects, "merge works")
        except:
            for r in fix_redirects:
                print r
            raise

    all_existing = set()
    work_keys = []
    print >> fh_log, 'edition_to_work:'
    print >> fh_log, `dict(edition_to_work)`
    print >> fh_log
    print >> fh_log, 'work_to_edition'
    print >> fh_log, `dict(work_to_edition)`
    print >> fh_log

#    open('edition_to_work', 'w').write(`dict(edition_to_work)`)
#    open('work_to_edition', 'w').write(`dict(work_to_edition)`)
#    open('work_by_key', 'w').write(`dict(work_by_key)`)

    work_title_match = {}
    works_by_title = {}
    for w in works: # 1st pass
        for e in w['editions']:
            ekey = e['key'] if isinstance(e, dict) else e
            for wkey in edition_to_work.get(ekey, []):
                try:
                    wtitle = work_by_key[wkey]['title']
                except:
                    print 'bad work:', wkey
                    raise
                if wtitle == w['title']:
                    work_title_match[wkey] = w['title']

    wkey_to_new_title = defaultdict(set)

    for w in works: # 2nd pass
        works_by_title[w['title']] = w
        w['existing_works'] = defaultdict(int)
        for e in w['editions']:
            ekey = e['key'] if isinstance(e, dict) else e
            for wkey in edition_to_work.get(ekey, []):
                if wkey in work_title_match and work_title_match[wkey] != w['title']:
                    continue
                wtitle = work_by_key[wkey]['title']
                w['existing_works'][wkey] += 1
                wkey_to_new_title[wkey].add(w['title'])

    existing_work_with_conflict = defaultdict(set)

    for w in works: # 3rd pass
        for wkey, v in w['existing_works'].iteritems():
            if any(title != w['title'] for title in wkey_to_new_title[wkey]):
                w['has_conflict'] = True
                existing_work_with_conflict[wkey].add(w['title'])
                break

    for wkey, v in existing_work_with_conflict.iteritems():
        cur_work = work_by_key[wkey]
        existing_titles = defaultdict(int)
        for ekey in work_to_edition[wkey]:
            e = withKey(ekey)
            title = e['title']
            if e.get('title_prefix', None):
                title = e['title_prefix'].strip() + ' ' + e['title']
            existing_titles[title] += 1
        best_match = max(v, key=lambda wt: existing_titles[wt])
        works_by_title[best_match]['best_match'] = work_by_key[wkey]
        for wtitle in v:
            del works_by_title[wtitle]['has_conflict']
            if wtitle != best_match:
                works_by_title[wtitle]['existing_works'] = {}

    def other_matches(w, existing_wkey):
        return [title for title in wkey_to_new_title[existing_wkey] if title != w['title']]

    works_updated_this_session = set()

    for w in works: # 4th pass
        if 'has_conflict' in w:
            pprint(w)
        assert 'has_conflict' not in w
        if len(w['existing_works']) == 1:
            existing_wkey = w['existing_works'].keys()[0]
            if not other_matches(w, existing_wkey):
                w['best_match'] = work_by_key[existing_wkey]
        if 'best_match' in w:
            updated = update_work_with_best_match(akey, w, work_to_edition, do_updates, fh_log)
            for wkey in updated:
                if wkey in works_updated_this_session:
                    print >> fh_log, wkey, 'already updated!'
                    print wkey, 'already updated!'
                works_updated_this_session.update(updated)
            continue
        if not w['existing_works']:
            updated = new_work(akey, w, do_updates, fh_log)
            for wkey in updated:
                assert wkey not in works_updated_this_session
                works_updated_this_session.update(updated)
            continue

        assert not any(other_matches(w, wkey) for wkey in w['existing_works'].iterkeys())
        best_match = max(w['existing_works'].iteritems(), key=lambda i:i[1])[0]
        w['best_match'] = work_by_key[best_match]
        updated = update_work_with_best_match(akey, w, work_to_edition, do_updates, fh_log)
        for wkey in updated:
            if wkey in works_updated_this_session:
                print >> fh_log, wkey, 'already updated!'
                print wkey, 'already updated!'
        works_updated_this_session.update(updated)

    #if not do_updates:
    #    return []

    return [withKey(key) for key in works_updated_this_session]

if __name__ == '__main__':
    akey = '/authors/' + sys.argv[1]

    title_redirects = find_title_redirects(akey)
    works = find_works(akey, get_books(akey, books_query(akey)), existing=title_redirects)
    to_update = update_works(akey, works, do_updates=True)

    requests = []
    for w in to_update:
        requests += update_work(w)

    if to_update:
        solr_update(requests + ['<commit />'], debug=True)

    requests = update_author(akey)
    solr_update(requests + ['<commit/>'], index='authors', debug=True)

########NEW FILE########
__FILENAME__ = find_work_for_edition
# try and find an existing work for a book

from openlibrary.api import OpenLibrary
from openlibrary.catalog.utils import mk_norm
import sys
from time import time

ol = OpenLibrary("http://openlibrary.org")

def find_matching_work(e):
    norm_title = mk_norm(e['title'])

    seen = set()
    for akey in e['authors']:
        q = {
            'type':'/type/work',
            'authors': {'author': {'key': akey}},
            'limit': 0,
            'title': None,
        }
        t0 = time()
        work_keys = list(ol.query(q))
        t1 = time() - t0
        print 'time to find books by author: %.1f seconds' % t1
        for w in work_keys:
            wkey = w['key']
            if wkey in seen:
                continue
            seen.add(wkey)
            if not w.get('title'):
                continue
            if mk_norm(w['title']) == norm_title:
                assert ol.query({'key': wkey, 'type': None})[0]['type'] == '/type/work'
                return wkey

def test_book():
    ekey = '/books/OL24335218M'
    wkey = find_matching_work(e)
    if wkey:
        print 'found match:', wkey
    else:
        print 'no match'

########NEW FILE########
__FILENAME__ = from_sample
import web, re, sys, codecs
from catalog.marc.fast_parse import *
from catalog.utils import pick_first_date
from pprint import pprint
import catalog.marc.new_parser as parser

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

re_recording = re.compile('\x1f(hsound ?record|[hn] ?\[\[?(sound|video|phonodisc))', re.I)
re_end_dot = re.compile('[^ ][^ ]\.$', re.UNICODE)
re_marc_name = re.compile('^(.*), (.*)$')

authors = {}
family_names = {}
by_author = {}
by_contrib = {}

def remove_trailing_dot(s):
    m = re_end_dot.search(s)
    if m:
        s = s[:-1]
    return s

def strip_q(q):
    if q.endswith(').'):
        q = q[:-1]
    q = q.strip(' ()/,;:')
    return q

def read(data):
    want = ['008', '041', '100', '110', '111', '130', '240', '245', '500', '700', '710', '711']
    fields = get_tag_lines(data, ['006', '008', '245', '260'] + want)
    seen_008 = False
    found = []
    for tag, line in fields:
        if tag in want:
            found.append((tag, line))
        if tag == '006':
            if line[0] == 'm': # don't want electronic resources
                return (fields, None)
            continue
        if tag == '008':
            if seen_008: # dup
                return (fields, None)
            seen_008 = True
            continue
        if tag in ('240', '245', '260'):
            if re_recording.search(line): # sound recording
                return (fields, None)
            continue
    return (fields, found)

def initials(s):
    return [i[0] for i in s.split(' ')]

def parse_person(line):
    contents = get_person_content(line)
    marc_orig = list(get_all_subfields(line)),
    if not ('a' in contents or 'c' in contents):
        return marc_orig, {}
    assert 'a' in contents or 'c' in contents

    if 'd' in contents:
        author = pick_first_date(contents['d'])
    else:
        author = {}
    #author['marc_orig'] = list(get_all_subfields(line)),
    for tag, f in [ ('b', 'numeration'), ('c', 'title') ]:
        if tag in contents:
            author[f] = ' '.join(x.strip(' /,;:') for x in contents[tag])

    if 'a' in contents:
        name = ' '.join(x.strip(' /,;:') for x in contents['a'])
        name = remove_trailing_dot(name)
        m = re_marc_name.match(name)
        if m:
            author['family_name'] = m.group(1)
            author['given_names'] = m.group(2)
            author['name'] = m.group(2) + ' ' + m.group(1)
        else:
            author['name'] = name
    name_subfields = get_subfield_values(line, ['a', 'b', 'c'])
    author['sort'] = ' '.join(v.strip(' /,;:') for v in name_subfields)


    if 'q' in contents:
        if len(contents['q']) != 1:
            print marc_orig
        assert len(contents['q']) == 1
        q = strip_q(contents['q'][0])
        if 'given_names' in authors:
            assert initials(q) == initials(author['given_names']) \
                    or q.startswith(author['given_names'])
        author['given_names'] = q
    return marc_orig, author

def test_parse_person():
    line = '1 \x1faMoeran, E. J.\x1fq(Ernest John)\x1fq(1894-1950)\x1e'
    person = ([('a', u'Moeran, E. J.'), ('q', u'(Ernest John)'), ('q', u'(1894-1950)')],)
    parse_person(line)

#test_parse_person()

def full_title(line):
    title = ' '.join(v for k, v in line if k in ('a', 'b')).strip(' /,;:')
    return remove_trailing_dot(title)

def test_strip_q():
    for i in ['(%s),', '(%s)', '(%s,']:
        k = i % ('foo')
        j = strip_q(k)
        print k, j
        assert j == 'foo'

    name = 'John X.'
    assert name == strip_q('(%s)' % name)

def print_author(a):
    for k in ('name', 'sort', 'numeration', 'title', 'given_names', 'family_name', 'birth_date', 'death_date'):
        print "%12s: %s" % (k, author.get(k, ''))


def person_as_tuple(p):
    return tuple(p.get(i, None) for i in ('sort', 'birth_date', 'death_date'))

def family_name(a):
    if 'family_name' not in a:
        return
    this = a['family_name']
    family_names.setdefault(this, {})
    as_tuple = tuple(a.get(i, None) for i in ('sort', 'birth_date', 'death_date'))
    as_tuple = person_as_tuple(a)
    family_names[this][as_tuple] = family_names[this].get(as_tuple, 0) + 1

interested = set(['Rowling', 'Shakespeare', 'Sagan', 'Darwin', 'Verne', 'Beckett', 'Churchill', 'Dickens', 'Twain', 'Doyle'])
sorted_interest = sorted(interested)

def edition_list(l):
    for e in l:
        print e['loc']
        for k in sorted((k for k in e.keys() if k.isdigit()), key=int):
            if k == '245':
                t = ' '.join(v.strip(' /,;:') for k, v in e[k][0] if k == 'a')
                title = remove_trailing_dot(t)
                full = full_title(e[k][0])
                print '     title:', title
                if title != full:
                    print 'full title:', full
            print '    ', k, e[k]
        print '---'

def print_interest():
    for k in sorted_interest:
        if k not in family_names:
            continue
        print k
        for a in sorted(family_names[k].keys()):
            if family_names[k][a] > 5:
                print "  %3d %s" % (family_names[k][a], a)
                if a in by_author:
                    print "  by: "
                    for i in sorted(by_author[a].keys()):
                        print ' WORK: %s (%d)' % (i, len(by_author[a][i]))
                        edition_list(by_author[a][i])
#                if a in by_contrib:
#                    print "  contrib: "
#                    edition_list(by_contrib[a])
    print

def work_title(edition):
    if '240' in edition:
        t = ' '.join(v for k, v in edition['240'][0] if k in ('a', 'm', 'n', 'p', 'r'))
    else:
        t = ' '.join(v.strip(' /,;:') for k, v in edition['245'][0] if k == 'a')
    return remove_trailing_dot(t)

#for line in open(sys.argv[1]):
for line in sys.stdin:
    loc, data = eval(line)
    (orig_fields, fields) = read(data)
    if not fields:
        continue
    new_interest = False
    edition = {}
    for tag, l in fields:
        #if tag in ('100', '700'):
        if tag == '100':
            try:
                marc, person = parse_person(l)
            except:
                print loc
                raise
            if not person:
                continue
            #print author['marc_orig']
#            print marc
            if person.get('family_name', None) in interested:
#                family_name(person)
                new_interest = True
#            print_author(author)
            continue
            tag_map = { '100': 'authors', '700': 'contribs' }
            person['marc'] = marc
            edition.setdefault(tag_map[tag], []).append(person)
        continue
        if tag == '008':
            lang = str(l)[35:38]
            edition['lang'] = lang
            continue
        edition.setdefault(tag, []).append(list(get_all_subfields(line)))
    #for k in sorted(family_names.keys()):

    if new_interest:
        edition['loc'] = loc
        print (loc, data)
        continue
        title = work_title(edition)
#        rec = parser.read_edition(loc, data)
        for p in edition.get('authors', []):
            a = by_author.setdefault(person_as_tuple(p), {})
            a.setdefault(title, []).append(edition)
#        for p in edition.get('contribs', []):
#            by_contrib.setdefault(person_as_tuple(p), []).append(edition)
for k, v in by_author.items():
    print (k, v)
#print_interest()


########NEW FILE########
__FILENAME__ = live
#!/usr/bin/python

# find works and create pages on production

import re, sys, codecs, web
from openlibrary.catalog.get_ia import get_from_archive, get_data
from openlibrary.catalog.marc.fast_parse import get_subfield_values, get_first_tag, get_tag_lines, get_subfields, BadDictionary
from openlibrary.catalog.utils.query import query_iter, set_staging, query
from openlibrary.catalog.utils import mk_norm
from openlibrary.catalog.read_rc import read_rc
from collections import defaultdict
from pprint import pprint, pformat
from openlibrary.catalog.utils.edit import fix_edition
from openlibrary.catalog.importer.db_read import get_mc
import urllib2
from openlibrary.api import OpenLibrary, Reference
from lxml import etree
from time import sleep, time

rc = read_rc()

ol = OpenLibrary("http://openlibrary.org")
ol.login('WorkBot', rc['WorkBot'])

def write_log(cat, key, title):
    print >> fh_log, (("%.2f" % time()), cat, key, title)
    fh_log.flush()

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon|etc)\.$')

re_ia_marc = re.compile('^(?:.*/)?([^/]+)_(marc\.xml|meta\.mrc)(:0:\d+)?$')

ns = '{http://www.loc.gov/MARC21/slim}'
ns_leader = ns + 'leader'
ns_data = ns + 'datafield'

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

#set_staging(True)

# sample title: The Dollar Hen (Illustrated Edition) (Dodo Press)
re_parens = re.compile('^(.*?)(?: \(.+ (?:Edition|Press)\))+$')

def key_int(key):
    # extract the number from a key like /a/OL1234A
    return int(web.numify(key))

def update_work_edition(ekey, wkey, use):
    print (ekey, wkey, use)
    e = ol.get(ekey)
    works = []
    for w in e['works']:
        if w == wkey:
            if use not in works:
                works.append(Reference(use))
        else:
            if w not in works:
                works.append(w)

    if e['works'] == works:
        return
    print 'before:', e['works']
    print 'after:', works
    e['works'] = works
    print ol.save(e['key'], e, 'remove duplicate work page')

def top_rev_wt(d):
    d_sorted = sorted(d.keys(), cmp=lambda i, j: cmp(d[j], d[i]) or cmp(len(j), len(i)))
    return d_sorted[0]

def books_query(akey): # live version
    q = {
        'type':'/type/edition',
        'authors': akey,
        'source_records': None,
        'title': None,
        'work_title': None,
        'languages': None,
        'title_prefix': None,
        'subtitle': None,
    }
    return query_iter(q)

def freq_dict_top(d):
    return sorted(d.keys(), reverse=True, key=lambda i:d[i])[0]

def get_marc_src(e):
    mc = get_mc(e['key'])
    if mc and mc.startswith('amazon:'):
        mc = None
    if mc and mc.startswith('ia:'):
        yield 'ia', mc[3:]
    elif mc:
        m = re_ia_marc.match(mc)
        if m:
            #print 'IA marc match:', m.group(1)
            yield 'ia', m.group(1)
        else:
            yield 'marc', mc
    source_records = e.get('source_records', [])
    if not source_records:
        return
    for src in source_records:
        if src.startswith('ia:'):
            if not mc or src != mc:
                yield 'ia', src[3:]
            continue
        if src.startswith('marc:'):
            if not mc or src != 'marc:' + mc:
                yield 'marc', src[5:]
            continue

def get_ia_work_title(ia):
    url = 'http://www.archive.org/download/' + ia + '/' + ia + '_marc.xml'
    try:
        root = etree.parse(urllib2.urlopen(url)).getroot()
    except KeyboardInterrupt:
        raise
    except:
        #print 'bad XML', ia
        #print url
        return
    #print etree.tostring(root)
    e = root.find(ns_data + "[@tag='240']")
    if e is None:
        return
    #print e.tag
    wt = ' '.join(s.text for s in e if s.attrib['code'] == 'a' and s.text)
    return wt

def get_work_title(e):
    # use first work title we find in source MARC records
    wt = None
    for src_type, src in get_marc_src(e):
        if src_type == 'ia':
            wt = get_ia_work_title(src)
            if wt:
                break
            continue
        assert src_type == 'marc'
        data = None
        #print 'get from archive:', src
        try:
            data = get_data(src)
        except ValueError:
            print 'bad record source:', src
            print 'http://openlibrary.org' + e['key']
            continue
        except urllib2.HTTPError, error:
            print 'HTTP error:', error.code, error.msg
            print e['key']
        if not data:
            continue
        try:
            line = get_first_tag(data, set(['240']))
        except BadDictionary:
            print 'bad dictionary:', src
            print 'http://openlibrary.org' + e['key']
            continue
        if line:
            wt = ' '.join(get_subfield_values(line, ['a'])).strip('. ')
            break
    if wt:
        return wt
    if not e.get('work_titles', []):
        return
    print 'work title in MARC, but not in OL'
    print 'http://openlibrary.org' + e['key']
    return e['work_titles'][0]

def get_books(akey, query):
    for e in query:
        if not e.get('title', None):
            continue
#        if len(e.get('authors', [])) != 1:
#            continue
        if 'title_prefix' in e and e['title_prefix']:
            prefix = e['title_prefix']
            if prefix[-1] != ' ':
                prefix += ' '
            title = prefix + e['title']
        else:
            title = e['title']

        title = title.strip(' ')
        if has_dot(title):
            title = title[:-1]
        if title.strip('. ') in ['Publications', 'Works', 'Report', \
                'Letters', 'Calendar', 'Bulletin', 'Plays', 'Sermons', 'Correspondence']:
            continue

        m = re_parens.match(title)
        if m:
            title = m.group(1)

        n = mk_norm(title)

        book = {
            'title': title,
            'norm_title': n,
            'key': e['key'],
        }

        lang = e.get('languages', [])
        if lang:
            book['lang'] = [l['key'][3:] for l in lang]

        if e.get('table_of_contents', None):
            if isinstance(e['table_of_contents'][0], basestring):
                book['table_of_contents'] = e['table_of_contents']
            else:
                assert isinstance(e['table_of_contents'][0], dict)
                if e['table_of_contents'][0]['type'] == '/type/text':
                    book['table_of_contents'] = [i['value'] for i in e['table_of_contents']]

        wt = get_work_title(e)
        if not wt:
            yield book
            continue
        if wt in ('Works', 'Selections'):
            yield book
            continue
        n_wt = mk_norm(wt)
        book['work_title'] = wt
        book['norm_wt'] = n_wt
        yield book

def build_work_title_map(equiv, norm_titles):
    # map of book titles to work titles
    title_to_work_title = defaultdict(set)
    for (norm_title, norm_wt), v in equiv.items():
        if v != 1:
            title_to_work_title[norm_title].add(norm_wt)

    title_map = {}
    for title, v in title_to_work_title.items():
        if len(v) == 1:
            title_map[title] = list(v)[0]
            continue
        most_common_title = max(v, key=lambda i:norm_titles[i])
        if title != most_common_title:
            title_map[title] = most_common_title
        for i in v:
            if i != most_common_title:
                title_map[i] = most_common_title
    return title_map

def find_works(akey, book_iter):
    equiv = defaultdict(int) # title and work title pairs
    norm_titles = defaultdict(int) # frequency of titles
    books_by_key = {}
    books = []
    rev_wt = defaultdict(lambda: defaultdict(int))

    for book in book_iter:
        if 'norm_wt' in book:
            pair = (book['norm_title'], book['norm_wt'])
            equiv[pair] += 1
            rev_wt[book['norm_wt']][book['work_title']] +=1
        norm_titles[book['norm_title']] += 1
        books_by_key[book['key']] = book
        books.append(book)

    title_map = build_work_title_map(equiv, norm_titles)

    works = defaultdict(lambda: defaultdict(list))
    work_titles = defaultdict(list)
    for b in books:
        if 'eng' not in b.get('lang', []) and 'norm_wt' in b:
            work_titles[b['norm_wt']].append(b['key'])
            continue
        n = b['norm_title']
        title = b['title']
        if n in title_map:
            n = title_map[n]
            title = top_rev_wt(rev_wt[n])
        works[n][title].append(b['key'])

    works = sorted([(sum(map(len, w.values() + [work_titles[n]])), n, w) for n, w in works.items()])

    for work_count, norm, w in works:
#        if work_count < 2:
#            continue
        first = sorted(w.items(), reverse=True, key=lambda i:len(i[1]))[0][0]
        titles = defaultdict(int)
        for key_list in w.values():
            for ekey in key_list:
                b = books_by_key[ekey]
                title = b['title']
                titles[title] += 1
        keys = work_titles[norm]
        for values in w.values():
            keys += values
        assert work_count == len(keys)
        title = max(titles.keys(), key=lambda i:titles[i])
        toc = [(k, books_by_key[k].get('table_of_contents', None)) for k in keys]
        yield {'title': first, 'editions': keys, 'toc': dict((k, v) for k, v in toc if v)}

def print_works(works):
    for w in works:
        print len(w['editions']), w['title']

def toc_items(toc_list):
    return [{'title': unicode(item), 'type': Reference('/type/toc_item')} for item in toc_list] 

def add_works(works):
    q = []
    for w in works:
        cur = {
            'authors': [{'author': Reference(w['author'])}],
            'type': '/type/work',
            'title': w['title']
        }
        if 'subjects' in w:
            cur['subjects'] = w['subjects']
        q.append(cur)
    try:
        return ol.new(q, comment='create work page')
    except:
        print q
        raise

def add_work(akey, w):
    q = {
        'authors': [{'author': Reference(akey)}],
        'type': '/type/work',
        'title': w['title']
    }
    try:
        wkey = ol.new(q, comment='create work page')
    except:
        print q
        raise
    write_log('work', wkey, w['title'])
    assert isinstance(wkey, basestring)
    for ekey in w['editions']:
        e = ol.get(ekey)
        fix_edition(ekey, e, ol)
        #assert 'works' not in e
        write_log('edition', ekey, e.get('title', 'title missing'))
        e['works'] = [Reference(wkey)]
        yield e

def save_editions(queue):
    print 'saving'
    try:
        print ol.save_many(queue, 'add edition to work page')
    except:
        print 'ol.save_many() failed, trying again in 30 seconds'
        sleep(30)
        print ol.save_many(queue, 'add edition to work page')
    print 'saved'

def merge_works(work_keys):
    use = "/works/OL%dW" % min(key_int(w) for w in work_keys)
    for wkey in work_keys:
        if wkey == use:
            continue
        w_query = {'type':'/type/edition', 'works':wkey, 'limit':False}
        for e in ol.query(w_query): # returns strings?
            print e
            update_work_edition(e, wkey, use)
        w = ol.get(wkey)
        assert w['type'] == '/type/work'
        w['type'] = '/type/redirect'
        w['location'] = use
        print ol.save(wkey, w, 'delete duplicate work page')

def update_edition(ekey, wkey):
    e = ol.get(ekey)
    fix_edition(ekey, e, ol)
    write_log('edition', ekey, e.get('title', 'title missing'))
    if e.get('works', []):
        assert len(e['works']) == 1
        if e['works'][0] != wkey:
            print 'e:', e
            print 'wkey:', wkey
            print 'ekey:', ekey
            print 'e["works"]:', e['works']
            #merge_works([e['works'][0], wkey])
        #assert e['works'][0] == wkey
        return None
    e['works'] = [Reference(wkey)]
    return e

def run_queue(queue):
    work_keys = add_works(queue)
    for w, wkey in zip(queue, work_keys):
        w['key'] = wkey
        write_log('work', wkey, w['title'])
        for ekey in w['editions']:
            e = update_edition(ekey, wkey)
            if e:
                yield e

def get_work_key(title, akey):
    q = {
        'type': '/type/work',
        'title': title,
        'authors': None,
    }
    matches = [w for w in ol.query(q) if any(a['author'] == akey for a in w['authors'])]
    if not matches:
        return None
    if len(matches) != 1:
        print 'time to fix duplicate works'
        print `title`
        print 'http://openlibrary.org' + akey
        print matches
    assert len(matches) == 1
    return matches[0]['key']

def by_authors():
    skip = '/a/OL25755A'
    q = { 'type':'/type/author', 'name': None }
    for a in query_iter(q):
        akey = a['key']
        if skip:
            if akey == skip:
                skip = None
            else:
                continue
        write_log('author', akey, a.get('name', 'name missing'))

        works = find_works(akey, get_books(akey, books_query(akey)))
        print akey, `a['name']`

        for w in works:
            w['author'] = akey
            wkey = get_work_key(w['title'], akey)
            if wkey:
                w['key'] = wkey
            yield w

if __name__ == '__main__':
    fh_log = open('/1/edward/logs/WorkBot', 'a')
    edition_queue = []
    work_queue = []

    for w in by_authors():
        if 'key' in w:
            for ekey in w['editions']:
                e = update_edition(ekey, w['key'])
                if e:
                    edition_queue.append(e)
            continue

        work_queue.append(w)
        if len(work_queue) > 1000:
            for e in run_queue(work_queue):
                print e['key'], `e['title']`
                edition_queue.append(e)
                if len(edition_queue) > 1000:
                    save_editions(edition_queue)
                    edition_queue = []
                    sleep(5)
            work_queue = []

    print 'almost finished'
    for e in run_queue(work_queue):
        edition_queue.append(e)
    save_editions(edition_queue)
    print 'finished'

    fh_log.close()

########NEW FILE########
__FILENAME__ = load_to_staging
import sys
sys.path.remove('/usr/local/lib/python2.5/site-packages/web.py-0.23-py2.5.egg')
from staging_save import Infogami
from catalog.read_rc import read_rc
import catalog.importer.db_read as db_read
import re, sys, codecs

db_read.set_staging(True)

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

rc = read_rc()
infogami = Infogami()
infogami.login('edward', rc['edward'])

for line in open('works_for_staging'):
    work_key, title, authors, editions = eval(line)
    if not all(db_read.withKey('/a/' + a) for a in authors):
        continue
    work = db_read.withKey(work_key)
    print work_key
    if work:
        continue
    if not work:
        q = {
            'create': 'unless_exists',
            'type': { 'key': '/type/work' },
            'key': work_key,
            'title': title,
            'authors': [{'key': '/a/' + a} for a in authors],
        }
        ret = infogami.write(q, comment='create work')
        print ret
    for edition_key in editions:
        edition = db_read.withKey(edition_key)
        if not edition: continue
        if 'works' in edition: continue
        q = {
            'key': edition_key,
            'works': { 'connect': 'update_list', 'value': [{'key': work_key}]}
        }
        ret = infogami.write(q, comment='add work to edition')
        print edition_key, ret
        assert ret['result']['updated']

########NEW FILE########
__FILENAME__ = sample_marc
from catalog.marc.all import iter_marc
import re

# random authors and subjects
terms = [
    'rowling', 'harry potter', 'shakespeare', 'hamlet', 'twain', 'darwin', 
    'sagan', 'huckleberry finn', 'tom sawyer', 'verne', 'waiting for godot', 
    'beckett', 'churchill', 'darwin', 'dickens', 'doyle', 'leonardo',
    'da vinci',
]

re_terms = re.compile('(' + '|'.join(terms) + ')', re.I)

out = open('/1/pharos/edward/sample_marc2', 'w')
for rec_no, pos, loc, data in iter_marc():
    if re_terms.search(data):
        print >> out, (loc, data)
out.close()

########NEW FILE########
__FILENAME__ = tests
from collections import defaultdict
import re
import catalog.merge.normalize as merge

def freq_dict_top(d):
    return sorted(d.keys(), reverse=True, key=lambda i:d[i])[0]

re_brackets = re.compile('^(.*)\[.*?\]$')
re_parens = re.compile('^(.*?)(?: \(.+ (?:Edition|Press)\))+$')

def mk_norm(title):
    m = re_brackets.match(title)
    if m:
        title = m.group(1)
    norm = merge.normalize(title).strip(' ')
    norm = norm.replace(' and ', ' ')
    if norm.startswith('the '):
        norm = norm[4:]
    elif norm.startswith('a '):
        norm = norm[2:]
    return norm.replace('-', '').replace(' ', '')

def build_work_title_map(equiv, norm_titles):
    title_to_work_title = defaultdict(set)
    for (norm_title, norm_wt), v in equiv.items():
        if v != 1:
            title_to_work_title[norm_title].add(norm_wt)

    title_map = {}
    for title, v in title_to_work_title.items():
        if len(v) == 1:
            title_map[title] = list(v)[0]
            continue
        most_common_title = max(v, key=lambda i:norm_titles[i])
        if title != most_common_title:
            title_map[title] = most_common_title
        for i in v:
            if i != most_common_title:
                title_map[i] = most_common_title
    return title_map


milo_m_hastings = [
    {'lang': ['eng'], 'key': '/b/OL7009753M', 'title': 'The dollar hen'},
    {'lang': ['eng'], 'key': '/b/OL9563276M', 'title': 'The Dollar Hen (Large Print Edition)'},
    {'lang': ['eng'], 'key': '/b/OL9636071M', 'title': 'The Dollar Hen'},
    {'lang': ['eng'], 'key': '/b/OL15083244M', 'title': 'The dollar hen'},
    {'lang': ['eng'], 'key': '/b/OL8566971M', 'title': 'The Dollar Hen'},
    {'lang': ['eng'], 'key': '/b/OL9353753M', 'title': 'City of Endless Night'},
    {'lang': ['eng'], 'key': '/b/OL9462083M', 'title': 'City of Endless Night (Large Print Edition)'},
    {'lang': ['eng'], 'key': '/b/OL9642528M', 'title': 'The Dollar Hen'},
    {'lang': ['eng'], 'key': '/b/OL9736536M', 'title': 'The Dollar Hen'},
    {'lang': ['eng'], 'key': '/b/OL9735362M', 'title': 'The Dollar Hen (Illustrated Edition) (Dodo Press)'},
    {'lang': ['eng'], 'key': '/b/OL9800490M', 'title': 'The Dollar Hen'},
    {'lang': ['eng'], 'key': '/b/OL11676559M', 'title': 'City of Endless Night (Dodo Press)'},
    {'lang': ['eng'], 'key': '/b/OL11752220M', 'title': 'The Dollar Hen'},
    {'lang': ['eng'], 'key': '/b/OL11985500M', 'title': 'The Dollar Hen'},
    {'lang': ['eng'], 'key': '/b/OL11985503M', 'title': 'The Dollar Hen'}
]

aaron_bancroft = [ # /a/OL17005A
    {'lang': ['eng'], 'key': '/b/OL595471M', 'title': 'A sermon preached before His Excellency Caleb Strong, Esq., Governour, the Honourable the Council, Senate, and House of Representatives of the commonwealth of Massachusetts, May 27, 1801'},
    {'lang': ['eng'], 'key': '/b/OL1247387M', 'title': 'A discourse delivered before the convention of Congregational ministers of Massachusetts, at their annual meeting in Boston, June 1, 1820'},
    {'lang': ['eng'], 'key': '/b/OL6472976M', 'title': 'The importance of a religious education illustrated and enforced'},
    {'lang': ['eng'], 'key': '/b/OL6919451M', 'title': 'A discourse delivered at Windsor, in the state of Vermont, on the 23rd of June, MDCCXC'},
    {'lang': ['eng'], 'key': '/b/OL6950265M', 'title': 'A sermon delivered in Worcester, January 31, 1836'},
    {'key': '/b/OL7048038M', 'title': 'Sermons on those doctrines of the gospel, and on those constituent principles of the church, which Christian professors have made the subject of controversy. ..'},
    {'key': '/b/OL7197334M', 'title': 'The life of George Washington ....'},
    {'lang': ['eng'], 'key': '/b/OL14572992M', 'title': 'A sermon, delivered at Worcester, on the eleventh of June, 1793'},
    {'lang': ['eng'], 'key': '/b/OL14588026M', 'title': 'An eulogy on the character of the late Gen. George Washington.'},
    {'lang': ['eng'], 'key': '/b/OL14601446M', 'title': 'A sermon, delivered at Brimfield, on the 20th of June, 1798'},
    {'lang': ['eng'], 'key': '/b/OL14608347M', 'title': 'The importance of a religious education illustrated and enforced.'},
    {'lang': ['eng'], 'key': '/b/OL14702050M', 'title': 'The nature and worth of Christian liberty'},
    {'lang': ['eng'], 'key': '/b/OL14981988M', 'title': 'A vindication of the result of the late Mutual Council convened in Princeton'},
    {'lang': ['eng'], 'key': '/b/OL14992328M', 'title': 'An essay on the life of George Washington'},
    {'lang': ['eng'], 'key': '/b/OL15054440M', 'title': 'Importance of education'},
    {'lang': ['eng'], 'key': '/b/OL15070888M', 'title': 'The leaf an emblem of human life'},
    {'lang': ['eng'], 'key': '/b/OL15075529M', 'title': 'The world passeth away, but the children of God abide forever'},
    {'lang': ['eng'], 'key': '/b/OL15085786M', 'title': 'The doctrine of immortality'},
    {'lang': ['eng'], 'key': '/b/OL15093560M', 'title': 'The comparative advantages of the ministerial profession'},
    {'lang': ['eng'], 'key': '/b/OL15115706M', 'title': 'The duties enjoined by the Fourth commandment'},
    {'lang': ['eng'], 'key': '/b/OL15120201M', 'title': 'A discourse on conversion'},
    {'lang': ['eng'], 'key': '/b/OL15120290M', 'title': 'The nature and worth of Christian liberty'},
    {'lang': ['eng'], 'key': '/b/OL17052663M', 'title': 'An eulogy on the character of the late Gen. George Washington'},
    {'lang': ['eng'], 'key': '/b/OL17704747M', 'title': 'The doctrine of immortality'},
    {'lang': ['eng'], 'key': '/b/OL17707429M', 'title': 'Importance of education'},
    {'lang': ['eng'], 'key': '/b/OL17709244M', 'title': 'A vindication of the result of the late mutual council convened in Princeton'},
    {'lang': ['eng'], 'key': '/b/OL18776110M', 'title': 'Sermons on those doctrines of the gospel, and on those constituent principles of the church, which Christian professors have made the subject of controversy'},
    {'lang': ['eng'], 'key': '/b/OL6573411M', 'title': 'The life of George Washington, commander in chief of the American army, through the revolutionary war'},
    {'lang': ['eng'], 'key': '/b/OL15592993M', 'title': 'A discourse on conversion'},
    {'lang': ['eng'], 'key': '/b/OL17712475M', 'title': 'A discourse on conversion'},
    {'lang': ['eng'], 'key': '/b/OL6290214M', 'title': 'The life of George Washington'},
    {'lang': ['eng'], 'key': '/b/OL6571503M', 'title': 'The life of George Washington'},
    {'lang': ['eng'], 'key': '/b/OL6573412M', 'title': 'Life of George Washington'},
    {'work_title': 'Essay on the life of George Washington', 'key': '/b/OL7168113M', 'title': 'Life of George Washington, commander in chief of the American army through the revolutionary war, and the first president of the United States.'},
    {'work_title': 'Essay on the life of George Washington', 'key': '/b/OL7243025M', 'title': 'The life of George Washington, commander in chief of the American army, through the revolutionary war, and the first president of the United States'},
    {'lang': ['eng'], 'key': '/b/OL28289M', 'title': 'The life of George Washington, commander-in-chief of the American Army through the Revolutionary War and the first President of the United States'},
    {'lang': ['eng'], 'key': '/b/OL6354818M', 'title': 'The life of George Washington, commander-in-chief of the American Army through the revolutionary war, and the first president of the United States.'},
    {'key': '/b/OL7113589M', 'title': 'The life of George Washington, Commander-in-Chief of the American Army, through the Revolutionary War; and the first President of the United States.'}
]

def find_works(books):
    for book in books:
        m = re_parens.match(book['title'])
        if m:
            book['title'] = m.group(1)
        n = mk_norm(book['title'])
        book['norm_title'] = n

    books_by_key = dict((b['key'], b) for b in books)
    norm_titles = defaultdict(int)

    for book in books:
        norm_titles[book['norm_title']] += 1

    title_map = build_work_title_map({}, norm_titles)

    works = defaultdict(lambda: defaultdict(list))
    work_titles = defaultdict(list)
    for b in books:
        if 'eng' not in b.get('lang', []) and 'norm_wt' in b:
            work_titles[b['norm_wt']].append(b['key'])
            continue
        n = b['norm_title']
        title = b['title']
        if n in title_map:
            n = title_map[n]
            title = freq_dict_top(rev_wt[n])
        works[n][title].append(b['key'])

    #for k, v in works.items():
    #    print k
    #    print '  ', sum(len(i) for i in v.values()), dict(v)
    #print

    works = sorted([(sum(map(len, w.values() + [work_titles[n]])), n, w) for n, w in works.items()])

    for a, b, c in works:
        print a, b, dict(c)

find_works(milo_m_hastings)
find_works(aaron_bancroft)

########NEW FILE########
__FILENAME__ = test_find_works
# -*- coding: utf-8 -*-
from find_works import top_rev_wt, has_dot, freq_dict_top, find_works, get_books, find_works2, build_work_title_map, find_works3, find_work_sort
from openlibrary.catalog.merge.normalize import normalize

def test_has_dot():
    assert has_dot('Magic.')
    assert not has_dot('Magic')
    assert not has_dot('Magic etc.')

def test_top_rev_wt():
    input_data = {
        'aaa': 'test data',
        'aaaa': 'more test data',
        'bbbb': 'test date',
        'cc': 'some more test data',
    }
    assert top_rev_wt(input_data) == 'bbbb'

def test_freq_dict_top():
    assert freq_dict_top({'a': 0}) == 'a'
    assert freq_dict_top({'a': 3, 'b': 6, 'c': 4}) == 'b'

def test_find_works():
    works = list(find_works([]))
    assert works == []

    books = [{'title': 'Magic', 'key': '/books/OL1M'}]
    book_iter = get_books('', books, do_get_mc=False)

    books2 = list(book_iter)
    assert books2 == [{'key': '/books/OL1M', 'norm_title': 'magic', 'title': 'Magic'}]

    var = find_works2(books2)
    assert var['equiv'] == {}
    assert var['norm_titles'] == {'magic': 1}
    assert var['books_by_key'] == {'/books/OL1M': books2[0]}
    assert var['books'] == books2
    assert var['rev_wt'] == {}

    assert build_work_title_map({}, {'magic': 1}) == {}
    assert build_work_title_map({}, {'magic': 2, 'test': 0}) == {}

    works = list(find_works(books2, do_get_mc=False))
    expect = [
    { 'title': 'Magic',
        'editions': [{
        'key': '/books/OL1M',
        'norm_title': 'magic',
        'title': 'Magic'}],
    }]
    assert works == expect


    books = [
        {'title': 'Magic', 'key': '/books/OL1M'},
        {'title': 'Magic', 'key': '/books/OL2M'},
    ]
    book_iter = get_books('', books, do_get_mc=False)
    books2 = list(book_iter)

    var = find_works2(books2)
    assert var['equiv'] == {}
    assert var['norm_titles'] == {'magic': 2}
    assert var['books_by_key'] == {'/books/OL1M': books2[0], '/books/OL2M': books2[1]}
    assert var['books'] == books2
    assert var['rev_wt'] == {}

    works = list(find_works(books2, do_get_mc=False))
    expect = [
    { 'title': 'Magic',
        'editions': [
            { 'key': '/books/OL1M', 'norm_title': 'magic', 'title': 'Magic'},
            { 'key': '/books/OL2M', 'norm_title': 'magic', 'title': 'Magic'},
        ],
    }]
    assert works == expect

    magico = u'm\xe1gico'

    assert normalize(magico) == magico

    books = [
        {'title': magico, 'work_title': ['magic'], 'key': '/books/OL1M'},
        {'title': 'magic', 'key': '/books/OL2M'},
        {'title': magico, 'work_title': ['magic'], 'key': '/books/OL3M'},
        {'title': 'magic', 'key': '/books/OL4M'},
    ]
    expect_keys = sorted(e['key'] for e in books)
    book_iter = get_books('', books, do_get_mc=False)
    books2 = list(book_iter)

    expect = [
        {'key': '/books/OL1M', 'norm_title': magico, 'work_title': 'magic', 'norm_wt': 'magic', 'title': magico},
        {'key': '/books/OL2M', 'norm_title': 'magic', 'title': 'magic'},
        {'key': '/books/OL3M', 'norm_title': magico, 'work_title': 'magic', 'norm_wt': 'magic', 'title': magico},
        {'key': '/books/OL4M', 'norm_title': 'magic', 'title': 'magic'},
    ]

    assert len(books2) == 4
    for i in range(4):
        assert books2[i] == expect[i]

    var = find_works2(books2)
    assert var['equiv'] == {(magico, 'magic'): 2}
    assert var['norm_titles'] == {magico: 2, 'magic': 2}
    assert len(var['books_by_key']) == 4
    bk = var['books_by_key']
    assert bk['/books/OL1M'] == books2[0]
    assert bk['/books/OL2M'] == books2[1]
    assert bk['/books/OL3M'] == books2[2]
    assert bk['/books/OL4M'] == books2[3]
    assert var['books'] == books2
    assert var['rev_wt'] == {'magic': {'magic': 2}}

    title_map = build_work_title_map(var['equiv'], var['norm_titles'])

    assert title_map == {magico: 'magic'}

    find_works3(var)
    assert var['works'] == {'magic': {'magic': expect_keys}}
    assert var['work_titles'] == {'magic': ['/books/OL1M', '/books/OL3M']}

    sorted_works = find_work_sort(var)
    assert sorted_works == [(6, 'magic', {'magic': expect_keys})]

    works = list(find_works(books2, do_get_mc=False))
    expect = [{
        'title': u'Magic',
        'editions': [
            {'key': '/books/OL2M', 'norm_title': 'magic', 'title': 'magic'},
            {'key': '/books/OL1M', 'norm_title': u'mágico', 'norm_wt': 'magic', 'title': u'Mágico'},
        ], 
    }]

    work_count = len(works)
    assert work_count == 1
    editions = works[0]['editions']
    edition_count = len(works[0]['editions'])
    edition_keys = sorted(e['key'] for e in editions) 
    assert edition_keys == expect_keys
    assert edition_count == 4
    del works[0]['editions']
    assert works[0] == {'title': 'magic'}
    #assert works == expect



########NEW FILE########
__FILENAME__ = use_amazon
import os, re, sys, codecs, dbhash
from catalog.amazon.other_editions import find_others
from catalog.infostore import get_site
from catalog.read_rc import read_rc
from catalog.get_ia import get_data
from catalog.marc.build_record import build_record
from catalog.marc.fast_parse import get_tag_lines, get_all_subfields

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
rc = read_rc()
db = dbhash.open(rc['index_path'] + 'isbn_to_marc.dbm', 'r')

site = get_site()

def get_records_from_marc(isbn):
    if isbn not in db:
        return
#    for loc in db[isbn].split(' '):
#        data = get_data(loc)
#        print loc
#        want = ['100', '110', '111', '240', '245', '260'] + [str(i) for i in range(500,600) if i not in (505, 520)]
#        for tag, line in get_tag_lines(data, set(want)):
#            sub = list(get_all_subfields(line))
#            if tag.startswith('5'):
#                assert len(sub) == 1 and sub[0][0] == 'a'
#                note = sub[0][1]
#                if note.find('ublish') != -1 or note.find('riginal') != -1:
#                    print '  note:', note
#                continue
#            print '  ', tag, sub
#        print
    recs = [(loc, build_record(get_data(loc))) for loc in db[isbn].split(' ')]
    keys = set()
    print
    for loc, rec in recs:
        print '  ', loc
#        keys.update([k for k in rec.keys() if k.find('title') != -1 or k in ('authors', 'title', 'contributions', 'work_title')])
        keys.update(rec.keys())
    print
    for k in keys:
        print k
        for loc, rec in recs:
            print "  ", rec.get(k, '###')
        print
    print

dir = sys.argv[1]
for filename in os.listdir(dir):
    if not filename[0].isdigit():
        continue
    l = find_others(filename, dir)
    if not l:
        continue
    print filename
    for k in site.things({'isbn_10': filename, 'type': '/type/edition'}):
        t = site.withKey(k)
        num = len(t.isbn_10)
        if num == 1:
            num = ''
        print '  OL:', k, t.title, num
        get_records_from_marc(filename)
    for asin, extra in l:
        print asin, extra
        things = site.things({'isbn_10': asin, 'type': '/type/edition'})
        if things:
            for k in things:
                t = site.withKey(k)
                num = len(t.isbn_10)
                if num == 1:
                    num = ''
                print '  OL:', k, t.title, num
        get_records_from_marc(asin)
    print "----"

########NEW FILE########
__FILENAME__ = use_amazon2
import os, re, sys, codecs, dbhash
from catalog.amazon.other_editions import find_others
from catalog.infostore import get_site
from catalog.read_rc import read_rc
from catalog.get_ia import get_data
from catalog.marc.build_record import build_record
from catalog.marc.fast_parse import get_tag_lines, get_all_subfields

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
rc = read_rc()
db = dbhash.open(rc['index_path'] + 'isbn_to_marc.dbm', 'r')

site = get_site()

dir = sys.argv[1]
for filename in os.listdir(dir):
    if not filename[0].isdigit():
        continue
    l = find_others(filename, dir)
    if len(l) < 8:
        continue
    print filename, len(l)

########NEW FILE########
__FILENAME__ = web_ui
import web, re
from time import time
from catalog.read_rc import read_rc
from catalog.infostore import get_site
#from catalog.db_read import get_things, withKey
from pprint import pprint
from catalog.amazon.other_editions import find_others
from catalog.merge.normalize import normalize

rc = read_rc()

re_translation_of = re.compile('^Translation of\b[: ]*([^\n]*?)\.?$', re.I | re.M)

site = get_site()

def isbn_link(i):
    return '<a href="http://wiki-beta.us.archive.org:8081/?isbn=%s">%s</a> (<a href="http://amazon.com/dp/%s">Amazon.com</a>)' % (i, i, i)

def ol_link(key):
    return '<a href="http://openlibrary.org%s">%s</a></td>' % (key, key)

def get_author_keys(name):
    authors = site.things({ 'type': '/type/author', 'name': name })
    if authors:
        return ','.join("'%s'" % a for a in authors)
    else:
        return None

def get_title_to_key(author):
    # get id to key mapping of all editions by author
    author_keys = get_author_keys(author)
    if not author_keys:
        return {}

    # get title to key mapping of all editions by author
    t0 = time()
    sql = "select key, value as title from thing, edition_str " \
        + "where thing.id = thing_id and key_id=3 and thing_id in (" \
        + "select thing_id from edition_ref, thing " \
        + "where edition_ref.key_id=11 and edition_ref.value = thing.id and thing.key in (" + author_keys + "))"
    print sql
    return {}
    title_to_key = {}
    for r in web.query(sql):
        t = normalize(r.title).strip('.')
        title_to_key.setdefault(t, []).append(r.key)
    return title_to_key

def search(title, author):

    title_to_key = get_title_to_key(author)
    norm_title = normalize(title).strip('.')

    if norm_title not in title_to_key:
        print 'title not found'
        return

    pool = set(title_to_key[norm_title])

    editions = []
    seen = set()
    found_titles = {}
    found_isbn = {}
    while pool:
        key = pool.pop()
        seen.add(key)
        e = site.withKey(key)
        translation_of = None
        if False and e.notes:
            m = re_translation_of.search(e.notes)
            if m:
                translation_of = m.group(1).lower()
                pool.update(k for k in title_to_key[translation_of] if k not in seen)
                found_titles.setdefault(translation_of, []).append(key)
        if False and e.isbn_10:
            for i in e.isbn_10:
                found_isbn.setdefault(i, []).append(key)
            join_isbn = ', '.join(map(isbn_link, e.isbn_10))
        else:
            join_isbn = ''
        rec = {
            'key': key,
            'publish_date': e.publish_date,
            'publishers': ', '.join(p.encode('utf-8') for p in (e.publishers or [])),
            'isbn': join_isbn,
        }
        editions.append(rec)

        if e.work_titles:
            for t in e.work_titles:
                t=t.strip('.')
                pool.update(k for k in title_to_key.get(t.lower(), []) if k not in seen)
                found_titles.setdefault(t, []).append(key)
        if e.other_titles:
            for t in e.other_titles:
                t=t.strip('.')
                pool.update(k for k in title_to_key.get(t.lower(), []) if k not in seen)
                found_titles.setdefault(t, []).append(key)

    print '<table>'
    for e in sorted(editions, key=lambda e: e['publish_date'] and e['publish_date'][-4:]):
        print '<tr>'
        print '<td>', ol_link(e['key'])
        print '<td>', e['publish_date'], '</td><td>', e['publishers'], '</td>'
        print '<td>', e['isbn'], '</td>'
        print '</tr>'
    print '</table>'

    if found_titles:
        print '<h2>Other titles</h2>'
        print '<ul>'
        for k, v in found_titles.iteritems():
            if k == title:
                continue
            print '<li><a href="/?title=%s&author=%s">%s</a>' % (k, author, k),
            print 'from', ', '.join(ol_link(i) for i in v)
        print '</ul>'

    extra_isbn = {}
    for k, v in found_isbn.iteritems():
        for isbn, note in find_others(k, rc['amazon_other_editions']):
            if note.lower().find('audio') != -1:
                continue
            if isbn not in found_isbn:
                extra_isbn.setdefault(isbn, []).extend(v)

    if extra_isbn:
        print '<h2>Other ISBN</h2>'
        print '<ul>'
        for k in sorted(extra_isbn):
            print '<li>', isbn_link(k),
            print 'from', ', '.join(ol_link(i) for i in extra_isbn[k])
        print '</ul>'

urls = (
    '/', 'index'
)

def textbox(name, input):
    if name in input:
        return '<input type="text" name="%s" value="%s" size="60">' % (name, web.htmlquote(input[name]))
    else:
        return '<input type="text" name="%s" size="60">' % (name)

class index:
    def GET(self):
        web.header('Content-Type','text/html; charset=utf-8', unique=True)
        input = web.input()
        title = None
        author = None
        if 'title' in input:
            title = input.title
        if 'author' in input:
            author = input.author
        html_title = 'Work finder'
        print "<html>\n<head>\n<title>%s</title>" % html_title
        print '''
<style>
th { text-align: left }
td { padding: 5px; background: #eee }
</style>'''

        print '</head><body><a name="top">'

        print "<body><html>"
        print '<form name="main" method="get">'
        print '<table><tr><td align="right">Title</td>',
        print '<td>', textbox('title', input), '</td></tr>'
        print '<tr><td align="right">Author</td>'
        print '<td>', textbox('author', input), '</td></tr>'
        print '<tr><td></td><td><input type="submit" value="find"></td></tr>'
        print '</table>'
        if title and author:
            search(title, author)
        print '</form>'

if __name__ == "__main__": web.run(urls, globals(), web.reloader)

########NEW FILE########
__FILENAME__ = code
"""Main entry point for openlibrary app.

Loaded from Infogami plugin mechanism.
"""
import sys, os
import logging, logging.config

from infogami.utils import template, macro, i18n
import infogami

old_plugins = ["openlibrary", "search", "worksearch", "inside", "books", "admin", "upstream", "importapi", "recaptcha"]

def setup():
    setup_logging()

    logger = logging.getLogger("openlibrary")
    logger.info("Application init")

    for p in old_plugins:
        logger.info("loading plugin %s", p)
        modname = "openlibrary.plugins.%s.code" % p
        path = "openlibrary/plugins/" + p
        template.load_templates(path, lazy=True)
        macro.load_macros(path, lazy=True)
        i18n.load_strings(path)
        __import__(modname, globals(), locals(), ['plugins'])

    load_views()

    # load actions
    from . import actions

    logger.info("loading complete.")

def setup_logging():
    """Reads the logging configuration from config file and configures logger.
    """
    try:
        logconfig = infogami.config.get("logging_config_file")
        if logconfig and os.path.exists(logconfig):
            logging.config.fileConfig(logconfig, disable_existing_loggers=False)
    except Exception, e:
        print >> sys.stderr, "Unable to set logging configuration:", str(e)
        raise

def load_views():
    """Registers all views by loading all view modules.
    """
    from .views import showmarc
    from .views import loanstats

setup()

########NEW FILE########
__FILENAME__ = config
"""Legacy config file. 

This will be removed soon.
"""

import yaml

runtime_config = {}

def load(config_file):
    global runtime_config
    runtime_config = yaml.load(open(config_file))

########NEW FILE########
__FILENAME__ = conftest
"""py.test configutation for openlibrary
"""
import glob

import web

from infogami.infobase.tests.pytest_wildcard import pytest_funcarg__wildcard
from infogami.utils import template
from infogami.utils.view import render_template
from openlibrary.i18n import gettext
from openlibrary.core import helpers

pytest_plugins = ["pytest_unittest"]

from openlibrary.mocks.mock_infobase import pytest_funcarg__mock_site
from openlibrary.mocks.mock_ia import pytest_funcarg__mock_ia
from openlibrary.mocks.mock_memcache import pytest_funcarg__mock_memcache
from openlibrary.mocks.mock_ol import pytest_funcarg__ol

def pytest_funcarg__render_template(request):
    """Utility to test templates.
    """    
    template.load_templates("openlibrary/plugins/openlibrary")
    template.load_templates("openlibrary/plugins/upstream")
    
    #TODO: call setup on upstream and openlibrary plugins to 
    # load all globals.
    web.template.Template.globals["_"] = gettext
    web.template.Template.globals.update(helpers.helpers)

    web.ctx.env = web.storage()
    web.ctx.headers = []
    web.ctx.lang = "en"

    # ol_infobase.init_plugin call is failing when trying to import plugins.openlibrary.code.
    # monkeypatch to avoid that.
    from openlibrary.plugins import ol_infobase
    
    init_plugin = ol_infobase.init_plugin
    ol_infobase.init_plugin = lambda: None
    def undo():
        ol_infobase.init_plugin = init_plugin
    request.addfinalizer(undo)    
    
    from openlibrary.plugins.openlibrary import code
    web.config.db_parameters = dict()
    code.setup_template_globals()

    def finalizer():
        template.disktemplates.clear()
        web.ctx.clear()

    request.addfinalizer(finalizer)
    
    def render(name, *a, **kw):
        as_string = kw.pop("as_string", True)
        d = render_template(name, *a, **kw)
        if as_string:
            return unicode(d)
        else:
            return d
    return render

########NEW FILE########
__FILENAME__ = ab
"""A/B testing for Open Library.

Setup:

The sixpack host and alternatives for each experiment are specified in the config file.

Sample config:

sixpack_url: http://localhost:1234/
ab:
    borrow-layout:
        - one-row
        - two-rows
"""
import web
from sixpack.sixpack import Session
import logging
from infogami import config

logger = logging.getLogger("openlibrary.ab")

def get_session():
    if "sixpack_session" not in web.ctx:
        cookies = web.cookies(sixpack_id=None)
        session = Session(client_id=cookies.sixpack_id, options=_get_sixpack_options())
        if session.client_id != cookies.sixpack_id:
            web.setcookie('sixpack_id', session.client_id)
        web.ctx.sixpack_session = session
    return web.ctx.sixpack_session

def _get_sixpack_options():
    host = config.get('sixpack_url')
    return {
        'host': host
    }

def get_ab_value(testname):
    cache = web.ctx.setdefault("sixpack_cache", {})
    if testname not in cache:
        cache[testname] = participate(testname)
    return cache[testname]

def participate(testname, alternatives=None):
    if alternatives is None:
        ab_config = config.get("ab", {})
        alternatives = ab_config.get(testname)
    if alternatives:
        force = web.input(_method="GET").get("sixpack-force-" + testname)
        response = get_session().participate(testname, alternatives, force=force)
        logger.info("participate %s %s -> %s", testname, alternatives, response)
        value = response['alternative']['name']
    else:
        # default value when no alternatives are provided in config.
        value = 'control'
    return value

def convert(testname):
    logger.info("convert %s", testname)
    return get_session().convert(testname)
########NEW FILE########
__FILENAME__ = admin
"""Admin functionality.
"""

import calendar
import datetime
import couchdb

from infogami import config
from infogami.utils import stats

from . import cache

class Stats:
    def __init__(self, docs, key, total_key):
        self.key = key
        self.docs = docs
        try:
            self.latest = docs[-1].get(key, 0)
        except IndexError:
            self.latest = 0

        try:
            self.previous = docs[-2].get(key, 0)
        except IndexError:
            self.previous = 0

        try:
            # Last available total count
            self.total = (x for x in reversed(docs) if total_key in x).next()[total_key]
        except (KeyError, StopIteration):
            self.total = ""
                
    def get_counts(self, ndays = 28, times = False):
        """Returns the stats for last n days as an array useful for
        plotting. i.e. an array of [x, y] tuples where y is the value
        and `x` the x coordinate.

        If times is True, the x coordinate in the tuple will be
        timestamps for the day.
        """
        def _convert_to_milli_timestamp(d):
            """Uses the `_id` of the document `d` to create a UNIX
            timestamp and coverts it to milliseconds"""
            t = datetime.datetime.strptime(d, "counts-%Y-%m-%d")
            return calendar.timegm(t.timetuple()) * 1000

        if times:
            return [[_convert_to_milli_timestamp(x.id), x.get(self.key,0)] for x in self.docs[-ndays:]]
        else:
            return zip(range(0, ndays*5, 5),
                       (x.get(self.key, 0) for x in self.docs[-ndays:])) # The *5 and 5 are for the bar widths
        
    def get_summary(self, ndays = 28):
        """Returns the summary of counts for past n days.
        
        Summary can be either sum or average depending on the type of stats.
        This is used to find counts for last 7 days and last 28 days.
        """
        return sum(x[1] for x in self.get_counts(ndays))

@cache.memoize(engine="memcache", key="admin._get_count_docs", expires=5*60)
def _get_count_docs(ndays):
    """Returns the count docs from admin couchdb database.
    
    This function is memoized to avoid accessing couchdb for every request.
    """
    admin_db = couchdb.Database(config.admin.counts_db)
    end      = datetime.datetime.now().strftime("counts-%Y-%m-%d")
    start    = (datetime.datetime.now() - datetime.timedelta(days = ndays)).strftime("counts-%Y-%m-%d")
        
    stats.begin("couchdb")
    docs = [x.doc for x in admin_db.view("_all_docs",
                                         startkey_docid = start,
                                         endkey_docid   = end,
                                         include_docs = True)]
    stats.end()
    return docs

def get_stats(ndays = 30):
    """Returns the stats for the past `ndays`"""
    docs = [couchdb.Document(doc) for doc in _get_count_docs(ndays)]
    retval = dict(human_edits = Stats(docs, "human_edits", "human_edits"),
                  bot_edits   = Stats(docs, "bot_edits", "bot_edits"),
                  lists       = Stats(docs, "lists", "total_lists"),
                  visitors    = Stats(docs, "visitors", "visitors"),
                  loans       = Stats(docs, "loans", "loans"),
                  members     = Stats(docs, "members", "total_members"),
                  works       = Stats(docs, "works", "total_works"),
                  editions    = Stats(docs, "editions", "total_editions"),
                  ebooks      = Stats(docs, "ebooks", "total_ebooks"),
                  covers      = Stats(docs, "covers", "total_covers"),
                  authors     = Stats(docs, "authors", "total_authors"),
                  subjects    = Stats(docs, "subjects", "total_subjects"))
    return retval
########NEW FILE########
__FILENAME__ = cache
"""Caching utilities.
"""
import random
import string
import time
import threading
import functools

import memcache
import simplejson
import web

from infogami import config
from infogami.utils import stats

from openlibrary.utils import olmemcache

__all__ = [
    "cached_property",
    "Cache", "MemoryCache", "MemcacheCache", "RequestCache",
    "memoize", "memcache_memoize"
]

class memcache_memoize:
    """Memoizes a function, caching its return values in memcached for each input.
    
    After the timeout, a new thread is spawned to update the value and the old
    value is used while the update is in progress.
    
    This expects that both the args and return value are json encodable.
    
    This uses the memcache servers specified in the configuration.
    
    :param f: function to be memozied
    :param key_prefix: key prefix used in memcache to store memoized results. A random value will be used if not specified.
    :param servers: list of  memcached servers, each specified as "ip:port"
    :param timeout: timeout in seconds after which the return value must be updated
    """
    def __init__(self, f, key_prefix=None, timeout=60):
        """Creates a new memoized function for ``f``.
        """
        self.f = f
        self.key_prefix = key_prefix or self._generate_key_prefix()
        self.timeout = timeout
        
        self._memcache = None
        
        self.stats = web.storage(
            calls=0,
            hits=0,
            updates=0,
            async_updates=0
        )
        self.active_threads = {}
    
    def _get_memcache(self):
        if self._memcache is None:
            servers = config.get("memcache_servers")
            if servers:
                self._memcache = memcache.Client(servers)
            else:
                web.debug("Could not find memcache_servers in the configuration. Used dummy memcache.")
                import mockcache
                self._memcache = mockcache.Client()
                
        return self._memcache
        
    memcache = property(_get_memcache)
        
    def _generate_key_prefix(self):
        try:
            prefix = self.f.__name__ + "_"
        except (AttributeError, TypeError):
            prefix = ""
        
        return prefix + self._random_string(10)
        
    def _random_string(self, n):
        chars = string.letters + string.digits
        return "".join(random.choice(chars) for i in range(n))
        
    def __call__(self, *args, **kw):
        """Memoized function call.
        
        Returns the cached value when avaiable. Computes and adds the result
        to memcache when not available. Updates asynchronously after timeout.
        """
        _cache = kw.pop("_cache", None)
        if _cache == "delete":
            self.memcache_delete(args, kw)
            return None

        self.stats.calls += 1
        
        value_time = self.memcache_get(args, kw)
        
        if value_time is None:
            self.stats.updates += 1
            value, t = self.update(*args, **kw)
        else:
            self.stats.hits += 1
            
            value, t = value_time
            if t + self.timeout < time.time():
                self.stats.async_updates += 1
                self.update_async(*args, **kw)
        
        return value
        
    def update_async(self, *args, **kw):
        """Starts the update process asynchronously.
        """
        t = threading.Thread(target=self._update_async_worker, args=args, kwargs=kw)
        self.active_threads[t.getName()] = t
        t.start()
        
    def _update_async_worker(self, *args, **kw):
        key = self.compute_key(args, kw) + "/flag"
        
        if not self.memcache.add(key, "true"):
            # already somebody else is computing this value.
            return
        
        try:
            self.update(*args, **kw)            
        finally:
            # Remove current thread from active threads
            self.active_threads.pop(threading.currentThread().getName(), None)
            
            # remove the flag
            self.memcache.delete(key)

    def update(self, *args, **kw):
        """Computes the value and adds it to memcache.

        Returns the computed value.
        """
        value = self.f(*args, **kw)
        t = time.time()
        
        self.memcache_set(args, kw, value, t)
        return value, t
        
    def join_threads(self):
        """Waits for all active threads to finish.
        
        Used only in testing.
        """
        for name, thread in self.active_threads.items():
            thread.join()
                
    def encode_args(self, args, kw={}):
        """Encodes arguments to construct the memcache key.
        """
        # strip [ and ] from key
        a = self.json_encode(list(args))[1:-1] 
        
        if kw:
            return a + "-" + self.json_encode(kw)
        else:
            return a
        
    def compute_key(self, args, kw):
        """Computes memcache key for storing result of function call with given arguments.
        """
        key = self.key_prefix + "-" + self.encode_args(args, kw)
        return key.replace(" ", "_") #XXX: temporary fix to handle spaces in the arguments
        
    def json_encode(self, value):
        """simplejson.dumps without extra spaces.
        
        memcache doesn't like spaces in the key.
        """
        return simplejson.dumps(value, separators=(",", ":"))
        
    def json_decode(self, json):
        return simplejson.loads(json)
                
    def memcache_set(self, args, kw, value, time):
        """Adds value and time to memcache. Key is computed from the arguments.
        """
        key = self.compute_key(args, kw)
        json = self.json_encode([value, time])

        stats.begin("memcache.set", key=key)
        self.memcache.set(key, json)
        stats.end()
        
    def memcache_delete(self, args, kw):
        key = self.compute_key(args, kw)
        stats.begin("memcache.delete", key=key)
        self.memcache.delete(key)
        stats.end()
        
    def memcache_get(self, args, kw):
        """Reads the value from memcache. Key is computed from the arguments.
        
        Returns (value, time) when the value is available, None otherwise.
        """
        key = self.compute_key(args, kw)
        stats.begin("memcache.get", key=key)
        json = self.memcache.get(key)
        stats.end(hit=bool(json))
        
        return json and self.json_decode(json)
        
####

def cached_property(getter):
    """Decorator like `property`, but the value is computed on first call and cached.
    
    class Foo:
        
        @cached_property
        def memcache_client(self):
            ...
    """
    name = getter.__name__
    def g(self):
        if name in self.__dict__:
            return self.__dict__[name]
            
        value = getter(self)
        self.__dict__[name] = value
        return value
    return property(g)

class Cache(object):
    """Cache interface."""
    def get(self, key):
        """Returns the value for given key. Returns None if that key is not present in the cache.
        """
        raise NotImplementedError()
    
    def set(self, key, value, expires=0):
        """Sets a value in the cache. 
        If expires is non-zero, the cache may delete that entry from the cache after expiry.
        The implementation can choose to ignore the expires argument.
        """
        raise NotImplementedError()
        
    def add(self, key, value, expires=0):
        """Adds a new entry in the cache. Nothing is done if there is already an entry with the same key.
        
        Returns True if a new entry is added to the cache.
        """
        raise NotImplementedError()
        
    def delete(self, key):
        """Deletes an entry from the cache. No error is raised if there is no entry in present in the cache with that key.
        
        Returns True if the key is deleted.
        """
        raise NotImplementedError()


class MemoryCache(Cache):
    """Cache implementation in memory.
    """
    def __init__(self):
        self.d = {}
        
    def get(self, key):
        return self.d.get(key)
        
    def set(self, key, value, expires=0):
        self.d[key] = value
    
    def add(self, key, value, expires=0):
        return self.d.setdefault(key, value) is value
    
    def delete(self, key):
        return self.d.pop(key, None) is not None
        
    def clear(self):
        self.d.clear()


class MemcacheCache(Cache):
    """Cache implementation using memcache.
    
    The values are json-encoded before adding to memcache and json-decoded on get.
    
    Expects that the memcache servers are specified in web.config.memcache_servers.
    """
    @cached_property
    def memcache(self):
        servers = config.get("memcache_servers", None)
        if servers:
            return olmemcache.Client(servers)
        else:
            web.debug("Could not find memcache_servers in the configuration. Used dummy memcache.")
            import mockcache
            return mockcache.Client()
    
    def get(self, key):
        key = web.safestr(key)
        stats.begin("memcache.get", key=key)
        value = self.memcache.get(key)
        stats.end(hit=value is not None)
        return value and simplejson.loads(value)

    def set(self, key, value, expires=0):
        key = web.safestr(key)
        value = simplejson.dumps(value)
        stats.begin("memcache.set", key=key)
        value = self.memcache.set(key, value, expires)
        stats.end()
        return value

    def add(self, key, value, expires=0):
        key = web.safestr(key)
        value = simplejson.dumps(value)
        stats.begin("memcache.add", key=key)
        value = self.memcache.add(key, value, expires)
        stats.end()
        return value
        
    def delete(self, key):
        key = web.safestr(key)
        stats.begin("memcache.delete", key=key)
        value = self.memcache.delete(key)
        stats.end()
        return value

class RequestCache(Cache):
    """Request-Local cache.
    
    The values are cached only in the context of the current request.
    """
    @property
    def d(self):
        return web.ctx.setdefault("request-local-cache", {})

    def get(self, key):
        return self.d.get(key)
        
    def set(self, key, value, expires=0):
        self.d[key] = value
    
    def add(self, key, value, expires=0):
        return self.d.setdefault(key, value) is value
    
    def delete(self, key):
        return self.d.pop(key, None) is not None

memory_cache = MemoryCache()
memcache_cache = MemcacheCache()
request_cache = RequestCache()

def get_memcache():
    return memcache_cache.memcache

def _get_cache(engine):
    d = {
        "memory": memory_cache,
        "memcache": memcache_cache,
        "memcache+memory": memcache_cache,
        "request": request_cache
    }
    return d.get(engine)

class memoize:
    """Memoize decorator to cache results in various cache engines.
    
    Usage::
        
        @cache.memoize(engine="memcache")
        def some_func(args):
            pass
    
    Arguments::
    
    * engine:
        Engine to store the results. Available options are:    
            * memory: stores the result in memory.
            * memcache: stores the result in memcached.
            * request: stores the result only in the context of the current request.

    * key:
        key to be used in the cache. If this is a string, arguments are append
        to it before making the cache-key. If this is a function, it's
        return-value is used as cache-key and this function is called with the
        arguments. If not specified, the default value is computed using the
        function name and module name.
    
    * expires:
        The amount of time in seconds the value should be cached. Pass expires=0 to cache indefinitely.
        (Not yet implemented)
        
    * background:
        Indicates that the value must be recomputed in the background after
        the timeout. Until the new value is ready, the function continue to
        return the same old value.
        (not yet implemented)
        
    * cacheable:
        Function to determine if the returned value is cacheable. Sometimes it
        is desirable to not cache return values generated due to error
        conditions. The cacheable function is called with (key, value) as
        arguments. 
        
    Advanced Usage:
    
    Sometimes, it is desirable to store results of related functions in the
    same cache entry to reduce memory usage. It can be achieved by making the
    ``key`` function return a tuple of two values. (Not Implemented yet)
    
        @cache.memoize(engine="memcache", key=lambda page: (page.key, "history"))
        def get_history(page):
            pass
            
        @cache.memoize(engine="memory", key=lambda key: (key, "doc"))
        def get_page(key):
            pass
    """
    def __init__(self, engine="memory", key=None, expires=0, background=False, cacheable=None):
        self.cache = _get_cache(engine)
        self.keyfunc = self._make_key_func(key)
        self.cacheable = cacheable
        self.expires = expires
        
    def _make_key_func(self, key):
        if isinstance(key, basestring):
            return PrefixKeyFunc(key)
        else:
            return key
        
    def __call__(self, f):
        """Returns the memoized version of f.
        """
        @functools.wraps(f)
        def func(*args, **kwargs):
            """The memoized function. 

            If this is the first call with these arguments, function :attr:`f` is called and the return value is cached.
            Otherwise, value from the cache is returned.
            """
            key = self.keyfunc(*args, **kwargs)
            value = self.cache_get(key)
            if value is None:
                value = f(*args, **kwargs)
                self.cache_set(key, value)
            return value
        return func
            
    def cache_get(self, key):
        """Reads value of a key from the cache.
        
        When key is a string, this is equvivalant to::
        
            return cache[key]
        
        When key is a 2-tuple, this is equvivalant to::
        
            k0, k1 = key
            return cache[k0][k1]
        """
        if isinstance(key, tuple):
            k0, k1 = key
            d = self.cache.get(k0)
            return d and d.get(k1)
        else:
            return self.cache.get(key)
        
    def cache_set(self, key, value):
        """Sets a key to a given value in the cache. 
        
        When key is a string, this is equvivalant to::
            
            cache[key] = value
        
        When key is a 2-tuple, this is equvivalant to::
        
            k0, k1 = key
            cache[k0][k1] = value
        """
        # When cacheable is provided, use it to determine whether or not the cache should be updated.
        if self.cacheable and self.cacheable(key, value) is False:
            return
            
        if isinstance(key, tuple):
            k1, k2 = key
            d = self.cache.get(k1) or {}
            d[k2] = value
            return self.cache.set(k1, d, expires=self.expires)
        else:
            return self.cache.set(key, value, expires=self.expires)

class PrefixKeyFunc:
    """A function to generate cache keys using a prefix and arguments.
    """
    def __init__(self, prefix):
        self.prefix = prefix
    
    def __call__(self, *a, **kw):
        return self.prefix + "-" + self.encode_args(a, kw)
        
    def encode_args(self, args, kw={}):
        """Encodes arguments to construct the memcache key.
        """
        # strip [ and ] from key
        a = self.json_encode(list(args))[1:-1] 

        if kw:
            return a + "-" + self.json_encode(kw)
        else:
            return a
    
    def json_encode(self, value):
        """simplejson.dumps without extra spaces and consistant ordering of dictionary keys.

        memcache doesn't like spaces in the key.
        """
        return simplejson.dumps(value, separators=(",", ":"), sort_keys=True)

def method_memoize(f):
    """object-local memoize. 
    
    Works only for functions without any arguments.
    
    TODO: support arguments.
    """
    @functools.wraps(f)
    def g(self):
        cache = self.__dict__.setdefault('_memoize_cache', {})
        if f.__name__ not in cache:
            cache[f.__name__] = f(self)
        return cache[f.__name__]
    return g

########NEW FILE########
__FILENAME__ = celery_couchdb
import calendar
import datetime
import json


from celery.backends.base import BaseDictBackend
from celery.exceptions import ImproperlyConfigured

try:
    import couchdb
except ImportError:
    couchdb = None


class CouchDBBackend(BaseDictBackend): 
    def __init__(self, *largs, **kargs):
        super(CouchDBBackend, self).__init__(**kargs)
        self.dburi = self.app.conf.CELERY_RESULT_DBURI
        if not self.dburi:
            raise ImproperlyConfigured(
                    "Missing connection string! Do you have "
                    "CELERY_RESULT_DBURI set to a real value?")

        if not couchdb:
            raise ImproperlyConfigured(
                "You need to install the couchdb library to use the "
                "CouchDB backend.")

        self.database = couchdb.Database(self.dburi)


    def _get_tombstone(self, result, status, traceback):
        if status == "FAILURE":
            # Pull out traceback, args and other things from exception in case of FAILURE
            traceback = result.args[1].pop('traceback')
            result = result.args[1]
        doc = dict(type = "task",
                   status = str(status),
                   traceback = str(traceback),
                   finished_at = calendar.timegm(datetime.datetime.utcnow().timetuple()))
        doc.update(result)
        return doc

    def _store_result(self, task_id, result, status, traceback=None):
        doc = self._get_tombstone(result, status, traceback)
        self.database[task_id] = doc
        
        

        


########NEW FILE########
__FILENAME__ = connections
"""Infobase connection middlewares used in Open Library.
"""

class ConnectionMiddleware:
    """Base class for all connection middlewares."""
    def __init__(self, conn):
        self.conn = conn
        
    def get_auth_token(self):
        return self.conn.get_auth_token()

    def set_auth_token(self, token):
        self.conn.set_auth_token(token)
        
    def dispatch():

    def request(self, sitename, path, method='GET', data=None):
        return self.conn.request(sitename, path, method, data)
        
    def get(self, sitename, data):
        return self.conn.request(sitename, '/get', 'GET', data)

    def get_many(self, sitename, data):
        return self.conn.request(sitename, '/get_many', 'GET', data)

    def write(self, sitename, data):
        return self.conn.request(sitename, '/write', 'POST', data)

    def save(self, sitename, path, data):
        return self.conn.request(sitename, path, 'POST', data)

    def save_many(self, sitename, data):
        return self.conn.request(sitename, '/save_many', 'POST', data)


class UpstreamMigrationMiddleware(ConnectionMiddleware):
    """Connection Middleware to handle change of urls during upstream migration. 

    Ideally those changes should happen in the database, but database is too
    slow to do handle such bulk updates. 
    
    This middleware is requied until the data table in the db is updated.
    """
    
    def request(self, sitename, path, method='GET', data=None):
        if path == "/get":
            return self.get(sitename, data)
        elif path

    def _process_key(self, key):
        mapping = (
            "/l/", "/languages/",
            "/a/", "/authors/",
            "/b/", "/books/",
            "/user/", "/people/"
        )
        
        if "/" in key and key.split("/")[1] in ['a', 'b', 'l', 'user']:
            for old, new in web.group(mapping, 2):
                if key.startswith(old):
                    return new + key[len(old):]
        return key
    
    def exists(self, key):
        try:
            d = ConnectionMiddleware.get(self, "openlibrary.org", {"key": key})
            return True
        except client.ClientException, e:
            return False
    
    def _process(self, data):
        if isinstance(data, list):
            return [self._process(d) for d in data]
        elif isinstance(data, dict):
            if 'key' in data:
                data['key'] = self._process_key(data['key'])
            return dict((k, self._process(v)) for k, v in data.iteritems())
        else:
            return data
    
    def get(self, sitename, data):
        # Hack to make the API work even with old keys.
        if web.ctx.get('path') == "/api/get" and 'key' in data:
            data['key'] = self._process_key(data['key'])
            
        response = ConnectionMiddleware.get(self, sitename, data)
        if response:
            data = simplejson.loads(response)
            
            type = data and data.get("type", {}).get("key") 
            
            if type == "/type/work":
                if data.get("authors"):
                    # some record got empty author records because of an error
                    # temporary hack to fix 
                    authors = [a for a in data['authors'] if 'author' in a]
                    if authors != data['authors']
            elif type == "/type/edition":
                # get rid of title_prefix.
                if 'title_prefix' in data:
                    data['title'] = data['title_prefix'] + ' ' + data['title']
                    del data['title_prefix']
            
            response = simplejson.dumps(self._process(data))
        return response
        
    def get_many(self, sitename, data):
        response = ConnectionMiddleware.get_many(self, sitename, data)
        if response:
            data = simplejson.loads(response)
            response = simplejson.dumps(self._process(data))
        return response


    

########NEW FILE########
__FILENAME__ = couch
"""Improved CouchDB client.

Unlike the couchdb python library, this returns an iterator view rows instead of a list.
"""

import simplejson

import couchdb.client
from couchdb.client import Row

class Database(couchdb.client.Database):
    def view(self, name, wrapper=None, **options):
        if not name.startswith('_'):
            design, name = name.split('/', 1)
            name = '/'.join(['_design', design, '_view', name])
        return PermanentView(self.resource(*name.split('/')), name,
                             wrapper=wrapper)(**options)

class ViewResults(couchdb.client.ViewResults):
    def _fetch(self):
        data = self.view._exec(self.options)
        wrapper = self.view.wrapper or Row
        
        #self._rows = [wrapper(row) for row in data['rows']]
        self._rows = (wrapper(row) for row in data['rows'])
        
        self._total_rows = data.get('total_rows')
        self._offset = data.get('offset', 0)

class PermanentView(couchdb.client.PermanentView):
    def _exec(self, options):
        if 'keys' in options:
            options = options.copy()
            keys = {'keys': options.pop('keys')}
            _, _, data = self.resource.post(body=keys,
                                                 **self._encode_options(options))
        else:
            _, _, data = self.resource.get(**self._encode_options(options))

        return self.parse_view_result(data)
        
    def __call__(self, **options):
        return ViewResults(self, options)
        
    def parse_view_result(self, rawdata):
        rawdata = iter(rawdata)
        header = rawdata.next().strip()
        
        if not header.endswith("}"):
            header += "]}"
        data = simplejson.loads(header)
        data["rows"] = self._parse_rows(rawdata)
        return data
        
    def _parse_rows(self, lineiter):
        for row in lineiter:
            row = row.strip()
            if not row or row == "]}":
                break
            if row != ",":
                yield simplejson.loads(row)

########NEW FILE########
__FILENAME__ = fetchmail
import re
import email
import imaplib
import logging as Logging
import logging.config
import ConfigParser
import optparse
import quopri
import base64

import yaml
import couchdb
import web

from openlibrary.core import support
from infogami.utils.markdown import markdown

subject_re = re.compile("^.*[Cc]ase #([0-9]+).*")

template = """

%(message)s

-- Links --------------------------------------------
Case page : http://openlibrary.org/admin/support/%(caseno)s

"""

class Error(Exception): pass

# Utility functions for the imap coneection
def parse_email_subject(message):
    subject = message.get('Subject',"")
    if subject:
        if subject.startswith("=?utf-8"):
            subject = base64.decodestring(subject.split("?")[3])
    return subject
        
def parse_imap_response(response):
    code, body = response
    message = email.message_from_string(body)
    subject = parse_email_subject(message)
    logger.debug("Message parsed : %s (Subject : %s)", code, subject)
    messageid = code.split()[0]
    return messageid, message, subject

def imap_reset_to_unseen(imap_conn, messageid):
    logger.debug(" Resetting %s to unseen", messageid)
    imap_conn.store(messageid, "-FLAGS", r'(\Seen)')

def imap_move_to_folder(imap_conn, messageid, mailboxname, debug):
    if debug:
        logger.info("Debug mode: Not moving emails")
        return
    logger.debug(" Moving message %s to  %s ", messageid, mailboxname)
    imap_conn.copy(messageid, mailboxname)
    imap_mark_for_deletion(imap_conn, messageid)

def imap_mark_for_deletion(imap_conn, messageid):
    imap_conn.store(messageid, "+FLAGS", r'(\Deleted)')

def imap_remove_delete_flag(imap_conn, messageid):
    imap_conn.store(messageid, "-FLAGS", r'(\Deleted \Seen)')


def set_up_imap_connection(settings):
    email_config_file = settings.config.get('email_config_file')
    ol_config = settings.config
    try:
        c = ConfigParser.ConfigParser()
        c.read(email_config_file)
        username = settings.user or c.get("support","username")
        password = settings.password or c.get("support","password")
        imap_server = settings.imap or ol_config.get("smtp_server") # The key is badly named but it is the IMAP server.
        mailbox = settings.mailbox
        logger.debug("Connecting to %s using %s:%s and using mailbox %s", imap_server, username, password, mailbox)
        conn = imaplib.IMAP4_SSL(imap_server)
        conn.login(username, password)
        conn.select(mailbox)
        logger.info("Connected to IMAP server")
        typ, data =  conn.status(mailbox, "(MESSAGES)")
        if typ == "OK":
            logger.info(" %s selected - status:%s", mailbox, data)
        return conn
    except imaplib.IMAP4.error, e:
        logger.critical("Connection setup failure : credentials (%s, %s)", username, password, exc_info = True)
        raise Error(str(e))

def connect_to_admindb(settings):
    if settings.debug:
        return None 
    config = settings.config
    db = settings.couch or config.get("admin",{}).get("admin_db",None)
    logger.debug("Connected to couch db : %s", db)
    support_db = support.Support(couchdb.Database(db))
    return support_db
    
def get_new_emails(conn):
    typ, data = conn.search(None, "ALL")
    logger.debug("Fetching new message headers")
    for num in data[0].split():
        typ, data = conn.fetch(num, '(RFC822)')
        if typ != "OK":
            logger.warn("Message %s reported non okay status - %s", num, typ)
        yield data[0]


def update_support_db(author, message, case):
    try: 
        case.add_worklog_entry(author, unicode(message, errors="ignore"))
        case.change_status("new", author)
        logger.info("  Updated case #%s"%case.caseno)
    except support.InvalidCase:
        logger.info("  Invalid case %s message from %s", case.caseno, author)

def get_casenote(message):
    "Try to extract the casenote out of the message"
    md = markdown.Markdown()        
    ctype = message.get_content_type()
    if ctype == "multipart/related" or ctype == "multipart/signed" or ctype == "multipart/mixed":
        # Look inside for something we can use.
        for i in message.get_payload():
            ctype2 = i.get_content_type()
            if ctype2 == "multipart/alternative" or ctype2 == "text/plain" or ctype2 == "text/html": 
                message = i
    post_process = lambda x : x
    if message.get('Content-Transfer-Encoding') == "base64":
        post_process = base64.decodestring

    if message.get_content_type() == "text/plain":
        return quopri.decodestring(post_process(message.get_payload()))

    if message.get_content_type() == "text/html":
        casenote = md.convert(post_process(message.get_payload()))
        return casenote

    if message.get_content_type() == "multipart/alternative":
        # Find something we can use
        plain = html = None
        for part in message.get_payload():
            content_type = part.get_content_type()
            if content_type == "text/plain":
                plain = post_process(part.get_payload(None, True))
            if content_type == "text/html":
                html  = post_process(part.get_payload(None, True))
        if not plain and not html:
            pieces = ",".join(x.get_content_type() for x in message.get_payload())
            logger.warning("This message has no usable payload Types : %s", pieces)
            return "ERROR : Unparseable message received"
        if plain:
            return quopri.decodestring(plain)
        if html:
            return md.convert(html)

def fetch_and_update(settings, imap_conn, db_conn = None):
    debug = settings.debug
    smtp_server = settings.smtp
    accept_mailbox = settings.accept_mailbox
    reject_mailbox = settings.reject_mailbox
    for resp in get_new_emails(imap_conn):
        try:
            messageid, message, subject = parse_imap_response(resp)
        except Exception, e:
            logger.warning(" Message parsing failed", exc_info = True)
            continue
        m = subject_re.search(subject)
        if m:
            caseid = m.groups()[0]
            logger.debug(" Updating case %s", caseid)
            try:
                frm = email.utils.parseaddr(message['From'])[1]
                casenote = get_casenote(message)
                if settings.debug:
                    logger.debug("Debug mode: Not touching couch database")
                    logger.debug("Case #%s would be updated by '%s' with \n-----\n%s\n-----",caseid, frm, casenote)
                    case = lambda:0 # To make the assignee checking work in debug mode
                    case.assignee = None
                else:
                    case = db_conn.get_case(caseid)
                    update_support_db(frm, casenote, case)

                imap_move_to_folder(imap_conn, messageid, accept_mailbox, debug)
                message = template%dict(caseno = caseid,
                                        message = casenote,
                                        author = frm)
                subject = "Case #%s updated"%(caseid)
                assignee = settings.to or case.assignee # Use the override address if specified. 
                                                        # Otherwise, to the assignee of the case
                if smtp_server:
                    web.config.smtp_server = smtp_server
                logger.debug("Sending notification from 'support@openlibrary.org' to '%s'", assignee)
                web.sendmail("support@openlibrary.org", assignee, subject, message)
            except Exception, e:
                logger.warning(" Couldn't update case. Resetting message", exc_info = True)
                imap_reset_to_unseen(imap_conn, messageid)
        else:
            logger.debug(" No regexp match on subject '%s'", subject)
            logger.debug("  Ignoring message and resetting to unread")
            imap_move_to_folder(imap_conn, messageid, reject_mailbox, debug)
    logger.debug("Expunging deleted messages")
    imap_conn.expunge()


def fetchmail(settings):
    global logger
    logging.config.fileConfig(settings.config.get('logging_config_file'))
    logger = Logging.getLogger("openlibrary")
    if settings.verbose:
        logger.setLevel(logging.DEBUG)
        for l in logger.handlers:
            l.setLevel(logging.DEBUG)
    try:
        imap_conn = set_up_imap_connection(settings)
        db_conn = connect_to_admindb(settings)
        fetch_and_update(settings, imap_conn, db_conn)
        imap_conn.close()
        imap_conn.logout()
        return 0
    except KeyboardInterrupt:
        logger.info("User interrupt. Aborting")
        imap_conn.close()
        imap_conn.logout()
        return -1
    except Error:
        logger.info("Abnormal termination")
        return -2

def parse_args(args):
    parser = optparse.OptionParser(usage = "usage: %prog [options] config_file")
    parser.add_option("-d", "--debug", dest="debug", action="store_true", help="Dry run (don't modify anything or send emails)")
    parser.add_option("-u", "--user", dest="user", action = "store", help="Specify IMAP username (overrides config)")
    parser.add_option("-p", "--password", dest="password", help="Specify IMAP password (overrides config)")
    parser.add_option("-i", "--imap", dest="imap", action = "store", help="IMAP server (overrides config)")
    parser.add_option("-s", "--smtp", dest="smtp", action = "store", default = "", help="SMTP server (overrides config)")
    parser.add_option("-m", "--mailbox", dest="mailbox", default = "INBOX", action = "store", help="Mailbox to look for emails in")
    parser.add_option("-a", "--accept-mailbox", dest="accept_mailbox", default = "Accepted", action = "store", help="Mailbox to move successfully parsed emails into")
    parser.add_option("-r", "--reject-mailbox", dest="reject_mailbox", default = "Rejected", action = "store", help="Mailbox to move emails which couldn't be processed into")
    parser.add_option("-t", "--to", dest="to", action = "store", default = "", help="Send notification emails to this address")
    parser.add_option("-v", "--verbose", dest="verbose", action = "store_true", default = False, help = "Enable debug output")
    parser.add_option("-c", "--couch", dest="couch", action = "store", help = "Couch database to use")

    opts, args = parser.parse_args(args)
    if not args:
        parser.error("No config file specified")
    return opts, args
        
def main(args):
    settings, args = parse_args(args)
    settings.config = yaml.load(open(args[0]))
    fetchmail(settings)

if __name__ == "__main__":
    import sys
    sys.exit(main(sys.argv[1:]))

########NEW FILE########
__FILENAME__ = formats
"""Library for loading and dumping data to json and yaml.
"""
import simplejson
import yaml

__all__ = [
    "load_json", "dump_json",
    "load_yaml", "dump_yaml"
]

def load_json(text):
    """Loads data from the given JSON text.
    """
    return simplejson.loads(text)

def dump_json(data):
    return simplejson.dumps(data)

def load_yaml(text):
    return yaml.safe_load(text)

def dump_yaml(data):
    return yaml.safe_dump(data, 
        indent=4, 
        allow_unicode=True,
        default_flow_style=False)

def load(text, format):
    if format == "json":
        return load_json(text)
    elif format == "yaml":
        return load_yaml(text)
    else:
        raise Exception("unsupported format %r" % format)
    
def dump(data, format):
    if format == "json":
        return dump_json(data)
    elif format == "yml":
        return dump_yaml(data)
    else:
        raise Exception("unsupported format %r" % format)
    
########NEW FILE########
__FILENAME__ = geo_ip
from infogami import config
import GeoIP
import web

@web.memoize
def get_db():
    try:
        geoip_db = config.get("geoip_database", '/usr/local/maxmind-geoip/GeoLiteCity.dat')
        return GeoIP.open(geoip_db, GeoIP.GEOIP_MEMORY_CACHE)
    except GeoIP.error:
        print "loading GeoIP file failed"

def get_region(ip):
    gi = get_db()
    if not gi:
        return None
    
    region = None
    try:
        record = gi.record_by_addr(ip)
        region = record['region']
    except TypeError:
        print 'geoip lookup failed for ' + ip

    return region

def get_country(ip):
    gi = get_db()
    if not gi:
        return None

    try:
        return gi.record_by_addr(ip)['country_code']
    except TypeError:
        print 'geoip lookup failed for ' + ip

########NEW FILE########
__FILENAME__ = helpers
"""Generic helper functions to use in the templates and the webapp.
"""
import web
import simplejson
import urlparse
import string
import re

import babel, babel.core, babel.dates, babel.numbers

try:
    import genshi
    import genshi.filters
except ImportError:
    genshi = None
    
try:
    from BeautifulSoup import BeautifulSoup
except ImportError:
    BeautifulSoup = None
    
from infogami import config

# handy utility to parse ISO date strings
from infogami.infobase.utils import parse_datetime
from infogami.utils.view import safeint

# TODO: i18n should be moved to core or infogami
from openlibrary.i18n import gettext as _

__all__ = [
    "sanitize", 
    "json_encode",
    "safesort", 
    "datestr", "format_date",    
    "sprintf", "cond", "commify", "truncate",
    "urlsafe", "texsafe", 
    "percentage",
    
    # functions imported from elsewhere
    "parse_datetime", "safeint"
]
__docformat__ = "restructuredtext en"

def sanitize(html):
    """Removes unsafe tags and attributes from html and adds
    ``rel="nofollow"`` attribute to all external links.
    """

    # Can't sanitize unless genshi module is available
    if genshi is None:
        return html
        
    def get_nofollow(name, event):
        attrs = event[1][1]
        href = attrs.get('href', '')

        if href:
            # add rel=nofollow to all absolute links
            _, host, _, _, _ = urlparse.urlsplit(href)
            if host:
                return 'nofollow'
                
    try:
        html = genshi.HTML(html)
    except (genshi.ParseError, UnicodeDecodeError):
        if BeautifulSoup:
            # Bad html. Tidy it up using BeautifulSoup
            html = str(BeautifulSoup(html))
            try:
                html = genshi.HTML(html)
            except Exception:
                # Failed to sanitize.
                # We can't do any better than returning the original HTML, without sanitizing.
                return html                
        else:
            raise

    stream = html \
        | genshi.filters.HTMLSanitizer() \
        | genshi.filters.Transformer("//a").attr("rel", get_nofollow)
    return stream.render()                                        


def json_encode(d, **kw):
    """Same as simplejson.dumps.
    """
    return simplejson.dumps(d, **kw)


def safesort(iterable, key=None, reverse=False):
    """Sorts heterogeneous of objects without raising errors.
    
    Sorting heterogeneous objects sometimes causes error. For example,
    datetime and Nones don't go well together. This function takes special
    care to make that work.
    """
    key = key or (lambda x: x)
    def safekey(x):
        k = key(x)
        return (k.__class__.__name__, k)
    return sorted(iterable, key=safekey, reverse=reverse)

def datestr(then, now=None, lang=None, relative = True):
    """Internationalized version of web.datestr."""
    if not relative:
        result = then.strftime("%b %d %Y")
    else:
        result = web.datestr(then, now)
    if not result:
        return result
    elif result[0] in string.digits: # eg: 2 milliseconds ago
        t, message = result.split(' ', 1)
        return _("%d " + message) % int(t)
    else:
        return format_date(then, lang=lang)


def format_date(date, lang=None):
    lang = lang or web.ctx.get('lang') or "en"
    locale = _get_babel_locale(lang)
    return babel.dates.format_date(date, format="long", locale=locale)

def _get_babel_locale(lang):
    try:
        return babel.Locale(lang)
    except babel.core.UnknownLocaleError:
        return babel.Locale("en")


def sprintf(s, *a, **kw):
    """Handy utility for string replacements.
    
        >>> sprintf('hello %s', 'python')
        'hello python'
        >>> sprintf('hello %(name)s', name='python')
        'hello python'
    """
    args = kw or a
    if args:
        return s % args
    else:
        return s
        

def cond(pred, true_value, false_value=""):
    """Lisp style cond function.
    
    Hanly to use instead of if-else expression.
    """
    if pred:
        return true_value
    else:
        return false_value


def commify(number, lang=None):
    """localized version of web.commify"""
    try:
        lang = lang or web.ctx.get("lang") or "en"
        return babel.numbers.format_number(int(number), lang)
    except:
        return unicode(number)
        

def truncate(text, limit):
    """Truncate text and add ellipses if it longer than specified limit."""
    if len(text) <= limit:
        return text
    return text[:limit] + "..."


def urlsafe(path):
    """Replaces the unsafe chars from path with underscores.
    """
    return _get_safepath_re().sub('_', path).strip('_')[:100]

@web.memoize
def _get_safepath_re():
    """Make regular expression that matches all unsafe chars."""
    # unsafe chars according to RFC 2396
    reserved = ";/?:@&=+$,"
    delims = '<>#%"'
    unwise = "{}|\\^[]`"
    space = ' \n\r'

    unsafe = reserved + delims + unwise + space
    pattern = '[%s]+' % "".join(re.escape(c) for c in unsafe)
    return re.compile(pattern)


def get_coverstore_url():
    """Returns the base url of coverstore by looking at the config."""
    return config.get('coverstore_url', 'http://covers.openlibrary.org').rstrip('/')


_texsafe_map = {
    '"': r'\textquotedbl{}',
    '#': r'\#',
    '$': r'\$',
    '%': r'\%',
    '&': r'\&',
    '<': r'\textless{}',
    '>': r'\textgreater{}',
    '\\': r'\textbackslash{}',
    '^': r'\^{}',
    '_': r'\_{}',
    '{': r'\{',
    '}': r'\}',
    '|': r'\textbar{}',
    '~': r'\~{}',
}

_texsafe_re = None

def texsafe(text):
    """Escapes the special characters in the given text for using it in tex type setting.
    
    Tex (or Latex) uses some characters in the ascii character range for
    special notations. These characters must be escaped when occur in the
    regular text. This function escapes those special characters.
    
    The list of special characters and the latex command to typeset them can
    be found in `The Comprehensive LaTeX Symbol List`_.
    
    .. _The Comprehensive LaTeX Symbol List: http://www.ctan.org/tex-archive/info/symbols/comprehensive/symbols-a4.pdf
    """
    global _texsafe_re
    if _texsafe_re is None:
        pattern = "[%s]" % re.escape("".join(_texsafe_map.keys()))
        _texsafe_re = re.compile(pattern)
        
    return _texsafe_re.sub(lambda m: _texsafe_map[m.group(0)], text)

def percentage(value, total):
    """Computes percentage.
        
        >>> percentage(1, 10)
        10.0
        >>> percentage(0, 0)
        0.0
    """
    if total == 0:
        return 0
    else:
        return (value * 100.0)/total

def uniq(values, key=None):
    """Returns the unique entries from the given values in the original order.
    
    The value of the optional `key` parameter should be a function that takes
    a single argument and returns a key to test the uniqueness.
    """
    key = key or (lambda x: x)
    s = set()
    result = []
    for v in values:
        k = key(v)
        if k not in s:
            s.add(k)
            result.append(v)
    return result

        
def _get_helpers():
    _globals = globals()
    return web.storage((k, _globals[k]) for k in __all__)
    

## This must be at the end of this module
helpers = _get_helpers()

########NEW FILE########
__FILENAME__ = ia
"""Library for interacting wih archive.org.
"""
import urllib2
from xml.dom import minidom
import simplejson
import web
import logging
from infogami.utils import stats
import cache

logger = logging.getLogger("openlibrary.ia")

def get_metadata(itemid):
    itemid = web.safestr(itemid.strip())
    url = 'http://archive.org/metadata/%s' % itemid
    try:
        stats.begin("archive.org", url=url)
        metadata_json = urllib2.urlopen(url).read()
        stats.end()
        d = simplejson.loads(metadata_json)
        metadata = process_metadata_dict(d.get("metadata", {}))

        # if any of the files is access restricted, consider it as an access-restricted item.
        files = d.get('files', [])
        metadata['access-restricted'] = any(f.get("private") == "true" for f in files)

        # remember the filenames to construct download links
        metadata['_filenames'] = [f['name'] for f in files]
        return metadata
    except IOError:
        stats.end()
        return {}

get_metadata = cache.memcache_memoize(get_metadata, key_prefix="ia.get_metadata", timeout=5*60)

def process_metadata_dict(metadata):
    """Process metadata dict to make sure multi-valued fields like
    collection and external-identifier are always lists.

    The metadata API returns a list only if a field has more than one value
    in _meta.xml. This puts burden on the application to handle both list and
    non-list cases. This function makes sure the known multi-valued fields are
    always lists.
    """
    mutlivalued = set(["collection", "external-identifier"])
    def process_item(k, v):
        if k in mutlivalued and not isinstance(v, list):
            v = [v]
        elif k not in mutlivalued and isinstance(v, list):
            v = v[0]
        return (k, v)
    return dict(process_item(k, v) for k, v in metadata.items() if v)

def _old_get_meta_xml(itemid):
    """Returns the contents of meta_xml as JSON.
    """
    itemid = web.safestr(itemid.strip())
    url = 'http://www.archive.org/download/%s/%s_meta.xml' % (itemid, itemid)
    try:
        stats.begin("archive.org", url=url)
        metaxml = urllib2.urlopen(url).read()
        stats.end()
    except IOError:
        logger.error("Failed to download _meta.xml for %s", itemid, exc_info=True)
        stats.end()
        return web.storage()
        
    # archive.org returns html on internal errors. 
    # Checking for valid xml before trying to parse it.
    if not metaxml.strip().startswith("<?xml"):
        return web.storage()
    
    try:
        defaults = {"collection": [], "external-identifier": []}
        return web.storage(xml2dict(metaxml, **defaults))
    except Exception, e:
        logger.error("Failed to parse metaxml for %s", itemid, exc_info=True)
        return web.storage()

def get_meta_xml(itemid):
    # use metadata API instead of parsing meta xml manually
    return get_metadata(itemid)

def xml2dict(xml, **defaults):
    """Converts xml to python dictionary assuming that the xml is not nested.
    
    To get some tag as a list/set, pass a keyword argument with list/set as value.
    
        >>> xml2dict('<doc><x>1</x><x>2</x></doc>')
        {'x': 2}
        >>> xml2dict('<doc><x>1</x><x>2</x></doc>', x=[])
        {'x': [1, 2]}
    """
    d = defaults
    dom = minidom.parseString(xml)
    
    for node in dom.documentElement.childNodes:
        if node.nodeType == node.TEXT_NODE or len(node.childNodes) == 0:
            continue
        else:
            key = node.tagName
            value = node.childNodes[0].data
            
            if key in d and isinstance(d[key], list):
                d[key].append(value)
            elif key in d and isinstance(d[key], set):
                d[key].add(value)
            else:
                d[key] = value
                
    return d
    
def _get_metadata(itemid):
    """Returns metadata by querying the archive.org metadata API.
    """
    print >> web.debug, "_get_metadata", itemid
    url = "http://www.archive.org/metadata/%s" % itemid
    try:
        stats.begin("archive.org", url=url)        
        text = urllib2.urlopen(url).read()
        stats.end()
        return simplejson.loads(text)
    except (IOError, ValueError):
        return None

# cache the results in memcache for a minute
_get_metadata = web.memoize(_get_metadata, expires=60)
        
def locate_item(itemid):
    """Returns (hostname, path) for the item. 
    """
    d = _get_metadata(itemid)
    return d.get('server'), d.get('dir')

########NEW FILE########
__FILENAME__ = init
"""Library for starting and monitoring Open Library services.

Many services need to be started to run an Open Library instance. It becomes
quite complicated to manage them manually. This library provides init, a
process manager like unix init, for starting and managing OL services.

This is used only for running the dev instance. In production, these services
are typically run on multiple nodes and monitored using upstart.
"""
import os
import shlex
import subprocess
import sys
import time

import formats

class Init:
    """Init process for starting and managing OL services.
    """
    def __init__(self, config):
        self.services = {}
        for name, value in config.items():
            self.services[name] = Service(self, name, value)
            
        self.files = []
        
    def start(self):
        services = self.services.values()
        
        for s in services:
            s.start()
        
        try:
            while True:
                time.sleep(0.5)
                for s in services:
                    if s.poll() is not None:
                        print "ERROR: %s stopped:" % s.name
                        print "Stopping all services and exiting."
                        self.stop()
                        return
        except KeyboardInterrupt:
            print "Detected keyboard interrupy. Stopping all services and exiting."
            self.stop()
                    
    def stop(self):
        services = self.services.values()
        
        for s in services:
            s.stop()
    
class Service:
    def __init__(self, init, name, config):
        self.init = init
        self.name = name
        self.config = config
        self.process = None
        
    def start(self):
        """Starts the service.
        """
        config = self.config
        
        print "start:", config['command']
        
        args = shlex.split(config['command'])
        cwd = config.get("root", os.getcwd())
        
        stdout = self._open_file(config.get('stdout'), 'a')
        stderr = self._open_file(config.get('stderr'), 'a')
        print self.name, stdout, stderr

        self.process = subprocess.Popen(args, cwd=cwd, stdout=stdout, stderr=stderr)
        return self
        
    def _open_file(self, filename, mode='r'):
        filename = filename or self.name + ".log"
        
        if filename == "-":
            return sys.stdout
        else:
            return open(filename, mode)
        
    def stop(self, timeout=3):
        """Stops the service.
        """
        print "stopping", self.name
        if self.process and self.is_alive():
            self._terminate()
            exit_status = self.wait(timeout)
            
            if exit_status is None:
                self._kill()
                time.sleep(0.1) # wait for process to get killed
                
            return self.poll()
    
    def _terminate(self):
        if self.process:
            print self.process.pid, "terminate"
            self.process.terminate()
        
    def _kill(self):
        if self.process:
            print self.process.pid, "kill"
            self.process.kill()
    
    def wait(self, timeout=None):
        """Wait for the service to complete and returns the exit status.
        """
        if self.process:
            if timeout:
                limit = time.time() + timeout
                exit_status = None
                while exit_status is None and time.time() < limit:
                    time.sleep(0.1)
                    exit_status = self.poll()
                
                return exit_status
            else:
                return self.process.wait()
        
    def poll(self):
        """Returns the exit status if the service is terminated.
        """
        return self.process and self.process.poll()
        
    def is_alive(self):
        """Returns True if the service is running."""
        if self.process is None:
            return False
        return self.poll() is None
        

def main(config_file):
    config = formats.load_yaml(open(config_file).read())
    init = Init(config)
    init.start()
########NEW FILE########
__FILENAME__ = inlibrary
"""Tools for in-library lending.
"""
import web
import cache
import iprange
import geo_ip
from infogami.utils import delegate

def _get_libraries(site=None):
    """Returns all the libraries each as a dict."""
    if 'env' not in web.ctx:
        delegate.fakeload()

    site = site or web.ctx.site

    keys = site.things(query={"type": "/type/library", "limit": 1000, "status": "approved"})
    libraries = site.get_many(sorted(keys))
    return [lib.dict() for lib in libraries]

# cache the result for an hour in memcache
_get_libraries_memoized = cache.memcache_memoize(_get_libraries, "inlibrary._get_libraries", timeout=60*60)

def _get_default_library():
    """Returns the default library when the IP doesn't fall in any of the registered libraries.

    This is used to enable lending world-wide by making everyone else part of "Open Library of Richmond".
    """
    libraries = _get_libraries_memoized()
    d = dict((lib['key'], lib) for lib in libraries)
    return d.get("/libraries/openlibrary_of_richmond")

@cache.memoize(engine="memcache", key=lambda: "inlibrary.libraries-hash")
def _get_libraries_hash():
    """Returns a hash of libraries. When any one of the libraries is modified, the hash changes.
    """
    libraries = _get_libraries_memoized()
    return hash(",".join("%s@%s" % (x['key'], x['revision']) for x in libraries))

_ip_dict = None
_region_dict = None
_libraries_hash = None

def _get_ip_region_dict():
    """Returns an :class:`iprange.IPDict` instance with mapping from ips to library keys.
    _region_dict keys are 2-letter ISO 3166-2 subcountry codes for the US and Canada
    http://www.maxmind.com/app/iso3166_2
    """
    global _ip_dict, _region_dict, _libraries_hash
    # Use the library-hash to decide whether or not the value need to be recomputed.
    h = _get_libraries_hash()
    if _libraries_hash != h:
        _libraries_hash = h
        _ip_dict = _make_ip_dict()
        _region_dict = _make_region_dict()
    return _ip_dict, _region_dict

def _make_ip_dict():
    libraries = _get_libraries_memoized()
    d = iprange.IPDict()
    for lib in libraries:
        # ip_ranges will be of the form {"type": "/type/text", "value": "foo"}
        ip_ranges = lib.get("ip_ranges", "")
        if isinstance(ip_ranges, dict):
            ip_ranges = ip_ranges['value']
        if ip_ranges:
            d.add_ip_range_text(ip_ranges, lib)
    return d

def _get_region_dict():
    """keys are 2-letter ISO 3166-2 region codes for the US and Canada
    http://www.maxmind.com/app/iso3166_2
    """

    global _region_dict, _libraries_hash
    # Use the library-hash to decide whether or not the value need to be recomputed.
    h = _get_libraries_hash()
    if _libraries_hash != h:
        _libraries_hash = h
        _region_dict = _make_region_dict()
    return _region_dict

def _make_region_dict():
    libraries = _get_libraries_memoized()
    d = {}
    for lib in libraries:
        region = lib.get("lending_region")
        if region:
            d[region] = lib
    return d


def get_libraries():
    """Returns all the libraries."""
    libraries = _get_libraries_memoized()
    libraries = [web.ctx.site.new(doc['key'], doc) for doc in libraries]
    return libraries

def get_library():
    """Returns library document if the IP of the current request is in the IP range of that library.
    """

    if "library" not in web.ctx:
        d_ip, d_region = _get_ip_region_dict()

        # try with ip
        lib = d_ip.get(web.ctx.ip)

        # if not try the region
        if not lib:
            region = geo_ip.get_region(web.ctx.ip)
            lib = d_region.get(region)

        # if not try the default library
        if not lib:
            lib = _get_default_library()

        web.ctx.library = lib and web.ctx.site.new(lib['key'], lib)
    return web.ctx.library

def filter_inlibrary():
    """Returns True if the IP of the current request is in the IP range of one of the libraries.
    """
    return bool(get_library())

########NEW FILE########
__FILENAME__ = iprange
"""Tools to parse ip ranges.
"""
import re
import iptools

four_octet = r'(\d+\.\d+\.\d+\.\d+)'
re_range_star = re.compile(r'^(\d+\.\d+)\.(\d+)\s*-\s*(\d+)\.\*$')
re_three = re.compile(r'^(\d+\.\d+\.\d+)\.$')
re_four = re.compile(r'^' + four_octet + r'(/\d+)?$')
re_range_in_last = re.compile(r'^(\d+\.\d+\.\d+)\.(\d+)\s*-\s*(\d+)$')
re_four_to_four = re.compile('^%s\s*-\s*%s$' % (four_octet, four_octet))

patterns = (re_four_to_four, re_four, re_range_star, re_three, re_range_in_last)

def parse_ip_ranges(text):
    """Parses IP ranges in various formats from a multi-line text and returns them in standard representation.
    
    Text after # is considered as comment. Comments and empty lines are ignored.
    
    Supported formats:
    
    "1.2.3.4" -> "1.2.3.4"
    "1.2.3 - 4.*" -> ("1.2.3.0", "1.2.4.255")
    "1.2.3.4-10" -> ("1.2.3.4", "1.2.3.10")
    "1.2.3.4 - 2.3.4.5" -> ("1.2.3.4", "2.3.4.5")
    "1.2.*.* "-> "1.2.0.0/16"
    """
    for line in text.splitlines():
        # strip comments
        line = line.split("#")[0].strip()
        
        # ignore empty lines
        if not line:
            continue
        
        # accept IPs
        m = re_four.match(line)
        if m:
            yield line
            continue
            
        # accept IP ranges
        m = re_range_star.match(line)
        if m:
            start = '%s.%s.0' % (m.group(1), m.group(2))
            end = '%s.%s.255' % (m.group(1), m.group(3))
            yield (start, end)
            continue
            
        # consider 1.2.3 as 1.2.3.0 - 1.2.3.255
        m = re_three.match(line)
        if m:
            yield ('%s.0' % m.group(1), '%s.255' % m.group(1))
            continue
        
        # consider 1.2.3.4-10 as 1.2.3.4 - 1.2.3.10
        m = re_range_in_last.match(line)
        if m:
            yield ('%s.%s' % (m.group(1), m.group(2)), '%s.%s' % (m.group(1), m.group(3)))
            continue
            
        # accept 1.2.3.4 - 2.3.4.5
        m = re_four_to_four.match(line)
        if m:
            yield m.groups()
            continue
            
        # consider 1.2.*.* as 1.2.0.0/16
        if '*' in line:
            collected = []
            octets = line.split('.')
            while octets[0].isdigit():
                collected.append(octets.pop(0))
            if collected and all(octet == '*' for octet in octets):
                yield '%s/%d' % ('.'.join(collected + ['0'] * len(octets)), len(collected) * 8)
            continue

def find_bad_ip_ranges(text):
    """Returns bad ip-ranges in the given text.
    
    Lines which don't match the supported IP range formats are considered bad.    
    See :func:`parse_ip_ranges` for the list of supported ip-range formats.
    """
    bad = []
    for orig in text.splitlines():
        line = orig.split("#")[0].strip()
        if not line:
            continue
        if any(pat.match(line) for pat in patterns):
            continue
        if '*' in line:
            collected = []
            octets = line.split('.')
            while octets[0].isdigit():
                collected.append(octets.pop(0))
            if collected and all(octet == '*' for octet in octets):
                continue
        bad.append(orig)
    return bad

class IPDict:
    """Efficient dictionary of IP ranges to values.
    
    IP ranges can be added by calling :meth:`add_ip_range`. And values can be accessed by IP.
    
        >>> ipmap = IPRangeMap()
        >>> ipmap.add_ip_range("1.2.3.0/8", "foo")
        >>> ipmap['1.2.3.4']
        >>> 'foo'
        >>> ipmap.get('1.2.3.4')
        'foo'
        >>> ipmap.get('1.2.5.5')
    """
    def __init__(self):
        # 2-level Dictionary for storing IP ranges
        #
        # For efficient lookup, IP ranges are stored in 2-levels. 
        # First key is the integer representation of first 2 parts of the ip
        # Second key in the IpRange object.
        #
        # When to look for an IP, the integer representation of the first 2
        # parts of the IP are used to get the IpRanges to check for.
        self.ip_ranges = {}
        
    def add_ip_range(self, ip_range, value):
        """Adds an entry to this map.
        
        ip_range can be in the following forms:
        
            "1.2.3.4"
            "1.2.3.0/8"
            ("1.2.3.4", "1.2.3.44")
        """
        # Convert ranges in CIDR format into (start, end) tuple
        if isinstance(ip_range, basestring) and "/" in ip_range:
            # ignore bad value
            if not iptools.validate_cidr(ip_range):
                return
            ip_range = iptools.cidr2block(ip_range)
            
        # Find the integer representation of first 2 parts of the start and end IPs
        if isinstance(ip_range, tuple):
            # ignore bad ips
            if not iptools.validate_ip(ip_range[0]) or not iptools.validate_ip(ip_range[1]):
                return
            
            # Take the first 2 parts of the begin and end ip as integer
            start = iptools.ip2long(ip_range[0]) >> 16
            end = iptools.ip2long(ip_range[1]) >> 16
        else:
            start = iptools.ip2long(ip_range) >> 16
            end = start
            
        # for each integer in the range add an entry.
        for i in range(start, end+1):
            self.ip_ranges.setdefault(i, {})[iptools.IpRange(ip_range)] = value
        
    def add_ip_range_text(self, ip_range_text, value):
        """Adds all ip_ranges from the givem multi-line text and associate
        each one of them with given value.
        
        See :func:`parse_ip_ranges` for the supported formats in the text.
        """
        for ip_range in parse_ip_ranges(ip_range_text):
            self.add_ip_range(ip_range, value)
        
    def __getitem__(self, ip):
        # integer representation of first 2 parts 
        base = iptools.ip2long(ip) >> 16
        for ip_range, value in self.ip_ranges.get(base, {}).items():
            if ip in ip_range:
                return value
        raise KeyError(ip)
        
    def __contains__(self, ip):
        try:
            self[ip]
            return True
        except KeyError:
            return False
    
    def get(self, ip, default=None):
        try:
            return self[ip]
        except KeyError:
            return None

########NEW FILE########
__FILENAME__ = engine
"""Utility functions for processing lists.
"""

import collections
import re

import simplejson
import web

def reduce_seeds(values):
    """Function to reduce the seed values got from works db.
    """
    d = {
        "works": 0,
        "editions": 0,
        "ebooks": 0,
        "last_update": "",
    }
    subject_processor = SubjectProcessor()
    
    for v in values:
        d["works"] += v[0]
        d['editions'] += v[1]
        d['ebooks'] += v[2]
        d['last_update'] = max(d['last_update'], v[3])
        subject_processor.add_subjects(v[4])
        
    d['subjects'] = subject_processor.top_subjects()
    return d
    
RE_SUBJECT = re.compile("[, _]+")

def get_seeds(work):
    """Returns all seeds of given work."""
    def get_authors(work):
        return [a['author'] for a in work.get('authors', []) if 'author' in a]

    def _get_subject(subject, prefix):
        if isinstance(subject, basestring):
            key = prefix + RE_SUBJECT.sub("_", subject.lower()).strip("_")
            return {"key": key, "name": subject}
            
    def get_subjects(work):
        subjects = [_get_subject(s, "subject:") for s in work.get("subjects", [])]
        places = [_get_subject(s, "place:") for s in work.get("subject_places", [])]
        people = [_get_subject(s, "person:") for s in work.get("subject_people", [])]
        times = [_get_subject(s, "time:") for s in work.get("subject_times", [])]
        d = dict((s['key'], s) for s in subjects + places + people + times if s is not None)
        return d.values()
    
    def get(work):
        yield work['key']
        for a in get_authors(work):
            yield a['key']
        
        for e in work.get('editions', []):
            yield e['key']
        
        for s in get_subjects(work):
            yield s['key']
            
    return list(get(work))
    
class SubjectProcessor:
    """Processor to take a dict of subjects, places, people and times and build a list of ranked subjects.
    """
    def __init__(self):
        self.subjects = collections.defaultdict(list)

    def add_subjects(self, subjects):
        for s in subjects.get("subjects", []):
            self._add_subject('subject:', s)

        for s in subjects.get("people", []):
            self._add_subject('person:', s)

        for s in subjects.get("places", []):
            self._add_subject('place:', s)

        for s in subjects.get("times", []):
            self._add_subject('time:', s)

    def _add_subject(self, prefix, name):
        s = self._get_subject(prefix, name)
        if s:
            self.subjects[s['key']].append(s['name'])

    def _get_subject(self, prefix, subject_name):
        if isinstance(subject_name, basestring):
            key = prefix + RE_SUBJECT.sub("_", subject_name.lower()).strip("_")
            return {"key": key, "name": subject_name}

    def _most_used(self, seq):
        d = collections.defaultdict(lambda: 0)
        for x in seq:
            d[x] += 1

        return sorted(d, key=lambda k: d[k], reverse=True)[0]

    def top_subjects(self, limit=100):
        subjects = [{"key": key, "name": self._most_used(names), "count": len(names)} for key, names in self.subjects.items()]
        subjects.sort(key=lambda s: s['count'], reverse=True)
        return subjects[:limit]

########NEW FILE########
__FILENAME__ = model
"""Helper functions used by the List model.
"""
from collections import defaultdict
import re
import urllib, urllib2

import couchdb
import simplejson
import web

from infogami import config
from infogami.infobase import client, common
from infogami.utils import stats

from openlibrary.core import helpers as h
from openlibrary.core import cache

from openlibrary.plugins.worksearch.search import get_works_solr

# this will be imported on demand to avoid circular dependency
subjects = None

def get_subject(key):
    global subjects
    if subjects is None:
        from openlibrary.plugins.worksearch import subjects
    return subjects.get_subject(key)

def cached_property(name, getter):
    """Just like property, but the getter is called only for the first access. 

    All subsequent accesses will use the cached value.
    
    The name argument must be same as the property name.
    
    Sample Usage:
    
        count = cached_property("count", get_count)
    """
    def f(self):
        value = getter(self)
        self.__dict__[name] = value
        return value

    return property(f)    

class ListMixin:
    def _get_rawseeds(self):
        def process(seed):
            if isinstance(seed, basestring):
                return seed
            else:
                return seed.key
                
        return [process(seed) for seed in self.seeds]
        
    def _get_seed_summary(self):
        rawseeds = self._get_rawseeds()
        
        db = self._get_seeds_db()

        zeros = {"editions": 0, "works": 0, "ebooks": 0, "last_update": ""}
        d = dict((seed, web.storage(zeros)) for seed in rawseeds)
        
        for row in self._couchdb_view(db, "_all_docs", keys=rawseeds, include_docs=True):
            if row.get('doc'):
                if 'edition' not in row.doc:
                    doc = web.storage(zeros, **row.doc)
                    
                    # if seed info is not yet created, display edition count as 1
                    if doc.editions == 0 and row.key.startswith("/books"):
                        doc.editions = 1
                    d[row.key] = doc
        return d
        
    def _couchdb_view(self, db, viewname, **kw):
        stats.begin("couchdb", db=db.name, view=viewname, kw=kw)
        try:
            result = db.view(viewname, **kw)
            
            # force fetching the results
            result.rows
        finally:
            stats.end()

        return result
        
    def _get_edition_count(self):
        #return sum(seed['editions'] for seed in self.seed_summary.values())
        return sum(seed.edition_count for seed in self.get_seeds())

    def _get_work_count(self):
        #return sum(seed['works'] for seed in self.seed_summary.values())
        return sum(seed.work_count for seed in self.get_seeds())

    def _get_ebook_count(self):
        #return sum(seed['ebooks'] for seed in self.seed_summary.values())
        return sum(seed.ebook_count for seed in self.get_seeds())
        
    def _get_last_update(self):
        last_updates = [seed.last_update for seed in self.get_seeds()]
        last_updates = [x for x in last_updates if x]
        if last_updates:
            return max(last_updates)
        else:
            return None

        if self.seed_summary:
            date = max(seed['last_update'] for seed in self.seed_summary.values()) or None
        else:
            date = None
        return date and h.parse_datetime(date)

    seed_summary = cached_property("seed_summary", _get_seed_summary)
    
    work_count = cached_property("work_count", _get_work_count)
    edition_count = cached_property("edition_count", _get_edition_count)
    ebook_count = cached_property("ebook_count", _get_ebook_count)
    last_update = cached_property("last_update", _get_last_update)
    
    def preview(self):
        """Return data to preview this list. 
        
        Used in the API.
        """
        return {
            "url": self.key,
            "full_url": self.url(),
            "name": self.name or "",
            "seed_count": len(self.seeds),
            "edition_count": self.edition_count,
            "last_update": self.last_update and self.last_update.isoformat() or None
        }
        
    def get_works(self, limit=50, offset=0):
        keys = [[seed, "works"] for seed in self._get_rawseeds()]
        rows = self._seeds_view(keys=keys, reduce=False, limit=limit, skip=offset)
        return web.storage({
            "count": self.work_count,
            "works": [row.value for row in rows]
        })
        
    def get_couchdb_docs(self, db, keys):
        try:
            stats.begin(name="_all_docs", keys=keys, include_docs=True)
            docs = dict((row.id, row.doc) for row in db.view("_all_docs", keys=keys, include_docs=True))
        finally:
            stats.end()
        return docs

    def get_editions(self, limit=50, offset=0, _raw=False):
        """Returns the editions objects belonged to this list ordered by last_modified. 
        
        When _raw=True, the edtion dicts are returned instead of edtion objects.
        """
        # show at max 10 pages
        MAX_OFFSET = min(self.edition_count, 50 * 10)
        
        if not self.seeds or offset > MAX_OFFSET:
            return {
                "count": 0,
                "offset": offset,
                "limit": limit,
                "editions": []
            }
        
        # We don't want to give more than 500 editions for performance reasons.
        if offset + limit > MAX_OFFSET:
            limit = MAX_OFFSET - offset
            
        key_data = []
        rawseeds = self._get_rawseeds()
        for seeds in web.group(rawseeds, 50):
            key_data += self._get_edition_keys(seeds, limit=MAX_OFFSET)
        keys = [key for key, last_modified in sorted(key_data, key=lambda x: x[1], reverse=True)]
        keys = keys[offset:limit]
        
        # Get the documents from couchdb 
        docs = self.get_couchdb_docs(self._get_editions_db(), keys)

        def get_doc(key):
            doc = docs[key]
            del doc['_id']
            del doc['_rev']
            if not _raw:
                data = self._site._process_dict(common.parse_query(doc))
                doc = client.create_thing(self._site, doc['key'], data)
            return doc
        
        d = {
            "count": self.edition_count,
            "offset": offset,
            "limit": limit,
            "editions": [get_doc(key) for key in keys]
        }
        
        if offset + limit < MAX_OFFSET:
            d['next_params'] = {
                'offset': offset+limit
            }
            
        if offset > 0:
            d['prev_params'] = {
                'offset': max(0, offset-limit)
            }
        return d
    
    def _get_edition_keys(self, rawseeds, offset=0, limit=500):
        """Returns a tuple of (key, last_modified) for all editions associated with given seeds.
        """
        d = self._editions_view(rawseeds,
            skip=offset, limit=limit,
            sort="last_modified", reverse="true",
            stale="ok")
        return [(row['id'], row['sort_order'][0]) for row in d['rows']]
        
    def get_all_editions(self):
        """Returns all the editions of this list in arbitrary order.
        
        The return value is an iterator over all the edtions. Each entry is a dictionary.
        (Compare the difference with get_editions.)
        
        This works even for lists with too many seeds as it doesn't try to
        return editions in the order of last-modified.
        """
        rawseeds = self._get_rawseeds()
        
        def get_edition_keys(seeds):
            d = self._editions_view(seeds, limit=10000, stale="ok")
            return [row['id'] for row in d['rows']]
            
        keys = set()
        
        # When there are too many seeds, couchdb-lucene fails because the query URL is too long.
        # Splitting the seeds into groups of 50 to avoid that trouble.
        for seeds in web.group(rawseeds, 50):
            keys.update(get_edition_keys(seeds))
        
        # Load docs from couchdb now.
        for chunk in web.group(keys, 1000):
            docs = self.get_couchdb_docs(self._get_editions_db(), chunk)
            for doc in docs.values():
                del doc['_id']
                del doc['_rev']
                yield doc
            
    def _preload(self, keys):
        keys = list(set(keys))
        return self._site.get_many(keys)
        
    def preload_works(self, editions):
        return self._preload(w.key for e in editions 
                                   for w in e.get('works', []))
        
    def preload_authors(self, editions):
        works = self.preload_works(editions)
        return self._preload(a.author.key for w in works
                                          for a in w.get("authors", [])
                                          if "author" in a)
                            
    def load_changesets(self, editions):
        """Adds "recent_changeset" to each edition.
        
        The recent_changeset will be of the form:
            {
                "id": "...",
                "author": {
                    "key": "..", 
                    "displayname", "..."
                }, 
                "timestamp": "...",
                "ip": "...",
                "comment": "..."
            }
         """
        for e in editions:
            if "recent_changeset" not in e:
                try:
                    e['recent_changeset'] = self._site.recentchanges({"key": e.key, "limit": 1})[0]
                except IndexError:
                    pass

    def _get_solr_query_for_subjects(self):
        terms = [seed.get_solr_query_term() for seed in self.get_seeds()]
        return " OR ".join(t for t in terms if t)
    
    def _get_all_subjects(self):
        solr = get_works_solr()
        q = self._get_solr_query_for_subjects()
        facet_names = ['subject_facet', 'place_facet', 'person_facet', 'time_facet']
        result = solr.select(q, 
            fields=[], 
            facets=facet_names,
            facet_limit=20,
            facet_mincount=1)

        def get_subject_prefix(facet_name):
            name = facet_name.replace("_facet", "")
            if name == 'subject':
                return ''
            else:
                return name + ":"

        def process_subject(facet_name, title, count):
            prefix = get_subject_prefix(facet_name)
            key = prefix + title.lower().replace(" ", "_")
            url = "/subjects/" + key
            return web.storage({
                "title": title,
                "name": title,
                "count": count,
                "key": key,
                "url": url
            })

        def process_all():
            facets = result['facets']
            for k in facet_names:
                for f in facets.get(k, []):
                    yield process_subject(f.name, f.value, f.count)

        return sorted(process_all(), reverse=True, key=lambda s: s["count"])

    def get_top_subjects(self, limit=20):
        return self._get_all_subjects()[:limit]
        
    def get_subjects(self, limit=20):
        def get_subject_type(s):
            if s.url.startswith("/subjects/place:"):
                return "places"
            elif s.url.startswith("/subjects/person:"):
                return "people"
            elif s.url.startswith("/subjects/time:"):
                return "times"
            else:
                return "subjects"
        
        d = web.storage(subjects=[], places=[], people=[], times=[])

        for s in self._get_all_subjects():
            kind = get_subject_type(s)
            if len(d[kind]) < limit:
                d[kind].append(s)
        return d
        
    def get_seeds(self, sort=False):
        seeds = [Seed(self, s) for s in self.seeds]
        if sort:
            seeds = h.safesort(seeds, reverse=True, key=lambda seed: seed.last_update)
        return seeds
        
    def get_seed(self, seed):
        if isinstance(seed, dict):
            seed = seed['key']
        return Seed(self, seed)
        
    def has_seed(self, seed):
        if isinstance(seed, dict):
            seed = seed['key']
        return seed in self._get_rawseeds()
    
    # cache the default_cover_id for 60 seconds
    @cache.memoize("memcache", key=lambda self: ("d" + self.key, "default-cover-id"), expires=60)
    def _get_default_cover_id(self):
        for s in self.get_seeds():
            cover = s.get_cover()
            if cover:
                return cover.id
    
    def get_default_cover(self):
        from openlibrary.core.models import Image
        cover_id = self._get_default_cover_id()
        return Image(self._site, 'b', cover_id)
        
    def _get_seeds_db(self):
        db_url = config.get("lists", {}).get("seeds_db")
        if not db_url:
            return None
        
        return couchdb.Database(db_url)
        
    def _get_editions_db(self):
        db_url = config.get("lists", {}).get("editions_db")
        if not db_url:
            return None
        
        return couchdb.Database(db_url)
        
    def _updates_view(self, **kw):
        view_url = config.get("lists", {}).get("updates_view")
        if not view_url:
            return []
            
        kw['stale'] = 'ok'
        view = couchdb.client.PermanentView(view_url, "updates_view")
        return view(**kw)

    def _editions_view(self, seeds, **kw):
        reverse = str(kw.pop("reverse", "")).lower()
        if 'sort' in kw and reverse == "true":
            # sort=\field is the couchdb-lucene's way of telling ORDER BY field DESC
            kw['sort'] = '\\' + kw['sort']
        view_url = config.get("lists", {}).get("editions_view")
        if not view_url:
            return {}

        def escape(value):
            special_chars = '+-&|!(){}[]^"~*?:\\'
            pattern = "([%s])" % re.escape(special_chars)
            
            quote = '"'
            return quote + web.re_compile(pattern).sub(r'\\\1', value) + quote
        
        q = " OR ".join("seed:" + escape(seed.encode('utf-8')) for seed in seeds)
        url = view_url + "?" + urllib.urlencode(dict(kw, q=q))
        
        stats.begin("couchdb", url=url)
        try:
            json = urllib2.urlopen(url).read()
        finally:
            stats.end()
        return simplejson.loads(json)

def valuesort(d):
    """Sorts the keys in the dictionary based on the values.
    """
    return sorted(d, key=lambda k: d[k])
    
class Seed:
    """Seed of a list.
    
    Attributes:
        * work_count
        * edition_count
        * ebook_count
        * last_update
        * type - "edition", "work" or "subject"
        * document - reference to the edition/work document
        * title
        * url
        * cover
    """
    def __init__(self, list, value):
        self._list = list
        
        self.value = value
        if isinstance(value, basestring):
            self.key = value
            self.type = "subject"
        else:
            self.key = value.key

        self._solrdata = None
            
    def get_document(self):
        if isinstance(self.value, basestring):
            doc = get_subject(self.get_subject_url(self.value))
        else:
            doc = self.value
            
        # overwrite the property with the actual value so that subsequent accesses don't have to compute the value.
        self.document = doc
        return doc
            
    document = property(get_document)

    def _get_document_basekey(self):
        return self.document.key.split("/")[-1]

    def get_solr_query_term(self):
        if self.type == 'edition':
            return "edition_key:" + self._get_document_basekey()
        elif self.type == 'work':
            return 'key:' + self._get_document_basekey()
        elif self.type == 'author':
            return "author_key:" + self._get_document_basekey()
        elif self.type == 'subject':
            type, value = self.key.split(":")
            return "%s_key:%s" % (type, value)

    def get_solrdata(self):
        if self._solrdata is None:
            self._solrdata = self._load_solrdata()
        return self._solrdata
    
    def _load_solrdata(self):
        if self.type == "edition":
            return {
                'ebook_count': int(bool(self.document.ocaid)),
                'edition_count': 1, 
                'work_count': 1,
                'last_update': self.document.last_modified
            }
        else:
            q = self.get_solr_query_term()
            if q:
                solr = get_works_solr()
                result = solr.select(q, fields=["edition_count", "ebook_count_i"])
                last_update_i = [doc['last_update_i'] for doc in result.docs if 'last_update_i' in doc]
                if last_update_i:
                    last_update = self._inttime_to_datetime(last_update_i)
                else:
                    # if last_update is not present in solr, consider last_modfied of
                    # that document as last_update
                    if self.type in ['work', 'author']:
                        last_update = self.document.last_modified
                    else:
                        last_update = None
                return {
                    'ebook_count': sum(doc.get('ebook_count_i', 0) for doc in result.docs),
                    'edition_count': sum(doc.get('edition_count', 0) for doc in result.docs),
                    'work_count': 0,
                    'last_update': last_update
                }
        return {}

    def _inttime_to_datetime(self, t):
        return datetime.datetime(*time.gmtime(t)[:6])

    def get_type(self):
        type = self.document.type.key
        
        if type == "/type/edition":
            return "edition"
        elif type == "/type/work":
            return "work"
        elif type == "/type/author":
            return "author"
        else:
            return "unknown"
            
    type = property(get_type)
    
    def _get_summary(self):
        summary = self._list.seed_summary
        return summary.get(self.key, defaultdict(lambda: 0))
        
    def get_title(self):
        if self.type == "work" or self.type == "edition":
            return self.document.title or self.key
        elif self.type == "author":
            return self.document.name or self.key
        elif self.type == "subject":
            return self._get_subject_title()
        else:
            return self.key
    
    def _get_subject_title(self):
        subjects = self._get_summary().get("subjects")
        if subjects:
            return subjects[0]['name']
        else:
            return self.key.replace("_", " ")
            
    def get_url(self):
        if self.document:
            return self.document.url()
        else:
            if self.key.startswith("subject:"):
                return "/subjects/" + web.lstrips(self.key, "subject:")
            else:
                return "/subjects/" + self.key
                
    def get_subject_url(self, subject):
        if subject.startswith("subject:"):
            return "/subjects/" + web.lstrips(subject, "subject:")
        else:
            return "/subjects/" + subject
            
    def get_cover(self):
        if self.type in ['work', 'edition']:
            return self.document.get_cover()
        elif self.type == 'author':
            return self.document.get_photo()
        elif self.type == 'subject':
            return self.document.get_default_cover()
        else:
            return None
            
    def _get_last_update(self):
        return self.get_solrdata().get("last_update") or None

    def _get_ebook_count(self):
        return self.get_solrdata().get('ebook_count', 0)

    def _get_edition_count(self):
        return self.get_solrdata().get('edition_count', 0)

    def _get_work_count(self):
        return self.get_solrdata().get('work_count', 0)
        
    work_count = property(_get_work_count)
    edition_count = property(_get_edition_count)
    ebook_count = property(_get_ebook_count)
    last_update = property(_get_last_update)

    
    title = property(get_title)
    url = property(get_url)
    cover = property(get_cover)
    
    def dict(self):
        if self.type == "subject":
            url = self.url
            full_url = self.url
        else:
            url = self.key
            full_url = self.url
        
        d = {
            "url": url,
            "full_url": full_url,
            "type": self.type,
            "title": self.title,
            "work_count": self.work_count,
            "edition_count": self.edition_count,
            "ebook_count": self.ebook_count,
            "last_update": self.last_update and self.last_update.isoformat() or None
        }
        cover = self.get_cover()
        if cover:
            d['picture'] = {
                "url": cover.url("S")
            }
        return d
    
    def __repr__(self):
        return "<seed: %s %s>" % (self.type, self.key)
    __str__ = __repr__
    
def crossproduct(A, B):
    return [(a, b) for a in A for b in B]

########NEW FILE########
__FILENAME__ = engine

########NEW FILE########
__FILENAME__ = test_engine
from openlibrary.core.lists import engine

def test_reduce():
    def test_reduce(self):
        d1 = [1, 2, 1, "2010-11-11 10:20:30", {
            "subjects": ["Love", "Hate"]
        }]

        d2 = [1, 1, 0, "2009-01-02 10:20:30", {
            "subjects": ["Love"]
        }]
        assert engine.reduce([d1, d2]) == {
            "works": 2,
            "editions": 3,
            "ebooks": 1,
            "last_modified": "2010-11-11 10:20:30",
            "subjects": [
                {"name": "Love", "key": "subject:love", "count": 2},
                {"name": "Hate", "key": "subject:hate", "count": 1}
            ]
        }
    
########NEW FILE########
__FILENAME__ = test_model
import web

from openlibrary.mocks import mock_couchdb
from openlibrary.core.lists import model

class MockCouchDB:
    def __init__(self, docs):
        self.docs = docs
    
    def view(self, name, keys=None, include_docs=False):
        return [web.storage(id=doc['_id'], key=doc['_id'], value=doc, doc=doc) for doc in self.docs]

class MockList(model.ListMixin):
    def __init__(self, db, seeds):
        self.db = db
        self.seeds = seeds
        
    def _get_seeds_db(self):
        return self.db

class TestListMixin:
    def test_simple(self):
        doc1 = {
            "_id": "/works/OL1W",
            "editions": 10, 
            "works": 1,
            "ebooks": 2, 
            "subjects": [
                {"key": "subject:love", "name": "Love", "count": 2}
            ]
        }

        doc2 = {
            "_id": "/authors/OL1A",
            "editions": 10, 
            "works": 5, 
            "ebooks": 2, 
            "subjects": [
                {"key": "place:san_francisco", "name": "San Francisco", "count": 3},
                {"key": "subject:love", "name": "Love", "count": 2}
            ]
        }
        
        db = mock_couchdb.Database()
        db.update([doc1, doc2])
        list = MockList(db, ['/works/OL1W', '/authors/OL1A'])
        
        assert list.edition_count == 20
        assert list.work_count == 6
        assert list.ebook_count == 4
        assert list.get_top_subjects() == [
            {"key": "subject:love", "url": "/subjects/love", "name": "Love", "title": "Love", "count": 4},
            {"key": "place:san_francisco", "url": "/subjects/place:san_francisco", "name": "San Francisco", "title": "San Francisco", "count": 3}
        ]
        assert list.get_top_subjects(limit=1) == [
            {"key": "subject:love", "url": "/subjects/love", "name": "Love", "title": "Love", "count": 4}
        ]
        
class TestSeed:
    def test_subject_url(self, monkeypatch):
        from openlibrary.plugins.worksearch import subjects
        from openlibrary.core import models
        monkeypatch.setattr(subjects, "get_subject", lambda key: models.Subject(key=key, name=key))
        
        Seed = model.Seed
        print Seed(None, "subject:foo").url
        assert Seed(None, "subject:foo").url == "/subjects/foo"
        assert Seed(None, "person:foo").url == "/subjects/person:foo"
        assert Seed(None, "place:foo").url == "/subjects/place:foo"
        assert Seed(None, "time:foo").url == "/subjects/time:foo"

########NEW FILE########
__FILENAME__ = test_updater
from openlibrary.mocks import mock_couchdb
from openlibrary.core.lists import updater
import re
import os
import simplejson

def read_couchapp(path):
    """Reads the couchapp from repository_root/couchapps/$path.
    """
    import openlibrary
    root = os.path.dirname(openlibrary.__file__) + "/../couchapps/" + path

    def read(path):
        if os.path.isdir(path):
            d = {}
            for f in os.listdir(path):
                name = os.path.splitext(f)[0]
                value = read(os.path.join(path, f))
                d[name] = value
            return d
        else:
            value = open(path).read().strip()
            # special care to make this function compatible with couchapp.
            # couchapp dumps the json after the second nested level.
            if value.startswith("{"):
                value = simplejson.loads(value)
            return value
    
    return read(root)
    
class TestWorksDB:
    def setup_method(self, method):
        db = mock_couchdb.Database()
        db.save(read_couchapp("works/seeds"))
        db['_design/seeds2'] = db['_design/seeds']
        self.works_db = updater.WorksDB(db)
        self.ctx = updater.UpdaterContext()
        
    def _get_doc(self, key):
        doc = self.works_db.db.get(key)
        if doc:
            del doc['_id']
            del doc['_rev']
        return doc
        
    def _add_doc(self, doc):
        doc['_id'] = doc['key']
        doc.setdefault("type", {"key": "/type/work"})
        self.works_db.db.save(doc)
        
    def test_update_work(self):
        # test adding for the first time
        self.works_db.update_work(self.ctx, {
            "key": "/works/OL1W",
            "title": "foo"
        })
        
        assert self._get_doc("/works/OL1W") == {
            "key": "/works/OL1W",
            "title": "foo",
            "editions": []
        }
        
        # test changing the title
        self.works_db.update_work(self.ctx, {
            "key": "/works/OL1W",
            "title": "bar"
        })
        assert self._get_doc('/works/OL1W') == {
            "key": "/works/OL1W",
            "title": "bar",
            "editions": []
        }
        
    def test_update_work_with_editions(self):
        # setup database
        self.works_db.db['/works/OL1W'] = {
            "key": "/works/OL1W",
            "title": "foo",
            "subjects": ["x"],
            "editions": [
                {"key": "/books/OL1M"}
            ]
        }

        self.works_db.update_work(self.ctx, {
            "key": "/works/OL1W",
            "title": "bar",
        })
        
        assert self._get_doc("/works/OL1W") == {
            "key": "/works/OL1W",
            "title": "bar",
            "editions": [
                {"key": "/books/OL1M"}
            ]
        }
        
    def test_update_edition(self):
        self._add_doc({
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "title": "foo",
            "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
        })
        
        self.works_db.update_edition(self.ctx, {
          "key": "/books/OL1M",
          "works": [{"key": "/works/OL1W"}],
          "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
        })
        
        assert self._get_doc("/works/OL1W") == {
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "title": "foo",
            "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"},
            "editions": [{
                "key": "/books/OL1M",
                "works": [{"key": "/works/OL1W"}],
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}                
            }]
        }
    
    def test_mock_view(self):
        """Test to makesure the mock couchdb is working as expected.
        """
        self._add_doc({
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "editions": [{"key": "/books/OL1M"}]
        })
        print self._view("seeds/seeds")
        assert self._view("seeds/seeds") == {
            "total_rows": 2,
            "offset": 0,
            "rows": [
                {"id": "/works/OL1W", "key": "/books/OL1M", "value": [1, 1, 0, "", {}]},
                {"id": "/works/OL1W", "key": "/works/OL1W", "value": [1, 1, 0, "", {}]}
            ]
        }
        
    def _view(self, name, **options):
        results = self.works_db.db.view(name, **options)
        return {
            "total_rows": results.total_rows,
            "offset": results.offset,
            "rows": [dict(row) for row in results.rows]
        }
        
    def test_update_edition_work_change(self):
        self.works_db.db['/works/OL1W'] = {
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "editions": [{"key": "/books/OL1M"}],
            "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
        }
        self.works_db.db['/works/OL2W'] = {
            "key": "/works/OL2W",
            "type": {"key": "/type/work"},
            "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
        }
        assert self.works_db._get_old_work_key({"key": "/books/OL1M"}) == "/works/OL1W"
        
        self.works_db.update_edition(self.ctx, {
          "key": "/books/OL1M",
          "works": [{"key": "/works/OL2W"}],
          "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
        })
        
        assert self._get_doc("/works/OL1W") == {
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"},
            "editions": []
        }
        assert self._get_doc("/works/OL2W") == {
            "key": "/works/OL2W",
            "type": {"key": "/type/work"},
            "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"},
            "editions": [{
                "key": "/books/OL1M",
                "works": [{"key": "/works/OL2W"}],
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}            
            }]
        }
        
class TestUpdater:
    def setup_method(self, method):
        
        databases = {}
        self.databases = databases

        databases["works"] = mock_couchdb.Database()
        databases["editions"] = mock_couchdb.Database()
        databases["seeds"] = mock_couchdb.Database()
        
        
        class Updater(updater.Updater):
            def create_db(self, url):
                return databases[url]
                
        config = dict(works_db="works", editions_db="editions", seeds_db="seeds")
        self.updater = Updater(config)
        
        databases["works"].save(read_couchapp("works/seeds"))
        databases['works']['_design/seeds2'] = databases['works']['_design/seeds']
        databases['seeds'].save(read_couchapp("seeds/sort"))
        databases['seeds'].save(read_couchapp("seeds/dirty"))
        
    def _get_doc(self, dbname, id):
        doc = self.databases[dbname].get(id)
        if doc:
            del doc["_id"]
            del doc["_rev"]
        return doc
        
    def _update_pending_seeds(self):
        seeds = [row.id for row in self.databases['seeds'].view("dirty/dirty")]
        self.updater.update_seeds(seeds)
        
    def test_process_changeset(self):
        # Add a new book and new work and make sure the works, editions and seeds databases are updated.
        changeset = {
            "id": "1234",
            "author": {"key": "/people/anand"},
            "docs": [{
                "key": "/works/OL1W",
                "type": {"key": "/type/work"},
                "subjects": ["Love"],
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
            }, {
                "key": "/books/OL1M",
                "type": {"key": "/type/edition"},
                "works": [{"key": "/works/OL1W"}],
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
            }]
        }
        
        self.updater.process_changeset(changeset)
        self._update_pending_seeds()
        
        def get_docids(db):
            return sorted(k for k in db if not k.startswith("_"))
        
        assert get_docids(self.databases['works']) == ["/works/OL1W"]
        assert get_docids(self.databases['editions']) == ["/books/OL1M"]
        assert get_docids(self.databases['seeds']) == ["/books/OL1M", "/works/OL1W", "subject:love"]
        
        assert self._get_doc("editions", "/books/OL1M") == {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "works": [{"key": "/works/OL1W"}],
            "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"},
            "seeds": ["/works/OL1W", "subject:love", "/books/OL1M"]
        }
        
        assert self._get_doc("seeds", "/works/OL1W") == {
            "works": 1,
            "editions": 1,
            "ebooks": 0,
            "last_update": "2010-10-20 10:20:30",
            "subjects": [
                {"name": "Love", "key": "subject:love", "count": 1}
            ]
        }
        
        # Add a new book to the same work and make sure the seed info is updated
        changeset = {
            "id": "1235",
            "author": {"key": "/people/anand"},
            "docs": [{
                "key": "/books/OL2M",
                "type": {"key": "/type/edition"},
                "works": [{"key": "/works/OL1W"}],
                "ocaid": "foobar",
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 15:25:35"}
            }]
        }
        
        self.updater.process_changeset(changeset)
        self._update_pending_seeds()
        
        assert self._get_doc("seeds", "/works/OL1W") == {
            "works": 1,
            "editions": 2,
            "ebooks": 1,
            "last_update": "2010-10-20 15:25:35",
            "subjects": [
                {"name": "Love", "key": "subject:love", "count": 1}
            ]
        }
        
        # Update an old edition and make sure the seed info is updated
        changeset = {
            "id": "1236",
            "author": {"key": "/people/anand"},
            "docs": [{
                "key": "/books/OL1M",
                "type": {"key": "/type/edition"},
                "works": [{"key": "/works/OL1W"}],
                "last_modified": {"type": "/type/datetime", "value": "2010-10-25 15:25:35"}
            }]
        }
        self.updater.process_changeset(changeset)
        self._update_pending_seeds()
        
        assert self._get_doc("seeds", "/works/OL1W") == {
            "works": 1,
            "editions": 2,
            "ebooks": 1,
            "last_update": "2010-10-25 15:25:35",
            "subjects": [
                {"name": "Love", "key": "subject:love", "count": 1}
            ]
        }

    def test_process_changeset_old_seed(self):
        # save a book and work 
        changeset = {
            "id": "1234",
            "author": {"key": "/people/anand"},
            "docs": [{
                "key": "/works/OL1W",
                "type": {"key": "/type/work"},
                "subjects": ["Love"],
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
            }, {
                "key": "/books/OL1M",
                "type": {"key": "/type/edition"},
                "works": [{"key": "/works/OL1W"}],
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 10:20:30"}
            }]
        }
        self.updater.process_changeset(changeset)
        self._update_pending_seeds()
        
        # change the subject and make sure the counts of old changeset are updated
        changeset = {
            "id": "1235",
            "author": {"key": "/people/anand"},
            "docs": [{
                "key": "/works/OL1W",
                "type": {"key": "/type/work"},
                "last_modified": {"type": "/type/datetime", "value": "2010-10-20 15:25:35"}
            }]
        }
        self.updater.process_changeset(changeset)
        self._update_pending_seeds()
        
        assert self._get_doc("seeds", "subject:love") == None

########NEW FILE########
__FILENAME__ = updater
import collections
import datetime
import itertools
import logging
import re
import time
import urllib2

try:
    import multiprocessing
except ImportError:
    multiprocessing = None

import simplejson
import web

from openlibrary.core import formats, couch
import engine

logger = logging.getLogger("openlibrary.lists")

class UpdaterContext:
    """The state of the updater.
    
    The updater context contains 3 dicts to store works, editions and seeds.
    """
    def __init__(self):
        self.editions = {}
        self.works = {}
        self.seeds = {}
        
    def add_editions(self, editions):
        for e in editions:
            self.editions[e['key']] = e
            
    def add_seeds(self, seeds):
        for s in seeds:
            self.seeds[s] = None
            
class Timer:
    def __init__(self):
        self.names = collections.defaultdict(lambda: web.storage(time=0.0, count=0))
        
    def start(self, name):
        d = self.names[name]
        d.start = time.time()
    
    def end(self, name):
        d = self.names[name]
        d.count += 1
        d.time += time.time() - d.start
        
    def print_summary(self):
        for k in sorted(self.names, key=lambda name: self.names[name].time):
            d = self.names[name]
            print "%s\t%s\t%s" % (k, d.time, d.count)
             
class CachedDatabase:
    """CouchDB Database that caches some documents locally for faster access.
    """
    def __init__(self, db):
        self.db = db
        self.docs = {}
        self.dirty = {}
        
    def preload(self, keys):
        for row in self.db.view("_all_docs", keys=keys, include_docs=True):
            if "doc" in row:
                self.docs[row.id] = row.doc
        
    def get(self, key, default=None):
        return self.docs.get(key) or self.db.get(key, default)
        
    def view(self, name, **kw):
        if kw.get("stale") != "ok":
            self.commit()
        return self.db.view(name, **kw)
    
    def save(self, doc):
        self.docs[doc['_id']] = doc
        self.dirty[doc['_id']] = doc
        
    def commit(self):
        if self.dirty:
            logger.info("commiting %d docs" % len(self.dirty))
            self.db.update(self.dirty.values())
            self.dirty.clear()
        
    def reset(self):
        self.commit()
        self.docs.clear()
        
    def __getattr__(self, name):
        return getattr(self.db, name)

class Updater:
    def __init__(self, config):
        """Creates an updater instatance from the given configuration.
        
        The config must be a dictionary containing works_db, editions_db and
        seeds_db, each a url of couchdb database.
        """
        def f(name):
            return self.create_db(config[name])
            
        def f2(name):
            return CachedDatabase(f(name))
            
        self.works_db = WorksDB(f2("works_db"))
        #self.works_db = WorksDB(f("works_db"))

        self.editions_db = EditionsDB(f("editions_db"), self.works_db)
        self.seeds_db = SeedsDB(f("seeds_db"), self.works_db)
        
    def create_db(self, url):
        return couch.Database(url)
    
    def process_changesets(self, changesets, update_seeds=False):
        """Updates the lists databases for given changesets.
        
        Seeds are updated in the seeds db if update_seeds is True, otherwise they are marked for later update.
        """
        logger.info("BEGIN process_changesets")
        ctx = UpdaterContext()
        for chunk in web.group(changesets, 50):
            chunk = list(chunk)
            logger.info("processing changesets %s", [c['id'] for c in chunk])
            
            works = [work for changeset in chunk 
                          for work in self._get_works(changeset)]

            editions = [e for changeset in chunk
                        for e in self._get_editions(changeset)]
                        
            logger.info("found %d works and %d editions", len(works), len(editions))
                        
            keys = [w['key'] for w in works] + [e['works'][0]['key'] for e in editions if e.get('works')] 
            keys = list(set(keys))
            self.works_db.db.preload(keys)
            
            for work in works:
                work = self.works_db.update_work(ctx, work)

            # works have been modified. Commit to update the views.
            logger.info("BEGIN commit works_db")
            self.works_db.db.commit()
            logger.info("END commit works_db")
            
            self.works_db.update_editions(ctx, editions)
            self.editions_db.update_editions(ctx.editions.values())
            ctx.editions.clear()
            
            t = datetime.datetime.utcnow().isoformat()
            if ctx.seeds:
                logger.info("BEGIN commit works_db")
                self.works_db.db.commit()
                logger.info("END commit works_db")
                
                logger.info("BEGIN mark %d seeds for update" % len(ctx.seeds))
                if update_seeds:
                    self.seeds_db.update_seeds(ctx.seeds.keys())
                else:
                    self.seeds_db.mark_seeds_for_update(ctx.seeds.keys())
                logger.info("END mark %d seeds for update" % len(ctx.seeds))
                ctx.seeds.clear()
            
            # reset to limit the make sure the size of cache never grows without any limit.
            if len(self.works_db.db.docs) > 1000:
                self.works_db.db.reset()
                
        self.works_db.db.commit()
        self.works_db.db.reset()
        logger.info("END process_changesets")
        
    def process_changeset(self, changeset, update_seeds=False):
        logger.info("processing changeset %s", changeset["id"])
        return self.process_changesets([changeset], update_seeds=update_seeds)

    def update_seeds(self, seeds):
        self.seeds_db.update_seeds(seeds)
        
    def update_pending_seeds(self, limit=100):
        logger.info("BEGIN update_pending_seeds")
        rows = self.seeds_db.db.view("dirty/dirty", limit=limit)
        seeds = [row.id for row in rows]
        logger.info("found %d seeds", len(seeds))
        if seeds:
            self.update_seeds(seeds)
        logger.info("END update_pending_seeds")
        return seeds
        
    def _get_works(self, changeset):
        return [doc for doc in changeset.get("docs", []) 
                    if doc['key'].startswith("/works/")]
    
    def _get_editions(self, changeset):
        return [doc for doc in changeset.get("docs", [])
                    if doc['key'].startswith("/books/")]
    
class WorksDB:
    """The works db contains the denormalized works.
    
    This class provides methods to update the database whenever a work or an
    edition is updated.
    """
    def __init__(self, db):
        self.db = db
        
    def update_works(self, ctx, works):
        pass
        
    def update_work(self, ctx, work, get=None, save=None):
        """Adds/Updates the given work in the database.
        """
        get = get or self.db.get
        save = save or self.db.save
        
        logger.info("works_db: updating work %s", work['key'])
        
        key = work['key']
        old_work = get(key, {})
        if old_work:
            if "_rev" in old_work:  # ugly side-effect of CachedDatabase.
                work['_rev'] = old_work['_rev']
            work['editions'] = old_work.get('editions', [])
            
            old_seeds = set(engine.get_seeds(old_work))
        else:
            work['editions'] = []
            old_seeds = set()

        work['_id'] = work['key']
        new_seeds = set(engine.get_seeds(work))
        
        # Add both old and new seeds to the queue
        ctx.add_seeds(old_seeds)
        ctx.add_seeds(new_seeds)
        
        def non_edition_seeds(seeds):
            return sorted(seed for seed in seeds if not seed.startswith("/books"))
        
        # Add editions to the queue if any of the seeds are modified
        if non_edition_seeds(old_seeds) != non_edition_seeds(new_seeds):
            ctx.add_editions(work['editions'])
            
        save(work)
        return work
        
    def update_editions(self, ctx, editions):
        editions = list(editions)
        logger.info("works_db: BEGIN update_editions %s", len(editions))
        # remove duplicate entries from editions
        editions = dict((e['key'], e) for e in editions).values()
        keys = [e['key'] for e in editions]
        
        old_works = dict((row.key, row.doc) 
                        for row in self.db.view("seeds/seeds", keys=keys, include_docs=True) 
                        if "doc" in row)
                        
        def get_work_key(e):
            if e.get('works'):
                return e['works'][0]['key']
                        
        work_keys = list(set(get_work_key(e) for e in editions if get_work_key(e) is not None))
        works = dict((row.id, row.doc) 
                    for row in self.db.view("_all_docs", keys=work_keys, include_docs=True) 
                    if "doc" in row)
                    
        def get_seeds(work):
            # returns non edition seeds
            if work:
                return sorted(engine.get_seeds(work))
            else:
                return []

        for e in editions:
            key = e['key']
            logger.info("works_db: updating edition %s", key)
            
            ctx.add_editions([e])
            ctx.add_seeds([key])
            
            work_key = get_work_key(e)
            
            if key in old_works:
                old_work = old_works[key]
            else:
                old_work = None
                
            old_seeds = get_seeds(old_work)
            
            # unlink the edition from old work if required
            if old_work and old_work['key'] != work_key:
                self._delete_edtion(old_work, e)
                self.db.save(old_work)
                
                # one edition has been removed. update the seeds.
                ctx.add_seeds(old_seeds)
            
            # Add/update the edition to the current work
            work = work_key and works.get(work_key)
            if work is None:
                # Either the edition has no work or the work is not found the db.
                return None

            # last update of all these seeds must be updated now.
            new_seeds = get_seeds(work)
            ctx.add_seeds(new_seeds)
                
            self._add_edition(work, e)
            self.db.save(work)
            
        logger.info("works_db: END update_editions %s", len(editions))
            
    def update_edition(self, ctx, edition):
        self.update_editions(ctx, [edition])
        
    def _delete_edtion(self, work, edition):
        editions = dict((e['key'], e) for e in work.get('editions', []))
        
        key = edition['key']
        if key in editions:
            del editions[key]
            work['editions'] = editions.values()
        
    def _add_edition(self, work, edition):
        editions = dict((e['key'], e) for e in work.get('editions', []))
        editions[edition['key']] = edition
        work['editions'] = editions.values()
        
    def _has_edition(self, work, edition):
        editions = dict((e['key'], e) for e in work.get('editions', []))
        return edition['key'] in editions
                
    def _get_work(self, edition):
        works = edition.get('works', [])
        if works:
            work_key = works[0]['key']
            return self.db.get(work_key)
        
    def _get_old_work_key(self, edition):
        try:
            return self.db.view("seeds/seeds", key=edition['key']).rows[0].id
        except IndexError:
            return None
            
    def get_works(self, seed, chunksize=1000):
        rows = self.db.view("seeds/seeds", key=seed, include_docs=True).rows
        return (row.doc for row in rows)
                        
class EditionsDB:
    def __init__(self, db, works_db):
        self.db = db
        self.works_db = works_db
            
    def update_editions(self, editions):
        logger.info("edition_db: BEGIN updating %d editions", len(editions))
        
        def get_work_key(e):
            if e.get('works'):
                return e['works'][0]['key']
                        
        work_keys = list(set(get_work_key(e) for e in editions if get_work_key(e) is not None))
        works = dict((row.id, row.doc) 
                    for row in self.works_db.db.view("_all_docs", keys=work_keys, include_docs=True) 
                    if "doc" in row)
                    
        def get_seeds(work):
            """Return non edition seeds of a work."""
            return [seed for seed in engine.get_seeds(work) if not seed.startswith("/books/")]
                    
        seeds = dict((w['key'], get_seeds(w)) for w in works.values())
        
        for e in editions:
            key = e['key']
            logger.info("edition_db: updating edition %s", key)
            if e['type']['key'] == '/type/edition':
                wkey = get_work_key(e)
                e['seeds'] = seeds.get(wkey, []) + [e['key']]
            else:
                e.clear()
                e['_deleted'] = True
            e['_id'] = key
        
        logger.info("edition_db: saving...")
        couchdb_bulk_save(self.db, editions)
        logger.info("edition_db: END updating %d editions", len(editions))

class SeedsDB:
    """The seed db stores summary like work_count, edition_count, ebook_count,
    last_update, subjects etc for each seed.
    
    The class provides methods to update the seeds database by reprocessing
    the associated work docs from works db.
    """
    def __init__(self, db, works_db):
        self.db = db
        self.works_db = works_db
        self._big_seeds = None
        
    def mark_seeds_for_update(self, seeds):
        # XXX-Anand: temporarily disable updates as the node hosting seeds_db is low on disk
        return
        docs = {}
        t = datetime.datetime.utcnow().isoformat()
        
        for row in self.db.view("_all_docs", keys=seeds, include_docs=True).rows:
            if 'doc' in row:
                # handle deleted docs
                doc = row.doc or {"_rev": row.value['rev'], "_id": row.id}
                doc = dict(doc, dirty=t)
                docs[row.key] = doc
            
        for seed in seeds:
            if seed not in docs:
                docs[seed] = {"_id": seed, "dirty": t}

        couchdb_bulk_save(self.db, docs.values())
    
    def update_seeds(self, seeds, chunksize=50):
        # XXX-Anand: temporarily disable updates as the node hosting seeds_db is low on disk
        return
        big_seeds = self.get_big_seeds()
        seeds2 = sorted(seed for seed in seeds if seed not in big_seeds)
        
        logger.info("update_seeds %s", len(seeds2))
        logger.info("ignored %d big seeds", len(seeds)-len(seeds2))

        for i, chunk in enumerate(web.group(seeds2, chunksize)):
            chunk = list(chunk)
            logger.info("update_seeds %d %d", i, len(chunk))
            self._update_seeds(chunk)
        
    def _update_seeds(self, seeds):
        counts = self.works_db.db.view("seeds/seeds", keys=list(seeds))

        docs = dict((seed, self._get_seed_doc(seed, rows))
                    for seed, rows 
                    in itertools.groupby(counts, lambda row: row['key']))
                
        for s in seeds:
            if s not in docs:
                docs[s] = {"_id": s, "_deleted": True}
        
        couchdb_bulk_save(self.db, docs.values())
    
    def _get_seed_doc(self, seed, rows):
        doc = engine.reduce_seeds(row['value'] for row in rows)
        doc['_id'] = seed
        return doc
        
    def get_big_seeds(self):
        if not self._big_seeds:
            # consider seeds having more than 500 works as big ones
            self._big_seeds =  set(row.key for row in self.db.view("sort/by_work_count", endkey=500, descending=True, stale="ok"))
            logger.info("found %d big seeds", len(self._big_seeds))
        return self._big_seeds
                
def couchdb_bulk_save(db, docs):
    """Saves/updates the given docs into the given couchdb database using bulk save.
    """
    keys = [doc["_id"] for doc in docs if "_id" in doc]
    revs = dict((row.key, row.value["rev"]) for row in db.view("_all_docs", keys=keys) if "value" in row) 
    
    for doc in docs:
        id = doc.get('_id')
        if id in revs:
            doc['_rev'] = revs[id]
    
    return db.update(docs)

def main(configfile):
    """Creates an updater using the config files and calls it with changesets read from stdin.
    
    Expects one changeset per line in JSON format.
    """
    config = formats.load_yaml(open(configfile).read())
    updater = Updater(config)
    
    changesets = (simplejson.loads(line.strip()) for line in sys.stdin)
    updater.process_changesets(changesets)

if __name__ == "__main__":
    import sys
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = loanstats
"""Module to manage OL loan stats.

Unlike other parts of openlibrary, this modules talks to the database directly.
"""
import re
import time
import datetime
import urllib
import logging
import simplejson
import web
from infogami import config
from . import inlibrary
from .. import i18n

logger = logging.getLogger(__name__)

re_solrescape = re.compile(r'([&|+\-!(){}\[\]^"~*?:])')

class LoanStats:
    def __init__(self, region=None, library=None, country=None, collection=None, subject=None):
        self.base_url = "http://%s/solr" % config.get("stats_solr")
        self.region = region
        self.library = library
        self.country = country
        self.collection = collection
        self.subject = subject
        self.time_period = None
        self.resource_type = None

        self._library_titles = None
        self._facet_counts = None
        self._total_loans = None

    def get_total_loans(self):
        # populate total loans
        self._get_all_facet_counts()
        return self._total_loans

    def solr_select(self, params):
        fq = params.get("fq", [])
        if not isinstance(fq, list):
            fq = [fq]
        params['fq'] = fq
        if self.region:
            params['fq'].append("region_s:" + self.solrescape(self.region))
        if self.library:
            params['fq'].append("library_s:" + self.solrescape(self.library))
        if self.country:
            params['fq'].append("country_s:" + self.solrescape(self.country))

        if self.collection:
            params['fq'].append("ia_collections_id:" + self.solrescape(self.collection))

        if self.subject:
            params['fq'].append(self._get_subject_filter(self.subject))

        if self.time_period:
            start, end = self.time_period
            def solrtime(t): 
                return t.isoformat() + "Z"
            params['fq'].append("start_time_dt:[%s TO %s]" % (solrtime(start), solrtime(end)))

        if self.resource_type:
            params['fq'].append("resource_type_s:%s" % self.resource_type)

        logger.info("SOLR query %s", params)

        q = urllib.urlencode(params, doseq=True)
        url = self.base_url + "/select?" + q
        logger.info("urlopen %s", url)
        response = urllib.urlopen(url).read()
        return simplejson.loads(response)

    def solrescape(self, text):
        return re_solrescape.sub(r'\\\1', text)

    def _get_subject_filter(self, subject):
        # subjects are stored as subject_key field as values like:
        # ["subject:fiction", "subject:history", "place:england"] 
        # etc.
        if ":" in subject:
            type, subject = subject.split(":", 1)
        else:
            type = "subject"
        return "subject_key:%s\\:%s" % (type, self.solrescape(subject))

    def solr_select_facet(self, facet_field):
        facet_counts = self._get_all_facet_counts()
        return facet_counts[facet_field]

    def _run_solr_facet_query(self, facet_fields, facet_limit=None):
        params = {
            "wt": "json",
            "fq": "type:stats", 
            "q": "*:*", 
            "rows": 0,
            "facet": "on",
            "facet.mincount": 1,
            "facet.field": facet_fields
        }
        if facet_limit:
            params["facet.limit"] = facet_limit

        response = self.solr_select(params)
        return dict((name, list(web.group(counts, 2))) for name, counts in response['facet_counts']['facet_fields'].items())

    def _get_all_facet_counts(self):
        if not self._facet_counts:
            facets = [
                "library_s","region_s", "country_s",
                "ia_collections_id", "sponsor_s", "contributor_s",
                "book_key_s", "author_keys_id", "resource_type_s",
                "subject_facet", "place_facet", "person_facet", "time_facet"]

            params = {
                "wt": "json",
                "fq": "type:stats", 
                "q": "*:*", 
                "rows": 0,
                "facet": "on",
                "facet.mincount": 1,
                "facet.field": facets,
                "facet.limit": 20
            }
            response = self.solr_select(params)
            self._total_loans = response['response']['numFound']
            self._facet_counts = dict((name, web.group(counts, 2)) for name, counts in response['facet_counts']['facet_fields'].items())
        return self._facet_counts

    def get_last_updated(self):
        params = {
            "wt": "json",
            "q": "*:*",
            "rows": 1, 
            "sort": "last_updated_dt desc"
        }
        response = self.solr_select(params)
        try:
            return response['response']['docs'][0]['last_updated_dt']
        except (IndexError, KeyError):
            # if last update timestamp is not found in solr,
            # use year 2000 to consider all docs
            return "2000-01-01T00:00:00Z"

    def get_loans_per_day(self, resource_type="total"):
        params = {
            "wt": "json",
            "fq": ["type:stats"],
            "q": "*:*", 
            "rows": 0,
            "facet": "on",
            "facet.mincount": 1,
            "facet.limit": 100000, # don't limit 
            "facet.field": ['start_day_s']
        }
        if resource_type != 'total':
            params['fq'].append("resource_type_s:" + resource_type)

        response = self.solr_select(params)
        counts0 = response['facet_counts']['facet_fields']['start_day_s']
        day_facet = web.group(counts0, 2)
        return sorted([[self.datestr2millis(day), count] for day, count in day_facet])

    def get_loans_per_type(self):
        rows = self.get_facet_counts("resource_type_s")
        return [{"label": row.title, "data": row.count} for row in rows]

    def get_facet_counts(self, name, limit=20):
        facets = list(self.solr_select_facet(name))[:limit]
        return [self.make_facet(name, key, count) for key, count in facets]

    def get_loans_by_published_year(self):
        d = self._run_solr_facet_query("publish_year")['publish_year']
        # strip bad years
        current_year = datetime.date.today().year
        min_year = 1800
        return [[int(y), count] for y, count in d if min_year < int(y) <= current_year]

    def get_loan_durations(self):
        params = {
            "wt": "json",
            "q": "*:*", 
            "rows": 0,
            "facet": "on",
            "facet.field": ['duration_hours_i']
        }
        response = self.solr_select(params)
        counts = [[int(hr), count] for hr, count in web.group(response['facet_counts']['facet_fields']['duration_hours_i'], 2)]
        one_hour = sum(count for hr, count in counts if hr == 0)
        one_day = sum(count for hr, count in counts if 1 <= hr < 24)
        one_week = sum(count for hr, count in counts if 24 <= hr < 24*7)
        two_week = sum(count for hr, count in counts if 24*7 <= hr < 24*14)
        expired = sum(count for hr, count in counts if 24*14 <= hr)
        return [
            {"label": "Less than one hour", "data": one_hour}, 
            {"label": "Less than one day", "data": one_day}, 
            {"label": "Less than one week", "data": one_week}, 
            {"label": "More than a week", "data": two_week}, 
            {"label": "Loan expired", "data": expired}]

    def make_facet(self, name, key, count):
        type = None
        if name == "library_s":
            title = self._get_library_title(key)
            slug = key
        elif name == "region_s":
            title = key.upper()
            slug = key
        elif name == "country_s":
            title = i18n.gettext_territory(key)
            slug = key   
        elif name == "book_key_s":
            # XXX-Anand: Optimize this by pre loading all books
            book = web.ctx.site.get(key)
            title = book and book.title or "untitled"
            slug = key
        elif name == "author_keys_id":
            # XXX-Anand: Optimize this by pre loading all the authors
            author = web.ctx.site.get(key)
            title = author and author.name or "unnamed"
            slug = key
        elif name == "subject_facet":
            type, title = key.split(":", 1)
            slug = title.lower().replace(" ", "_").replace(",", "")
            if type != "subject":
                slug = type + ":" + slug
        else:
            title = key
            slug = key.lower().replace(" ", "_")
        return web.storage(title=title, count=count, slug=slug, type=type)

    def _get_library_title(self, key):
        if self._library_titles is None:
            libraries = inlibrary.get_libraries()
            self._library_titles = dict((lib.key.split("/")[-1], lib.title) for lib in libraries)
        return self._library_titles.get(key, key)

    def date2millis(self, date):
        return time.mktime(date.timetuple()) * 1000

    def parse_date(self, datestr):
        yyyy, mm, dd = datestr.split("-")
        return datetime.date(int(yyyy), int(mm), int(dd))

    def datestr2millis(self, datestr):
        return self.date2millis(self.parse_date(datestr))

########NEW FILE########
__FILENAME__ = middleware
"""WSGI middleware used in Open Library.
"""
import web
import StringIO
import gzip

class GZipMiddleware:
    """WSGI middleware to gzip the response."""
    def __init__(self, app):
        self.app = app
        
    def __call__(self, environ, start_response):
        accept_encoding = environ.get("HTTP_ACCEPT_ENCODING", "")
        if not 'gzip' in accept_encoding:
            return self.app(environ, start_response)
        
        response = web.storage(compress=False)
        
        def get_response_header(name, default=None):
            for hdr, value in response.headers:
                if hdr.lower() == name.lower():
                    return value
            return default
            
        def compress(text, level=9):
            f = StringIO.StringIO()
            gz = gzip.GzipFile(None, 'wb', level, fileobj=f)
            gz.write(text)
            gz.close()
            return f.getvalue()
        
        def new_start_response(status, headers):
            response.status = status
            response.headers = headers
            
            if status.startswith("200") and get_response_header("Content-Type", "").startswith("text/"):
                headers.append(("Content-Encoding", "gzip"))
                headers.append(("Vary", "Accept-Encoding"))
                response.compress = True
            return start_response(status, headers)
        
        data = self.app(environ, new_start_response)
        if response.compress:
            return [compress("".join(data), 9)]
        else:
            return data

########NEW FILE########
__FILENAME__ = minicron
import os
import time
import sched
import logging
import datetime
import subprocess

class BadCronLine(ValueError): pass

class Minicron(object):
    def __init__(self, cronfile, inittime = None, tickfreq = 60):
        """Creates a cron runner that runs starting at
        `inittime` (default is the current time).
        A 'minute' goes by once every `tickfreq` seconds (default is 60)

        `inittime` can be used to make the cron believe it's running
        at a different time. `tickfreq` can be used to scale the
        runner up or down. Passing _1_ for example will make it
        believe that a minute has gone by every second (useful for
        testing).
        """
        logging.basicConfig(level=logging.INFO, format = "[%(levelname)s] : %(filename)s:%(lineno)d : %(message)s")
        self.ctime = inittime
        if self.ctime == None:
            self.ctime = datetime.datetime.fromtimestamp(time.time())
        self.tickfreq = tickfreq
        self.cronfile = cronfile
        self.scheduler = sched.scheduler(time.time, time.sleep)
        self.scheduler.enter(self.tickfreq, 1, self._tick, ())

    def _matches_cron_expression(self, ctime, cronline):
        """Returns True if the provided time matches the expression in
        the given cronline"""
        def match_minute(ctime, exp):
            if exp == "*":
                return True
            if "/" in exp:
                a,b = exp.split("/")
                return not ctime.minute % int(b)
            if ctime.minute == int(exp):
                return True

        def match_hour(ctime, exp):
            if exp == "*":
                return True
            if "/" in exp:
                a,b = exp.split("/")
                return not ctime.hour % int(b)
            if ctime.hour == int(exp):
                return True

        mm, hh, dom, moy, dow, cmd = cronline.split(None, 5)
        
        if not all(x == "*" for x in [dom, moy, dow]):
            raise BadCronLine("Only minutes and hours may be set. The others have to be *")
        return all([match_minute(ctime, mm),
                    match_hour  (ctime, hh)])

    def _run_command(self, cmd):
        "Runs the given command"
        logging.debug(" Running command %s"%cmd)
        p = subprocess.Popen(cmd, shell = True)
        p.wait()
        
    def _check_and_run_commands(self, ctime):
        """Checks each line of the cron input file to see if the
        command is to be run. If so, it runs it"""
        f = open(self.cronfile)
        for cronline in f:
            if self._matches_cron_expression(ctime, cronline):
                mm, hh, dom, moy, dow, cmd = cronline.split(None, 5)
                self._run_command(cmd.strip())
        f.close()

    def _tick(self):
        "The ticker that gets called once a minute"
        self.ctime += datetime.timedelta(seconds = 60)
        logging.debug("Ticker waking up at %s"%self.ctime)
        self._check_and_run_commands(self.ctime)
        if self.times == None:
            self.scheduler.enter(self.tickfreq, 1, self._tick, ())
        elif self.times > 0:
            self.times -= 1
            self.scheduler.enter(self.tickfreq, 1, self._tick, ())
        

    def run(self, times = None):
        self.times = times
        self.scheduler.run()

########NEW FILE########
__FILENAME__ = models
"""Models of various OL objects.
"""
import urllib, urllib2
import simplejson
import web
import re

import iptools
from infogami.infobase import client

import helpers as h

#TODO: fix this. openlibrary.core should not import plugins.
from openlibrary.plugins.upstream.utils import get_history
from openlibrary.plugins.upstream.account import Account
from openlibrary import accounts

# relative imports
from lists.model import ListMixin, Seed
from . import cache, iprange, inlibrary, waitinglist

def _get_ol_base_url():
    # Anand Oct 2013
    # Looks like the default value when called from script
    if "[unknown]" in web.ctx.home:
        return "https://openlibrary.org"
    else:
        return web.ctx.home

class Image:
    def __init__(self, site, category, id):
        self._site = site
        self.category = category
        self.id = id
        
    def info(self):
        url = '%s/%s/id/%s.json' % (h.get_coverstore_url(), self.category, self.id)
        if url.startswith("//"):
            url = "http:" + url
        try:
            d = simplejson.loads(urllib2.urlopen(url).read())
            d['created'] = h.parse_datetime(d['created'])
            if d['author'] == 'None':
                d['author'] = None
            d['author'] = d['author'] and self._site.get(d['author'])
            
            return web.storage(d)
        except IOError:
            # coverstore is down
            return None
                
    def url(self, size="M"):
        return "%s/%s/id/%s-%s.jpg" % (h.get_coverstore_url(), self.category, self.id, size.upper())
        
    def __repr__(self):
        return "<image: %s/%d>" % (self.category, self.id)

class Thing(client.Thing):
    """Base class for all OL models."""    
    
    @cache.method_memoize
    def get_history_preview(self):
        """Returns history preview.
        """
        history = self._get_history_preview()
        history = web.storage(history)
        
        history.revision = self.revision
        history.lastest_revision = self.revision
        history.created = self.created
        
        def process(v):
            """Converts entries in version dict into objects.
            """
            v = web.storage(v)
            v.created = h.parse_datetime(v.created)
            v.author = v.author and self._site.get(v.author, lazy=True)
            return v
        
        history.initial = [process(v) for v in history.initial]
        history.recent = [process(v) for v in history.recent]
        
        return history

    @cache.memoize(engine="memcache", key=lambda self: ("d" + self.key, "h"))
    def _get_history_preview(self):
        h = {}
        if self.revision < 5:
            h['recent'] = self._get_versions(limit=5)
            h['initial'] = h['recent'][-1:]
            h['recent'] = h['recent'][:-1]
        else:
            h['initial'] = self._get_versions(limit=1, offset=self.revision-1)
            h['recent'] = self._get_versions(limit=4)
        return h
            
    def _get_versions(self, limit, offset=0):
        q = {"key": self.key, "limit": limit, "offset": offset}
        versions = self._site.versions(q)
        for v in versions:
            v.created = v.created.isoformat()
            v.author = v.author and v.author.key

            # XXX-Anand: hack to avoid too big data to be stored in memcache.
            # v.changes is not used and it contrinutes to memcache bloat in a big way.
            v.changes = '[]'
        return versions
                
    def get_most_recent_change(self):
        """Returns the most recent change.
        """
        preview = self.get_history_preview()
        if preview.recent:
            return preview.recent[0]
        else:
            return preview.initial[0]
    
    def prefetch(self):
        """Prefetch all the anticipated data."""
        preview = self.get_history_preview()
        authors = set(v.author.key for v in preview.initial + preview.recent if v.author)
        # preload them
        self._site.get_many(list(authors))
        
    def _make_url(self, label, suffix, relative=True, **params):
        """Make url of the form $key/$label$suffix?$params.
        """
        if label is not None:
            u = self.key + "/" + h.urlsafe(label) + suffix
        else:
            u = self.key + suffix
        if params:
            u += '?' + urllib.urlencode(params)
        if not relative:
            u = _get_ol_base_url() + u            
        return u

    def get_url(self, suffix="", **params):
        """Constructs a URL for this page with given suffix and query params.
        
        The suffix is added to the URL of the page and query params are appended after adding "?".
        """
        return self._make_url(label=self.get_url_suffix(), suffix=suffix, **params)
        
    def get_url_suffix(self):
        """Returns the additional suffix that is added to the key to get the URL of the page.
        
        Models of Edition, Work etc. should extend this to return the suffix.
        
        This is used to construct the URL of the page. By default URL is the 
        key of the page. If this method returns None, nothing is added to the 
        key. If this method returns a string, it is sanitized and added to key 
        after adding a "/".
        """
        return None
                
    def _get_lists(self, limit=50, offset=0, sort=True):
        # cache the default case
        if limit == 50 and offset == 0:
            keys = self._get_lists_cached()
        else:
            keys = self._get_lists_uncached(limit=limit, offset=offset)
            
        lists = self._site.get_many(keys)
        if sort:
            lists = h.safesort(lists, reverse=True, key=lambda list: list.last_modified)
        return lists
        
    @cache.memoize(engine="memcache", key=lambda self: ("d" + self.key, "l"))
    def _get_lists_cached(self):
        return self._get_lists_uncached(limit=50, offset=0)
        
    def _get_lists_uncached(self, limit, offset):
        q = {
            "type": "/type/list",
            "seeds": {"key": self.key},
            "limit": limit,
            "offset": offset
        }
        return self._site.things(q)
        
    def _get_d(self):
        """Returns the data that goes into memcache as d/$self.key.
        Used to measure the memcache usage.
        """
        return {
            "h": self._get_history_preview(),
            "l": self._get_lists_cached(),
        }
        
class Edition(Thing):
    """Class to represent /type/edition objects in OL.
    """
    def url(self, suffix="", **params):
        return self.get_url(suffix, **params)
        
    def get_url_suffix(self):
        return self.title or "untitled"

    def __repr__(self):
        return "<Edition: %s>" % repr(self.title)
    __str__ = __repr__

    def full_title(self):
        # retained for backward-compatibility. Is anybody using this really?
        return self.title
    
    def get_publish_year(self):
        if self.publish_date:
            m = web.re_compile("(\d\d\d\d)").search(self.publish_date)
            return m and int(m.group(1))

    def get_lists(self, limit=50, offset=0, sort=True):
        return self._get_lists(limit=limit, offset=offset, sort=sort)
        

    def get_ebook_info(self):
        """Returns the ebook info with the following fields.
        
        * read_url - url to read the book
        * borrow_url - url to borrow the book
        * borrowed - True if the book is already borrowed
        * daisy_url - url to access the daisy format of the book
        
        Sample return values:

            {
                "read_url": "http://www.archive.org/stream/foo00bar",
                "daisy_url": "/books/OL1M/foo/daisy"
            }

            {
                "daisy_url": "/books/OL1M/foo/daisy",
                "borrow_url": "/books/OL1M/foo/borrow",
                "borrowed": False
            }
        """
        d = {}
        if self.ocaid:
            d['daisy_url'] = self.url('/daisy')

            collections = self.get_ia_collections()
            borrowable = ('lendinglibrary' in collections or
                         ('inlibrary' in collections and inlibrary.get_library() is not None))

            if borrowable:
                d['borrow_url'] = self.url("/borrow")
                key = "ebooks" + self.key
                doc = self._site.store.get(key) or {}
                d['borrowed'] = doc.get("borrowed") == "true"
            elif 'printdisabled' in collections:
                pass # ebook is not available 
            else:
                d['read_url'] = "http://www.archive.org/stream/%s" % self.ocaid
        return d

    def get_ia_collections(self):
        return self.get_ia_meta_fields().get("collection", [])

    def is_access_restricted(self):
        collections = self.get_ia_collections()
        return ('printdisabled' in collections
                or 'lendinglibrary' in collections
                or self.get_ia_meta_fields().get("access-restricted") is True)

    def can_borrow(self):
        collections = self.get_ia_collections()
        return (
            'lendinglibrary' in collections or
            ('inlibrary' in collections and inlibrary.get_library() is not None))

    def get_waitinglist(self):
        """Returns list of records for all users currently waiting for this book."""
        return waitinglist.get_waitinglist_for_book(self.key)

    def get_waitinglist_size(self):
        """Returns the number of people on waiting list to borrow this book.
        """
        return waitinglist.get_waitinglist_size(self.key)

    def get_waitinglist_position(self, user):
        """Returns the position of this user in the waiting list."""
        return waitinglist.get_waitinglist_position(user.key, self.key)

    def get_scanning_contributor(self):
        return self.get_ia_meta_fields().get("contributor")

    def get_loans(self):
        from ..plugins.upstream import borrow
        return borrow.get_edition_loans(self)

    def get_ebook_status(self):
        """
            None
            "read-online"
            "borrow-available"
            "borrow-checkedout"
            "borrow-user-checkedout"
            "borrow-user-waiting"
            "protected"
        """
        if self.get("ocaid"):
            if not self.is_access_restricted():
                return "read-online"
            if not self.is_lendable_book():
                return "protected"

            if self.get_available_loans():
                return "borrow-available"

            user = web.ctx.site.get_user()
            if not user:
                return "borrow-checkedout"

            checkedout_by_user = any(loan.get('user') == user.key for loan in self.get_current_loans())
            if checkedout_by_user:
                return "borrow-user-checkedout"
            if user.is_waiting_for(self):
                return "borrow-user-waiting"
            else:
                return "borrow-checkedout"

    def is_lendable_book(self):
        """Returns True if the book is lendable.
        """
        return self.can_borrow()

    def get_ia_download_link(self, suffix):
        """Returns IA download link for given suffix.
        The suffix is usually one of '.pdf', '.epub', ''.djvu', '.mobi', '_djvu.txt'
        """
        if self.ocaid:
            metadata = self.get_ia_meta_fields()
            # The _filenames field is set by ia.get_metadata function
            filenames = metadata.get("_filenames")
            if filenames:
                filename = some(f for f in filenames if f.endswith(suffix))
            else:
                # filenames is not in cache. 
                # This is required only until all the memcache entries expire
                filename = self.ocaid + suffix

            if filename is None and self.is_ia_scan():                
                # IA scans will have all the required suffixes. 
                # Sometimes they are generated on the fly. 
                filename = self.ocaid + suffix

            if filename:
                return "https://archive.org/download/%s/%s" % (self.ocaid, filename)

    def is_ia_scan(self):
        metadata = self.get_ia_meta_fields()
        # all IA scans will have scanningcenter field set
        return bool(metadata.get("scanningcenter"))

def some(values):
    """Returns the first value that is True from the values iterator.
    Works like any, but returns the value instead of bool(value).
    Returns None if none of the values is True.
    """
    for v in values:
        if v:
            return v


class Work(Thing):
    """Class to represent /type/work objects in OL.
    """
    def url(self, suffix="", **params):
        return self.get_url(suffix, **params)
        
    def get_url_suffix(self):
        return self.title or "untitled"

    def __repr__(self):
        return "<Work: %s>" % repr(self.key)
    __str__ = __repr__

    @property
    @cache.method_memoize
    @cache.memoize(engine="memcache", key=lambda self: ("d" + self.key, "e"))
    def edition_count(self):
        return self._site._request("/count_editions_by_work", data={"key": self.key})

    def get_one_edition(self):
        """Returns any one of the editions.
        
        Used to get the only edition when edition_count==1.
        """
        # If editions from solr are available, use that. 
        # Otherwise query infobase to get the editions (self.editions makes infobase query).
        editions = self.get_sorted_editions() or self.editions
        return editions and editions[0] or None

    def get_lists(self, limit=50, offset=0, sort=True):
        return self._get_lists(limit=limit, offset=offset, sort=sort)

    def _get_d(self):
        """Returns the data that goes into memcache as d/$self.key.
        Used to measure the memcache usage.
        """
        return {
            "h": self._get_history_preview(),
            "l": self._get_lists_cached(),
            "e": self.edition_count
        }

    def _make_subject_link(self, title, prefix=""):
        slug = web.safestr(title.lower().replace(' ', '_').replace(',',''))
        key = "/subjects/%s%s" % (prefix, slug)
        return web.storage(key=key, title=title, slug=slug)

    def get_subject_links(self, type="subject"):
        """Returns all the subjects as link objects.         
        Each link is a web.storage object with title and key fields.

        The type should be one of subject, place, person or time.
        """
        if type == 'subject':
            return [self._make_subject_link(s) for s in self.get_subjects()]
        elif type == 'place':
            return [self._make_subject_link(s, "place:") for s in self.subject_places]
        elif type == 'person':
            return [self._make_subject_link(s, "person:") for s in self.subject_people]
        elif type == 'time':
            return [self._make_subject_link(s, "time:") for s in self.subject_times]
        else:
            return []

class Author(Thing):
    """Class to represent /type/author objects in OL.
    """
    def url(self, suffix="", **params):
        return self.get_url(suffix, **params)
        
    def get_url_suffix(self):
        return self.name or "unnamed"
    
    def __repr__(self):
        return "<Author: %s>" % repr(self.key)
    __str__ = __repr__

    def get_edition_count(self):
        return self._site._request(
                '/count_editions_by_author', 
                data={'key': self.key})
    edition_count = property(get_edition_count)
    
    def get_lists(self, limit=50, offset=0, sort=True):
        return self._get_lists(limit=limit, offset=offset, sort=sort)
    
class User(Thing):
    def get_status(self):
        account = self.get_account() or {}
        return account.get("status")

    def get_usergroups(self):
        keys = self._site.things({
            'type': '/type/usergroup', 
            'members': self.key})
        return self._site.get_many(keys)
    usergroups = property(get_usergroups)
    
    def get_account(self):
        username = self.get_username()
        return accounts.find(username=username)
        
    def get_email(self):
        account = self.get_account() or {}
        return account.get("email")
    
    def get_username(self):
        return self.key.split("/")[-1]

    def is_admin(self):
        return '/usergroup/admin' in [g.key for g in self.usergroups]
        
    def get_lists(self, seed=None, limit=100, offset=0, sort=True):
        """Returns all the lists of this user.
        
        When seed is specified, this returns all the lists which contain the
        given seed.
        
        seed could be an object or a string like "subject:cheese".
        """
        # cache the default case
        if seed is None and limit == 100 and offset == 0:
            keys = self._get_lists_cached()
        else:
            keys = self._get_lists_uncached(seed=seed, limit=limit, offset=offset)
        
        lists = self._site.get_many(keys)
        if sort:
            lists = h.safesort(lists, reverse=True, key=lambda list: list.last_modified)
        return lists

    @cache.memoize(engine="memcache", key=lambda self: ("d" + self.key, "l"))
    def _get_lists_cached(self):
        return self._get_lists_uncached(limit=100, offset=0)
        
    def _get_lists_uncached(self, seed=None, limit=100, offset=0):
        q = {
            "type": "/type/list", 
            "key~": self.key + "/lists/*",
            "limit": limit,
            "offset": offset
        }
        if seed:
            if isinstance(seed, Thing):
                seed = {"key": seed.key}
            q['seeds'] = seed
            
        return self._site.things(q)
        
    def new_list(self, name, description, seeds, tags=[]):
        """Creates a new list object with given name, description, and seeds.

        seeds must be a list containing references to author, edition, work or subject strings.

        Sample seeds:

            {"key": "/authors/OL1A"}
            {"key": "/books/OL1M"}
            {"key": "/works/OL1W"}
            "subject:love"
            "place:san_francisco"
            "time:1947"
            "person:gerge"

        The caller must call list._save(...) to save the list.
        """
        id = self._site.seq.next_value("list")

        # since the owner is part of the URL, it might be difficult to handle
        # change of ownerships. Need to think of a way to handle redirects.
        key = "%s/lists/OL%sL" % (self.key, id)
        doc = {
            "key": key,
            "type": {
                "key": "/type/list"
            },
            "name": name,
            "description": description,
            "seeds": seeds,
            "tags": tags
        }
        return self._site.new(key, doc)

    def is_waiting_for(self, book):
        """Returns True if this user is waiting to loan given book.
        """
        return waitinglist.is_user_waiting_for(self.key, book.key)

    def get_waitinglist(self):
        """Returns list of records for all the books the user is currently waiting for."""
        return waitinglist.get_waitinglist_for_user(self.key)

    def has_borrowed(self, book):
        """Returns True if this user has borrowed given book.
        """
        loan = self.get_loan_for(book)
        return loan is not None

    def get_loan_for(self, book):
        """Returns the loan object for given book.

        Returns None if this user hasn't borrowed the given book.
        """
        from ..plugins.upstream import borrow
        loans = borrow.get_loans(self)
        for loan in loans:
            if book.key == loan['book'] or book.ocaid == loan['ocaid']:
                return loan

    def get_waiting_loan_for(self, book):
        return waitinglist.get_waiting_loan_object(self.key, book.key)

    def __repr__(self):
        return "<User: %s>" % repr(self.key)
    __str__ = __repr__

class List(Thing, ListMixin):
    """Class to represent /type/list objects in OL.
    
    List contains the following properties:
    
        * name - name of the list
        * description - detailed description of the list (markdown)
        * members - members of the list. Either references or subject strings.
        * cover - id of the book cover. Picked from one of its editions.
        * tags - list of tags to describe this list.
    """
    def url(self, suffix="", **params):
        return self.get_url(suffix, **params)
        
    def get_url_suffix(self):
        return self.name or "unnamed"
    
    def get_owner(self):
        match = web.re_compile(r"(/people/[^/]+)/lists/OL\d+L").match(self.key)
        if match:
            key = match.group(1)
            return self._site.get(key)
            
    def get_cover(self):
        """Returns a cover object.
        """
        return self.cover and Image(self._site, "b", self.cover)
        
    def get_tags(self):
        """Returns tags as objects.
        
        Each tag object will contain name and url fields.
        """
        return [web.storage(name=t, url=self.key + u"/tags/" + t) for t in self.tags]
        
    def _get_subjects(self):
        """Returns list of subjects inferred from the seeds.
        Each item in the list will be a storage object with title and url.
        """
        # sample subjects
        return [
            web.storage(title="Cheese", url="/subjects/cheese"),
            web.storage(title="San Francisco", url="/subjects/place:san_francisco")
        ]
        
    def add_seed(self, seed):
        """Adds a new seed to this list.
        
        seed can be:
            - author, edition or work object
            - {"key": "..."} for author, edition or work objects
            - subject strings.
        """
        if isinstance(seed, Thing):
            seed = {"key": seed.key}

        index = self._index_of_seed(seed)
        if index >= 0:
            return False
        else:
            self.seeds = self.seeds or []
            self.seeds.append(seed)
            return True
        
    def remove_seed(self, seed):
        """Removes a seed for the list.
        """
        if isinstance(seed, Thing):
            seed = {"key": seed.key}
            
        index = self._index_of_seed(seed)
        if index >= 0:
            self.seeds.pop(index)
            return True
        else:
            return False
        
    def _index_of_seed(self, seed):
        for i, s in enumerate(self.seeds):
            if isinstance(s, Thing):
                s = {"key": s.key}
            if s == seed:
                return i
        return -1

    def __repr__(self):
        return "<List: %s (%r)>" % (self.key, self.name)

class Library(Thing):
    """Library document.
    
    Each library has a list of IP addresses belongs to that library. 
    """
    def url(self, suffix="", **params):
        return self.get_url(suffix, **params)

    def find_bad_ip_ranges(self, text):
        return iprange.find_bad_ip_ranges(text)
    
    def parse_ip_ranges(self, text):
        return iprange.parse_ip_ranges(text)
    
    def get_ip_range_list(self):
        """Returns IpRangeList object for the range of IPs of this library.
        """
        ranges = list(self.parse_ip_ranges(self.ip_ranges or ""))
        return iptools.IpRangeList(*ranges)
        
    def has_ip(self, ip):
        """Return True if the the given ip is part of the library's ip range.
        """
        return ip in self.get_ip_range_list()
        
    def get_branches(self):
        # Library Name | Street | City | State | Zip | Country | Telephone | Website | Lat, Long
        columns = ["name", "street", "city", "state", "zip", "country", "telephone", "website", "latlong"]
        def parse(line):
            branch = web.storage(zip(columns, line.strip().split("|")))
            
            # add empty values for missing columns
            for c in columns:
                branch.setdefault(c, "")
            
            try:
                branch.lat, branch.lon = branch.latlong.split(",", 1)
            except ValueError:
                branch.lat = "0"
                branch.lon = "0"
            return branch
        return [parse(line) for line in self.addresses.splitlines() if line.strip()]

    def get_loans_per_day(self, resource_type="total"):
        from openlibrary.plugins.openlibrary.libraries import LoanStats
        return LoanStats().get_loans_per_day(resource_type=resource_type, library=self.key)
        
class Subject(web.storage):
    def get_lists(self, limit=1000, offset=0, sort=True):
        q = {
            "type": "/type/list",
            "seeds": self.get_seed(),
            "limit": limit,
            "offset": offset
        }
        keys = web.ctx.site.things(q)
        lists = web.ctx.site.get_many(keys)
        if sort:
            lists = h.safesort(lists, reverse=True, key=lambda list: list.last_modified)
        return lists
        
    def get_seed(self):
        seed = self.key.split("/")[-1]
        if seed.split(":")[0] not in ["place", "person", "time"]:
            seed = "subject:" + seed
        return seed
        
    def url(self, suffix="", relative=True, **params):
        u = self.key + suffix
        if params:
            u += '?' + urllib.urlencode(params)
        if not relative:
            u = _get_ol_base_url() + u
        return u

    # get_url is a common method available in all Models. 
    # Calling it `get_url` instead of `url` because there are some types that 
    # have a property with name `url`.
    get_url = url
        
    def get_default_cover(self):
        for w in self.works:
            cover_id = w.get("cover_id")
            if cover_id:
                return Image(web.ctx.site, "b", cover_id)

def register_models():
    client.register_thing_class(None, Thing) # default
    client.register_thing_class('/type/edition', Edition)
    client.register_thing_class('/type/work', Work)
    client.register_thing_class('/type/author', Author)
    client.register_thing_class('/type/user', User)
    client.register_thing_class('/type/list', List)
    client.register_thing_class('/type/library', Library)
    
def register_types():
    """Register default types for various path patterns used in OL.
    """
    from infogami.utils import types

    types.register_type('^/authors/[^/]*$', '/type/author')
    types.register_type('^/books/[^/]*$', '/type/edition')
    types.register_type('^/works/[^/]*$', '/type/work')
    types.register_type('^/languages/[^/]*$', '/type/language')
    types.register_type('^/libraries/[^/]*$', '/type/library')

    types.register_type('^/usergroup/[^/]*$', '/type/usergroup')
    types.register_type('^/permission/[^/]*$', '/type/permission')

    types.register_type('^/(css|js)/[^/]*$', '/type/rawtext')

########NEW FILE########
__FILENAME__ = msgbroker
"""Broker for dispaching messages in Open Library.

Message Broker or the pub/sub messaging patten allows publishers and
subscribers of messages to communite without the knowledge of each other. This
usally leads to a loosely-coupled design.

For examaple, the borrow module can send messages about loan-created and
loan-completed and the stats module can subscribe to these events and update
the database used for displaying loan graphs.
"""

import eventer

send_message = eventer.trigger
subscribe = eventer.bind
unsubscribe = eventer.unbind

########NEW FILE########
__FILENAME__ = olmarkdown
"""Open Library Flavored Markdown, inspired by [Github Flavored Markdown][GFM].

GFM: http://github.github.com/github-flavored-markdown/

Differences from traditional Markdown:
* new lines in paragraph are treated as line breaks
* URLs are autolinked
* generated HTML is sanitized

The custom changes done here to markdown are also reptead in WMD editor, 
the javascript markdown editor used in OL.
"""

import helpers as h
import re
from infogami.utils.markdown import markdown

# regexp to match urls and emails. 
# Adopted from github-flavored-markdown (BSD-style open source license)
# http://github.com/github/github-flavored-markdown/blob/gh-pages/scripts/showdown.js#L158
AUTOLINK_RE = r'''(^|\s)(https?\:\/\/[^"\s<>]*[^.,;'">\:\s\<\>\)\]\!]|[a-z0-9_\-+=.]+@[a-z0-9\-]+(?:\.[a-z0-9-]+)+)'''

LINK_REFERENCE_RE = re.compile(r' *\[[^\[\] ]*\] *:')

class LineBreaksPreprocessor(markdown.Preprocessor):
    def run(self, lines) :
        for i in range(len(lines)-1):
            # append <br/> to all lines expect blank lines and the line before blankline.
            if (lines[i].strip() and lines[i+1].strip()
                and not markdown.RE.regExp['tabbed'].match(lines[i])
                and not LINK_REFERENCE_RE.match(lines[i])):
                lines[i] += "<br />"
        return lines

LINE_BREAKS_PREPROCESSOR = LineBreaksPreprocessor()

class AutolinkPreprocessor(markdown.Preprocessor):
    rx = re.compile(AUTOLINK_RE)
    def run(self, lines):
        for i in range(len(lines)):
            if not markdown.RE.regExp['tabbed'].match(lines[i]):
                lines[i] = self.rx.sub(r'\1<\2>', lines[i])
        return lines

AUTOLINK_PREPROCESSOR = AutolinkPreprocessor()
    
class OLMarkdown(markdown.Markdown):
    """Open Library flavored Markdown, inspired by [Github Flavored Markdown][GFM].
    
    GFM: http://github.github.com/github-flavored-markdown/

    Differences from traditional Markdown:
    * new lines in paragraph are treated as line breaks
    * URLs are autolinked
    * generated HTML is sanitized    
    """
    def __init__(self, *a, **kw):
        markdown.Markdown.__init__(self, *a, **kw)
        self._patch()
        
    def _patch(self):
        p = self.preprocessors
        p[p.index(markdown.LINE_BREAKS_PREPROCESSOR)] = LINE_BREAKS_PREPROCESSOR
        p.append(AUTOLINK_PREPROCESSOR)
        
    def convert(self):
        html = markdown.Markdown.convert(self)
        return h.sanitize(html)

########NEW FILE########
__FILENAME__ = invalidation
import web
import datetime
from infogami.infobase import client

from openlibrary.core import helpers as h

__all__ = [
    "InvalidationProcessor"
]

class InvalidationProcessor:
    """Application processor to invalidate/update locally cached documents.
    
    The openlibrary application caches some documents like templates, macros,
    javascripts etc. locally for variety of reasons. This class implements a
    way to make sure those documents are kept up-to-date with the db within
    some allowed constraints.
    
    This implements a kind of lazy consistancy, which guaranties the following:
    
    * If a client makes an update, he will continue to see that update on
      subsequent requests.
    * If a client sees an update made by somebody else, he will continue to
      see that update on subsequent requests.
    * A client sees older version of a doucment no longer than the specified
      timeout (in seconds) after the document is updated.
    
    It means that the following conditions will never happen:

    * A client edits a page and reloading the same page shows an older
      version.
    * A client loads a page and reloading the same page shows an older version.
    * A client continue to see an older version of a document for very long time.
    
    It is implemented as follows:

    * If there is an update, set a cookie with time of the update as value.
    * If the cookie timestamp is more than the last_poll_time, trigger reload.
    * If the cookie timestamp is less than the last_update_time, set the
      cookie with last_update_time.
    * If the current time is more than timeout seconds since last_poll_time,
      trigger reload.
      
    When the reload is triggered:
    
    * A request to the datebase is made to find list of documents modified after the last_poll_time. 
    * Trigger on_new_version event for each modified document. The application
      code that is handling the caching must listen to that event and
      invalidate/update its cached copy.
      
    How to use::
     
        from infogami.utils import delegate
        from infogami.infobase import client
        
        p = InvalidationProcessor(["/templates/", "/macros/"])
        
        # install the application processor
        delegate.app.add_processor(p)
        
        # add the hook to get notifications when a document is modified
        client.hooks.append(p.hook)
        
    Glossary:

    * cookie_timestamp: value of the invalidation cookie.
    * last_poll_time: timestamp of the latest reload
    * last_update_time: timestamp of the most recent update known to this
      process.
    """
    def __init__(self, prefixes, timeout=60, cookie_name="lastupdate"):
        self.prefixes = prefixes
        self.timeout = datetime.timedelta(0, timeout)

        self.cookie_name = cookie_name
        self.last_poll_time = datetime.datetime.utcnow()
        self.last_update_time = self.last_poll_time
        
        # set expire_time slightly more than timeout
        self.expire_time = 3 * timeout
        self.hook = _InvalidationHook(prefixes=prefixes, cookie_name=cookie_name, expire_time=self.expire_time)

    def __call__(self, handler):
        def t(date):
            return date.isoformat().split("T")[-1]
            
        cookie_time = self.get_cookie_time()
        
        if cookie_time and cookie_time > self.last_poll_time:
            self.reload()
        elif self.is_timeout():
            self.reload()

        # last update in recent timeout seconds?
        has_recent_update = (self.last_poll_time - self.last_update_time) < self.timeout
        if has_recent_update and (cookie_time is None or cookie_time < self.last_update_time):
            web.setcookie(self.cookie_name, self.last_update_time.isoformat(), expires=self.expire_time)

        return handler()

    def is_timeout(self):
        t = datetime.datetime.utcnow()
        dt = t - self.last_poll_time
        return dt > self.timeout
        
    def get_cookie_time(self):
        cookies = web.cookies()

        if self.cookie_name in cookies:
            return self.parse_datetime(cookies[self.cookie_name])

    def parse_datetime(self, datestr):
        try:
            return h.parse_datetime(datestr)
        except ValueError:
            return None

    def reload(self):
        """Triggers on_new_version event for all the documents modified since last_poll_time.
        """
        t = datetime.datetime.utcnow()
        reloaded = False

        keys = []
        for prefix in self.prefixes:
            q = {"key~": prefix + "*", "last_modified>": self.last_poll_time.isoformat(), "limit": 1000}
            keys += web.ctx.site.things(q)
    
        if keys:
            web.ctx._invalidation_inprogress = True
            docs = web.ctx.site.get_many(keys)
            for doc in docs:
                try:
                    client._run_hooks("on_new_version", doc)
                except Exception:
                    pass
            self.last_update_time = max(doc.last_modified for doc in docs)
            reloaded = True
            del web.ctx._invalidation_inprogress

        self.last_poll_time = t
        return reloaded
        
class _InvalidationHook:
    """Infogami client hook to get notification on edits. 
    
    This sets a cookie when any of the documents under the given prefixes is modified.
    """
    def __init__(self, prefixes, cookie_name, expire_time):
        self.prefixes = prefixes
        self.cookie_name = cookie_name
        self.expire_time = expire_time
        
    def __call__(self):
        return self
        
    def on_new_version(self, doc):
        if web.ctx.get("_invalidation_inprogress"):
            # This event is triggered from invalidation. ignore it.
            return
            
        if any(doc.key.startswith(prefix) for prefix in self.prefixes):
            # The supplied does doesn't have the updated last_modified time. 
            # Fetch the document afresh to get the correct last_modified time.
            doc = web.ctx.site.get(doc.key)
            t = doc.last_modified
            
            web.setcookie(self.cookie_name, t.isoformat(), expires=self.expire_time)

########NEW FILE########
__FILENAME__ = readableurls
"""Various web.py application processors used in OL.
"""
import os
import urllib
import web

from openlibrary.core import helpers as h

class ReadableUrlProcessor:
    """Open Library code works with urls like /books/OL1M and
    /books/OL1M/edit. This processor seemlessly changes the urls to
    /books/OL1M/title and /books/OL1M/title/edit.
    
    The changequery function is also customized to support this.    
    """
    patterns = [
        (r'/\w+/OL\d+M', '/type/edition', 'title', 'untitled'),
        (r'/\w+/ia:[a-zA-Z0-9_\.-]+', '/type/edition', 'title', 'untitled'),
        (r'/\w+/OL\d+A', '/type/author', 'name', 'noname'),
        (r'/\w+/OL\d+W', '/type/work', 'title', 'untitled'),
        (r'/[/\w]+/OL\d+L', '/type/list', 'name', 'unnamed')
    ]
        
    def __call__(self, handler):
        # temp hack to handle languages and users during upstream-to-www migration
        if web.ctx.path.startswith("/l/"):
            raise web.seeother("/languages/" + web.ctx.path[len("/l/"):])
                
        if web.ctx.path.startswith("/user/"):
            if not web.ctx.site.get(web.ctx.path):
                raise web.seeother("/people/" + web.ctx.path[len("/user/"):])
        
        real_path, readable_path = get_readable_path(web.ctx.site, web.ctx.path, self.patterns, encoding=web.ctx.encoding)

        #@@ web.ctx.path is either quoted or unquoted depends on whether the application is running
        #@@ using builtin-server or lighttpd. Thats probably a bug in web.py. 
        #@@ take care of that case here till that is fixed.
        # @@ Also, the redirection must be done only for GET requests.
        if readable_path != web.ctx.path and readable_path != urllib.quote(web.utf8(web.ctx.path)) and web.ctx.method == "GET":
            raise web.redirect(web.safeunicode(readable_path) + web.safeunicode(web.ctx.query))

        web.ctx.readable_path = readable_path
        web.ctx.path = real_path
        web.ctx.fullpath = web.ctx.path + web.ctx.query
        return handler()

def _get_object(site, key):
    """Returns the object with the given key.
    
    If the key has an OLID and no object is found with that key, it tries to
    find object with the same OLID. OL database makes sures that OLIDs are
    unique.
    """    
    obj = site.get(key)

    if obj is None and key.startswith("/a/"):
        key = "/authors/" + key[len("/a/"):]
        obj = key and site.get(key)
        
    if obj is None and key.startswith("/b/"):
        key = "/books/" + key[len("/b/"):]
        obj = key and site.get(key)

    if obj is None and key.startswith("/user/"):
        key = "/people/" + key[len("/user/"):]
        obj = key and site.get(key)

    basename = key.split("/")[-1]

    # redirect all /.*/ia:foo to /books/ia:foo
    if obj is None and basename.startswith("ia:"):
        key = "/books/" + basename
        obj = site.get(key)

    # redirect all /.*/OL123W to /works/OL123W
    if obj is None and basename.startswith("OL") and basename.endswith("W"):
        key = "/works/" + basename
        obj = site.get(key)

    # redirect all /.*/OL123M to /books/OL123M
    if obj is None and basename.startswith("OL") and basename.endswith("M"):
        key = "/books/" + basename
        obj = site.get(key)

    # redirect all /.*/OL123A to /authors/OL123A
    if obj is None and basename.startswith("OL") and basename.endswith("A"):
        key = "/authors/" + basename
        obj = site.get(key)

    # Disabled temporarily as the index is not ready the db
    
    #if obj is None and web.re_compile(r"/.*/OL\d+[A-Z]"):
    #    olid = web.safestr(key).split("/")[-1]
    #    key = site._request("/olid_to_key", data={"olid": olid}).key
    #    obj = key and site.get(key)
    return obj
    
def get_readable_path(site, path, patterns, encoding=None):
    """Returns real_path and readable_path from the given path.
    
    The patterns is a list of (path_regex, type, property_name, default_value)
    tuples.
    """
    def match(path):    
        for pat, type, property, default_title in patterns:
            m = web.re_compile('^' + pat).match(path)
            if m:
                prefix = m.group()
                extra = web.lstrips(path, prefix)
                tokens = extra.split("/", 2)
                
                # `extra` starts with "/". So first token is always empty.
                middle = web.listget(tokens, 1, "")
                suffix = web.listget(tokens, 2, "")
                if suffix:
                    suffix = "/" + suffix
                
                return type, property, default_title, prefix, middle, suffix
        return None, None, None, None, None, None

    type, property, default_title, prefix, middle, suffix = match(path)
    if type is None:
        path = web.safeunicode(path)
        return (path, path)

    if encoding is not None \
       or path.endswith(".json") or path.endswith(".yml") or path.endswith(".rdf"):
        key, ext = os.path.splitext(path)
        
        thing = _get_object(site, key)
        if thing:
            path = thing.key + ext
        path = web.safeunicode(path)
        return (path, path)

    thing = _get_object(site, prefix)
    
    # get_object may handle redirections.
    if thing:
        prefix = thing.key

    if thing and thing.type.key == type:
        title = thing.get(property) or default_title
        middle = '/' + h.urlsafe(title.strip())
    else:
        middle = ""
    
    prefix = web.safeunicode(prefix)
    middle = web.safeunicode(middle)
    suffix = web.safeunicode(suffix)
    
    return (prefix + suffix, prefix + middle + suffix)

########NEW FILE########
__FILENAME__ = test_invalidation
import web
import datetime

from infogami.infobase import client
from openlibrary.core.processors import invalidation
from openlibrary.mocks.mock_infobase import MockSite

class MockHook:
    def __init__(self):
        self.call_count = 0
        self.recent_doc = None
        
    def on_new_version(self, doc):
        self.recent_doc = doc
        self.call_count += 1
        
class MockDatetime:
    """Class to mock datetime.datetime to overwrite utcnow method.
    """
    def __init__(self, utcnow):
        self._now = utcnow
    
    def utcnow(self):
        return self._now

class TestInvalidationProcessor:
    def test_hook(self, monkeypatch):
        """When a document is saved, cookie must be set with its timestamp.
        """
        self._monkeypatch_web(monkeypatch)
        
        doc = {
            "key": "/templates/site.tmpl",
            "type": "/type/template"
        }
        web.ctx.site.save(doc, timestamp=datetime.datetime(2010, 01, 01))
        
        hook = invalidation._InvalidationHook("/templates/site.tmpl", cookie_name="invalidation-cookie", expire_time=120)
        hook.on_new_version(web.ctx.site.get(doc['key']))
        
        assert self.cookie == {
            "name": "invalidation-cookie",
            "value": "2010-01-01T00:00:00",
            "expires": 120
        }
        
    def test_reload(self, monkeypatch):
        """If reload is called and there are some modifications, each hook should get called.
        """
        self._monkeypatch_web(monkeypatch)
        self._monkeypatch_hooks(monkeypatch)
        
        # create the processor
        p = invalidation.InvalidationProcessor(prefixes=['/templates/'])
        
        # save a doc after creating the processor
        doc = {
            "key": "/templates/site.tmpl",
            "type": "/type/template"
        }
        web.ctx.site.save(doc)
                
        # reload and make sure the hook gets called
        p.reload()
        
        assert self.hook.call_count == 1
        assert self.hook.recent_doc.dict() == web.ctx.site.get("/templates/site.tmpl").dict()
        
        # last_update_time must get updated
        assert p.last_update_time == web.ctx.site.get("/templates/site.tmpl").last_modified

    def test_reload_on_timeout(self, monkeypatch):
        # create the processor at 60 seconds past in time
        mock_now = datetime.datetime.utcnow() - datetime.timedelta(seconds=60)
        monkeypatch.setattr(datetime, "datetime", MockDatetime(mock_now))
        
        p = invalidation.InvalidationProcessor(prefixes=['/templates'], timeout=60)
        
        # come back to real time
        monkeypatch.undo()

        # monkeypatch web
        self._monkeypatch_web(monkeypatch)
        self._monkeypatch_hooks(monkeypatch)
        
        # save a doc
        doc = {
            "key": "/templates/site.tmpl",
            "type": "/type/template"
        }
        web.ctx.site.save(doc)
                
        # call the processor
        p(lambda: None)
        assert self.hook.call_count == 1
        assert self.hook.recent_doc.dict() == web.ctx.site.get("/templates/site.tmpl").dict()
        
    def test_is_timeout(self, monkeypatch):
        # create the processor at 60 seconds past in time
        mock_now = datetime.datetime.utcnow() - datetime.timedelta(seconds=60)
        monkeypatch.setattr(datetime, "datetime", MockDatetime(mock_now))
        
        p = invalidation.InvalidationProcessor(prefixes=['/templates'], timeout=60)
        
        # come back to real time
        monkeypatch.undo()
        
        # monkeypatch web
        self._monkeypatch_web(monkeypatch)
        self._monkeypatch_hooks(monkeypatch)
        
        p.reload()
        
        # until next 60 seconds, is_timeout must be false.
        assert p.is_timeout() == False
        
    def test_reload_on_cookie(self, monkeypatch):
        self._monkeypatch_web(monkeypatch)
        self._monkeypatch_hooks(monkeypatch)
        
        p = invalidation.InvalidationProcessor(prefixes=['/templates'], cookie_name="invalidation_cookie")
        
        # save a doc
        doc = {
            "key": "/templates/site.tmpl",
            "type": "/type/template"
        }
        web.ctx.site.save(doc)
        
        # call the processor
        p(lambda: None)
        
        # no cookie, no hook call
        assert self.hook.call_count == 0
        
        web.ctx.env['HTTP_COOKIE'] = "invalidation_cookie=" + datetime.datetime.utcnow().isoformat()
        p(lambda: None)
        
        # cookie is set, hook call is expetected
        assert self.hook.call_count == 1
        assert self.hook.recent_doc.dict() == web.ctx.site.get("/templates/site.tmpl").dict()
    
    def test_setcookie_after_reload(self, monkeypatch):
        self._monkeypatch_web(monkeypatch)
        self._monkeypatch_hooks(monkeypatch)
        
        p = invalidation.InvalidationProcessor(prefixes=['/templates'], cookie_name="invalidation_cookie", timeout=60)
        
        # save a doc
        doc = {
            "key": "/templates/site.tmpl",
            "type": "/type/template"
        }
        web.ctx.site.save(doc)
        
        p.reload()
        
        # A cookie must be set when there is a recent update known to the processor
        p(lambda: None)
        
        assert self.cookie == {
            "name": "invalidation_cookie", 
            "expires": p.expire_time, 
            "value": web.ctx.site.get("/templates/site.tmpl").last_modified.isoformat()
        }


    def _load_fake_context(self):
        app = web.application()
        env = {
            'PATH_INFO': '/',
            'HTTP_METHOD': 'GET'
        }
        app.load(env)

    def _monkeypatch_web(self, monkeypatch):
        monkeypatch.setattr(web, "ctx", web.storage(x=1))
        monkeypatch.setattr(web.webapi, "ctx", web.ctx)

        self._load_fake_context()
        web.ctx.site = MockSite()

        def setcookie(name, value, expires):
            self.cookie = dict(name=name, value=value, expires=expires)

        monkeypatch.setattr(web, "setcookie", setcookie)


    def _monkeypatch_hooks(self, monkeypatch):
        self.hook = MockHook()
        monkeypatch.setattr(client, "hooks", [self.hook])

########NEW FILE########
__FILENAME__ = schema
"""Infobase schema for Open Library
"""
from infogami.infobase import dbstore
import web

def get_schema():
    schema = dbstore.Schema()
    schema.add_table_group('type', '/type/type')
    schema.add_table_group('type', '/type/property')
    schema.add_table_group('type', '/type/backreference')
    schema.add_table_group('user', '/type/user')
    schema.add_table_group('user', '/type/usergroup')
    schema.add_table_group('user', '/type/permission')
    
    datatypes = ["str", "int", "ref", "boolean"]
    
    schema.add_table_group('edition', '/type/edition', datatypes)
    schema.add_table_group('author', '/type/author', datatypes)
    schema.add_table_group('scan', '/type/scan_location', datatypes)
    schema.add_table_group('scan', '/type/scan_record', datatypes)

    schema.add_table_group('work', '/type/work', datatypes) 
    schema.add_table_group('publisher', '/type/publisher', datatypes)
    schema.add_table_group('subject', '/type/subject', datatypes)
    
    schema.add_seq('/type/edition', '/books/OL%dM')
    schema.add_seq('/type/author', '/authors/OL%dA')
    
    schema.add_seq('/type/work', '/works/OL%dW')
    schema.add_seq('/type/publisher', '/publishers/OL%dP')
    
    _sql = schema.sql
    
    # custom postgres functions required by OL.
    more_sql = """
    CREATE OR REPLACE FUNCTION get_olid(text) RETURNS text AS $$
        select regexp_replace($1, '.*(OL[0-9]+[A-Z])', E'\\1') where $1 ~ '^/.*/OL[0-9]+[A-Z]$';
    $$ LANGUAGE SQL IMMUTABLE;
    
    CREATE INDEX thing_olid_idx ON thing(get_olid(key));

    CREATE TABLE stats (
        id serial primary key,
        key text unique,
        type text,
        created timestamp without time zone,
        updated timestamp without time zone,
        json text
    );
    CREATE INDEX stats_type_idx ON stats(type);
    CREATE INDEX stats_created_idx ON stats(created);
    CREATE INDEX stats_updated_idx ON stats(updated);
    """
        
    # monkey patch schema.sql to include the custom functions
    schema.sql = lambda: web.safestr(_sql()) + more_sql    
    return schema
    
def register_schema():
    """Register the schema definied in this module as the default schema."""
    dbstore.default_schema = get_schema()

if __name__ == "__main__":
    print get_schema().sql()

########NEW FILE########
__FILENAME__ = sendmail
from infogami import config
from infogami.utils.view import render_template
import web

def sendmail_with_template(template, to, cc=None, frm=None, **kwargs):
    msg = render_template(template, **kwargs)
    _sendmail(to, msg, cc=cc, frm=frm)

def _sendmail(to, msg, cc=None, frm=None):
    cc = cc or []
    frm = frm or config.from_address
    if config.get('dummy_sendmail'):
        message = ('' +
            'To: ' + to + '\n' +
            'From:' + config.from_address + '\n' +
            'Subject:' + msg.subject + '\n' +
            '\n' +
            web.safestr(msg))

        print >> web.debug, "sending email", message
    else:
        web.sendmail(frm, to, subject=msg.subject.strip(), message=web.safestr(msg), cc=cc)


########NEW FILE########
__FILENAME__ = seq
"""Library to generate keys for new documents using database sequences.

Currently new keys are generated for author, edition and work types.
"""

__all__ = [
    "get_new_key",
    "get_new_keys"
]

def get_new_key(site, type):
    """Returns a new key for the given type of document.
    """
    return site.new_key(type)
    
def get_new_keys(site, type, n):
    """Returns n new keys for given type of documents.
    
    Example:
    
        >>> get_new_keys("/type/edition", 2)
        ["/books/OL12M", "/books/OL13M"]
    """
    return [get_new_key(site, type) for i in range(n)]
    
########NEW FILE########
__FILENAME__ = stats
"""
StatsD client to be used in the application to log various metrics

Based on the code in http://www.monkinetic.com/2011/02/statsd.html (pystatsd client)

"""

# statsd.py

# Steve Ivy <steveivy@gmail.com>
# http://monkinetic.com

import logging
import socket
import random

from pystatsd import Client

from infogami import config

l = logging.getLogger("openlibrary.pystats")

def create_stats_client(cfg = config):
    "Create the client which can be used for logging statistics"
    logger = logging.getLogger("pystatsd.client")
    logger.addHandler(logging.StreamHandler())
    try:
        stats_server = cfg.get("admin", {}).get("statsd_server",None)
        if stats_server:
            host, port = stats_server.rsplit(":", 1)
            return Client(host, port)
        else:
            logger.critical("Couldn't find statsd_server section in config")
            return False
    except Exception, e:
        logger.critical("Couldn't create stats client - %s", e, exc_info = True)
        return False

def put(key, value):
    "Records this ``value`` with the given ``key``. It is stored as a millisecond count"
    global client
    if client:
        l.debug("Putting %s as %s"%(value, key))
        client.timing(key, value)

def increment(key, n=1):
    "Increments the value of ``key`` by ``n``"
    global client
    if client:
        l.debug("Incrementing %s"% key)
        for i in range(n):
            client.increment(key)


client = create_stats_client()




    

########NEW FILE########
__FILENAME__ = statsdb
"""Interface to Open Library stats database.

The stats table in the openlibrary database is of the following schema:

    CREATE TABLE stats (
        id serial primary key,
        key text unique,
        type text,
        timestamp timestamp without time zone,
        json text
    );

see schema.py for more details.
"""
import logging
import web
import simplejson
import datetime

logger = logging.getLogger("openlibrary.statsdb")

@web.memoize
def get_db():
    return web.database(**web.config.db_parameters)

def add_entry(key, data, timestamp=None):
    """Adds a new entry to the stats table.

    If an entry is already present in the table, a warn message is logged
    and no changes will be made to the database.
    """
    jsontext = simplejson.dumps(data)
    timestamp = timestamp or datetime.datetime.utcnow()
    t = timestamp.isoformat()

    db = get_db()
    result = db.query("SELECT * FROM stats WHERE key=$key", vars=locals())
    if result:
        logger.warn("Failed to add stats entry with key %r. An entry is already present.")
    else:
        db.insert("stats", type='loan', key=key, created=t, updated=t, json=jsontext)

def get_entry(key):
    result = db.query("SELECT * FROM stats WHERE key=$key", vars=locals())
    if result:
        return result[0]

def update_entry(key, data, timestamp=None):
    """Updates an already existing entry in the stats table.

    If there is no entry with the given key, a new one will be added
    after logging a warn message.
    """
    jsontext = simplejson.dumps(data)
    timestamp = timestamp or datetime.datetime.utcnow()
    t = timestamp.isoformat()

    db = get_db()
    result = db.query("SELECT * FROM stats WHERE key=$key", vars=locals())
    if result:
        db.update("stats", json=jsontext, updated=t, where="key=$key", vars=locals())
    else:
        logger.warn("stats entry with key %r doesn't exist to update. adding new entry...", key)
        db.insert("stats", type='loan', key=key, created=t, updated=t, json=jsontext)

########NEW FILE########
__FILENAME__ = support
import datetime
from collections import defaultdict

import couchdb
from couchdb.mapping import TextField, IntegerField, DateTimeField, ListField, DictField, Mapping, Document, ViewField
from couchdb.design import ViewDefinition

import web
from infogami import config


class InvalidCase(KeyError): pass
class DatabaseConnectionError(Exception): pass

@web.memoize
def get_admin_database():
    admin_db = config.get("admin", {}).get("admin_db",None)
    if admin_db:
        return couchdb.Database(admin_db)
    else:
        raise DatabaseConnectionError("No admin_db specified in config")
        

class Support(object):
    def __init__(self, db = None):
        if db:
            self.db = db
        else:
            self.db = get_admin_database()
    
    def create_case(self, creator_name, creator_email, creator_useragent, creator_username, subject, description, assignee, url = ""):
        "Creates a support case with the given parameters"
        seq = web.ctx.site.seq.next_value("support-case")
        created = datetime.datetime.utcnow()
        caseid = "case-%s"%seq
        c = Case.new(_id = caseid,
                     creator_name = creator_name,
                     creator_email = creator_email,
                     creator_useragent = creator_useragent,
                     creator_username  = creator_username,
                     subject = subject,
                     description = description,
                     assignee = assignee,
                     created = created,
                     status = "new",
                     url = url,
                     support_db = self.db)
        c.store(self.db)
        return c

    def get_case(self, caseid):
        "Returns the case with the given id"
        if not str(caseid).startswith("case"):
            caseid = "case-%s"%caseid
        c = Case.load(self.db, caseid)
        return c
        
    def get_all_cases(self, typ = "all", summarise = False, sortby = "lastmodified", user = False, desc = "false", staleok = False):
        "Return all the cases in the system"
        args = {}
        if summarise:
            d = defaultdict(lambda: 0)
            if user:
                args = dict(startkey = [user, "closed"], 
                            endkey = [user, {}])
            else:
                args = dict(startkey = [None, "closed"], 
                            endkey = [None,{}])
            if staleok:
                args.update(stale = "ok")
            v = ViewDefinition("cases", "assignee", "", group_level = 2, **args)
            for i in v(self.db):
                d[i.key[1]] += i.value
            return d
        else:
            return Case.all(self.db, typ, sortby, desc, staleok)
            
            
class Case(Document):
    _id               = TextField()
    type              = TextField(default = "case")
    status            = TextField()
    assignee          = TextField()
    description       = TextField()
    subject           = TextField()
    creator_email     = TextField()
    creator_useragent = TextField()
    creator_name      = TextField()
    creator_username  = TextField()
    url               = TextField()
    created           = DateTimeField()
    history           = ListField(DictField(Mapping.build(at    = DateTimeField(),
                                                          by    = TextField(),
                                                          text  = TextField())))

    def __repr__(self):
        return "<Case ('%s')>"%self._id

    def change_status(self, new_status, by):
        self.status = new_status
        self.store(self.db)


    def reassign(self, new_assignee, by, text = ""):
        self.assignee = new_assignee
        entry = dict(by = by,
                     at = datetime.datetime.utcnow(),
                     text = "Case reassigned to '%s'\n\n%s"%(new_assignee, text))
        self.history.append(entry)
        self.store(self.db)

    def add_worklog_entry(self, by, text):
        entry = dict(by = by,
                     at = datetime.datetime.utcnow(),
                     text = text)
        self.history.append(entry)
        self.store(self.db)
        
    # Override base class members to hold the database connection
    @classmethod
    def load(cls, db, id):
        ret = super(Case, cls).load(db, id)
        if not ret:
            raise InvalidCase("No case with id %s"%id)
        ret.db = db
        return ret

    def store(self, db):
        super(Case, self).store(db)
        self.db = db
        
    @property
    def last_modified(self):
        return self.history[-1].at

    @property
    def caseid(self):
        return self._id

    @property
    def caseno(self):
        "Returns case number"
        return int(self._id.replace("case-",""))

    @classmethod
    def new(cls, **kargs):
        ret = cls(**kargs)
        item = dict (at = ret.created,
                     by = ret.creator_name or ret.creator_email,
                     text = "Case created")
        ret.history.append(item)
        return ret

    @classmethod
    def all(cls, db, typ="all", sort = "status", desc = "false", staleok = False):
        view = {"created"      : "cases/sort-created",
                "caseid"       : "cases/sort-caseid",
                "assigned"     : "cases/sort-assignee",
                "user"         : "cases/sort-creator",
                "lastmodified" : "cases/sort-lastmodified",
                "status"       : "cases/sort-status",
                "subject"      : "cases/sort-subject",
                "notes"        : "cases/sort-numnotes"}[sort]
        if sort == "status":
            extra = dict(reduce = False,
                         descending = desc)
        else:
            extra = dict(descending = desc)
        if staleok:
            extra['stale'] = "ok"
        if typ == "all":
            view = view.replace("-","-all-") 
            result = cls.view(db, view, include_docs = True, **extra)
            return result.rows
        elif typ == "new":
            startkey, endkey = (["new"], ["replied"])
        elif typ == "closed":
            startkey, endkey = (["closed"], ["new"])
        elif typ == "replied":
            startkey, endkey = (["replied"], False)
        else:
            raise KeyError("No such case type '%s'"%typ)

        if desc == "true":
            startkey, endkey = endkey, startkey
        if startkey:
            extra['startkey'] = startkey
        if endkey:
            extra['endkey'] = endkey
        result = cls.view(db, view, include_docs = True,**extra)
        return result.rows
        
    def __eq__(self, second):
        return self._id == second._id

########NEW FILE########
__FILENAME__ = task
"""
Utilities that interface with celery for the openlibrary application.
"""

import json
import datetime
import logging
import StringIO
import traceback
import calendar


import web
import yaml

# from celery.task import task
from celery.task import task
from functools import wraps

from openlibrary.core import stats

@web.memoize
def setup_stats():
    import celeryconfig
    c = yaml.load(open(celeryconfig.OL_CONFIG))
    stats.client = stats.create_stats_client(c)

class ExceptionWrapper(Exception):
    def __init__(self, exception, extra_info):
        self.exception = exception
        self.data = extra_info
        super(ExceptionWrapper, self).__init__(exception,  extra_info)
    

task_context = {}

def set_task_data(**kargs):
    "Sets a global task data that can be used in the the tombstone"
    global task_context
    task_context = kargs.copy()


def create_patched_delay(original_fn):
    """Returns a patched version of delay that we can use to
    monkeypatch the actual one"""
    # @wraps(original_fn.delay)
    def delay2(*largs, **kargs):
        t = calendar.timegm(datetime.datetime.utcnow().timetuple())
        celery_extra_info = dict(enqueue_time = t)
        if "celery_parent_task" in kargs:
            celery_extra_info["parent_task"] = kargs.pop("celery_parent_task")
        else:
            celery_extra_info["parent_task"] = ""
        kargs.update(celery_extra_info = celery_extra_info)
        return original_fn.original_delay(*largs, **kargs)
    return delay2

def oltask(fn):
    """Openlibrary specific decorator for celery tasks that records
    some extra information in the database which we use for
    tracking"""
    @wraps(fn)
    def wrapped(*largs, **kargs):
        setup_stats()
        global task_context
        celery_extra_info = kargs.pop("celery_extra_info",{})
        enqueue_time = celery_extra_info.get('enqueue_time',None)
        parent_task = celery_extra_info.get('parent_task',None)
        s = StringIO.StringIO()
        h = logging.StreamHandler(s)
        h.setFormatter(logging.Formatter("%(asctime)s [%(name)s] [%(levelname)s] %(message)s"))
        logging.root.addHandler(h)
        try:
            started_at = calendar.timegm(datetime.datetime.utcnow().timetuple())
            try:
                wait_time = started_at - enqueue_time
                stats.put("ol.celery.task_wait_time", wait_time * 1000)
                stats.put("ol.celery.%s_wait_time"%fn.__name__, wait_time * 1000)
            except:
                pass
            ret = fn(*largs, **kargs)
        except Exception,e:
            log = s.getvalue()
            tb = traceback.format_exc()
            d = dict(largs = json.dumps(largs),
                     kargs = json.dumps(kargs),
                     command = fn.__name__,
                     enqueued_at = enqueue_time,
                     started_at = started_at,
                     log = log,
                     traceback = tb,
                     result = None,
                     context = task_context,
                     parent_task = parent_task)
            logging.root.removeHandler(h)
            try:
                end_time = calendar.timegm(datetime.datetime.utcnow().timetuple())
                run_time = end_time - started_at
                stats.put("ol.celery.task_run_time", run_time * 1000)
                stats.put("ol.celery.%s_run_time"%fn.__name__, run_time * 1000)
            except:
                pass
            raise ExceptionWrapper(e, d)
        log = s.getvalue()
        d = dict(largs = json.dumps(largs),
                 kargs = json.dumps(kargs),
                 command = fn.__name__,
                 enqueued_at = enqueue_time,
                 started_at = started_at,
                 log = log,
                 result = ret,
                 context = task_context,
                 parent_task = parent_task)
        logging.root.removeHandler(h)
        task_context = {}
        try:
            end_time = calendar.timegm(datetime.datetime.utcnow().timetuple())
            run_time = end_time - started_at
            stats.put("ol.celery.task_run_time", run_time * 1000)
            stats.put("ol.celery.%s_run_time"%fn.__name__, run_time * 1000)
        except:
            pass
        return d
    retval = task(wrapped)
    patched_delay = create_patched_delay(retval)
    retval.original_delay = retval.delay
    retval.delay = patched_delay
    return retval

########NEW FILE########
__FILENAME__ = conftest
import os

def pytest_funcarg__dummy_crontabfile(request):
    "Creates a dummy crontab file that can be used for to try things"
    cronfile = os.tmpnam()
    ip = """* * * * * /bin/true
* * * * * /bin/true"""
    f = open(cronfile,"w")
    f.write(ip)
    f.close()
    request.addfinalizer(lambda : os.remove(cronfile))
    return cronfile
    
def pytest_funcarg__crontabfile(request):
    """Creates a file with an actual command that we can use to test
    running of cron lines"""
    if os.path.exists("/tmp/crontest"):
        os.unlink("/tmp/crontest")
    cronfile = os.tmpnam()
    ip = "* * * * * touch /tmp/crontest"
    f = open(cronfile,"w")
    f.write(ip)
    f.close()
    request.addfinalizer(lambda : os.remove(cronfile))
    return cronfile

def pytest_funcarg__counter(request):
    """Returns a decorator that will create a 'counted' version of the
    functions. The number of times it's been called is kept in the
    .invocations attribute"""
    def counter(fn):
        def _counted(*largs, **kargs):
            _counted.invocations += 1
            fn(*largs, **kargs)
        _counted.invocations = 0
        return _counted
    return counter

def pytest_funcarg__couchdb(request):
    "Returns a mock couchdb database"
    from openlibrary.mocks import mock_couchdb
    db = mock_couchdb.Database()    
    db.save({
            "_id": "_design/cases",
            "views": {
                "all": {
                    "map": "" +  
                    "def f(doc):\n" + 
                    "    if doc.get('type','') == 'case':\n"
                    "        yield doc['_id'], doc"
                    }
                }
            })
    return db



def pytest_funcarg__sequence(request):
    """Returns a function that can be called for sequence numbers
    similar to web.ctx.site.sequence.get_next"""
    t = (x for x in range(100))
    def seq_counter(*largs, **kargs):
        return t.next()
    import web
    # Clean up this mess to mock sequences
    web.ctx = lambda:0
    web.ctx.site = lambda:0
    web.ctx.site.seq = lambda: 0
    web.ctx.site.seq.next_value = seq_counter
    # Now run the test
    return seq_counter


########NEW FILE########
__FILENAME__ = mocklib
"""Simple mock utility.
"""
import urllib, urllib2
from StringIO import StringIO

class Mock:
    def __init__(self):
        self.calls = []
        self.default = None

    def __call__(self, *a, **kw):
        for a2, kw2, _return in self.calls:
            if (a, kw) == (a2, kw2):
                return _return
        return self.default

    def setup_call(self, *a, **kw):
        _return = kw.pop("_return", None)
        call = a, kw, _return
        self.calls.append(call)

def monkeypatch_urllib(monkeypatch, url, response_string):
    """Monkey-patches urllib.urlopen to return the given response
    when urlopen is called with the given url.
    """
    _urlopen = urllib.urlopen
    given_url = url
    
    def urlopen(url, *a, **kw): 
        if url == given_url:
            return urllib.addinfourl(StringIO(response_string), [], url)
        else:
            return _urlopen(url, *a, **kw)
            
    monkeypatch.setattr(urllib, "urlopen", urlopen)

def monkeypatch_urllib2(monkeypatch, url, response_string):
    """Monkey-patches urllib2.urlopen to return the given response
    when urlopen is called with the given url.
    """
    _urlopen = urllib2.urlopen
    given_url = url

    def urlopen(url, *a, **kw): 
        if url == given_url:
            return urllib.addinfourl(StringIO(response_string), [], url)
        else:
            return _urlopen(url, *a, **kw)

    monkeypatch.setattr(urllib2, "urlopen", urlopen)

########NEW FILE########
__FILENAME__ = test_cache
import time
import simplejson

from .. import cache
from ...mocks import mock_memcache

class Test_memcache_memoize:
    def test_encode_args(self):
        m = cache.memcache_memoize(None, key_prefix="foo")
        
        assert m.encode_args([]) == ''
        assert m.encode_args(["a"]) == '"a"'
        assert m.encode_args([1]) == '1' 
        assert m.encode_args(["a", 1]) == '"a",1'
        assert m.encode_args([{"a": 1}]) == '{"a":1}'
        assert m.encode_args([["a", 1]]) == '["a",1]'
        
    def test_generate_key_prefix(self):
        def foo():
            pass
        m = cache.memcache_memoize(foo)
        assert m.key_prefix[:4] == "foo_"
        
    def test_random_string(self):
        m = cache.memcache_memoize(None, "foo")
        assert m._random_string(0) == ""
        
        s1 = m._random_string(1)
        assert isinstance(s1, str)
        assert len(s1) == 1

        s10 = m._random_string(10)
        assert isinstance(s10, str)
        assert len(s10) == 10
        
    def square_memoize(self):
        def square(x):
            return x * x
            
        m = cache.memcache_memoize(square, key_prefix="square")
        m._memcache = mock_memcache.Client([])
        return m
        
    def test_call(self):
        m = self.square_memoize()
        s = m.stats
        
        assert m(10) == 100
        assert [s.calls, s.hits, s.updates, s.async_updates] == [1, 0, 1, 0]
        
        assert m(10) == 100
        assert [s.calls, s.hits, s.updates, s.async_updates] == [2, 1, 1, 0]
        
    def test_update_async(self):
        m = self.square_memoize()
        
        m.update_async(20)
        m.join_threads()
        
        assert m.memcache_get([20], {})[0] == 400

    def test_timeout(self):
        m = self.square_memoize()
        m.timeout = 0.1
        s = m.stats
        
        assert m(10) == 100
        time.sleep(0.1)
        
        assert m(10) == 100
        assert [s.calls, s.hits, s.updates, s.async_updates] == [2, 1, 1, 1]
        
    def test_delete(self):
        m = self.square_memoize()
        
        m(10)
        m(10)
        assert m.stats.updates == 1

        # this should clear the cache and the next call should update the cache.
        m(10, _cache="delete")

        m(10)
        assert m.stats.updates == 2
        
class Test_memoize:
    def teardown_method(self, method):
        cache.memory_cache.clear()
    
    def get(self, key):
        return cache.memory_cache.get(key)
        
    def set(self, key, value):
        cache.memory_cache.set(key, value)
    
    def test_signatures(self):
        def square(x):
            """Returns square x.
            """
            return x * x
        msquare = cache.memoize(engine="memory", key="square")(square)
        assert msquare.__name__ == square.__name__
        assert msquare.__doc__ == square.__doc__
        assert help(msquare) == help(square)

    def test_cache(self):
        @cache.memoize(engine="memory", key="square")
        def square(x):
            return x * x
        
        assert square(2) == 4
        assert self.get("square-2") == 4
        
        # It should read from cache instead of computing if entry is present in the cache
        self.set('square-42', 43)
        assert square(42) == 43
    
    def test_cache_with_tuple_keys(self):
        @cache.memoize(engine="memory", key=lambda x: (str(x), "square"))
        def square(x):
            return x * x
        
        @cache.memoize(engine="memory", key=lambda x: (str(x), "double"))
        def double(x):
            return x + x
        
        assert self.get("3") is None
        assert square(3) == 9
        assert self.get("3") == {"square": 9}
        assert double(3) == 6
        assert self.get("3") == {"square": 9, "double": 6}

########NEW FILE########
__FILENAME__ = test_connections
# This will be moved to core soon.
from openlibrary.plugins.openlibrary import connection as connections
import simplejson

class MockConnection:
    def __init__(self):
        self.docs = {}
        
    def request(self, sitename, path, method="GET", data={}):
        if path == "/get":
            key = data['key']
            if key in self.docs:
                return simplejson.dumps(self.docs[key])
        if path == "/get_many":
            keys = simplejson.loads(data['keys'])
            return simplejson.dumps(dict((k, self.docs[k]) for k in keys))
        else:
            return None

class TestMigrationMiddleware:
    def test_title_prefix(self):
        conn = connections.MigrationMiddleware(MockConnection())

        def add(doc):
            conn.conn.docs[doc['key']] = doc
        
        def get(key):
            json = conn.request("openlibrary.org", "/get", data={"key": key}) 
            return simplejson.loads(json)
            
        add({
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "title_prefix": "The",
            "title": "Book"
        })
            
        assert get("/books/OL1M") == {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "title": "The Book"
        }

        add({
            "key": "/books/OL2M",
            "type": {"key": "/type/edition"},
            "title_prefix": "The ",
            "title": "Book"
        })
            
        assert get("/books/OL2M") == {
            "key": "/books/OL2M",
            "type": {"key": "/type/edition"},
            "title": "The Book"
        }

        add({
            "key": "/books/OL3M",
            "type": {"key": "/type/edition"},
            "title_prefix": "The Book",
        })
        
        assert get("/books/OL3M") == {
            "key": "/books/OL3M",
            "type": {"key": "/type/edition"},
            "title": "The Book"
        }

    def test_authors(self):
        conn = connections.MigrationMiddleware(MockConnection())

        def add(doc):
            conn.conn.docs[doc['key']] = doc
        
        def get(key):
            json = conn.request("openlibrary.org", "/get", data={"key": key}) 
            return simplejson.loads(json)

        def get_many(keys):
            data = dict(keys=simplejson.dumps(keys))
            json = conn.request("openlibrary.org", "/get_many", data=data) 
            return simplejson.loads(json)
        
        add({
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "authors": [{
                "type": {"key": "/type/author_role"}
            }]
        })
        
        assert get("/works/OL1W") == {
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "authors": []
        }
        assert get_many(["/works/OL1W"]) == {
            "/works/OL1W": {
                "key": "/works/OL1W",
                "type": {"key": "/type/work"},
                "authors": []
            }
        }
        
        OL2W = {
            "key": "/works/OL2W",
            "type": {"key": "/type/work"},
            "authors": [{
                "type": {"key": "/type/author_role"},
                "author": {"key": "/authors/OL2A"}
            }]
        }
        add(OL2W)
        
        assert get("/works/OL2W") == OL2W
########NEW FILE########
__FILENAME__ = test_helpers
import web
from openlibrary.core import helpers as h

def test_sanitize():
    # plain html should pass through
    assert h.sanitize("hello") == "hello"
    assert h.sanitize("<p>hello</p>") == "<p>hello</p>"
    
    # broken html must be corrected
    assert h.sanitize("<p>hello") == "<p>hello</p>"
    
    # css class is fine
    assert h.sanitize('<p class="foo">hello</p>') == '<p class="foo">hello</p>'

    # style attribute must be stripped
    assert h.sanitize('<p style="color: red">hello</p>') == '<p>hello</p>'
    
    # style tags must be stripped
    assert h.sanitize('<style type="text/css">p{color: red;}</style><p>hello</p>') == '<p>hello</p>'
    
    # script tags must be stripped
    assert h.sanitize('<script>alert("dhoom")</script>hello') == 'hello'
    
    # rel="nofollow" must be added absolute links
    assert h.sanitize('<a href="http://example.com">hello</a>') == '<a href="http://example.com" rel="nofollow">hello</a>'
    # relative links should pass through
    assert h.sanitize('<a href="relpath">hello</a>') == '<a href="relpath">hello</a>'

def test_safesort():
    from datetime import datetime
    
    y2000 = datetime(2000, 1, 1)
    y2005 = datetime(2005, 1, 1)
    y2010 = datetime(2010, 1, 1)
    
    assert h.safesort([y2005, y2010, y2000, None]) == [None, y2000, y2005, y2010]
    assert h.safesort([y2005, y2010, y2000, None], reverse=True) == [y2010, y2005, y2000, None]
    
    assert h.safesort([[y2005], [None]], key=lambda x: x[0]) == [[None], [y2005]]

def test_datestr():
    from datetime import datetime
    then = datetime(2010, 1, 1, 0, 0, 0)
    
    #assert h.datestr(then, datetime(2010, 1, 1, 0, 0, 0, 10)) == u"just moments ago"
    assert h.datestr(then, datetime(2010, 1, 1, 0, 0, 1)) == u"1 second ago"
    assert h.datestr(then, datetime(2010, 1, 1, 0, 0, 9)) == u"9 seconds ago" 
    
    assert h.datestr(then, datetime(2010, 1, 1, 0, 1, 1)) == u"1 minute ago"
    assert h.datestr(then, datetime(2010, 1, 1, 0, 9, 1)) == u"9 minutes ago"
    
    assert h.datestr(then, datetime(2010, 1, 1, 1, 0, 1)) == u"1 hour ago"
    assert h.datestr(then, datetime(2010, 1, 1, 9, 0, 1)) == u"9 hours ago"
    
    assert h.datestr(then, datetime(2010, 1, 2, 0, 0, 1)) == u"1 day ago"
    
    assert h.datestr(then, datetime(2010, 1, 9, 0, 0, 1)) == u"January 1, 2010"    
    assert h.datestr(then, datetime(2010, 1, 9, 0, 0, 1), lang='fr') == u'1 janvier 2010'

def test_sprintf():
    assert h.sprintf('hello %s', 'python') == 'hello python'
    assert h.sprintf('hello %(name)s', name='python') == 'hello python'

def test_commify():
    assert h.commify(123) == "123"
    assert h.commify(1234) == "1,234"
    assert h.commify(1234567) == "1,234,567"

    assert h.commify(123, lang="te") == "123"
    assert h.commify(1234, lang="te") == "1,234"
    assert h.commify(1234567, lang="te") == "12,34,567"
    
def test_truncate():
    assert h.truncate("hello", 6) == "hello"
    assert h.truncate("hello", 5) == "hello"    
    assert h.truncate("hello", 4) == "hell..."
    
def test_urlsafe():
    assert h.urlsafe("a b") == "a_b"
    assert h.urlsafe("a?b") == "a_b"
    assert h.urlsafe("a?&b") == "a_b"

    assert h.urlsafe("?a") == "a"
    assert h.urlsafe("a?") == "a"
    
def test_get_coverstore_url(monkeypatch):
    assert h.get_coverstore_url() == "http://covers.openlibrary.org"
    
    from infogami import config
    
    monkeypatch.setattr(config, "coverstore_url", "http://0.0.0.0:8090", raising=False)
    assert h.get_coverstore_url() == "http://0.0.0.0:8090"

    # make sure trailing / is always stripped
    monkeypatch.setattr(config, "coverstore_url", "http://0.0.0.0:8090/", raising=False)
    assert h.get_coverstore_url() == "http://0.0.0.0:8090"

def test_texsafe():
    assert h.texsafe("hello") == r"hello"
    assert h.texsafe("a_b") == r"a\_{}b"  
    assert h.texsafe("a < b") == r"a \textless{} b"
    
def test_percentage():
    assert h.percentage(1, 10) == 10.0
    assert h.percentage(0, 0) == 0
########NEW FILE########
__FILENAME__ = test_i18n
import web

# The i18n module should be moved to core.
from openlibrary import i18n

class MockTranslations(dict):
    def gettext(self, message):
        return self.get(message, message)
    
    def ungettext(self, message1, message2, n):
        if n == 1:
            return self.gettext(message1)
        else:
            return self.gettext(message2)
            
class MockLoadTranslations(dict):
    def __call__(self, lang):
        return self.get(lang)
        
    def init(self, lang, translations):
        self[lang] = MockTranslations(translations)

class Test_ungettext:
    def setup_monkeypatch(self, monkeypatch):
        self.d = MockLoadTranslations()
        ctx = web.storage()

        monkeypatch.setattr(i18n, "load_translations", self.d)
        monkeypatch.setattr(web, "ctx", ctx)

    def test_ungettext(self, monkeypatch):
        self.setup_monkeypatch(monkeypatch)
        
        i18n.ungettext("book", "books", 1) == "book"
        i18n.ungettext("book", "books", 2) == "books"
    
        web.ctx.lang = 'fr'
        self.d.init('fr', {
            'book': 'libre',
            'books': 'libres',
        })
        
        i18n.ungettext("book", "books", 1) == "libre"
        i18n.ungettext("book", "books", 2) == "libres"
    
        web.ctx.lang = 'te'
        i18n.ungettext("book", "books", 1) == "book"
        i18n.ungettext("book", "books", 2) == "books"
        
    def test_ungettext_with_args(self, monkeypatch):
        self.setup_monkeypatch(monkeypatch)
        
        i18n.ungettext("one book", "%(n)d books", 1, n=1) == "one book"
        i18n.ungettext("one book", "%(n)d books", 2, n=2) == "2 books"

        web.ctx.lang = 'fr'
        self.d.init('fr', {
            'one book': 'un libre',
            '%(n)d books': '%(n)d libres',
        })

        i18n.ungettext("one book", "%(n)d books", 1, n=1) == "un libre"
        i18n.ungettext("one book", "%(n)d books", 2, n=2) == "2 libres"
        
########NEW FILE########
__FILENAME__ = test_ia
from openlibrary.core import ia

def test_xml2dict():
    assert ia.xml2dict("<metadata><x>1</x><y>2</y></metadata>") == {"x": "1", "y": "2"}
    assert ia.xml2dict("<metadata><x>1</x><y>2</y></metadata>", x=[]) == {"x": ["1"], "y": "2"}
    
    assert ia.xml2dict("<metadata><x>1</x><x>2</x></metadata>") == {"x": "2"}
    assert ia.xml2dict("<metadata><x>1</x><x>2</x></metadata>", x=[]) == {"x": ["1", "2"]}
    
def test_get_metaxml(monkeypatch, mock_memcache):
    import StringIO
    import urllib2
    
    metaxml = None
    def urlopen(url):
        return StringIO.StringIO(metaxml)
    
    monkeypatch.setattr(urllib2, "urlopen", urlopen)
    
    
    # test with correct xml
    metaxml = """<?xml version="1.0" encoding="UTF-8"?>
    <metadata>
        <title>Foo</title>
        <identifier>foo00bar</identifier>
        <collection>printdisabled</collection>
        <collection>inlibrary</collection>
    </metadata>
    """
    
    assert ia.get_meta_xml("foo00bar") == {
        "title": "Foo", 
        "identifier": "foo00bar",
        "collection": ["printdisabled", "inlibrary"],
        'external-identifier': [],
    }
    
    # test with html errors
    metaxml = """<html>\n<head>\n <title>Internet Archive: Error</title>..."""
    assert ia.get_meta_xml("foo01bar") == {}
    
    # test with bad xml
    metaxml = """<?xml version="1.0" encoding="UTF-8"?>
    <metadata>
        <title>Foo</title>
        <identifier>foo00bar
    """
    assert ia.get_meta_xml("foo02bar") == {}
########NEW FILE########
__FILENAME__ = test_init
import tempfile

from openlibrary.core.init import Init

class TestService:
    def create_service(self, name, config):
        config2 = {
            name: config
        }
        return Init(config2).services[name]

    
    def test_stdout(self, tmpdir):
        s = self.create_service("echo", {
            "command": "echo hello",
            "stdout": tmpdir.join("echo.txt").strpath
        })
        
        s.start().wait()
        assert tmpdir.join("echo.txt").read().strip() == "hello"

    def test_change_root(self, tmpdir):
        s = self.create_service("pwd", {
            "command": "pwd",
            "root": tmpdir.strpath,
            "stdout": tmpdir.join("pwd.txt").strpath
        })
    
        s.start().wait()
        assert tmpdir.join("pwd.txt").read().strip() == tmpdir.strpath
        
    def test_poll_and_wait(self):
        s = self.create_service("sleep", {
            "command": "sleep 0.2"
        })
        s.start()
        assert s.poll() is None
        assert s.wait() == 0
        assert s.poll() is 0
        
    def test_stop(self):
        s = self.create_service("sleep", {
            "command": "sleep 100"
        })
        s.start()
        assert s.is_alive() is True
        s.stop(timeout=0.2)
        assert s.is_alive() is False
    
########NEW FILE########
__FILENAME__ = test_iprange
from .. import iprange

def test_parse_ip_ranges():
    def f(text):
        return list(iprange.parse_ip_ranges(text))
    assert f("1.2.3.4") == ["1.2.3.4"]
    assert f("1.2.3 - 4.*") == [("1.2.3.0", "1.2.4.255")]
    assert f("1.2.3.4-10") == [("1.2.3.4", "1.2.3.10")]
    assert f("1.2.3.4 - 2.3.4.40") == [("1.2.3.4", "2.3.4.40")]
    assert f("1.2.*.* ") == ["1.2.0.0/16"]
    assert f("1.2.3.* ") == ["1.2.3.0/24"]

class TestIPDict:
    def test_simple(self):
        d = iprange.IPDict()
        d.add_ip_range("1.2.3.0/24", "foo")
        
        assert d.get("1.2.3.4") == "foo"
        assert d.get("1.2.3.44") == "foo"
        assert d.get("1.2.4.5") == None
        assert d.get("100.2.4.5") == None

    def test_add_ip_range_text(self):
        text = (
            "#ip ranges\n" +
            "1.2.3.0/24\n" +
            "9.8.*.*")
        
        d = iprange.IPDict()
        d.add_ip_range_text(text, 'foo')
        
        assert '1.2.3.2' in d
        assert '1.2.4.2' not in d
        assert '9.8.1.2' in d
        assert '9.8.3.2' in d
        assert '9.9.3.2' not in d
########NEW FILE########
__FILENAME__ = test_minicron
"""
Tests to test the minicron implementation.

A simple cron substitute for the dev instance.
"""

import os
import time
import datetime

import py

from openlibrary.core import minicron

def test_nonexistentinputfile(dummy_crontabfile):
    "Create a cron parser with an non existent input file"
    cron = minicron.Minicron("/non/existent/file.tests", None, 0.01)
    py.test.raises(IOError, cron.run)

def test_malformed_cron_line(dummy_crontabfile):
    cron = minicron.Minicron(dummy_crontabfile, 1)
    d = datetime.datetime.now()
    py.test.raises(minicron.BadCronLine, cron._matches_cron_expression, d, "* * 5 * * do_something")

def test_ticker(dummy_crontabfile, monkeypatch, counter):
    "Checks that the ticker executes once a minute"
    cron = minicron.Minicron(dummy_crontabfile, None, 0.1) # Make the clock tick once in 0.1 seconds (so that the test finishes quickly)
    monkeypatch.setattr(cron, '_tick', counter(cron._tick))
    cron.run(3) 
    assert cron._tick.invocations == 3, "Ticker ran %d times (should be 3)"%cron._tick.invocations

def test_cronline_parser_everyminute(dummy_crontabfile):
    "Checks the cronline parser for executing every minute/hour"
    cron = minicron.Minicron(dummy_crontabfile, 1)
    d = datetime.datetime.now()
    assert cron._matches_cron_expression(d, "* * * * * do_something"), "* * * * * should be executed every minute/hour but isn't"

def test_cronline_parser_fifthminuteofeveryhour(dummy_crontabfile):
    "Checks the cronline parser for executing at the fifth minute of every hour"
    cron = minicron.Minicron(dummy_crontabfile, 1)
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 1, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, "5 * * * * do_something") == False, "5 * * * * should be executed only at the fifth minute but is executed at the first"
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 1, minute = 5, second = 0)
    assert cron._matches_cron_expression(d, "5 * * * * do_something"), "5 * * * * should be executed at the fifth minute but is not"

def test_cronline_parser_everyfifthminute(dummy_crontabfile):
    "Checks the cronline parser for executing at every fifth minute"
    cron = minicron.Minicron(dummy_crontabfile, 1)
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 1, minute = 2, second = 0)
    assert cron._matches_cron_expression(d, "*/5 * * * * do_something") == False, "*/5 * * * * should be executed only at every fifth minute but is executed at the second"
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 1, minute = 5, second = 0)
    assert cron._matches_cron_expression(d, "*/5 * * * * do_something"), "*/5 * * * * should be executed at the fifth minute but is not"

def test_cronline_parser_thirdhourofeveryday(dummy_crontabfile):
    "Checks the cronline parser for executing at the third hour of every day"
    cron = minicron.Minicron(dummy_crontabfile, 1)
    expression = "* 3 * * * do_something"
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 1, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, expression) == False, " %s should be executed only at the third hour but is executed at the first"%expression
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 3, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, expression), "%s should be executed at the third hour but is not"%expression
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 6, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, expression) == False, "%s should not be executed in the 6th hour but it is"%expression

def test_cronline_parser_everythirdhour(dummy_crontabfile):
    "Checks the cronline parser for executing every third hour"
    cron = minicron.Minicron(dummy_crontabfile, 1)
    expression = "* */3 * * * do_something"
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 1, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, expression) == False, " %s should be executed only every third hour but is executed at the first"%expression
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 3, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, expression), "%s should be executed at the third hour but is not"%expression
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 6, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, expression), "%s should be executed at the sixth hour but is not"%expression
    d = datetime.datetime(year = 2010, month = 8, day = 10, hour = 5, minute = 1, second = 0)
    assert cron._matches_cron_expression(d, expression) == False, "%s should not be executed at the fifth hour but it is"%expression

def test_cronline_running(crontabfile):
    "Checks if the cron actually executes commands"
    assert not os.path.exists("/tmp/crontest") # Make sure that out test file doesn't exist
    cron = minicron.Minicron(crontabfile, None, 0.1)
    cron.run(1)
    assert os.path.exists("/tmp/crontest"), "/tmp/crontest should have been created by the cron but its not"
    os.unlink("/tmp/crontest")

def test_proper_run(monkeypatch, counter):
    "Tries a cron run with a non-trivial time pattern"
    cronfile = os.tmpnam()
    f = open(cronfile,"w")
    f.write("*/2 0 * * * touch /tmp/foo\n") # Every alternate minute in the first hour only
    f.close()
    cron = minicron.Minicron(cronfile, datetime.datetime(hour =0, minute = 0, second = 0, year = 2011, month = 1, day =1), 0.01)
    monkeypatch.setattr(cron, '_run_command', counter(cron._run_command))
    cron.run(120) # Run for 2 hours
    assert cron._run_command.invocations == 29, "The function should have run every alternate minute in the first hour (29 times) but ran %s times"%cron._run_command.invocations 

    f = open(cronfile,"w")
    f.write("*/2 0 * * * touch /tmp/foo\n") # Every alternate minute in the first hour only
    f.write("* 1 * * * touch /tmp/foo\n") # Every minute in the second hour
    f.close()
    cron = minicron.Minicron(cronfile, datetime.datetime(hour =0, minute = 0, second = 0, year = 2011, month = 1, day =1), 0.01)
    monkeypatch.setattr(cron, '_run_command', counter(cron._run_command))
    cron.run(180) # Run for 3 hours
    assert cron._run_command.invocations == 89, "The function should have run every alternate minute in the first hour (29 times) and every minute in the second (60 times) i.e. totally 89 times but ran %s times"%cron._run_command.invocations

    f = open(cronfile,"w")
    f.write("1 1 * * * touch /tmp/foo\n") # Every alternate minute in the first hour only
    f.close()
    cron = minicron.Minicron(cronfile, datetime.datetime(hour =0, minute = 0, second = 0, year = 2011, month = 1, day =1), 0.01)
    monkeypatch.setattr(cron, '_run_command', counter(cron._run_command))
    cron.run(180) # Run for 3 hours
    assert cron._run_command.invocations == 1, "The function should have run just once in the second minute of the second hour but ran %s times"%cron._run_command.invocations

    

########NEW FILE########
__FILENAME__ = test_models
from openlibrary.core import models

# this should be moved to openlibrary.core
from openlibrary.plugins.upstream.models import UnitParser

class MockSite:
    def get(self, key):
        return models.Thing(self, key, data={})
        
    def _get_backreferences(self, thing):
        return {}


class TestEdition:
    def test_url(self):
        data = {
            "key": "/books/OL1M", 
            "type": {"key": "/type/edition"},
            "title": "foo"
        }
        
        e = models.Edition(MockSite(), "/books/OL1M", data=data)
        
        assert e.url() == "/books/OL1M/foo"
        assert e.url(v=1) == "/books/OL1M/foo?v=1"
        assert e.url(suffix="/add-cover") == "/books/OL1M/foo/add-cover"
        

        data = {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
        }
        e = models.Edition(MockSite(), "/books/OL1M", data=data)
        assert e.url() == "/books/OL1M/untitled"
        
    def test_get_ebook_info(self):
        data = {
            "key": "/books/OL1M", 
            "type": {"key": "/type/edition"},
            "title": "foo"
        }
        
        e = models.Edition(MockSite(), "/books/OL1M", data=data)
        assert e.get_ebook_info() == {}

class TestAuthor:
    def test_url(self):
        data = {
            "key": "/authors/OL1A", 
            "type": {"key": "/type/author"},
            "name": "foo"
        }
        
        e = models.Author(MockSite(), "/authors/OL1A", data=data)
        
        assert e.url() == "/authors/OL1A/foo"
        assert e.url(v=1) == "/authors/OL1A/foo?v=1"
        assert e.url(suffix="/add-photo") == "/authors/OL1A/foo/add-photo"        

        data = {
            "key": "/authors/OL1A",
            "type": {"key": "/type/author"},
        }
        e = models.Author(MockSite(), "/authors/OL1A", data=data)
        assert e.url() == "/authors/OL1A/unnamed"


class TestSubject:
    def test_url(self):
        subject = models.Subject({
            "key": "/subjects/love"
        })
        assert subject.url() == "/subjects/love"
        assert subject.url("/lists") == "/subjects/love/lists"
        
        
class TestList:
    def test_owner(self):
        models.register_models()
        self._test_list_owner("/people/anand")
        self._test_list_owner("/people/anand-test")
        self._test_list_owner("/people/anand_test")
        
    def _test_list_owner(self, user_key):
        from openlibrary.mocks.mock_infobase import MockSite
        site = MockSite()
        list_key = user_key + "/lists/OL1L"
        
        self.save_doc(site, "/type/user", user_key)
        self.save_doc(site, "/type/list", list_key)
        
        list =  site.get(list_key)
        assert list is not None
        assert isinstance(list, models.List)
        
        assert list.get_owner() is not None
        assert list.get_owner().key == user_key
        
    def save_doc(self, site, type, key, **fields):
        d = {
            "key": key,
            "type": {"key": type}
        }
        d.update(fields)
        site.save(d)
        
class TestLibrary:
    def test_class(self, mock_site):
        mock_site.save({
            "key": "/libraries/ia",
            "type": {"key": "/type/library"}
        })
        doc = mock_site.get("/libraries/ia")
        assert doc.__class__.__name__ == "Library"
        
    def test_parse_ip_ranges(self):
        doc = models.Library(None, "/libraries/foo")
        def compare_ranges(test, expect):
            result = list(doc.parse_ip_ranges(test))
            assert result == expect
        compare_ranges("", [])
        compare_ranges("1.2.3.4", ["1.2.3.4"])
        compare_ranges("1.2.3.4", ["1.2.3.4"])
        compare_ranges("1.1.1.1\n2.2.2.2", ["1.1.1.1", "2.2.2.2"])
        compare_ranges("1.1.1.1-2.2.2.2", [("1.1.1.1", "2.2.2.2")])
        compare_ranges("1.1.1.1 # comment \n2.2.2.2", ["1.1.1.1", "2.2.2.2"])
        compare_ranges("1.1.1.1\n # comment \n2.2.2.2", ["1.1.1.1", "2.2.2.2"])
        compare_ranges("1.2.3.0/24", ["1.2.3.0/24"])
        compare_ranges("1.2.3.*", ["1.2.3.0/24"])
        compare_ranges("1.2.*.*", ["1.2.0.0/16"])
        compare_ranges("1.*.*.*", ["1.0.0.0/8"])
        compare_ranges("*", [])
        compare_ranges("*.1", [])
        compare_ranges("1.2.3-10.*", [("1.2.3.0", "1.2.10.255")])
        compare_ranges("1.2.3.", [("1.2.3.0", "1.2.3.255")])
        compare_ranges("1.1.", [])
        compare_ranges("1.2.3.1-254", [("1.2.3.1", "1.2.3.254")])
        compare_ranges("216.63.14.0/24\n207.193.121.0/24\n207.193.118.0/24", ["216.63.14.0/24", "207.193.121.0/24", "207.193.118.0/24"])
        compare_ranges("208.70.20-30.", [])

    def test_bad_ip_ranges(self):
        doc = models.Library(None, "/libraries/foo")
        def test_ranges(test, expect):
            result = doc.find_bad_ip_ranges(test)
            assert result == expect
        test_ranges("", [])
        test_ranges("1.2.3.4", [])
        test_ranges("1.1.1.1\n2.2.2.2", [])
        test_ranges("1.1.1.1-2.2.2.2", [])
        test_ranges("1.1.1.1 # comment \n2.2.2.2", [])
        test_ranges("1.1.1.1\n # comment \n2.2.2.2", [])
        test_ranges("1.2.3.0/24", [])
        test_ranges("1.2.3.*", [])
        test_ranges("1.2.*.*", [])
        test_ranges("1.*.*.*", [])
        test_ranges("*", ["*"])
        test_ranges("*.1", ["*.1"])
        test_ranges("1.2.3-10.*", [])
        test_ranges("1.2.3.", [])
        test_ranges("1.1.", ['1.1.'])
        test_ranges("1.2.3.1-254", [])
        test_ranges("216.63.14.0/24\n207.193.121.0/24\n207.193.118.0/24", [])
        test_ranges("1.2.3.4,2.3.4.5", ["1.2.3.4,2.3.4.5"])
        test_ranges("1.2-3.*", ["1.2-3.*"])
    
    def test_has_ip(self, mock_site):
        mock_site.save({
            "key": "/libraries/ia",
            "type": {"key": "/type/library"},
            "ip_ranges": "1.1.1.1\n2.2.2.0/24"
        })
        
        ia = mock_site.get("/libraries/ia")
        assert ia.has_ip("1.1.1.1") 
        assert not ia.has_ip("1.1.1.2")

        assert ia.has_ip("2.2.2.10")
        assert not ia.has_ip("2.2.10.2")

        mock_site.save({
            "key": "/libraries/ia",
            "type": {"key": "/type/library"},
            "ip_ranges": "1.1.1.",
        })

        ia = mock_site.get("/libraries/ia")
        assert ia.has_ip("1.1.1.1")
        assert ia.has_ip("1.1.1.2")

        assert not ia.has_ip("2.2.2.10")
        assert not ia.has_ip("2.2.10.2")

        mock_site.save({
            "key": "/libraries/ia",
            "type": {"key": "/type/library"},
            "ip_ranges": "1.1.",
        })

        ia = mock_site.get("/libraries/ia")

        assert not ia.has_ip("2.2.2.2")

########NEW FILE########
__FILENAME__ = test_olmarkdown
from openlibrary.core.olmarkdown import OLMarkdown

def test_olmarkdown():
    def md(text):
        return OLMarkdown(text).convert().strip()
        
    def p(html):
        # markdown always wraps the result in <p>.
        return "<p>%s\n</p>" % html
        
    assert md("**foo**") == p("<strong>foo</strong>")
    assert md("<b>foo</b>") == p('<b>foo</b>')
    assert md("http://openlibrary.org") == p(
            '<a href="http://openlibrary.org" rel="nofollow">' +
                'http://openlibrary.org' +
            '</a>'
        )
        
    # why extra spaces?
    assert md("a\nb") == p("a<br/>\n   b")
########NEW FILE########
__FILENAME__ = test_processors
from openlibrary.core.processors import readableurls as processors
from infogami.infobase import client, common
import web

class MockSite:
    def __init__(self):
        self.docs = {}
        self.olids = {}
    
    def get(self, key):
        return self.docs.get(key)
        
    def add(self, doc):
        #@@ UGLY!
        doc = common.parse_query(doc)
        doc = client.Site(None, None)._process_dict(doc)
        
        key = doc['key']
        self.docs[key] = client.create_thing(self, key, doc)
        
        olid = key.split("/")[-1]
        if web.re_compile(r'OL\d+[A-Z]').match(olid):
            self.olids[olid] = key
            
    def _request(self, path, method=None, data=None):
        if path == "/olid_to_key":
            olid = data['olid']
            return web.storage(key=self.olids.get(olid))
        
    def _get_backreferences(self):
        return {}
        
def test_MockSite():
    site = MockSite()
    assert site.get("/books/OL1M") == None
    
    book = {
        "key": "/books/OL1M",
        "type": {
            "key": "/type/edition"
        },
        "title": "foo"
    }
    site.add(book)
    
    assert site.get("/books/OL1M") is not None
    assert site.get("/books/OL1M").dict() == book
    
    assert site._request("/olid_to_key", data={"olid": "OL1M"}) == {"key": "/books/OL1M"}
    
def _get_mock_site():
    site = MockSite()
    
    book = {
        "key": "/books/OL1M",
        "type": {
            "key": "/type/edition"
        },
        "title": "foo"
    }
    site.add(book)
    
    list = {
        "key": "/people/joe/lists/OL1L",
        "type": {
            "key": "/type/list"
        },
        "name": "foo"
    }
    site.add(list)    
    return site
    
def test_get_object():
    site = _get_mock_site()
    
    def f(key):
        doc = processors._get_object(site, key)
        return doc and doc.key
    
    assert f("/books/OL1M") == "/books/OL1M"
    assert f("/b/OL1M") == "/books/OL1M"
    assert f("/whatever/OL1M") == "/books/OL1M"
    assert f("/not-there") == None
    
_mock_site = _get_mock_site()
def get_readable_path(path, encoding=None):
    patterns = processors.ReadableUrlProcessor.patterns
    return processors.get_readable_path(_mock_site, path, patterns, encoding=encoding)
    
def test_book_urls():
    f = get_readable_path
    
    # regular pages
    assert f(u"/books/OL1M") == (u"/books/OL1M", u"/books/OL1M/foo")
    assert f(u"/books/OL1M/foo") == (u"/books/OL1M", u"/books/OL1M/foo")
    assert f(u"/books/OL1M/foo/edit") == (u"/books/OL1M/edit", u"/books/OL1M/foo/edit")

    # with bad title
    assert f(u"/books/OL1M/bar") == (u"/books/OL1M", u"/books/OL1M/foo")
    assert f(u"/books/OL1M/bar/edit") == (u"/books/OL1M/edit", u"/books/OL1M/foo/edit")
    
    # test /b/ redirects
    assert f(u"/b/OL1M") == (u"/books/OL1M", u"/books/OL1M/foo")
    assert f(u"/b/OL1M/foo/edit") == (u"/books/OL1M/edit", u"/books/OL1M/foo/edit")
    
    # test olid redirects
    assert f(u"/whatever/OL1M") == (u"/books/OL1M", u"/books/OL1M/foo")
    
    # test encoding
    assert f(u"/books/OL1M.json") == (u"/books/OL1M.json", u"/books/OL1M.json")
    assert f(u"/books/OL1M", encoding="json") == (u"/books/OL1M", u"/books/OL1M")

def test_list_urls():
    f = get_readable_path
    
    print f(u"/people/joe/lists/OL1L")

    assert f(u"/people/joe/lists/OL1L") == (
        u"/people/joe/lists/OL1L", 
        u"/people/joe/lists/OL1L/foo"
    )

    assert f(u"/people/joe/lists/OL1L/bar") == (
        u"/people/joe/lists/OL1L", 
        u"/people/joe/lists/OL1L/foo"
    )

    assert f(u"/people/joe/lists/OL1L/bar/edit") == (
        u"/people/joe/lists/OL1L/edit", 
        u"/people/joe/lists/OL1L/foo/edit"
    )
    
    assert f(u"/people/sam/lists/OL1L/foo") == (
        u"/people/joe/lists/OL1L", 
        u"/people/joe/lists/OL1L/foo"
    )

########NEW FILE########
__FILENAME__ = test_support
import datetime

import py


def test_create_case(couchdb, sequence):
    "Tries to create a case"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    c = s.create_case(creator_name      = "Noufal Ibrahim",
                      creator_email     = "noufal@archive.org",
                      creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                      subject           = "Testing",
                      description       = "This is a test request",
                      assignee          = "anand@archive.org")
    assert c.caseid == "case-0"
    assert c.creator_name == "Noufal Ibrahim"
    assert c.creator_email == "noufal@archive.org"
    assert c.creator_useragent == "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)"
    assert c.subject == "Testing"
    assert c.description == "This is a test request"
    assert c.assignee == "anand@archive.org"
    assert c.status == "new"
    assert c.type == "case"
    created_date = c.created
    current_date = datetime.datetime.utcnow()
    assert created_date.day == current_date.day
    assert created_date.month == current_date.month
    assert created_date.year == current_date.year

def test_sequence_numbers(couchdb, sequence):
    "Creates a bunch of cases and checks their case numbers"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    for i in range(0,10):
        c = s.create_case(creator_name      = "Noufal Ibrahim",
                          creator_email     = "noufal@archive.org",
                          creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                          subject           = "Testing",
                          description       = "This is a test request",
                          assignee          = "anand@archive.org")
        assert c.caseid == "case-%d"%i

def test_history_entry(couchdb, sequence):
    "Test history entries upon creation of a new case"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    c = s.create_case(creator_name      = "Noufal Ibrahim",
                      creator_email     = "noufal@archive.org",
                      creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                      subject           = "Testing",
                      description       = "This is a test request",
                      assignee          = "anand@archive.org")    
    entry = c.history[0]
    assert entry.at == c.created
    assert entry.by == "Noufal Ibrahim"
    assert entry.text == "Case created"


def test_readback(couchdb, sequence):
    "Test all ways of reading the case back"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    c = s.create_case(creator_name      = "Noufal Ibrahim",
                      creator_email     = "noufal@archive.org",
                      creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                      subject           = "Testing",
                      description       = "This is a test request",
                      assignee          = "anand@archive.org")
    
    c0 = s.get_case("case-0") #Full string id
    c1 = s.get_case(0)        #Numeric id
    c2 = s.get_case("0")      #Partial string id
    assert c0 == c1 == c2
    assert c0.caseid == "case-0"
    

def test_change_status(couchdb, sequence):
    "Check the API to change case statuses"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    c = s.create_case(creator_name      = "Noufal Ibrahim",
                      creator_email     = "noufal@archive.org",
                      creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                      subject           = "Testing",
                      description       = "This is a test request",
                      assignee          = "anand@archive.org")
    assert c.caseid == "case-0"
    assert c.status == "new"
    c.change_status("assigned", "mary@archive.org")
    c = s.get_case("case-0")
    assert c.status == "assigned"
    entry = c.history[-1]
    assert entry.by == "mary@archive.org"
    assert entry.text == "Case status changed to 'assigned'"
        

def test_reassign(couchdb, sequence):
    "Checks if the case can be reassigned"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    c = s.create_case(creator_name      = "Noufal Ibrahim",
                      creator_email     = "noufal@archive.org",
                      creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                      subject           = "Testing",
                      description       = "This is a test request",
                      assignee          = "anand@archive.org")
    assert c.caseid == "case-0"
    assert c.assignee == "anand@archive.org"
    c.reassign("george@archive.org", "mary@archive.org")
    c = s.get_case("case-0")
    assert c.assignee == "george@archive.org"
    entry = c.history[-1]
    assert entry.by == "mary@archive.org"
    assert entry.text == "Case reassigned to 'george@archive.org'"

def test_add_worklog_entry(couchdb, sequence):
    "Checks if we can add worklog entries"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    c = s.create_case(creator_name      = "Noufal Ibrahim",
                      creator_email     = "noufal@archive.org",
                      creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                      subject           = "Testing",
                      description       = "This is a test request",
                      assignee          = "anand@archive.org")
    assert c.caseid == "case-0"
    assert len(c.history) == 1
    c.add_worklog_entry("george@archive.org", "Test entry")
    c = s.get_case(0)
    assert len(c.history) == 2
    entry = c.history[-1]
    assert entry.by == "george@archive.org"
    assert entry.text == "Test entry"

def test_modification_date(couchdb, sequence):
    "Tests the last modified time"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    c = s.create_case(creator_name      = "Noufal Ibrahim",
                      creator_email     = "noufal@archive.org",
                      creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                      subject           = "Testing",
                      description       = "This is a test request",
                      assignee          = "anand@archive.org")
    assert c.last_modified == c.created
    c.add_worklog_entry("noufal@archive.org", "Test entry")
    c = s.get_case(0)
    assert c.last_modified == c.history[-1].at

def test_get_all_cases(couchdb, sequence):
    "Try to create a bunch of cases and get them all back"
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    for i in range(0,10):
        c = s.create_case(creator_name      = "Noufal Ibrahim",
                          creator_email     = "noufal@archive.org",
                          creator_useragent = "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.18) Gecko/20110323 Iceweasel/3.5.18 (like Firefox/3.5.18)",
                          subject           = "Testing",
                          description       = "This is a test request",
                          assignee          = "anand@archive.org")
    returned_caseids = sorted([x.caseid for x in s.get_all_cases()])
    expected_caseids = ["case-%s"%x for x in range(0,10)]
    assert returned_caseids == expected_caseids

    
def test_nonexistentcase(couchdb):
    from openlibrary.core import support
    s = support.Support(db = couchdb)
    py.test.raises (support.InvalidCase, s.get_case, 12345)

    

########NEW FILE########
__FILENAME__ = waitinglist
"""Implementation of waiting-list feature for OL loans.

Each waiting instance is represented as a document in the store as follows:

    {
        "_key": "waiting-loan-OL123M-anand",
        "type": "waiting-loan",
        "user": "/people/anand",
        "book": "/books/OL123M",
        "status": "waiting",
        "since": "2013-09-16T06:09:16.577942",
        "last-update": "2013-10-01T06:09:16.577942"
    }
"""
import datetime
import web
from . import helpers as h
from .sendmail import sendmail_with_template
import logging
from infogami.infobase.client import ClientException

logger = logging.getLogger("openlibrary.waitinglist")

class WaitingLoan(dict):
    def get_book(self):
        return web.ctx.site.get(self['book'])

    def get_user(self):
        return web.ctx.site.get(self['user'])

    def get_position(self):
        return self['position']

    def get_waitinglist_size(self):
        return self['wl_size']

    def get_waiting_in_days(self):
        since = h.parse_datetime(self['since'])
        delta = datetime.datetime.utcnow() - since
        # Adding 1 to round off the the extra seconds in the delta
        return delta.days + 1

    def get_expiry_in_hours(self):
        if "expiry" in self:
            delta = h.parse_datetime(self['expiry']) - datetime.datetime.utcnow()
            delta_seconds = delta.days * 24 * 3600 + delta.seconds
            delta_hours = delta_seconds / 3600
            return max(0, delta_hours)
        return 0

def _query_values(name, value):
    docs = web.ctx.site.store.values(type="waiting-loan", name=name, value=value, limit=1000)
    return [WaitingLoan(doc) for doc in docs]

def _query_keys(name, value):
    return web.ctx.site.store.keys(type="waiting-loan", name=name, value=value, limit=1000)

def get_waitinglist_for_book(book_key):
    """Returns the lost of  records for the users waiting for given book.

    This is admin-only feature. Works only if the current user is an admin.
    """
    wl = _query_values(name="book", value=book_key)
    # sort the waiting list by timestamp
    return sorted(wl, key=lambda doc: doc['since'])

def get_waitinglist_size(book_key):
    """Returns size of the waiting list for given book.
    """
    key = "ebooks" + book_key
    ebook = web.ctx.site.store.get(key) or {}
    size = ebook.get("wl_size", 0)
    return int(size)

def get_waitinglist_for_user(user_key):
    """Returns the list of records for all the books that a user is waiting for.
    """
    return _query_values(name="user", value=user_key)

def is_user_waiting_for(user_key, book_key):
    """Returns True if the user is waiting for specified book.
    """
    return get_waiting_loan_object(user_key, book_key) is not None

def get_waiting_loan_object(user_key, book_key):
    ukey = user_key.split("/")[-1]
    bkey = book_key.split("/")[-1]
    key = "waiting-loan-%s-%s" % (ukey, bkey)
    doc = web.ctx.site.store.get(key)
    if doc and doc['status'] != 'expired':
        return WaitingLoan(doc)

def get_waitinglist_position(user_key, book_key):
    ukey = user_key.split("/")[-1]
    bkey = book_key.split("/")[-1]
    key = "waiting-loan-%s-%s" % (ukey, bkey)

    wl = get_waitinglist_for_book(book_key)
    keys = [doc['_key'] for doc in wl]
    try:
        # Adding one to start the position from 1 instead of 0
        return keys.index(key) + 1
    except ValueError:
        return -1

def join_waitinglist(user_key, book_key):
    """Adds a user to the waiting list of given book.

    It is done by createing a new record in the store.
    """
    ukey = user_key.split("/")[-1]
    bkey = book_key.split("/")[-1]
    key = "waiting-loan-%s-%s" % (ukey, bkey)
    timestamp = datetime.datetime.utcnow().isoformat()

    d = {
        "_key": key,
        "type": "waiting-loan",
        "user": user_key,
        "book": book_key,
        "status": "waiting",
        "since": timestamp,
        "last-update": timestamp,
    }
    web.ctx.site.store[key] = d
    update_waitinglist(book_key)

def leave_waitinglist(user_key, book_key):
    """Removes the given user from the waiting list of the given book.
    """
    ukey = user_key.split("/")[-1]
    bkey = book_key.split("/")[-1]
    key = "waiting-loan-%s-%s" % (ukey, bkey)
    web.ctx.site.store.delete(key)
    update_waitinglist(book_key)

def update_waitinglist(book_key):
    """Updates the status of the waiting list.

    It does the following things:

    * updates the position of each person the waiting list (do we need this?)
    * marks the first one in the waiting-list as active if the book is available to borrow
    * updates the waiting list size in the ebook document (this is used by solr to index wl size)
    * If the person who borrowed the book is in the waiting list, removed it (should never happen)

    This function should be called on the following events:
    * When a book is checked out or returned
    * When a person joins or leaves the waiting list
    """
    logger.info("BEGIN updating %r", book_key)
    wl = get_waitinglist_for_book(book_key)
    checkedout = _is_loaned_out(book_key)

    ebook_key = "ebooks" + book_key
    ebook = web.ctx.site.store.get(ebook_key) or {}

    documents = {}
    def save_later(doc):
        """Remembers to save on commit."""
        documents[doc['_key']] = doc

    def update_doc(doc, **kwargs):
        dirty = False
        for name, value in kwargs.items():
            if doc.get(name) != value:
                doc[name] = value
                dirty = True
        if dirty:
            save_later(doc)

    def commit():
        """Saves all the documents """
        try:
            web.ctx.site.store.update(documents)
        except ClientException, e:
            logger.error("Failed to save documents.", exc_info=True)
            logger.error("Error data: %r", e.get_data())

    if checkedout:
        book = web.ctx.site.get(book_key)
        loans = book.get_loans()
        
        loaned_users = [loan['user'] for loan in loans]
        for doc in wl[:]:
            # Delete from waiting list if a user has already borrowed this book
            if doc['user'] in loaned_users:
                update_doc(doc, _delete=True)
                wl.remove(doc)

    for i, doc in enumerate(wl):
        update_doc(doc, position=i+1, wl_size=len(wl))

    # Mark the first entry in the waiting-list as available if the book
    # is not checked out.
    if not checkedout and wl and wl[0]['status'] != 'available':
        # one day
        expiry = datetime.datetime.utcnow() + datetime.timedelta(1)
        update_doc(wl[0], status='available', expiry=expiry.isoformat())

    # for the end user, a book is not available if it is either
    # checked out or someone is waiting.
    not_available = bool(checkedout or wl)

    # update ebook document.
    ebook.update({
        "_key": ebook_key,
        "type": "ebook",
        "book_key": book_key,
        "borrowed": str(not_available).lower(), # store as string "true" or "false"
        "wl_size": len(wl)
    })
    save_later(ebook)
    commit()

    book = web.ctx.site.get(book_key)
    if wl:
        # If some people are waiting and the book is checked out,
        # send email to the person who borrowed the book.
        # 
        # If the book is not checked out, inform the first person 
        # in the waiting list
        if checkedout:
            sendmail_people_waiting(book)        
        else:
            sendmail_book_available(book)
    logger.info("END updating %r", book_key)

def _is_loaned_out(book_key):
    book = web.ctx.site.get(book_key)
    return book.get_available_loans() == []

def sendmail_book_available(book):
    """Informs the first person in the waiting list that the book is available.

    Safe to call multiple times. This'll make sure the email is sent only once.
    """
    wl = book.get_waitinglist()
    if wl and wl[0]['status'] == 'available' and not wl[0].get('available_email_sent'):
        record = wl[0]
        user = record.get_user()
        email = user and user.get_email()
        sendmail_with_template("email/waitinglist_book_available", to=email, user=user, book=book)
        record['available_email_sent'] = True
        web.ctx.site.store[record['_key']] = record

def sendmail_people_waiting(book):
    """Send mail to the person who borrowed the book when the first person joins the waiting list.

    Safe to call multiple times. This'll make sure the email is sent only once.
    """
    # also supports multiple loans per book
    loans = [loan for loan in book.get_loans() if not loan.get("waiting_email_sent")]
    for loan in loans:
        # don't bother the person if the he has borrowed less than 2 days back
        ndays = 2
        if _get_loan_timestamp_in_days(loan) < ndays:
            continue

        # Only send email reminder for bookreader loans.
        # It seems it is hard to return epub/pdf loans, esp. with bluefire reader and overdrive
        if loan.get("resource_type") != "bookreader":
            return

        # Anand - Oct 2013
        # unfinished PDF/ePub loan?
        # Added temporarily to avoid crashing
        if not loan.get('expiry'):
            continue
        user = web.ctx.site.get(loan["user"])
        email = user and user.get_email()
        sendmail_with_template("email/waitinglist_people_waiting", to=email, 
            user=user, 
            book=book, 
            expiry_days=_get_expiry_in_days(loan))
        loan['waiting_email_sent'] = True
        web.ctx.site.store[loan['_key']] = loan

def _get_expiry_in_days(loan):
    if loan.get("expiry"):
        delta = h.parse_datetime(loan['expiry']) - datetime.datetime.utcnow()
        # +1 to count the partial day
        return delta.days + 1

def _get_loan_timestamp_in_days(loan):
    t = datetime.datetime.fromtimestamp(loan['loaned_at'])
    delta = datetime.datetime.utcnow() - t
    return delta.days

def prune_expired_waitingloans():
    """Removes all the waiting loans that are expired.

    A waiting loan expires if the person fails to borrow a book with in 
    24 hours after his waiting loan becomes "available".
    """
    records = web.ctx.site.store.values(type="waiting-loan", name="status", value="available", limit=-1)
    now = datetime.datetime.utcnow().isoformat()
    expired = [r for r in records if 'expiry' in r and r['expiry'] < now]
    for r in expired:
        logger.info("Deleting waiting loan for %r", r['book'])
        # should mark record as expired instead of deleting
        r['_delete'] = True
    web.ctx.site.store.update(dict((r['_key'], r) for r in expired))

    # Update the checkedout status and position in the WL for each entry
    for r in expired:
        update_waitinglist(r['book'])

def update_all_ebook_documents():
    """Updates the status of all ebook documents which are marked as checkedout.

    It is safe to call this function multiple times.
    """
    records = web.ctx.site.store.values(type="ebook", name="borrowed", value="true", limit=-1)
    for r in records:
        update_waitinglist(r['book_key'])
########NEW FILE########
__FILENAME__ = archive
"""Utility to move files from local disk to tar files and update the paths in the db.
"""
import sys
import tarfile
import web
import os
import time

from openlibrary.coverstore import config, db
from openlibrary.coverstore.coverlib import find_image_path

#logfile = open('log.txt', 'a')

def log(*args):
    msg = " ".join(args)
    print msg
    #print >> logfile, msg
    #logfile.flush()

class TarManager:
    def __init__(self):
        self.tarfiles = {}
        self.tarfiles[''] = (None, None, None)
        self.tarfiles['S'] = (None, None, None)
        self.tarfiles['M'] = (None, None, None)
        self.tarfiles['L'] = (None, None, None)

    def get_tarfile(self, name):
        id = web.numify(name)
        tarname = "covers_%s_%s.tar" % (id[:4], id[4:6])
                
        # for id-S.jpg, id-M.jpg, id-L.jpg
        if '-' in name: 
            size = name[len(id + '-'):][0].lower()
            tarname = size + "_" + tarname
        else:
            size = ""
        
        _tarname, _tarfile, _indexfile = self.tarfiles[size.upper()]
        if _tarname != tarname:
            _tarname and _tarfile.close()
            _tarfile, _indexfile = self.open_tarfile(tarname)
            self.tarfiles[size.upper()] = tarname, _tarfile, _indexfile
            log('writing', tarname)

        return _tarfile, _indexfile

    def open_tarfile(self, name):
        path = os.path.join(config.data_root, "items", name[:-len("_XX.tar")], name)
        dir = os.path.dirname(path)
        if not os.path.exists(dir):
            os.makedirs(dir)
            
        indexpath = path.replace('.tar', '.index')
            
        if os.path.exists(path):
            return tarfile.TarFile(path, 'a'), open(indexpath, 'a')
        else:
            return tarfile.TarFile(path, 'w'), open(indexpath, 'w')

    def add_file(self, name, fileobj, mtime):
        tarinfo = tarfile.TarInfo(name)
        tarinfo.mtime = mtime
        tarinfo.size = os.stat(fileobj.name).st_size
        
        tar, index = self.get_tarfile(name)
        # tar.offset is current size of tar file. 
        # Adding 512 bytes for header gives us the starting offset of next file.
        offset = tar.offset + 512
        
        tar.addfile(tarinfo, fileobj=fileobj)
        
        index.write('%s\t%s\t%s\n' % (name, offset, tarinfo.size))
        return "%s:%s:%s" % (os.path.basename(tar.name), offset, tarinfo.size)

    def close(self):
        for name, _tarfile, _indexfile in self.tarfiles.values():
            if name:
                _tarfile.close()
                _indexfile.close()
idx = id
def archive():
    """Move files from local disk to tar files and update the paths in the db."""
    tar_manager = TarManager()
    
    _db = db.getdb()

    try:
        covers = _db.select('cover', where='archived=$f', order='id', vars={'f': False})
        for cover in covers:
            id = "%010d" % cover.id

            print 'archiving', cover
            
            files = {
                'filename': web.storage(name=id + '.jpg', filename=cover.filename),
                'filename_s': web.storage(name=id + '-S.jpg', filename=cover.filename_s),
                'filename_m': web.storage(name=id + '-M.jpg', filename=cover.filename_m),
                'filename_l': web.storage(name=id + '-L.jpg', filename=cover.filename_l),
            }
            
            for d in files.values():
                d.path = d.filename and os.path.join(config.data_root, "localdisk", d.filename)
                    
            if any(d.path is None or not os.path.exists(d.path) for d in files.values()):
                print >> web.debug, "Missing image file for %010d" % cover.id
                continue
            
            if isinstance(cover.created, basestring):
                from infogami.infobase import utils
                cover.created = utils.parse_datetime(cover.created)    
            
            timestamp = time.mktime(cover.created.timetuple())
                
            for d in files.values():
                d.newname = tar_manager.add_file(d.name, open(d.path), timestamp)
                
            _db.update('cover', where="id=$cover.id",
                archived=True, 
                filename=files['filename'].newname,
                filename_s=files['filename_s'].newname,
                filename_m=files['filename_m'].newname,
                filename_l=files['filename_l'].newname,
                vars=locals()
            )
        
            for d in files.values():
                print 'removing', d.path            
                os.remove(d.path)
    finally:
        #logfile.close()
        tar_manager.close()

########NEW FILE########
__FILENAME__ = code
import web
import simplejson
import urllib
import os
import Image
import datetime
import time
import couchdb
import logging
import array
import memcache

import db
import config
from utils import safeint, rm_f, random_string, ol_things, ol_get, changequery, download

from coverlib import save_image, read_image, read_file

import ratelimit

logger = logging.getLogger("coverstore")

urls = (
    '/', 'index',
    '/([^ /]*)/upload', 'upload',
    '/([^ /]*)/upload2', 'upload2',    
    '/([^ /]*)/([a-zA-Z]*)/(.*)-([SML]).jpg', 'cover',
    '/([^ /]*)/([a-zA-Z]*)/(.*)().jpg', 'cover',
    '/([^ /]*)/([a-zA-Z]*)/(.*).json', 'cover_details',
    '/([^ /]*)/query', 'query',
    '/([^ /]*)/touch', 'touch',
    '/([^ /]*)/delete', 'delete',
)
app = web.application(urls, locals())

def get_cover_id(olkeys):
    """Return the first cover from the list of ol keys."""
    for olkey in olkeys:
        doc = ol_get(olkey)
        if not doc:
            continue
        
        if doc['key'].startswith("/authors"):
            covers = doc.get('photos', [])
        else:
            covers = doc.get('covers', [])
            
        # Sometimes covers is stored as [-1] to indicate no covers. Consider it as no covers.
        if covers and covers[0] >= 0:
            return covers[0]
            
_couchdb = None
def get_couch_database():
    global _couchdb
    if config.get("couchdb_database"):
        _couchdb = couchdb.Database(config.couchdb_database)
    return _couchdb
    
def find_coverid_from_couch(db, key, value):
    rows = db.view("covers/by_id", key=[key, value], limit=10, stale="ok")
    rows = list(rows)
    
    if rows:
        row = max(rows, key=lambda row: row.value['last_modified'])
        return row.value['cover']

def _query(category, key, value):
    if key == 'olid':
        prefixes = dict(a="/authors/", b="/books/", w="/works/")
        if category in prefixes:
            olkey = prefixes[category] + value
            return get_cover_id([olkey])
    else:
        if category == 'b':
            db = get_couch_database()
            if db:
                return find_coverid_from_couch(db, key, value)
            
            if key == 'isbn':
                value = value.replace("-", "").strip()
                key = "isbn_"
            if key == 'oclc':
                key = 'oclc_numbers'
            olkeys = ol_things(key, value)
            return get_cover_id(olkeys)
    return None

ERROR_EMPTY = 1, "No image found"
ERROR_INVALID_URL = 2, "Invalid URL"
ERROR_BAD_IMAGE = 3, "Invalid Image"

class index:
    def GET(self):
        return '<h1>Open Library Book Covers Repository</h1><div>See <a href="https://openlibrary.org/dev/docs/api/covers">Open Library Covers API</a> for details.</div>'

def _cleanup():
    web.ctx.pop("_fieldstorage", None)
    web.ctx.pop("_data", None)
    web.ctx.env = {}
        
class upload:
    def POST(self, category):
        i = web.input('olid', author=None, file={}, source_url=None, success_url=None, failure_url=None)

        success_url = i.success_url or web.ctx.get('HTTP_REFERRER') or '/'
        failure_url = i.failure_url or web.ctx.get('HTTP_REFERRER') or '/'
        
        def error((code, msg)):
            print >> web.debug, "ERROR: upload failed, ", i.olid, code, repr(msg)
            _cleanup()
            url = changequery(failure_url, errcode=code, errmsg=msg)
            raise web.seeother(url)
        
        if i.source_url:
            try:
                data = download(i.source_url)
            except:
                error(ERROR_INVALID_URL)
            source_url = i.source_url
        elif i.file is not None and i.file != {}:
            data = i.file.value
            source_url = None
        else:
            error(ERROR_EMPTY)

        if not data:
            error(ERROR_EMPTY)

        try:
            save_image(data, category=category, olid=i.olid, author=i.author, source_url=i.source_url, ip=web.ctx.ip)
        except ValueError:
            error(ERROR_BAD_IMAGE)

        _cleanup()
        raise web.seeother(success_url)
        
class upload2:
    """Temporary upload handler for handling upstream.openlibrary.org cover upload.
    """
    def POST(self, category):
        i = web.input(olid=None, author=None, data=None, source_url=None, ip=None, _unicode=False)

        web.ctx.pop("_fieldstorage", None)
        web.ctx.pop("_data", None)
        
        def error((code, msg)):
            _cleanup()            
            e = web.badrequest()
            e.data = simplejson.dumps({"code": code, "message": msg})
            raise e
            
        source_url = i.source_url
        data = i.data
            
        if source_url:
            try:
                data = download(source_url)
            except:
                error(ERROR_INVALID_URL)
            
        if not data:
            error(ERROR_EMPTY)

        try:
            d = save_image(data, category=category, olid=i.olid, author=i.author, source_url=i.source_url, ip=i.ip)
        except ValueError:
            error(ERROR_BAD_IMAGE)
        
        _cleanup()
        return simplejson.dumps({"ok": "true", "id": d.id})

def trim_microsecond(date):
    # ignore microseconds
    return datetime.datetime(*date.timetuple()[:6])


@web.memoize
def get_memcache():
    servers = config.get("memcache_servers")
    return memcache.Client(servers)

def _locate_item(item):
    """Locates the archive.org item in the cluster and returns the server and directory.
    """
    print >> web.debug, time.asctime(), "_locate_item", item
    text = urllib.urlopen("https://archive.org/metadata/" + item).read()
    d = simplejson.loads(text)
    return d['server'], d['dir']

def locate_item(item):
    mc = get_memcache()
    if not mc:
        return _locate_item(item)
    else:
        x = mc.get(item)
        if not x:
            x = _locate_item(item)
            print >> web.debug, time.asctime(), "mc.set", item, x
            mc.set(item, x, time=600) # cache it for 10 minutes
        return x

def zipview_url(item, zipfile, filename):
    server, dir = locate_item(item)

    # http or https
    protocol = web.ctx.protocol
    return "%(protocol)s://%(server)s/zipview.php?zip=%(dir)s/%(zipfile)s&file=%(filename)s" % locals()
    
# Number of images stored in one archive.org item
IMAGES_PER_ITEM = 10000

def zipview_url_from_id(coverid, size):
    suffix = size and ("-" + size.upper())
    item_index = coverid/IMAGES_PER_ITEM
    itemid = "olcovers%d" % item_index
    zipfile = itemid + suffix + ".zip"
    filename = "%d%s.jpg" % (coverid, suffix)
    return zipview_url(itemid, zipfile, filename)

class cover:
    def GET(self, category, key, value, size):
        i = web.input(default="true")
        key = key.lower()

        def is_valid_url(url):
            return url.startswith("http://") or url.startswith("https://")
        
        def notfound():
            if key in ["id", "olid"] and config.get("upstream_base_url"):
                # this is only used in development
                base = web.rstrips(config.upstream_base_url, "/")
                raise web.redirect(base + web.ctx.fullpath)
            elif config.default_image and i.default.lower() != "false" and not is_valid_url(i.default):
                return read_file(config.default_image)
            elif is_valid_url(i.default):
                raise web.seeother(i.default)
            else:
                raise web.notfound("")
                
        def redirect(id):
            size_part = size and ("-" + size) or ""
            url = "/%s/id/%s%s.jpg" % (category, id, size_part)
            
            query = web.ctx.env.get('QUERY_STRING')
            if query:
                url += '?' + query
            raise web.found(url)
        
        if key == 'isbn':
            value = value.replace("-", "").strip() # strip hyphens from ISBN
            # Disabling ratelimit as iptables is taking care of botnets.
            #value = self.ratelimit_query(category, key, value)
            value = self.query(category, key, value)
            
            # Redirect isbn requests to archive.org. 
            # This will heavily reduce the load on coverstore server.
            # The max_coveritem_index config parameter specifies the latest 
            # olcovers items uploaded to archive.org.
            if value and self.is_cover_in_cluster(value):
                url = zipview_url_from_id(int(value), size)
                raise web.found(url)
        elif key == 'ia':
            url = self.get_ia_cover_url(value, size)
            if url:
                raise web.found(url)
            else:
                value = None # notfound or redirect to default. handled later.
        elif key != 'id':
            value = self.query(category, key, value)

        if value and safeint(value) in config.blocked_covers:
            raise web.notfound()
            
        # redirect to archive.org cluster for large size and original images whenever possible
        if value and (size == "L" or size == "") and self.is_cover_in_cluster(value):
            url = zipview_url_from_id(int(value), size)
            raise web.found(url)            
        
        d = value and self.get_details(value, size.lower())
        if not d:
            return notfound()
            
        # set cache-for-ever headers only when requested with ID
        if key == 'id':
            etag = "%s-%s" % (d.id, size.lower())
            if not web.modified(trim_microsecond(d.created), etag=etag):
                raise web.notmodified()

            web.header('Cache-Control', 'public')
            web.expires(100 * 365 * 24 * 3600) # this image is not going to expire in next 100 years.
        else:
            web.header('Cache-Control', 'public')
            web.expires(10*60) # Allow the client to cache the image for 10 mins to avoid further requests
        
        web.header('Content-Type', 'image/jpeg')
        try:
            return read_image(d, size)
        except IOError:
            raise web.notfound()

    def get_ia_cover_url(self, identifier, size="M"):
        url = "https://archive.org/metadata/%s/metadata" % identifier
        try:
            jsontext = urllib.urlopen(url).read()
            d = simplejson.loads(jsontext).get("result", {})
        except (IOError, ValueError):
            return

        # Not a text item or no images or scan is not complete yet
        if d.get("mediatype") != "texts" or d.get("repub_state", "4") not in ["4", "6"] or "imagecount" not in d:
            return

        w, h = config.image_sizes[size.upper()]
        return "https://archive.org/download/%s/page/cover_w%d_h%d.jpg" % (identifier, w, h)

    def get_details(self, coverid, size=""):
        try:
            coverid = int(coverid)
        except ValueError:
            return None
            
        # Use tar index if available to avoid db query. We have 0-6M images in tar balls. 
        if isinstance(coverid, int) and coverid < 6000000 and size in "sml":
            path = self.get_tar_filename(coverid, size)
            
            if path:
                if size:
                    key = "filename_%s" % size
                else:
                    key = "filename"
                return web.storage({"id": coverid, key: path, "created": datetime.datetime(2010, 1, 1)})
            
        return db.details(coverid)
        
    def is_cover_in_cluster(self, coverid):
        """Returns True if the cover is moved to archive.org cluster.
        It is found by looking at the config variable max_coveritem_index.
        """
        try:
            return int(coverid) < IMAGES_PER_ITEM * config.get("max_coveritem_index", 0)
        except (TypeError, ValueError):
            return False
        
    def get_tar_filename(self, coverid, size):
        """Returns tarfile:offset:size for given coverid.
        """
        tarindex = coverid / 10000
        index = coverid % 10000
        array_offset, array_size = get_tar_index(tarindex, size)
        
        offset = array_offset and array_offset[index]
        imgsize = array_size and array_size[index]
        
        if size:
            prefix = "%s_covers" % size
        else:
            prefix = "covers"
        
        if imgsize:
            name = "%010d" % coverid
            return "%s_%s_%s.tar:%s:%s" % (prefix, name[:4], name[4:6], offset, imgsize)
        
    def query(self, category, key, value):
        return _query(category, key, value)
        
    ratelimit_query = ratelimit.ratelimit()(query)

@web.memoize
def get_tar_index(tarindex, size):
    path = os.path.join(config.data_root, get_tarindex_path(tarindex, size))    
    if not os.path.exists(path):
        return None, None
    
    return parse_tarindex(open(path))

def get_tarindex_path(index, size):
    name = "%06d" % index
    if size:
        prefix = "%s_covers" % size
    else:
        prefix = "covers"
    
    itemname = "%s_%s" % (prefix, name[:4])
    filename = "%s_%s_%s.index" % (prefix, name[:4], name[4:6])
    return os.path.join('items', itemname, filename)

def parse_tarindex(file):
    """Takes tarindex file as file objects and returns array of offsets and array of sizes. The size of the returned arrays will be 10000.
    """
    array_offset = array.array('L', [0 for i in range(10000)])
    array_size = array.array('L', [0 for i in range(10000)])
    
    for line in file:
        line = line.strip()
        if line:
            name, offset, imgsize = line.split("\t")
            coverid = int(name[:10]) # First 10 chars is coverid, followed by ".jpg"
            index = coverid % 10000
            array_offset[index] = int(offset)
            array_size[index] = int(imgsize)
    return array_offset, array_size
    
class cover_details:
    def GET(self, category, key, value):
        d = _query(category, key, value)
        
        if key == 'id':
            web.header('Content-Type', 'application/json')
            d = db.details(value)
            if d:
                if isinstance(d['created'], datetime.datetime):
                    d['created'] = d['created'].isoformat()
                    d['last_modified'] = d['last_modified'].isoformat()
                return simplejson.dumps(d)
            else:
                raise web.notfound("")
        else:
            value = _query(category, key, value)
            if value is None:
                return web.notfound("")
            else:
                return web.found("/%s/id/%s.json" % (category, value))
            
class query:
    def GET(self, category):
        i = web.input(olid=None, offset=0, limit=10, callback=None, details="false", cmd=None)
        offset = safeint(i.offset, 0)
        limit = safeint(i.limit, 10)
        details = i.details.lower() == "true"
        
        if limit > 100:
            limit = 100
            
        if i.olid and ',' in i.olid:
            i.olid = i.olid.split(',')
        result = db.query(category, i.olid, offset=offset, limit=limit)
        
        if i.cmd == "ids":
            result = dict((r.olid, r.id) for r in result)
        elif not details:
            result = [r.id for r in result]
        else:
            def process(r):
                return {
                    'id': r.id,
                    'olid': r.olid,
                    'created': r.created.isoformat(),
                    'last_modified': r.last_modified.isoformat(),
                    'source_url': r.source_url,
                    'width': r.width,
                    'height': r.height
                }
            result = [process(r) for r in result]
                        
        json = simplejson.dumps(result)
        web.header('Content-Type', 'text/javascript')
        if i.callback:
            return "%s(%s);" % (i.callback, json)
        else:
           return json
           
class touch:
    def POST(self, category):
        i = web.input(id=None, redirect_url=None)
        redirect_url = i.redirect_url or web.ctx.get('HTTP_REFERRER')

        id = i.id and safeint(i.id, None)
        if id:
            db.touch(id)
            raise web.seeother(redirect_url)
        else:
            return 'no such id: %s' % id

class delete:
    def POST(self, category):
        i = web.input(id=None, redirect_url=None)
        redirect_url = i.redirect_url

        id = i.id and safeint(i.id, None)
        if id:
            db.delete(id)
            if redirect_url:
                raise web.seeother(redirect_url)
            else:
                return 'cover has been deleted successfully.' 
        else:
            return 'no such id: %s' % id

########NEW FILE########
__FILENAME__ = config

image_engine = "pil"
image_sizes = dict(S=(116, 58), M=(180, 360), L=(500, 500))

default_image = None
data_root = None

ol_url = "http://openlibrary.org/"

# ids of the blocked covers
# this is used to block covers when someone requests
# an image to be blocked.
blocked_covers = []

def get(name, default=None):
    return globals().get(name, default)
########NEW FILE########
__FILENAME__ = coverlib
"""Cover management."""

import Image
import os
from cStringIO import StringIO
import web
import datetime

import config
import db
from utils import random_string, rm_f

__all__ = [
    "save_image",
    "read_image",
    "read_file"
]

def save_image(data, category, olid, author=None, ip=None, source_url=None):
    """Save the provided image data, creates thumbnails and adds an entry in the database.
    
    ValueError is raised if the provided data is not a valid image.
    """
    prefix = make_path_prefix(olid)
    
    img = write_image(data, prefix)
    if img is None:
        raise ValueError("Bad Image")

    d = web.storage({
        'category': category,
        'olid': olid,
        'author': author,
        'source_url': source_url,
    })
    d['width'], d['height'] = img.size

    filename = prefix + '.jpg'
    d['ip'] = ip
    d['filename'] = filename
    d['filename_s'] = prefix + '-S.jpg'
    d['filename_m'] = prefix + '-M.jpg'
    d['filename_l'] = prefix + '-L.jpg'
    d.id = db.new(**d)
    return d
    
def make_path_prefix(olid, date=None):
    """Makes a file prefix for storing an image.
    """
    date = date or datetime.date.today()
    return "%04d/%02d/%02d/%s-%s" % (date.year, date.month, date.day, olid, random_string(5))
    
def write_image(data, prefix):
    path_prefix = find_image_path(prefix)
    dirname = os.path.dirname(path_prefix)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    try:
        # save original image
        f = open(path_prefix + '.jpg', 'w')
        f.write(data)
        f.close()

        img = Image.open(StringIO(data))
        if img.mode != 'RGB':
            img = img.convert('RGB')

        for name, size in config.image_sizes.items():
            path = "%s-%s.jpg" % (path_prefix, name)
            resize_image(img, size).save(path, quality=90)
        return img
    except IOError, e:
        print 'ERROR:', str(e)

        # cleanup
        rm_f(prefix + '.jpg')
        rm_f(prefix + '-S.jpg')
        rm_f(prefix + '-M.jpg')
        rm_f(prefix + '-L.jpg')

        return None

def resize_image(image, size):
    """Resizes image to specified size while making sure that aspect ratio is maintained."""
    # from PIL
    x, y = image.size
    if x > size[0]: y = max(y * size[0] / x, 1); x = size[0]
    if y > size[1]: x = max(x * size[1] / y, 1); y = size[1]
    size = x, y

    return image.resize(size, Image.ANTIALIAS)

def find_image_path(filename):
    if ':' in filename: 
        return os.path.join(config.data_root,'items', filename.rsplit('_', 1)[0], filename)
    else:
        return os.path.join(config.data_root, 'localdisk', filename)

def read_file(path):
    if ':' in path:
        path, offset, size = path.rsplit(':', 2)
        offset = int(offset)
        size = int(size)
        f = open(path)
        f.seek(offset)
        data = f.read(size)
        f.close()
    else:
        f = open(path)
        data = f.read()
        f.close()
    return data

def read_image(d, size):
    if size:
        filename = d['filename_' + size.lower()] or d.filename + "-%s.jpg" % size.upper()
    else:
        filename = d.filename
    path = find_image_path(filename)
    return read_file(path)

########NEW FILE########
__FILENAME__ = db
import web
import config
import datetime

_categories = None
_db = None

def getdb():
    global _db
    if _db is None:
        _db = web.database(**config.db_parameters)
    return _db

def get_category_id(category):
    global _categories
    if _categories is None:
        _categories = {}
        for c in getdb().select('category'):
            _categories[c.name] = c.id
    return _categories.get(category)
    
def new(category, olid, filename, filename_s, filename_m, filename_l,
    author, ip, source_url, width, height):
    category_id = get_category_id(category)
    now=datetime.datetime.utcnow()
    
    db = getdb()
    
    t = db.transaction()
    try:
        cover_id = db.insert('cover', category_id=category_id, 
            filename=filename, filename_s=filename_s, filename_m=filename_m, filename_l=filename_l,
            olid=olid, author=author, ip=ip,
            source_url=source_url, width=width, height=height, 
            created=now, last_modified=now, deleted=False, archived=False)
    
        db.insert("log", action="new", timestamp=now, cover_id=cover_id)
    except:
        t.rollback()
        raise
    else:
        t.commit()
    return cover_id
        
def query(category, olid, offset=0, limit=10):
    category_id = get_category_id(category)
    deleted = False
    
    if isinstance(olid, list):
        if len(olid) == 0:
            olid = [-1]
        where = web.reparam('deleted=$deleted AND category_id = $category_id AND olid IN $olid', locals())
    elif olid is None:
        where = web.reparam('deleted=$deleted AND category_id=$category_id', locals())
    else:
        where = web.reparam('deleted=$deleted AND category_id=$category_id AND olid=$olid', locals())
    
    result = getdb().select('cover',
        what='*',
        where= where,
        order='last_modified desc', 
        offset=offset,
        limit=limit)
    return result.list()

def details(id):
    try:
        return getdb().select('cover', what='*', where="id=$id", vars=locals())[0]
    except IndexError:
        return None
    
def touch(id):
    """Sets the last_modified of the specified cover to the current timestamp.
    By doing so, this cover become comes in the top in query because the results are ordered by last_modified.
    """
    now = datetime.datetime.utcnow()
    db = getdb()
    t = db.transaction()
    try:
        db.query("UPDATE cover SET last_modified=$now where id=$id", vars=locals())
        db.insert("log", action="touch", timestamp=now, cover_id=id)
    except:
        t.rollback()
        raise
    else:
        t.commit()

def delete(id):
    true = True
    now = datetime.datetime.utcnow()

    db = getdb()
    t = db.transaction()
    try:    
        db.query('UPDATE cover set deleted=$true AND last_modified=$now WHERE id=$id', vars=locals())
        db.insert("log", action="delete", timestamp=now, cover_id=id)
    except:
        t.rollback()
        raise
    else:
        t.commit()    

def get_filename(id):
    d = getdb().select('cover', what='filename', where='id=$id',vars=locals())
    return d and d[0].filename or None

########NEW FILE########
__FILENAME__ = disk
import random
import os
import string
import warc

chars = string.letters + string.digits
def random_string(n):
    return "".join([random.choice(chars) for i in range(n)])

class Disk:
    """Disk interface to store files.

    >>> import os, string
    >>> _ = os.system("rm -rf test_disk")
    >>> disk = Disk("test_disk")
    >>> f1 = disk.write("hello, world!")
    >>> f2 = disk.write(string.letters)
    >>> f3 = disk.write(string.letters)
    >>> disk.read(f1)
    'hello, world!'
    >>> disk.read(f2)
    'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
    >>> disk.read(f3)
    'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
    """
    def __init__(self, root):
        self.root = root
        if not os.path.exists(root):
            os.makedirs(root)

    def write(self, data, params={}):
        prefix = params.get('olid', '')
        filename = self.make_filename(prefix)
        path = os.path.join(self.root, filename)
        f = open(path, 'w')
        f.write(data)
        f.close()
        return filename

    def read(self, filename):
        path = os.path.join(self.root, filename)
        if os.path.exists(path):
            return open(path).read()
        
    def make_filename(self, prefix=""):
        def exists(filename):
            return os.path.exists(os.path.join(self.root, filename))
        filename = prefix + "_" + random_string(4)
        while exists(filename):
            filename = prefix + "_"  + random_string(4)
        return filename

class WARCDisk:
    def __init__(self, root, prefix="file", maxsize=500 * 1024 * 1024):
        """Creates a disk to write read and write resources in warc files.

        WARNING: This class is not thread-safe. Multiple threads trying to write at the same time may result in race-conditions.

        >>> import os, string
        >>> _ = os.system("rm -rf test_disk")
        >>> disk = WARCDisk("test_disk", maxsize=200)
        >>> f1 = disk.write("hello, world!")
        >>> f2 = disk.write(string.letters)
        >>> f3 = disk.write(string.letters)
        >>> disk.read(f1)
        'hello, world!'
        >>> disk.read(f2)
        'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
        >>> disk.read(f3)
        'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
        """
        # this is required for warc.WARCRecord. Testing to fail early.
        import uuid

        self.root = root
        if not os.path.exists(root):
            os.makedirs(root)

        self.next_warcfile = None
        self.maxsize = maxsize
        self.warcfile_prefix = prefix

    def get_item_name(self, warcfilename):
        # warc file file_xxxx_yy.warc is stored in item file_xxxx.
        itemname, _ = warcfilename.rsplit('_', 1)
        return itemname

    def read(self, filename):
        if filename.count(':') != 2:
            return None
        warcfilename, offset, size = filename.split(':')
        offset = int(offset)
        size = int(size)
        path = self.get_path(warcfilename)
        if os.path.exists(path):
            f = open(path)
            f.seek(offset)
            return f.read(size)

    def get_path(self, warcfilename, create_dirs=False):
        dir = os.path.join(self.root, self.get_item_name(warcfilename))
        if create_dirs and not os.path.exists(dir):
            os.mkdir(dir)
        return os.path.join(dir, warcfilename)

    def write(self, data, headers={}):
        warcfilename = self.get_next_warcfile()
        path = self.get_path(warcfilename, create_dirs=True)
        w = warc.WARCWriter(open(path, 'a'))

        headers = dict(headers)
        subject_uri = headers.pop('subject_uri', 'xxx')
        mimetype = headers.pop('mimetype', 'application/octet-stream')
        
        warc_record = warc.WARCRecord('resource', subject_uri, mimetype, headers, data)
        offset = w.write(warc_record)
        w.close()
        filename = '%s:%d:%d' % (warcfilename, offset, len(data))
        return filename

    def find(self, dir):
        """Find all files in the given directory."""
        for dirpath, dirnames, filenames in os.walk(dir):
            for f in filenames:
                yield os.path.join(dirpath, f)

    def get_next_warcfile(self):
        """Find the next warc file.
        
        For a new disk, next_warcfile should be file_0000_00.warc.
        
            >>> import os, string
            >>> _ = os.system("rm -rf test_disk")
            >>> disk = WARCDisk("test_disk", maxsize=100)
            >>> disk.get_next_warcfile()
            'file_0000_00.warc'
            
        After writing enough data, it should move to next file.
        
            >>> _ = disk.write('x' * 100)
            >>> disk.get_next_warcfile()
            'file_0000_01.warc'
            
        A a new disk with existing data should be able to find the correct value of next_warcfile.
        
            >>> disk = WARCDisk("test_disk", maxsize=100)
            >>> disk.get_next_warcfile()
            'file_0000_01.warc'
        """
        from web.utils import numify, denumify
    
        #@@ this could be dangerous. If we clear the directory, it starts again from count 0. 
        #@@ Probably, this should be taken from database.
        if self.next_warcfile is None:
            files = [os.path.basename(f) for f in self.find(self.root) if f.endswith('.warc')]
            if files:
                files.sort()
                self.next_warcfile = files[-1]
            else:
                self.next_warcfile = self.warcfile_prefix + '_0000_00.warc'

        path = self.get_path(self.next_warcfile)
        if os.path.exists(path) and self.filesize(path) >= self.maxsize:
            count = int(numify(self.next_warcfile)) + 1
            self.next_warcfile = self.warcfile_prefix + denumify("%06d" % count, "_XXXX_XX.warc")

        return self.next_warcfile

    def filesize(self, filename):
        return os.stat(filename).st_size

class ArchiveDisk(WARCDisk):
    """Disk interface to internet archive storage.
    
    There is a convention that is used to name files and items. 
    prefix_xxxx_yy.ext is saved in item named prefix_xxxx.
    """
    def make_warcfile(self, warcfilename):
        path = self.get_path(warcfilename)
        if os.path.exists(path):
            return open(path)
        else:
            itemname = self.get_item_name(warcfilename)
            url = self.item_url(itemname) + '/' + warcfilename
            return warc.HTTPFile(url)
    
    def read(self, filename):
        # if the file is locally available then read it from there.
        # else contact the server
        try:
            data = WARCDisk.read(self, filename)
        except IOError:
            data = None

        if data:
            return data

        warcfilename, offset, size = filename.split(':')
        offset = int(offset)
        size = int(size)
        f = self.make_warcfile(warcfilename)
        f.seek(offset)
        return f.read(size)

    def create_file(filename):
        itemname = self.get_item_name(filename)
        url = self.item_url(itemname) + '/' + filename
        return warc.HTTPFile(url)

    def get_item_name(self, warcfilename):
        # warc file file_xxxx_yy.warc is stored in item file_xxxx.
        itemname, _ = warcfilename.rsplit('_', 1)
        return itemname

    def item_url(self, itemname):
        """Returns http url to access files from the item specified by the itemname."""
        from xml.dom import minidom
        import urllib
        base_url = 'http://www.archive.org/services/find_file.php?loconly=1&file='
        try:
            data= urllib.urlopen(base_url + itemname).read()
            doc = minidom.parseString(data)
            vals = ['http://' + e.getAttribute('host') + e.getAttribute('dir') for e in doc.getElementsByTagName('location')]
            return vals and vals[0]
        except Exception:
            return None
  
class LayeredDisk:
    """Disk interface over multiple disks.
    Write always happens to the first disk and 
    read happens on the first disk where the file is available.
    """
    def __init__(self, disks):
        self.disks = disks
        
    def read(self, filename):
        for disk in self.disks:
            data = disk.read(filename)
            if data:
                return data
        
    def write(self, data, headers={}):
        return self.disks[0].write(data, headers)

if __name__ == "__main__":
    import doctest
    doctest.testmod()
    

########NEW FILE########
__FILENAME__ = imagecache
"""
LRU cache implementation on disk for storing images.
"""

import os
import time
import config
import web

import db

NBLOCKS = 10000
BLOCK_SIZE = 100
OVERLOAD_FACTOR = 1.25

class ImageCache:
    """LRU cache implementation on disk for storing images.
    """
    def __init__(self, root, disk):
        self.root = root
        self.disk = disk
        self.engine = self._create_image_engine()
        # access times
        self.atimes = _BlockDict(NBLOCKS)
        self._setup()
        
    def _setup(self):
        # create all directories
        for i in range(NBLOCKS):
            d = self._dirname(i)
            if not os.path.exists(d):
                os.makedirs(d)
                            
    def _create_image_engine(self):
        engines = dict(pil=PIL, imagemagick=ImageMagick)
        if config.image_engine not in engines:
            raise Exception, 'Unknown image engine: ' + config.image_engine
        return engines[config.image_engine]()
                        
    def get_image(self, id, size):
        """Returns image with the specified id in the specified size."""
        if self.populate_cache(id):
            self.atimes[id] = time.time()
            return self._imgpath(id, size)
        
    def populate_cache(self, id):
        """Populates cache with images of the given id."""
        def write(path, data):
            #print >> web.debug, 'write', path
            f = open(path, 'w')
            f.write(data)
            f.close()
            
        if id not in self.atimes:
            # Lazily add entry to atimes if the file exists
            # Doing this lazily saves lot of startup time.
            path = self._imgpath(id, 'S')
            if os.path.exists(path):
                self.atimes[id] = time.time()
                return True

            # get the original image
            filename = db.get_filename(id)
            if filename is None:
                return False
            
            # prune the folder if it has more files    
            block_number = id % NBLOCKS
            self._prune(block_number)
            
            write(self._imgpath(id, 'original'), self.disk.read(filename))
            
            # create thumbnails
            for size in config.image_sizes:
                self.engine.thumbnail(self._imgpath(id, 'original'), self._imgpath(id, size), config.image_sizes[size])    
            
            # remove original
            os.remove(self._imgpath(id, 'original'))
            self.atimes[id] = time.time()
            
        return True

    def _imgpath(self, id, size):
        return os.path.join(self._dirname(id), '%d-%s.jpg' % (id, size))

    def _delete_images(self, id):
        for size in config.image_sizes:
            path = self._imgpath(id, size)
            try:
                os.remove(path)
            except:
                pass
        del self.atimes[id]    
        
    def _prune(self, block_number):
        block = self.atimes.get_block(block_number)
        threshold = int(OVERLOAD_FACTOR * BLOCK_SIZE)
        if len(block) > threshold:
            # other process could be updating the imagecache. 
            # update the atimes to remove the real unused files.
            self.atimes[block_number] = self.get_atimes(block_number)

            ids = sorted(block.keys(), key=lambda id: block[id], reverse=True)
            for id in ids[BLOCK_SIZE:]:
                self._delete_images(id)
                
    def get_atimes(block_number):
        atimes = {}
        d = self._dirname(block_number) # _dirname works even with block_number
        for f in os.listdir(d):
            id = int(web.numify(f))
            atime = os.stat(os.path.join(d, f)).st_atime
            atimes[id] = max(self.atimes.get(id, 0), atime)
        return atimes
        
    def _dirname(self, id):
        block = "%04d" % (id % NBLOCKS)
        a, b = block[:2], block[2:]
        return os.path.join(self.root, a, b)

class _BlockDict:
    """A dictionary with integer keys to use with ImageCache.
    
    Values can be accessed just like normal dictionary and also blockwise.
    Keys are grouped into blocks. Key i belongs to block number i % nblocks.
    """
    def __init__(self, nblocks):
        self.nblocks = nblocks
        self.data = [{} for i in range(nblocks)]
        
    def __getitem__(self, i):
        block = i % self.nblocks
        return self.data[block][i]
        
    def __setitem__(self, i, value):
        block = i % self.nblocks
        self.data[block][i] = value
        
    def __delitem__(self, i):
        block = i % self.nblocks
        del self.data[block][i]
        
    def __contains__(self, i):
        block = i % self.nblocks
        return i in self.data[block]
        
    def get(self, i, default=None):
        block = i % self.nblocks
        return self.data[block].get(i, default)
        
    def get_block(self, block_number):
        """Returns the dictionary for the specified block."""
        return self.data[block_number]
    
class PIL:
    """Image engine using PythonImagingLibrary."""

    def thumbnail(self, src_file, dest_file, size):
        """Converts src image to thumnail of specified size."""
        #print >> web.debug, 'thumbnail', src_file, dest_file
        import Image
        image = Image.open(src_file)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image.thumbnail(size, resample=Image.ANTIALIAS)
        image.save(dest_file)
        
    def mimetype(self, filename):
        import Image
        image = Image.open(src_file)
        types = dict(JPEG='image/jpeg', GIF='image/gif')
        return types.get(image.format, 'application/octet-stream')

class ImageMagick:
    """Image engine using ImageMagick commands."""
    def thumbnail(self, src_file, dest_file, size):
        size = '%sx%s' % size
        cmd = 'convert -size %s -thumbnail %s %s %s' % (size, size, src_file, dest_file)
        os.system(cmd)
        
    def mimetype(self, filename):
        # TODO
        return 'application/octet-stream'

########NEW FILE########
__FILENAME__ = oldb
"""Library to talk directly to OL database to avoid expensive API calls.
"""
import web
import simplejson

import config

from openlibrary.utils import olmemcache

__all__ = [
    "query", "get"
]

def is_supported():
    return bool(config.get("ol_db_parameters"))

_db = None
def get_db():
    global _db
    if _db is None and config.get("ol_db_parameters"):
        _db = web.database(**config.ol_db_parameters)
    return _db
    
_memcache = None
def get_memcache():
    global _memcache
    if _memcache is None and config.get("ol_memcache_servers"):
        _memcache = olmemcache.Client(config.ol_memcache_servers)
    return _memcache

@web.memoize
def get_property_id(name):
    db = get_db()
    try:
        type_id = db.query("SELECT * FROM thing WHERE key='/type/edition'")[0].id
        rows = db.query("SELECT * FROM property WHERE name=$name AND type=$type_id", vars=locals())
        return rows[0].id
    except IndexError:
        return None
    
def query(key, value):
    key_id = get_property_id(key)
    
    db = get_db()
    rows = db.query("SELECT thing.key" + 
        " FROM thing, edition_str" +
        " WHERE thing.id=edition_str.thing_id" + 
            " AND key_id=$key_id" +
            " AND value=$value" +
        " ORDER BY thing.last_modified LIMIT 10",
        vars=locals())
    return [row.key for row in rows]

def get(key):
    # try memcache
    memcache = get_memcache()
    if memcache:
        json = memcache.get(key)
        if json:
            return simplejson.loads(json)

    # try db
    db = get_db()
    try:
        thing = db.query("SELECT * FROM thing WHERE key=$key", vars=locals())[0]
        data = db.query("SELECT * FROM data WHERE data.thing_id=$thing.id AND data.revision=$thing.latest_revision", vars=locals())[0]
        return simplejson.loads(data.data)
    except IndexError:
        return None
########NEW FILE########
__FILENAME__ = ratelimit
"""Ratelimit implementation for coverstore.

Adopted from https://github.com/simonw/ratelimitcache
"""
import functools
import memcache
import datetime
import web

from openlibrary.coverstore import config

class ratelimit(object):
    "Instances of this class can be used as decorators"
    
    # This class is designed to be sub-classed
    minutes = 5 # The time period
    requests = 100 # Number of allowed requests in that time period
    
    prefix = 'rl-' # Prefix for memcache key
    
    def __init__(self, **options):
        for key, value in options.items():
            setattr(self, key, value)
            
        self._cache = None
            
    def get_cache(self):
        if self._cache is None:
            self._cache = memcache.Client(config.memcache_servers)
        return self._cache
            
    def __call__(self, fn):
        def wrapper(*args, **kwargs):
            return self.view_wrapper(fn, *args, **kwargs)
        functools.update_wrapper(wrapper, fn)
        return wrapper
    
    def view_wrapper(self, fn, *args, **kwargs):
        if not self.should_ratelimit():
            return fn(*args, **kwargs)
        
        counts = self.get_counters().values()
        
        # Have they failed?
        if sum(counts) >= self.requests:
            return self.disallowed()
        else:
            # Increment rate limiting counter
            self.cache_incr(self.current_key())        
        
        return fn(*args, **kwargs)
    
    def cache_get_many(self, keys):
        keys = [web.safestr(key) for key in keys]
        return self.get_cache().get_multi(keys)
    
    def cache_incr(self, key):
        key = web.safestr(key)
        # add first, to ensure the key exists
        self.get_cache().incr(key, 1) or self.get_cache().set(key, 1, time=self.expire_after())
    
    def should_ratelimit(self):
        # ratelimit only if memcache is enabled
        return bool(config.get("memcache_servers"))
    
    def get_counters(self):
        return self.cache_get_many(self.keys_to_check())
    
    def keys_to_check(self):
        now = datetime.datetime.now()
        return [
            '%s%s-%s' % (
                self.prefix,
                web.ctx.ip,
                (now - datetime.timedelta(minutes = minute)).strftime('%Y%m%d%H%M')
            ) for minute in range(self.minutes + 1)
        ]
    
    def current_key(self):
        return '%s%s-%s' % (
            self.prefix,
            web.ctx.ip,
            datetime.datetime.now().strftime('%Y%m%d%H%M')
        )
        
    def disallowed(self):
        "Over-ride this method if you want to log incidents"
        raise web.Forbidden('Rate limit exceeded')
    
    def expire_after(self):
        "Used for setting the memcached cache expiry"
        return (self.minutes + 1) * 60

########NEW FILE########
__FILENAME__ = schema
"""Coverstore schema."""

from openlibrary.utils import schema

def get_schema(engine='postgres'):
    s = schema.Schema()
    
    s.add_table('category',
        s.column('id', 'serial', primary_key=True),
        s.column('name', 'string')
    )
    
    s.add_table('cover',
        s.column('id', 'serial', primary_key=True),
        s.column('category_id', 'integer', references='category'),
        s.column('olid', 'string'),
        s.column('filename', 'string'),
        s.column('filename_s', 'string'),
        s.column('filename_m', 'string'),
        s.column('filename_l', 'string'),
        s.column('author', 'string'),
        s.column('ip', 'string'),
        s.column('source_url', 'string'),
        s.column('isbn', 'string'),
        s.column('width', 'integer'),
        s.column('height', 'integer'),
        s.column('archived', 'boolean'),
        s.column('deleted', 'boolean', default=False),
        s.column('created', 'timestamp', default=s.CURRENT_UTC_TIMESTAMP),
        s.column('last_modified', 'timestamp', default=s.CURRENT_UTC_TIMESTAMP),
    )
    
    s.add_index('cover', 'olid')
    s.add_index('cover', 'last_modified')
    s.add_index('cover', 'created')
    s.add_index('cover', 'deleted')
    s.add_index('cover', 'archived')

    s.add_table("log",
        s.column("id", "serial", primary_key=True),
        s.column("cover_id", "integer", references="cover"),
        s.column("action", "text"),
        s.column("timestamp", "timestamp")
    )
    s.add_index("log", "timestamp")

    sql = s.sql(engine)
    if engine == 'sqlite':
        # quick hack to fix bug in openlibrary.utils.schema
        sql = sql.replace('autoincrement primary key', 'primary key autoincrement')
    return sql
    
########NEW FILE########
__FILENAME__ = server
#! /usr/bin/env python
"""coverstore server.
"""

import sys
import yaml
import web

from openlibrary.coverstore import config, code, archive


def runfcgi(func, addr=('localhost', 8000)):
    """Runs a WSGI function as a FastCGI pre-fork server."""
    config = dict(web.config.get("fastcgi", {}))

    mode = config.pop("mode", None)
    if mode == "prefork":    
        import flup.server.fcgi_fork as flups
    else:
        import flup.server.fcgi as flups
        
    return flups.WSGIServer(func, multiplexed=True, bindAddress=addr, **config).run()

web.wsgi.runfcgi = runfcgi

def load_config(configfile):
    d = yaml.load(open(configfile))
    for k, v in d.items():
        setattr(config, k, v)

    if 'fastcgi' in d:
        web.config.fastcgi = d['fastcgi']

def main(configfile, *args):
    load_config(configfile)

    if '--archive' in args:
        archive.archive()
    else:
        sys.argv = [sys.argv[0]] + list(args)
        code.app.run()

if __name__ == "__main__":
    main(*sys.argv[1:])

########NEW FILE########
__FILENAME__ = test_code
from .. import code
from cStringIO import StringIO
import web
import datetime

def test_tarindex_path():
    assert code.get_tarindex_path(0, "") == "items/covers_0000/covers_0000_00.index"
    assert code.get_tarindex_path(0, "s") == "items/s_covers_0000/s_covers_0000_00.index"
    assert code.get_tarindex_path(0, "m") == "items/m_covers_0000/m_covers_0000_00.index"
    assert code.get_tarindex_path(0, "l") == "items/l_covers_0000/l_covers_0000_00.index"

    assert code.get_tarindex_path(99, "") == "items/covers_0000/covers_0000_99.index"
    assert code.get_tarindex_path(100, "") == "items/covers_0001/covers_0001_00.index"

    assert code.get_tarindex_path(1, "") == "items/covers_0000/covers_0000_01.index"
    assert code.get_tarindex_path(21, "") == "items/covers_0000/covers_0000_21.index"
    assert code.get_tarindex_path(321, "") == "items/covers_0003/covers_0003_21.index"
    assert code.get_tarindex_path(4321, "") == "items/covers_0043/covers_0043_21.index"

def test_parse_tarindex():
    f = StringIO("")
    
    offsets, sizes = code.parse_tarindex(f) 
    assert list(offsets) == [0 for i in range(10000)]
    assert list(sizes) == [0 for i in range(10000)]
    
    f = StringIO("0000010000.jpg\t0\t10\n0000010002.jpg\t512\t20\n")

    offsets, sizes = code.parse_tarindex(f) 
    assert (offsets[0], sizes[0]) == (0, 10)
    assert (offsets[1], sizes[1]) == (0, 0)
    assert (offsets[2], sizes[2]) == (512, 20)
    assert (offsets[42], sizes[42]) == (0, 0)
    
class Test_cover:
    def test_get_tar_filename(self, monkeypatch):
        offsets = {}
        sizes = {}
        def _get_tar_index(index, size):
            array_offsets = [offsets.get(i, 0) for i in range(10000)]
            array_sizes = [sizes.get(i, 0) for i in range(10000)]
            return array_offsets, array_sizes
            
        monkeypatch.setattr(code, "get_tar_index", _get_tar_index)
        f = code.cover().get_tar_filename
        
        
        assert f(42, "s") == None
        
        offsets[42] = 1234
        sizes[42] = 567
        
        assert f(42, "s") == "s_covers_0000_00.tar:1234:567"
        assert f(30042, "s") == "s_covers_0000_03.tar:1234:567"
        
        d = code.cover().get_details(42, "s")
        assert isinstance(d, web.storage)
        assert d == {"id": 42, "filename_s": "s_covers_0000_00.tar:1234:567", "created": datetime.datetime(2010, 1, 1)}
########NEW FILE########
__FILENAME__ = test_coverstore
import py.test
import os.path
import web
import unittest

from openlibrary.coverstore import config, coverlib, disk, schema, utils
import _setup

def setup_module(mod):
    _setup.setup_module(mod, db=False)
    
    mod.root.mkdir('items', 'covers_0000')
    mod.root.mkdir('items', 's_covers_0000')
    mod.root.mkdir('items', 'm_covers_0000')
    mod.root.mkdir('items', 'l_covers_0000')
    
def teardown_module(mod):
    _setup.teardown_module(mod)

def test_write_image():
    """Test writing jpg, gif and png images"""
    yield _test_write_image, 'a', static_dir + '/images/ajaxImage.jpg'
    yield _test_write_image, 'b', static_dir + '/logos/logo-en.gif'
    yield _test_write_image, 'c', static_dir + '/logos/logo-en.png'
    
def _test_write_image(prefix, path):
    data = open(path).read()
    assert coverlib.write_image(data, prefix) != None
    
    def exists(filename):
        return os.path.exists(coverlib.find_image_path(filename))
    
    assert exists(prefix + '.jpg')
    assert exists(prefix + '-S.jpg')
    assert exists(prefix + '-M.jpg')
    assert exists(prefix + '-L.jpg')
    
    assert open(coverlib.find_image_path(prefix + '.jpg')).read() == data

def test_bad_image():
    prefix = config.data_root + '/bad'
    assert coverlib.write_image('', prefix) == None

    prefix = config.data_root + '/bad'
    assert coverlib.write_image('not an image', prefix) == None
    
def test_resize_image_aspect_ratio():
    """make sure the aspect-ratio is maintained"""
    import Image
    img = Image.new('RGB', (100, 200))
    
    img2 = coverlib.resize_image(img, (40, 40))
    assert img2.size == (20, 40)

    img2 = coverlib.resize_image(img, (400, 400))
    assert img2.size == (100, 200)
    
    img2 = coverlib.resize_image(img, (75, 100))
    assert img2.size == (50, 100)
    
    img2 = coverlib.resize_image(img, (75, 200))
    assert img2.size == (75, 150)

def test_serve_file():
    path = static_dir + "/logos/logo-en.png"
        
    assert coverlib.read_file('/dev/null') == ''
    assert coverlib.read_file(path) == open(path).read()

    assert coverlib.read_file(path + ":10:20") == open(path).read()[10:10+20] 
    
def test_server_image():
    def write(filename, data):
        f = open(os.path.join(config.data_root, filename), 'w')
        f.write(data)
        f.close()

    def do_test(d):
        def serve_image(d, size):
            return "".join(coverlib.read_image(d, size))

        assert serve_image(d, '') == 'main image'
        assert serve_image(d, None) == 'main image'    

        assert serve_image(d, 'S') == 'S image'
        assert serve_image(d, 'M') == 'M image'
        assert serve_image(d, 'L') == 'L image'

        assert serve_image(d, 's') == 'S image'
        assert serve_image(d, 'm') == 'M image'
        assert serve_image(d, 'l') == 'L image'
        
    # test with regular images
    write('localdisk/a.jpg', 'main image')
    write('localdisk/a-S.jpg', 'S image')
    write('localdisk/a-M.jpg', 'M image')
    write('localdisk/a-L.jpg', 'L image')
    
    d = web.storage(id=1, filename='a.jpg', filename_s='a-S.jpg', filename_m='a-M.jpg', filename_l='a-L.jpg')
    do_test(d)
    
    # test with offsets
    write('items/covers_0000/covers_0000_00.tar', 'xxmain imagexx')
    write('items/s_covers_0000/s_covers_0000_00.tar', 'xxS imagexx')
    write('items/m_covers_0000/m_covers_0000_00.tar', 'xxM imagexx')
    write('items/l_covers_0000/l_covers_0000_00.tar', 'xxL imagexx')

    d = web.storage(
        id=1,
        filename='covers_0000_00.tar:2:10', 
        filename_s='s_covers_0000_00.tar:2:7', 
        filename_m='m_covers_0000_00.tar:2:7',
        filename_l='l_covers_0000_00.tar:2:7')
    do_test(d)

def test_image_path():
    assert coverlib.find_image_path('a.jpg') == config.data_root + '/localdisk/a.jpg'
    assert coverlib.find_image_path('covers_0000_00.tar:1234:10') == config.data_root + '/items/covers_0000/covers_0000_00.tar:1234:10'

########NEW FILE########
__FILENAME__ = test_doctests
from infogami.infobase.tests.test_doctests import find_doctests, run_doctest

def test_doctests():
    modules = [
        'openlibrary.coverstore.archive',
        'openlibrary.coverstore.code',
        'openlibrary.coverstore.db',
        'openlibrary.coverstore.server',
        'openlibrary.coverstore.utils',
        'openlibrary.coverstore.warc',
    ]
    for t in find_doctests(modules):
        yield _run_doctest, t

def _run_doctest(t):
    # dummy function to make py.test think that test belongs in this module instead of run_doctest's module
    run_doctest(t)
########NEW FILE########
__FILENAME__ = test_webapp
import py.test
import os.path
import web
import simplejson
import time
import urllib
import unittest

from openlibrary.coverstore import config, disk, schema, code, coverlib, utils, archive
import _setup

def setup_module(mod):
    _setup.setup_module(mod, db=True)
    mod.app = code.app
    
def teardown_module(mod):
    _setup.teardown_module(mod)
    
def pytest_funcarg__foo(request):
    return "foo"
    
def test_foo(foo):
    assert foo == "foo"

class Mock:
    def __init__(self):
        self.calls = []
        self.default = None

    def __call__(self, *a, **kw):
        for a2, kw2, _return in self.calls:
            if (a, kw) == (a2, kw2):
                return _return
        return self.default

    def setup_call(self, *a, **kw):
        _return = kw.pop("_return", None)
        call = a, kw, _return
        self.calls.append(call)
    
class WebTestCase:
    def setup_method(self, method):
        db.delete("log", where="1=1")
        db.delete('cover', where='1=1')
        self.browser = app.browser()

    def jsonget(self, path):
        self.browser.open(path)
        return simplejson.loads(self.browser.data)

    def upload(self, olid, path):
        """Uploads an image in static dir"""
        b = self.browser

        path = os.path.join(static_dir, path)
        content_type, data = utils.urlencode({'olid': olid, 'data': open(path).read()}) 
        b.open('/b/upload2', data, {'Content-Type': content_type})
        return simplejson.loads(b.data)['id']

    def delete(self, id, redirect_url=None):
        b = self.browser

        params = {'id': id}
        if redirect_url:
            params['redirect_url'] = redirect_url
        b.open('/b/delete', urllib.urlencode(params))
        return b.data
    
    def static_path(self, path):
        return os.path.join(static_dir, path)
        
        
class DBTest:
    def setUp(self):
        db.delete('cover', where='1=1')

    def test_write(self):
        path = static_dir + "/logos/logo-en.png"
        data = open(path).read()
        d = coverlib.save_image(data, category='b', olid='OL1M')
        
        assert 'OL1M' in d.filename
        path = config.data_root + '/localdisk/' + d.filename
        assert open(path).read() == data

class TestWebapp(WebTestCase):
    def test_touch(self):
        py.test.skip("TODO: touch is no more used. Remove or fix this test later.")
        
        b = self.browser

        id1 = self.upload('OL1M', 'logos/logo-en.png')
        time.sleep(1)
        id2 = self.upload('OL1M', 'logos/logo-it.png')
        
        assert id1 < id2
        assert b.open('/b/olid/OL1M.jpg').read() == open(static_dir + '/logos/logo-it.png').read()

        b.open('/b/touch', urllib.urlencode({'id': id1}))
        assert b.open('/b/olid/OL1M.jpg').read() == open(static_dir + '/logos/logo-en.png').read()

    def test_delete(self):
        b = self.browser
        
        id1 = self.upload('OL1M', 'logos/logo-en.png')
        data = self.delete(id1)
        assert data == 'cover has been deleted successfully.'
    
    def test_get(self):
        assert app.request('/').status == "200 OK"

    def test_upload(self):
        b = self.browser
        
        path = os.path.join(static_dir, 'logos/logo-en.png')
        filedata = open(path).read()
        content_type, data = utils.urlencode({'olid': 'OL1234M', 'data': filedata}) 
        b.open('/b/upload2', data, {'Content-Type': content_type})
        id = simplejson.loads(b.data)['id']

        assert b.status == 200
        self.verify_upload(id, filedata, {'olid': 'OL1234M'})
        
    def test_upload_with_url(self, monkeypatch):
        filedata = open(static_dir + '/logos/logo-en.png').read()
        source_url = "http://example.com/bookcovers/1.jpg"
        
        mock = Mock()
        mock.setup_call(source_url, _return=filedata)
        monkeypatch.setattr(code, "download", mock)
        monkeypatch.setattr(utils, "download", mock)
        
        content_type, data = utils.urlencode({'olid': 'OL1234M', 'source_url': source_url}) 
        self.browser.open('/b/upload2', data, {'Content-Type': content_type})
        
        print "data", self.browser.data
        
        id = simplejson.loads(self.browser.data)['id']
        self.verify_upload(id, filedata, {"source_url": source_url, "olid": "OL1234M"})

    def verify_upload(self, id, data, expected_info={}):
        b = self.browser
        b.open('/b/id/%d.json' % id)
        info = simplejson.loads(b.data)
        for k, v in expected_info.items():
            assert info[k] == v

        response = b.open('/b/id/%d.jpg' % id)
        assert b.status == 200
        assert response.info().getheader('Content-Type') == 'image/jpeg'
        assert b.data == data

        b.open('/b/id/%d-S.jpg' % id)
        assert b.status == 200

        b.open('/b/id/%d-M.jpg' % id)
        assert b.status == 200

        b.open('/b/id/%d-L.jpg' % id)
        assert b.status == 200
        
    def test_archive_status(self):
        id = self.upload('OL1M', 'logos/logo-en.png')
        d = self.jsonget('/b/id/%d.json' % id)
        assert d['archived'] == False
        assert d['deleted'] == False

    def test_archive(self):
        b = self.browser
        
        f1 = web.storage(olid='OL1M', filename='logos/logo-en.png')
        f2 = web.storage(olid='OL2M', filename='logos/logo-it.png')
        files = [f1, f2]
        
        for f in files:
            f.id = self.upload(f.olid, f.filename)
            f.path = os.path.join(static_dir, f.filename)
            assert b.open('/b/id/%d.jpg' % f.id).read() == open(f.path).read()
        
        archive.archive()
        
        for f in files:
            d = self.jsonget('/b/id/%d.json' % f.id)
            print f.id, d
            assert 'tar:' in d['filename']
            assert b.open('/b/id/%d.jpg' % f.id).read() == open(f.path).read()

########NEW FILE########
__FILENAME__ = utils
"""Utilities for coverstore"""

import urllib, urllib2
import socket
import os
import mimetypes
import Image
import simplejson
import web

import random
import string

import config
import oldb

class AppURLopener(urllib.FancyURLopener):
    version = "Mozilla/5.0 (Compatible; coverstore downloader http://covers.openlibrary.org)"

socket.setdefaulttimeout(10.0)
urllib._urlopener = AppURLopener()

def safeint(value, default=None):
    """
        >>> safeint('1')
        1
        >>> safeint('x')
        >>> safeint('x', 0)
        0
    """
    try:
        return int(value)
    except:
        return default

def get_ol_url():
    return web.rstrips(config.ol_url, "/")
    
def ol_things(key, value):
    if oldb.is_supported():
        return oldb.query(key, value)
    else:
        query = {
            'type': '/type/edition',
            key: value, 
            'sort': 'last_modified',
            'limit': 10
        }
        try:
            d = dict(query=simplejson.dumps(query))
            result = download(get_ol_url() + '/api/things?' + urllib.urlencode(d))
            result = simplejson.loads(result)
            return result['result']
        except IOError:
            import traceback
            traceback.print_exc()
            return []
        
def ol_get(olkey):
    if oldb.is_supported():
        return oldb.get(olkey)
    else:
        try:
            result = download(get_ol_url() + olkey + ".json")
            return simplejson.loads(result)
        except IOError:
            return None

USER_AGENT = "Mozilla/5.0 (Compatible; coverstore downloader http://covers.openlibrary.org)"
def download(url):
    req = urllib2.Request(url, headers={'User-Agent': USER_AGENT})
    r = urllib2.urlopen(req)
    return r.read()

def urldecode(url):
    """
        >>> urldecode('http://google.com/search?q=bar&x=y')
        ('http://google.com/search', {'q': 'bar', 'x': 'y'})
        >>> urldecode('http://google.com/')
        ('http://google.com/', {})
    """
    base, query = urllib.splitquery(url)
    query = query or ""
    items = [item.split('=', 1) for item in query.split('&') if '=' in item]
    d = dict((urllib.unquote(k), urllib.unquote_plus(v)) for (k, v) in items)
    return base, d

def changequery(url, **kw):
    """
        >>> changequery('http://google.com/search?q=foo', q='bar', x='y')
        'http://google.com/search?q=bar&x=y'
    """
    base, params = urldecode(url)
    params.update(kw)
    return base + '?' + urllib.urlencode(params)

def read_file(path, offset, size, chunk=50*1024):
    """Returns an iterator over file data at specified offset and size.    
    
        >>> len("".join(read_file('/dev/urandom', 100, 10000)))
        10000
    """
    f = open(path)
    f.seek(offset)
    while size:
        data = f.read(min(chunk, size))
        size -= len(data)
        if data:
            yield data
        else:
            f.close()
            raise IOError("file truncated")
    f.close()
    
def rm_f(filename):
    try:
        os.remove(filename)
    except OSError:
        pass

chars = string.letters + string.digits
def random_string(n):
    return "".join([random.choice(chars) for i in range(n)])
    
def urlencode(data):
    """
    urlencodes the given data dictionary. If any of the value is a file object, data is multipart encoded.
    
    @@@ should go into web.browser
    """
    multipart = False
    for v in data.values():
        if isinstance(v, file):
            multipart = True
            break
            
    if not multipart:
        return 'application/x-www-form-urlencoded', urllib.urlencode(data)
    else:
        # adopted from http://code.activestate.com/recipes/146306/
        def get_content_type(filename):
            return mimetypes.guess_type(filename)[0] or 'application/octet-stream'
        
        def encode(key, value, out):
            if isinstance(value, file):
                out.append('--' + BOUNDARY)
                out.append('Content-Disposition: form-data; name="%s"; filename="%s"' % (key, value.name))
                out.append('Content-Type: %s' % get_content_type(value.name))
                out.append('')
                out.append(value.read())
            elif isinstance(value, list):
                for v in value:
                    encode(key, v)
            else:
                out.append('--' + BOUNDARY)
                out.append('Content-Disposition: form-data; name="%s"' % key)
                out.append('')
                out.append(value)

        BOUNDARY = "----------ThIs_Is_tHe_bouNdaRY_$"
        CRLF = '\r\n'
        out = []
        for k, v in data.items():
            encode(k, v, out)
        body = CRLF.join(out)
        content_type = 'multipart/form-data; boundary=%s' % BOUNDARY
        return content_type, body

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = warc
"""
Reader and writer for WARC file format version 0.10.

http://archive-access.sourceforge.net/warc/warc_file_format-0.10.html
"""

import urllib
import httplib
import datetime

WARC_VERSION = "0.10"
CRLF = "\r\n"

class WARCReader:
    """Reader to read records from a warc file.
    
    >>> import StringIO
    >>> f = StringIO.StringIO()
    >>> r1 = WARCRecord("resource", "subject_uri", "image/jpeg", {"hello": "world"}, "foo")
    >>> r2 = WARCRecord("resource", "subject_uri", "image/jpeg", {"hello": "world"}, "bar")
    >>> w = WARCWriter(f)
    >>> _ = w.write(r1)
    >>> _ = w.write(r2)
    >>> f.seek(0)
    >>> reader = WARCReader(f)
    >>> records = list(reader.read())
    >>> records == [r1, r2]
    True
    """
    def __init__(self, file):
        self._file = file
        
    def read(self):
        """Returns an iterator over all the records in the WARC file."""
        def consume_crlf():
            assert self._readline() == CRLF

        while True:
            header = self._read_header()
            if header is None:
                break
            yield LazyWARCRecord(self._file, self._file.tell(), header)
            self._file.seek(int(header.data_length), 1)
            consume_crlf()
            consume_crlf()

    def _read_header(self):
        """Reads the header of a record from the WARC file."""
        def consume_crlf():
            line = self._file.readline()
            assert line == CRLF

        line = self._file.readline()
        if not line: 
            return None
        
        tokens = line.strip().split()
        warc_id, data_length, record_type, subject_uri, creation_date, record_id, content_type = tokens
        header = WARCHeader(warc_id, 
            data_length, record_type, subject_uri, 
            creation_date, record_id, content_type, {})
            
        while True:
            line = self._file.readline()
            if line == CRLF:
                break
            k, v = line.strip().split(':', 1)
            header.headers[k.strip()] = v.strip()
        return header
        
    def _readline(self):        
        line = self._file.readline()
        if line[-2:-1] == '\r':
            return line
        else:
            return line + self._readline()

class HTTPFile:
    """File like interface to HTTP url."""
    def __init__(self, url, chunk_size=1024):
        self.url = url
        self.offset = 0
        self.chunk_size = chunk_size
        
    def seek(self, offset, whence=0):
        if whence == 0:
            self.offset = offset
        elif whence == 1:
            self.offset += offset
        else:
            raise "Invalid whence", whence
    
    def tell(self):
        return self.offset
        
    def readline(self):
        """Reads a line from file."""
        data = ''
        offset = self.tell()
        data = "".join(self._readuntil(lambda chunk: '\n' in chunk or chunk == ''))
        data = data[:data.find('\n') + 1]
        self.seek(offset + len(data))
        return data

    def _readuntil(self, condition):
        while True:
            data = self.read(self.chunk_size)
            yield data
            if condition(data):
                break     

    def read(self, size):
        protocol, host, port, path = self.urlsplit(self.url)
        conn = httplib.HTTPConnection(host, port)
        headers = {'Range': 'bytes=%d-%d' % (self.offset, self.offset + size - 1)}
        conn.request('GET', path, None, headers)
        response = conn.getresponse()
        data = response.read()
        self.offset += len(data)
        return data
            
    def urlsplit(self, url):
        """Splits url into protocol, host, port and path.

            >>> f = HTTPFile('')
            >>> f.urlsplit("http://www.google.com/search?q=hello")
            ('http', 'www.google.com', None, '/search?q=hello')
        """
        protocol, rest = urllib.splittype(url)
        hostport, path = urllib.splithost(rest)
        host, port = urllib.splitport(hostport)
        return protocol, host, port, path        
    
class WARCHeader:
    r"""WARCHeader class represents the header in the WARC file format.
    
    header      = header-line CRLF *anvl-field CRLF
    header-line = warc-id tsp data-length tsp record-type tsp
                  subject-uri tsp creation-date tsp
                  record-id tsp content-type
    anvl-field  =  field-name ":" [ field-body ] CRLF
    
    >>> WARCHeader("WARC/0.10", 10, "resource", "subject_uri", "20080808080808", "record_42", "image/jpeg", {'hello': 'world'})
    <header: 'WARC/0.10 10 resource subject_uri 20080808080808 record_42 image/jpeg\r\nhello: world\r\n\r\n'>
    """
    def __init__(self, warc_id, 
            data_length, record_type, subject_uri, 
            creation_date, record_id, content_type, headers):
        self.warc_id = warc_id
        self.data_length = str(data_length)
        self.record_type = record_type
        self.subject_uri = subject_uri
        self.creation_date = creation_date
        self.record_id = record_id
        self.content_type = content_type
        self.headers = headers
        
    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        return not (self == other)
        
    def __str__(self):
        params = [self.warc_id, self.data_length, self.record_type, self.subject_uri, 
                self.creation_date, self.record_id, self.content_type]
                
        a = " ".join([str(p) for p in params]) 
        b = "".join(["%s: %s\r\n" % (k, v) for k, v in self.headers.items()])
        return a + CRLF + b + CRLF

    def dict(self):
        d = dict(self.headers)
        for k in ["warc_id", "data_length", "record_type", "subject_uri", "creation_date", "record_id", "content_type"]:
            d[k] = getattr(self, k)
        return d
        
    def __repr__(self):
        return "<header: %s>" % repr(str(self))

class WARCRecord:
    r"""A record in a WARC file.
    
    >>> WARCRecord("resource", "subject_uri", "image/jpeg", {"hello": "world"}, "foo bar", creation_date="20080808080808", record_id="record_42")
    <record: 'WARC/0.10 7 resource subject_uri 20080808080808 record_42 image/jpeg\r\nhello: world\r\n\r\nfoo bar'>
    """
    def __init__(self, record_type, subject_uri, content_type, headers, data, creation_date=None, record_id=None):        
        warc_id = "WARC/" + WARC_VERSION
        data_length = len(data)
        creation_date = creation_date or datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')
        record_id = record_id or self.create_uuid()
        
        self._header = WARCHeader(warc_id, data_length, record_type, subject_uri, 
                                creation_date, record_id, content_type, headers)
        self._data = data

    def create_uuid(self):
        import uuid
        return 'urn:uuid:' + str(uuid.uuid1())
        
    def get_header(self):
        return self._header
        
    def get_data(self):
        return self._data
        
    def __eq__(self, other):
        return self.get_header() == other.get_header() and self.get_data() == other.get_data()
        
    def __ne__(self, other):
        return not (self == other)
        
    def __str__(self):
        return str(self.get_header()) + self.get_data()
        
    def __repr__(self):
        return "<record: %s>" % repr(str(self))
        
class LazyWARCRecord(WARCRecord):
    """Class to create WARCRecord lazily.
    
    >>> import StringIO
    >>> r1 = WARCRecord("resource", "subject_uri", "image/jpeg", {"hello": "world"}, "foo bar", creation_date="20080808080808", record_id="record_42")
    >>> f = StringIO.StringIO(str(r1))
    >>> offset = len(str(r1.get_header()))
    >>> r2 = LazyWARCRecord(f, offset, r1.get_header())
    >>> r1 == r2
    True
    """
    def __init__(self, file, offset, header):
        self.header = header
        self.file = file
        self.offset = offset
        self._data = None
        
    def get_header(self):
        return self.header
        
    def get_data(self):
        if self._data is None:
            offset = self.file.tell()
            self.file.seek(int(self.offset))
            self._data = self.file.read(int(self.header.data_length))
            self.file.seek(offset)
        return self._data

class WARCWriter:
    r"""Writes to write warc records to file.
    
    >>> import re, StringIO
    >>> f = StringIO.StringIO()
    >>> r1 = WARCRecord("resource", "subject_uri", "image/jpeg", {"hello": "world"}, "foo", creation_date="20080808080808", record_id="record_42")
    >>> r2 = WARCRecord("resource", "subject_uri", "image/jpeg", {"hello": "world"}, "bar", creation_date="20080808090909", record_id="record_43")
    >>> w = WARCWriter(f)
    >>> w.write(r1)
    86
    >>> w.write(r2)
    179
    >>> lines = re.findall('[^\r\n]*\r\n', f.getvalue()) # break at \r\n to print in a readable way
    >>> for line in lines: print repr(line)
    'WARC/0.10 3 resource subject_uri 20080808080808 record_42 image/jpeg\r\n'
    'hello: world\r\n'
    '\r\n'
    'foo\r\n'
    '\r\n'
    'WARC/0.10 3 resource subject_uri 20080808090909 record_43 image/jpeg\r\n'
    'hello: world\r\n'
    '\r\n'
    'bar\r\n'
    '\r\n'
    """
    def __init__(self, file):
        self.file = file
        
    def close(self):
        self.file.close()
        
    def write(self, record):
        """Writes a record into the WARC file. 
        Assumes that data_length and other attributes are correctly set in record.header.
        """
        self.file.write(str(record.get_header()))
        offset = self.file.tell()
        self.file.write(record.get_data())
        self.file.write(CRLF + CRLF)
        self.file.flush()
        return offset

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = db
#! /usr/bin/env python
"""Library to provide fast access to Open Library database.

How to use:
    
    from openlibrary.data import db
    
    db.setup_database(db='openlibrary', user='anand', pw='')
    db.setup_memcache(['host1:port1', 'host2:port2'])
    
    # get a set of docs
    docs = db.get_docs(['/sandbox'])
    
    # get all books
    books = dc.itedocs(type="/type/edition")
    
    # update docs
    db.update_docs(docs)
    
Each doc is a storage object with "id", "key", "revision" and "data".
"""

from openlibrary.utils import olmemcache
import simplejson
import web
import os
import datetime
import sys
import time

__all__ = [
    "setup_database", "setup_memcache",
    "longquery",
    "iterdocs",
    "get_docs", "update_docs"
]

db_parameters = None
db = None
mc = None

def setup_database(**db_params):
    """Setup the database. This must be called before using any other functions in this module.
    """
    global db, db_parameters
    db_params.setdefault('dbn', 'postgres')
    db = web.database(**db_params)
    db.printing = False
    db_parameters = db_params
    
def setup_memcache(servers):
    """Setup the memcached servers. 
    This must be called along with setup_database, if memcached servers are used in the system.
    """
    global mc
    mc = olmemcache.Client(servers)
    
def iterdocs(type=None):
    """Returns an iterator over all docs in the database.
    If type is specified, then only docs of that type will be returned.
    """
    q = 'SELECT id, key, latest_revision as revision FROM thing'
    if type:
        type_id = get_thing_id(type)
        q += ' WHERE type=$type_id'
    q += ' ORDER BY id'
        
    for chunk in longquery(q, locals()):
        docs = chunk
        _fill_data(docs)
        for doc in docs:
            yield doc
            
def longquery(query, vars, chunk_size=10000):
    """Execute an expensive query using db cursors.
    
    USAGE:
    
        for chunk in longquery("SELECT * FROM bigtable"):
            for row in chunk:
                print row
    """
    # DB cursor is valid only in the transaction
    # Create a new database to avoid this transaction interfere with the application code
    db = web.database(**db_parameters)
    db.printing = False
    
    tx = db.transaction()
    try:
        db.query("DECLARE longquery NO SCROLL CURSOR FOR " + query, vars=vars)
        while True:
            chunk = db.query("FETCH FORWARD $chunk_size FROM longquery", vars=locals()).list()
            if chunk:
                yield chunk
            else:
                break
    finally:
        tx.rollback()
        
def _fill_data(docs):
    """Add `data` to all docs by querying memcache/database.
    """
    def get(keys):
        if not keys:
            return []
        return db.query("SELECT thing.id, thing.key, data.revision, data.data"
            + " FROM thing, data"
            + " WHERE thing.id = data.thing_id"
            +   " AND thing.latest_revision = data.revision"
            +   " AND key in $keys", 
            vars=locals())
        
    keys = [doc.key for doc in docs]
    
    d = mc and mc.get_multi(keys) or {}
    debug("%s/%s found in memcache" % (len(d), len(keys)))

    keys = [doc.key for doc in docs if doc.key not in d]
    for row in get(keys):
        d[row.key] = row.data
    
    for doc in docs:
        doc.data = simplejson.loads(d[doc.key])
    return docs

def read_docs(keys, for_update=False):
    """Read the docs the docs from DB."""
    if not keys:
        return []
        
    debug("BEGIN SELECT")
    q = "SELECT thing.id, thing.key, thing.latest_revision as revision FROM thing WHERE key IN $keys"
    if for_update:
        q += " FOR UPDATE"
    docs = db.query(q, vars=locals())
    docs = docs.list()
    debug("END SELECT")
    _fill_data(docs)
    return docs
    
def update_docs(docs, comment, author, ip="127.0.0.1"):
    """Updates the given docs in the database by writing all the docs in a chunk. 
    
    This doesn't update the index tables. Avoid this function if you have any change that requires updating the index tables.
    """
    now = datetime.datetime.utcnow()
    author_id = get_thing_id(author)
    t = db.transaction()
    try:
        docdict = dict((doc.id, doc) for doc in docs)
        thing_ids = docdict.keys()

        # lock the rows in the table
        rows = db.query("SELECT id, key, latest_revision FROM thing where id IN $thing_ids FOR UPDATE", vars=locals())
        
        # update revision and last_modified in each document
        for row in rows:
            doc = docdict[row.id]
            doc.revision = row.latest_revision + 1
            doc.data['revision'] = doc.revision
            doc.data['latest_revision'] = doc.revision
            doc.data['last_modified']['value'] = now.isoformat()
        
        tx_id = db.insert("transaction", author_id=author_id, action="bulk_update", ip="127.0.0.1", created=now, comment=comment)
        debug("INSERT version")
        db.multiple_insert("version", [dict(thing_id=doc.id, transaction_id=tx_id, revision=doc.revision) for doc in docs], seqname=False)
        
        debug("INSERT data")
        data = [web.storage(thing_id=doc.id, revision=doc.revision, data=simplejson.dumps(doc.data)) for doc in docs]        
        db.multiple_insert("data", data, seqname=False)
        
        debug("UPDATE thing")
        db.query("UPDATE thing set latest_revision=latest_revision+1 WHERE id IN $thing_ids", vars=locals())
    except:
        t.rollback()
        debug("ROLLBACK")
        raise
    else:
        t.commit()
        debug("COMMIT")
        
        mapping = dict((doc.key, d.data) for doc, d in zip(docs, data))
        mc and mc.set_multi(mapping)
        debug("MC SET")

def debug(*a):
    print >> sys.stderr, time.asctime(), a
    
@web.memoize
def get_thing_id(key):
    return db.query("SELECT * FROM thing WHERE key=$key", vars=locals())[0].id


########NEW FILE########
__FILENAME__ = dump
"""Library for generating and processing Open Library data dumps.

Glossary:

* dump - Dump of latest revisions of all documents.
* cdump - Complete dump. Dump of all revisions of all documents.
* idump - Incremental dump. Dump of all revisions created in the given day.
"""

import sys, os
import web
import re
import time
import simplejson
import itertools
import gzip

import db

def print_dump(json_records, filter=None):
    """Print the given json_records in the dump format.
    """
    for i, json in enumerate(json_records):
        if i % 1000000 == 0:
            log(i)
        d = simplejson.loads(json)
        d.pop('id', None)
        d = _process_data(d)
        
        key = web.safestr(d['key'])
        type = d['type']['key']
        timestamp = d['last_modified']['value']
        json = simplejson.dumps(d)
        
        # skip user and admin pages
        if key.startswith("/people/") or key.startswith("/admin/"):
            continue
        
        # skip obsolete pages. Obsolete pages include volumes, scan_records and users marked as spam.
        if key.startswith("/b/") or key.startswith("/scan") or key.startswith("/old/") or not key.startswith("/"):
            continue
        
        if filter and filter(d) is False:
            continue
            
        print "\t".join([type, key, str(d['revision']), timestamp, json])
        
def read_data_file(filename):
    for line in xopen(filename):
        thing_id, revision, json = line.strip().split("\t")
        yield pgdecode(json)
        
def log(*args):
    print >> sys.stderr, time.asctime(), " ".join(str(a) for a in args)
    
def xopen(path, mode='r'):
    if path.endswith(".gz"):
        return gzip.open(path, mode)
    else:
        return open(path, mode)

def read_tsv(file, strip=True):
    """Read a tab seperated file and return an iterator over rows."""    
    log("reading", file)
    if isinstance(file, basestring):
        file = xopen(file)
        
    for i, line in enumerate(file):
        if i % 1000000 == 0:
            log(i)
        if strip:
            line = line.strip()
        yield line.split("\t")

def generate_cdump(data_file, date=None):
    """Generates cdump from a copy of data table.
    If date is specified, only revisions created on or before that date will be considered.
    """
    
    # adding Z to the date will make sure all the timestamps of that day are less than date.
    #
    #   >>> "2010-05-17T10:20:30" < "2010-05-17"
    #   False
    #   >>> "2010-05-17T10:20:30" < "2010-05-17Z"
    #   True
    filter = date and (lambda doc: doc['last_modified']['value'] < date + "Z")
    
    print_dump(read_data_file(data_file), filter=filter)
        
def sort_dump(dump_file=None, tmpdir="/tmp/", buffer_size="1G"):
    """Sort the given dump based on key."""
    tmpdir = os.path.join(tmpdir, "oldumpsort")
    if not os.path.exists(tmpdir):
        os.makedirs(tmpdir)
        
    M = 1024*1024
    
    filenames = [os.path.join(tmpdir, "%02x.txt.gz" % i) for i in range(256)]
    files = [gzip.open(f, 'w') for f in filenames]
    
    if dump_file is None:
        stdin = sys.stdin
    else:
        stdin = xopen(dump_file)
    
    # split the file into 256 chunks using hash of key
    log("splitting", dump_file)
    for i, line in enumerate(stdin):
        if i % 1000000 == 0:
            log(i)
        
        type, key, revision, timestamp, json = line.strip().split("\t")
        findex = hash(key) % 256
        files[findex].write(line)
        
    for f in files:
        f.flush()
        f.close()
    files = []
        
    for fname in filenames:
        log("sorting", fname)
        status = os.system("gzip -cd %(fname)s | sort -S%(buffer_size)s -k2,3" % locals())
        if status != 0:
            raise Exception("sort failed with status %d" % status)
    
def pmap(f, tasks):
    """Run tasks parallelly."""
    try:
        from subprocess import Pool
        
        from multiprocessing import Pool
        r = pool.map_async(f, tasks, callback=results.append)
        r.wait() # Wait on the results
        
    except ImportError:
        Pool = None
    
def generate_dump(cdump_file=None):
    """Generate dump from cdump.
    
    The given cdump must be sorted by key.
    """   
    def process(data):
        revision = lambda cols: int(cols[2])
        for key, rows in itertools.groupby(data, key=lambda cols: cols[1]):
            row = max(rows, key=revision)
            yield row
            
    tjoin = "\t".join        
    data = read_tsv(cdump_file or sys.stdin, strip=False)
    # group by key and find the max by revision
    sys.stdout.writelines(tjoin(row) for row in process(data))
        
def generate_idump(day, **db_parameters):
    """Generate incremental dump for the given day.
    """
    db.setup_database(**db_parameters)
    rows = db.longquery("SELECT data.* FROM data, version, transaction " 
        + " WHERE data.thing_id=version.thing_id" 
        + "     AND data.revision=version.revision"
        + "     AND version.transaction_id=transaction.id"
        + "     AND transaction.created >= $day AND transaction.created < date $day + interval '1 day'"
        + " ORDER BY transaction.created",
        vars=locals(), chunk_size=10000)
    print_dump(row.data for chunk in rows for row in chunk)
    
def split_dump(dump_file=None, format="oldump_%s.txt"):
    """Split dump into authors, editions and works."""
    types = ["/type/edition", "/type/author", "/type/work", "/type/redirect"]
    files = {}
    for t in types:
        tname = t.split("/")[-1] + "s"
        files[t] = xopen(format % tname, "w")
        
    if dump_file is None:
        stdin = sys.stdin
    else:
        stdin = xopen(dump_file)
        
    for i, line in enumerate(stdin):
        if i % 1000000 == 0:
            log(i)
        type, rest = line.split("\t", 1)
        if type in files:
            files[type].write(line)
            
    for f in files.values():
        f.close()
    
def make_index(dump_file):
    """Make index with "path", "title", "created" and "last_modified" columns."""
    
    from openlibrary.plugins.openlibrary.processors import urlsafe
            
    for type, key, revision, timestamp, json in read_tsv(dump_file):
        data = simplejson.loads(json)
        if type == '/type/edition' or type == '/type/work':
            title = data.get('title', 'untitled')
            path = key + '/' + urlsafe(title)
        elif type == '/type/author':
            title = data.get('name', 'unnamed')
            path = key + '/' + urlsafe(title)
        else:
            title = data.get('title', key)
            path = key
            
        title = title.replace("\t", " ")
        
        if 'created' in data:
            created = data['created']['value']
        else:
            created = "-"
        print "\t".join([web.safestr(path), web.safestr(title), created, timestamp])
        
def make_bsddb(dbfile, dump_file):
    import bsddb 
    db = bsddb.btopen(dbfile, 'w', cachesize=1024*1024*1024)
    
    from infogami.infobase.utils import flatten_dict
    
    indexable_keys = set([
        "authors.key",  "works.key", # edition
        "authors.author.key", "subjects", "subject_places", "subject_people", "subject_times" # work
    ])
    for type, key, revision, timestamp, json in read_tsv(dump_file):
        db[key] = json
        d = simplejson.loads(json)
        index = [(k, v) for k, v in flatten_dict(d) if k in indexable_keys]
        for k, v in index:
            k = web.rstrips(k, ".key")
            if k.startswith("subject"):
                v = '/' + v.lower().replace(" ", "_")
                
            dbkey  = web.safestr('by_%s%s' % (k, v))
            if dbkey in db:
                db[dbkey] = db[dbkey] + " " + key
            else:
                db[dbkey] = key
    db.close()
    log("done")

def _process_key(key):
    mapping = (
        "/l/", "/languages/",
        "/a/", "/authors/",
        "/b/", "/books/",
        "/user/", "/people/"
    )
    for old, new in web.group(mapping, 2):
        if key.startswith(old):
            return new + key[len(old):]
    return key

def _process_data(data):
    """Convert keys from /a/, /b/, /l/ and /user/ to /authors/, /books/, /languages/ and /people/ respectively.
    """
    if isinstance(data, list):
        return [_process_data(d) for d in data]
    elif isinstance(data, dict):
        if 'key' in data:
            data['key'] = _process_key(data['key'])
            
        # convert date to ISO format
        if 'type' in data and data['type'] == '/type/datetime':
            data['value'] = data['value'].replace(' ', 'T')
            
        return dict((k, _process_data(v)) for k, v in data.iteritems())
    else:
        return data

def _make_sub(d):
    """Make substituter.

        >>> f = _make_sub(dict(a='aa', bb='b'))
        >>> f('aabbb')
        'aaaabb'
    """
    def f(a):
        return d[a.group(0)]
    rx = re.compile("|".join(map(re.escape, d.keys())))
    return lambda s: s and rx.sub(f, s)

def _invert_dict(d):
    return dict((v, k) for (k, v) in d.items())

_pgencode_dict = {'\n': r'\n', '\r': r'\r', '\t': r'\t', '\\': r'\\'}
_pgencode = _make_sub(_pgencode_dict)
_pgdecode = _make_sub(_invert_dict(_pgencode_dict))

def pgencode(text):
    """Reverse of pgdecode."""
    return _pgdecode(text)

def pgdecode(text):
    r"""Decode postgres encoded text.
        
        >>> pgdecode('\\n')
        '\n'
    """
    return _pgdecode(text)

def main(cmd, args):
    """Command Line interface for generating dumps.
    """
    iargs = iter(args)

    args = []
    kwargs = {}
    
    for a in iargs:
        if a.startswith('--'):
            name = a[2:].replace("-", "_")
            value = iargs.next()
            kwargs[name] = value
        else:
            args.append(a)
    
    if cmd == 'cdump':
        generate_cdump(*args, **kwargs)
    elif cmd == 'dump':
        generate_dump(*args, **kwargs)
    elif cmd == 'idump':
        generate_idump(*args, **kwargs)
    elif cmd == 'sort':
        sort_dump(*args, **kwargs)
    elif cmd == 'split':
        split_dump(*args, **kwargs)
    elif cmd == 'index':
        make_index(*args, **kwargs)
    elif cmd == 'bsddb':
        make_bsddb(*args, **kwargs)
    elif cmd == "solrdump":
        import solr
        solr.generate_dump(*args, **kwargs)
    elif cmd == 'sitemaps':
        from sitemap import generate_sitemaps
        generate_sitemaps(*args, **kwargs)
    elif cmd == 'htmlindex':
        from sitemap import generate_html_index
        generate_html_index(*args, **kwargs)
    else:
        print >> sys.stderr, "Unknown command:", cmd

if __name__ == "__main__":
    main(sys.argv[1], sys.argv[2:])

########NEW FILE########
__FILENAME__ = mapreduce
"""Simple library to process large datasets using map-reduce.

This works as follows:

* Takes an iterator of key-value pairs as input
* Applies the map function for each key-value pair. The map function does the
  required processing to yield zero or more key-value pairs.
* The result of map are stored in the disk in to multiple files based on the 
  hash of the key. This makes sure the all the entries with same key goes to 
  the same file.
* Each of the file is sorted on key to group all the values of a key and the
  reduce function is applied for each key and its values. 
* The reduced key, value pairs are returned as an iterator.
"""
import sys
import itertools
import os
import subprocess
import logging
import gzip

logger = logging.getLogger("mapreduce")

class Task:
    """Abstraction of a map-reduce task.

    Each task should extend this class and implement map and reduce functions.
    """
    def __init__(self, tmpdir="mapreduce", filecount=100, hashfunc=None):
        self.tmpdir = tmpdir
        self.filecount = 100
        self.hashfunc = None

    def map(self, key, value):
        """Function to map given key-value pair into zero or more key-value pairs.
        
        The implementation should yield the key-value pairs.
        """
        raise NotImplementedError()

    def reduce(self, key, values):
        """Function to reduce given values.
        
        The implementation should return a key-value pair, with the reduced value.
        """
        raise NotImplementedError()
        
    def read(self):
        for line in sys.sydin:
            key, value = line.strip().split("\t", 1)
            yield key, value
            
    def map_all(self, records, disk):
        for key, value in records:
            for k, v in self.map(key, value):
                disk.write(k, v)
        disk.close()
                
    def reduce_all(self, records):
        for key, chunk in itertools.groupby(records, lambda record: record[0]):
            values = [value for key, value in chunk]
            yield self.reduce(key, values)

    def process(self, records):
        """Takes key-value pairs, applies map-reduce and returns the resultant key-value pairs.
        """
        # Map the record and write to disk
        disk = Disk(self.tmpdir, mode="w", hashfunc=self.hashfunc)
        self.map_all(records, disk)
        disk.close()
        
        # Read from the disk in the sorted order and reduce
        disk = Disk(self.tmpdir, mode="r", hashfunc=self.hashfunc)
        records = disk.read_semisorted()
        return self.reduce_all(records)
                
class Disk:
    """Map Reduce Disk to manage key values.
    
    The data is stored over multiple files based on the key. All records with same key will fall in the same file.
    """
    def __init__(self, dir, prefix="shard", filecount=100, hashfunc=None, mode="r"):
        self.dir = dir
        self.prefix = prefix
        self.hashfunc = hashfunc or (lambda key: hash(key))        
        self.buffersize = 1024 * 1024
        
        if not os.path.exists(dir):
            os.makedirs(dir)
        self.files = [self.openfile(i, mode) for i in range(filecount)]
        
    def openfile(self, index, mode):
        filename = "%s-%03d.txt.gz" % (self.prefix, index)
        path = os.path.join(self.dir, filename)
        return gzip.open(path, mode)

    def write(self, key, value):
        index = self.hashfunc(key) % len(self.files)
        f = self.files[index]
        f.write(key + "\t" + value + "\n")
    
    def close(self):
        for f in self.files:
            f.close()
            
    def read_semisorted(self):
        """Sorts each file in the disk and returns an iterator over the key-values in each file. 

        All the values with same key will come together as each file is sorted, but there is no guaranty on the global order of keys.
        """
        for f in self.files:
            cmd = "gzip -cd %s | sort -S1G" % f.name
            logger.info(cmd)
            p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
            for line in p.stdout:
                key, value = line.split("\t", 1)
                yield key, value
            status = p.wait()
            if status != 0:
                raise Exception("sort failed with status %d" % status)


########NEW FILE########
__FILENAME__ = sitemap
"""Library for generating sitemaps from Open Library dump.

Input for generating sitemaps is a tsv file with "path", "title", "created"
and "last_modified" columns. It is desirable that the file is sorted on 
"created" and "path".

http://www.archive.org/download/ol-sitemaps/sitemap-books-0001.xml.gz
http://www.archive.org/download/ol-sitemaps/sitemap-books-0001.xml.gz

http://www.archive.org/download/ol-sitemaps/sitindex-books.xml.gz
http://www.archive.org/download/ol-sitemaps/sitindex-authors.xml.gz
http://www.archive.org/download/ol-sitemaps/sitindex-works.xml.gz
http://www.archive.org/download/ol-sitemaps/sitindex-subjects.xml.gz
"""

import sys, os
import web
import datetime
from gzip import open as gzopen

from openlibrary.plugins.openlibrary.processors import urlsafe

t = web.template.Template

t_sitemap = t("""$def with (docs)
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
$for path, title, created, last_modified in docs:
    <url><loc>http://openlibrary.org$path</loc><lastmod>${last_modified}Z</lastmod></url>
</urlset>
""")

t_siteindex = t("""$def with (base_url, rows)
<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
$for filename, timestamp in rows:
    <sitemap><loc>$base_url/$filename</loc><lastmod>$timestamp</lastmod></sitemap>
</sitemapindex>
""")


t_html_layout = t("""$def with (page)
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="robots" content="noindex,follow" />
    <link href="/css/all.css" rel="stylesheet" type="text/css" />
    <title>$page.title</title>
</head>
<body id="edit">
<div id="background">
<div id="placement">
<div id="position">$:page</div>
</div>
</div>
</body></html>""")

t_html_sitemap = t("""$def with (back, docs)
$var title: Index
<p><a href="$back">&larr; Back to Index</a></p>
<ul>
$for path, title in docs:
    <li><a href="$path">$title</a></li>
</ul>
""")

def find_path(key, type, json):
    if type in ['/type/edition', '/type/work']:
        data = simplejson.loads(json)
        return key + '/' + urlsafe(data.get('title', 'untitled'))
    elif doc.type == '/type/author':
        data = simplejson.loads(json)
        return key + '/' + urlsafe(data.get('name', 'unnamed'))
    else:
        return doc.key

def gzwrite(path, data):
    f = gzopen(path, 'w')
    f.write(data)
    f.close()
    
def write_sitemaps(data, outdir, prefix):
    timestamp = datetime.datetime.utcnow().isoformat() + 'Z'
    
    # maximum permitted entries in one sitemap is 50K. 
    for i, rows in enumerate(web.group(data, 50000)):
        filename = "sitemap_%s_%04d.xml.gz" % (prefix, i)
        print >> sys.stderr, "generating", filename
        
        sitemap = web.safestr(t_sitemap(rows))
        
        path = os.path.join(outdir, filename)
        gzwrite(path, sitemap)
        yield filename, timestamp
        
def write_siteindex(data, outdir, prefix):
    rows = write_sitemaps(data, outdir, prefix)
    base_url = "http://openlibrary.org/static/sitemaps/"
    
    filename = "siteindex_%s.xml.gz" % prefix
    print >> sys.stderr, "generating", filename
    
    path = os.path.join(outdir, filename)
    siteindex = web.safestr(t_siteindex(base_url, rows))
    
    gzwrite(path, siteindex)
    
def parse_index_file(index_file):
    data = (line.strip().split("\t") for line in open(index_file))
    data = ([t[0], " ".join(t[1:-2]), t[-2], t[-1]] for t in data)    
    return data
    
def generate_sitemaps(index_file, outdir, prefix):
    data = parse_index_file(index_file)
    write_siteindex(data, outdir, prefix)
    
def mkdir_p(dir):
    if not os.path.exists(dir):
        os.makedirs(dir)
        
def write(path, data):
    print "writing", path
    mkdir_p(os.path.dirname(path))
    
    f = open(path, "w")
    f.write(data)
    f.close()
    
def dirindex(dir, back=".."):
    data = [(f, f) for f in sorted(os.listdir(dir))]
    index = t_html_layout(t_html_sitemap(back, data))
    
    path = dir + "/index.html"
    write(path, web.safestr(index))

def generate_html_index(index_file, outdir):
    data = parse_index_file(index_file)
    data = ((d[0], d[1]) for d in data)
    
    for i, chunk in enumerate(web.group(data, 1000)):
        back = ".."
        index = t_html_layout(t_html_sitemap(back, chunk))
        
        path = outdir + "/%02d/%05d.html" % (i/1000, i)
        write(path, web.safestr(index))

    for f in os.listdir(outdir):
        path = os.path.join(outdir, f)
        if os.path.isdir(path):
            dirindex(path)
    dirindex(outdir, back=".")

########NEW FILE########
__FILENAME__ = solr
"""Library to process edition, work and author records and emit (key, property, value) triples that can be combined later for solr indexing.
"""
import os, sys
import re
import web
import simplejson
import subprocess
import collections
import glob
import itertools

from dump import read_tsv, log

def subdict(d, properties):
    return dict((k, v) for k, v in d.iteritems() if k in set(properties))

def process_edition(doc):    
    properties = [
        'key',
        'isbn_10', 'isbn_13', 'lccn', 'oclc',
        'dewey_decimal_class', 'lc_classifications', 
        'publishers', 'publish_places', 'publish_date',
        'title', 'subtitle', 'languages', 'covers',
        'number_of_pages', 'pagination',
        'contributions'
    ]
    json = simplejson.dumps(subdict(doc, properties))
    return [(w['key'], 'edition', json) for w in doc.get('works', [])]

def fix_subjects(doc):
    """In some records, the subjects are references/text instead of string. This function fixes that."""
    def fix(s):
        if isinstance(s, dict):
            if 'value' in s:
                s = s['value']
            elif 'key' in s:
                s = s['key'].split("/")[-1].replace("_", " ")
        return s

    for name in ['subjects', 'subject_places', 'subject_people', 'subject_times']:
        if name in doc:
            doc[name] = [fix(s) for s in doc[name]]
    return doc
    
def get_subjects(doc):
    for s in doc.get('subjects', []):
        yield s, '/subjects/' + s.lower().replace(' ', '_')

    for s in doc.get('subject_places', []):
        yield s, '/subjects/place:' + s.lower().replace(' ', '_')

    for s in doc.get('subject_people', []):
        yield s, '/subjects/person:' + s.lower().replace(' ', '_')

    for s in doc.get('subject_times', []):
        yield s, '/subjects/time:' + s.lower().replace(' ', '_')    

def process_work(doc, author_db, redirect_db):
    doc = fix_subjects(doc)
    
    properties = [
        "title", "subtitle", "translated_titles", "other_titles",
        "subjects", "subject_places", "subject_people", "subject_times", "genres",
    ]
    yield doc['key'], "json", simplejson.dumps(subdict(doc, properties))
    
    authors = [a['author']['key'] for a in doc.get('authors', []) if 'author' in a and 'key' in a['author']]
    for akey in set(authors):
        akey = find_redirect(redirect_db, akey) or akey
        olid = akey.split("/")[-1]
        
        try:
            yield doc['key'], 'author', author_db[olid]
            yield akey, 'work', doc['key']
        except KeyError:
            print >> sys.stderr, "notfound", akey
    for name, key in get_subjects(doc):
        yield key, "name", name
        yield key, "work", doc['key']
        for a in authors:
            yield a, "subject", key
        
def process_author(doc):
    key = doc['key']
    properties = ["name", "personal_name", "alternate_names", "birth_date", "death_date", "date"]
    return [(key, 'json', simplejson.dumps(subdict(doc, properties)))]

class Writer:
    def __init__(self):
        self.files = {}
        
    def get_filename(self, key):
        if key.startswith("/authors/"):
            return "authors_%02d.txt" % (hash(key) % 8)
        elif key.startswith("/works/"):
            return "works_%02d.txt" % (hash(key) % 64)
        elif key.startswith("/subjects/place:"):
            return "places.txt"
        elif key.startswith("/subjects/person:"):
            return "people.txt"
        elif key.startswith("/subjects/time:"):
            return "times.txt"
        elif key.startswith("/subjects/"):
            return "subjects.txt"
        else:
            return "unexpected.txt"
            
    def get_file(self, key):
        filename = self.get_filename(key)
        if filename not in self.files:
            self.files[filename] = open('solrdump/' + filename, 'w', 5*1024*1024)
        return self.files[filename]
        
    def write(self, triples):
        tjoin = lambda *cols: "\t".join([web.safestr(c) for c in cols]) + "\n"
        for key, property, value in triples:
            self.get_file(key).write(tjoin(key, property, value))
            
    def close(self):
        for f in self.files.values():
            f.close()
        self.files.clear()
        
    def flush(self):
        for f in self.files.values():
            f.flush()
            
def process_author_dump(writer, authors_dump):
    import bsddb 
    db = bsddb.btopen('solrdump/authors.db', 'w', cachesize=1024*1024*1024)
    
    properties = ['key', 'name', 'alternate_names', 'personal_name']
    for type, key, revision, timestamp, json in read_tsv(authors_dump):
        author = simplejson.loads(json)
        
        olid = key.split("/")[-1]        
        db[olid] = simplejson.dumps(subdict(author, properties))
        
        writer.write(process_author(author))
    return db

def process_redirect_dump(writer, redirects_dump):
    import bsddb 
    db = bsddb.btopen('solrdump/redirects.db', 'w', cachesize=1024*1024*1024)

    for type, key, revision, timestamp, json in read_tsv(redirects_dump):
        d = simplejson.loads(json)
        if not key.startswith("/authors/") and not key.startswith("/works/"):
            continue

        location = d.get('location')
        if location:
            # Old redirects still start with /a/ instead of /authors/.
            location = location.replace("/a/", "/authors/")
            db[key] = location
    
    for key in db:
        if key.startswith("/works/"):
            redirect = find_redirect(db, key)
            if redirect:
                writer.write([(redirect, "redirect", key)])
            
    return db
            
def find_redirect(redirect_db, key):
    """Finds the redirection of the given key if any.
    
    When there is a redirection for the given key, the redirection is returned.
    
        >>> db = {'/authors/OL1A': '/authors/OL2A'}
        >>> find_redirect(db, '/authors/OL1A')
        '/authors/OL2A'
        
    If there is no redirection, same key is returned.
        
        >>> find_redirect(db, '/authors/OL3A')
        '/authors/OL3A'
        
    Multiple levels of redirections are followed.

        >>> db = {'/authors/OL1A': '/authors/OL2A', '/authors/OL2A': '/authors/OL3A'}
        >>> find_redirect(db, '/authors/OL1A')
        '/authors/OL3A'

    In case of cyclic redirections, None is returned.

        >>> db = {'/authors/OL1A': '/authors/OL2A', '/authors/OL2A': '/authors/OL1A'}
        >>> find_redirect(db, '/authors/OL1A')
    """
    for i in range(5):
        redirect = redirect_db.get(key)
        if not redirect:
            return key
        else:
            key = redirect
            
    return None
    
def process_work_dump(writer, works_dump, author_db, redirect_db):
    for type, key, revision, timestamp, json in read_tsv(works_dump):
        doc = simplejson.loads(json)
        writer.write(process_work(doc, author_db, redirect_db))
        
def process_edition_dump(writer, editions_dump):
    for type, key, revision, timestamp, json in read_tsv(editions_dump):
        doc = simplejson.loads(json)
        writer.write(process_edition(doc))
    
def generate_dump(editions_dump, works_dump, authors_dump, redirects_dump):
    pharse1_process_dumps(editions_dump, works_dump, authors_dump, redirects_dump)
    phase2_process_files()
    
def pharse1_process_dumps(editions_dump, works_dump, authors_dump, redirects_dump):    
    writer = Writer()
    if not os.path.exists("solrdump"):
        os.makedirs("solrdump")

    redirect_db = process_redirect_dump(writer, redirects_dump)
    author_db = process_author_dump(writer, authors_dump)    
    process_work_dump(writer, works_dump, author_db, redirect_db)
    author_db.close()
    author_db = None
    
    process_edition_dump(writer, editions_dump)
    writer.close()
    writer = None
    log("done")
    
def phase2_process_files():
    f = open("solrdump/solrdump_works.txt", "w", 5*1024*1024)
    
    for path in glob.glob("solrdump/works_*"):
        f.writelines("%s\t%s\n" % (key, simplejson.dumps(process_solr_work_record(key, d))) 
            for key, d in process_triples(path))
        
def process_work_triples(path):
    for key, work in process_triples(path):
        yield key, process_work_triples(key, work)

re_year = re.compile(r'(\d{4})$')
        
def process_solr_work_record(key, d):            
    editions = d.pop('edition', [])
    authors = d.pop('author', [])
    redirects = d.pop('redirect', [])    
    work = d.pop('json')[0]

    def basename(path):
        return path.split("/")[-1]

    def put(k, v):
        if v:
            r[k] = v
    
    def mput(k, v):
        if v is None:
            return
        elif isinstance(v, list):
            r[k].extend(v)
        else:
            r[k].append(v)
        
    r = collections.defaultdict(list)

    put("key", basename(key))
    put("title", work.get("title"))
    put("subtitle", work.get("subtitle"))
    
    r['edition_count'] = len(editions)
    
    for e in editions:
        mput("oclc", e.get("oclc_numbers"))
        mput("oclc", e.get("lccn"))
        mput("isbn", e.get("isbn_10"))
        mput("isbn", e.get("isbn_13"))
        
        mput("publisher", e.get("publishers"))
        mput("publish_place", e.get("publish_places"))
        
        mput("ia", e.get("ocaid"))
        mput("language", e.get("languages"))
        mput("number_of_pages", e.get("number_of_pages"))
        
        mput("contributor", e.get("contributions"))
        mput("publish_date", e.get("publish_date"))
        
    for a in authors:
        mput("author_name", a.get("name", ""))
        mput("author_key", basename(a['key']))
        mput("author_alternative_name", a.get("alternate_names"))
        
    r['ia_count'] = len(r['ia'])
    r['has_fulltext'] = bool(r['ia'])
    
    r['publish_year'] = [m.group(1) for m in [re_year.match(date) for date in r['publish_date']] if m]
    if r['publish_year']:
        r['first_publish_year'] = min(r['publish_year'])
        
    ## special processing
    # strip - from ISBNS
    r['isbn'] = [isbn.replace("-", "") for isbn in r['isbn']]
    
    # {"key": "/languages/eng"} -> "eng"
    r['language'] = [basename(lang['key']) for lang in r['language']]
    
    # push google scans to the end
    r['ia'] = [ia for ia in r['ia'] if not ia.startswith("goog")] + [ia for ia in r['ia'] if ia.startwith("goog")]

    r['author_facet'] =  [' '.join(v) for v in zip(r['author_keys'], r['author_names'])]
    return r

def process_triples(path):
    """Takes a file with triples, sort it using first column and groups it by first column.
    """
    print "processing triples from", path
    cmd = ["sort", path]
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
    for key, chunk in itertools.groupby(read_tsv(p.stdout), lambda t: t[0]):
        d = collections.defaultdict(list)
        for k, name, value in chunk:
            if name in ['json', 'edition', 'author']:
                value = simplejson.loads(value)
            d[name].append(value)
        yield key, d
    
if __name__ == "__main__":
    import doctest
    doctest.testmod()
    
########NEW FILE########
__FILENAME__ = mock_couchdb
"""Library to mock python couchdb library.
"""
import uuid
from couchdb.client import View

class Database:
    def __init__(self, name=None):
        self.docs = {}
        self._views = {}
        self.name = name or "mock-couchdb"
        
        self._views["_all_docs"] = _View(self._map_all_docs)
        
    def _map_all_docs(self, doc):
        yield doc['_id'], {"rev": doc['_rev']}
            
    def compact(self, ddoc=None):
        return True
        
    def save(self, doc, **options):
        doc = dict(doc)
        
        self._add_rev(doc)
        self.docs[doc['_id']] = doc
        
        for view in self._views.values():
            view.add_doc(doc)
            
        if doc['_id'].startswith("_design/"):
            self._process_design_doc(doc)
        
        return (doc['_id'], doc['_rev'])
        
    def delete(self, doc):
        del self.docs[doc['_id']]
        
        for view in self._views.values():
            view.remove_doc(doc)
    
    def __iter__(self):
        return iter(self.docs)
        
    def _process_design_doc(self, doc):
        id = doc['_id']
        
        # remove old views
        self._views = dict((k, v) for k, v in self._views.items() if not k.startswith(id + "/"))
        
        for name, d in doc.get('views', {}).items():
            viewname = id.split("/")[1] + "/" + name
            if "map" in d:
                map = self._compile(d['map'])
                
                view = _View(map)
                for doc in self.docs.values():
                    view.add_doc(doc)
                
                self._views[viewname] = view
        
    def _compile(self, code):
        globals_ = {}
        _log = lambda *a: None
        
        exec code in {'log': _log}, globals_
        
        err = {'error': {
            'id': 'map_compilation_error',
            'reason': 'string must eval to a function '
                      '(ex: "def(doc): return 1")'
        }}
        if len(globals_) != 1:
            return err
        
        return globals_.values()[0]
        
    def __getitem__(self, key):
        return dict(self.docs[key])
    
    def __setitem__(self, key, doc):
        doc['_id'] = key
        self.save(doc)
        
    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError:
            return default
        
    def update(self, docs):
        for doc in docs:
            if doc.get("_deleted") == True:
                self.delete(doc)
            else:
                self.save(doc)
                
    def view(self, name, wrapper=None, **options):
        rows = self._views[name].get_rows()
        view = MockView(self, rows, wrapper)
        return view(**options)
    
    def _view(self, name):
        if name == "_all_docs":
            rows = [{"id": doc["_id"], "key": doc["_id"], "value": {"rev": doc["_rev"]}} for doc in docs]
            rows.sort(key=lambda row: row['key'])
            return rows
    
    def _ensure_id(self, doc):
        if '_id' not in doc:
            doc['_id'] = self._uuid()
        return doc
    
    def _uuid(self):
        return str(uuid.uuid1().replace("-", ""))
    
    def _add_rev(self, doc):
        self._ensure_id(doc)
        
        if '_rev' in doc:
            rev = int(doc['_rev']) + 1
        else:
            rev = 1
        doc['_rev'] = str(rev)
        return doc

class _View:
    def __init__(self, map):
        self.map = map
        self.rows = []        
        
    def add_doc(self, doc):
        id = doc['_id']
        self.rows = [row for row in self.rows if row['id'] != id]
        self.rows += [{"id": id, "key": key, "value": value} for key, value in self.map(doc)]
        
    def remove_doc(self, doc):
        id = doc['_id']
        self.rows = [row for row in self.rows if row['id'] != id]
        
    def get_rows(self):
        return sorted(self.rows, key=lambda row: row['key'])
            
class MockView(View):
    def __init__(self, db, rows, wrapper):
        self.db = db
        self.wrapper = wrapper
        self.rows = rows
    
    def _exec(self, options):
        offset = 0
        rows = self.rows[:]
        
        if options.get('startkey_docid') == "0099":
            raise StopIteration
            
        if "keys" in options:
          keys = set(options['keys'])
          rows = [row for row in self.rows if row['key'] in keys]
          offset = None
        elif "key" in options:
            key = options['key']
            try:
                keys = [row['key'] for row in rows]
                offset = keys.index(key)
            except ValueError:
                offset = len(keys)
            rows = [row for row in self.rows if row['key'] == key]
            
        if "startkey_docid" in options:
            start_docid = options['startkey_docid']
            index = [row['id'] for row in rows].index(start_docid)
            rows = rows[index:]
            offset += index
                
        if "skip" in options:
            skip = options['skip']
            offset += skip
            rows = rows[skip:]
            
        if "limit" in options:
            limit = options['limit']
            rows = rows[:limit]
            
        if options.get("include_docs") == True:
            for row in rows:
                row["doc"] = self.db[row['id']]
            
        result = {
            "total_rows": len(self.rows),
            "rows": rows,
        }
        if offset is not None:
            result['offset'] = offset
        return result
########NEW FILE########
__FILENAME__ = mock_ia
"""Mock of openlibrary.core.ia module.
"""

from openlibrary.core import ia
from _pytest.monkeypatch import monkeypatch

def pytest_funcarg__mock_ia(request):
    """py.test funcarg to mock openlibrary.core.ia module.
    
        from openlibrary.core import ia
        
        def test_ia(mock_ia):
            assert ia.get_meta_xml("foo") == {}
            
            mock_ia.set_meta_xml("foo", {"collection": ["a", "b"]})
            assert ia.get_meta_xml("foo") == {"collection": ["a", "b"]}
    """
    m = monkeypatch()
    request.addfinalizer(m.undo)
    
    metaxml = {}
    
    class IA:
        def set_meta_xml(self, itemid, meta):
            metaxml[itemid] = meta
        
        def get_meta_xml(self, itemid):
            return metaxml.get(itemid, {})

    mock_ia = IA()
    m.setattr(ia, "get_meta_xml", ia.get_meta_xml)
    
    return mock_ia

########NEW FILE########
__FILENAME__ = mock_infobase
"""Simple implementation of mock infogami site to use in testing.
"""
import datetime
import web
import glob
import simplejson

from infogami.infobase import client, common, account, config as infobase_config
from infogami import config

key_patterns = {
    'work': '/works/OL%dW',
    'edition': '/books/OL%dM',
    'author': '/authors/OL%dA',
}

class MockSite:
    def __init__(self):
        self.reset()
        
    def reset(self):
        self.store = MockStore()
        if config.get('infobase') is None:
            config.infobase = {}
            
        infobase_config.secret_key = "foobar"
        config.infobase['secret_key'] = "foobar"
        
        self.account_manager = self.create_account_manager()
        
        self._cache = {}
        self.docs = {}
        self.changesets = []
        self.index = []
        self.keys = {'work': 0, 'author': 0, 'edition': 0}
        
    def create_account_manager(self):
        # Hack to use the accounts stuff from Infogami
        infobase_config.user_root = "/people"
        
        store = web.storage(store=self.store)
        site = web.storage(store=store, save_many=self.save_many)
        return account.AccountManager(site, config.infobase['secret_key'])
                
    def save(self, query, comment=None, action=None, data=None, timestamp=None):
        timestamp = timestamp or datetime.datetime.utcnow()
        
    def _save_doc(self, query, timestamp):
        key = query['key']
        
        if key in self.docs:
            rev = self.docs[key]['revision'] + 1
        else:
            rev = 1
            
        doc = dict(query)
        doc['revision'] = rev
        doc['last_modified'] = {
            "type": "/type/datetime",
            "value": timestamp.isoformat()
        }
        
        self.docs[key] = doc

        return doc
         
    def save(self, query, comment=None, action=None, data=None, timestamp=None):
        timestamp = timestamp or datetime.datetime.utcnow()
        
        doc = self._save_doc(query, timestamp)
       
        changes = [{"key": doc['key'], "revision": doc['revision']}]
        changeset = self._make_changeset(timestamp=timestamp, kind=action, comment=comment, data=data, changes=changes)
        self.changesets.append(changeset)
        
        self.reindex(doc)

    def save_many(self, query, comment=None,  action=None, data=None, timestamp=None, author=None):
        timestamp = timestamp or datetime.datetime.utcnow()
        docs = [self._save_doc(doc, timestamp) for doc in query]
        
        if author:
            author = {"key": author.key}

        changes = [{"key": doc['key'], "revision": doc['revision']} for doc in docs]
        changeset = self._make_changeset(timestamp=timestamp, kind=action, comment=comment, data=data, changes=changes, author=author)

        self.changesets.append(changeset)
        for doc in docs:
            self.reindex(doc)

    def quicksave(self, key, type="/type/object", **kw):
        """Handy utility to save an object with less code and get the saved object as return value.
        
            foo = mock_site.quicksave("/books/OL1M", "/type/edition", title="Foo")
        """
        query = {
            "key": key,
            "type": {"key": type},
        }
        query.update(kw)
        self.save(query)
        return self.get(key)

    def _make_changeset(self, timestamp, kind, comment, data, changes, author=None):
        id = len(self.changesets)
        return {
            "id": id,
            "kind": kind or "update",
            "comment": comment,
            "data": data,
            "changes": changes,
            "timestamp": timestamp.isoformat(),

            "author": author,
            "ip": "127.0.0.1",
            "bot": False
        }
        
    def get(self, key, revision=None):
        data = self.docs.get(key)
        data = data and web.storage(common.parse_query(data))
        return data and client.create_thing(self, key, self._process_dict(data))

    def _process(self, value):
        if isinstance(value, list):
            return [self._process(v) for v in value]
        elif isinstance(value, dict):
            d = {}
            for k, v in value.items():
                d[k] = self._process(v)
            return client.create_thing(self, d.get('key'), d)
        elif isinstance(value, common.Reference):
            return client.create_thing(self, unicode(value), None)
        else:
            return value

    def _process_dict(self, data):
        d = {}
        for k, v in data.items():
            d[k] = self._process(v)
        return d
        
    def get_many(self, keys):
        return [self.get(k) for k in keys if k in self.docs]
    
    def things(self, query):
        limit = query.pop('limit', 100)
        offset = query.pop('offset', 0)
        
        keys = set(self.docs.keys())

        for k, v in query.items():
            keys = set(k for k in self.filter_index(self.index, k, v) if k in keys)
            
        keys = sorted(keys)
        return keys[offset:offset+limit]
                    
    def filter_index(self, index, name, value):
        operations = {
            "~": lambda i, value: isinstance(i.value, basestring) and i.value.startswith(web.rstrips(value, "*")),
            "<": lambda i, value: i.value < value,
            ">": lambda i, value: i.value > value,
            "!": lambda i, value: i.value != value,
            "=": lambda i, value: i.value == value,
        }
        pattern = ".*([%s])$" % "".join(operations)
        rx = web.re_compile(pattern)
        m = rx.match(name)
        
        if m: 
            op = m.group(1)
            name = name[:-1]
        else:
            op = "="
            
        f = operations[op]

        if isinstance(value, list): # Match any of the elements in value if it's a list
            for i in index:
                if i.name == name and any(f(i, v) for v in value):
                    yield i.key
        else: # Otherwise just match directly
            for i in index:
                if i.name == name and f(i, value):
                    yield i.key
                    
    def compute_index(self, doc):
        key = doc['key']
        index = common.flatten_dict(doc)
        
        for k, v in index:
            # for handling last_modified.value
            if k.endswith(".value"):
                k = web.rstrips(k, ".value")
                
            if k.endswith(".key"):
                yield web.storage(key=key, datatype="ref", name=web.rstrips(k, ".key"), value=v)
            elif isinstance(v, basestring):
                yield web.storage(key=key, datatype="str", name=k, value=v)
            elif isinstance(v, int):
                yield web.storage(key=key, datatype="int", name=k, value=v)
        
    def reindex(self, doc):
        self.index = [i for i in self.index if i.key != doc['key']]
        self.index.extend(self.compute_index(doc))
        
    def find_user_by_email(self, email):
        return None
        
    def versions(self, q):
        return []
        
    def _get_backreferences(self, doc):
        return {}
        
    def _load(self, key, revision=None):
        doc = self.get(key, revision=revision)
        data = doc.dict()
        data = web.storage(common.parse_query(data))
        return self._process_dict(data)
        
    def new(self, key, data=None):
        """Creates a new thing in memory.
        """
        data = common.parse_query(data)
        data = self._process_dict(data or {})
        return client.create_thing(self, key, data)

    def new_key(self, type):
        assert type.startswith('/type/')
        t = type[6:]
        self.keys[t] += 1
        return key_patterns[t] % self.keys[t]
        
    def register(self, username, displayname, email, password):
        try:
            self.account_manager.register(
                username=username, 
                email=email, 
                password=password, 
                data={"displayname": displayname})
        except common.InfobaseException, e:
            raise client.ClientException("bad_data", str(e))
            
    def activate_account(self, username):
        try:
            self.account_manager.activate(username=username)
        except common.InfobaseException, e:
            raise client.ClientException(str(e))
            
    def update_account(self, username, **kw):
        status = self.account_manager.update(username, **kw)
        if status != "ok":
            raise client.ClientException("bad_data", "Account activation failed.")
            
    def login(self, username, password):
        status = self.account_manager.login(username, password)
        if status == "ok":
            self.account_manager.set_auth_token("/people/" + username)
        else:
            d = {"code": status}
            raise client.ClientException("bad_data", msg="Login failed", json=simplejson.dumps(d))
            
    def find_account(self, username=None, email=None):
        if username is not None:
            return self.store.get("account/" + username)
        else:
            try:
                return self.store.values(type="account", name="email", value=email)[0]
            except IndexError:
                return None

    def get_user(self):
        auth_token = web.ctx.get("infobase_auth_token", "")
        
        if auth_token:
            try:
                user_key, login_time, digest = auth_token.split(',')
            except ValueError:
                return
                
            a = self.account_manager
            if a._check_salted_hash(a.secret_key, user_key + "," + login_time, digest):
                return self.get(user_key)
    
class MockConnection:
    def get_auth_token(self):
        return web.ctx.infobase_auth_token
        
    def set_auth_token(self, token):
        web.ctx.infobase_auth_token = token
        
class MockStore(dict):
    def __setitem__(self, key, doc):
        doc['_key'] = key
        dict.__setitem__(self, key, doc)
        
    put = __setitem__
        
    def put_many(self, docs):
        self.update((doc['_key'], doc) for doc in docs)
        
    def _query(self, type=None, name=None, value=None, limit=100, offset=0):
        for doc in dict.values(self):
            if type is not None and doc.get("type", "") != type:
                continue
            if name is not None and doc.get(name) != value:
                continue
            
            yield doc

    def keys(self, **kw):
        return [doc['_key'] for doc in self._query(**kw)]

    def values(self, **kw):
        return [doc for doc in self._query(**kw)]
        
    def items(self, **kw):
        return [(doc["_key"], doc) for doc in self._query(**kw)]
        
        
def pytest_funcarg__mock_site(request):
    """mock_site funcarg.
    
    Creates a mock site, assigns it to web.ctx.site and returns it.
    """
    def read_types():
        for path in glob.glob("openlibrary/plugins/openlibrary/types/*.type"):
            text = open(path).read()
            doc = eval(text, dict(true=True, false=False))
            if isinstance(doc, list):
                for d in doc:
                    yield d
            else:
                yield doc
    
    def setup_models():
        from openlibrary.plugins.upstream import models
        models.setup()

    site = MockSite()

    setup_models()
    for doc in read_types():
        site.save(doc)

    old_ctx = dict(web.ctx)
    web.ctx.clear()
    web.ctx.site = site
    web.ctx.conn = MockConnection()
    web.ctx.env = web.ctx.environ = web.storage()
    web.ctx.headers = []
    
    def undo():
        web.ctx.clear()
        web.ctx.update(old_ctx)
    
    request.addfinalizer(undo)
    
    return site

########NEW FILE########
__FILENAME__ = mock_memcache
"""Library to mock memcache functionality.
"""
from _pytest.monkeypatch import monkeypatch
import memcache

class Client:
    """Mock memcache client."""
    def __init__(self, servers=[]):
        self.servers = servers
        self.cache = {}
        
    def set(self, key, value):
        self.cache[key] = value
        
    def get(self, key):
        return self.cache.get(key)
        
    def add(self, key, value):
        if key not in self.cache:
            self.cache[key] = value
            return True
        else:
            return False
        
    def delete(self, key):
        try:
            del self.cache[key]
        except KeyError:
            pass
            
def pytest_funcarg__mock_memcache(request):
    """This patches all the existing memcache connections to use mock memcache instance.
    """
    m = monkeypatch()
    request.addfinalizer(m.undo)
    
    mock_memcache = Client()
    
    
    def proxy(name):
        method = getattr(mock_memcache, name)
        def f(self, *a, **kw):
            return method(*a, **kw)
        return f
    
    m.setattr(memcache.Client, "get", proxy("get"))
    m.setattr(memcache.Client, "set", proxy("set"))
    m.setattr(memcache.Client, "add", proxy("add"))
    
    return mock_memcache
    

########NEW FILE########
__FILENAME__ = mock_ol
import os
import re
import web
from infogami import config
from infogami.infobase import client
from infogami.utils import delegate

from openlibrary.plugins import ol_infobase

from mock_infobase import pytest_funcarg__mock_site, MockConnection
from _pytest.monkeypatch import pytest_funcarg__monkeypatch

def pytest_funcarg__ol(request):
    """ol funcarg for py.test tests.

    The ol objects exposes the following:
    
        * ol.browser(): web.py browser object that works with OL webapp
        * ol.site: mock site (also accessible from web.ctx.site)
        * ol.sentmail: Last mail sent
    """
    return OL(request)
    
@web.memoize
def load_plugins():
    config.plugin_path = ["openlibrary.plugins", ""]
    config.plugins = ["openlibrary", "worksearch", "upstream", "admin"]
    
    delegate._load()
    
class EMail(web.storage):
    def extract_links(self):
        """Extracts link from the email message."""
        return re.findall(r"http://[^\s]*", self.message)
        
class OLBrowser(web.AppBrowser):
    def get_text(self, e=None, name=None, **kw):
        if name or kw:
            e = self.get_soup().find(name=name, **kw)
        return web.AppBrowser.get_text(self, e)        

class OL:
    """Mock OL object for all tests.
    """
    def __init__(self, request):
        self.request = request
        
        self.monkeypatch = pytest_funcarg__monkeypatch(request)
        self.site = pytest_funcarg__mock_site(request)
                
        self.monkeypatch.setattr(ol_infobase, "init_plugin", lambda: None)
        
        self._load_plugins(request)
        self._mock_sendmail(request)
        
        self.setup_config()
        
    def browser(self):
        return OLBrowser(delegate.app)
        
    def setup_config(self):
        config.from_address = "Open Library <noreply@openlibrary.org>"
        
    def _load_plugins(self, request):
        def create_site():
            web.ctx.conn = MockConnection()
            
            if web.ctx.get('env'):
                auth_token = web.cookies().get(config.login_cookie_name)
                web.ctx.conn.set_auth_token(auth_token)
            return self.site
            
        self.monkeypatch.setattr(delegate, "create_site", create_site)
        
        load_plugins()
        
    def _mock_sendmail(self, request):
        self.sentmail = None

        def sendmail(from_address, to_address, subject, message, headers={}, **kw):
            self.sentmail = EMail(kw, 
                from_address=from_address,
                to_address=to_address,
                subject=subject,
                message=message,
                headers=headers)
                
        self.monkeypatch.setattr(web, "sendmail", sendmail)

########NEW FILE########
__FILENAME__ = test_mock_couchdb
from ..mock_couchdb import Database

class TestMockCouchDB:
    def _add_design_doc(self, db):
        db.save({
            "_id": "_design/names",
            "views": {
                "names": {
                    "map": "" +  
                        "def f(doc):\n" + 
                        "    if 'name' in doc:\n"
                        "        yield doc['name'], None"
                }
            }
        })

    def test_save(self):
        db = Database()
        assert db.get("foo") is None
        
        db.save({"_id": "foo", "name": "foo"})
        
        assert db.get("foo") == {
            "_id": "foo",
            "_rev": "1",
            "name": "foo"
        }
        
    def _view(self, db, name, **options):
        result = db.view(name, wrapper=dict, **options)
        return {
            "total_rows": result.total_rows,
            "offset": result.offset,
            "rows": result.rows
        }

    def test_all_docs(self):
        db = Database()
        assert self._view(db, "_all_docs") == {
            "total_rows": 0,
            "offset": 0,
            "rows": []
        }
        
        for id in "abc":
            db.save({"_id": id})
        
        assert self._view(db, "_all_docs") == {
            "total_rows": 3,
            "offset": 0,
            "rows": [
                {"id": "a", "key": "a", "value": {"rev": "1"}},
                {"id": "b", "key": "b", "value": {"rev": "1"}},
                {"id": "c", "key": "c", "value": {"rev": "1"}},
            ]
        }
        
    def test_view(self):
        db = Database()
        self._add_design_doc(db)
        
        db.save({"_id": "a", "name": "a"})
        db.save({"_id": "b", "name": "b"})
        db.save({"_id": "c", "name": "c"})
        
        assert self._view(db, "names/names") == {
            "total_rows": 3,
            "offset": 0,
            "rows": [
                {"id": "a", "key": "a", "value": None},
                {"id": "b", "key": "b", "value": None},
                {"id": "c", "key": "c", "value": None},
            ]
        }

        assert self._view(db, "names/names", key='b') == {
            "total_rows": 3,
            "offset": 1,
            "rows": [
                {"id": "b", "key": "b", "value": None},
            ]
        }
        
    def test_delete(self):
        db = Database()
        
        db.save({"_id": "a", "name": "a"})
        db.delete({"_id": "a", "_rev": 1})
        
        assert db.get("a") == None
########NEW FILE########
__FILENAME__ = test_mock_infobase
import datetime
from openlibrary.mocks.mock_infobase import MockSite


class TestMockSite:
    def test_new_key(self, mock_site):
        ekey = mock_site.new_key('/type/edition')
        assert ekey == '/books/OL1M'
        ekey = mock_site.new_key('/type/edition')
        assert ekey == '/books/OL2M'

        wkey = mock_site.new_key('/type/work')
        assert wkey == '/works/OL1W'
        wkey = mock_site.new_key('/type/work')
        assert wkey == '/works/OL2W'

        akey = mock_site.new_key('/type/author')
        assert akey == '/authors/OL1A'
        akey = mock_site.new_key('/type/author')
        assert akey == '/authors/OL2A'

    def test_get(self, mock_site):
        doc = {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "title": "The Test Book"
        }
        timestamp = datetime.datetime(2010, 1, 2, 3, 4, 5)
        
        mock_site.save(doc, timestamp=timestamp)
        
        assert mock_site.get("/books/OL1M").dict() == {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "title": "The Test Book",
            "revision": 1,
            "last_modified": {
                "type": "/type/datetime",
                "value": "2010-01-02T03:04:05"
            }
        }
        assert mock_site.get("/books/OL1M").__class__.__name__ == "Edition"

    def test_query(self, mock_site):
        doc = {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "title": "The Test Book",
            "subjects": ["love", "san_francisco"]
        }
        timestamp = datetime.datetime(2010, 1, 2, 3, 4, 5)

        mock_site.reset()
        mock_site.save(doc, timestamp=timestamp)
        
        for i in mock_site.index:
            print dict(i)
            
        assert mock_site.things({"type": "/type/edition"}) == ["/books/OL1M"]
        assert mock_site.things({"type": "/type/work"}) == []
        
        assert mock_site.things({"type": "/type/edition", "subjects": "love"}) == ["/books/OL1M"]
        assert mock_site.things({"type": "/type/edition", "subjects": "hate"}) == []

        assert mock_site.things({"key~": "/books/*"}) == ["/books/OL1M"]
        assert mock_site.things({"key~": "/works/*"}) == []
        
        assert mock_site.things({"last_modified>": "2010-01-01"}) == ["/books/OL1M"]
        assert mock_site.things({"last_modified>": "2010-01-03"}) == []
        
    def test_work_authors(self, mock_site):
        a2 = mock_site.quicksave("/authors/OL2A", "/type/author", name="A2")
        work = mock_site.quicksave("/works/OL1W", "/type/work", title="Foo", authors=[{"author": {"key": "/authors/OL2A"}}])
        book = mock_site.quicksave("/books/OL1M", "/type/edition", title="Foo", works=[{"key": "/works/OL1W"}])
         
        w = book.works[0]
        
        assert w.dict() == work.dict()
        
        a = w.authors[0].author
        assert a.dict() == a2.dict()
        
        
        assert a.key == "/authors/OL2A"
        assert a.type.key == "/type/author"
        assert a.name == "A2"
        
        assert [a.type.key for a in work.get_authors()] == ['/type/author']
        assert [a.type.key for a in work.get_authors()] == ['/type/author']
        

########NEW FILE########
__FILENAME__ = test_mock_memcache
from .. import mock_memcache
import memcache

class Test_mock_memcache:
    def test_set(self):
        m = mock_memcache.Client([])
        
        m.set("a", 1)
        assert m.get("a") == 1
        
        m.set("a", "foo")
        assert m.get("a") == "foo"

        m.set("a", ["foo", "bar"])
        assert m.get("a") == ["foo", "bar"]
        
    def test_add(self):
        m = mock_memcache.Client([])
        
        assert m.add("a", 1) == True
        assert m.get("a") == 1
        
        assert m.add("a", 2) == False

mc = memcache.Client(servers=[])

def test_mock_memcache_func_arg(mock_memcache):
    mc.set("a", 1)
    assert mc.get("a") == 1
########NEW FILE########
__FILENAME__ = events
"""Infobase event hooks for Open Library.

Triggers and handles various events from Infobase. All the events are triggered using eventer.

List of events:

    * infobase.all: Triggered for any change in Infobase. The infobase event object is passed as argument.
    * infobase.edit: Triggered for edits. Changeset is passed as argument.
"""

import logging
import web
import eventer
from infogami.infobase import config, server
from openlibrary.utils import olmemcache

logger = logging.getLogger("openlibrary.olbase")

def setup():
    setup_event_listener()
    
def setup_event_listener():
    logger.info("setting up infobase events for Open Library")
    
    ol = server.get_site('openlibrary.org')
    ib = server._infobase

    # Convert infobase event into generic eventer event
    ib.add_event_listener(lambda event: eventer.trigger("infobase.all", event))
    
@eventer.bind("infobase.all")
def trigger_subevents(event):
    """Trigger infobase.edit event for edits.
    """
    if event.name in ['save', 'save_many']:
        changeset = event.data['changeset']

        author = changeset['author'] or changeset['ip']
        keys = [c['key'] for c in changeset['changes']]
        logger.info("Edit by %s, changeset_id=%s, changes=%s", author, changeset["id"], keys)
        
        eventer.trigger("infobase.edit", changeset)
    
@eventer.bind("infobase.edit")
def invalidate_memcache(changeset):
    """Invalidate memcache entries effected by this change.
    """
    memcache_client = get_memcache()
    if memcache_client:
        keys = MemcacheInvalidater().find_keys(changeset)
        if keys:
            logger.info("invalidating %s", keys)
            memcache_client.delete_multi(keys)

class MemcacheInvalidater:
    """Class to find keys to invalidate from memcache on edit.
    """
    def find_keys(self, changeset):
        """Returns keys for the effected entries by this change.
        """
        methods = [
            self.find_data,
            self.find_lists,
            self.find_edition_counts,
            self.find_libraries
        ]
        
        keys = set()        
        for m in methods:
            keys.update(m(changeset))
        return list(keys)
        
    def find_data(self, changeset):
        """Returns the data entries effected by this change.
        
        The data entry stores the history, lists and edition_count of a page.
        """
        return ["d" + c['key'] for c in changeset['changes']]
        
    def find_lists(self, changeset):
        """Returns the list entires effected by this change.
        
        When a list is modified, the data of the user and the data of each
        seed are invalidated.
        """
        docs = changeset['docs'] + changeset['old_docs']
        rx = web.re_compile("(/people/[^/]*)/lists/OL\d+L")
        for doc in docs:
            match = doc and rx.match(doc['key'])
            if match:
                yield "d" + match.group(1) # d/users/foo
                for seed in doc.get('seeds', []):
                    yield "d" + self.seed_to_key(seed)
        
    def find_edition_counts(self, changeset):
        """Returns the edition_count entries effected by this change."""
        docs = changeset['docs'] + changeset['old_docs']
        return set(k for doc in docs 
                     for k in self.find_edition_counts_for_doc(doc))
    
    def find_edition_counts_for_doc(self, doc):
        """Returns the memcache keys to be invalided for edition_counts effected by editing this doc.
        """
        if doc and doc['type']['key'] == '/type/edition':
            return ["d" + w['key'] for w in doc.get("works", [])]
        else:
            return []
            
    def find_libraries(self, changeset):
        """When any of the library page is changed, invalidate all library entries.
        """
        if any(c['key'].startswith("/libraries/") for c in changeset['changes']):
            return ['inlibrary.libraries-hash', 'inlibrary.libraries']
        else:
            return []
            
    def seed_to_key(self, seed):
        """Converts seed to key.
        
            >>> seed_to_key({"key": "/books/OL1M"})
            "/books/OL1M"
            >>> seed_to_key("subject:love")
            "/subjects/love"
            >>> seed_to_key("place:san_francisco")
            "/subjects/place:san_francisco"
        """
        if isinstance(seed, dict):
            return seed['key']
        elif seed.startswith("subject:"):
            return "/subjects/" + seed[len("subject:"):]
        else:
            return "/subjects/" + seed    
    
@web.memoize
def get_memcache():
    """Returns memcache client created from infobase configuration.
    """
    cache = config.get("cache", {})
    if cache.get("type") == "memcache":
        return olmemcache.Client(cache['servers'])

########NEW FILE########
__FILENAME__ = test_events
from .. import events

class TestMemcacheInvalidater:
    def test_seed_to_key(self):
        m = events.MemcacheInvalidater()        
        assert m.seed_to_key({"key": "/books/OL1M"}) == "/books/OL1M"
        assert m.seed_to_key("subject:love") == "/subjects/love"
        assert m.seed_to_key("place:san_francisco") == "/subjects/place:san_francisco"
        assert m.seed_to_key("person:mark_twain") == "/subjects/person:mark_twain"
        assert m.seed_to_key("time:2000") == "/subjects/time:2000"
        
    def test_find_lists(self):
        changeset = {
            "changes": [
                {"key": "/people/anand/lists/OL1L", "revision": 1}
            ],
            "old_docs": [None],
            "docs": [{
                "key": "/people/anand/lists/OL1L",
                "type": {"key": "/type/list"},
                "revision": 1,
                "seeds": [
                    {"key": "/books/OL1M"}, 
                    "subject:love"
                ]
            }]
        }
        m = events.MemcacheInvalidater()
        assert sorted(m.find_lists(changeset)) == ["d/books/OL1M", "d/people/anand", "d/subjects/love"]
                
    def test_find_lists2(self):
        changeset = {
            "changes": [
                {"key": "/people/anand/lists/OL1L", "revision": 2}
            ],
            "old_docs": [{
                "key": "/people/anand/lists/OL1L",
                "type": {"key": "/type/list"},
                "revision": 1,
                "seeds": [
                    {"key": "/books/OL1M"}, 
                    "subject:love"
                ]
            }],
            "docs": [{
                "key": "/people/anand/lists/OL1L",
                "type": {"key": "/type/list"},
                "revision": 2,
                "seeds": [
                    {"key": "/authors/OL1A"}, 
                    "subject:love",
                    "place:san_francisco"
                ]            
            }]
        }
        
        m = events.MemcacheInvalidater()
        keys = sorted(set(m.find_lists(changeset)))
        assert keys == [
            "d/authors/OL1A", 
            "d/books/OL1M", 
            "d/people/anand",
            "d/subjects/love", 
            "d/subjects/place:san_francisco"
        ]
        
    def test_edition_count_for_doc(self):
        m = events.MemcacheInvalidater()
        
        assert m.find_edition_counts_for_doc(None) == []

        doc = {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "works": [{"key": "/works/OL1W"}]
        }
        assert m.find_edition_counts_for_doc(doc) == ["d/works/OL1W"]
        
    def test_find_keys(self):
        m = events.MemcacheInvalidater()
        
        changeset = {
            "changes": [
                {"key": "/sandbox", "revision": 1}
            ],
            "old_docs": [None],
            "docs": [{
                "key": "/sandbox",
                "type": {"key": "/type/page"},
                "revision": 1,
                "title": "Sandbox"
            }]
        }
        m.find_keys(changeset) == ["d/sandbox"]
########NEW FILE########
__FILENAME__ = test_ol_infobase
from openlibrary.plugins.ol_infobase import OLIndexer

class TestOLIndexer:
    def test_normalize_isbn(self):
        indexer = OLIndexer()
        assert indexer.normalize_isbn("123456789X") == "123456789X"
        assert indexer.normalize_isbn("123-456-789-X") == "123456789X"
        assert indexer.normalize_isbn("123-456-789-X ") == "123456789X"
        
    def test_expand_isbns(self):
        indexer = OLIndexer()
        assert indexer.expand_isbns([]) == []
        assert indexer.expand_isbns(["123456789X"]) == ["123456789X", "9781234567897"]
        assert indexer.expand_isbns(["9781234567897"]) == ["123456789X", "9781234567897"]
        assert indexer.expand_isbns(["123456789X", "9781234567897"]) == ["123456789X", "9781234567897"]

########NEW FILE########
__FILENAME__ = code
"""Plugin to provide admin interface.
"""
import os
import sys
import web
import subprocess
import datetime
import urllib, urllib2
import traceback
import logging

import couchdb
import yaml

from infogami import config
from infogami.utils import delegate
from infogami.utils.view import render, public
from infogami.utils.context import context

from infogami.utils.view import add_flash_message
import openlibrary
from openlibrary.core import admin as admin_stats
from openlibrary.plugins.upstream import forms
from openlibrary.plugins.upstream.account import send_forgot_password_email
from openlibrary import accounts
from openlibrary.core import helpers as h

from openlibrary.plugins.admin import services, support, tasks, inspect_thing

logger = logging.getLogger("openlibrary.admin")


def render_template(name, *a, **kw):
    if "." in name:
        name = name.rsplit(".", 1)[0]
    return render[name](*a, **kw)

admin_tasks = []

def register_admin_page(path, cls, label=None, visible=True):
    label = label or cls.__name__
    t = web.storage(path=path, cls=cls, label=label, visible=visible)
    admin_tasks.append(t)

class admin(delegate.page):
    path = "/admin(?:/.*)?"
    
    def delegate(self):
        if web.ctx.path == "/admin":
            return self.handle(admin_index)
            
        for t in admin_tasks:
            m = web.re_compile('^' + t.path + '$').match(web.ctx.path)
            if m:
                return self.handle(t.cls, m.groups())
        raise web.notfound()
        
    def handle(self, cls, args=()):
        # Use admin theme
        context.bodyid = "admin"
        
        m = getattr(cls(), web.ctx.method, None)
        if not m:
            raise web.nomethod(cls=cls)
        else:
            if self.is_admin():
                return m(*args)
            else:
                return render.permission_denied(web.ctx.path, "Permission denied.")
        
    GET = POST = delegate
        
    def is_admin(self):
        """Returns True if the current user is in admin usergroup."""
        return context.user and context.user.key in [m.key for m in web.ctx.site.get('/usergroup/admin').members]

class admin_index:
    def GET(self):
        return render_template("admin/index",get_counts())
        
class gitpull:
    def GET(self):
        root = os.path.join(os.path.dirname(openlibrary.__file__), os.path.pardir)
        root = os.path.normpath(root)
        
        p = subprocess.Popen('cd %s && git pull' % root, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        out = p.stdout.read()
        p.wait()
        return '<pre>' + web.websafe(out) + '</pre>'
        
class reload:
    def GET(self):
        servers = config.get("plugin_admin", {}).get("webservers", [])
        if servers:
            body = "".join(self.reload(servers))
        else:
            body = "No webservers specified in the configuration file."
        
        return render_template("message", "Reload", body)
        
    def reload(self, servers):
        for s in servers:
            s = web.rstrips(s, "/") + "/_reload"
            yield "<h3>" + s + "</h3>"
            try:
                response = urllib.urlopen(s).read()
                yield "<p><pre>" + response[:100] + "</pre></p>"
            except:
                yield "<p><pre>%s</pre></p>" % traceback.format_exc()
        
@web.memoize
def local_ip():
    import socket
    return socket.gethostbyname(socket.gethostname())

class _reload(delegate.page):
    def GET(self):
        # make sure the request is coming from the LAN.
        if web.ctx.ip not in ['127.0.0.1', '0.0.0.0'] and web.ctx.ip.rsplit(".", 1)[0] != local_ip().rsplit(".", 1)[0]:
            return render.permission_denied(web.ctx.fullpath, "Permission denied to reload templates/macros.")
        
        from infogami.plugins.wikitemplates import code as wikitemplates
        wikitemplates.load_all()

        from openlibrary.plugins.upstream import code as upstream
        upstream.reload()
        return delegate.RawText("done")

class any:
    def GET(self):
        path = web.ctx.path

class people:
    def GET(self):
        i = web.input(email=None)
        
        if i.email:
            account = accounts.find(email=i.email)
            if account:
                raise web.seeother("/admin/people/" + account.username)
        return render_template("admin/people/index", email=i.email)

class people_view:
    def GET(self, key):
        account = accounts.find(username = key) or accounts.find(email = key)
        if account:
            if "@" in key:
                raise web.seeother("/admin/people/" + account.username)
            else:
                return render_template('admin/people/view', account)
        else:
            raise web.notfound()
            
    def POST(self, key):
        user = accounts.find(username = key)
        if not user:
            raise web.notfound()
            
        i = web.input(action=None, tag=None, bot=None)
        if i.action == "update_email":
            return self.POST_update_email(user, i)
        elif i.action == "update_password":
            return self.POST_update_password(user, i)
        elif i.action == "resend_link":
            return self.POST_resend_link(user)
        elif i.action == "activate_account":
            return self.POST_activate_account(user)
        elif i.action == "send_password_reset_email":
            return self.POST_send_password_reset_email(user)
        elif i.action == "block_account":
            return self.POST_block_account(user)
        elif i.action == "unblock_account":
            return self.POST_unblock_account(user)
        elif i.action == "add_tag":
            return self.POST_add_tag(user, i.tag)
        elif i.action == "remove_tag":
            return self.POST_remove_tag(user, i.tag)
        elif i.action == "set_bot_flag":
            return self.POST_set_bot_flag(user, i.bot)
        else:
            raise web.seeother(web.ctx.path)

    def POST_activate_account(self, user):
        user.activate()
        raise web.seeother(web.ctx.path)        

    def POST_send_password_reset_email(self, user):
        send_forgot_password_email(user.username, user.email)
        raise web.seeother(web.ctx.path)        

    def POST_block_account(self, account):
        account.block()
        raise web.seeother(web.ctx.path)

    def POST_unblock_account(self, account):
        account.unblock()
        raise web.seeother(web.ctx.path)

    def POST_resend_link(self, user):
        key = "account/%s/verify"%user.username
        activation_link = web.ctx.site.store.get(key)
        del activation_link
        user.send_verification_email()
        add_flash_message("info", "Activation mail has been resent.")
        raise web.seeother(web.ctx.path)
    
    def POST_update_email(self, account, i):
        user = account.get_user()
        if not forms.vemail.valid(i.email):
            return render_template("admin/people/view", user, i, {"email": forms.vemail.msg})

        if not forms.email_not_already_used.valid(i.email):
            return render_template("admin/people/view", user, i, {"email": forms.email_not_already_used.msg})
        
        account.update_email(i.email)
        
        add_flash_message("info", "Email updated successfully!")
        raise web.seeother(web.ctx.path)
    
    def POST_update_password(self, account, i):
        user = account.get_user()
        if not forms.vpass.valid(i.password):
            return render_template("admin/people/view", user, i, {"password": forms.vpass.msg})

        account.update_password(i.password)
        
        logger.info("updated password of %s", user.key)
        add_flash_message("info", "Password updated successfully!")
        raise web.seeother(web.ctx.path)
        
    def POST_add_tag(self, account, tag):
        account.add_tag(tag)
        return delegate.RawText('{"ok": "true"}', content_type="application/json")

    def POST_remove_tag(self, account, tag):
        account.remove_tag(tag)
        return delegate.RawText('{"ok": "true"}', content_type="application/json")
    
    def POST_set_bot_flag(self, account, bot):
        bot = (bot and bot.lower()) == "true"
        account.set_bot_flag(bot)
        raise web.seeother(web.ctx.path)

class people_edits:
    def GET(self, username):
        account = accounts.find(username=username)
        if not account:
            raise web.notfound()
        else:
            return render_template("admin/people/edits", account)
        
    def POST(self, username):
        i = web.input(changesets=[], comment="Revert", action="revert")
        if i.action == "revert" and i.changesets:
            ipaddress_view().revert(i.changesets, i.comment)
        raise web.redirect(web.ctx.path)        
    
class ipaddress:
    def GET(self):
        return render_template('admin/ip/index')
        
class ipaddress_view:
    def GET(self, ip):
        return render_template('admin/ip/view', ip)

    def POST(self, ip):
        i = web.input(changesets=[], comment="Revert", action="revert")
        if i.action == "block":
            self.block(ip)
        else:
            self.revert(i.changesets, i.comment)
        raise web.redirect(web.ctx.path)

    def block(self, ip):
        ips = get_blocked_ips()
        if ip not in ips:
            ips.append(ip)
        block().block_ips(ips)

    def get_doc(self, key, revision):
        if revision == 0:
            return {
                "key": key,
                "type": {"key": "/type/delete"}
            }
        else:
            return web.ctx.site.get(key, revision).dict()

    def revert(self, changeset_ids, comment):
        logger.debug("Reverting changesets %s", changeset_ids)
        site = web.ctx.site
        docs = [self.get_doc(c['key'], c['revision']-1) 
                for cid in changeset_ids 
                for c in site.get_change(cid).changes]

        logger.debug("Reverting %d docs", len(docs))
        data = {
            "reverted_changesets": [str(cid) for cid in changeset_ids]
        }
        return web.ctx.site.save_many(docs, action="revert", data=data, comment=comment)

        
class stats:
    def GET(self, today):
        json = web.ctx.site._conn.request(web.ctx.site.name, '/get', 'GET', {'key': '/admin/stats/' + today})
        return delegate.RawText(json)
        
    def POST(self, today):
        """Update stats for today."""
        doc = self.get_stats(today)
        doc._save()
        raise web.seeother(web.ctx.path)

    def get_stats(self, today):
        stats = web.ctx.site._request("/stats/" + today)
        
        key = '/admin/stats/' + today
        doc = web.ctx.site.new(key, {
            'key': key,
            'type': {'key': '/type/object'}
        })
        doc.edits = {
            'human': stats.edits - stats.edits_by_bots,
            'bot': stats.edits_by_bots,
            'total': stats.edits
        }
        doc.members = stats.new_accounts
        return doc

class ipstats:
    def GET(self):
        web.header('Content-Type', 'application/json')
        json = urllib.urlopen("http://www.archive.org/download/stats/numUniqueIPsOL.json").read()
        return delegate.RawText(json)
        
class block:
    def GET(self):
        page = web.ctx.site.get("/admin/block") or web.storage(ips=[web.storage(ip="127.0.0.1", duration="1 week", since="1 day")])
        return render_template("admin/block", page)
    
    def POST(self):
        i = web.input()
        ips = [ip.strip() for ip in i.ips.splitlines()]
        self.block_ips(ips)
        add_flash_message("info", "Saved!")
        raise web.seeother("/admin/block")
        
    def block_ips(self, ips):
        page = web.ctx.get("/admin/block") or web.ctx.site.new("/admin/block", {"key": "/admin/block", "type": "/type/object"})
        page.ips = [{'ip': ip} for ip in ips]
        page._save("updated blocked IPs")

def get_blocked_ips():
    doc = web.ctx.site.get("/admin/block")
    if doc:
        return [d.ip for d in doc.ips]
    else:
        return []

def block_ip_processor(handler):
    if not web.ctx.path.startswith("/admin") \
        and (web.ctx.method == "POST" or web.ctx.path.endswith("/edit")) \
        and web.ctx.ip in get_blocked_ips():
        
        return render_template("permission_denied", web.ctx.path, "Your IP address is blocked.")
    else:
        return handler()
        
def daterange(date, *slice):
    return [date + datetime.timedelta(i) for i in range(*slice)]
    
def storify(d):
    if isinstance(d, dict):
        return web.storage((k, storify(v)) for k, v in d.items())
    elif isinstance(d, list):
        return [storify(v) for v in d]
    else:
        return d

def get_counts():
    """Generate counts for various operations which will be given to the
    index page"""
    retval = admin_stats.get_stats(100)
    return storify(retval)

def get_admin_stats():
    def f(dates):
        keys = ["/admin/stats/" + date.isoformat() for date in dates]
        docs = web.ctx.site.get_many(keys)
        return g(docs)

    def has_doc(date):
        return bool(web.ctx.site.get('/admin/stats/' + date.isoformat()))

    def g(docs):
        return {
            'edits': {
                'human': sum(doc['edits']['human'] for doc in docs),
                'bot': sum(doc['edits']['bot'] for doc in docs),
                'total': sum(doc['edits']['total'] for doc in docs),
            },
            'members': sum(doc['members'] for doc in docs)
        }
    date = datetime.datetime.utcnow().date()
    
    if has_doc(date):
        today = f([date])
    else:
        today =  g([stats().get_stats(date.isoformat())])
    yesterday = f(daterange(date, -1, 0, 1))
    thisweek = f(daterange(date, 0, -7, -1))
    thismonth = f(daterange(date, 0, -30, -1))
    
    xstats = {
        'edits': {
            'today': today['edits'],
            'yesterday': yesterday['edits'],
            'thisweek': thisweek['edits'],
            'thismonth': thismonth['edits']
        },
        'members': {
            'today': today['members'],
            'yesterday': yesterday['members'],
            'thisweek': thisweek['members'],
            'thismonth': thismonth['members'] 
        }
    }
    return storify(xstats)
    
from openlibrary.plugins.upstream import borrow
class loans_admin:
    
    def GET(self):
        i = web.input(page=1, pagesize=200)

        total_loans = len(web.ctx.site.store.keys(type="/type/loan", limit=100000))
        pdf_loans = len(web.ctx.site.store.keys(type="/type/loan", name="resource_type", value="pdf", limit=100000))
        epub_loans = len(web.ctx.site.store.keys(type="/type/loan", name="resource_type", value="epub", limit=100000))

        pagesize = h.safeint(i.pagesize, 200)
        pagecount = 1 + (total_loans-1) / pagesize
        pageindex = max(h.safeint(i.page, 1), 1)

        begin = (pageindex-1) * pagesize # pagecount starts from 1
        end = min(begin + pagesize, total_loans)

        loans = web.ctx.site.store.values(type="/type/loan", offset=begin, limit=pagesize)

        stats = {
            "total_loans": total_loans,
            "pdf_loans": pdf_loans,
            "epub_loans": epub_loans,
            "bookreader_loans": total_loans - pdf_loans - epub_loans,
            "begin": begin+1, # We count from 1, not 0.
            "end": end
        }

        # Preload books
        web.ctx.site.get_many([loan['book'] for loan in loans])

        return render_template("admin/loans", loans, None, pagecount=pagecount, pageindex=pageindex, stats=stats)
        
    def POST(self):
        i = web.input(action=None)
        
        # Sanitize
        action = None
        actions = ['updateall']
        if i.action in actions:
            action = i.action
            
        if action == 'updateall':
            borrow.update_all_loan_status()
        raise web.seeother(web.ctx.path) # Redirect to avoid form re-post on re-load

class service_status(object):
    def GET(self):
        try:
            f = open("%s/olsystem.yml"%config.admin.olsystem_root)
            nodes = services.load_all(yaml.load(f), config.admin.nagios_url)
            f.close()
        except IOError, i:
            f = None
            nodes = []
        return render_template("admin/services", nodes)

class inspect:
    def GET(self, section):
        if section == "/store":
            return self.GET_store()
        elif section == "/memcache":
            return self.GET_memcache()
        else:
            return inspect_thing.get_thing_info(section)
        # else:
        #     raise web.notfound()
        
    def GET_store(self):
        i = web.input(key=None, type=None, name=None, value=None)
        
        if i.key:
            doc = web.ctx.site.store.get(i.key)
            if doc:
                docs = [doc]
            else:
                docs = []
        else:
            docs = web.ctx.site.store.values(type=i.type or None, name=i.name or None, value=i.value or None, limit=100)
            
        return render_template("admin/inspect/store", docs, input=i)
        
    def GET_memcache(self):
        i = web.input(action="read")
        i.setdefault("keys", "")
        
        from openlibrary.core import cache
        mc = cache.get_memcache()
        
        keys = [k.strip() for k in i["keys"].split() if k.strip()]        
        if i.action == "delete":
            mc.delete_multi(keys)
            add_flash_message("info", "Deleted %s keys from memcache" % len(keys))
            return render_template("admin/inspect/memcache", [], {})
        else:
            mapping = keys and mc.get_multi(keys)
            return render_template("admin/inspect/memcache", keys, mapping)

        
class deploy:
    def GET(self):
        return render_template("admin/deploy")
        
    def POST(self):
        i = web.input(deploy=None, restart=None, merge="false")
        
        tasks = []
        
        if i.deploy == "openlibrary":
            if i.merge == "true":
                tasks.append("git_merge:openlibrary,branch=dev")
            tasks.append("deploy:openlibrary")
        elif i.deploy == "olsystem":
            tasks.append("deploy:olsystem")
    
        if i.restart:
            tasks.append("restart:%s" % i.restart)
            
        if tasks:
            return self.fab(tasks)
        else:
            return render_template("admin/deploy")
            
    def fab(self, tasks):
        cmd = "cd /olsystem && /olsystem/bin/olenv fab --no-pty " + " ".join(tasks)
        d = self.system(cmd)
        return render_template("admin/command", d)
        
    def system(self, cmd, input=None):
        """Executes the command returns the stdout.
        """
        if input:
            stdin = subprocess.PIPE
        else:
            stdin = None
        p = subprocess.Popen(cmd, shell=True, stdin=stdin, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = p.communicate(input)
        status = p.wait()
        return web.storage(cmd=cmd, status=status, stdout=out, stderr=err)

class _graphs:
    def GET(self):
        return render_template("admin/graphs")

class permissions:
    def GET(self):
        perm_pages = self.get_permission("/")
        # assuming that the permission of books and authors is same as works
        perm_records = self.get_permission("/works")
        return render_template("admin/permissions", perm_records, perm_pages)

    def get_permission(self, key):
        doc = web.ctx.site.get(key)
        perm = doc and doc.child_permission
        return perm and perm.key or "/permission/open"

    def set_permission(self, key, permission):
        """Returns the doc with permission set.
        The caller must save the doc.
        """
        doc = web.ctx.site.get(key)
        doc = doc and doc.dict() or { "key": key, "type": {"key": "/type/page"}}

        # so that only admins can modify the permission
        doc["permission"] = {"key": "/permission/restricted"}

        doc["child_permission"] = {"key": permission}
        return doc

    def POST(self):
        i = web.input(
            perm_pages="/permission/loggedinusers",
            perm_records="/permission/loggedinusers")
        
        root = self.set_permission("/", i.perm_pages)
        works = self.set_permission("/works", i.perm_records)
        books = self.set_permission("/books", i.perm_records)
        authors = self.set_permission("/authors", i.perm_records)
        web.ctx.site.save_many([root, works, books, authors], comment="Updated edit policy.")

        add_flash_message("info", "Edit policy has been updated!")
        return self.GET()

class solr:
    def GET(self):
        return render_template("admin/solr")

    def POST(self):
        i = web.input(keys="")
        keys = i['keys'].strip().split()
        web.ctx.site.store['solr-force-update'] = dict(type="solr-force-update", keys=keys, _rev=None)
        add_flash_message("info", "Added the specified keys to solr update queue.!")
        return self.GET()

def setup():
    register_admin_page('/admin/git-pull', gitpull, label='git-pull')
    register_admin_page('/admin/reload', reload, label='Reload Templates')
    register_admin_page('/admin/people', people, label='People')
    register_admin_page('/admin/people/([^/]*)', people_view, label='View People')
    register_admin_page('/admin/people/([^/]*)/edits', people_edits, label='Edits')
    register_admin_page('/admin/ip', ipaddress, label='IP')
    register_admin_page('/admin/ip/(.*)', ipaddress_view, label='View IP')
    register_admin_page('/admin/stats/(\d\d\d\d-\d\d-\d\d)', stats, label='Stats JSON')
    register_admin_page('/admin/ipstats', ipstats, label='IP Stats JSON')
    register_admin_page('/admin/block', block, label='')
    register_admin_page('/admin/loans', loans_admin, label='')
    register_admin_page('/admin/status', service_status, label = "Open Library services")
    # register_admin_page('/admin/support', support.cases, label = "All Support cases")
    # register_admin_page('/admin/support/(all|new|replied|closed)?', support.cases, label = "Filtered Support cases")
    # register_admin_page('/admin/support/(\d+)', support.case, label = "Support cases")
    register_admin_page('/admin/inspect(?:(/.+))?', inspect, label="")
    register_admin_page('/admin/tasks', tasks.tasklist, label = "Task queue")
    register_admin_page('/admin/tasks/(.*)', tasks.tasks, label = "Task details")
    register_admin_page('/admin/deploy', deploy, label="")
    register_admin_page('/admin/graphs', _graphs, label="")
    register_admin_page('/admin/permissions', permissions, label="")
    register_admin_page('/admin/solr', solr, label="")

    inspect_thing.setup()
    support.setup()
    import mem

    for p in [mem._memory, mem._memory_type, mem._memory_id]:
        register_admin_page('/admin' + p.path, p)

    public(get_admin_stats)
    public(get_blocked_ips)
    delegate.app.add_processor(block_ip_processor)
    
    import graphs
    graphs.setup()
    
setup()

########NEW FILE########
__FILENAME__ = graphs
"""Utilities for rendering Graphite graphs.
"""
import urllib
import web
from infogami import config

def get_graphite_base_url():
    return config.get("graphite_base_url", "")

class GraphiteGraph:
    """Representation of Graphite graph.
    
    Usage:
    
        g = GraphiteGraph()
        g.add("stats.timers.ol.pageload.all.mean").apply("movingAverage", 20).alias("all")
        print g.render()
    
    In templates:

        $ g = GraphiteGraph()
        $g.add("stats.timers.ol.pageload.all.mean").apply("movingAverage", 20).alias("all")
        $:g.render()
    """
    def __init__(self):
        self.series_list = []
        
    def add(self, name):
        s = Series(name)
        self.series_list.append(s)
        return s
        
    def get_queryparams(self, **options):
        """Returns query params to be passed to the image URL for rendering this graph.
        """
        options["target"] = [s.name for s in self.series_list]
        return options
        
    def render(self, **options):
        """Renders the graphs as an img tag.
        
        Usage in templates:
        
            $:g.render(yLimit=100, width=300, height=400)
        """
        return '<img src="%s/render/?%s"/>' % (get_graphite_base_url(), urllib.urlencode(self.get_queryparams(**options), doseq=True))
        
class Series:
    """One series in the GraphiteGraph.
    """
    def __init__(self, name):
        self.name = name
        
    def apply(self, funcname, *args):
        """Applies a function to this series.
        
        :return: Returns self
        """
        self.name = "%s(%s, %s)" % (funcname, self.name, ", ".join(repr(a) for a in args))
        return self
        
    def alias(self, name):
        """Shorthand for calling s.apply("alias", name)
        """
        return self.apply("alias", name)
        
    def __repr__(self):
        return "<series: %r>" % self.name
        
    def __str__(self):
        # Returning empty string to allow template use $g.add("foo") without printing anything.
        return ""
        
def setup():
    web.template.Template.globals.update({
        'GraphiteGraph': GraphiteGraph,
    })
########NEW FILE########
__FILENAME__ = inspect_thing
import datetime
import logging

import web
import couchdb

from infogami import config
from infogami.utils.view import render_template

tombstone_db = None
logger = None

class ComponentMissing(KeyError): pass
    


@web.memoize
def connect_to_tombstone():
    global tombstone_db
    try:
        tombstone_db_uri = config.get("celery",{})["tombstone_db"]
        tombstone_db = couchdb.Database(tombstone_db_uri)
    except Exception,e:
        logger.warning("Couldn't connect to tombstone database", exc_info = True)

def process_task_row(task):
    """Makes some changes to the task row from couch so that thepp
    template can display it properly"""
    ret = {}
    for k,v in task.doc.iteritems():
        ret[k] = v
    ret['started_at'] = datetime.datetime.utcfromtimestamp(ret['started_at'])
    ret['finished_at'] = datetime.datetime.utcfromtimestamp(ret['finished_at'])
    ret['enqueued_at'] = datetime.datetime.utcfromtimestamp(ret['enqueued_at'])
    return ret

def get_tasks_info(thing, tombstone_db):
    if not tombstone_db:
        return "Couldn't initialise connection to tombstone database. No task information available"
    events = tombstone_db.view("history/by_key",startkey=[thing], endkey=[thing,{}], include_docs=True, stale = "ok")
    return (process_task_row(x) for x in events)


def get_thing_info(thing):
    global tombstone_db
    try:
        tasks = get_tasks_info(thing, tombstone_db)
    except couchdb.http.ResourceNotFound:
        return("No such thing to inspect")
    return render_template("admin/inspect/thing", thing, tasks)

def setup():
    global logger
    logger = logging.getLogger("openlibrary.inspect_thing")
    connect_to_tombstone()



########NEW FILE########
__FILENAME__ = mem
from infogami.utils import delegate
from infogami.utils.view import render, safeint
import memory
import web
import gc

def render_template(name, *a, **kw):
    return render[name](*a, **kw)
    
class Object:
    def __init__(self, obj, name=None):
        self.obj = obj
        self.name = name
        
    def get_id(self):
        return id(self.obj)
    
    def get_type(self):
        return memory._get_type(self.obj)
        
    def repr(self):
        try:
            if isinstance(self.obj, (dict, web.threadeddict)):
                from infogami.infobase.utils import prepr
                return prepr(self.obj)
            else:
                return repr(self.obj)
        except:
            return "failed"
        
        return render_template("admin/memory/object", self.obj)
        
    def get_referrers(self):
        d = []
        
        for o in gc.get_referrers(self.obj):
            name = None
            if type(o) == type({}):
                name = web.dictfind(o, self.obj)
                for r in gc.get_referrers(o):
                    if getattr(r, "__dict__", None) is o:
                        o = r
                        break
            elif isinstance(o, dict): # other dict types
                name = web.dictfind(o, self.obj)
                
            if not isinstance(name, basestring):
                name = None
            
            d.append(Object(o, name))
        return d
        
    def get_referents(self):
        d = []
        
        _dict = getattr(self.obj, "__dict__", None)
        if _dict:
            for k, v in self.obj.__dict__.items():
                d.append(Object(v, name=k))
                
        for o in gc.get_referents(self.obj):
            if o is not _dict:
                d.append(Object(o))
        return d

class _memory:
    path = "/memory"
    def GET(self):
        i = web.input(page=1, sort="diff", prefix="")
        
        page = safeint(i.page, 1)
        end = page * 50
        begin = end - 50
        
        if i.sort not in ["count", "mark", "diff"]:
            i.sort = "diff"
        
        counts = [c for c in memory.get_counts() if c.type.startswith(i.prefix)]
        counts.sort(key=lambda c: c[i.sort], reverse=True)
        return render_template("admin/memory/index", counts[begin:end], page, sort=i.sort)
        
    def POST(self):
        memory.mark()
        raise web.seeother(web.ctx.fullpath)

class _memory_type:
    path = "/memory/type/(.*)"
    
    def GET(self, type):
        objects = memory.get_objects_by_type(type)
        
        i = web.input(page=1, diff="false")
        
        page = safeint(i.page, 1)
        end = page * 50
        begin = end - 50
        
        objects = [Object(obj) for obj in memory.get_objects_by_type(type)]
        
        if i.diff == "true":
            marked = memory._mark_ids.get(type, [])
            objects = [obj for obj in objects if obj.get_id() not in marked]
        
        return render_template("admin/memory/type", type, objects, page)
        
def first(it):
    try:
        return it.next()
    except StopIteration:
        return None
        
class _memory_id:
    path = "/memory/id/(.*)"
    
    def get_object(self, _id):
        for obj in memory.get_objects():
            if str(id(obj)) == _id:
                return Object(obj)
    
    def GET(self, _id):
        obj = self.get_object(_id)
        if not obj:
            raise web.notfound()
        return render_template("admin/memory/object", obj)


########NEW FILE########
__FILENAME__ = memory
"""memory profiler
"""
import gc
import web
from collections import defaultdict

_mark = {}
_mark_ids = {}

class Storage(web.Storage):
    pass

def mark():
    """Mark the current counts to show the difference."""
    global _mark, _mark_ids
    
    objects= get_objects()
    d = defaultdict(set)
    for obj in objects:
        d[_get_type(obj)].add(id(obj))
        
    _mark_ids = d
    _mark = get_all_counts()
    
def get_counts():
    counts = get_all_counts()
    
    d = [Storage(type=type, count=count, mark=_mark.get(type, 0), diff=count - _mark.get(type, 0))
        for type, count in counts.items()]
    return d

def get_all_counts():
    """Returns the counts of live objects."""
    objects= get_objects()

    d = defaultdict(lambda: 0)
    for obj in objects:
        d[_get_type(obj)] += 1

    return d
    
def get_objects():
    """Returns a list of live objects."""
    objects = gc.get_objects()
    dicts = set(id(o.__dict__) for o in objects if hasattr(o, "__dict__"))    
    return (obj for obj in gc.get_objects() if obj is not _mark and id(obj) not in dicts)
    
def get_objects_by_type(type):
    return (obj for obj in get_objects() if _get_type(obj) == type)

def _get_type(obj):
    """Returns the type of given object as string.
    """
    try:
        t = obj.__class__
    except:
        t = type(obj)

    mod = t.__module__
    name = t.__name__
    if mod != "__builtin__":
        name = mod + "." + name
    return name

########NEW FILE########
__FILENAME__ = services
"""
Contains stuff needed to list services and modules run by OpenLibrary
for the admin panel
"""

import re
import urllib
from collections import defaultdict

import BeautifulSoup

class Nagios(object):
    def __init__(self, url):
        try:
            self.data = BeautifulSoup.BeautifulSoup(urllib.urlopen(url).read())
        except Exception, m:
            print m
            self.data = None

    def get_service_status(self, service):
        "Returns the stats of the service `service`"
        if not self.data:
            return "error-api"
        # The service name is kept inside a bunch of nested nodes We
        # walk up the nodes to find the enclosing <tr> that contains
        # the service in question. A single step is not enough since
        # there are nested tables in the layout.
        service = self.data.find(text = re.compile(service))
        if service: 
            service_tr = service.findParents("tr")[2]
            status_td  = service_tr.find("td", attrs = {"class" : re.compile(r"status(OK|RECOVERY|UNKNOWN|WARNING|CRITICAL)")})
            return status_td['class'].replace("status","")
        else:
            return "error-nosuchservice"

class Service(object):
    """
    An OpenLibrary service with all the stuff that we need to
    manipulate it.
    """

    def __init__(self, node, name, nagios, logs = False):
        self.node = node
        self.name = name
        self.logs = logs
        self.status = "Service status(TBD)"
        self.nagios = nagios.get_service_status(name)

    def __repr__(self):
        return "Service(name = '%s', node = '%s', logs = '%s')"%(self.name, self.node, self.logs)
    

def load_all(config, nagios_url):
    """Loads all services specified in the config dictionary and returns
    the list of Service"""
    d = defaultdict(list)
    nagios = Nagios(nagios_url)
    for node in config:
        services = config[node].get('services', [])
        if services:
            for service in services:
                d[node].append(Service(node = node, name = service, nagios = nagios))
    return d


########NEW FILE########
__FILENAME__ = support
import textwrap
import urllib

import web
from infogami.utils.view import render_template, add_flash_message
from infogami import config

from openlibrary.core import support, cache
from openlibrary import accounts

support_db = None

# Cache the admin list for 5 minutes. 
# Storing it along with /usergroup/admin so that it gets invalidated whenever the admin usergoup is updated.
@cache.memoize(engine="memcache", key=lambda: ("d/usergroup/admin", "admin-email-list"), expires=5*60)
def get_admins():
    """Returns a list of [email, name] for each admin member.
    """
    return [[m.get_email(), m.get_name()] for m in accounts.get_group("admin").members]

class cases(object):
    def GET(self, typ = "new"):
        i = web.input(sort="status", desc = "false", all = "false", filter = "new")
        typ = i.filter
        current_user = accounts.get_current_user()
        if not support_db:
            return render_template("admin/cases", None, None, True, False)
        sortby = i['sort']
        desc = i['desc']
        cases = support_db.get_all_cases(typ, summarise = False, sortby = sortby, desc = desc)
        if i['all'] == "false":
            email = current_user.get_email()
            cases = (x for x in cases if x.assignee == email)
            summary = support_db.get_all_cases(typ, summarise = True, user = email)
        else:
            summary = support_db.get_all_cases(typ, summarise = True)
        total = sum(int(x) for x in summary.values())
        desc = desc == "false" and "true" or "false"
        return render_template("admin/cases", summary, total, cases, desc)
    POST = GET

class case(object):
    def GET(self, caseid):
        if not support_db:
            return render_template("admin/cases", None, None, True, False)
        case = support_db.get_case(caseid)
        date_pretty_printer = lambda x: x.strftime("%B %d, %Y")
        if len(case.history) == 1:
            last_email = case.description
        else:
            last_email = case.history[-1]['text']
        try:
            last_email = "\n".join("  > %s"%x for x in last_email.split("\n")) + "\n\n"
        except Exception:
            last_email = ""
        admins = ((email, name, email == case.assignee) for email, name in get_admins())
        
        prev_url, next_url = self._get_prevnext(case.caseid)
        return render_template("admin/case", case, last_email, admins, date_pretty_printer,
            prev_url=prev_url,
            next_url=next_url)

    def POST(self, caseid):
        if not support_db:
            return render_template("admin/cases", None, None, True, False)
        case = support_db.get_case(caseid)
        form = web.input()
        action = form.get("button","")
        {"SEND REPLY" : self.POST_sendreply,
         "UPDATE"     : self.POST_update,
         "CLOSE CASE" : self.POST_closecase,
         "REOPEN CASE": self.POST_reopencase}[action](form,case)
        date_pretty_printer = lambda x: x.strftime("%B %d, %Y")
        last_email = case.history[-1]['text']
        last_email = "\n".join("> %s"%x for x in textwrap.wrap(last_email))
        admins = ((email, name, email == case.assignee) for email, name in get_admins())
        return render_template("admin/case", case, last_email, admins, date_pretty_printer)
    
    def POST_sendreply(self, form, case):
        # Next/Prev links should be computed before the case is updated
        prev_url, next_url = self._get_prevnext(case.caseid)
        
        user = accounts.get_current_user()
        assignee = case.assignee
        casenote = form.get("casenote1", "")
        casenote = "%s replied:\n\n%s"%(user.get_name(), casenote)
        case.add_worklog_entry(by = user.get_email(),
                               text = casenote)
        case.change_status("replied", user.get_email())
        email_to = form.get("email", False)
        subject = "Case #%s: %s"%(case.caseno, case.subject)
        if assignee != user.get_email():
            case.reassign(user.get_email(), user.get_name(), "")
        if email_to:
            message = render_template("admin/email", case, casenote)
            web.sendmail(config.get("support_case_control_address","support@openlibrary.org"), email_to, subject, message)
        add_flash_message("info", "Reply sent")

        raise web.redirect(next_url or "/admin/support")

    def POST_update(self, form, case):
        casenote = form.get("casenote2", False)
        assignee = form.get("assignee", False)
        user = accounts.get_current_user()
        by = user.get_email()
        text = casenote or ""
        if case.status == "closed":
            case.change_status("new", by)
        if assignee != case.assignee:
            case.reassign(assignee, by, text)
            subject = "Case #%s has been assigned to you"%case.caseno
            message = render_template("admin/email_reassign", case, text)
            web.sendmail(config.get("support_case_control_address","support@openlibrary.org"), assignee, subject, message)
        else:
            case.add_worklog_entry(by = by,
                                   text = text)
        add_flash_message("info", "Case updated")

    def _get_prevnext(self, caseid):
        """Redirects prev and next urls.
        """
        i = web.input(all="false", filter="new", _method="GET")
        sort = "status"
        desc = "false"
        cases = support_db.get_all_cases(i.filter, summarise=False, sortby=sort, desc=desc)
        cases = list(cases)
        if i.all == "false":
            current_user = accounts.get_current_user()
            email = current_user.get_email()
            cases = [x for x in cases if x.assignee == email]
            
        try:
            index = [case.caseid for case in cases].index(caseid)
        except ValueError:
            return None, None
            
        if index > 0:
            case = cases[index-1]
            prev = "/admin/support/" + str(case.caseno) + "?" + urllib.urlencode(i)
        else:
            prev = None
        
        if index < len(cases)-1:
            case = cases[index+1]
            next = "/admin/support/" + str(case.caseno) + "?" + urllib.urlencode(i)
        else:
            next = None
        
        return prev, next

    def POST_closecase(self, form, case):
        # Next/Prev links should be computed before the case is updated
        prev_url, next_url = self._get_prevnext(case.caseid)
        
        user = accounts.get_current_user()
        by = user.get_email()
        text = "Case closed"
        case.add_worklog_entry(by = by,
                               text = text)
        case.change_status("closed", by)
        add_flash_message("info", "Case closed")
        
        raise web.redirect(next_url or "/admin/support")

    def POST_reopencase(self, form, case):
        user = accounts.get_current_user()
        by = user.get_email()
        text = "Case reopened"
        case.add_worklog_entry(by = by,
                               text = text)
        case.change_status("new", by)
        add_flash_message("info", "Case reopened")

def setup():
    global support_db
    try:
        support_db = support.Support()
    except support.DatabaseConnectionError:
        support_db = None



########NEW FILE########
__FILENAME__ = tasks
import calendar
import pickle
import logging
import datetime
import urlparse
import urllib
import json

import couchdb
from celery.task.control import inspect 
from infogami.utils.view import render_template, add_flash_message
from infogami import config
import web

import openlibrary.tasks

logger = logging.getLogger("admin.tasks")

@web.memoize
def connect_to_taskdb():
    db_uri = config.get("celery",{})["tombstone_db"]
    return couchdb.Database(db_uri)


def process_task_row(taskdoc):
    """Makes some changes to the task row from couch so that the
    template can display it properly"""
    taskdoc['started_at'] = datetime.datetime.utcfromtimestamp(taskdoc['started_at'])
    taskdoc['finished_at'] = datetime.datetime.utcfromtimestamp(taskdoc['finished_at'])
    try:
        taskdoc['enqueued_at'] = datetime.datetime.utcfromtimestamp(taskdoc['enqueued_at'])
    except:
        taskdoc['enqueued_at'] = taskdoc['started_at']
    taskdoc["keys"] = taskdoc['context'].get("keys",[])
    taskdoc["changeset"] = taskdoc['context'].get("changeset",None)
    return taskdoc

class tasklist(object):
    def GET(self):
        db = connect_to_taskdb()
        filters = web.input(command = None,
                            finishedat_start = None,
                            finishedat_end = None,
                            limit = 20,
                            offset = 0)
        command = filters["command"]
        limit = int(filters["limit"])
        offset = int(filters["offset"])
        if not command: 
            command = None # To make the view parameters work properly. otherwise, command becomes ''

        if filters["finishedat_start"]:
            finishedat_start = datetime.datetime.strptime(filters['finishedat_start'],"%Y-%m-%d %H:%M")
        else:
            finishedat_start = datetime.datetime(year = 2000, day = 1, month = 1)
            
        if filters["finishedat_end"]:
            finishedat_end = datetime.datetime.strptime(filters['finishedat_end'],"%Y-%m-%d %H:%M")
        else:
            finishedat_end = datetime.datetime.utcnow()
        
        finishedat_start = calendar.timegm(finishedat_start.timetuple())
        finishedat_end = calendar.timegm(finishedat_end.timetuple())

        completed_tasks = (process_task_row(x.doc) for x in db.view("history/tasks",
                                                                    startkey = [command,finishedat_end],
                                                                    endkey   = [command,finishedat_start],
                                                                    limit = limit,
                                                                    skip = offset,
                                                                    include_docs = True,
                                                                    descending = True,
                                                                    stale = "ok"))
        return render_template("admin/tasks/index", completed_tasks)

        
class tasks(object):
    def GET(self, taskid):
        try:
            db = connect_to_taskdb()
            try:
                task = db[taskid]
            except couchdb.http.ResourceNotFound:
                return "No such task"
            return render_template("admin/tasks/task", process_task_row(task))
        except Exception:
            logger.warning("Problem while obtaining task information '%s'", taskid, exc_info = True)
            return "Error in obtaining task information"

    def POST(self, taskid):
        try:
            db = connect_to_taskdb()
            try:
                task = db[taskid]
            except couchdb.http.ResourceNotFound:
                return "No such task"
        except Exception:
            logger.warning("Problem while obtaining task information '%s'", taskid, exc_info = True)
            return "Error in obtaining task information"
        
        function = getattr(openlibrary.tasks,task['command'])

        largs = json.loads(task['largs'])
        kargs = json.loads(task['kargs'])
        kargs["celery_parent_task"] = taskid
        add_flash_message("info", "%s (%s) refired!"%(taskid, task['command']))
        function.delay(*largs, **kargs)

        return render_template("admin/tasks/task", process_task_row(task))
            

########NEW FILE########
__FILENAME__ = conftest
def pytest_funcarg__serviceconfig(request):
    import yaml
    f = open("tests/sample_services.yml")
    d = yaml.load(f)
    f.close()
    return d

########NEW FILE########
__FILENAME__ = test_services
"""
Tests for the services module used by the admin interface.
"""

def test_loader(serviceconfig):
    "Make sure services are loaded"
    from .. import services
    services = services.load_all(serviceconfig)
    assert len(services.keys()) == 2
    s = sorted(services.keys())
    assert s[0] == "ol-web0"
    assert s[1] == "ol-web1"
    assert services['ol-web0'][0].name == "7071-ol-gunicorn"
    assert services['ol-web0'][1].name == "7060-memcached"
    assert services['ol-web1'][0].name == "7072-ol-gunicorn"
    assert services['ol-web1'][1].name == "7061-memcached"
    
    
    
    
    
    
    

########NEW FILE########
__FILENAME__ = akismet
# Version 0.1.5
# 2006/02/05

# Copyright Michael Foord 2005 - 2007
# akismet.py
# Python interface to the akismet API

# http://www.voidspace.org.uk/python/modules.shtml
# http://akismet.com

# Released subject to the BSD License
# Please see http://www.voidspace.org.uk/python/license.shtml

# For information about bugfixes, updates and support, please join the Pythonutils mailing list.
# http://groups.google.com/group/pythonutils/
# Comments, suggestions and bug reports welcome.
# Scripts maintained at http://www.voidspace.org.uk/python/index.shtml
# E-mail fuzzyman@voidspace.org.uk

"""
A python interface to the `Akismet <http://akismet.com>`_ 
{acro;API;Application Programmers Interface}. This is a web service for
blocking SPAM comments to blogs - or other online services.

You will need a Wordpress API key, from `wordpress.com <http://wordpress.com>`_.

You should pass in the keyword argument 'agent' to the name of your program,
when you create an Akismet instance. This sets the ``user-agent`` to a useful
value.

The default is : ::

    Python Interface by Fuzzyman | akismet.py/0.1.3

Whatever you pass in, will replace the *Python Interface by Fuzzyman* part.
**0.1.2** will change with the version of this interface.

"""
import os, sys
import urllib2
from urllib import urlencode

import socket
if hasattr(socket, 'setdefaulttimeout'):
    # Set the default timeout on sockets to 5 seconds
    socket.setdefaulttimeout(5)

isfile = os.path.isfile

__version__ = '0.1.5'

__all__ = (
    '__version__',
    'Akismet',
    'AkismetError',
    'APIKeyError',
    )

__author__ = 'Michael Foord <fuzzyman AT voidspace DOT org DOT uk>'

__docformat__ = "restructuredtext en"

user_agent = "%s | akismet.py/%s"
DEFAULTAGENT = 'Python Interface by Fuzzyman/%s'

class AkismetError(Exception):
    """Base class for all akismet exceptions."""

class APIKeyError(AkismetError):
    """Invalid API key."""

class Akismet(object):
    """A class for working with the akismet API"""

    baseurl = 'rest.akismet.com/1.1/'

    def __init__(self, key=None, blog_url=None, agent=None):
        """Automatically calls ``setAPIKey``."""
        if agent is None:
            agent = DEFAULTAGENT % __version__
        self.user_agent = user_agent % (agent, __version__)
        self.setAPIKey(key, blog_url)


    def _getURL(self):
        """
        Fetch the url to make requests to.
        
        This comprises of api key plus the baseurl.
        """
        return 'http://%s.%s' % (self.key, self.baseurl)
    
    
    def _safeRequest(self, url, data, headers):
        print "_safeRequest", url
        try:
            req = urllib2.Request(url, data, headers)
            h = urllib2.urlopen(req)
            resp = h.read()
        except (urllib2.HTTPError, urllib2.URLError, IOError), e:
            raise AkismetError(str(e))       
        return resp


    def setAPIKey(self, key=None, blog_url=None):
        """
        Set the wordpress API key for all transactions.
        
        If you don't specify an explicit API ``key`` and ``blog_url`` it will
        attempt to load them from a file called ``apikey.txt`` in the current
        directory.
        
        This method is *usually* called automatically when you create a new
        ``Akismet`` instance.
        """
        if key is None and isfile('apikey.txt'):
            the_file = [l.strip() for l in open('apikey.txt').readlines()
                if l.strip() and not l.strip().startswith('#')]
            try:
                self.key = the_file[0]
                self.blog_url = the_file[1]
            except IndexError:
                raise APIKeyError("Your 'apikey.txt' is invalid.")
        else:
            self.key = key
            self.blog_url = blog_url


    def verify_key(self):
        """
        This equates to the ``verify-key`` call against the akismet API.
        
        It returns ``True`` if the key is valid.
        
        The docs state that you *ought* to call this at the start of the
        transaction.
        
        It raises ``APIKeyError`` if you have not yet set an API key.
        
        If the connection to akismet fails, it allows the normal ``HTTPError``
        or ``URLError`` to be raised.
        (*akismet.py* uses `urllib2 <http://docs.python.org/lib/module-urllib2.html>`_)
        """
        if self.key is None:
            raise APIKeyError("Your have not set an API key.")
        data = { 'key': self.key, 'blog': self.blog_url }
        # this function *doesn't* use the key as part of the URL
        url = 'http://%sverify-key' % self.baseurl
        # we *don't* trap the error here
        # so if akismet is down it will raise an HTTPError or URLError
        headers = {'User-Agent' : self.user_agent}
        resp = self._safeRequest(url, urlencode(data), headers)
        if resp.lower() == 'valid':
            return True
        else:
            return False

    def _build_data(self, comment, data):
        """
        This function builds the data structure required by ``comment_check``,
        ``submit_spam``, and ``submit_ham``.
        
        It modifies the ``data`` dictionary you give it in place. (and so
        doesn't return anything)
        
        It raises an ``AkismetError`` if the user IP or user-agent can't be
        worked out.
        """
        data['comment_content'] = comment
        if not 'user_ip' in data:
            try:
                val = os.environ['REMOTE_ADDR']
            except KeyError:
                raise AkismetError("No 'user_ip' supplied")
            data['user_ip'] = val
        if not 'user_agent' in data:
            try:
                val = os.environ['HTTP_USER_AGENT']
            except KeyError:
                raise AkismetError("No 'user_agent' supplied")
            data['user_agent'] = val
        #
        data.setdefault('referrer', os.environ.get('HTTP_REFERER', 'unknown'))
        data.setdefault('permalink', '')
        data.setdefault('comment_type', 'comment')
        data.setdefault('comment_author', '')
        data.setdefault('comment_author_email', '')
        data.setdefault('comment_author_url', '')
        data.setdefault('SERVER_ADDR', os.environ.get('SERVER_ADDR', ''))
        data.setdefault('SERVER_ADMIN', os.environ.get('SERVER_ADMIN', ''))
        data.setdefault('SERVER_NAME', os.environ.get('SERVER_NAME', ''))
        data.setdefault('SERVER_PORT', os.environ.get('SERVER_PORT', ''))
        data.setdefault('SERVER_SIGNATURE', os.environ.get('SERVER_SIGNATURE',
            ''))
        data.setdefault('SERVER_SOFTWARE', os.environ.get('SERVER_SOFTWARE',
            ''))
        data.setdefault('HTTP_ACCEPT', os.environ.get('HTTP_ACCEPT', ''))
        data.setdefault('blog', self.blog_url)


    def comment_check(self, comment, data=None, build_data=True, DEBUG=False):
        """
        This is the function that checks comments.
        
        It returns ``True`` for spam and ``False`` for ham.
        
        If you set ``DEBUG=True`` then it will return the text of the response,
        instead of the ``True`` or ``False`` object.
        
        It raises ``APIKeyError`` if you have not yet set an API key.
        
        If the connection to Akismet fails then the ``HTTPError`` or
        ``URLError`` will be propogated.
        
        As a minimum it requires the body of the comment. This is the
        ``comment`` argument.
        
        Akismet requires some other arguments, and allows some optional ones.
        The more information you give it, the more likely it is to be able to
        make an accurate diagnosise.
        
        You supply these values using a mapping object (dictionary) as the
        ``data`` argument.
        
        If ``build_data`` is ``True`` (the default), then *akismet.py* will
        attempt to fill in as much information as possible, using default
        values where necessary. This is particularly useful for programs
        running in a {acro;CGI} environment. A lot of useful information
        can be supplied from evironment variables (``os.environ``). See below.
        
        You *only* need supply values for which you don't want defaults filled
        in for. All values must be strings.
        
        There are a few required values. If they are not supplied, and
        defaults can't be worked out, then an ``AkismetError`` is raised.
        
        If you set ``build_data=False`` and a required value is missing an
        ``AkismetError`` will also be raised.
        
        The normal values (and defaults) are as follows : ::
        
            'user_ip':          os.environ['REMOTE_ADDR']       (*)
            'user_agent':       os.environ['HTTP_USER_AGENT']   (*)
            'referrer':         os.environ.get('HTTP_REFERER', 'unknown') [#]_
            'permalink':        ''
            'comment_type':     'comment' [#]_
            'comment_author':   ''
            'comment_author_email': ''
            'comment_author_url': ''
            'SERVER_ADDR':      os.environ.get('SERVER_ADDR', '')
            'SERVER_ADMIN':     os.environ.get('SERVER_ADMIN', '')
            'SERVER_NAME':      os.environ.get('SERVER_NAME', '')
            'SERVER_PORT':      os.environ.get('SERVER_PORT', '')
            'SERVER_SIGNATURE': os.environ.get('SERVER_SIGNATURE', '')
            'SERVER_SOFTWARE':  os.environ.get('SERVER_SOFTWARE', '')
            'HTTP_ACCEPT':      os.environ.get('HTTP_ACCEPT', '')
        
        (*) Required values
        
        You may supply as many additional 'HTTP_*' type values as you wish.
        These should correspond to the http headers sent with the request.
        
        .. [#] Note the spelling "referrer". This is a required value by the
            akismet api - however, referrer information is not always
            supplied by the browser or server. In fact the HTTP protocol
            forbids relying on referrer information for functionality in 
            programs.
        .. [#] The `API docs <http://akismet.com/development/api/>`_ state that this value
            can be " *blank, comment, trackback, pingback, or a made up value*
            *like 'registration'* ".
        """
        if self.key is None:
            raise APIKeyError("Your have not set an API key.")
        if data is None:
            data = {}
        if build_data:
            self._build_data(comment, data)
        url = '%scomment-check' % self._getURL()
        # we *don't* trap the error here
        # so if akismet is down it will raise an HTTPError or URLError
        headers = {'User-Agent' : self.user_agent}
        resp = self._safeRequest(url, urlencode(data), headers)
        if DEBUG:
            return resp
        resp = resp.lower()
        if resp == 'true':
            return True
        elif resp == 'false':
            return False
        else:
            # NOTE: Happens when you get a 'howdy wilbur' response !
            raise AkismetError('missing required argument.')


    def submit_spam(self, comment, data=None, build_data=True):
        """
        This function is used to tell akismet that a comment it marked as ham,
        is really spam.
        
        It takes all the same arguments as ``comment_check``, except for
        *DEBUG*.
        """
        if self.key is None:
            raise APIKeyError("Your have not set an API key.")
        if data is None:
            data = {}
        if build_data:
            self._build_data(comment, data)
        url = '%ssubmit-spam' % self._getURL()
        # we *don't* trap the error here
        # so if akismet is down it will raise an HTTPError or URLError
        headers = {'User-Agent' : self.user_agent}
        self._safeRequest(url, urlencode(data), headers)


    def submit_ham(self, comment, data=None, build_data=True):
        """
        This function is used to tell akismet that a comment it marked as spam,
        is really ham.
        
        It takes all the same arguments as ``comment_check``, except for
        *DEBUG*.
        """
        if self.key is None:
            raise APIKeyError("Your have not set an API key.")
        if data is None:
            data = {}
        if build_data:
            self._build_data(comment, data)
        url = '%ssubmit-ham' % self._getURL()
        # we *don't* trap the error here
        # so if akismet is down it will raise an HTTPError or URLError
        headers = {'User-Agent' : self.user_agent}
        self._safeRequest(url, urlencode(data), headers)

"""

USAGE
=====

.. raw:: html

    {+coloring}
    
    api = Akismet(agent='Test Script')
    # if apikey.txt is in place,
    # the key will automatically be set
    # or you can call ``api.setAPIKey()``
    #
    if api.key is None:
        print "No 'apikey.txt' file."
    elif not api.verify_key():
        print "The API key is invalid."
    else:
        # data should be a dictionary of values
        # They can all be filled in with defaults
        # from a CGI environment
        if api.comment_check(comment, data):
            print 'This comment is spam.'
        else:
            print 'This comment is ham.'
    
    {-coloring}

TODO
====

Make the timeout adjustable ?

Should we fill in a default value for permalink ?

What about automatically filling in the 'HTTP_*' values from os.environ ?

"""

########NEW FILE########
__FILENAME__ = code
"""Spam control using akismet api.
"""
import socket

# akismet module changes socket default timeout. 
# Undoing it as it might effect other parts of the system.
timeout = socket.getdefaulttimeout()

from akismet import Akismet
socket.setdefaulttimeout(timeout)
import web

from infogami import config
from infogami.infobase import client

if config.get('plugin_akismet') is not None:
    api_key = config.plugin_akismet.api_key
    spamlog = config.plugin_akismet.get('log')
    baseurl = config.plugin_akismet.get('baseurl')
else:
    api_key = config.akismet_api_key
    spamlog = config.get('akismet_log')
    baseurl = config.get('akismet_baseurl')

blog_url = 'http://' + config.site
if baseurl:
    Akismet.antispam_baseurl = baseurl

api = Akismet(key=api_key, blog_url=blog_url, agent='OpenLibrary')

class hooks(client.hook):
    def before_register(self, d):
        data = {}
        data['comment_type'] = "registration"
        data['comment_author'] = d['displayname']
        data['comment_author_email'] = d['email']
        comment = ""
        self.checkspam(comment, data)

    def before_new_version(self, page):
        if page.type.key == '/type/user':
            comment = page['description']
            data = {}
            data['comment_author'] = page['displayname']
            if page.website:
                data['comment_author_url'] = page.website[0]
            self.checkspam(comment, data)
    
    def checkspam(self, comment, data):
        data['user_ip'] = web.ctx.get('ip', '127.0.0.1')

        data['user_agent'] = web.ctx.env.get('HTTP_USER_AGENT', 'Mozilla/5.0')
        data['referrer'] = web.ctx.env.get('HTTP_REFERER', 'unknown')
        data['SERVER_ADDR'] = web.ctx.env.get('SERVER_ADDR', '')
        data['SERVER_ADMIN'] = web.ctx.env.get('SERVER_ADMIN', '')
        data['SERVER_NAME'] = web.ctx.env.get('SERVER_NAME', '')
        data['SERVER_PORT'] = web.ctx.env.get('SERVER_PORT', '')
        data['SERVER_SIGNATURE'] = web.ctx.env.get('SERVER_SIGNATURE', '')
        data['SERVER_SOFTWARE'] = web.ctx.env.get('SERVER_SOFTWARE', '')
        data['HTTP_ACCEPT'] = web.ctx.env.get('HTTP_ACCEPT', '')

        data = dict((web.safestr(k), web.safestr(v)) for k, v in data.items())

        spam = api.comment_check(web.safestr(comment), data)
        if spamlog:
            f = open(spamlog, 'a')
            print >> f, spam, data
            f.close()

        if spam:
            raise Exception("The content not saved becaused it loooked like spam.")

########NEW FILE########
__FILENAME__ = code
"""
Book reviews plugin.
"""
import web, infogami
from infogami.utils import delegate
from infogami.utils.template import render

import db, dev, forms, reviewsources, utils
import schema as _

@infogami.action
def install_bookrev():
    for name, properties in [_.reviewsource, _.bookreview, _.vote, _.comment]:
        db.create_type(name, properties)
    for type, prop, ref_type, ref_prop in _.backreferences:
        db.insert_backreference(type, prop, ref_type, ref_prop)

@infogami.action
def _write_a_review():
    dev.simple_shell()

@infogami.action
def _list_reviews():
    dev.list_latest_reviews()

error = web.notfound

class addreview(delegate.page):
    @utils.require_user
    def GET(self, site, user):
        i = web.input('edition')
        edition = db.get_thing(i.edition, db.get_type('type/edition'))
        if not edition:
            return error()
        form = forms.review_form()
        form.fill(edition=edition.name)
        return render.addreview(user, edition, form)

    @utils.require_user
    def POST(self, site, user):
        form = forms.review_form()
        if form.validates():
	    edition = db.get_thing(form.d.edition, db.get_type('type/edition'))
	    if not edition:
		return error()
            review = db.insert_book_review(edition, 
                                           user, 
                                           reviewsources.data.get('web'), 
                                           form.d.text,
                                           title=form.d.title)
            return web.redirect('/' + edition.name + '#reviews')
        else:
	    edition = db.get_thing(form.d.edition, db.get_type('type/edition'))
	    if not edition:
		return error()
            return render.addreview(user, edition, form)


########NEW FILE########
__FILENAME__ = db
import web
from infogami import config, tdb
from infogami.core import db, thingutil

def get_site():
    return db.get_site(config.site)

def get_type(type_name):
    site = get_site()
    return db.get_type(site, type_name)

def create_type(type_name, properties={}):
    def props():
        for pname, ptype_name in properties.items():
            yield dict(name=pname, type=get_type(ptype_name), unique=True)

    return db._create_type(get_site(), type_name, list(props()))


def insert_backreference(type_name, prop_name, ref_type_name, ref_prop_name):

    type = get_type(type_name)

    def create_backreference(): 
        d = {'type': get_type(ref_type_name), 'property_name': ref_prop_name}
        return db._get_thing(type, 
                             prop_name, 
                             get_type('type/backreference'), 
                             d)

    type.backreferences = type.get('backreferences', [])
    type.backreferences.append(create_backreference())
    type.save()


fill_backreferences = thingutil.thingtidy

def get_thing(name, type):
    site = get_site()
    try:
        thing = tdb.withName(name, site)
        if thing.type.id == type.id:
            fill_backreferences(thing)
            return thing
    except tdb.NotFound:
        pass
    return None

def insert_book_review(edition, user, review_source, text, title=None):
    def unique_name():
        review_count = len(list(edition.get('reviews', [])))
        id = web.to36(review_count + 1)
        return '%s/review/%s' % (edition.name, id)
    site = get_site()
    d = dict(book=edition, author=user, source=review_source)
    review = tdb.new(unique_name(), site, get_type('type/bookreview'), d)
    review.text = text
    review.title = title or edition.title
    review.save(author=user)
    return review

def get_review_source(name, description='', create=True):
    type = get_type('type/reviewsource')
    rs = get_thing(name, type) 
    if not rs and create:
        site = get_site()
        rs = tdb.new(name, site, type, d=None)
        rs.description = description
        rs.save()
    return rs

########NEW FILE########
__FILENAME__ = dev
import web, infogami
from infogami import tdb

import db, reviewsources, utils

def list_latest_reviews():
    for review in tdb.Things(type=db.get_type('type/bookreview'), limit=40):
        print '[%s] %s (%s)' % (review.author, review.book, review.source)

def simple_shell():
    """A simple shell for creating book reviews. For dev use only."""
    
    def create_dummy_user(username, password, displayname=None, email=''):

        username = web.lstrips(username, 'user/')
        displayname = displayname or username

        from infogami.core.db import new_user # hack
        user = new_user(db.get_site(), username, displayname, email, password)
        user.displayname = username
        user.save()

        return user


    class quit(Exception):
        pass


    class Shell:
        def __init__(self):
            self.edition = None
            self.user = None
            self.text = None
            self.rs = reviewsources.data.get('dev')

        def safety_lock(self, query):
            confirm = raw_input('\n%s [n] ' % query)
            if ('y' in confirm) or ('Y' in confirm):
                return
            else:
                raise quit()

        def input_edition_name(self, default=''):
            name = raw_input('\nbook edition? [%s] ' % default) or default
            self.edition = db.get_thing(name, db.get_type('type/edition'))
            if not self.edition:
                print '\nbook edition not found.'
            else:
                print '\nbook edition %s found.' % self.edition

        def input_user_name(self, default=''):
            if not self.edition:
                raise quit()
            name = raw_input('\nreview author? [%s] ' % default) or default
            self.user = db.get_thing(utils.lpad(name, 'user/'), 
                                     db.get_type('type/user'))
            if not self.user:
                print '\nreview author not found.'
                self.safety_lock('create a dummy user \'%s\'?' % name)
                self.user = create_dummy_user(name, password='test')
                print '\nok.'
            else:
                print '\nuser %s found.' % self.user

        def input_text(self): 
            if not self.edition or not self.user:
                raise quit()
            print '\ntype in the review. (exit by typing two continuous line breaks.)\n'
            self.text = utils.read_text()        
            print '\nthanks.'

    try:

        _ = Shell()
        _.safety_lock('\nthis shell is meant for dev use only. continue?')

        _.input_edition_name(default='b/Heartbreaking_of_Genius_1')
        _.input_user_name(default='test')
        _.input_text()

        review = db.insert_book_review(_.edition, _.user, _.rs, _.text)
        print '\ncreated review %s.' % review

    except quit:
        print 'exiting.'


########NEW FILE########
__FILENAME__ = forms
from web.form import Form, Hidden, Textarea, Textbox, Validator

required = Validator("Required", lambda x: x and x.strip())

review_form = Form(
    Hidden('edition', required),
    Textbox('title'),
    Textarea('text', required)
)

########NEW FILE########
__FILENAME__ = reviewsources
import db

_d = {
    'web': 'User reviews (from web).',
    'dev': 'Dev/test reviews.'
}

class ReviewSources:

    def __init__(self):
        self.data = {}

    def load_review_source(self, key):
        self.data[key] = db.get_review_source('rs/%s_reviews' % key, 
                                              description=_d[key],
                                              create=True)

    def get(self, key):
        if key not in _d.keys():
            raise KeyError('Review source \'%s\' does not exist.' % key)
        if key not in self.data.keys():
            self.load_review_source(key) 
        return self.data[key]

data = ReviewSources()

########NEW FILE########
__FILENAME__ = schema
# type definitions: (<name>, <properties>)

reviewsource = ('type/reviewsource', {
    'description': 'type/string'
})

bookreview = ('type/bookreview', {
    'book': 'type/edition', 
    'author': 'type/user', 
    'title': 'type/string',
    'source': 'type/reviewsource', 
    'text': 'type/text',
    'url': 'type/string'
})

vote = ('type/vote', {
    'review': 'type/bookreview', 
    'user': 'type/user',
    'weight': 'type/int'
})

comment = ('type/comment', {
    'running_id': 'type/int',
    'target': 'type/type',
    'author': 'type/user',
    'parent_comment': 'type/type', # makes impl simpler
    'text': 'type/text'
})


# backreferences: (<type_name>, <prop_name>, <ref_type_name>, <ref_prop_name>)

backreferences = [
    ('type/user', 'reviews', 'type/bookreview', 'author'),
    ('type/edition', 'reviews', 'type/bookreview', 'book'),
    ('type/bookreview', 'votes', 'type/vote', 'review'),
    ('type/bookreview', 'comments', 'type/comment', 'target')
]


########NEW FILE########
__FILENAME__ = utils
import web
from infogami.core import auth

def require_user(f):
    def g(*a, **kw):    
        self, site, params = a[0], a[1], a[2:]
        user = auth.get_user(site)
        if user:
            a = [self, site, user] + list(params)
            return f(*a, **kw)
        else:            
            return web.redirect('/account/login')
    return g

def lpad(s, lpad):
    if not s:
        return lpad
    elif s.startswith(lpad):
        return s
    else:
        return '%s%s' % (lpad, s)

def read_text():
    lines = []
    line = raw_input()
    while line:
        lines.append(line)
        line = raw_input()
    return "\n\n".join(lines)

########NEW FILE########
__FILENAME__ = code
"""Open Library Books API
"""

from infogami.plugins.api.code import add_hook
import dynlinks
import readlinks

import web
from infogami.infobase import _json as simplejson

from infogami.utils import delegate
from infogami.plugins.api.code import jsonapi

import urlparse
import re
import urllib2

class books:
    def GET(self):
        i = web.input(bibkeys='', callback=None, details="false")
        
        web.ctx.headers = []
        if i.get("format") == "json":
            web.header('Content-Type', 'application/json')
        else:
            web.header('Content-Type', 'text/javascript')
        
        return dynlinks.dynlinks(i.bibkeys.split(","), i)
        
add_hook("books", books)


class read_singleget(delegate.page):
    """Handle the single-lookup form of the Hathi-style API
    """
    path = r"/api/volumes/(brief|full)/(oclc|lccn|issn|isbn|htid|olid|recordnumber)/(.+)"
    encoding = "json"
    @jsonapi
    def GET(self, brief_or_full, idtype, idval):
        i = web.input()

        web.ctx.headers = []
        req = '%s:%s' % (idtype, idval)
        result = readlinks.readlinks(req, i)
        if req in result:
            result = result[req]
        else:
            result = []
        return simplejson.dumps(result)


class read_multiget(delegate.page):
    """Handle the multi-lookup form of the Hathi-style API
    """
    path = r"/api/volumes/(brief|full)/json/(.+)"
    path_re = re.compile(path)
    @jsonapi
    def GET(self, brief_or_full, req): # params aren't used, see below
        i = web.input()

        # Work around issue with gunicorn where semicolon and after
        # get truncated.  (web.input() still seems ok)
        # see https://github.com/benoitc/gunicorn/issues/215
        raw_uri = web.ctx.env.get("RAW_URI")
        raw_path = urlparse.urlsplit(raw_uri).path

        # handle e.g. '%7C' for '|'
        decoded_path = urllib2.unquote(raw_path)

        m = self.path_re.match(decoded_path)
        if not len(m.groups()) == 2:
            return simplejson.dumps({})
        (brief_or_full, req) = m.groups()

        web.ctx.headers = []
        result = readlinks.readlinks(req, i)
        return simplejson.dumps(result)

########NEW FILE########
__FILENAME__ = dynlinks
import simplejson
import web
import sys
import traceback

from openlibrary.plugins.openlibrary.processors import urlsafe
from openlibrary.core import helpers as h
from openlibrary.core import ia

from infogami.utils.delegate import register_exception

def split_key(bib_key):
    """
        >>> split_key('1234567890')
        ('isbn_', '1234567890')
        >>> split_key('ISBN:1234567890')
        ('isbn_', '1234567890')
        >>> split_key('ISBN1234567890')
        ('isbn_', '1234567890')
        >>> split_key('ISBN1234567890123')
        ('isbn_', '1234567890123')
        >>> split_key('LCCNsa 64009056')
        ('lccn', 'sa 64009056')
        >>> split_key('badkey')
        (None, None)
    """
    bib_key = bib_key.strip()
    if not bib_key:
        return None, None

    valid_keys = ['isbn', 'lccn', 'oclc', 'ocaid', 'olid']
    key, value = None, None

    # split with : when possible
    if ':' in bib_key:
        key, value = bib_key.split(':', 1)
        key = key.lower()
    else:
        # try prefix match
        for k in valid_keys:
            if bib_key.lower().startswith(k):
                key = k
                value = bib_key[len(k):]
                continue
                
    # treat plain number as ISBN
    if key is None and bib_key[0].isdigit():
        key = 'isbn'
        value = bib_key
        
    # treat OLxxxM as OLID
    re_olid = web.re_compile('OL\d+M(@\d+)?')
    if key is None and re_olid.match(bib_key.upper()):
        key = 'olid'
        value = bib_key.upper()
    
    if key == 'isbn':
        # 'isbn_' is a special indexed filed that gets both isbn_10 and isbn_13 in the normalized form.
        key = 'isbn_'
        value = value.replace("-", "") # normalize isbn by stripping hyphens

    if key == 'oclc':
        key = 'oclc_numbers'
        
    if key == 'olid':
        key = 'key'
        value = '/books/' + value.upper()

    return key, value


def ol_query(name, value):
    query = {
        'type': '/type/edition',
        name: value,
    }
    keys = web.ctx.site.things(query)
    if keys:
        return keys[0]
        
def ol_get_many_as_dict(keys):
    keys_with_revisions = [k for k in keys if '@' in k]
    keys2 = [k for k in keys if '@' not in k]
    
    result = dict((doc['key'], doc) for doc in ol_get_many(keys2))
    
    for k in keys_with_revisions:
        key, revision = k.split('@', 1)
        revision = h.safeint(revision, None)
        doc = web.ctx.site.get(key, revision)
        result[k] = doc and doc.dict()
    
    return result

def ol_get_many(keys):
    return [doc.dict() for doc in web.ctx.site.get_many(keys)]
    
def query_keys(bib_keys):
    """Given a list of bibkeys, returns a mapping from bibkey to OL key.
    
        >> query(["isbn:1234567890"])
        {"isbn:1234567890": "/books/OL1M"}
    """
    def query(bib_key):
        name, value = split_key(bib_key)
        if name is None:
            return None
        elif name == 'key':
            return value
        else:
            return ol_query(name, value)
    
    d = dict((bib_key, query(bib_key)) for bib_key in bib_keys)
    return dict((k, v) for k, v in d.items() if v is not None)
    
def query_docs(bib_keys):
    """Given a list of bib_keys, returns a mapping from bibkey to OL doc.
    """
    mapping = query_keys(bib_keys)
    thingdict = ol_get_many_as_dict(uniq(mapping.values()))
    return dict((bib_key, thingdict[key]) for bib_key, key in mapping.items() if key in thingdict)
    
def uniq(values):
    return list(set(values))
    
def process_result(result, jscmd):
    d = {
        "details": process_result_for_details,
        "data": DataProcessor().process,
        "viewapi": process_result_for_viewapi
    }
    
    f = d.get(jscmd) or d['viewapi']
    return f(result)
    
def get_many_as_dict(keys):
    return dict((doc['key'], doc) for doc in ol_get_many(keys))
    
def get_url(doc):
    base = web.ctx.get("home", "https://openlibrary.org")
    if base == 'http://[unknown]':
        base = "https://openlibrary.org"
    if doc['key'].startswith("/books/") or doc['key'].startswith("/works/"):
        return base + doc['key'] + "/" + urlsafe(doc.get("title", "untitled"))
    elif doc['key'].startswith("/authors/"):
        return base + doc['key'] + "/" + urlsafe(doc.get("name", "unnamed"))
    else:
        return base + doc['key']
    
class DataProcessor:
    """Processor to process the result when jscmd=data.
    """
    def process(self, result):
        work_keys = [w['key'] for doc in result.values() for w in doc.get('works', [])]
        self.works = get_many_as_dict(work_keys)
        
        author_keys = [a['author']['key'] for w in self.works.values() for a in w.get('authors', [])]
        self.authors = get_many_as_dict(author_keys)
        
        return dict((k, self.process_doc(doc)) for k, doc in result.items())
        
    def get_authors(self, work):
        author_keys = [a['author']['key'] for a in work.get('authors', [])]
        return [{"url": get_url(self.authors[key]), "name": self.authors[key].get("name", "")} for key in author_keys]
    
    def get_work(self, doc):
        works = [self.works[w['key']] for w in doc.get('works', [])]
        if works:
            return works[0]
        else:
            return {}
        
    def process_doc(self, doc):
        """Processes one document.
        Should be called only after initializing self.authors and self.works.
        """
        w = self.get_work(doc)
        
        def subject(name, prefix):
            # handle bad subjects loaded earlier.
            if isinstance(name, dict):
                if 'value' in name:
                    name = name['value']
                elif 'key' in name:
                    name = name['key'].split("/")[-1].replace("_", " ")
                else:
                    return {}
                
            return {
                "name": name,
                "url": "https://openlibrary.org/subjects/%s%s" % (prefix, name.lower().replace(" ", "_"))
            }
            
        def get_subjects(name, prefix):
            return [subject(s, prefix) for s in w.get(name, '')]
            
        def get_value(v):
            if isinstance(v, dict):
                return v.get('value', '')
            else:
                return v
            
        def format_excerpt(e):
            return {
                "text": get_value(e.get("excerpt", {})),
                "comment": e.get("comment", "")
            }

        def format_table_of_contents(toc):
            # after openlibrary.plugins.upstream.models.get_table_of_contents
            def row(r):
                if isinstance(r, basestring):
                    level = 0
                    label = ""
                    title = r
                    pagenum = ""
                else:
                    level = h.safeint(r.get('level', '0'), 0)
                    label = r.get('label', '')
                    title = r.get('title', '')
                    pagenum = r.get('pagenum', '')
                r = dict(level=level, label=label, title=title, pagenum=pagenum)
                return r
            d = [row(r) for r in toc]
            return [row for row in d if any(row.values())]

        d = {
            "url": get_url(doc),
            "key": doc['key'],
            "title": doc.get("title", ""),
            "subtitle": doc.get("subtitle", ""),
            
            "authors": self.get_authors(w),

            "number_of_pages": doc.get("number_of_pages", ""),
            "pagination": doc.get("pagination", ""),
            
            "weight": doc.get("weight", ""),
            
            "by_statement": doc.get("by_statement", ""),

            'identifiers': web.dictadd(doc.get('identifiers', {}), {
                'isbn_10': doc.get('isbn_10', []),
                'isbn_13': doc.get('isbn_13', []),
                'lccn': doc.get('lccn', []),
                'oclc': doc.get('oclc_numbers', []),
                'openlibrary': [doc['key'].split("/")[-1]]
            }),
            
            'classifications': web.dictadd(doc.get('classifications', {}), {
                'lc_classifications': doc.get('lc_classifications', []),
                'dewey_decimal_class': doc.get('dewey_decimal_class', [])
            }),
            
            "publishers": [{"name": p} for p in doc.get("publishers", "")],
            "publish_places": [{"name": p} for p in doc.get("publish_places", "")],
            "publish_date": doc.get("publish_date"),
            
            "subjects": get_subjects("subjects", ""),
            "subject_places": get_subjects("subject_places", "place:"),
            "subject_people": get_subjects("subject_people", "person:"),
            "subject_times": get_subjects("subject_times", "time:"),
            "excerpts": [format_excerpt(e) for e in w.get("excerpts", [])],

            "notes": get_value(doc.get("notes", "")),
            "table_of_contents": format_table_of_contents(doc.get("table_of_contents", [])),

            "links": [dict(title=link.get("title"), url=link['url']) for link in w.get('links', '') if link.get('url')],
        }

        for fs in [doc.get("first_sentence"), w.get('first_sentence')]:
            if fs:
                e = {
                    "text": get_value(fs),
                    "comment": "",
                    "first_sentence": True
                    }
                d['excerpts'].insert(0, e)
                break
        
        def ebook(doc):
            itemid = doc['ocaid']
            availability = get_ia_availability(itemid)
            
            d = {
                "preview_url": "https://archive.org/details/" + itemid,
                "availability": availability
            }
                
            prefix = "https://archive.org/download/%s/%s" % (itemid, itemid)
            if availability == 'full':
                d["read_url"] = "https://archive.org/stream/%s" % (itemid)
                d['formats'] = {
                    "pdf": {
                        "url": prefix + ".pdf"
                    },
                    "epub": {
                        "url": prefix + ".epub"
                    },
                    "text": {
                        "url": prefix + "_djvu.txt"
                    },
                    "djvu": {
                        "url": prefix + ".djvu",
                        "permission": "open"
                    }
                }
            elif availability == "borrow":
                d['borrow_url'] = u"https://openlibrary.org%s/%s/borrow" % (doc['key'], h.urlsafe(doc.get("title", "untitled")))
                loanstatus =  web.ctx.site.store.get('ebooks' + doc['key'], {'borrowed': 'false'})
                d['checkedout'] = (loanstatus['borrowed'] == 'true')
                d['formats'] = {
                    "djvu": {
                        "url": prefix + ".djvu",
                        "permission": "restricted"
                    }
                }
            else:
                d['formats'] = {
                    "djvu": {
                        "url": prefix + ".djvu",
                        "permission": "restricted"
                    }
                }
                
            return d

        if doc.get("ocaid"):
            d['ebooks'] = [ebook(doc)]
        
        if doc.get('covers'):
            cover_id = doc['covers'][0]
            d['cover'] = {
                "small": "https://covers.openlibrary.org/b/id/%s-S.jpg" % cover_id,
                "medium": "https://covers.openlibrary.org/b/id/%s-M.jpg" % cover_id,
                "large": "https://covers.openlibrary.org/b/id/%s-L.jpg" % cover_id,
            }

        d['identifiers'] = trim(d['identifiers'])
        d['classifications'] = trim(d['classifications'])
        return trim(d)
        
def trim(d):
    """Remote empty values from given dictionary.
    
        >>> trim({"a": "x", "b": "", "c": [], "d": {}})
        {'a': 'x'}
    """
    return dict((k, v) for k, v in d.iteritems() if v)
    
def get_authors(docs):
    """Returns a dict of author_key to {"key", "...", "name": "..."} for all authors in docs.
    """
    authors = [a['key'] for doc in docs for a in doc.get('authors', [])]
    author_dict = {}
    
    if authors:
        for a in ol_get_many(uniq(authors)):
            author_dict[a['key']] = {"key": a['key'], "name": a.get("name", "")}
    
    return author_dict

def process_result_for_details(result):
    def f(bib_key, doc):
        d = process_doc_for_viewapi(bib_key, doc)
        
        if 'authors' in doc:
            doc['authors'] = [author_dict[a['key']] for a in doc['authors']]
            
        d['details'] = doc
        return d
    
    author_dict = get_authors(result.values())
    return dict((k, f(k, doc)) for k, doc in result.items())

def process_result_for_viewapi(result):
    return dict((k, process_doc_for_viewapi(k, doc)) for k, doc in result.items())
    

def get_ia_availability(itemid):
    collections = ia.get_meta_xml(itemid).get("collection", [])

    if 'lendinglibrary' in collections:
        return 'borrow'
    elif 'printdisabled' in collections:
        return 'restricted'
    else:
        return 'full'

def process_doc_for_viewapi(bib_key, page):
    key = page['key']
    
    url = get_url(page)
    
    if 'ocaid' in page:
        preview = get_ia_availability(page['ocaid'])
        preview_url = 'https://archive.org/details/' + page['ocaid']
    else:
        preview = 'noview'
        preview_url = url
        
    d = {
        'bib_key': bib_key,
        'info_url': url,
        'preview': preview,
        'preview_url': preview_url,
    }
    
    if page.get('covers'):
        d['thumbnail_url'] = 'https://covers.openlibrary.org/b/id/%s-S.jpg' % page["covers"][0]

    return d      

def format_result(result, options):
    """Format result as js or json.
    
        >>> format_result({'x': 1}, {})
        'var _OLBookInfo = {"x": 1};'
        >>> format_result({'x': 1}, {'callback': 'f'})
        'f({"x": 1});'
    """
    format = options.get('format', '').lower()
    if format == 'json':
        return simplejson.dumps(result)
    else: # js
        json = simplejson.dumps(result)
        callback = options.get("callback")
        if callback:
            return "%s(%s);" % (callback, json)
        else:
            return "var _OLBookInfo = %s;" % json    

def dynlinks(bib_keys, options):
    # for backward-compatibility
    if options.get("details", "").lower() == "true":
        options["jscmd"] = "details"
    
    try:    
        result = query_docs(bib_keys)
        result = process_result(result, options.get('jscmd'))
    except:
        print >> sys.stderr, "Error in processing Books API"
        register_exception()
        
        result = {}
    return format_result(result, options)
    
if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = readlinks
""" 'Read' api implementation.  This is modeled after the HathiTrust
Bibliographic API, but also includes information about loans and other
editions of the same work that might be available.
"""
import sys
import urllib
import re

import web
from openlibrary.core import inlibrary
from openlibrary.core import ia
from openlibrary.core import helpers
from openlibrary.api import OpenLibrary
from infogami.infobase import _json as simplejson
from infogami.utils.delegate import register_exception
from infogami.utils import stats
from infogami import config

import dynlinks


def key_to_olid(key):
    return key.split('/')[-1]


def ol_query(name, value):
    query = {
        'type': '/type/edition',
        name: value,
    }
    keys = web.ctx.site.things(query)
    if keys:
        return keys[0]

def get_works_solr_select_url():
    c = config.get("plugin_worksearch")
    host = c and c.get('solr')
    return host and ("http://" + host + "/solr/works/select")


def get_work_iaids(wkey):
    wid = wkey.split('/')[2]
    solr_select_url = get_works_solr_select_url()
    filter = 'ia'
    q = 'key:' + wid
    stats.begin('solr', url=wkey)
    solr_select = solr_select_url + "?version=2.2&q.op=AND&q=%s&rows=10&fl=%s&qt=standard&wt=json" % (q, filter)
    json_data = urllib.urlopen(solr_select).read()
    stats.end()
    print json_data
    reply = simplejson.loads(json_data)
    if reply['response']['numFound'] == 0:
        return []
    return reply["response"]['docs'][0].get(filter, [])


# multi-get version (not yet used)
def get_works_iaids(wkeys):
    solr_select_url = get_works_solr_select_url()
    filter = 'ia'
    q = '+OR+'.join(['key:' + wkey.split('/')[2] for wkey in wkeys])
    solr_select = solr_select_url + "?version=2.2&q.op=AND&q=%s&rows=10&fl=%s&qt=standard&wt=json" % (q, filter)
    json_data = urllib.urlopen(solr_select).read()
    reply = simplejson.loads(json_data)
    if reply['response']['numFound'] == 0:
        return []
    return reply


def get_eids_for_wids(wids):
    """ To support testing by passing in a list of work-ids - map each to
    it's first edition ID """
    solr_select_url = get_works_solr_select_url()
    filter = 'edition_key'
    q = '+OR+'.join(wids)
    solr_select = solr_select_url + "?version=2.2&q.op=AND&q=%s&rows=10&fl=key,%s&qt=standard&wt=json" % (q, filter)
    json_data = urllib.urlopen(solr_select).read()
    reply = simplejson.loads(json_data)
    if reply['response']['numFound'] == 0:
        return []
    rows = reply['response']['docs']
    result = dict((r['key'], r[filter][0]) for r in rows if len(r.get(filter, [])))
    return result

# Not yet used.  Solr editions aren't up-to-date (6/2011)
def get_solr_edition_records(iaids):
    solr_select_url = get_works_solr_select_url()
    filter = 'title'
    q = '+OR+'.join('ia:' + id for id in iaids)
    solr_select = solr_select_url + "?version=2.2&q.op=AND&q=%s&rows=10&fl=key,%s&qt=standard&wt=json" % (q, filter)
    json_data = urllib.urlopen(solr_select).read()
    reply = simplejson.loads(json_data)
    if reply['response']['numFound'] == 0:
        return []
    rows = reply['response']['docs']
    return rows
    result = dict((r['key'], r[filter][0]) for r in rows if len(r.get(filter, [])))
    return result


class ReadProcessor:
    def __init__(self, options):
        self.options = options
        self.set_inlibrary = False

    def get_inlibrary(self):
        if not self.set_inlibrary:
            self.set_inlibrary = True
            self.inlibrary = inlibrary.get_library()
        return self.inlibrary
        

    def get_item_status(self, ekey, collections, subjects):
        if 'lendinglibrary' in collections:
            if not 'Lending library' in subjects:
                status = 'restricted'
            else:
                status = 'lendable'
        elif 'inlibrary' in collections:
            if not 'In library' in subjects:
                status = 'restricted'
            elif not self.get_inlibrary():
                status = 'restricted'
                if self.options.get('debug_items'):
                    status = 'restricted - not inlib'
                elif self.options.get('show_inlibrary'):
                    status = 'lendable'
            else:
                status = 'lendable'
        elif 'printdisabled' in collections:
            status = 'restricted'
        else:
            status = 'full access'

        if status == 'lendable':
            loanstatus =  web.ctx.site.store.get('ebooks' + ekey, {'borrowed': 'false'})
            if loanstatus['borrowed'] == 'true':
                status = 'checked out'

        return status


    def get_readitem(self, iaid, orig_iaid, orig_ekey, wkey, status, publish_date):
        meta = self.iaid_to_meta.get(iaid)
        if meta is None:
            return None

        collections = meta.get("collection", [])

        if status == 'missing':
            return None

        if (status.startswith('restricted') or status == 'checked out') and not self.options.get('show_all_items'):
            return None

        edition = self.iaid_to_ed.get(iaid)
        ekey = edition.get('key', '')

        if status == 'full access':
            itemURL = 'http://www.archive.org/stream/%s' % (iaid)
        else:
            # this could be rewrit in terms of iaid...
            itemURL = u'http://openlibrary.org%s/%s/borrow' % (ekey,
                                                               helpers.urlsafe(edition.get('title',
                                                                                           'untitled')))
        result = {
            # XXX add lastUpdate
            'enumcron': False,
            'match': 'exact' if iaid == orig_iaid else 'similar',
            'status': status,
            'fromRecord': orig_ekey,
            'ol-edition-id': key_to_olid(ekey),
            'ol-work-id': key_to_olid(wkey),
            'publishDate': publish_date,
            'contributor': '',
            'itemURL': itemURL,
            }

        if edition.get('covers'):
            cover_id = edition['covers'][0]
            # can be rewrit in terms of iaid
            # XXX covers url from yaml?
            result['cover'] = {
                "small": "http://covers.openlibrary.org/b/id/%s-S.jpg" % cover_id,
                "medium": "http://covers.openlibrary.org/b/id/%s-M.jpg" % cover_id,
                "large": "http://covers.openlibrary.org/b/id/%s-L.jpg" % cover_id,
                }

        return result

    date_pat = r'\D*(\d\d\d\d)\D*'
    date_re = re.compile(date_pat)

    def make_record(self, bib_keys):
        # XXX implement hathi no-match logic?
        found = False
        for k in bib_keys:
            if k in self.docs:
                found = True
                break
        if not found:
            return None
        doc = self.docs[k]
        data = self.datas[k]
        details = self.detailss.get(k)
       
        # determine potential ia items for this identifier,
        orig_iaid = doc.get('ocaid')
        doc_works = doc.get('works')
        if doc_works and len(doc_works) > 0:
            wkey = doc_works[0]['key']
        else:
            wkey = None
        work = None
        subjects = []
        if wkey:
            work = self.works.get(wkey)
            subjects = work.get('subjects', [])
            iaids = self.wkey_to_iaids[wkey]
            # rearrange so any scan for this edition is first
            if orig_iaid and orig_iaid in iaids:
                iaids.pop(iaids.index(orig_iaid))
                iaids.insert(0, orig_iaid)
        elif orig_iaid:
            # attempt to handle work-less editions
            iaids = [ orig_iaid ]
        else:
            iaids = []
        orig_ekey = data['key']

        # Sort iaids.  Is there a more concise way?

        def getstatus(self, iaid):
            meta = self.iaid_to_meta.get(iaid)
            if not meta:
                status = 'missing'
                edition = None
            else:
                collections = meta.get("collection", [])
                edition = self.iaid_to_ed.get(iaid)
            if not edition:
                status = 'missing'
            else:
                ekey = edition.get('key', '')
                status = self.get_item_status(ekey, collections, subjects)
            return status

        def getdate(self, iaid):
            edition = self.iaid_to_ed.get(iaid)
            if edition:
                m = self.date_re.match(edition.get('publish_date', ''))
                if m:
                    return m.group(1)
            return ''

        iaids_tosort = [(iaid, getstatus(self, iaid), getdate(self, iaid))
                        for iaid in iaids]

        def sortfn(sortitem):
            iaid, status, date = sortitem
            if iaid == orig_iaid and (status == 'full access' or status == 'lendable'):
                isexact = '000'
            else:
                isexact = '999'
            # sort dateless to end
            if date == '':
                date = 5000
            date = int(date)
            # reverse-sort modern works by date
            if status == 'lendable' or status == 'checked out':
                date = 10000 - date
            statusvals = { 'full access': 1,
                           'lendable': 2,
                           'checked out': 3,
                           'restricted': 4,
                           'restricted - not inlib': 4,
                           'missing': 5 }
            return (isexact, statusvals[status], date)

        iaids_tosort.sort(key=sortfn)

        items = [self.get_readitem(iaid, orig_iaid, orig_ekey, wkey, status, date)
                 for iaid, status, date in iaids_tosort] # if status != 'missing'
        items = [item for item in items if item]

        ids = data.get('identifiers', {})
        if self.options.get('no_data'):
            returned_data = None
        else:
            returned_data = data
        result = {'records':
            { data['key']:
                  { 'isbns': sum((ids.get('isbn_10', []), ids.get('isbn_13', [])), []),
                    'issns': [],
                    'lccns': ids.get('lccn', []),
                    'oclcs': ids.get('oclc', []),
                    'olids': [ key_to_olid(data['key']) ],
                    'publishDates': [ data.get('publish_date', '') ],
                    'recordURL': data['url'],
                    'data': returned_data,
                    'details': details,
                    } },
            'items': items }

        if self.options.get('debug_items'):
            result['tosort'] = iaids_tosort
        return result


    def process(self, req):
        requests = req.split('|')
        bib_keys = sum([r.split(';') for r in requests], [])

        # filter out 'id:foo' before passing to dynlinks
        bib_keys = [k for k in bib_keys if k[:3].lower() != 'id:']

        self.docs = dynlinks.query_docs(bib_keys)
        if not self.options.get('no_details'):
            self.detailss = dynlinks.process_result_for_details(self.docs)
        else:
            self.detailss = {}
        dp = dynlinks.DataProcessor()
        self.datas = dp.process(self.docs)
        self.works = dp.works

        # XXX control costs below with [:iaid_limit] - note that this may result
        # in no 'exact' item match, even if one exists
        # Note that it's available thru above works/docs
        iaid_limit = 500
        self.wkey_to_iaids = dict((wkey, get_work_iaids(wkey)[:iaid_limit])
                                  for wkey in self.works)
        iaids = sum(self.wkey_to_iaids.values(), [])
        self.iaid_to_meta = dict((iaid, ia.get_meta_xml(iaid)) for iaid in iaids)

        def lookup_iaids(iaids):
            step = 10
            if len(iaids) > step and not self.options.get('debug_things'):
                result = []
                while iaids:
                    result += lookup_iaids(iaids[:step])
                    iaids = iaids[step:]
                return result
            query = {
                'type': '/type/edition',
                'ocaid': iaids,
            }
            result = web.ctx.site.things(query)
            return result

        ekeys = lookup_iaids(iaids)

        # If returned order were reliable, I could skip the below.
        eds = dynlinks.ol_get_many_as_dict(ekeys)
        self.iaid_to_ed = dict((ed['ocaid'], ed) for ed in eds.values())
        # self.iaid_to_ekey = dict((iaid, ed['key'])
        #                            for iaid, ed in self.iaid_to_ed.items())

        # Work towards building a dict of iaid loanability,
        # def has_lending_collection(meta):
        #     collections = meta.get("collection", [])
        #     return 'lendinglibrary' in collections or 'inlibrary' in collections
        # in case site.store supports get_many (unclear)
        # maybe_loanable_iaids = [iaid for iaid in iaids
        #                         if has_lending_collection(self.iaid_to_meta.get(iaid, {}))]
        # loanable_ekeys = [self.iaid_to_ekey.get(iaid) for iaid in maybe_loanable_iaids]
        # loanstatus =  web.ctx.site.store.get('ebooks' + ekey, {'borrowed': 'false'})

        result = {}
        for r in requests:
            bib_keys = r.split(';')
            if r.lower().startswith('id:'):
                result_key = bib_keys.pop(0)[3:]
            else:
                result_key = r
            sub_result = self.make_record(bib_keys)
            if sub_result:
                result[result_key] = sub_result

        if self.options.get('debug_items'):
            result['ekeys'] = ekeys
            result['eds'] = eds
            result['iaids'] = iaids

        return result


def readlinks(req, options):
    try:
        dbstr = 'debug|'
        if req.startswith(dbstr):
            options = {
                'stats': True,
                'show_exception': True,
                'no_data': True,
                'no_details': True,
                'show_all_items': True
            }
            req = req[len(dbstr):]
        rp = ReadProcessor(options)

        if options.get('listofworks'):
            """ For load-testing, handle a special syntax """
            wids = req.split('|')
            mapping = get_eids_for_wids(wids[:5])
            req = '|'.join(('olid:' + k) for k in mapping.values())

        result = rp.process(req)

        if options.get('stats'):
            summary = stats.stats_summary()
            s = {}
            result['stats'] = s
            s['summary'] = summary
            s['stats'] = web.ctx.get('stats', [])
    except:
        print >> sys.stderr, 'Error in processing Read API'
        if options.get('show_exception'):
            register_exception()
            result = {'success': False}
        else:
            register_exception()
        result = {}
    return result

########NEW FILE########
__FILENAME__ = test_doctests

import doctest
import py.test

def test_doctest():
    modules = [
        "openlibrary.plugins.books.dynlinks"
    ]
    for t in find_doctests(modules):
        yield run_doctest, t

def find_doctests(modules):
    finder = doctest.DocTestFinder()
    for m in modules:
        mod = __import__(m, None, None, ['x'])
        for t in finder.find(mod, mod.__name__):
            yield t
        
def run_doctest(test):
    runner = doctest.DocTestRunner(verbose=True)
    failures, tries = runner.run(test)
    if failures:
        py.test.fail("doctest failed: " + test.name)

########NEW FILE########
__FILENAME__ = test_dynlinks
"""Test suite for dynlinks.

Most of the tests here use 3 sets of data. 

data0: This contains OL0A, OL0M and OL0W with each having just name/title.
data1: This contains OL1A, OL1M, OL1W with each having name/tile and interconnections.
data9: This contans OL9A, OL9M and OL9W with interconnections and almost all fields.
"""
from .. import dynlinks

import re
import simplejson
import web
from openlibrary.mocks import mock_infobase
from openlibrary.core import ia

def pytest_funcarg__data0(request):
    return {
        "/books/OL0M": {
            "key": "/books/OL0M",
            "title": "book-0"
        },
        "/authors/OL0A": {
            "key": "/authors/OL0A",
            "name": "author-0"
        },
        "/works/OL0W": {
            "key": "/works/OL0W",
            "title": "work-0"
        },
        "result": {
            "data": {
                "url": "http://openlibrary.org/books/OL0M/book-0",
                "key": "/books/OL0M",
                "title": "book-0",
                "identifiers": {
                    "openlibrary": ["OL0M"]
                }
            }
        }
    }

def pytest_funcarg__data1(request):
    return {
        "/books/OL1M": {
            "key": "/books/OL1M",
            "title": "foo",
            "works": [{"key": "/works/OL1W"}]
        },
        "/authors/OL1A": {
            "key": "/authors/OL1A",
            "name": "Mark Twain"
        },
        "/works/OL1W": {
            "key": "/works/OL1W",
            "title": "Foo",
            "authors": [{
                "author": {"key": "/authors/OL1A"}
            }]
        }
    }
    
def pytest_funcarg__data9(request):
    return {
        "/authors/OL9A": {
            "key": "/authors/OL9A",
            "name": "Mark Twain"
        },
        "/works/OL9W": {
            "key": "/works/OL9W",
            "title": "Foo",
            "authors": [{
                "author": {"key": "/authors/OL9A"}
            }],
            "links": [{
                "title": "wikipedia article",
                "url": "http://en.wikipedia.org/wiki/foo"
            }],
            "subjects": ["Test Subject"],
            "subject_people": ["Test Person"],
            "subject_places": ["Test Place"],
            "subject_times": ["Test Time"],
            "excerpts": [{
                "excerpt": {
                    "type": "/type/text",
                    "value": "This is an excerpt."
                },
                "comment": "foo"
            }, {
                # sometimes excerpt was plain string instead of /type/text.
                "excerpt": "This is another excerpt.",
                "comment": "bar"
            }]
        },
        "/books/OL9M": {
            "key": "/books/OL9M",
            "title": "foo",
            "subtitle": "bar",
            "by_statement":  "Mark Twain",
            "works": [{"key": "/works/OL9W"}],
            "publishers": ["Dover Publications"],
            "publish_places": ["New York"],
            "identifiers": {
                "goodreads": ["12345"]
            },
            "isbn_10": ["1234567890"],
            "lccn": ["lccn-1"],
            "oclc_numbers": ["oclc-1"],
            "classifications": {
                "indcat": ["12345"]
            },
            "lc_classifications": ["LC1234"],
            "covers": [42, 53],
            "ocaid": "foo12bar",
            "number_of_pages": "100",
            "pagination": "100 p."
        },
        "result": {
            "viewapi": {
                "info_url": "http://openlibrary.org/books/OL9M",
                "thumbnail_url": "http://covers.openlibrary.org/b/id/42-S.jpg",
                "preview": "noview",
                "preview_url": "http://openlibrary.org/books/OL9M",
            },
            "data": {
                "url": "http://openlibrary.org/books/OL9M/foo",
                "key": "/books/OL9M",
                "title": "foo",
                "subtitle": "bar",
                "by_statement": "Mark Twain",
                "authors": [{
                    "url": "http://openlibrary.org/authors/OL9A/Mark_Twain",
                    "name": "Mark Twain"
                }],
                "identifiers": {
                    "isbn_10": ["1234567890"],
                    "lccn": ["lccn-1"],
                    "oclc": ["oclc-1"],
                    "goodreads": ["12345"],
                    "openlibrary": ["OL9M"]
                },
                "classifications": {
                    "lc_classifications": ["LC1234"],
                    "indcat": ["12345"]
                },
                "publishers": [{
                    "name": "Dover Publications"
                }],
                "publish_places": [{
                    "name": "New York"
                }],
                "links": [{
                    "title": "wikipedia article",
                    "url": "http://en.wikipedia.org/wiki/foo"
                }],
                'subjects': [{
                    'url': 'http://openlibrary.org/subjects/test_subject', 
                    'name': 'Test Subject'
                }], 
                'subject_places': [{
                    'url': 'http://openlibrary.org/subjects/place:test_place', 
                    'name': 'Test Place'
                }],
                'subject_people': [{
                    'url': 'http://openlibrary.org/subjects/person:test_person', 
                    'name': 'Test Person'
                }], 
                'subject_times': [{
                    'url': 'http://openlibrary.org/subjects/time:test_time', 
                    'name': 'Test Time'
                }],
                "cover": {
                    "small": "http://covers.openlibrary.org/b/id/42-S.jpg",
                    "medium": "http://covers.openlibrary.org/b/id/42-M.jpg",
                    "large": "http://covers.openlibrary.org/b/id/42-L.jpg",
                },
                "excerpts": [{
                    "text": "This is an excerpt.",
                    "comment": "foo",
                }, {
                    "text": "This is another excerpt.",
                    "comment": "bar"
                }],
                "ebooks": [{
                    "preview_url": "http://www.archive.org/details/foo12bar",
                    "read_url": "http://www.archive.org/stream/foo12bar",
                    "availability": "full",
                    "formats": {
                        "pdf": {
                            "url": "http://www.archive.org/download/foo12bar/foo12bar.pdf"
                        },
                        "epub": {
                            "url": "http://www.archive.org/download/foo12bar/foo12bar.epub"
                        },
                        "text": {
                            "url": "http://www.archive.org/download/foo12bar/foo12bar_djvu.txt"
                        },
                        "djvu": {
                            "url": "http://www.archive.org/download/foo12bar/foo12bar.djvu",
                            "permission": "open"
                        },
                    }
                }],
                "number_of_pages": "100",
                "pagination": "100 p."
            }
        }
    }

class Mock:
    def __init__(self):
        self.calls = []
        self.default = None
        
    def __call__(self, *a, **kw):
        for a2, kw2, _return in self.calls:
            if (a, kw) == (a2, kw2):
                return _return
        return self.default
        
    def setup_call(self, *a, **kw):
        _return = kw.pop("_return", None)
        call = a, kw, _return
        self.calls.append(call)
        
def monkeypatch_ol(monkeypatch):
    mock = Mock()
    mock.setup_call("isbn_10", "1234567890", _return="/books/OL1M")
    mock.setup_call("key", "/books/OL2M", _return="/books/OL2M")
    monkeypatch.setattr(dynlinks, "ol_query", mock)

    mock = Mock()
    mock.setup_call(["/books/OL1M"], _return=[{"key": "/books/OL1M", "title": "foo"}])
    mock.setup_call(["/books/OL2M"], _return=[{"key": "/books/OL2M", "title": "bar", "ocaid": "ia-bar"}])
    mock.default = []
    monkeypatch.setattr(dynlinks, "ol_get_many", mock)
    
    monkeypatch.setattr(ia, "get_meta_xml", lambda itemid: web.storage())

def test_query_keys(monkeypatch):
    monkeypatch_ol(monkeypatch)
    
    assert dynlinks.query_keys(["isbn:1234567890"]) == {"isbn:1234567890": "/books/OL1M"}
    assert dynlinks.query_keys(["isbn:9876543210"]) == {}
    assert dynlinks.query_keys(["isbn:1234567890", "isbn:9876543210"]) == {"isbn:1234567890": "/books/OL1M"}

def test_query_docs(monkeypatch):
    monkeypatch_ol(monkeypatch)
    
    assert dynlinks.query_docs(["isbn:1234567890"]) == {"isbn:1234567890": {"key": "/books/OL1M", "title": "foo"}}
    assert dynlinks.query_docs(["isbn:9876543210"]) == {}
    assert dynlinks.query_docs(["isbn:1234567890", "isbn:9876543210"]) == {"isbn:1234567890": {"key": "/books/OL1M", "title": "foo"}}
    
def test_process_doc_for_view_api(monkeypatch):
    monkeypatch_ol(monkeypatch)
    
    bib_key = "isbn:1234567890"
    doc = {"key": "/books/OL1M", "title": "foo"}
    expected_result = {
        "bib_key": "isbn:1234567890",
        "info_url": "http://openlibrary.org/books/OL1M/foo",
        "preview": "noview",
        "preview_url": "http://openlibrary.org/books/OL1M/foo"
    }
    assert dynlinks.process_doc_for_viewapi(bib_key, doc) == expected_result
    
    doc['ocaid'] = "ia-foo"
    expected_result["preview"] = "full"
    expected_result["preview_url"] = "http://www.archive.org/details/ia-foo"
    assert dynlinks.process_doc_for_viewapi(bib_key, doc) == expected_result
    
    doc['covers'] = [42, 53]
    expected_result["thumbnail_url"] = "http://covers.openlibrary.org/b/id/42-S.jpg"
    assert dynlinks.process_doc_for_viewapi(bib_key, doc) == expected_result

def test_process_result_for_details(monkeypatch):
    assert dynlinks.process_result_for_details({
        "isbn:1234567890": {"key": "/books/OL1M", "title": "foo"}}) == {
            "isbn:1234567890": {
                    "bib_key": "isbn:1234567890",
                    "info_url": "http://openlibrary.org/books/OL1M/foo",
                    "preview": "noview",
                    "preview_url": "http://openlibrary.org/books/OL1M/foo",
                    "details": {
                        "key": "/books/OL1M",
                        "title": "foo"
                    }
            }}
            
    OL1A = {
        "key": "/authors/OL1A",
        "type": {"key": "/type/author"},
        "name": "Mark Twain",
    }
    mock = Mock()
    mock.setup_call(["/authors/OL1A"], _return=[OL1A])
    monkeypatch.setattr(dynlinks, "ol_get_many", mock)

    result = {
        "isbn:1234567890": {
            "key": "/books/OL1M", 
            "title": "foo", 
            "authors": [{"key": "/authors/OL1A"}]
        }
    }
    
    expected_result = {
        "isbn:1234567890": {
            "bib_key": "isbn:1234567890",
            "info_url": "http://openlibrary.org/books/OL1M/foo",
            "preview": "noview",
            "preview_url": "http://openlibrary.org/books/OL1M/foo",
            "details": {
                "key": "/books/OL1M",
                "title": "foo",
                "authors": [{
                    "key": "/authors/OL1A",
                    "name": "Mark Twain"
                }]
            }
        }
    }
    
    assert dynlinks.process_result_for_details(result) == expected_result
    
def test_dynlinks(monkeypatch):
    monkeypatch_ol(monkeypatch)
    
    expected_result = {
        "isbn:1234567890": {
            "bib_key": "isbn:1234567890",
            "info_url": "http://openlibrary.org/books/OL1M/foo",
            "preview": "noview",
            "preview_url": "http://openlibrary.org/books/OL1M/foo"
        }
    }
    
    js = dynlinks.dynlinks(["isbn:1234567890"], {})    
    match = re.match('^var _OLBookInfo = ({.*});$', js)
    assert match is not None
    assert simplejson.loads(match.group(1)) == expected_result

    js = dynlinks.dynlinks(["isbn:1234567890"], {"callback": "func"})
    match = re.match('^func\(({.*})\);$', js)
    assert match is not None
    assert simplejson.loads(match.group(1)) == expected_result

    js = dynlinks.dynlinks(["isbn:1234567890"], {"format": "json"})
    assert simplejson.loads(js) == expected_result

def test_isbnx(monkeypatch):
    site = mock_infobase.MockSite()
    site.save({
        "key": "/books/OL1M",
        "type": {"key": "/type/edition"},
        "isbn_10": "123456789X"
    })
    
    monkeypatch.setattr(web.ctx, "site", site, raising=False)
    json = dynlinks.dynlinks(["isbn:123456789X"], {"format": "json"})
    d = simplejson.loads(json)
    assert d.keys() == ["isbn:123456789X"] 

def test_dynlinks_ia(monkeypatch):
    monkeypatch_ol(monkeypatch)

    expected_result = {
        "OL2M": {
            "bib_key": "OL2M",
            "info_url": "http://openlibrary.org/books/OL2M/bar",
            "preview": "full",
            "preview_url": "http://www.archive.org/details/ia-bar"
        }
    }
    json = dynlinks.dynlinks(["OL2M"], {"format": "json"})
    assert simplejson.loads(json) == expected_result

def test_dynlinks_details(monkeypatch):
    monkeypatch_ol(monkeypatch)

    expected_result = {
        "OL2M": {
            "bib_key": "OL2M",
            "info_url": "http://openlibrary.org/books/OL2M/bar",
            "preview": "full",
            "preview_url": "http://www.archive.org/details/ia-bar", 
            "details": {
                "key": "/books/OL2M", 
                "title": "bar", 
                "ocaid": "ia-bar"
            }
        },
    }
    json = dynlinks.dynlinks(["OL2M"], {"format": "json", "details": "true"})
    assert simplejson.loads(json) == expected_result
    
class TestDataProcessor:        
    def test_get_authors0(self, data0):
        p = dynlinks.DataProcessor()
        p.authors = data0
        assert p.get_authors(data0['/books/OL0M']) == []
        
    def test_get_authors1(self, data1):
        p = dynlinks.DataProcessor()
        p.authors = data1
        assert p.get_authors(data1['/works/OL1W']) == [{"url": "http://openlibrary.org/authors/OL1A/Mark_Twain", "name": "Mark Twain"}]
        
    def test_process_doc0(self, data0):
        p = dynlinks.DataProcessor()
        assert p.process_doc(data0['/books/OL0M']) == data0['result']['data']
        
    def test_process_doc9(self, monkeypatch, data9):
        monkeypatch_ol(monkeypatch)
        
        p = dynlinks.DataProcessor()
        p.authors = data9
        p.works = data9
        print p.process_doc(data9['/books/OL9M'])
        assert p.process_doc(data9['/books/OL9M']) == data9['result']['data']

########NEW FILE########
__FILENAME__ = code
import re
from infogami.utils import view, delegate
import copyrightstatus
import web
from infogami.utils.view import render

r_year = re.compile(r'(?:[^\d]|^)(\d\d\d\d)(?:[^\d]|$)')

@view.public
def copyright_status(edition):
    # computes copyright status of edition, first guessing year of
    # publication and storing it on edition.publish_year (i.e.
    # mutating its argument :-( ).

    year = r_year.findall(str(edition.get('publish_date', '')))
    try:
        year = int(year[0])
    except (IndexError, ValueError):
        return None
    #assert not hasattr(edition, 'publish_year')
    edition.publish_year = year
    return copyrightstatus.copyright_status(edition)

class copyright(delegate.page):
    def POST(self):
        i = web.input(status='U',
                      edition='zzzzzzz')

        edition = web.ctx.site.get(i.edition)
        status = i.status
        assert status in ('U','PD','C')
        return render.copyright(
            i.edition,
            status,
            edition)

    GET = POST

########NEW FILE########
__FILENAME__ = ca
from __future__ import with_statement
import time

OLDEST_PERSON_EVER_IN_CANADA = 117
current_year = int(time.strftime('%Y'))

def mmax(*args):
  """Return maximum of args given, ignoring any None values.  If there are
  no non-None values, return None.
  >>> print mmax()
  None
  >>> print mmax(1)
  1
  >>> print mmax(2,None,4,3)
  4
  >>> print mmax(None,None,None)
  None"""

  a = list(x for x in args if x is not None)
  if len(a) == 0:
    return None
  return max(a)

def copyright_status(edition):
  """Determine copyright status of edition.  Return value is a dictionary containing
  the date at which the edition is conservatively expected to enter the public domain,
  and a list of assumptions made during the calculation."""

  assumptions = []
  assume = assumptions.append

  assume("We're assuming the current year is %d."% current_year)

  pubyear = edition.publish_year
  if pubyear is None:
    pubyear = current_year
    assume("The publication year is unspecified, so we're assuming that the year of publication is %d."% pubyear)
  else:
    assume("We're assuming the year of publication is %d."% pubyear)

  if len(edition.authors) > 0:
    assume("We're assuming the list of authors is complete.")
  else:
    assume("We're assuming the authorship is anonymous.")

  maxauthordeath = None
      
  def y(author, attr):
    """Extract attribute (i.e. a string-valued date field) from author and
    convert it to integer.  If field is absent or no valid conversion, return None."""
    r = author.get(attr, None)
    try:
      return int(r)
    except (ValueError, AttributeError, TypeError):
      return None

  for author in edition.authors:
    ydeath, ybirth = y(author, 'death_date'), y(author, 'birth_date')
    aname = author.name

    death_year = None
    if aname == 'Crown':
      """We don't set death_year for Crown authorship because items with sole Crown authorship will default to pdyear = death_year + 50,
      and items with joint Crown and non-Crown authorship are determined by the other authors' information."""
      assume("We're assuming this item is under Crown copyright. Non-Crown authors, if any, render this irrelevant.")
    else:
      if ydeath:
        death_year = ydeath
        assume("We're assuming the author died in %d." % ydeath)
      elif ybirth:
        death_year = ybirth + OLDEST_PERSON_EVER_IN_CANADA
        assume("We're assuming the author was born in %d." % ybirth)
      elif pubyear:
        death_year = pubyear + OLDEST_PERSON_EVER_IN_CANADA
        assume("We're assuming the author was born at the time of publication, since we don't have a known birthdate.")

    if death_year is not None and ydeath is None:
      if death_year < current_year:
        assume("We're assuming the author didn't live longer than the oldest person ever in Canada, \
and therefore died no later than the year %d, since we have no known death date." % death_year)
      else:
        assume("We're assuming the author will live to age %d, the same as the oldest person ever in Canada so far." % OLDEST_PERSON_EVER_IN_CANADA)

    maxauthordeath = mmax(maxauthordeath, death_year)

  if maxauthordeath:
    if maxauthordeath < pubyear and pubyear < 1999:
      pdyear = pubyear + 50            
    else:
      pdyear = maxauthordeath + 50
  else:
    pdyear = pubyear + 50

  def nominal_death(author):
    ydeath, ybirth = y(author, 'death_date'), y(author, 'birth_date')
    if ydeath is not None:
      return ydeath
    if ybirth is not None:
      return ybirth + OLDEST_PERSON_EVER_IN_CANADA
    return None

  if any(nominal_death(author) is None for author in edition.authors):
    assume("We're assuming that the authors whose death dates are missing didn't die after those whose are available.")

  # assume(repr({ 'date': pdyear, 'assumptions': assumptions })) # debugging diagnostic @@
  return { 'date': pdyear, 'assumptions': assumptions }

if __name__ == '__main__':
  import doctest
  doctest.testmod()
  

########NEW FILE########
__FILENAME__ = us
OLDEST_PERSON_EVER = 123

def copyright_status(edition):
    pubyear = edition.publish_year
    assumptions = ["We're assuming the year of publication is %d."% pubyear]
    assumptions.append("We're assuming that the data is correct.")
    assumptions.append("We're assuming it was published.")
    assumptions.append("We're assuming it was published in the US.")
    assumptions.append("We're assuming it was published with a valid copyright notice.")

    if pubyear < 1923:
        pdyear =  pubyear + 28 + 28
    elif pubyear < 1964:
        assumptions.append("We're assuming its copyright was renewed.")
        pdyear = pubyear + 95
    elif pubyear < 1978:
        pdyear = pubyear + 95
    else:
        assumptions.append("We're assuming it wasn't published by a corporation or under a pseudonym.")
        maxauthordeath = None
        for author in edition.authors:
            if author.get('death_year'):
                if not maxauthordeath or author.death_year > maxauthordeath:
                    maxauthordeath = author.death_year
                else:
                    assumptions.append("We're assuming that the author whose death dates are missing didn't die after those whose are available.")
        if maxauthordeath:
            pdyear = maxauthordeath + 70
        else:
            assumptions.append("We're assuming that the author lived as long as the oldest person ever and published the work at birth.")
            #TODO: look for author birth years
            pdyear = pubyear + OLDEST_PERSON_EVER
    return { 'date': pdyear, 'assumptions': assumptions }

########NEW FILE########
__FILENAME__ = db
# this is copied from the bookrev module, need to put it
# into a common location @@

import web
from infogami import config, tdb
from infogami.core import db, thingutil

def get_site():
    return db.get_site(config.site)

def get_type(type_name):
    site = get_site()
    return db.get_type(site, type_name)

def create_type(type_name, properties={}):
    def props():
        for pname, ptype_name in properties.items():
            yield dict(name=pname, type=get_type(ptype_name), unique=True)

    return db._create_type(get_site(), type_name, list(props()))


def insert_backreference(type_name, prop_name, ref_type_name, ref_prop_name):

    type = get_type(type_name)

    def create_backreference(): 
        d = {'type': get_type(ref_type_name), 'property_name': ref_prop_name}
        return db._get_thing(type, 
                             prop_name, 
                             get_type('type/backreference'), 
                             d)

    type.backreferences = type.get('backreferences', [])
    type.backreferences.append(create_backreference())
    type.save()


fill_backreferences = thingutil.thingtidy

def get_thing(name, type):
    site = get_site()
    try:
        thing = tdb.withName(name, site)
        if thing.type.id == type.id:
            fill_backreferences(thing)
            return thing
    except tdb.NotFound:
        pass
    return None

########NEW FILE########
__FILENAME__ = code
"""Open Library Import API
"""

from infogami.plugins.api.code import add_hook
from infogami import config
from openlibrary.plugins.openlibrary.code import can_write
from openlibrary.catalog.marc.marc_binary import MarcBinary
from openlibrary.catalog.marc.marc_xml import MarcXml
from openlibrary.catalog.marc.parse import read_edition
from openlibrary.catalog import add_book
from openlibrary import accounts
from openlibrary import records

#import openlibrary.tasks
from ... import tasks

import web


import base64
import json
import re
import urllib

import import_opds
import import_rdf
import import_edition_builder
from lxml import etree

def parse_meta_headers(edition_builder):
    # parse S3-style http headers
    # we don't yet support augmenting complex fields like author or language
    # string_keys = ['title', 'title_prefix', 'description']

    re_meta = re.compile('HTTP_X_ARCHIVE_META(?:\d{2})?_(.*)')
    for k, v in web.ctx.env.items():
        m = re_meta.match(k)
        if m:
            meta_key = m.group(1).lower()
            edition_builder.add(meta_key, v, restrict_keys=False)


def parse_data(data):
    data = data.strip()
    if -1 != data[:10].find('<?xml'):
        root = etree.fromstring(data)
        #print root.tag
        if '{http://www.w3.org/1999/02/22-rdf-syntax-ns#}RDF' == root.tag:
            edition_builder = import_rdf.parse(root)
            format = 'rdf'
        elif '{http://www.w3.org/2005/Atom}entry' == root.tag:
            edition_builder = import_opds.parse(root)
            format = 'opds'
        elif '{http://www.loc.gov/MARC21/slim}record' == root.tag:
            if root.tag == '{http://www.loc.gov/MARC21/slim}collection':
                root = root[0]
            rec = MarcXml(root)
            edition = read_edition(rec)
            edition_builder = import_edition_builder.import_edition_builder(init_dict=edition)
            format = 'marcxml'
        else:
            print 'unrecognized XML format'
            return None, None
    elif data.startswith('{') and data.endswith('}'):
        obj = json.loads(data)
        edition_builder = import_edition_builder.import_edition_builder(init_dict=obj)
        format = 'json'
    else:
        #Marc Binary
        if len(data) != int(data[:5]):
            return json.dumps({'success':False, 'error':'Bad MARC length'})
    
        rec = MarcBinary(data)
        edition = read_edition(rec)
        edition_builder = import_edition_builder.import_edition_builder(init_dict=edition)
        format = 'marc'

    parse_meta_headers(edition_builder)
    
    return edition_builder.get_dict(), format

def get_next_count():
    store = web.ctx.site.store
    counter = store.get('import_api_s3_counter')
    print 'counter: ',
    print counter
    if None == counter:
        store['import_api_s3_counter'] = {'count':0}
        return 0
    else:
        count = counter['count'] + 1
        store['import_api_s3_counter'] = {'count':count, '_rev':counter['_rev']}
        return count

def queue_s3_upload(data, format):
    s3_key = config.plugin_importapi.get('s3_key')
    s3_secret = config.plugin_importapi.get('s3_secret')
    counter = get_next_count()
    filename = '%03d.%s' % (counter, format)
    s3_item_id = config.plugin_importapi.get('s3_item', 'test_ol_import')
    s3_item_id += '_%03d' % (counter/1000)

    #print 'attempting to queue s3 upload with %s:%s file=%s item=%s' % (s3_key, s3_secret, filename, s3_item_id)
    tasks.upload_via_s3.delay(s3_item_id, filename, data, s3_key, s3_secret)
    #print 'done queuing s3 upload'

    source_url = 'http://www.archive.org/download/%s/%s' % (s3_item_id, filename)
    return source_url

class importapi:
    def GET(self):
        web.header('Content-Type', 'text/plain')
        tasks.add.delay(777, 777)
        return 'Import API only supports POST requests.'

    def POST(self):
        web.header('Content-Type', 'application/json')

        if not can_write():
            return json.dumps({'success':False, 'error':'Permission Denied'})

        data = web.data()
       
        edition, format = parse_data(data)
        #print edition

        source_url = None
        if 'source_records' not in edition:
            source_url = queue_s3_upload(data, format)
            edition['source_records'] = [source_url]

        #call Edward's code here with the edition dict
        if edition:
            reply = add_book.load(edition)
            if source_url:
                reply['source_record'] = source_url
            return json.dumps(reply)
        else:
            return json.dumps({'success':False, 'error':'Failed to parse Edition data'})


class ils_search:
    """Search and Import API to use in Koha. 
    
    When a new catalog record is added to Koha, it makes a request with all
    the metadata to find if OL has a matching record. OL returns the OLID of
    the matching record if exists, if not it creates a new record and returns
    the new OLID.
    
    Request Format:
    
        POST /api/ils_search
        Content-type: application/json
        Authorization: Basic base64-of-username:password
    
        {
            'title': '',
            'authors': ['...','...',...]
            'publisher': '...',
            'publish_year': '...',
            'isbn': [...],
            'lccn': [...],
        }
        
    Response Format:
    
        {
            'status': 'found | notfound | created',
            'olid': 'OL12345M',
            'key': '/books/OL12345M',
            'cover': {
                'small': 'http://covers.openlibrary.org/b/12345-S.jpg',
                'medium': 'http://covers.openlibrary.org/b/12345-M.jpg',
                'large': 'http://covers.openlibrary.org/b/12345-L.jpg',
            },
            ...
        }
        
    When authorization header is not provided and match is not found,
    status='notfound' is returned instead of creating a new record.
    """
    def POST(self):
        try:
            rawdata = json.loads(web.data())
        except ValueError,e:
            raise self.error("Unparseable JSON input \n %s"%web.data())

        # step 1: prepare the data
        data = self.prepare_input_data(rawdata)
    
        # step 2: search 
        matches = self.search(data)

        # step 3: Check auth
        try:
            auth_header = http_basic_auth()
            self.login(auth_header)
        except accounts.ClientException:
            raise self.auth_failed("Invalid credentials")

        # step 4: create if logged in
        keys = []
        if auth_header:
            keys = self.create(matches)
        
        # step 4: format the result
        d = self.format_result(matches, auth_header, keys)
        return json.dumps(d)

    def error(self, reason):
        d = json.dumps({ "status" : "error", "reason" : reason})
        return web.HTTPError("400 Bad Request", {"Content-type": "application/json"}, d)


    def auth_failed(self, reason):
        d = json.dumps({ "status" : "error", "reason" : reason})
        return web.HTTPError("401 Authorization Required", {"WWW-Authenticate": 'Basic realm="http://openlibrary.org"', "Content-type": "application/json"}, d)

    def login(self, authstring):
        if not authstring:
            return
        authstring = authstring.replace("Basic ","")
        username, password = base64.decodestring(authstring).split(':')
        accounts.login(username, password)
        
    def prepare_input_data(self, rawdata):
        data = dict(rawdata)
        identifiers = rawdata.get('identifiers',{})
        #TODO: Massage single strings here into lists. e.g. {"google" : "123"} into {"google" : ["123"]}.
        for i in ["oclc_numbers", "lccn", "ocaid", "isbn"]:
            if i in data:
                val = data.pop(i)
                if not isinstance(val, list):
                    val = [val]
                identifiers[i] = val
        data['identifiers'] = identifiers

        if "authors" in data:
            authors = data.pop("authors")
            data['authors'] = [{"name" : i} for i in authors]

        return {"doc" : data}
        
    def search(self, params):
        matches = records.search(params)
        return matches

    def create(self, items):
        return records.create(items)
            
    def format_result(self, matches, authenticated, keys):
        doc = matches.pop("doc", {})
        if doc and doc['key']:
            doc = web.ctx.site.get(doc['key']).dict()
            # Sanitise for only information that we want to return.
            for i in ["created", "last_modified", "latest_revision", "type", "revision"]: 
                doc.pop(i)
            # Main status information
            d = {
                'status': 'found',
                'key': doc['key'],
                'olid': doc['key'].split("/")[-1]
            }
            # Cover information
            covers = doc.get('covers') or []
            if covers and covers[0] > 0:
                d['cover'] = {
                    "small": "http://covers.openlibrary.org/b/id/%s-S.jpg" % covers[0],
                    "medium": "http://covers.openlibrary.org/b/id/%s-M.jpg" % covers[0],
                    "large": "http://covers.openlibrary.org/b/id/%s-L.jpg" % covers[0],
                }

            # Pull out identifiers to top level
            identifiers = doc.pop("identifiers",{})
            for i in identifiers:
                d[i] = identifiers[i]
            d.update(doc)

        else:
            if authenticated:
                d = { 'status': 'created' , 'works' : [], 'authors' : [], 'editions': [] }
                for i in keys:
                    if i.startswith('/books'):
                        d['editions'].append(i)
                    if i.startswith('/works'):
                        d['works'].append(i)
                    if i.startswith('/authors'):
                        d['authors'].append(i)
            else:
                d = {
                    'status': 'notfound'
                    }
        return d
        
def http_basic_auth():
    auth = web.ctx.env.get('HTTP_AUTHORIZATION')
    return auth and web.lstrips(auth, "")
        
        
class ils_cover_upload:
    """Cover Upload API for Koha.
    
    Request Format: Following input fields with enctype multipart/form-data
    
        * olid: Key of the edition. e.g. OL12345M
        * file: image file 
        * url: URL to image
        * redirect_url: URL to redirect after upload

        Other headers:
           Authorization: Basic base64-of-username:password
    
    One of file or url can be provided. If the former, the image is
    directly used. If the latter, the image at the URL is fetched and
    used.

    On Success: 
          If redirect URL specified, 
                redirect to redirect_url?status=ok
          else 
                return 
                {
                  "status" : "ok"
                }
    
    On Failure: 
          If redirect URL specified, 
                redirect to redirect_url?status=error&reason=bad+olid
          else 
                return
                {
                  "status" : "error",
                  "reason" : "bad olid"
                }
    """
    def error(self, i, reason):
        if i.redirect_url:
            url = self.build_url(i.redirect_url, status="error", reason=reason)
            return web.seeother(url)
        else:
            d = json.dumps({ "status" : "error", "reason" : reason})
            return web.HTTPError("400 Bad Request", {"Content-type": "application/json"}, d)


    def success(self, i):
        if i.redirect_url:
            url = self.build_url(i.redirect_url, status="ok")
            return web.seeother(url)
        else:
            d = json.dumps({ "status" : "ok" })
            return web.ok(d, {"Content-type": "application/json"})

    def auth_failed(self, reason):
        d = json.dumps({ "status" : "error", "reason" : reason})
        return web.HTTPError("401 Authorization Required", {"WWW-Authenticate": 'Basic realm="http://openlibrary.org"', "Content-type": "application/json"}, d)

    def build_url(self, url, **params):
        if '?' in url:
            return url + "&" + urllib.urlencode(params)    
        else:
            return url + "?" + urllib.urlencode(params)

    def login(self, authstring):
        if not authstring:
            raise self.auth_failed("No credentials provided")
        authstring = authstring.replace("Basic ","")
        username, password = base64.decodestring(authstring).split(':')
        accounts.login(username, password)

    def POST(self):
        i = web.input(olid=None, file={}, redirect_url=None, url="")

        if not i.olid:
            self.error(i, "olid missing")
            
        key = '/books/' + i.olid
        book = web.ctx.site.get(key)
        if not book:
            raise self.error(i, "bad olid")

        try:
            auth_header = http_basic_auth()
            self.login(auth_header)
        except accounts.ClientException:
            raise self.auth_failed("Invalid credentials")

        from openlibrary.plugins.upstream import covers
        add_cover = covers.add_cover()
        
        data = add_cover.upload(key, i)
        coverid = data.get('id')
        
        if coverid:
            add_cover.save(book, coverid)
            raise self.success(i)
        else:
            raise self.error(i, "upload failed")
    

add_hook("import", importapi)
add_hook("ils_search", ils_search)
add_hook("ils_cover_upload", ils_cover_upload)

########NEW FILE########
__FILENAME__ = import_edition_builder
"""
Create a edition dict that can be passed to catalog.add_book.load()

This class encapsulates the logic of creating edition dicts.

You can use add(key) to add a new key to the edition dict. This class
will take care of whether it should be a string or a list. For example,
you can use add('subject') to add an entry to the 'subjects' list.

This class also takes care of creating complex types, such as authors.
For example, you can add an author using add('author', 'Mark Twain') and
we will create {'personal_name': ..., 'name': ..., 'entity_type': 'person'} 
which is stored as a list of authors in the edition dict.

A sample dict looks like one of these:
{'edition_name': u'3rd ed.', 'pagination': u'xii, 444 p.', 'title': u'A course of pure mathematics', 'publishers': [u'At the University Press'], 'number_of_pages': 444, 'languages': ['eng'], 'publish_date': '1921', 'location': [u'GLAD', u'GLAD'], 'authors': [{'birth_date': u'1877', 'personal_name': u'Hardy, G. H.', 'death_date': u'1947', 'name': u'Hardy, G. H.', 'entity_type': 'person'}], 'by_statement': u'by G.H. Hardy', 'publish_places': [u'Cambridge'], 'publish_country': 'enk'}
{'publishers': [u'Ace Books'], 'pagination': u'271 p. ;', 'title': u'Neuromancer', 'lccn': [u'91174394'], 'notes': u'Hugo award book, 1985; Nebula award ; Philip K. Dick award', 'number_of_pages': 271, 'isbn_13': [u'9780441569595'], 'languages': ['eng'], 'dewey_decimal_class': [u'813/.54'], 'lc_classifications': [u'PS3557.I2264 N48 1984', u'PR9199.3.G53 N49 1984'], 'publish_date': '1984', 'publish_country': 'nyu', 'authors': [{'birth_date': u'1948', 'personal_name': u'Gibson, William', 'name': u'Gibson, William', 'entity_type': 'person'}], 'by_statement': u'William Gibson', 'oclc_numbers': ['24379880'], 'publish_places': [u'New York'], 'isbn_10': [u'0441569595']}
{'publishers': [u'Grosset & Dunlap'], 'pagination': u'156 p.', 'title': u'Great trains of all time', 'lccn': [u'62051844'], 'number_of_pages': 156, 'languages': ['eng'], 'dewey_decimal_class': [u'625.2'], 'lc_classifications': [u'TF147 .H8'], 'publish_date': '1962', 'publish_country': 'nyu', 'authors': [{'birth_date': u'1894', 'personal_name': u'Hubbard, Freeman H.', 'name': u'Hubbard, Freeman H.', 'entity_type': 'person'}], 'by_statement': u'Illustrated by Herb Mott', 'oclc_numbers': [u'1413013'], 'publish_places': [u'New York']}
"""

class import_edition_builder:    

    def add_string(self, key, val):
        self.edition_dict[key] = val    

    def add_list(self, key, val):
        if key in self.edition_dict:
            self.edition_dict[key].append(val)
        else:
            self.edition_dict[key] = [val]

    def add_author(self, key, val):
        # We don't know birth_date or death_date.
        # Should name and personal_name be the same value?
        author_dict = {
           'personal_name': val, 
           'name': val, 
           'entity_type': 'person'
        }
        self.add_list('authors', author_dict)

    def add_illustrator(self, key, val):
        self.add_list('contributions', val + u' (Illustrator)')
    
    def __init__(self, init_dict={}):
        self.edition_dict = init_dict.copy()

        self.type_dict = {
            'title'              : ['title',          self.add_string],
            'author'             : ['authors',        self.add_author],
            'publisher'          : ['publishers',     self.add_list],
            'publish_place'      : ['publish_places', self.add_list],
            'publish_date'       : ['publish_date',   self.add_string],
            'pagination'         : ['pagination',     self.add_string],
            'subject'            : ['subjects',       self.add_list],
            'language'           : ['languages',      self.add_list],
            'description'        : ['description',    self.add_string],
            'lccn'               : ['lccn',           self.add_list],
            'oclc_number'        : ['oclc_numbers',   self.add_list],
            'isbn_10'            : ['isbn_10',        self.add_list],
            'isbn_13'            : ['isbn_13',        self.add_list],
            'ocaid'              : ['ocaid',          self.add_string],
            'illustrator'        : ['contributions',  self.add_illustrator],
            'source_record'      : ['source_records', self.add_list],
            'dewey_decimal_class': ['dewey_decimal_class', self.add_list],
            'lc_classification'  : ['lc_classifications',  self.add_list],
        }


    def get_dict(self):
        return self.edition_dict

    def add(self, key, val, restrict_keys=True):
        if restrict_keys and not key in self.type_dict:
            print 'import_edition_builder invalid key: ' + key
            return

        if key in self.type_dict:
            new_key  = self.type_dict[key][0]
            add_func = self.type_dict[key][1]
            add_func(new_key, val)
        else:
            self.add_string(key, val)

########NEW FILE########
__FILENAME__ = import_opds
"""
OL Import API OPDS parser
"""

import import_edition_builder

def parse_string(e, key):
    return (key, e.text)

def parse_author(e, key):
    name = e.find('{http://www.w3.org/2005/Atom}name')    
    return (key, name.text)

def parse_category(e, key):
    return (key, e.get('label'))

def parse_identifier(e, key):
    val = e.text
    isbn_str = 'urn:ISBN:'
    ia_str   = 'http://www.archive.org/details/'
    if val.startswith(isbn_str):
        isbn = val[len(isbn_str):]
        if 10 == len(isbn):
            return ('isbn_10', isbn)
        elif 13 == len(isbn):
            return ('isbn_13', isbn)
    elif val.startswith(ia_str):
        return ( 'ocaid', val[len(ia_str):] )
    else:
        return (None, None)

parser_map = {
    '{http://www.w3.org/2005/Atom}title':      ['title',         parse_string],
    '{http://www.w3.org/2005/Atom}author':     ['author',        parse_author],
    '{http://purl.org/dc/terms/}publisher':    ['publisher',     parse_string],
    '{http://purl.org/dc/terms/}issued':       ['publish_date',  parse_string],
    '{http://purl.org/dc/terms/}extent':       ['pagination',    parse_string],
    '{http://www.w3.org/2005/Atom}category':   ['subject',       parse_category],
    '{http://purl.org/dc/terms/}language':     ['language',      parse_string],
    '{http://www.w3.org/2005/Atom}summary':    ['description',   parse_string],
    '{http://purl.org/ontology/bibo/}lccn':    ['lccn',          parse_string],
    '{http://purl.org/ontology/bibo/}oclcnum': ['oclc_number',   parse_string],
    '{http://purl.org/dc/terms/}identifier':   ['identifier',    parse_identifier],
    '{http://RDVocab.info/elements/}placeOfPublication': ['publish_place', parse_string],    
}
#TODO: {http://purl.org/dc/terms/}identifier (could be ocaid)
#TODO: {http://www.w3.org/2005/Atom}link     (could be cover image)

def parse(root):
    edition_builder = import_edition_builder.import_edition_builder()
    
    for e in root:
        if isinstance(e.tag, basestring): 
            #print e.tag
            if e.tag in parser_map:
                key = parser_map[e.tag][0]
                (new_key, val) = parser_map[e.tag][1](e, key)
                if new_key:
                    edition_builder.add(new_key, val)

    return edition_builder

########NEW FILE########
__FILENAME__ = import_rdf
"""
OL Import API RDF parser
"""

import import_edition_builder

def parse_string(e, key):
    return (key, e.text)

def parse_authors(e, key):
    authors = []
    for name in e.iterfind('.//{http://www.w3.org/1999/02/22-rdf-syntax-ns#}value'):
        authors.append(name.text)
    return (key, authors)

#Note that RDF can have subject elements in both dc and dcterms namespaces
#dc:subject is simply parsed by parse_string()
def parse_subject(e, key):
    member_of = e.find('.//{http://purl.org/dc/dcam/}memberOf')
    resource_type = member_of.get('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}resource')
    val = e.find('.//{http://www.w3.org/1999/02/22-rdf-syntax-ns#}value')
    if 'http://purl.org/dc/terms/DDC' == resource_type:
        new_key = 'dewey_decimal_class'
        return (new_key, val.text)
    elif 'http://purl.org/dc/terms/LCC' == resource_type:
        new_key = 'lc_classification'
        return (new_key, val.text)
    else:        
        return (None, None)

def parse_category(e, key):
    return (key, e.get('label'))

def parse_identifier(e, key):
    val = e.text
    isbn_str = 'urn:ISBN:'
    ia_str   = 'http://www.archive.org/details/'
    if val.startswith(isbn_str):
        isbn = val[len(isbn_str):]
        if 10 == len(isbn):
            return ('isbn_10', isbn)
        elif 13 == len(isbn):
            return ('isbn_13', isbn)
    elif val.startswith(ia_str):
        return ( 'ocaid', val[len(ia_str):] )
    else:
        return (None, None)

parser_map = {
    '{http://purl.org/ontology/bibo/}authorList': ['author',        parse_authors],
    '{http://purl.org/dc/terms/}title':           ['title',         parse_string],
    '{http://purl.org/dc/terms/}publisher':       ['publisher',     parse_string],
    '{http://purl.org/dc/terms/}issued':          ['publish_date',  parse_string],
    '{http://purl.org/dc/terms/}extent':          ['pagination',    parse_string],
    '{http://purl.org/dc/elements/1.1/}subject':  ['subject',       parse_string],
    '{http://purl.org/dc/terms/}subject':         ['subject',       parse_subject],
    '{http://purl.org/dc/terms/}language':        ['language',      parse_string],
    '{http://purl.org/ontology/bibo/}lccn':       ['lccn',          parse_string],
    '{http://purl.org/ontology/bibo/}oclcnum':    ['oclc_number',   parse_string],
    '{http://RDVocab.info/elements/}placeOfPublication': ['publish_place', parse_string],
    
}
#TODO: {http://purl.org/dc/terms/}identifier (could be ocaid)
#TODO: {http://www.w3.org/2005/Atom}link     (could be cover image)

def parse(root):
    edition_builder = import_edition_builder.import_edition_builder()
    
    for e in root.iter():
        if isinstance(e.tag, basestring): 
            #print e.tag
            if e.tag in parser_map:
                key = parser_map[e.tag][0]
                (new_key, val) = parser_map[e.tag][1](e, key)
                if new_key:
                    if isinstance(val, list):
                        for v in val:
                            edition_builder.add(new_key, v)
                    else:
                        edition_builder.add(new_key, val)
    return edition_builder

########NEW FILE########
__FILENAME__ = metaxml_to_json
#!/usr/bin/env python

"""
This example uses the import_edition_builder class to convert 
an IA meta.xml into a json object that the Import API can consume.

usage:
> python metaxml_to_json.py romanceonthreele00hafnrich_meta.xml 
{"publishers": ["New York : Bloomsbury"], "description": "Includes bibliographical references (p. [243]-247) and index", "title": "A romance on three legs : Glenn Gould's obsessive quest for the perfect piano", "isbn_10": ["1596915250"], "isbn_13": ["9781596915251"], "languages": ["eng"], "subjects": ["Lending library", "protected DAISY", "Accessible book", "Gould, Glenn, 1932-1982", "Steinway piano"], "publish_date": "2009", "authors": [{"entity_type": "person", "name": "Hafner, Katie", "personal_name": "Hafner, Katie"}], "ocaid": "romanceonthreele00hafnrich"}
"""

from import_edition_builder import import_edition_builder

def parse_collection(collection):
    collection_dict = {
        'printdisabled'  : ['Protected DAISY', 'Accessible book'],
        'lendinglibrary' : ['Lending library', 'Protected DAISY', 'Accessible book'],
        'inlibrary'      : ['In library'],
    }
    
    return collection_dict.get(collection, [])

def parse_isbn(isbn):
    if 13 == len(isbn):
        return ('isbn_13', [isbn])
    elif 10 == len(isbn):
        return ('isbn_10', [isbn])
    else:
        return ('isbn', [])
    
def metaxml_to_edition_dict(root):
    
    ia_to_ol_map = {
        'identifier' : 'ocaid',
        'creator'    : 'author',
        'date'       : 'publish_date',
        'boxid'      : 'ia_box_id',
    }
    
    edition_builder = import_edition_builder()
    
    for element in root.iter():
        #print("got %s -> %s" % (element.tag, element.text))
        
        if 'collection' == element.tag:
            key = 'subject'
            values = parse_collection(element.text)
        elif 'isbn' == element.tag:
            key, values = parse_isbn(element.text)
        elif element.tag in ia_to_ol_map:
            key = ia_to_ol_map[element.tag]
            values = [element.text]
        else:
            key = element.tag
            values = [element.text]

        for value in values:
            if key.startswith('ia_'):
                edition_builder.add(key, value, restrict_keys=False)
            else:
                edition_builder.add(key, value)
        
    return edition_builder.get_dict()
        
if __name__ == '__main__':
    from lxml import etree
    import sys
    assert 2 == len(sys.argv)
    
    tree = etree.parse(sys.argv[1])
    root = tree.getroot()
    
    edition_dict = metaxml_to_edition_dict(root)
    
    import json
    json_str = json.dumps(edition_dict)
    print json_str


########NEW FILE########
__FILENAME__ = test_code
from .. import code

class Test_ils_cover_upload:
    def test_build_url(self):
        build_url = code.ils_cover_upload().build_url
        assert build_url("http://example.com/foo", status="ok") == "http://example.com/foo?status=ok"
        assert build_url("http://example.com/foo?bar=true", status="ok") == "http://example.com/foo?bar=true&status=ok"
        
class Test_ils_search:
    def test_format_result(self):
        format_result = code.ils_search().format_result
        
        assert format_result(None) == {
            'status': 'notfound'
        }
        
        doc = {
            'key': '/books/OL1M',
            'type': {'key': '/type/edition'}
        }
        assert format_result(doc) == {
            'status': 'found',
            'olid': 'OL1M',
            'key': '/books/OL1M'
        }
        
        doc = {
            'key': '/books/OL1M',
            'type': {'key': '/type/edition'},
            'covers': [12345]
        }
        assert format_result(doc) == {
            'status': 'found',
            'olid': 'OL1M',
            'key': '/books/OL1M',
            'cover': {
                'small': 'http://covers.openlibrary.org/b/id/12345-S.jpg',
                'medium': 'http://covers.openlibrary.org/b/id/12345-M.jpg',
                'large': 'http://covers.openlibrary.org/b/id/12345-L.jpg',
            }
        }
        
    def test_prepare_data(self):
        prepare_data = code.ils_search().prepare_data
        
        data = {
            'isbn': ['1234567890', '9781234567890', '123-4-56789-0', '978-1-935928-32-4']
        }
        assert prepare_data(data) == {
            'isbn_10': ['1234567890', '123-4-56789-0'],
            'isbn_13': ['9781234567890', '978-1-935928-32-4']
        }

########NEW FILE########
__FILENAME__ = code
from infogami.utils import delegate, stats
from infogami.utils.view import render_template, public
from infogami import config
from lxml import etree
from openlibrary.utils import escape_bracket
import logging
import re, web, urllib, urlparse, simplejson, httplib

re_query_parser_error = re.compile(r'<pre>([^<]+?)</pre>', re.S)
re_inside_fields = re.compile(r'(ia|body|page_count|body_length):')
bad_fields = ['title', 'author', 'authors', 'lccn', 'ia', 'oclc', 'isbn', 'publisher', 'subject', 'person', 'place', 'time']
re_bad_fields = re.compile(r'\b(' + '|'.join(bad_fields) + '):')

logger = logging.getLogger("openlibrary.inside")

def urlopen(url, timeout=5):
    """Like urllib.urlopen, but built using httplib with timeout support.
    """
    o = urlparse.urlparse(url)
    selector = o.path
    if o.query:
        selector += "?" + o.query

    # set 5 second timeout. 
    # report error if it takes longer than that.
    # TODO: move the timeout to config
    conn = httplib.HTTPConnection(o.hostname, o.port, timeout=timeout)
    conn.request("GET", selector)
    return conn.getresponse()

def escape_q(q):
    if re_inside_fields.match(q):
        return q
    return escape_bracket(q).replace(':', '\\:')

trans = { '\n': '<br>', '{{{': '<b>', '}}}': '</b>', }
re_trans = re.compile(r'(\n|\{\{\{|\}\}\})')
def quote_snippet(snippet):
    return re_trans.sub(lambda m: trans[m.group(1)], web.htmlquote(snippet))

if hasattr(config, 'plugin_inside'):
    solr_host = config.plugin_inside['solr']
    solr_select_url = "http://" + solr_host + "/solr/inside/select"

def inside_solr_select(params):
    params.setdefault("wt", "json")
    #solr_select = solr_select_url + '?' + '&'.join("%s=%s" % (k, unicode(v)) for k, v in params)
    solr_select = solr_select_url + "?" + urllib.urlencode(params)
    stats.begin("solr", url=solr_select)

    try:
        json_data = urlopen(solr_select).read()
    except IOError, e:
        logger.error("Unable to query search inside solr", exc_info=True)
        return {"error": web.htmlquote(str(e))}
    finally:
        stats.end()
   
    try:
        return simplejson.loads(json_data)
    except:
        m = re_query_parser_error.search(json_data)
        return { 'error': web.htmlunquote(m.group(1)) }

def editions_from_ia(ia):
    q = {'type': '/type/edition', 'ocaid': ia, 'title': None, 'covers': None, 'works': None, 'authors': None}
    editions = web.ctx.site.things(q)
    if not editions:
        del q['ocaid']
        q['source_records'] = 'ia:' + ia
        editions = web.ctx.site.things(q)
    return editions

def read_from_archive(ia):
    meta_xml = 'http://archive.org/download/' + ia + '/' + ia + '_meta.xml'
    stats.begin("archive.org", url=meta_xml)
    xml_data = urllib.urlopen(meta_xml)
    item = {}
    try:
        tree = etree.parse(xml_data)
    except etree.XMLSyntaxError:
        return {}
    finally:
        stats.end()
    root = tree.getroot()

    fields = ['title', 'creator', 'publisher', 'date', 'language']

    for k in 'title', 'date', 'publisher':
        v = root.find(k)
        if v is not None:
            item[k] = v.text

    for k in 'creator', 'language', 'collection':
        v = root.findall(k)
        if len(v):
            item[k] = [i.text for i in v if i.text]
    return item

@public
def search_inside_result_count(q):
    q = escape_q(q)
    params = {
        'fl': 'ia',
        'q.op': 'AND',
        'q': web.urlquote(q)
    }
    results = inside_solr_select(params)
    if 'error' in results:
        return None

    return results['response']['numFound']

class search_inside(delegate.page):
    path = '/search/inside'

    def GET(self):
        def get_results(q, offset=0, limit=100, snippets=3, fragsize=200, hl_phrase=False):
            m = re_bad_fields.match(q)
            if m:
                return { 'error': m.group(1) + ' search not supported' }
            q = escape_q(q)
            solr_params = [
                ('fl', 'ia,body_length,page_count'),
                ('hl', 'true'),
                ('hl.fl', 'body'),
                ('hl.snippets', snippets),
                ('hl.mergeContiguous', 'true'),
                ('hl.usePhraseHighlighter', 'true' if hl_phrase else 'false'),
                ('hl.simple.pre', '{{{'),
                ('hl.simple.post', '}}}'),
                ('hl.fragsize', fragsize),
                ('q.op', 'AND'),
                ('q', web.urlquote(q)),
                ('start', offset),
                ('rows', limit),
                ('qf', 'body'),
                ('qt', 'standard'),
                ('hl.maxAnalyzedChars', '-1'),
                ('wt', 'json'),
            ]
            results = inside_solr_select(dict(solr_params))
            # If there is any error in gettig the response, return the error
            if 'error' in results:
                return results

            ekey_doc = {}
            for doc in results['response']['docs']:
                ia = doc['ia']
                q = {'type': '/type/edition', 'ocaid': ia}
                ekeys = web.ctx.site.things(q)
                if not ekeys:
                    del q['ocaid']
                    q['source_records'] = 'ia:' + ia
                    ekeys = web.ctx.site.things(q)
                if ekeys:
                    ekey_doc[ekeys[0]] = doc

            editions = web.ctx.site.get_many(ekey_doc.keys())
            for e in editions:
                ekey_doc[e['key']]['edition'] = e
            return results

        return render_template('search/inside.tmpl', get_results, quote_snippet, editions_from_ia, read_from_archive)

def ia_lookup(path):
    h1 = httplib.HTTPConnection("archive.org")

    for attempt in range(5):
        h1.request("GET", path)
        res = h1.getresponse()
        res.read()
        if res.status != 200:
            break
    assert res.status == 302
    new_url = res.getheader('location')

    re_new_url = re.compile('^http://([^/]+\.us\.archive\.org)(/.+)$')

    m = re_new_url.match(new_url)
    return m.groups()

re_h1_error = re.compile('<center><h1>(.+?)</h1></center>')

class snippets(delegate.page):
    path = '/search/inside/(.+)'
    def GET(self, ia):
        def find_doc(ia, host, ia_path):
            abbyy_gz = '_abbyy.gz'
            files_xml = 'http://%s%s/%s_files.xml' % (host, ia_path, ia)
            xml_data = urllib.urlopen(files_xml)
            for e in etree.parse(xml_data).getroot():
                if e.attrib['name'].endswith(abbyy_gz):
                    return e.attrib['name'][:-len(abbyy_gz)]

        def find_matches(ia, q):
            q = escape_q(q)
            host, ia_path = ia_lookup('/download/' + ia)
            doc = find_doc(ia, host, ia_path) or ia

            url = 'http://' + host + '/fulltext/inside.php?item_id=' + ia + '&doc=' + doc + '&path=' + ia_path + '&q=' + web.urlquote(q)
            ret = urllib.urlopen(url).read().replace('"matches": [],\n}', '"matches": []\n}')
            try:
                return simplejson.loads(ret)
            except:
                m = re_h1_error.search(ret)
                return { 'error': web.htmlunquote(m.group(1)) }
        return render_template('search/snippets.tmpl', find_matches, ia)

########NEW FILE########
__FILENAME__ = test_inside
from openlibrary.plugins.inside.code import escape_q, quote_snippet

def test_escape_q():
    unchanged = ['test', 'ia:aaa', 'ia: aaa']
    for s in unchanged:
        assert escape_q(s) == s

    assert escape_q('aaa: bbb') == 'aaa\\: bbb'

def test_quote_snippet():
    assert quote_snippet('test') == 'test'
    assert quote_snippet('aaa {{{bbb ccc}}} ddd') == 'aaa <b>bbb ccc</b> ddd'
    assert quote_snippet('aaa {{{bbb\nccc}}} ddd') == 'aaa <b>bbb<br>ccc</b> ddd'
    assert quote_snippet('aaa {{{bbb & ccc}}} ddd') == 'aaa <b>bbb &amp; ccc</b> ddd'
    assert quote_snippet('aaa {{{bbb}}} {{{ccc}}} ddd') == 'aaa <b>bbb</b> <b>ccc</b> ddd'

########NEW FILE########
__FILENAME__ = code
import web
import Cookie
import urllib

from infogami.utils.view import render_template
from infogami.utils import delegate
from infogami import config

from openlibrary.plugins.worksearch import code as worksearch

class MobileMiddleware:
    """WSGI middleware to delegate requests to the mobile app when the mobile
    cookie is on.
    """
    def __init__(self, wsgi_app):

        self.wsgi_app = wsgi_app

    def cookies(self, env, **defaults):
        # adopted from web.cookies
        cookie = Cookie.SimpleCookie()
        cookie.load(env.get('HTTP_COOKIE', ''))
        d = web.storify(cookie, **defaults)
        for k, v in d.items():
            d[k] = v and urllib.unquote(v)
        return d
        
    def is_mobile_device(self, environ):
        useragent = environ.get('HTTP_USER_AGENT', '').lower()
        return 'ipad' in useragent or 'iphone' in useragent
        
    def __call__(self, environ, start_response):
        cookies = self.cookies(environ, mobile="true")
        # delegate to mobile app when cookie mobile is set to true. 
        if (cookies.mobile == "true" 
            and self.is_mobile_device(environ) 
            # XXXarielb there's probably a better way to do this
            # we need to serve the query entrypoint on mobile too
            and not environ.get('PATH_INFO', '').startswith("/query")):
            return mobile_wsgi_app(environ, start_response)
        else:
            return self.wsgi_app(environ, start_response)

config.middleware.append(MobileMiddleware)
            
urls = (
    "/", "index",
    "(/books/OL\d+M)(?:/.*)?", "book",
    "(/authors/OL\d+A)(?:/.*)?", "author",
    "/search", "search",
    "/images/.*", "static",
)

app = web.application(urls, globals())
# to setup ctx.site
app.add_processor(web.loadhook(delegate.initialize_context))
mobile_wsgi_app = app.wsgifunc()

def layout(page, title=None):
    return render_template("mobile/site", page, title=title)

class index:
    def GET(self):
        rc = web.ctx.site.recentchanges({"bot": True, "limit": 1000, "author": "/people/ImportBot"})
        edition_keys = []
        for change in rc:
            edition_keys.extend([c.key for c in change.changes 
                                 if c.revision == 1 and c.key.startswith("/books/")])
        editions = [ed for ed in web.ctx.site.get_many(edition_keys) if ed.ocaid]
        return layout(render_template("mobile/index", new_books=editions[:10]))

def _editions_for_works(works):
    ocaids = set()

    for work in works:
        if work.get('ia'):
            # just use the first one for now
            ocaids.add(work['ia'][0])

    edition_keys = web.ctx.site.things({"type": "/type/edition", "ocaid": list(ocaids)})
    editions = web.ctx.site.get_many(edition_keys)

    work_key_to_edition = {}
    for edition in editions:
        for work in edition.works:
            work_key_to_edition[work.key] = edition

    for work in works:
        if work.key in work_key_to_edition:
            edition = work_key_to_edition[work.key]
            yield edition, work
    

class search:
    def _do_search(self, q):
        ugly = worksearch.do_search({"q": q, "has_fulltext": "true", "public_scan": "true"}, None)
        results = web.storage({'num_found': ugly['num_found'], 'books': []})
        works = [worksearch.get_doc(doc) for doc in ugly['docs']]
        for work in works:
            work.key = '/works/%s' % work.key
        for edition, work in _editions_for_works(works):
            results['books'].append((edition, work))
        return results

    def GET(self):
        i = web.input(q="")
        results = self._do_search(i.q)
        return layout(render_template("mobile/search", q=i.q, results=results))

class book:
    def GET(self, key):
        book = web.ctx.site.get(key)
        return layout(render_template("mobile/book", book=book), title=book.title)

class author:
    def GET(self, key):
        author = web.ctx.site.get(key)
        books = []
        works = author.get_books()
        works = [work for work in works['works'] if work.get('has_fulltext')]
        for edition, work in _editions_for_works(works):
            books.append((edition, work))
        
        return layout(render_template("mobile/author", author=author, books=books), title=author.title)

class static:
    # just used in the development. 
    # on production /images/.* is handled by lighttpd.
    
    def GET(self):
        raise web.seeother('/static/upstream' + web.ctx.path)

########NEW FILE########
__FILENAME__ = code
"""
Open Library templates and macros.
"""

########NEW FILE########
__FILENAME__ = olmanage
#! /usr/bin/env python

import os
import urllib
import simplejson
import glob
import ConfigParser

import olapi
import subcommand

def to_local_path(path):
    """Convert path on server to local path.
    >>> to_local('/templates/site.tmpl')
    'templates/site.html'
    """
    if path.startswith('/templates'):
        return path[1:].replace('.tmpl', '.html')
    elif path.startswith('/macros'):
        return path[1:] + '.html'
    elif path.startswith('/css'):
        return path[1:]
    else:
        error

def to_server_path(path):
    """
    >>> to_server_path('templates/site.html')
    '/templates/site.tmpl'
    >>> to_server_path('macros/RecentChanges.html')
    '/macros/RecentChanges'
    """
    if path.startswith('templates/'):
        return '/' + path.replace('.html', '.tmpl')
    elif path.startswith('macros/'):
        return '/' + path.replace('.html', '')
    elif path.startswith('css/'):
        return '/' + path
    else:
        error

def jsonget(url):
    return simplejson.loads(urllib.urlopen(url).read())

def thing2data(d):
    if d['type']['key'] == '/type/template':
        return d['body']['value']
    elif d['type']['key'] == '/type/macro':
        return d['macro']['value']
    if d['type']['key'] == '/type/rawtext':
        return d['body']['value']
    else:
        error

def update_thing(d, filename):
    data = open(filename).read()
    if d is None:
        if filename.startswith('templates'):
            d = {'type': '/type/template'}
        elif filename.startswith('macros'):
            d = {'type': '/type/macro'}
        elif filename.startswith('css'):
            d = {'type': '/type/rawtext', 'content_type': 'text/css'}
        else:
            error
        d['key'] = to_server_path(filename)

    if d['type'] == '/type/template':
        d['body'] = data
    elif d['type'] == '/type/macro':
        d['macro'] = data
    elif d['type'] == '/type/rawtext':
        d['body'] = data
    else:
        print d
        error
    return d

def olget(server, key):
    d = jsonget(server + key + '.json')
    return thing2data(d)

def get_ol(server):
    config = ConfigParser.ConfigParser()
    config.read(os.path.expanduser('~/.olrc'))

    ol = olapi.OpenLibrary(server)
    username = config.get('account', 'username')
    password = config.get('account', 'password')
    ol.login(username, password)
    return ol

def olput(server, key, filename, comment):
    print "olput", server, repr(key), repr(comment)
    def get(key):
        try:
            return ol.get(key)
        except olapi.OLError:
            return None

    ol = get_ol(server)
    d = update_thing(get(key), filename)
    print ol.save(key, d, comment=comment)

def find_files(dir):
    """Find all files in the given dir.
    """
    for dirpath, dirnames, filenames in os.walk(dir):
        for f  in filenames:
            yield os.path.join(dirpath, f)

def write(path, data):
    dir = os.path.dirname(path)
    if not os.path.exists(dir):
        os.makedirs(dir)

    f = open(path, 'w')
    f.write(data)
    f.close()

def pull_one(server, filename):
    print 'pull', filename
    key = to_server_path(filename)
    data = olget(server, key)
    write(filename, data)

def push_one(server, filename, comment=None):
    print 'push', filename
    key = to_server_path(filename)
    olput(server, key, filename, comment=comment)

def diff_one(server, filename):
    print 'diff', filename
    key = to_server_path(filename)
    data = olget(server, key)
    write('/tmp/server-file', data)
    os.system('diff -u /tmp/server-file ' + filename)

@subcommand.subcommand()
def pull(options, *args):
    """Pull templates/macros from openlibrary.org website.

    Usage: pull [--server server] file1 file2

    Options:
    --server server : server address (default: http://openlibrary.org)
    """
    #pull_stuff(options.server, args, type='/type/template', prefix='/templates', extn='.html')
    for f in args:
        pull_one(options.server, f)

@subcommand.subcommand()
def pullall(options, *args):
    """Pull all templates/macros from openlibrary.org website.

    Usage: pullall [--server server]

    Options:
    --server server : server address (default: http://openlibrary.org)
    -O --output output: output directory (default: .)
    """
    pages = get_templates(options.server) + get_macros(options.server) + get_css(options.server)

    for d in pages:
        path = os.path.join(options.output, to_local_path(d['key']))
        print 'writing', path
        write(path, thing2data(d))

def get_templates(server):
    return jsonget(server + "/query.json?type=/type/template&key~=/templates/*&*=&limit=1000")

def get_macros(server):
    return jsonget(server + "/query.json?type=/type/macro&key~=/macros/*&*=&limit=1000")

def get_css(server):
    return jsonget(server + "/query.json?type=/type/rawtext&key~=/css/*&*=&limit=1000")

@subcommand.subcommand()
def pushall(options, *args):
    """Push all templates/macros to openlibrary.org website.

    Usage: pushall [--server server]

    Options:
    --server server : server address (default: http://openlibrary.org)
    -m [--message] message: commit message (default: push templates and macros)
    """
    pages = get_templates(options.server) + get_macros(options.server) + get_css(options.server)
    pages = olapi.unmarshal(pages)
    pages = dict((to_local_path(p['key']), p) for p in pages)

    query = []
    files = list(find_files('templates/')) + list(find_files('macros/'))
    for f in files:
        page = pages.get(f)
        d = update_thing(page and page.copy(), f)
        if page != d:
            query.append(d)

    for q in query:
        print q['key']

    get_ol(options.server).save_many(query, comment=options.message)

@subcommand.subcommand()
def push(options, *args):
    """Push templates/macros to openlibrary.org website.

    Usage: push [--server server] file1 file2

    Options:
    --server server : server address (default: http://openlibrary.org)
    -m [--message] message: commit message (default: )
    """
    for f in args:
        push_one(options.server, f, comment=options.message)

@subcommand.subcommand()
def diff(options, *args):
    """Compare local templates/macros with corresponding items on openlibrary.org website.

    Usage: diff [--server server] file1 file2

    Options:
    --server server : server address (default: http://openlibrary.org)
    """
    for f in args:
        diff_one(options.server, f)

@subcommand.subcommand()
def diffall(options, *args):
    """Compare all local templates/macros with corresponding items on openlibrary.org website.

    Usage: diffall [--server server]

    Options:
    --server server : server address (default: http://openlibrary.org)
    """
    output = options.server.replace('http://', '/tmp/')

    pullall(['--server', options.server, '--output', output])
    os.system("diff -u %s/templates templates" % output)
    os.system("diff -u %s/macros macros" % output)
    os.system("rm -rf " + output)

if __name__ == "__main__":
    subcommand.main()

########NEW FILE########
__FILENAME__ = ol_infobase
#!/usr/bin/env python
"""Open Library plugin for infobase.
"""
import os
import datetime
import urllib
import simplejson
import logging, logging.config
import sys
import traceback
import re, unicodedata

import web
from infogami.infobase import config, common, server, cache, dbstore

# relative import
from .openlibrary import schema
from ..utils.isbn import isbn_10_to_isbn_13, isbn_13_to_isbn_10

logger = logging.getLogger("infobase.ol")

def init_plugin():
    """Initialize infobase plugin."""
    from infogami.infobase import common, dbstore, server, logger as infobase_logger
    dbstore.default_schema = schema.get_schema()
    
    # Replace infobase Indexer with OL custom Indexer
    dbstore.Indexer = OLIndexer

    if config.get('errorlog'):
        common.record_exception = lambda: save_error(config.errorlog, 'infobase')

    ol = server.get_site('openlibrary.org')
    ib = server._infobase
    
    if config.get('writelog'):
        ib.add_event_listener(infobase_logger.Logger(config.writelog))
        
    ib.add_event_listener(invalidate_most_recent_change)
    
    # When setting up the dev instance, celery is not available. 
    # Using DISABLE_CELERY environment variable to decide whether or not to trigger celery events. 
    # DISABLE_CELERY will be set to true when setting up the dev instance.
    if os.getenv("DISABLE_CELERY", "").lower() != "true":
        ib.add_event_listener(notify_celery)

    setup_logging()

    if ol:
        # install custom indexer
        #XXX-Anand: this might create some trouble. Commenting out.
        # ol.store.indexer = Indexer()
        
        if config.get('http_listeners'):
            logger.info("setting up http listeners")
            ol.add_trigger(None, http_notify)
            
        ## memcache invalidator is not required now. It was added for future use.
        #_cache = config.get("cache", {})
        #if _cache.get("type") == "memcache":
        #    logger.info("setting up memcache invalidater")
        #    ol.add_trigger(None, MemcacheInvalidater())
    
    # hook to add count functionality
    server.app.add_mapping("/([^/]*)/count_editions_by_author", __name__ + ".count_editions_by_author")
    server.app.add_mapping("/([^/]*)/count_editions_by_work", __name__ + ".count_editions_by_work")
    server.app.add_mapping("/([^/]*)/count_edits_by_user", __name__ + ".count_edits_by_user")
    server.app.add_mapping("/([^/]*)/most_recent", __name__ + ".most_recent")
    server.app.add_mapping("/([^/]*)/clear_cache", __name__ + ".clear_cache")
    server.app.add_mapping("/([^/]*)/stats/(\d\d\d\d-\d\d-\d\d)", __name__ + ".stats")
    server.app.add_mapping("/([^/]*)/has_user", __name__ + ".has_user")
    server.app.add_mapping("/([^/]*)/olid_to_key", __name__ + ".olid_to_key")
    server.app.add_mapping("/_reload_config", __name__ + ".reload_config")
    server.app.add_mapping("/_inspect", __name__ + "._inspect")

def setup_logging():
    try:
        logconfig = config.get("logging_config_file")
        if logconfig and os.path.exists(logconfig):
            logging.config.fileConfig(logconfig, disable_existing_loggers=False)
        logger.info("logging initialized")
        logger.debug("debug")
    except Exception, e:
        print >> sys.stderr, "Unable to set logging configuration:", str(e)
        raise

class reload_config:
    @server.jsonify
    def POST(self):
        logging.info("reloading logging config")
        setup_logging()
        return {"ok": "true"}

class _inspect:
    """Backdoor to inspect the running process.

    Tries to import _inspect module and executes inspect function in that. The module is reloaded on every invocation.
    """
    def GET(self):
        sys.modules.pop("_inspect", None)
        try:
            import _inspect
            return _inspect.inspect()
        except Exception, e:
            return traceback.format_exc()

def get_db():
    site = server.get_site('openlibrary.org')
    return site.store.db
    
@web.memoize
def get_property_id(type, name):
    db = get_db()
    type_id = get_thing_id(type)
    try:
        return db.where('property', type=type_id, name=name)[0].id
    except IndexError:
        return None
    
def get_thing_id(key):
    try:
        return get_db().where('thing', key=key)[0].id
    except IndexError:
        return None

def count(table, type, key, value):
    pid = get_property_id(type, key)

    value_id = get_thing_id(value)
    if value_id is None:
        return 0                
    return get_db().query("SELECT count(*) FROM " + table + " WHERE key_id=$pid AND value=$value_id", vars=locals())[0].count
        
class count_editions_by_author:
    @server.jsonify
    def GET(self, sitename):
        i = server.input('key')
        return count('edition_ref', '/type/edition', 'authors', i.key)
        
class count_editions_by_work:
    @server.jsonify
    def GET(self, sitename):
        i = server.input('key')
        return count('edition_ref', '/type/edition', 'works', i.key)
        
class count_edits_by_user:
    @server.jsonify
    def GET(self, sitename):
        i = server.input('key')
        author_id = get_thing_id(i.key)
        return get_db().query("SELECT count(*) as count FROM transaction WHERE author_id=$author_id", vars=locals())[0].count

class has_user:
    @server.jsonify
    def GET(self, sitename):
        i = server.input("username")
        
        # Don't allows OLIDs to be usernames
        if web.re_compile(r"OL\d+[A-Z]").match(i.username.upper()):
            return True
        
        key = "/user/" + i.username.lower()
        type_user = get_thing_id("/type/user")
        d = get_db().query("SELECT * from thing WHERE lower(key) = $key AND type=$type_user", vars=locals())
        return bool(d)
    
class stats:
    @server.jsonify
    def GET(self, sitename, today):
        return dict(self.stats(today))
        
    def stats(self, today):
        tomorrow = self.nextday(today)
        yield 'edits', self.edits(today, tomorrow)
        yield 'edits_by_bots', self.edits(today, tomorrow, bots=True)
        yield 'new_accounts', self.new_accounts(today, tomorrow)
        
    def nextday(self, today):
        return get_db().query("SELECT date($today) + 1 AS value", vars=locals())[0].value

    def edits(self, today, tomorrow, bots=False):
        tables = 'version v, transaction t'
        where = 'v.transaction_id=t.id AND t.created >= date($today) AND t.created < date($tomorrow)'

        if bots:
            where += " AND t.author_id IN (SELECT thing_id FROM account WHERE bot = 't')"

        return self.count(tables=tables, where=where, vars=locals())
        
    def new_accounts(self, today, tomorrow):
        type_user = get_thing_id('/type/user')
        return self.count(
            'thing', 
            'type=$type_user AND created >= date($today) AND created < date($tomorrow)',
            vars=locals())
    
    def total_accounts(self):
        type_user = get_thing_id('/type/user')
        return self.count(tables='thing', where='type=$type_user', vars=locals())
        
    def count(self, tables, where, vars):
        return get_db().select(
            what="count(*) as value",
            tables=tables,
            where=where,
            vars=vars
        )[0].value
    
most_recent_change = None

def invalidate_most_recent_change(event):
    global most_recent_change
    most_recent_change = None

class most_recent:
    @server.jsonify
    def GET(self, sitename):
        global most_recent_change
        if most_recent_change is None:
            site = server.get_site('openlibrary.org')
            most_recent_change = site.versions({'limit': 1})[0]
        return most_recent_change
        
class clear_cache:
    @server.jsonify
    def POST(self, sitename):
        from infogami.infobase import cache
        cache.global_cache.clear()
        return {'done': True}
        
class olid_to_key:
    @server.jsonify
    def GET(self, sitename):
        i = server.input("olid")
        d = get_db().query("SELECT key FROM thing WHERE get_olid(key) = $i.olid", vars=locals())
        key = d and d[0].key or None
        return {"olid": i.olid, "key": key}
        
def write(path, data):
    dir = os.path.dirname(path)
    if not os.path.exists(dir):
        os.makedirs(dir)
    f = open(path, 'w')
    f.write(data)
    f.close()
    
from .. import tasks

def notify_celery(event):
    """Called on infobase events to notify celery for on every edit.
    """
    if event.name in ['save', 'save_many']:
        changeset = event.data['changeset']
        tasks.on_edit.delay(changeset)
    
def save_error(dir, prefix):
    try:
        logger.error("Error", exc_info=True)
        error = web.djangoerror()
        now = datetime.datetime.utcnow()
        path = '%s/%04d-%02d-%02d/%s-%02d%02d%02d.%06d.html' % (dir, \
            now.year, now.month, now.day, prefix,
            now.hour, now.minute, now.second, now.microsecond)
        logger.error("Error saved to %s", path)
        write(path, web.safestr(error))
    except:
        logger.error("Exception in saving the error", exc_info=True)
    
def get_object_data(site, thing):
    """Return expanded data of specified object."""
    def expand(value):
        if isinstance(value, list):
            return [expand(v) for v in value]
        elif isinstance(value, common.Reference):
            t = site._get_thing(value)
            return t and t._get_data()
        else:
            return value

    d = thing._get_data()
    for k, v in d.iteritems():
        # save some space by not expanding type
        if k != 'type':
            d[k] = expand(v)
    return d

def http_notify(site, old, new):
    """Notify listeners over http."""
    if isinstance(new, dict):
        data = new
    else:
        # new is a thing. call format_data to get the actual data.
        data = new.format_data()
        
    json = simplejson.dumps(data)
    key = data['key']

    # optimize the most common case. 
    # The following prefixes are never cached at the client. Avoid cache invalidation in that case.
    not_cached = ['/b/', '/a/', '/books/', '/authors/', '/works/', '/subjects/', '/publishers/', '/user/', '/usergroup/', '/people/']
    for prefix in not_cached:
        if key.startswith(prefix):
            return
    
    for url in config.http_listeners:
        try:
            response = urllib.urlopen(url, json).read()
            print >> web.debug, "http_notify", repr(url), repr(key), repr(response)
        except:
            print >> web.debug, "failed to send http_notify", repr(url), repr(key)
            import traceback
            traceback.print_exc()
            
class MemcacheInvalidater:
    def __init__(self):
        self.memcache = self.get_memcache_client()
        
    def get_memcache_client(self):
        _cache = config.get("cache", {})
        if _cache.get("type") == "memcache" and "servers" in _cache:
            return olmemcache.Client(_cache['servers'])
            
    def to_dict(self, d):
        if isinstance(d, dict):
            return d
        else:
            # new is a thing. call format_data to get the actual data.
            return d.format_data()
        
    def __call__(self, site, old, new):
        if not old:
            return
            
        old = self.to_dict(old)
        new = self.to_dict(new)
        
        type = old['type']['key']
        
        if type == '/type/author':
            keys = self.invalidate_author(site, old)
        elif type == '/type/edition':
            keys = self.invalidate_edition(site, old)
        elif type == '/type/work':
            keys = self.invalidate_work(site, old)
        else:
            keys = self.invalidate_default(site, old)
            
        self.memcache.delete_multi(['details/' + k for k in keys])
        
    def invalidate_author(self, site, old):
        yield old.key

    def invalidate_edition(self, site, old):
        yield old.key
        
        for w in old.get('works', []):
            if 'key' in w:
                yield w['key']

    def invalidate_work(self, site, old):
        yield old.key
        
        # invalidate all work.editions
        editions = site.things({"type": "/type/edition", "work": old.key})
        for e in editions:
            yield e['key']
            
        # invalidate work.authors
        authors = work.get('authors', [])
        for a in authors:
            if 'author' in a and 'key' in a['author']:
                yield a['author']['key']
    
    def invalidate_default(self, site, old):
        yield old.key
            
# openlibrary.utils can't be imported directly because 
# openlibrary.plugins.openlibrary masks openlibrary module
olmemcache = __import__("openlibrary.utils.olmemcache", None, None, ['x'])

def MemcachedDict(servers=[]):
    """Cache implementation with OL customized memcache client."""
    client = olmemcache.Client(servers)
    return cache.MemcachedDict(memcache_client=client)

cache.register_cache('memcache', MemcachedDict)

def _process_key(key):
    mapping = (
        "/l/", "/languages/",
        "/a/", "/authors/",
        "/b/", "/books/",
        "/user/", "/people/"
    )
    for old, new in web.group(mapping, 2):
        if key.startswith(old):
            return new + key[len(old):]
    return key

def _process_data(data):
    if isinstance(data, list):
        return [_process_data(d) for d in data]
    elif isinstance(data, dict):
        if 'key' in data:
            data['key'] = _process_key(data['key'])
        return dict((k, _process_data(v)) for k, v in data.iteritems())
    else:
        return data

def safeint(value, default=0):
    """Convers the value to integer. Returns 0, if the conversion fails."""
    try:
        return int(value)
    except Exception:
        return default
    
def fix_table_of_contents(table_of_contents):
    """Some books have bad table_of_contents. This function converts them in to correct format.
    """
    def row(r):
        if isinstance(r, basestring):
            level = 0
            label = ""
            title = web.safeunicode(r)
            pagenum = ""
        elif 'value' in r:
            level = 0
            label = ""
            title = web.safeunicode(r['value'])
            pagenum = ""
        elif isinstance(r, dict):
            level = safeint(r.get('level', '0'), 0)
            label = r.get('label', '')
            title = r.get('title', '')
            pagenum = r.get('pagenum', '')
        else:
            return {}

        return dict(level=level, label=label, title=title, pagenum=pagenum)

    d = [row(r) for r in table_of_contents]
    return [row for row in d if any(row.values())]

def process_json(key, json):
    if key is None or json is None:
        return None
    base = key[1:].split("/")[0]
    if base in ['authors', 'books', 'works', 'languages', 'people', 'usergroup', 'permission']:
        data = simplejson.loads(json)
        data = _process_data(data)
        
        if base == 'books' and 'table_of_contents' in data:
            data['table_of_contents'] = fix_table_of_contents(data['table_of_contents'])
        
        json = simplejson.dumps(data)
    return json
    
dbstore.process_json = process_json

_Indexer = dbstore.Indexer

re_normalize = re.compile('[^[:alphanum:] ]', re.U)

class OLIndexer(_Indexer):
    """OL custom indexer to index normalized_title etc.
    """
    def compute_index(self, doc):
        type = self.get_type(doc)

        if type == '/type/edition':
            doc = self.process_edition_doc(doc)

        return _Indexer.compute_index(self, doc)

    def get_type(self, doc):
        return doc.get("type", {}).get("key")

    def process_edition_doc(self, doc):
        """Process edition doc to add computed fields used for import.

        Make the computed field names end with an underscore to avoid conflicting with regular fields.
        """
        doc = dict(doc)

        title = doc.get("title", "")
        doc['normalized_title_'] = self.normalize_edition_title(title)

        isbns = doc.get("isbn_10", []) + doc.get("isbn_13", [])
        isbns = [self.normalize_isbn(isbn) for isbn in isbns]
        doc['isbn_'] = self.expand_isbns(isbns)
        return doc

    def normalize_edition_title(self, title):
        if isinstance(title, str):
            title = title.decode('utf-8', "ignore")
            
        if not isinstance(title, unicode):
            return ""
            
        # http://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-in-a-python-unicode-string
        def strip_accents(s):
           return ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'))

        norm = strip_accents(title).lower()
        norm = norm.replace(' and ', ' ')
        if norm.startswith('the '):
            norm = norm[4:]
        elif norm.startswith('a '):
            norm = norm[2:]
        return norm.replace(' ', '')[:25]
    
    def normalize_isbn(self, isbn):
        return isbn.strip().upper().replace(" ", "").replace("-", "")

    def expand_isbns(self, isbns):
        """Expands the list of isbns by adding ISBN-10 for ISBN-13 and vice-verse.
        """
        s = set(isbns)
        for isbn in isbns:
            isbn = isbn.replace("-", "")
            if len(isbn) == 10:
                s.add(isbn_10_to_isbn_13(isbn))
            else:
                s.add(isbn_13_to_isbn_10(isbn))
        return [isbn for isbn in s if isbn is not None]
########NEW FILE########
__FILENAME__ = api
"""API stuff."""
import web
import simplejson

from infogami.utils import delegate
from openlibrary.core import helpers as h

def setup():
    # placeholder
    pass

class work_editions(delegate.page):
    path = "(/works/OL\d+W)/editions"
    encoding = "json"
    
    def GET(self, key):
        doc = web.ctx.site.get(key)
        if not doc or doc.type.key != "/type/work":
            raise web.notfound('')
        else:
            i = web.input(limit=50, offset=0)
            limit = h.safeint(i.limit) or 50
            offset = h.safeint(i.offset) or 0
            
            data = self.get_editions_data(doc, limit=limit, offset=offset)
            return delegate.RawText(simplejson.dumps(data), content_type="application/json")
            
    def get_editions_data(self, work, limit, offset):
        if limit > 1000:
            limit = 1000
            
        keys = web.ctx.site.things({"type": "/type/edition", "works": work.key, "limit": limit, "offset": offset})
        editions = web.ctx.site.get_many(keys, raw=True)
        
        size = work.edition_count
        links = {
            "self": web.ctx.fullpath,
            "work": work.key,
        }
        
        if offset > 0:
            links['prev'] = web.changequery(offset=min(0, offset-limit))
            
        if offset + len(editions) < size:
            links['next'] = web.changequery(offset=offset+limit)
        
        return {
            "links": links,
            "size": size,
            "entries": editions
        }

class author_works(delegate.page):
    path = "(/authors/OL\d+A)/works"
    encoding = "json"
    
    def GET(self, key):
        doc = web.ctx.site.get(key)
        if not doc or doc.type.key != "/type/author":
            raise web.notfound('')
        else:
            i = web.input(limit=50, offset=0)
            limit = h.safeint(i.limit) or 50
            offset = h.safeint(i.offset) or 0
            
            data = self.get_works_data(doc, limit=limit, offset=offset)
            return delegate.RawText(simplejson.dumps(data), content_type="application/json")
            
    def get_works_data(self, author, limit, offset):
        if limit > 1000:
            limit = 1000
            
        keys = web.ctx.site.things({"type": "/type/work", "authors": {"author": {"key": author.key}}, "limit": limit, "offset": offset})
        works = web.ctx.site.get_many(keys, raw=True)
        
        size = author.get_work_count()
        links = {
            "self": web.ctx.fullpath,
            "author": author.key,
        }
        
        if offset > 0:
            links['prev'] = web.changequery(offset=min(0, offset-limit))
            
        if offset + len(works) < size:
            links['next'] = web.changequery(offset=offset+limit)
        
        return {
            "links": links,
            "size": size,
            "entries": works
        }
########NEW FILE########
__FILENAME__ = authors
from infogami.utils import delegate
from infogami.utils.view import render_template

def setup():
    pass

class author(delegate.page):
    path = "/authors"
    
    def GET(self):
        return render_template("authors/index.html")


########NEW FILE########
__FILENAME__ = borrow_home
"""Controller for /borrow page.
"""
import simplejson
import web
import random
import datetime

from infogami.plugins.api.code import jsonapi
from infogami.utils import delegate
from infogami.utils.view import render_template

from openlibrary.core import helpers as h
from openlibrary.core import inlibrary
from openlibrary.plugins.worksearch.subjects import SubjectEngine

from libraries import LoanStats

class borrow(delegate.page):
    path = "/borrow"
    
    def is_enabled(self):
        return "inlibrary" in web.ctx.features
    
    def GET(self):
        rand = random.randint(0, 9999)
        sort = "random_%d desc" % rand
        is_inlibrary = inlibrary.get_library() is not None
        subject = get_lending_library(web.ctx.site, details=True, inlibrary=is_inlibrary, limit=24, sort=sort)
        return render_template("borrow/index", subject, stats=LoanStats(), rand=rand, inlibrary=is_inlibrary)

class borrow(delegate.page):
    path = "/borrow"
    encoding = "json"

    def is_enabled(self):
        return "inlibrary" in web.ctx.features

    @jsonapi
    def GET(self):
        i = web.input(offset=0, limit=24, rand=-1, details="false", has_fulltext="false")

        filters = {}
        if i.get("has_fulltext") == "true":
            filters["has_fulltext"] = "true"

        if i.get("published_in"):
            if "-" in i.published_in:
                begin, end = i.published_in.split("-", 1)

                if h.safeint(begin, None) is not None and h.safeint(end, None) is not None:
                    filters["publish_year"] = [begin, end]
            else:
                y = h.safeint(i.published_in, None)
                if y is not None:
                    filters["publish_year"] = i.published_in

        i.limit = h.safeint(i.limit, 12)
        i.offset = h.safeint(i.offset, 0)

        i.rand = h.safeint(i.rand, -1)

        if i.rand > 0:
            sort = 'random_%d desc' % i.rand
            filters['sort'] = sort

        subject = get_lending_library(web.ctx.site, 
            offset=i.offset, 
            limit=i.limit, 
            details=i.details.lower() == "true", 
            inlibrary=inlibrary.get_library() is not None,
            **filters)
        return simplejson.dumps(subject)

class read(delegate.page):
    path = "/read"
    
    def GET(self):
        rand = random.randint(0, 9999)
        sort = "random_%d desc" % rand
        subject = get_readable_books(web.ctx.site, details=True, limit=24, sort=sort)
        return render_template("borrow/read", subject, stats=LoanStats(), rand=rand)

class read(delegate.page):
    path = "/read"
    encoding = "json"

    @jsonapi
    def GET(self):
        i = web.input(offset=0, limit=24, rand=-1, details="false", has_fulltext="false")

        filters = {}
        if i.get("has_fulltext") == "true":
            filters["has_fulltext"] = "true"

        if i.get("published_in"):
            if "-" in i.published_in:
                begin, end = i.published_in.split("-", 1)

                if h.safeint(begin, None) is not None and h.safeint(end, None) is not None:
                    filters["publish_year"] = [begin, end]
            else:
                y = h.safeint(i.published_in, None)
                if y is not None:
                    filters["publish_year"] = i.published_in

        i.limit = h.safeint(i.limit, 12)
        i.offset = h.safeint(i.offset, 0)

        i.rand = h.safeint(i.rand, -1)

        if i.rand > 0:
            sort = 'random_%d desc' % i.rand
            filters['sort'] = sort

        subject = get_readable_books(web.ctx.site, 
            offset=i.offset, 
            limit=i.limit, 
            details=i.details.lower() == "true",
            **filters)
        return simplejson.dumps(subject)

class borrow_about(delegate.page):
    path = "/borrow/about"
    
    def GET(self):
        return render_template("borrow/about")
        
def convert_works_to_editions(site, works):
    """Takes work docs got from solr and converts them into appropriate editions required for lending library.
    """
    ekeys = ['/books/' + w['lending_edition'] for w in works if w.get('lending_edition')]
    editions = {}
    for e in site.get_many(ekeys):
        editions[e['key']] = e.dict()
    
    for w in works:
        if w.get('lending_edition'):
            ekey = '/books/' + w['lending_edition']
            e = editions.get(ekey)
            if e and 'ocaid' in e:
                covers = e.get('covers') or [None]
                w['key'] = e['key']
                w['cover_id'] = covers[0]
                w['ia'] = e['ocaid']
                w['title'] = e.get('title') or w['title']

def get_lending_library(site, inlibrary=False, **kw):
    kw.setdefault("sort", "first_publish_year desc")

    if inlibrary:
        subject = CustomSubjectEngine().get_subject("/subjects/lending_library", in_library=True, **kw)
    else:
        subject = CustomSubjectEngine().get_subject("/subjects/lending_library", in_library=False, **kw)
    
    subject['key'] = '/borrow'
    convert_works_to_editions(site, subject['works'])
    return subject

def get_readable_books(site, **kw):
    kw.setdefault("sort", "first_publish_year desc")
    subject = ReadableBooksEngine().get_subject("/subjects/dummy", **kw)
    subject['key'] = '/read'
    return subject

class ReadableBooksEngine(SubjectEngine):
    """SubjectEngine for readable books.
    
    This doesn't take subject into account, but considers the public_scan_b
    field, which is derived from ia collections.

    There is a subject "/subjects/accessible_book", but it has some
    inlibrary/lendinglibrary books as well because of errors in OL data. Using
    public_scan_b derived from ia collections is more accurate.
    """

    def make_query(self, key, filters):
        return {
            "public_scan_b": "true"
        }

    def get_ebook_count(self, name, value, publish_year):
        # we are not displaying ebook count. 
        # No point making a solr query
        return 0
    

class CustomSubjectEngine(SubjectEngine):
    """SubjectEngine for inlibrary and lending_library combined."""
    def make_query(self, key, filters):
        meta = self.get_meta(key)

        q = {
            meta.facet_key: ["lending_library"], 
            'public_scan_b': "false",
            'NOT borrowed_b': "true",

            # show only books in last 20 or so years
            'publish_year': (str(1990), str(datetime.date.today().year)) # range
        }

        if filters:
            if filters.get('in_library') is True:
                q[meta.facet_key].append('in_library')
            if filters.get("has_fulltext") == "true":
                q['has_fulltext'] = "true"
            if filters.get("publish_year"):
                q['publish_year'] = filters['publish_year']

        return q
    
    def get_ebook_count(self, name, value, publish_year):
        # we are not displaying ebook count. 
        # No point making a solr query
        return 0


def setup():
    pass

########NEW FILE########
__FILENAME__ = code
"""
Open Library Plugin.
"""
import web
import simplejson
import os
import sys
import urllib
import socket
import random
import datetime
import logging
from time import time

import infogami

# make sure infogami.config.features is set
if not hasattr(infogami.config, 'features'):
    infogami.config.features = []
    
from infogami.utils import delegate
from infogami.utils.view import render, public, safeint, add_flash_message
from infogami.infobase import client
from infogami.core.db import ValidationException

from openlibrary.utils.isbn import isbn_13_to_isbn_10
import openlibrary.core.stats

import processors

delegate.app.add_processor(processors.ReadableUrlProcessor())
delegate.app.add_processor(processors.ProfileProcessor())


try:
    from infogami.plugins.api import code as api
except:
    api = None

# http header extension for OL API
infogami.config.http_ext_header_uri = "http://openlibrary.org/dev/docs/api"

# setup special connection with caching support
import connection
client._connection_types['ol'] = connection.OLConnection
infogami.config.infobase_parameters = dict(type="ol")

# set up infobase schema. required when running in standalone mode.
from openlibrary.core import schema
schema.register_schema()

from openlibrary.core import models
models.register_models()
models.register_types()

# Remove movefiles install hook. openlibrary manages its own files.
infogami._install_hooks = [h for h in infogami._install_hooks if h.__name__ != "movefiles"]

import lists
lists.setup()

logger = logging.getLogger("openlibrary")

class hooks(client.hook):
    def before_new_version(self, page):
        user = web.ctx.site.get_user()
        account = user and user.get_account()
        if account and account.is_blocked():
            raise ValidationException("Your account has been suspended. You are not allowed to make any edits.")
        
        if page.type.key == '/type/library':
            bad = list(page.find_bad_ip_ranges(page.ip_ranges or ""))
            if bad:
                raise ValidationException('Bad IPs: ' + '; '.join(bad))

        if page.key.startswith('/a/') or page.key.startswith('/authors/'):
            if page.type.key == '/type/author':
                return
                
            books = web.ctx.site.things({"type": "/type/edition", "authors": page.key})
            books = books or web.ctx.site.things({"type": "/type/work", "authors": {"author": {"key": page.key}}})
            if page.type.key == '/type/delete' and books:
                raise ValidationException("Deleting author pages is not allowed.")
            elif page.type.key != '/type/author' and books:
                raise ValidationException("Changing type of author pages is not allowed.")

@infogami.action
def sampledump():
    """Creates a dump of objects from OL database for creating a sample database."""
    def expand_keys(keys):
        def f(k):
            if isinstance(k, dict):
                return web.ctx.site.things(k)
            elif k.endswith('*'):
                return web.ctx.site.things({'key~': k})
            else:
                return [k]
        result = []
        for k in keys:
            d = f(k)
            result += d
        return result
        
    def get_references(data, result=None):
        if result is None:
            result = []
            
        if isinstance(data, dict):
            if 'key' in data:
                result.append(data['key'])
            else:
                get_references(data.values(), result)
        elif isinstance(data, list):
            for v in data:
                get_references(v, result)
        return result
        
    visiting = {}
    visited = set()
    
    def visit(key):
        if key in visited or key.startswith('/type/'):
            return
        elif key in visiting:
            # This is a case of circular-dependency. Add a stub object to break it.
            print simplejson.dumps({'key': key, 'type': visiting[key]['type']})
            visited.add(key)
            return
        
        thing = web.ctx.site.get(key)
        if not thing:
            return

        d = thing.dict()
        d.pop('permission', None)
        d.pop('child_permission', None)
        d.pop('table_of_contents', None)
        
        visiting[key] = d
        for ref in get_references(d.values()):
            visit(ref)
        visited.add(key)
        
        print simplejson.dumps(d)

    keys = [
        '/scan_record',
        '/scanning_center',
        {'type': '/type/scan_record', 'limit': 10},
    ]
    keys = expand_keys(keys) + ['/b/OL%dM' % i for i in range(1, 100)]
    visited = set()

    for k in keys:
        visit(k)
    
@infogami.action
def sampleload(filename="sampledump.txt.gz"):
    if filename.endswith('.gz'):
        import gzip
        f = gzip.open(filename)
    else:
        f = open(filename)
    
    queries = [simplejson.loads(line) for  line in f]
    print web.ctx.site.save_many(queries)

class addbook(delegate.page):
    path = "/addbook"
    
    def GET(self):
        d = {'type': web.ctx.site.get('/type/edition')}

        i = web.input()
        author = i.get('author') and web.ctx.site.get(i.author)
        if author:
            d['authors'] = [author]
        
        page = web.ctx.site.new("", d)
        return render.edit(page, self.path, 'Add Book')
        
    def POST(self):
        from infogami.core.code import edit
        key = web.ctx.site.new_key('/type/edition')
        web.ctx.path = key
        return edit().POST(key)

class addauthor(delegate.page):
    path = '/addauthor'
        
    def POST(self):
        i = web.input("name")
        if len(i.name) < 2:
            return web.badrequest()
        key = web.ctx.site.new_key('/type/author')
        web.ctx.path = key
        web.ctx.site.save({'key': key, 'name': i.name, 'type': dict(key='/type/author')}, comment='New Author')
        raise web.HTTPError("200 OK", {}, key)

class clonebook(delegate.page):
    def GET(self):
        from infogami.core.code import edit
        i = web.input("key")
        page = web.ctx.site.get(i.key)
        if page is None:
            raise web.seeother(i.key)
        else:
            d =page._getdata()
            for k in ['isbn_10', 'isbn_13', 'lccn', "oclc"]:
                 d.pop(k, None)
            return render.edit(page, '/addbook', 'Clone Book')

class search(delegate.page):
    path = "/suggest/search"
    
    def GET(self):
        i = web.input(prefix="")
        if len(i.prefix) > 2:
            q = {'type': '/type/author', 'name~': i.prefix + '*', 'sort': 'name', 'limit': 5}
            things = web.ctx.site.things(q)
            things = [web.ctx.site.get(key) for key in things]
        
            result = [dict(type=[{'id': t.key, 'name': t.key}], name=web.utf8(t.name), guid=t.key, id=t.key, article=dict(id=t.key)) for t in things]
        else:
            result = []
        callback = i.pop('callback', None)
        d = dict(status="200 OK", query=dict(i, escape='html'), code='/api/status/ok', result=result)

        if callback:
            data = '%s(%s)' % (callback, simplejson.dumps(d))
        else:
            data = simplejson.dumps(d)
        raise web.HTTPError('200 OK', {}, data)
        
class blurb(delegate.page):
    path = "/suggest/blurb/(.*)"
    def GET(self, path):
        i = web.input()        
        callback = i.pop('callback', None)
        author = web.ctx.site.get('/' +path)
        body = ''
        if author.birth_date or author.death_date:
            body = "%s - %s" % (author.birth_date, author.death_date)
        else:
            body = "%s" % author.date

        body += "<br/>"
        if author.bio:
            body += web.utf8(author.bio)

        result = dict(body=body, media_type="text/html", text_encoding="utf-8")
        d = dict(status="200 OK", code="/api/status/ok", result=result)
        if callback:
            data = '%s(%s)' % (callback, simplejson.dumps(d))
        else:
            data = simplejson.dumps(d)

        raise web.HTTPError('200 OK', {}, data)

class thumbnail(delegate.page):
    path = "/suggest/thumbnail"

@public
def get_property_type(type, name):
    for p in type.properties:
        if p.name == name:
            return p.expected_type
    return web.ctx.site.get("/type/string")

def save(filename, text):
    root = os.path.dirname(__file__)
    path = root + filename
    dir = os.path.dirname(path)
    if not os.path.exists(dir):
        os.makedirs(dir)
    f = open(path, 'w')
    f.write(text)
    f.close()

def change_ext(filename, ext):
    filename, _ = os.path.splitext(filename)
    if ext:
        filename = filename + ext
    return filename

def get_pages(type, processor):
    pages = web.ctx.site.things(dict(type=type))
    for p in pages:
        processor(web.ctx.site.get(p))

class flipbook(delegate.page):
    path = "/details/([a-zA-Z0-9_-]*)(?:/leaf(\d+))?"

    SCRIPT_PATH = "/petabox/sw/bin/find_item.php"

    def GET(self, identifier, leaf):
        if leaf:
            hash = '#page/n%s' % leaf
        else:
            hash = ""
        
        url = "http://www.archive.org/stream/%s%s" % (identifier, hash)
        raise web.seeother(url)

class bookreader(delegate.page):
    path = "/bookreader/(.*)"

    def GET(self, id):
        data = render.bookreader(id)
        raise web.HTTPError("200 OK", {}, data)

class robotstxt(delegate.page):
    path = "/robots.txt"
    def GET(self):
        web.header('Content-Type', 'text/plain')
        try:
            data = open('static/robots.txt').read()
            raise web.HTTPError("200 OK", {}, data)
        except IOError:
            raise web.notfound()

class health(delegate.page):
    path = "/health"
    def GET(self):
        web.header('Content-Type', 'text/plain')
        raise web.HTTPError("200 OK", {}, 'OK')

class change_cover(delegate.mode):
    def GET(self, key):
        page = web.ctx.site.get(key)
        if page is None or page.type.key not in  ['/type/edition', '/type/author']:
            raise web.seeother(key)
        return render.change_cover(page)
        
class bookpage(delegate.page):
    path = r"/(isbn|oclc|lccn|ia|ISBN|OCLC|LCCN|IA)/([^/]*)(/.*)?"

    def GET(self, key, value, suffix):
        key = key.lower()
        suffix = suffix or ""
        
        if key == "isbn":
            if len(value) == 13:
                key = "isbn_13"
            else:
                key = "isbn_10"
        elif key == "oclc":
            key = "oclc_numbers"
        elif key == "ia":
            key = "ocaid"

        if key != 'ocaid': # example: MN41558ucmf_6
            value = value.replace('_', ' ')

        if web.ctx.encoding and web.ctx.path.endswith("." + web.ctx.encoding): 
            ext = "." + web.ctx.encoding
        else:
            ext = ""
        
        if web.ctx.env.get('QUERY_STRING'):
            ext += '?' + web.ctx.env['QUERY_STRING']
            
        def redirect(key, ext, suffix):
            if ext:
                return web.found(key + ext)
            else:
                book = web.ctx.site.get(key)
                return web.found(book.url(suffix))

        q = {"type": "/type/edition", key: value}
        try:
            result = web.ctx.site.things(q)
            if result:
                raise redirect(result[0], ext, suffix)
            elif key =='ocaid':
                q = {"type": "/type/edition", 'source_records': 'ia:' + value}
                result = web.ctx.site.things(q)
                if result:
                    raise redirect(result[0], ext, suffix)
                q = {"type": "/type/volume", 'ia_id': value}
                result = web.ctx.site.things(q)
                if result:
                    raise redirect(result[0], ext, suffix)
                else:
                    raise redirect("/books/ia:" + value, ext, suffix)
            web.ctx.status = "404 Not Found"
            return render.notfound(web.ctx.path, create=False)
        except web.HTTPError:
            raise
        except:
            logger.error("unexpected error", exc_info=True)
            web.ctx.status = "404 Not Found"
            return render.notfound(web.ctx.path, create=False)

delegate.media_types['application/rdf+xml'] = 'rdf'
class rdf(delegate.mode):
    name = 'view'
    encoding = 'rdf'

    def GET(self, key):
        page = web.ctx.site.get(key)
        if not page:
            raise web.notfound("")
        else:
            from infogami.utils import template
            try:
                result = template.typetemplate('rdf')(page)
            except:
                raise web.notfound("")
            else:
                return delegate.RawText(result, content_type="application/rdf+xml; charset=utf-8")

delegate.media_types[' application/atom+xml;profile=opds'] = 'opds'
class opds(delegate.mode):
    name = 'view'
    encoding = 'opds'

    def GET(self, key):
        page = web.ctx.site.get(key)
        if not page:
            raise web.notfound("")
        else:
            from infogami.utils import template
            import opds
            try:
                result = template.typetemplate('opds')(page, opds)
            except:
                raise web.notfound("")
            else:
                return delegate.RawText(result, content_type=" application/atom+xml;profile=opds")

delegate.media_types['application/marcxml+xml'] = 'marcxml'
class marcxml(delegate.mode):
    name = 'view'
    encoding = 'marcxml'
    
    def GET(self, key):
        page = web.ctx.site.get(key)
        
        if page is None or page.type.key != '/type/edition':
            raise web.notfound("")
        else:
            from infogami.utils import template
            try:
                result = template.typetemplate('marcxml')(page)
            except:
                raise web.notfound("")
            else:
                return delegate.RawText(result, content_type="application/marcxml+xml; charset=utf-8")

delegate.media_types['text/x-yaml'] = 'yml'
class _yaml(delegate.mode):
    name = "view"
    encoding = "yml"

    def GET(self, key):
        d = self.get_data(key)
        
        if web.input(text="false").text.lower() == "true":
            web.header('Content-Type', 'text/plain')
        else:
            web.header('Content-Type', 'text/x-yaml')
            
        raise web.ok(self.dump(d))
        
    def get_data(self, key):
        i = web.input(v=None)
        v = safeint(i.v, None)
        data = dict(key=key, revision=v)
        try:
            d = api.request('/get', data=data)
        except client.ClientException, e:
            if e.json:
                msg = self.dump(simplejson.loads(e.json))
            else:
                msg = e.message
            raise web.HTTPError(e.status, data=msg)
    
        return simplejson.loads(d)
        
    def dump(self, d):
        import yaml
        return yaml.safe_dump(d, indent=4, allow_unicode=True, default_flow_style=False)
        
    def load(self, data):
        import yaml
        return yaml.safe_load(data)

class _yaml_edit(_yaml):
    name = "edit"
    encoding = "yml"
    
    def is_admin(self):
        u = delegate.context.user
        return u and u.is_admin()
    
    def GET(self, key):
        # only allow admin users to edit yaml
        if not self.is_admin():
            return render.permission_denied(key, 'Permission Denied')
            
        try:
            d = self.get_data(key)
        except web.HTTPError, e:
            if web.ctx.status.lower() == "404 not found":
                d = {"key": key}
            else:
                raise
        return render.edit_yaml(key, self.dump(d))
        
    def POST(self, key):
        # only allow admin users to edit yaml
        if not self.is_admin():
            return render.permission_denied(key, 'Permission Denied')
            
        i = web.input(body='', _comment=None)
        
        if '_save' in i:
            d = self.load(i.body)
            p = web.ctx.site.new(key, d)
            try:
                p._save(i._comment)
            except (client.ClientException, ValidationException), e:            
                add_flash_message('error', str(e))
                return render.edit_yaml(key, i.body)                
            raise web.seeother(key + '.yml')
        elif '_preview' in i:
            add_flash_message('Preview not supported')
            return render.edit_yaml(key, i.body)
        else:
            add_flash_message('unknown action')
            return render.edit_yaml(key, i.body)        

def _get_user_root():
    user_root = infogami.config.get("infobase", {}).get("user_root", "/user")
    return web.rstrips(user_root, "/")

def _get_bots():
    bots = web.ctx.site.store.values(type="account", name="bot", value="true")
    user_root = _get_user_root()
    return [user_root + "/" + account['username'] for account in bots]

def _get_members_of_group(group_key):
    """Returns keys of all members of the group identifier by group_key.
    """
    usergroup = web.ctx.site.get(group_key) or {}
    return [m.key for m in usergroup.get("members", [])]

def can_write():
    """Any user with bot flag set can write.
    For backward-compatability, all admin users and people in api usergroup are also allowed to write.
    """
    user_key = delegate.context.user and delegate.context.user.key
    bots = _get_members_of_group("/usergroup/api") + _get_members_of_group("/usergroup/admin") + _get_bots()
    return user_key in bots
    
# overwrite the implementation of can_write in the infogami API plugin with this one.
api.can_write = can_write

class Forbidden(web.HTTPError):
    def __init__(self, msg=""):
        web.HTTPError.__init__(self, "403 Forbidden", {}, msg)

class BadRequest(web.HTTPError):
    def __init__(self, msg=""):
        web.HTTPError.__init__(self, "400 Bad Request", {}, msg)

class new:
    """API to create new author/edition/work/publisher/series.
    """
    def prepare_query(self, query):
        """Add key to query and returns the key.
        If query is a list multiple queries are returned.
        """
        if isinstance(query, list):
            return [self.prepare_query(q) for q in query]
        else:
            type = query['type']
            if isinstance(type, dict):
                type = type['key']
            query['key'] = web.ctx.site.new_key(type)
            return query['key']
            
    def verify_types(self, query):
        if isinstance(query, list):
            for q in query:
                self.verify_types(q)
        else:
            if 'type' not in query:
                raise BadRequest("Missing type")
            type = query['type']
            if isinstance(type, dict):
                if 'key' not in type:
                    raise BadRequest("Bad Type: " + simplejson.dumps(type))
                type = type['key']
                
            if type not in ['/type/author', '/type/edition', '/type/work', '/type/series', '/type/publisher']:
                raise BadRequest("Bad Type: " + simplejson.dumps(type))
 
    def POST(self):
        if not can_write():
            raise Forbidden("Permission Denied.")
            
        try:
            query = simplejson.loads(web.data())
            h = api.get_custom_headers()
            comment = h.get('comment')
            action = h.get('action')
        except Exception, e:
            raise BadRequest(str(e))
            
        self.verify_types(query)
        keys = self.prepare_query(query)
        
        try:
            if not isinstance(query, list):
                query = [query]
            web.ctx.site.save_many(query, comment=comment, action=action)
        except client.ClientException, e:
            raise BadRequest(str(e))

        #graphite/statsd tracking of bot edits
        user = delegate.context.user and delegate.context.user.key
        if user.lower().endswith('bot'):
            botname = user.replace('/people/', '', 1)
            botname = botname.replace('.', '-')
            key = 'ol.edits.bots.'+botname
            openlibrary.core.stats.increment(key)

        return simplejson.dumps(keys)
        
api and api.add_hook('new', new)

@public
def changequery(query=None, **kw):
    if query is None:
        query = web.input(_method='get', _unicode=False)
    for k, v in kw.iteritems():
        if v is None:
            query.pop(k, None)
        else:
            query[k] = v

    query = dict((k, (map(web.safestr, v) if isinstance(v, list) else web.safestr(v))) for k, v in query.items())
    out = web.ctx.get('readable_path', web.ctx.path)
    if query:
        out += '?' + urllib.urlencode(query, doseq=True)
    return out

# Hack to limit recent changes offset.
# Large offsets are blowing up the database.

from infogami.core.db import get_recent_changes as _get_recentchanges
@public
def get_recent_changes(*a, **kw):
    if 'offset' in kw and kw['offset'] > 5000:
        return []
    else:
        return _get_recentchanges(*a, **kw)

@public
def most_recent_change():
    if 'cache_most_recent' in infogami.config.features:
        v = web.ctx.site._request('/most_recent')
        v.thing = web.ctx.site.get(v.key)
        v.author = v.author and web.ctx.site.get(v.author)
        v.created = client.parse_datetime(v.created)
        return v
    else:
        return get_recent_changes(limit=1)[0]
    
def wget(url):
    try:
        return urllib.urlopen(url).read()
    except:
        return ""
    
@public
def get_cover_id(key):
    try:
        _, cat, oln = key.split('/')
        return simplejson.loads(wget('http://covers.openlibrary.org/%s/query?olid=%s&limit=1' % (cat, oln)))[0]
    except (ValueError, IndexError, TypeError):
        return None

local_ip = None
class invalidate(delegate.page):
    path = "/system/invalidate"
    def POST(self):
        global local_ip
        if local_ip is None:
            local_ip = socket.gethostbyname(socket.gethostname())

        if web.ctx.ip != "127.0.0.1" and web.ctx.ip.rsplit(".", 1)[0] != local_ip.rsplit(".", 1)[0]:
            raise Forbidden("Allowed only in the local network.")

        data = simplejson.loads(web.data())
        if not isinstance(data, list):
            data = [data]
        for d in data:
            thing = client.Thing(web.ctx.site, d['key'], client.storify(d))
            client._run_hooks('on_new_version', thing)
        return delegate.RawText("ok")
            
def save_error():
    t = datetime.datetime.utcnow()
    name = '%04d-%02d-%02d/%02d%02d%02d%06d' % (t.year, t.month, t.day, t.hour, t.minute, t.second, t.microsecond)
    
    path = infogami.config.get('errorlog', 'errors') + '/'+ name + '.html'
    dir = os.path.dirname(path)
    if not os.path.exists(dir):
        os.makedirs(dir)
    
    error = web.safestr(web.djangoerror())
    f = open(path, 'w')
    f.write(error)
    f.close()
    
    print >> web.debug, 'error saved to', path
    
    return name

def internalerror():
    i = web.input(_method='GET', debug='false')
    name = save_error()
    
    openlibrary.core.stats.increment('ol.internal-errors', 1)

    if i.debug.lower() == 'true':
        raise web.debugerror()
    else:
        msg = render.site(render.internalerror(name))
        raise web.internalerror(web.safestr(msg))
    
delegate.app.internalerror = internalerror
delegate.add_exception_hook(save_error)

class memory(delegate.page):
    path = "/debug/memory"

    def GET(self):
        import guppy
        h = guppy.hpy()
        return delegate.RawText(str(h.heap()))

class backdoor(delegate.page):
    path = "/debug/backdoor"
    
    def GET(self):
        import backdoor
        reload(backdoor)
        result = backdoor.inspect()
        if isinstance(result, basestring):
            result = delegate.RawText(result)
        return result

def setup_template_globals():
    web.template.Template.globals.update({
        "sorted": sorted,
        "zip": zip,
        "tuple": tuple,
        "isbn_13_to_isbn_10": isbn_13_to_isbn_10,
        "NEWLINE": "\n",
        "random": random.Random(),
        
        # bad use of globals
        "time": time,
        "input": web.input,
        "dumps": simplejson.dumps,
    })


def setup_context_defaults():
    from infogami.utils import context
    context.defaults.update({
        'features': [],
        'user': None
    })

def setup():
    import home, inlibrary, borrow_home, libraries, stats, support, events, status, merge_editions, authors
    
    home.setup()
    inlibrary.setup()
    borrow_home.setup()
    libraries.setup()
    stats.setup()
    support.setup()
    events.setup()
    status.setup()
    merge_editions.setup()
    authors.setup()
    
    import api
    api.setup()
    
    from stats import stats_hook
    delegate.app.add_processor(web.unloadhook(stats_hook))
    
    if infogami.config.get("dev_instance") is True:
        import dev_instance
        dev_instance.setup()

    setup_context_defaults()
    setup_template_globals()
    
setup()

########NEW FILE########
__FILENAME__ = connection
"""Open Library extension to provide a new kind of client connection with caching support.
"""
from infogami import config
from infogami.infobase import client, lru
from infogami.utils import stats

import web
import simplejson
import datetime

from openlibrary.core import ia

import logging

logger = logging.getLogger("openlibrary")

class ConnectionMiddleware:
    response_type = "json"
    
    def __init__(self, conn):
        self.conn = conn
        
    def get_auth_token(self):
        return self.conn.get_auth_token()

    def set_auth_token(self, token):
        self.conn.set_auth_token(token)

    def request(self, sitename, path, method='GET', data=None):
        if path == '/get':
            return self.get(sitename, data)
        elif path == '/get_many':
            return self.get_many(sitename, data)
        elif path == '/versions':
            return self.versions(sitename, data)
        elif path == '/_recentchanges':
            return self.recentchanges(sitename, data)
        elif path == '/things':
            return self.things(sitename, data)
        elif path == '/write':
            return self.write(sitename, data)
        elif path.startswith('/save/'):
            return self.save(sitename, path, data)
        elif path == '/save_many':
            return self.save_many(sitename, data)
        elif path.startswith("/_store/") and not path.startswith("/_store/_"):
            if method == 'GET':
                return self.store_get(sitename, path)
            elif method == 'PUT':
                return self.store_put(sitename, path, data)
            elif method == 'DELETE':
                return self.store_delete(sitename, path, data)
        elif path == "/_store/_save_many" and method == 'POST':
            # save multiple things at once
            return self.store_put_many(sitename, data)
        elif path.startswith("/account"):
            return self.account_request(sitename, path, method, data)
                
        return self.conn.request(sitename, path, method, data)
        
    def account_request(self, sitename, path, method="GET", data=None):
        return self.conn.request(sitename, path, method, data)

    def get(self, sitename, data):
        return self.conn.request(sitename, '/get', 'GET', data)

    def get_many(self, sitename, data):
        return self.conn.request(sitename, '/get_many', 'GET', data)

    def versions(self, sitename, data):
        return self.conn.request(sitename, '/versions', 'GET', data)

    def recentchanges(self, sitename, data):
        return self.conn.request(sitename, '/_recentchanges', 'GET', data)

    def things(self, sitename, data):
        return self.conn.request(sitename, '/things', 'GET', data)

    def write(self, sitename, data):
        return self.conn.request(sitename, '/write', 'POST', data)

    def save(self, sitename, path, data):
        return self.conn.request(sitename, path, 'POST', data)

    def save_many(self, sitename, data):
        return self.conn.request(sitename, '/save_many', 'POST', data)
        
    def store_get(self, sitename, path):
        return self.conn.request(sitename, path, 'GET')
        
    def store_put(self, sitename, path, data):
        return self.conn.request(sitename, path, 'PUT', data)

    def store_put_many(self, sitename, data):
        return self.conn.request(sitename, "/_store/_save_many", 'POST', data)
    
    def store_delete(self, sitename, path, data):
        return self.conn.request(sitename, path, 'DELETE', data)
        
_memcache = None

class IAMiddleware(ConnectionMiddleware):

    def _get_itemid(self, key):
        """Returns internet archive item id from the key.

        If the key is of the form "/books/ia:.*", the part ofter "/books/ia:"
        is returned, otherwise None is returned.
        """
        if key and key.startswith("/books/ia:") and key.count("/") == 2:
            return key[len("/books/ia:"):]

    def get(self, sitename, data):
        key = data.get('key')

        itemid = self._get_itemid(key)
        if itemid:
            edition_key = self._find_edition(sitename, itemid)
            if edition_key:
                return self._make_redirect(itemid, edition_key)
            else:
                doc = self._get_ia_item(itemid)

                if doc is None:
                    # Delete store entry, if it exists.
                    # When an item is darked on archive.org, it should be
                    # automatically removed from OL. Removing entry from store
                    # will trigger the solr-updater to delete it from solr as well.
                    self._ensure_no_store_entry(sitename, itemid)

                    raise client.ClientException(
                        "404 Not Found", "notfound", 
                        simplejson.dumps({"key": "/books/ia:" + itemid, "error": "notfound"}))

                storedoc = self._ensure_store_entry(sitename, itemid)

                # Hack to add additional subjects /books/ia: pages
                # Adding subjects to store docs, will add thise subjects to the books.
                # These subjects are used when indexing the books in solr.
                if storedoc.get("subjects"):
                    doc.setdefault("subjects", []).extend(storedoc['subjects'])
                return simplejson.dumps(doc)
        else:
            return ConnectionMiddleware.get(self, sitename, data)

    def _find_edition(self, sitename, itemid):
        q = {"type": "/type/edition", "ocaid": itemid}
        keys_json = ConnectionMiddleware.things(self, sitename, {"query": simplejson.dumps(q)})
        keys = simplejson.loads(keys_json)
        if keys:
            return keys[0]

    def _make_redirect(self, itemid, location):
        timestamp = {"type": "/type/datetime", "value": "2010-01-01T00:00:00"}
        d = {
            "key": "/books/ia:" +  itemid,
            "type": {"key": "/type/redirect"}, 
            "location": location,
            "revision": 1,
            "created": timestamp,
            "last_modified": timestamp
        }
        return simplejson.dumps(d)

    def _is_valid_item(self, itemid, metadata):
        # Not a book, or scan not complete or no images uploaded
        if metadata.get("mediatype") != "texts" or metadata.get("repub_state", "4") not in ["4", "6"] or "imagecount" not in metadata:
            return False

        # items start with these prefixes are not books
        ignore_prefixes = config.get("ia_ignore_prefixes", [])
        for prefix in ignore_prefixes:
            # ignore all JSTOR items
            if itemid.startswith(prefix):
                return False

        # Anand - Oct 2013
        # If an item is with noindex=true and it is not marked as lending or printdisabled, ignore it.
        # It would have been marked as noindex=true for some reason.
        collections = metadata.get("collection", [])
        if not isinstance(collections, list):
            collections = [collections]
        if metadata.get("noindex") == "true" \
            and "printdisabled" not in collections \
            and "inlibrary" not in collections \
            and "lendinglibrary" not in collections:
            return

        return True

    def _get_ia_item(self, itemid):
        timestamp = {"type": "/type/datetime", "value": "2010-01-01T00:00:00"}
        metadata = ia.get_metadata(itemid)

        if not self._is_valid_item(itemid, metadata):
            return None

        d = {   
            "key": "/books/ia:" + itemid,
            "type": {"key": "/type/edition"}, 
            "title": itemid, 
            "ocaid": itemid,
            "revision": 1,
            "created": timestamp,
            "last_modified": timestamp
        }

        def add(key, key2=None):
            key2 = key2 or key
            # sometimes the empty values are represneted as {} in metadata API. Avoid them.
            if key in metadata and metadata[key] != {}:
                value = metadata[key]
                if isinstance(value, list):
                    value = [v for v in value if v != {}]
                    if value:
                        if isinstance(value[0], basestring):
                            value = "\n\n".join(value)
                        else:
                            value = value[0]
                    else:
                        # empty list. Ignore.
                        return

                d[key2] = value

        def add_list(key, key2):
            key2 = key2 or key
            # sometimes the empty values are represneted as {} in metadata API. Avoid them.
            if key in metadata and metadata[key] != {}:
                value = metadata[key]
                if not isinstance(value, list):
                    value = [value]
                d[key2] = value

        def add_isbns():
            isbns = metadata.get('isbn')
            isbn_10 = []
            isbn_13 = []
            if isbns:
                for isbn in isbns:
                    isbn = isbn.replace("-", "").strip()
                    if len(isbn) == 13:
                        isbn_13.append(isbn)
                    elif len(isbn) == 10:
                        isbn_10.append(isbn)
            if isbn_10:
                d["isbn_10"] = isbn_10
            if isbn_13: 
                d["isbn_13"] = isbn_13

        def add_subjects():
            collections = metadata.get("collection", [])
            mapping = {
                "inlibrary": "In library",
                "lendinglibrary": "Lending library"
            }
            subjects = [subject for c, subject in mapping.items() if c in collections]
            if subjects:
                d['subjects'] = subjects

        add('title')
        add('description', 'description')
        add_list('publisher', 'publishers')
        add_list("creator", "author_names")
        add('date', 'publish_date')

        add_isbns()
        add_subjects()
        
        return d

    def _ensure_no_store_entry(self, sitename, identifier):
        key = "ia-scan/" + identifier
        store_key = "/_store/" + key
        # If the entry is not found, create an entry
        try:
            jsontext = self.store_get(sitename, store_key)
            self.store_delete(sitename, store_key, {"_rev": None})
        except client.ClientException, e:
            # nothing to do if that doesn't exist
            pass

    def _ensure_store_entry(self, sitename, identifier):
        key = "ia-scan/" + identifier
        store_key = "/_store/" + key
        # If the entry is not found, create an entry
        try:
            jsontext = self.store_get(sitename, store_key)
            return simplejson.loads(jsontext)
        except client.ClientException, e:
            logger.error("error", exc_info=True)            
            if e.status.startswith("404"):
                doc = {
                    "_key": key,
                    "type": "ia-scan",
                    "identifier": identifier,
                    "created": datetime.datetime.utcnow().isoformat()
                }
                self.store_put(sitename, store_key, simplejson.dumps(doc))
                return doc
        except:
            logger.error("error", exc_info=True)

    def versions(self, sitename, data):
        # handle the query of type {"query": '{"key": "/books/ia:foo00bar", ...}}
        if 'query' in data:
            q = simplejson.loads(data['query'])
            itemid = self._get_itemid(q.get('key'))
            if itemid:
                key = q['key']
                return simplejson.dumps([self.dummy_edit(key)])

        # if not just go the default way
        return ConnectionMiddleware.versions(self, sitename, data)

    def recentchanges(self, sitename, data):
        # handle the query of type {"query": '{"key": "/books/ia:foo00bar", ...}}
        if 'query' in data:
            q = simplejson.loads(data['query'])
            itemid = self._get_itemid(q.get('key'))
            if itemid:
                key = q['key']
                return simplejson.dumps([self.dummy_recentchange(key)])

        # if not just go the default way
        return ConnectionMiddleware.recentchanges(self, sitename, data)

    def dummy_edit(self, key):
        return {
            "comment": "", 
            "author": None, 
            "ip": "127.0.0.1", 
            "created": "2012-01-01T00:00:00", 
            "bot": False, 
            "key": key, 
            "action": "edit-book", 
            "changes": simplejson.dumps({"key": key, "revision": 1}),
            "revision": 1,

            "kind": "update",
            "id": "0",
            "timestamp": "2010-01-01T00:00:00",
            "data": {}
        }

    def dummy_recentchange(self, key):
        return {
            "comment": "", 
            "author": None, 
            "ip": "127.0.0.1", 
            "timestamp": "2012-01-01T00:00:00", 
            "data": {}, 
            "changes": [{"key": key, "revision": 1}],
            "kind": "update",
            "id": "0",
        }
        
class MemcacheMiddleware(ConnectionMiddleware):
    def __init__(self, conn, memcache_servers):
        ConnectionMiddleware.__init__(self, conn)
        self.memcache = self.get_memcache(memcache_servers)
        
    def get_memcache(self, memcache_servers):
        global _memcache
        if _memcache is None:
            from openlibrary.utils import olmemcache
            _memcache = olmemcache.Client(memcache_servers)
        return _memcache

    def get(self, sitename, data):
        key = data.get('key')
        revision = data.get('revision')
                
        if revision is None:
            stats.begin("memcache.get", key=key)
            result = self.memcache.get(key)
            stats.end(hit=bool(result))
            
            return result or ConnectionMiddleware.get(self, sitename, data)
        else:
            # cache get requests with revisions for a minute
            mc_key = "%s@%d" % (key, revision)
            result = self.mc_get(mc_key)
            if result is None:
                result = ConnectionMiddleware.get(self, sitename, data)
                if result:
                    self.mc_set(mc_key, result, time=60) # cache for a minute
            return result
    
    def get_many(self, sitename, data):
        keys = simplejson.loads(data['keys'])
        
        stats.begin("memcache.get_multi")
        result = self.memcache.get_multi(keys)
        stats.end(found=len(result))
        
        keys2 = [k for k in keys if k not in result]
        if keys2:
            data['keys'] = simplejson.dumps(keys2)
            result2 = ConnectionMiddleware.get_many(self, sitename, data)
            result2 = simplejson.loads(result2)

            # Memcache expects dict with (key, json) mapping and we have (key, doc) mapping.
            # Converting the docs to json before passing to memcache.
            self.mc_set_multi(dict((key, simplejson.dumps(doc)) for key, doc in result2.items()))

            result.update(result2)
        
        #@@ too many JSON conversions
        for k in result:
            if isinstance(result[k], basestring):
                result[k] = simplejson.loads(result[k])
                
        return simplejson.dumps(result)

    def mc_get(self, key):
        stats.begin("memcache.get", key=key)
        result = self.memcache.get(key)
        stats.end(hit=bool(result))
        return result
    
    def mc_delete(self, key):
        stats.begin("memcache.delete", key=key)
        self.memcache.delete(key)
        stats.end()
        
    def mc_add(self, key, value):
        stats.begin("memcache.add", key=key)
        self.memcache.add(key, value)
        stats.end()
        
    def mc_set(self, key, value, time=0):
        stats.begin("memcache.set", key=key)
        self.memcache.add(key, value, time=time)
        stats.end()
    
    def mc_set_multi(self, mapping):
        stats.begin("memcache.set_multi")
        self.memcache.set_multi(mapping)
        stats.end()

    def mc_delete_multi(self, keys):
        stats.begin("memcache.delete_multi")
        self.memcache.delete_multi(keys)
        stats.end()

    def store_get(self, sitename, path):
        # path will be "/_store/$key"
        result = self.mc_get(path)

        if result is None:
            result = ConnectionMiddleware.store_get(self, sitename, path)
            if result:
                self.mc_add(path, result)
        return result

    def store_put(self, sitename, path, data):
        # path will be "/_store/$key"

        # deleting before put to make sure the entry is deleted even if the
        # process dies immediately after put.
        # Still there is very very small chance of invalid cache if someone else
        # updates memcache after stmt-1 and this process dies after stmt-2.
        self.mc_delete(path)
        result = ConnectionMiddleware.store_put(self, sitename, path, data)
        self.mc_delete(path)
        return result

    def store_put_many(self, sitename, datajson):
        data = simplejson.loads(datajson)
        mc_keys = ["/_store/" + doc['_key'] for doc in data]
        self.mc_delete_multi(mc_keys)
        result = ConnectionMiddleware.store_put_many(self, sitename, datajson)
        self.mc_delete_multi(mc_keys)
        return result

    def store_delete(self, sitename, key, data):
        # see comment in store_put
        self.mc_delete(key)
        result = ConnectionMiddleware.store_delete(self, sitename, key, data)
        self.mc_delete(key)
        return result
        
    def account_request(self, sitename, path, method="GET", data=None):
        # For post requests, remove the account entry from the cache.
        if method == "POST" and isinstance(data, dict) and "username" in data:
            self.mc_delete("/_store/account/" + data["username"])
            result = ConnectionMiddleware.account_request(self, sitename, path, method, data)
            self.mc_delete("/_store/account/" + data["username"])
        else:
            result = ConnectionMiddleware.account_request(self, sitename, path, method, data)
        return result

class MigrationMiddleware(ConnectionMiddleware):
    """Temporary middleware to handle upstream to www migration."""
    def _process_key(self, key):
        mapping = (
            "/l/", "/languages/",
            "/a/", "/authors/",
            "/b/", "/books/",
            "/user/", "/people/"
        )
        
        if "/" in key and key.split("/")[1] in ['a', 'b', 'l', 'user']:
            for old, new in web.group(mapping, 2):
                if key.startswith(old):
                    return new + key[len(old):]
        return key
    
    def exists(self, key):
        try:
            d = ConnectionMiddleware.get(self, "openlibrary.org", {"key": key})
            return True
        except client.ClientException, e:
            return False
    
    def _process(self, data):
        if isinstance(data, list):
            return [self._process(d) for d in data]
        elif isinstance(data, dict):
            if 'key' in data:
                data['key'] = self._process_key(data['key'])
            return dict((k, self._process(v)) for k, v in data.iteritems())
        else:
            return data
    
    def get(self, sitename, data):
        if web.ctx.get('path') == "/api/get" and 'key' in data:
            data['key'] = self._process_key(data['key'])
            
        response = ConnectionMiddleware.get(self, sitename, data)
        if response:
            data = simplejson.loads(response)
            data = self._process(data)
            data = data and self.fix_doc(data)
            response = simplejson.dumps(data)
        return response
        
    def fix_doc(self, doc):
        type = doc.get("type", {}).get("key") 
        
        if type == "/type/work":
            if doc.get("authors"):
                # some record got empty author records because of an error
                # temporary hack to fix 
                doc['authors'] = [a for a in doc['authors'] if 'author' in a and 'key' in a['author']]
        elif type == "/type/edition":
            # get rid of title_prefix.
            if 'title_prefix' in doc:
                title = doc['title_prefix'].strip() + ' ' + doc.get('title', '')
                doc['title'] = title.strip()
                del doc['title_prefix']

        return doc
        
    def fix_broken_redirect(self, key):
        """Some work/edition records references to redirected author records
        and that is making save fail.

        This is a hack to work-around that isse.
        """
        json = self.get("openlibrary.org", {"key": key})
        if json:
            doc = simplejson.loads(json)
            if doc.get("type", {}).get("key") == "/type/redirect" and doc.get('location') is not None:
                return doc['location']
        return key

    def get_many(self, sitename, data):
        response = ConnectionMiddleware.get_many(self, sitename, data)
        if response:
            data = simplejson.loads(response)
            data = self._process(data)
            data = dict((key, self.fix_doc(doc)) for key, doc in data.items())
            response = simplejson.dumps(data)
        return response
        
class HybridConnection(client.Connection):
    """Infobase connection made of both local and remote connections. 
    
    The local connection is used for reads and the remote connection is used for writes.
    
    Some services in the OL infrastructure depends of the log written by the
    writer, so remote connection is used, which takes care of writing logs. By
    using a local connection for reads improves the performance by cutting
    down the overhead of http calls present in case of remote connections.
    """
    def __init__(self, reader, writer):
        client.Connection.__init__(self)
        self.reader = reader
        self.writer = writer
        
    def set_auth_token(self, token):
        self.reader.set_auth_token(token)
        self.writer.set_auth_token(token)
    
    def get_auth_token(self):
        return self.writer.get_auth_token()
        
    def request(self, sitename, path, method="GET", data=None):
        if method == "GET":
            return self.reader.request(sitename, path, method, data=data)
        else:
            return self.writer.request(sitename, path, method, data=data)

@web.memoize
def _update_infobase_config():
    """Updates infobase config when this function is called for the first time.
    
    From next time onwards, it doens't do anything becase this function is memoized.
    """
    # update infobase configuration
    from infogami.infobase import server
    if not config.get("infobase"):
        config.infobase = {}
    # This sets web.config.db_parameters
    server.update_config(config.infobase)
            
def create_local_connection():
    _update_infobase_config()
    return client.connect(type='local', **web.config.db_parameters)
    
def create_remote_connection():
    return client.connect(type='remote', base_url=config.infobase_server)
    
def create_hybrid_connection():
    local = create_local_connection()
    remote = create_remote_connection()
    return HybridConnection(local, remote)

def OLConnection():
    """Create a connection to Open Library infobase server."""
    def create_connection():
        if config.get("connection_type") == "hybrid":
            return create_hybrid_connection()
        elif config.get('infobase_server'):
            return create_remote_connection()
        elif config.get("infobase", {}).get('db_parameters'):
            return create_local_connection()
        else:
            raise Exception("db_parameters are not specified in the configuration")

    conn = create_connection()
    if config.get('memcache_servers'):
        conn = MemcacheMiddleware(conn, config.get('memcache_servers'))
    
    if config.get('upstream_to_www_migration'):
        conn = MigrationMiddleware(conn)

    conn = IAMiddleware(conn)
    return conn


########NEW FILE########
__FILENAME__ = dev_instance
"""Special customizations for dev instance.

This module is imported only if dev_instance is set to True in openlibrary config.
"""
import web
import infogami
from infogami.utils import delegate
from openlibrary.core.task import oltask
from openlibrary.tasks import other_on_edit_tasks

def setup():
    setup_solr_updater()
    
    # Run update_solr on every edit
    other_on_edit_tasks.append(update_solr)
        
    from openlibrary.catalog.utils import query
    # monkey-patch query to make solr-updater work with-in the process instead of making http requests.
    query.query = ol_query
    query.withKey = ol_get
    
    infogami.config.middleware.append(CoverstoreMiddleware)
    
    # Borrow code tries to find the loan-status by making a URL call
    from openlibrary.plugins.upstream import borrow
    borrow.get_loan_status = lambda resource_id: []
    
class CoverstoreMiddleware:
    """Middleware to delegate all /cover/* requests to coverstore.
    
    This avoids starting a new service for coverstore. 
    Assumes that coverstore config is at conf/coverstore.yml
    """
    def __init__(self, app):
        self.app = app
        
        from openlibrary.coverstore import code, server
        server.load_config("conf/coverstore.yml")
        self.coverstore_app = code.app.wsgifunc()
        
    def __call__(self, environ, start_response):
        root = "/covers"
        if environ['PATH_INFO'].startswith(root):
            environ['PATH_INFO'] = environ['PATH_INFO'][len(root):]
            environ['SCRIPT_NAME'] = environ['SCRIPT_NAME'] + root
            return self.coverstore_app(environ, start_response)
        else:
            return self.app(environ, start_response)

def ol_query(q):
    return web.ctx.site.things(q, details=True)

def ol_get(key):
    d = web.ctx.site.get(key)
    return d and d.dict()

def setup_solr_updater():
    from infogami import config
    
    # solr-updater reads configuration from openlibrary.config.runtime_config
    from openlibrary import config as olconfig
    olconfig.runtime_config = config.__dict__
    
    # The solr-updater makes a http call to the website insted of using the 
    # infobase API. It requires setting the host before start using it.
    from openlibrary.catalog.utils.query import set_query_host
    
    dev_instance_url = config.get("dev_instance_url", "http://127.0.0.1:8080/")
    host = web.lstrips(dev_instance_url, "http://").strip("/")
    set_query_host(host)

class is_loaned_out(delegate.page):
    path = "/is_loaned_out/.*"
    def GET(self):
        return delegate.RawText("[]", content_type="application/json")

class process_ebooks(delegate.page):
    """Hack to add ebooks to store so that books are visible in the returncart.
    """
    path = "/_dev/process_ebooks"
    
    def GET(self):
        from openlibrary.plugins.worksearch.search import get_works_solr
        result = get_works_solr().select(query='borrowed_b:false', fields=['key', 'lending_edition_s'], limit=100)
        
        def make_doc(d):
            # Makes a store doc from solr doc
            return {
                "_key": "ebooks/books/" + d['lending_edition_s'],
                "_rev": None, # Don't worry about consistancy
                "type": "ebook",
                "book_key": "/books/" + d['lending_edition_s'],
                "borrowed": "false"
            }
        
        docs = [make_doc(d) for d in result['docs']]
        docdict = dict((d['_key'], d) for d in docs)
        web.ctx.site.store.update(docdict)
        return delegate.RawText("ok\n")

@oltask
def update_solr(changeset):
    """Updates solr on edit.
    """
    from openlibrary.solr import update_work
    
    keys = set()
    docs = changeset['docs'] + changeset['old_docs']
    docs = [doc for doc in docs if doc] # doc can be None if it is newly created.
    for doc in docs:
        if doc['type']['key'] == '/type/edition':
            keys.update(w['key'] for w in doc.get('works', []))
        elif doc['type']['key'] == '/type/work':
            keys.add(doc['key'])
            keys.update(a['author']['key'] for a in doc.get('authors', []) if 'author' in a)
        elif doc['type']['key'] == '/type/author':
            keys.add(doc['key'])
            
    update_work.update_keys(list(keys))
    
@infogami.install_hook
def add_ol_user():
    """Creates openlibrary user with admin previleges.
    """
    # Create openlibrary user
    if web.ctx.site.get("/people/openlibrary") is None:
        web.ctx.site.register(
            username="openlibrary",
            email="openlibrary@example.com",
            password="openlibrary",
            displayname="Open Library")
        web.ctx.site.activate_account(username="openlibrary")
        
    if web.ctx.site.get("/usergroup/api") is None:
        g_api = web.ctx.site.new("/usergroup/api", {
            "key": "/usergroup/api",
            "type": "/type/usergroup",
            "members": [{"key": "/people/openlibrary"}]
        })
        g_api._save(comment="Added openlibrary user to API usergroup.")
        
    g_admin = web.ctx.site.get("/usergroup/admin").dict()
    g_admin.setdefault('members', [])
    members = [m['key'] for m in g_admin["members"]]
    if 'openlibrary' not in members:
        g_admin['members'].append({"key": "/people/openlibrary"})
        web.ctx.site.save(g_admin, "Added openlibrary user to admin usergroup.")

@infogami.action
def load_sample_data():
    """Action to load sample data.

    This was an experiment to load sample data as part of install. But it
    doesn't seem to be working well on linux dev-instance because of some weird
    supervisor log issues.

    This is unused as of now.
    """
    env = {}
    execfile("scripts/copydocs.py", env, env)
    src = env['OpenLibrary']()
    dest = web.ctx.site
    comment = "Loaded sample data."
    list_key = "/people/anand/lists/OL1815L"
    env['copy_list'](src, dest, list_key, comment=comment)

########NEW FILE########
__FILENAME__ = events
"""Handling various events triggered by Open Library.
"""
from openlibrary.core import inlibrary
from openlibrary import tasks
from infogami.infobase import client
import logging
import web

import eventer

logger = logging.getLogger("openlibrary.events")

def on_library_edit(page):
    logger.info("%s is edited. Clearing library cache.", page['key'])
    inlibrary._get_libraries_memoized(_cache="delete")
    
def on_page_edit(page):
    if page.key.startswith("/libraries/"):
        eventer.trigger("page.edit.libraries", page)
    
class EditHook(client.hook):
    """Ugly Interface proivided by Infobase to get event notifications.
    """
    def on_new_version(self, page):
        """Fires page.edit event using msg broker."""
        # The argument passes by Infobase is not a thing object. 
        # Create a thing object to pass to event listeners.
        page = web.ctx.site.get(page['key'])
        eventer.trigger("page.edit", page)

def setup():
    """Installs handlers for various events.
    """
    eventer.bind("page.edit.libraries", on_library_edit)
    eventer.bind("page.edit", on_page_edit)

########NEW FILE########
__FILENAME__ = filters
"""
Filters used to check if a certain statistic should be recorded
"""
import re
import logging
l = logging.getLogger("openlibrary.stats_filters")

import web
from infogami import config

def all(**params):
    "Returns true for all requests"
    return True

def url(**params):
    l.debug("Evaluate url '%s'"%web.ctx.path)
    if re.search(params['pattern'], web.ctx.path):
        return True
    else:
        return False
    
def loggedin(**kw):
    """Returns True if any user is logged in.
    """
    # Assuming that presence of cookie is an indication of logged-in user.
    # Avoiding validation or calling web.ctx.site.get_user() as they are expensive.
    return config.login_cookie_name in web.cookies()

def not_loggedin(**kw):
    """Returns True if no user is logged in.
    """
    return not loggedin()
########NEW FILE########
__FILENAME__ = home
"""Controller for home page.
"""
import random
import web
import logging

from infogami.utils import delegate
from infogami.utils.view import render_template, public
from infogami.infobase.client import storify
from infogami import config

from openlibrary.core import admin, cache, ia, inlibrary, helpers as h
from openlibrary.plugins.upstream.utils import get_blog_feeds
from openlibrary.plugins.worksearch import search

logger = logging.getLogger("openlibrary.home")

class home(delegate.page):
    path = "/"
    
    def is_enabled(self):
        return "lending_v2" in web.ctx.features
    
    def GET(self):
        try:
            if 'counts_db' in config.admin:
                stats = admin.get_stats()
            else:
                stats = None
        except Exception:
            logger.error("Error in getting stats", exc_info=True)
            stats = None
        blog_posts = get_blog_feeds()
        
        lending_list = config.get("home", {}).get("lending_list")
        returncart_list = config.get("home", {}).get("returncart_list")
        
        return render_template("home/index", 
            stats=stats,
            blog_posts=blog_posts,
            lending_list=lending_list,
            returncart_list=returncart_list)

@public
def carousel_from_list(key, randomize=False, limit=60):
    id = key.split("/")[-1] + "_carousel"
    
    data = format_list_editions(key)
    if randomize:
        random.shuffle(data)
    data = data[:limit]
    add_checkedout_status(data)
    return render_template("books/carousel", storify(data), id=id)
    
def add_checkedout_status(books):
    # This is not very efficient approach.
    # Todo: Implement the following apprach later.
    # * Store the borrow status of all books in the list in memcache
    # * Use that info to add checked_out status
    # * Invalidate that on any borrow/return
    for book in books:
        if book.get("borrow_url"):
            doc = web.ctx.site.store.get("ebooks" + book['key']) or {}
            checked_out = doc.get("borrowed") == "true"
        else:
            checked_out = False
        book['checked_out'] = checked_out

@public
def render_returncart(limit=60, randomize=True):
    data = get_returncart(limit*5)

    # Remove all inlibrary books if we not in a participating library
    if not inlibrary.get_library():
        data = [d for d in data if 'inlibrary_borrow_url' not in d]
    
    if randomize:
        random.shuffle(data)
    data = data[:limit]
    return render_template("books/carousel", storify(data), id="returncart_carousel")

def get_returncart(limit):
    if 'env' not in web.ctx:
        delegate.fakeload()
    
    items = web.ctx.site.store.items(type='ebook', name='borrowed', value='false', limit=limit)
    keys = [doc['book_key'] for k, doc in items if 'book_key' in doc]
    books = web.ctx.site.get_many(keys)
    return [format_book_data(book) for book in books if book.type.key == '/type/edition']
    
# cache the results of get_returncart in memcache for 60 sec
get_returncart = cache.memcache_memoize(get_returncart, "home.get_returncart", timeout=60)

@public
def readonline_carousel(id="read-carousel"):
    try:
        data = random_ebooks()
        if len(data) > 120:
            data = random.sample(data, 120)
        return render_template("books/carousel", storify(data), id=id)
    except Exception:
        logger.error("Failed to compute data for readonline_carousel", exc_info=True)
        return None
        
def random_ebooks(limit=2000):
    solr = search.get_works_solr()
    sort = "edition_count desc"
    result = solr.select(
        query='has_fulltext:true -public_scan_b:false', 
        rows=limit, 
        sort=sort,
        fields=[
            'has_fulltext',
            'key',
            'ia',
            "title",
            "cover_edition_key",
            "author_key", "author_name",
        ])

    def process_doc(doc):
        d = {}

        key = doc['key']
        # New solr stores the key as /works/OLxxxW
        if not key.startswith("/works/"):
            key = "/works/" + key

        d['url'] = key
        d['title'] = doc.get('title', '')
        
        if 'author_key' in doc and 'author_name' in doc:
            d['authors'] = [{"key": key, "name": name} for key, name in zip(doc['author_key'], doc['author_name'])]
            
        if 'cover_edition_key' in doc:
            d['cover_url'] = h.get_coverstore_url() + "/b/olid/%s-M.jpg" % doc['cover_edition_key']
            
        d['read_url'] = "//archive.org/stream/" + doc['ia'][0]
        return d
        
    return [process_doc(doc) for doc in result['docs'] if doc.get('ia')]

# cache the results of random_ebooks in memcache for 15 minutes
random_ebooks = cache.memcache_memoize(random_ebooks, "home.random_ebooks", timeout=15*60)

def format_list_editions(key):
    """Formats the editions of the list suitable for display in carousel.
    """
    if 'env' not in web.ctx:
        delegate.fakeload()
    
    list = web.ctx.site.get(key)
    if not list:
        return []
    
    editions = {}
    for seed in list.seeds:
        if not isinstance(seed, basestring):
            if seed.type.key == "/type/edition": 
                editions[seed.key] = seed
            else:
                try:
                    e = pick_best_edition(seed)
                except StopIteration:
                    continue
                editions[e.key] = e
    return [format_book_data(e) for e in editions.values()]
    
# cache the results of format_list_editions in memcache for 5 minutes
format_list_editions = cache.memcache_memoize(format_list_editions, "home.format_list_editions", timeout=5*60)

def pick_best_edition(work):
    return (e for e in work.editions if e.ocaid).next()

def format_book_data(book):
    d = web.storage()
    d.key = book.key
    d.url = book.url()
    d.title = book.title or None
    
    def get_authors(doc):
        return [web.storage(key=a.key, name=a.name or None) for a in doc.get_authors()]
        
    work = book.works and book.works[0]
    if work:
        d.authors = get_authors(work)
    else:
        d.authors = get_authors(book)

    cover = book.get_cover()
    if cover:
        d.cover_url = cover.url("M")
        
    overdrive = book.get("identifiers", {}).get('overdrive')
    if overdrive:
        d.overdrive_url = "http://search.overdrive.com/SearchResults.aspx?ReserveID={%s}" % overdrive

    ia_id = book.get("ocaid")
    if ia_id:
        collections = ia.get_meta_xml(ia_id).get("collection", [])
        if 'printdisabled' in collections or 'lendinglibrary' in collections:
            d.daisy_url = book.url("/daisy")
            
        if 'lendinglibrary' in collections:
            d.borrow_url = book.url("/borrow")
        elif 'inlibrary' in collections:
            d.inlibrary_borrow_url = book.url("/borrow")
        else:
            d.read_url = book.url("/borrow")
    return d

def setup():
    pass

########NEW FILE########
__FILENAME__ = infobase_hook
"""Infobase hook for openlibrary.

    * Log all modified book pages as required for the search engine.
"""

from infogami.infobase import config
from infogami.infobase.logger import Logger

import datetime

root = getattr(config, 'booklogroot', 'booklog')

_logger = Logger(root)

def hook(object):
    """
    Add this hook to infobase.hooks to log all book modifications.
    """
    site = object._site
    timestamp = datetime.datetime.utcnow()
    if object.type.key == '/type/edition':
        d = object._get_data(expand=True)
        # save some space by not expanding type
        d['type'] = {'key': '/type/edition'}
        _logger.write('book', site.name, timestamp, d)

    #TODO: take care of author modifications


########NEW FILE########
__FILENAME__ = inlibrary
"""Hooks for in-library lending.
"""
import web
from infogami.utils import features
from openlibrary.core import inlibrary

def setup():
    web.template.Template.globals.update({
        "get_library": inlibrary.get_library,
        "get_libraries": inlibrary.get_libraries
    })
    
    features.register_filter("inlibrary", inlibrary.filter_inlibrary)
########NEW FILE########
__FILENAME__ = libraries
"""Controller for /libraries.
"""
import time
import logging
import datetime
import itertools
from cStringIO import StringIO
import csv

import web
import couchdb

from infogami import config
from infogami.utils import delegate
from infogami.utils.view import render_template, add_flash_message, public
from openlibrary.core import inlibrary, statsdb, geo_ip
from openlibrary import accounts
from openlibrary.core.iprange import find_bad_ip_ranges

logger = logging.getLogger("openlibrary.libraries")

class libraries(delegate.page):
    def GET(self):
        return render_template("libraries/index", get_libraries_by_country())

    def get_branches(self):
        branches = sorted(get_library_branches(), key=lambda b: b.name.upper())
        return itertools.groupby(branches, lambda b: b.name and b.name[0])

class libraries_notes(delegate.page):
    path = "(/libraries/[^/]+)/notes"

    def POST(self, key):
        doc = web.ctx.site.get(key)
        if doc is None or doc.type.key != "/type/library":
            raise web.notfound()
        elif not web.ctx.site.can_write(key):
            raise render_template("permission_denied")
        else:
            i = web.input(note="")

            user = accounts.get_current_user()
            author = user and {"key": user.key}
            timestamp = {"type": "/type/datetime", "value": datetime.datetime.utcnow().isoformat()}

            note = {"note": i.note, "author": {"key": user.key}, "timestamp": timestamp}

            if not doc.notes:
                doc.notes = []
            doc.notes.append(note)
        doc._save(comment="Added a note.")
        raise web.seeother(key)

def get_library_branches():
    """Returns library branches grouped by first letter."""
    libraries = inlibrary.get_libraries()
    for lib in libraries:
        for branch in lib.get_branches():
            branch.library = lib.name
            yield branch

#source: https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations
US_STATE_CODES = """
AL  01  Alabama
AK  02  Alaska
AZ  04  Arizona
AR  05  Arkansas
CA  06  California
CO  08  Colorado
CT  09  Connecticut
DE  10  Delaware
FL  12  Florida
GA  13  Georgia
HI  15  Hawaii
ID  16  Idaho
IL  17  Illinois
IN  18  Indiana
IA  19  Iowa
KS  20  Kansas
KY  21  Kentucky
A   22  Louisiana
ME  23  Maine
MD  24  Maryland
MA  25  Massachusetts
MI  26  Michigan
MN  27  Minnesota
MS  28  Mississippi
MO  29  Missouri
MT  30  Montana
NE  31  Nebraska
NV  32  Nevada
NH  33  New Hampshire
NJ  34  New Jersey
NM  35  New Mexico
NY  36  New York
NC  37  North Carolina
ND  38  North Dakota
OH  39  Ohio
OK  40  Oklahoma
OR  41  Oregon
PA  42  Pennsylvania
RI  44  Rhode Island
SC  45  South Carolina
SD  46  South Dakota
TN  47  Tennessee
TX  48  Texas
UT  49  Utah
VT  50  Vermont
VA  51  Virginia
WA  53  Washington
WV  54  West Virginia
WI  55  Wisconsin
WY  56  Wyoming
"""
def parse_state_codes():
    tokens = (line.split(None, 2) for line in US_STATE_CODES.strip().splitlines())
    return dict((code, name) for code, _, name in tokens)

US_STATE_CODES_DICT = parse_state_codes()

@public
def group_branches_by_state(branches):
    d = {}
    for branch in branches:
        state = branch.state.strip()
        if state.upper() in US_STATE_CODES_DICT:
            state = US_STATE_CODES_DICT[state.upper()]
        d.setdefault(state, []).append(branch)
    return d

def get_libraries_by_country():
    libraries = inlibrary.get_libraries()
    d = {}
    
    usa = "United States of America"
    aliases = {
        "US": usa,
        "U.S.": usa,
        "USA": usa,
        "U.S.A.": usa,
        "United States": usa,
        "UK": "United Kingdom"
    }
    for lib in libraries:
        for branch in lib.get_branches():
            country = aliases.get(branch.country.strip(), branch.country).strip()
            branch.library = lib
            d.setdefault(country, []).append(branch)
    return d

class libraries_dashboard(delegate.page):
    path = "/libraries/dashboard"

    def GET(self):
        keys = web.ctx.site.things(query={"type": "/type/library", "limit": 10000})
        libraries = web.ctx.site.get_many(keys)
        libraries.sort(key=lambda lib: lib.name.lstrip('The '))
        return render_template("libraries/dashboard", libraries, self.get_pending_libraries())

    def get_pending_libraries(self):
        docs = web.ctx.site.store.values(type="library", name="current_status", value="pending", limit=10000)
        return [self._create_pending_library(doc) for doc in docs]

    def _create_pending_library(self, doc):
        """Creates a library object from store doc.
        """
        doc = dict(doc)

        key = doc.pop("_key")
        if not key.startswith("/"):
            key = "/" + key

        for k in doc.keys():
            if k.startswith("_"):
                del doc[k]

        doc['key'] = key
        doc['type'] = {"key": '/type/library'}
        doc['title'] = doc.get("title", doc['name'])
        return web.ctx.site.new(key, doc)

class pending_libraries(delegate.page):
    path = "/(libraries/pending-\d+)"

    def GET(self, key):
        doc = web.ctx.site.store.get(key)
        if not doc:
            raise web.notfound()

        doc["_key"] = self.generate_key(doc)

        page = libraries_dashboard()._create_pending_library(doc)
        return render_template("type/library/edit", page)

    def generate_key(self, doc):
        key = "/libraries/" + doc['name'].lower().replace(" ", "_")

        _key = key
        count = 1
        while web.ctx.site.get(key) is not None:
            key = "%s_%s" % (_key, count)
            count += 1
        return key

    def POST(self, key):
        i = web.input(_method="POST")

        if "_delete" in i:
            doc = web.ctx.site.store.get(key)
            if doc:
                doc['current_status'] = "deleted"
                web.ctx.site.store[doc['_key']] = doc
                add_flash_message("info", "The requested library has been deleted.")
                raise web.seeother("/libraries/dashboard")

        i._key = web.rstrips(i.key, "/").replace(" ", "_")
        page = libraries_dashboard()._create_pending_library(i)

        if web.ctx.site.get(page.key):
            add_flash_message("error", "URL %s is already used. Please choose a different one." % page.key)
            return render_template("type/library/edit", page)
        elif not i.key.startswith("/libraries/"):
            add_flash_message("error", "The key must start with /libraries/.")
            return render_template("type/library/edit", page)

        doc = web.ctx.site.store.get(key)
        if doc and "registered_on" in doc:
            page.registered_on = {"type": "/type/datetime", "value": doc['registered_on']}

        page._save()

        if doc:
            doc['current_status'] = "approved"
            doc['page_key'] = page.key
            web.ctx.site.store[doc['_key']] = doc
        raise web.seeother(page.key)

class libraries_register(delegate.page):
    path = "/libraries/register"
    def GET(self):
        return render_template("libraries/add")

    def POST(self):
        i = web.input()
        doc = dict(i)
        errors = {}
        if not doc.get('name'):
            errors['name'] = 'name is a required field'
        addresses = doc.get('addresses', '').strip()
        if addresses:
            for line in addresses.splitlines():
                tokens = line.split('|')
                if len(tokens) != 9:
                    errors['addresses'] = 'address field is invalid'
                    break
                latlong = tokens[8]
                if ',' not in latlong or len(latlong.split(',')) != 2:
                    errors['addresses'] = 'Lat, Long is invalid'
                    break
        else:
            errors['addresses'] = 'addresses is a required field'

        ip_ranges = doc.get('ip_ranges', '').strip()
        if ip_ranges:
            bad = find_bad_ip_ranges(ip_ranges)
            if bad:
                errors['ip_ranges'] = 'Invalid IP range(s): ' + '; '.join(bad)
        else:
            errors['ip_ranges'] = 'IP ranges is a required field'

        if errors:
            return render_template("libraries/add", errors)

        seq = web.ctx.site.seq.next_value("libraries")

        doc.update({
            "_key": "libraries/pending-%d" % seq,
            "type": "library",
            "current_status": "pending",
            "registered_on": datetime.datetime.utcnow().isoformat()
        })
        web.ctx.site.store[doc['_key']] = doc

        self.sendmail(i.contact_email,
            render_template("libraries/email_confirmation"))

        if config.get("libraries_admin_email"):
            self.sendmail(config.libraries_admin_email,
                render_template("libraries/email_notification", i))

        return render_template("libraries/postadd")

    def sendmail(self, to, msg, cc=None):
        cc = cc or []
        subject = msg.subject.strip()
        body = web.safestr(msg).strip()

        if config.get('dummy_sendmail'):
            print >> web.debug, 'To:', to
            print >> web.debug, 'From:', config.from_address
            print >> web.debug, 'Subject:', subject
            print >> web.debug
            print >> web.debug, body
        else:
            web.sendmail(config.from_address, to, subject=subject, message=body, cc=cc)


class locations(delegate.page):
    path = "/libraries/locations.txt"

    def GET(self):
        libraries = inlibrary.get_libraries()
        web.header("Content-Type", "text/plain")
        return delegate.RawText(render_template("libraries/locations", libraries))

class stats(delegate.page):
    path = "/libraries/stats"

    def GET(self):
        stats  = LoanStats()
        return render_template("libraries/stats", stats);

class stats_per_library(delegate.page):
    path = "/libraries/stats/(.*).csv"

    def GET(self, libname):
        key = "/libraries/" + libname
        lib = web.ctx.site.get(key)
        if not lib:
            raise web.notfound()

        rows = lib.get_loans_per_day("total")        

        dates = [self.to_datestr(row[0]) for row in rows]
        total = [str(row[1]) for row in rows]
        pdf = [str(row[1]) for row in lib.get_loans_per_day("pdf")]
        epub = [str(row[1]) for row in lib.get_loans_per_day("epub")]
        bookreader = [str(row[1]) for row in lib.get_loans_per_day("bookreader")]

        fileobj = StringIO()
        writer = csv.writer(fileobj)
        writer.writerow(["Date", "Total Loans", "PDF Loans", "ePub Loans", "Bookreader Loans"])
        writer.writerows(zip(dates, total, pdf, epub, bookreader))

        return delegate.RawText(fileobj.getvalue(), content_type="application/csv")

    def to_datestr(self, millis):
        t = time.gmtime(millis/1000)
        return "%04d-%02d-%02d" % (t.tm_year, t.tm_mon, t.tm_mday)

@web.memoize
def get_admin_couchdb():
    db_url = config.get("admin", {}).get("counts_db")
    return db_url and couchdb.Database(db_url)

class LoanStats:
    def __init__(self):
        self.db = get_admin_couchdb()

    def view(self, viewname, **kw):
        if self.db:
            return self.db.view(viewname, **kw)
        else:
            return web.storage(rows=[])

    def percent(self, value, total):
        if total == 0:
            return 0
        else:
            return (100.0 * value)/total

    def get_summary(self):
        d = web.storage({
            "total_loans": 0,
            "one_hour_loans": 0,
            "expired_loans": 0
        })

        rows = self.view("loans/duration").rows
        if not rows:
            return d

        d['total_loans']  = rows[0].value['count']

        freq = rows[0].value['freq']

        d['one_hour_loans'] = freq.get("0", 0)
        d['expired_loans'] = sum(count for time, count in freq.items() if int(time) >= 14*24)
        return d

    def get_loans_per_day(self, resource_type="total", library=None):
        if library is None:
            library = ""
        rows = self.view("loans/loans", group=True, startkey=[library], endkey=[library,{}]).rows
        return [[self.date2timestamp(*row.key[1:])*1000, row.value.get(resource_type, 0)] for row in rows]

    def date2timestamp(self, year, month=1, day=1):
        return time.mktime((year, month, day, 0, 0, 0, 0, 0, 0)) # time.mktime takes 9-tuple as argument

    def date2millis(self, year, month=1, day=1):
        return self.date2timestamp(year, month, day) * 1000

    def get_loan_duration_frequency(self):
        rows = self.view("loans/duration").rows
        if not rows:
            return []

        row = rows[0]
        d = {}
        for time, count in row.value['freq'].items():
            n = 1 + int(time)/24

            # The loan entry gets added to couch only when the loan is deleted in the database, which is probably triggered by a cron job.
            # Even though the max loan duration is 2 weeks, there is a chance that duration is more than 14.
            if n > 14:
                n =15

            d[n] = d.get(n, 0) + count
        return sorted(d.items())

    def get_average_duration_per_month(self):
        """Returns average duration per month."""
        rows = self.view("loans/duration", group_level=2).rows
        minutes_per_day = 60.0 * 24.0
        return [[self.date2timestamp(*row.key)*1000, min(14, row.value['avg']/minutes_per_day)] for row in rows]

    def get_average_duration_per_day(self):
        """Returns average duration per day."""
        return [[self.date2millis(*key), value] for key, value in self._get_average_duration_per_day()]

    def _get_average_duration_per_day(self):
        """Returns (date, duration-in-days) for each day with duration averaged per day."""
        rows = self.view("loans/duration", group=True).rows
        minutes_per_day = 60.0 * 24.0
        return [[row.key, min(14, row.value['avg']/minutes_per_day)] for row in rows]

    def _get_average_duration_per_month(self):
        """Returns (date, duration-in-days) for each day with duration averaged per month."""
        for month, chunk in itertools.groupby(self._get_average_duration_per_day(), lambda x: x[0][:2]):
            chunk = list(chunk)
            avg = sum(v for k, v in chunk) / len(chunk)
            for k, v in chunk:
                yield k, avg

    def get_average_duration_per_month(self):
        """Returns (date-in-millis, duration-in-days) for each day with duration averaged per month."""
        return [[self.date2millis(*key), value] for key, value in self._get_average_duration_per_month()]

    def get_popular_books(self, limit=10):
        rows = self.view("loans/books", reduce=False, startkey=["", {}], endkey=[""], descending=True, limit=limit).rows
        counts = [row.key[-1] for row in rows]
        keys = [row.id for row in rows]
        books = web.ctx.site.get_many(keys)
        return zip(books, counts)

    def get_loans_per_book(self, key="", limit=1):
        """Returns the distribution of #loans/book."""
        rows = self.view("loans/books", group=True, startkey=[key], endkey=[key, {}]).rows
        return [[row.key[-1], row.value] for row in rows if row.value >= limit]

    def get_loans_per_user(self, key="", limit=1):
        """Returns the distribution of #loans/user."""
        rows = self.view("loans/people", group=True, startkey=[key], endkey=[key, {}]).rows
        return [[row.key[-1], row.value] for row in rows if row.value >= limit]

    def get_loans_per_library(self):
        counts = self._get_lib_counts()
        return [((lib.key, lib.name), count) for lib, count in counts]

    def get_loans_per_state(self):
        counts = self._get_lib_counts()
        return [((lib.key, lib.name), count) for lib, count in counts if lib.lending_region]

    def _get_lib_counts(self):
        # view contains:
        #   [lib_key, status], 1
        # Need to use group_level=1 and take key[0] to get the library key.
        rows = self.view("loans/libraries", group=True, group_level=1).rows
        libraries = self._get_libraries()
        return [(libraries[row.key[0]], row.value) for row in rows if row.key[0] in libraries]

    def get_active_loans_of_libraries(self):
        """Returns count of current active loans per library as a dictionary.
        """
        rows = self.view("loans/libraries", group=True).rows
        return dict((row.key[0], row.value) for row in rows if row.key[1] == "active")

    def _get_library_names(self):
        return dict((lib.key, lib.name) for lib in inlibrary.get_libraries())

    def _get_libraries(self):
        return dict((lib.key, lib) for lib in inlibrary.get_libraries())

@public
def get_active_loans_of_libraries():
    return LoanStats().get_active_loans_of_libraries()

def on_loan_created(loan):
    """Adds the loan info to the admin stats database.
    """
    logger.debug("on_loan_created")
    db = get_admin_couchdb()

    # The loan key is now changed from uuid to fixed key.
    # Using _key as key for loan stats will result in overwriting previous loans.
    # Using the unique uuid to create the loan key and falling back to _key
    # when uuid is not available.
    key = "loans/" + loan.get("uuid") or loan["_key"]

    t_start = datetime.datetime.utcfromtimestamp(loan['loaned_at'])

    d = {
        "_id": key,
        "book": loan['book'],
        "resource_type": loan['resource_type'],
        "t_start": t_start.isoformat(),
        "status": "active"
    }

    library = inlibrary.get_library()
    d['library'] = library and library.key
    d['geoip_country'] = geo_ip.get_country(web.ctx.ip)

    if key in db:
        logger.warn("loan document is already present in the stats database: %r", key)
    else:
        db[d['_id']] = d

    yyyy_mm = t_start.strftime("%Y-%m")

    logger.debug("incrementing loan count of %s", d['book'])

    # Increment book loan count
    # Loan count is maintained per month so that it is possible to find popular books per month, year and overall.
    book = db.get(d['book']) or {"_id": d['book']}
    book["loans"][yyyy_mm] = book.setdefault("loans", {}).setdefault(yyyy_mm, 0) + 1
    db[d['book']] = book

    # Increment user loan count
    user_key = loan['user']
    user = db.get(user_key) or {"_id": user_key}
    user["loans"][yyyy_mm] = user.setdefault("loans", {}).setdefault(yyyy_mm, 0) + 1
    db[user_key] = user

def on_loan_completed(loan):
    """Marks the loan as completed in the admin stats database.
    """
    logger.debug("on_loan_completed")
    db = get_admin_couchdb()

    # The loan key is now changed from uuid to fixed key.
    # Using _key as key for loan stats will result in overwriting previous loans.
    # Using the unique uuid to create the loan key and falling back to _key
    # when uuid is not available.
    key = "loans/" + loan.get("uuid") or loan["_key"]
    doc = db.get(key)

    t_end = datetime.datetime.utcfromtimestamp(loan['returned_at'])

    if doc:
        doc.update({
            "status": "completed",
            "t_end": t_end.isoformat()
        })
        db[doc['_id']] = doc
    else:
        logger.warn("loan document missing in the stats database: %r", key)

def on_loan_created_statsdb(loan):
    """Adds the loan info to the stats database.
    """
    key = _get_loan_key(loan)
    t_start = datetime.datetime.utcfromtimestamp(loan['loaned_at'])
    d = {
        "book": loan['book'],
        "resource_type": loan['resource_type'],
        "t_start": t_start.isoformat(),
        "status": "active"
    }
    library = inlibrary.get_library()
    d['library'] = library and library.key
    d['geoip_country'] = geo_ip.get_country(web.ctx.ip)
    statsdb.add_entry(key, d)

def on_loan_completed_statsdb(loan):
    """Marks the loan as completed in the stats database.
    """
    key = _get_loan_key(loan)
    t_start = datetime.datetime.utcfromtimestamp(loan['loaned_at'])
    t_end = datetime.datetime.utcfromtimestamp(loan['returned_at'])
    d = {
        "book": loan['book'],
        "resource_type": loan['resource_type'],
        "t_start": t_start.isoformat(),
        "t_end": t_end.isoformat(),
        "status": "completed",
    }
    old = statsdb.get_entry(key)
    if old:
        d = dict(old, **d)
    statsdb.update_entry(key, d)

def _get_loan_key(loan):
    # The loan key is now changed from uuid to fixed key.
    # Using _key as key for loan stats will result in overwriting previous loans.
    # Using the unique uuid to create the loan key and falling back to _key
    # when uuid is not available.
    return "loans/" + loan.get("uuid") or loan["_key"]

def setup():
    from openlibrary.core import msgbroker

    msgbroker.subscribe("loan-created", on_loan_created)
    msgbroker.subscribe("loan-completed", on_loan_completed)

    msgbroker.subscribe("loan-created", on_loan_created_statsdb)
    msgbroker.subscribe("loan-completed", on_loan_completed_statsdb)

########NEW FILE########
__FILENAME__ = lists
"""Lists implementaion.
"""
import random
import web

from infogami.utils import delegate
from infogami.utils.view import render_template, public
from infogami.infobase import client, common

from openlibrary.core import formats, cache
import openlibrary.core.helpers as h

from openlibrary.plugins.worksearch import subjects

class lists_home(delegate.page):
    path = "/lists"
    
    def GET(self):
        return render_template("lists/home")

class lists(delegate.page):
    """Controller for displaying lists of a seed or lists of a person.
    """
    path = "(/(?:people|books|works|authors|subjects)/[^/]+)/lists"

    def is_enabled(self):
        return "lists" in web.ctx.features
            
    def GET(self, path):
        doc = self.get_doc(path)
        if not doc:
            raise web.notfound()
            
        lists = doc.get_lists()
        return self.render(doc, lists)
        
    def get_doc(self, key):
        if key.startswith("/subjects/"):
            s = subjects.get_subject(key)
            if s.work_count > 0:
                return s
            else:
                return None
        else:
            return web.ctx.site.get(key)
        
    def render(self, doc, lists):
        return render_template("lists/lists.html", doc, lists)
        
class lists_delete(delegate.page):
    path = "(/people/\w+/lists/OL\d+L)/delete"
    encoding = "json"

    def POST(self, key):
        doc = web.ctx.site.get(key)
        if doc is None or doc.type.key != '/type/list':
            raise web.notfound()
        
        doc = {
            "key": key,
            "type": {"key": "/type/delete"}
        }
        try:
            result = web.ctx.site.save(doc, action="lists", comment="Deleted list.")
        except client.ClientException, e:
            web.ctx.status = e.status
            web.header("Content-Type", "application/json")
            return delegate.RawText(e.json)
            
        web.header("Content-Type", "application/json")
        return delegate.RawText('{"status": "ok"}')
        
class lists_json(delegate.page):
    path = "(/(?:people|books|works|authors|subjects)/[^/]+)/lists"
    encoding = "json"
    content_type = "application/json"
    
    def GET(self, path):
        if path.startswith("/subjects/"):
            doc = subjects.get_subject(path)
        else:
            doc = web.ctx.site.get(path)
        if not doc:
            raise web.notfound()
            
        i = web.input(offset=0, limit=50)
        i.offset = h.safeint(i.offset, 0)
        i.limit = h.safeint(i.limit, 50)
        
        i.limit = min(i.limit, 100)
        i.offset = max(i.offset, 0)
        
        lists = self.get_lists(doc, limit=i.limit, offset=i.offset)
        return delegate.RawText(self.dumps(lists))
        
    def get_lists(self, doc, limit=50, offset=0):
        lists = doc.get_lists(limit=limit, offset=offset)
        size = len(lists)
        
        if offset or len(lists) == limit:
            # There could be more lists than len(lists)
            size = len(doc.get_lists(limit=1000))
        
        d = {
            "links": {
                "self": web.ctx.path
            },
            "size": size,
            "entries": [list.preview() for list in lists]
        }
        if offset + len(lists) < size:
            d['links']['next'] = web.changequery(limit=limit, offset=offset + limit)
            
        if offset:
            offset = max(0, offset-limit)
            d['links']['prev'] = web.changequery(limit=limit, offset=offset)
            
        return d
            
    def forbidden(self):
        headers = {"Content-Type": self.get_content_type()}
        data = {
            "message": "Permission denied."
        }
        return web.HTTPError("403 Forbidden", data=self.dumps(data), headers=headers)
        
    def POST(self, user_key):
        # POST is allowed only for /people/foo/lists
        if not user_key.startswith("/people/"):
            raise web.nomethod()
        
        site = web.ctx.site
        user = site.get(user_key)
        
        if not user:
            raise web.notfound()
            
        if not site.can_write(user_key):
            raise self.forbidden()
        
        data = self.loads(web.data())
        # TODO: validate data
        
        seeds = self.process_seeds(data.get('seeds', []))
        
        list = user.new_list(
            name=data.get('name', ''),
            description=data.get('description', ''),
            tags=data.get('tags', []),
            seeds=seeds
        )
        
        try:
            result = site.save(list.dict(), 
                comment="Created new list.",
                action="lists",
                data={
                    "list": {"key": list.key},
                    "seeds": seeds
                }
            )
        except client.ClientException, e:
            headers = {"Content-Type": self.get_content_type()}
            data = {
                "message": e.message
            }
            raise web.HTTPError(e.status, 
                data=self.dumps(data),
                headers=headers)
        
        web.header("Content-Type", self.get_content_type())
        return delegate.RawText(self.dumps(result))
        
    def process_seeds(self, seeds):
        def f(seed):
            if isinstance(seed, dict):
                return seed
            elif seed.startswith("/subjects/"):
                seed = seed.split("/")[-1]
                if seed.split(":")[0] not in ["place", "person", "time"]:
                    seed = "subject:" + seed
                seed = seed.replace(",", "_").replace("__", "_")
            elif seed.startswith("/"):
                seed = {"key": seed}
            return seed
        return [f(seed) for seed in seeds]
                
    def get_content_type(self):
        return self.content_type
        
    def dumps(self, data):
        return formats.dump(data, self.encoding)
    
    def loads(self, text):
        return formats.load(text, self.encoding)

class lists_yaml(lists_json):
    encoding = "yml"
    content_type = "text/yaml"

class list_view_json(delegate.page):
    path = "(/people/[^/]+/lists/OL\d+L)"
    encoding = "json"
    content_type = "application/json"

    def GET(self, key):
        list = web.ctx.site.get(key)
        if not list or list.type.key == '/type/delete':
            raise web.notfound()
        
        data = self.get_list_data(list)
        return delegate.RawText(self.dumps(data))
            
    def get_list_data(self, list):
        return {
            "links": {
                "self": list.key,
                "seeds": list.key + "/seeds",
                "subjects": list.key + "/subjects",
                "editions": list.key + "/editions",
            },
            "name": list.name or None,
            "description": list.description and unicode(list.description) or None,
            "seed_count": len(list.seeds),
            "edition_count": list.edition_count,
            
            "meta": {
                "revision": list.revision,
                "created": list.created.isoformat(),
                "last_modified": list.last_modified.isoformat(),
            }
        }

    def dumps(self, data):
        web.header("Content-Type", self.content_type)
        return formats.dump(data, self.encoding)
    
class list_view_yaml(list_view_json):
    encoding = "yml"
    content_type = "text/yaml"
        
class list_seeds(delegate.page):
    path = "(/people/\w+/lists/OL\d+L)/seeds"
    encoding = "json"
    
    content_type = "application/json"
    
    def GET(self, key):
        list = web.ctx.site.get(key)
        if not list:
            raise web.notfound()
        
        seeds = [seed.dict() for seed in list.get_seeds()]
        
        data = {
            "links": {
                "self": key + "/seeds",
                "list": key
            },
            "size": len(seeds),
            "entries": seeds
        }
        
        text = formats.dump(data, self.encoding)
        return delegate.RawText(text)
        
    def POST(self, key):
        site = web.ctx.site
        
        list = site.get(key)
        if not list:
            raise web.notfound()
            
        if not site.can_write(key):
            raise self.forbidden()
            
        data = formats.load(web.data(), self.encoding)
        
        data.setdefault("add", [])
        data.setdefault("remove", [])
        
        # support /subjects/foo and /books/OL1M along with subject:foo and {"key": "/books/OL1M"}.
        process_seeds = lists_json().process_seeds
        
        for seed in process_seeds(data["add"]):
            list.add_seed(seed)
            
        for seed in process_seeds(data["remove"]):
            list.remove_seed(seed)
            
        seeds = []
        for seed in data["add"] + data["remove"]:
            if isinstance(seed, dict):
                seeds.append(seed['key'])
            else:
                seeds.append(seed)
                
        changeset_data = {
            "list": {"key": key},
            "seeds": seeds,
            "add": data.get("add", []),
            "remove": data.get("remove", [])
        }
            
        d = list._save(comment="updated list seeds.", action="lists", data=changeset_data)
        web.header("Content-Type", self.content_type)
        return delegate.RawText(formats.dump(d, self.encoding))

class list_seed_yaml(list_seeds):
    encoding = "yml"
    content_type = 'text/yaml; charset="utf-8"'
    

class list_editions(delegate.page):
    """Controller for displaying lists of a seed or lists of a person.
    """
    path = "(/people/\w+/lists/OL\d+L)/editions"

    def is_enabled(self):
        return "lists" in web.ctx.features

    def GET(self, path):
        list = web.ctx.site.get(path)
        if not list:
            raise web.notfound()
        
        i = web.input(limit=50, page=1)
        limit = h.safeint(i.limit, 50)
        page = h.safeint(i.page, 1) - 1
        offset = page * limit

        editions = list.get_editions(limit=limit, offset=offset)
        
        list.preload_authors(editions['editions'])
        list.load_changesets(editions['editions'])
        
        return render_template("type/list/editions.html", list, editions)

class list_editions_json(delegate.page):
    path = "(/people/\w+/lists/OL\d+L)/editions"
    encoding = "json"

    content_type = "application/json"

    def GET(self, key):
        list = web.ctx.site.get(key)
        if not list:
            raise web.notfound()
            
        i = web.input(limit=50, offset=0)
        
        limit = h.safeint(i.limit, 50)
        offset = h.safeint(i.offset, 0)
        
        editions = list.get_editions(limit=limit, offset=offset, _raw=True)
        
        data = make_collection(
            size=editions['count'], 
            entries=[self.process_edition(e) for e in editions['editions']],
            limit=limit,
            offset=offset
        )
        data['links']['list'] = key 
        text = formats.dump(data, self.encoding)
        return delegate.RawText(text, content_type=self.content_type)
        
    def process_edition(self, e):
        e.pop("seeds", None)
        return e
        
class list_editions_yaml(list_editions_json):
    encoding = "yml"
    content_type = 'text/yaml; charset="utf-8"'    
    
def make_collection(size, entries, limit, offset):
    d = {
        "size": size,
        "entries": entries,
        "links": {
            "self": web.changequery(),
        }
    }
    
    if offset + len(entries) < size:
        d['links']['next'] = web.changequery(limit=limit, offset=offset+limit)
    
    if offset:
        d['links']['prev'] = web.changequery(limit=limit, offset=max(0, offset-limit))
    
    return d

class list_subjects_json(delegate.page):
    path = "(/people/\w+/lists/OL\d+L)/subjects"
    encoding = "json"
    content_type = "application/json"

    def GET(self, key):
        list = web.ctx.site.get(key)
        if not list:
            raise web.notfound()
            
        i = web.input(limit=20)
        limit = h.safeint(i.limit, 20)

        data = self.get_subjects(list, limit=limit)
        data['links'] = {
            "self": key + "/subjects",
            "list": key
        }
        
        text = formats.dump(data, self.encoding)
        return delegate.RawText(text, content_type=self.content_type)
        
    def get_subjects(self, list, limit):
        data = list.get_subjects(limit=limit)
        for key, subjects in data.items():
            data[key] = [self._process_subject(s) for s in subjects]
        return dict(data)
        
    def _process_subject(self, s):
        key = s['key']
        if key.startswith("subject:"):
            key = "/subjects/" + web.lstrips(key, "subject:")
        else:
            key = "/subjects/" + key
        return {
            "name": s['name'],
            "count": s['count'],
            "url": key
        }
        
class list_editions_yaml(list_subjects_json):
    encoding = "yml"
    content_type = 'text/yaml; charset="utf-8"'


class export(delegate.page):
    path = "(/people/\w+/lists/OL\d+L)/export"

    def GET(self, key):
        list = web.ctx.site.get(key)
        if not list:
            raise web.notfound()
            
        format = web.input(format="html").format
                
        if format == "html":
            html = render_template("lists/export_as_html", list, self.get_editions(list))
            return delegate.RawText(html)
        elif format == "bibtex":
            html = render_template("lists/export_as_bibtex", list, self.get_editions(list))
            return delegate.RawText(html)
        elif format == "json":
            data = {"editions": self.get_editions(list, raw=True)}
            web.header("Content-Type", "application/json")
            return delegate.RawText(formats.dump_json(data))
        elif format == "yaml":
            data = {"editions": self.get_editions(list, raw=True)}
            web.header("Content-Type", "application/yaml")
            return delegate.RawText(formats.dump_yaml(data))
        else:
            raise web.notfound()
            
    def get_editions(self, list, raw=False):
        editions = sorted(list.get_all_editions(), key=lambda doc: doc['last_modified']['value'], reverse=True)
        
        if not raw:
            editions = [self.make_doc(e) for e in editions]
            list.preload_authors(editions)
        return editions
        
    def make_doc(self, rawdata):
        data = web.ctx.site._process_dict(common.parse_query(rawdata))
        doc = client.create_thing(web.ctx.site, data['key'], data)
        return doc
        
class feeds(delegate.page):
    path = "(/people/[^/]+/lists/OL\d+L)/feeds/(updates).(atom)"
    
    def GET(self, key, name, format):
        list = web.ctx.site.get(key)
        if list is None:
            raise web.notfound()
        text = getattr(self, 'GET_' + name + '_' + format)(list)
        return delegate.RawText(text)
    
    def GET_updates_atom(self, list):
        web.header("Content-Type", 'application/atom+xml; charset="utf-8"')
        return render_template("lists/feed_updates.xml", list)
    
def setup():
    pass

def get_recently_modified_lists(limit, offset=0):
    """Returns the most recently modified lists as list of dictionaries.
    
    This function is memoized for better performance.
    """
    # this function is memozied with background=True option. 
    # web.ctx must be initialized as it won't be avaiable to the background thread.
    if 'env' not in web.ctx:
        delegate.fakeload()
    
    keys = web.ctx.site.things({"type": "/type/list", "sort": "-last_modified", "limit": limit, "offset": offset})
    lists = web.ctx.site.get_many(keys)
    
    # XXX-Anand, March 2014: This is not required any more. We switched to using solr
    # instead of relying on this data from couch.
    #
    # Cache seed_summary, so that it can be reused later without recomputing.
    #for list in lists:
    #    list.seed_summary_cached = list.seed_summary
        
    return [list.dict() for list in lists]
    
get_recently_modified_lists = cache.memcache_memoize(get_recently_modified_lists, key_prefix="get_recently_modified_lists", timeout=5*60)
    
def _preload_lists(lists):
    """Preloads all referenced documents for each list.
    List can be either a dict of a model object.
    """
    keys = set()
    
    for xlist in lists:
        if not isinstance(xlist, dict):
            xlist = xlist.dict()
        
        owner = xlist['key'].rsplit("/lists/", 1)[0]
        keys.add(owner)
                
        for seed in xlist.get("seeds", []):
            if isinstance(seed, dict) and "key" in seed:
                keys.add(seed['key'])
            
    web.ctx.site.get_many(list(keys))

@public
def get_active_lists_in_random(limit=20, preload=True):
    lists = []
    offset = 0
    
    while len(lists) < limit:
        result = get_recently_modified_lists(limit*5, offset=offset)
        if not result:
            break
            
        offset += len(result)
        # ignore lists with 4 or less seeds
        lists += [xlist for xlist in result if len(xlist.get("seeds", [])) > 4]
    
    if len(lists) > limit:
        lists = random.sample(lists, limit)

    if preload:
        _preload_lists(lists)
        
    seed_summaries = {}
    
    for xlist in lists:
        seed_summaries[xlist['key']] = xlist.pop("seed_summary_cached", None)
    
    # convert rawdata into models.
    lists = [web.ctx.site.new(xlist['key'], xlist) for xlist in lists]
    
    # Initialize seed_summary to avoid recomputing it for all the lists.
    for xlist in lists:
        if xlist.key in seed_summaries:
            xlist.__dict__['seed_summary'] = seed_summaries[xlist.key]

    return lists
########NEW FILE########
__FILENAME__ = merge_editions
'Merge editions'
import web, re
from openlibrary.utils import uniq, dicthash
from infogami.utils import delegate
from infogami.utils.view import render_template
from collections import defaultdict

re_nonword = re.compile(r'\W', re.U)

class merge_editions(delegate.page):
    path = '/books/merge'

    def is_enabled(self):
        return "merge-editions" in web.ctx.features

    def GET(self):
        i = web.input(key=[],merge_key=[])
        keys = uniq(i.key)
        merge_keys = uniq(i.merge_key)
        assert all(k is not None for k in merge_keys)
        if not merge_keys:
            return render_template('merge/editions', keys)

        full_keys = ['/books/' + k for k in merge_keys]
        editions = [web.ctx.site.get('/books/' + k) for k in merge_keys]
        master = None
        for e in editions:
            if e.key == '/books/' + i.master:
                master = e
                break

        all_keys = set()
        for e in editions:
            for k in e.keys():
                if e[k] is not None and e[k] != {}:
                    all_keys.add(k)

        merged = {}
        possible_values = defaultdict(lambda: defaultdict(int))

        k = 'publish_date'
        publish_dates = set(e[k] for e in editions if k in e and len(e[k]) != 4)

        k = 'pagination'
        all_pagination = set(e[k] for e in editions if e.get(k))

        one_item_lists = {}
        for k in 'lc_classifications', 'publishers', 'contributions', 'series':
            one_item_lists[k] = set(e[k][0].strip('.') for e in editions if e.get(k) and len(set(e[k])) == 1)

        for k in ['other_titles', 'isbn_10', 'series']:
            if k not in all_keys:
                continue
            merged[k] = []
            for e in editions:
                for v in e.get(k, []):
                    if v not in merged[k]:
                        possible_values[k][v] += 1
                        merged[k].append(v)

        k = 'ocaid'
        for e in editions:
            v = e.get(k)
            if not v:
                continue
            possible_values[k][v] += 1
            if 'ia:' + v not in merged.get('source_records', []):
                merged.setdefault('source_records', []).append(v)

        k = 'identifiers'
        if k in all_keys:
            merged[k] = {}
            for e in editions:
                if k not in e:
                    continue
                for a, b in e[k].items():
                    for c in b:
                        if c in merged[k].setdefault(a, []):
                            continue
                        merged[k][a].append(c)

        any_publish_country = False
        k = 'publish_country'
        if k in all_keys:
            for e in editions:
                if e.get(k) and not e[k].strip().startswith('xx'):
                    any_publish_country = True

        for k in 'source_records', 'ia_box_id':
            merged[k] = []
            for e in editions:
                if e.get(k) and isinstance(e[k], basestring):
                    e[k] = [e[k]]
                if e.get(k):
                    assert isinstance(e[k], list)
                for sr in e.get(k, []):
                    if sr not in merged[k]:
                        merged[k].append(sr)

        for k in all_keys:
            if k in ('source_records', 'ia_box_id', 'identifiers', 'ocaid', 'other_titles', 'series'):
                continue
            uniq_values = defaultdict(list)
            for num, e in enumerate(editions):
                v = e.get(k) 
                if v:
                    if isinstance(v, list):
                        for lv in v:
                            possible_values[k][lv] += 1
                    elif not isinstance(v, dict):
                        possible_values[k][v] += 1
                    if k == 'publish_date' and len(v) == 4 and v.isdigit and any(v in pd for pd in publish_dates):
                        continue
                    if k == 'pagination' and any(len(i) > len(v) and v in i for i in all_pagination):
                        continue
                    if k in one_item_lists and len(set(e.get(k, []))) == 1 and any(len(i) > len(v[0].strip('.')) and v[0].strip('.') in i for i in one_item_lists[k]):
                        continue
                    if k == 'publish_country' and any_publish_country and e.get(k, '').strip().startswith('xx'):
                        continue
                    if k == 'edition_name' and v.endswith(' ed edition'):
                        v = v[:-len(' edition')]
                    uniq_values[re_nonword.sub('', `v`.lower())].append(num)

            if len(uniq_values) == 1:
                merged[k] = editions[uniq_values.values()[0][0]][k]
                continue

            if k == 'covers':
                assert all(isinstance(e[k], list) for e in editions if k in e)
                covers = set()
                for e in editions:
                    if k in e:
                        covers.update(c for c in e[k] if c != -1)
                merged['covers'] = sorted(covers)
                continue

            if k == 'notes':
                merged['notes'] = ''
                for e in editions:
                    if e.get('notes'):
                        merged['notes'] += e['notes'] + '\n'
                continue

            if k == 'ocaid':
                for e in editions:
                    if e.get('ocaid'):
                        #assert not e['ocaid'].endswith('goog')
                        merged['ocaid'] = e['ocaid']
                        break
                assert merged['ocaid']
                continue

        return render_template('merge/editions2', master, editions, all_keys, merged, possible_values)

def setup():
    pass

########NEW FILE########
__FILENAME__ = opds
"""
OPDS helper class.
A lightweight version of github.com/internetarchive/bookserver
"""

import lxml.etree as ET
from infogami.infobase.utils import parse_datetime

class OPDS():
    xmlns_atom    = 'http://www.w3.org/2005/Atom'
    xmlns_dcterms = 'http://purl.org/dc/terms/'
    xmlns_opds    = 'http://opds-spec.org/'
    xmlns_rdvocab = 'http://RDVocab.info/elements/'
    xmlns_bibo    = 'http://purl.org/ontology/bibo/'
    xmlns_xsi     = 'http://www.w3.org/2001/XMLSchema-instance'
    

    nsmap = {
        None     : xmlns_atom,
        'dcterms': xmlns_dcterms,
        'opds'   : xmlns_opds,
        'rdvocab': xmlns_rdvocab,
        'bibo'   : xmlns_bibo,
        'xsi'    : xmlns_xsi
    }

    atom          = "{%s}" % xmlns_atom
    dcterms       = "{%s}" % xmlns_dcterms
    opdsNS        = "{%s}" % xmlns_opds
    rdvocab       = "{%s}" % xmlns_rdvocab
    bibo          = "{%s}" % xmlns_bibo
    xsi           = "{%s}" % xmlns_xsi
    
    fileExtMap = {
        'pdf'  : 'application/pdf',
        'epub' : 'application/epub+zip',
        'mobi' : 'application/x-mobipocket-ebook'
    }
    
    ebookTypes = ('application/pdf',
                  'application/epub+zip',
                  'application/x-mobipocket-ebook'
    )
    
    # create_text_element()
    #___________________________________________________________________________
    def create_text_element(self, parent, name, value):
        element = ET.SubElement(parent, name)
        element.text = value
        return element

    # add()
    #___________________________________________________________________________
    def add(self, name, value, attrs={}):
        element = self.create_text_element(self.root, name, value)
        for a in attrs:
            element.attrib[a] = attrs[a]

    # add_list()
    #___________________________________________________________________________
    def add_list(self, name, values, prefix='', attrs={}):
        if isinstance(values, list) or isinstance(values, tuple):
            for v in values:
                self.add(name, prefix+unicode(v), attrs)
        elif values:
            self.add(name, prefix+unicode(values), attrs)

    # add_author()
    #___________________________________________________________________________
    def add_author(self, name, uri=None):
        element = ET.SubElement(self.root, 'author')
        self.create_text_element(element, 'name', name)
        if uri:
            self.create_text_element(element, 'uri', uri)
        return element

    # create_rel_link()
    #___________________________________________________________________________
    def create_rel_link(self, parent, rel, absurl, type='application/atom+xml', title=None):
        if None == parent:
            parent = self.root
            
        element = ET.SubElement(parent, 'link')
        element.attrib['rel']  = rel
        element.attrib['type'] = type
        element.attrib['href'] = absurl;
        if title:
            element.attrib['title'] = title;

        return element            
            
    # to_string()
    #___________________________________________________________________________
    def to_string(self):
        return ET.tostring(self.root, pretty_print=True)
        
    # create_root()
    #___________________________________________________________________________
    def create_root(self, root_name):
        ### TODO: add updated element and uuid element
        opds = ET.Element(OPDS.atom + root_name, nsmap=OPDS.nsmap)                    

        return opds

    # __init__()
    #___________________________________________________________________________    
    def __init__(self, root_name="feed"):

        self.root = self.create_root(root_name)

class OPDSEntry(OPDS):
    def _add_subelement(self, tagname, **attrs):
        """Adds a sub element with given tagname and attributes.

        Ensures all attribute values are xml-safe before setting in the
        element. Returns the element added.
        """
        element = ET.SubElement(self.root, tagname)
        for name, value in attrs.items():
            element.attrib[name] = xmlsafe(value)
        return element
        
    # add_category()
    #___________________________________________________________________________
    def add_category(self, term, label):
        return self._add_subelement("category", term=term, label=label)

    # add_indirect_acq()
    #___________________________________________________________________________
    def add_indirect_acq(self, parent, type):
        element = ET.SubElement(parent, self.opdsNS+'indirectAcquisition')
        element.attrib['type'] = type
        return element
                
    # add_acquisition_links()
    #___________________________________________________________________________
    def add_acquisition_links(self, book, collection):
        if not book.ocaid:
            return
        
        if 'inlibrary' in collection or 'lendinglibrary' in collection:
            available_loans = book.get_available_loans()
            loan_types = [loan['resource_type'] for loan in available_loans]
            got_epub = 'epub' in loan_types
            got_pdf  = 'pdf' in loan_types
            
            if got_epub or got_pdf:
                link = self.create_rel_link(None, 'http://opds-spec.org/acquisition/borrow', 'https://openlibrary.org'+book.url('/borrow'), 'text/html')
                indirect_acq = self.add_indirect_acq(link, 'application/vnd.adobe.adept+xml')
                if got_epub:
                    self.add_indirect_acq(indirect_acq, 'application/epub+zip')
                if got_pdf:
                    self.add_indirect_acq(indirect_acq, 'application/pdf')
        elif 'printdisabled' not in collection:
            self.create_rel_link(None, 'http://opds-spec.org/acquisition/open-access', 'https://archive.org/download/%s/%s.pdf'%(book.ocaid, book.ocaid), 'application/pdf')
            self.create_rel_link(None, 'http://opds-spec.org/acquisition/open-access', 'https://archive.org/download/%s/%s.epub'%(book.ocaid, book.ocaid), 'application/epub+zip')

    # add_rel_links()
    #___________________________________________________________________________
    def add_rel_links(self, book, work):
        links = []
        if work:
            self.create_rel_link(None, 'related', 'https://openlibrary.org'+work.key, 'text/html', 'Open Library Work')

        for name, values in book.get_identifiers().multi_items():
            for id in values:
                if id.url and name not in ['oclc_numbers', 'lccn', 'ocaid']: #these go in other elements
                    self.create_rel_link(None, 'related', id.url, 'text/html', 'View on '+id.label)

    # __init__()
    #___________________________________________________________________________    
    def __init__(self, book):

        self.root = self.create_root('entry')

        bookID = book.key
        atomID = 'https://openlibrary.org' + bookID + '.opds'
        title = book.title
        if book.subtitle:
            title += " " + book.subtitle
        updated = parse_datetime(book.last_modified).strftime('%Y-%m-%dT%H:%M:%SZ')
    
        work = book.works and book.works[0]
    
        if work:
            authors  = work.get_authors()
            subjects = work.get_subjects()
        else:
            authors  = book.get_authors()  
            subjects = book.get_subjects()
            
        if book.pagination:
            pages = book.pagination
        else:
            pages = book.number_of_pages
    
        # the collection and inlibrary check is coped from databarWork.html
        collection = set()
        meta_fields = book.get_ia_meta_fields()
        if meta_fields:
            collection = meta_fields.get('collection', [])
            contrib = meta_fields.get('contributor')

        coverLarge = book.get_cover_url('L')
        coverThumb = book.get_cover_url('S')
                    
        self.add('id', atomID)
        self.create_rel_link(None, 'self', atomID)
        self.create_rel_link(None, 'alternate', 'https://openlibrary.org'+book.url(), 'text/html')
        self.add('title', title)
        self.add('updated', updated)
        
        for a in authors:
            self.add_author(a.name, 'https://openlibrary.org'+a.url())
        
        self.add_list(self.dcterms + 'publisher', book.publishers)
        self.add_list(self.rdvocab + 'placeOfPublication', book.publish_places)
        self.add_list(self.dcterms + 'issued', book.publish_date)
        self.add_list(self.dcterms + 'extent', pages)
        self.add_list(self.rdvocab + 'dimensions', book.physical_dimensions)
        self.add_list(self.bibo    + 'edition', book.edition_name)
        
        for subject in subjects:
            self.add_category('/subjects/'+subject.lower().replace(' ', '_').replace(',',''), subject)
    
        self.add_list('summary', book.description)
        self.add_list(self.rdvocab + 'note', book.notes)
    
        for lang in book.languages:
            self.add_list(self.dcterms + 'language', lang.code)
    
        self.add_list(self.dcterms + 'identifier', book.key,     'https://openlibrary.org', {self.xsi+'type':'dcterms:URI'})
        self.add_list(self.dcterms + 'identifier', book.ocaid,   'https://archive.org/details/', {self.xsi+'type':'dcterms:URI'})
        self.add_list(self.dcterms + 'identifier', book.isbn_10, 'urn:ISBN:', {self.xsi+'type':'dcterms:ISBN'})
        self.add_list(self.dcterms + 'identifier', book.isbn_13, 'urn:ISBN:', {self.xsi+'type':'dcterms:ISBN'})
        self.add_list(self.bibo + 'oclcnum', book.oclc_numbers)
        self.add_list(self.bibo + 'lccn', book.lccn)
        
        if coverLarge:
            self.create_rel_link(None, 'http://opds-spec.org/image', coverLarge, 'image/jpeg')
        if coverThumb:
            self.create_rel_link(None, 'http://opds-spec.org/image/thumbnail', coverThumb, 'image/jpeg')

        self.add_acquisition_links(book, collection)
        self.add_rel_links(book, work)        

def xmlsafe(s):
    """Removes all the XML-unsafe characters from given string.

    XML cannot include certain characters mainly control ones with
    byte value below 32. This function strips them all.
    """
    if isinstance(s, str):
        s = s.decode('utf-8')
    # ignore the first 32 bytes of ASCII, which are not allowd in XML
    return u"".join(c for c in s if ord(c) >= 0x20)


########NEW FILE########
__FILENAME__ = processors
"""web.py application processors for Open Library.
"""
import web

from openlibrary.core.processors import ReadableUrlProcessor

from openlibrary.core import helpers as h

urlsafe = h.urlsafe
_safepath = h.urlsafe

class ProfileProcessor:
    """Processor to profile the webpage when ?_profile=true is added to the url.
    """
    def __call__(self, handler):
        i = web.input(_method="GET", _profile="")
        if i._profile.lower() == "true":
            out, result = web.profile(handler)()
            if isinstance(out, web.template.TemplateResult):
                out.__body__ = out.get('__body__', '') + '<pre class="profile">' + web.websafe(result) + '</pre>'
                return out
            elif isinstance(out, basestring):
                return out + '<br/>' + '<pre class="profile">' + web.websafe(result) + '</pre>'
            else:
                # don't know how to handle this.
                return out
        else:
            return handler()
    
if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = readbooks
"""Script to get all books from the database and print every book as python dict.

WARNING: This gets only the new books with revision=1.

"""
import web
import time
import sys

def select(query, chunk_size=50000):
    """Selects large number of rows efficiently using cursors."""
    web.transact()
    web.query('DECLARE select_cursor CURSOR FOR ' + query)
    while True:
        result = web.query('FETCH FORWARD $chunk_size FROM select_cursor', vars=locals())
        if not result:
            break
        for r in result:
            yield r
    web.rollback()
        
def parse_datum(rows):
    thing = None
    for r in rows:
        if thing is None:
            thing = dict(id=r.thing_id)
        elif thing.get('id') != r.thing_id:
            yield thing
            thing = dict(id=r.thing_id)
        
        if r.ordering is None:
            thing[r.key] = r.value
        else:
            thing.setdefault(r.key, []).append(r.value)
            
def books(fbooks, fauthors):        
    authors = {}
    type_author = str(web.query("SELECT * FROM thing WHERE site_id=1 AND key='/type/author'")[0].id)
    type_edition = str(web.query("SELECT * FROM thing WHERE site_id=1 AND key='/type/edition'")[0].id)
    result = select("SELECT * FROM datum ORDER BY thing_id WHERE end_revision=2147483647")
    t1 = time.time()
    for i, t in enumerate(parse_datum(result)):
        if t['type'] == type_author:
            fauthors.write(str(t))
            fauthors.write("\n")
        elif t['type'] == type_edition:
            fbooks.write(str(t))
            fbooks.write("\n")

        if i and i%10000 == 0:
            t2 = time.time()
            dt = t2 - t1
            t1 = t2
            print "%d: 10000 books read in %f time. %f things/sec" % (i, dt, 10000/dt)
 
def main():
    web.config.db_parameters = dict(dbn='postgres', db='infobase_data4', host='pharosdb', user='anand', pw='')
    web.config.db_printing = True
    web.load()
    
    fbooks = open("books.txt", "w")
    fauthors = open("authors.txt", "w")
    books(fbooks, fauthors)
    fbooks.close()
    fauthors.close()

if __name__ == "__main__":
    main()
    

########NEW FILE########
__FILENAME__ = schema
"""OpenLibrary schema."""
from openlibrary.core.schema import get_schema

if __name__ == "__main__":
    print get_schema().sql()

########NEW FILE########
__FILENAME__ = stats
"""Hooks for collecting performance stats.
"""
import logging
import traceback

from openlibrary.core import stats as graphite_stats

import web
from infogami import config
from infogami.utils import stats
import openlibrary.core.stats

import filters as stats_filters

l = logging.getLogger("openlibrary.stats")

filters = {}

def evaluate_and_store_stat(name, stat, summary):
    """Evaluates whether the given statistic is to be recorded and if
    so, records it."""
    global filters
    if not summary:
        return
    try:
        f = filters[stat.filter]
    except KeyError:
        l.warning("Filter %s not registered", stat.filter)
        return
    try:
        if f(**stat):
            if stat.has_key("time"):
                graphite_stats.put(name, summary[stat.time]["time"] * 100)
            elif stat.has_key("count"):
                #print "Storing count for key %s"%stat.count
                # XXX-Anand: where is the code to update counts?
                pass
            else:
                l.warning("No storage item specified for stat %s", name)
    except Exception, k:
        l.warning("Error while storing stats (%s). Complete traceback follows"%k)
        l.warning(traceback.format_exc())
    
def update_all_stats(stats_summary):
    """
    Run through the filters and record requested items in graphite
    """
    for stat in config.get("stats", []):
        evaluate_and_store_stat(stat, config.stats.get(stat), stats_summary)
        
def stats_hook():
    """web.py unload hook to add X-OL-Stats header.
    
    This info can be written to lighttpd access log for collecting
    
    Also, send stats to graphite using statsd
    """
    stats_summary = stats.stats_summary()
    update_all_stats(stats_summary)
    try:
        if "stats-header" in web.ctx.features:
            web.header("X-OL-Stats", format_stats(stats_summary))
    except Exception, e:
        # don't let errors in stats collection break the app.
        print >> web.debug, str(e)
        
    openlibrary.core.stats.increment('ol.pageviews')
    
    memcache_hits = 0
    memcache_misses = 0
    for s in web.ctx.get("stats", []):
        if s.name == 'memcache.get':
            if s.data['hit']:
                memcache_hits += 1
            else:
                memcache_misses += 1
    
    if memcache_hits:
        openlibrary.core.stats.increment('ol.memcache.hits', memcache_hits)
    if memcache_misses:
        openlibrary.core.stats.increment('ol.memcache.misses', memcache_misses)
    
    for name, value in stats_summary.items():
        name = name.replace(".", "_")
        time = value.get("time", 0.0) * 1000
        key  = 'ol.'+name
        openlibrary.core.stats.put(key, time)
    
def format_stats(stats):
    s = " ".join("%s %d %0.03f" % entry for entry in process_stats(stats))
    return '"%s"' %s

labels = {
    "total": "TT",
    "memcache": "MC",
    "infobase": "IB",
    "solr": "SR",
    "archive.org": "IA",
    "couchdb": "CD",
}

def process_stats(stats):
    """Process stats and returns a list of (label, count, time) for each entry.
    
    Entries like "memcache.get" and "memcache.set" will be collapsed into "memcache".
    """
    d = {}
    for name, value in stats.items():
        name = name.split(".")[0]
        
        label = labels.get(name, "OT")
        count = value.get("count", 0)
        time = value.get("time", 0.0)
        
        xcount, xtime = d.get(label, [0, 0])
        d[label] = xcount + count, xtime + time
        
    return [(label, count, time) for label, (count, time) in sorted(d.items())]

def register_filter(name, function):
    global filters
    filters[name] = function
    

def setup():
    """This function is called from the main application startup
    routine to set things up. Right now, it just initialises the stats
    filters"""
    register_filter("all", stats_filters.all)
    register_filter("url", stats_filters.url)
    register_filter("loggedin", stats_filters.loggedin)
    register_filter("not_loggedin", stats_filters.not_loggedin)

########NEW FILE########
__FILENAME__ = status
import web

import socket
import datetime
import subprocess

from infogami import config
from infogami.utils import delegate
from infogami.utils.view import render_template

status_info = {}
feature_flags = {}

class status(delegate.page):
    def GET(self):
        return render_template("status", status_info, feature_flags)

def get_software_version():
    return subprocess.Popen("git rev-parse --short HEAD --".split(), stdout = subprocess.PIPE, stderr = subprocess.STDOUT).stdout.read().strip()    

def get_features_enabled():
    return config.features

def setup():
    "Basic startup status for the server"
    global status_info, feature_flags
    status_info = {"Software version" : get_software_version(),
                   "Host" : socket.gethostname(),
                   "Start time" : datetime.datetime.utcnow()
                   }
    feature_flags = get_features_enabled()


########NEW FILE########
__FILENAME__ = support
import datetime

import web
import logging

from infogami import config
from infogami.utils import delegate
from infogami.utils.view import render_template

from openlibrary.core import support as S
from openlibrary import accounts
from openlibrary.core import stats

support_db = None

logger = logging.getLogger("openlibrary")

class contact(delegate.page):
    def GET(self):
        if not support_db:
            return "The Openlibrary support system is currently offline. Please try again later."
        i = web.input(path=None)
        user = accounts.get_current_user()
        email = user and user.email
        return render_template("support", email=email, url=i.path)

    def POST(self):
        # if not support_db:
        #     return "Couldn't initialise connection to support database"
        form = web.input()
        email = form.get("email", "")
        topic = form.get("topic", "")
        description = form.get("question", "")
        url = form.get("url", "")
        user = accounts.get_current_user()
        useragent = web.ctx.env.get("HTTP_USER_AGENT","")
        if not all([email, topic, description]):
            return ""

        default_assignees = config.get("support_default_assignees",{})
        topic_key = str(topic.replace(" ","_").lower())
        if topic_key in default_assignees:
            # This is set to False to prevent cases from being created
            # even if there is a designated assignee. This prevents
            # the database from being updated.
            create_case = False 
            assignee = default_assignees.get(topic_key)
        else:
            create_case = False
            assignee = default_assignees.get("default", "mary@openlibrary.org")
        if create_case:
            c = support_db.create_case(creator_name      = user and user.get_name() or "",
                                       creator_email     = email,
                                       creator_useragent = useragent,
                                       creator_username  = user and user.get_username() or "",
                                       subject           = topic,
                                       description       = description,
                                       url               = url,
                                       assignee          = assignee)
            stats.increment("support.all")
        else:
            stats.increment("support.all")
            subject = "Support case *%s*"%topic

            url = web.ctx.home + url
            displayname = user and user.get_name() or ""
            username = user and user.get_username() or ""

            message = SUPPORT_EMAIL_TEMPLATE % locals()
            sendmail(email, assignee, subject, message)
        return render_template("email/case_created", assignee)

def sendmail(from_address, to_address, subject, message):
    if config.get('dummy_sendmail'):
        msg = ('' +
            'To: ' + to_address + '\n' +
            'From:' + from_address + '\n' +
            'Subject:' + subject + '\n' +
            '\n' +
            web.safestr(message))

        logger.info("sending email:\n%s", msg)
    else:
        web.sendmail(from_address, to_address, subject, message)

            
SUPPORT_EMAIL_TEMPLATE = """

Description:\n
%(description)s

A new support case has been filed by %(displayname)s <%(email)s>.

Topic: %(topic)s
URL: %(url)s
User-Agent: %(useragent)s
OL-username: %(username)s
"""

def setup():
    global support_db
    try:
        support_db = S.Support()
    except S.DatabaseConnectionError:
        support_db = None



########NEW FILE########
__FILENAME__ = conftest
collect_ignore = ['test_listapi.py']

def pytest_addoption(parser):
    parser.addoption("--server", default=None)
    parser.addoption("--username")
    parser.addoption("--password")

def pytest_configure(config):
    print "pytest_configure", config.getvalue("server")

    if config.getvalue("server"):
        collect_ignore[:] = []


########NEW FILE########
__FILENAME__ = test_borrow_home
from .. import borrow_home

class Test_convert_works_to_editions:
    
    def convert(self, site, works):
        borrow_home.convert_works_to_editions(site, works)
        return works
        
    def test_no_lending_edition(self, mock_site):
        """if there is no lending edition, work should stay the same."""
        work = {
            "key": "/works/OL1W",
            "title": "foo"
        }
        assert self.convert(mock_site, [work]) == [work]
    
    def test_lending_edition(self, mock_site):
        """if there is no lending edition, work should stay the same."""
        mock_site.save({
            "key": "/books/OL1M",
            "ocaid": "foo2010bar",
            "title": "bar",
            "covers": [1234]
        })
        
        work = {
            "key": "/works/OL1W",
            "title": "foo",
            "ia": "foofoo",
            "lending_edition": "OL1M",
            "cover_id": 1111
        }
        assert self.convert(mock_site, [work]) == [{
            "key": "/books/OL1M",
            "title": "bar",
            "lending_edition": "OL1M",
            "ia": "foo2010bar",
            "cover_id": 1234
        }]
    
    def test_edition_with_no_title(self, mock_site):
        """When the editon has no title, work's title should be retained."""
        mock_site.save({
            "key": "/books/OL1M",
            "ocaid": "foofoo"
        })
        work = {
            "key": "/works/OL1W",
            "title": "foo",
            "ia": "foofoo",
            "lending_edition": "OL1M"
        }
        assert self.convert(mock_site, [work]) == [{
            "key": "/books/OL1M",
            "title": "foo",
            "lending_edition": "OL1M",
            "ia": "foofoo",
            "cover_id": None
        }]
        
    def test_edition_with_no_cover(self, mock_site):
        """When the editon has no cover, work's cover should *not* be retained."""
        mock_site.save({
            "key": "/books/OL1M",
            "ocaid": "foofoo"
        })
        work = {
            "key": "/works/OL1W",
            "title": "foo",
            "ia": "foofoo",
            "lending_edition": "OL1M",
            "cover_id": 1234
        }
        assert self.convert(mock_site, [work]) == [{
            "key": "/books/OL1M",
            "title": "foo",
            "lending_edition": "OL1M",
            "ia": "foofoo",
            "cover_id": None
        }]
        
        
########NEW FILE########
__FILENAME__ = test_doctest
import doctest

def test_doctest():
    modules = [
        "openlibrary.plugins.openlibrary.processors"
    ]
    for name in modules:
        mod = __import__(name, None, None, ['x'])
        yield do_doctest, mod

def do_doctest(mod):
    doctest.testmod(mod)
        


########NEW FILE########
__FILENAME__ = test_home
import datetime
import web
import sys

from infogami.utils.view import render_template
from infogami.utils import template, context
from openlibrary.i18n import gettext
from openlibrary.core.admin import Stats
from BeautifulSoup import BeautifulSoup

from openlibrary.plugins.openlibrary import home

def pytest_funcarg__olconfig(request):
    from infogami import config
    import copy
    
    def safecopy(data):
        if isinstance(data, list):
            return [safecopy(d) for d in data]
        elif isinstance(data, web.storage):
            return web.storage((k, safecopy(v)) for k, v in data.items())
        elif isinstance(data, dict):
            return dict((k, safecopy(v)) for k, v in data.items())
        else:
            return data
    
    old_config = safecopy(config.__dict__)
    
    def undo():
        config.__dict__.clear()
        config.__dict__.update(old_config)
    
    request.addfinalizer(undo)
    return config.__dict__

class MockDoc(dict):
    def __init__(self, _id, *largs, **kargs):
        self.id = _id
        super(MockDoc,self).__init__(*largs, **kargs)

    def __repr__(self):
        o = super(MockDoc, self).__repr__()
        return "<%s - %s>"%(self.id, o)
        

class TestHomeTemplates:
    def test_about_template(self, render_template):
        html = unicode(render_template("home/about"))
        assert "About the Project" in html
    
        blog = BeautifulSoup(html).find("ul", {"id": "olBlog"})
        assert blog is not None
        assert len(blog.findAll("li")) == 0
        
        posts = [web.storage({
            "title": "Blog-post-0",
            "link": "http://blog.openlibrary.org/2011/01/01/blog-post-0",
            "pubdate": datetime.datetime(2011, 01, 01)
        })]
        html = unicode(render_template("home/about", blog_posts=posts))
        assert "About the Project" in html
        assert "Blog-post-0" in html
        assert "http://blog.openlibrary.org/2011/01/01/blog-post-0" in html

        blog = BeautifulSoup(html).find("ul", {"id": "olBlog"})
        assert blog is not None
        assert len(blog.findAll("li")) == 1
        
    def test_stats_template(self, render_template):
        # Make sure that it works fine without any input (skipping section)
        html = unicode(render_template("home/stats"))
        assert html == ""
        
    def test_read_template(self, render_template):
        # getting read-online books fails because solr is not defined.
        # Empty list should be returned when there is error.
        html = unicode(render_template("home/read"))
        assert html.strip() == ""
        
    def test_lending_template(self, render_template, mock_site, olconfig):
        html = unicode(render_template("home/lendinglibrary"))
        assert html.strip() == ""
        
        mock_site.quicksave("/people/foo/lists/OL1L", "/type/list")
        olconfig.setdefault("home", {})['lending_list'] = "/people/foo/lists/OL1L"

        html = unicode(render_template("home/lendinglibrary", "/people/foo/lists/OL1L"))
        assert "Lending Library" in html

    def test_home_template(self, render_template, mock_site, olconfig, monkeypatch):
        docs = [MockDoc(_id = datetime.datetime.now().strftime("counts-%Y-%m-%d"),
                        human_edits = 1, bot_edits = 1, lists = 1,
                        visitors = 1, loans = 1, members = 1,
                        works = 1, editions = 1, ebooks = 1,
                        covers = 1, authors = 1, subjects = 1)]* 100
        stats = dict(human_edits = Stats(docs, "human_edits", "human_edits"),
                     bot_edits   = Stats(docs, "bot_edits", "bot_edits"),
                     lists       = Stats(docs, "lists", "total_lists"),
                     visitors    = Stats(docs, "visitors", "visitors"),
                     loans       = Stats(docs, "loans", "loans"),
                     members     = Stats(docs, "members", "total_members"),
                     works       = Stats(docs, "works", "total_works"),
                     editions    = Stats(docs, "editions", "total_editions"),
                     ebooks      = Stats(docs, "ebooks", "total_ebooks"),
                     covers      = Stats(docs, "covers", "total_covers"),
                     authors     = Stats(docs, "authors", "total_authors"),
                     subjects    = Stats(docs, "subjects", "total_subjects"))
                     
        mock_site.quicksave("/people/foo/lists/OL1L", "/type/list")
        olconfig.setdefault("home", {})['lending_list'] = "/people/foo/lists/OL1L"
        
        monkeypatch.setattr(home, "get_returncart", lambda limit: [])
                     
        html = unicode(render_template("home/index", 
            stats=stats, 
            lending_list="/people/foo/lists/OL1L"))
        assert '<div class="homeSplash"' in html
        #assert "Books to Read" in html
        assert "Return Cart" in html
        assert "Around the Library" in html
        assert "About the Project" in html

class TestCarouselItem:
    def setup_method(self, m):
        context.context.features = []
        
    def render(self, book):
        # Anand: sorry for the hack.
        print sys._getframe(1)
        render_template = sys._getframe(1).f_locals['render_template']
        
        if "authors" in book:
            book["authors"] = [web.storage(a) for a in book['authors']]
        
        return unicode(render_template("books/carousel_item", web.storage(book)))
        
    def link_count(self, html):
        links = BeautifulSoup(html).findAll("a") or []
        return len(links)
    
    def test_with_cover_url(self, render_template):
        book = {
            "url": "/books/OL1M",
            "title": "The Great Book",
            "authors": [{"key": "/authors/OL1A", "name": "Some Author"}],
            "cover_url": "http://covers.openlibrary.org/b/id/1-M.jpg"
        }
        assert book['title'] in self.render(book)
        assert book['cover_url'] in self.render(book)
        assert self.link_count(self.render(book)) == 1

    def test_without_cover_url(self, render_template):
        book = {
            "url": "/books/OL1M",
            "title": "The Great Book",
            "authors": [{"key": "/authors/OL1A", "name": "Some Author"}],
        }
        assert book['title'] in self.render(book)
        assert self.link_count(self.render(book)) == 1
        
        del book['authors']
        assert book['title'] in self.render(book)
        assert self.link_count(self.render(book)) == 1
        
    def test_urls(self, render_template):
        book = {
            "url": "/books/OL1M",
            "title": "The Great Book",
            "authors": [{"key": "/authors/OL1A", "name": "Some Author"}],
            "cover_url": "http://covers.openlibrary.org/b/id/1-M.jpg",
            "read_url": "http://www.archive.org/stream/foo",
            "borrow_url": "/books/OL1M/foo/borrow",
            "daisy_url": "/books/OL1M/foo/daisy",
            "overdrive_url": "http://overdrive.com/foo",
        }
        
        # Remove urls on order and make sure the template obeys the expected priority
        assert 'Read online' in self.render(book)
        assert book['read_url'] in self.render(book)
        assert self.link_count(self.render(book)) == 2        
        
        del book['read_url']
        assert 'Borrow this book' in self.render(book)
        assert book['borrow_url'] in self.render(book)
        assert self.link_count(self.render(book)) == 2

        del book['borrow_url']
        assert 'DAISY' in self.render(book)
        assert book['daisy_url'] in self.render(book)
        assert self.link_count(self.render(book)) == 2

        del book['daisy_url']
        assert 'Borrow this book' in self.render(book)
        assert book['overdrive_url'] in self.render(book)
        assert self.link_count(self.render(book)) == 2

        del book['overdrive_url']
        assert self.link_count(self.render(book)) == 1
        
    def test_inlibrary(self, monkeypatch, render_template):
        book = {
            "url": "/books/OL1M",
            "title": "The Great Book",
            "authors": [{"key": "/authors/OL1A", "name": "Some Author"}],
            "cover_url": "http://covers.openlibrary.org/b/id/1-M.jpg",
            "inlibrary_borrow_url": "/books/OL1M/foo/borrow-inlibrary",
        }
        
        assert book['inlibrary_borrow_url'] not in self.render(book)
        assert self.link_count(self.render(book)) == 1
        
        g = web.template.Template.globals
        monkeypatch.setattr(web.template.Template, "globals", dict(g, get_library=lambda: {"name": "IA"}))
        monkeypatch.setattr(context.context, "features", ["inlibrary"], raising=False)

        assert book['inlibrary_borrow_url'] in self.render(book)
        assert self.link_count(self.render(book)) == 2
        
class Test_carousel:
    def test_carousel(self, render_template):
        book = web.storage({
            "url": "/books/OL1M",
            "title": "The Great Book",
            "authors": [web.storage({"key": "/authors/OL1A", "name": "Some Author"})],
            "cover_url": "http://covers.openlibrary.org/b/id/1-M.jpg"
        })
        html = unicode(render_template("books/carousel", [book]))
        
        assert book['title'] in html
        assert book['cover_url'] in html
        
        soup = BeautifulSoup(html)
        assert len(soup.findAll("li")) == 1
        assert len(soup.findAll("a")) == 1

class Test_format_book_data:        
    def test_all(self, mock_site, mock_ia):
        book = mock_site.quicksave("/books/OL1M", "/type/edition", title="Foo")
        work = mock_site.quicksave("/works/OL1W", "/type/work", title="Foo")
                
    def test_cover_url(self, mock_site, mock_ia):
        book = mock_site.quicksave("/books/OL1M", "/type/edition", title="Foo")
        assert home.format_book_data(book).get("cover_url") is None
        
        book = mock_site.quicksave("/books/OL1M", "/type/edition", title="Foo", covers=[1, 2])
        assert home.format_book_data(book).get("cover_url") == "http://covers.openlibrary.org/b/id/1-M.jpg"
        
    def test_authors(self, mock_site, mock_ia):
        a1 = mock_site.quicksave("/authors/OL1A", "/type/author", name="A1")
        a2 = mock_site.quicksave("/authors/OL2A", "/type/author", name="A2")
        work = mock_site.quicksave("/works/OL1W", "/type/work", title="Foo", authors=[{"author": {"key": "/authors/OL2A"}}])
        
        book = mock_site.quicksave("/books/OL1M", "/type/edition", title="Foo")
        assert home.format_book_data(book)['authors'] == []
        
        # when there is no work and authors, the authors field must be picked from the book
        book = mock_site.quicksave("/books/OL1M", "/type/edition", title="Foo", authors=[{"key": "/authors/OL1A"}])
        assert home.format_book_data(book)['authors'] == [{"key": "/authors/OL1A", "name": "A1"}]
        
        # when there is work, the authors field must be picked from the work
        book = mock_site.quicksave("/books/OL1M", "/type/edition", 
            title="Foo", 
            authors=[{"key": "/authors/OL1A"}], 
            works=[{"key": "/works/OL1W"}]
        )
        assert home.format_book_data(book)['authors'] == [{"key": "/authors/OL2A", "name": "A2"}]
########NEW FILE########
__FILENAME__ = test_listapi
from py.test import config
import web
import simplejson

import urllib, urllib2
import cookielib

def pytest_funcarg__config(request):
    return request.config
        
class ListAPI:
    def __init__(self, config):
        self.server = config.getvalue('server')
        self.username = config.getvalue("username")
        self.password = config.getvalue("password")
        
        self.cookiejar = cookielib.CookieJar()

        self.opener = urllib2.build_opener()
        self.opener.add_handler(
            urllib2.HTTPCookieProcessor(self.cookiejar))
        
    def urlopen(self, path, data=None, method=None, headers={}):
        """url open with cookie support."""
        if not method:
            if data:
                method = "POST"
            else:
                method = "GET"
        
        req = urllib2.Request(self.server + path, data=data, headers=headers)
        req.get_method = lambda: method
        return self.opener.open(req)
        
    def login(self):
        data = dict(username=self.username, password=self.password)
        self.urlopen("/account/login", data=urllib.urlencode(data), method="POST")
        print self.cookiejar
    
    def create_list(self, data):
        json = simplejson.dumps(data)
        headers = {
            "content-type": "application/json"
        }
        response = self.urlopen(
            "/people/" + self.username + "/lists",
            data=json, 
            headers=headers)
        return simplejson.loads(response.read())
        
    def get_lists(self):
        data = self.urlopen("/people/" + self.username + "/lists.json").read()
        return simplejson.loads(data)
        
    def get_list(self, key):
        data = self.urlopen(key + ".json").read()
        return simplejson.loads(data)
        
    def get_seeds(self, key):
        data = self.urlopen(key + "/seeds.json").read()
        return simplejson.loads(data)
        
    def update_seeds(self, key, additions, removals):
        data = {
            "add": additions, 
            "remove": removals,
        }
        json = simplejson.dumps(data)
        response = self.urlopen(key + "/seeds.json", json)
        return simplejson.loads(response.read())
        
def test_create(config):
    api = ListAPI(config)
    api.login()

    data = {
        "name": "foo", 
        "description": "foo bar",
        "tags": ["t1", "t2"],
        "seeds": ["subject:cheese"]
    }
    result = api.create_list(data)
    assert "key" in result and result['revision'] == 1
    list_key = result['key']

    # test get
    list = api.get_list(list_key) 
    for k in ["created", "last_modified"]:
        list.pop(k)
    
    assert list == {
        "key": result['key'],
        "type": {"key": "/type/list"},
        "revision": 1,
        "latest_revision": 1,
        
        "name": "foo",
        "description": {
            "type": "/type/text",
            "value": "foo bar"
        },
        "tags": ["t1", "t2"],
        "seeds": ["subject:cheese"]
    }
    
    # test get seeds
    assert api.get_seeds(list_key) == ["subject:cheese"]

def test_add_seeds(config):
    api = ListAPI(config)
    api.login()

    data = {
        "name": "foo", 
        "description": "foo bar",
        "tags": ["t1", "t2"],
        "seeds": ["subject:cheese"]
    }
    result = api.create_list(data)
    key = result['key']
    
    # remove cheese and add apple
    api.update_seeds(key, ["subject:apple"], ["subject:cheese"])
    assert api.get_seeds(key) == ["subject:apple"]    
    

def test_lists(config):
    api = ListAPI(config)
    api.login()
    
    count = api.get_lists()['list_count']

    api.create_list({"name": "foo"})
    
    new_count = api.get_lists()['list_count']
    # counts are not accurate yet.
    #assert new_count == count + 1

########NEW FILE########
__FILENAME__ = test_lists
from openlibrary.plugins.openlibrary import lists

def test_process_seeds():
    process_seeds = lists.lists_json().process_seeds
    def f(s):
        return process_seeds([s])[0]
    
    assert f("/books/OL1M") == {"key": "/books/OL1M"}
    assert f({"key": "/books/OL1M"}) == {"key": "/books/OL1M"}
    assert f("/subjects/love") == "subject:love"
    assert f("subject:love") == "subject:love"
    
########NEW FILE########
__FILENAME__ = test_stats
"""
Tests the stats gathering systems. 
"""
import calendar
import datetime

from .. import stats
from openlibrary.core.admin import Stats

class MockDoc(dict):
    def __init__(self, _id, *largs, **kargs):
        self.id = _id
        super(MockDoc,self).__init__(*largs, **kargs)

    def __repr__(self):
        o = super(MockDoc, self).__repr__()
        return "<%s - %s>"%(self.id, o)

def test_format_stats_entry():
    "Tests the stats performance entries"
    stats.process_stats({"total": {"time": 0.1}}) == [("TT", 0, 0.1)]
    stats.process_stats({"total": {"time": 0.1346}}) == [("TT", 0, 0.135)]
    
    stats.process_stats({"memcache": {"count": 2, "time": 0.1}}) == [("MC", 2, 0.100)]    
    stats.process_stats({"infobase": {"count": 2, "time": 0.1}}) == [("IB", 2, 0.100)]
    stats.process_stats({"couchdb": {"count": 2, "time": 0.1}}) == [("CD", 2, 0.100)]
    stats.process_stats({"solr": {"count": 2, "time": 0.1}}) == [("SR", 2, 0.100)]
    stats.process_stats({"archive.org": {"count": 2, "time": 0.1}}) == [("IA", 2, 0.100)]
    stats.process_stats({"something-else": {"count": 2, "time": 0.1}}) == [("OT", 2, 0.100)]

def test_format_stats():
    "Tests whether the performance status are output properly in the the X-OL-Stats header"
    stats.format_stats({"total": {"time": 0.2}, "infobase": {"count": 2, "time": 0.13}}) == '"IB 2 0.130 TT 0 0.200"'
    
def test_stats_container():
    "Tests the Stats container used in the templates"
    # Test basic API and null total count
    ipdata = [{"foo":1}]*100
    s = Stats(ipdata, "foo", "nothing")
    expected_op = [(x, 1) for x in range(0, 140, 5)]
    assert s.get_counts() == expected_op
    assert s.get_summary() == 28
    assert s.total == ""
    
def test_status_total():
    "Tests the total attribute of the stats container used in the templates"
    ipdata = [{"foo":1, "total": x*2} for x in range(1,100)]
    s = Stats(ipdata, "foo", "total")
    assert s.total == 198
    # Test a total before the last
    ipdata = [{"foo":1, "total": x*2} for x in range(1,100)]
    for i in range(90,99):
        del ipdata[i]["total"]
        ipdata[90]["total"] = 2
    s = Stats(ipdata, "foo", "total")
    assert s.total == 2

def test_status_timerange():
    "Tests the stats container with a time X-axis"
    d = datetime.datetime.now().replace(hour = 0, minute = 0, second = 0, microsecond = 0)
    ipdata = []
    expected_op = []
    for i in range(10):
        doc = MockDoc(_id = d.strftime("counts-%Y-%m-%d"), foo = 1)
        ipdata.append(doc)
        expected_op.append([calendar.timegm(d.timetuple()) * 1000, 1])
        d += datetime.timedelta(days = 1)
    s = Stats(ipdata, "foo", "nothing")
    assert s.get_counts(10, True) == expected_op[:10]
    


########NEW FILE########
__FILENAME__ = utils
from openlibrary.core.helpers import sanitize

########NEW FILE########
__FILENAME__ = code

# from infogami.core.forms import register
# from infogami import config
#
# import recaptcha
#
# if config.get('plugin_recaptcha') is not None:
#     public_key = config.plugin_recaptcha.public_key
#     private_key = config.plugin_recaptcha.private_key
# else:
#     public_key = config.recaptcha_public_key
#     private_key = config.recaptcha_private_key
#
# register.inputs = list(register.inputs)
# register.inputs.append(recaptcha.Recaptcha(public_key, private_key))

########NEW FILE########
__FILENAME__ = recaptcha
"""Recapcha Input to use in web.py forms."""

import web
import urllib

_recaptcha_html = """
<script type="text/javascript">
var RecaptchaOptions = {
    theme : 'clean'
};
</script>
<script type="text/javascript"
   src="//www.google.com/recaptcha/api/challenge?k=KEY">
</script>

<noscript>
   <iframe src="//www.google.com/recaptcha/api/noscript?k=KEY"
       height="300" width="500" frameborder="0"></iframe><br>
   <textarea name="recaptcha_challenge_field" rows="3" cols="40">
   </textarea>
   <input type="hidden" name="recaptcha_response_field"
       value="manual_challenge">
</noscript>
"""

class Recaptcha(web.form.Input):
    def __init__(self, public_key, private_key):
        self.public_key = public_key
        self._private_key = private_key
        validator = web.form.Validator('Recaptcha failed', self.validate)

        web.form.Input.__init__(self, 'recaptcha', validator)
        self.description = ''
        self.help = ''

        self.error = None

    def render(self):
        if self.error:
            key = self.public_key + '&error=' + self.error
        else:
            key = self.public_key
        return _recaptcha_html.replace('KEY', key)

    def validate(self, value=None):
        i = web.input(recaptcha_challenge_field="", recaptcha_response_field="")

        data = dict(
            privatekey=self._private_key,
            challenge=i.recaptcha_challenge_field,
            response=i.recaptcha_response_field,
            remoteip=web.ctx.ip)

        response = urllib.urlopen('http://www.google.com/recaptcha/api/verify', urllib.urlencode(data)).read()
        if '\n' in response:
            success, error = response.split('\n', 1)
            if success.lower() != 'true':
                self.error = error.strip()
                return False
            else:
                return True
        else:
            return False


########NEW FILE########
__FILENAME__ = code
import web
import datetime

from infogami.utils import delegate
from infogami.utils.view import render, require_login, public, permission_denied, safeint
from infogami import config

from openlibrary import accounts


@public
def get_scan_status(key):
    record = get_scan_record(key)
    return record and record.scan_status

def get_email(user):
    try:
        delegate.admin_login()
        return web.utf8(web.ctx.site.get_user_email(user.key).email)
    finally:
        web.ctx.headers = []

def get_scan_record(key):
    key = "/scan_record" + key
    record = web.ctx.site.get(key)
    if record and record.type and record.type.key == '/type/scan_record':
        return record
    else:
        return None

def error(message):
    raise web.HTTPError("200 OK", {}, "ERROR: " + message)

def get_book(path, check_scanned=True, check_ocaid=True):
    page = web.ctx.site.get(path)
    if not page:
        error('Invalid book: ' + page)
    elif page.type.key != '/type/edition':
        error(path + ' is not a book')
    elif check_ocaid and page.ocaid:
        error('This book is already scanned')
    elif check_scanned and get_scan_status(path) != 'NOT_SCANNED':
        error(path + ' is not scannable.')
    else:    
        return page
        
class scan_confirm(delegate.mode):
    def GET(self, path):
        book = get_book(path)
        print render.scan_confirm(book)

class scan_login(delegate.mode):
    def GET(self, path):
        book = get_book(path)
        print render.scan_login(book)
        
class scan_review(delegate.mode):
    @require_login
    def GET(self, path):
        book = get_book(path)
        scanning_center = get_scan_record(path).locations[0].name
        return render.scan_review(book, scanning_center)
    
    @require_login
    def POST(self, path):
        book = get_book(path)
        record = get_scan_record(path)
        user = accounts.get_current_user()
        delegate.admin_login()
        q = {
            'key': '/scan_record' + path,
            'scan_status': {
                'connect': 'update',
                'value': 'WAITING_FOR_BOOK'
            },
            'sponsor': {
                'connect': 'update',
                'key': user.key
            },
            'request_date': {
                'connect': 'update',
                'value': datetime.datetime.utcnow().isoformat()
            }
        }
        try:
            web.ctx.site.write(q)
        finally:
            web.ctx.headers = []

        def get_to():
            if config.get('plugin_scod') is not None:
                return config.plugin_scod.get('email_recipients', [])
            else:
                return config.get('scan_email_recipients', [])
        
        to = get_to()
        if to:
            scan_record = get_scan_record(path)
            message = render.scan_request_email(book, scan_record)
            web.sendmail(config.from_address, to, message.subject.strip(), message)

        to = get_email(user)
        message = render.scan_waiting_email(book, scan_record)
        web.sendmail(config.from_address, to, message.subject.strip(), message)
        return render.scan_inprogress(book)
        
class scan_book_notfound(delegate.mode):
    def is_scan_user(self):
        usergroup = web.ctx.site.get('/usergroup/scan')
        user = accounts.get_current_user()
        return user and usergroup and user.key in (m.key for m in usergroup.members)

    def POST(self, path):
        if not self.is_scan_user():
            return permission_denied('Permission denied.')

        book = web.ctx.site.get(path)
        i = web.input("scan_status", _comment=None)
        
        q = {
            'key': '/scan_record' + path,
            'scan_status': {
                'connect': 'update',
                'value': i.scan_status
            }
        }
        web.ctx.site.write(q, i._comment)

        def get_email(user):
            try:
                delegate.admin_login()
                return web.utf8(web.ctx.site.get_user_email(user.key).email)
            finally:
                web.ctx.headers = []

        scan_record = get_scan_record(path)
        to = scan_record.sponsor and get_email(scan_record.sponsor)
        cc = getattr(config, 'scan_email_recipients', [])
        if to:
            if i.scan_status == 'SCAN_IN_PROGRESS':
                message = render.scan_inprogress_email(book, scan_record, i._comment)
            else:    
                message = render.scan_book_notfound_email(book, scan_record, i._comment)
            web.sendmail(config.from_address, to, message.subject.strip(), str(message), cc=cc)
        raise web.seeother(web.changequery(query={}))
        
class scan_inprogress(scan_book_notfound):
    pass

class scan_complete(delegate.mode):
    def is_scan_user(self):
        usergroup = web.ctx.site.get('/usergroup/scan')
        user = accounts.get_current_user()
        return user and usergroup and user.key in (m.key for m in usergroup.members)

    def GET(self, path):
        if not self.is_scan_user():
            return permission_denied('Permission denied.')

        book = get_book(path, check_scanned=False, check_ocaid=False)
        return render.scan_complete(book)
        
    def POST(self, path):
        if not self.is_scan_user():
            return permission_denied('Permission denied.')

        book = get_book(path, check_scanned=False, check_ocaid=False)
        i = web.input("ocaid", volumes=None, multivolume_work=None, _comment=None)
        
        q = [
            {
                'key': path,
                'ocaid': {
                    'connect': 'update',
                    'value': i.ocaid
                }
            },
            {
                'key': '/scan_record' + path,
                'scan_status': {
                    'connect': 'update',
                    'value': 'SCAN_COMPLETE'
                },
                'completion_date': {
                    'connect': 'update',
                    'value': datetime.datetime.utcnow().isoformat()
                }
            }
        ]

        if i.multivolume_work:
            def volume(index, ia_id):
                return {
                    'type': {'key': '/type/volume'},
                    'volume_number': index,
                    'ia_id': ia_id
                }
                    
            volumes = i.volumes and i.volumes.split() or []
            q[0]['volumes'] = {
                'connect': 'update_list',
                'value': [volume(index+1, v) for index, v in enumerate(volumes)]
            }
            q[0]['ocaid'] = {
                'connect': 'update',
                'value': ''
            }

        web.ctx.site.write(q, i._comment)

        scan_record = get_scan_record(path)
        to = scan_record.sponsor and get_email(scan_record.sponsor)
        cc = getattr(config, 'scan_email_recipients', [])
        if to:
            message = render.scan_complete_email(book, scan_record, i._comment)
            web.sendmail(config.from_address, to, message.subject.strip(), str(message), cc=cc)

        raise web.seeother(web.changequery(query={}))

def get_scan_queue(scan_status, limit=None, offset=None):
    q = {
        'type': '/type/scan_record',
        'scan_status': scan_status,
        'sort': '-last_modified'
    } 
    if limit:
        q['limit'] = limit
    
    if offset:
        q['offset'] = offset
        
    keys = web.ctx.site.things(q)
    result = web.ctx.site.get_many(keys)

    # prefetch editions
    web.ctx.site.get_many([web.lstrips(key, '/scan_record') for key in keys])

    # prefetch scanning centers
    scanning_centers = set(r.locations[0].key for r in result if r.locations)
    web.ctx.site.get_many(list(scanning_centers))

    return result

@public
def to_datetime(iso_date_string):
    """
        >>> t = '2008-01-01T01:01:01.010101'
        >>> to_timestamp(t).isoformat()
        '2008-01-01T01:01:01.010101'
    """
    if not iso_date_string:
        return None

    try:
        #@@ python datetime module is ugly. 
        #@@ It takes so much of work to create datetime from isoformat.
        date, time = iso_date_string.split('T', 1)
        y, m, d = date.split('-')
        H, M, S = time.split(':')
        S, ms = S.split('.')
        return datetime.datetime(*map(int, [y, m, d, H, M, S, ms]))
    except:
        return None

class scan_queue(delegate.page):
    def GET(self):
        i = web.input(status="WAITING_FOR_BOOK", p=None)
        options = ["NOT_SCANNED", "WAITING_FOR_BOOK", "BOOK_NOT_SCANNED", "SCAN_IN_PROGRESS", "SCAN_COMPLETE"]
        if i.status not in options:
            raise web.seeother(web.changequery({}))
            
        offset = safeint(i.p, 0) * 50
            
        records = get_scan_queue(i.status, limit=50, offset=offset)
        return render.scan_queue(records)

########NEW FILE########
__FILENAME__ = code
from __future__ import with_statement
import web
import stopword
import pdb

from infogami import utils
from infogami.utils import delegate
from infogami.infobase.client import Thing
from infogami.utils import view, template
from infogami import config
from infogami.plugins.api.code import jsonapi

import re, web
import solr_client
import time
import simplejson
from functools import partial
from gzip import open as gzopen
import cPickle
from collections import defaultdict

render = template.render

sconfig = web.storage()
if hasattr(config, 'plugin_search'):
    sconfig = config.plugin_search

sconfig.setdefault('solr', None)
sconfig.setdefault('fulltext_solr', None)
sconfig.setdefault('fulltext_shards', [])

def parse_host(host_and_port):
    """
    >>> print parse_host('alice:1234')
    ('alice', 1234)
    """
    if host_and_port is None:
        return (None, None)
    h,p = host_and_port.split(':')
    return (h, int(p))

solr_server_address = parse_host(sconfig.solr)
solr_fulltext_address = parse_host(sconfig.fulltext_solr)
solr_fulltext_shards = map(parse_host, sconfig.fulltext_shards)

if solr_fulltext_address is not None:
    if hasattr(sconfig, 'solr_pagetext_address'):
        solr_pagetext_address = parse_host(sconfig.solr_pagetext_address)
    else:
        solr_pagetext_address = solr_fulltext_address

if solr_server_address:
    solr = solr_client.Solr_client(solr_server_address)
    solr.query_processor = solr_client.create_query_processor(sconfig.get('query_processor'))
else:
    solr = None

if solr_fulltext_address:
    solr_fulltext = solr_client.Solr_client(solr_fulltext_address,
                                            shards=solr_fulltext_shards)
    solr_pagetext = solr_client.Solr_client(solr_pagetext_address,
                                            shards=solr_fulltext_shards)

def lookup_ocaid(ocaid):
    ocat = web.ctx.site.things(dict(type='/type/edition', ocaid=ocaid))
    assert type(ocat)==list, (ocaid,ocat)
    w = web.ctx.site.get(ocat[0]) if ocat else None
    return w

from collapse import collapse_groups
class fullsearch(delegate.page):
    def POST(self):
        errortext = None
        out = []

        i = web.input(q = None,
                      rows = 20,
                      offset = 0,
                      _unicode=False
                      )

        class Result_nums: pass
        nums = Result_nums()
        timings = Timestamp()

        nums.offset = int(i.get('offset', '0') or 0)
        nums.rows = int(i.get('rows', '0') or 20)
        nums.total_nbr = 0
        q = i.q

        if not q:
            errortext='you need to enter some search terms'
            return render.fullsearch(q, out,
                                     nums,
                                     [], # timings
                                     errortext=errortext)

        try:
            q = re.sub('[\r\n]+', ' ', q).strip()
            nums.total_nbr, results = \
                       solr_fulltext.fulltext_search(q,
                                                     start=nums.offset,
                                                     rows=nums.rows)
            timings.update('fulltext done')
            t_ocaid = 0.0
            for ocaid in results:
                try:
                    pts = solr_pagetext.pagetext_search(ocaid, q)
                    t_temp = time.time()
                    oln_thing = lookup_ocaid(ocaid)
                    t_ocaid += time.time() - t_temp
                    if oln_thing is None:
                        # print >> web.debug, 'No oln_thing found for', ocaid
                        pass
                    else:
                        out.append((oln_thing, ocaid,
                                    collapse_groups(solr_pagetext.pagetext_search
                                                    (ocaid, q))))
                except IndexError, e:
                    print >> web.debug, ('fullsearch index error', e, e.args)
                    pass
            timings.update('pagetext done (oca lookups: %.4f sec)'% t_ocaid)
        except IOError, e:
            errortext = 'fulltext search is temporarily unavailable (%s)' % \
                        str(e)

        return render.fullsearch(q,
                                 out,
                                 nums,
                                 timings.results(),
                                 errortext=errortext)

    GET = POST

import facet_hash
facet_token = view.public(facet_hash.facet_token)

class Timestamp(object):
    def __init__(self):
        self.t0 = time.time()
        self.ts = []
    def update(self, msg):
        self.ts.append((msg, time.time()-self.t0))
    def results(self):
        return (time.ctime(self.t0), self.ts)

# this is in progress, not used yet.
class Timestamp1(object):
    def __init__(self, key=None):
        self.td = defaultdict(float)
        self.last_t = time.time()
        self.key = key
        self.switch(key)
    def switch(self, key):
        t = time.time()
        self.td[self.key] += self.last_t - t
        self.last_t = t
        self.key = key
        
class search(delegate.page):
    def POST(self):
        i = web.input(wtitle='',
                      wauthor='',
                      wtopic='',
                      wisbn='',
                      wpublisher='',
                      wdescription='',
                      psort_order='',
                      pfulltext='',
                      ftokens=[],
                      q='',
                      _unicode=False
                      )
        timings = Timestamp()
        results = []
        qresults = web.storage(begin=0, total_results=0)
        facets = []
        errortext = None

        if solr is None:
            errortext = 'Solr is not configured.'
        if i.q:
            q0 = [clean_punctuation(i.q)]
        else:
            q0 = []
        for formfield, searchfield in \
                (('wtitle', 'title'),
                 ('wauthor', 'authors'),
                 ('wtopic', 'subjects'),
                 ('wisbn', ['isbn_10', 'isbn_13']),
                 ('wpublisher', 'publishers'),
                 ('wdescription', 'description'),
                 ('pfulltext', 'has_fulltext'),
                 ):
            v = clean_punctuation(i.get(formfield))
            if v:
                if type(searchfield) == str:
                    q0.append('%s:(%s)'% (searchfield, v))
                elif type(searchfield) == list:
                    q0.append('(%s)'% \
                              ' OR '.join(('%s:(%s)'%(s,v))
                                          for s in searchfield))
            # @@
            # @@ need to unpack date range field and sort order here
            # @@

        # print >> web.debug, '** i.q=(%s), q0=(%s)'%(i.q, q0)

        # get list of facet tokens by splitting out comma separated
        # tokens, and remove duplicates.  Also remove anything in the
        # initial set `init'.
        def strip_duplicates(seq, init=[]):
            """>>> print strip_duplicates((1,2,3,3,4,9,2,0,3))
            [1, 2, 3, 4, 9, 0]
            >>> print strip_duplicates((1,2,3,3,4,9,2,0,3), [3])
            [1, 2, 4, 9, 0]"""
            fs = set(init)
            return list(t for t in seq if not (t in fs or fs.add(t)))

        # we use multiple tokens fields in the input form so we can support
        # date_range and fulltext_only in advanced search, and can add
        # more like that if needed.
        tokens2 = ','.join(i.ftokens)
        ft_list = strip_duplicates((t for t in tokens2.split(',') if t),
                                   (i.get('remove'),))
        # reassemble ftokens string in case it had duplicates
        i.ftokens = ','.join(ft_list)

        # don't throw a backtrace if there's junk tokens.  Robots have
        # been sending them, so just throw away any invalid ones.
        # assert all(re.match('^[a-z]{5,}$', a) for a in ft_list), \
        #       ('invalid facet token(s) in',ft_list)

        ft_list = filter(partial(re.match, '^[a-z]{5,}$'), ft_list)

        qtokens = ' facet_tokens:(%s)'%(' '.join(ft_list)) if ft_list else ''
        ft_pairs = list((t, solr.facet_token_inverse(t)) for t in ft_list)

        # we have somehow gotten some queries for facet tokens with no
        # inverse.  remove these from the list.
        ft_pairs = filter(lambda (a,b): b, ft_pairs)

        if not q0 and not qtokens:
            errortext = 'You need to enter some search terms.'
            return render.advanced_search(i.get('wtitle',''),
                                          qresults,
                                          results,
                                          [], # works_groups
                                          [], # facets
                                          i.ftokens,
                                          ft_pairs,
                                          [], # timings
                                          errortext=errortext)

        out = []
        works_groups = []
        i.q = ' '.join(q0)
        try:
            # work around bug in PHP module that makes queries
            # containing stopwords come back empty.
            query = stopword.basic_strip_stopwords(i.q.strip()) + qtokens
            bquery = solr.basic_query(query)
            offset = int(i.get('offset', '0') or 0)
            qresults = solr.advanced_search(bquery, start=offset)
            # print >> web.debug,('qresults',qresults.__dict__)
            # qresults = solr.basic_search(query, start=offset)
            timings.update("begin faceting")
            facets = solr.facets(bquery, maxrows=5000)
            timings.update("done faceting")
            # results = munch_qresults(qresults.result_list)
            results = munch_qresults_stored(qresults)
            results = filter(bool, results)
            timings.update("done expanding, %d results"% len(results))

            if 0:
                # temporarily disable computing works, per
                # launchpad bug # 325843
                results, works_groups = collect_works(results)
                print >> web.debug, ('works', results, works_groups)

            timings.update("done finding works, (%d,%d) results"%
                           (len(results), len(works_groups)))

            # print >> web.debug, ('works result',
            #                    timings.ts,
            #                    (len(results),results),
            #                    (len(works_groups),works_groups))

        except (solr_client.SolrError, Exception), e:
            import traceback
            errortext = 'Sorry, there was an error in your search.'
            if i.get('safe')=='false':
                errortext +=  '(%r)' % (e.args,)
                errortext += '<p>' + traceback.format_exc()

        # print >> web.debug, 'basic search: about to advanced search (%r)'% \
        #     list((i.get('q', ''),
        #           qresults,
        #           results,
        #           facets,
        #           i.ftokens,
        #           ft_pairs))

        return render.advanced_search(i.get('q', ''),
                                      qresults,
                                      results,
                                      works_groups,
                                      facets,
                                      i.ftokens,
                                      ft_pairs,
                                      timings.results(),
                                      errortext=errortext)

    GET = POST

def munch_qresults_stored(qresults):
    def mk_author(a,ak):
        class Pseudo_thing(Thing):
            def _get(self, key, revision=None):
                return self
            def __setattr__(self, a, v):
                self.__dict__[a] = v

        authortype = Thing(web.ctx.site,u'/type/author')
        d = Pseudo_thing(web.ctx.site, unicode(ak))
        d.name = a
        d.type = authortype
        # print >> web.debug, ('mk_author made', d)
        # experimentally try db retrieval to compare with our test object
        # dba = web.ctx.site.get(ak)
        # print >> web.debug, ('mk_author db retrieval', dba)
        return d
    def mk_book(d):
        assert type(d)==dict
        d['key'] = d['identifier']
        for x in ['title_prefix', 'ocaid','publish_date',
                  'publishers', 'physical_format']:
            if x not in d:
                d[x] = ''

        def dget(attr):
            a = d.get(attr, [])
            a = [] if a is None else a
            return a
        da, dak = dget('authors'), dget('author_keys')
        # print >> web.debug, ('da,dak',da,dak)
        d['authors'] = list(mk_author(a,k) for a,k in zip(da,dak) if k is not None)
        return web.storage(**d)
    return map(mk_book, qresults.raw_results)
    
def collect_works(result_list):
    wds = defaultdict(list)
    rs = []
    # split result_list into two lists, those editions that have been assigned
    # to a work and those that have not.
    for r in result_list:
        ws = r.get('works')
        if ws:
            for w in ws:
                wds[w['key']].append(r)
        else:
            rs.append(r)

    # print >> web.debug, ('collect works', rs,wds)

    s_works = sorted(wds.items(), key=lambda (a,b): len(b), reverse=True)
    return rs, [(web.ctx.site.get(a), b) for a,b in s_works]


# somehow the leading / got stripped off the book identifiers during some
# part of the search import process.  figure out where that happened and
# fix it later.  for now, just put the slash back.
def restore_slash(book):
    if not book.startswith('/'): return '/'+book
    return book

@view.public
def exact_facet_count(query, selected_facets, facet_name, facet_value):
    t0 = time.time()
    r = solr.exact_facet_count(query, selected_facets,
                               facet_name, facet_value)
    t1 = time.time()-t0
    qfn = (query, facet_name, facet_value)
    print >> web.debug, ('*** efc', qfn, r, t1)
    return r

def get_books(keys):
    """Get all books specified by the keys in a single query and also prefetch all the author records.
    """
    books = web.ctx.site.get_many(keys)

    # prefetch the authors so they will be cached by web.ctx.site for
    # later use.  Avoid trapping in case some author record doesn't
    # have a key, since this seems to happen sometimes.
    author_keys = set(getattr(a, 'key', None)
                      for b in books for a in b.authors)
    author_keys.discard(None)

    # actually retrieve the authors and don't do anything with them.
    # this is just to get them into cache.
    web.ctx.site.get_many(list(author_keys))
    return books

def munch_qresults(qlist):
    raise NotImplementedError   # make sure we're not using this func

    results = []
    rset = set()

    # make a copy of qlist with duplicates removed, but retaining
    # original order
    for res in qlist:
        if res not in rset:
            rset.add(res)
            results.append(res)

    # this is supposed to be faster than calling site.get separately
    # for each result
    return get_books(map(restore_slash, results))

# disable the above function by redefining it as a do-nothing.
# This replaces a version that removed all punctuation from the
# query (see change history, 2008-05-01).  Something a bit smarter
# than this is probably better.

# additionally: if term is 10 or 13 digits with some hyphens,
# treat it as an ISBN and strip the hyphens, since they are
# indexed with no hyphens.  LP #375277.

# hmm, this should be done only for unqualified and isbn-specific
# fields, not other named fields.  test like this for now. @@
def clean_punctuation(s,field=None):
    def clean1(w):
        x = w.lstrip(':')
        # return x                # actually don't compress ISBN for now.
        maybe_isbn = list(c for c in x if c != '-')
        if len(maybe_isbn) in [10,13] and all(c.isdigit() for c in maybe_isbn):
            x = ''.join(maybe_isbn)
        return x
    ws = map(clean1, s.split())
    r = ' '.join(filter(bool,ws))
    return r

class search_api:
    error_val = {'status':'error'}
    def GET(self):
        def format(val, prettyprint=False, callback=None):
            if callback is not None:
                if type(callback) != str or \
                       not re.match('[a-z][a-z0-9\.]*$', callback, re.I):
                    val = self.error_val
                    callback = None

            if prettyprint:
                json = simplejson.dumps(val, indent = 4)
            else:
                json = simplejson.dumps(val)

            if callback is None:
                return json
            else:
                return '%s(%s)'% (callback, json)

        i = web.input(q = None,
                      rows = 20,
                      offset = 0,
                      format = None,
                      callback = None,
                      prettyprint=False,
                      _unicode=False)

        offset = int(i.get('offset', '0') or 0)
        rows = int(i.get('rows', '0') or 20)

        try:
            query = simplejson.loads(i.q).get('query')
        except (ValueError, TypeError):
            return format(self.error_val, i.prettyprint, i.callback)

        dval = dict()

        if type(query) == list:
            qval = list(self._lookup(i, q, offset, rows) for q in query)
            dval["result_list"] = qval
        else:
            dval = self._lookup(i, query, offset, rows)

        return format(dval, i.prettyprint, i.callback)

    def _lookup(self, *args):
        try:
            return self._lookup_1(*args)
        except solr_client.SolrError:
            return self.error_val

    def _lookup_1(self, i, query, offset, rows):
        qresult = query and \
                   solr.basic_search(query.encode('utf8'),
                                     start=offset,
                                     rows=rows
                                     )
        if not qresult:
            result = []
        else:
            result = map(restore_slash, qresult.result_list)

        dval = dict()

        if i.format == "expanded":
            eresult = list(book.dict() for book in munch_qresults(result)
                           if book)
            for e in eresult:
                for a in e.get("authors", []):
                    ak = web.ctx.site.get(a["key"])
                    if ak:
                        akd = ak.dict()
                        if 'books' in akd:
                            del akd['books']
                        a["expanded"] = akd

            dval["expanded_result"] = eresult
        else:
            dval["result"] = result

        dval["status"] = "ok"
        return dval

class SearchProcessor:
    def _process_query(self, query):
        """Process a query dictionary and returns a query string."""
        query = dict(query)
        q = query.pop('q', None)
        
        parts = []
        if q:
            parts.append(self.normalize(q))
                
        for k, v in query.items():
            k = k.lower()
            v = self.normalize(v)
            if k == 'isbn':
                part = '(isbn_10:(%s) OR isbn_13:(%s))' % (v, v)
            else:
                part = '%s:(%s)' % (k, v)
            parts.append(part)
        return " ".join(parts)
        
    def normalize(self, value):
        """Normalize string value by remove unnecessary punctuation."""
        return clean_punctuation(value)
        
    def _solr_query(self, q):
        """Takes a query string and expands it"""
        return solr.basic_query(q)
        
    def _process_doc(self, doc):
        d = {
            'key': doc['identifier'],
            'type': {'key': '/type/edition'},
            'title': doc.get('title', '')
        }
        
        if 'authors' in doc and 'author_keys' in doc:
            d['authors'] = [{'key': key, 'name': name} for key, name in zip(doc['author_keys'], doc['authors'])]
            
        keys = ['title', 'publishers', 'languages', 'subjects']
        for k in keys:
            if k in doc:
                d[k] = doc[k]
                
        return d

    def _process_result(self, result, facets=None):
        out = {
            'matches': result.total_results,
            'docs': [self._process_doc(d) for d in result.raw_results]
        } 
        if facets is not None:
            out['facets'] = facets
        return out
        
    def search(self, query):
        """Constructs solr query from given query dict, executes it and returns the results.

        Sample queries:

            {'q': 'Tom Sawyer'}
            {'authors': 'Mark Twain'}
            {'q': 'Tom Sawyer'}
            {'title': 'Tom Sawyer'}
            {'title': 'Tom Sawyer', 'authors': 'Mark Twain', 'lccn': '49049011'}
            {'title': 'Tom Sawyer', 'authors': 'Mark Twain', 'publisher': '49049011'}
        """
        t1 = time.time()

        query = dict(query)
        offset = query.pop('offset', 0)
        try:
            limit = int(query.pop('limit', 20))
        except ValueError:
            limit = 20
            
        if limit > 1000:
            limit = 1000
        
        facets = str(query.pop('facets', 'false')).lower() == 'true'

        query_string = self._process_query(query)
        solr_query = self._solr_query(query_string)

        if facets:
            # what if offset it more than 5000?
            
            # query for 5000 rows and take the required part from the results to avoid another request
            result = solr.advanced_search(solr_query, start=offset, rows=5000)
            facet_counts = solr_client.facet_counts(result.raw_results, solr_client.default_facet_list)
            result.raw_results = result.raw_results[offset:offset+limit]

            d = self._process_result(result, dict(facet_counts))
        else:
            result = solr.advanced_search(solr_query, start=offset, rows=limit)
            d = self._process_result(result)
            
        t2 = time.time()
        d['time_taken'] = t2-t1
        return d
        
class search_json(delegate.page):
    path = "/search"
    encoding = "json"
    
    @jsonapi
    def GET(self):
        i = web.input(q='', query=None, _unicode=False)
        # query can be either specified as json with parameter query or just query parameters
        query = i.pop('query')
        if query:
            query = simplejson.loads(i.query)
        else:
            query = i
        
        result = SearchProcessor().search(i)
        return simplejson.dumps(result)

# add search API if api plugin is enabled.
if 'api' in delegate.get_plugins():
    from infogami.plugins.api import code as api
    api.add_hook('search', search_api)

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = collapse
from itertools import groupby

def collapse_groups(page_numbers):
    """
    Given a list of page numbers, return a list of (pagenum, rdesc) pairs
    corresponding to ranges of consecutive page numbers.  For each range,
    pagenum is the first page number in the range, and rdesc is a string
    describing the range.

    >>> print collapse_groups([3,9,23,4,25,1,7,12,18,11,8,24,10])
    [(1, '1'), (3, '3-4'), (7, '7-12'), (18, '18'), (23, '23-25')]
    """

    # first use groupby to make an iterator whose elts are groups that
    # are consecutive by leaf number.
    # see  http://python.org/doc/lib/itertools-example.html
    # for another example of this construction

    g_iter = groupby(enumerate(sorted(page_numbers)),
                     lambda (i,n): i-n)

    # now flatten that iterator into a list of lists of leaf numbers
    groups = list(list(z for _, z in y) for _, y in g_iter)

    # finally, we'll make text strings for the leaf groups
    return list((g[0], collapse_one_group(g)) for g in groups)

def collapse_one_group(leaf_group):
    """collapse list of leaves into a single string, i.e.
    >>> collapse_one_group([2,3,4])
    '2-4'
    >>> collapse_one_group([85])
    '85'
    >>> # next example used to say 27-9, but we ditched that.
    >>> collapse_one_group([27,28,29])
    '27-29'
    >>> collapse_one_group([238,239,240,241])
    '238-241'
    >>> # don't say 280-5 any more either
    >>> collapse_one_group([280,281,282,283,284,285])
    '280-285'
    """
    # get the first and last leaf numbers in the group
    a,b = leaf_group[0], leaf_group[-1]

    # if there's just one leaf number, convert it, otherwise
    # convert the range.
    if a == b: return '%d'% a
    return '%d-%d' % (a,b)

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = facet_hash
import string
from hashlib import sha1 as mkhash

# choose token length to make collisions unlikely (if there is a
# rare collision once in a while, we tolerate it, it just means
# that users may occasionally see some extra search results.
# don't make it excessively large because the tokens do use index space.
# The probability of a collision is approx.  1 - exp(-k**2 / (2*n)) where
# k = total # of facet tokens (= # of books * avg # of fields)
# n = 26 ** facet_token_length
# so for k = 10**8 and facet_token_length = 12,
# this probability is 1 - exp(-1e16/(2*26**12)) = approx 0.05.
# (That's the prob of EVER getting a collision, not the prob. of
# seeing a collision on any particular query).

facet_token_length = 12

# turn v into a str object, by encoding from unicode or numeric
# if necessary.
def coerce_str(v):
    if type(v) == unicode:
        v=v.encode('utf-8')
    v = str(v)    # in case v is a numeric type
    assert type(v) == str,(type(v),v)
    return v

# str, str -> str
def facet_token(field, v):
    token = []
    v = coerce_str(v)
    field = coerce_str(field)

    q = int(mkhash('FT,%s,%s'%(field,v)).hexdigest(), 16)
    for i in xrange(facet_token_length):
        q,r = divmod(q, 26)
        token.append(string.lowercase[r])
    return ''.join(token)

########NEW FILE########
__FILENAME__ = solr_client
#!/usr/bin/python
# from __future__ import with_statement
from urllib import quote_plus, urlopen
from xml.etree.cElementTree import ElementTree
from cStringIO import StringIO
import os, re
from collections import defaultdict
import cgi
import web
import simplejson
from facet_hash import facet_token
import pdb

php_location = "/petabox/setup.inc"

# server_addr = ('pharosdb.us.archive.org', 8983)

# Solr search client; fancier version will have multiple persistent
# connections, etc.

solr_server_addr = ('pharosdb.us.archive.org', 8983)
solr_server_addr = ('h02.us.archive.org', 8993)
# solr_server_addr = ('127.0.0.1', 8983)

default_facet_list = ('has_fulltext', 
                      'authors',
                      'subjects',
                      'facet_year',
                      'language',
                      'language_code',
                      'languages',
                      'publishers',
                      )
                      
class PetaboxQueryProcessor:
    """Utility to expand search query using petabox php script."""
    def __init__(self):
        self.cache = {}
        
    def process(self, query):
        if query in self.cache:
            return self.cache[query]

        # this hex conversion is to defeat attempts at shell or PHP code injection
        # by including escape characters, quotes, etc. in the query.
        qhex = query.encode('hex')

        f = os.popen("""php -r 'require_once("%s");
                                 echo Search::querySolr(pack("H*", "%s"),
                                 false,
                                 array("title"=>100,
                                       # "description"=>0.000,
                                       "authors"=>15,
                                       "subjects"=>10,
                                       "language"=>10,
                                       "text"=>1,
                                       "fulltext"=>1));'""" %
                     (php_location, qhex))
        aq = f.read()
        if aq and aq[0] == '\n':
            raise SolrError, ('invalid response from basic query conversion', aq, php_location)
            
        self.cache[query] = aq
        return aq
        
class SimpleQueryProcessor:
    """Alternate query processor to be used when petabox php script is unavailable. To be used in development.
    
        >>> SimpleQueryProcessor().process("hello")
        '(title:hello^100 OR authors:hello^15 OR subjects:hello^10 OR language:hello^10 OR text:hello^1 OR fulltext:hello^1)'
        >>> SimpleQueryProcessor().process("hello world") #doctest: +NORMALIZE_WHITESPACE
        '(title:hello^100 OR authors:hello^15 OR subjects:hello^10 OR language:hello^10 OR text:hello^1 OR fulltext:hello^1)
         (title:world^100 OR authors:world^15 OR subjects:world^10 OR language:world^10 OR text:world^1 OR fulltext:world^1)'
    """
    def process(self, query):
        query = web.utf8(query)
        tokens = query.split(' ')
        return " ".join(self.process_token(t) for t in tokens)
        
    def process_token(self, token):
        return '(title:%s^100 OR authors:%s^15 OR subjects:%s^10 OR language:%s^10 OR text:%s^1 OR fulltext:%s^1)' % (token, token, token, token, token, token)

def create_query_processor(type):
    if type == 'simple':
        return SimpleQueryProcessor()
    else:
        return PetaboxQueryProcessor()

class SolrError(Exception): pass

import traceback                        # @@

def ocaid_to_olid(ocaid):
    return web.ctx.site.things(type='/type/edition',
                               ocaid=ocaid)

class Solr_result(object):
    def __init__(self, result_xml):
        et = ElementTree()
        try:
            w = result_xml.encode('utf-8')
            def tx(a): return (type(a), len(a))
            et.parse(StringIO(w))
        except SyntaxError, e:
            ptb = traceback.extract_stack()
            raise SolrError, (e, result_xml, traceback.format_list(ptb))
        range_info = et.find('info').find('range_info')

        def gn(tagname):
            return int(range_info.findtext(tagname))
        self.total_results = gn('total_nbr')
        self.begin = gn('begin')
        self.end = gn('end')
        self.results_this_page = gn('contained_in_this_set')

        self.result_list = list(str(a.text) \
                                for a in et.getiterator('identifier'))
        
# rewrite of solr result class, to use python or json format result
class SR2(Solr_result):
    def __init__(self, result_json):
        try:
            e = simplejson.loads(result_json)
            # h = e['responseHeader']
            r = e['response']
            self.total_results = r['numFound']
            self.begin = r['start']
            self.end = self.begin + len(r['docs'])
            self.contained_in_this_set = len(r['docs'])
            self.result_list = list(d['identifier'] for d in r['docs'])
            self.raw_results = r['docs']
        except Exception, e:
            ptb = traceback.extract_stack()
            raise SolrError, (e, result_json, traceback.format_list(ptb))
            
# Solr search client; fancier version will have multiple persistent
# connections, etc.
class Solr_client(object):
    def __init__(self,
                 server_addr = solr_server_addr,
                 shards = [],
                 pool_size = 1):
        self.server_addr = server_addr
        self.shards = shards
        
        self.query_processor = PetaboxQueryProcessor()

        # for caching expanded query strings
        self._cache = {} 

    def __query_fmt(self, query, **attribs):
        # rows=None, start=None, wt=None, sort=None):
        fshards = ','.join('%s:%s/solr'%(host,port)
                           for host,port in self.shards)
        ax = list((k,v) for k,v in attribs.items() if v is not None)
        if fshards:
            ax.append(('shards', fshards))

        q = [quote_plus(query)] + ['%s=%s'%(k, quote_plus(str(v))) \
                       for k,v in ax]
        r = '&'.join(q)
        # print >> web.debug, "* query fmt: returning (%r)"% r
        return r
    
    @staticmethod
    def prefix_query(prefix, query):
        if '"' in query:
            query = prefix + ':' + query
        else:
            query = ' '.join(prefix + ':' + x for x in query.split(' '))
        return query

    def _prefix_query(self, prefix, query):
        return Solr_client.prefix_query(prefix, query)

    def facet_token_inverse(self, *a,**k):
        r = self.Xfacet_token_inverse(*a,**k)
        return r

    def Xfacet_token_inverse(self,
                            token,
                            facet_list = default_facet_list):
        # for now, just pull this straight from the SE
        # need to add an LRU cache for performance.  @@

        if not re.match('^[a-z]+$', token):
            raise SolrError, 'invalid facet token'
        m = simplejson.loads(self.raw_search('facet_tokens:%s'% token,
                                             rows=1, wt='json'))
        facet_set = set(facet_list)
        for d in m['response']['docs']:
            for k,vx in d.iteritems():
                kfs = k in facet_set
                # if not kfs: continue
                vvx = {str:(vx,), list:vx}.get(type(vx),())
                for v in map(unicode, vvx):
                    if facet_token(k,v) == token:
                        return (k,v)
        return None

    def isearch(self, query, loc=0):
        # iterator interface to search
        while True:
            s = search(self, query, start=loc)
            if len(s) == 0: return
            loc += len(s)
            for y in s:
                if not y.startswith('OCA/'):
                    yield y
                    
    def search(self, query, **params):
        # advanced search: directly post a Solr search which uses fieldnames etc.        
        # return list of document id's
        assert type(query) == str

        server_url = 'http://%s:%d/solr/select' % self.server_addr
        query_url = '%s?q=%s&wt=json&fl=*'% \
            (server_url, self.__query_fmt(query, **params))

        try:
            ru = urlopen(query_url)
            py = ru.read()
            ru.close()
        except IOError:
            raise SolrError, "Search temporarily unavailable, please try later"
        return SR2(py)

    advanced_search = search

    def fulltext_search(self, query, rows=None, start=None):
        """Does an advanced search on fulltext:blah.
        You get back a pair (x,y) where x is the total # of hits
        and y is a list of identifiers like ["foo", "bar", etc.]"""
        
        query = self._prefix_query('fulltext', query)
        result_list = self.raw_search(query, rows=rows, start=start)
        e = ElementTree()
        try:
            e.parse(StringIO(result_list))
        except SyntaxError, e:
            raise SolrError, e

        total_nbr_text = e.find('info/range_info/total_nbr').text
        # total_nbr_text = e.find('result').get('numFound')  # for raw xml
        total_nbr = int(total_nbr_text) if total_nbr_text else 0

        out = []
        for r in e.getiterator('hit'):
            for d in r.find('metadata'):
                for x in list(d.getiterator()):
                    if x.tag == "identifier":
                        xid = unicode(x.text).encode('utf-8')
                        if xid.startswith('OCA/'):
                            xid = xid[4:]
                        elif xid.endswith('.txt'):
                            xid = xid.split('/')[-1].split('_')[0]
                        elif xid.endswith('_ZZ'):
                            xid = xid[:-3]
                        out.append(xid)
                        break
        return (total_nbr, out)
                    

    def pagetext_search(self, locator, query, rows=None, start=None):
        """Does an advanced search on
               pagetext:blah locator:identifier
        where identifier is one of the id's from fulltext search.
        You get back a list of page numbers like [21, 25, 39]."""
        
        def extract(page_id):
            """A page id is something like
            'adventsuburbanit00butlrich_0065.djvu',
            which this function extracts asa a locator and
            a leaf number ('adventsuburbanit00butlrich', 65). """
            
            g = re.search('(.*)_(\d{4})\.djvu$', page_id)
            a,b = g.group(1,2)
            return a, int(b)
        
        # try using qf= parameter here and see if it gives a speedup. @@
        # pdb.set_trace()
        query = self._prefix_query('pagetext', query)
        page_hits = self.raw_search(query,
                                    fq='locator:' + locator,
                                    rows=rows,
                                    start=start)
        XML = ElementTree()
        try:
            XML.parse(StringIO(page_hits))            
        except SyntaxError, e:
            raise SolrError, e
        page_ids = list(e.text for e in XML.getiterator('identifier'))
        return [extract(x)[1] for x in page_ids]

    def exact_facet_count(self, query, selected_facets,
                          facet_name, facet_value):
        ftoken = facet_token(facet_name, facet_value)


        # this function is temporarily broken because of how facets are handled
        # under dismax.  @@
        # well, ok, the use of dismax is temporarily backed out, but leave
        # this signal here to verify that we're not actually using exact
        # counts right now.
        raise NotImplementedError

        sf = list(s for s in selected_facets if re.match('^[a-z]{12}$', s))
        fs = ' '.join(sf+[ftoken])
        result_json = self.raw_search(
            self.basic_query(query),
            fq='facet_tokens:(%s)'% fs,
            rows=0,
            wt='json')
        result = simplejson.loads(result_json)
        n = result['response']['numFound']
        return n

    def facets(self,
               query,
               facet_list = default_facet_list,
               maxrows=5000):
        """Get facet counts for query.  Todo: statistical faceting."""

        result_set = self.raw_search(query, rows=maxrows, wt='json')

        # TODO: avoid using json here, by instead using xml response format
        # and parsing it with elementtree, if speed is acceptable.  That
        # should reduce memory usage by counting facets incrementally instead
        # of building an in-memory structure for the whole response before
        # counting the facets.
        try:
            # print >> web.debug, '*** parsing result_set=', result_set
            h1 = simplejson.loads(result_set)
        except SyntaxError, e:   # we got a solr stack dump
            # print >> web.debug, '*** syntax error result_set=(%r)'% result_set
            raise SolrError, (e, result_set)

        docs = h1['response']['docs']
        r = facet_counts(docs, facet_list)
        return r

    def raw_search(self, query, **params):
        # raw search: directly post a Solr search which uses fieldnames etc.
        # return the raw xml or json result that comes from solr
        # need to refactor this class to combine some of these methods @@
        assert type(query) == str

        server_url = 'http://%s:%d/solr/select' % self.server_addr
        query_url = '%s?q=%s'% (server_url, self.__query_fmt(query, **params))
        # print >> web.debug, ('raw_search', ((query,params),query_url))
        ru = urlopen(query_url)
        return ru.read()

    # translate a basic query into an advanced query, by launching PHP
    # script, passing query to it, and getting result back.
    def basic_query(self, query):
        return self.query_processor.process(query)

    def basic_search(self, query, **params):
        # basic search: use archive.org PHP script to transform the basic
        # search query into an advanced (i.e. expanded) query.  "Basic" searches
        # can actually use complicated syntax that the PHP script transforms
        # by adding search weights, range expansions, and so forth.
        assert type(query)==str         # not sure what to do with unicode @@

        bquery = self.basic_query(query)
        # print >> web.debug, '* basic search: query=(%r)'% bquery
        return self.advanced_search(bquery, **params)

# get second element of a tuple
def snd((a,b)): return b

def facet_counts(result_list, facet_fields):
    """Return list of facet counts for a search result set.

    The list of field names to fact on is `facet_fields'.
    The result list from solr is `result_list'.  The structures
    look like:
       result_list = [ { fieldname1 : [values...] }, ... ]
       facet_fields = ('author', 'media_type', ...)


    >>> results = [  \
           {'title': ['Julius Caesar'],                         \
            'author': ['William Shakespeare'],                  \
            'format': ['folio'] },                              \
           {'title': ['Richard III'],                           \
            'author': ['William Shakespeare'],                  \
            'format': ['folio'] },                              \
           {'title': ['Tom Sawyer'],                            \
            'author': ['Mark Twain'],                           \
            'format': ['hardcover'] },                          \
           {'title': ['The Space Merchants'],                   \
            'author': ['Frederik Pohl', 'C. M. Kornbluth'],     \
            'format': ['paperback'] },                          \
           ]
    >>> fnames = ('author', 'topic', 'format')
    >>> facet_counts(results, fnames)  #doctest: +NORMALIZE_WHITESPACE
    [('author', [('William Shakespeare', 2),
                 ('C. M. Kornbluth', 1),
                 ('Frederik Pohl', 1),
                 ('Mark Twain', 1)]),
     ('format', [('folio', 2),
                 ('hardcover', 1),
                 ('paperback', 1)])]
    """

    facets = defaultdict(lambda: defaultdict(int))
    for r in result_list:
        for k in set(r.keys()) & set(facet_fields):
            facets_k = facets[k]        # move lookup out of loop for speed
            for x in r[k]:
                facets_k[x] += 1

    return filter(snd, ((f, sorted(facets[f].items(),
                                   key=lambda (a,b): (-b,a)))
                        for f in facet_fields))

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = stopword
def basic_strip_stopwords(q):
    """Strip stopwords from an all-alphabetic query, needed to avoid some bugs
    in the php basic query expander.  do something more sensible later @@
    
    >>> print basic_strip_stopwords('soul of man')
    soul man
    >>> print basic_strip_stopwords('Rubber soul')
    Rubber soul
    >>> print basic_strip_stopwords('title:(soul of man)')
    title:(soul of man)
    """
    
    # standard list of Lucene stopwords, from solr distribution
    stopwords = set("""an and are as at be but by for if in into is it no
    not of on or s such t that the their then there these they this to
    was will with""".split())

    w = q.strip().split()
    if all(all(map(str.isalpha, a)) for a in w):
        return ' '.join(a for a in w if a not in stopwords)
    else:
        return q

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = code
import os
import re
import web
import simplejson
import logging
from collections import defaultdict

from infogami.utils import delegate
from infogami.utils.view import render_template, add_flash_message

from openlibrary import accounts

from .git import Git, CommandError

logger = logging.getLogger("openlibrary.theme")


def admin_only(f):
    def g(*a, **kw):
        user = accounts.get_current_user()
        if user is None or not user.is_admin():
            return render_template("permission_denied",  web.ctx.path, "Permission denied.")
        return f(*a, **kw)
    return g


def find_files(root, filter):
    '''Find all files that pass the filter function in and below
    the root directory.
    '''
    absroot = os.path.abspath(root)
    for path, dirs, files in os.walk(os.path.abspath(root)):
        path = root + web.lstrips(path, absroot)
        
        for file in files:
            f = os.path.join(path, file)
            if filter(f):
                yield f

RE_PATH = re.compile("(/templates/.*.html|/macros/.*.html|/js/.*.js|/css/.*.css)$")
        
def list_files():
    dirs = [
        "openlibrary/plugins/openlibrary", 
        "openlibrary/plugins/upstream", 
        "openlibrary/plugins/admin",
        "openlibrary/plugins/worksearch",
        "openlibrary/plugins/theme",
        "openlibrary/admin", 
        "static"
    ]
        
    files = []
    for d in dirs:
        files += list(find_files(d, RE_PATH.search))    
    return sorted(files)

class index(delegate.page):
    path = "/theme"
    
    def GET(self):
        raise web.seeother("/theme/files")

class file_index(delegate.page):
    path = "/theme/files"

    @admin_only
    def GET(self):
        files = list_files()
        modified = Git().status()
        return render_template("theme/files", files, modified)

class file_view(delegate.page):
    path = "/theme/files/(.+)"

    @admin_only
    def delegate(self, path):
        if not os.path.isfile(path) or not RE_PATH.search(path):
            raise web.seeother("/theme/files#" + path)
            
        i = web.input(_method="GET")
        name = web.ctx.method.upper() + "_" + i.get("m", "view")
        f = getattr(self, name, None)
        if f:
            return f(path)
        else:
            return self.GET_view(path)
            
    GET = POST = delegate
    
    def GET_view(self, path):
        text = open(path).read()
        diff = Git().diff(path)
        return render_template("theme/viewfile", path, text, diff=diff)

    def GET_edit(self, path):
        text = open(path).read()
        return render_template("theme/editfile", path, text)
        
    def POST_edit(self, path):
        i = web.input(_method="POST", text="")
        i.text = i.text.replace("\r\n", "\n").replace("\r", "\n").encode("utf-8")
        f = open(path, 'w')
        f.write(i.text)
        f.close()
        
        logger.info("Saved %s", path)
        
        # run make after editing js or css files
        if not path.endswith(".html"):
            logger.info("Running make")
            cmd = Git().system("make")
            logger.info(cmd.stdout)
        
        add_flash_message("info", "Page has been saved successfully.")
        raise web.seeother(web.ctx.path)
        
    def POST_revert(self, path):
        logger.info("running git checkout %s", path)
        cmd = Git().system("git checkout " + path)
        logger.info(cmd.stdout)
        add_flash_message("info", "All changes to this page have been reverted successfully.")
        raise web.seeother(web.ctx.path)
                
class gitview(delegate.page):
    path = "/theme/modifications"
    
    @admin_only
    def GET(self):
        modified = [f for f in Git().modified() if RE_PATH.search(f.name)]
        return render_template("theme/git", modified)

    @admin_only
    def POST(self):
        i = web.input(files=[], message="")
                
        git = Git()
        commit = git.commit(i.files, author=self.get_author(), message=i.message or "Changes from dev.")
        push = git.push()
        
        return render_template("theme/committed", commit, push)

    def get_author(self):
        user = accounts.get_current_user()
        return "%s <%s>" % (user.displayname, user.get_email())
        
class manage(delegate.page):
    path = "/theme/manage"
    
    @admin_only
    def GET(self):
        return render_template("theme/manage")
        
class gitmerge(delegate.page):
    path = "/theme/git-merge"
    
    @admin_only
    def POST(self):
        git = Git()
        
        d = web.storage(commands=[], success=True)
        
        def run(command):
            if d.success:
                cmd = git.system(command, check_status=False)
                d.commands.append(cmd)
                d.success = (cmd.status == 0)
                
        run("git fetch origin")
        run("git merge origin/master")
        run("git push")
        run("make")
        # Send SIGUP signal to master gunicorn process to reload
        run("kill -HUP %s" % os.getppid())
        return render_template("theme/commands", d.success, d.commands)
    

########NEW FILE########
__FILENAME__ = git
import subprocess
import web

from pygments import highlight
from pygments.lexers import DiffLexer
from pygments.formatters import HtmlFormatter

class CommandError(Exception):
    def __init__(self, status, message):
        self.status = status
        self.message = message
        msg = "Failed with status %s\n" % status + message
        Exception.__init__(self, msg)

class Git:
    """Simple interface to git.
    Expects that the working directory is the root of git repository.
    """
    def __init__(self):
        pass
    
    def add_file(self, path):
        """Adds an empty file to the repository.
        """
        pass
        
    def status(self):
        """Returns the list of modified files.
        """
        out = self.system("git status --short").stdout
        return [line.strip().split()[-1] for line in out.splitlines() if not line.startswith("??")]
        
    def diff(self, path):
        diff = self.system("git diff --ignore-space-at-eol " + path).stdout.strip()
        html = highlight(diff, DiffLexer(), HtmlFormatter(full=True, style="trac", nowrap=True))
        return web.storage(name=path, diff=diff, htmldiff=html)
        
    def system(self, cmd, input=None, check_status=True):
        """Executes the command returns the stdout.
        
        Raises CommandError on non-zero status unless check_status is set to False.
        """
        print >> web.debug, "system", repr(cmd), input
        if input:
            stdin = subprocess.PIPE
        else:
            stdin = None
        p = subprocess.Popen(cmd, shell=True, stdin=stdin, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = p.communicate(input)
        status = p.wait()
        if check_status and status != 0:
            raise CommandError(status, err)
        else:
            return web.storage(cmd=cmd, status=status, stdout=out, stderr=err)
        
    def modified(self):
        return [self.diff(f) for f in self.status()]
        
    def commit(self, files, author, message):
        author = author.replace("'", "")
        cmd = "git commit --author '%s' -F - " % author + " ".join(files)
        return self.system(cmd, input=message)
        
    def push(self):
        return self.system("git push")
        

########NEW FILE########
__FILENAME__ = account
import web
import hmac
import logging
import random
import urllib
import uuid
import datetime, time

from infogami.utils import delegate
from infogami import config
from infogami.utils.view import require_login, render, render_template, add_flash_message
from infogami.infobase.client import ClientException
from infogami.utils.context import context
import infogami.core.code as core

from openlibrary.i18n import gettext as _
from openlibrary.core import helpers as h
from openlibrary import accounts
import forms
import utils
import borrow

from openlibrary.plugins.recaptcha import recaptcha


logger = logging.getLogger("openlibrary.account")

# XXX: These need to be cleaned up
Account = accounts.Account
send_verification_email = accounts.send_verification_email
create_link_doc = accounts.create_link_doc
sendmail = accounts.sendmail

class account(delegate.page):
    """Account preferences.
    """
    @require_login
    def GET(self):
        user = accounts.get_current_user()
        return render.account(user)

class account_create(delegate.page):
    """New account creation.

    Account will in the pending state until the email is activated.
    """
    path = "/account/create"

    def GET(self):
        f = self.get_form()
        return render['account/create'](f)

    def get_form(self):
        f = forms.Register()
        recap = self.get_recap()
        f.has_recaptcha = recap is not None
        if f.has_recaptcha:
            f.inputs = list(f.inputs) + [recap]
        return f

    def get_recap(self):
        if 'recaptcha' in config.get('plugins'):
            public_key = config.plugin_recaptcha.public_key
            private_key = config.plugin_recaptcha.private_key
            return recaptcha.Recaptcha(public_key, private_key)

    def POST(self):
        i = web.input('email', 'password', 'username', agreement="no")
        i.displayname = i.get('displayname') or i.username

        f = self.get_form()
        if not f.validates(i):
            return render['account/create'](f)

        if i.agreement != "yes":
            f.note = utils.get_error("account_create_tos_not_selected")
            return render['account/create'](f)

        try:
            accounts.register(username=i.username,
                              email=i.email,
                              password=i.password,
                              displayname=i.displayname)
        except ClientException, e:
            f.note = str(e)
            return render['account/create'](f)

        send_verification_email(i.username, i.email)
        return render['account/verify'](username=i.username, email=i.email)

del delegate.pages['/account/register']

class account_login(delegate.page):
    """Account login.

    Login can fail because of the following reasons:

    * account_not_found: Error message is displayed.
    * account_bad_password: Error message is displayed with a link to reset password.
    * account_not_verified: Error page is dispalyed with button to "resend verification email".
    """
    path = "/account/login"

    def GET(self):
        referer = web.ctx.env.get('HTTP_REFERER', '/')
        i = web.input(redirect=referer)
        f = forms.Login()
        f['redirect'].value = i.redirect
        return render.login(f)

    def POST(self):
        i = web.input(remember=False, redirect='/', action="login")

        if i.action == "resend_verification_email":
            return self.POST_resend_verification_email(i)
        else:
            return self.POST_login(i)

    def error(self, name, i):
        f = forms.Login()
        f.fill(i)
        f.note = utils.get_error(name)
        return render.login(f)

    def POST_login(self, i):
        # make sure the username is valid
        if not forms.vlogin.valid(i.username):
            return self.error("account_user_notfound", i)

        # Try to find account with exact username, failing which try for case variations.
        account = accounts.find(username=i.username) or accounts.find(lusername=i.username)

        if not account:
            return self.error("account_user_notfound", i)

        if i.redirect == "/account/login" or i.redirect == "":
            i.redirect = "/"

        status = account.login(i.password)
        if status == 'ok':
            expires = (i.remember and 3600*24*7) or ""
            web.setcookie(config.login_cookie_name, web.ctx.conn.get_auth_token(), expires=expires)
            raise web.seeother(i.redirect)
        elif status == "account_not_verified":
            return render_template("account/not_verified", username=account.username, password=i.password, email=account.email)
        elif status == "account_not_found":
            return self.error("account_user_notfound", i)
        elif status == "account_blocked":
            return self.error("account_blocked", i)
        else:
            return self.error("account_incorrect_password", i)

    def POST_resend_verification_email(self, i):
        try:
            accounts.login(i.username, i.password)
        except ClientException, e:
            code = e.get_data().get("code")
            if code != "account_not_verified":
                return self.error("account_incorrect_password", i)

        account = accounts.find(username=i.username)
        account.send_verification_email()

        title = _("Hi %(user)s", user=account.displayname)
        message = _("We've sent the verification email to %(email)s. You'll need to read that and click on the verification link to verify your email.", email=account.email)
        return render.message(title, message)

class account_verify(delegate.page):
    """Verify user account.
    """
    path = "/account/verify/([0-9a-f]*)"

    def GET(self, code):
        docs = web.ctx.site.store.values(type="account-link", name="code", value=code)
        if docs:
            doc = docs[0]

            account = accounts.find(username = doc['username'])
            if account:
                if account['status'] != "pending":
                    return render['account/verify/activated'](account)
            account.activate()
            user = web.ctx.site.get("/people/" + doc['username']) #TBD
            return render['account/verify/success'](account)
        else:
            return render['account/verify/failed']()

    def POST(self, code=None):
        """Called to regenerate account verification code.
        """
        i = web.input(email=None)
        account = accounts.find(email=i.email)
        if not account:
            return render_template("account/verify/failed", email=i.email)
        elif account['status'] != "pending":
            return render['account/verify/activated'](account)
        else:
            account.send_verification_email()
            title = _("Hi %(user)s", user=account.displayname)
            message = _("We've sent the verification email to %(email)s. You'll need to read that and click on the verification link to verify your email.", email=account.email)
            return render.message(title, message)

class account_verify_old(account_verify):
    """Old account verification code.

    This takes username, email and code as url parameters. The new one takes just the code as part of the url.
    """
    path = "/account/verify"
    def GET(self):
        # It is too long since we switched to the new account verification links.
        # All old links must be expired by now.
        # Show failed message without thinking.
        return render['account/verify/failed']()

class account_email(delegate.page):
    """Change email.
    """
    path = "/account/email"

    def get_email(self):
        user = accounts.get_current_user()
        return user.get_account()['email']

    @require_login
    def GET(self):
        f = forms.ChangeEmail()
        return render['account/email'](self.get_email(), f)

    @require_login
    def POST(self):
        f = forms.ChangeEmail()
        i = web.input()

        if not f.validates(i):
            return render['account/email'](self.get_email(), f)
        else:
            user = accounts.get_current_user()
            username = user.key.split('/')[-1]

            displayname = user.displayname or username

            send_email_change_email(username, i.email)

            title = _("Hi %(user)s", user=user.displayname or username)
            message = _("We've sent an email to %(email)s. You'll need to read that and click on the verification link to update your email.", email=i.email)
            return render.message(title, message)

class account_email_verify(delegate.page):
    path = "/account/email/verify/([0-9a-f]*)"

    def GET(self, code):
        link = accounts.get_link(code)
        if link:
            username = link['username']
            email = link['email']
            link.delete()
            return self.update_email(username, email)
        else:
            return self.bad_link()

    def update_email(self, username, email):
        if accounts.find(email=email):
            title = _("Email address is already used.")
            message = _("Your email address couldn't be updated. The specified email address is already used.")
        else:
            logger.info("updated email of %s to %s", username, email)
            accounts.update_account(username=username, email=email, status="active")
            title = _("Email verification successful.")
            message = _('Your email address has been successfully verified and updated in your account.')
        return render.message(title, message)

    def bad_link(self):
        title = _("Email address couldn't be verified.")
        message = _("Your email address couldn't be verified. The verification link seems invalid.")
        return render.message(title, message)

class account_email_verify_old(account_email_verify):
    path = "/account/email/verify"

    def GET(self):
        # It is too long since we switched to the new email verification links.
        # All old links must be expired by now.
        # Show failed message without thinking.
        return self.bad_link()

class account_password(delegate.page):
    path = "/account/password"

    @require_login
    def GET(self):
        f = forms.ChangePassword()
        return render['account/password'](f)

    @require_login
    def POST(self):
        f = forms.ChangePassword()
        i = web.input()

        if not f.validates(i):
            return render['account/password'](f)

        user = accounts.get_current_user()
        username = user.key.split("/")[-1]

        if self.try_login(username, i.password):
            accounts.update_account(username, password=i.new_password)
            add_flash_message('note', _('Your password has been updated successfully.'))
            raise web.seeother('/account')
        else:
            f.note = "Invalid password"
            return render['account/password'](f)

    def try_login(self, username, password):
        account = accounts.find(username=username)
        return account and account.verify_password(password)

class account_password_forgot(delegate.page):
    path = "/account/password/forgot"

    def GET(self):
        f = forms.ForgotPassword()
        return render['account/password/forgot'](f)

    def POST(self):
        i = web.input(email='')

        f = forms.ForgotPassword()

        if not f.validates(i):
            return render['account/password/forgot'](f)

        account = accounts.find(email=i.email)

        if account.is_blocked():
            f.note = utils.get_error("account_blocked")
            return render_template('account/password/forgot', f)

        send_forgot_password_email(account.username, i.email)
        return render['account/password/sent'](i.email)

class account_password_reset(delegate.page):

    path = "/account/password/reset/([0-9a-f]*)"

    def GET(self, code):
        docs = web.ctx.site.store.values(type="account-link", name="code", value=code)
        if not docs:
            title = _("Password reset failed.")
            message = "Your password reset link seems invalid or expired."
            return render.message(title, message)

        f = forms.ResetPassword()
        return render['account/password/reset'](f)

    def POST(self, code):
        link = accounts.get_link(code)
        if not link:
            title = _("Password reset failed.")
            message = "The password reset link seems invalid or expired."
            return render.message(title, message)

        username = link['username']
        i = web.input()

        accounts.update_account(username, password=i.password)
        link.delete()
        return render_template("account/password/reset_success", username=username)

class account_notifications(delegate.page):
    path = "/account/notifications"

    @require_login
    def GET(self):
        user = accounts.get_current_user()
        prefs = web.ctx.site.get(user.key + "/preferences")
        d = (prefs and prefs.get('notifications')) or {}
        email = accounts.get_current_user().email
        return render['account/notifications'](d, email)

    @require_login
    def POST(self):
        user = accounts.get_current_user()
        key = user.key + '/preferences'
        prefs = web.ctx.site.get(key)

        d = (prefs and prefs.dict()) or {'key': key, 'type': {'key': '/type/object'}}

        d['notifications'] = web.input()

        web.ctx.site.save(d, 'save notifications')

        add_flash_message('note', _("Notification preferences have been updated successfully."))
        web.seeother("/account")

class account_loans(delegate.page):
    path = "/account/loans"

    @require_login
    def GET(self):
        user = accounts.get_current_user()
        user.update_loan_status()
        loans = borrow.get_loans(user)
        return render['account/borrow'](user, loans)

class account_others(delegate.page):
    path = "(/account/.*)"

    def GET(self, path):
        return render.notfound(path, create=False)


####


def send_email_change_email(username, email):
    key = "account/%s/email" % username

    doc = create_link_doc(key, username, email)
    web.ctx.site.store[key] = doc

    link = web.ctx.home + "/account/email/verify/" + doc['code']
    msg = render_template("email/email/verify", username=username, email=email, link=link)
    sendmail(email, msg)

def send_forgot_password_email(username, email):
    key = "account/%s/password" % username

    doc = create_link_doc(key, username, email)
    web.ctx.site.store[key] = doc

    link = web.ctx.home + "/account/password/reset/" + doc['code']
    msg = render_template("email/password/reminder", username=username, link=link)
    sendmail(email, msg)




def as_admin(f):
    """Infobase allows some requests only from admin user. This decorator logs in as admin, executes the function and clears the admin credentials."""
    def g(*a, **kw):
        try:
            delegate.admin_login()
            return f(*a, **kw)
        finally:
            web.ctx.headers = []
    return g

########NEW FILE########
__FILENAME__ = acs4
../../../vendor/acs4_py/acs4.py
########NEW FILE########
__FILENAME__ = adapter
"""Adapter to provide upstream URL structure over existing Open Library Infobase interface.

Upstream requires:

    /user/.* -> /people/.*
    /b/.* -> /books/.*
    /a/.* -> /authors/.*
    
This adapter module is a filter that sits above an Infobase server and fakes the new URL structure.
"""
import urllib, urllib2
import simplejson
import web

urls = (
    '/([^/]*)/get', 'get',
    '/([^/]*)/get_many', 'get_many',
    '/([^/]*)/things', 'things',
    '/([^/]*)/versions', 'versions',
    '/([^/]*)/new_key', 'new_key',
    '/([^/]*)/save(/.*)', 'save',
    '/([^/]*)/save_many', 'save_many',
    '/([^/]*)/reindex', 'reindex',    
    '/([^/]*)/account/(.*)', 'account',
    '/([^/]*)/count_edits_by_user', 'count_edits_by_user',
    '/.*', 'proxy'
)
app = web.application(urls, globals())

convertions = {
#    '/people/': '/user/',
#    '/books/': '/b/',
#    '/authors/': '/a/',
#    '/languages/': '/l/',
    '/templates/': '/upstream/templates/',
    '/macros/': '/upstream/macros/',
    '/js/': '/upstream/js/',
    '/css/': '/upstream/css/',
    '/old/templates/': '/templates/',
    '/old/macros/': '/macros/',
    '/old/js/': '/js/',
    '/old/css/': '/css/',
}

# inverse of convertions
iconversions = dict((v, k) for k, v in convertions.items())

class proxy:
    def delegate(self, *args):
        self.args = args
        self.input = web.input(_method='GET', _unicode=False)
        self.path = web.ctx.path
                
        if web.ctx.method in ['POST', 'PUT']:
            self.data = web.data()
        else:
            self.data = None
            
        headers = dict((k[len('HTTP_'):].replace('-', '_').lower(), v) for k, v in web.ctx.environ.items())
        
        self.before_request()
        try:
            server = web.config.infobase_server
            req = urllib2.Request(server + self.path + '?' + urllib.urlencode(self.input), self.data, headers=headers)
            req.get_method = lambda: web.ctx.method
            response = urllib2.urlopen(req)
        except urllib2.HTTPError, e:
            response = e
        self.status_code = response.code
        self.status_msg = response.msg
        self.output = response.read()
        
        self.headers = dict(response.headers.items())
        for k in ['transfer-encoding', 'server', 'connection', 'date']:
            self.headers.pop(k, None)
            
        if self.status_code == 200:
            self.after_request()
        else:
            self.process_error()

        web.ctx.status = "%s %s" % (self.status_code, self.status_msg)
        web.ctx.headers = self.headers.items()
        return self.output

    GET = POST = PUT = DELETE = delegate
    
    def before_request(self):
        if 'key' in self.input:
            self.input.key = convert_key(self.input.key)
        
    def after_request(self):
        if self.output:
            d = simplejson.loads(self.output)
            d = unconvert_dict(d)
            self.output = simplejson.dumps(d)
    
    def process_error(self):
        if self.output:
            d = simplejson.loads(self.output)
            if 'key' in d:
                d['key'] = unconvert_key(d['key'])
            self.output = simplejson.dumps(d)

def convert_key(key, mapping=convertions):
    """
        >>> convert_key("/authors/OL1A", {'/authors/': '/a/'})
        '/a/OL1A'
    """
    if key is None:
        return None
    elif key == '/':
        return '/upstream'
        
    for new, old in mapping.items():
        if key.startswith(new):
            key2 = old + key[len(new):]
            return key2
    return key
    
def convert_dict(d, mapping=convertions):
    """
        >>> convert_dict({'author': {'key': '/authors/OL1A'}}, {'/authors/': '/a/'})
        {'author': {'key': '/a/OL1A'}}
    """
    if isinstance(d, dict):
        if 'key' in d:
            d['key'] = convert_key(d['key'], mapping)
        for k, v in d.items():
            d[k] = convert_dict(v, mapping)
        return d
    elif isinstance(d, list):
        return [convert_dict(x, mapping) for x in d]
    else:
        return d

def unconvert_key(key):
    if key == '/upstream':
        return '/'
    return convert_key(key, iconversions)

def unconvert_dict(d):
    return convert_dict(d, iconversions)
        
class get(proxy):
    def before_request(self):
        i = self.input
        if 'key' in i:
            i.key = convert_key(i.key)
                            
class get_many(proxy):
    def before_request(self):
        if 'keys' in self.input:
            keys = self.input['keys']
            keys = simplejson.loads(keys)
            keys = [convert_key(k) for k in keys]
            self.input['keys'] = simplejson.dumps(keys)
                    
    def after_request(self):
        d = simplejson.loads(self.output)
        d = dict((unconvert_key(k), unconvert_dict(v)) for k, v in d.items())
        self.output = simplejson.dumps(d)
                    
class things(proxy):
    def before_request(self):
        if 'query' in self.input:
            q = self.input.query
            q = simplejson.loads(q)
            
            def convert_keys(q):
                if isinstance(q, dict):
                    return dict((k, convert_keys(v)) for k, v in q.items())
                elif isinstance(q, list):
                    return [convert_keys(x) for x in q]
                elif isinstance(q, basestring):
                    return convert_key(q)
                else:
                    return q
            self.input.query = simplejson.dumps(convert_keys(q))
            
    def after_request(self):
        if self.output:
            d = simplejson.loads(self.output)

            if self.input.get('details', '').lower() == 'true':
                d = unconvert_dict(d)
            else:
                d = [unconvert_key(key) for key in d]

            self.output = simplejson.dumps(d)
        
class versions(proxy):
    def before_request(self):
        if 'query' in self.input:
            q = self.input.query
            q = simplejson.loads(q)
            if 'key' in q:
                q['key'] = convert_key(q['key'])
            if 'author' in q:
                q['author'] = convert_key(q['author'])
            self.input.query = simplejson.dumps(q)

    def after_request(self):
        if self.output:
            d = simplejson.loads(self.output)
            for v in d:
                v['author'] = v['author'] and unconvert_key(v['author'])
                v['key'] = unconvert_key(v['key'])
            self.output = simplejson.dumps(d)
            
class new_key(proxy):
    def after_request(self):
        if self.output:
            d = simplejson.loads(self.output)
            d = unconvert_key(d)
            self.output = simplejson.dumps(d)
            
class save(proxy):
    def before_request(self):
        self.path = '/%s/save%s' % (self.args[0], convert_key(self.args[1]))
        d = simplejson.loads(self.data)
        d = convert_dict(d)
        self.data = simplejson.dumps(d)

class save_many(proxy):
    def before_request(self):
        i = web.input(_method="POST")
        if 'query' in i:
            q = simplejson.loads(i['query'])
            q = convert_dict(q)
            i['query'] = simplejson.dumps(q)
            self.data = urllib.urlencode(i)

class reindex(proxy):
    def before_request(self):
        i = web.input(_method="POST")
        if 'keys' in i:
            keys = [convert_key(k) for k in simplejson.loads(i['keys'])]
            i['keys'] = simplejson.dumps(keys)
            self.data = urllib.urlencode(i)

class account(proxy):
    def before_request(self):
        i = self.input
        if 'username' in i and i.username.startswith('/'):
            i.username = convert_key(i.username)

def main():
    import sys, os
    web.config.infobase_server = sys.argv[1].rstrip('/')
    os.environ['REAL_SCRIPT_NAME'] = ''
    
    sys.argv[1:] = sys.argv[2:]
    app.run() 

if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = addbook
"""Handlers for adding and editing books."""

import web
import urllib, urllib2
import simplejson
from collections import defaultdict
from StringIO import StringIO
import csv
import datetime

from infogami import config
from infogami.core import code as core
from infogami.core.db import ValidationException
from infogami.utils import delegate
from infogami.utils.view import safeint, add_flash_message
from infogami.infobase.client import ClientException

from openlibrary.plugins.openlibrary.processors import urlsafe
from openlibrary.utils.solr import Solr
from openlibrary.i18n import gettext as _
from openlibrary import accounts
import logging

import utils
from utils import render_template, fuzzy_find

from account import as_admin
from openlibrary.plugins.recaptcha import recaptcha

logger = logging.getLogger("openlibrary.book")

SYSTEM_SUBJECTS = ["Accessible Book", "Lending Library", "In Library", "Protected DAISY"]

def get_works_solr():
    if config.get('single_core_solr'):
        base_url = "http://%s/solr" % config.plugin_worksearch.get('solr')
    else:
        base_url = "http://%s/solr/works" % config.plugin_worksearch.get('solr')

    return Solr(base_url)

def get_authors_solr():
    if config.get('single_core_solr'):
        base_url = "http://%s/solr" % config.plugin_worksearch.get('author_solr')
    else:
        base_url = "http://%s/solr/authors" % config.plugin_worksearch.get('author_solr')
    return Solr(base_url)

def make_work(doc):
    w = web.storage(doc)
    w.key = "/works/" + w.key

    def make_author(key, name):
        key = "/authors/" + key
        return web.ctx.site.new(key, {
            "key": key,
            "type": {"key": "/type/author"},
            "name": name
        })

    w.authors = [make_author(key, name) for key, name in zip(doc['author_key'], doc['author_name'])]
    w.cover_url="/images/icons/avatar_book-sm.png"

    w.setdefault('ia', [])
    w.setdefault('first_publish_year', None)
    return w

def new_doc(type, **data):
    key = web.ctx.site.new_key(type)
    data['key'] = key
    data['type'] = {"key": type}
    return web.ctx.site.new(key, data)

class DocSaveHelper:
    """Simple utility to collct the saves and save all of the togeter at the end.
    """
    def __init__(self):
        self.docs = []

    def save(self, doc):
        """Adds the doc to the list of docs to be saved.
        """
        if not isinstance(doc, dict): # thing
            doc = doc.dict()
        self.docs.append(doc)

    def commit(self, **kw):
        """Saves all the collected docs."""
        if self.docs:
            web.ctx.site.save_many(self.docs, **kw)

class addbook(delegate.page):
    path = "/books/add"

    def GET(self):

        if not self.has_permission():
            return render_template("permission_denied", "/books/add", "Permission denied to add a book to Open Library.")

        i = web.input(work=None, author=None)
        work = i.work and web.ctx.site.get(i.work)
        author = i.author and web.ctx.site.get(i.author)

        recap_plugin_active = 'recaptcha' in config.get('plugins')
        if recap_plugin_active:
            public_key = config.plugin_recaptcha.public_key
            private_key = config.plugin_recaptcha.private_key
            recap = recaptcha.Recaptcha(public_key, private_key)
        else:
            recap = None

        return render_template('books/add', work=work, author=author, recaptcha=recap)

    def has_permission(self):
        return web.ctx.site.can_write("/books/add")

    def POST(self):
        i = web.input(title="", author_name="", author_key="", publisher="", publish_date="", id_name="", id_value="", _test="false")

        recap_plugin_active = 'recaptcha' in config.get('plugins')
        if recap_plugin_active and not web.ctx.site.get_user():
            public_key = config.plugin_recaptcha.public_key
            private_key = config.plugin_recaptcha.private_key
            recap = recaptcha.Recaptcha(public_key, private_key)

            if not recap.validate():
                return 'Recaptcha solution was incorrect. Please <a href="javascript:history.back()">go back</a> and try again.'

        saveutil = DocSaveHelper()

        match = self.find_matches(saveutil, i)

        if i._test == "true" and not isinstance(match, list):
            if match:
                return 'Matched <a href="%s">%s</a>' % (match.key, match.key)
            else:
                return 'No match found'

        if isinstance(match, list):
            # multiple matches
            return render_template("books/check", i, match)

        elif match and match.key.startswith('/books'):
            # work match and edition match
            return self.work_edition_match(match)

        elif match and match.key.startswith('/works'):
            # work match but not edition
            work = match
            return self.work_match(saveutil, work, i)
        else:
            # no match
            return self.no_match(saveutil, i)

    def find_matches(self, saveutil, i):
        """Tries to find an edition or a work or multiple works that match the given input data.

        Case#1: No match. None is returned.
        Case#2: Work match but not editon. Work is returned.
        Case#3: Work match and edition match. Edition is returned
        Case#3A: Work match and multiple edition match. List of works is returned
        Case#4: Multiple work match. List of works is returned.
        """
        i.publish_year = i.publish_date and self.extract_year(i.publish_date)

        work_key = i.get('work')

        # work_key is set to none-of-these when user selects none-of-these link.
        if work_key == 'none-of-these':
            return None

        work = work_key and web.ctx.site.get(work_key)
        if work:
            edition = self.try_edition_match(work=work,
                publisher=i.publisher, publish_year=i.publish_year,
                id_name=i.id_name, id_value=i.id_value)
            return edition or work

        if i.author_key == "__new__":
            a = new_doc("/type/author", name=i.author_name)
            comment = utils.get_message("comment_new_author")
            saveutil.save(a)
            i.author_key = a.key
            # since new author is created it must be a new record
            return None

        edition = self.try_edition_match(
            title=i.title,
            author_key=i.author_key,
            publisher=i.publisher,
            publish_year=i.publish_year,
            id_name=i.id_name,
            id_value=i.id_value)

        if edition:
            return edition

        solr = get_works_solr()
        author_key = i.author_key and i.author_key.split("/")[-1]
        result = solr.select({'title': i.title, 'author_key': author_key}, doc_wrapper=make_work, q_op="AND")

        if result.num_found == 0:
            return None
        elif result.num_found == 1:
            return result.docs[0]
        else:
            return result.docs

    def extract_year(self, value):
        m = web.re_compile(r"(\d\d\d\d)").search(value)
        return m and m.group(1)

    def try_edition_match(self,
        work=None, title=None, author_key=None,
        publisher=None, publish_year=None, id_name=None, id_value=None):

        # insufficient data
        if not publisher and not publish_year and not id_value:
            return

        q = {}
        work and q.setdefault('key', work.key.split("/")[-1])
        title and q.setdefault('title', title)
        author_key and q.setdefault('author_key', author_key.split('/')[-1])
        publisher and q.setdefault('publisher', publisher)
        # There are some errors indexing of publish_year. Use publish_date until it is fixed
        publish_year and q.setdefault('publish_date', publish_year)

        mapping = {
            'isbn_10': 'isbn',
            'isbn_13': 'isbn',
            'lccn': 'lccn',
            'oclc_numbers': 'oclc',
            'ocaid': 'ia'
        }
        if id_value and id_name in mapping:
            if id_name.startswith('isbn'):
                id_value = id_value.replace('-', '')
            q[mapping[id_name]] = id_value

        solr = get_works_solr()
        result = solr.select(q, doc_wrapper=make_work, q_op="AND")

        if len(result.docs) > 1:
            return result.docs
        elif len(result.docs) == 1:
            # found one edition match
            work = result.docs[0]
            publisher = publisher and fuzzy_find(publisher, work.publisher,
                                                 stopwords=("publisher", "publishers", "and"))

            editions = web.ctx.site.get_many(["/books/" + key for key in work.edition_key])
            for e in editions:
                d = {}
                if publisher:
                    if not e.publishers or e.publishers[0] != publisher:
                        continue
                if publish_year:
                    if not e.publish_date or publish_year != self.extract_year(e.publish_date):
                        continue
                if id_value and id_name in mapping:
                    if not id_name in e or id_value not in e[id_name]:
                        continue
                return e

    def work_match(self, saveutil, work, i):
        edition = self._make_edition(work, i)

        saveutil.save(edition)
        comment = utils.get_message("comment_add_book")
        saveutil.commit(comment=comment, action="add-book")

        raise web.seeother(edition.url("/edit?mode=add-book"))

    def _make_edition(self, work, i):
        edition = new_doc("/type/edition",
            works=[{"key": work.key}],
            title=i.title,
            publishers=[i.publisher],
            publish_date=i.publish_date,
        )
        if i.get("id_name") and i.get("id_value"):
            edition.set_identifiers([dict(name=i.id_name, value=i.id_value)])
        return edition

    def work_edition_match(self, edition):
        raise web.seeother(edition.url("/edit?mode=found"))

    def no_match(self, saveutil, i):
        # TODO: Handle add-new-author
        work = new_doc("/type/work",
            title=i.title,
            authors=[{"author": {"key": i.author_key}}]
        )

        edition = self._make_edition(work, i)

        saveutil.save(work)
        saveutil.save(edition)

        comment = utils.get_message("comment_add_book")
        saveutil.commit(action="add-book", comment=comment)

        raise web.seeother(edition.url("/edit?mode=add-work"))

# remove existing definations of addbook and addauthor
delegate.pages.pop('/addbook', None)
delegate.pages.pop('/addauthor', None)

class addbook(delegate.page):
    def GET(self):
        raise web.redirect("/books/add")

class addauthor(delegate.page):
    def GET(self):
        raise web.redirect("/authors")

def trim_value(value):
    """Trim strings, lists and dictionaries to remove empty/None values.

        >>> trim_value("hello ")
        'hello'
        >>> trim_value("")
        >>> trim_value([1, 2, ""])
        [1, 2]
        >>> trim_value({'x': 'a', 'y': ''})
        {'x': 'a'}
        >>> trim_value({'x': [""]})
        None
    """
    if isinstance(value, basestring):
        value = value.strip()
        return value or None
    elif isinstance(value, list):
        value = [v2 for v in value
                    for v2 in [trim_value(v)]
                    if v2 is not None]
        return value or None
    elif isinstance(value, dict):
        value = dict((k, v2) for k, v in value.items()
                             for v2 in [trim_value(v)]
                             if v2 is not None)
        return value or None
    else:
        return value

def trim_doc(doc):
    """Replace empty values in the document with Nones.
    """
    return web.storage((k, trim_value(v)) for k, v in doc.items() if k[:1] not in "_{")

class SaveBookHelper:
    """Helper to save edition and work using the form data coming from edition edit and work edit pages.

    This does the required trimming and processing of input data before saving.
    """
    def __init__(self, work, edition):
        self.work = work
        self.edition = edition

    def save(self, formdata):
        """Update work and edition documents according to the specified formdata."""
        comment = formdata.pop('_comment', '')

        user = accounts.get_current_user()
        delete = user and user.is_admin() and formdata.pop('_delete', '')

        formdata = utils.unflatten(formdata)
        work_data, edition_data = self.process_input(formdata)

        self.process_new_fields(formdata)

        saveutil = DocSaveHelper()

        if delete:
            if self.edition:
                self.delete(self.edition.key, comment=comment)

            if self.work and self.work.edition_count == 0:
                self.delete(self.work.key, comment=comment)
            return

        for i, author in enumerate(work_data.get("authors") or []):
            if author['author']['key'] == "__new__":
                a = self.new_author(formdata['authors'][i])
                author['author']['key'] = a.key
                saveutil.save(a)

        if work_data:
            if self.work is None:
                self.work = self.new_work(self.edition)
                self.edition.works = [{'key': self.work.key}]
            self.work.update(work_data)
            saveutil.save(self.work)

        if self.edition and edition_data:
            identifiers = edition_data.pop('identifiers', [])
            self.edition.set_identifiers(identifiers)

            classifications = edition_data.pop('classifications', [])
            self.edition.set_classifications(classifications)

            self.edition.set_physical_dimensions(edition_data.pop('physical_dimensions', None))
            self.edition.set_weight(edition_data.pop('weight', None))
            self.edition.set_toc_text(edition_data.pop('table_of_contents', ''))

            if edition_data.pop('translation', None) != 'yes':
                edition_data.translation_of = None
                edition_data.translated_from = None

            self.edition.update(edition_data)
            saveutil.save(self.edition)

        saveutil.commit(comment=comment, action="edit-book")

    def new_work(self, edition):
        work_key = web.ctx.site.new_key("/type/work")
        work = web.ctx.site.new(work_key, {
            "key": work_key,
            "type": {'key': '/type/work'},
        })
        return work

    def new_author(self, name):
        key =  web.ctx.site.new_key("/type/author")
        return web.ctx.site.new(key, {
            "key": key,
            "type": {"key": "/type/author"},
            "name": name
        })

    def delete(self, key, comment=""):
        doc = web.ctx.site.new(key, {
            "key": key,
            "type": {"key": "/type/delete"}
        })
        doc._save(comment=comment)

    def process_new_fields(self, formdata):
        def f(name):
            val = formdata.get(name)
            return val and simplejson.loads(val)

        new_roles = f('select-role-json')
        new_ids = f('select-id-json')
        new_classifications = f('select-classification-json')

        if new_roles or new_ids or new_classifications:
            edition_config = web.ctx.site.get('/config/edition')

            #TODO: take care of duplicate names

            if new_roles:
                edition_config.roles += [d.get('value') or '' for d in new_roles]

            if new_ids:
                edition_config.identifiers += [{
                        "name": d.get('value') or '',
                        "label": d.get('label') or '',
                        "website": d.get("website") or '',
                        "notes": d.get("notes") or ''}
                    for d in new_ids]

            if new_classifications:
                edition_config.classifications += [{
                        "name": d.get('value') or '',
                        "label": d.get('label') or '',
                        "website": d.get("website") or '',
                        "notes": d.get("notes") or ''}
                    for d in new_classifications]

            as_admin(edition_config._save)("add new fields")

    def process_input(self, i):
        if 'edition' in i:
            edition = self.process_edition(i.edition)
        else:
            edition = None

        if 'work' in i:
            work = self.process_work(i.work)
        else:
            work = None

        return work, edition

    def process_edition(self, edition):
        """Process input data for edition."""
        edition.publishers = edition.get('publishers', '').split(';')
        edition.publish_places = edition.get('publish_places', '').split(';')
        edition.distributors = edition.get('distributors', '').split(';')

        edition = trim_doc(edition)

        if edition.get('physical_dimensions') and edition.physical_dimensions.keys() == ['units']:
            edition.physical_dimensions = None

        if edition.get('weight') and edition.weight.keys() == ['units']:
            edition.weight = None

        for k in ['roles', 'identifiers', 'classifications']:
            edition[k] = edition.get(k) or []

        self._prevent_ocaid_deletion(edition)
        return edition

    def process_work(self, work):
        """Process input data for work."""
        def read_subject(subjects):
            if not subjects:
                return

            f = StringIO(subjects.encode('utf-8')) # no unicode in csv module
            dedup = set()
            for s in csv.reader(f, dialect='excel', skipinitialspace=True).next():
                s = s.decode('utf-8')
                if s.lower() not in dedup:
                    yield s
                    dedup.add(s.lower())

        work.subjects = list(read_subject(work.get('subjects', '')))
        work.subject_places = list(read_subject(work.get('subject_places', '')))
        work.subject_times = list(read_subject(work.get('subject_times', '')))
        work.subject_people = list(read_subject(work.get('subject_people', '')))
        if ': ' in work.get('title', ''):
            work.title, work.subtitle = work.title.split(': ', 1)
        else:
            work.subtitle = None

        for k in ('excerpts', 'links'):
            work[k] = work.get(k) or []

        # ignore empty authors
        work.authors = [a for a in work.get('authors', []) if a.get('author', {}).get('key', '').strip()]

        self._prevent_system_subjects_deletion(work)
        return trim_doc(work)

    def _prevent_system_subjects_deletion(self, work):
        # Allow admins to modify system systems
        user = accounts.get_current_user()
        if user and user.is_admin():
            return

        # Note: work is the new work object from the formdata and self.work is the work doc from the database.
        old_subjects = self.work and self.work.get("subjects") or []

        # If condition is added to handle the possibility of bad data
        set_old_subjects = set(s.lower() for s in old_subjects if isinstance(s, basestring))
        set_new_subjects = set(s.lower() for s in work.subjects)

        for s in SYSTEM_SUBJECTS:
            # if a system subject has been removed
            if s.lower() in set_old_subjects and s.lower() not in set_new_subjects:
                work_key = self.work and self.work.key
                logger.warn("Prevented removal of system subject %r from %s.", s, work_key)
                work.subjects.append(s)

    def _prevent_ocaid_deletion(self, edition):
        # Allow admins to modify ocaid
        user = accounts.get_current_user()
        if user and user.is_admin():
            return

        # read ocaid from form data
        try:
            ocaid = [id['value'] for id in edition.get('identifiers', []) if id['name'] == 'ocaid'][0]
        except IndexError:
            ocaid = None

        # 'self.edition' is the edition doc from the db and 'edition' is the doc from formdata
        if self.edition and self.edition.get('ocaid') and self.edition.get('ocaid') != ocaid:
            logger.warn("Attempt to change ocaid of %s from %r to %r.", self.edition.key, self.edition.get('ocaid'), ocaid)
            raise ValidationException("Changing Internet Archive ID is not allowed.")

class book_edit(delegate.page):
    path = "(/books/OL\d+M)/edit"

    def GET(self, key):
        i = web.input(v=None)
        v = i.v and safeint(i.v, None)

        if not web.ctx.site.can_write(key):
            return render_template("permission_denied", web.ctx.fullpath, "Permission denied to edit " + key + ".")

        edition = web.ctx.site.get(key, v)
        if edition is None:
            raise web.notfound()

        work = edition.works and edition.works[0]

        if not work:
            # HACK: create dummy work when work is not available to make edit form work
            work = web.ctx.site.new('', {
                'key': '', 
                'type': {'key': '/type/work'}, 
                'title': edition.title,
                'authors': [{'type': '/type/author_role', 'author': {'key': a['key']}} for a in edition.get('authors', [])]
            })

        recap_plugin_active = 'recaptcha' in config.get('plugins')

        #check to see if account is more than two years old
        old_user = False
        user = web.ctx.site.get_user()
        account = user and user.get_account()
        if account:
            create_dt = account.creation_time()
            now_dt = datetime.datetime.utcnow()
            delta = now_dt - create_dt
            if delta.days > 365*2:
                old_user = True

        if recap_plugin_active and not old_user:
            public_key = config.plugin_recaptcha.public_key
            private_key = config.plugin_recaptcha.private_key
            recap = recaptcha.Recaptcha(public_key, private_key)
        else:
            recap = None

        return render_template('books/edit', work, edition, recaptcha=recap)


    def POST(self, key):
        i = web.input(v=None, _method="GET")

        recap_plugin_active = 'recaptcha' in config.get('plugins')

        #check to see if account is more than two years old
        old_user = False
        user = web.ctx.site.get_user()
        account = user and user.get_account()
        if account:
            create_dt = account.creation_time()
            now_dt = datetime.datetime.utcnow()
            delta = now_dt - create_dt
            if delta.days > 365*2:
                old_user = True

        if recap_plugin_active and not old_user:
            public_key = config.plugin_recaptcha.public_key
            private_key = config.plugin_recaptcha.private_key
            recap = recaptcha.Recaptcha(public_key, private_key)

            if not recap.validate():
                return 'Recaptcha solution was incorrect. Please <a href="javascript:history.back()">go back</a> and try again.'

        v = i.v and safeint(i.v, None)
        edition = web.ctx.site.get(key, v)

        if edition is None:
            raise web.notfound()
        if edition.works:
            work = edition.works[0]
        else:
            work = None

        add = (edition.revision == 1 and work and work.revision == 1 and work.edition_count == 1)

        try:
            helper = SaveBookHelper(work, edition)
            helper.save(web.input())

            if add:
                add_flash_message("info", utils.get_message("flash_book_added"))
            else:
                add_flash_message("info", utils.get_message("flash_book_updated"))

            raise web.seeother(edition.url())
        except (ClientException, ValidationException), e:
            add_flash_message('error', str(e))
            return self.GET(key)

class work_edit(delegate.page):
    path = "(/works/OL\d+W)/edit"

    def GET(self, key):
        i = web.input(v=None, _method="GET")
        v = i.v and safeint(i.v, None)

        if not web.ctx.site.can_write(key):
            return render_template("permission_denied", web.ctx.fullpath, "Permission denied to edit " + key + ".")

        work = web.ctx.site.get(key, v)
        if work is None:
            raise web.notfound()

        recap_plugin_active = 'recaptcha' in config.get('plugins')
        if recap_plugin_active:
            public_key = config.plugin_recaptcha.public_key
            private_key = config.plugin_recaptcha.private_key
            recap = recaptcha.Recaptcha(public_key, private_key)
        else:
            recap = None

        return render_template('books/edit', work, recaptcha=recap)


    def POST(self, key):
        i = web.input(v=None, _method="GET")

        recap_plugin_active = 'recaptcha' in config.get('plugins')
        if recap_plugin_active:
            public_key = config.plugin_recaptcha.public_key
            private_key = config.plugin_recaptcha.private_key
            recap = recaptcha.Recaptcha(public_key, private_key)

            if not recap.validate():
                return 'Recaptcha solution was incorrect. Please <a href="javascript:history.back()">go back</a> and try again.'

        v = i.v and safeint(i.v, None)
        work = web.ctx.site.get(key, v)
        if work is None:
            raise web.notfound()

        try:
            helper = SaveBookHelper(work, None)
            helper.save(web.input())
            add_flash_message("info", utils.get_message("flash_work_updated"))
            raise web.seeother(work.url())
        except (ClientException, ValidationException), e:
            add_flash_message('error', str(e))
            return self.GET(key)

class author_edit(delegate.page):
    path = "(/authors/OL\d+A)/edit"

    def GET(self, key):
        if not web.ctx.site.can_write(key):
            return render_template("permission_denied", web.ctx.fullpath, "Permission denied to edit " + key + ".")

        author = web.ctx.site.get(key)
        if author is None:
            raise web.notfound()
        return render_template("type/author/edit", author)

    def POST(self, key):
        author = web.ctx.site.get(key)
        if author is None:
            raise web.notfound()

        i = web.input(_comment=None)
        formdata = self.process_input(i)
        try:
            if not formdata:
                raise web.badrequest()
            elif "_save" in i:
                author.update(formdata)
                author._save(comment=i._comment)
                raise web.seeother(key)
            elif "_delete" in i:
                author = web.ctx.site.new(key, {"key": key, "type": {"key": "/type/delete"}})
                author._save(comment=i._comment)
                raise web.seeother(key)
        except (ClientException, ValidationException), e:
            add_flash_message('error', str(e))
            author.update(formdata)
            author['comment_'] = i._comment
            return render_template("type/author/edit", author)

    def process_input(self, i):
        i = utils.unflatten(i)
        if 'author' in i:
            author = trim_doc(i.author)
            alternate_names = author.get('alternate_names', None) or ''
            author.alternate_names = [name.strip() for name in alternate_names.replace("\n", ";").split(';') if name.strip()]
            author.links = author.get('links') or []
            return author

class edit(core.edit):
    """Overwrite ?m=edit behaviour for author, book and work pages"""
    def GET(self, key):
        page = web.ctx.site.get(key)

        if web.re_compile('/(authors|books|works)/OL.*').match(key):
            if page is None:
                raise web.seeother(key)
            else:
                raise web.seeother(page.url(suffix="/edit"))
        else:
            return core.edit.GET(self, key)

class daisy(delegate.page):
    path = "(/books/.*)/daisy"

    def GET(self, key):
        page = web.ctx.site.get(key)

        if not page:
            raise web.notfound()

        return render_template("books/daisy", page)

def to_json(d):
    web.header('Content-Type', 'application/json')
    return delegate.RawText(simplejson.dumps(d))

class languages_autocomplete(delegate.page):
    path = "/languages/_autocomplete"

    def GET(self):
        i = web.input(q="", limit=5)
        i.limit = safeint(i.limit, 5)

        languages = [lang for lang in utils.get_languages() if lang.name.lower().startswith(i.q.lower())]
        return to_json(languages[:i.limit])

class authors_autocomplete(delegate.page):
    path = "/authors/_autocomplete"

    def GET(self):
        i = web.input(q="", limit=5)
        i.limit = safeint(i.limit, 5)

        solr = get_authors_solr()

        name = solr.escape(i.q) + "*"
        q = 'name:(%s) OR alternate_names:(%s)' % (name, name)
        data = solr.select(q, q_op="AND", sort="work_count desc")
        docs = data['docs']
        for d in docs:
            d.key = "/authors/" + d.key

            if 'top_work' in d:
                d['works'] = [d.pop('top_work')]
            else:
                d['works'] = []
            d['subjects'] = d.pop('top_subjects', [])
        return to_json(docs)


class work_identifiers(delegate.view):
    suffix = "identifiers"
    types = ["/type/edition"]

    def POST(self, edition):
        saveutil = DocSaveHelper()
        i = web.input(isbn = "")
        isbn = i.get("isbn")
        # Need to do some simple validation here. Perhaps just check if it's a number?
        if len(isbn) == 10:
            typ = "ISBN 10"
            data = [{'name': u'isbn_10', 'value': isbn}]
        elif len(isbn) == 13:
            typ = "ISBN 13"
            data = [{'name': u'isbn_13', 'value': isbn}]
        else:
            add_flash_message("error", "The ISBN number you entered was not valid")
            raise web.redirect(web.ctx.path)
        if edition.works:
            work = edition.works[0]
        else:
            work = None
        edition.set_identifiers(data)
        saveutil.save(edition)
        saveutil.commit(comment="Added an %s identifier."%typ, action="edit-book")
        add_flash_message("info", "Thank you very much for improving that record!")
        raise web.redirect(web.ctx.path)



def setup():
    """Do required setup."""
    pass

########NEW FILE########
__FILENAME__ = borrow
"""Handlers for borrowing books"""

import copy
import datetime, time
import hmac
import re
import simplejson
import string
import urllib2
import uuid
import logging

import web

from infogami import config
from infogami.utils import delegate
from infogami.utils.view import public, add_flash_message
from infogami.infobase.utils import parse_datetime

import utils
from utils import render_template

from openlibrary.core import inlibrary
from openlibrary.core import stats
from openlibrary.core import msgbroker
from openlibrary.core import waitinglist
from openlibrary import accounts
from openlibrary.core import ab

from lxml import etree

import acs4

logger = logging.getLogger("openlibrary.borrow")

########## Constants

lending_library_subject = u'Lending library'
in_library_subject = u'In library'
lending_subjects = set([lending_library_subject, in_library_subject])
loanstatus_url = config.get('loanstatus_url')

content_server = None

# ACS4 resource ids start with 'urn:uuid:'.  The meta.xml on archive.org
# adds 'acs:epub:' or 'acs:pdf:' to distinguish the file type.
acs_resource_id_prefixes = ['urn:uuid:', 'acs:epub:', 'acs:pdf:']

# Max loans a user can have at once
user_max_loans = 5

# When we generate a loan offer (.acsm) for a user we assume that the loan has occurred.
# Once the loan fulfillment inside Digital Editions the book status server will know
# the loan has occurred.  We allow this timeout so that we don't delete the OL loan
# record before fulfillment because we can't find it in the book status server.
# $$$ If a user borrows an ACS4 book and immediately returns book loan will show as
#     "not yet downloaded" for the duration of the timeout.
#     BookReader loan status is always current.
loan_fulfillment_timeout_seconds = 60*5

# How long bookreader loans should last
bookreader_loan_seconds = 60*60*24*14

# How long the auth token given to the BookReader should last.  After the auth token
# expires the BookReader will not be able to access the book.  The BookReader polls
# OL periodically to get fresh tokens.
bookreader_auth_seconds = 10*60

# Base URL for BookReader
try:
    bookreader_host = config.bookreader_host
except AttributeError:
    bookreader_host = 'archive.org'
    
bookreader_stream_base = 'https://' + bookreader_host + '/stream'

########## Page Handlers

# Handler for /books/{bookid}/{title}/borrow
class borrow(delegate.page):
    path = "(/books/.*)/borrow"
    
    def GET(self, key):
        edition = web.ctx.site.get(key)
        
        if not edition:
            raise web.notfound()

        edition.update_loan_status()
            
        loans = []
        user = accounts.get_current_user()
        if user:
            user.update_loan_status()
            loans = get_loans(user)
        
        # Check if we recently did a return
        i = web.input(r=None)
        if i.r == 't':
            have_returned = True
        else:
            have_returned = False

        ab.participate("borrow-layout")
        return render_template("borrow", edition, loans, have_returned)
        
    def POST(self, key):
        """Called when the user wants to borrow the edition"""
        
        i = web.input(action='borrow', format=None, ol_host=None)

        if i.ol_host:
            ol_host = i.ol_host
        else:        
            ol_host = 'openlibrary.org'
        
        edition = web.ctx.site.get(key)
        if not edition:
            raise web.notfound()
        error_redirect = edition.url("/borrow")
        
        user = accounts.get_current_user()
        if not user:
            raise web.seeother(error_redirect)

        if i.action == 'borrow':
            resource_type = i.format
            
            if resource_type not in ['epub', 'pdf', 'bookreader']:
                raise web.seeother(error_redirect)

            if resource_type == 'bookreader':
                ab.convert("borrow-layout")
            
            if user_can_borrow_edition(user, edition, resource_type):
                loan = Loan(user.key, key, resource_type)
                if resource_type == 'bookreader':
                    # The loan expiry should be utc isoformat
                    loan.expiry = datetime.datetime.utcfromtimestamp(time.time() + bookreader_loan_seconds).isoformat()
                loan_link = loan.make_offer() # generate the link and record that loan offer occurred
                                
                # $$$ Record fact that user has done a borrow - how do I write into user? do I need permissions?
                # if not user.has_borrowed:
                #   user.has_borrowed = True
                #   user.save()
                
                if resource_type == 'bookreader':
                    stats.increment('loans.bookreader')
                elif resource_type == 'pdf':
                    stats.increment('loans.pdf')
                elif resource_type == 'epub':
                    stats.increment('loans.epub')
                    
                if resource_type == 'bookreader':
                    raise web.seeother(make_bookreader_auth_link(loan.get_key(), edition.ocaid, '/stream/' + edition.ocaid, ol_host))
                else:
                    raise web.seeother(loan_link)
            else:
                # Send to the borrow page
                raise web.seeother(error_redirect)
                
        elif i.action == 'return':
            # Check that this user has the loan
            user.update_loan_status()
            loans = get_loans(user)

            # We pick the first loan that the user has for this book that is returnable.
            # Assumes a user can't borrow multiple formats (resource_type) of the same book.
            user_loan = None
            for loan in loans:
                # Handle the case of multiple edition records for the same 
                # ocaid and the user borrowed from one and returning from another
                has_loan = (loan['book'] == edition.key or loan['ocaid'] == edition.ocaid)
                if has_loan and can_return_resource_type(loan['resource_type']):
                    user_loan = loan
                    break
                    
            if not user_loan:
                # $$$ add error message
                raise web.seeother(error_redirect)
                
            # They have it -- return it
            return_resource(user_loan['resource_id'])
            
            # Show the page with "you've returned this"
            # $$$ this would do better in a session variable that can be cleared
            #     after the message is shown once
            raise web.seeother(edition.url('/borrow?r=t'))
            
        elif i.action == 'read':
            # Look for loans for this book
            user.update_loan_status()
            loans = get_loans(user)
            for loan in loans:
                if loan['book'] == edition.key:
                    raise web.seeother(make_bookreader_auth_link(loan['_key'], edition.ocaid, '/stream/' + edition.ocaid, ol_host))
        elif i.action == 'join-waitinglist':
            return self.POST_join_waitinglist(edition, user)
        elif i.action == 'leave-waitinglist':
            return self.POST_leave_waitinglist(edition, user, i)

        # Action not recognized
        raise web.seeother(error_redirect)

    def POST_join_waitinglist(self, edition, user):
        waitinglist.join_waitinglist(user.key, edition.key)
        raise web.redirect(edition.url("/borrow"))

    def POST_leave_waitinglist(self, edition, user, i):
        waitinglist.leave_waitinglist(user.key, edition.key)
        if i.get("redirect"):
            raise web.redirect(i.redirect)
        else:
            raise web.redirect(edition.url("/borrow"))

# Handler for /books/{bookid}/{title}/_borrow_status
class borrow_status(delegate.page):
    path = "(/books/.*)/_borrow_status"
    
    def GET(self, key):
    	global lending_subjects
    	
        i = web.input(callback=None)

        edition = web.ctx.site.get(key)
        
        if not edition:
            raise web.notfound()

        edition.update_loan_status()            
        available_formats = [loan['resource_type'] for loan in edition.get_available_loans()]
        loan_available = len(available_formats) > 0
        subjects = set([])
        
        for work in edition.get('works', []):
            for subject in work.get_subjects():
                if subject in lending_subjects:
                    subjects.add(subject)
        
        output = {
        	'id' : key,
        	'loan_available': loan_available,
        	'available_formats': available_formats,
        	'lending_subjects': [lending_subject for lending_subject in subjects]
        }

        output_text = simplejson.dumps( output )
        
        content_type = "application/json"
        if i.callback:
            content_type = "text/javascript"
            output_text = '%s ( %s );' % (i.callback, output_text)
        
        return delegate.RawText(output_text, content_type=content_type)


class borrow_admin(delegate.page):
    path = "(/books/.*)/borrow_admin"
    
    def GET(self, key):
        if not is_admin():
            return render_template('permission_denied', web.ctx.path, "Permission denied.")
    
        edition = web.ctx.site.get(key)
        ebook_key = "ebooks" + key
        ebook = web.ctx.site.store.get(ebook_key) or {}
        
        if not edition:
            raise web.notfound()

        i = web.input(updatestatus=None)
        if i.updatestatus == 't':
            edition.update_loan_status()
        edition_loans = get_edition_loans(edition)
            
        user_loans = []
        user = accounts.get_current_user()
        if user:
            user_loans = get_loans(user)
            
        return render_template("borrow_admin", edition, edition_loans, ebook, user_loans, web.ctx.ip)
        
    def POST(self, key):
        if not is_admin():
            return render_template('permission_denied', web.ctx.path, "Permission denied.")

        edition = web.ctx.site.get(key)
        if not edition:
            raise web.notfound()
            
        i = web.input(action=None, loan_key=None)

        if i.action == 'delete' and i.loan_key:
            delete_loan(i.loan_key)
        elif i.action == 'update_loan_info':
            waitinglist.update_waitinglist(key)

        raise web.seeother(web.ctx.path + '/borrow_admin')
        
class borrow_admin_no_update(delegate.page):
    path = "(/books/.*)/borrow_admin_no_update"
    
    def GET(self, key):
        if not is_admin():
            return render_template('permission_denied', web.ctx.path, "Permission denied.")
    
        edition = web.ctx.site.get(key)
        
        if not edition:
            raise web.notfound()

        edition_loans = get_edition_loans(edition)
            
        user_loans = []
        user = accounts.get_current_user()
        if user:
            user_loans = get_loans(user)
            
        return render_template("borrow_admin_no_update", edition, edition_loans, user_loans, web.ctx.ip)
        
    def POST(self, key):
        if not is_admin():
            return render_template('permission_denied', web.ctx.path, "Permission denied.")
            
        i = web.input(action=None, loan_key=None)

        if i.action == 'delete' and i.loan_key:
            delete_loan(i.loan_key)
            
        raise web.seeother(web.ctx.path) # $$$ why doesn't this redirect to borrow_admin_no_update?

class ia_loan_status(delegate.page):
    path = r"/ia_loan_status/(.*)"

    def GET(self, itemid):
        loan = web.ctx.site.store.get("loan-" + itemid)
        d = {
            'identifier': itemid,
            'checkedout': bool(loan)
        }
        return delegate.RawText(simplejson.dumps(d), content_type="application/json")

# Handler for /iauth/{itemid}
class ia_auth(delegate.page):
    path = r"/ia_auth/(.*)"
    
    def GET(self, item_id):
        i = web.input(_method='GET', callback=None, loan=None, token=None)
        
        resource_id = 'bookreader:%s' % item_id
        content_type = "application/json"
        
        # check that identifier is valid
        
        user = accounts.get_current_user()
        auth_json = simplejson.dumps( get_ia_auth_dict(user, item_id, resource_id, i.loan, i.token ) )
                
        output = auth_json
        
        if i.callback:
            content_type = "text/javascript"
            output = '%s ( %s );' % (i.callback, output)
        
        return delegate.RawText(output, content_type=content_type)

# Handler for /borrow/receive_notification - receive ACS4 status update notifications
class borrow_receive_notification(delegate.page):
    path = r"/borrow/receive_notification"

    def GET(self):
        web.header('Content-Type', 'application/json')
        output = simplejson.dumps({'success': False, 'error': 'Only POST is supported'})
        return delegate.RawText(output, content_type='application/json')

    def POST(self):
        data = web.data()
        try:
            notify_xml = etree.fromstring(data)

            # XXX verify signature?  Should be acs4 function...
            notify_obj = acs4.el_to_o(notify_xml)

            # print simplejson.dumps(notify_obj, sort_keys=True, indent=4)
            output = simplejson.dumps({'success':True})
        except Exception, e:
            output = simplejson.dumps({'success':False, 'error': str(e)})
        return delegate.RawText(output, content_type='application/json')
        
########## Public Functions

@public
def overdrive_id(edition):
    identifier = None
    if edition.get('identifiers', None) and edition.identifiers.get('overdrive', None):
        identifier = edition.identifiers.overdrive[0]
    return identifier

@public
def can_borrow(edition):
    return edition.can_borrow()
    
@public
def is_loan_available(edition, type):    
    resource_id = edition.get_lending_resource_id(type)
    
    if not resource_id:
        return False
        
    return not is_loaned_out(resource_id)

@public
def datetime_from_isoformat(expiry):
    """Returns datetime object, or None"""
    if expiry is None:
        return None
    return parse_datetime(expiry)
        
@public
def datetime_from_utc_timestamp(seconds):
    return datetime.datetime.utcfromtimestamp(seconds)

@public
def can_return_resource_type(resource_type):
    """Returns true if this resource can be returned from the OL site."""
    if resource_type.startswith('bookreader'):
        return True
    return False
    
@public
def ia_identifier_is_valid(item_id):
    """Returns false if the item id is obviously malformed. Not currently checking length."""
    if re.match(r'^[a-zA-Z0-9][a-zA-Z0-9\.\-_]*$', item_id):
        return True
    return False
    
@public
def get_bookreader_stream_url(itemid):
    return bookreader_stream_base + '/' + itemid
    
@public
def get_bookreader_host():
    return bookreader_host
    
    
        
########## Helper Functions

def get_all_store_values(**query):
    """Get all values by paging through all results. Note: adds store_key with the row id."""
    query = copy.deepcopy(query)
    if not query.has_key('limit'):
        query['limit'] = 500
    query['offset'] = 0
    values = []
    got_all = False
    
    while not got_all:
        #new_values = web.ctx.site.store.values(**query)
        new_items = web.ctx.site.store.items(**query)
        for new_item in new_items:
            new_item[1].update({'store_key': new_item[0]})
            # XXX-Anand: Handling the existing loans
            new_item[1].setdefault("ocaid", None)
            values.append(new_item[1])
        if len(new_items) < query['limit']:
            got_all = True
        query['offset'] += len(new_items)
    return values

def get_all_loans():
    # return web.ctx.site.store.values(type='/type/loan')
    return get_all_store_values(type='/type/loan')

def get_loans(user):
    # return web.ctx.site.store.values(type='/type/loan', name='user', value=user.key)
    return get_all_store_values(type='/type/loan', name='user', value=user.key)    

def get_edition_loans(edition):
    # An edition can't have loans if it doens't have an IA ID. 
    # This check avoids a lot of unnecessary queries.
    if edition.ocaid:
        # Get the loans only if the book is borrowed. Since store.get requests
        # are memcache-able, checking this will be very fast.
        # This avoids making expensive infobase query for each book.
        has_loan = web.ctx.site.store.get("ebooks" + edition['key'], {}).get("borrowed") == "true"
        
        # The implementation is changed to store the loan as loan-$ocaid.
        # If there is a loan on this book, we'll find it.
        # Sometimes there are multiple editions with same ocaid. This takes care of them as well. 
        loan_record = web.ctx.site.store.get("loan-" + edition.ocaid)
        if has_loan or loan_record:
            if loan_record:
                return [loan_record]
            # for legacy reasons.
            records = get_all_store_values(type='/type/loan', name='book', value=edition.key)
            return records
    return []

def get_loan_link(edition, type):
    """Get the loan link, which may be an ACS4 link or BookReader link depending on the loan type"""
    global content_server

    resource_id = edition.get_lending_resource_id(type)
    
    if type == 'bookreader':
        # link to bookreader
        return (resource_id, get_bookreader_stream_url(edition.ocaid))
        
    if type in ['pdf','epub']:
        # ACS4
        if not content_server:
            if not config.content_server:
                # $$$ log
                return None
            content_server = ContentServer(config.content_server)
            
        if not resource_id:
            raise Exception('Could not find resource_id for %s - %s' % (edition.key, type))
        return (resource_id, content_server.get_loan_link(resource_id))
        
    raise Exception('Unknown resource type %s for loan of edition %s', edition.key, type)
    
# def get_bookreader_link(edition):
#     """Returns the link to the BookReader for the edition"""
#     return "%s/%s" % (bookreader_stream_base, edition.ocaid)

def get_loan_key(resource_id):
    """Get the key for the loan associated with the resource_id"""
    # Find loan in OL
    loan_keys = web.ctx.site.store.query('/type/loan', 'resource_id', resource_id)
    if not loan_keys:
        # No local records
        return None

    # Only support single loan of resource at the moment
    if len(loan_keys) > 1:
        #raise Exception('Found too many local loan records for resource %s' % resource_id)
        logger.error("Found too many loan records for resource %s: %s", resource_id, loan_keys)
        
    loan_key = loan_keys[0]['key']
    return loan_key    
    
def get_loan_status(resource_id):
    """Should only be used for ACS4 loans.  Get the status of the loan from the ACS4 server,
       via the Book Status Server (BSS)
    """
    global loanstatus_url
    
    if not loanstatus_url:
        raise Exception('No loanstatus_url -- cannot check loan status')
    
    # BSS response looks like this:
    #
    # [
    #     {
    #         "loanuntil": "2010-06-25T00:52:04", 
    #         "resourceid": "a8b600e2-32fd-4aeb-a2b5-641103583254", 
    #         "returned": "F", 
    #         "until": "2010-06-25T00:52:04"
    #     }
    # ]

    url = '%s/is_loaned_out/%s' % (loanstatus_url, resource_id)
    try:
        response = simplejson.loads(urllib2.urlopen(url).read())
        if len(response) == 0:
            # No outstanding loans
            return None
        
        else:
            return response[0]
            
    except IOError:
        # status server is down
        # $$$ be more graceful
        #raise Exception('Loan status server not available - tried at %s', url)

        # XXX-Anand: don't crash
        return None
    
    raise Exception('Error communicating with loan status server for resource %s' % resource_id)

def get_all_loaned_out():
    """Returns array of BSS status for all resources currently loaned out (according to BSS)"""
    global loanstatus_url
    
    if not loanstatus_url:
        raise Exception('No loanstatus_url -- cannot check loan status')
        
    url = '%s/is_loaned_out/' % loanstatus_url
    try:
        response = simplejson.loads(urllib2.urlopen(url).read())
        return response
    except IOError:
        raise Exception('Loan status server not available')

def is_loaned_out(resource_id):
    # bookreader loan status is stored in the private data store
    if resource_id.startswith('bookreader'):
        # Check our local status
        loan_key = get_loan_key(resource_id)
        if not loan_key:
            # No loan recorded
            identifier = resource_id[len('bookreader:'):]
            return is_loaned_out_on_ia(identifier)

        # Find the loan and check if it has expired
        loan = web.ctx.site.store.get(loan_key)
        if loan:
            if datetime_from_isoformat(loan['expiry']) < datetime.datetime.utcnow():
                return True
                
        return False
    
    # Assume ACS4 loan - check status server
    status = get_loan_status(resource_id)
    return is_loaned_out_from_status(status)

def is_loaned_out_on_ia(identifier):
    url = "https://archive.org/services/borrow/%s?action=status" % identifier
    response = simplejson.loads(urllib2.urlopen(url).read())
    return response and response.get('checkedout')
    
def is_loaned_out_from_status(status):
    if not status:
        return False
    else:
        if status['returned'] == 'T':
            # Current loan has been returned
            return False
            
    # Has status and not returned
    return True
    
def update_loan_status(resource_id):
    """Update the loan status in OL based off status in ACS4.  Used to check for early returns."""
    
    # Get local loan record
    loan_key = get_loan_key(resource_id)
    
    if not loan_key:
        # No loan recorded, nothing to do
        return
        
    loan = web.ctx.site.store.get(loan_key)
    _update_loan_status(loan_key, loan, None)
    
def _update_loan_status(loan_key, loan, bss_status = None):
    # If this is a BookReader loan, local version of loan is authoritative
    if loan['resource_type'] == 'bookreader':
        # delete loan record if has expired
        # $$$ consolidate logic for checking expiry.  keep loan record for some time after it expires.
        if loan['expiry'] and loan['expiry'] < datetime.datetime.utcnow().isoformat():
            logger.info("%s", loan)            
            logger.info("%s: loan expired. deleting...", loan_key) 
            web.ctx.site.store.delete(loan_key)
            on_loan_delete(loan)
        return
        
    # Load status from book status server
    if bss_status is None:
        bss_status = get_loan_status(loan['resource_id'])
    update_loan_from_bss_status(loan_key, loan, bss_status)
        
def update_loan_from_bss_status(loan_key, loan, status):
    """Update the loan status in the private data store from BSS status"""
    global loan_fulfillment_timeout_seconds
    
    if not resource_uses_bss(loan['resource_id']):
        raise Exception('Tried to update loan %s with ACS4/BSS status when it should not use BSS' % loan_key)
    
    if not is_loaned_out_from_status(status):
        # No loan record, or returned or expired
        
        # Check if our local loan record is fresh -- allow some time for fulfillment
        if loan['expiry'] is None:
            now = time.time()
            # $$$ loan_at in the store is in timestamp seconds until updated (from BSS) to isoformat string
            if now - loan['loaned_at'] < loan_fulfillment_timeout_seconds:
                # Don't delete the loan record - give it time to complete
                return
    
        # Was returned, expired, or timed out
        web.ctx.site.store.delete(loan_key)
        logger.info("%s", loan)
        logger.info("%s: loan returned or expired or timedout, deleting...", loan_key)
        on_loan_delete(loan)
        return

    # Book has non-returned status
    # Update expiry
    if loan['expiry'] != status['until']:
        loan['expiry'] = status['until']
        web.ctx.site.store[loan_key] = loan
        logger.info("%s: updated expiry to %s", loan_key, loan['expiry'])
        on_loan_update(loan)

def update_all_loan_status():
    """Update the status of all loans known to Open Library by cross-checking with the book status server.
    
    This is called once an hour from a cron job.
    """
    # Get book status records of everything loaned out
    bss_statuses = get_all_loaned_out()
    bss_resource_ids = [status['resourceid'] for status in bss_statuses]

    loans = web.ctx.site.store.values(type='/type/loan', limit=-1)
    acs4_loans = [loan for loan in loans if loan['resource_type'] in ['epub', 'pdf']]
    for i, loan in enumerate(acs4_loans):
        logger.info("processing loan %s (%s)", loan['_key'], i)
        bss_status = None
        if resource_uses_bss(loan['resource_id']):
            try:
                bss_status = bss_statuses[bss_resource_ids.index(loan['resource_id'])]
            except ValueError:
                bss_status = None
        _update_loan_status(loan['_key'], loan, bss_status)

def resource_uses_bss(resource_id):
    """Returns true if the resource should use the BSS for status"""
    global acs_resource_id_prefixes
    
    if resource_id:
        for prefix in acs_resource_id_prefixes:
            if resource_id.startswith(prefix):
                return True
    return False

def user_can_borrow_edition(user, edition, type):
    """Returns true if the user can borrow this edition given their current loans.  Returns False if the
       user holds a current loan for the edition."""
       
    global user_max_loans

    if not can_borrow(edition):
        return False
    if user.get_loan_count() >= user_max_loans:
        return False
    if edition.get_waitinglist_size() > 0:
        # There some people are already waiting for the book,
        # it can't be borrowed unless the user is the first in the waiting list.
        waiting_loan = user.get_waiting_loan_for(edition)

        if not waiting_loan or waiting_loan['status'] != 'available':
            return False
    
    if type in [loan['resource_type'] for loan in edition.get_available_loans()]:
        return True
    
    return False

def is_admin():
    """"Returns True if the current user is in admin usergroup."""
    user = accounts.get_current_user()
    return user and user.key in [m.key for m in web.ctx.site.get('/usergroup/admin').members]
    
def return_resource(resource_id):
    """Return the book to circulation!  This object is invalid and should not be used after
       this is called.  Currently only possible for bookreader loans."""
    loan_key = get_loan_key(resource_id)
    if not loan_key:
        raise Exception('Asked to return %s but no loan recorded' % resource_id)
    
    loan = web.ctx.site.store.get(loan_key)
    if loan['resource_type'] != 'bookreader':
        raise Exception('Not possible to return loan %s of type %s' % (loan['resource_id'], loan['resource_type']))
    delete_loan(loan_key, loan)
    
def delete_loan(loan_key, loan = None):
    if not loan:
        loan = web.ctx.site.store.get(loan_key)
        if not loan:
            raise Exception('Could not find store record for %s', loan_key)
                        
    if loan['type'] != '/type/loan':
        raise Exception('Record from store for %s is not of type /type/loan. Record: %s', loan_key, loan)
        
    web.ctx.site.store.delete(loan_key)
    on_loan_delete(loan)

def get_ia_auth_dict(user, item_id, resource_id, user_specified_loan_key, access_token):
    """Returns response similar to one of these:
    {'success':true,'token':'1287185207-fa72103dd21073add8f87a5ad8bce845','borrowed':true}
    {'success':false,'msg':'Book is checked out','borrowed':false, 'resolution': 'You can visit <a href="http://openlibary.org/ia/someid">this book\'s page on Open Library</a>.'}
    """
    
    base_url = 'http://' + web.ctx.host
    resolution_dict = { 'base_url': base_url, 'item_id': item_id }
    
    error_message = None
    user_has_current_loan = False
    
    # Sanity checks
    if not ia_identifier_is_valid(item_id):
        return {'success': False, 'msg': 'Invalid item id', 'resolution': 'This book does not appear to have a valid item identifier.' }

    if not resource_id.startswith('bookreader'):
        error_message = 'Bad resource id type'
        resolution_message = 'This book cannot be borrowed for in-browser loan. You can <a href="%(base_url)s/ia/%(item_id)s">visit this book\'s page</a> on openlibrary.org to learn more about the book.' % resolution_dict
        return {'success': False, 'msg': error_message, 'resolution': resolution_message }
    
    # Lookup loan information
    loan_key = get_loan_key(resource_id)

    if loan_key is None:
        # Book is not checked out as a BookReader loan - may still be checked out in ACS4
        error_message = 'Lending Library Book'
        resolution_message = 'This book is part of the <a href="%(base_url)s/subjects/Lending_library">lending library</a>. Please <a href="%(base_url)s/ia/%(item_id)s/borrow">visit this book\'s page on Open Library</a> to access the book.' % resolution_dict
        
    else:
        # Book is checked out (by someone) - get the loan information
        loan = web.ctx.site.store.get(loan_key)
        
        # Check that this is a bookreader loan
        if loan['resource_type'] != 'bookreader':
            error_message = 'No BookReader loan'
            resolution_message = 'This book was borrowed as ' + loan['resource_type'] + '. You can <a href="%(base_url)s/ia/%(item_id)s">visit this book\'s page</a> on openlibrary.org to access the book in that format.' % resolution_dict
            return {'success': False, 'msg': error_message, 'resolution': resolution_message }

        # If we know who this user is, from third-party cookies and they are logged into openlibrary.org, check if they have the loan
        if user:
            if loan['user'] != user.key:
                # Borrowed by someone else - OR possibly came in through ezproxy and there's a stale login in on openlibrary.org
                
                
                
                error_message = 'This book is checked out'
                resolution_message = 'This book is currently checked out.  You can <a href="%(base_url)s/ia/%(item_id)s">visit this book\'s page on Open Library</a> or <a href="%(base_url)s/subjects/Lending_library">look at other books available to borrow</a>.' % resolution_dict
            
            elif loan['expiry'] < datetime.datetime.utcnow().isoformat():
                # User has the loan, but it's expired
                error_message = 'Your loan has expired'
                resolution_message = 'Your loan for this book has expired.  You can <a href="%(base_url)s/ia/%(item_id)s">visit this book\'s page on Open Library</a>.' % resolution_dict
            
            else:
                # User holds the loan - win!
                user_has_current_loan = True
        else:
            # Don't have user context - not logged in or third-party cookies disabled
            
            # Check if the loan id + token is valid
            if user_specified_loan_key and access_token and ia_token_is_current(item_id, access_token):
                # Win!
                user_has_current_loan = True
                    
            else:
                # Couldn't validate using token - they need to go to Open Library
                error_message = "Lending Library Book"
                resolution_message = 'This book is part of the <a href="%(base_url)s/subjects/Lending_library" title="Open Library Lending Library">lending library</a>. Please <a href="%(base_url)s/ia/%(item_id)s/borrow" title="Borrow book page on Open Library">visit this book\'s page on Open Library</a> to access the book.  You must have cookies enabled for archive.org and openlibrary.org to access borrowed books.' % resolution_dict
    
    if error_message:
        return { 'success': False, 'msg': error_message, 'resolution': resolution_message }
    else:
        # No error message, make sure we thought the loan was current as sanity check
        if not user_has_current_loan:
            raise Exception('lending: no current loan for this user found but no error condition specified')
    
    return { 'success': True, 'token': make_ia_token(item_id, bookreader_auth_seconds) }
    

def make_ia_token(item_id, expiry_seconds):
    """Make a key that allows a client to access the item on archive.org for the number of
       seconds from now.
    """
    # $timestamp = $time+600; //access granted for ten minutes
    # $hmac = hash_hmac('md5', "{$id}-{$timestamp}", configGetValue('ol-loan-secret'));
    # return "{$timestamp}-{$hmac}";
    
    try:
        access_key = config.ia_access_secret
    except AttributeError:
        raise Exception("config value config.ia_access_secret is not present -- check your config")
        
    timestamp = int(time.time() + expiry_seconds)
    token_data = '%s-%d' % (item_id, timestamp)
    
    token = '%d-%s' % (timestamp, hmac.new(access_key, token_data).hexdigest())
    return token
    
def ia_token_is_current(item_id, access_token):
    try:
        access_key = config.ia_access_secret
    except AttributeError:
        raise Exception("config value config.ia_access_secret is not present -- check your config")
    
    # Check if token has expired
    try:
        token_timestamp = access_token.split('-')[0]
    except:
        return False
        
    token_time = int(token_timestamp)
    now = int(time.time())
    if token_time < now:
        return False
    
    # Verify token is valid
    try:
        token_hmac = access_token.split('-')[1]
    except:
        return False
        
    expected_data = '%s-%s' % (item_id, token_timestamp)
    expected_hmac = hmac.new(access_key, expected_data).hexdigest()
    
    if token_hmac == expected_hmac:
        return True
        
    return False
    
def make_bookreader_auth_link(loan_key, item_id, book_path, ol_host):
    """
    Generate a link to BookReaderAuth.php that starts the BookReader with the information to initiate reading
    a borrowed book
    """
    
    access_token = make_ia_token(item_id, bookreader_auth_seconds)
    auth_url = 'https://%s/bookreader/BookReaderAuth.php?uuid=%s&token=%s&id=%s&bookPath=%s&olHost=%s' % (
        bookreader_host, loan_key, access_token, item_id, book_path, ol_host
    )
    
    return auth_url
    
def on_loan_update(loan):
    store = web.ctx.site.store

    # key = "ebooks" + loan['book']
    # doc = store.get(key) or {}
    # doc.update({
    #     "type": "ebook",
    #     "book_key": loan['book'],
    #     "borrowed": "true"
    # })
    # store[key] = doc

    # update the waiting list and ebook document.
    waitinglist.update_waitinglist(loan['book'])

    # TODO: differentiate between loan-updated and loan-created
    msgbroker.send_message("loan-created", loan)
    
def on_loan_delete(loan):
    store = web.ctx.site.store
    loan['returned_at'] = time.time()

    # Check if the book still has an active loan
    
    # borrowed = "false"
    # loan_keys = store.query('/type/loan', 'resource_id', loan['resource_id'])
    # if loan_keys:
    #     borrowed = "true"

    # key = "ebooks" + loan['book']
    # doc = store.get(key) or {}
    # doc.update({
    #     "type": "ebook",
    #     "book_key": loan['book'],
    #     "borrowed": borrowed
    # })
    # store[key] = doc

    # update the waiting list and ebook document.
    waitinglist.update_waitinglist(loan['book'])

    msgbroker.send_message("loan-completed", loan)

########## Classes

class Loan:
    """The model class for Loan. 
    
    This is used only to make a loan offer. In other cases, the dict from store is used directly.
    """
    def __init__(self, user_key, book_key, resource_type, loaned_at = None):
        # store a uuid in the loan so that the loan can be uniquely identified. Required for stats.
        self.uuid = uuid.uuid4().hex
        self.key = None # Set after resource_id is initialized

        self.rev = 1 # This triggers the infobase consistency check - if there is an existing record on save
                     # the consistency check will fail (revision mismatch)
        self.user_key = user_key
        self.book_key = book_key
        self.ocaid = None
        self.resource_type = resource_type
        self.type = '/type/loan'
        self.resource_id = None
        self.loan_link = None
        self.expiry = None # We leave the expiry blank until we get confirmation of loan from ACS4
        
        if loaned_at is not None:
            self.loaned_at = loaned_at
        else:
            self.loaned_at = time.time()
        
    def get_key(self):
        return self.key

    def get_dict(self):
        return { '_key': self.get_key(),
                 '_rev': self.rev,
                 'type': '/type/loan',
                 'user': self.user_key, 
                 'book': self.book_key,
                 'ocaid': self.ocaid, 
                 'expiry': self.expiry,
                 'uuid': self.uuid,
                 'loaned_at': self.loaned_at, 'resource_type': self.resource_type,
                 'resource_id': self.resource_id, 'loan_link': self.loan_link }
                 
    def set_dict(self, loan_dict):
        self.rev = loan_dict['_rev'] 
        self.user_key = loan_dict['user']
        self.type = loan_dict['type']
        self.book_key = loan_dict['book']
        self.resource_type = loan_dict['resource_type']
        self.expiry = loan_dict['expiry']
        self.loaned_at = loan_dict['loaned_at']
        self.resource_id = loan_dict['resource_id']
        self.loan_link = loan_dict['loan_link']
        self.uuid = loan_link.get('uuid')
        
    def save(self):
        web.ctx.site.store[self.get_key()] = self.get_dict()        
        on_loan_update(self.get_dict())
        
    def remove(self):
        web.ctx.site.delete(self.get_key())
        on_loan_delete(self.get_dict())
        
    def make_offer(self):
        """Create loan url and record that loan was offered.  Returns the link URL that triggers
           Digital Editions to open or the link for the BookReader."""
           
        edition = web.ctx.site.get(self.book_key)
        resource_id, loan_link = get_loan_link(edition, self.resource_type)
        if not loan_link:
            raise Exception('Could not get loan link for edition %s type %s' % self.book_key, self.resource_type)
        self.loan_link = loan_link
        self.resource_id = resource_id
        self.key = "loan-" + edition.ocaid
        self.ocaid = edition.ocaid or None
        self.save()
        return self.loan_link
        
class ContentServer:
    def __init__(self, config):
        self.host = config.host
        self.port = config.port
        self.password = config.password
        self.distributor = config.distributor
        
        # Contact server to get shared secret for signing
        result = acs4.get_distributor_info(self.host, self.password, self.distributor)
        self.shared_secret = result['sharedSecret']
        self.name = result['name']

    def get_loan_link(self, resource_id):
        loan_link = acs4.mint(self.host, self.shared_secret, resource_id, 'enterloan', self.name, port = self.port)
        return loan_link

########NEW FILE########
__FILENAME__ = code
"""Upstream customizations."""

import os.path
import web
import random
import simplejson
import md5
import datetime 

from infogami import config
from infogami.infobase import client
from infogami.utils import delegate, app, types
from infogami.utils.view import public, safeint, render
from infogami.utils.context import context

from utils import render_template

from openlibrary.plugins.openlibrary.processors import ReadableUrlProcessor
from openlibrary.plugins.openlibrary import code as ol_code

import utils
import addbook
import models
import covers
import borrow
import recentchanges
import merge_authors

if not config.get('coverstore_url'):
    config.coverstore_url = "http://covers.openlibrary.org"

class static(delegate.page):
    path = "/images/.*"
    def GET(self):
        raise web.seeother('/static/' + web.ctx.path)

# handlers for change photo and change cover

class change_cover(delegate.page):
    path = "(/books/OL\d+M)/cover"
    def GET(self, key):
        return ol_code.change_cover().GET(key)
    
class change_photo(change_cover):
    path = "(/authors/OL\d+A)/photo"

del delegate.modes['change_cover']     # delete change_cover mode added by openlibrary plugin

@web.memoize
@public
def vendor_js():
    pardir = os.path.pardir 
    path = os.path.abspath(os.path.join(__file__, pardir, pardir, pardir, pardir, 'static', 'upstream', 'js', 'vendor.js'))
    digest = md5.md5(open(path).read()).hexdigest()
    return '/static/upstream/js/vendor.js?v=' + digest

@web.memoize
@public
def static_url(path):
    """Takes path relative to static/ and constructs url to that resource with hash.
    """
    pardir = os.path.pardir 
    fullpath = os.path.abspath(os.path.join(__file__, pardir, pardir, pardir, pardir, "static", path))
    digest = md5.md5(open(fullpath).read()).hexdigest()
    return "/static/%s?v=%s" % (path, digest)
    
class DynamicDocument:
    """Dynamic document is created by concatinating various rawtext documents in the DB.
    Used to generate combined js/css using multiple js/css files in the system.
    """
    def __init__(self, root):
        self.root = web.rstrips(root, '/')
        self.docs = None 
        self._text = None
        self.last_modified = None
        
    def update(self):
        keys = web.ctx.site.things({'type': '/type/rawtext', 'key~': self.root + '/*'})
        docs = sorted(web.ctx.site.get_many(keys), key=lambda doc: doc.key) 
        if docs:
            self.last_modified = min(doc.last_modified for doc in docs)
            self._text = "\n\n".join(doc.get('body', '') for doc in docs)
        else:
            self.last_modified = datetime.datetime.utcnow()
            self._text = ""
        
    def get_text(self):
        """Returns text of the combined documents"""
        if self._text is None:
            self.update()
        return self._text
        
    def md5(self):
        """Returns md5 checksum of the combined documents"""
        return md5.md5(self.get_text()).hexdigest()

def create_dynamic_document(url, prefix):
    """Creates a handler for `url` for servering combined js/css for `prefix/*` pages"""
    doc = DynamicDocument(prefix)
    
    if url.endswith('.js'):
        content_type = "text/javascript"
    elif url.endswith(".css"):
        content_type = "text/css"
    else:
        content_type = "text/plain"
    
    class page(delegate.page):
        """Handler for serving the combined content."""
        path = "__registered_later_without_using_this__"
        def GET(self):
            i = web.input(v=None)
            v = doc.md5()
            if v != i.v:
                raise web.seeother(web.changequery(v=v))
                
            if web.modified(etag=v):
                oneyear = 365 * 24 * 3600
                web.header("Content-Type", content_type)
                web.header("Cache-Control", "Public, max-age=%d" % oneyear)
                web.lastmodified(doc.last_modified)
                web.expires(oneyear)
                return delegate.RawText(doc.get_text())
                
        def url(self):
            return url + "?v=" + doc.md5()
            
        def reload(self):
            doc.update()
            
    class hook(client.hook):
        """Hook to update the DynamicDocument when any of the source pages is updated."""
        def on_new_version(self, page):
            if page.key.startswith(doc.root):
                doc.update()

    # register the special page
    delegate.pages[url] = {}
    delegate.pages[url][None] = page
    return page
            
all_js = create_dynamic_document("/js/all.js", config.get("js_root", "/js"))
web.template.Template.globals['all_js'] = all_js()

all_css = create_dynamic_document("/css/all.css", config.get("css_root", "/css"))
web.template.Template.globals['all_css'] = all_css()

def reload():
    """Reload all.css and all.js"""
    all_css().reload()
    all_js().reload()

def setup_jquery_urls():
    if config.get('use_google_cdn', True):
        jquery_url = "http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"
        jqueryui_url = "http://ajax.googleapis.com/ajax/libs/jqueryui/1.7.2/jquery-ui.min.js"
    else:
        jquery_url = "/static/upstream/js/jquery-1.3.2.min.js" 
        jqueryui_url = "/static/upstream/js/jquery-ui-1.7.2.min.js" 
        
    web.template.Template.globals['jquery_url'] = jquery_url
    web.template.Template.globals['jqueryui_url'] = jqueryui_url
    web.template.Template.globals['use_google_cdn'] = config.get('use_google_cdn', True)
        
@public
def get_document(key):
    return web.ctx.site.get(key)
    
class revert(delegate.mode):
    def GET(self, key):
        raise web.seeother(web.changequery(m=None))
        
    def POST(self, key):
        i = web.input("v", _comment=None)
        v = i.v and safeint(i.v, None)
        if v is None:
            raise web.seeother(web.changequery({}))

        if not web.ctx.site.can_write(key):
            return render.permission_denied(web.ctx.fullpath, "Permission denied to edit " + key + ".")
        
        thing = web.ctx.site.get(key, i.v)
        
        if not thing:
            raise web.notfound()
            
        def revert(thing):
            if thing.type.key == "/type/delete" and thing.revision > 1:
                prev = web.ctx.site.get(thing.key, thing.revision-1)
                if prev.type.key in ["/type/delete", "/type/redirect"]:
                    return revert(prev)
                else:
                    prev._save("revert to revision %d" % prev.revision)
                    return prev
            elif thing.type.key == "/type/redirect":
                redirect = web.ctx.site.get(thing.location)
                if redirect and redirect.type.key not in ["/type/delete", "/type/redirect"]:
                    return redirect
                else:
                    # bad redirect. Try the previous revision
                    prev = web.ctx.site.get(thing.key, thing.revision-1)
                    return revert(prev)
            else:
                return thing
                
        def process(value):
            if isinstance(value, list):
                return [process(v) for v in value]
            elif isinstance(value, client.Thing):
                if value.key:
                    if value.type.key in ['/type/delete', '/type/revert']:
                        return revert(value)
                    else:
                        return value
                else:
                    for k in value.keys():
                        value[k] = process(value[k])
                    return value
            else:
                return value
            
        for k in thing.keys():
            thing[k] = process(thing[k])
                    
        comment = i._comment or "reverted to revision %d" % v
        thing._save(comment)
        raise web.seeother(key)

def setup():
    """Setup for upstream plugin"""
    models.setup()
    utils.setup()
    addbook.setup()
    covers.setup()
    merge_authors.setup()
    
    import data
    data.setup()
    
    # setup template globals
    from openlibrary.i18n import ugettext, ungettext, gettext_territory
            
    web.template.Template.globals.update({
        "gettext": ugettext,
        "ugettext": ugettext,
        "_": ugettext,
        "ungettext": ungettext,
        "gettext_territory": gettext_territory,
        "random": random.Random(),
        "commify": web.commify,
        "group": web.group,
        "storage": web.storage,
        "all": all,
        "any": any,
        "locals": locals
    });
    
    import jsdef
    web.template.STATEMENT_NODES["jsdef"] = jsdef.JSDefNode
    
    setup_jquery_urls()
setup()


########NEW FILE########
__FILENAME__ = covers
"""Handle book cover/author photo upload.
"""
import urllib, urllib2
import web
import simplejson

from infogami.utils import delegate
from infogami.utils.view import safeint
from utils import get_coverstore_url, render_template
from models import Image
from openlibrary import accounts

def setup():
    pass
    
class add_cover(delegate.page):
    path = "(/books/OL\d+M)/add-cover"
    cover_category = "b"
    
    def GET(self, key):
        book = web.ctx.site.get(key)
        return render_template('covers/add', book)
        
    def POST(self, key):
        book = web.ctx.site.get(key)
        if not book:            
            raise web.notfound("")
            
        i = web.input(file={}, url="")
        
        # remove references to field storage objects
        web.ctx.pop("_fieldstorage", None)
        
        data = self.upload(key, i)
        coverid = data.get('id')
        
        if coverid:
            self.save(book, coverid, url=i.url)
            cover = Image(web.ctx.site, "b", coverid)
            return render_template("covers/saved", cover)
        else:
            return render_template("covers/add", book, {'url': i.url}, data)
            
    def upload(self, key, i):
        """Uploads a cover to coverstore and returns the response."""
        olid = key.split("/")[-1]
        
        if i.file is not None and hasattr(i.file, 'value'):
            data = i.file.value
        else:
            data = None
            
        if i.url and i.url.strip() == "http://":
            i.url = ""

        user = accounts.get_current_user()
        params = dict(author=user and user.key, data=data, source_url=i.url, olid=olid, ip=web.ctx.ip)

        upload_url = '%s/%s/upload2' % (get_coverstore_url(), self.cover_category)

        if upload_url.startswith("//"):
            upload_url = "http:" + upload_url

        try:
            response = urllib2.urlopen(upload_url, urllib.urlencode(params))
            out = response.read()
        except urllib2.HTTPError, e:
            out = e.read()
        
        return web.storage(simplejson.loads(out))
        
    def save(self, book, coverid, url=None):
        book.covers = [coverid] + [cover.id for cover in book.get_covers()]
        book._save("Added new cover", action="add-cover", data={"url": url})

class add_work_cover(add_cover):
    path = "(/works/OL\d+W)/add-cover"
    cover_category = "w"
    
    def upload(self, key, i):
        if "coverid" in i and safeint(i.coverid):
            return web.storage(id=int(i.coverid))
        else:
            return add_cover.upload(self, key, i)

class add_photo(add_cover):
    path = "(/authors/OL\d+A)/add-photo"
    cover_category = "a"
    
    def save(self, author, photoid, url=None):
        author.photos = [photoid] + [photo.id for photo in author.get_photos()]
        author._save("Added new photo", action="add-photo", data={"url": url})

class manage_covers(delegate.page):
    path = "(/books/OL\d+M)/manage-covers"
    def GET(self, key):
        book = web.ctx.site.get(key)
        if not book:
            raise web.notfound()
        return render_template("covers/manage", key, self.get_images(book))
        
    def get_images(self, book):
        return book.get_covers()
        
    def get_image(self, book):
        return book.get_cover()
        
    def save_images(self, book, covers):
        book.covers = covers
        book._save('Update covers')
        
    def POST(self, key):
        book = web.ctx.site.get(key)
        if not book:
            raise web.notfound()
            
        images = web.input(image=[]).image
        if '-' in images:
            images = [int(id) for id in images[:images.index('-')]]
            self.save_images(book, images)
            return render_template("covers/saved", self.get_image(book), showinfo=False)
        else:
            # ERROR
            pass

class manage_work_covers(manage_covers):
    path = "(/works/OL\d+W)/manage-covers"


class manage_photos(manage_covers):
    path = "(/authors/OL\d+A)/manage-photos"
    
    def get_images(self, author):
        return author.get_photos()
    
    def get_image(self, author):
        return author.get_photo()
        
    def save_images(self, author, photos):
        author.photos = photos
        author._save('Update photos')

########NEW FILE########
__FILENAME__ = data
"""Code for handling /data/*.txt.gz URLs.
"""
import web
from infogami.utils import delegate
from infogami.utils.view import public

import simplejson
import urllib2

def wget(url):
    return urllib2.urlopen(url).read()

def get_ol_dumps():
    """Get list of all archive.org items in the in the ol_exports collection uploaded of archive.org staff."""
    url = 'http://www.archive.org/advancedsearch.php?q=(ol_dump+OR+ol_cdump)+AND+collection:ol_exports&fl[]=identifier&output=json&rows=100'
    
    d = simplejson.loads(wget(url))
    return sorted(doc['identifier'] for doc in d['response']['docs'])
    
# cache the result for half an hour
get_ol_dumps = web.memoize(get_ol_dumps, 30*60, background=True)
#public(get_ol_dumps)

def download_url(item, filename):
    return "http://www.archive.org/download/%s/%s" % (item, filename)

class ol_dump_latest(delegate.page):
    path = "/data/ol_dump(|_authors|_editions|_works|_deworks)_latest.txt.gz"
    def GET(self, prefix):
        items = [item for item in get_ol_dumps() if item.startswith("ol_dump")]
        if not items:
            raise web.notfound()
            
        item = items[-1]
        filename = item.replace("dump", "dump" + prefix) + ".txt.gz"
        raise web.found(download_url(item, filename))
        
class ol_cdump_latest(delegate.page):
    path = "/data/ol_cdump_latest.txt.gz"
    
    def GET(self):
        items = [item for item in get_ol_dumps() if item.startswith("ol_cdump")]
        if not items:
            raise web.notfound()
            
        item = items[-1]
        raise web.found(download_url(item, item + ".txt.gz"))
        
class ol_dumps(delegate.page):
    path = "/data/ol_dump(|_authors|_editions|_works)_(\d\d\d\d-\d\d-\d\d).txt.gz"
    
    def GET(self, prefix, date):
        item = "ol_dump_" + date
        if item not in get_ol_dumps():
            raise web.notfound()
        else:
            filename = "ol_dump" + prefix + "_" + date + ".txt.gz"
            raise web.found(download_url(item, filename))
            
class ol_cdumps(delegate.page):
    path = "/data/ol_cdump_(\d\d\d\d-\d\d-\d\d).txt.gz"
    def GET(self, date):
        item = "ol_cdump_" + date
        if item not in get_ol_dumps():
            raise web.notfound()
        else:
            raise web.found(download_url(item, item + ".txt.gz"))

def setup():
    pass

########NEW FILE########
__FILENAME__ = forms
import web
from infogami.infobase.client import ClientException
from infogami.core import forms

from openlibrary.i18n import lgettext as _
from openlibrary.utils.form import Form, Textbox, Password, Hidden, Validator, RegexpValidator
from openlibrary import accounts

def find_account(username=None, lusername=None, email=None):
    return accounts.find(username=username, lusername=lusername, email=email)

Login = Form(
    Textbox('username', description=_('Username'), klass='required'),
    Password('password', description=_('Password'), klass='required'),
    Hidden('redirect')
)
forms.login = Login

email_already_used = Validator(_("No user registered with this email address"), lambda email: find_account(email=email) is not None)
email_not_already_used = Validator(_("Email already used"), lambda email: find_account(email=email) is None)
email_not_disposable = Validator(_("Disposable email not permitted"), lambda email: not email.lower().endswith('dispostable.com'))
username_validator = Validator(_("Username already used"), lambda username: not find_account(lusername=username.lower()))

vlogin = RegexpValidator(r"^[A-Za-z0-9-_]{3,20}$", _('Must be between 3 and 20 letters and numbers')) 
vpass = RegexpValidator(r".{3,20}", _('Must be between 3 and 20 characters'))
vemail = RegexpValidator(r".*@.*", _("Must be a valid email address"))

Register = Form(
    Textbox("displayname", description=_("Your Full Name")),
    Textbox('email', description=_('Your Email Address'), klass='required', validators=[vemail, email_not_already_used, email_not_disposable]),
    Textbox('username', description=_('Choose a Username'), klass='required', help=_("Only letters and numbers, please, and at least 3 characters."), 
        validators=[vlogin, username_validator]),
    Password('password', description=_('Choose a Password'), klass='required', validators=[vpass])
)

forms.register = Register

def verify_password(password):
    user = accounts.get_current_user()
    if user is None:
        return False
    
    try:
        username = user.key.split('/')[-1]
        web.ctx.site.login(username, password)
    except ClientException:
        return False
        
    return True
    
validate_password = Validator(_("Invalid password"), verify_password)

ChangePassword = Form(
    Password('password', description=_("Current Password"), klass='pwmask required', validators=[validate_password]),
    Password('new_password', description=_("Choose a New Password"), klass='pwmask required')
)

ChangeEmail = Form(
    Textbox('email', description=_("Your New Email Address"), validators=[vemail, email_not_already_used])
)

ForgotPassword = Form(
    Textbox('email', description=_("Your Email Address"), validators=[vemail, email_already_used])
)

ResetPassword = Form(
    Password('password', description=_("Choose a Password"), validators=[vpass])
)

########NEW FILE########
__FILENAME__ = jsdef
"""Templetor extension to support javascript templates.

During AJAX development, there will be need to generate HTML and update
some part of the DOM. It it clumsy to do that in javascript. Even though
there are some javascript template engines, it often ends up in duplication
because of writing a Python template and a Javascript template for doing
the same thing.

This extension adds a new block `jsdef` to Templetor, which provides a
a template function just like `def` and also generates an equivalent
javascript function.

USAGE::

    import jsdef
    render = web.tempalte.render("templates/", extensions=[jsdef.extension])

Sample Template::

    $def with (page)
    
    <h1>$page.title</h1>
    
    $jsdef render_books(books):
        <ul>
            $for book in books:
                <li><a href="$book.key">$book.title</a></li>
        </ul>
        
    <div id="books">
        $:render_books(page.books)
    </div>
    
    <script type="text/javascript">
        function udpate_books(books) {
            document.getElementById("books").innerHTML = render_books(books);
        }
    </script>

For more details, see:

http://github.com/anandology/notebook/tree/master/2010/03/jsdef/

"""

__author__ = "Anand Chitipothu <anandology@gmail.com>"
__version__ = "0.3"

"""change notes:

0.1: first release
0.2: python to javascript conversion for "and", "or" and "not" keywords
0.3: Added support for elif.
"""

import simplejson

import web
from web.template import Template, Parser, LineNode, SuiteNode, DefNode, PythonTokenizer, INDENT

def extension(parser):
    r"""jsdef extension. Adds support for `jsdef` block to template parser.::
    
        >>> t = Template("$jsdef hello(name):\n    Hello $name!", extensions=[extension])
        >>> print t() #doctest:+NORMALIZE_WHITESPACE
        <script type="text/javascript">
        function hello(name){
            var self = [], loop;
            self.push("Hello "); self.push(websafe(name)); self.push("!\n");
            return self.join("");
        }
        </script>        
    """
    parser.statement_nodes['jsdef'] = JSDefNode
    return parser

class JSDefNode(DefNode):
    """Node to represent jsdef block.
    """
    def __init__(self, *a, **kw):
        DefNode.__init__(self, *a, **kw)
        self.suite.sections.append(JSNode(self))
        self.stmt = self.stmt.replace("jsdef", "def")
    
INDENT = "    "

class JSNode:
    def __init__(self, node):
        self.node = node
        self._count = 0
        
    def emit(self, indent, text_indent=""):
        # Code generation logic is changed in version 0.34
        if web.__version__ < "0.34":
            return indent[4:] + 'yield "", %s\n' % repr(self.jsemit(self.node, ""))
        else:
            return indent[4:] + 'self.extend(%s)\n' % repr(self.jsemit(self.node, ""))
    
    def jsemit(self, node, indent):
        r"""Emit Javascript for given node.::
        
            >>> jsemit = JSNode(None).jsemit
            >>> jsemit(web.template.StatementNode("break"), "")
            'break;\n'
            >>> jsemit(web.template.AssignmentNode("x = 1"), "")
            'var x = 1;\n'
        """
        name = "jsemit_" + node.__class__.__name__
        f= getattr(self, name, None)
        if f:
            return f(node, indent)
        else:
            return ""
            
    def jsemit_SuiteNode(self, node, indent):
        return u"".join(self.jsemit(s, indent) for s in node.sections)
            
    def jsemit_LineNode(self, node, indent):
        text = ["self.push(%s);" % self.jsemit(n, "") for n in node.nodes]
        return indent + " ".join(text) + "\n"
        
    def jsemit_TextNode(self, node, indent):
        return simplejson.dumps(node.value)
    
    def jsemit_ExpressionNode(self, node, indent):
        if node.escape:
            return "websafe(%s)" % py2js(node.value)
        else:
            return py2js(node.value)

    def jsemit_AssignmentNode(self, node, indent):
        return indent + "var " + py2js(node.code) + ";\n"
        
    def jsemit_StatementNode(self, node, indent):
        return indent + py2js(node.stmt) + ";\n"
                
    def jsemit_BlockNode(self, node, indent):
        text = ""
        
        jsnames = {
            "elif": "else if"
        }
        
        for n in ["if", "elif", "else", "for"]:
            if node.stmt.startswith(n):
                name = n
                break
        else:
            return ""
            
        expr = node.stmt[len(name):].strip(": ")        
        expr = expr and "(" + expr + ")"
        
        jsname = jsnames.get(name, name)
        text += indent + "%s %s {\n" % (jsname, py2js(expr))
        text += self.jsemit(node.suite, indent + INDENT)
        text += indent + "}\n"
        return text
        
    jsemit_IfNode = jsemit_BlockNode
    jsemit_ElseNode = jsemit_BlockNode
    jsemit_ElifNode = jsemit_BlockNode
    
    def jsemit_ForNode(self, node, indent):
        tok = PythonTokenizer(node.stmt)
        tok.consume_till('in')
        a = node.stmt[:tok.index].strip() # for i in
        a = a[len("for"):-len("in")].strip() # strip `for` and `in`
        
        b = node.stmt[tok.index:-1].strip() # rest of for stmt excluding :
        b = web.re_compile("loop.setup\((.*)\)").match(b).group(1)

        text = ""
        text += indent + "foreach(%s, loop, function(loop, %s) {\n" % (py2js(b), a)
        text += self.jsemit(node.suite, indent + INDENT)
        text += indent + "});\n"
        return text
        
    def jsemit_JSDefNode(self, node, indent):
        text = ""
        text += '<script type="text/javascript"><!--\n'
        
        text += node.stmt.replace("def ", "function ").strip(": ") + "{\n"
        text += '    var self = [], loop;\n'
        text += self.jsemit(node.suite, indent + INDENT)
        text += '    return self.join("");\n' 
        text += "}\n"
        
        text += "//--></script>\n"
        return text
        
def tokenize(code):
    """Tokenize python code.::
    
        >>> list(tokenize("x + y"))
        ['x', ' ', '+', ' ', 'y']
    """
    end = 0
    tok = PythonTokenizer(code)
    try:
        while True:
            x = tok.next()
            begin = x.begin[1]
            if begin > end:
                yield ' ' * (begin - end)
            if x.value:
                yield x.value
            end = x.end[1]
    except StopIteration:
        pass
            
def py2js(expr):
    """Converts a python expression to javascript.::
    
        >>> py2js("x + y")
        'x + y'
        >>> py2js("x and y")
        'x && y'
        >>> py2js("x or not y")
        'x || ! y'
    """
    d = {"and": "&&", "or": "||", "not": "!"}
    def f(tokens):
       for t in tokens:
           yield d.get(t, t) 
         
    return "".join(f(tokenize(expr)))
    
def _testrun(code):
    parser = extension(web.template.Parser())
    root = parser.parse(code)
    node = root.suite
    jnode = JSNode(node)
    return jnode.jsemit(node, "")    
    
def _test():
    r"""
        >>> t = _testrun
        >>> t("$x")
        'self.push(websafe(x));\n'
        >>> t("$:x")
        'self.push(x);\n'
        >>> t("$ x = 1")
        'var x = 1;\n'
        >>> t("$ x = a and b")
        'var x = a && b;\n'
        >>> t("$if a or not b: $a")
        u'if (a || ! b) {\n    self.push(websafe(a));\n}\n'
        >>> t("$for i in a and a.data or []: $i")
        u'foreach(a && a.data || [], loop, function(loop, i) {\n    self.push(websafe(i));\n});\n'
    """
    
if __name__ == "__main__":
    import doctest
    doctest.testmod()
########NEW FILE########
__FILENAME__ = merge_authors
"""Merge authors.
"""
import web, re
import simplejson
from infogami.utils import delegate
from infogami.utils.view import render_template, safeint

from openlibrary.plugins.worksearch.code import top_books_from_author
from openlibrary.utils import uniq, dicthash

class BasicMergeEngine:
    """Generic merge functionality useful for all types of merges.
    """
    def merge(self, master, duplicates):
        docs = self.do_merge(master, duplicates)
        return self.save(docs, master, duplicates)
        
    def do_merge(self, master, duplicates):
        """Performs the merge and returns the list of docs to save.
        """
        docs_to_save = []
        
        # mark all the duplcates as redirects to the master
        docs_to_save.extend(self.make_redirect_doc(key, master) for key in duplicates)
        
        # find the references of each duplicate and covert them
        references = self.find_all_backreferences(duplicates)
        docs = self.get_many(references)
        docs_to_save.extend(self.convert_doc(doc, master, duplicates) for doc in docs)
        
        # finally, merge all the duplicates into the master.
        master_doc = web.ctx.site.get(master).dict()
        dups = self.get_many(duplicates)
        for d in dups:
            master_doc = self.merge_docs(master_doc, d)
            
        docs_to_save.append(master_doc)
        return docs_to_save
        
    def get_many(self, keys):
        def process(doc):
            # some books have bad table_of_contents. Fix them to avoid failure on save.
            if doc['type']['key'] == "/type/edition":
                if 'table_of_contents' in doc:
                    doc['table_of_contents'] = fix_table_of_contents(doc['table_of_contents'])
            return doc
        return [process(thing.dict()) for thing in web.ctx.site.get_many(list(keys))]
        
    def find_all_backreferences(self, duplicates):
        references = set()
        for key in duplicates:
            references.update(self.find_backreferences(key))
        return list(references)

    def find_backreferences(self, key):
        """Returns keys of all the docs which have a reference to the given key.

        All the subclasses must provide an implementation for this method.
        """
        raise NotImplementedError()
        
    def save(self, docs, master, duplicates):
        """Saves the effected docs because of merge.
        
        All the subclasses must provide an implementation for this method.
        """
        raise NotImplementedError()
            
    def merge_docs(self, master, dup, ignore=[]):
        """Merge duplicate doc into master doc.
        """
        keys = set(master.keys() + dup.keys())
        return dict((k, self.merge_property(master.get(k), dup.get(k))) for k in keys)
        
    def merge_property(self, a, b):
        if isinstance(a, list) and isinstance(b, list):
            return uniq(a + b, key=dicthash)
        elif not a:
            return b
        else:
            return a
        
    def make_redirect_doc(self, key, redirect):
        return {
            "key": key,
            "type": {"key": "/type/redirect"},
            "location": redirect
        }
    
    def convert_doc(self, doc, master, duplicates):
        """Converts references to any of the duplicates in the given doc to the master.
        """
        if isinstance(doc, dict):
            if len(doc) == 1 and doc.keys() == ['key']:
                key = doc['key']
                if key in duplicates:
                    return {"key": master}
                else:
                    return doc
            else:
                return dict((k, self.convert_doc(v, master, duplicates)) for k, v in doc.iteritems())
        elif isinstance(doc, list):
            values = [self.convert_doc(v, master, duplicates) for v in doc]
            return uniq(values, key=dicthash)
        else:
            return doc

class AuthorMergeEngine(BasicMergeEngine):
    def merge_docs(self, master, dup):
        # avoid merging other types.
        if dup['type']['key'] == '/type/author':
            master = BasicMergeEngine.merge_docs(self, master, dup)
            if dup.get('name') and not name_eq(dup['name'], master.get('name') or ''):
                master.setdefault('alternate_names', []).append(dup['name'])
            if 'alternate_names' in master:
                master['alternate_names'] = uniq(master['alternate_names'], key=space_squash_and_strip)
        return master
        
    def save(self, docs, master, duplicates):
        data = {
            "master": master,
            "duplicates": list(duplicates)
        }
        
        # There is a bug (#89) due to which old revisions of the docs are being sent to save.
        # Collecting all the possible information to detect the problem and saving it in datastore.
        debug_doc = {}
        debug_doc['type'] = 'merge-authors-debug' 
        mc = self._get_memcache()
        debug_doc['memcache'] = mc and dict((k, simplejson.loads(v)) for k, v in mc.get_multi([doc['key'] for doc in docs]).items())
        debug_doc['docs'] = docs
        
        result = web.ctx.site.save_many(docs, comment='merge authors', action="merge-authors", data=data)
        
        docrevs= dict((doc['key'], doc.get('revision')) for doc in docs)
        revs = dict((row['key'], row['revision']) for row in result)
        
        # Bad merges are happening when we are getting non-recent docs.
        # That can be identified by checking difference in the revision numbers before and after save
        bad_merge = any(revs[k]-docrevs[k] > 1 for k in revs if docrevs[k] is not None)
        
        debug_doc['bad_merge'] = str(bad_merge).lower()
        debug_doc['result'] = result
        key = 'merge_authors/%d' % web.ctx.site.seq.next_value('merge-authors-debug')
        web.ctx.site.store[key] = debug_doc
        
        return result
        
    def _get_memcache(self):
        from openlibrary.plugins.openlibrary import connection
        return connection._memcache
    
    def find_backreferences(self, key):
        q = {
            "type": "/type/edition",
            "authors": key,
            "limit": 10000
        }
        edition_keys = web.ctx.site.things(q)
        
        editions = self.get_many(edition_keys)
        work_keys_1 = [w['key'] for e in editions for w in e.get('works', [])]

        q = {
            "type": "/type/work",
            "authors": {"author": {"key": key}},
            "limit": 10000
        }
        work_keys_2 = web.ctx.site.things(q)
        return edition_keys + work_keys_1 + work_keys_2

re_whitespace = re.compile('\s+')
def space_squash_and_strip(s):
    return re_whitespace.sub(' ', s).strip()

def name_eq(n1, n2):
    return space_squash_and_strip(n1) == space_squash_and_strip(n2)
    
def fix_table_of_contents(table_of_contents):
    """Some books have bad table_of_contents. This function converts them in to correct format.
    """
    def row(r):
        if isinstance(r, basestring):
            level = 0
            label = ""
            title = web.safeunicode(r)
            pagenum = ""
        elif 'value' in r:
            level = 0
            label = ""
            title = web.safeunicode(r['value'])
            pagenum = ""            
        else:
            level = safeint(r.get('level', '0'), 0)
            label = r.get('label', '')
            title = r.get('title', '')
            pagenum = r.get('pagenum', '')
            
        r = web.storage(level=level, label=label, title=title, pagenum=pagenum)
        return r
    
    d = [row(r) for r in table_of_contents]
    return [row for row in d if any(row.values())]

class merge_authors(delegate.page):
    path = '/authors/merge'

    def is_enabled(self):
        return "merge-authors" in web.ctx.features
        
    def filter_authors(self, keys):
        docs = web.ctx.site.get_many(["/authors/" + k for k in keys])
        d = dict((doc.key, doc.type.key) for doc in docs)
        return [k for k in keys if d.get("/authors/" + k) == '/type/author']

    def GET(self):
        i = web.input(key=[])
        keys = uniq(i.key)
        
        # filter bad keys
        keys = self.filter_authors(keys)
        return render_template('merge/authors', keys, top_books_from_author=top_books_from_author)

    def POST(self):
        i = web.input(key=[], master=None, merge_key=[])
        keys = uniq(i.key)
        selected = uniq(i.merge_key)
        
        # filter bad keys
        keys = self.filter_authors(keys)        

        # doesn't make sense to merge master with it self.
        if i.master in selected:
            selected.remove(i.master)

        formdata = web.storage(
            master=i.master, 
            selected=selected
        )

        if not i.master or len(selected) == 0:
            return render_template("merge/authors", keys, top_books_from_author=top_books_from_author, formdata=formdata)
        else:                
            # redirect to the master. The master will display a progressbar and call the merge_authors_json to trigger the merge.
            master = web.ctx.site.get("/authors/" + i.master)
            raise web.seeother(master.url() + "?merge=true&duplicates=" + ",".join(selected))

class merge_authors_json(delegate.page):
    """JSON API for merge authors. 

    This is called from the master author page to trigger the merge while displaying progress.
    """
    path = "/authors/merge"
    encoding = "json"

    def is_enabled(self):
        return "merge-authors" in web.ctx.features

    def POST(self):
        json = web.data()
        data = simplejson.loads(json)
        master = data['master']
        duplicates = data['duplicates']
        
        engine = AuthorMergeEngine()
        result = engine.merge(master, duplicates)
        return delegate.RawText(simplejson.dumps(result),  content_type="application/json")

def setup():
    pass

########NEW FILE########
__FILENAME__ = models

import web
import urllib2
import simplejson
import re
from collections import defaultdict

from infogami import config
from infogami.infobase import client
from infogami.utils.view import safeint
from infogami.utils import stats

from openlibrary.core import models, ia
from openlibrary.core.models import Image

from openlibrary.plugins.search.code import SearchProcessor
from openlibrary.plugins.worksearch.code import works_by_author, sorted_work_editions
from openlibrary.utils.solr import Solr

from utils import get_coverstore_url, MultiDict, parse_toc, get_edition_config
import account
import borrow

def follow_redirect(doc):    
    if isinstance(doc, basestring) and doc.startswith("/a/"):
        #Some edition records have authors as ["/a/OL1A""] insead of [{"key": "/a/OL1A"}].
        # Hack to fix it temporarily.
        doc = web.ctx.site.get(doc.replace("/a/", "/authors/"))
    
    if doc and doc.type.key == "/type/redirect":
        key = doc.location
        return web.ctx.site.get(key)
    else:
        return doc

class Edition(models.Edition):

    def get_title(self):
        if self['title_prefix']:
            return self['title_prefix'] + ' ' + self['title']
        else:
            return self['title']
            
    def get_title_prefix(self):
        return ''

    # let title be title_prefix + title
    title = property(get_title)
    title_prefix = property(get_title_prefix)
    
    def get_authors(self):
        """Added to provide same interface for work and edition"""
        authors = [follow_redirect(a) for a in self.authors]
        authors = [a for a in authors if a and a.type.key == "/type/author"]
        return authors
        
    def get_next(self):
        """Next edition of work"""
        if len(self.get('works', [])) != 1:
            return
        wkey = self.works[0].get_olid()
        if not wkey:
            return
        editions = sorted_work_editions(wkey)
        try:
            i = editions.index(self.get_olid())
        except ValueError:
            return
        if i + 1 == len(editions):
            return
        return editions[i + 1]

    def get_prev(self):
        """Previous edition of work"""
        if len(self.get('works', [])) != 1:
            return
        wkey = self.works[0].get_olid()
        if not wkey:
            return
        editions = sorted_work_editions(wkey)
        try:
            i = editions.index(self.get_olid())
        except ValueError:
            return
        if i == 0:
            return
        return editions[i - 1]
 
    def get_covers(self):
        return [Image(self._site, 'b', c) for c in self.covers if c > 0]
        
    def get_cover(self):
        covers = self.get_covers()
        return covers and covers[0] or None
        
    def get_cover_url(self, size):
        cover = self.get_cover()
        if cover:
            return cover.url(size)
        elif self.ocaid:
            return self.get_ia_cover(self.ocaid, size)

    def get_ia_cover(self, itemid, size):
        image_sizes = dict(S=(116, 58), M=(180, 360), L=(500, 500))
        w, h = image_sizes[size.upper()]
        return "https://archive.org/download/%s/page/cover_w%s_h%s.jpg" % (itemid, w, h)

    def get_identifiers(self):
        """Returns (name, value) pairs of all available identifiers."""
        names = ['ocaid', 'isbn_10', 'isbn_13', 'lccn', 'oclc_numbers']
        return self._process_identifiers(get_edition_config().identifiers, names, self.identifiers)

    def get_ia_meta_fields(self):
        # Check for cached value
        # $$$ we haven't assigned _ia_meta_fields the first time around but there's apparently
        #     some magic that lets us check this way (and breaks using hasattr to check if defined)
        if self._ia_meta_fields:
            return self._ia_meta_fields
            
        if not self.get('ocaid', None):
            meta = {}
        else:
            meta = ia.get_meta_xml(self.ocaid)
            meta.setdefault("external-identifier", [])
            meta.setdefault("collection", [])
        
        self._ia_meta_fields = meta
        return self._ia_meta_fields

    def is_daisy_encrypted(self):
        meta_fields = self.get_ia_meta_fields()
        if not meta_fields:
            return
        v = meta_fields['collection']
        return 'printdisabled' in v or 'lendinglibrary' in v

#      def is_lending_library(self):
#         collections = self.get_ia_collections()
#         return 'lendinglibrary' in collections
        
    def get_lending_resources(self):
        """Returns the loan resource identifiers (in meta.xml format for ACS4 resources) for books hosted on archive.org
        
        Returns e.g. ['bookreader:lettertoannewarr00west',
                      'acs:epub:urn:uuid:0df6f344-7ce9-4038-885e-e02db34f2891',
                      'acs:pdf:urn:uuid:7f192e62-13f5-4a62-af48-be4bea67e109']
        """
        
        # The entries in meta.xml look like this:
        # <external-identifier>
        #     acs:epub:urn:uuid:0df6f344-7ce9-4038-885e-e02db34f2891
        # </external-identifier>
        
        itemid = self.ocaid
        if not itemid:
            # Could be e.g. OverDrive
            return []
        
        lending_resources = []
        # Check if available for in-browser lending - marked with 'browserlending' collection
        browserLendingCollections = ['browserlending']
        for collection in self.get_ia_meta_fields()['collection']:
            if collection in browserLendingCollections:
                lending_resources.append('bookreader:%s' % self.ocaid)
                break

        lending_resources.extend(self.get_ia_meta_fields()['external-identifier'])
        
        return lending_resources
        
    def get_lending_resource_id(self, type):
        if type == 'bookreader':
            desired = 'bookreader:'
        else:
            desired = 'acs:%s:' % type
            
        for urn in self.get_lending_resources():
            if urn.startswith(desired):            
                # Got a match                
                # $$$ a little icky - prune the acs:type if present
                if urn.startswith('acs:'):
                    urn = urn[len(desired):]
                    
                return urn

        return None
        
    def get_current_and_available_loans(self):
        current_loans = borrow.get_edition_loans(self)
        return (current_loans, self._get_available_loans(current_loans))

    def get_current_loans(self):
        return borrow.get_edition_loans(self)
        
    def get_available_loans(self):
        """
        Get the resource types currently available to be loaned out for this edition.  Does NOT
        take into account the user's status (e.g. number of books out, in-library status, etc).
        This is like checking if this book is on the shelf.
        
        Returns [{'resource_id': uuid, 'resource_type': type, 'size': bytes}]
        
        size may be None"""
        
        return self._get_available_loans(borrow.get_edition_loans(self))
        
    def _get_available_loans(self, current_loans):
        
        default_type = 'bookreader'
        
        loans = []
        
        # Check if we have a possible loan - may not yet be fulfilled in ACS4
        if current_loans:
            # There is a current loan or offer
            return []
        
        # Create list of possible loan formats
        resource_pattern = r'acs:(\w+):(.*)'
        for resource_urn in self.get_lending_resources():
            if resource_urn.startswith('acs:'):
                (type, resource_id) = re.match(resource_pattern, resource_urn).groups()
                loans.append( { 'resource_id': resource_id, 'resource_type': type, 'size': None } )
            elif resource_urn.startswith('bookreader'):
                loans.append( { 'resource_id': resource_urn, 'resource_type': 'bookreader', 'size': None } )
                    
        # Put default type at start of list, then sort by type name
        def loan_key(loan):
            if loan['resource_type'] == default_type:
                return '1-%s' % loan['resource_type']
            else:
                return '2-%s' % loan['resource_type']        
        loans = sorted(loans, key=loan_key)
                    
        # For each possible loan, check if it is available
        # We shouldn't be out of sync (we already checked get_edition_loans for current loans) but we fail safe, for example
        # the book may have been borrowed in a dev instance against the live ACS4 server
        for loan in loans:
            if borrow.is_loaned_out(loan['resource_id']):
                # Only a single loan of an item is allowed
                # $$$ log out of sync state
                return []
                    
        return loans
    
    def update_loan_status(self):
        """Update the loan status"""
        # $$$ search in the store and update
        loans = borrow.get_edition_loans(self)
        for loan in loans:
            borrow.update_loan_status(loan['resource_id'])
            
#         urn_pattern = r'acs:\w+:(.*)'
#         for ia_urn in self.get_lending_resources():
#             if ia_urn.startswith('acs:'):
#                 resource_id = re.match(urn_pattern, ia_urn).group(1)
#             else:
#                 resource_id = ia_urn
# 
#             borrow.update_loan_status(resource_id)

    def _process_identifiers(self, config, names, values):
        id_map = {}
        for id in config:
            id_map[id.name] = id
            id.setdefault("label", id.name)
            id.setdefault("url_format", None)
        
        d = MultiDict()
        
        def process(name, value):
            if value:
                if not isinstance(value, list):
                    value = [value]
                    
                id = id_map.get(name) or web.storage(name=name, label=name, url_format=None)
                for v in value:
                    d[id.name] = web.storage(
                        name=id.name, 
                        label=id.label, 
                        value=v, 
                        url=id.get('url') and id.url.replace('@@@', v))
                
        for name in names:
            process(name, self[name])
            
        for name in values:
            process(name, values[name])
        
        return d
    
    def set_identifiers(self, identifiers):
        """Updates the edition from identifiers specified as (name, value) pairs."""
        names = ('isbn_10', 'isbn_13', 'lccn', 'oclc_numbers', 'ocaid', 
                 'dewey_decimal_class', 'lc_classifications')
        
        d = {}
        for id in identifiers:
            # ignore bad values
            if 'name' not in id or 'value' not in id:
                continue
            name, value = id['name'], id['value']
            d.setdefault(name, []).append(value)
        
        # clear existing value first        
        for name in names:
           self._getdata().pop(name, None)
           
        self.identifiers = {}
            
        for name, value in d.items():
            # ocaid is not a list
            if name == 'ocaid':
                self.ocaid = value[0]
            elif name in names:
                self[name] = value
            else:
                self.identifiers[name] = value

    def get_classifications(self):
        names = ["dewey_decimal_class", "lc_classifications"]
        return self._process_identifiers(get_edition_config().classifications, 
                                         names, 
                                         self.classifications)
        
    def set_classifications(self, classifications):
        names = ["dewey_decimal_class", "lc_classifications"]
        d = defaultdict(list)
        for c in classifications:
            if 'name' not in c or 'value' not in c or not web.re_compile("[a-z0-9_]*").match(c['name']):
                continue
            d[c['name']].append(c['value'])
            
        for name in names:
            self._getdata().pop(name, None)
        self.classifications = {}
        
        for name, value in d.items():
            if name in names:
                self[name] = value
            else:
                self.classifications[name] = value
            
    def get_weight(self):
        """returns weight as a storage object with value and units fields."""
        w = self.weight
        return w and UnitParser(["value"]).parse(w)
        
    def set_weight(self, w):
        self.weight = w and UnitParser(["value"]).format(w)
        
    def get_physical_dimensions(self):
        d = self.physical_dimensions
        return d and UnitParser(["height", "width", "depth"]).parse(d)
    
    def set_physical_dimensions(self, d):
        # don't overwrite physical dimensions if nothing was passed in - there
        # may be dimensions in the database that don't conform to the d x d x d format
        if d:
            self.physical_dimensions = UnitParser(["height", "width", "depth"]).format(d)
        
    def get_toc_text(self):
        def format_row(r):
            return "*" * r.level + " " + " | ".join([r.label, r.title, r.pagenum])
            
        return "\n".join(format_row(r) for r in self.get_table_of_contents())
        
    def get_table_of_contents(self):
        def row(r):
            if isinstance(r, basestring):
                level = 0
                label = ""
                title = r
                pagenum = ""
            else:
                level = safeint(r.get('level', '0'), 0)
                label = r.get('label', '')
                title = r.get('title', '')
                pagenum = r.get('pagenum', '')
                
            r = web.storage(level=level, label=label, title=title, pagenum=pagenum)
            return r
            
        d = [row(r) for r in self.table_of_contents]
        return [row for row in d if any(row.values())]

    def set_toc_text(self, text):
        self.table_of_contents = parse_toc(text)
        
    def get_links(self):
        links1 = [web.storage(url=url, title=title) 
                  for url, title in zip(self.uris, self.uri_descriptions)] 
        links2 = list(self.links)
        return links1 + links2
        
    def get_olid(self):
        return self.key.split('/')[-1]
    
    @property
    def wp_citation_fields(self):
        """
        Builds a wikipedia citation as defined by http://en.wikipedia.org/wiki/Template:Cite#Citing_books
        """
        result = {
            "title": self.works[0].title.replace("[", "&#91").replace("]", "&#93"),
            "publication-date": self.get('publish_date'),
            "url": "http://openlibrary.org%s" % self.url()
        }

        if self.title != self.works[0].title:
            result['edition'] = self.title

        if self.get('isbn_10'):
            result['id'] = self['isbn_10'][0]
            result['isbn'] = self['isbn_13'][0] if self.get('isbn_13') else self['isbn_10'][0]

        if self.get('oclc_numbers'):
            result['oclc'] = self.oclc_numbers[0]

        if self.works[0].get('first_publish_year'):
            result['origyear'] = self.works[0]['first_publish_year']

        if self.get('publishers'):
            result['publisher'] = self['publishers'][0]

        if self.get('publish_places'):
            result['publication-place'] = self['publish_places'][0]

        authors = [ar.author for ar in self.works[0].authors]
        if len(authors) == 1:
            result['author'] = authors[0].name
        else:
            for i, a in enumerate(authors):
                result['author%s' % (i + 1)] = a.name 
        return result

    def is_fake_record(self):
        """Returns True if this is a record is not a real record from database, 
        but created on the fly.

        The /books/ia:foo00bar records are not stored in the database, but 
        created at runtime using the data from archive.org metadata API.
        """
        return "/ia:" in self.key

class Author(models.Author):
    def get_photos(self):
        return [Image(self._site, "a", id) for id in self.photos if id > 0]
        
    def get_photo(self):
        photos = self.get_photos()
        return photos and photos[0] or None
        
    def get_photo_url(self, size):
        photo = self.get_photo()
        return photo and photo.url(size)
    
    def get_olid(self):
        return self.key.split('/')[-1]

    def get_books(self):
        i = web.input(sort='editions', page=1)
        try:
            # safegaurd from passing zero/negative offsets to solr
            page = max(1, int(i.page))
        except ValueError:
            page = 1
        return works_by_author(self.get_olid(), sort=i.sort, page=page, rows=100)
        
    def get_work_count(self):
        """Returns the number of works by this author.
        """
        # TODO: avoid duplicate works_by_author calls
        result = works_by_author(self.get_olid(), rows=0)
        return result.num_found

re_year = re.compile(r'(\d{4})$')

def get_works_solr():
    if config.get("single_core_solr"):
        base_url = "http://%s/solr" % config.plugin_worksearch.get('solr')
    else:
        base_url = "http://%s/solr/works" % config.plugin_worksearch.get('solr')
    return Solr(base_url)
        
class Work(models.Work):
    def get_olid(self):
        return self.key.split('/')[-1]

    def get_covers(self, use_solr=True):
        if self.covers:
            return [Image(self._site, "w", id) for id in self.covers if id > 0]
        elif use_solr:
            return self.get_covers_from_solr()
        else:
            return []
            
    def get_covers_from_solr(self):
        w = self._solr_data
        if w:
            if 'cover_id' in w:
                return [Image(self._site, "w", int(w['cover_id']))]
            elif 'cover_edition_key' in w:
                cover_edition = web.ctx.site.get("/books/" + w['cover_edition_key'])
                cover = cover_edition and cover_edition.get_cover()
                if cover:
                    return [cover]
        return []
        
    def _get_solr_data(self):
        if config.get("single_core_solr"):
            key = self.key
        else:
            key = self.get_olid()

        fields = ["cover_edition_key", "cover_id", "edition_key", "first_publish_year"]
        
        solr = get_works_solr()
        stats.begin("solr", query={"key": key}, fields=fields)
        try:
            d = solr.select({"key": key}, fields=fields)
        finally:
            stats.end()
            
        if d.num_found > 0:
            w = d.docs[0]
        else:
            w = None
                
        # Replace _solr_data property with the attribute
        self.__dict__['_solr_data'] = w
        return w
        
    _solr_data = property(_get_solr_data)
    
    def get_cover(self, use_solr=True):
        covers = self.get_covers(use_solr=use_solr)
        return covers and covers[0] or None
    
    def get_cover_url(self, size, use_solr=True):
        cover = self.get_cover(use_solr=use_solr)
        return cover and cover.url(size)
        
    def get_authors(self):
        authors =  [a.author for a in self.authors]
        authors = [follow_redirect(a) for a in authors]
        authors = [a for a in authors if a and a.type.key == "/type/author"]
        return authors
    
    def get_subjects(self):
        """Return subject strings."""
        subjects = self.subjects
        
        def flip(name):
            if name.count(",") == 1:
                a, b = name.split(",")
                return b.strip() + " " + a.strip()
            return name
                
        if subjects and not isinstance(subjects[0], basestring):
            subjects = [flip(s.name) for s in subjects]
        return subjects
        
    def get_sorted_editions(self):
        """Return a list of works sorted by publish date"""
        w = self._solr_data
        editions = w and w.get('edition_key') or []

        # solr is stale
        if len(editions) < self.edition_count:
            q = {"type": "/type/edition", "works": self.key, "limit": 10000}
            editions = [k[len("/books/"):] for k in web.ctx.site.things(q)]
        
        if editions:
            return web.ctx.site.get_many(["/books/" + olid for olid in editions])
        else:
            return []

    def has_ebook(self):
        w = self._solr_data or {}
        return w.get("has_fulltext", False)

    first_publish_year = property(lambda self: self._solr_data.get("first_publish_year"))
        
    def get_edition_covers(self):
        editions = web.ctx.site.get_many(web.ctx.site.things({"type": "/type/edition", "works": self.key, "limit": 1000}))
        exisiting = set(int(c.id) for c in self.get_covers())
        covers = [e.get_cover() for e in editions]
        return [c for c in covers if c and int(c.id) not in exisiting]

class Subject(client.Thing):
    def _get_solr_result(self):
        if not self._solr_result:
            name = self.name or ""
            q = {'subjects': name, "facets": True}
            self._solr_result = SearchProcessor().search(q)
        return self._solr_result
        
    def get_related_subjects(self):
        # dummy subjects
        return [web.storage(name='France', key='/subjects/places/France'), web.storage(name='Travel', key='/subjects/Travel')]
        
    def get_covers(self, offset=0, limit=20):
        editions = self.get_editions(offset, limit)
        olids = [e['key'].split('/')[-1] for e in editions]
        
        try:
            url = '%s/b/query?cmd=ids&olid=%s' % (get_coverstore_url(), ",".join(olids))
            data = urllib2.urlopen(url).read()
            cover_ids = simplejson.loads(data)
        except IOError, e:
            print >> web.debug, 'ERROR in getting cover_ids', str(e) 
            cover_ids = {}
            
        def make_cover(edition):
            edition = dict(edition)
            edition.pop('type', None)
            edition.pop('subjects', None)
            edition.pop('languages', None)
            
            olid = edition['key'].split('/')[-1]
            if olid in cover_ids:
                edition['cover_id'] = cover_ids[olid]
            
            return edition
            
        return [make_cover(e) for e in editions]
    
    def get_edition_count(self):
        d = self._get_solr_result()
        return d['matches']
        
    def get_editions(self, offset, limit=20):
        if self._solr_result and offset+limit < len(self._solr_result):
            result = self._solr_result[offset:offset+limit]
        else:
            name = self.name or ""
            result = SearchProcessor().search({"subjects": name, 'offset': offset, 'limit': limit})
        return result['docs']
        
    def get_author_count(self):
        d = self._get_solr_result()
        return len(d['facets']['authors'])
        
    def get_authors(self):
        d = self._get_solr_result()
        return [web.storage(name=a, key='/authors/OL1A', count=count) for a, count in d['facets']['authors']]
    
    def get_publishers(self):
        d = self._get_solr_result()
        return [web.storage(name=p, count=count) for p, count in d['facets']['publishers']]


class SubjectPlace(Subject):
    pass
    

class SubjectPerson(Subject):
    pass


class User(models.User):
    
    def get_name(self):
        return self.displayname or self.key.split('/')[-1]
    name = property(get_name)
    
    def get_edit_history(self, limit=10, offset=0):
        return web.ctx.site.versions({"author": self.key, "limit": limit, "offset": offset})
                
    def get_creation_info(self):
        if web.ctx.path.startswith("/admin"):
            d = web.ctx.site.versions({'key': self.key, "sort": "-created", "limit": 1})[0]
            return web.storage({"ip": d.ip, "member_since": d.created})
            
    def get_edit_count(self):
        if web.ctx.path.startswith("/admin"):
            return web.ctx.site._request('/count_edits_by_user', data={"key": self.key})
        else:
            return 0
            
    def get_loan_count(self):
        return len(borrow.get_loans(self))
        
    def get_loans(self):
        self.update_loan_status()
        return borrow.get_loans(self)
        
    def update_loan_status(self):
        """Update the status of this user's loans."""
        loans = borrow.get_loans(self)
        for resource_id in [loan['resource_id'] for loan in loans]:
            borrow.update_loan_status(resource_id)
            
class UnitParser:
    """Parsers values like dimentions and weight.

        >>> p = UnitParser(["height", "width", "depth"])
        >>> p.parse("9 x 3 x 2 inches")
        <Storage {'units': 'inches', 'width': '3', 'depth': '2', 'height': '9'}>
        >>> p.format({"height": "9", "width": 3, "depth": 2, "units": "inches"})
        '9 x 3 x 2 inches'
    """
    def __init__(self, fields):
        self.fields = fields

    def format(self, d):
        return " x ".join(str(d.get(k, '')) for k in self.fields) + ' ' + d.get('units', '')

    def parse(self, s):
        """Parse the string and return storage object with specified fields and units."""
        pattern = "^" + " *x *".join("([0-9.]*)" for f in self.fields) + " *(.*)$"
        rx = web.re_compile(pattern)
        m = rx.match(s)
        return m and web.storage(zip(self.fields + ["units"], m.groups()))

class Changeset(client.Changeset):
    def can_undo(self):
        return False

    def _get_doc(self, key, revision):
        if revision == 0:
            return {
                "key": key,
                "type": {"key": "/type/delete"}
            }
        else:
            d = web.ctx.site.get(key, revision).dict()
            if d['type']['key'] == '/type/edition':
                d.pop('authors', None)
            return d

    def process_docs_before_undo(self, docs):
        """Hook to process docs before saving for undo.

        This is called by _undo method to allow subclasses to check
        for validity or redirects so that undo doesn't fail.

        The subclasses may overwrite this as required.
        """
        return docs
        
    def _undo(self):
        """Undo this transaction."""
        docs = [self._get_doc(c['key'], c['revision']-1) for c in self.changes]
        docs = self.process_docs_before_undo(docs)

        data = {
            "parent_changeset": self.id
        }
        comment = 'undo ' + self.comment
        return web.ctx.site.save_many(docs, action="undo", data=data, comment=comment)
            
    def get_undo_changeset(self):
        """Returns the changeset that undone this transaction if one exists, None otherwise.
        """
        try:
            return self._undo_changeset
        except AttributeError:
            pass
        
        changesets = web.ctx.site.recentchanges({
            "kind": "undo", 
            "data": {
                "parent_changeset": self.id
            }
        })
        # return the first undo changeset
        self._undo_changeset = changesets and changesets[-1] or None
        return self._undo_changeset
        
class NewAccountChangeset(Changeset):
    def get_user(self):
        keys = [c.key for c in self.get_changes()]
        user_key = "/people/" + keys[0].split("/")[2]
        return web.ctx.site.get(user_key)

class MergeAuthors(Changeset):
    def can_undo(self):
        return self.get_undo_changeset() is None

    def process_docs_before_undo(self, docs):
        works = [doc for doc in docs if doc['key'].startswith("/works/")]
        for w in works:
            if w.get("authors"):
                authors = [follow_redirect(web.ctx.site.get(a['author']['key']))
                            for a in w.get('authors') 
                            if 'author' in a 
                            and 'key' in a['author']]
                w['authors'] = [{"author": {"key": a.key}} for a in authors]
        return docs        
        
    def get_master(self):
        master = self.data.get("master")
        return master and web.ctx.site.get(master, lazy=True)
        
    def get_duplicates(self):
        duplicates = self.data.get("duplicates")
        changes = dict((c['key'], c['revision']) for c in self.changes)
        
        return duplicates and [web.ctx.site.get(key, revision=changes[key]-1, lazy=True) for key in duplicates if key in changes]
        
class Undo(Changeset):
    def can_undo(self):
        return False
    
    def get_undo_of(self):
        undo_of = self.data['undo_of']
        return web.ctx.site.get_change(undo_of)
        
    def get_parent_changeset(self):
        parent = self.data['parent_changeset']
        return web.ctx.site.get_change(parent)
        
class AddBookChangeset(Changeset):
    def get_work(self):
        book = self.get_edition()
        return (book and book.works and book.works[0]) or None
    
    def get_edition(self):
        for doc in self.get_changes():
            if doc.key.startswith("/books/"):
                return doc
        
    def get_author(self):
        for doc in self.get_changes():
            if doc.key.startswith("/authors/"):
                return doc

class ListChangeset(Changeset):
    def get_added_seed(self):
        added = self.data.get("add")
        if added and len(added) == 1:
            return self.get_seed(added[0])
            
    def get_removed_seed(self):
        removed = self.data.get("remove")
        if removed and len(removed) == 1:
            return self.get_seed(removed[0])
            
    def get_list(self):
        return self.get_changes()[0]
        
    def get_seed(self, seed):
        """Returns the seed object."""
        if isinstance(seed, dict):
            seed = self._site.get(seed['key'])
        return models.Seed(self.get_list(), seed)

def setup():
    models.register_models()
    
    client.register_thing_class('/type/edition', Edition)
    client.register_thing_class('/type/author', Author)
    client.register_thing_class('/type/work', Work)

    client.register_thing_class('/type/subject', Subject)
    client.register_thing_class('/type/place', SubjectPlace)
    client.register_thing_class('/type/person', SubjectPerson)
    client.register_thing_class('/type/user', User)

    client.register_changeset_class(None, Changeset) # set the default class
    client.register_changeset_class('merge-authors', MergeAuthors)
    client.register_changeset_class('undo', Undo)

    client.register_changeset_class('add-book', AddBookChangeset)
    client.register_changeset_class('lists', ListChangeset)
    client.register_changeset_class('new-account', NewAccountChangeset)

########NEW FILE########
__FILENAME__ = recentchanges
"""New recentchanges implementation.

This should go into infogami.
"""
import web
import simplejson
import yaml

from infogami.utils import delegate
from infogami.utils.view import public, render, render_template, add_flash_message, safeint
from infogami.utils import features

from openlibrary.utils import dateutil
from utils import get_changes


@public
def recentchanges(query):
    return web.ctx.site.recentchanges(query)
    
class index2(delegate.page):
    path = "/recentchanges"
    
    def GET(self):
        if features.is_enabled("recentchanges_v2"):
            return index().render()
        else:
            return render.recentchanges()

class index(delegate.page):
    path = "/recentchanges(/[^/0-9][^/]*)"

    def is_enabled(self):
        return features.is_enabled("recentchanges_v2")    
    
    def GET(self, kind):
        return self.render(kind=kind)
    
    def render(self, date=None, kind=None):
        query = {}
        
        if date:
            begin_date, end_date = dateutil.parse_daterange(date)
            query['begin_date'] = begin_date.isoformat()
            query['end_date'] = end_date.isoformat()
        
        if kind:
            query['kind'] = kind and kind.strip("/")

        if web.ctx.encoding in ["json", "yml"]:
            return self.handle_encoding(query, web.ctx.encoding)
        
        return render_template("recentchanges/index", query)

    def handle_encoding(self, query, encoding):
        i = web.input(bot="", limit=100, offset=0, text="false")
        
        # The bot stuff is handled in the template for the regular path. 
        # We need to handle it here for api.
        if i.bot.lower() == "true":
            query['bot'] = True
        elif i.bot.lower()  == "false":
            query['bot'] = False
            
        # and limit and offset business too
        limit = safeint(i.limit, 100)
        offset = safeint(i.offset, 0)
        
        def constrain(value, low, high):
            if value < low:
                return low
            elif value > high:
                return high
            else:
                return value

        # constrain limit and offset for performance reasons
        limit = constrain(limit, 0, 1000)
        offset = constrain(offset, 0, 10000)
         
        query['limit'] = limit
        query['offset'] = offset
        
        result = [c.dict() for c in web.ctx.site.recentchanges(query)]
        
        if encoding == "json":
            response = simplejson.dumps(result)
            content_type = "application/json"
        elif encoding == "yml":
            response = self.yaml_dump(result)
            content_type = "text/x-yaml"
        else:
            response = ""
            content_type = "text/plain"
        
        if i.text.lower() == "true":
            web.header('Content-Type', 'text/plain')
        else:
            web.header('Content-Type', content_type)
        
        return delegate.RawText(response)

    def yaml_dump(self, d):
        return yaml.safe_dump(d, indent=4, allow_unicode=True, default_flow_style=False)

class index_with_date(index):
    path = "/recentchanges/(\d\d\d\d(?:/\d\d)?(?:/\d\d)?)(/[^/]*)?"
    
    def GET(self, date, kind):
        date = date.replace("/", "-")
        return self.render(kind=kind, date=date)
        
class recentchanges_redirect(delegate.page):
    path = "/recentchanges/goto/(\d+)"

    def is_enabled(self):
        return features.is_enabled("recentchanges_v2")

    def GET(self, id):
        id = int(id)
        change = web.ctx.site.get_change(id)
        if not change:
            web.ctx.status = "404 Not Found"
            return render.notfound(web.ctx.path)
    
        raise web.found(change.url())

class recentchanges_view(delegate.page):
    path = "/recentchanges/\d\d\d\d/\d\d/\d\d/[^/]*/(\d+)"
    
    def is_enabled(self):
        return features.is_enabled("recentchanges_v2")
    
    def get_change_url(self, change):
        t = change.timestamp
        return "/recentchanges/%04d/%02d/%02d/%s/%s" % (t.year, t.month, t.day, change.kind, change.id)
    
    def GET(self, id):
        id = int(id)
        change = web.ctx.site.get_change(id)
        if not change:
            web.ctx.status = "404 Not Found"
            return render.notfound(web.ctx.path)
            
        if web.ctx.encoding == 'json':
            return self.render_json(change)
            
        path = self.get_change_url(change)
        if path != web.ctx.path:
            raise web.redirect(path)
        else:
            tname = "recentchanges/" + change.kind + "/view"
            if tname in render:
                return render_template(tname, change)
            else:
                return render_template("recentchanges/default/view", change)
                
    def render_json(self, change):
        return delegate.RawText(simplejson.dumps(change.dict()), content_type="application/json")
                
    def POST(self, id):
        if not features.is_enabled("undo"):
            return render_template("permission_denied", web.ctx.path, "Permission denied to undo.")
            
        id = int(id)
        change = web.ctx.site.get_change(id)
        change._undo()
        raise web.seeother(change.url())

class history(delegate.mode):
    def GET(self, path):
        page = web.ctx.site.get(path)
        if not page:
            raise web.seeother(path)
        i = web.input(page=0)
        offset = 20 * safeint(i.page)
        limit = 20
        history = get_changes(dict(key=path, limit=limit, offset=offset))
        return render.history(page, history)

########NEW FILE########
__FILENAME__ = test_account
from .. import account
import web
import re

def test_create_list_doc(wildcard):
    key = "account/foo/verify"
    username = "foo"
    email = "foo@example.com"
    
    doc = account.create_link_doc(key, username, email)
    
    assert doc == {
        "_key": key,
        "_rev": None,
        "type": "account-link",
        "username": username,
        "email": email,
        "code": wildcard,
        "created_on": wildcard,
        "expires_on": wildcard
    }
    
def test_verify_hash():
    secret_key = "aqXwLJVOcV"
    hash = account.generate_hash(secret_key, "foo")
    assert account.verify_hash(secret_key, "foo", hash) == True


class TestAccount:
    def signup(self, b, displayname, username, password, email):
        b.open("/account/create")
        b.select_form(name="signup")

        b['displayname'] = displayname
        b['username'] = username
        b['password'] = password
        b['email'] = email
        b['agreement'] = ['yes']
        b.submit()
        
    def login(self, b, username, password):
        """Attempt login and return True if successful.
        """
        b.open("/account/login")
        b.select_form(name="register") # wrong name
        b["username"] = username
        b["password"] = password
        b.submit()
        
        return b.path == "/"

    def test_create(self, ol):
        b = ol.browser()
        self.signup(b, displayname="Foo", username="foo", password="blackgoat", email="foo@example.com")
                
        assert "Hi, foo!" in b.get_text(id="contentHead")
        assert "sent an email to foo@example.com" in b.get_text(id="contentBody")
        
        assert ol.sentmail["from_address"] == "Open Library <noreply@openlibrary.org>"
        assert ol.sentmail["to_address"] == "foo@example.com"
        assert ol.sentmail["subject"] == "Welcome to Open Library"
        link = ol.sentmail.extract_links()[0]
        
        assert re.match("^http://0.0.0.0:8080/account/verify/[0-9a-f]{32}$", link)
        
    def test_activate(self, ol):
        b = ol.browser()
        
        self.signup(b, displayname="Foo", username="foo", password="secret", email="foo@example.com")
        link = ol.sentmail.extract_links()[0]
        b.open(link)
        
        assert "Hi, Foo!" in b.get_text(id="contentHead")        
        assert "Yay! Your email address has been verified." in b.get_text(id="contentBody")
        
        self.login(b, "foo", "secret")
        
        assert b.path == "/"
        assert "Log out" in b.get_text()        

    def test_forgot_password(self, ol):
        b = ol.browser()
        
        self.signup(b, displayname="Foo", username="foo", password="secret", email="foo@example.com")
        link = ol.sentmail.extract_links()[0]
        b.open(link)
        
        b.open("/account/password/forgot")
        b.select_form(name="register") # why is the form called register?
        b['email'] = "foo@example.com"
        b.submit()
        
        assert "Thanks" in b.get_text(id="contentHead")
        assert "We've sent an email to foo@example.com with instructions" in b.get_text(id="contentBody")
        
        link = ol.sentmail.extract_links()[0]
        assert re.match("^http://0.0.0.0:8080/account/password/reset/[0-9a-f]{32}$", link)
        
        b.open(link)
        assert "Reset Password" in b.get_text(id="contentHead")
        assert "Please enter a new password for your Open Library account" in b.get_text(id="contentBody")
        b.select_form(name="reset")
        b['password'] = "secret2"
        b.submit()
                
        self.login(b, "foo", "secret2")
        assert b.path == "/"
        assert "Log out" in b.get_text()
        
        b.reset()
        self.login(b, "foo", "secret")
        assert b.path == "/account/login"
        assert "That password seems incorrect" in b.get_text()

    def test_change_password(self, ol):
        b = ol.browser()
        self.signup(b, displayname="Foo", username="foo", password="secret", email="foo@example.com")
        link = ol.sentmail.extract_links()[0]
        b.open(link)
        self.login(b, "foo", "secret")
        
        b.open("/account/password")
        b.select_form(name="register")
        b['password'] = "secret"
        b['new_password'] = "more_secret"
        b.submit()
        
        assert b.path == "/account"
        
        b.reset()
        assert self.login(b, "foo", "more_secret") == True

    def test_change_email(self, ol):
        b = ol.browser()
        self.signup(b, displayname="Foo", username="foo", password="secret", email="foo@example.com")

        link = ol.sentmail.extract_links()[0]
        b.open(link)

        self.login(b, "foo", "secret")

        b.open("/account/email")
        
        assert "foo@example.com" in b.data
        
        b.select_form(name="register")
        b['email'] = "foobar@example.com"
        b.submit()
        
        assert "Hi Foo" in b.get_text(id="contentHead")
        assert "We've sent an email to foobar@example.com" in b.get_text(id="contentBody")
        
        link = ol.sentmail.extract_links()[0]
        b.open(link)
        assert "Email verification successful" in b.get_text(id="contentHead")
        
        b.open("/account/email")
        assert "foobar@example.com" in b.data
        
########NEW FILE########
__FILENAME__ = test_addbook
"""py.test tests for addbook"""
import web
from .. import addbook

def strip_nones(d):
    return dict((k, v) for k, v in d.items() if v is not None)

class TestSaveBookHelper:
    def test_authors(self):
        s = addbook.SaveBookHelper(None, None)
        def f(data):
            return strip_nones(s.process_work(web.storage(data)))
        
        assert f({}) == {}
        assert f({"authors": []}) == {}
        assert f({"authors": [{"type": "/type/author_role"}]}) == {}    
########NEW FILE########
__FILENAME__ = test_merge_authors
import web

from infogami.infobase import client, common
from infogami.utils import delegate

from infogami.infobase.tests.pytest_wildcard import *

from .. import merge_authors

def setup_module(mod):
    #delegate.fakeload()

    # models module imports openlibrary.code, which imports ol_infobase and that expects db_parameters.
    web.config.db_parameters = dict(dbn="sqlite", db=":memory:")
    from .. import models
    models.setup()

class MockSite(client.Site):
    def __init__(self):
        self.docs = {}
        self.query_results = {}
        
    def get(self, key):
        doc = self.docs[key]
        
        # black magic
        data = self._process_dict(common.parse_query(doc))
        return client.create_thing(self, key, data)
        
    def things(self, query):
        return self.query_results.get(merge_authors.dicthash(query), [])
        
    def add_query(self, query, result):
        self.query_results[merge_authors.dicthash(query)] = result
        
    def get_dict(self, key):
        return self.get(key).dict()
    
    def get_many(self, keys):
        return [self.get(k) for k in keys]
        
    def _get_backreferences(self, thing):
        return {}
        
    def save_many(self, docs, comment=None, data={}, action=None):
        self.add(docs)
        
    def add(self, docs):
        self.docs.update((doc['key'], doc) for doc in docs)

def test_MockSite():
    site = MockSite()
    assert site.docs.keys() == []
    
    site.add([{
            "key": "a",
            "type": {"key": "/type/object"}
        }, {
            "key": "b",
            "type": {"key": "/type/object"}
        }
    ])
    assert site.docs.keys() == ["a", "b"]
    
testdata = web.storage({
    "a": {
        "key": "/authors/a",
        "type": {"key": "/type/author"},
        "name": "a"
    },
    "b": {
        "key": "/authors/b",
        "type": {"key": "/type/author"},
        "name": "b"
    },
    "c": {
        "key": "/authors/c",
        "type": {"key": "/type/author"},
        "name": "c"
    }
})

class TestBasicMergeEngine:
    def setup_method(self, method):
        self.engine = merge_authors.BasicMergeEngine()
        web.ctx.site = MockSite()
        
    def test_make_redirect_doc(self):
        assert self.engine.make_redirect_doc("/a", "/b") == {
            "key": "/a",
            "type": {"key": "/type/redirect"},
            "location": "/b"
        }
    
    def test_convert_doc(self):
        doc = {
            "key": "/a",
            "type": {"key": "/type/object"},
            "x1": [{"key": "/b"}],
            "x2": [{"key": "/b"}, {"key": "/c"}],
            "y1": {
                "a": "foo",
                "b": {"key": "/b"}
            },
            "y2": [{
                "a": "foo",
                "b": {"key": "/b"}
            }, {
                "a": "foo",
                "b": {"key": "/c"}
            }]
        }
        
        assert self.engine.convert_doc(doc, "/c", ["/b"]) == {
            "key": "/a",
            "type": {"key": "/type/object"},
            "x1": [{"key": "/c"}],
            "x2": [{"key": "/c"}],
            "y1": {
                "a": "foo",
                "b": {"key": "/c"}
            },
            "y2": [{
                "a": "foo",
                "b": {"key": "/c"}
            }]
        }
    
    def test_merge_property(self):
        assert self.engine.merge_property(None, "hello") == "hello"
        assert self.engine.merge_property("hello", None) == "hello"
        assert self.engine.merge_property("foo", "bar") == "foo"
        assert self.engine.merge_property(["foo"], ["bar"]) == ["foo", "bar"]
        assert self.engine.merge_property(None, ["bar"]) == ["bar"]
 
class TestAuthorMergeEngine:
    def setup_method(self, method):
        self.engine = merge_authors.AuthorMergeEngine()
        web.ctx.site = MockSite()
        
    def test_get_many(self):
        # get_many should handle bad table_of_contents in the edition.
        edition = {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "table_of_contents": [{
                "type": "/type/text",
                "value": "foo"
            }]
        }
        type_edition = {
            "key": "/type/edition",
            "type": {"key": "/type/type"}
        }
        web.ctx.site.add([edition, type_edition])
        
        t = web.ctx.site.get("/books/OL1M")
        
        assert web.ctx.site.get("/books/OL1M").type.key == "/type/edition"
        
        assert self.engine.get_many(["/books/OL1M"])[0] == {
            "key": "/books/OL1M",
            "type": {"key": "/type/edition"},
            "table_of_contents": [{
                "label": "",
                "level": 0,
                "pagenum": "",
                "title": "foo"
            }]
        }
        
    def test_fix_edition(self):
        edition = {
            "key": "/books/OL2M",
            "authors": [{"key": "/authors/OL2A"}],
            "title": "book 1"
        }
        
        # edition having duplicate author
        self.engine.convert_doc(edition, "/authors/OL1A", ["/authors/OL2A"]) == {
            "key": "/books/OL2M",
            "authors": [{"key": "/authors/OL1A"}],
            "title": "book 1"
        }

        # edition not having duplicate author
        self.engine.convert_doc(edition, "/authors/OL1A", ["/authors/OL3A"]) == {
            "key": "/books/OL2M",
            "authors": [{"key": "/authors/OL2A"}],
            "title": "book 1"
        }        
        
    def test_fix_work(self):
        work = {
            "key": "/works/OL2W",
            "authors": [{
                "type": {"key": "/type/author_role"},
                "author": {"key": "/authors/OL2A"}
            }],
            "title": "book 1"
        }
        
        # work having duplicate author
        self.engine.convert_doc(work, "/authors/OL1A", ["/authors/OL2A"]) == {
            "key": "/works/OL2W",
            "authors": [{
                "type": {"key": "/type/author_role"},
                "author": {"key": "/authors/OL1A"}
            }],
            "title": "book 1"
        }

        # work not having duplicate author
        self.engine.convert_doc(work, "/authors/OL1A", ["/authors/OL3A"]) == {
            "key": "/works/OL2W",
            "authors": [{
                "type": {"key": "/type/author_role"},
                "author": {"key": "/authors/OL2A"}
            }],
            "title": "book 1"
        }
        
    def test_redirection(self):
        web.ctx.site.add([testdata.a, testdata.b, testdata.c])
        self.engine.merge("/authors/a", ["/authors/b", "/authors/c"])
        
        # assert redirection
        assert web.ctx.site.get("/authors/b").dict() == {
            "key": "/authors/b",
            "type": {"key": "/type/redirect"},
            "location": "/authors/a"
        }
        assert web.ctx.site.get("/authors/c").dict() == {
            "key": "/authors/c",
            "type": {"key": "/type/redirect"},
            "location": "/authors/a"
        }
        
    def test_alternate_names(self):
        web.ctx.site.add([testdata.a, testdata.b, testdata.c])
        self.engine.merge("/authors/a", ["/authors/b", "/authors/c"])
        assert web.ctx.site.get("/authors/a").alternate_names == ["b", "c"]

    def test_photos(self):
        a = dict(testdata.a, photos=[1, 2])
        b = dict(testdata.b, photos=[3, 4])

        web.ctx.site.add([a, b])
        self.engine.merge("/authors/a", ["/authors/b"])
        
        photos = web.ctx.site.get("/authors/a").photos
        assert photos == [1, 2, 3, 4]
        
    def test_links(self):
        link_a = {
            "title": "link a",
            "url": "http://example.com/a"
        }
        link_b = {
            "title": "link b",
            "url": "http://example.com/b"
        }
        
        a = dict(testdata.a, links=[link_a])
        b = dict(testdata.b, links=[link_b])
        web.ctx.site.add([a, b])
        
        self.engine.merge("/authors/a", ["/authors/b"])
        links = web.ctx.site.get("/authors/a").dict()['links']
        assert links == [link_a, link_b]
        
    def test_new_field(self):
        """When the duplicate has a new field which is not there in the master,
        the new filed must be copied to the master.
        """
        birth_date = "1910-01-02"
        a = testdata.a
        b = dict(testdata.b, birth_date=birth_date)
        web.ctx.site.add([a, b])
        
        self.engine.merge("/authors/a", ["/authors/b"])
        master_birth_date = web.ctx.site.get("/authors/a").get('birth_date')
        assert master_birth_date == birth_date
        
    def test_work_authors(self):
        a = testdata.a
        b = testdata.b
        work_b = {
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "authors": [{
                "type": "/type/author_role",
                "author": {"key": "/authors/b"}
            }]
        }
        web.ctx.site.add([a, b, work_b])
        
        q = {
            "type": "/type/work", 
            "authors": {
                "author": {"key": "/authors/b"}
            },
            "limit": 10000
        }
        web.ctx.site.add_query(q, ["/works/OL1W"])
        
        self.engine.merge("/authors/a", ["/authors/b"])
        assert web.ctx.site.get_dict("/works/OL1W") == {
            "key": "/works/OL1W",
            "type": {"key": "/type/work"},
            "authors": [{
                "type": "/type/author_role",
                "author": {"key": "/authors/a"}
            }]
        }

def test_dicthash():
    uniq = merge_authors.uniq
    dicthash = merge_authors.dicthash
    
    a = {"a": 1}
    assert uniq([a, a], key=dicthash) == [a]
########NEW FILE########
__FILENAME__ = utils
import web
import simplejson
import babel, babel.core, babel.dates
from UserDict import DictMixin
from collections import defaultdict
import random
import urllib
import xml.etree.ElementTree as etree
import datetime
import gzip
import StringIO
import logging

from infogami import config
from infogami.utils import view, delegate, stats
from infogami.utils.view import render, get_template, public
from infogami.utils.macro import macro
from infogami.utils.context import context
from infogami.infobase.client import Thing, Changeset, storify

from openlibrary.core.helpers import commify, parse_datetime
from openlibrary.core.middleware import GZipMiddleware
from openlibrary.core import cache, ab
    
class MultiDict(DictMixin):
    """Ordered Dictionary that can store multiple values.
    
        >>> d = MultiDict()
        >>> d['x'] = 1
        >>> d['x'] = 2
        >>> d['y'] = 3
        >>> d['x']
        2
        >>> d['y']
        3
        >>> d['z']
        Traceback (most recent call last):
            ...
        KeyError: 'z'
        >>> d.keys()
        ['x', 'x', 'y']
        >>> d.items()
        [('x', 1), ('x', 2), ('y', 3)]
        >>> d.multi_items()
        [('x', [1, 2]), ('y', [3])]
    """
    def __init__(self, items=(), **kw):
        self._items = []
        
        for k, v in items:
            self[k] = v
        self.update(kw)
    
    def __getitem__(self, key):
        values = self.getall(key)
        if values:
            return values[-1]
        else:
            raise KeyError, key
    
    def __setitem__(self, key, value):
        self._items.append((key, value))
        
    def __delitem__(self, key):
        self._items = [(k, v) for k, v in self._items if k != key]
        
    def getall(self, key):
        return [v for k, v in self._items if k == key]
        
    def keys(self):
        return [k for k, v in self._items]
        
    def values(self):
        return [v for k, v in self._items]
        
    def items(self):
        return self._items[:]
        
    def multi_items(self):
        """Returns items as tuple of key and a list of values."""
        items = []
        d = {}
        
        for k, v in self._items:
            if k not in d:
                d[k] = []
                items.append((k, d[k]))
            d[k].append(v)
        return items
        
@macro
@public
def render_template(name, *a, **kw):
    if "." in name:
        name = name.rsplit(".", 1)[0]
    return render[name](*a, **kw)
    
@public
def get_error(name, *args):
    """Return error with the given name from errors.tmpl template."""
    return get_message_from_template("errors", name, args)
        
@public        
def get_message(name, *args):
    """Return message with given name from messages.tmpl template"""
    return get_message_from_template("messages", name, args)
    
def get_message_from_template(template_name, name, args):
    d = render_template(template_name).get("messages", {})
    msg = d.get(name) or name.lower().replace("_", " ")
    
    if msg and args:
        return msg % args
    else:
        return msg
    
@public
def list_recent_pages(path, limit=100, offset=0):
    """Lists all pages with name path/* in the order of last_modified."""
    q = {}
        
    q['key~' ] = path + '/*'
    # don't show /type/delete and /type/redirect
    q['a:type!='] = '/type/delete'
    q['b:type!='] = '/type/redirect'
    
    q['sort'] = 'key'
    q['limit'] = limit
    q['offset'] = offset
    q['sort'] = '-last_modified'
    # queries are very slow with != conditions
    # q['type'] != '/type/delete'
    return web.ctx.site.get_many(web.ctx.site.things(q))
        
@public
def json_encode(d):
    return simplejson.dumps(d)

def unflatten(d, seperator="--"):
    """Convert flattened data into nested form.
    
        >>> unflatten({"a": 1, "b--x": 2, "b--y": 3, "c--0": 4, "c--1": 5})
        {'a': 1, 'c': [4, 5], 'b': {'y': 3, 'x': 2}}
        >>> unflatten({"a--0--x": 1, "a--0--y": 2, "a--1--x": 3, "a--1--y": 4})
        {'a': [{'x': 1, 'y': 2}, {'x': 3, 'y': 4}]}
        
    """
    def isint(k):
        try:
            int(k)
            return True
        except ValueError:
            return False
        
    def setvalue(data, k, v):
        if '--' in k:
            k, k2 = k.split(seperator, 1)
            setvalue(data.setdefault(k, {}), k2, v)
        else:
            data[k] = v
            
    def makelist(d):
        """Convert d into a list if all the keys of d are integers."""
        if isinstance(d, dict):
            if all(isint(k) for k in d.keys()):
                return [makelist(d[k]) for k in sorted(d.keys(), key=int)]
            else:
                return web.storage((k, makelist(v)) for k, v in d.items())
        else:
            return d
            
    d2 = {}
    for k, v in d.items():
        setvalue(d2, k, v)
    return makelist(d2)

def fuzzy_find(value, options, stopwords=[]):
    """Try find the option nearest to the value.
    
        >>> fuzzy_find("O'Reilly", ["O'Reilly Inc", "Addison-Wesley"])
        "O'Reilly Inc"
    """
    if not options:
        return value
        
    rx = web.re_compile("[-_\.&, ]+")
    
    # build word frequency
    d = defaultdict(list)
    for option in options:
        for t in rx.split(option):
            d[t].append(option)
    
    # find score for each option
    score = defaultdict(lambda: 0)
    for t in rx.split(value):
        if t.lower() in stopwords:
            continue
        for option in d[t]:
            score[option] += 1
            
    # take the option with maximum score
    return max(options, key=score.__getitem__)
    
@public
def radio_input(checked=False, **params):
    params['type'] = 'radio'
    if checked:
        params['checked'] = "checked"
    return "<input %s />" % " ".join(['%s="%s"' % (k, web.websafe(v)) for k, v in params.items()])
    
@public
def radio_list(name, args, value):
    html = []
    for arg in args:
        if isinstance(arg, tuple):
            arg, label = arg
        else:
            label = arg
        html.append(radio_input())
        
@public
def get_coverstore_url():
    return config.get('coverstore_url', 'http://covers.openlibrary.org').rstrip('/')
    
def _get_changes_v1_raw(query, revision=None):
    """Returns the raw versions response. 
    
    Revision is taken as argument to make sure a new cache entry is used when a new revision of the page is created.
    """
    if 'env' not in web.ctx:
        delegate.fakeload()
    
    versions = web.ctx.site.versions(query)
    
    for v in versions:
        v.created = v.created.isoformat()
        v.author = v.author and v.author.key
        
        # XXX-Anand: hack to avoid too big data to be stored in memcache.
        # v.changes is not used and it contrinutes to memcache bloat in a big way.
        v.changes = '[]'
            
    return versions

def get_changes_v1(query, revision=None):
    # uses the cached function _get_changes_v1_raw to get the raw data
    # and processes to before returning.
    def process(v):
        v = web.storage(v)
        v.created = parse_datetime(v.created)
        v.author = v.author and web.ctx.site.get(v.author, lazy=True)
        return v
    
    return [process(v) for v in _get_changes_v1_raw(query, revision)]
    
def _get_changes_v2_raw(query, revision=None):
    """Returns the raw recentchanges response. 
    
    Revision is taken as argument to make sure a new cache entry is used when a new revision of the page is created.
    """
    if 'env' not in web.ctx:
        delegate.fakeload()
    
    changes = web.ctx.site.recentchanges(query)
    return [c.dict() for c in changes]

# XXX-Anand: disabled temporarily to avoid too much memcache usage.
#_get_changes_v2_raw = cache.memcache_memoize(_get_changes_v2_raw, key_prefix="upstream._get_changes_v2_raw", timeout=10*60)

def get_changes_v2(query, revision=None):
    page = web.ctx.site.get(query['key'])
    
    def first(seq, default=None):
        try:
            return seq.next()
        except StopIteration:
            return default
    
    def process_change(change):
        change = Changeset.create(web.ctx.site, storify(change))
        change.thing = page
        change.key = page.key
        change.revision = first(c.revision for c in change.changes if c.key == page.key)
        change.created = change.timestamp

        change.get = change.__dict__.get
        change.get_comment = lambda: get_comment(change)        
        change.machine_comment = change.data.get("machine_comment")
    
        return change
        
    def get_comment(change):
        t = get_template("recentchanges/" + change.kind + "/comment") or get_template("recentchanges/default/comment")
        return t(change, page)

    query['key'] = page.key
    changes = _get_changes_v2_raw(query, revision=page.revision)
    return [process_change(c) for c in changes]
    
def get_changes(query, revision=None):
    if 'history_v2' in web.ctx.features:
        return get_changes_v2(query, revision=revision)
    else:
        return get_changes_v1(query, revision=revision)
        
@public
def get_history(page):
    h = web.storage(revision=page.revision, lastest_revision=page.revision, created=page.created)
    if h.revision < 5:
        h.recent = get_changes({"key": page.key, "limit": 5}, revision=page.revision)
        h.initial = h.recent[-1:]
        h.recent = h.recent[:-1]
    else:
        h.initial = get_changes({"key": page.key, "limit": 1, "offset": h.revision-1}, revision=page.revision)
        h.recent = get_changes({"key": page.key, "limit": 4}, revision=page.revision)
    
    return h
    
    if 'history_v2' in web.ctx.features:
        return get_history_v2(page)
    else:
        return get_history_v1(page)    
    
@public
def get_version(key, revision):
    try:
        return web.ctx.site.versions({"key": key, "revision": revision, "limit": 1})[0]
    except IndexError:
        return None

@public
def get_recent_author(doc):
    versions = get_changes_v1({'key': doc.key, 'limit': 1, "offset": 0}, revision=doc.revision)
    if versions:
        return versions[0].author

@public
def get_recent_accounts(limit=5, offset=0):
    versions = web.ctx.site.versions({'type': '/type/user', 'revision': 1, 'limit': limit, 'offset': offset})
    return web.ctx.site.get_many([v.key for v in versions])

def get_locale():
    try:
        return babel.Locale(web.ctx.get("lang") or "en")
    except babel.core.UnknownLocaleError:
        return babel.Locale("en")
    
@public
def process_version(v):
    """Looks at the version and adds machine_comment required for showing "View MARC" link."""
    importers = set(['/people/ImportBot', '/people/EdwardBot', '/people/LCImportBot'])
    comments = [
        "found a matching marc record",
        "add publisher and source",
    ]
    if v.author and v.author.key in importers and v.key.startswith('/books/') and not v.get('machine_comment'):
        thing = v.get('thing') or web.ctx.site.get(v.key, v.revision)
        if thing.source_records and v.revision == 1 or (v.comment and v.comment.lower() in comments):
            marc = thing.source_records[-1]
            if marc.startswith('marc:'):
                v.machine_comment = marc[len("marc:"):]
            else:
                v.machine_comment = marc
    return v

@public
def is_thing(t):
    return isinstance(t, Thing)
    
@public
def putctx(key, value):
    """Save a value in the context."""
    context[key] = value
    return ""
    
def pad(seq, size, e=None):
    """
        >>> pad([1, 2], 4, 0)
        [1, 2, 0, 0]
    """
    seq = seq[:]
    while len(seq) < size:
        seq.append(e)
    return seq

def parse_toc_row(line):
    """Parse one row of table of contents.

        >>> def f(text):
        ...     d = parse_toc_row(text)
        ...     return (d['level'], d['label'], d['title'], d['pagenum'])
        ...
        >>> f("* chapter 1 | Welcome to the real world! | 2")
        (1, 'chapter 1', 'Welcome to the real world!', '2')
        >>> f("Welcome to the real world!")
        (0, '', 'Welcome to the real world!', '')
        >>> f("** | Welcome to the real world! | 2")
        (2, '', 'Welcome to the real world!', '2')
        >>> f("|Preface | 1")
        (0, '', 'Preface', '1')
        >>> f("1.1 | Apple")
        (0, '1.1', 'Apple', '')
    """
    RE_LEVEL = web.re_compile("(\**)(.*)")
    level, text = RE_LEVEL.match(line.strip()).groups()

    if "|" in text:
        tokens = text.split("|", 2)
        label, title, page = pad(tokens, 3, '')
    else:
        title = text
        label = page = ""

    return web.storage(level=len(level), label=label.strip(), title=title.strip(), pagenum=page.strip())

def parse_toc(text):
    """Parses each line of toc"""
    if text is None:
        return []
    return [parse_toc_row(line) for line in text.splitlines() if line.strip(" |")]

_languages = None
    
@public
def get_languages():
    global _languages
    if _languages is None:
        keys = web.ctx.site.things({"type": "/type/language", "key~": "/languages/*", "limit": 1000})
        _languages = sorted([web.storage(name=d.name, code=d.code, key=d.key) for d in web.ctx.site.get_many(keys)], key=lambda d: d.name.lower())
    return _languages
    
@public
def get_edition_config():
    return _get_edition_config()
    
@web.memoize
def _get_edition_config():
    """Returns the edition config.
    
    The results are cached on the first invocation. Any changes to /config/edition page require restarting the app.
    
    This is is cached because fetching and creating the Thing object was taking about 20ms of time for each book request.
    """
    thing = web.ctx.site.get('/config/edition')
    classifications = [web.storage(t.dict()) for t in thing.classifications if 'name' in t]
    identifiers = [web.storage(t.dict()) for t in thing.identifiers if 'name' in t]
    roles = thing.roles
    return web.storage(classifications=classifications, identifiers=identifiers, roles=roles)

from openlibrary.core.olmarkdown import OLMarkdown
def get_markdown(text, safe_mode=False):
    md = OLMarkdown(source=text, safe_mode=safe_mode)
    view._register_mdx_extensions(md)
    md.postprocessors += view.wiki_processors
    return md


class HTML(unicode):
    def __init__(self, html):
        unicode.__init__(self, web.safeunicode(html))
        
    def __repr__(self):
        return "<html: %s>" % unicode.__repr__(self)

_websafe = web.websafe
def websafe(text):
    if isinstance(text, HTML):
        return text
    elif isinstance(text, web.template.TemplateResult):
        return web.safestr(text)
    else:
        return _websafe(text)
        

from openlibrary.utils.olcompress import OLCompressor
from openlibrary.utils import olmemcache
import adapter
import memcache

class UpstreamMemcacheClient:
    """Wrapper to memcache Client to handle upstream specific conversion and OL specific compression.
    Compatible with memcache Client API.
    """
    def __init__(self, servers):
        self._client = memcache.Client(servers)
        compressor = OLCompressor()
        self.compress = compressor.compress
        def decompress(*args, **kw):
            d = simplejson.loads(compressor.decompress(*args, **kw))
            return simplejson.dumps(adapter.unconvert_dict(d))
        self.decompress = decompress

    def get(self, key):
        key = adapter.convert_key(key)
        if key is None:
            return None
        
        try:
            value = self._client.get(web.safestr(key))
        except memcache.Client.MemcachedKeyError:
            return None

        return value and self.decompress(value)

    def get_multi(self, keys):
        keys = [adapter.convert_key(k) for k in keys]
        keys = [web.safestr(k) for k in keys]
        
        d = self._client.get_multi(keys)
        return dict((web.safeunicode(adapter.unconvert_key(k)), self.decompress(v)) for k, v in d.items())

if config.get('upstream_memcache_servers'):
    olmemcache.Client = UpstreamMemcacheClient
    # set config.memcache_servers only after olmemcache.Client is updated
    config.memcache_servers = config.upstream_memcache_servers
    
def _get_recent_changes():
    site = web.ctx.get('site') or delegate.create_site()
    web.ctx.setdefault("ip", "127.0.0.1")
    
    # The recentchanges can have multiple revisions for a document if it has been modified more than once. 
    # Take only the most recent revision in that case.
    visited = set()
    def is_visited(key):
        if key in visited:
            return True
        else:
            visited.add(key)
            return False
       
    # ignore reverts
    re_revert = web.re_compile("reverted to revision \d+")
    def is_revert(r):
        return re_revert.match(r.comment or "")

    # take the 100 recent changes, filter them and take the first 50
    q = {"bot": False, "limit": 100}
    result = site.versions(q)
    result = [r for r in result if not is_visited(r.key) and not is_revert(r)]
    result = result[:50]

    def process_thing(thing):
        t = web.storage()
        for k in ["key", "title", "name", "displayname"]:
            t[k] = thing[k]
        t['type'] = web.storage(key=thing.type.key)
        return t
    
    for r in result:
        r.author = r.author and process_thing(r.author)
        r.thing = process_thing(site.get(r.key, r.revision))
        
    return result
    
def _get_recent_changes2():
    """New recent changes for around the library.
    
    This function returns the message to display for each change.
    The message is get by calling `recentchanges/$kind/message.html` template.
    
    If `$var ignore=True` is set by the message template, the change is ignored.
    """
    if 'env' not in web.ctx:
        delegate.fakeload()
    
    q = {"bot": False, "limit": 100}
    changes = web.ctx.site.recentchanges(q)
    
    def render(c):
        t = get_template("recentchanges/" + c.kind + "/message") or get_template("recentchanges/default/message")
        return t(c)

    messages = [render(c) for c in changes]
    messages = [m for m in messages if str(m.get("ignore", "false")).lower() != "true"]
    return messages
    
_get_recent_changes = web.memoize(_get_recent_changes, expires=5*60, background=True)
_get_recent_changes2 = web.memoize(_get_recent_changes2, expires=5*60, background=True)

@public
def get_random_recent_changes(n):
    if "recentchanges_v2" in web.ctx.get("features", []):
        changes = _get_recent_changes2()
    else:
        changes = _get_recent_changes()
    
    if len(changes) > n:
        return random.sample(changes, n)  
    else:
        return changes
        
def _get_blog_feeds():
    url = "http://blog.openlibrary.org/feed/"
    try:
        stats.begin("get_blog_feeds", url=url)
        tree = etree.parse(urllib.urlopen(url))
    except Exception:
        # Handle error gracefully.
        logging.getLogger("openlibrary").error("Failed to fetch blog feeds", exc_info=True)
        return []
    finally:
        stats.end()
    
    def parse_item(item):
        pubdate = datetime.datetime.strptime(item.find("pubDate").text, '%a, %d %b %Y %H:%M:%S +0000').isoformat()
        return dict(
            title=item.find("title").text,
            link=item.find("link").text,
            pubdate=pubdate
        )
    return [parse_item(item) for item in tree.findall("//item")]
    
_get_blog_feeds = cache.memcache_memoize(_get_blog_feeds, key_prefix="upstream.get_blog_feeds", timeout=5*60)

@public
def get_blog_feeds():
    def process(post):
        post = web.storage(post)
        post.pubdate = parse_datetime(post.pubdate)
        return post
    return [process(post) for post in _get_blog_feeds()]

class Request:
    path = property(lambda self: web.ctx.path)
    home = property(lambda self: web.ctx.home)
    domain = property(lambda self: web.ctx.host)

    @property
    def canonical_url(self):
        """Returns the https:// version of the URL.

        Used for adding <meta rel="canonical" ..> tag in all web pages.
        Required to make OL retain the page rank after https migration.
        """
        return "https://" + web.ctx.host + web.ctx.get('readable_path', web.ctx.path) + web.ctx.query


def setup():
    """Do required initialization"""
    # monkey-patch get_markdown to use OL Flavored Markdown
    view.get_markdown = get_markdown

    # Provide alternate implementations for websafe and commify
    web.websafe = websafe
    web.template.Template.FILTERS['.html'] = websafe
    web.template.Template.FILTERS['.xml'] = websafe

    web.commify = commify
    
    web.template.Template.globals.update({
        'HTML': HTML,
        'request': Request(),
        'logger': logging.getLogger("openlibrary.template"),
        'get_ab_value': ab.get_ab_value
    })
    
    from openlibrary.core import helpers as h
    web.template.Template.globals.update(h.helpers)

    if config.get('use_gzip') == True:
        config.middleware.append(GZipMiddleware)

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = code
import web, re, urllib
from lxml.etree import XML, XMLSyntaxError
from infogami.utils import delegate, stats
from infogami import config
from infogami.utils.view import render, render_template, safeint
import simplejson as json
from openlibrary.plugins.openlibrary.processors import urlsafe
from openlibrary.utils import url_quote, read_isbn, escape_bracket
from unicodedata import normalize
import logging

ftoken_db = None

from openlibrary.plugins.search.code import search as _edition_search

logger = logging.getLogger("openlibrary.worksearch")

re_to_esc = re.compile(r'[\[\]:]')

class edition_search(_edition_search):
    path = "/search/edition"

def get_solr_select_url(host, core):
    if config.get('single_core_solr'):
        return "http://%s/solr/select" % host 
    else:
        return "http://%s/solr/%s/select" % (host, core)

if hasattr(config, 'plugin_worksearch'):
    solr_host = config.plugin_worksearch.get('solr', 'localhost')

    solr_select_url = get_solr_select_url(solr_host, 'works')

    solr_subject_host = config.plugin_worksearch.get('subject_solr', 'localhost')
    solr_subject_select_url = get_solr_select_url(solr_subject_host, 'subjects')

    solr_author_host = config.plugin_worksearch.get('author_solr', 'localhost')
    solr_author_select_url = get_solr_select_url(solr_author_host, 'authors')

    solr_edition_host = config.plugin_worksearch.get('edition_solr', 'localhost')
    solr_edition_select_url = get_solr_select_url(solr_edition_host, 'editions')
    
    default_spellcheck_count = config.plugin_worksearch.get('spellcheck_count', 10)

re_author_facet = re.compile('^(OL\d+A) (.*)$')
def read_author_facet(af):
    # example input: "OL26783A Leo Tolstoy"
    return re_author_facet.match(af).groups()

search_fields = ["key", "redirects", "title", "subtitle", "alternative_title", "alternative_subtitle", "edition_key", "by_statement", "publish_date", "lccn", "ia", "oclc", "isbn", "contributor", "publish_place", "publisher", "first_sentence", "author_key", "author_name", "author_alternative_name", "subject", "person", "place", "time"]

all_fields = search_fields + ["has_fulltext", "title_suggest", "edition_count", "publish_year", "language", "number_of_pages", "ia_count", "publisher_facet", "author_facet", "first_publish_year"] + ['%s_key' % f for f in ('subject', 'person', 'place', 'time')]

facet_fields = ["has_fulltext", "author_facet", "language", "first_publish_year", "publisher_facet", "subject_facet", "person_facet", "place_facet", "time_facet", "public_scan_b"]

facet_list_fields = [i for i in facet_fields if i not in ("has_fulltext")]

def get_language_name(code):
    l = web.ctx.site.get('/languages/' + code)
    return l.name if l else "'%s' unknown" % code

def read_facets(root):
    bool_map = dict(true='yes', false='no')
    e_facet_counts = root.find("lst[@name='facet_counts']")
    e_facet_fields = e_facet_counts.find("lst[@name='facet_fields']")
    facets = {}
    for e_lst in e_facet_fields:
        assert e_lst.tag == 'lst'
        name = e_lst.attrib['name']
        if name == 'author_facet':
            name = 'author_key'
        if name == 'has_fulltext': # boolean facets
            e_true = e_lst.find("int[@name='true']")
            true_count = e_true.text if e_true is not None else 0
            e_false = e_lst.find("int[@name='false']")
            false_count = e_false.text if e_false is not None else 0
            facets[name] = [
                ('true', 'yes', true_count),
                ('false', 'no', false_count),
            ]
            continue
        facets[name] = []
        for e in e_lst:
            if e.text == '0':
                continue
            k = e.attrib['name']
            if name == 'author_key':
                k, display = read_author_facet(k)
            elif name == 'language':
                display = get_language_name(k)
            else:
                display = k
            facets[name].append((k, display, e.text))
    return facets


re_isbn_field = re.compile('^\s*(?:isbn[:\s]*)?([-0-9X]{9,})\s*$', re.I)

re_author_key = re.compile(r'(OL\d+A)')

field_name_map = {
    'author': 'author_name',
    'authors': 'author_name',
    'by': 'author_name',
    'publishers': 'publisher',
}

all_fields += field_name_map.keys()
re_fields = re.compile('(-?' + '|'.join(all_fields) + r'):', re.I)

plurals = dict((f + 's', f) for f in ('publisher', 'author'))

re_op = re.compile(' +(OR|AND)$')

def parse_query_fields(q):
    found = [(m.start(), m.end()) for m in re_fields.finditer(q)]
    first = q[:found[0][0]].strip() if found else q.strip()
    if first:
        yield {'field': 'text', 'value': first.replace(':', '\:')}
    for field_num in range(len(found)):
        op_found = None
        f = found[field_num]
        field_name = q[f[0]:f[1]-1].lower()
        if field_name in field_name_map:
            field_name = field_name_map[field_name]
        if field_num == len(found)-1:
            v = q[f[1]:].strip()
        else:
            v = q[f[1]:found[field_num+1][0]].strip()
            m = re_op.search(v)
            if m:
                v = v[:-len(m.group(0))]
                op_found = m.group(1)
        if field_name == 'isbn':
            isbn = read_isbn(v)
            if isbn:
                v = isbn
        yield {'field': field_name, 'value': v.replace(':', '\:')}
        if op_found:
            yield {'op': op_found }

def build_q_list(param):
    q_list = []
    if 'q' in param:
        q_param = param['q'].strip()
    else:
        q_param = None
    use_dismax = False
    if q_param:
        if q_param == '*:*':
            q_list.append(q_param)
        elif 'NOT ' in q_param: # this is a hack
            q_list.append(q_param.strip())
        elif re_fields.search(q_param):
            q_list.extend(i['op'] if 'op' in i else '%s:(%s)' % (i['field'], i['value']) for i in parse_query_fields(q_param))
        else:
            isbn = read_isbn(q_param)
            if isbn:
                q_list.append('isbn:(%s)' % isbn)
            else:
                q_list.append(q_param.strip().replace(':', '\:'))
                use_dismax = True
    else:
        if 'author' in param:
            v = param['author'].strip()
            m = re_author_key.search(v)
            if m: # FIXME: 'OL123A OR OL234A'
                q_list.append('author_key:(' + m.group(1) + ')')
            else:
                v = re_to_esc.sub(lambda m:'\\' + m.group(), v)
                q_list.append('(author_name:(' + v + ') OR author_alternative_name:(' + v + '))')

        check_params = ['title', 'publisher', 'oclc', 'lccn', 'contribtor', 'subject', 'place', 'person', 'time']
        q_list += ['%s:(%s)' % (k, param[k]) for k in check_params if k in param]
        if param.get('isbn'):
            q_list.append('isbn:(%s)' % (read_isbn(param['isbn']) or param['isbn']))
    return (q_list, use_dismax)

def run_solr_query(param = {}, rows=100, page=1, sort=None, spellcheck_count=None, offset=None, fields=None):
    # called by do_search
    if spellcheck_count == None:
        spellcheck_count = default_spellcheck_count

    # use page when offset is not specified
    if offset is None:
        offset = rows * (page - 1)

    (q_list, use_dismax) = build_q_list(param)

    if fields is None:
        fields = [
            'key', 'author_name', 'author_key', 
            'title', 'subtitle', 'edition_count', 
            'ia', 'has_fulltext', 'first_publish_year', 
            'cover_i','cover_edition_key', 'public_scan_b', 
            'lending_edition_s', 'overdrive_s', 'ia_collection_s']
    fl = ','.join(fields)
    if use_dismax:
        q = web.urlquote(' '.join(q_list))
        solr_select = solr_select_url + "?defType=dismax&q.op=AND&q=%s&qf=text+title^5+author_name^5&bf=sqrt(edition_count)^10&start=%d&rows=%d&fl=%s" % (q, offset, rows, fl)
    else:
        q = web.urlquote(' '.join(q_list + ['_val_:"sqrt(edition_count)"^10']))
        solr_select = solr_select_url + "?q.op=AND&q=%s&start=%d&rows=%d&fl=%s" % (q, offset, rows, fl)
    solr_select += '&spellcheck=true&spellcheck.count=%d' % spellcheck_count
    solr_select += "&facet=true&" + '&'.join("facet.field=" + f for f in facet_fields)

    if 'public_scan' in param:
        v = param.pop('public_scan').lower()
        if v in ('true', 'false'):
            if v == 'false':
                # also constrain on print disabled since the index may not be in sync
                param.setdefault('print_disabled', 'false')
            solr_select += '&fq=public_scan_b:%s' % v

    if 'print_disabled' in param:
        v = param.pop('print_disabled').lower()
        if v in ('true', 'false'):
            solr_select += '&fq=%ssubject_key:protected_daisy' % ('-' if v == 'false' else '')

    k = 'has_fulltext'
    if k in param:
        v = param[k].lower()
        if v not in ('true', 'false'):
            del param[k]
        param[k] == v
        solr_select += '&fq=%s:%s' % (k, v)

    for k in facet_list_fields:
        if k == 'author_facet':
            k = 'author_key'
        if k not in param:
            continue
        v = param[k]
        solr_select += ''.join('&fq=%s:"%s"' % (k, url_quote(l)) for l in v if l)
    if sort:
        solr_select += "&sort=" + url_quote(sort)

    solr_select += '&wt=' + url_quote(param.get('wt', 'standard'))

    # For single-core solr, filter the results by type:work
    if config.get("single_core_solr"):
        solr_select += "&fq=type:work"

    stats.begin("solr", url=solr_select)
    reply = urllib.urlopen(solr_select).read()
    stats.end()
    return (reply, solr_select, q_list)

re_pre = re.compile(r'<pre>(.*)</pre>', re.S)

def do_search(param, sort, page=1, rows=100, spellcheck_count=None):
    (reply, solr_select, q_list) = run_solr_query(param, rows, page, sort, spellcheck_count)
    is_bad = False
    if reply.startswith('<html'):
        is_bad = True
    if not is_bad:
        try:
            root = XML(reply)
        except XMLSyntaxError:
            is_bad = True
    if is_bad:
        m = re_pre.search(reply)
        return web.storage(
            facet_counts = None,
            docs = [],
            is_advanced = bool(param.get('q')),
            num_found = None,
            solr_select = solr_select,
            q_list = q_list,
            error = (web.htmlunquote(m.group(1)) if m else reply),
        )

    spellcheck = root.find("lst[@name='spellcheck']")
    spell_map = {}
    if spellcheck is not None and len(spellcheck):
        for e in spellcheck.find("lst[@name='suggestions']"):
            assert e.tag == 'lst'
            a = e.attrib['name']
            if a in spell_map or a in ('sqrt', 'edition_count'):
                continue
            spell_map[a] = [i.text for i in e.find("arr[@name='suggestion']")]

    docs = root.find('result')
    return web.storage(
        facet_counts = read_facets(root),
        docs = docs,
        is_advanced = bool(param.get('q')),
        num_found = (int(docs.attrib['numFound']) if docs is not None else None),
        solr_select = solr_select,
        q_list = q_list,
        error = None,
        spellcheck = spell_map,
    )

def get_doc(doc): # called from work_search template
    e_ia = doc.find("arr[@name='ia']")
    first_pub = None
    e_first_pub = doc.find("int[@name='first_publish_year']")
    if e_first_pub is not None:
        first_pub = e_first_pub.text
    e_first_edition = doc.find("str[@name='first_edition']")
    first_edition = None
    if e_first_edition is not None:
        first_edition = e_first_edition.text

    work_subtitle = None
    e_subtitle = doc.find("str[@name='subtitle']")
    if e_subtitle is not None:
        work_subtitle = e_subtitle.text

    if doc.find("arr[@name='author_key']") is None:
        assert doc.find("arr[@name='author_name']") is None
        authors = []
    else:
        ak = [e.text for e in doc.find("arr[@name='author_key']")]
        an = [e.text for e in doc.find("arr[@name='author_name']")]
        authors = [web.storage(key=key, name=name, url="/authors/%s/%s" % (key, (urlsafe(name) if name is not None else 'noname'))) for key, name in zip(ak, an)]

    cover = doc.find("str[@name='cover_edition_key']")
    e_public_scan = doc.find("bool[@name='public_scan_b']")
    e_overdrive = doc.find("str[@name='overdrive_s']")
    e_lending_edition = doc.find("str[@name='lending_edition_s']")
    e_collection = doc.find("str[@name='ia_collection_s']")
    collections = set()
    if e_collection is not None:
        collections = set(e_collection.text.split(';'))

    doc = web.storage(
        key = doc.find("str[@name='key']").text,
        title = doc.find("str[@name='title']").text,
        edition_count = int(doc.find("int[@name='edition_count']").text),
        ia = [e.text for e in (e_ia if e_ia is not None else [])],
        has_fulltext = (doc.find("bool[@name='has_fulltext']").text == 'true'),
        public_scan = ((e_public_scan.text == 'true') if e_public_scan is not None else (e_ia is not None)),
        overdrive = (e_overdrive.text.split(';') if e_overdrive is not None else []),
        lending_edition = (e_lending_edition.text if e_lending_edition is not None else None),
        collections = collections,
        authors = authors,
        first_publish_year = first_pub,
        first_edition = first_edition,
        subtitle = work_subtitle,
        cover_edition_key = (cover.text if cover is not None else None),
    )

    doc.url = '/works/' + doc.key + '/' + urlsafe(doc.title)
    
    if not doc.public_scan and doc.lending_edition:
        store_doc = web.ctx.site.store.get("ebooks/books/" + doc.lending_edition) or {}
        doc.checked_out = store_doc.get("borrowed") == "true"
    else:
        doc.checked_out = "false"
    return doc

re_subject_types = re.compile('^(places|times|people)/(.*)')
subject_types = {
    'places': 'place',
    'times': 'time',
    'people': 'person',
    'subjects': 'subject',
}

re_year_range = re.compile('^(\d{4})-(\d{4})$')

def work_object(w): # called by works_by_author
    ia = w.get('ia', [])
    obj = dict(
        authors = [web.storage(key='/authors/' + k, name=n) for k, n in zip(w['author_key'], w['author_name'])],
        edition_count = w['edition_count'],
        key = '/works/' + w['key'],
        title = w['title'],
        public_scan = w.get('public_scan_b', bool(ia)),
        lending_edition = w.get('lending_edition_s', ''),
        overdrive = (w['overdrive_s'].split(';') if 'overdrive_s' in w else []),
        collections = set(w['ia_collection_s'].split(';') if 'ia_collection_s' in w else []),
        url = '/works/' + w['key'] + '/' + urlsafe(w['title']),
        cover_edition_key = w.get('cover_edition_key'),
        first_publish_year = (w['first_publish_year'] if 'first_publish_year' in w else None),
        ia = w.get('ia', [])
    )
    if obj['lending_edition']:
        doc = web.ctx.site.store.get("ebooks/books/" + obj['lending_edition']) or {}
        obj['checked_out'] = doc.get("borrowed") == "true"
    else:
        obj['checked_out'] = "false"
    
    for f in 'has_fulltext', 'subtitle':
        if w.get(f):
            obj[f] = w[f]
    return web.storage(obj)

def get_facet(facets, f, limit=None):
    return list(web.group(facets[f][:limit * 2] if limit else facets[f], 2))


re_olid = re.compile('^OL\d+([AMW])$')
olid_urls = {'A': 'authors', 'M': 'books', 'W': 'works'}

class search(delegate.page):
    def redirect_if_needed(self, i):
        params = {}
        need_redirect = False
        for k, v in i.items():
            if k in plurals:
                params[k] = None
                k = plurals[k]
                need_redirect = True
            if isinstance(v, list):
                if v == []:
                    continue
                clean = [normalize('NFC', b.strip()) for b in v]
                if clean != v:
                    need_redirect = True
                if len(clean) == 1 and clean[0] == u'':
                    clean = None
            else:
                clean = normalize('NFC', v.strip())
                if clean == '':
                    need_redirect = True
                    clean = None
                if clean != v:
                    need_redirect = True
            params[k] = clean
        if need_redirect:
            raise web.seeother(web.changequery(**params))

    def isbn_redirect(self, isbn_param):
        isbn = read_isbn(isbn_param)
        if not isbn:
            return
        editions = []
        for f in 'isbn_10', 'isbn_13':
            q = {'type': '/type/edition', f: isbn}
            editions += web.ctx.site.things(q)
        if len(editions) == 1:
            raise web.seeother(editions[0])

    def GET(self):
        global ftoken_db
        i = web.input(author_key=[], language=[], first_publish_year=[], publisher_facet=[], subject_facet=[], person_facet=[], place_facet=[], time_facet=[], public_scan_b=[])
        if i.get('ftokens') and ',' not in i.ftokens:
            token = i.ftokens
            #if ftoken_db is None:
            #    ftoken_db = dbm.open('/olsystem/ftokens', 'r')
            #if ftoken_db.get(token):
            #    raise web.seeother('/subjects/' + ftoken_db[token].decode('utf-8').lower().replace(' ', '_'))

        if i.get('wisbn'):
            i.isbn = i.wisbn

        self.redirect_if_needed(i)

        if 'isbn' in i and all(not v for k, v in i.items() if k != 'isbn'):
            self.isbn_redirect(i.isbn)

        q_list = []
        q = i.get('q', '').strip()
        if q:
            m = re_olid.match(q)
            if m:
                raise web.seeother('/%s/%s' % (olid_urls[m.group(1)], q))
            m = re_isbn_field.match(q)
            if m:
                self.isbn_redirect(m.group(1))
            q_list.append(q)
        for k in ('title', 'author', 'isbn', 'subject', 'place', 'person', 'publisher'):
            if k in i:
                v = re_to_esc.sub(lambda m:'\\' + m.group(), i[k].strip())
                q_list.append(k + ':' + v)

        return render.work_search(i, ' '.join(q_list), do_search, get_doc)



def works_by_author(akey, sort='editions', page=1, rows=100):
    # called by merge_author_works
    q='author_key:' + akey
    offset = rows * (page - 1)
    fields = ['key', 'author_name', 'author_key', 'title', 'subtitle',
        'edition_count', 'ia', 'cover_edition_key', 'has_fulltext',
        'first_publish_year', 'public_scan_b', 'lending_edition_s',
        'overdrive_s', 'ia_collection_s']
    fl = ','.join(fields)
    solr_select = solr_select_url + "?q.op=AND&q=%s&fq=&start=%d&rows=%d&fl=%s&wt=json" % (q, offset, rows, fl)
    facet_fields = ["author_facet", "language", "publish_year", "publisher_facet", "subject_facet", "person_facet", "place_facet", "time_facet"]
    if sort == 'editions':
        solr_select += '&sort=edition_count+desc'
    elif sort.startswith('old'):
        solr_select += '&sort=first_publish_year+asc'
    elif sort.startswith('new'):
        solr_select += '&sort=first_publish_year+desc'
    elif sort.startswith('title'):
        solr_select += '&sort=title+asc'
    solr_select += "&facet=true&facet.mincount=1&f.author_facet.facet.sort=count&f.publish_year.facet.limit=-1&facet.limit=25&" + '&'.join("facet.field=" + f for f in facet_fields)
    stats.begin("solr", url=solr_select)
    reply = json.load(urllib.urlopen(solr_select))
    stats.end()
    facets = reply['facet_counts']['facet_fields']
    works = [work_object(w) for w in reply['response']['docs']]

    def get_facet(f, limit=None):
        return list(web.group(facets[f][:limit * 2] if limit else facets[f], 2))

    return web.storage(
        num_found = int(reply['response']['numFound']),
        works = works,
        years = [(int(k), v) for k, v in get_facet('publish_year')],
        get_facet = get_facet,
        sort = sort,
    )

def sorted_work_editions(wkey, json_data=None):
    q='key:' + wkey
    if not json_data: # for testing
        solr_select = solr_select_url + "?version=2.2&q.op=AND&q=%s&rows=10&fl=edition_key&qt=standard&wt=json" % q
        stats.begin("solr", url=solr_select)
        json_data = urllib.urlopen(solr_select).read()
        stats.end()
    reply = json.loads(json_data)

    if reply['response']['numFound'] == 0:
        return []
    return reply["response"]['docs'][0].get('edition_key', [])

def simple_search(q, offset=0, rows=20, sort=None):
    solr_select = solr_select_url + "?version=2.2&q.op=AND&q=%s&fq=&start=%d&rows=%d&fl=*%%2Cscore&qt=standard&wt=json" % (web.urlquote(q), offset, rows)
    if sort:
        solr_select += "&sort=" + web.urlquote(sort)

    stats.begin("solr", url=solr_select)
    json_data = urllib.urlopen(solr_select)
    stats.end()
    return json.load(json_data)

def top_books_from_author(akey, rows=5, offset=0):
    q = 'author_key:(' + akey + ')'
    solr_select = solr_select_url + "?q=%s&start=%d&rows=%d&fl=key,title,edition_count,first_publish_year&wt=json&sort=edition_count+desc" % (q, offset, rows)
    stats.begin("solr", url=solr_select)
    response = json.load(urllib.urlopen(solr_select))['response']
    stats.end()
    return {
        'books': [web.storage(doc) for doc in response['docs']],
        'total': response['numFound'],
    }

def do_merge():
    return

class improve_search(delegate.page):
    def GET(self):
        i = web.input(q=None)
        boost = dict((f, i[f]) for f in search_fields if f in i)
        return render.improve_search(search_fields, boost, i.q, simple_search)

class merge_author_works(delegate.page):
    path = "/authors/(OL\d+A)/merge-works"
    def GET(self, key):
        works = works_by_author(key)

def escape_colon(q, vf):
    if ':' not in q:
        return q
    parts = q.split(':')
    result = parts.pop(0)
    while parts:
        if not any(result.endswith(f) for f in vf):
            result += '\\'
        result += ':' + parts.pop(0)
    return result

def run_solr_search(solr_select):
    stats.begin("solr", url=solr_select)
    json_data = urllib.urlopen(solr_select).read()
    stats.end()
    return parse_search_response(json_data)

def parse_search_response(json_data):
    try:
        return json.loads(json_data)
    except json.JSONDecodeError:
        m = re_pre.search(json_data)
        error = web.htmlunquote(m.group(1))
        solr_error = 'org.apache.lucene.queryParser.ParseException: '
        if error.startswith(solr_error):
            error = error[len(solr_error):]
        return {'error': error}

class subject_search(delegate.page):
    path = '/search/subjects'
    def GET(self):
        return render_template('search/subjects.tmpl', self.get_results)

    def get_results(self, q, offset=0, limit=100):
        if config.get('single_core_solr'):            
            valid_fields = ['key', 'name', 'subject_type', 'work_count']
        else:
            valid_fields = ['key', 'name', 'type', 'count']

        q = escape_colon(escape_bracket(q), valid_fields)
        params = {
            "q.op": "AND",
            "q": web.urlquote(q),
            "start": offset,
            "rows": limit,
            "fl": ",".join(valid_fields),
            "qt": "standard",
            "wt": "json"
        }
        if config.get('single_core_solr'):
            params['fq'] = 'type:subject'
            params['sort'] = 'work_count desc'
        else:                
            params['sort'] = 'count desc'

        solr_select = solr_subject_select_url + "?" + urllib.urlencode(params)
        results = run_solr_search(solr_select)
        response = results['response']

        if config.get('single_core_solr'):
            response['docs'] = [self.process_doc(doc) for doc in response['docs']]

        return results

    def process_doc(self, doc):
        doc['type'] = doc.get('subject_type', 'subject')
        doc['count'] = doc.get('work_count', 0)
        return doc

class author_search(delegate.page):
    path = '/search/authors'
    def GET(self):
        return render_template('search/authors.tmpl', self.get_results)
    
    def get_results(self, q, offset=0, limit=100):
        valid_fields = ['key', 'name', 'alternate_names', 'birth_date', 'death_date', 'date', 'work_count']
        q = escape_colon(escape_bracket(q), valid_fields)

        solr_select = solr_author_select_url + "?q.op=AND&q=%s&fq=&start=%d&rows=%d&fl=*&qt=standard&wt=json" % (web.urlquote(q), offset, limit)
        solr_select += '&sort=work_count+desc'

        if config.get('single_core_solr'):
            solr_select += '&fq=type:author'

        d = run_solr_search(solr_select)

        if config.get('single_core_solr'):
            docs = d.get('response', {}).get('docs', [])
            for doc in docs:
                # replace /authors/OL1A with OL1A
                # The template still expects the key to be in the old format
                doc['key'] = doc['key'].split("/")[-1]
        return d
        
class author_search_json(author_search):
    path = '/search/authors'
    encoding = 'json'
    
    def GET(self):
        i = web.input(q='', offset=0, limit=100)
        offset = safeint(i.offset, 0)
        limit = safeint(i.limit, 100)
        limit = min(1000, limit) # limit limit to 1000.
        
        response = self.get_results(i.q, offset=offset, limit=limit)['response']
        web.header('Content-Type', 'application/json')
        return delegate.RawText(json.dumps(response))

class edition_search(delegate.page):
    path = '/search/editions'
    def GET(self):
        def get_results(q, offset=0, limit=100):
            q = escape_bracket(q)
            solr_select = solr_edition_select_url + "?q.op=AND&q=%s&start=%d&rows=%d&fl=*&qt=standard&wt=json" % (web.urlquote(q), offset, limit)

            if config.get('single_core_solr'):
                solr_select += '&fq=type:edition'

            return run_solr_search(solr_select)
        return render_template('search/editions.tmpl', get_results)

class search_json(delegate.page):
    path = "/search"
    encoding = "json"

    def GET(self):
        i = web.input()
        if 'query' in i:
            query = json.loads(i.query)
        else:
            query = i

        sorts = dict(
            editions='edition_count desc', 
            old='first_publish_year asc', 
            new='first_publish_year desc', 
            scans='ia_count desc')
        sort_name = query.get('sort', None)
        sort_value = sort_name and sorts[sort_name] or None

        limit = safeint(query.pop("limit", "100"), default=100)
        if "offset" in query:
            offset = safeint(query.pop("offset", 0), default=0)
            page = None
        else:
            offset = None
            page = safeint(query.pop("page", "1"), default=1)

        query['wt'] = 'json'
        
        (reply, solr_select, q_list) = run_solr_query(query, 
                                                rows=limit, 
                                                page=page, 
                                                sort=sort_value, 
                                                offset=offset,
                                                fields="*")

        response = json.loads(reply)['response']

        # backward compatibility
        response['num_found'] = response['numFound']
        return delegate.RawText(json.dumps(response, indent=True))

def setup():
    import searchapi
    searchapi.setup()
    
    from . import subjects
    
    # subjects module needs read_author_facet and solr_select_url.
    # Importing this module to access them will result in circular import.
    # Setting them like this to avoid circular-import.
    subjects.read_author_facet = read_author_facet
    if hasattr(config, 'plugin_worksearch'):
        subjects.solr_select_url = solr_select_url
    
    subjects.setup()
    
    from . import publishers, languages
    publishers.setup()
    languages.setup()
    
setup()

########NEW FILE########
__FILENAME__ = languages
"""Language pages
"""
from infogami.utils import delegate, stats
from infogami.utils.view import render_template, safeint
import web
import simplejson
import logging
import urllib

from . import subjects
from . import search

logger = logging.getLogger("openlibrary.worksearch")


def get_language_name(code):
    doc = web.ctx.site.get('/languages/' + code)
    name = doc and doc.name
    return name or code

class languages(subjects.subjects):
    path = '(/languages/[^_][^/]*)'

    def GET(self, key):
        page = subjects.get_subject(key, details=True)
        page.name = get_language_name(key.split("/")[-1])

        if page.work_count == 0:
            return render_template('languages/notfound.tmpl', key)

        return render_template("languages/view", page)

    def is_enabled(self):
        return "languages" in web.ctx.features

class languages_json(subjects.subjects_json):
    path = '(/languages/[^_][^/]*)'
    encoding = "json"

    def is_enabled(self):
        return "languages" in web.ctx.features
        
    def normalize_key(self, key):
        return key

    def process_key(self, key):
        return key.replace("_", " ")

class language_works_json(subjects.subject_works_json):
    path = '(/languages/[^/]+)/works'
    encoding = "json"

    def is_enabled(self):
        return "languages" in web.ctx.features

    def normalize_key(self, key):
        return key

    def process_key(self, key):
        return key.replace("_", " ")

class index(delegate.page):
    path = "/languages"
    
    def GET(self):
        from . import search
        result = search.get_works_solr().select('*:*', rows=0, facets=['language'], facet_limit=500)
        languages = [web.storage(name=get_language_name(row.value), key='/languages/' + row.value, count=row.count) 
                    for row in result['facets']['language']]
        print >> web.debug, languages[:10]
        return render_template("languages/index", languages)
        
    def is_enabled(self):
        return "languages" in web.ctx.features

class language_search(delegate.page):
    path = '/search/languages'
    
    def GET(self):
        i = web.input(q="")
        solr = search.get_works_solr()
        q = {"language": i.q}
        
        result = solr.select(q, facets=["language"], fields=["language"], rows=0)
        result = self.process_result(result)
        return render_template('search/languages', i.q, result)
        
    def process_result(self, result):
        solr = search.get_works_solr()
        
        def process(p):
            return web.storage(
                name=p.value,
                key="/languages/" + p.value.replace(" ", "_"),
                count=solr.select({"language": p.value}, rows=0)['num_found']
            )
        language_facets = result['facets']['language'][:25]
        return [process(p) for p in language_facets]
        
class LanguageEngine(subjects.SubjectEngine):
    def normalize_key(self, key):
        return key
    
    def get_ebook_count(self, name, value, publish_year):
        # Query solr for this publish_year and publish_year combination and read the has_fulltext=true facet
        solr = search.get_works_solr()
        q = {
            "language": value
        }
        
        if isinstance(publish_year, list):
            q['publish_year'] = tuple(publish_year) # range
        elif publish_year:
            q['publish_year'] = publish_year
            
        result = solr.select(q, facets=["has_fulltext"], rows=0)        
        counts = dict((v.value, v.count) for v in result["facets"]["has_fulltext"])
        return counts.get('true')

def setup():
    d = web.storage(name="language", key="languages", prefix="/languages/", facet="language", facet_key="language", engine=LanguageEngine)
    subjects.SUBJECTS.append(d)

########NEW FILE########
__FILENAME__ = publishers
"""Publisher pages
"""
from infogami.utils import delegate, stats
from infogami.utils.view import render_template, safeint
import web
import simplejson
import logging
import urllib

from . import subjects
from . import search

logger = logging.getLogger("openlibrary.worksearch")

class publishers(subjects.subjects):
    path = '(/publishers/[^/]+)'

    def GET(self, key):
        key = key.replace("_", " ")
        page = subjects.get_subject(key, details=True)

        if page.work_count == 0:
            return render_template('publishers/notfound.tmpl', key)

        return render_template("publishers/view", page)

    def is_enabled(self):
        return "publishers" in web.ctx.features

class publishers_json(subjects.subjects_json):
    path = '(/publishers/[^/]+)'
    encoding = "json"

    def is_enabled(self):
        return "publishers" in web.ctx.features
        
    def normalize_key(self, key):
        return key

    def process_key(self, key):
        return key.replace("_", " ")

class publisher_works_json(subjects.subject_works_json):
    path = '(/publishers/[^/]+)/works'
    encoding = "json"

    def is_enabled(self):
        return "publishers" in web.ctx.features

    def normalize_key(self, key):
        return key

    def process_key(self, key):
        return key.replace("_", " ")

class index(delegate.page):
    path = "/publishers"
    
    def GET(self):
        return render_template("publishers/index")
        
    def is_enabled(self):
        return "publishers" in web.ctx.features

class publisher_search(delegate.page):
    path = '/search/publishers'
    
    def GET(self):
        i = web.input(q="")
        solr = search.get_works_solr()
        q = {"publisher": i.q}
        
        result = solr.select(q, facets=["publisher_facet"], fields=["publisher", "publisher_facet"], rows=0)
        result = self.process_result(result)
        return render_template('search/publishers', i.q, result)
        
    def process_result(self, result):
        solr = search.get_works_solr()
        
        def process(p):
            return web.storage(
                name=p.value,
                key="/publishers/" + p.value.replace(" ", "_"),
                count=solr.select({"publisher_facet": p.value}, rows=0)['num_found']
            )
        publisher_facets = result['facets']['publisher_facet'][:25]
        return [process(p) for p in publisher_facets]
        
class PublisherEngine(subjects.SubjectEngine):
    def normalize_key(self, key):
        return key
    
    def get_ebook_count(self, name, value, publish_year):
        # Query solr for this publish_year and publish_year combination and read the has_fulltext=true facet
        solr = search.get_works_solr()
        q = {
            "publisher_facet": value
        }
        
        if isinstance(publish_year, list):
            q['publish_year'] = tuple(publish_year) # range
        elif publish_year:
            q['publish_year'] = publish_year
            
        result = solr.select(q, facets=["has_fulltext"], rows=0)        
        counts = dict((v.value, v.count) for v in result["facets"]["has_fulltext"])
        return counts.get('true')

def setup():
    d = web.storage(name="publisher", key="publishers", prefix="/publishers/", facet="publisher_facet", facet_key="publisher_facet", engine=PublisherEngine)
    subjects.SUBJECTS.append(d)
########NEW FILE########
__FILENAME__ = search
"""Search utilities.
"""
from openlibrary.utils.solr import Solr
from infogami import config
from infogami.utils import stats
import web

def get_works_solr():
    if config.get('single_core_solr'):
        base_url = "http://%s/solr" % config.plugin_worksearch.get('solr')
    else:
        base_url = "http://%s/solr/works" % config.plugin_worksearch.get('solr')

    return Solr(base_url)

def get_authors_solr():
    if config.get('single_core_solr'):
        base_url = "http://%s/solr" % config.plugin_worksearch.get('author_solr')
    else:
        base_url = "http://%s/solr/authors" % config.plugin_worksearch.get('author_solr')
    return Solr(base_url)

def get_subjects_solr():
    if config.get('single_core_solr'):
        base_url = "http://%s/solr" % config.plugin_worksearch.get('subjects_solr')
    else:
        base_url = "http://%s/solr/subjects" % config.plugin_worksearch.get('subjects_solr')
    return Solr(base_url)

def work_search(query, limit=20, offset=0, **kw):
    """Search for works."""

    kw.setdefault("doc_wrapper", work_wrapper)
    fields = [
        "key", 
        "author_name", 
        "author_key", 
        "title",
        "edition_count",
        "ia",
        "cover_edition_key",
        "has_fulltext",
        "subject",
        "ia_collection_s",
        "public_scan_b",
        "overdrive_s",
        "lending_edition_s",
    ]
    kw.setdefault("fields", fields)

    if config.get('single_core_solr'):
        kw.setdefault("fq", "type:work")

    query = process_work_query(query)
    solr = get_works_solr()
    
    stats.begin("solr", query=query, start=offset, rows=limit, kw=kw)
    try:
        result = solr.select(query, start=offset, rows=limit, **kw)
    finally:
        stats.end()
    
    return result

def process_work_query(query):
    if "author" in query and isinstance(query["author"], dict):
        author = query.pop("author")
        query["author_key"] = author["key"]

    ebook = query.pop("ebook", None)
    if ebook == True or ebook == "true":
        query["has_fulltext"] = "true"

    return query

def work_wrapper(w):
    key = w['key']
    if not key.startswith("/works/"):
        key += "/works/"

    d = web.storage(
        key=key,
        title=w["title"],
        edition_count=w["edition_count"]
    )

    if "cover_id" in w:
        d.cover_id = w["cover_id"]
    elif "cover_edition_key" in w:
        book = web.ctx.site.get("/books/" + w["cover_edition_key"])
        cover = book and book.get_cover()
        d.cover_id = cover and cover.id or None
        d.cover_edition_key = w['cover_edition_key']
    else:
        d.cover_id = None
    d.subject = w.get('subject', [])
    ia_collection = w['ia_collection_s'].split(';') if 'ia_collection_s' in w else []
    d.ia_collection = ia_collection
    d.lendinglibrary = 'lendinglibrary' in ia_collection
    d.printdisabled = 'printdisabled' in ia_collection
    d.lending_edition = w.get('lending_edition_s', '')
    d.overdrive = w['overdrive_s'].split(';')[0] if 'overdrive_s' in w else ''

    # special care to handle missing author_key/author_name in the solr record
    w.setdefault('author_key', [])
    w.setdefault('author_name', [])
    
    d.authors = [web.storage(key='/authors/' + k, name=n)
                 for k, n in zip(w['author_key'], w['author_name'])]

    d.first_publish_year = (w['first_publish_year'][0] if 'first_publish_year' in w else None)
    d.ia = w.get('ia', [])
    d.public_scan = w.get('public_scan_b', bool(d.ia))
    d.has_fulltext = w.get('has_fulltext', "false")
    return d

########NEW FILE########
__FILENAME__ = searchapi
"""Leagacy editions search API at /api/search.
"""
import simplejson
import traceback
import web

from infogami.utils import delegate
from infogami import config

from openlibrary.utils import solr

class search_api:
    error_val = {'status':'error'}
    def GET(self):
        i = web.input(q = None,
                      rows = 20,
                      offset = 0,
                      format = None,
                      callback = None,
                      prettyprint=False,
                      _unicode=False)        
        offset = int(i.get('offset', '0') or 0)
        rows = int(i.get('rows', '0') or 20)
        
        result = self.process(i.q, rows=rows, offset=offset)
        return self.format(result, i.prettyprint, i.callback)
        
    def process(self, q, rows, offset):
        try:
            query = simplejson.loads(q).get('query')
            query = self.process_query(query)
            
            result = self.solr_select(query)
            return {"status": "ok", "result": result}
        except Exception:
            traceback.print_exc()
            return {'status':'error'}
            
    def get_editions_solr(self):
        c = config.get("plugin_worksearch")
        host = c and c.get('edition_solr')
        return host and solr.Solr("http://" + host + "/solr/editions")
    
    def solr_select(self, query):
        solr = self.get_editions_solr()
        response = solr.select(query)
        docs = response['docs']
        return ["/books/" + doc["key"] for doc in docs]
            
    def process_query(self, query):
        if ":" in query:
            # The new editions solr indexed both isbn_10 and isbn_13 as isbn.
            query = query.replace("isbn_10:", "isbn:")
            query = query.replace("isbn_13:", "isbn:")
        else:
            query = SimpleQueryProcessor().process(query)
        return query

    def format(self, val, prettyprint=False, callback=None):
        if prettyprint:
            json = simplejson.dumps(val, indent = 4)
        else:
            json = simplejson.dumps(val)

        if callback is None:
            return json
        else:
            return '%s(%s)'% (callback, json)

class SimpleQueryProcessor:
    """Utility to expand search queries.

        >>> SimpleQueryProcessor().process("hello")
        '(title:hello^100 OR author_name:hello^15 OR subject:hello^10 OR language:hello^10)'
        >>> SimpleQueryProcessor().process("hello world") #doctest: +NORMALIZE_WHITESPACE
        '(title:hello^100 OR author_name:hello^15 OR subject:hello^10 OR language:hello^10)
         (title:world^100 OR author_name:world^15 OR subject:world^10 OR language:world^10)'
    """
    def process(self, query):
        query = web.utf8(query)
        tokens = query.split(' ')
        return " ".join(self.process_token(t) for t in tokens if t.strip())

    def process_token(self, token):
        return '(title:%s^100 OR author_name:%s^15 OR subject:%s^10 OR language:%s^10)' % (token, token, token, token)
        
def setup():
    from infogami.plugins.api import code as api
    api.add_hook('search', search_api)
########NEW FILE########
__FILENAME__ = subjects
"""Subject pages.
"""

import web
import re
import simplejson as json
import logging
from collections import defaultdict
import urllib
import datetime

from infogami import config
from infogami.plugins.api.code import jsonapi
from infogami.utils import delegate, stats
from infogami.utils.view import render_template, safeint

from openlibrary.core.models import Subject
from openlibrary.utils import str_to_key, finddict

__all__ = [
    "SubjectEngine", "get_subject"
]

# These two are available in .code module. Importing it here will result in a
# circular import. To avoid that, these values are set by the code.setup
# function.
read_author_facet = None
solr_select_url = None

logger = logging.getLogger("openlibrary.worksearch")

re_chars = re.compile("([%s])" % re.escape(r'+-!(){}[]^"~*?:\\'))
re_year = re.compile(r'\b(\d+)$')

SUBJECTS = [
    web.storage(name="person", key="people", prefix="/subjects/person:", facet="person_facet", facet_key="person_key"),
    web.storage(name="place", key="places", prefix="/subjects/place:", facet="place_facet", facet_key="place_key"),
    web.storage(name="time", key="times", prefix="/subjects/time:", facet="time_facet", facet_key="time_key"),
    web.storage(name="subject", key="subjects", prefix="/subjects/", facet="subject_facet", facet_key="subject_key"),
]

class subjects_index(delegate.page):
    path = "/subjects"
    
    def GET(self):
        return render_template("subjects/index.html")

class subjects(delegate.page):
    path = '(/subjects/[^/]+)'

    def GET(self, key):
        nkey = self.normalize_key(key)
        if nkey != key:
            raise web.redirect(nkey)
            
        page = get_subject(key, details=True)

        if page.work_count == 0:
            return render_template('subjects/notfound.tmpl', key)

        return render_template("subjects", page)
        
    def normalize_key(self, key):
        key = key.lower()

        # temporary code to handle url change from /people/ to /person:
        if key.count("/") == 3:
            key = key.replace("/people/", "/person:")
            key = key.replace("/places/", "/place:")
            key = key.replace("/times/", "/time:")
        return key

class subjects_json(delegate.page):
    path = '(/subjects/[^/]+)'
    encoding = "json"

    @jsonapi
    def GET(self, key):
        # If the key is not in the normalized form, redirect to the normalized form.
        nkey = self.normalize_key(key)
        if nkey != key:
            raise web.redirect(nkey)
            
        # Does the key requires any processing before passing using it to query solr?
        key = self.process_key(key)

        i = web.input(offset=0, limit=12, details='false', has_fulltext='false', sort='editions')

        filters = {}
        if i.get('has_fulltext') == 'true':
            filters['has_fulltext'] = 'true'

        if i.get('published_in'):
            if '-' in i.published_in:
                begin, end = i.published_in.split('-', 1)

                if safeint(begin, None) is not None and safeint(end, None) is not None:
                    filters['publish_year'] = (begin, end) # range
            else:
                y = safeint(i.published_in, None)
                if y is not None:
                    filters['publish_year'] = i.published_in

        i.limit = safeint(i.limit, 12)
        i.offset = safeint(i.offset, 0)

        subject = get_subject(key, offset=i.offset, limit=i.limit, sort=i.sort, details=i.details.lower() == 'true', **filters)
        return json.dumps(subject)
        
    def normalize_key(self, key):
        return key.lower() 
        
    def process_key(self, key):
        return key

class subject_works_json(delegate.page):
    path = '(/subjects/[^/]+)/works'
    encoding = "json"

    @jsonapi
    def GET(self, key):
        # If the key is not in the normalized form, redirect to the normalized form.
        nkey = self.normalize_key(key)
        if nkey != key:
            raise web.redirect(nkey)
            
        # Does the key requires any processing before passing using it to query solr?
        key = self.process_key(key)

        i = web.input(offset=0, limit=12, has_fulltext="false")

        filters = {}
        if i.get("has_fulltext") == "true":
            filters["has_fulltext"] = "true"

        if i.get("published_in"):
            if "-" in i.published_in:
                begin, end = i.published_in.split("-", 1)

                if safeint(begin, None) is not None and safeint(end, None) is not None:
                    filters["publish_year"] = (begin, end)
            else:
                y = safeint(i.published_in, None)
                if y is not None:
                    filters["publish_year"] = i.published_in

        i.limit = safeint(i.limit, 12)
        i.offset = safeint(i.offset, 0)

        subject = get_subject(key, offset=i.offset, limit=i.limit, details=False, **filters)
        return json.dumps(subject)

    def normalize_key(self, key):
        return key.lower() 

    def process_key(self, key):
        return key


def get_subject(key, details=False, offset=0, sort='editions', limit=12, **filters):
    """Returns data related to a subject.

    By default, it returns a storage object with key, name, work_count and works.
    The offset and limit arguments are used to get the works.

        >>> get_subject("/subjects/Love") #doctest: +SKIP
        {
            "key": "/subjects/Love", 
            "name": "Love",
            "work_count": 5129, 
            "works": [...]
        }

    When details=True, facets and ebook_count are additionally added to the result.

    >>> get_subject("/subjects/Love", details=True) #doctest: +SKIP
    {
        "key": "/subjects/Love", 
        "name": "Love",
        "work_count": 5129, 
        "works": [...],
        "ebook_count": 94, 
        "authors": [
            {
                "count": 11, 
                "name": "Plato.", 
                "key": "/authors/OL12823A"
            }, 
            ...
        ],
        "subjects": [
            {
                "count": 1168,
                "name": "Religious aspects", 
                "key": "/subjects/religious aspects"
            }, 
            ...
        ],
        "times": [...],
        "places": [...],
        "people": [...],
        "publishing_history": [[1492, 1], [1516, 1], ...],
        "publishers": [
            {
                "count": 57, 
                "name": "Sine nomine"        
            },
            ...
        ]
    }

    Optional arguments limit and offset can be passed to limit the number of works returned and starting offset.

    Optional arguments has_fulltext and published_in can be passed to filter the results.
    """
    def create_engine():
        for d in SUBJECTS:
            if key.startswith(d.prefix):
                Engine = d.get("engine") or SubjectEngine
                return Engine()
        return SubjectEngine()
        
    sort_options = {
        'editions': 'edition_count desc',
        'new': 'first_publish_year desc',
    }
    sort_order = sort_options.get(sort) or sort_options['editions']
    
    engine = create_engine()
    return engine.get_subject(key, details=details, offset=offset, sort=sort_order, limit=limit, **filters)

class SubjectEngine:
    def get_subject(self, key, details=False, offset=0, limit=12, sort='first_publish_year desc', **filters):
        meta = self.get_meta(key)

        q = self.make_query(key, filters)
        subject_type = meta.name
        name = meta.path.replace("_", " ")

        if details:
            kw = self.query_optons_for_details()
        else:
            kw = {}

        from search import work_search
        result = work_search(q, offset=offset, limit=limit, sort=sort, **kw)
        for w in result.docs:
            w.ia = w.ia and w.ia[0] or None
            if not w.get('public_scan') and w.ia and w.get('lending_edition'):
                doc = web.ctx.site.store.get("ebooks/books/" + w['lending_edition']) or {}
                w['checked_out'] = doc.get("borrowed") == "true"

            # XXX-Anand: Oct 2013
            # Somewhere something is broken, work keys are coming as OL1234W/works/
            # Quick fix it solve that issue.
            if w.key.endswith("/works/"):
                w.key = "/works/" + w.key.replace("/works/", "")
                
        subject = Subject(
            key=key,
            name=name,
            subject_type=subject_type,
            work_count = result['num_found'],
            works=result['docs']
        )

        if details:
            subject.ebook_count = dict(result.facets["has_fulltext"]).get("true", 0)
            #subject.ebook_count = self.get_ebook_count(meta.name, q[meta.facet_key], q.get('publish_year'))

            subject.subjects = result.facets["subject_facet"]
            subject.places = result.facets["place_facet"]
            subject.people = result.facets["person_facet"]
            subject.times = result.facets["time_facet"]

            subject.authors = result.facets["author_facet"]
            subject.publishers = result.facets["publisher_facet"]
            subject.languages = result.facets['language']

            # Ignore bad dates when computing publishing_history
            # year < 1000 or year > current_year+1 are considered bad dates
            current_year = datetime.datetime.utcnow().year
            subject.publishing_history = [[year, count] for year, count in result.facets["publish_year"] if 1000 < year <= current_year+1]

            # strip self from subjects and use that to find exact name
            for i, s in enumerate(subject[meta.key]):
                if "key" in s and s.key.lower() == key.lower():
                    subject.name = s.name;
                    subject[meta.key].pop(i)
                    break

        return subject

    def get_meta(self, key):
        prefix = self.parse_key(key)[0]
        meta = finddict(SUBJECTS, prefix=prefix)

        meta = web.storage(meta)
        meta.path = web.lstrips(key, meta.prefix)
        return meta

    def parse_key(self, key):
        """Returns prefix and path from the key.
        """
        for d in SUBJECTS:
            if key.startswith(d.prefix):
                return d.prefix, key[len(d.prefix):]
        return None, None

    def make_query(self, key, filters):
        meta = self.get_meta(key)
        
        q = {meta.facet_key: self.normalize_key(meta.path)}
        
        if filters:
            if filters.get("has_fulltext") == "true":
                q['has_fulltext'] = "true"
            if filters.get("publish_year"):
                q['publish_year'] = filters['publish_year']
        return q
        
    def normalize_key(self, key):
        return str_to_key(key).lower()

    def get_ebook_count(self, name, value, publish_year):
        return get_ebook_count(name, value, publish_year)
    
    def facet_wrapper(self, facet, value, count):
        if facet == "publish_year":
            return [int(value), count]
        elif facet == "publisher_facet":
            return web.storage(name=value, count=count, key="/publishers/" + value.replace(" ", "_"))
        elif facet == "author_facet":
            author = read_author_facet(value)
            return web.storage(name=author[1], key="/authors/" + author[0], count=count)
        elif facet in ["subject_facet", "person_facet", "place_facet", "time_facet"]:
            return web.storage(key=finddict(SUBJECTS, facet=facet).prefix + str_to_key(value).replace(" ", "_"), name=value, count=count)
        elif facet == "has_fulltext":
            return [value, count]
        else:
            return web.storage(name=value, count=count)
            
    def query_optons_for_details(self):
        """Additional query options to be added when details=True.
        """
        kw = {}
        kw['facets'] = [
            {"name": "author_facet", "sort": "count"},
            "language",
            "publisher_facet",
            {"name": "publish_year", "limit": -1},
            "subject_facet", "person_facet", "place_facet", "time_facet",
            "has_fulltext", "language"]
        kw['facet.mincount'] = 1
        kw['facet.limit'] = 25
        kw['facet_wrapper'] = self.facet_wrapper
        return kw


def get_ebook_count(field, key, publish_year=None):
    ebook_count_db = get_ebook_count_db()
    
    # Handle the case of ebook_count_db_parametres not specified in the config.
    if ebook_count_db is None:
        return 0
    
    def db_lookup(field, key, publish_year=None):
        sql = 'select sum(ebook_count) as num from subjects where field=$field and key=$key'
        if publish_year:
            if isinstance(publish_year, (tuple, list)):
                sql += ' and publish_year between $y1 and $y2'
                (y1, y2) = publish_year
            else:
                sql += ' and publish_year=$publish_year'
        return list(ebook_count_db.query(sql, vars=locals()))[0].num

    total = db_lookup(field, key, publish_year)
    if total:
        return total
    elif publish_year:
        sql = 'select ebook_count as num from subjects where field=$field and key=$key limit 1'
        if len(list(ebook_count_db.query(sql, vars=locals()))) != 0:
            return 0
    years = find_ebook_count(field, key)
    if not years:
        return 0
    for year, count in sorted(years.iteritems()):
        ebook_count_db.query('insert into subjects (field, key, publish_year, ebook_count) values ($field, $key, $year, $count)', vars=locals())

    return db_lookup(field, key, publish_year)

@web.memoize
def get_ebook_count_db():
    """Returns the ebook_count database. 
    
    The database object is created on the first call to this function and
    cached by memoize. Subsequent calls return the same object.
    """
    params = config.plugin_worksearch.get('ebook_count_db_parameters')
    if params:
        params.setdefault('dbn', 'postgres')
        return web.database(**params)
    else:
        logger.warn("ebook_count_db_parameters is not specified in the config. ebook-count on subject pages will be displayed as 0.")
        return None

def find_ebook_count(field, key):
    q = '%s_key:%s+AND+(overdrive_s:*+OR+ia:*)' % (field, re_chars.sub(r'\\\1', key).encode('utf-8'))
    return execute_ebook_count_query(q)
    
def execute_ebook_count_query(q):
    root_url = solr_select_url + '?wt=json&indent=on&rows=%d&start=%d&q.op=AND&q=%s&fl=edition_key'
    rows = 1000

    ebook_count = 0
    start = 0
    solr_url = root_url % (rows, start, q)
    
    stats.begin("solr", url=solr_url)
    response = json.load(urllib.urlopen(solr_url))['response']
    stats.end()

    num_found = response['numFound']
    years = defaultdict(int)
    while start < num_found:
        if start:
            solr_url = root_url % (rows, start, q)
            stats.begin("solr", url=solr_url)
            response = json.load(urllib.urlopen(solr_url))['response']
            stats.end()
        for doc in response['docs']:
            for k in doc['edition_key']:
                e = web.ctx.site.get('/books/' + k)
                ia = set(i[3:] for i in e.get('source_records', []) if i.startswith('ia:'))
                if e.get('ocaid'):
                    ia.add(e['ocaid'])
                pub_date = e.get('publish_date')
                pub_year = -1
                if pub_date:
                    m = re_year.search(pub_date)
                    if m:
                        pub_year = int(m.group(1))
                ebook_count = len(ia)
                if 'overdrive' in e.get('identifiers', {}):
                    ebook_count += len(e['identifiers']['overdrive'])
                if ebook_count:
                    years[pub_year] += ebook_count
        start += rows

    return dict(years)

def setup():
    """Placeholder for doing any setup required.
    
    This function is called from code.py.
    """
    pass
########NEW FILE########
__FILENAME__ = test_worksearch
import unittest
from openlibrary.plugins.worksearch.code import read_facets, sorted_work_editions, parse_query_fields, escape_bracket, run_solr_query, get_doc, build_q_list, escape_colon, parse_search_response
from lxml import etree
from infogami import config

def test_escape_bracket():
    assert escape_bracket('foo') == 'foo'
    assert escape_bracket('foo[') == 'foo\\['
    assert escape_bracket('[ 10 TO 1000]') == '[ 10 TO 1000]'

def test_escape_colon():
    vf = ['key', 'name', 'type', 'count']
    assert escape_colon('test key:test http://test/', vf) == 'test key:test http\\://test/'

def test_read_facet():
    xml = '''<response>
        <lst name="facet_counts">
            <lst name="facet_fields">
                <lst name="has_fulltext">
                    <int name="false">46</int>
                    <int name="true">2</int>
                </lst>
            </lst>
        </lst>
    </response>'''

    expect = {'has_fulltext': [('true', 'yes', '2'), ('false', 'no', '46')]}
    assert read_facets(etree.fromstring(xml)) == expect

def test_sorted_work_editions():
    json_data = '''{
"responseHeader":{
"status":0,
"QTime":1,
"params":{
"fl":"edition_key",
"indent":"on",
"wt":"json",
"q":"key:OL100000W"}},
"response":{"numFound":1,"start":0,"docs":[
{
 "edition_key":["OL7536692M","OL7825368M","OL3026366M"]}]
}}'''
    expect = ["OL7536692M","OL7825368M","OL3026366M"]
    assert sorted_work_editions('OL100000W', json_data=json_data) == expect

def test_query_parser_fields():
    func = parse_query_fields

    expect = [{'field': 'text', 'value': 'query here'}]
    q = 'query here'
    print q
    assert list(func(q)) == expect

    expect = [
        {'field': 'title', 'value': 'food rules'},
        {'field': 'author_name', 'value': 'pollan'},
    ]

    q = 'title:food rules author:pollan'
    assert list(func(q)) == expect

    q = 'title:food rules by:pollan'
    assert list(func(q)) == expect

    q = 'Title:food rules By:pollan'
    assert list(func(q)) == expect

    expect = [
        {'field': 'title', 'value': '"food rules"'},
        {'field': 'author_name', 'value': 'pollan'},
    ]
    q = 'title:"food rules" author:pollan'
    assert list(func(q)) == expect

    expect = [
        {'field': 'text', 'value': 'query here'},
        {'field': 'title', 'value': 'food rules'},
        {'field': 'author_name', 'value': 'pollan'},
    ]
    q = 'query here title:food rules author:pollan'
    assert list(func(q)) == expect

    expect = [
        {'field': 'text', 'value': 'flatland\:a romance of many dimensions'},
    ]
    q = 'flatland:a romance of many dimensions'
    assert list(func(q)) == expect

    expect = [
        { 'field': 'title', 'value': 'flatland\:a romance of many dimensions'},
    ]
    q = 'title:flatland:a romance of many dimensions'
    assert list(func(q)) == expect

    expect = [
        { 'field': 'author_name', 'value': 'Kim Harrison' },
        { 'op': 'OR' },
        { 'field': 'author_name', 'value': 'Lynsay Sands' },
    ]
    q = 'authors:Kim Harrison OR authors:Lynsay Sands'
    assert list(func(q)) == expect

#     def test_public_scan(lf):
#         param = {'subject_facet': ['Lending library']}
#         (reply, solr_select, q_list) = run_solr_query(param, rows = 10, spellcheck_count = 3)
#         print solr_select
#         print q_list
#         print reply
#         root = etree.XML(reply)
#         docs = root.find('result')
#         for doc in docs:
#             assert get_doc(doc).public_scan == False

def test_get_doc():
    sample_doc = etree.fromstring('''<doc>
<arr name="author_key"><str>OL218224A</str></arr>
<arr name="author_name"><str>Alan Freedman</str></arr>
<str name="cover_edition_key">OL1111795M</str>
<int name="edition_count">14</int>
<int name="first_publish_year">1981</int>
<bool name="has_fulltext">true</bool>
<arr name="ia"><str>computerglossary00free</str></arr>
<str name="key">OL1820355W</str>
<str name="lending_edition_s">OL1111795M</str>
<bool name="public_scan_b">false</bool>
<str name="title">The computer glossary</str>
</doc>''')

    doc = get_doc(sample_doc)
    assert doc.public_scan == False

def test_build_q_list():
    param = {'q': 'test'}
    expect = (['test'], True)
    assert build_q_list(param) == expect

    param = {'q': 'title:(Holidays are Hell) authors:(Kim Harrison) OR authors:(Lynsay Sands)'}
    expect = (['title:((Holidays are Hell))', 'author_name:((Kim Harrison))', 'OR', 'author_name:((Lynsay Sands))'], False)
    query_fields = [
        {'field': 'title', 'value': '(Holidays are Hell)'},
        {'field': 'author_name', 'value': '(Kim Harrison)'},
        {'op': 'OR'},
        {'field': 'author_name', 'value': '(Lynsay Sands)'}
    ]
    assert list(parse_query_fields(param['q'])) == query_fields
    assert build_q_list(param) == expect

def test_parse_search_response():
    test_input = '<pre>org.apache.lucene.queryParser.ParseException: This is an error</pre>'
    expect = {'error': 'This is an error'}
    assert parse_search_response(test_input) == expect
    assert parse_search_response('{"aaa": "bbb"}') == {'aaa': 'bbb'}


########NEW FILE########
__FILENAME__ = driver
"""
Low level import API
====================

The Low level import API functions has 2 stages.

1. Matchers 
-----------

The matchers are functions present in the ``matchers`` module and
exposed via the ``match_functions`` list. These are functions that try
to search for entries in the database that match the input criteria in
various ways. The results of all the matchings are chained into a
single iterable and returned.

The matching is done liberally. The idea is to not miss any
records. We might have extra records that are bad matches.

The idea is to isolate the ugliness and complexity of the searches
into a small set of functions inside a single module. The rest of the
API can be clean and understandable then.


2. Filter
---------

The filter is a function that consumes the output of the matchers and
discards any items that are bad matches. This narrows the list of
matches and then returns the list of good ones.


Finally, the list of matched keys are massaged into the proper output
expected from the search API and returned to the client.

"""
import copy
import itertools
import logging as Logging

import web

from .functions import massage_search_results, thing_to_doc
from .matchers import match_functions

logger = Logging.getLogger("openlibrary.importapi")

def search(params):
    params = params["doc"]
    matched_keys = run_matchers(params)
    filtered_keys = run_filter(matched_keys, params)
    return massage_search_results(list(filtered_keys))


def run_matchers(params):
    """
    Run all the matchers in the match_functions list and produce a list of keys which match the 
    """
    keys = []
    for i in match_functions:
        logger.debug("Running %s", i.__name__)
        keys.append(i(params))
    return itertools.chain.from_iterable(keys)

def run_filter(matched_keys, params):
    """
    Will check all the matched keys for the following conditions and
    emit only the ones that pass all of them.

    This function compensates for the permissiveness of the matchers.

    The rules are as follows

    1. All the fields provided in params should either be matched or
       missing in the record.
    2. In case of the title and author, if provided in params, it
          *should* match (absence is not acceptable).
       TODO: Don't create if title missing

    *match* needn't mean an exact match. This is especially true for
     publishers and such ('Dover publishers' and 'Dover' are
     equivalent).
    """

    def compare(i1, i2):
        """Compares `i1` to see if it matches `i2`
        according to the rules stated above.
        
        `i1` is originally the `thing` and `i2` the search parameters.
        """
        if i1 == i2: # Trivially the same
            return True

        if isinstance(i1, list) and isinstance(i2, list):
            # i2 should be a subset of i1.  Can't use plain old set
            # operations since we have to match recursively using
            # compare
            for i in i2:
                matched = False
                for j in i1:
                    if compare(i, j):
                        matched = True
                        break
                if not matched: # A match couldn't be found for atleast one element
                    logger.debug("Couldn't match %s in %s", i, i1)
                    return False
            return True

        if isinstance(i1, dict) and isinstance(i2, dict):
            # Every key in i2 should either be in i1 and matching
            #    OR
            # In case of the 'title' and 'authors', if it's there in
            # the search params, it *should* match.
            for k in i2:
                if k == "title" or k == "authors":
                    # Special case title and authors. Return False if not present in thing
                    # TODO: Convert author names to keys.
                    if k not in i1 or not compare(i1[k], i2[k]):
                        return False
                elif k in i1:
                    # Recursively match for other keys
                    if compare(i1[k], i2[k]):
                        pass
                    else:
                        return False
                else:
                    return False
            return True

        return False

    docs = (thing_to_doc(web.ctx.site.get(x)) for x in matched_keys)

    return itertools.imap(lambda x: web.ctx.site.get(x['key']), 
                          itertools.ifilter(lambda y: compare(y, params), docs))

########NEW FILE########
__FILENAME__ = functions
"""
Functions which are used by the records package. The two public ones
are `search` and `create` which are callable from the outside world.
"""

import copy

import web

from openlibrary.catalog.add_book import normalize
import openlibrary.core.helpers as h

class NoQueryParam(KeyError):
    """
    Exception that is used internally when a find_by_X function is
    called but no X parameters were provided.
    """
    pass
    


def search(params):
    """
    Takes a search parameter and returns a result set

    Input:
    ------
    {'doc': {'authors': [{'name': 'Arthur Conan Doyle'}],
             'identifiers': {'isbn': ['1234567890']},
             'title': 'A study in Scarlet'}}

    Output:
    -------
    {'doc': {'authors': [
                         {
                          'key': '/authors/OL1A', 
                          'name': 'Arthur Conan Doyle'
                         }
                        ],
             'identifiers': {'isbn': ['1234567890']},
             'key': '/books/OL1M',
             'title': 'A study in Scarlet'
             'work' : { 'key' : '/works/OL1W'}
             },

     'matches': [{'edition': '/books/OL1M', 'work': '/works/OL1W'},
                 {'edition': None, 'work': '/works/OL234W'}]}

    'doc' is the best fit match. It contains only the keys that were
    provided as input and one extra key called 'key' which will be
    openlibrary identifier if one was found or None if nothing was.

    There will be two extra keys added to the 'doc'. 

     1. 'work' which is a dictionary with a single element 'key' that
        contains a link to the work of the matched edition.
     2. 'authors' is a list of dictionaries each of which contains an
        element 'key' that links to the appropriate author.

     If a work, author or an edition is not matched, the 'key' at that
     level will be None. 

     To update fields in a record, add the extra keys to the 'doc' and
     send the resulting structure to 'create'.

     'matches' contain a list of possible matches ordered in
     decreasing order of certainty. The first one will be same as
     'doc' itself.

     TODO: Things to change

     1. For now, if there is a work match, the provided authors
        will be replaced with the ones that are stored.

    """
    params = copy.deepcopy(params)
    doc = params.pop("doc")
    
    matches = []
    # TODO: We are looking only at edition searches here. This should be expanded to works.
    if "isbn" in doc.get('identifiers',{}):
        matches.extend(find_matches_by_isbn(doc['identifiers']['isbn']))

    if "identifiers" in doc:
        d = find_matches_by_identifiers(doc['identifiers'])
        matches.extend(d['all'])
        matches.extend(d['any']) # TODO: These are very poor matches. Maybe we should put them later.

    if "publisher" in doc or "publish_date" in doc or "title" in doc:
        matches.extend(find_matches_by_title_and_publishers(doc))

    return massage_search_results(matches, doc)



def find_matches_by_isbn(isbns):
    "Find matches using isbns."
    q = {
        'type':'/type/edition',
        'isbn_': str(isbns[0])
        }
    print "ISBN query : ", q
    ekeys = list(web.ctx.site.things(q))
    if ekeys:
        return ekeys[:1] # TODO: We artificially match only one item here
    else:
        return []


def find_matches_by_identifiers(identifiers):
    """Find matches using all the identifiers in the given doc.

    We consider only oclc_numbers, lccn and ocaid. isbn is dealt with
    separately.
    
    Will return two lists of matches: 
      all : List of items that match all the given identifiers (better
            matches).
      any : List of items that match any of the given identifiers
            (poorer matches).

    """

    identifiers = copy.deepcopy(identifiers)
    # Find matches that match everything.
    q = {'type':'/type/edition'}
    for i in ["oclc_numbers", "lccn", "ocaid"]:
        if i in identifiers:
            q[i] = identifiers[i]
    matches_all = web.ctx.site.things(q)

    # Find matches for any of the given parameters and take the union
    # of all such matches
    matches_any = set()
    for i in ["oclc_numbers", "lccn", "ocaid"]:
        q = {'type':'/type/edition'}
        if i in identifiers:
            q[i] = identifiers[i]
            matches_any.update(web.ctx.site.things(q))
    matches_any = list(matches_any)
    return dict(all = matches_all, any = matches_any)

def find_matches_by_title_and_publishers(doc):
    "Find matches using title and author in the given doc"
    #TODO: Use normalised_title instead of the regular title
    #TODO: Use catalog.add_book.load_book:build_query instead of this
    q = {'type'  :'/type/edition'}
    for key in ["title", 'publishers', 'publish_date']:
        if key in doc:
            q[key] = doc[key]
    ekeys = web.ctx.site.things(q)
    return ekeys

def massage_search_results(things, input_query = {}):
    """Converts list of things into the output expected by users of the search API.

    If input_query is non empty, narrow return keys to the ones in
    this dictionary. Also, if the keys list is empty, use this to
    construct a response with key = None.
    """
    if things:
        best = things[0]
        doc = thing_to_doc(best, input_query.keys())
        matches = things_to_matches(things)
    else:
        doc = build_create_input(input_query)
        matches = [dict(edition = None, work = None)]
    return {'doc' : doc,
            'matches' : matches}

def build_create_input(params):
    params['key'] = None
    params['type'] = '/type/edition'
    params['work'] = {'key' : None}
    params['authors'] = [{'name' : x['name'], 'key' : None} for x in params.get('authors',[])]
    return params
    

def edition_to_doc(thing):
    """Converts an edition document from infobase into a 'doc' used by
    the search API.
    """
    doc = thing.dict()

    # Process identifiers
    identifiers = doc.get("identifiers",{})
    for i in ["oclc_numbers", "lccn", "ocaid"]:
        if i in doc:
            identifiers[i] = doc.pop(i)
    for i in ["isbn_10", "isbn_13"]:
        if i in doc:
            identifiers.setdefault('isbn',[]).extend(doc.pop(i)) 
    doc['identifiers'] = identifiers

    # TODO : Process classifiers here too

    # Unpack works and authors
    if "works" in doc:
        work = doc.pop("works")[0]
        doc['work'] = work
        authors = [{'key': str(x.author) } for x in thing.works[0].authors]
        doc['authors'] = authors

    return doc
    

def work_to_doc(thing):
    """
    Converts the given work into a 'doc' used by the search API.
    """
    doc = thing.dict()

    # Unpack works and authors
    authors = [{'key': x.author.key } for x in thing.authors]
    doc['authors'] = authors

    return doc

def author_to_doc(thing):
    return thing.dict()


def thing_to_doc(thing, keys = []):
    """Converts an infobase 'thing' into an entry that can be used in
    the 'doc' field of the search results.

    If keys provided, it will remove all keys in the item except the
    ones specified in the 'keys'.
    """
    typ = str(thing['type'])
    key = str(thing['key'])

    processors = {'/type/edition' : edition_to_doc,
                  '/type/work' : work_to_doc,
                  '/type/author' : author_to_doc}

    doc = processors[typ](thing)

    # Remove version info
    for i in ['latest_revision', 'last_modified', 'revision']:
        if i in doc:
            doc.pop(i)

    # Unpack 'type'
    doc['type'] = doc['type']['key']

    if keys:
        keys += ['key', 'type', 'authors', 'work']
        keys = set(keys)
        for i in doc.keys():
            if i not in keys:
                doc.pop(i)

    return doc

def things_to_matches(things):
    """Converts a list of things into a list of 'matches' used by the search API"""
    matches = []
    for thing in things:
        key = thing['key']
        if key.startswith("/books"):
            edition = key
            work = thing.works[0].key
        if key.startswith("/works"):
            work = key
            edition = None
        matches.append(dict(edition = edition, work = work))
    return matches
            
            
            
        
        

# Creation/updation entry point
def create(records):
    """
    Creates one or more new records in the system.
    TODO: Describe Input/output
    """
    doc = records["doc"]
    if doc:
        things = doc_to_things(copy.deepcopy(doc))
        ret = web.ctx.site.save_many(things, 'Import new records.')
        return [x.key for x in ret]
    

# Creation helpers
def edition_doc_to_things(doc):
    """
    unpack identifers, classifiers

    Process work and author fields if present
    """
    retval = []
    # Unpack identifiers
    identifiers = doc.get("identifiers",{})
    for i in ["oclc_numbers", "isbn_10", "isbn_13", "lccn", "ocaid"]:
        if i in identifiers:
            doc[i] = identifiers.pop(i)
    if "isbn" in identifiers:
        isbns = identifiers.pop("isbn")
        isbn_10 = [x for x in isbns if len(x) == 10]
        isbn_13 = [x for x in isbns if len(x) == 13]
        if isbn_10: doc["isbn_10"] = isbn_10
        if isbn_13: doc["isbn_13"] = isbn_13

    # TODO: Unpack classifiers

    work = authors = None
    if 'work' in doc:
        work = doc.pop('work')
        work['type'] = '/type/work'
        work = doc_to_things(work)
        retval.extend(work)

    if 'authors' in doc:
        authors = doc.pop('authors')
        for i in authors:
            i['type'] = '/type/author'
        a = []
        for i in authors:
            a.extend(doc_to_things(i))
        retval.extend(a)
        authors = a

    # Attach authors to the work
    # TODO: Consider updation here?
    if work and authors:
        for i in authors:
            a = {'type': '/type/author_role', 'author': i['key']} #TODO : Check this with Anandb
            work[0].setdefault('authors',[]).append(a) # Attach this author to the work
    return retval


def work_doc_to_things(doc):
    new_things = []
    if 'authors' in doc:
        if all(isinstance(x, dict) for x in doc['authors']): # Ugly hack to prevent Things from being processed
            authors = doc['authors']
            author_entries = []
            for i in authors:
                i['type'] = '/type/author'
                new_author = doc_to_things(i)
                new_things.extend(new_author)
                a = {'type': '/type/author_role', 'author': new_author[0]['key']} #TODO : Check this with Anandb
                author_entries.append(a)
            doc['authors'] = author_entries
    return new_things


def author_doc_to_things(doc):
    return []

def doc_to_things(doc):
    """
    Receives a 'doc' (what the search API returns and receives) and
    returns a list of dictionaries that can be added into infobase.

    Expects the `type` to figure out type of object.

    Has separate sub functions to convert editions, works and
    authors. Logic is different for these three.

    This function will call itself for the 'work' and 'authors' fields
    if present.

    If the doc has a 'key', the thing corresponding to that key will
    be fetched from the database and the fields of the original doc
    updated.

    If the doc doesn't have a key, the function will call
    web.ctx.site.new_key, generate one for it and add that as the key.
    """
    retval = []
    doc = copy.deepcopy(doc)
    key = doc.get('key')
    typ = doc['type']
    # Handle key creation and updation of data
    if key:
        db_thing = web.ctx.site.get(key).dict()
        # Remove extra version related fields
        for i in ['latest_revision', 'last_modified', 'revision']:
            if i in db_thing:
                db_thing.pop(i)

        for i in db_thing.keys():
            if i in doc:
                db_thing.pop(i)
        doc.update(db_thing)
    else:
        key = web.ctx.site.new_key(typ)
        doc['key'] = key
    
    # Type specific processors
    processors = {'/type/edition' : edition_doc_to_things,
                  '/type/work'    : work_doc_to_things,
                  '/type/author'  : author_doc_to_things}
    extras = processors[typ](doc)
    retval.append(doc)
    retval.extend(extras)

    return retval

    

########NEW FILE########
__FILENAME__ = matchers
"""
Matchers
========

This module contains a list of functions that are used to search for
records in the database.

Each function will receive a dictionary that contains the search
parameters. This shouldn't be modified (make a copy if you have to
modify it). It can do whatever kinds of searches it wants to and then
should return an iterable of keys of matched things.

The `match_functions` is a list of functions in order of running. To
reduce computation, it's a good idea to put the more accurately
matching ones which require less queries on the top (e.g. exact ISBN
searches etc.). Adding a new matcher means creating a new function and
then adding the function to this list.


"""

import copy
from collections import defaultdict
import logging as Logging

from infogami import config
from openlibrary.utils.solr import Solr
import web


logger = Logging.getLogger(__name__)


def get_works_solr():
    base_url = "http://%s/solr/works" % config.plugin_worksearch.get('solr')
    return Solr(base_url)

def get_authors_solr():
    base_url = "http://%s/solr/authors" % config.plugin_worksearch.get('author_solr')
    return Solr(base_url)


def match_isbn(params):
    "Search by ISBN for exact matches"
    if "isbn" in params.get("identifiers",{}):
        isbns = params["identifiers"]["isbn"]
        q = {
            'type':'/type/edition',
            'isbn_': [str(x) for x in isbns]
            }
        logger.debug("ISBN query : %s", q)
        ekeys = list(web.ctx.site.things(q))
        if ekeys:
            return ekeys
    return []

def match_identifiers(params):
    "Match by identifiers"
    print params
    counts = defaultdict(int)
    identifiers = copy.deepcopy(params.get("identifiers",{}))
    for i in ["oclc_numbers", "lccn", "ocaid"]:
        if i in identifiers:
            val = identifiers.pop(i)
            query = {'type':'/type/edition',
                     i : val}
            matches = web.ctx.site.things(query)
            for i in matches:
                counts[i] += 1
    for k,v in identifiers.iteritems(): # Rest of the identifiers
        print "Trying ", k , v
        query = {'type':'/type/edition',
                 'identifiers' : {k : v}}
        matches = web.ctx.site.things(query)
        for i in matches:
            counts[i] += 1

    return sorted(counts, key = counts.__getitem__, reverse = True)

def match_tap_infogami(params):
    "Search infogami using title, author and publishers"
    return []

def match_tap_solr(params):
    """Search solr for works using title and author and narrow using
    publishers.

    Note:
    This function is ugly and the idea is to contain ugliness here
    itself so that it doesn't leak into the rest of the library.
    
    """

    asolr = get_authors_solr()
    wsolr = get_works_solr()
    # First find author keys. (if present in query) (TODO: This could be improved)
    # if "authors" in params:
    #     q = 'name:(%s) OR alternate_names:(%s)' % (name, name)
    
    return []


match_functions = [match_isbn,
                   match_identifiers,
                   match_tap_infogami,
                   match_tap_solr
                   ]




########NEW FILE########
__FILENAME__ = conftest
def pytest_funcarg__compare_results(request):
    """Returns a function to compare two objects d1 an d2 recursively
    skipping the 'key', "last_modified" and "revision" keys if
    present"""
    def compare_results(d1, d2):
        
        for i in ["revision", "last_modified", "key"]:
            if i in d1: d1.pop(i)
            if i in d2: d2.pop(i)

        # print compare_results.depth * 2 * "--" +  "Comparing\n","1 ==> ",d1, "\n2 ==> ", d2
        if d1 == d2: # Trivially the same
            return True
        if isinstance(d1, list) and isinstance(d2, list) and len(d1) == len(d2):
            for i,j in zip(d1, d2):
                # compare_results.depth += 1
                ret = compare_results(i, j)
                # compare_results.depth -= 1
                # print " ==> ",ret, "\n"
                if ret:
                    pass
                else:
                    return False
            return True

        if isinstance(d1, dict) and isinstance(d2, dict) and len(d1.keys()) == len(d2.keys()):
            for k,v in d1.iteritems():
                # compare_results.depth += 1
                ret = compare_results(d1.get(k), d2.get(k))
                # print " ==> ",ret, "\n"
                # compare_results.depth -= 1
                if ret:
                    pass
                else:
                    return False
            return True
        return False

    # compare_results.depth = 0
    return compare_results

########NEW FILE########
__FILENAME__ = test_functions
import pytest


from ..functions import doc_to_things, search, create, thing_to_doc, things_to_matches, find_matches_by_isbn, find_matches_by_identifiers, find_matches_by_title_and_publishers, massage_search_results


def populate_infobase(site):
    "Dumps some documents into infobase"
    ## Create two authors
    atype = '/type/author'
    akey0 = site.new_key(atype)
    a0 = {'name'         : 'Test author 1',
          'type'         : {'key': atype},
          'key'          : akey0}
    
    akey1 = site.new_key(atype)
    a1 = {'name'         : 'Test author 1',
          'type'         : {'key': atype},
          'key'          : akey1}
    
    
    ## Create a work
    wtype = '/type/work'
    wkey = site.new_key(wtype)
    w = { 
        'title'        : 'test1',
        'type'         : {'key': wtype},
        'key'          : wkey,
        'authors'      : [ {'author' : a0}, {'author' : a1}]
        }
    site.save(w)    
    
    ## Create two editions for this work
    editions = []
    etype = '/type/edition'
    for i in range(2):
        ekey = site.new_key(etype)
        e = {
            'title'        : 'test1',
            'type'         : {'key': etype},
            'lccn'         : ['123%d'%i],
            'oclc_numbers' : ['456%d'%i],
            'key'          : ekey,
            'ocaid'        : "12345%d"%i,
            'isbn_10'      : ["123456789%d"%i],
            "works"        : [{ "key": wkey }]
            }
        site.save(e)
        editions.append(ekey)
        
    ## Now create a work without any edition
    wkey = site.new_key(wtype)
    w = {
        'title'        : 'editionless',
        'type'         : {'key': wtype},
        'key'          : wkey,
        }
    site.save(w)
    
    

def test_doc_to_thing_adds_key_to_edition(mock_site):
    "Test whether doc_to_things adds a key to an edition"
    doc = {'type' : '/type/edition'}
    thing = doc_to_things(doc)
    assert 'key' in thing[0]
    assert thing[0]['key'] == '/books/OL1M'

def test_doc_to_thing_adds_key_to_work(mock_site):
    "Test whether doc_to_things adds a key to a work"
    doc = {'type' : '/type/work'}
    thing = doc_to_things(doc)
    assert 'key' in thing[0]
    assert thing[0]['key'] == '/works/OL1W'

def test_doc_to_thing_adds_key_to_author(mock_site):
    "Test whether doc_to_things adds a key to an author"
    doc = {'type' : '/type/author'}
    thing = doc_to_things(doc)
    assert 'key' in thing[0]
    assert thing[0]['key'] == '/authors/OL1A'

def test_doc_to_thing_updation_of_edition(mock_site):
    "Tests whether edition records are populated with fields from the database"
    populate_infobase(mock_site)
    doc = {'type' : '/type/edition', 'key' : '/books/OL1M'}
    thing = doc_to_things(doc)
    expected = {'title': 'test1',
                'lccn': ['1230'],
                'isbn_10': ['1234567890'],
                'key': '/books/OL1M',
                'ocaid': '123450',
                'oclc_numbers': ['4560'],
                'works': [{'key' : '/works/OL1W'}],
                'type': '/type/edition'}
    assert thing[0] == expected

def test_doc_to_thing_updation_of_work(mock_site):
    "Tests whether work records are populated with fields from the database"
    populate_infobase(mock_site)
    doc = {'type' : '/type/work', 'key' : '/works/OL1W'}
    thing = doc_to_things(doc)
    authors = thing[0].pop('authors')
    expected = {'type': '/type/work', 'key': '/works/OL1W', 'title': 'test1'}
    assert thing[0] == expected
    assert set(i['author'] for i in authors) == set(['/authors/OL3A', '/authors/OL4A'])

def test_doc_to_thing_unpack_work_and_authors_from_edition(mock_site):
    "Tests if the 'work' and 'author' fields in a an edition doc are unpacked and converted."
    doc = {'type' : '/type/edition', 
           'work' : { 'title' : 'Test title for work'},
           'authors' : [ {'name' : 'Test author'} ]
           }
    things = doc_to_things(doc)
    expected = [{'key': '/books/OL1M', 'type': '/type/edition'}, # The edition
                
                {'authors': [{'author': '/authors/OL1A', 'type': '/type/author_role'}],
                 'key': '/works/OL1W',
                 'title': 'Test title for work',
                 'type': '/type/work'}, # The work
                
                {'key': '/authors/OL1A', 'name': 'Test author', 'type': '/type/author'} # The author
                ]
    assert expected  == things

def test_doc_to_thing_unpack_authors_from_work(mock_site):
    "Tests if the 'authors' fields in a work doc are unpacked and converted."
    doc = {'type' : '/type/work', 
           'title' : 'This is a test book',
           'authors' : [ {'name' : 'Test author'} ]
           }
    things = doc_to_things(doc)
    expected = [                
                {'authors': [{'author': '/authors/OL1A', 'type': '/type/author_role'}],
                 'key': '/works/OL1W',
                 'title': 'This is a test book',
                 'type': '/type/work'}, # The work
                
                {'key': '/authors/OL1A', 'name': 'Test author', 'type': '/type/author'} # The author
                ]
    assert expected  == things

def test_doc_to_thing_unpack_identifiers(mock_site):
    "Tests if the identifiers are unpacked from an edition"
    doc = {'type' : '/type/edition', 
           'identifiers' : {"oclc_numbers" : ['1234'],
                            "isbn_10" : ['1234567890'],
                            "isbn_13" : ['1234567890123'],
                            "lccn" : ['5678'],
                            "ocaid" : ['90']}}
    things = doc_to_things(doc)
    for k,v in doc['identifiers'].iteritems():
        assert things[0][k] == v



def test_create(mock_site):
    "Tests the create API"
    doc = {'type' : '/type/edition', 
           'publisher' : "Test publisher",
           'work' : { 'title' : 'Test title for work'},
           'authors' : [{'name' : 'Test author'}],
           'identifiers' : {"oclc_numbers" : ['1234'],
                            "isbn_10" : ['1234567890'],
                            "isbn_13" : ['1234567890123'],
                            "lccn" : ['5678'],
                            "ocaid" : ['90']}}
    create({'doc' : doc})
    work = mock_site.get("/works/OL1W")
    edition = mock_site.get("/books/OL1M")
    author = mock_site.get("/authors/OL1A")
    # Check work
    assert work.title == "Test title for work"
    assert len(work.authors) == 1
    assert work.authors[0].author == "/authors/OL1A"
    # Check edition
    for k,v in doc['identifiers'].iteritems():
        assert edition[k] == v
    edition.publisher = "Test publisher"
    # Check author
    assert author.name == "Test author"

def test_thing_to_doc_edition(mock_site):
    "Tests whether an edition is properly converted back into a doc"
    populate_infobase(mock_site)
    edition = mock_site.get('/books/OL1M')
    doc = thing_to_doc(edition)
    expected = {'authors': [{'key': '/authors/OL1A'}, {'key': '/authors/OL2A'}],
                'identifiers': {'isbn': ['1234567890'],
                                'lccn': ['1230'],
                                'ocaid': '123450',
                                'oclc_numbers': ['4560']},
                'key': '/books/OL1M',
                'title': 'test1',
                'type': u'/type/edition',
                'work': {'key': u'/works/OL1W'}}
    assert doc == expected

def test_thing_to_doc_edition_key_limiting(mock_site):
    "Tests whether extra keys are removed during converting an edition into a doc"
    populate_infobase(mock_site)
    edition = mock_site.get('/books/OL1M')
    doc = thing_to_doc(edition, ["title"])
    expected = {'authors': [{'key': '/authors/OL1A'}, {'key': '/authors/OL2A'}],
                'key': '/books/OL1M',
                'title': 'test1',
                'type': u'/type/edition',
                'work': {'key': u'/works/OL1W'}}
    assert doc == expected


def test_thing_to_doc_work(mock_site):
    "Tests whether a work is properly converted back into a doc"
    populate_infobase(mock_site)
    edition = mock_site.get('/works/OL1W')
    doc = thing_to_doc(edition)
    expected = {'authors': [{'key': '/authors/OL1A'}, {'key': '/authors/OL2A'}],
                'key': '/works/OL1W',
                'title': 'test1',
                'type': u'/type/work'}
    assert doc == expected

def test_things_to_matches(mock_site):
    """Tests whether a list of keys is converted into a list of
    'matches' as returned by the search API"""
    populate_infobase(mock_site)
    matches = things_to_matches(['/books/OL1M', '/works/OL2W'])
    expected = [{'edition': '/books/OL1M', 'work': u'/works/OL1W'},
                {'edition': None, 'work': '/works/OL2W'}]
    assert matches == expected

@pytest.mark.skipif('"isbn_ not supported by mock_site"')
def test_find_matches_by_isbn(mock_site):
    """Tests whether books are matched by ISBN"""
    populate_infobase(mock_site)
    matches = find_matches_by_isbn(['1234567890'])
    assert matches == ['/books/OL1M']

def test_find_matches_by_identifiers(mock_site):
    "Validates the all and any return values of find_matches_by_identifiers"
    # First create 2 records
    record0 = {'doc': {'identifiers' : {"oclc_numbers" : ["1807182"],
                                        "lccn": [ "34029558"],
                                        'isbn_10': ['1234567890']},
                      'key': None,
                      'title': 'THIS IS A TEST BOOK 1',
                      'type': '/type/edition'}}

    record1 = {'doc': {'identifiers' : {"oclc_numbers" : ["2817081"],
                                        "lccn": [ "34029558"],
                                        'isbn_10': ['09876543210']},
                       'key': None,
                       'title': 'THIS IS A TEST BOOK 2',
                       'type': '/type/edition'}}
    
    create(record0)
    create(record1)

    q = {'oclc_numbers': "1807182", 'lccn': '34029558'}
                              
    results = find_matches_by_identifiers(q)

    assert results["all"] == ['/books/OL1M']
    assert results["any"] == ['/books/OL1M', '/books/OL2M']

def test_find_matches_by_title_and_publishers(mock_site):
    "Try to search for a record that should match by publisher and year of publishing"
    record0 = {'doc': {'isbn_10': ['1234567890'],
                       'key': None,
                       'title': 'Bantam book',
                       'type': '/type/edition',
                       'publishers' : ['Bantam'],
                       'publish_year': '1992'}}

    record1 = {'doc': {'isbn_10': ['0987654321'],
                       'key': None,
                       'title': 'Dover book',
                       'type': '/type/edition',
                       'publishers' : ['Dover'],
                       'publish_year': '2000'}}
               

    create(record0)
    create(record1)
    
    # A search that should fail
    q = {'publishers': ["Bantam"], 
         'publish_year': '2000'}
    result = find_matches_by_title_and_publishers(q)
    assert not result, "Found a match '%s' where there should have been none"%result

    # A search that should return the first entry (title, publisher and year)
    q = {'title': 'Bantam book',
         'publishers': ["Bantam"], 
         'publish_year': '1992'}
    result = find_matches_by_title_and_publishers(q)
    assert result == ['/books/OL1M']

    # A search that should return the second entry (title only)
    q = {'title': 'Dover book'}
    result = find_matches_by_title_and_publishers(q)
    assert result == ['/books/OL2M']
    # TODO: Search by title and then filter for publisher in the application directly.

    
def test_search_by_title(mock_site):
    "Drill the main search API using title"
    populate_infobase(mock_site)
    q = {'title' : "test1"}
    matches = search({"doc" : q})
    expected = {'doc': {'authors': [{'key': '/authors/OL1A'}, {'key': '/authors/OL2A'}],
                        'key': '/books/OL1M',
                        'title': 'test1',
                        'type': u'/type/edition',
                        'work': {'key': u'/works/OL1W'}},
                'matches': [{'edition': '/books/OL1M', 'work': u'/works/OL1W'},
                            {'edition': '/books/OL2M', 'work': u'/works/OL1W'}]}
    assert matches == expected

    
@pytest.mark.skipif('"isbn_ not supported by mock_site"')
def test_search_by_isbn(mock_site):
    "Drill the main search API using isbn"
    populate_infobase(mock_site)
    q = ['1234567890']
    matches = search({"doc" : {"identifiers" : {"isbn" : q}}})
    assert matches == {'doc': {'authors': [{'key': '/authors/OL1A'}, {'key': '/authors/OL2A'}],
                               'identifiers': {'isbn': ['1234567890'],
                                               'lccn': ['1230'],
                                               'ocaid': '123450',
                                               'oclc_numbers': ['4560']},
                               'key': '/books/OL1M',
                               'title': 'test1',
                               'type': u'/type/edition',
                               'work': {'key': u'/works/OL1W'}},
                       'matches': [{'edition': '/books/OL1M', 'work': u'/works/OL1W'}]}


def test_massage_search_results_edition(mock_site):
    "Test if search results are properly massaged"
    populate_infobase(mock_site)
    matches = ['/books/OL1M', '/books/OL2M']
    # With limiting
    massaged = massage_search_results(matches, {"title":None})
    expected = {'doc': {'authors': [{'key': '/authors/OL1A'}, {'key': '/authors/OL2A'}],
                        'key': '/books/OL1M',
                        'title': 'test1',
                        'type': u'/type/edition',
                        'work': {'key': u'/works/OL1W'}},
                'matches': [{'edition': '/books/OL1M', 'work': u'/works/OL1W'},
                            {'edition': '/books/OL2M', 'work': u'/works/OL1W'}]}
    assert massaged == expected
    
    # Without limiting
    massaged = massage_search_results(matches)
    expected = {'doc': {'authors': [{'key': '/authors/OL1A'}, {'key': '/authors/OL2A'}],
                        'identifiers': {'isbn': ['1234567890'],
                                        'lccn': ['1230'],
                                        'ocaid': '123450',
                                        'oclc_numbers': ['4560']},
                        'key': '/books/OL1M',
                        'title': 'test1',
                        'type': u'/type/edition',
                        'work': {'key': u'/works/OL1W'}},
                'matches': [{'edition': '/books/OL1M', 'work': u'/works/OL1W'},
                            {'edition': '/books/OL2M', 'work': u'/works/OL1W'}]}
    assert massaged == expected
    
#TODO : Test when no matches at all are found
    

########NEW FILE########
__FILENAME__ = db_load_authors
import simplejson as json
import web, re

re_author_key = re.compile('^/a/OL(\d+)A$')

db = web.database(dbn='mysql', db='openlibrary')
db.printing = False

total = 6540759

sizes = dict(name=512, birth_date=256, death_date=256, date=256)

num = 0
for line in open('author_file'):
    num += 1
    if num % 10000 == 0:
        print "%d %d %.2f%%" % (num, total, (float(num) * 100.0) / total)
    src_a = json.loads(line[:-1])
    m = re_author_key.match(src_a['key'])
    akey_num = int(m.group(1))
    db_a = { 'akey': akey_num }

    for f in 'name', 'birth_date', 'death_date', 'date':
        if not src_a.get(f, None):
            continue
        db_a[f] = src_a[f]
        if len(db_a[f]) > sizes[f]-1:
            print f, len(db_a[f]), db_a[f]

    if 'alternate_names' in src_a:
        assert all('\t' not in n for n in src_a['alternate_names'])
        db_a['alt_names'] = '\t'.join(src_a['alternate_names'])
    db.insert('authors', **db_a)

########NEW FILE########
__FILENAME__ = db_load_works
import web, re
import simplejson as json

db = web.database(dbn='mysql', user='root', passwd='', db='openlibrary')
db.printing = False

re_work_key = re.compile('^/works/OL(\d+)W$')

total = 13941626
num = 0
for line in open('work_file'):
    num += 1
    if num % 10000 == 0:
        print "%d %d %.2f%%" % (num, total, (float(num) * 100.0) / total)
    #src_w = json.loads(line[:-1])
    w = eval(line)
    wkey = int(re_work_key.match(w['key']).group(1))
    vars={'k':wkey, 'title':w['title']}
    db.query('replace into works (wkey, title) values ($k, $title)', vars=vars)

########NEW FILE########
__FILENAME__ = facet_hash
import string
from hashlib import sha1 as mkhash

# choose token length to make collisions unlikely (if there is a
# rare collision once in a while, we tolerate it, it just means
# that users may occasionally see some extra search results.
# don't make it excessively large because the tokens do use index space.
# The probability of a collision is approx.  1 - exp(-k**2 / (2*n)) where
# k = total # of facet tokens (= # of books * avg # of fields)
# n = 26 ** facet_token_length
# so for k = 10**8 and facet_token_length = 12,
# this probability is 1 - exp(-1e16/(2*26**12)) = approx 0.05.
# (That's the prob of EVER getting a collision, not the prob. of
# seeing a collision on any particular query).

facet_token_length = 12

# turn v into a str object, by encoding from unicode or numeric
# if necessary.
def coerce_str(v):
    if type(v) == unicode:
        v=v.encode('utf-8')
    v = str(v)    # in case v is a numeric type
    assert type(v) == str,(type(v),v)
    return v

# str, str -> str
def facet_token(field, v):
    token = []
    v = coerce_str(v)
    field = coerce_str(field)

    q = int(mkhash('FT,%s,%s'%(field,v)).hexdigest(), 16)
    for i in xrange(facet_token_length):
        q,r = divmod(q, 26)
        token.append(string.lowercase[r])
    return ''.join(token)

########NEW FILE########
__FILENAME__ = find_modified_works
#!/usr/bin/env python

import argparse
import datetime
import itertools
import json
import os
import sys
import time
import urllib2

BASE_URL = "http://openlibrary.org/recentchanges/"
# BASE_URL = "http://0.0.0.0:8080/recentchanges/"

def parse_options(args):
    parser = argparse.ArgumentParser(description="""Find works that have been changed in the given time period.

If the `from` or `to` options are specified. Prints works modified in that time period and quits.

Without these options, goes into loop mode which will keep polling openlibrary and print works modified on stdout. """)
    parser.add_argument('-f', '--from', dest='frm', type=str, 
                        help='From date (yyyy/mm/dd)', default = False)
    parser.add_argument('-t', '--to', dest='to', type=str, 
                        help='To date (yyyy/mm/dd)', default = False)
    parser.add_argument('-s', '--start-time-file', dest='start_file', type=str, 
                        help='File to store last time polling was done in loop mode', 
                        default = "find_modified_works.date")
    parser.add_argument('-d', '--delay', dest='delay', type=int, default = 3,
                        help='Number of seconds to wait between polling openlibrary in loop mode')
    parser.add_argument('-m', '--max-chunk-size', dest='max_chunk_size', default = 100, type=int,
                        help='maximum number of works returned in each loop of the loop mode')
    return parser.parse_args(args)

def extract_works(data):
    for i in data:
        for change in i['changes']:
            if change['key'].startswith("/works/"):
                yield change['key']

def get_modified_works(frm, to):
    one_day = datetime.timedelta(days = 1)
    ret = []
    logging.debug("Querying between %s and %s", frm, to)
    while frm < to:
        url = frm.strftime(BASE_URL+"%Y/%m/%d.json")
        logging.debug("Fetching changes from %s", url)
        ret.append(extract_works(json.load(urllib2.urlopen(url))))
        frm += one_day
    return itertools.chain(*ret)


def poll_for_changes(start_time_file, max_chunk_size, delay):
    try:
        with open(start_time_file) as f:
            date = datetime.datetime.strptime(f.read(), "%Y/%m/%d")
            logging.debug("Obtained last end time from file '%s'"%start_time_file)
    except IOError:
        date = datetime.datetime.now()
        logging.info("No state file. Starting from now.")
    current_day = date.day
    logging.debug("Starting at %s with current day %d", date, current_day)
    logging.debug("Will emit at most %d works", max_chunk_size)
    seen = set()
    rest = []
    while True:
        url = date.strftime(BASE_URL+"%Y/%m/%d.json")
        logging.debug("-- Fetching changes from %s", url)
        changes = list(json.load(urllib2.urlopen(url)))
        unseen_changes = list(x for x in changes if x['id'] not in seen)
        logging.debug("%d changes fetched", len(changes))
        logging.debug(" of which %d are unseen", len(unseen_changes))

        # Fetch works for all changesets we've not seen yet. Add them
        # to the ones left over from the previous iteration.
        works = list(extract_works(unseen_changes)) 
        logging.debug("  in which %d have works modified.", len(works))
        logging.debug("  There are %d left over works from the last iteration.", len(rest))
        works += rest
        logging.debug("    Totally %d works to be emitted"%len(works))

        # Record all the ones we've already emitted for this day
        for i in (x['id'] for x in unseen_changes):
            seen.add(i)

        logging.debug("Number of Changes seen so far %d", len(list(seen)))
        # If the current day is over.
        if current_day != datetime.datetime.now().day:
            seen = set() # Clear things seen so far
            date = datetime.datetime.now() # Update date
            current_day = date.day
            logging.debug("Flipping the clock to %s and clearing seen changes", date)

        # If there are too many works, emit only max_chunk_size
        # works. Keep the rest for the next iteration
        if len(works) > max_chunk_size:
            logging.debug("  Number of works to be emitted (%d) is more than %s", len(works), max_chunk_size)
            to_be_emitted, rest = works[:max_chunk_size], works[max_chunk_size:]
            logging.debug("    Retaining %s", len(rest))
        else:
            to_be_emitted, rest = works, []
            

            
        if to_be_emitted:
            logging.debug("Emitting %d works", len(to_be_emitted))
            for i in to_be_emitted:
                print i

        logging.debug("Sleeping for %d seconds", delay)
        time.sleep(delay)
        
        with open(start_time_file, "w") as f:
            logging.debug("Writing %s to state file", date.strftime("%Y/%m/%d"))
            f.write(date.strftime("%Y/%m/%d"))


    

def main():
    args = parse_options(sys.argv[1:])
    loop = not args.frm and not args.to
    if args.frm:
        frm = datetime.datetime.strptime(args.frm, "%Y/%m/%d")
    else:
        frm = datetime.datetime.now() - datetime.timedelta(days = 1)

    if args.to:
        frm = datetime.datetime.strptime(frm, "%Y/%m/%d")
    else:
        to = datetime.datetime.now()
        
    if loop:
        poll_for_changes(args.start_file, args.max_chunk_size, args.delay)
    else:
        for i in get_modified_works(frm, to):
            print i
    


if __name__ == "__main__":
    import logging
    logging.basicConfig(level = logging.DEBUG, format="%(levelname)-7s (%(asctime)s) : %(message)s")#, filename="/home/noufal/works.log")
    sys.exit(main())


########NEW FILE########
__FILENAME__ = index_data
import os, httplib, sys
import simplejson as json
from collections import defaultdict
from lxml.etree import tostring, Element

def index_publishers(data_dir):
    dir = data_dir + '/b/'
    publishers = defaultdict(int)
    for f in os.listdir(dir):
        d = json.load(open(dir + f))
        for p in d.get('publishers', None) or []:
            publishers[p] += 1
    return publishers.items()

def index_authors(data_dir):
    dir = data_dir + '/a/'
    authors = {}
    for f in os.listdir(dir):
        d = json.load(open(dir + f))
        authors[d['key']] = dict((k, d[k]) for k in ('name', 'alternate_names', 'birth_date', 'death_date', 'dates') if k in d)
    return authors.items()

def solr_post(h, solr_url, body):
    h.request('POST', solr_url, body, { 'Content-type': 'text/xml;charset=utf-8'})
    response = h.getresponse()
    response_body = response.read()
    print response.reason

def add_field(doc, name, value):
    field = Element("field", name=name)
    field.text = unicode(value)
    doc.append(field)

def load_indexes(data_dir, solr_port):
    add = {}
    add['publishers'] = Element('add')
    for k, v in index_publishers(data_dir):
        doc = Element("doc")
        add_field(doc, 'name', k)
        add_field(doc, 'count', v)
        add['publishers'].append(doc)

    add['authors'] = Element('add')
    for key, a in index_authors(data_dir):
        doc = Element("doc")
        add_field(doc, 'key', key)
        for k, v in a.items():
            if k == 'alternate_names':
                assert isinstance(v, list)
                for value in v:
                    add_field(doc, k, value)
            else:
                add_field(doc, k, v)
        add['authors'].append(doc)

    #for t in 'works', 'authors', 'publishers':
    #for t in 'authors', 'publishers':
    for t in 'publishers', 'authors':
        h1 = httplib.HTTPConnection('localhost:%d' % solr_port)
        h1.connect()
        add_xml = tostring(add[t], pretty_print=True).encode('utf-8')
        solr_url = 'http://localhost:%d/solr/%s/update' % (solr_port, t)
        print solr_url
        solr_post(h1, solr_url, '<del><query>*:*</query></del>')
        solr_post(h1, solr_url, '<commit />')
        h1.close()

        h2 = httplib.HTTPConnection('localhost:%d' % solr_port)
        h2.connect()
        solr_post(h2, solr_url, add_xml)
        solr_post(h2, solr_url, '<commit />')
        solr_post(h2, solr_url, '<optimize />')
        h2.close()

data_dir = sys.argv[1]
solr_port = int(sys.argv[2])
assert solr_port > 1024
load_indexes(data_dir, solr_port)

########NEW FILE########
__FILENAME__ = index_works
import sys, httplib, re
from time import time
from lxml.etree import tostring, Element
from openlibrary.solr.work_subject import find_subjects

t0 = time
total = 13844390

re_year = re.compile(r'(\d{4})$')
re_edition_key = re.compile('^/b(?:ooks)?/(OL\d+M)$')
re_work_key = re.compile('^/works/(OL\d+W)$')

solr_host = 'localhost:' + sys.argv[1]
update_url = 'http://' + solr_host + '/solr/works/update'
print update_url
def chunk_works(filename, size=2000):
    queue = []
    for line in open(filename):
        if len(line) > 100000000:
            print 'skipping long line:', len(line)
            continue
        queue.append(eval(line))
        if len(queue) == size:
            yield queue
            queue = []
    yield queue

re_bad_char = re.compile('[\x01\x19-\x1e]')
def strip_bad_char(s):
    if not isinstance(s, basestring):
        return s
    return re_bad_char.sub('', s)

def add_field(doc, name, value):
    field = Element("field", name=name)
    field.text = unicode(strip_bad_char(value))
    doc.append(field)

def add_field_list(doc, name, field_list):
    for value in field_list:
        add_field(doc, name, value)

to_drop = set('''!*"'();:@&=+$,/?%#[]''')

def str_to_key(s):
    return ''.join(c for c in s.lower() if c not in to_drop)

re_not_az = re.compile('[^a-zA-Z]')
def is_sine_nomine(pub):
    return re_not_az.sub('', pub).lower() == 'sn'

def build_doc(w):
    editions = w['editions']
    if len(editions) > 300:
        print `w['title'], len(editions)`
    authors = []
    if 'authors' not in w:
        print 'no authors'
    for a in w['authors']:
        if a is None:
            continue
        cur = {'key': a['key'], 'name': a.get('name', '')}
        if a.get('alternate_names', None):
            cur['alternate_names'] = a['alternate_names']
        authors.append(cur)

    subjects = find_subjects(w, marc_subjects=w['subjects']) if 'subjects' in w else {}

    doc = Element("doc")

    m = re_work_key.match(w['key'])
    add_field(doc, 'key', m.group(1))
    add_field(doc, 'title', w['title'])
    add_field(doc, 'title_suggest', w['title'])
    has_fulltext = any(e.get('ocaid', None) for e in editions)

    add_field(doc, 'has_fulltext', has_fulltext)
    if w.get('subtitle', None):
        add_field(doc, 'subtitle', w['subtitle'])

    alt_titles = set()
    for e in editions:
        if 'title' in e and e['title'] != w['title']:
            alt_titles.add(e['title'])
        for f in 'work_titles', 'other_titles':
            for t in e.get(f, []):
                if t != w['title']:
                    alt_titles.add(t)
    add_field_list(doc, 'alternative_title', alt_titles)

    alt_subtitles = set( e['subtitle'] for e in editions if e.get('subtitle', None) and e['subtitle'] != w.get('subtitle', None))
    add_field(doc, 'alternative_subtitle', alt_subtitles)

    add_field(doc, 'edition_count', len(editions))
    for e in editions:
        m = re_edition_key.match(e['key'])
        if not m:
            print 'bad edition key:', e['key']
            continue
        add_field(doc, 'edition_key', m.group(1))

    k = 'by_statement'
    add_field_list(doc, k, set( e[k] for e in editions if e.get(k, None)))

    k = 'publish_date'
    pub_dates = set(e[k] for e in editions if e.get(k, None))
    add_field_list(doc, k, pub_dates)
    pub_years = set(m.group(1) for m in (re_year.match(i) for i in pub_dates) if m)
    if pub_years:
        add_field_list(doc, 'publish_year', pub_years)
        add_field(doc, 'first_publish_year', min(int(i) for i in pub_years))

    k = 'first_sentence'
    fs = set( e[k]['value'] if isinstance(e[k], dict) else e[k] for e in editions if e.get(k, None))
    add_field_list(doc, k, fs)

    publishers = set()
    for e in editions:
        publishers.update('Sine nomine' if is_sine_nomine(i) else i for i in e.get('publishers', []))
    add_field_list(doc, 'publisher', publishers)
    add_field_list(doc, 'publisher_facet', publishers)

    field_map = [
        ('lccn', 'lccn'),
        ('publish_places', 'publish_place'),
        ('oclc_numbers', 'oclc'),
        ('contributions', 'contributor'),
    ]

    for db_key, search_key in field_map:
        v = set()
        for e in editions:
            if db_key not in e:
                continue
            v.update(e[db_key])
        add_field_list(doc, search_key, v)

    isbn = set()
    for e in editions:
        for f in 'isbn_10', 'isbn_13':
            for v in e.get(f, []):
                isbn.add(v.replace('-', ''))
    add_field_list(doc, 'isbn', isbn)

    lang = set()
    for e in editions:
        for l in e.get('languages', []):
            assert l['key'].startswith('/l/') and len(l['key']) == 6
            lang.add(l['key'][3:])
    if lang:
        add_field_list(doc, 'language', lang)

    v = set( e['ocaid'].strip() for e in editions if 'ocaid' in e)
    add_field_list(doc, 'ia', v)
    author_keys = [a['key'] for a in authors]
    assert not any(ak.startswith('/a/') for ak in author_keys)
    author_names = [a.get('name', '') for a in authors]
    assert not any('\t' in n for n in author_names)

    add_field_list(doc, 'author_key', author_keys)
    add_field_list(doc, 'author_name', author_names)

    alt_names = set()
    for a in authors:
        if 'alternate_names' in a:
            alt_names.update(a['alternate_names'])

    add_field_list(doc, 'author_alternative_name', alt_names)
    add_field_list(doc, 'author_facet', (k + '\t' + n for k, n in zip(author_keys, author_names)))
    add_field(doc, 'fiction', subjects['fiction'])

    for k in 'person', 'place', 'subject', 'time':
        if k not in subjects:
            continue
        add_field_list(doc, k, subjects[k].keys())

    for k in 'person', 'place', 'subject', 'time':
        if k not in subjects:
            continue
        add_field_list(doc, k + '_facet', subjects[k].keys())

    for k in 'person', 'place', 'subject', 'time':
        if k not in subjects:
            continue
        add_field_list(doc, k + '_key', (str_to_key(s) for s in subjects[k].keys()))

    return doc

def solr_post(h1, body):
    h1.request('POST', update_url, body.encode('utf-8'), { 'Content-type': 'text/xml;charset=utf-8'})
    return h1.getresponse()

def post_queue(h1, queue):
    add = Element("add")
    for w in queue:
        try:
            doc = build_doc(w)
            if doc is not None:
                add.append(doc)
        except:
            print w
            raise
    add_xml = tostring(add)
    del add
    print add_xml[:60]
    return solr_post(h1, add_xml)

connect = True
if connect:
    h1 = httplib.HTTPConnection(solr_host)
    h1.connect()
    response = solr_post(h1, '<delete><query>*:*</query></delete>')
    response_body = response.read()
    print response.reason
    response = solr_post(h1, '<commit/>')
    response_body = response.read()
    print response.reason

num = 0

filename = sys.argv[2]
print filename
t_prev = time()
for queue in chunk_works(filename):
    num += len(queue)
    percent = (float(num) * 100.0) / total

    if connect:
        response = post_queue(h1, queue)
        response_body = response.read()

    t = time() - t_prev
    rec_per_sec = float(len(queue)) / t
    remain = total - num
    sec_left = float(remain) / rec_per_sec
    if connect:
        print "%d / %d %.2f%%" % (num, total, percent), response.reason, 'rec/sec=%.2f  %.2f hours left' % (rec_per_sec, sec_left / 3600)
    else:
        print "%d / %d %.2f%%" % (num, total, percent), 'rec/sec=%.2f  %.2f mins left' % (rec_per_sec, sec_left / 60)

    if num % 50000 == 0:
        if connect:
            response = solr_post(h1, '<commit/>')
            response_body = response.read()
            print 'commit:', response.reason
    t_prev = time()

if connect:
    response = solr_post(h1, '<commit/>')
    response_body = response.read()
    print response.reason
    response = solr_post(h1, '<optimize/>')
    response_body = response.read()
    print response.reason

########NEW FILE########
__FILENAME__ = index_all
from Queue import Queue
import threading, datetime, re, httplib
from collections import defaultdict
from socket import socket, AF_INET, SOCK_DGRAM, SOL_UDP, SO_BROADCAST, timeout
from urllib import urlopen
from time import sleep, time
from lxml.etree import Element, tostring
from unicodedata import normalize

def add_field(doc, name, value):
    field = Element("field", name=name)
    field.text = normalize('NFC', unicode(value))
    doc.append(field)

solr_host = 'ia331509:8984'

def find_abbyy(dir_html, ia):
    if 'abbyy' not in dir_html:
        return

    for line in dir_html.splitlines():
        m = re_href.search(line)
        if not m:
            continue
        href = m.group(1)
        if href.endswith('abbyy.gz') or href.endswith('abbyy.zip') or href.endswith('abbyy.xml'):
            return href
        elif 'abbyy' in href:
            print 'bad abbyy:', `href, ia`

item_queue = Queue(maxsize=10000)
item_and_host_queue = Queue(maxsize=10000)
host_queues = defaultdict(lambda: Queue())
host_threads = {}
solr_queue = Queue(maxsize=10000)
counter_lock = threading.Lock()
items_processed = 0
input_counter_lock = threading.Lock()
input_count = 0
total = 1899481
current_book_lock = threading.Lock()
current_book = None
solr_ia_status_lock = threading.Lock()
solr_ia_status = None

re_ia_host = re.compile('^ia(\d+).us.archive.org$')
def use_secondary(host):
    m = re_ia_host.match(host)
    num = int(m.group(1))
    return host if num % 2 else 'ia%d.us.archive.org' % (num + 1)

def use_primary(host):
    m = re_ia_host.match(host)
    num = int(m.group(1))
    return host if num % 2 == 0 else 'ia%d.us.archive.org' % (num - 1)

def add_to_item_queue():
    global input_count
    skip = None
    for line in open('/home/edward/scans/book_data_2010-10-15'):
        input_counter_lock.acquire()
        input_count += 1
        input_counter_lock.release()
        ia = line[:-1]
        if skip:
            if ia == skip:
                skip = None
            continue
        current_book_lock.acquire()
        current_book = ia
        current_book_lock.release()
        item_queue.put(ia)

re_loc = re.compile('^(ia\d+\.us\.archive\.org):(/\d+/items/(.*))$')

class FindItemError(Exception):
    pass

def find_item(ia):
    s = socket(AF_INET, SOCK_DGRAM, SOL_UDP)
    s.setblocking(1)
    s.settimeout(4.0)
    s.setsockopt(1, SO_BROADCAST, 1)
    s.sendto(ia, ('<broadcast>', 8010))
    for attempt in range(5):
        (loc, address) = s.recvfrom(1024)
        m = re_loc.match(loc)

        ia_host = m.group(1)
        ia_path = m.group(2)
        if m.group(3) == ia:
            return (ia_host, ia_path)
    raise FindItemError

def run_find_item():
    while True:
        ia = item_queue.get()
        try:
            (host, path) = find_item(ia)
        except (timeout, FindItemError):
            item_queue.task_done()
            continue
        item_and_host_queue.put((ia, host, path))
        item_queue.task_done()

def run_queues():
    live = dict((t.name, t) for t in threading.enumerate())

    for host, queue in host_queues.items():
        if host in live and live[host].is_alive():
            continue
        ia, filename = queue.pop()
        t = threading.Thread(name=host, target=read_text_from_node, args=(ia, host, filename))
        t.start()
        print >> log, ('thread started', host, ia)
        log.flush()
        if not queue:
            del host_queues[host]

nl_page_count = 'page count: '
def read_text_from_node(host):
    global items_processed
    while True:
        ia, filename = host_queues[host].get()
        url = 'http://%s/fulltext/abbyy_to_text.php?file=%s' % (host, filename)
        reply = urlopen(url).read()
        if not reply:
            host_queues[host].task_done()
            continue
        index = reply.rfind(nl_page_count)
        last_nl = reply.rfind('\n')
        assert last_nl != -1
        body = reply[:index].decode('utf-8')
        assert reply[-1] == '\n'
        page_count = reply[index+len(nl_page_count):-1]
        if not page_count.isdigit():
            print url
        assert page_count.isdigit()
        solr_queue.put((ia, body, page_count))
        counter_lock.acquire()
        items_processed += 1
        counter_lock.release()
        host_queues[host].task_done()

re_href = re.compile('href="([^"]+)"')

def find_abbyy(dir_html, ia):
    if 'abbyy' not in dir_html:
        return

    for line in dir_html.splitlines():
        m = re_href.search(line)
        if not m:
            continue
        href = m.group(1)
        if href.endswith('abbyy.gz') or href.endswith('abbyy.zip') or href.endswith('abbyy.xml'):
            return href
        elif 'abbyy' in href:
            print 'bad abbyy:', `href, ia`


def index_items():
    while True:
        (ia, host, path) = item_and_host_queue.get()
        host = use_secondary(host)
        if not host:
            item_and_host_queue.task_done()
            continue
        filename = ia + '_abbyy'
        filename_gz = filename + '.gz'

        try:
            dir_html = urlopen('http://%s/%s' % (host, path)).read()
        except:
            host = use_primary(host)
            dir_html = urlopen('http://%s/%s' % (host, path)).read()
        filename = find_abbyy(dir_html, ia)
        if not filename:
            item_and_host_queue.task_done()
            continue

        host_queues[host].put((ia, path + '/' + filename))
        if host not in host_threads:
            t = threading.Thread(name=host, target=read_text_from_node, args=(host,))
            host_threads[host] = t
            t.start()
        item_and_host_queue.task_done()

def build_doc(ia, body, page_count):
    doc = Element('doc')
    add_field(doc, 'ia', ia)
    add_field(doc, 'body', body)
    add_field(doc, 'body_length', len(body))
    add_field(doc, 'page_count', page_count)
    return doc

def run_solr_queue():
    h1 = httplib.HTTPConnection(solr_host)
    h1.connect()
    while True:
        (ia, body, page_count) = solr_queue.get()
        add = Element("add")
        doc = build_doc(ia, body, page_count)
        add.append(doc)
        r = tostring(add).encode('utf-8')
        url = 'http://%s/solr/inside/update' % solr_host
        h1.request('POST', url, r, { 'Content-type': 'text/xml;charset=utf-8'})
        response = h1.getresponse()
        response_body = response.read()
        assert response.reason == 'OK'
        solr_ia_status_lock.acquire()
        solr_ia_status = ia
        solr_ia_status_lock.release()
        solr_queue.task_done()

t0 = time()

def status_thread():
    sleep(1)
    while True:
        run_time = time() - t0
        print 'run time:         %8.2f minutes' % (float(run_time) / 60)
        print 'input queue:      %8d' % item_queue.qsize()
        print 'after find_item:  %8d' % item_and_host_queue.qsize()
        print 'solr queue:       %8d' % solr_queue.qsize()

        input_counter_lock.acquire()
        rec_per_sec = float(input_count) / run_time
        remain = total - input_count
        input_counter_lock.release()

        sec_left = remain / rec_per_sec
        hours_left = float(sec_left) / (60 * 60)
        print 'input count:      %8d (%.2f items/second)' % (input_count, rec_per_sec)
        print '                  %8.2f hours left (%.1f days/left)' % (hours_left, hours_left / 24)

        counter_lock.acquire()
        print 'items processed:  %8d (%.2f items/second)' % (items_processed, float(items_processed) / run_time)
        counter_lock.release()
        current_book_lock.acquire()
        print 'current book:', current_book
        current_book_lock.release()
        solr_ia_status_lock.acquire()
        print 'most recently feed to solr:', solr_ia_status
        solr_ia_status_lock.release()

        host_count = 0
        queued_items = 0
        for host, host_queue in host_queues.items():
            if not host_queue.empty():
                host_count += 1
            qsize = host_queue.qsize()
            queued_items += qsize
        print 'host queues:      %8d' % host_count
        print 'items queued:     %8d' % queued_items
        print
        if run_time < 120:
            sleep(1)
        else:
            sleep(5)

t1 = threading.Thread(target=add_to_item_queue)
t1.start()
t2 = threading.Thread(target=run_find_item)
t2.start()
t3 = threading.Thread(target=index_items)
t3.start()
t_solr1 = threading.Thread(target=run_solr_queue)
t_solr1.start()
t_solr2 = threading.Thread(target=run_solr_queue)
t_solr2.start()
t5 = threading.Thread(target=status_thread)
t5.start()

item_queue.join()
item_and_host_queue.join()
for host, host_queue in host_queues.items():
    host_queue.join()
solr_queue.join()

########NEW FILE########
__FILENAME__ = index_gevent
from gevent import sleep, spawn, spawn_link_exception, monkey
from gevent.queue import JoinableQueue
from datetime import datetime
monkey.patch_socket()
import re, httplib, json, sys, os, codecs
from openlibrary.utils.ia import find_item
from time import time
from collections import defaultdict
from lxml.etree import Element, tostring, parse, fromstring
import urllib2
from unicodedata import normalize

scan_list = '/home/edward/scans/book_data_2011-01-07'
input_count = 0
current_book = None
find_item_book = None
item_queue = JoinableQueue(maxsize=100)
solr_queue = JoinableQueue(maxsize=1000)
locator_times = []
#item_and_host_queue = JoinableQueue(maxsize=10000)
t0 = time()
total = 1936324
items_processed = 0
items_skipped = 0
solr_ia_status = None
solr_error = False
host_queues = defaultdict(lambda: JoinableQueue(maxsize=1000))
#solr_src_host = 'localhost:8983'
solr_host = 'localhost:8983'
re_href = re.compile('href="([^"]+)"')
host_threads = {}
load_log = open('/1/log/index_inside', 'w')
good_log = open('/1/log/book_good', 'a')
bad_log = open('/1/log/book_bad', 'a')
done_count = 0
good_count = 0
bad_count = 0
two_threads_per_host = True

page_counts = dict(eval(line) for line in open('/1/abbyy_text/page_count'))

# http://www.archive.org/services/find_file.php?file=bostonharborunce00bost&loconly=1

def done(ia, was_good):
    global done_count, good_count, bad_count
    book_log = good_log if was_good else bad_log
    if was_good:
        good_count += 1
    else:
        bad_count += 1
    print >> book_log, ia
    done_count += 1

re_ia_host = re.compile('^ia(\d+).us.archive.org$')
def use_secondary(host):
    m = re_ia_host.match(host)
    num = int(m.group(1))
    return host if num % 2 else 'ia%d.us.archive.org' % (num + 1)

def use_primary(host):
    m = re_ia_host.match(host)
    num = int(m.group(1))
    return host if num % 2 == 0 else 'ia%d.us.archive.org' % (num - 1)

def urlread_keep_trying(url):
    for i in range(3):
        try:
            return urllib2.urlopen(url).read()
        except urllib2.HTTPError, error:
            if error.code in (403, 404):
                #print "404 for '%s'" % url
                raise
            else:
                print 'error:', error.code, error.msg
            pass
        except httplib.BadStatusLine:
            print 'bad status line'
        except httplib.IncompleteRead:
            print 'incomplete read'
        except urllib2.URLError:
            pass
        print url, "failed"
        sleep(2)
        print "trying again"

def find_abbyy(dir_html, ia):
    if 'abbyy' not in dir_html:
        return

    for line in dir_html.splitlines():
        m = re_href.search(line)
        if not m:
            continue
        href = m.group(1)
        if href.endswith('abbyy.gz') or href.endswith('abbyy.zip') or href.endswith('abbyy.xml'):
            return href
        elif 'abbyy' in href:
            print 'bad abbyy:', `href, ia`

nl_meta = 'meta: '
re_meta = re.compile('meta: ([a-z]+) (\d+)')
def read_text_from_node(host):
    global items_processed
    while True:
        #print 'host_queues[%s].get()' % host
        num, ia, path = host_queues[host].get()

        filename = ia + '_abbyy'
        filename_gz = filename + '.gz'

        url = 'http://%s/%s' % (host, path)
        try:
            dir_html = urlread_keep_trying('http://%s/%s' % (host, path))
        except urllib2.HTTPError, error:
            if error.code == 403:
                print '403 on directory listing for:', ia
                dir_html = None
        if not dir_html:
            done(ia, False)
            host_queues[host].task_done()
            continue

        filename = find_abbyy(dir_html, ia)
        if not filename:
            done(ia, False)
            host_queues[host].task_done()
            continue

        url = 'http://%s/~edward/abbyy_to_text.php?ia=%s&path=%s&file=%s' % (host, ia, path, filename)
        try:
            reply = urlread_keep_trying(url)
        except urllib2.HTTPError, error:
            if error.code != 403:
                raise
            url = 'http://%s/~edward/abbyy_to_text_p.php?ia=%s&path=%s&file=%s' % (host, ia, path, filename)
            reply = urlread_keep_trying(url)
        if not reply or 'language not currently OCRable' in reply[:200]:
            done(ia, False)
            host_queues[host].task_done()
            continue
        index = reply.rfind(nl_meta)
        if index == -1:
            print 'bad reply'
            print url
            done(ia, False)
            host_queues[host].task_done()
            continue
        last_nl = reply.rfind('\n')
        assert last_nl != -1
        body = reply[:index].decode('utf-8')
        assert reply[-1] == '\n'
        try:
            (lang, page_count) = re_meta.match(reply[index:-1]).groups()
        except:
            print 'searching:', index, `reply[index:-1]`
            raise
        assert page_count.isdigit()
        if body != '':
            meta_xml = urlread_keep_trying('http://%s%s/%s_meta.xml' % (host, path, ia))
            root = fromstring(meta_xml)
            collection = [e.text for e in root.findall('collection')]

            #print 'solr_queue.put((ia, body, page_count))'
            solr_queue.put((ia, body, lang, page_count, collection))
            #print 'solr_queue.put() done'
            items_processed += 1
        else:
            done(ia, False)
        host_queues[host].task_done()

#def index_items():
#    while True:
#        (num, ia, host, path) = item_and_host_queue.get()
#
#        host_queues[host].put((num, ia, path, filename))
#        if host not in host_threads:
#            host_threads[host] = spawn_link_exception(read_text_from_node, host)
#        item_and_host_queue.task_done()

def add_to_item_queue():
    global input_count, current_book, items_skipped
    skip = False
    check_for_existing = False
    items_done = set(line[:-1] for line in open('/1/log/book_good'))
    items_done.update(line[:-1] for line in open('/1/log/book_bad'))
    for line in open('/home/edward/scans/book_data_2010-12-09'):
        input_count += 1
        ia = line[:-1]
        if ia.startswith('WIDE-2010'):
            continue
        if ia in items_done:
            items_skipped += 1
            continue

        current_book = ia
        if check_for_existing:
            url = 'http://' + solr_host + '/solr/inside/select?indent=on&wt=json&rows=0&q=ia:' + ia
            num_found = json.load(urllib2.urlopen(url))['response']['numFound']
            if num_found != 0:
                continue

        #print 'item_queue.put((input_count, ia))'
        item_queue.put((input_count, ia))
        #print 'item_queue.put((input_count, ia)) done'

lang_map = [
    ('eng', ['english', 'en']),
    ('fre', ['french', 'fr']),
    ('ger', ['german', 'de', 'deu']),
    ('spa', ['spanish', 'spa', 'es']),
    ('ita', ['italian', 'it']),
    ('rus', ['russian', 'ru']),
    ('dut', ['dutch']),
    ('por', ['portuguese']),
    ('dan', ['danish']),
    ('swe', ['swedish']),
]

lang_dict = {}
for a, b in lang_map:
    lang_dict[a] = a
    for c in b:
        lang_dict[c] = a

def tidy_lang(l):
    return lang_dict.get(l.lower().strip('.'))

def run_find_item():
    global find_item_book
    while True:
        (num, ia) = item_queue.get()
        find_item_book = ia
        #print 'find_item:', ia
        t0_find_item = time()
        try:
            (host, path) = find_item(ia)
        except FindItemError:
            t1_find_item = time() - t0_find_item
            #print 'fail find_item:', ia, t1_find_item
            item_queue.task_done()
            done(ia, False)
            continue
        t1_find_item = time() - t0_find_item
        #print 'find_item:', ia, t1_find_item
        if len(locator_times) == 100:
            locator_times.pop(0)
        locator_times.append((t1_find_item, host))

        body = None
        if False:
            url = 'http://' + solr_src_host + '/solr/inside/select?wt=json&rows=10&q=ia:' + ia
            response = json.load(urllib2.urlopen(url))['response']
            if response['numFound']:
                doc = response['docs'][0]
                for doc_lang in ['eng', 'fre', 'deu', 'spa', 'other']:
                    if doc.get('body_' + doc_lang):
                        body = doc['body_' + doc_lang]
                        break
                assert body
        filename = '/1/abbyy_text/data/' + ia[:2] + '/' + ia
        if os.path.exists(filename):
            body = codecs.open(filename, 'r', 'utf-8').read()
        if body:
            try:
                meta_xml = urlread_keep_trying('http://%s%s/%s_meta.xml' % (host, path, ia))
            except urllib2.HTTPError, error:
                if error.code != 403:
                    raise
                print '403 on meta XML for:', ia
                item_queue.task_done() # skip
                done(ia, False)
                continue
            try:
                root = fromstring(meta_xml)
            except:
                print 'identifer:', ia
            collection = [e.text for e in root.findall('collection')]
            elem_noindex = root.find('noindex')
            if elem_noindex is not None and elem_noindex.text == 'true' and ('printdisabled' not in collection and 'lendinglibrary' not in collection):
                item_queue.task_done() # skip
                done(ia, False)
                continue
            lang_elem = root.find('language')
            if lang_elem is None:
                print meta_xml
            if lang_elem is not None:
                lang = tidy_lang(lang_elem.text) or 'other'
            else:
                lang = 'other'

            #print 'solr_queue.put((ia, body, page_count))'
            solr_queue.put((ia, body, lang, page_counts[ia], collection))
            #print 'solr_queue.put() done'
        else:
            host_queues[host].put((num, ia, path))
            if host not in host_threads:
                host_threads[host] = spawn_link_exception(read_text_from_node, host)
        item_queue.task_done()

def add_field(doc, name, value):
    field = Element("field", name=name)
    field.text = normalize('NFC', unicode(value))
    doc.append(field)

def build_doc(ia, body, page_count):
    doc = Element('doc')
    add_field(doc, 'ia', ia)
    add_field(doc, 'body', body)
    add_field(doc, 'body_length', len(body))
    add_field(doc, 'page_count', page_count)
    return doc


def run_solr_queue(queue_num):
    def log(line):
        print >> load_log, queue_num, datetime.now().isoformat(), line
    global solr_ia_status, solr_error
    while True:
        log('solr_queue.get()')
        (ia, body, lang, page_count, collection) = solr_queue.get()
        assert lang != 'deu'
        log(ia + ' - solr_queue.get() done')
        add = Element("add")
        esc_body = normalize('NFC', body.replace(']]>', ']]]]><![CDATA[>'))
        r = '<add commitWithin="10000000"><doc>\n'
        r += '<field name="ia">%s</field>\n' % ia
        if lang != 'other': # also in schema copyField -> body
            r += '<field name="body_%s"><![CDATA[%s]]></field>\n' % (lang, esc_body)
        else: 
            r += '<field name="body"><![CDATA[%s]]></field>\n' % esc_body
        r += '<field name="body_length">%s</field>\n' % len(body)
        r += '<field name="page_count">%s</field>\n' % page_count
        for c in collection:
            r += '<field name="collection">%s</field>\n' % c
        if lang != 'other':
            r += '<field name="language">%s</field>\n' % lang
        r += '</doc></add>\n'

        #doc = build_doc(ia, body, page_count)
        #add.append(doc)
        #r = tostring(add).encode('utf-8')
        url = 'http://%s/solr/inside/update' % solr_host
        #print '       solr post:', ia
        log(ia + ' - solr connect and post')
        h1 = httplib.HTTPConnection(solr_host)
        h1.connect()
        h1.request('POST', url, r.encode('utf-8'), { 'Content-type': 'text/xml;charset=utf-8'})
        #print '    request done:', ia
        try:
            response = h1.getresponse()
        #print 'getresponse done:', ia
            response_body = response.read()
        #print '        response:', ia, response.reason
        except:
            codecs.open('bad.xml', 'w', 'utf-8').write(r)
            open('error_reply', 'w').write(response_body)
            raise
        h1.close()
        log(ia + ' - read solr connect and post done')
        if response.reason != 'OK':
            print r[:100]
            print '...'
            print r[-100:]

            print 'reason:', response.reason
            print 'reason:', response_body
            solr_error = (response.reason, response_body)
            break
        assert response.reason == 'OK'
        solr_ia_status = ia
        log(ia + ' - solr_queue.task_done()')
        done(ia, True)
        solr_queue.task_done()
        log(ia + ' - solr_queue.task_done() done')

def status_thread():
    sleep(2)
    while True:
        run_time = time() - t0
        if solr_error:
            print '***solr error***'
            print solr_error
        print 'run time:            %8.2f minutes' % (float(run_time) / 60)
        print 'input queue:         %8d' % item_queue.qsize()
        #print 'after find_item:     %8d' % item_and_host_queue.qsize()
        print 'solr queue:          %8d' % solr_queue.qsize()

        #rec_per_sec = float(input_count - items_skipped) / run_time
        if done_count:
            rec_per_sec = float(done_count) / run_time
            remain = total - (done_count + items_skipped)

            sec_left = remain / rec_per_sec
            hours_left = float(sec_left) / (60 * 60)
            print 'done count:          %8d (%.2f items/second)' % (done_count, rec_per_sec)
            print '%8d good (%8.2f%%)   %8d bad (%8.2f%%)' % (good_count, ((float(good_count) * 100) / done_count), bad_count, ((float(bad_count) * 100) / done_count))
            print '       %8.2f%%       %8.2f hours left (%.1f days/left)' % (((float(items_skipped + done_count) * 100.0) / total), hours_left, hours_left / 24)

        #print 'items processed:     %8d (%.2f items/second)' % (items_processed, float(items_processed) / run_time)
        print 'current book:              ', current_book
        print 'most recently feed to solr:', solr_ia_status

        host_count = 0
        queued_items = 0
        for host, host_queue in host_queues.items():
            if not host_queue.empty():
                host_count += 1
            qsize = host_queue.qsize()
            queued_items += qsize
        print 'host queues:         %8d' % host_count
        print 'items queued:        %8d' % queued_items
        if locator_times:
            print 'average locator time: %8.2f secs' % (float(sum(t[0] for t in locator_times)) / len(locator_times))
            #print sorted(locator_times, key=lambda t:t[0], reverse=True)[:10]
        print
        if run_time < 120:
            sleep(1)
        else:
            sleep(5)

if __name__ == '__main__':
    t_status = spawn_link_exception(status_thread)
    t_item_queue = spawn_link_exception(add_to_item_queue)
    for i in range(80):
        spawn_link_exception(run_find_item)
    #t_index_items = spawn_link_exception(index_items)
    for i in range(8):
        spawn_link_exception(run_solr_queue, i)

    #joinall([t_run_find_item, t_item_queue, t_index_items, t_solr])

    sleep(1)
    print 'join item_queue thread'
    t_item_queue.join()
    print 'item_queue thread complete'
    #print 'join item_and_host_queue:', item_and_host_queue.qsize()
    #item_and_host_queue.join()
    #print 'item_and_host_queue complete'
    for host, host_queue in host_queues.items():
        qsize = host_queue.qsize()
        print 'host:', host, qsize
        host_queue.join()

    print 'join solr_queue:', solr_queue.qsize()
    solr_queue.join()
    print 'solr_queue complete'

########NEW FILE########
__FILENAME__ = post_authors
import web, re, httplib, sys, codecs
from lxml.etree import tostring, Element
from time import time

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

db = web.database(dbn='mysql', db='openlibrary')
db.printing = False

re_year = re.compile(r'(\d{4})$')
re_author_key = re.compile('^/a/OL(\d+)A$')
re_work_key = re.compile('^/works/OL(\d+)W$')

solr_host = 'localhost:8983'
update_url = 'http://' + solr_host + '/solr/authors/update'

connect = False

def solr_post(h1, body):
    if not connect:
        return 'not connected'
    h1.request('POST', update_url, body, { 'Content-type': 'text/xml;charset=utf-8'})
    response = h1.getresponse()
    response.read()
    return response.reason

h1 = None
if connect:
    h1 = httplib.HTTPConnection(solr_host)
    h1.connect()
    print solr_post(h1, '<delete><query>*:*</query></delete>')
    print solr_post(h1, '<commit/>')

re_bad_char = re.compile('[\x01\x19-\x1e]')
def strip_bad_char(s):
    if not isinstance(s, basestring):
        return s
    return re_bad_char.sub('', s)

def add_field(doc, name, value):
    field = Element("field", name=name)
    field.text = unicode(strip_bad_char(value))
    doc.append(field)

def add_field_list(doc, name, field_list):
    for value in field_list:
        add_field(doc, name, value)

def u8(s):
    if s is not None:
        return s.decode('utf-8')

def work_title(wkey):
    return u8(db.query('select title from works where wkey=$wkey', vars=locals())[0].title)

total = db.query('select count(*) as total from authors')[0].total
num = 0
add = Element("add")
for row in db.query('select * from authors'):
    num += 1

    doc = Element("doc")
    add_field(doc, 'key', 'OL%dA' % row.akey)
    name = u8(row.name)
    if name is None:
        continue
    add_field(doc, 'name', name)
    add_field(doc, 'name_str', name.lower())
    if row.alt_names is not None:
        add_field_list(doc, 'alternate_names', u8(row.alt_names).split('\t'))
    for f in 'birth_date', 'death_date', 'date':
        if row[f] is not None:
            add_field(doc, f, u8(row[f]))

    if row.top_work:
        wt = work_title(row.top_work)
        if wt:
            add_field(doc, 'top_work', wt)

    f = 'top_subjects'
    if row[f] is not None:
        add_field_list(doc, f, u8(row[f]).split('\t'))

    add.append(doc)

    if len(add) == 100:
        add_xml = tostring(add, pretty_print=True).encode('utf-8')
        del add
        print "%d/%d %.4f%%" % (num,total,(float(num)*100.0/total)), solr_post(h1, add_xml)
        add = Element("add")

if len(add):
    add_xml = tostring(add, pretty_print=True).encode('utf-8')
    del add
    print solr_post(h1, add_xml)
print 'end'
print solr_post(h1, '<commit/>')
print solr_post(h1, '<optimize/>')

########NEW FILE########
__FILENAME__ = process_stats
"""Script to process various stats collected by the system and load them into solr.

The loan stats are currently sorted in couchdb. Solr provides wonderful
faceting that allows us to build beautiful views of data.

This file provides all the functionality to take one loan stats record, 
query OL and archive.org for other related info like subjects, collections
etc. and massages that into a form that can be fed into solr.

How to run:

    ./scripts/openlibrary-server openlibrary.yml runmain openlibrary.solr.process_stats --load 

"""
import sys
import web
import simplejson
import logging
import os

from infogami import config
from openlibrary.solr.solrwriter import SolrWriter
from ..core import inlibrary, ia, helpers as h
from ..core.loanstats import LoanStats

logger = logging.getLogger("openlibrary.solr")

@web.memoize
def get_db():
    return web.database(**web.config.db_parameters)

def get_document(key):
    return web.ctx.site.get(key)

def is_region(library):
    return bool(library.lending_region)

def get_region(library):
    if library.lending_region:
        return library.lending_region

    # Take column #3 from address. The available columns are:
    # name, street, city, state, ...        
    try:
        # Open library of Richmond is really rest of the world 
        if library.addresses and library.key != "/libraries/openlibrary_of_richmond":
            return library.addresses.split("|")[3]
    except IndexError:
        pass
    return "WORLD"

_libraries = None
def get_library(key):
    global _libraries
    if _libraries is None:
        _libraries = dict((lib.key, lib) for lib in inlibrary.get_libraries())
    return _libraries.get(key, None)

_ia_db = None
def get_ia_db():
    """Metadata API is slow. 
    Talk to archive.org database directly if it is specified in the configuration.
    """
    if not config.get("ia_db"):
        return
    global _ia_db
    if not _ia_db:
        settings = config.ia_db
        host = settings['host']
        db = settings['db']
        user = settings['user']
        pw = os.popen(settings['pw_file']).read().strip()
        _ia_db = web.database(dbn="postgres", host=host, db=db, user=user, pw=pw)
    return _ia_db

@web.memoize
def get_metadata(ia_id):
    if not ia_id:
        return {}
    db = get_ia_db()
    if db:        
        result = db.query("SELECT collection, sponsor, contributor FROM metadata WHERE identifier=$ia_id", vars=locals())
        meta = result and result[0] or {}
        if meta:
            meta['collection'] = meta['collection'].split(";")
    else:
        meta = ia.get_meta_xml(ia_id)
    return meta

class LoanEntry(web.storage):
    @property
    def book_key(self):
        return self['book']

    @property
    def book(self):
        return get_document(self['book'])

    def get_subjects(self, type="subject"):
        w = self.book.works[0]
        if w:
            return w.get_subject_links(type)
        else:
            return []

    def get_author_keys(self):
        w = self.book and self.book.works and self.book.works[0]
        if w:
            return [a.key for a in w.get_authors()]
        else:
            return []

    @property
    def metadata(self):
        return get_metadata(self.book.ocaid)

    @property
    def library(self):  
        key = self.get("library")
        lib = key and get_library(key)
        if lib and not is_region(lib):
            return lib.key.split("/")[-1]

    @property
    def region(self):
        key = self.get("library")
        lib = key and get_library(key)
        if lib:
            region =  get_region(lib).lower().strip()
            # some regions are specified with multiple names.
            # maintaining this dict to collapse them into single entry.
            region_aliases = {"california": "ca"}
            return region_aliases.get(region, region)

def process(data):
    doc = LoanEntry(data)
    if not doc.book:
        logger.error("Book not found for %r. Ignoring this loan", doc['book'])
        return

    solrdoc = {
        "key": doc.key,
        "type": "stats",
        "stats_type_s": "loan",
        "book_key_s": doc.book_key,
        "author_keys_id": doc.get_author_keys(),
        "title": doc.book.title or "Untitled",
        "ia": doc.book.ocaid or None,
        "resource_type_s": doc.resource_type,
        "ia_collections_id": doc.metadata.get("collection", []),
        "sponsor_s": doc.metadata.get("sponsor"),
        "contributor_s": doc.metadata.get("contributor"),
        "library_s": doc.library,
        "region_s": doc.region,
        "start_time_dt": doc.t_start + "Z",
        "start_day_s":doc.t_start.split("T")[0],
    }

    if doc.get('t_end'):
        dt = h.parse_datetime(doc.t_end) - h.parse_datetime(doc.t_start)
        hours = dt.days * 24 + dt.seconds / 3600
        solrdoc['duration_hours_i'] = hours

    #last_updated = h.parse_datetime(doc.get('t_end') or doc.get('t_start'))
    solrdoc['last_updated_dt'] = (doc.get('t_end') or doc.get('t_start')) + 'Z'

    solrdoc['subject_key'] = []
    solrdoc['subject_facet'] = []
    def add_subjects(type):
        subjects = doc.get_subjects(type)
        if type == 'subject':
            system_subjects = ['protected_daisy', 'accessible_book', 'in_library', 'lending_library']
            subjects = [s for s in subjects if s.slug not in system_subjects]
        solrdoc['subject_key'] += [type+":"+s.slug for s in subjects]
        solrdoc['subject_facet'] += [type+":"+s.title for s in subjects]

    add_subjects("subject")
    add_subjects("place")
    add_subjects("person")
    add_subjects("time")

    year = doc.book.get_publish_year()
    if year:
        solrdoc['publish_year'] = year

    if "geoip_country" in doc:
        solrdoc['country_s'] = doc['geoip_country']

    # Remove None values
    solrdoc = dict((k, v) for k, v in solrdoc.items() if v is not None)
    return solrdoc

def read_events():
    for line in sys.stdin:
        doc = simplejson.loads(line.strip())
        yield doc

def read_events_from_db(keys=None, day=None):
    if keys:
        result = get_db().query("SELECT key, json FROM stats WHERE key in $keys ORDER BY updated", vars=locals())
    elif day:
        last_updated = day
        result = get_db().query("SELECT key, json FROM stats WHERE updated >= $last_updated AND updated < $last_updated::timestamp + interval '1' day ORDER BY updated", vars=locals())
    else:
        last_updated = LoanStats().get_last_updated()
        result = get_db().query("SELECT key, json FROM stats WHERE updated > $last_updated ORDER BY updated", vars=locals())
    for row in result.list():
        doc = simplejson.loads(row.json)
        doc['key'] = row.key
        yield doc

def debug():
    """Prints debug info about solr.
    """

def add_events_to_solr(events):
    solrdocs = (process(e) for e in events)
    solrdocs = (doc for doc in solrdocs if doc) # ignore Nones
    update_solr(solrdocs)

def main(*args):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    if "--load" in args:        
        docs = read_events()
        update_solr(docs)
    elif args and args[0] == "--load-from-db":
        events = read_events_from_db()
        add_events_to_solr(events)
    elif args and args[0] == "--load-keys":
        keys = args[1:]
        events = read_events_from_db(keys=keys)
        add_events_to_solr(events)
    elif args and args[0] == "--day":
        day = args[1]
        events = read_events_from_db(day=day)
        add_events_to_solr(events)
    elif args and args[0] == "--debug":
        debug()
    else:
        docs = read_events()
        # each doc is one row from couchdb view response when called with include_docs=True
        for e in docs:
            try:
                result = process(e['doc'])
                print simplejson.dumps(result)
            except Exception:
                logger.error("Failed to process %s", e['doc']['_id'], exc_info=True)

def fix_subject_key(doc, name, prefix):
    if name in doc:
        doc[name] = [v.replace(prefix, '') for v in doc[name]]

def update_solr(docs):
    solr = SolrWriter("localhost:8983")
    for doc in docs:
        # temp fix for handling already processed data
        doc = dict((k, v) for k, v in doc.items() if v is not None)
        if isinstance(doc.get("ia_collections_id"), str):
            doc['ia_collections_id'] = doc['ia_collections_id'].split(";")

        fix_subject_key(doc, 'subject_key', '/subjects/')
        fix_subject_key(doc, 'place_key', '/subjects/place:')
        fix_subject_key(doc, 'person_key', '/subjects/person:')
        fix_subject_key(doc, 'time_key', '/subjects/time:')

        system_subjects = ['subject:Protected DAISY', 'subject:Accessible book', 'subject:In library', 'subject:Lending library']
        doc['subject_facet'] = [s for s in doc['subject_facet'] if s not in system_subjects]

        solr.update(doc)
    solr.commit()

########NEW FILE########
__FILENAME__ = read_dump
import re
import simplejson as json
from time import time
import web

re_author_key = re.compile('^/a/OL(\d+)A$')
re_work_key = re.compile('^/works/OL(\d+)W$')
re_edition_key = re.compile('^/b/OL(\d+)M$')

_escape_dict = {'\n': r'\n', '\r': r'\r', '\t': r'\t', '\\': r'\\'}
def make_sub(d):
    """
        >>> f = make_sub(dict(a='aa', bb='b'))
        >>> f('aabbb')
        'aaaabb'
    """
    def f(a):
        return d[a.group(0)]
    rx = re.compile("|".join(map(re.escape, d.keys())))
    return lambda s: s and rx.sub(f, s)
def invert_dict(d):
    return dict((v, k) for (k, v) in d.items())
unescape = make_sub(invert_dict(_escape_dict))

def read_input():
    max_v = 0
    xdata = None
    xthing_id = None

    for line in open('data_sorted.txt'):
        thing_id, v, data = line[:-1].split('\t')
        v = int(v)
        if xthing_id and xthing_id != thing_id:
            yield unescape(xdata)
            max_v = v
            xdata = data
        elif v > max_v:
            max_v = v
            xdata = data
        xthing_id = thing_id
    yield unescape(xdata)

out_edition = open('edition_file', 'w')
out_author = open('author_file', 'w')
out_edition_work = open('edition_work_file', 'w')
out_work = open('work_file', 'w')
works = []
authors = {}
misc_fields = ['created', 'modified', 'last_modified', 'latest_revision', 'personal_name', 'id', 'revision', 'type']
rec_no = 0
t0 = time()
for data in read_input():
    rec_no += 1
    if rec_no % 100000 == 0:
        t1 = time() - t0
        print "%s %.2f minutes" % (web.commify(rec_no), (float(t1) / 60.0))
    try:
        d = json.loads(data)
    except:
        print data
        raise
    t = d['type']['key']
    k = d['key']
    if t == '/type/edition':
        m = re_edition_key.match(k)
        if not m:
            print 'bad edition key:', k
            print data
            continue
        print >> out_edition, data
        for w in d.get('works', []):
            m2 = re_work_key.match(w['key'])
            if not m2:
                continue
            wkey_num = m2.group(1)
            print >> out_edition_work, wkey_num + '\t' + data
        continue
    if t == '/type/work':
        m = re_work_key.match(k)
        if not m:
            print 'bad work key:', k
            print data
            continue
        wkey_num = m.group(1)
        w = {
            'key': d['key'],
            'title': d['title'],
            'authors': [a['author']['key'] for a in d.get('authors', [])]
        }
        for f in 'subtitle', 'subjects', 'subject_places', 'subject_times', 'subject_people':
            if f not in d:
                continue
            w[f] = d[f]
        f = 'cover_edition'
        if f in d:
            w[f] = d[f]['key']
#        works.append(w)
        print >> out_work, w
        continue
    if t == '/type/author':
        m = re_author_key.match(k)
        if not m:
            print 'bad author key:', k
            print data
            continue
        for f in misc_fields:
            if f in d:
                del d[f]
        print >> out_author, json.dumps(d)
        #authors[k] = d
        continue
out_edition.close()
out_edition_work.close()
out_author.close()

print 'end'
print 'total records:', rec_no

sys.exit(0)

out_work = open('work_file', 'w')
for w in works:
    m = re_work_key.match(w['key'])
    wkey_num = m.group(1)
    w['authors'] = [authors[akey] for akey in w['authors'] if (akey in authors)]
    print >> out_work, wkey_num, w
out_work.close()


########NEW FILE########
__FILENAME__ = solrdump
#! /usr/bin/env python
"""Generates a JSON dump suitable for solr import from openlibrary dumps.

Glossary:

xwork document:

	A dictionary contains all informatation about a work. It contains the following fields.

	work - The work document
	editions - The list of edition documents belong to the work
	authors - The list of authors of this work
	ia - dictionary of ia metadata of all ia-ids referenced in the editions
	duplicates - dict of duplicates mapping for key to all it's duplicates

solr document:

	A dictionary with elements from Solr schema of Open Library. These 
	documents can be imported to solr after converting to xml.
"""
import sys
import json
import gzip
import web

from update_work import process_edition_data, process_work_data

def process_xwork(doc):
	"""Process xwork document and yield multiple solr documents.
	"""
	work = doc['work']
	editions = doc['editions']
	authors = doc['authors']
	ia = doc['ia']

	d = dict(work=work, editions=editions, authors=authors, ia=ia, duplicates={})
	yield process_work_data(d)

	for e in editions:
		d = web.storage(edition=e, work=work, authors=authors)
		yield process_edition_data(d)

def xopen(path, mode="r"):
	if path.endswith(".gz"):
		return gzip.open(path, mode)
	else:
		return open(path, mode)

def read_xworks_dump(filename):
	for line in xopen(filename):
		key, jsontext = line.strip().split("\t")
		yield json.loads(jsontext)

def write_solr_dump(docs):
	for doc in docs:
		print json.dumps(doc)

def main(xworks_filename):
	solr_docs = (doc for xwork in read_xworks_dump(xworks_filename) 
					 for doc in process_xwork(xwork))
	write_solr_dump(solr_docs)

if __name__ == '__main__':
	main(sys.argv[1])

########NEW FILE########
__FILENAME__ = solrwriter
"""Interface to update solr.
"""
import httplib
import logging
import re

from lxml.etree import tostring, Element
from unicodedata import normalize

logger = logging.getLogger("openlibrary.solrwriter")

class SolrWriter(object):
    """Interface to update solr.
    """
    def __init__(self, host, core=None):
        self.host = host
        if core:
            self.update_url = "/solr/%s/update" % core
        else:
            self.update_url = "/solr/update"

        self.conn = None
        self.identifier_field = "key"
        self.pending_updates = []

    def get_conn(self):
        if self.conn is None:
            self.conn = httplib.HTTPConnection(self.host)
        return self.conn

    def request(self, xml):
        """Sends an update request to solr with given XML.
        """
        conn = self.get_conn()

        logger.info('request: %r', xml[:65] + '...' if len(xml) > 65 else xml)
        conn.request('POST', self.update_url, xml, { 'Content-type': 'text/xml;charset=utf-8'})
        response = conn.getresponse()
        response_body = response.read()

        logger.info(response.reason)
        if response.reason != 'OK':
            logger.error(response_body)
        assert response.reason == 'OK'

    def delete(self, key):
        logger.info("deleting %s", key)
        q = '<delete><id>%s</id></delete>' % key
        self.request(q)

    def update(self, document):        
        logger.info("updating %s", document.get(self.identifier_field))
        self.pending_updates.append(document)
        if len(self.pending_updates) >= 100:
            self.flush()
        return

    def flush(self):
        if self.pending_updates:
            root = Element("add")
            for doc in self.pending_updates:
                node = dict2element(doc)
                root.append(node)
            logger.info("flushing %d documents", len(self.pending_updates))
            self.pending_updates = []
            xml = tostring(root).encode('utf-8')
            self.request(xml)

    def commit(self):
        self.flush()
        logger.info("<commit/>")
        self.request("<commit/>")

    def optimize(self):
        logger.info("<optimize/>")
        self.request("<optimize/>")

re_bad_char = re.compile('[\x01\x0b\x1a-\x1e]')
def strip_bad_char(s):
    if not isinstance(s, basestring):
        return s
    return re_bad_char.sub('', s)

def add_field(doc, name, value):
    if isinstance(value, (list, set)):
        for v in value:
            add_field(doc, name, v)
        return
    else:
        field = Element("field", name=name)
        if not isinstance(value, basestring):
            value = str(value)
        try:
            value = strip_bad_char(value)
            if isinstance(value, str):
                value = value.decode('utf-8')
            field.text = normalize('NFC', value)
        except:
            logger.error('Error in normalizing %r', value)
            raise
        doc.append(field)

def dict2element(d):
    doc = Element("doc")
    for k, v in d.items():
        add_field(doc, k, v)
    return doc

########NEW FILE########
__FILENAME__ = test_update_work
from update_work import build_data

author_counter = 0
edition_counter = 0
work_counter = 0

def make_author(**kw):
    global author_counter
    author_counter += 1
    kw.setdefault("key", "/authors/OL%dA" % author_counter)
    kw.setdefault("type", {"key": "/type/author"})
    kw.setdefault("name", "Foo")
    return kw

def make_edition(**kw):
    global edition_counter
    edition_counter += 1
    kw.setdefault("key", "/books/OL%dM" % edition_counter)
    kw.setdefault("type", {"key": "/type/edition"})
    kw.setdefault("title", "Foo")
    return kw

def make_work(**kw):
    global work_counter
    work_counter += 1
    kw.setdefault("key", "/works/OL%dW" % work_counter)
    kw.setdefault("type", {"key": "/type/work"})
    kw.setdefault("title", "Foo")
    return kw

class Test_build_data:

    def strip_empty_lists(self, d):
        """strips empty lists from a dict"""
        def emptylist(x):
            return isinstance(x, list) and len(x) == 0

        return dict((k, v) for k, v  in d.items() if not emptylist(v))

    def match_dicts(self, d, expected):
        """Returns True if d has all the keys in expected and all those values are equal.
        """
        return all(k in d for k in expected) and all(d[k] == expected[k] for k in expected)

    def test_simple(self):
        work = {
            "key": "/works/OL1M",
            "type": {"key": "/type/work"},
            "title": "Foo"
        }

        d = build_data(work)
        assert self.match_dicts(self.strip_empty_lists(d), {
            "key": "OL1M",
            "title": "Foo",
            "has_fulltext": False,
            "edition_count": 0,
        })

    def test_edition_count(self):
        work = make_work()

        d = build_data(work)
        assert d['edition_count'] == 0

        work['editions'] = [make_edition()]
        d = build_data(work)
        assert d['edition_count'] == 1
    
        work['editions'] = [make_edition(), make_edition()]
        d = build_data(work)
        assert d['edition_count'] == 2

    def test_edition_key(self):
        work = make_work(editions=[
            make_edition(key="/books/OL1M"),
            make_edition(key="/books/OL2M"),
            make_edition(key="/books/OL3M")])

        d = build_data(work)
        assert d['edition_key'] == ["OL1M", "OL2M", "OL3M"]

    def test_publish_year(self):
        work = make_work(editions=[
                    make_edition(publish_date="2000"),
                    make_edition(publish_date="Another 2000"),

                    ## Doesn't seems to be handling this case
                    #make_edition(publish_date="2001-01-02"),

                    make_edition(publish_date="Jan 2002"),
                    make_edition(publish_date="Bad date 12")])

        d = build_data(work)
        assert set(d['publish_year']) == set(["2000", "2002"])
        assert d["first_publish_year"] == 2000

    def test_isbns(self):
        work = make_work(editions=[
                    make_edition(isbn_10=["123456789X"])])
        d = build_data(work)
        assert d['isbn'] == ['123456789X', '9781234567897']

        work = make_work(editions=[
                    make_edition(isbn_10=["9781234567897"])])
        d = build_data(work)
        assert d['isbn'] == ['123456789X', '9781234567897']

    def test_other_identifiers(self):
        work = make_work(editions=[
                    make_edition(oclc_numbers=["123"], lccn=["lccn-1", "lccn-2"]),
                    make_edition(oclc_numbers=["234"], lccn=["lccn-2", "lccn-3"]),
        ])
        d = build_data(work)
        assert sorted(d['oclc']) == ['123', '234']
        assert sorted(d['lccn']) == ['lccn-1', 'lccn-2', 'lccn-3']
        
    def test_identifiers(self):
        work = make_work(editions=[
                    make_edition(identifiers={"librarything": ["lt-1"]}),
                    make_edition(identifiers={"librarything": ["lt-2"]})
        ])
        d = build_data(work)
        assert sorted(d['id_librarything']) == ['lt-1', 'lt-2']
        assert 'overdrive_s' not in d

        e1 = make_edition(identifiers={"overdrive": ["o-1"]})
        e2 = make_edition(identifiers={"overdrive": ["o-2"]})
        work = make_work(editions=[e1, e2])
        d = build_data(work)
        assert sorted(d['id_overdrive']) == ['o-1', 'o-2']
        assert sorted(d['overdrive_s'].split(";")) == ['o-1', 'o-2']

    def test_ia_boxid(self):
        e = make_edition()
        w = make_work(editions=[e])
        d = build_data(w)
        assert 'ia_box_id' not in d

        e = make_edition(ia_box_id='foo')
        w = make_work(editions=[e])
        d = build_data(w)
        assert 'ia_box_id' in d
        assert d['ia_box_id'] == ['foo']

    def test_with_one_lending_edition(self):
        e = make_edition(key="/books/OL1M", ocaid='foo00bar', _ia_meta={"collection":['lendinglibrary', 'americana']})
        w = make_work(editions=[e])
        d = build_data(w)
        assert d['has_fulltext'] == True
        assert d['public_scan_b'] == False
        assert 'printdisabled_s' not in d
        assert d['lending_edition_s'] == 'OL1M'
        assert d['ia'] == ['foo00bar']
        assert d['ia_collection_s'] == "lendinglibrary;americana"
        assert d['edition_count'] == 1
        assert d['ebook_count_i'] == 1

    def test_with_two_lending_editions(self):
        e1 = make_edition(key="/books/OL1M", ocaid='foo01bar', _ia_meta={"collection":['lendinglibrary', 'americana']})
        e2 = make_edition(key="/books/OL2M", ocaid='foo02bar', _ia_meta={"collection":['lendinglibrary', 'internetarchivebooks']})
        w = make_work(editions=[e1, e2])
        d = build_data(w)
        assert d['has_fulltext'] == True
        assert d['public_scan_b'] == False
        assert 'printdisabled_s' not in d
        assert d['lending_edition_s'] == 'OL1M'
        assert d['ia'] == ['foo01bar', 'foo02bar']
        assert d['ia_collection_s'] == "lendinglibrary;americana;internetarchivebooks"
        assert d['edition_count'] == 2
        assert d['ebook_count_i'] == 2

    def test_with_one_inlibrary_edition(self):
        e = make_edition(key="/books/OL1M", ocaid='foo00bar', _ia_meta={"collection":['printdisabled', 'inlibrary']})
        w = make_work(editions=[e])
        d = build_data(w)
        assert d['has_fulltext'] == True
        assert d['public_scan_b'] == False
        assert d['printdisabled_s'] == 'OL1M'
        assert d['lending_edition_s'] == 'OL1M'
        assert d['ia'] == ['foo00bar']
        assert d['ia_collection_s'] == "printdisabled;inlibrary"
        assert d['edition_count'] == 1
        assert d['ebook_count_i'] == 1

    def test_with_one_printdisabled_edition(self):
        e = make_edition(key="/books/OL1M", ocaid='foo00bar', _ia_meta={"collection":['printdisabled', 'americana']})
        w = make_work(editions=[e])
        d = build_data(w)
        assert d['has_fulltext'] == True
        assert d['public_scan_b'] == False
        assert d['printdisabled_s'] == 'OL1M'
        assert 'lending_edition_s' not in d
        assert d['ia'] == ['foo00bar']
        assert d['ia_collection_s'] == "printdisabled;americana"
        assert d['edition_count'] == 1
        assert d['ebook_count_i'] == 1

    def test_with_multliple_editions(self):
        e1 = make_edition(key="/books/OL1M")
        e2 = make_edition(key="/books/OL2M", ocaid='foo00bar', _ia_meta={"collection":['americana']})
        e3 = make_edition(key="/books/OL3M", ocaid='foo01bar', _ia_meta={"collection":['lendinglibrary', 'americana']})
        e4 = make_edition(key="/books/OL4M", ocaid='foo02bar', _ia_meta={"collection":['printdisabled', 'inlibrary']})
        w = make_work(editions=[e1, e2, e3, e4])
        d = build_data(w)
        assert d['has_fulltext'] == True
        assert d['public_scan_b'] == True
        assert d['printdisabled_s'] == 'OL4M'
        assert d['lending_edition_s'] == 'OL3M'
        assert d['ia'] == ['foo00bar', 'foo01bar', 'foo02bar']
        assert sorted(d['ia_collection_s'].split(";")) == ["americana", "inlibrary", "lendinglibrary", "printdisabled"]

        assert d['edition_count'] == 4
        assert d['ebook_count_i'] == 3

    def test_subjects(self):
        w = make_work(subjects=["a", "b c"])
        d = build_data(w)

        assert d['subject'] == ['a', "b c"]
        assert d['subject_facet'] == ['a', "b c"]
        assert d['subject_key'] == ['a', "b_c"]

        assert "people" not in d
        assert "place" not in d
        assert "time" not in d

        w = make_work(
                subjects=["a", "b c"], 
                subject_places=["a", "b c"],
                subject_people=["a", "b c"],
                subject_times=["a", "b c"])
        d = build_data(w)

        for k in ['subject', 'person', 'place', 'time']:
            assert d[k] == ['a', "b c"]
            assert d[k + '_facet'] == ['a', "b c"]
            assert d[k + '_key'] == ['a', "b_c"]

    def test_language(self):
        pass

    def test_author_info(self):
        w = make_work(authors=[
                {"author": make_author(key="/authors/OL1A", name="Author One", alternate_names=["Author 1"])},
                {"author": make_author(key="/authors/OL2A", name="Author Two")}
            ])
        d = build_data(w)
        assert d['author_name'] == ["Author One", "Author Two"]
        assert d['author_key'] == ['OL1A', 'OL2A']
        assert d['author_facet'] ==  ['OL1A Author One', 'OL2A Author Two']
        assert d['author_alternative_name'] == ["Author 1"]


########NEW FILE########
__FILENAME__ = update
from openlibrary.utils import str_to_key
import simplejson, re
from urllib import urlopen, quote_plus

re_escape = re.compile("([%s])" % re.escape(r'+-!(){}[]^"~*?:\\'))

solr_works = 'ol-solr:8983'
solr_subjects = 'ol-solr:8983'

def subject_count(field, subject):
    if len(subject) > 256:
        subject = subject[:256]
    key = re_escape.sub(r'\\\1', str_to_key(subject)).encode('utf-8')
    url = 'http://%s/solr/works/select?indent=on&wt=json&rows=0&q=%s_key:%s' % (solr_works, field, key)
    for attempt in range(5):
        try:
            data = urlopen(url).read()
            break
        except IOError:
            pass
        print 'exception in subject_count, retry, attempt:', attempt
        print url
    try:
        ret = simplejson.loads(data)
    except:
        print data
        return 0
    return ret['response']['numFound']

def subject_need_update(key, count):
    escape_key = quote_plus(re_escape.sub(r'\\\1', key).encode('utf-8'))

    reply = urlopen('http://%s/solr/subjects/select?indent=on&wt=json&q=key:"%s"' % (solr_subjects, escape_key)).read()

    try:
        docs = simplejson.loads(reply)['response']['docs']
    except:
        print (key, escape_key)
        print reply
        raise
    if not docs:
        return True
    assert len(docs) == 1
    return count != docs[0]['count']


########NEW FILE########
__FILENAME__ = update_work
import httplib, re, sys
from openlibrary.catalog.utils.query import query_iter, withKey, has_cover, set_query_host, base_url as get_ol_base_url
#from openlibrary.catalog.marc.marc_subject import get_work_subjects, four_types
from lxml.etree import tostring, Element, SubElement
from pprint import pprint
import urllib2, urllib
from urllib2 import URLError, HTTPError
import simplejson as json
import time
import web
from openlibrary import config
from unicodedata import normalize
from collections import defaultdict
from openlibrary.utils.isbn import opposite_isbn
from openlibrary.core import helpers as h
from infogami.infobase.client import ClientException
import logging
import datetime

logger = logging.getLogger("openlibrary.solr")

re_lang_key = re.compile(r'^/(?:l|languages)/([a-z]{3})$')
re_author_key = re.compile(r'^/(?:a|authors)/(OL\d+A)')
#re_edition_key = re.compile(r'^/(?:b|books)/(OL\d+M)$')
re_edition_key = re.compile(r"/books/([^/]+)")

solr_host = {}

def urlopen(url, data=None):
    version = "%s.%s.%s" % sys.version_info[:3]
    user_agent = 'Mozilla/5.0 (openlibrary; %s) Python/%s' % (__file__, version)
    headers = {
        'User-Agent': user_agent
    }
    req = urllib2.Request(url, data, headers)
    return urllib2.urlopen(req)

def get_solr(index):
    global solr_host

    if not config.runtime_config:
        config.load('openlibrary.yml')

    if not solr_host:
        solr_host = {
            'works': config.runtime_config['plugin_worksearch']['solr'],
            'authors': config.runtime_config['plugin_worksearch']['author_solr'],
            'subjects': config.runtime_config['plugin_worksearch']['subject_solr'],
            'editions': config.runtime_config['plugin_worksearch']['edition_solr'],
        }
    return solr_host[index]
    
def load_config():
    if not config.runtime_config:
        config.load('openlibrary.yml')

def is_single_core():
    """Returns True if we are using new single core solr setup that maintains
    all type of documents in a single core."""
    return config.runtime_config.get("single_core_solr", False)

def is_borrowed(edition_key):
    """Returns True of the given edition is borrowed.
    """
    key = "/books/" + edition_key
    
    load_config()
    infobase_server = config.runtime_config.get("infobase_server")
    if infobase_server is None:
        logger.error("infobase_server not defined in the config. Unabled to find borrowed status.")
        return False
            
    url = "http://%s/openlibrary.org/_store/ebooks/books/%s" % (infobase_server, edition_key)
    
    try:
        d = json.loads(urlopen(url).read())
    except HTTPError, e:
        # Return False if that store entry is not found 
        if e.getcode() == 404:
            return False
        # Ignore errors for now
        return False
    return d.get("borrowed", "false") == "true"

re_collection = re.compile(r'<(collection|boxid)>(.*)</\1>', re.I)

def get_ia_collection_and_box_id(ia):
    if len(ia) == 1:
        return

    def get_list(d, key):
        value = d.get(key, [])
        if not isinstance(value, list):
            value = [value]
        return value

    matches = {'boxid': set(), 'collection': set() }
    url = "http://archive.org/metadata/%s/metadata" % ia
    logger.info("loading metadata from %s", url)
    for attempt in range(5):
        try:
            d = json.loads(urlopen(url).read()).get('result', {})
            matches['boxid'] = set(get_list(d, 'boxid'))
            matches['collection'] = set(get_list(d, 'collection'))
        except URLError:
            logger.warn("retry %s %s", attempt, url)
            time.sleep(5)
    return matches

class AuthorRedirect (Exception):
    pass

re_bad_char = re.compile('[\x01\x0b\x1a-\x1e]')
re_year = re.compile(r'(\d{4})$')
re_iso_date = re.compile(r'^(\d{4})-\d\d-\d\d$')
def strip_bad_char(s):
    if not isinstance(s, basestring):
        return s
    return re_bad_char.sub('', s)

def add_field(doc, name, value):
    field = Element("field", name=name)
    try:
        field.text = normalize('NFC', unicode(strip_bad_char(value)))
    except:
        logger.error('Error in normalizing %r', value)
        raise
    doc.append(field)

def add_field_list(doc, name, field_list):
    for value in field_list:
        add_field(doc, name, value)

to_drop = set(''';/?:@&=+$,<>#%"{}|\\^[]`\n\r''')

def str_to_key(s):
    return ''.join(c if c != ' ' else '_' for c in s.lower() if c not in to_drop)

re_not_az = re.compile('[^a-zA-Z]')
def is_sine_nomine(pub):
    return re_not_az.sub('', pub).lower() == 'sn'

def pick_cover(w, editions):
    w_cover = w['covers'][0] if w.get('covers', []) else None
    first_with_cover = None
    for e in editions:
        if 'covers' not in e:
            continue
        if w_cover and e['covers'][0] == w_cover:
            return e['key']
        if not first_with_cover:
            first_with_cover = e['key']
        for l in e.get('languages', []):
            if 'eng' in l:
                return e['key']
    return first_with_cover

def get_work_subjects(w):
    wkey = w['key']
    assert w['type']['key'] == '/type/work'

    subjects = {}
    field_map = {
        'subjects': 'subject',
        'subject_places': 'place',
        'subject_times': 'time',
        'subject_people': 'person',
    }

    for db_field, solr_field in field_map.iteritems():
        if not w.get(db_field, None):
            continue
        cur = subjects.setdefault(solr_field, {})
        for v in w[db_field]:
            try:
                if isinstance(v, dict):
                    if 'value' not in v:
                        continue
                    v = v['value']
                cur[v] = cur.get(v, 0) + 1
            except:
                logger.error("Failed to process subject: %r", v)
                raise

    return subjects

def four_types(i):
    want = set(['subject', 'time', 'place', 'person'])
    ret = dict((k, i[k]) for k in want if k in i)
    for j in (j for j in i.keys() if j not in want):
        for k, v in i[j].items():
            if 'subject' in ret:
                ret['subject'][k] = ret['subject'].get(k, 0) + v
            else:
                ret['subject'] = {k: v}
    return ret

def datetimestr_to_int(datestr):
    if isinstance(datestr, dict):
        datestr = datestr['value']

    if datestr:
        try:
            t = h.parse_datetime(datestr)
        except (TypeError, ValueError):
            t = datetime.datetime.utcnow()
    else:
        t = datetime.datetime.utcnow()

    return int(time.mktime(t.timetuple()))

class SolrProcessor:
    """Processes data to into a form suitable for adding to works solr.
    """
    def __init__(self, obj_cache=None, resolve_redirects=False):
        if obj_cache is None:
            obj_cache = {}
        self.obj_cache = obj_cache
        self.resolve_redirects = resolve_redirects
        
    def process(data):
        """Builds solr document from data. 

        The data is expected to have all the required information to build the doc.
        If some information is not found, it is considered to be missing. The
        expected format is:

            {
                "work": {...},
                "editions": [{...}, {...}],
            }
        
        This functions returns a dictionary containing the following fields:

            title
            subtitle
            has_fulltext
            alternative_title
            alternative_subtitle
            edition_count
            edition_key
            cover_edition_key
            covers_i
            by_statement
            

        """
        # Not yet implemented
        pass

    def process_edition(self, e):
        """Processes an edition and returns a new dictionary with fields required for solr indexing.
        """
        d = {}
        pub_year = self.get_pub_year(e)
        if pub_year:
            d['pub_year'] = pub_year

        # Assuming the ia collections info is added to the edition as "_ia"
        if "_ia" in e:
            ia = e["ia"]
        else:
            # TODO
            pass

        ia = e.get("ocaid") or e.get("ia_loaded_id") or None
        if isinstance(ia, list):
            ia = ia[0]
            

    def get_ia_id(self, edition):
        """Returns ia identifier from an edition dict.
        """
        if "ocaid" in e:
            return e["ocaid"]
        elif e.get("ia_loaded_id"):
            return self.ensure_string(e['ia_loaded_id'][0])

    def ensure_string(self, value):
        if isinstance(value, basestring):
            return value


    def sanitize_edition(self, e):
        """Takes an edition and corrects bad data.

        This will make sure:

        * ia_loaded_id - is a list of strings
        * ia_box_id - is a list of strings
        """
        e["ia_loaded_id"] = self.ensure_list(e.get("ia_loaded_id"), basestring)
        e["ia_box_id"] = self.ensure_list(e.get("ia_box_id"), basestring)


    def ensure_list(self, value, elem_type):
        """Ensures that value is a list of elem_type elements.

            >>> ensure_list([1, "foo", 2], int)
            [1, 2]
            >>> ensure_list("foo", int)
            []
            >>> ensure_list(None, int)
            []
            >>> ensure_list(1, int)
            [1]
        """
        if not value:
            return []
        elif isinstance(value, list):
            return [v for v in value if isinstance(v, elem_type)]
        elif isinstance(value, elem_type):
            # value is supposed to be of type (listof elem_type) but it is of type elem_type by mistake.
            return [value]
        else:
            return []

    def process_editions(self, w, editions, ia_metadata, identifiers):
        for e in editions:
            pub_year = self.get_pub_year(e)
            if pub_year:
                e['pub_year'] = pub_year
            ia = None
            if 'ocaid' in e:
                ia = e['ocaid']
            elif 'ia_loaded_id' in e:
                loaded = e['ia_loaded_id']
                ia = loaded if isinstance(loaded, basestring) else loaded[0]

            # If the _ia_meta field is already set in the edition, use it instead of querying archive.org.
            # This is useful to when doing complete reindexing of solr.
            if ia and '_ia_meta' in e:
                ia_meta_fields = e['_ia_meta']
            elif ia:
                ia_meta_fields = ia_metadata.get(ia)
            else:
                ia_meta_fields = None

            if ia_meta_fields:
                collection = ia_meta_fields['collection']
                if 'ia_box_id' in e and isinstance(e['ia_box_id'], basestring):
                    e['ia_box_id'] = [e['ia_box_id']]
                if ia_meta_fields.get('boxid'):
                    box_id = list(ia_meta_fields['boxid'])[0]
                    e.setdefault('ia_box_id', [])
                    if box_id.lower() not in [x.lower() for x in e['ia_box_id']]:
                        e['ia_box_id'].append(box_id)
                e['ia_collection'] = collection
                e['public_scan'] = ('lendinglibrary' not in collection) and ('printdisabled' not in collection)
            overdrive_id = e.get('identifiers', {}).get('overdrive', None)
            if overdrive_id:
                e['overdrive'] = overdrive_id
            if 'identifiers' in e:
                for k, id_list in e['identifiers'].iteritems():
                    k_orig = k
                    k = k.replace('.', '_').replace(',', '_').replace('(', '').replace(')', '').replace(':', '_').replace('/', '').replace('#', '').lower()
                    m = re_solr_field.match(k)
                    if not m:
                        logger.error('bad identifier key %s %s', k_orig, k)
                    assert m
                    for v in id_list:
                        v = v.strip()
                        if v not in identifiers[k]:
                            identifiers[k].append(v)
        return sorted(editions, key=lambda e: e.get('pub_year', None))

    def get_author(self, a):
        """Returns the author dict from author entry in the work.

            get_author({"author": {"key": "/authors/OL1A"}})
        """
        if 'author' not in a: # OL Web UI bug
            return # http://openlibrary.org/works/OL15365167W.yml?m=edit&v=1

        author = a['author']

        if 'type' in author:
            # means it is already the whole object. 
            # It'll be like this when doing re-indexing of solr.
            return author
        
        key = a['author']['key']
        m = re_author_key.match(key)
        if not m:
            logger.error('invalid author key: %s', key)
            return
        return withKey(key)
    
    def extract_authors(self, w):
        authors = [self.get_author(a) for a in w.get("authors", [])]
        work_authors = [a['key'] for a in authors]
        author_keys = [a['key'].split("/")[-1] for a in authors]

        if any(a['type']['key'] == '/type/redirect' for a in authors):
            if self.resolve_redirects:
                def resolve(a):
                    if a['type']['key'] == '/type/redirect':
                        a = withKey(a['location'])
                    return a
                authors = [resolve(a) for a in authors]
            else:
                raise AuthorRedirect
        assert all(a['type']['key'] == '/type/author' for a in authors)
        return authors

    def get_pub_year(self, e):
        pub_date = e.get('publish_date', None)
        if pub_date:
            m = re_iso_date.match(pub_date)
            if m:
                return m.group(1)
            m = re_year.search(pub_date)
            if m:
                return m.group(1)
                
    def get_subject_counts(self, w, editions, has_fulltext):
        try:
            subjects = four_types(get_work_subjects(w))
        except:
            logger.error('bad work: %s', w['key'])
            raise

        field_map = {
            'subjects': 'subject',
            'subject_places': 'place',
            'subject_times': 'time',
            'subject_people': 'person',
        }

        for db_field, solr_field in field_map.iteritems():
            if not w.get(db_field, None):
                continue
            cur = subjects.setdefault(solr_field, {})
            for v in w[db_field]:
                try:
                    if isinstance(v, dict):
                        if 'value' not in v:
                            continue
                        v = v['value']
                    cur[v] = cur.get(v, 0) + 1
                except:
                    logger.error("bad subject: %r", v)
                    raise

        if any(e.get('ocaid', None) for e in editions):
            subjects.setdefault('subject', {})
            subjects['subject']['Accessible book'] = subjects['subject'].get('Accessible book', 0) + 1
            if not has_fulltext:
                subjects['subject']['Protected DAISY'] = subjects['subject'].get('Protected DAISY', 0) + 1
        return subjects
        
    def build_data(self, w, editions, subjects, has_fulltext):
        d = {}
        def add(name, value):
            if value is not None:
                d[name] = value
                
        def add_list(name, values):
            d[name] = list(values)
        
        # when using common solr core for all types of documents, 
        # use the full key and add type to the doc.
        if is_single_core():
            add('key', w['key'])
            add('type', 'work')
            add('seed', BaseDocBuilder().compute_seeds(w, editions))
        else:    
            add('key', w['key'][7:]) # strip /works/

        add('title', w.get('title'))
        add('subtitle', w.get('subtitle'))
        add('has_fulltext', has_fulltext)

        add_list("alternative_title", self.get_alternate_titles(w, editions))
        add_list('alternative_subtitle', self.get_alternate_subtitles(w, editions))

        add('edition_count', len(editions))


        add_list("edition_key", [re_edition_key.match(e['key']).group(1) for e in editions])
        add_list("by_statement", set(e["by_statement"] for e in editions if "by_statement" in e))
        
        k = 'publish_date'
        pub_dates = set(e[k] for e in editions if e.get(k))
        add_list(k, pub_dates)
        pub_years = set(m.group(1) for m in (re_year.search(date) for date in pub_dates) if m)
        if pub_years:
            add_list('publish_year', pub_years)
            add('first_publish_year', min(int(y) for y in pub_years))
            
        field_map = [
            ('lccn', 'lccn'),
            ('publish_places', 'publish_place'),
            ('oclc_numbers', 'oclc'),
            ('contributions', 'contributor'),
        ]
        for db_key, solr_key in field_map:
            values = set(v for e in editions 
                           if db_key in e
                           for v in e[db_key])
            add_list(solr_key, values)
            
        add_list("isbn", self.get_isbns(editions))
        add("last_modified_i", self.get_last_modified(w, editions))

        self.add_ebook_info(d, editions)

        # Anand - Oct 2013
        # If not public scan then add the work to Protected DAISY subject.
        # This is not the right place to add it, but seems to the quickest way.
        if has_fulltext and not d.get('public_scan_b'):
            subjects['subject']['Protected DAISY'] = 1

        return d
        
        
    def get_alternate_titles(self, w, editions):
        result = set()
        for e in editions:
            result.add(e.get('title'))
            result.update(e.get('work_titles', []))
            result.update(e.get('other_titles', []))
            
        # Remove original title and None.
        # None would've got in if any of the editions has no title.
        result.discard(None)
        result.discard(w.get('title'))
        return result

    def get_alternate_subtitles(self, w, editions):
        subtitle = w.get('subtitle')
        return set(e['subtitle'] for e in editions if e.get('subtitle') and e['subtitle'] != subtitle)
        
    def get_isbns(self, editions):
        isbns = set()

        isbns.update(v.replace("_", "").strip() for e in editions for v in e.get("isbn_10", []))
        isbns.update(v.replace("_", "").strip() for e in editions for v in e.get("isbn_13", []))
        
        # Get the isbn13 when isbn10 is present and vice-versa.
        alt_isbns = [opposite_isbn(v) for v in isbns]
        isbns.update(v for v in alt_isbns if v is not None)
        
        return isbns        

    def get_last_modified(self, work, editions):
        return max(datetimestr_to_int(doc.get('last_modified')) for doc in [work] + editions)
        
    def add_ebook_info(self, doc, editions):
        def add(name, value):
            if value is not None:
                doc[name] = value
                
        def add_list(name, values):
            doc[name] = list(values)

        pub_goog = set() # google
        pub_nongoog = set()
        nonpub_goog = set()
        nonpub_nongoog = set()

        public_scan = False
        all_collection = set()
        all_overdrive = set()
        lending_edition = None
        in_library_edition = None
        printdisabled = set()
        for e in editions:
            if 'overdrive' in e:
                all_overdrive.update(e['overdrive'])
            if 'ocaid' not in e:
                continue
            if not lending_edition and 'lendinglibrary' in e.get('ia_collection', []):
                lending_edition = re_edition_key.match(e['key']).group(1)
            if not in_library_edition and 'inlibrary' in e.get('ia_collection', []):
                in_library_edition = re_edition_key.match(e['key']).group(1)
            if 'printdisabled' in e.get('ia_collection', []):
                printdisabled.add(re_edition_key.match(e['key']).group(1))
            all_collection.update(e.get('ia_collection', []))
            assert isinstance(e['ocaid'], basestring)
            i = e['ocaid'].strip()
            if e.get('public_scan'):
                public_scan = True
                if i.endswith('goog'):
                    pub_goog.add(i)
                else:
                    pub_nongoog.add(i)
            else:
                if i.endswith('goog'):
                    nonpub_goog.add(i)
                else:
                    nonpub_nongoog.add(i)
        ia_list = list(pub_nongoog) + list(pub_goog) + list(nonpub_nongoog) + list(nonpub_goog)
        add("ebook_count_i", len(ia_list))

        has_fulltext = any(e.get('ocaid', None) for e in editions)

        add_list('ia', ia_list)
        if has_fulltext:
            add('public_scan_b', public_scan)
        if all_collection:
            add('ia_collection_s', ';'.join(all_collection))
        if all_overdrive:
            add('overdrive_s', ';'.join(all_overdrive))
        if lending_edition:
            add('lending_edition_s', lending_edition)
        elif in_library_edition:
            add('lending_edition_s', in_library_edition)
        if printdisabled:
            add('printdisabled_s', ';'.join(list(printdisabled)))
        

re_solr_field = re.compile('^[-\w]+$', re.U)

def build_doc(w, obj_cache=None, resolve_redirects=False):
    if obj_cache is None:
        obj_cache = {}
    d = build_data(w, obj_cache=obj_cache, resolve_redirects=resolve_redirects)
    return dict2element(d)
    
def dict2element(d):
    doc = Element("doc")
    for k, v in d.items():
        if isinstance(v, (list, set)):
            add_field_list(doc, k, v)
        else:
            add_field(doc, k, v)
    return doc

def build_data(w, obj_cache=None, resolve_redirects=False):
    wkey = w['key']

    # Anand - Oct 2013
    # For /works/ia:xxx, editions are already suplied. Querying will empty response.
    if "editions" in w:
        editions = w['editions']
    else:
        q = { 'type':'/type/edition', 'works': wkey, '*': None }
        editions = list(query_iter(q))
    authors = SolrProcessor().extract_authors(w)

    iaids = [e["ocaid"] for e in editions if "ocaid" in e]
    ia = dict((iaid, get_ia_collection_and_box_id(iaid)) for iaid in iaids)

    duplicates = {}

    return build_data2(w, editions, authors, ia, duplicates)

def build_data2(w, editions, authors, ia, duplicates):
    obj_cache = {}
    resolve_redirects = False

    wkey = w['key']
    assert w['type']['key'] == '/type/work'
    title = w.get('title', None)
    if not title:
        return
        
    p = SolrProcessor(obj_cache, resolve_redirects)
    get_pub_year = p.get_pub_year

    identifiers = defaultdict(list)
    editions = p.process_editions(w, editions, ia, identifiers)

    has_fulltext = any(e.get('ocaid', None) for e in editions)
    
    subjects = p.get_subject_counts(w, editions, has_fulltext)
            
    def add_field(doc, name, value):
        doc[name] = value

    def add_field_list(doc, name, field_list):
        doc[name] = list(field_list)
    
    doc = p.build_data(w, editions, subjects, has_fulltext)
    
    cover_edition = pick_cover(w, editions)
    if cover_edition:
        add_field(doc, 'cover_edition_key', re_edition_key.match(cover_edition).group(1))
    if w.get('covers'):
        cover = w['covers'][0]
        assert isinstance(cover, int)
        add_field(doc, 'cover_i', cover)

    k = 'first_sentence'
    fs = set( e[k]['value'] if isinstance(e[k], dict) else e[k] for e in editions if e.get(k, None))
    add_field_list(doc, k, fs)

    publishers = set()
    for e in editions:
        publishers.update('Sine nomine' if is_sine_nomine(i) else i for i in e.get('publishers', []))
    add_field_list(doc, 'publisher', publishers)
#    add_field_list(doc, 'publisher_facet', publishers)

    lang = set()
    ia_loaded_id = set()
    ia_box_id = set()

    last_modified_i = datetimestr_to_int(w.get('last_modified'))

    for e in editions:
        for l in e.get('languages', []):
            m = re_lang_key.match(l['key'] if isinstance(l, dict) else l)
            lang.add(m.group(1))
        if e.get('ia_loaded_id'):
            if isinstance(e['ia_loaded_id'], basestring):
                ia_loaded_id.add(e['ia_loaded_id'])
            else:
                try:
                    assert isinstance(e['ia_loaded_id'], list) and isinstance(e['ia_loaded_id'][0], basestring)
                except AssertionError:
                    logger.error("AssertionError: ia=%s, ia_loaded_id=%s", e.get("ia"), e['ia_loaded_id'])
                    raise
                ia_loaded_id.update(e['ia_loaded_id'])
        if e.get('ia_box_id'):
            if isinstance(e['ia_box_id'], basestring):
                ia_box_id.add(e['ia_box_id'])
            else:
                try:
                    assert isinstance(e['ia_box_id'], list) and isinstance(e['ia_box_id'][0], basestring)
                except AssertionError:
                    logger.error("AssertionError: %s", e['key'])
                    raise
                ia_box_id.update(e['ia_box_id'])
    if lang:
        add_field_list(doc, 'language', lang)

        
    #if lending_edition or in_library_edition:
    #    add_field(doc, "borrowed_b", is_borrowed(lending_edition or in_library_edition))

    author_keys = [re_author_key.match(a['key']).group(1) for a in authors]
    author_names = [a.get('name', '') for a in authors]
    add_field_list(doc, 'author_key', author_keys)
    add_field_list(doc, 'author_name', author_names)

    alt_names = set()
    for a in authors:
        if 'alternate_names' in a:
            alt_names.update(a['alternate_names'])

    add_field_list(doc, 'author_alternative_name', alt_names)
    add_field_list(doc, 'author_facet', (' '.join(v) for v in zip(author_keys, author_names)))
    #if subjects:
    #    add_field(doc, 'fiction', subjects['fiction'])

    for k in 'person', 'place', 'subject', 'time':
        if k not in subjects:
            continue
        add_field_list(doc, k, subjects[k].keys())
        add_field_list(doc, k + '_facet', subjects[k].keys())
        subject_keys = [str_to_key(s) for s in subjects[k].keys()]
        add_field_list(doc, k + '_key', subject_keys)

    for k in sorted(identifiers.keys()):
        add_field_list(doc, 'id_' + k, identifiers[k])

    if ia_loaded_id:
        add_field_list(doc, 'ia_loaded_id', ia_loaded_id)

    if ia_box_id:
        add_field_list(doc, 'ia_box_id', ia_box_id)
        
    return doc
    
def solr_update(requests, debug=False, index='works', commitWithin=None):
    # As of now, only works are added to single core solr. 
    # Need to work on supporting other things later
    if is_single_core() and index not in ['works', 'authors', 'editions', 'subjects']:
        return

    h1 = httplib.HTTPConnection(get_solr(index))

    if is_single_core():
        url = 'http://%s/solr/update' % get_solr(index)
    else:
        url = 'http://%s/solr/%s/update' % (get_solr(index), index)

    logger.info("POSTing update to %s", url)
    if commitWithin is not None:
        url = url + "?commitWithin=%d" % commitWithin

    h1.connect()
    for r in requests:
        if debug:
            logger.info('request: %r', r[:65] + '...' if len(r) > 65 else r)
        assert isinstance(r, basestring)
        h1.request('POST', url, r, { 'Content-type': 'text/xml;charset=utf-8'})
        response = h1.getresponse()
        response_body = response.read()
        if response.reason != 'OK':
            logger.error(response.reason)
            logger.error(response_body)
        assert response.reason == 'OK'
        if debug:
            logger.info(response.reason)
    h1.close()

def withKey_cached(key, obj_cache={}):
    if key not in obj_cache:
        obj_cache[key] = withKey(key)
    return obj_cache[key]

def listify(f):
    """Decorator to transform a generator function into a function
    returning list of values.
    """
    def g(*a, **kw):
        return list(f(*a, **kw))
    return g

class BaseDocBuilder:
    re_subject = re.compile("[, _]+")

    @listify
    def compute_seeds(self, work, editions, authors=None):
        """Computes seeds from given work, editions and authors.

        If authors is not supplied, it is infered from the work.
        """

        for e in editions:
            yield e['key']

        if work:
            yield work['key']
            for s in self.get_subject_seeds(work):
                yield s

            if authors is None:
                authors = [a['author'] for a in work.get("authors", []) 
                           if 'author' in a and 'key' in a['author']]

        if authors:
            for a in authors:
                yield a['key']

    def get_subject_seeds(self, work):
        """Yields all subject seeds from the work.
        """
        return (
            self._prepare_subject_keys("/subjects/", work.get("subjects")) +
            self._prepare_subject_keys("/subjects/person:", work.get("subject_people")) +
            self._prepare_subject_keys("/subjects/place:", work.get("subject_places")) +
            self._prepare_subject_keys("/subjects/time:", work.get("subject_times")))

    def _prepare_subject_keys(self, prefix, subject_names):
        subject_names = subject_names or []
        return [self.get_subject_key(prefix, s) for s in subject_names]

    def get_subject_key(self, prefix, subject):
        if isinstance(subject, basestring):
            key = prefix + self.re_subject.sub("_", subject.lower()).strip("_")
            return key

class EditionBuilder(BaseDocBuilder):
    """Helper to edition solr data.
    """
    def __init__(self, edition, work, authors):
        self.edition = edition
        self.work = work
        self.authors = authors

    def build(self):
        return dict(self._build())

    def _build(self):
        yield 'key', self.edition['key']
        yield 'type', 'edition'
        yield 'title', self.edition.get('title') or ''
        yield 'seed', self.compute_seeds(self.work, [self.edition])

        isbns = self.edition.get("isbn_10", []) + self.edition.get("isbn_13", [])
        isbn_set = set()
        for isbn in isbns:
            isbn_set.add(isbn)
            isbn_set.add(isbn.strip().replace("-", ""))
        yield "isbn", list(isbn_set)

        has_fulltext = bool(self.edition.get("ocaid"))
        yield 'has_fulltext', has_fulltext

        if self.authors:
            author_names = [a.get('name', '') for a in self.authors]
            author_keys = [a['key'].split("/")[-1] for a in self.authors]
            yield 'author_name', author_names
            yield 'author_key', author_keys

        last_modified = datetimestr_to_int(self.edition.get('last_modified'))
        yield 'last_modified_i', last_modified

class SolrRequestSet:
    def __init__(self):
        self.deletes = []
        self.docs = []

    def delete(self, key):
        self.deletes.append(key)

    def add(self, doc):
        self.docs.append(doc)

    def get_requests(self):
        return list(self._get_requests())

    def _get_requests(self):
        requests = []
        requests += [make_delete_query(self.deletes)]
        requests += [self._add_request(doc) for doc in self.docs]
        return requests

    def _add_request(self, doc):
        """Constructs add request using doc dict.
        """
        node = dict2element(doc)
        root = Element("add")
        root.append(node)
        return tostring(root).encode('utf-8')

def process_edition_data(edition_data):
    """Returns a solr document corresponding to an edition using given edition data.
    """
    builder = EditionBuilder(edition_data['edition'], edition_data['work'], edition_data['authors'])
    return builder.build()

def process_work_data(work_data):
    """Returns a solr document corresponding to a work using the given work_data.
    """
    # Force single core
    config.runtime_config['single_core_solr'] = True

    return build_data2(
        work_data['work'], 
        work_data['editions'], 
        work_data['authors'], 
        work_data['ia'], 
        work_data['duplicates'])

def update_edition(e):
    if not is_single_core():
        return []

    ekey = e['key']
    logger.info("updating edition %s", ekey)

    wkey = e.get('works') and e['works'][0]['key']
    w = wkey and withKey(wkey)
    authors = []

    if w:
        authors = [withKey(a['author']['key']) for a in w.get("authors", []) if 'author' in a]

    request_set = SolrRequestSet()
    request_set.delete(ekey)

    q = {'type': '/type/redirect', 'location': ekey}
    redirect_keys = [r['key'] for r in query_iter(q)]
    for k in redirect_keys:
        request_set.delete(k)

    doc = EditionBuilder(e, w, authors).build()
    request_set.add(doc)
    return request_set.get_requests()

def get_subject(key):
    # This works only for single-core-solr
    subject_key = key.split("/")[-1]

    if ":" in subject_key:
        subject_type, subject_key = subject_key.split(":", 1)
    else:
        subject_type = "subject"

    search_field = "%s_key" % subject_type
    facet_field = "%s_facet" % subject_type

    # Handle upper case or any special characters that may be present
    subject_key = str_to_key(subject_key)
    key = "/subjects/%s:%s" % (subject_type, subject_key)

    params = {
        'wt': 'json',
        'json.nl': 'arrarr',
        'q': '%s:%s' % (search_field, subject_key),
        'rows': '0',
        'facet': 'true',
        'facet.field': facet_field,
        'facet.mincount': 1,
        'facet.limit': 100
    }
    base_url = 'http://' + get_solr('works') + '/solr/select'
    url = base_url + '?' + urllib.urlencode(params)
    result = json.load(urlopen(url))

    work_count = result['response']['numFound']
    facets = result['facet_counts']['facet_fields'].get(facet_field, []);

    names = [name for name, count in facets if str_to_key(name) == subject_key]

    if names:
        name = names[0]
    else:
        name = key.replace("_", " ")

    return {
        "key": key,
        "type": "subject",
        "subject_type": subject_type,
        "name": name,
        "work_count": work_count,
    }

def update_subject(key):
    # updating subject is available only for single-core-solr
    if not is_single_core():
        return

    subject = get_subject(key)
    request_set = SolrRequestSet()
    request_set.delete(subject['key'])

    if subject['work_count'] > 0:
        request_set.add(subject)
    return request_set.get_requests()    

def update_work(w, obj_cache=None, debug=False, resolve_redirects=False):
    if obj_cache is None:
        obj_cache = {}

    wkey = w['key']
    #assert wkey.startswith('/works')
    #assert '/' not in wkey[7:]
    deletes = []
    requests = []

    q = {'type': '/type/redirect', 'location': wkey}
    redirect_keys = [r['key'][7:] for r in query_iter(q)]

    deletes += redirect_keys
    deletes += [wkey[7:]] # strip /works/ from /works/OL1234W

    # handle edition records as well
    # When an edition is not belonged to a work, create a fake work and index it.
    if w['type']['key'] == '/type/edition' and w.get('title'):
        edition = w
        w = {
            # Use key as /works/OL1M. 
            # In case of single-core-solr, we are using full path as key. So it is required
            # to be unique across all types of documents.
            # The website takes care of redirecting /works/OL1M to /books/OL1M.
            'key': edition['key'].replace("/books/", "/works/"),
            'type': {'key': '/type/work'},
            'title': edition['title'],
            'editions': [edition],
            'authors': [{'type': '/type/author_role', 'author': {'key': a['key']}} for a in edition.get('authors', [])]
        }
        # Hack to add subjects when indexing /books/ia:xxx
        if edition.get("subjects"):
            w['subjects'] = edition['subjects']

    if w['type']['key'] == '/type/work' and w.get('title'):
        try:
            d = build_data(w, obj_cache=obj_cache, resolve_redirects=resolve_redirects)
            doc = dict2element(d)
        except:
            logger.error("failed to update work %s", w['key'], exc_info=True)
        else:
            if d is not None:
                # Delete all ia:foobar keys
                # XXX-Anand: The works in in_library subject were getting wiped off for unknown reasons.
                # I suspect that this might be a cause. Disabling temporarily.
                #if d.get('ia'):
                #    deletes += ["ia:" + iaid for iaid in d['ia']]

                # In single core solr, we use full path as key, not just the last part
                if is_single_core():
                    deletes = ["/works/" + k for k in deletes]

                requests.append(make_delete_query(deletes))

                add = Element("add")
                add.append(doc)
                add_xml = tostring(add).encode('utf-8')
                requests.append(add_xml)
    elif w['type']['key'] == '/type/delete':
        # In single core solr, we use full path as key, not just the last part
        if is_single_core():
            deletes = ["/works/" + k for k in deletes]
        requests.append(make_delete_query(deletes))

    return requests

def make_delete_query(keys):
    # Escape ":" in the keys.
    # ":" is a special charater and keys like "ia:foo00bar" will
    # fail if ":" is not escaped
    keys = [key.replace(":", r"\:") for key in keys]
    queries = ['<query>key:%s</query>' % key for key in keys]
    return '<delete>%s</delete>' % ''.join(queries)

def update_author(akey, a=None, handle_redirects=True):
    # http://ia331507.us.archive.org:8984/solr/works/select?indent=on&q=author_key:OL22098A&facet=true&rows=1&sort=edition_count%20desc&fl=title&facet.field=subject_facet&facet.mincount=1
    if akey == '/authors/':
        return
    m = re_author_key.match(akey)
    if not m:
        logger.error('bad key: %s', akey)
    assert m
    author_id = m.group(1)
    if not a:
        a = withKey(akey)
    if a['type']['key'] in ('/type/redirect', '/type/delete') or not a.get('name', None):
        return ['<delete><query>key:%s</query></delete>' % author_id] 
    try:
        assert a['type']['key'] == '/type/author'
    except AssertionError:
        logger.error("AssertionError: %s", a['type']['key'])
        raise

    facet_fields = ['subject', 'time', 'person', 'place']

    if is_single_core():
        base_url = 'http://' + get_solr('works') + '/solr/select'
    else:
        base_url = 'http://' + get_solr('works') + '/solr/works/select'

    url = base_url + '?wt=json&json.nl=arrarr&q=author_key:%s&sort=edition_count+desc&rows=1&fl=title,subtitle&facet=true&facet.mincount=1' % author_id
    url += ''.join('&facet.field=%s_facet' % f for f in facet_fields)

    logger.info("urlopen %s", url)

    reply = json.load(urlopen(url))
    work_count = reply['response']['numFound']
    docs = reply['response'].get('docs', [])
    top_work = None
    if docs:
        top_work = docs[0]['title']
        if docs[0].get('subtitle', None):
            top_work += ': ' + docs[0]['subtitle']
    all_subjects = []
    for f in facet_fields:
        for s, num in reply['facet_counts']['facet_fields'][f + '_facet']:
            all_subjects.append((num, s))
    all_subjects.sort(reverse=True)
    top_subjects = [s for num, s in all_subjects[:10]]

    add = Element("add")
    doc = SubElement(add, "doc")
    
    if is_single_core():
        add_field(doc, 'key', "/authors/" + author_id)
        add_field(doc, 'type', "author")
    else:
        add_field(doc, 'key', author_id)

    if a.get('name', None):
        add_field(doc, 'name', a['name'])
    for f in 'birth_date', 'death_date', 'date':
        if a.get(f, None):
            add_field(doc, f, a[f])
    if top_work:
        add_field(doc, 'top_work', top_work)
    add_field(doc, 'work_count', work_count)
    add_field_list(doc, 'top_subjects', top_subjects)

    requests = []
    if handle_redirects:
        q = {'type': '/type/redirect', 'location': akey}
        try:
            redirects = ''.join('<id>%s</id>' % re_author_key.match(r['key']).group(1) for r in query_iter(q))
        except AttributeError:
            logger.error('AssertionError: redirects: %r', [r['key'] for r in query_iter(q)])
            raise
        if redirects:
            requests.append('<delete>' + redirects + '</delete>')

    requests.append(tostring(add).encode('utf-8'))
    return requests

def commit_and_optimize(debug=False):
    requests = ['<commit />', '<optimize />']
    solr_update(requests, debug)

def get_document(key):
    url = get_ol_base_url() + key + ".json"
    for i in range(10):
        try:
            logger.info("urlopen %s", url)
            contents = urlopen(url).read()
            return json.loads(contents)
        except urllib2.HTTPError, e:
            contents = e.read()
            # genueue 404, not a server error
            if e.getcode() == 404 and '"error": "notfound"' in contents:
                return {"key": key, "type": {"key": "/type/delete"}}

        print >> sys.stderr, "Failed to get document from %s" % url
        print >> sys.stderr, "retry", i



re_edition_key_basename = re.compile("^[a-zA-Z0-9:.-]+$")

def solr_select_work(edition_key):
    """Returns work for given edition key in solr.
    """
    # solr only uses the last part as edition_key
    edition_key = edition_key.split("/")[-1]

    if not re_edition_key_basename.match(edition_key):
        return None

    edition_key = edition_key.replace(":", r"\:")

    url = 'http://' + get_solr('works') + '/solr/works/select?wt=json&q=edition_key:%s&rows=1&fl=key' % edition_key
    reply = json.load(urlopen(url))
    docs = reply['response'].get('docs', [])
    if docs:
        return '/works/' + docs[0]['key'] # Need to add /works/ to make the actual key

def update_keys(keys, commit=True):
    logger.info("BEGIN update_keys")
    wkeys = set()

    # Get works for all the editions
    ekeys = set(k for k in keys if k.startswith("/books/"))
    for k in ekeys:
        logger.info("processing edition %s", k)
        edition = get_document(k)

        if edition and edition['type']['key'] == '/type/redirect':
            logger.warn("Found redirect to %s", edition['location'])
            edition = withKey(edition['location'])

        if not edition:
            logger.warn("No edition found for key %r. Ignoring...", k)
            continue
        elif edition['type']['key'] != '/type/edition':
            logger.info("%r is a document of type %r. Checking if any work has it as edition in solr...", k, edition['type']['key'])
            wkey = solr_select_work(k)
            if wkey:
                logger.info("found %r, updating it...", wkey)
                wkeys.add(wkey)

            if edition['type']['key'] == '/type/delete':
                logger.info("Found a document of type %r. queuing for deleting it solr..", edition['type']['key'])
                # Also remove if there is any work with that key in solr.
                wkeys.add(k)
            else:
                logger.warn("Found a document of type %r. Ignoring...", edition['type']['key'])
        else:
            if edition.get("works"):
                wkeys.add(edition["works"][0]['key'])
            else:
                # index the edition as it does not belong to any work
                wkeys.add(k)
        
    # Add work keys
    wkeys.update(k for k in keys if k.startswith("/works/"))
    
    # update works
    requests = []
    for k in wkeys:
        logger.info("updating %s", k)
        try:
            w = get_document(k)
            requests += update_work(w, debug=True)
        except:
            logger.error("Failed to update work %s", k, exc_info=True)

    if requests:    
        if commit:
            requests += ['<commit />']
        solr_update(requests, debug=True)

    # update editions
    requests = []
    for k in ekeys:
        try:
            e = withKey(k)
            requests += update_edition(e)
        except:
            logger.error("Failed to update edition %s", k, exc_info=True)
    if requests:
        if commit:
            requests += ['<commit/>']
        solr_update(requests, index="editions", debug=True)
    
    # update authors
    requests = []
    akeys = set(k for k in keys if k.startswith("/authors/"))
    for k in akeys:
        logger.info("updating %s", k)
        try:
            requests += update_author(k)
        except:
            logger.error("Failed to update author %s", k, exc_info=True)

    if requests:  
        if commit:
            requests += ['<commit />']
        solr_update(requests, index="authors", debug=True, commitWithin=1000)

    # update subjects
    skeys = set(k for k in keys if k.startswith("/subjects/"))
    requests = []
    for k in skeys:
        logger.info("updating %s", k)
        try:
            requests += update_subject(k)
        except:
            logger.error("Failed to update subject %s", k, exc_info=True)
    if requests:  
        if commit:
            requests += ['<commit />']
        solr_update(requests, index="subjects", debug=True)

    logger.info("END update_keys")

def parse_options(args=None):
    from optparse import OptionParser
    parser = OptionParser(args)
    parser.add_option("-s", "--server", dest="server", default="http://openlibrary.org/", help="URL of the openlibrary website (default: %default)")
    parser.add_option("-c", "--config", dest="config", default="openlibrary.yml", help="Open Library config file")
    parser.add_option("--nocommit", dest="nocommit", action="store_true", default=False, help="Don't commit to solr")
    parser.add_option("--monkeypatch", dest="monkeypatch", action="store_true", default=False, help="Monkeypatch query functions to access DB directly")

    options, args = parser.parse_args()
    return options, args

def new_query_iter(q, limit=500, offset=0):
    """Alternative implementation of query_iter, that talks to the
    database directly instead of accessing the website API.

    This is set to `query_iter` when this script is called with
    --monkeypatch option.
    """
    q['limit'] = limit
    q['offset'] = offset
    site = web.ctx.site

    while True:
        keys = site.things(q)
        logger.info("query_iter %s", q)
        docs = keys and site.get_many(keys, raw=True) 
        for doc in docs:
            yield doc

        # We haven't got as many we have requested. No point making one more request
        if len(keys) < limit:
            break
        q['offset'] += limit

def new_withKey(key):
    """Alternative implementation of withKey, that talks to the database
    directly instead of using the website API.

    This is set to `withKey` when this script is called with --monkeypatch
    option.
    """
    logger.info("withKey %s", key)
    return web.ctx.site._request('/get', data={'key': key})

def new_get_document(key):
    try:
        return new_withKey(key)
    except ClientException, e:
        if e.status.startswith('404'):
            logger.warn("%s is not found, considering it as deleted.", key)
            return {"key": key, "type": {"key": "/type/delete"}}
        else:
            raise

def monkeypatch(config_file):
    """Monkeypatch query functions to avoid hitting openlibrary.org.
    """
    def load_infogami(config_file):
        import web
        import infogami
        from infogami import config
        from infogami.utils import delegate

        config.plugin_path += ['openlibrary.plugins']
        config.site = "openlibrary.org"
        
        infogami.load_config(config_file)
        setup_infobase_config(config_file)

        infogami._setup()
        delegate.fakeload()
        
    def setup_infobase_config(config_file):
        """Reads the infoabse config file and assign it to config.infobase.
        The config_file is used as base to resolve relative path, if specified in the config.
        """
        from infogami import config
        import os
        import yaml

        if config.get("infobase_config_file"):
            dir = os.path.dirname(config_file)
            path = os.path.join(dir, config.infobase_config_file)
            config.infobase = yaml.safe_load(open(path).read())

    global query_iter, withKey, get_document

    load_infogami(config_file)

    query_iter = new_query_iter
    withKey = new_withKey
    get_document = new_get_document

def main():
    options, keys = parse_options()

    # set query host
    host = web.lstrips(options.server, "http://").strip("/")
    set_query_host(host)

    if options.monkeypatch:
        monkeypatch(options.config)

    # load config
    config.load(options.config)

    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    update_keys(keys, commit=not options.nocommit)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = work_post
import sys, codecs, re, web, httplib
sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
from time import time
from lxml.etree import tostring, Element
from openlibrary.solr.work_subject import find_subjects, four_types

db = web.database(dbn='mysql', db='openlibrary', user='root', passwd='') # , charset='utf8')
db.printing = False

re_year = re.compile(r'(\d{4})$')
re_author_key = re.compile('^/a/OL(\d+)A$')
re_work_key = re.compile('^/works/OL(\d+)W$')

covers = {}
for line in open('../work_covers/list'):
    i, j = line[:-1].split(' ')
    covers[int(i)] = int(j)

connect = False
connect = True

long_subjects = set([11369047, 11388034, 11404100, 11408860, 12548230, 7623678])

solr_host = 'localhost:8983'
update_url = 'http://' + solr_host + '/solr/works/update'

def work_subjects(wkey):
    ret = db.query('select subjects as s from work_subject where work=$wkey', vars=locals())
    if not ret:
        return
    return eval(ret[0].s)

to_drop = set(''';/?:@&=+$,<>#%"{}|\\^[]`\n\r''')

def str_to_key(s):
    return ''.join(c if c != ' ' else '_' for c in s.lower() if c not in to_drop)

def solr_post(h1, body):
    if not connect:
        return 'not connected'
    h1.request('POST', update_url, body, { 'Content-type': 'text/xml;charset=utf-8'})
    response = h1.getresponse()
    response.read()
    return response.reason

h1 = None
if connect:
    h1 = httplib.HTTPConnection(solr_host)
    h1.connect()
    print solr_post(h1, '<delete><query>*:*</query></delete>')
    print solr_post(h1, '<commit/>')
    print solr_post(h1, '<optimize/>')

def add_field(doc, name, value):
    field = Element("field", name=name)
    field.text = unicode(strip_bad_char(value))
    doc.append(field)

def add_field_list(doc, name, field_list):
    for value in field_list:
        add_field(doc, name, value)

re_bad_char = re.compile('[\x01\x19-\x1e]')
def strip_bad_char(s):
    if not isinstance(s, basestring):
        return s
    return re_bad_char.sub('', s)

re_not_az = re.compile('[^a-zA-Z]')
def is_sine_nomine(pub):
    return re_not_az.sub('', pub).lower() == 'sn'

def build_doc(w):
    wkey = w['key']

    m = re_work_key.match(wkey)
    wkey_num = int(m.group(1))
    if wkey_num in long_subjects:
        return

    def get_pub_year(e):
        pub_date = e.get('publish_date', None)
        if pub_date:
            m = re_year.search(pub_date)
            if m:
                return m.group(1)
    editions = []
    for e in w['editions']:
        pub_year = get_pub_year(e)
        if pub_year:
            e['pub_year'] = pub_year
        editions.append(e)

    editions.sort(key=lambda e: e.get('pub_year', None))

    doc = Element("doc")
    add_field(doc, 'key', 'OL%dW' % wkey_num)
    add_field(doc, 'title', w['title'])
    #add_field(doc, 'title_suggest', w['title'])

    has_fulltext = any(e.get('ia', None) for e in editions)
    add_field(doc, 'has_fulltext', has_fulltext)
    if w.get('subtitle', None):
        add_field(doc, 'subtitle', w['subtitle'])

    alt_titles = set()
    for e in editions:
        if e.get('title', None):
            t = e['title']
            if t != w['title']:
                alt_titles.add(t)
        for f in 'work_titles', 'other_titles':
            if f not in e:
                continue
            assert isinstance(e[f], list)
            for t in e[f]:
                if t != w['title']:
                    alt_titles.add(t)

    add_field_list(doc, 'alternative_title', alt_titles)

    alt_subtitles = set( e['subtitle'] for e in editions if e.get('subtitle', None) and e['subtitle'] != w.get('subtitle', None))
    add_field(doc, 'alternative_subtitle', alt_subtitles)

    add_field(doc, 'edition_count', len(editions))
    for e in editions:
        add_field(doc, 'edition_key', 'OL%dM' % e['ekey'])
    if wkey_num in covers:
        add_field(doc, 'cover_edition_key', 'OL%dM' % covers[wkey_num])

    k = 'by_statement'
    add_field_list(doc, k, set( e[k] for e in editions if e.get(k, None)))

    k = 'publish_date'
    pub_dates = set(e[k] for e in editions if e.get(k, None))
    add_field_list(doc, k, pub_dates)
    pub_years = set(e['pub_year'] for e in editions if 'pub_year' in e)
    if pub_years:
        add_field_list(doc, 'publish_year', pub_years)
        add_field(doc, 'first_publish_year', min(int(i) for i in pub_years))

    k = 'first_sentence'
    fs = set( e[k] for e in editions if e.get(k, None))
    add_field_list(doc, k, fs)

    field_map = [
        ('lccn', 'lccn'),
        ('publishers', 'publisher'),
        ('publish_places', 'publish_place'),
        ('oclc_numbers', 'oclc'),
        ('contributions', 'contributor'),
    ]

    for db_key, search_key in field_map:
        v = set()
        for e in editions:
            if db_key not in e:
                continue
            if db_key == 'publishers':
                e[db_key] = ['Sine nomine' if is_sine_nomine(i) else i for i in e[db_key].split('\t')]
            assert isinstance(e[db_key], list)
            v.update(e[db_key])
        add_field_list(doc, search_key, v)
#        if db_key == 'publishers':
#            add_field_list(doc, search_key + '_facet', v)

    isbn = set()
    for e in editions:
        for f in 'isbn_10', 'isbn_13':
            if f not in e:
                continue
            assert isinstance(e[f], list)
            for v in e[f]:
                isbn.add(v.replace('-', ''))
    add_field_list(doc, 'isbn', isbn)

    lang = set()
    for e in editions:
        if 'languages' not in e:
            continue
        assert isinstance(e['languages'], list)
        for l in e['languages']:
            for l2 in l.split('\t'):
                if len(l2) != 3:
                    print e['languages']
                
                assert len(l2) == 3
                lang.add(l2)
    if lang:
        add_field_list(doc, 'language', lang)

    goog = set() # google
    non_goog = set()
    for e in editions:
        if 'ia' in e:
            assert isinstance(e['ia'], list)
            for i in e['ia']:
                i = i.strip()
                if i.endswith('goog'):
                    goog.add(i)
                else:
                    non_goog.add(i)
    add_field_list(doc, 'ia', list(non_goog) + list(goog))

    authors = w['authors']
    author_keys = ['OL%dA' % a['akey'] for a in authors]
    author_names = [a.get('name', '') or '' for a in authors]

    add_field_list(doc, 'author_key', author_keys)
    add_field_list(doc, 'author_name', author_names)

    alt_names = set()
    for a in authors:
        if 'alt_names' not in a:
            continue
        assert isinstance(a['alt_names'], list)
        alt_names.update(a['alt_names'])

    add_field_list(doc, 'author_alternative_name', alt_names)
    add_field_list(doc, 'author_facet', (' '.join(v) for v in zip(author_keys, author_names)))

#    if 'subjects' in w:
#        if isinstance(w['subjects'][0], list):
#            try:
#                subjects = find_subjects(w['subjects'])
#            except ValueError:
#                print w['subjects']
#                raise
#        else:
#            subjects = work_subjects(wkey_num)
#            if not subjects:
#                subjects = {}
#
    if 'marc_subjects' in w:
        try:
            marc_subjects = eval(w['marc_subjects'])
        except:
            print 'error parsing marc subjects (%d)' % len(w['marc_subjects'])
            marc_subjects = []
        try:
            subjects = find_subjects(marc_subjects)
        except ValueError:
            print w['marc_subjects']
            raise

        subjects = four_types(subjects)

        for k in 'person', 'place', 'subject', 'time':
            if k not in subjects:
                continue
            add_field_list(doc, k, subjects[k].keys())
            #add_field_list(doc, k + '_facet', subjects[k].keys())
            subject_keys = [str_to_key(s) for s in subjects[k].keys()]
            add_field_list(doc, k + '_key', subject_keys)

    return doc

def all_works():
    for line in open('work_full3'):
        yield line
    for line in open('work_full4'):
        yield line

def index():
    t0 = time()
    t_prev = time()
    total = 13941626
    num = 0
    add = Element("add")
    chunk = 500
    chunk_count = 0
    print 'reading works'
    # 9260000
    # OL9177682W, OL9177683W, OL9177684W, OL9177685W, OL9177686W, OL9177687W, OL9177688W, OL9177689W
    skip = 'OL9177682W' #, OL9177683W, OL9177684W, OL9177685W, OL9177686W, OL9177687W, OL9177688W, OL9177689W
    skip = None
    for line in open('work_full5'):
        num += 1
#        if num < 13600000:
#            if num % 100000 == 0:
#                print num, 'skipping'
#            continue
        if skip and skip not in line:
            continue
        w = eval(line)
        if skip:
            if w['key'] == '/works/' + skip:
                print 'finished skipping'
                skip = None
            else:
                continue
#        for e in w['editions']:
#            pub_date_missing = True
#            if 'publish_date' in e:
#                if all(len(l) == 3 for l in e['publish_date'].split('\t')):
#                    e['languages'] = e['publish_date'].split('\t')
#                    del e['publish_date']
#                else:
#                    pub_date_missing = False
#            if pub_date_missing:
#                pub_dates = list(db.select('pub_dates', what="v", where="k='/b/OL%dM'" % e['ekey']))
#                if pub_dates:
#                    e['publish_date'] = pub_dates[0].v.decode('utf-8')

        doc = build_doc(w)
        if doc is not None:
            add.append(doc)

        if len(add) == chunk:
            chunk_count += 1
            add_xml = tostring(add, pretty_print=True).encode('utf-8')
            #add_xml = tostring(add, pretty_print=True)
            del add # memory
            secs = float(time() - t_prev)
            rec_per_sec = float(chunk) / secs
            rec_left = total - num
            sec_left = rec_left / rec_per_sec
            print "%d/%d %.4f%% %.2f rec/sec %.2f hours left" % (num,total,(float(num)*100.0/total), rec_per_sec, float(sec_left) / 3600.0), solr_post(h1, add_xml)
#            if chunk_count % 10000 == 0:
#                print 'commit:', solr_post(h1, '<commit/>')
            add = Element("add")
            t_prev = time()

    if len(add):
        add_xml = tostring(add, pretty_print=True).encode('utf-8')
        del add # memory
        print solr_post(h1, add_xml),
        print 'commit:', solr_post(h1, '<commit/>')
    print 'optimize'
    print solr_post(h1, '<optimize/>')
    print 'end'

index()

########NEW FILE########
__FILENAME__ = work_subject
import re, urllib2
from openlibrary.catalog.marc.fast_parse import get_tag_lines, get_all_subfields, get_subfield_values, get_subfields, BadDictionary
from openlibrary.catalog.utils import remove_trailing_dot, remove_trailing_number_dot, flip_name
from openlibrary.catalog.importer.db_read import get_mc
from collections import defaultdict

subject_fields = set(['600', '610', '611', '630', '648', '650', '651', '662'])

re_large_book = re.compile('large.*book', re.I)

re_edition_key = re.compile(r'^/(?:b|books)/(OL\d+M)$')

re_ia_marc = re.compile('^(?:.*/)?([^/]+)_(marc\.xml|meta\.mrc)(:0:\d+)?$')
def get_marc_source(w):
    found = set()
    for e in w['editions']:
        sr = e.get('source_records', [])
        if sr:
            found.update(i[5:] for i in sr if i.startswith('marc:'))
        else:
            m = re_edition_key.match(e['key'])
            if not m:
                print e['key']
            mc = get_mc('/b/' + m.group(1))
            if mc and not mc.startswith('amazon:') and not re_ia_marc.match(mc):
                found.add(mc)
    return found

def get_marc_subjects(w):
    for src in get_marc_source(w):
        data = None
        from openlibrary.catalog.get_ia import get_data
        try:
            data = get_data(src)
        except ValueError:
            print 'bad record source:', src
            print 'http://openlibrary.org' + w['key']
            continue
        except urllib2.HTTPError, error:
            print 'HTTP error:', error.code, error.msg
            print 'http://openlibrary.org' + w['key']
        if not data:
            continue
        try:
            lines = list(get_tag_lines(data, subject_fields))
        except BadDictionary:
            print 'bad dictionary:', src
            print 'http://openlibrary.org' + w['key']
            continue
        if lines:
            yield lines

re_place_comma = re.compile('^(.+), (.+)$')
re_paren = re.compile('[()]')
def flip_place(s):
    s = remove_trailing_dot(s)
    # Whitechapel (London, England)
    # East End (London, England)
    # Whitechapel (Londres, Inglaterra)
    if re_paren.search(s):
        return s
    m = re_place_comma.match(s)
    return m.group(2) + ' ' + m.group(1) if m else s

# 'Rhodes, Dan (Fictitious character)'
re_fictitious_character = re.compile('^(.+), (.+)( \(.* character\))$')
re_etc = re.compile('^(.+?)[, .]+etc[, .]?$', re.I)
re_aspects = re.compile(' [Aa]spects$')
re_comma = re.compile('^([A-Z])([A-Za-z ]+?) *, ([A-Z][A-Z a-z]+)$')

def tidy_subject(s):
    s = s.strip()
    if len(s) < 2:
        print 'short subject:', `s`
    else:
        s = s[0].upper() + s[1:]
    m = re_etc.search(s)
    if m:
        return m.group(1)
    s = remove_trailing_dot(s)
    m = re_fictitious_character.match(s)
    if m:
        return m.group(2) + ' ' + m.group(1) + m.group(3)
    m = re_comma.match(s)
    if m:
        return m.group(3) + ' ' + m.group(1) + m.group(2)
    return s

def flip_subject(s):
    m = re_comma.match(s)
    if m:
        return m.group(3) + ' ' + m.group(1).lower()+m.group(2)
    else:
        return s

def find_aspects(line):
    cur = [(i, j) for i, j in get_subfields(line, 'ax')]
    if len(cur) < 2 or cur[0][0] != 'a' or cur[1][0] != 'x':
        return
    a, x = cur[0][1], cur[1][1]
    x = x.strip('. ')
    a = a.strip('. ')
    if not re_aspects.search(x):
        return
    if a == 'Body, Human':
        a = 'the Human body'
    return x + ' of ' + flip_subject(a)

# 'Ram Singh, guru of Kuka Sikhs'
re_flip_name = re.compile('^(.+), ([A-Z].+)$')

def find_subjects(marc_subjects):
    person = defaultdict(int)
    event = defaultdict(int)
    work = defaultdict(int)
    org = defaultdict(int)
    time = defaultdict(int)
    place = defaultdict(int)
    subject = defaultdict(int)
    #fiction = False
    for lines in marc_subjects:
        for tag, line in lines:
            aspects = find_aspects(line)
            if aspects:
                subject[aspects] += 1
            if re_large_book.search(line):
                continue
            if tag == '600': # people
                name_and_date = []
                for k, v in get_subfields(line, ['a', 'b', 'c', 'd']):
                    v = '(' + v.strip('.() ') + ')' if k == 'd' else v.strip(' /,;:')
                    if k == 'a':
                        if v == 'Mao, Zedong':
                            v = 'Mao Zedong'
                        else:
                            m = re_flip_name.match(v)
                            if m:
                                v = flip_name(v)
                    name_and_date.append(v)
                name = remove_trailing_dot(' '.join(name_and_date)).strip()
                if name != '':
                    person[name] += 1
            elif tag == '610': # org
                v = ' '.join(get_subfield_values(line, 'abcd'))
                v = v.strip()
                if v:
                    v = remove_trailing_dot(v).strip()
                if v:
                    v = tidy_subject(v)
                if v:
                    org[v] += 1

                for v in get_subfield_values(line, 'a'):
                    v = v.strip()
                    if v:
                        v = remove_trailing_dot(v).strip()
                    if v:
                        v = tidy_subject(v)
                    if v:
                        org[v] += 1
            elif tag == '611': # event
                v = ' '.join(j.strip() for i, j in get_all_subfields(line) if i not in 'vxyz')
                if v:
                    v = v.strip()
                v = tidy_subject(v)
                if v:
                    event[v] += 1
            elif tag == '630': # work
                for v in get_subfield_values(line, ['a']):
                    v = v.strip()
                    if v:
                        v = remove_trailing_dot(v).strip()
                    if v:
                        v = tidy_subject(v)
                    if v:
                        work[v] += 1
            elif tag == '650': # topical
                for v in get_subfield_values(line, ['a']):
                    if v:
                        v = v.strip()
                    v = tidy_subject(v)
                    if v:
                        subject[v] += 1
            elif tag == '651': # geo
                for v in get_subfield_values(line, ['a']):
                    if v:
                        place[flip_place(v).strip()] += 1
            else:
                print 'other', tag, list(get_all_subfields(line))

            cur = [v for k, v in get_all_subfields(line) if k=='a' or v.strip('. ').lower() == 'fiction']

            # skip: 'Good, Sally (Fictitious character) in fiction'
            if len(cur) > 1 and cur[-1].strip('. ').lower() == 'fiction' and ')' not in cur[-2]:
                subject[flip_subject(cur[-2]) + ' in fiction'] += 1

            for v in get_subfield_values(line, ['y']):
                v = v.strip()
                if v:
                    time[remove_trailing_dot(v).strip()] += 1
            for v in get_subfield_values(line, ['v']):
                v = v.strip()
                if v:
                    v = remove_trailing_dot(v).strip()
                v = tidy_subject(v)
                if v:
                    subject[v] += 1
            for v in get_subfield_values(line, ['z']):
                v = v.strip()
                if v:
                    place[flip_place(v).strip()] += 1
            for v in get_subfield_values(line, ['x']):
                v = v.strip()
                if not v:
                    continue
                if aspects and re_aspects.search(v):
                    continue
                v = tidy_subject(v)
                if v:
                    subject[v] += 1

            v_and_x = get_subfield_values(line, ['v', 'x'])
            #if 'Fiction' in v_and_x or 'Fiction.' in v_and_x:
            #    fiction = True
    #if 'Fiction' in subject:
    #    del subject['Fiction']
    ret = {}
    if person:
        ret['person'] = dict(person)
    if time:
        ret['time'] = dict(time)
    if place:
        ret['place'] = dict(place)
    if subject:
        ret['subject'] = dict(subject)
    if event:
        ret['event'] = dict(event)
    if org:
        ret['org'] = dict(org)
    if work:
        ret['work'] = dict(work)
    return ret

def four_types(i):
    want = set(['subject', 'time', 'place', 'person'])
    ret = dict((k, i[k]) for k in want if k in i)
    for j in (j for j in i.keys() if j not in want):
        for k, v in i[j].items():
            if 'subject' in ret:
                ret['subject'][k] = ret['subject'].get(k, 0) + v
            else:
                ret['subject'] = {k: v}
    return ret


########NEW FILE########
__FILENAME__ = tasks
import logging
import eventer
import urllib2
from openlibrary.core.task import oltask, set_task_data
from openlibrary.core.fetchmail import fetchmail
from openlibrary.core import formats
from openlibrary.core.lists.updater import Updater as ListUpdater

logger = logging.getLogger("openlibrary.tasks")

try:
    import celeryconfig
except ImportError:
    logger.error("failed to import celeryconfig")
    celeryconfig = None

@oltask
def add(x, y):
    return x + y

# @oltask
# def update_support_from_email():
#     configfile = celeryconfig.OL_CONFIG
#     ol_config = formats.load_yaml(open(configfile).read())
#     fetchmail(ol_config)

@oltask
def trigger_offline_event(event, *a, **kw):
    logger.info("trigger_offline_event %s %s %s", event, a, kw)
    
    # Importing events to add an offline event to eventer. 
    # TODO: load the plugins from OL configuration
    from openlibrary.plugins.openlibrary import events
    
    eventer.trigger(event, *a, **kw)

@oltask
def on_edit(changeset):
    """This gets triggered whenever an edit happens on Open Library.
    """
    update_lists.delay(changeset)
    for t in other_on_edit_tasks:
        t.delay(changeset)

# Hook to add other on-edit tasks.
# Used by dev_instance.py
other_on_edit_tasks = []
    
@oltask
def update_lists(changeset):
    """Updates the lists database on edit.
    """
    keys = [x['key'] for x in changeset['docs']]
    set_task_data(keys = keys, changeset = changeset['id'])
    logger.info("BEGIN update_lists: %s", changeset['id'])
    configfile = celeryconfig.OL_CONFIG
    ol_config = formats.load_yaml(open(configfile).read())
    
    updater = ListUpdater(ol_config.get("lists"))
    updater.process_changeset(changeset, update_seeds=False)
    logger.info("END update_lists")

@oltask
def upload_via_s3(item_id, filename, data, s3_key, s3_secret):
    logger.info("s3 upload: %s %s" % (item_id, filename) )

    path = "http://s3.us.archive.org/%s/%s" % (item_id, filename)
    auth = "LOW %s:%s" % (s3_key, s3_secret)

    req = urllib2.Request(path)
    req.add_header('x-archive-auto-make-bucket', '1')
    req.add_header('x-archive-meta-collection',  'ol_data')
    req.add_header('x-archive-meta-mediatype',   'data')
    req.add_header('x-archive-keep-old-version', '1')
    req.add_header('x-archive-queue-derive',     '0')
    req.add_header('authorization', auth)

    req.add_data(data)
    req.get_method = lambda: 'PUT'

    f = urllib2.urlopen(req)
    logger.info('s3 upload of %s %s returned status: %d' % (item_id, filename, f.getcode()))


def setup():
    # make sure openlibrary logger has at least INFO level
    ol_logger = logging.getLogger("openlibrary")
    if ol_logger.level < logging.INFO:
        ol_logger.level = logging.INFO
    
setup()

########NEW FILE########
__FILENAME__ = test_work_subjects
import unittest
from openlibrary.solr.work_subject import four_types, find_subjects

class TestWorkSubjects(unittest.TestCase):
    def testFourTypes(self):
        input = {
            'subject': { 'Science': 2 },
            'event': { 'Party': 1 },
        }
        expect = {
            'subject': { 'Science': 2, 'Party': 1 },
        }
        self.assertEqual(four_types(input), expect)

        input = {
            'event': { 'Party': 1 },
        }
        expect = {
            'subject': { 'Party': 1 },
        }
        self.assertEqual(four_types(input), expect)

        marc = [[
            ('650', ' 0\x1faRhodes, Dan (Fictitious character)\x1fvFiction.\x1e'),
            ('650', ' 0\x1faSheriffs\x1fvFiction.\x1e'),
            ('651', ' 0\x1faTexas\x1fvFiction.\x1e')
        ]]

        expect = {
            'place': {u'Texas': 1},
            'subject': {u'Dan Rhodes (Fictitious character)': 1, u'Sheriffs': 1, u'Sheriffs in fiction': 1, u'Texas in fiction': 1, u'Fiction': 3}
        }

        self.assertEqual(find_subjects(marc), expect)

        marc = [[
            ('650', ' 0\x1faSpies\x1fzFrance\x1fzParis\x1fvFiction.\x1e'),
            ('651', ' 0\x1faFrance\x1fxHistory\x1fyDirectory, 1795-1799\x1fvFiction.\x1e')
        ]]

        expect = {
            'subject': {u'History': 1, u'France in fiction': 1, u'Spies': 1, u'Spies in fiction': 1, u'Fiction': 2},
            'place': {u'Paris': 1, u'France': 2},
            'time': {u'Directory, 1795-1799': 1}
        }

        self.assertEqual(find_subjects(marc), expect)
        


########NEW FILE########
__FILENAME__ = bulkimport
"""Utility to bulk import documents into Open Library database without
going through infobase API.
"""

import os
import web
import datetime
import simplejson
from collections import defaultdict

class DocumentLoader:
    def __init__(self, **params):
        params = params.copy()
        params.setdefault('dbn', 'postgres')
        params.setdefault('user', os.getenv('USER'))
        self.db = web.database(**params)
        self.db.printing = False
        
    def new_work_keys(self, n):
        """Returns n new works keys."""
        return ['/works/OL%dW' % i for i in self.incr_seq('type_work_seq', n)]
        
    def incr_seq(self, seqname, n):
        """Increment a sequence by n and returns the latest value of
        that sequence and returns list of n numbers.
        """
        rows = self.db.query(
            "SELECT setval($seqname, $n + (select last_value from %s)) as value" % seqname, 
            vars=locals())
        end = rows[0].value + 1 # lastval is inclusive
        begin = end - n
        return range(begin, end)

    def get_thing_ids(self, keys):
        keys = list(set(keys))
        rows = self.db.query("SELECT id, key FROM thing WHERE key in $keys", vars=locals())
        return dict((r.key, r.id) for r in rows)

    def get_thing_id(self, key):
        return self.get_thing_ids([key]).get(key)

    def _with_transaction(f):
        """Decorator to run a method in a transaction.
        """
        def g(self, *a, **kw):
            t = self.db.transaction()
            try:
                value = f(self, *a, **kw)
            except:
                t.rollback()
                raise
            else:
                t.commit()
            return value
        return g

    def bulk_new(self, documents, author="/user/ImportBot", comment=None):
        """Create new documents in the database without going through
        infobase.  This approach is very fast, but this can lead to
        errors in the database if the caller is not careful.
        
        All records must contain "key" and "type"
        properties. "last_modified" and "created" properties are
        automatically added to all records.

        Entries are not added to xxx_str, xxx_ref ... tables. reindex
        method must be called separately to do that.
        """
        return self._bulk_new(documents, author, comment)

    @_with_transaction
    def _bulk_new(self, documents, author, comment):
        timestamp = datetime.datetime.utcnow()
        type_ids = self.get_thing_ids(doc['type']['key'] for doc in documents)

        # insert things
        things = [dict(key=doc['key'], 
                       type=type_ids[doc['type']['key']], 
                       created=timestamp, 
                       last_modified=timestamp)
                  for doc in documents]
        thing_ids = self.db.multiple_insert('thing', things)

        # prepare documents
        created = {'type': '/type/datetime', "value": timestamp.isoformat()}
        for doc, thing_id in zip(documents, thing_ids):
            doc['id'] = thing_id
            doc['revision'] = 1
            doc['latest_revision'] = 1
            doc['created'] = created
            doc['last_modified'] = created

        # insert data
        return self._insert_data(documents, author=author, timestamp=timestamp, comment=comment)

    def _insert_data(self, documents, author, timestamp, comment, ip="127.0.0.1"):
        """Add entries in transaction and version tables for inseting
        above documents.

        It is assumed that correct value of id, revision and
        last_modified is already set in all documents.
        """
        author_id = author and self.get_thing_id(author)

        # add an entry in the transaction table
        txn_id = self.db.insert('transaction',
                                action="import",
                                comment=comment,
                                author_id=author_id,
                                created=timestamp,
                                ip=ip)

        # add versions
        versions = [dict(transaction_id=txn_id,
                         thing_id=doc['id'],  
                         revision=doc['revision'])
                    for doc in documents]
        self.db.multiple_insert('version', versions, seqname=False)

        result = [{'key': doc['key'], 'revision': doc['revision'], 'id': doc['id']} for doc in documents]

        # insert data
        try:
            data = [dict(thing_id=doc.pop('id'),
                         revision=doc['revision'], 
                         data=simplejson.dumps(doc))
                    for doc in documents]
        except UnicodeDecodeError:
            print `doc`
            raise
        self.db.multiple_insert('data', data, seqname=False)
        return result
        
    def bulk_update(self, documents, author='/user/ImportBot', comment=None):
        """Update existing documents in the database. 

        When adding new properties, it is sufficient to specify key and
        new properties.

        db.bulk_update([
            {'key': '/b/OL1M', 'work': {'key': '/works/OL1W'}}
            {'key': '/b/OL2M', 'work': {'key': '/works/OL2M'}}],
            comment="link works")

        When updating an existing property, it sufficient to specify key and new value of that property.

        db.bulk_update([
            {'key': '/b/OL1M', 'title': 'New title'}],
            comment="unicode normalize titles")

        When append new value to an existing property, entire list must be provied.

        db.bulk_update([{
                'key': '/a/OL1A', 
                'links': ['http://en.wikipedia.org/wiki/Foo', 'http://de.wikipedia.org/wiki/Foo']
            }, comment="add german wikipedia links")

        WARNING: This function should not be used to change the "type" property of documents.
        """
        return self._bulk_update(documents, author, comment)

    @_with_transaction
    def _bulk_update(self, documents, author, comment):
        timestamp = datetime.datetime.utcnow()

        keys = [doc['key'] for doc in documents]

        # update latest_revision and last_modified in thing table
        self.db.query("UPDATE thing" + 
                      " SET last_modified=$timestamp, latest_revision=latest_revision+1" + 
                      " WHERE key IN $keys",
                      vars=locals())

        # fetch the current data
        rows = self.db.query("SELECT thing.id, thing.key, thing.created, thing.latest_revision, data.data" + 
                             " FROM thing, data" +
                             " WHERE data.thing_id=thing.id AND data.revision=thing.latest_revision-1 and thing.key in $keys",
                             vars=locals())

        rows = dict((r.key, r) for r in rows)
        last_modified = {'type': '/type/datetime', 'value': timestamp.isoformat()}
        def prepare(doc):
            """Takes the existing document from db, update it with doc
            and add revision, latest_revision, last_modified
            properties.
            """
            r = rows[doc['key']]
            d = simplejson.loads(r.data)
            d.update(doc, 
                     revision=r.latest_revision,
                     latest_revision=r.latest_revision,
                     last_modified=last_modified,
                     id=r.id)
            return d

        documents = [prepare(doc) for doc in documents]
        return self._insert_data(documents, author=author, timestamp=timestamp, comment=comment)


    def reindex(self, keys, tables=None):
        """Delete existing entries and add new entries to xxx_str,
        xxx_ref .. tables for the documents specified by keys.

        If optional tables argument is specified then reindex is done only for values in those tables.
        """
        return Reindexer(self.db).reindex(keys, tables)

    # this is not required anymore
    del _with_transaction

class Reindexer:
    """Utility to reindex documents."""
    def __init__(self, db):
        self.db = db

        import openlibrary.plugins.openlibrary.schema
        self.schema = openlibrary.plugins.openlibrary.schema.get_schema()
        self.noindex = set(["id", "key", "type", "type_id",
                       "revision", "latest_revision", 
                       "created", "last_modified", 
                       "permission", "child_permission"])
        self._property_cache = {}

    def reindex(self, keys, tables=None):
        """Reindex documents specified by the keys.

        If tables is specified, index is recomputed only for those tables and other tables are ignored.
        """
        t = self.db.transaction()
        
        try:
            documents = self.get_documents(keys)
            self.delete_earlier_index(documents, tables)
            self.create_new_index(documents, tables)
        except:
            t.rollback()
            raise
        else:
            t.commit()

    def get_documents(self, keys):
        """Get documents with given keys from database and add "id" and "type_id" to them.
        """
        rows = self.db.query("SELECT thing.id, thing.type, data.data" + 
                             " FROM thing, data" +
                             " WHERE data.thing_id=thing.id AND data.revision=thing.latest_revision and thing.key in $keys",
                             vars=locals())

        documents = [dict(simplejson.loads(row.data), 
                          id=row.id,
                          type_id=row.type)
                     for row in rows]
        return documents

    def delete_earlier_index(self, documents, tables=None):
        """Remove all prevous entries corresponding to the given documents"""
        all_tables = tables or set(r.relname for r in self.db.query(
                "SELECT relname FROM pg_class WHERE relkind='r'"))

        data = defaultdict(list)
        for doc in documents:
            for table in self.schema.find_tables(doc['type']['key']):
                if table in all_tables:
                    data[table].append(doc['id'])

        for table, thing_ids in data.items():
            self.db.delete(table, where="thing_id IN $thing_ids", vars=locals())
    
    def create_new_index(self, documents, tables=None):
        """Insert data in to index tables for the specified documents."""
        data = defaultdict(list)
        
        def insert(doc, name, value, ordering=None):
            # these are present in thing table. No need to index these keys
            if name in ["id", "type", "created", "last_modified", "permission", "child_permission"]:
                return
            if isinstance(value, list):
                for i, v in enumerate(value):
                    insert(doc, name, v, ordering=i)
            elif isinstance(value, dict) and 'key' not in value:
                for k, v in value.items():
                    if k == "type": # no need to index type
                        continue
                    insert(doc, name + '.' + k, v, ordering=ordering)
            else:
                datatype = self._find_datatype(value)
                table = datatype and self.schema.find_table(doc['type']['key'], datatype, name)
                # when asked to index only some tables
                if tables and table not in tables:
                    return
                if table:
                    self.prepare_insert(data[table], doc['id'], doc['type_id'], name, value, ordering=ordering)

        for doc in documents:
            for name, value in doc.items():
                insert(doc, name, value)

        # replace keys with thing ids in xxx_ref tables
        self.process_refs(data)

        # insert the data
        for table, rows in data.items():
            self.db.multiple_insert(table, rows, seqname=False)


    def get_property_id(self, type_id, name):
        if (type_id, name) not in self._property_cache:
            self._property_cache[type_id, name] = self._get_property_id(type_id, name)
        return self._property_cache[type_id, name]

    def _get_property_id(self, type_id, name):
        d = self.db.select('property', where='name=$name AND type=$type_id', vars=locals())
        if d:
            return d[0].id
        else:
            return self.db.insert('property', type=type_id, name=name)

    def prepare_insert(self, rows, thing_id, type_id, name, value, ordering=None):
        """Add data to be inserted to rows list."""
        if name in self.noindex:
            return
        elif isinstance(value, list):
            for i, v in enumerate(value):
                self.prepare_insert(rows, thing_id, type_id, name, v, ordering=i)
        else:
            rows.append(
                dict(thing_id=thing_id,
                     key_id=self.get_property_id(type_id, name),
                     value=value, 
                     ordering=ordering))

    def process_refs(self, data):
        """Convert key values to thing ids for xxx_ref tables."""
        keys = []
        for table, rows in data.items():
            if table.endswith('_ref'):
                keys += [r['value']['key'] for r in rows]

        if not keys:
            return

        thing_ids = dict((r.key, r.id) for r in self.db.query(
                "SELECT id, key FROM thing WHERE key in $keys", 
                vars=locals()))

        for table, rows in data.items():
            if table.endswith('_ref'):
                for r in rows:
                    r['value'] = thing_ids[r['value']['key']]

    def _find_datatype(self, value):
        """Find datatype of given value.

        >>> _find_datatype = Reindexer(None)._find_datatype
        >>> _find_datatype(1)
        'int'
        >>> _find_datatype('hello')
        'str'
        >>> _find_datatype({'key': '/a/OL1A'})
        'ref'
        >>> _find_datatype([{'key': '/a/OL1A'}])
        'ref'
        >>> _find_datatype({'type': '/type/text', 'value': 'foo'})
        >>> _find_datatype({'type': '/type/datetime', 'value': '2009-10-10'})
        'datetime'
        """
        if isinstance(value, int):
            return 'int'
        elif isinstance(value, basestring):
            return 'str'
        elif isinstance(value, dict):
            if 'key' in value:
                return 'ref'
            elif 'type' in value:
                return {
                    '/type/int': 'int',
                    '/type/string': 'str',
                    '/type/datetime': 'datetime'
                }.get(value['type'])
        elif isinstance(value, list):
            return value and self._find_datatype(value[0])
        else:
             return None

def _test():
    loader = DocumentLoader(db='ol')
    loader.db.printing = True
    
    n = 2

    print loader.bulk_new([dict(
                key="/b/OL%dM" % i, 
                title="book %d" % i, 
                type={"key": "/type/edition"}, 
                table_of_contents=[{"type": {"key": "/type/toc_item"}, "class": "part", "label": "test", "title": "test", "pagenum": "10"}])
            for i in range(1, n+1)], 
        comment="add books")
    
    loader.reindex(["/b/OL%dM" % i for i in range(1, n+1)])
    
if __name__ == "__main__":
    _test()

########NEW FILE########
__FILENAME__ = compress
# incremental zlib compression, written by solrize, August 2009
import zlib

__doc__ = """
Compressor object for medium-sized, statistically-similar strings.

The idea is that you have a lot of moderate-sized strings (short email
messages or the like) that you would like to compress independently,
for storage in a lookup table where space is at a premium.  They
strings might be a few hundred bytes long on average.  That's not
enough to get much compression by gzipping without context.  gzip
works by starting with no knowledge, then building up knowledge (and
improving its compression ratio) as it goes along.

The trick is to "pre-seed" the gzip compressor with a bunch of text
(say a few kilobytes of messages concatenated) similar to the ones
that you want to compress separately, and pre-seed the gzip
decompressor with the same initial text.  That lets the compressor and
decompressor both start with enough knowledge to get good compression
even for fairly short strings.  This class puts a compressor and
decompressor into the same object, called a Compressor for convenience.

Usage: running the three lines

    compressor = Compressor(initial_seed)
    compressed_record = compressor.compress(some_record)
    restored_record = compressor.decompress(compressed_record)

where initial_seed is a few kilobytes of messages, and some_record is
a single record of maybe a few hundred bytes, for typical text, should
result in compressed_record being 50% or less of the size of
some_record, and restored_record being identical to some_record.
"""

class Compressor(object):
    def __init__(self, seed):
        c = zlib.compressobj(9)
        d_seed = c.compress(seed)
        d_seed += c.flush(zlib.Z_SYNC_FLUSH)
        self.c_context = c.copy()

        d = zlib.decompressobj()
        d.decompress(d_seed)
        while d.unconsumed_tail:
            d.decompress(d.unconsumed_tail)
        self.d_context = d.copy()

    def compress(self, text):
        c = self.c_context.copy()
        t = c.compress(text)
        t2 = c.flush(zlib.Z_FINISH)
        return t + t2

    def decompress(self, ctext):
        d = self.d_context.copy()
        t = d.decompress(ctext)
        while d.unconsumed_tail:
            t += d.decompress(d.unconsumed_tail)
        return t

def test():
    c = Compressor(__doc__)
    test_string = "zlib is a pretty good compression algorithm"
    ct = c.compress(test_string)
    # print 'initial length=%d, compressed=%d'% (len(test_string), len(ct))
    # the above string compresses from 43 bytes to 29 bytes using the
    # current doc text as compression seed, not bad for such short input.
    dt = c.decompress(ct)
    assert dt == test_string

test()

########NEW FILE########
__FILENAME__ = dateutil
"""Generic date utilities.
"""
import datetime

def parse_date(datestr):
    """Parses date string.
    
        >>> parse_date("2010")
        datetime.date(2010, 01, 01)
        >>> parse_date("2010-02")
        datetime.date(2010, 02, 01)
        >>> parse_date("2010-02-04")
        datetime.date(2010, 02, 04)
    """
    tokens = datestr.split("-")
    _resize_list(tokens, 3)
    
    yyyy, mm, dd = tokens[:3]
    return datetime.date(int(yyyy), mm and int(mm) or 1, dd and int(dd) or 1)
    
def parse_daterange(datestr):
    """Parses date range.
        
        >>> parse_daterange("2010-02")
        (datetime.date(2010, 02, 01), datetime.date(2010, 03, 01))
    """
    date = parse_date(datestr)
    tokens = datestr.split("-")
    
    if len(tokens) == 1: # only year specified
        return date, nextyear(date)
    elif len(tokens) == 2: # year and month specified
        return date, nextmonth(date)
    else:
        return date, nextday(date)
    
def nextday(date):
    return date + datetime.timedelta(1)

def nextmonth(date):
    """Returns a new date object with first day of the next month."""
    year, month = date.year, date.month
    month = month + 1
    
    if month > 12:
        month = 1
        year += 1
         
    return datetime.date(year, month, 1)

def nextyear(date):
    """Returns a new date object with first day of the next year."""
    return datetime.date(date.year+1, 1, 1)

def _resize_list(x, size):
    """Increase the size of the list x to the specified size it is smaller.
    """
    if len(x) < size:
        x += [None] * (size - len(x))

########NEW FILE########
__FILENAME__ = form
"""New form library to use instead of web.form.

(this should go to web.py)
"""
import web
import copy
import re

from infogami.utils.view import render

class AttributeList(dict):
    """List of atributes of input.
    
    >>> a = AttributeList(type='text', name='x', value=20)
    >>> a
    <attrs: 'type="text" name="x" value="20"'>x
    """
    def copy(self):
        return AttributeList(self)
        
    def __str__(self):
        return " ".join('%s="%s"' % (k, web.websafe(v)) for k, v in self.items())
        
    def __repr__(self):
        return '<attrs: %s>' % repr(str(self))

class Input:
    def __init__(self, name, description=None, value=None, **kw):
        self.name = name
        self.description = description or ""
        self.value = value
        self.validators = kw.pop('validators', [])
        
        self.help = kw.pop('help', None)
        self.note = kw.pop('note', None)
        
        self.id = kw.pop('id', name)
        self.__dict__.update(kw)
        
        if 'klass' in kw:
            kw['class'] = kw.pop('klass')
        
        self.attrs = AttributeList(kw)
        
    def get_type(self):
        raise NotImplementedError
        
    def is_hidden(self):
        return False
        
    def render(self):
        attrs = self.attrs.copy()
        attrs['id'] = self.id
        attrs['type'] = self.get_type()
        attrs['name'] = self.name
        attrs['value'] = self.value or ''
            
        return '<input ' + str(attrs) + ' />'

    def validate(self, value):
        self.value = value
        for v in self.validators:
            if not v.valid(value):
                self.note = v.msg
                return False
        return True        
        
class Textbox(Input):
    """Textbox input.
    
    >>> t = Textbox("name", description='Name', value='joe')
    >>> t.render()
    '<input type="text" id="name" value="joe" name="name" />'

    >>> t = Textbox("name", description='Name', value='joe', id='name', klass='input', size=10)
    >>> t.render()
    '<input name="name" value="joe" class="input" type="text" id="name" size="10" />'
    """
    def get_type(self):
        return "text"

class Password(Input):
    """Password input.
    
        >>> Password("password", description='Password', value='secret').render()
        '<input type="password" id="password" value="secret" name="password" />'
    """
    def get_type(self):
        return "password"
        
class Checkbox(Input):
    """Checkbox input."""
    
    def get_type(self):
        return "checkbox"
        
class Hidden(Input):
    """Hidden input.
    """
    def is_hidden(self):
        return True

    def get_type(self):
        return "hidden"

class Form:
    def __init__(self, *inputs, **kw):
        self.inputs = inputs
        self.validators = kw.pop('validators', [])
        self.note = None
        
    def __call__(self):
        return copy.deepcopy(self)
        
    def __str__(self):
        return web.safestr(self.render())
        
    def __getitem__(self, key):
        for i in self.inputs:
            if i.name == key:
                return i
        raise KeyError, key
        
    def __getattr__(self, name):
        # don't interfere with deepcopy
        inputs = self.__dict__.get('inputs') or []
        for x in inputs:
            if x.name == name: return x
        raise AttributeError, name

    def render(self):
        return render.form(self)
        
    def validates(self, source):
        valid = True
        
        for i in self.inputs:
            v = source.get(i.name)
            valid = i.validate(v) and valid

        valid = self._validate(source) and valid
        self.valid = valid
        return valid
        
    fill = validates
        
    def _validate(self, value):
        for v in self.validators:
            if not v.valid(value):
                self.note = v.msg
                return False
        return True

class Validator:
    def __init__(self, msg, test): 
        self.msg = msg
        self.test = test
        
    def __deepcopy__(self, memo): 
        return copy.copy(self)

    def valid(self, value): 
        try: 
            return self.test(value)
        except: 
            raise
            return False
            
    def __repr__(self):
        return "<validator: %r >" % self.msg

notnull = Validator("Required", bool)

class RegexpValidator(Validator):
    def __init__(self, rexp, msg):
        self.rexp = re.compile(rexp)
        self.msg = msg
    
    def valid(self, value):
        return bool(self.rexp.match(value))

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = httpserver
"""http server for testing.

    >>> server = HTTPServer(port=8090)
    >>> server.request('/hello/world', method='GET').should_return('hello, world!', headers={'content-type': 'text/plain'})
    
    >>> response = urllib.urlopen('http://0.0.0.0:8090/hello/world')
    >>> response.read()
    'hello, world!'
    >>> response.info().getheader('Content-Type')
    'text/plain'
    
    >>> server.stop()
    >>> urllib.urlopen('http://0.0.0.0:8090/hello/world')
    Traceback (most recent call last):
    ...
    IOError: [Errno socket error] (61, 'Connection refused')
"""
from web.wsgiserver import CherryPyWSGIServer
import urllib
import threading
import time

class HTTPServer:
    def __init__(self, port=8090):
        self.mappings = {}
        self.port = port
        self.server = CherryPyWSGIServer(("0.0.0.0", port), self, server_name="localhost")        
        self.started = False
        self.t = threading.Thread(target=self._start)
        self.t.start()
        time.sleep(0.1)
        
    def _start(self):
        try:
            self.server.start()
        except Exception, e:
            print 'ERROR: failed to start server', str(e)
        
    def stop(self):
        self.server.stop()
        self.t.join()
        
    def request(self, path, method='GET', query={}):
        response = Respose()
        
        if isinstance(query, dict):
            query = urllib.urlencode(query)
            
        self.mappings[path, method, query] = response
        return response
        
    def __call__(self, environ, start_response):
        _method = environ.get('REQUEST_METHOD', 'GET')
        _path = environ.get('PATH_INFO')
        
        for (path, method, query_string), response in self.mappings.items():
            if _path == path and _method == method:
                return response(start_response)
               
        return Respose()(start_response)

class Respose:
    def __init__(self):
        self.status = '404 Not Found'
        self.data = "not found"
        self.headers = {}
        
    def should_return(self, data, status="200 OK", headers={}):
        self.status = status
        self.data = data
        self.headers = headers
        
    def __call__(self, start_response):
        start_response(self.status, self.headers.items())
        return self.data
        
if __name__ == "__main__":
    import doctest
    doctest.testmod()
########NEW FILE########
__FILENAME__ = ia
from socket import socket, AF_INET, SOCK_DGRAM, SOL_UDP, SO_BROADCAST, timeout
import re

re_loc = re.compile('^(ia\d+\.us\.archive\.org):(/\d+/items/(.*))$')

class FindItemError(Exception):
    pass

def find_item(ia):
    s = socket(AF_INET, SOCK_DGRAM, SOL_UDP)
    s.setblocking(1)
    s.settimeout(2.0)
    s.setsockopt(1, SO_BROADCAST, 1)
    s.sendto(ia, ('<broadcast>', 8010))
    for attempt in range(5):
        (loc, address) = s.recvfrom(1024)
        m = re_loc.match(loc)

        ia_host = m.group(1)
        ia_path = m.group(2)
        if m.group(3) == ia:
            return (ia_host, ia_path)
    raise FindItemError

########NEW FILE########
__FILENAME__ = isbn
import logging

logger = logging.getLogger("openlibrary")

def check_digit_10(isbn):
    if len(isbn) != 9:
        raise ValueError("%s is not a valid ISBN 10" % isbn)
    sum = 0
    for i in range(len(isbn)):
        c = int(isbn[i])
        w = i + 1
        sum += w * c
    r = sum % 11
    if r == 10:
        return 'X'
    else:
        return str(r)

def check_digit_13(isbn):
    if len(isbn) != 12:
        raise ValueError
    sum = 0
    for i in range(len(isbn)):
        c = int(isbn[i])
        if i % 2: w = 3
        else: w = 1
        sum += w * c
    r = 10 - (sum % 10)
    if r == 10:
        return '0'
    else:
        return str(r)

def isbn_13_to_isbn_10(isbn_13):
    isbn_13 = isbn_13.replace('-', '')
    try:
        if len(isbn_13) != 13 or not isbn_13.isdigit()\
        or not isbn_13.startswith('978')\
        or check_digit_13(isbn_13[:-1]) != isbn_13[-1]:
            raise ValueError("%s is not a valid ISBN 13" % isbn_13)
    except ValueError as e:
        logger.info("Exception caught in ISBN transformation: %s" % e)
        return
    return isbn_13[3:-1] + check_digit_10(isbn_13[3:-1])

def isbn_10_to_isbn_13(isbn_10):
    isbn_10 = isbn_10.replace('-', '')
    try:
        if len(isbn_10) != 10 or not isbn_10[:-1].isdigit()\
        or check_digit_10(isbn_10[:-1]) != isbn_10[-1]:
            raise ValueError("%s is not a valid ISBN 10" % isbn_10)
    except ValueError as e:
        logger.info("Exception caught in ISBN transformation: %s" % e)
        return
    isbn_13 = '978' + isbn_10[:-1]
    return isbn_13 + check_digit_13(isbn_13)

def opposite_isbn(isbn): # ISBN10 -> ISBN13 and ISBN13 -> ISBN10
    isbn = isbn.replace('-', '')
    for f in isbn_13_to_isbn_10, isbn_10_to_isbn_13:
        alt = f(isbn)
        if alt:
            return alt

########NEW FILE########
__FILENAME__ = olcompress
"""Initial seed to create compress object.

see compress module for details.
"""

from compress import Compressor

# using 4 random records from OL seed

seed1= '{"subject_place": ["Great Britain", "Great Britain."], "lc_classifications": ["RT85.5 .K447 1994"], "contributions": ["Richardson, Eileen, RGN."], "id": 1875537, "title": "nursing process and quality care", "languages": [{"key": "/l/eng"}], "subjects": ["Nursing -- Quality control.", "Nursing -- Standards.", "Nursing audit.", "Nursing -- Great Britain -- Quality control.", "Nursing -- Standards -- Great Britain.", "Nursing audit -- Great Britain."], "publish_country": "cau", "title_prefix": "The ", "type": {"key": "/type/edition"}, "by_statement": "Nan Kemp, Eileen Richardson.", "revision": 1, "other_titles": ["Nursing process & quality care."], "publishers": ["Singular Pub. Group"], "last_modified": {"type": "/type/datetime", "value": "2008-04-01 03:28:50.625462"}, "key": "/b/OL1234567M", "authors": [{"key": "/a/OL448883A"}], "publish_places": ["San Diego, Calif"], "pagination": "132 p. :", "dewey_decimal_class": ["362.1/73/0685"], "notes": {"type": "/type/text", "value": "Includes bibliographical references and index.\nCover title: The nursing process & quality care."}, "number_of_pages": 132, "lccn": ["94237442"], "isbn_10": ["1565933834"], "publish_date": "1994"}'
seed2 = '{"subtitle": "exploration & celebration : papers delivered at an academic conference honoring twenty years of women in the rabbinate, 1972-1992", "subject_place": ["United States"], "lc_classifications": ["BM652 .W66 1996"], "contributions": ["Zola, Gary Phillip."], "id": 1523482, "title": "Women rabbis", "languages": [{"key": "/l/eng"}], "subjects": ["Women rabbis -- United States -- Congresses.", "Reform Judaism -- United States -- Congresses.", "Women in Judaism -- Congresses."], "publish_country": "ohu", "by_statement": "edited by Gary P. Zola.", "type": {"key": "/type/edition"}, "revision": 1, "publishers": ["HUC-JIR Rabbinic Alumni Association Press"], "last_modified": {"type": "/type/datetime", "value": "2008-04-01 03:28:50.625462"}, "key": "/b/OL987654M", "publish_places": ["Cincinnati"], "pagination": "x, 135 p. ;", "dewey_decimal_class": ["296.6/1/082"], "notes": {"type": "/type/text", "value": "Includes bibliographical references and index."}, "number_of_pages": 135, "lccn": ["96025781"], "isbn_10": ["0878202145"], "publish_date": "1996"}'
seed3 = '{"name": "W. Wilkins Davis", "personal_name": "W. Wilkins Davis", "death_date": "1866.", "last_modified": {"type": "/type/datetime", "value": "2008-08-21 07:57:48.336414"}, "key": "/a/OL948765A", "birth_date": "1842", "type": {"key": "/type/author"}, "id": 2985386, "revision": 2}'
seed4 = '{"name": "Alberto Tebechrani", "personal_name": "Alberto Tebechrani", "last_modified": {"type": "/type/datetime", "value": "2008-09-08 15:29:34.798941"}, "key": "/a/OL94792A", "type": {"key": "/type/author"}, "id": 238564, "revision": 2}'

seed = seed1 + seed2 + seed3 + seed4

def OLCompressor():
    """Create a compressor object for compressing OL data.
    """
    return Compressor(seed)
########NEW FILE########
__FILENAME__ = olmemcache
"""Memcache interface to store data in memcached."""

import memcache
from olcompress import OLCompressor
import web

class Client:
    """Wrapper to memcache Client to enable OL specific compression and unicode keys.
    Compatible with memcache Client API.
    """
    def __init__(self, servers):
        self._client = memcache.Client(servers)
        compressor = OLCompressor()
        self.compress = compressor.compress
        self.decompress = compressor.decompress
        
    def get(self, key):
        try:
            value = self._client.get(web.safestr(key))
        except memcache.Client.MemcachedKeyError:
            return None
        
        return value and self.decompress(value)
        
    def get_multi(self, keys):
        keys = [web.safestr(k) for k in keys]
        d = self._client.get_multi(keys)
        return dict((web.safeunicode(k), self.decompress(v)) for k, v in d.items())
        
    def set(self, key, val, time=0):
        return self._client.set(web.safestr(key), self.compress(val), time=time)
    
    def set_multi(self, mapping, time=0):
        mapping = dict((web.safestr(k), self.compress(v)) for k, v in mapping.items())
        return self._client.set_multi(mapping, time=time)
        
    def add(self, key, val, time=0):
        return self._client.add(web.safestr(key), self.compress(val), time=time)

    def delete(self, key, time=0):
        key = web.safestr(key)
        return self._client.delete(key, time=time)
        
    def delete_multi(self, keys, time=0):
        keys = [web.safestr(k) for k in keys]
        return self._client.delete_multi(keys, time=time)
########NEW FILE########
__FILENAME__ = processors
"""Generic web.py application processors.
"""
import web
import time

__all__ = [
    "RateLimitProcessor"
]

class RateLimitProcessor:
    """Application processor to ratelimit the access per ip.
    """
    def __init__(self, limit, window_size=600, path_regex="/.*"):
        """Creates a rate-limit processor to limit the number of
        requests/ip in the time frame.

        :param limit: the maxinum number of requests allowed in the given time window.
        :param window_size: the time frame in seconds during which the requests are measured.
        :param path_regex: regular expression to specify which urls are rate-limited.
        """
        self.path_regex = web.re_compile(path_regex)
        self.limit = limit
        self.window_size = window_size
        self.reset(None)

    def reset(self, timestmap):
        self.window = {}
        self.window_timestamp = timestmap

    def get_window(self):
        t = int(time.time() / self.window_size)
        if t != self.window_timestamp:
            self.reset(t)
        return self.window

    def check_rate(self):
        """Returns True if access rate for the current IP address is
        less than the allowed limit.
        """
        window = self.get_window()
        ip = web.ctx.ip

        if window.get(ip, 0) < self.limit:
            window[ip] = 1 + window.get(ip, 0)
            return True
        else:
            return False

    def __call__(self, handler):
        if self.path_regex.match(web.ctx.path):
            if self.check_rate():
                return handler()
            else:
                raise web.HTTPError("503 Service Unavailable")

########NEW FILE########
__FILENAME__ = schema
"""utility to generate db schema for any database engine.
(should go to web.py)
"""

__all__ = [
    "Schema", "Table", "Column",
    "register_datatype", "register_constant",
]

_datatypes = {}
def register_datatype(name, datatype):
    _datatypes[name] = datatype

_adapters = {}
def register_adapter(name, adapter):
    _adapters[name] = adapter
    
def get_adapter(name):
    if isinstance(name, AbstractAdapter):
        return name
    else:
        return _adapters[name]()

_constants = {}
def register_constant(name, constant):
    _constants[name] = constant
    
def get_constant(name):
    return _constants(name)

class AbstractAdapter:
    def type_to_sql(self, type, limit=None):
        sql = self.get_native_type(type)
        if limit:
            sql += '(%s)' % limit
        return sql
        
    def get_native_type(self, type):
        return self.native_types[type]
        
    def index_name(self, table, columns):
        return "_".join([table] + columns + ["idx"])
        
    def references_to_sql(self, column_name, value):
        # foreign key constraints are not supported by default
        return None
        
    def column_option_to_sql(self, column, option, value):
        if option == 'primary_key' and value is True:
            return 'primary key'
        elif option == 'unique' and value is True:
            return 'unique'
        elif option == 'default':
            if hasattr(value, 'sql'):
                value = value.sql(self)
            else:
                value = sqlrepr(value)
            return "default %s" % (value)
        elif option == 'null':
            return {True: 'null', False: 'not null'}[value]
        elif option == 'references':
            return self.references_to_sql(column, value)
    
    def get_constant(self, name):
        return self.constants[name]
        
    def quote(self):
        raise NotImplementedError()
        
class MockAdapter(AbstractAdapter):
    def get_native_type(self, type):
        return type
        
    def references_to_sql(self, column_name, value):
        return 'references ' + value
        
    def quote(self, value):
        return repr(value)

class MySQLAdapter(AbstractAdapter):
    native_types = {
        'serial': 'int auto_increment not null',
        'integer': 'int',
        'float': 'float',
        'string': 'varchar',
        'text': 'text',
        'datetime': 'datetime',
        'timestamp': 'datetime',
        'time': 'time',
        'date': 'date',
        'binary': 'blob',
        'boolean': 'boolean'
    }
    constants = {
        'CURRENT_TIMESTAMP': 'CURRENT_TIMESTAMP',
        'CURRENT_DATE': 'CURRENT_DATE',
        'CURRENT_TIME': 'CURRENT_TIME',
        'CURRENT_UTC_TIMESTAMP': 'UTC_TIMESTAMP',
        'CURRENT_UTC_DATE': 'UTC_DATE',
        'CURRENT_UTC_TIME': 'UTC_TIME',
    }
    def references_to_sql(self, column_name, value):
        return {'constraint': 'foreign key (%s) references %s' % (column_name, value)}

class PostgresAdapter(AbstractAdapter):  
    native_types = {
        'serial': 'serial',
        'integer': 'int',
        'float': 'float',
        'string': 'character varying',
        'text': 'text',
        'datetime': 'timestamp',
        'timestamp': 'timestamp',
        'time': 'time',
        'date': 'date',
        'binary': 'bytea',
        'boolean': 'boolean'
    }
    constants = {
        'CURRENT_TIMESTAMP': 'current_timestamp',
        'CURRENT_DATE': 'current_date',
        'CURRENT_TIME': 'current_time',
        'CURRENT_UTC_TIMESTAMP': "(current_timestamp at time zone 'utc')",
        'CURRENT_UTC_DATE': "(date (current_timestamp at timezone 'utc'))",
        'CURRENT_UTC_TIME': "(current_time at time zone 'utc')",
    }
    
    def references_to_sql(self, column_name, value):
        return 'references ' + value
    
class SQLiteAdapter(AbstractAdapter):
    native_types = {
        'serial': 'integer autoincrement',
        'integer': 'integer',
        'float': 'float',
        'string': 'varchar',
        'text': 'text',
        'datetime': 'datetime',
        'timestamp': 'datetime',
        'time': 'datetime',
        'date': 'date',
        'binary': 'blob',
        'boolean': 'boolean'
    }
    constants = {
        'CURRENT_TIMESTAMP': "CURRENT_TIMESTAMP",
        'CURRENT_DATE': "CURRENT_DATE",
        'CURRENT_TIME': "CURRENT_TIME",
        'CURRENT_UTC_TIMESTAMP': "CURRENT_TIMESTAMP",
        'CURRENT_UTC_DATE': "CURRENT_DATE",
        'CURRENT_UTC_TIME': "CURRENT_TIME",
    }

register_adapter('mysql', MySQLAdapter)
register_adapter('postgres', PostgresAdapter)
register_adapter('sqlite', SQLiteAdapter)

def sqlrepr(s):
    if isinstance(s, str):
        return repr(s)
    else:
        return s

class Datatype:
    def __init__(self, name=None):
        self.name = name
        
    def sql(self, engine):
        return get_adapter(engine).type_to_sql(self.name)
        
class Constant:
    def __init__(self, name=None):
        self.name = name
        
    def sql(self, engine):
        return get_adapter(engine).get_constant(self.name)
        
for c in ['CURRENT_TIMESTAMP', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_UTC_TIMESTAMP', 'CURRENT_UTC_DATE', 'CURRENT_UTC_TIME']:
    register_constant(c, Constant(c))

class Schema:
    def __init__(self):
        self.tables = [] 
        self.indexes = []
        
        for name, value in _constants.items():
            setattr(self, name, value)
    
    def column(self, name, type, **options):
        return Column(name, type, **options)
        
    def add_table(self, name, *columns, **options):
        t = Table(name, *columns, **options)
        self.tables.append(t)
        
    def add_index(self, table, columns, **options):
        i = Index(table, columns, **options)
        self.indexes.append(i)
        
    def sql(self, engine):
        return "\n".join(x.sql(engine) for x in self.tables + self.indexes)

class Table:
    """Database table.
        >>> t = Table('user', Column('name', 'string'))
        >>> print t.sql('postgres')
        create table user (
            name character varying(255)
        );
    """
    def __init__(self, name, *columns, **options):
        self.name = name
        self.columns = columns
        self.options = options

    def sql(self, engine):
        columns = [c.sql(engine) for c in self.columns]
        for c in self.columns:
            for constraint in c.constraints:
                columns.append(constraint)
        return "create table %s (\n    %s\n);" % (self.name, ",\n    ".join(columns))

class Column:
    """Column in a database table.
        
        >>> Column('name', 'text').sql('mock')
        'name text'
        >>> Column('id', 'serial', primary_key=True).sql('mock')
        'id serial primary key'
        >>> Column('revision', 'integer', default=1).sql('mock')
        'revision integer default 1'
        >>> Column('name', 'string', default='joe').sql('mock')
        "name string(255) default 'joe'"
    """
    def __init__(self, name, type, **options):
        self.name = name
        self.type = type
        self.options = options
        self.limit = self.options.pop('limit', None)
        self.constraints = []
        
        self.primary_key = self.options.get('primary_key')
        self.unique = self.options.get('unique')
        self.references = self.options.get('references')
        
        # string type is of variable length. set default length as 255.
        if type == 'string':
            self.limit = self.limit or 255
            
    def sql(self, engine):
        adapter = get_adapter(engine)

        tokens = [self.name, adapter.type_to_sql(self.type, self.limit)]
        for k, v in self.options.items():
            result = adapter.column_option_to_sql(self.name, k, v)
            if result is None:
                continue
            elif isinstance(result, dict): # a way for column options to add constraints
                self.constraints.append(result['constraint'])
            else:
                tokens.append(result)
            
        return " ".join(tokens)
        
class Index:
    """Database index.
    
        >>> Index('user', 'email').sql('mock')
        'create index user_email_idx on user(email);'
        >>> Index('user', 'email', unique=True).sql('mock')
        'create unique index user_email_idx on user(email);'
        >>> Index('page', ['path', 'revision']).sql('mock')
        'create index page_path_revision_idx on page(path, revision);'
    """
    def __init__(self, table, columns, **options):
        self.table = table
        if not isinstance(columns, list):
            self.columns = [columns]
        else:
            self.columns = columns
            
        self.unique = options.get('unique')
        self.name = options.get('name')
        
    def sql(self, engine):
        adapter = get_adapter(engine)        
        name = self.name or adapter.index_name(self.table, self.columns)
        
        if self.unique:
            s = 'create unique index '
        else:
            s = 'create index '
        
        s += adapter.index_name(self.table, self.columns)
        s += ' on %s(%s);' % (self.table, ", ".join(self.columns))
        return s

def _test():
    """
    Define a sample schema.
    
        >>> s = Schema()
        >>> s.add_table('posts',
        ...     s.column('id', 'serial', primary_key=True),
        ...     s.column('slug', 'string', unique=True, null=False),
        ...     s.column('title', 'string', null=False),
        ...     s.column('body', 'text'),
        ...     s.column('created_on', 'timestamp', default=s.CURRENT_UTC_TIMESTAMP))
        ...
        >>> s.add_table('comments',
        ...     s.column('id', 'serial', primary_key=True),
        ...     s.column('post_id', 'integer', references='posts(id)'),
        ...     s.column('comment', 'text'))
        ...
        >>> s.add_index('posts', 'slug')
    
    Validate postgres schema.
    
        >>> print s.sql('postgres')
        create table posts (
            id serial primary key,
            slug character varying(255) unique not null,
            title character varying(255) not null,
            body text,
            created_on timestamp default (current_timestamp at time zone 'utc')
        );
        create table comments (
            id serial primary key,
            post_id int references posts(id),
            comment text
        );
        create index posts_slug_idx on posts(slug);
    
    Validate MySQL schema.
    
        >>> print s.sql('mysql')
        create table posts (
            id int auto_increment not null primary key,
            slug varchar(255) unique not null,
            title varchar(255) not null,
            body text,
            created_on datetime default UTC_TIMESTAMP
        );
        create table comments (
            id int auto_increment not null primary key,
            post_id int,
            comment text,
            foreign key (post_id) references posts(id)
        );
        create index posts_slug_idx on posts(slug);
    
    Thats all.
    """
    
if __name__ == "__main__":
    register_adapter('mock', MockAdapter)    
    
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = solr
"""Python library for accessing Solr.
"""
import urlparse
import urllib, urllib2
import re
import web
import simplejson

def urlencode(d, doseq=False):
    """There is a bug in urllib when used with unicode data.

        >>> d = {"q": u"\u0C05"}
        >>> urllib.urlencode(d)
        'q=%E0%B0%85'
        >>> urllib.urlencode(d, doseq=True)
        'q=%3F'

    This function encodes all the unicode strings in utf-8 before passing them to urllib.
    """
    def utf8(d):
        if isinstance(d, dict):
            return dict((utf8(k), utf8(v)) for k, v in d.iteritems())
        elif isinstance(d, list):
            return [utf8(v) for v in d]
        else:
            return web.safestr(d)

    return urllib.urlencode(utf8(d), doseq=doseq)

class Solr:
    def __init__(self, base_url):
        self.base_url = base_url
        self.host = urlparse.urlsplit(self.base_url)[1]

    def escape(self, query):
        r"""Escape special characters in the query string

            >>> solr = Solr("")
            >>> solr.escape("a[b]c")
            'a\\[b\\]c'
        """
        chars = r'+-!(){}[]^"~*?:\\'
        pattern = "([%s])" % re.escape(chars)
        return web.re_compile(pattern).sub(r'\\\1', query)

    def select(self, query, fields=None, facets=None,
               rows=None, start=None,
               doc_wrapper=None, facet_wrapper=None,
               **kw):
        """Execute a solr query.

        query can be a string or a dicitonary. If query is a dictionary, query
        is constucted by concatinating all the key-value pairs with AND condition.
        """
        params = {'wt': 'json'}

        for k, v in kw.items():
            # convert keys like facet_field to facet.field
            params[k.replace('_', '.')] = v

        params['q'] = self._prepare_select(query)

        if rows is not None:
            params['rows'] = rows
        params['start'] = start or 0

        if fields:
            params['fl'] = ",".join(fields)

        if facets:
            params['facet'] = "true"
            params['facet.field'] = []

            for f in facets:
                if isinstance(f, dict):
                    name = f.pop("name")
                    for k, v in f.items():
                        params["f.%s.facet.%s" % (name, k)] = v
                else:
                    name = f
                params['facet.field'].append(name)

        # switch to POST request when the payload is too big.
        # XXX: would it be a good idea to swithc to POST always?
        payload = urlencode(params, doseq=True)
        url = self.base_url + "/select"        
        if len(payload) < 500:
            url = url + "?" + payload
            data = urllib2.urlopen(url).read()
        else:
            data = urllib2.urlopen(url, payload).read()
        return self._parse_solr_result(
            simplejson.loads(data), 
            doc_wrapper=doc_wrapper, 
            facet_wrapper=facet_wrapper)

    def _parse_solr_result(self, result, doc_wrapper, facet_wrapper):
        response = result['response']

        doc_wrapper = doc_wrapper or web.storage
        facet_wrapper = facet_wrapper or (lambda name, value, count: web.storage(locals()))

        d = web.storage()
        d.num_found = response['numFound']
        d.docs = [doc_wrapper(doc) for doc in response['docs']]

        if 'facet_counts' in result:
            d.facets = {}
            for k, v in result['facet_counts']['facet_fields'].items():
                d.facets[k] = [facet_wrapper(k, value, count) for value, count in web.group(v, 2)]

        if 'highlighting' in result:
            d.highlighting = result['highlighting']

        if 'spellcheck' in result:
            d.spellcheck = result['spellcheck']

        return d

    def _prepare_select(self, query):
        def escape(v):
            # TODO: improve this
            return v.replace('"', r'\"').replace("(", "\\(").replace(")", "\\)")

        def escape_value(v):
            if isinstance(v, tuple): # hack for supporting range
                return "[%s TO %s]" % (escape(v[0]), escape(v[1]))
            elif isinstance(v, list): # one of 
                return "(%s)" % " OR ".join(escape_value(x) for x in v)
            else:
                return '"%s"' % escape(v)

        if isinstance(query, dict):
            op = query.pop("_op", "AND")
            if op.upper() != "OR":
                op = "AND"
            op = " " + op + " "

            q = op.join('%s:%s' % (k, escape_value(v)) for k, v in query.items())
        else:
            q = query
        return q

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = test_dateutil
from .. import dateutil
import datetime

def test_parse_date():
    assert dateutil.parse_date("2010") == datetime.date(2010, 1, 1)
    assert dateutil.parse_date("2010-02") == datetime.date(2010, 2, 1)
    assert dateutil.parse_date("2010-02-03") == datetime.date(2010, 2, 3)
    
def test_nextday():
    assert dateutil.nextday(datetime.date(2008, 1, 1)) == datetime.date(2008, 1, 2)
    assert dateutil.nextday(datetime.date(2008, 1, 31)) == datetime.date(2008, 2, 1)
    
    assert dateutil.nextday(datetime.date(2008, 2, 28)) == datetime.date(2008, 2, 29)
    assert dateutil.nextday(datetime.date(2008, 2, 29)) == datetime.date(2008, 3, 1)
    
    assert dateutil.nextday(datetime.date(2008, 12, 31)) == datetime.date(2009, 1, 1)
    
def test_nextmonth():
    assert dateutil.nextmonth(datetime.date(2008, 1, 1)) == datetime.date(2008, 2, 1)
    assert dateutil.nextmonth(datetime.date(2008, 1, 12)) == datetime.date(2008, 2, 1)
    
    assert dateutil.nextmonth(datetime.date(2008, 12, 12)) == datetime.date(2009, 1, 1)

def test_nextyear():
    assert dateutil.nextyear(datetime.date(2008, 1, 1)) == datetime.date(2009, 1, 1)
    assert dateutil.nextyear(datetime.date(2008, 2, 12)) == datetime.date(2009, 1, 1)

def test_parse_daterange():
    assert dateutil.parse_daterange("2010") == (datetime.date(2010, 1, 1), datetime.date(2011, 1, 1))
    assert dateutil.parse_daterange("2010-02") == (datetime.date(2010, 2, 1), datetime.date(2010, 3, 1))
    assert dateutil.parse_daterange("2010-02-03") == (datetime.date(2010, 2, 3), datetime.date(2010, 2, 4)) 
########NEW FILE########
__FILENAME__ = test_isbn
from .. import isbn

def test_isbn_13_to_isbn_10():
    assert isbn.isbn_13_to_isbn_10("978-0-940787-08-7") == "0940787083"
    assert isbn.isbn_13_to_isbn_10("9780940787087") == "0940787083"
    assert isbn.isbn_13_to_isbn_10("BAD-ISBN") is None
    
def test_isbn_10_to_isbn_13():
    assert isbn.isbn_10_to_isbn_13("0-940787-08-3") == "9780940787087"
    assert isbn.isbn_10_to_isbn_13("0940787083") == "9780940787087"
    assert isbn.isbn_10_to_isbn_13("BAD-ISBN") is None
########NEW FILE########
__FILENAME__ = test_processors
import web
import time

from ..processors import RateLimitProcessor

class TestRateLimitProcessor:
    """py.test testcase for testing RateLimitProcessor.
    """
    def setup_method(self, method):
        web.ctx.ip = "127.0.0.1"

    def test_check_rate(self, monkeypatch):
        monkeypatch.setattr(time, "time", lambda: 123456)
        p = RateLimitProcessor(10)

        for i in range(10):
            assert p.check_rate() == True
        assert p.check_rate() == False

    def test_get_window(self, monkeypatch):
        p = RateLimitProcessor(10, window_size=10)

        d = web.storage(time=1)
        monkeypatch.setattr(time, "time", lambda: d.time)

        # window should continue to be the same from time 1 to 9.
        w = p.get_window()
        w['foo'] = 'bar'

        d.time = 9
        assert p.get_window() == {'foo': 'bar'}

        # and the window should get cleared when time becomes 10.
        d.time = 10
        assert p.get_window() == {}

########NEW FILE########
__FILENAME__ = test_solr
from ..solr import Solr

def test_prepare_select():
    solr = Solr("http://localhost:8983/solr")
    assert solr._prepare_select("foo") == "foo"
        
    assert solr._prepare_select({"isbn": "1234567890"}) == 'isbn:"1234567890"'
    assert solr._prepare_select({"isbn": ["1234567890", "9876543210"]}) == 'isbn:("1234567890" OR "9876543210")'
    
    assert solr._prepare_select({"publish_year": ("1990", "2000")}) == 'publish_year:[1990 TO 2000]'
########NEW FILE########
__FILENAME__ = test_utils
# coding=utf-8
from openlibrary.utils import str_to_key, read_isbn, url_quote, finddict, escape_bracket

def test_isbn():
    assert read_isbn('x') is None
    assert read_isbn('1841151866') == '1841151866'
    assert read_isbn('184115186x') == '184115186x'
    assert read_isbn('184115186X') == '184115186X'
    assert read_isbn('184-115-1866') == '1841151866'
    assert read_isbn('9781841151861') == '9781841151861'
    assert read_isbn('978-1841151861') == '9781841151861'

def test_str_to_key():
    assert str_to_key('x') == 'x'
    assert str_to_key('X') == 'x'
    assert str_to_key('[X]') == 'x'
    assert str_to_key('!@<X>;:') == '!x'
    assert str_to_key('!@(X);:') == '!(x)'

def test_url_quote():
    assert url_quote('x') == 'x'
    result = url_quote(u'£20') 
    assert result == '%C2%A320'
    assert url_quote('test string') == 'test+string'

def test_finddict():
    dicts = [{"x": 1, "y": 2}, {"x": 3, "y": 4}]
    assert finddict(dicts, x=1) == {'x': 1, 'y': 2}

def test_escape_bracket():
    assert escape_bracket('test') == 'test'
    assert escape_bracket('this [is a] test') == 'this \\[is a\\] test'
    assert escape_bracket('aaa [10 TO 500] bbb') == 'aaa [10 TO 500] bbb'


########NEW FILE########
__FILENAME__ = loanstats
"""Loan Stats"""
import re
import datetime
import web

from .. import app
from ..core.loanstats import LoanStats

class stats(app.view):
    path = "/stats"

    def GET(self):
        raise web.seeother("/stats/lending")

re_time_period1 = re.compile("(\d+)days")
re_time_period2 = re.compile("(\d\d\d\d)(\d\d)(\d\d)-(\d\d\d\d)(\d\d)(\d\d)")

class lending_stats(app.view):
    path = "/stats/lending(?:/(libraries|regions|countries|collections|subjects|format)/(.+))?"

    def is_enabled(self):
        return "loanstats" in web.ctx.features

    def GET(self, key, value):
        stats = LoanStats()
        if key == 'libraries':
            stats.library = value
        elif key == 'regions':
            stats.region = value
        elif key == 'countries':
            stats.country = value
        elif key == 'collections':
            stats.collection = value
        elif key == 'subjects': 
            stats.subject = value
        elif key == 'subjects': 
            stats.subject = value
        elif key == 'format': 
            stats.resource_type = value

        i = web.input(t="30days")
        stats.time_period = self.parse_time(i.t)

        return app.render_template("stats/lending.html", stats)

    def parse_time(self, t):
        m = re_time_period2.match(t)
        if m:
            y0, m0, d0, y1, m1, d1 = m.groups()
            begin = datetime.datetime(int(y0), int(m0), int(d0))
            end = datetime.datetime(int(y1), int(m1), int(d1))
        else:
            m = re_time_period1.match(t)
            if m:
                delta_days = int(m.group(1))
            else:
                delta_days = 30
            now = datetime.datetime.utcnow()
            begin = now.replace(hour=0, minute=0, second=0, microsecond=0) - datetime.timedelta(days=delta_days)
            end = now
        return begin, end

########NEW FILE########
__FILENAME__ = showmarc
"""
Hook to show mark details in Open Library.
"""
from .. import app

import web
import urllib2
import os.path
import sys, re

class old_show_marc(app.view):
    path = "/show-marc/(.*)"

    def GET(self, param):
        raise web.seeother('/show-records/' + param)

class show_ia(app.view):
    path = "/show-records/ia:(.*)"

    def GET(self, ia):
        error_404 = False
        url = 'http://www.archive.org/download/%s/%s_meta.mrc' % (ia, ia)
        try:
            data = urllib2.urlopen(url).read()
        except urllib2.HTTPError, e:
            if e.code == 404:
                error_404 = True
            else:
                return "ERROR:" + str(e)

        if error_404: # no MARC record
            url = 'http://www.archive.org/download/%s/%s_meta.xml' % (ia, ia)
            try:
                data = urllib2.urlopen(url).read()
            except urllib2.HTTPError, e:
                return "ERROR:" + str(e)
            raise web.seeother('http://www.archive.org/details/' + ia)

        books = web.ctx.site.things({
            'type': '/type/edition',
            'source_records': 'ia:' + ia,
        }) or web.ctx.site.things({
            'type': '/type/edition',
            'ocaid': ia,
        })

        from openlibrary.catalog.marc import html

        try:
            leader_len = int(data[:5])
        except ValueError:
            return "ERROR reading MARC for " + ia

        if len(data) != leader_len:
            data = data.decode('utf-8').encode('raw_unicode_escape')
        assert len(data) == int(data[:5])

        try:
            record = html.html_record(data)
        except ValueError:
            record = None

        return app.render_template("showia", ia, record, books)
        
class show_amazon(app.view):
    path = "/show-records/amazon:(.*)"
    
    def GET(self, asin):
        return app.render_template("showamazon", asin)

re_bad_meta_mrc = re.compile('^([^/]+)_meta\.mrc$')
re_lc_sanfranpl = re.compile('^sanfranpl(\d+)/sanfranpl(\d+)\.out')

class show_marc(app.view):
    path = "/show-records/(.*):(\d+):(\d+)"
	
    def GET(self, filename, offset, length):
        m = re_bad_meta_mrc.match(filename)
        if m:
            raise web.seeother('/show-records/ia:' + m.group(1))
        m = re_lc_sanfranpl.match(filename)
        if m: # archive.org is case-sensative
            mixed_case = 'SanFranPL%s/SanFranPL%s.out:%s:%s' % (m.group(1), m.group(2), offset, length)
            raise web.seeother('/show-records/' + mixed_case)
        if filename == 'collingswoodlibrarymarcdump10-27-2008/collingswood.out':
            loc = 'CollingswoodLibraryMarcDump10-27-2008/Collingswood.out:%s:%s' % (offset, length)
            raise web.seeother('/show-records/' + loc)

        loc = ':'.join(['marc', filename, offset, length])

        books = web.ctx.site.things({
            'type': '/type/edition',
            'source_records': loc,
        })

        offset = int(offset)
        length = int(length)

        #print "record_locator: <code>%s</code><p/><hr>" % locator

        r0, r1 = offset, offset+100000
        url = 'http://www.archive.org/download/%s'% filename

        ureq = urllib2.Request(url,
                               None,
                               {'Range':'bytes=%d-%d'% (r0, r1)},
                               )

        try:        
            result = urllib2.urlopen(ureq).read(100000)
        except urllib2.HTTPError, e:
            return "ERROR:" + str(e)

        len_in_rec = int(result[:5])
        if len_in_rec != length:
            raise web.seeother('/show-records/%s:%d:%d' % (filename, offset, len_in_rec))

        from openlibrary.catalog.marc import html

        try:
            record = html.html_record(result[0:length])
        except ValueError:
            record = None

        return app.render_template("showmarc", record, filename, offset, length, books)

########NEW FILE########
__FILENAME__ = dbmirror
"""
Script to mirror the Open Library production databse by replaying the logs.
"""

import infogami
import web
import time

web.config.db_parameters = dict(dbn='postgres', db="pharos_backup", user='anand', pw='')
#web.config.db_printing = True

from infogami.infobase.logreader import LogReader, RsyncLogFile, LogPlayback
from infogami.infobase.infobase import Infobase
import datetime

def playback():
    web.load()
    reader = LogReader(RsyncLogFile("wiki-beta::pharos/log", "log"))

    # skip the log till the latest entry in the database
    timestamp = web.query('SELECT last_modified FROM thing ORDER BY last_modified DESC LIMIT 1')[0].last_modified
    reader.skip_till(timestamp)

    playback = LogPlayback(Infobase())

    while True:
        for entry in reader:
            print reader.logfile.tell(), entry.timestamp
            playback.playback(entry)

        time.sleep(60)

if __name__ == '__main__':
    playback()


########NEW FILE########
__FILENAME__ = jsondump
"""Script to generate json dumps.

The script deals with 3 data formats.

1. dump of data table in OL postgres database.
2. rawdump: tab seperated file with key, type and json of page in each row
3. bookdump: dump containing only books with each property expanded. (used for solr import)

"""
import sys
import simplejson
import re

commands = {}
def command(f):
    commands[f.__name__] = f
    return f

@command
def rawdump(datafile):
    """Generates a json dump from copy of data table from OL database.
    
    Usage:
    
        $ python jsondump.py rawdump datafile > dumpfile
    """
    write_rawdump(sys.stdout, read_json(read_data_table(datafile)))

@command
def merge(dump, idump):
    """Merges a large dump with increamental dump.

        $ python jsondump.py bigdump.txt dailydump.txt > bigdump2.txt
    """
    def read(path):
        for line in xopen(path):
            key, _ = line.split("\t", 1)[0]
            yield key, line

    def do_merge():
        d = make_dict(read(idump))
        for key, line in read(dump):
            yield d.pop(key, line)
    
    sys.stdout.writelines(do_merge())

@command
def json2rawdump(jsonfile):
    """Converts a file containing json rows to rawdump format.
    """
    write_rawdump(sys.stdout, read_json(jsonfile))

@command
def bookdump(rawdump):
    """Generates bookdump from rawdump.
    """
    pass

@command
def modified(db, date):
    """Display list of modified keys on a given day.
    
        $ python jsondump.py modified dbname YYYY-MM-DD
    """
    import os
    os.system("""psql %s -t -c "select key from thing where last_modified >= '%s' and last_modified < (date '%s' + interval '1 day')" """ % (db, date, date))

@command
def help(cmd=None):
    """Displays this help."""
    action = cmd and get_action(cmd)
    if action:
        print "python jsondump.py " + cmd
        print 
        print action.__doc__
    else:
        print __doc__
        print "List of commands:"
        print

        for k in sorted(commands.keys()):
            doc = commands[k].__doc__ or " "
            print "  %-10s\t%s" % (k, doc.splitlines()[0])

def get_action(cmd):
    if cmd in commands:
        return commands[cmd]
    else:
        print >> sys.stderr, "No such command:", cmd
        return help

def listget(x, i, default=None):
    try:
        return x[i]
    except IndexError:
        return default
    
def main():
    action = get_action(listget(sys.argv, 1, "help"))
    action(*sys.argv[2:])

#---
def make_sub(d):
    """
        >>> f = make_sub(dict(a='aa', bb='b'))
        >>> f('aabbb')
        'aaaabb'
    """
    def f(a):
        return d[a.group(0)]
    rx = re.compile("|".join(map(re.escape, d.keys())))
    return lambda s: s and rx.sub(f, s)

def invert_dict(d):
    return dict((v, k) for (k, v) in d.items())

_escape_dict = {'\n': r'\n', '\r': r'\r', '\t': r'\t', '\\': r'\\'}

escape = make_sub(_escape_dict)
unescape = make_sub(invert_dict(_escape_dict))

def doctest_escape():
    r"""
        >>> escape("\n\t")
        '\\n\\t'
        >>> unescape('\\n\\t')
        '\n\t'
    """

def read_data_table(path):
    r"""Read dump of postgres data table assuming that it is sorted by first column.
    
        >>> list(read_data_table(['1\t1\tJSON-1-1\n', '1\t2\tJSON-1-2\n', '2\t1\tJSON-2-1\n']))
        ['JSON-1-2\n', 'JSON-2-1\n']
        >>> list(read_data_table(['1\t1\tJSON\\t1-1\n']))
        ['JSON\t1-1\n']
    
    """
    xthing_id = 1 # assuming that thing_id starts from 1 to simplify the code
    xrev = 0
    xjson = ""

    for line in xopen(path):
        thing_id, rev, json = line.split("\t")
        thing_id = int(thing_id)
        rev = int(rev)
        if xthing_id == thing_id:
            # take the json with higher rev.
            if rev > xrev:
                xrev = rev
                xjson = json
        else:
            yield unescape(xjson)
            xthing_id = thing_id
            xrev = rev
            xjson = json

    yield unescape(xjson)

def read_rawdump(file):
    r"""
        >>> list(read_rawdump(["/foo\t/type/page\tfoo-json\n", "/bar\t/type/doc\tbar-json\n"]))
        [['/foo', '/type/page', 'foo-json\n'], ['/bar', '/type/doc', 'bar-json\n']]
    """
    return (line.split("\t", 2) for line in xopen(file))

def write_rawdump(file, data):
    # assuming that newline is already present in json (column#3).
    file.writelines("%s\t%s\t%s" % row for row in data)

def read_json(file):
    r"""
        >>> list(read_json(['{"key": "/foo", "type": {"key": "/type/page"}, "title": "foo"}\n']))
        [('/foo', '/type/page', '{"key": "/foo", "type": {"key": "/type/page"}, "title": "foo"}\n')]
    """
    for json in xopen(file):
        d = simplejson.loads(json)        
        yield d['key'], d['type']['key'], json

def xopen(file):
    if isinstance(file, str):
        return open(file)
    else:
        return file

def make_dict(items):
    return dict(items)

def capture_stdout(f):
    import StringIO
    def g(*a):
        stdout, sys.stdout = sys.stdout, StringIO.StringIO()
        f(*a)
        out, sys.stdout = sys.stdout.getvalue(), stdout
        return out
    return g
        
@command
def test(*args):
    r"""Test this module.

        >>> 1 + 1
        2


    """
    sys.argv = args
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = make
import os

def system(cmd):
    print cmd
    os.system(cmd)

def copy_src():
    system('cp -r src/* dist/openlibrary')

def copy_gnubook():
    if os.path.exists('dist/bookreader'):
        system('cd dist/bookreader && git pull')
    else:
        system('git clone http://github.com/anandology/bookreader.git dist/bookreader > /dev/null')

    system('cp -r dist/bookreader/GnuBook dist/openlibrary/')

def copy_books():
    def copy(id):
        system('cd dist/downloads && wget -nv http://www.archive.org/download/%s/%s_flippy.zip' % (id, id))
        system('mkdir -p dist/openlibrary/books/' + id)
        system('unzip -u dist/downloads/%s_flippy.zip -d dist/openlibrary/books/%s > /dev/null' % (id, id))

    for id in open('books.index').read().split():
        copy(id)

def make_books_js():
    books = open('books.index').read().split()
    print 'creating dist/openlibrary/js/books.js'
    f = open('dist/openlibrary/js/books.js', 'w')
    f.write('var books = ')
    f.write(repr(books));
    f.write(';\n');
    f.close()

def main():
    system('mkdir -p dist/openlibrary dist/downloads')
    copy_src()
    copy_gnubook()
    copy_books()
    make_books_js()
    system('cd dist && zip -r openlibrary.xol openlibrary > /dev/null')
    print
    print "Activity file generated at: dist/openlibrary.xol"
    
if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = get_scan_records
import simplejson
import urllib
import os
import sys
import datetime

#base_url = "http://openlibrary.org/api"
base_url = "http://pharosdb:7070/openlibrary.org"

def wget(url):
    #print url
    return urllib.urlopen(url).read()

def things(query):
    if 'limit' not in query:
        query = dict(query, limit=1000)

    _query = simplejson.dumps(query)
    result = wget(base_url + '/things?' + urllib.urlencode(dict(query=_query)))
    result = simplejson.loads(result)['result']

    if len(result) < 1000:
        return result
    else:
        return result + things(dict(query, offset=query.get('offset', 0) + 1000))
    
def get_many(keys):
    def f(keys):
        keys = simplejson.dumps(keys)
        result = wget(base_url + '/get_many?' + urllib.urlencode(dict(keys=keys)))
        return simplejson.loads(result)['result']

    d = {}
    while keys:
        d.update(f(keys[:100]))
        keys = keys[100:]
    return d

def versions(query):
    query = simplejson.dumps(query)
    result = wget(base_url + '/versions?' + urllib.urlencode(dict(query=query)))
    return simplejson.loads(result)['result']

def write(path, data):
    print 'writing', path
    dir = os.path.dirname(path)
    if dir and not os.path.exists(dir):
        os.makedirs(dir)

    f = open(path, 'w')
    f.write(data)
    f.close()

def fill_reasons(records):
    """For books with status BOOK_NOT_SCANNED, fill the reasons."""
    def get_title(e):
        if 'title_prefix' in e:
            return e['title_prefix'] + ' ' + e['title']
        else:
            return e['title']

    records = [r for r in records if r['scan_status'] == 'BOOK_NOT_SCANNED']
    editions = get_many([r['edition']['key'] for r in records])

    for r in records:
        comment = versions({'key': r['key'], 'sort': '-revision', 'limit': 1})[0]['comment']
        title = get_title(editions[r['edition']['key']])
        r['comment'] = {'reason': comment, 'title': title}

def get_scan_records(last_modified):
    """Download all scan records"""
    q = {'type': '/type/scan_record'}
    if last_modified:
        q['last_modified>'] = last_modified

    records = get_many(things(q))
    fill_reasons(records.values())

    for k, r in records.items():
        write('data' + k + '.json', simplejson.dumps(r))

def main():
    t = datetime.datetime.utcnow().isoformat()

    if os.path.exists('last_updated.txt'):
        last_updated = open('last_updated.txt').read().strip()
    else:
        last_updated = None

    get_scan_records(last_updated)
    write('last_updated.txt', t)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = make_report
"""
Generate scod report.
"""
import simplejson
import os

def parse():
    for f in os.listdir('data/scan_record/b'):
        path = 'data/scan_record/b/' + f
        d = simplejson.loads(open(path).read())
        status = d['scan_status']
        if status in ['WAITING_FOR_BOOK', 'SCAN_IN_PROGRESS', 'BOOK_NOT_SCANNED', 'SCAN_COMPLETE']:
            if d.get('request_date'):
                yield 'request', d['request_date'], d

        if status == 'SCAN_COMPLETE':
            if 'completion_date' in d:
                yield 'complete', d['completion_date'], d

        if status == 'BOOK_NOT_SCANNED':
            yield 'notfound', d['last_modified']['value'], d

def main():
    data = {}
    for action, date, d in parse():
        date = date[:10] # keep only YYYY-MM-DD
        data.setdefault(date, dict(request=[], complete=[], notfound=[]))[action].append(d)

    for k in sorted(data.keys()):
        d = data[k]
        reasons = [r['comment'] for r in d['notfound']]
        print "%s\t%s\t%s\t%s\t%s" % (k, len(d['request']), len(d['complete']), len(d['notfound']), reasons)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = index
"""Script to generate HTML index of openlibrary.org website.
"""
import gzip
import os
import itertools
import web

def read_titles(filename):
    data = []
    for line in open(filename):
        id, title = line.strip().split('\t', 1)
        id = int(id)
        while len(data) <= id:
            data.extend(itertools.repeat(None, 1000000))
        data[id] = title
    return data

def read(titles_file, keys_file):
    titles = read_titles(titles_file)

    for line in open(keys_file):
        id, key, _ = line.split('\t', 2)
        id = int(id)
        title = titles[id] or key
        yield key, title

def take(n, seq):
    for i in xrange(n):
        yield seq.next()

def group(seq, n):
    while True:
        x = list(take(n, seq))
        if x: 
            yield x
        else:
            break

t_sitemap = """$def with (title, items)
<html>
<head><title>$title</title><head>
<body>

<h1>Books</h1>
<a href="../index.html">Back</a> | <a href="../../index.html">Back to index</a>
<ul>
$for key, title in seq:
    <li><a href="$key">$title</a></li>
</ul>
<a href="../index.html">Back</a> | <a href="../../index.html">Back to index</a>
</body>
</html>
"""

t_index = """$def with (title, files)
<html>
<head><title>$title</title><head>
<body>

<h1>$title</h1>

$if title != "index":
    <a href="../index.html">Back to index</a>

<ul>
$for key, title in seq:
    <li><a href="$key">$title</a></li>
</ul>

$if title != "index":
    <a href="../index.html">Back to index</a>
</body>
</html>
"""

make_sitemap = web.template.Template(t_sitemap)
make_index = web.template.Template(t_index)

def write(filename, text):
    f = open(filename, 'w')
    f.write(text)
    f.close()

def write_sitemap(i, seq):
    dir = 'index/%02d' % (i/1000)
    filename = "%s/index_%05d.html" % (dir, i)
    if not os.path.exists(dir):
        os.mkdir(dir)
    print filename
    write(filename, make_sitemap(filename, seq))

def write_sitemaps(data):
    for i, x in enumerate(group(data, 1000)):
        write_sitemap(i, x)

def main():
    import sys
    data = read(sys.argv[1], sys.argv[2])
    write_indexes(data)

    dirs = os.listdir('index'):
    write('index/index.html', make_index('index', dirs))

    for d in dirs:
        d = os.path.join('index', d)
        write(d + '/index.html', make_index('index', os.listdir(d))

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = sitemap
"""Script to generate XML sitemap of openlibrary.org website.

USAGE:
    
    python sitemaps.py suffix dump.txt.gz
"""

import web
import os
import itertools
import datetime
import gzip
#import json
import re
import time

t_sitemap = """$def with (things)
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    $for t in things:
    <url>
        <loc>https://openlibrary.org$t.path</loc>
        <lastmod>$t.last_modified</lastmod>
    </url>
</urlset>
"""

t_siteindex = """$def with (names, timestamp)
<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    $for name in names:
    <sitemap>
        <loc>https://openlibrary.org/static/sitemaps/$name</loc>
        <lastmod>$timestamp</lastmod>
    </sitemap>
</sitemapindex>
"""

sitemap = web.template.Template(t_sitemap)
siteindex = web.template.Template(t_siteindex)

def xopen(filename):
    if filename.endswith(".gz"):
        return gzip.open(filename)
    else:
        return open(filename)

def process_dump(dumpfile):
    """Generates a summary file used to generate sitemaps.

    The summary file contains: sort-key, path and last_modified columns.
    """
    rows = (line.strip().split("\t") for line in xopen(dumpfile))
    for type, key, revision, last_modified, jsontext in rows:
        if type not in ['/type/edition', '/type/work', '/type/author']:
            continue

        """
        doc = json.loads(jsontext)
        if type == '/type/author':
            title = doc.get('name', 'unnamed')
        else:
            title = doc.get('title', 'untitled')
        
        path = key + "/" + h.urlsafe(title.strip())
        """
        path = key
        last_modified = last_modified.replace(' ', 'T') + 'Z'
        sortkey = get_sort_key(key)
        if sortkey:
            yield [sortkey, path.encode('utf-8'), last_modified]

re_key = re.compile("^/(authors|books|works)/OL\d+[AMW]$")

def get_sort_key(key):
    """Returns a sort key used to group urls in 10K batches.

    >>> get_sort_key("/books/OL12345678M")
    'books_1234'
    >>> get_sort_key("/authors/OL123456A")
    'authors_0012'
    """
    m = re_key.match(key)
    if not m:
        return
    prefix = m.group(1)
    num = int(web.numify(key)) / 10000
    return "%s_%04d" % (prefix, num)

def generate_sitemaps(filename):
    rows = (line.strip().split("\t") for line in open(filename))
    for sortkey, chunk in itertools.groupby(rows, lambda row: row[0]):
        things = [web.storage(path=path, last_modified=last_modified) for sortkey, path, last_modified in chunk]
        if things:
            write("sitemaps/sitemap_%s.xml.gz" % sortkey, sitemap(things))

def generate_siteindex():
    filenames = sorted(os.listdir("sitemaps"))
    if "siteindex.xml.gz" in filenames:
        filenames.remove("siteindex.xml.gz")
    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S') + 'Z'
    index = siteindex(filenames, timestamp)
    write("sitemaps/siteindex.xml.gz", index)

def write(path, text):
    text = web.safestr(text)
    log('writing', path, text.count('\n'))
    f = gzip.open(path, 'w')
    f.write(text)
    f.close()
    #os.system("gzip " + path)

def write_tsv(path, rows):
    lines = ("\t".join(row) + "\n" for row in rows)
    f = open(path, "w")
    f.writelines(lines)
    f.close()

def system_memory():
    """Returns system memory in MB."""
    try:
        x = os.popen("cat /proc/meminfo | grep MemTotal | sed 's/[^0-9]//g'").read()
        # proc gives memory in KB, converting it to MB
        return int(x)/1024
    except IOError:
        # default to 1024MB
        return 1024

def system(cmd):
    log("executing:", cmd)
    status = os.system(cmd)
    if status != 0:
        raise Exception("%r failed with exit status: %d" % (cmd, status))

def log(*args):
    msg = " ".join(map(str, args))
    print time.asctime(), msg

def main(dumpfile):
    system("rm -rf sitemaps sitemaps_data.txt*; mkdir sitemaps")

    log("processing the dump")
    rows = process_dump(dumpfile)
    write_tsv("sitemaps_data.txt", rows)

    log("sorting sitemaps_data.txt")
    # use half of system of 3GB whichever is smaller
    sort_mem = min(system_memory()/2, 3072)
    system("sort -S%dM sitemaps_data.txt > sitemaps_data.txt.sorted" % sort_mem)

    log("generating sitemaps")
    generate_sitemaps("sitemaps_data.txt.sorted") 
    generate_siteindex()

    log("done")

if __name__ == "__main__":
    import sys
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = add_language
from catalog.utils.query import query_iter, set_staging, withKey, get_mc
import sys, codecs, re
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, Reference
from catalog.read_rc import read_rc
from catalog.get_ia import get_from_archive, get_from_local
from catalog.marc.fast_parse import get_first_tag, get_all_subfields
rc = read_rc()

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)
set_staging(True)

ol = OpenLibrary("http://dev.openlibrary.org")
ol.login('EdwardBot', rc['EdwardBot'])

q = { 'type': '/type/edition', 'table_of_contents': None, 'subjects': None }
queue = []
count = 0
for e in query_iter(q, limit=100):
    key = e['key']
    mc = get_mc(key)
    if not mc:
        continue
    data = get_from_local(mc)
    line = get_first_tag(data, set(['041']))
    if not line:
        continue
    print key, line[0:2], list(get_all_subfields(line))


########NEW FILE########
__FILENAME__ = add_oclc
import web, sys, codecs, re, urllib2
from catalog.get_ia import read_marc_file
from catalog.read_rc import read_rc
from catalog.marc.fast_parse import get_tag_lines, get_all_subfields, get_first_tag
from catalog.marc.new_parser import read_edition
from catalog.utils.query import query_iter
from catalog.marc.utils import files
sys.path.append('/home/edward/src/olapi')
from olapi import OpenLibrary, unmarshal
import simplejson as json
from catalog.importer.load import build_query, east_in_by_statement, import_author

rc = read_rc()
marc_index = web.database(dbn='postgres', db='marc_index')
marc_index.printing = False

ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

#ocm04775229
re_oclc = re.compile ('^oc[mn]0*(\d+)$')

def get_keys(loc):
    assert loc.startswith('marc:')
    vars = {'loc': loc[5:]}
    db_iter = marc_index.query('select k from machine_comment where v=$loc', vars)
    mc = list(db_iter)
    if mc:
        return [r.k for r in mc]
    iter = query_iter({'type': '/type/edition', 'source_records': loc})
    return [e['key'] for e in iter]

re_meta_mrc = re.compile('^([^/]*)_meta.mrc:0:\d+$')

def fix_toc(e):
    toc = e.get('table_of_contents', None)
    if not toc:
        return
    if isinstance(toc[0], dict) and toc[0]['type'] == '/type/toc_item':
        return
    return [{'title': unicode(i), 'type': '/type/toc_item'} for i in toc if i != u'']

re_skip = re.compile('\b([A-Z]|Co|Dr|Jr|Capt|Mr|Mrs|Ms|Prof|Rev|Revd|Hon)\.$')

def has_dot(s):
    return s.endswith('.') and not re_skip.search(s)

def undelete_authors(key):
    a = ol.get(key)
    if a['type'] == '/type/author':
        return
    assert a['type'] == '/type/delete'
    url = 'http://openlibrary.org' + key + '.json?v=' + str(a['revision'] - 1)
    prev = unmarshal(json.load(urllib2.urlopen(url)))
    assert prev['type'] == '/type/author'
    ol.save(key, prev, 'undelete author')

def author_from_data(loc, data):
    edition = read_edition(loc, data)
    assert 'authors' in edition
    east = east_in_by_statement(edition)
    assert len(edition['authors']) == 1
    print `edition['authors'][0]`
    a = import_author(edition['authors'][0], eastern=east)
    if 'key' in a:
        return {'key': a['key']}
    ret = ol.new(a, comment='new author')
    print 'ret:', ret
    assert isinstance(ret, basestring)
    return {'key': ret}


def add_oclc(key, sr, oclc, data):
    assert sr and oclc
    e = ol.get(key)
    if 'oclc_numbers' in e:
        return
    e['oclc_numbers'] = [oclc]
    if 'source_records' not in e:
        e['source_records'] = [sr]

    # fix other bits of the record as well
    new_toc = fix_toc(e)
    if new_toc:
        e['table_of_contents'] = new_toc
    if e.get('subjects', None) and any(has_dot(s) for s in e['subjects']):
        subjects = [s[:-1] if has_dot(s) else s for s in e['subjects']]
        e['subjects'] = subjects
    if 'authors' in e:
        if any(a=='None' for a in e['authors']):
            assert len(e['authors']) == 1
            new_author = author_from_data(sr, data)
            e['authors'] = [new_author]
        else:
            for a in e['authors']:
                undelete_authors(a)

    print ol.save(key, e, 'add OCLC number')
    if new_toc:
        new_edition = ol.get(key)
        # [{u'type': <ref: u'/type/toc_item'>}, ...]
        assert 'title' in new_edition['table_of_contents'][0]

skipping = True
for name, part, size in files():
    f = open(name)
    print part
    if skipping:
        if part != 'marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc':
            print 'skipping'
            continue
    for pos, loc, data in read_marc_file(part, f):
        if skipping:
            if loc.startswith('marc:marc_western_washington_univ/wwu_bibs.mrc_revrev.mrc:668652795:1299'):
                skipping = False
            continue

        if str(data)[6:8] != 'am': # only want books
            continue
        tag_003 = get_first_tag(data, ['003'])
        if not tag_003 or not tag_003.lower().startswith('ocolc'):
            continue
        oclc = get_first_tag(data, ['001'])
        if not oclc:
#            print get_first_tag(data, ['010'])
            continue
        assert oclc[-1] == '\x1e'
        oclc = oclc[:-1].strip()
        if not oclc.isdigit():
            m = re_oclc.match(oclc)
            if not m:
                print "can't read:", `oclc`
                continue
            oclc = m.group(1)
        keys = get_keys(loc)
        if not keys:
            continue
        print loc, keys, oclc
        for key in keys:
            add_oclc(key, loc, oclc, data)

########NEW FILE########
__FILENAME__ = check_scribe_vol
import web, re, urllib2, sys
from catalog.read_rc import read_rc
from catalog.get_ia import find_item
import xml.etree.ElementTree as et

re_ia_vol = re.compile('[^0-9](\d{2,3})[^0-9]+$')
re_txt_vol = re.compile(r'\bVOL(?:UME)?\b', re.I)

rc = read_rc()

def get_vol_from_xml(base, ia):
    meta = base + '/' + ia + '_meta.xml'
    try:
        tree = et.parse(urllib2.urlopen(meta))
    except urllib2.HTTPError:
        return None
    except urllib2.URLError:
        return None
    for e in tree.getroot():
        if e.tag =='volume':
            return e.text
    return None

def check_text(base, ia, vol):
    url = base + '/' + ia + '_djvu.txt'
    ureq = urllib2.Request(url, None, {'Range':'bytes=0-5000'})
    return urllib2.urlopen(ureq).read()
    for line in urllib2.urlopen(ureq):
        if re_txt_vol.search(line):
            return line[:-1]

def vol_ia():
    assert False
    ia_db = web.database(dbn='mysql', db='archive', user=rc['ia_db_user'], pw=rc['ia_db_pass'], host=rc['ia_db_host'])
    ia_db.printing = False
    iter = ia_db.query("select identifier from metadata where scanner is not null and scanner != 'google' and noindex is null and mediatype='texts' and curatestate='approved'")
    out = open('vol_ia', 'w')
    print 'start iter'
    for row in iter:
        ia = row.identifier
        m = re_ia_vol.search(ia)
        if not m or m.group(1) == '00':
            continue
        print >> out, ia
    out.close()
    sys.exit(0)

out = open('vol_check3', 'w')
skip = True
for line in open('vol_ia'):
    ia = line[:-1]
    if skip:
        if ia == 'bengalin175657se02hilluoft':
            skip = False
        continue
    m = re_ia_vol.search(ia)
    if not m or m.group(1) == '00':
        continue
    (host, path) = find_item(ia)
    print ia
    if not host or not path:
        continue
    base = "http://" + host + path
    vol = get_vol_from_xml(base, ia)
    if not vol or not vol.isdigit():
        continue
    try:
#        vol_line = check_text(base, ia, vol)
        txt = check_text(base, ia, vol)
    except urllib2.HTTPError:
        continue
    except urllib2.URLError:
        continue
    #print >> out, (vol, ia, vol_line)
    print >> out, `(vol, ia, txt)`
out.close()

########NEW FILE########
__FILENAME__ = Abraham_ibn_Daud
#!/usr/bin/python
from openlibrary.catalog.marc.fast_parse import get_subfields
from openlibrary.catalog.wikipedia.lookup import name_lookup, look_for_match, pick_from_match, more_than_one_match
from openlibrary.catalog.utils import pick_first_date
from pprint import pprint

marc = [
    '  \x1faAbraham ben David,\x1fcha-Levi,\x1fdca. 1110-ca. 1180\x1e',
    '  \x1faAbraham ben David,\x1fcha-Lev\xe2i,\x1fdca.1110-ca.1180\x1e',
    '  \x1faIbn Daud, Abraham ben David,\x1fcha-Levi,\x1fdca. 1100-1180\x1e',
    '  \x1faIbn Daud, Abraham ben David,\x1fcHalevi,\x1fdca. 1110-ca. 1180\x1e',
]

# wiki names: abraham ibn daud; abraham ibn david; ibn daub; ibn daud; avraham ibn daud

def test_lookup():
    for line in marc:
        fields = tuple((k, v.strip(' /,;:')) for k, v in get_subfields(line, 'abcd'))
        found = name_lookup(fields)
        for i in found:
            print i
        dates = pick_first_date(v for k, v in fields if k == 'd')
        print dates
        match = look_for_match(found, dates, False)
        print len(match)
        for i in match:
            print i
        #pprint(match)
        if len(match) != 1:
            match = pick_from_match(match)
        if len(match) != 1:
            for i in more_than_one_match(match):
                print i
        print

test_lookup()

########NEW FILE########
__FILENAME__ = auto
from openlibrary.catalog.utils.query import query, withKey
from openlibrary.catalog.importer.update import add_source_records

for num, line in enumerate(open('/1/edward/imagepdf/possible_match2')):
    doc = eval(line)
    if 'publisher' not in doc:
        continue
    item_id = doc['item_id']
    if query({'type':'/type/edition','source_records':'ia:' + item_id}):
        continue
    e = withKey(doc['ol'])
    if 'publishers' not in e:
        continue
    title_match = False
    if doc['title'] == e['title']:
        title_match = True
    elif doc['title'] == e.get('title_prefix', '') + e['title']:
        title_match = True
    elif doc['title'] == e.get('title_prefix', '') + e['title'] + e.get('subtitle', ''):
        title_match = True
    elif doc['title'] == e['title'] + e.get('subtitle', ''):
        title_match = True
    if not title_match:
        continue
    if doc['publisher'] != e['publishers'][0]:
        continue
    print 'match:', item_id, doc['ol']
    add_source_records(doc['ol'], item_id)


########NEW FILE########
__FILENAME__ = delete_spam_users
"""
Earlier to block all the spam users, user's key has been prefixed with SPAM. That broke log-replay.

This script removes all the SPAM/* users to restore log-replay.
"""

import sys

def main(database):
    db = web.database(dbn='postgres', db=database, user=os.getenv('USER'), pw='')

    t = db.transaction()
    ids = [r.id for r in db.select('thing', where="key LIKE $pat", vars={'pat': 'SPAM%'})]
    
    db.update("transaction", author=None, where="author_id in $ids", vars=locals())
    db.update("version", author=None, where="author_id in $ids", vars=locals())

    for table in ['account', 'data', 'user_str', 'user_ref', 'user_int', 'user_boolean', 'user_float']:
        db.delete(table, where="thing_id in $ids", vars=locals())

    db.delete('thing', where='id in $ids', vars=locals())
    t.commit()

if __name__ == "__main__":
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = tarindex
#! /usr/bin/env python
import sys
import tarfile

def get_index(tar):
    for tarinfo in tar.getmembers():
        yield "%s\t%s\t%s\n" % (tarinfo.name, tarinfo.offset_data, tarinfo.size)

for f in sys.argv[1:]:
    print f
    t = tarfile.TarFile(f)
    out = open(f.replace('.tar', '.index'), 'w')
    out.writelines(get_index(t))
    out.close()
########NEW FILE########
__FILENAME__ = tarsplit
"""In the process of moving coverstore images into tar files, 
one tar file for each warc file has been created with original, 
small, medium and large images in them.

This script splits them into separate tar files each containing only
one type of images and contain exactly 10K images.
"""
import sys
import tarfile
import web
import os.path

logfile = open('log.txt', 'a')

def log(*args):
    msg = " ".join(args)
    print msg
    print >> logfile, msg
    logfile.flush()

class TarManager:
    def __init__(self):
        self.tarfiles = {}
        self.tarfiles[''] = (None, None)
        self.tarfiles['S'] = (None, None)
        self.tarfiles['M'] = (None, None)
        self.tarfiles['L'] = (None, None)

    def get_tarfile(self, name):
        id = web.numify(name)
        prefix = "covers_%s_%s" % (id[:4], id[4:6])

        if '-' in name:
            size = name.split('-')[1].split('.')[0]
            tarname = size.lower() + "_" + prefix + ".tar"
        else:
            size = ''
            tarname = prefix + ".tar"

        _tarname, _tarfile = self.tarfiles[size]
        if _tarname != tarname:
            _tarname and _tarfile.close()
            _tarfile = self.open_tarfile(tarname)
            self.tarfiles[size] = tarname, _tarfile
            log('writing', tarname)

        return _tarfile

    def open_tarfile(self, name):
        path = "out/" + name
        if os.path.exists(path):
            log('WARNING: Appending to ', path)
            return tarfile.TarFile(path, 'a')
        else:
            return tarfile.TarFile(path, 'w')

    def add_file(self, name, fileobj, mtime):
        tarinfo = tarfile.TarInfo(name)
        tarinfo.mtime = mtime
        tarinfo.size = fileobj.size
        
        self.get_tarfile(name).addfile(tarinfo, fileobj=fileobj)

    def close(self):
        for name, _tarfile in self.tarfiles.values():
            if name:
                print 'closing', name
                _tarfile.close()

def main(files):
    tar_manager = TarManager()

    for f in files:
        log("reading ", f)
        # open in append mode and close it to make sure EOF headers are written correctly.
        #_tarfile = tarfile.TarFile(f, 'a')
        #_tarfile.close()
        
        _tarfile = tarfile.TarFile(f)
        try:
            for tarinfo in _tarfile:
                name = tarinfo.name
                mtime = tarinfo.mtime
                fileobj = _tarfile.extractfile(tarinfo)

                if name.endswith('-O.jpg'):
                    name = name.replace('-O', '')

                tar_manager.add_file(name, fileobj, mtime)
        except Exception, e:
            import traceback
            traceback.print_exc()
            log("Error", str(e))

    logfile.close()
    tar_manager.close()
            
if __name__ == "__main__":
    main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = thumbnail
"""On disk datastructure for storing thumbnail images on disk.

Usage:

    $ python thumbnail.py f1.warc f2.warc f3.warc
"""
import sys, os
import Image
from cStringIO import StringIO
import time
import tarfile
import web

import warc

def makedirs(d):
    if not os.path.exists(d):
        os.makedirs(d)

def make_thumbnail(record):
    """Make small and medium thumbnails of given record."""
    id = record.get_header().subject_uri.split('/')[-1].split('.')[0]
    id = "%010d" % int(id)
    path = "/".join([id[0:3], id[3:6], id[6:9]])

    data = record.get_data()
    image = Image.open(StringIO(data))

    sizes = dict(S=(116, 58), M=(180, 360), L=(500, 500))

    yield id + "-O.jpg", data

    for size in "SML":
        imgpath = "%s-%s.jpg" % (id, size)
        try:
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            thumbnail = StringIO()
            image.resize(sizes[size], resample=Image.ANTIALIAS).save(thumbnail, format='jpeg')
            yield imgpath, thumbnail.getvalue()
        except Exception, e:
            print 'ERROR:', id, str(e)
            sys.stdout.flush()

def add_image(tar, name, data, mtime=None):
    tarinfo = tarfile.TarInfo(name)
    tarinfo.mtime = mtime or int(time.time())
    tarinfo.size = len(data)
    file = StringIO(data)
    tar.addfile(tarinfo, file)

def read(warc_files):
    for w in warc_files:
        print time.asctime(), w
        sys.stdout.flush()
        reader = warc.WARCReader(open(w))
        for r in reader.read():
            yield r

def process(warcfile, dir):
    outpath = dir + '/' + os.path.basename(warcfile).replace('.warc', '.tar')
    print 'process', warcfile, dir, outpath
    tar = tarfile.TarFile(outpath, 'w')

    def f(records):
        for r in records:
            timestamp = time.mktime(time.strptime(r.get_header().creation_date, '%Y%m%d%H%M%S'))
            try:
                for name, data in make_thumbnail(r):
                    yield name, data, timestamp
            except Exception, e:
                print >> sys.stderr, str(e)

    for name, data, timestamp in f(read([warcfile])):
        add_image(tar, name, data, mtime=timestamp)
    tar.close()

def main(dir, *warc_files):
    if not os.path.exists(dir):
        os.makedirs(dir)
    for w in warc_files:
        process(w, dir)

if __name__ == "__main__":
    main(*sys.argv[1:])

########NEW FILE########
__FILENAME__ = warc
../../../openlibrary/coverstore/warc.py
########NEW FILE########
__FILENAME__ = link_scan_records
"""Because edition object doesn't have scan_records property, an
infobase query is made to check whether there exists a scan_record for
that edition. This is done for rendering each and every edition
page. This query can be eliminated of scan_records are linked to the
edition pages. By doing so, only the editions which has scan_records
will make additional query.

This script finds all scan_record objects in the system and links them to corresponding edition object.

USAGE:

    python link_scan_records.py http://dev.openlibrary.org
"""

import _init_path
from openlibrary.api import OpenLibrary

def take(n, items):
    def f(items):
        for i in range(n):
            yield items.next()
    return list(f(iter(items)))

def main(server):
    ol = OpenLibrary(server)
    ol.autologin()

    scan_records = ol.query(type='/type/scan_record', limit=False, edition={'*': None})
    editions = (r['edition'] for r in scan_records)

    # process 1000 editions at a time.
    while True:
        chunk = take(1000, editions)
        if not chunk:
            break

        print 'linking %d editions' % len(chunk)
        
        for e in chunk:
            e['scan_records'] = [{'key': '/scan_record' + e['key']}]
            
        ol.save_many(chunk, 'link scan records')

if __name__ == "__main__":
    import sys
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = link_volumes
"""Because edition objects has volumes as back-reference instead of
property, every time an edition page is rendered an infobase query is
made to find the volumes.

This script adds all volumes as embeddable objects in the edition
itself. Please make sure you have marked /type/volume as embeddable
before running this script.

USAGE:

    python link_volumes.py http://dev.openlibrary.org
"""
import _init_path
from openlibrary.api import OpenLibrary

def take(n, items):
    def f(items):
        for i in range(n):
            yield items.next()
    return list(f(iter(items)))

def main(server):
    ol = OpenLibrary(server)
    ol.autologin()

    volumes = ol.query({'type': '/type/volume', 'limit': False, '*': None, 'edition': {'*': None}})

    volumes = dict((v['key'], v) for v in volumes)
    editions = dict((v['edition']['key'], v['edition']) for v in volumes.values() if v['edition'])

    def make_volume(v):
        d = {}
        v.pop('edition')
        v['type'] = {'key': '/type/volume'}
        for k in ['type', 'ia_id', 'volume_number']:
            if k in v:
                d[k] = v[k]
        return d

    for e in editions.values():
        e['volumes'] = []

    for v in volumes.values():
        if v.get('edition'):
            e = editions[v.get('edition')['key']]
            e['volumes'].append(make_volume(v))

    for e in editions.values():
        e['volumes'] = sorted(e['volumes'], key=lambda v: v['volume_number'])

    print 'linking volumes to %d editions' % len(editions)
    ol.save_many(editions.values(), 'link volumes')
    
if __name__ == "__main__":
    import sys
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = _init_path
../../_init_path.py
########NEW FILE########
__FILENAME__ = couchload
"""Load couchdb with OL data.
"""

import sys
import time
import couchdb

def group(items, size):
    """
        >>> list(group(range(7), 3))
        [[0, 1, 2], [3, 4, 5], [6]]
    """
    items = iter(items)
    while True:
        d = list(items.next() for i in xrange(size))
        if d:
            yield d
        else:
            break

def main(dbname, filename):
    server = couchdb.Server("http://localhost:5984/")
    db = server[dbname]
    
    t0 = time.time()
    for i, chunk in enumerate(group(open(filename), 1000)):
        if i%100 == 0:
            t1 = time.time()
            print i, "%.3f" % (t1-t0)
            t0 = t1

        json = '{"docs": [' + ",".join(chunk) +']}'
        print db.resource.post('_bulk_docs', content=json)

    print 'done'
    
if __name__ == '__main__':
    if len(sys.argv) == 1:
        import doctest
        doctest.testmod()
    else:
        main(sys.argv[1], sys.argv[2])

########NEW FILE########
__FILENAME__ = dbstats
"""Postgres performance stats.
"""
import time
import web
import sys
import random

M = 1000000

def _timeit(db, n=1000):
    ids = [random.randint(1, 20*M) for i in range(n)]
    t0 = time.time()
    for id in ids:
        #db.query('SELECT * FROM data where thing_id=$id AND revision=1', vars=locals())
        db.query('SELECT * FROM thing WHERE key=$key', vars={'key': '/b/OL%dM' % id})
    t1 = time.time()
    return (t1-t0)/n

def timeit(f, repeat, *a):
    t0 = time.time()
    for i in range(repeat):
        f(*a)
    t1 = time.time()
    return (t1-t0)/repeat

def f(db, n):
    """Test with n random rows"""
    keys = ["/b/OL%dM" % random.randint(1, 20*1000000) for i in range(n)]
    db.query("SELECT * FROM thing WHERE key in $keys", vars=locals()).list()

def g(db, n):
    """Fetch n rows with consequtive keys"""
    start = random.randint(1, 20*1000000)
    keys = ["/b/OL%dM" % i  for i in range(start, start+n)]
    db.query("SELECT * FROM thing WHERE key in $keys", vars=locals()).list()

def main(host, dbname, n=1000):
    db = web.database(dbn='postgres', db=dbname, user='anand', pw='', host=host)
    db.printing = False
    print 'f', timeit(f, 10, db, 1000)
    print 'g', timeit(g, 10, db, 1000)

if __name__ == "__main__":
    if len(sys.argv) > 3:
        main(sys.argv[1], sys.argv[2], int(sys.argv[3]))
    else:
        main(sys.argv[1], sys.argv[2])

########NEW FILE########
__FILENAME__ = loadmemcache
import _init_path
from openlibrary.utils import olmemcache
import web
import time

def main():
    m = olmemcache.Client(["ia331532:7060", "ia331533:7060"])
    db = web.database(dbn="postgres", db="openlibrary", user="anand", pw="", host="ia331526")

    t = db.transaction()
    try:
        db.query("DECLARE datacur CURSOR FOR SELECT thing.key, data.data FROM thing, data WHERE thing.id=data.thing_id and data.revision=thing.latest_revision ORDER BY thing.id")
        limit = 10000
        i = 0
        while True:
            i += 1
            result = db.query('FETCH FORWARD $limit FROM datacur', vars=locals()).list()
            if not result:
                break
            t1 = time.time()
            d = dict((r.key, r.data) for r in result)
            try:
                m.set_multi(d)
                #m.add_multi(d)
            except:
                m.delete_multi(d.keys())
                print >> web.debug, 'failed to add to memcached', repr(r.key)

            t2 = time.time()
            print >> web.debug, "%.3f" % (t2-t1), i, "adding memcache records"
    finally:
        t.rollback()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = loadworks
"""Script to load works fast."""
import sys, os
import time
import simplejson
import web

import _init_path
from openlibrary.utils.bulkimport import DocumentLoader, Reindexer

class WorkLoader:
    def __init__(self, **dbparams):
        self.loader = DocumentLoader(**dbparams)
        self.tmpdir = "/tmp"

        # a bug in web.group has been fixed in 0.33
        assert web.__version__ == "0.33"

    def load_works(self, filename, author="/user/ImportBot"):
        self.author = author
        
        root = os.path.dirname(filename)
        editions_file = open(os.path.join(root, 'editions.txt'), 'a')
        
        try:
            for i, lines in enumerate(web.group(open(filename), 1000)):
                t0 = time.time()
                self.load_works_chunk(lines, editions_file)
                t1 = time.time()
                log(i, "%.3f sec" % (t1-t0))
        finally:
            editions_file.close()

    def load_works_chunk(self, lines, editions_file):
        authors = [eval(line) for line in lines]

        editions = {}
        for akey, works in authors:
            keys = self.loader.new_work_keys(len(works))
            for work, key in zip(works, keys):
                work['key'] = key
                work['type'] = {'key': "/type/work"}
                work['authors'] = [{'author': {'key': akey}, 'type': '/type/author_role'}]
                if 'subjects' in work:
                    del work['subjects']
                if 'toc' in work:
                    del work['toc']
                editions[key] = work.pop('editions')
                
        result = self.loader.bulk_new(works, comment="add works page", author=self.author)

        def process(result):
            for r in result:
                for e in editions[r['key']]:
                    yield "\t".join([e, r['key'], str(r['id'])]) + "\n"
        
        editions_file.writelines(process(result))
        
    def update_editions(self, filename, author="/user/ImportBot"):
        self.author = author
        
        root = os.path.dirname(filename)
        index_file = open(os.path.join(root, 'edition_ref.txt'), 'a')
            
        type_edition_id = self.loader.get_thing_id("/type/edition")
        keyid = Reindexer(self.loader.db).get_property_id(type_edition_id, "works")
        
        log("begin")
        try:
            for i, lines in enumerate(web.group(open(filename), 1000)):
                t0 = time.time()
                self.update_editions_chunk(lines, index_file, keyid)
                t1 = time.time()
                log(i, "%.3f sec" % (t1-t0))
        finally:
            index_file.close()

        log("end")
    
    def update_editions_chunk(self, lines, index_file, keyid):
        data = [line.strip().split("\t") for line in lines]
        editions = [{"key": e, "works": [{"key": w}]} for e, w, wid in data]
        result = self.loader.bulk_update(editions, comment="link works", author=self.author)
    
        def process():
            edition_map = dict((row[0], row) for row in data)
            for row in result:
                eid = row['id']
                wid = edition_map[row['key']]
                ordering = 0
                yield "\t".join(map(str, [eid, keyid, wid, ordering])) + "\n"
        index_file.writelines(process())    
        
    def add_index(self, editions, keys2id):
        rows = []
        for e in editions:
            row = dict(thing_id=keys2id[e['key']],
                    key_id=self.key_id_works,
                    value=keys2id[e['works'][0]['key']],
                    ordering=0)
            rows.append(row)
        self.loader.db.multiple_insert("edition_ref", rows, seqname=False)

def make_documents(lines):
    rows = [eval(line) for line in lines]
    return [dict(type={'key': '/type/work'},
                title=r['title'],
                authors=[dict(author=a, type='/type/author_role') for a in r['authors']])
            for r in rows]

def main(filename):
    #loader = WorkLoader(db="staging", host="ia331525")
    loader = WorkLoader(db="openlibrary", host="ia331526")
#    loader.loader.db.printing = True
    loader.loader.db.printing = False
    #loader.load_works(filename)
    loader.update_editions(filename)

def log(*args):
    args = [time.asctime()] + list(args)
    sys.stdout.write(" ".join(str(a) for a in args))
    sys.stdout.write("\n")
    sys.stdout.flush()

if __name__ == "__main__":
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = _init_path
../../_init_path.py
########NEW FILE########
__FILENAME__ = fetch
#! /usr/bin/env python
"""Fetch a document and its references recursively upto specified depth from Open Library website.

USAGE: 
    
    Fetch an author and it's references upto depth 3
    
    $ python fetch.py /authors/OL23919A

    Load all the fetched documents into local instance

    $ python fetch.py --load

"""
import _init_path
import urllib
import simplejson
import os

MAX_LEVEL = 3

def get(key):
    path = "data" + key
    if os.path.exists(path):
        json = open(path).read()
    else:
        json = urllib.urlopen("http://openlibrary.org%s.json" % key).read()
    return simplejson.loads(json)

def query(**kw):
    json = urllib.urlopen("http://openlibrary.org/query.json?" + urllib.urlencode(kw)).read()
    return simplejson.loads(json)

def get_backreferences(key, limit=5):
    if key.startswith("/works"):
        return [d['key'] for d in query(type="/type/edition", works=key, limit=limit)]
    if key.startswith("/authors"):
        return [d['key'] for d in query(type="/type/edition", authors=key, limit=limit)]
    else:
        return []

def get_references(doc, result=None):
    if result is None:
        result = []

    if isinstance(doc, list):
        for v in doc:
            get_references(v, result)
    elif isinstance(doc, dict):
        if 'key' in doc:
            result.append(doc['key'])

        for k, v in doc.items():
            get_references(v, result)
    return result

def fetch(key, depth, limit):
    _fetch(key, maxlevel=depth, limit=limit)

def _fetch(key, visited=None, level=0, maxlevel=0, limit=5):
    """Fetch documents using depth-first traversal"""
    if visited is None:
        visited = set()

    if key in visited:
        return

    prefix = ("|  " * level) + "|--"
    print prefix, key

    doc = get(key)
    write(key, doc)
    visited.add(key)

    refs = get_references(doc)

    if level < maxlevel:
        refs += get_backreferences(key, limit=limit)

    for k in refs:
        if k not in visited and not k.startswith("/type/"):
            _fetch(k, visited, level=level+1, maxlevel=maxlevel, limit=limit)

def makedirs(dir):
    if not os.path.exists(dir):
        os.makedirs(dir)

def write(key, doc):
    path = "data" + key
    makedirs(os.path.dirname(path))

    f = open(path, "w")
    f.write(simplejson.dumps(doc))
    f.close()

def main(key, level=0, limit=100):
    fetch(key, int(level), int(limit))

def topological_sort(nodes, get_children):
    def visit(n):
        if n not in visited:
            visited.add(n)
            for c in get_children(n):
                visit(c)
            result.append(n)

    result = []
    visited = set()
    for n in nodes:
        visit(n)
    return result

def find(path):
    for dirname, dirnames, filenames in os.walk(path):
        for f in filenames:
            yield os.path.join(dirname, f)

def load():
    """Loads documents to http://0.0.0.0:8080"""
    documents = {}
    for f in find("data"):
        doc = simplejson.load(open(f))
        documents[doc['key']] = doc

    keys = topological_sort(documents.keys(), 
            get_children=lambda key: [k for k in get_references(documents[key]) if not k.startswith("/type/")])

    from openlibrary.api import OpenLibrary
    ol = OpenLibrary("http://0.0.0.0:8080")
    ol.autologin()
    print ol.save_many([documents[k] for k in keys], comment="documents copied from openlibrary.org")
    
if __name__ == "__main__":
    import sys
    if '--load' in sys.argv:
        load()
    else:
        main(*sys.argv[1:])

########NEW FILE########
__FILENAME__ = get_machine_comments
"""Script to print key, machine_comment for all documents in OL.

USAGE: python get_machine_comments.py host dbname
"""
import web
import os
import sys

def main(host, db):
    user = os.getenv("USER")
    db = web.database(dbn="postgres", db="openlibrary", user=user, pw="", host=host)

    t = db.transaction()
    try:
        db.query("DECLARE datacur CURSOR FOR " + 
            " SELECT thing.key, version.machine_comment FROM thing, version" 
            " WHERE thing.id=version.thing_id AND version.machine_comment is not NULL"
            " ORDER BY thing.id")

        limit = 10000
        i = 0
        while True:
            i += 1
            result = db.query('FETCH FORWARD $limit FROM datacur', vars=locals())
            if not result:
                break
            sys.stdout.writelines("%s\t%s\n" % (row.key, row.machine_comment) for row in result)
    finally:
        t.rollback()

if __name__ == "__main__":
    main(sys.argv[1], sys.argv[2])


########NEW FILE########
__FILENAME__ = find_dark
import web, re, httplib, sys, urllib2
from openlibrary.catalog.read_rc import read_rc

rc = read_rc()
db = web.database(dbn='mysql', host=rc['ia_db_host'], user=rc['ia_db_user'], passwd=rc['ia_db_pass'], db='archive')
db.printing = False

iter = db.query("select identifier, updated from metadata where scanner is not null and noindex is not null and mediatype='texts' and (curatestate='approved' or curatestate is null) and scandate is not null order by updated")

for row in iter:
    print row.identifier

########NEW FILE########
__FILENAME__ = fix_no_index
from openlibrary.catalog.utils.query import query, withKey
from openlibrary.api import OpenLibrary, unmarshal
from openlibrary.catalog.read_rc import read_rc

rc = read_rc()
ol = OpenLibrary("http://openlibrary.org")
ol.login('ImportBot', rc['ImportBot']) 

to_fix = []
num = 0
for line in open('no_index'):
    for e in query({'type': '/type/edition', 'title': None, 'ocaid': line[:-1]}):
        num += 1
        print num, e['key'], `e['title']`, line[:-1]
        e2 = ol.get(e['key'])
        del e2['ocaid']
        to_fix.append(e2)

ol.save_many(to_fix, 'remove link')

########NEW FILE########
__FILENAME__ = updatestats
#! /usr/bin/env python
"""Script to update stats.
"""

import sys
import datetime

import _init_path
from openlibrary.api import OpenLibrary

def main(site, date=None):
    ol = OpenLibrary(site)
    ol.autologin("StatsBot")

    today = date or datetime.date.today().isoformat()
    print ol._request("/admin/stats/" + today, method='POST', data="").read()

if __name__ == "__main__":
    main(*sys.argv[1:])
    

########NEW FILE########
__FILENAME__ = _init_path
../../_init_path.py
########NEW FILE########
__FILENAME__ = report_errors
"""Script to send report of internal errors."""

import sys
import web
import os
import socket
from collections import defaultdict
from BeautifulSoup import BeautifulSoup
from optparse import OptionParser


TEMPLATE = """\
$def with (hostname, date, dir, errors)
$var subject: $dir: $date

$ error_places = group(errors, lambda e: (e.message, e.code))
$hostname.split('.')[0]/$date: $len(errors) errors at $len(error_places) places
$ newline = ""

$for (msg, code), elist in error_places[:10]:
    [$len(elist) times] $msg 
    at $code
    
    $ errgroup = group(elist, lambda e: e.url)
        $for url, x in errgroup[:3]:
            [$len(x) times] $url
            $x[0].error_url
            $newline
        $if len(errgroup) > 3:
            ...
            $newline        
$if len(error_places) > 10:
    ...
     
"""

hostname = socket.gethostname()
def group(items, key):
    d = defaultdict(list)
    for item in items:
        d[key(item)].append(item)
        
    return sorted(d.items(), reverse=True, key=lambda (k, vlist): len(vlist))

web.template.Template.globals.update({
    "sum": sum,
    "group": group,
})
t = web.template.Template(TEMPLATE)

def main():
    parser = OptionParser()
    parser.add_option("--email", dest="email", help="address to send email", action="append")
    options, args = parser.parse_args()
    
    dir, date = args
    
    msg = process_errors(dir, date)
    if options.email:
        web.sendmail(
            from_address='Open Library Errors<noreply@openlibrary.org>',
            to_address=options.email,
            subject=msg.subject,
            message=web.safestr(msg))
        print "email sent to", ", ".join(options.email)
    else:
        print msg
    
def process_errors(dir, date):
    root = os.path.join("/var/log/openlibrary", dir, date)
    
    basename = os.path.basename(dir)
    
    def parse(f):
        e = parse_error(os.path.join(root, f))
        e.error_url = "http://%s/logs/%s/%s/%s" % (hostname, basename, date, f)
        return e

    if os.path.exists(root):
        errors = [parse(f) for f in os.listdir(root)]
    else:
        errors = []
    
    return t(hostname, date, basename, errors)
    
def parse_error(path):
    html = open(path).read(10000)
    soup = BeautifulSoup(html)
    
    h1 = web.htmlunquote(soup.body.h1.string or "")
    h2 = web.htmlunquote(soup.body.h2.string or "")
    message = h1.split('at')[0].strip() + ': ' + (h2 and h2.splitlines()[0])

    code, url = [web.htmlunquote(td.string) for td in soup.body.table.findAll('td')]
    
    # strip common prefixes
    code = web.re_compile(".*/(?:staging|production)/(openlibrary|infogami|web)").sub(r'\1', code)
    
    m = web.re_compile('(\d\d)(\d\d)(\d\d)(\d{6})').match(web.numify(os.path.basename(path)))
    hh, mm, ss, microsec = m.groups()
    
    return web.storage(url=url, message=message, code=code, time="%s:%s:%s" % (hh, mm, ss))

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = fix_records
# - remove title_prefix
# - fix bad unicode
# - source_records field
# - change table_of_contents from to a list of /type/toc_item
# - undelete any deleted authors
# - delete blank toc

# [{'type': {'key': '/type/toc_item'}, 'class': 'section'}]

import sys, codecs
import simplejson as json

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

for line in open('/2/edward/fix_bad/edition_file'):
    cur = json.loads(line)
    has_blank_toc = False
    print cur
    if 'table_of_contents' in cur and len(cur['table_of_contents']) == 1:
        toc = cur['table_of_contents'][0]
        if isinstance(toc, dict) and toc['type']['key'] == '/type/toc_item' \
            and 'label' not in toc and 'title' not in toc:
                has_blank_toc = True
    for k, v in cur.items():
        print "%20s: %s" % (k, v)
    print 'blank toc:', has_blank_toc
    print

########NEW FILE########
__FILENAME__ = makedump
"""Parse data table dump and generate tsv with "key", "type", "revision", "json" columns.
"""
import _init_path
import sys
from openlibrary.data import parse_data_table

def main(filename):
    for cols in parse_data_table(filename):
        print "\t".join(cols)

if __name__ == "__main__":
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = olload
"""Script to load Open Library data into various databases.

USAGE:
    python olload.py bsddb ol.db ol_cdump.txt
    python olload.py memcache localhost:11211 ol_cdump.txt
"""
import _init_path
from openlibrary.data import parse_data_table

import web
import time

def load_memcache(server, data):
    import memcache
    mc = memcache.Client([server])
    for chunk in data:
        mc.set_multi()

def load_bsddb(filename, data):
    import bsddb
    GB = 1234 ** 3
    db = bsddb.btopen(filename, cachesize=1 * GB)

    for chunk in data:
        db.update(chunk)

    db.sync()

def load_redis(server, data):
    import redis
    host, port = server.split(":")
    r = redis.Redis(host=host, port=int(port))
    for chunk in data:
        r.mset(chunk)

def load_couchdb(url, data):
    import urllib

    for chunk in data:
        docs = ('{"_id": "%s", ' % k + v[1:] for k, v in chunk.iteritems())
        json = '{"docs": [\n%s\n]}' % ',\n'.join(docs)
        urllib.urlopen(url + "/_bulk_docs", json).read()

def load_dummy(arg1, data):
    for chunk in data:
        pass

def parse(filename, chunk_size=10000):
    t0 = time.time()
    i = 0
    for chunk in web.group(open(filename), chunk_size):
        print i, time.time() - t0
        d = {}
        for line in chunk:
            key, type, revision, json = line.strip().split("\t")
            d["%s@@%s" % (key, revision)] = json

        i += len(d)
        yield d
    print i, time.time() - t0
        
def main(dbname, param, filename):
    data = parse(filename, 10000)
    fname = "load_" + dbname
    f = globals()[fname]
    f(param, data)

if __name__ == "__main__":
    import sys
    main(sys.argv[1], sys.argv[2], sys.argv[3])

########NEW FILE########
__FILENAME__ = _init_path
../../_init_path.py
########NEW FILE########
__FILENAME__ = add_to_editions
import os, sys, re
from openlibrary.catalog.marc.all import files
from openlibrary.catalog.get_ia import read_marc_file
from openlibrary.catalog.marc.fast_parse import get_first_tag, get_contents, get_subfield_values, handle_wrapped_lines, get_tag_lines, get_all_subfields
from openlibrary.catalog.marc.new_parser import read_oclc, has_dot

base = '/1/edward/marc/'

langs = set([
    'aar', 'abk', 'ace', 'ach', 'ada', 'ady', 'afa', 'afr', 'aka', 'akk',
    'alb', 'ale', 'alg', 'amh', 'ang', 'apa', 'ara', 'arc', 'arm', 'arn',
    'arp', 'art', 'arw', 'ase', 'asm', 'ath', 'aus', 'ava', 'ave', 'awa',
    'aym', 'aze', 'bai', 'bak', 'bal', 'bam', 'ban', 'baq', 'bas', 'bat',
    'bel', 'bem', 'ben', 'ber', 'bho', 'bik', 'bin', 'bis', 'bla', 'bnt',
    'bos', 'bra', 'bre', 'btk', 'bua', 'bug', 'bul', 'bur', 'cai', 'cam',
    'car', 'cat', 'cau', 'ceb', 'cel', 'che', 'chg', 'chi', 'chk', 'chm',
    'chn', 'cho', 'chr', 'chu', 'chv', 'chy', 'cmc', 'cmn', 'cop', 'cor',
    'cos', 'cpe', 'cpf', 'cpp', 'cre', 'crh', 'crp', 'cus', 'cze', 'dak',
    'dan', 'dar', 'day', 'del', 'din', 'doi', 'dra', 'dua', 'dum', 'dut',
    'dyu', 'dzo', 'efi', 'egy', 'eka', 'elx', 'eng', 'enm', 'epo', 'esk',
    'esp', 'est', 'eth', 'ewe', 'ewo', 'fan', 'fao', 'far', 'fat', 'fij',
    'fil', 'fin', 'fiu', 'fon', 'fre', 'fri', 'frm', 'fro', 'fry', 'ful',
    'fur', 'gaa', 'gae', 'gag', 'gal', 'gay', 'gba', 'gem', 'geo', 'ger',
    'gez', 'gil', 'gla', 'gle', 'glg', 'glv', 'gmh', 'goh', 'gon', 'gor',
    'got', 'grb', 'grc', 'gre', 'grn', 'gua', 'guj', 'gul', 'hat', 'hau',
    'haw', 'hbs', 'heb', 'her', 'hil', 'him', 'hin', 'hmn', 'hmo', 'hun',
    'iba', 'ibo', 'ice', 'ido', 'ijo', 'iku', 'ilo', 'ina', 'inc', 'ind',
    'ine', 'inh', 'int', 'ipk', 'ira', 'iri', 'iro', 'ita', 'jav', 'jpn',
    'jpr', 'jrb', 'kaa', 'kab', 'kac', 'kal', 'kam', 'kan', 'kar', 'kas',
    'kau', 'kaw', 'kaz', 'kbd', 'kha', 'khi', 'khm', 'kho', 'kik', 'kin',
    'kir', 'kmb', 'kok', 'kom', 'kon', 'kor', 'kos', 'kpe', 'krc', 'kro',
    'kru', 'kua', 'kum', 'kur', 'lad', 'lah', 'lam', 'lan', 'lao', 'lap',
    'lat', 'lav', 'lez', 'lin', 'lit', 'lol', 'loz', 'ltz', 'lua', 'lub',
    'lug', 'lun', 'luo', 'lus', 'mac', 'mad', 'mag', 'mah', 'mai', 'mak',
    'mal', 'man', 'mao', 'map', 'mar', 'mas', 'may', 'men', 'mic', 'min',
    'mis', 'mkh', 'mla', 'mlg', 'mlt', 'mnc', 'mni', 'mno', 'moh', 'mol',
    'mon', 'mos', 'mul', 'mun', 'mus', 'mwr', 'myn', 'nah', 'nai', 'nau',
    'nav', 'nbl', 'nde', 'ndo', 'nds', 'nep', 'new', 'nic', 'niu', 'non',
    'nor', 'nso', 'nub', 'nya', 'nyn', 'nyo', 'nzi', 'oci', 'oji', 'ori',
    'orm', 'oss', 'ota', 'oto', 'paa', 'pag', 'pal', 'pam', 'pan', 'pap',
    'pau', 'peo', 'per', 'phi', 'pli', 'pol', 'pon', 'por', 'pra', 'pro',
    'pus', 'que', 'raj', 'rar', 'roa', 'roh', 'rom', 'rum', 'run', 'rus',
    'sag', 'sah', 'sai', 'sal', 'sam', 'san', 'sao', 'sas', 'sat', 'scc',
    'sco', 'scr', 'sel', 'sem', 'shn', 'sho', 'sid', 'sin', 'sio', 'sit',
    'sla', 'slo', 'slv', 'smi', 'smo', 'sna', 'snd', 'snh', 'snk', 'sog',
    'som', 'son', 'sot', 'spa', 'srd', 'srr', 'ssa', 'sso', 'ssw', 'suk',
    'sun', 'sus', 'sux', 'swa', 'swe', 'swz', 'syc', 'syr', 'tag', 'tah',
    'taj', 'tam', 'tar', 'tat', 'tel', 'tem', 'ter', 'tet', 'tgk', 'tgl',
    'tha', 'tib', 'tig', 'tir', 'tiv', 'tkl', 'tli', 'tmh', 'tog', 'ton',
    'tpi', 'tsi', 'tsn', 'tso', 'tsw', 'tuk', 'tum', 'tur', 'tut', 'tvl',
    'twi', 'tyv', 'udm', 'uga', 'uig', 'ukr', 'umb', 'und', 'urd', 'uzb',
    'vai', 'ven', 'vie', 'vls', 'wak', 'wal', 'war', 'wel', 'wen', 'wol',
    'xal', 'xho', 'yao', 'yap', 'yid', 'yor', 'ypk', 'yue', 'zap', 'znd',
    'zul', 'zun'])

def sources():
    for d in os.listdir(base):
#        if d.startswith('talis'):
#            continue
        if d.endswith('old'):
            continue
        if d == 'indcat':
            continue
        if not os.path.isdir(base + d):
            continue
        yield d

def iter_marc():
    rec_no = 0
    for ia in sources():
        print ia
        for part, size in files(ia):
            full_part = ia + "/" + part
            filename = base + full_part
            assert os.path.exists(filename)
            print filename
            f = open(filename)
            for pos, loc, data in read_marc_file(full_part, f):
                rec_no +=1
                yield rec_no, pos, loc, data

# source_record,oclc,accompanying_material,translated_from,title

re_oclc = re.compile ('^\(OCoLC\).*?0*(\d+)')

out = open('/3/edward/updates', 'w')
want = set(['001', '003', '035', '041', '245', '300'])
for rec_no, pos, loc, data in iter_marc():
    fields = {}
    rec = {}
    title_seen = False
    for tag, line in handle_wrapped_lines(get_tag_lines(data, want)):
        if tag == '245':
            if title_seen:
                continue
            title_seen = True
            if line[1] == '0': # no prefix
                continue
            contents = get_contents(line, ['a', 'b'])
            if 'a' in contents:
                rec['title'] = ' '.join(x.strip(' /,;:') for x in contents['a'])
            elif 'b' in contents:
                rec['title'] = contents['b'][0].strip(' /,;:')
            if 'title' in rec and has_dot(rec['title']):
                rec['title'] = rec['title'][:-1]
            continue
        if tag == '300':
            if 'accompanying_material' in rec:
                continue
            subtag_e = ' '.join(i.strip('. ') for i in get_subfield_values(line, set(['e'])))
            if subtag_e:
                if subtag_e.lower() in ('list', 'notes', 'book'):
                    continue
                rec['accompanying_material'] = subtag_e
            continue
        fields.setdefault(tag, []).append(line)

    for line in fields.get('041', []):
        found = []
        marc_h = list(get_subfield_values(line, 'h'))
        if not marc_h:
            continue
        for h in marc_h:
            if len(h) % 3 != 0:
                print 'bad:', list(get_all_subfields(line))
                continue
            found += ['/l/' + i for i in (h[i * 3:(i+1) * 3].lower() for i in range(len(h) / 3)) if i in langs]
        if found:
            rec.setdefault('translated_from', []).extend(found)

    rec.update(read_oclc(fields))

    if rec:
        rec['source_record'] = loc
        print >> out, rec
out.close()

########NEW FILE########
__FILENAME__ = changes_dump
"""Create changes dump of OL.

USAGE: python chanages_dump.py dbname
"""
import web
import os
import simplejson

def fetch(db, query, size=10000):
    t = db.transaction()
    try:
        db.query("DECLARE _fetch NO SCROLL CURSOR FOR " + query)
        while True:
            rows = db.query("FETCH FORWARD $size FROM _fetch", vars=locals())
            if rows:
                for row in rows:
                    yield row
            else:
                break
    finally:
        t.rollback()

def load_authors(db):
    d = {}
    for row in fetch(db, "SELECT thing.id, thing.key FROM thing, account WHERE account.thing_id=thing.id"):
        d[row.id] = row.key
    return d

def main(dbname):
    db = web.database(dbn="postgres", db=dbname, user=os.getenv("USER"), pw="")
    authors = load_authors(db)

    q = ( "SELECT t.*, thing.key, version.revision"
        + " FROM transaction t, thing, version"
        + " WHERE version.transaction_id=t.id AND version.thing_id=thing.id"
        + " ORDER BY t.created"
        )

    for row in fetch(db, q, 10000):
        row.author = {"key": row.author_id and authors.get(row.author_id)}
        del row.author_id
        row.comment = row.comment or ""
        row.ip = row.ip or ""
        row.created = row.created.isoformat()
        print simplejson.dumps(row) + "\t" + row.created

if __name__ == "__main__":
    import sys
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = fix_dups
"""Noticed some duplicate entries in the thing table of OL database.
There should have been a unique constrant on thing.key, but that was missing. 
Quick analysis revealed that there are many work records with single dup and the dup has only one revision.

This script removes those dups.
"""
import web
import os
import simplejson
from collections import defaultdict

db = web.database(dbn="postgres", db="openlibrary", user=os.getenv("USER"), pw="")

def get_json(thing_id, revision):
    json = db.where('data', thing_id=thing_id, revision=revision)[0].data
    d = simplejson.loads(json)
    del d['created']
    del d['last_modified']
    return d

def fix_work_dup(key):
    rows = db.where("thing", key=key).list()
    if len(rows) == 1:
        print key, "already fixed"
        return 
    elif len(rows) == 2:
        w1, w2 = rows
        if w1.latest_revision < w2.latest_revision:
            w1, w2 = w2, w1
        print key, "fixing single dup", w1.id, w1.latest_revision, w2.id, w2.latest_revision
        if w2.latest_revision == 1 and get_json(w1.id, 1) == get_json(w2.id, 1):
            print "RENAME", w2.id, w2.key + "--dup"
    else:
        print key, "many dups"

@web.memoize
def get_property_id(type, name):
    type_id = db.where("thing", key=type)[0].id
    return db.where("property", type=type_id, name=name)[0].id

def find_edition_count(thing_ids):
    key_id = get_property_id("/type/edition", "works")
    rows = db.query("SELECT * FROM edition_ref WHERE key_id=$key_id AND value IN $thing_ids", vars=locals())

    d = defaultdict(lambda: 0)
    for row in rows:
        d[row.thing_id] += 1
    return d

def get_data(thing_ids):
    rows = db.query("SELECT * FROM data WHERE thing_id IN $thing_ids and revision=1", vars=locals())
    d = {}
    for row in rows:
        data = simplejson.loads(row.data)
        del data['created']
        del data['last_modified']
        d[row.thing_id] = data
    return d

def find_more_info(key):
    rows = db.where("thing", key=key).list()
    for row in rows:
        editions = db.where("edition_ref", key_id=get_property_id("/type/edition", "works"), value=row.id)
        row.edition_count = len(editions)
        row.data = get_json(row.id, 1)
        row.created = row.created.isoformat()
        row.last_modified = row.last_modified.isoformat()

    print key, simplejson.dumps(rows)

def main(dups_file):
    keys = [key.strip() for key in open(dups_file) if key.startswith("/works/")]
    for key in keys:
        find_more_info(key)

def generate_tsv(jsonfile):
    for line in open(jsonfile):
        key, json = line.strip().split(None, 1)
        works = simplejson.loads(json)
        for w in sorted(works, key=lambda w: w['latest_revision'], reverse=True):
            cols = key, w['id'], w['latest_revision'], w['edition_count'], w['data']['title'], ",".join(a['author']['key'] for a in w['data']['authors'])
            print "\t".join(web.safestr(c) for c in cols)

if __name__ == "__main__":
    import sys
    #main(sys.argv[1])
    generate_tsv(sys.argv[1])


########NEW FILE########
__FILENAME__ = import_goodreads_ids
#!/usr/bin/env python

from time import localtime, sleep, strftime
from olapi import OpenLibrary

ol = OpenLibrary()
ol.login("someBot", "somePassword")

def print_log(msg):
  timestamp = strftime("%Y%m%d_%H:%M:%S", localtime())
  print("[" + timestamp + "] " + msg)

def set_identifier(book, id_name, id_value):
  # OL handles the standard identifiers in a different way.
  if id_name in ["isbn_10", "isbn_13", "oclc_numbers", "lccn"]:
    ids = book.setdefault(id_name, [])
    if id_value not in ids:
      ids.append(id_value)
  else:
    ids = book.setdefault("identifiers", {})
    ids[id_name] = [id_value]

def set_goodreads_id(olid, goodreads_id):
  book = ol.get(olid)
  set_identifier(book, "goodreads", goodreads_id)
  ol.save(book['key'], book, "Added goodreads ID.")

def map_id(olid, isbn, goodreads_id):
  book = ol.get(olid)
  if book.has_key('identifiers'):
    if book['identifiers'].has_key('goodreads'):
      if goodreads_id in book['identifiers']['goodreads']:
        return
  print_log("Adding Goodreads ID \"" + goodreads_id + "\" to Openlibrary ID \"" + olid + "\"")
  set_goodreads_id(olid, goodreads_id)

def load(filename):
  n = 0
  for line in open(filename):
    olid, isbn, goodreads_id = line.strip().split()
    n = n+1
    if (n % 100000) == 0:
      print_log("(just read line " + str(n) + " from the map file)")
    is_good = False
    while (not is_good):
      try:
        map_id(olid, isbn, goodreads_id)
        is_good = True
      except:
        print_log("Exception for Goodreads ID \"" + goodreads_id + "\", message: \"" + str(sys.exc_info()[1]) + "\"")
        sleep(30)

if __name__ == "__main__":
  import sys
  load(sys.argv[1])


########NEW FILE########
__FILENAME__ = reindex
"""Reindex work_ref table.

USAGE: python reindex.py dbname work_keys.txt

TODO: generalize this to work with any table.
"""
import web
import simplejson
import os

def read_keys(keysfile):
    """Returns all keys in the keys file."""
    for line in open(keysfile):
        yield line.strip()
        
def get_docs(keys):
    """Returns docs for the specified keys."""
    for key in keys:
        result = db.query("SELECT thing.id, data.data FROM data, thing WHERE data.thing_id=thing.id AND data.revision=thing.latest_revision and thing.key=$key", vars=locals())
        d = result[0]
        doc = simplejson.loads(d.data)
        d['id'] = d.id
        yield doc

def read_refs(thing_id):
    rows = db.query("SELECT * FROM work_ref WHERE thing_id=$thing_id", vars=locals())
    for r in rows:
        name = get_property_name(r.key_id)
        value = r.value
        yield name, value

def get_thing_id(key):
    return db.query("SELECT id FROM thing WHERE key=$key", vars=locals())[0].id

@web.memoize
def get_property_id(type, name):
    type_id = get_thing_id(type)
    return db.query("SELECT id FROM property WHERE type=$type_id AND name=$name", vars=locals())[0].id
    
@web.memoize
def get_property_name(key_id):
    return db.query("SELECT name FROM property WHERE id=$key_id", vars=locals())[0].name

def flat_items(d):
    """
        >>> flat_items({"a": {"b": "c"}, "x": [1, 2]})
        [('a.b', 'c'), ('x', 1), ('x', 2)]
    """
    items = []
    sep = "."
    
    def process(key, value):
        if isinstance(value, dict):
            for k, v in value.items():
                process(key + sep + k, v)
        elif isinstance(value, list):
            for v in value:
                process(key, v)
        else:
            items.append((key, value))
    
    for k, v in d.items():
        process(k, v)
    return items
    
def find_references(doc):
    """Find the references to be indexed from the given doc.
    
        >>> refs = find_references({"title": "foo", "authors": [{"key": "/a/OL1A"}]})
        >>> list(refs)
        [('authors', '/a/OL1A')]
    """
    for k, v in flat_items(doc):
        if k.endswith(".key") and not k.endswith("type.key") and not k.startswith("excerpts."):
            yield k[:-len(".key")], v

def make_index(docs):
    for doc in docs:
        doc["_refs"] = [(name, get_thing_id(value)) for name, value in find_references(doc)]
        yield doc

def filter_index(docs):
    for doc in docs:
        refs = list(read_refs(doc["id"]))
        #print refs, doc['_refs']
        if refs != doc["_refs"]:
            yield doc
    
def write_index(docs):
    for chunk in web.group(docs, 1000):
        chunk = list(chunk)
        
        thing_ids = [doc['id'] for doc in chunk]
        
        t = db.transaction()
        db.query("DELETE FROM work_ref WHERE thing_id IN $thing_ids", vars=locals())
        
        data = []
        for doc in chunk:
            thing_id = doc['id']
            type = doc['type']['key']
            if type == "/type/work":
                for name, value in doc['_refs']:
                    key_id = get_property_id(type, name)
                    data.append(dict(thing_id=thing_id, key_id=key_id, value=value))
        
        if data:
            db.multiple_insert("work_ref", data, seqname=False)
        
        t.commit()

def reindex(keys):
    pass
    
def pipe(*funcs):
    def g(arg):
        for f in funcs:
            arg = f(arg)
        return arg
    return g
    
def display(data):
    for row in data:
        print row
    
def main(dbname, keysfile):
    global db
    db = web.database(dbn="postgres", db=dbname, user=os.getenv("USER"), pw="")
    
    f = pipe(read_keys, get_docs, make_index, filter_index, write_index)
    f(keysfile)
    
if __name__ == "__main__":
    import sys
    main(sys.argv[1], sys.argv[2])

########NEW FILE########
__FILENAME__ = update_docs
#! /usr/bin/env python
"""Script to update documents in the Open Library database.

    USAGE: python update_docs.py database docs_file
    
The following example adds covers to edtions.
    
    $ cat docs.txt
    {"key": "/b/OL1M", "covers": [1, 2]}
    {"key": "/b/OL2M", "covers": [3, 4]}
    ...
    $ python update_docs.py openlibrary docs.txt
    
WARNING: This script doesn't update the index tables.
"""
import simplejson
import web
import os
import datetime

def read_docs(filename):
    """Read docs from the given file."""
    for line in open(filename):
        yield simplejson.loads(line.strip())    
        
def get_docs(db, keys):
    """Get the docs from DB."""
    rows = db.query("SELECT thing.id, thing.key, data.revision, data.data FROM thing, data WHERE thing.id = data.thing_id AND thing.latest_revision = data.revision AND key in $keys", vars=locals())
    rows = rows.list()
    for row in rows:
        row.doc = simplejson.loads(row.data)
        del row.data
    return rows
    
@web.memoize
def get_thing_id(db, key):
    return db.query("SELECT * FROM thing WHERE key=$key", vars=locals())[0].id

def update_docs(db, all_docs, chunk_size=10000, comment=""):
    now = datetime.datetime.utcnow()
    
    for chunk in web.group(all_docs, chunk_size):
        print chunk
        d = dict((doc['key'], doc) for doc in chunk)
        rows = get_docs(db, d.keys())
        
        for row in rows:
            row.doc.update(d[row.key])
            row.doc['revision'] = row.revision + 1
            row.doc['latest_revision'] = row.revision + 1
            row.doc['last_modified']['value'] = now.isoformat()
            
        data = [web.storage(thing_id=row.id, revision=row.revision+1, data=simplejson.dumps(row.doc)) for row in rows]
            
    author_id = get_thing_id(db, "/user/anand")
            
    t = db.transaction()
    try:
        tx_id = db.insert("transaction", author_id=author_id, action="bulk_update", ip="127.0.0.1", bot=True, created=now, comment=comment)
        db.multiple_insert("version", [dict(thing_id=d.thing_id, transaction_id=tx_id, revision=d.revision) for d in data], seqname=False)
        db.multiple_insert("data", data, seqname=False)
        db.query("UPDATE thing set latest_revision=latest_revision+1 WHERE key in $d.keys()", vars=locals())
    except:
        t.rollback()
        raise
    else:
        t.commit()

def main(dbname, docsfile, comment):
    db = web.database(dbn="postgres", db=dbname, user=os.getenv("USER"), pw="")
    docs = read_docs(docsfile)
    update_docs(db, docs, chunk_size=10000, comment=comment)
    
if __name__ == "__main__":
    import sys
    main(sys.argv[1], sys.argv[2], sys.argv[3])
########NEW FILE########
__FILENAME__ = load_print_disabled
from openlibrary.catalog.marc.parse_xml import parse, xml_rec
from openlibrary.catalog.marc import read_xml
from openlibrary.api import OpenLibrary, unmarshal
from openlibrary.catalog.marc import fast_parse
from openlibrary.catalog.importer import pool
from openlibrary.catalog.merge.merge_marc import build_marc
from openlibrary.catalog.utils.query import query, withKey
from openlibrary.catalog.importer.merge import try_merge
import os, re, sys

ol = OpenLibrary("http://openlibrary.org")

re_census = re.compile('^\d+(st|nd|rd|th)census')
re_edition_key = re.compile('^/(?:b|books)/(OL\d+M)$')

scan_dir = '/2/edward/20century/scans/'

def read_short_title(title):
    return str(fast_parse.normalize_str(title)[:25])

def make_index_fields(rec):
    fields = {}
    for k, v in rec.iteritems():
        if k in ('lccn', 'oclc', 'isbn'):
            fields[k] = v
            continue
        if k == 'full_title':
            fields['title'] = [read_short_title(v)]
    return fields

# no maps or cartograph

total = 74933
load_count = 0
num = 0
show_progress = True
check_for_existing = False
skip = 'internetforphysi00smit'
skip = None
add_src_rec = open('add_source_record', 'w')
new_book = open('new_book', 'w')
for line in open('/2/edward/20century/records_to_load'):
    num += 1
    full_rec = eval(line)
    item = full_rec['ia_id']
    if skip:
        if skip == item:
            skip = None
        else:
            continue
    if show_progress:
        print '%d/%d %d %.2f%% %s' % (num, total, load_count, (float(num) * 100.0) / total, item)
    if check_for_existing:
        if ol.query({'type': '/type/edition', 'ocaid': item}):
            print item, 'already loaded'
            load_count += 1
            continue
        if ol.query({'type': '/type/edition', 'source_records': 'ia:' + ia}):
            print 'already loaded'
            load_count += 1
            continue
    try:
        assert not re_census.match(item)
        assert 'passportapplicat' not in item
        assert len(full_rec.keys()) != 1
    except AssertionError:
        print item
        raise
    filename = '/2/edward/20century/scans/' + item[:2] + '/' + item + '/' + item + '_marc.xml'
    rec = read_xml.read_edition(open(filename))
    if 'full_title' not in rec:
        print "full_title missing", item
        continue
    if 'physical_format' in rec:
        format = rec['physical_format'].lower()
        if format.startswith('[graphic') or format.startswith('[cartograph'):
            print item, format
    index_fields = make_index_fields(rec)
    if not index_fields:
        print "no index_fields"
        continue
    #print index_fields

    edition_pool = pool.build(index_fields)
    if not edition_pool or not any(v for v in edition_pool.itervalues()):
        print >> new_book, full_rec
        continue

    print item, edition_pool
    e1 = build_marc(rec)
    print e1

    match = False
    seen = set()
    for k, v in edition_pool.iteritems():
        for edition_key in v:
#            edition_key = '/books/' + re_edition_key.match(edition_key).match(1)
            if edition_key in seen:
                continue
            thing = None
            while not thing or thing['type']['key'] == '/type/redirect':
                seen.add(edition_key)
                thing = withKey(edition_key)
                assert thing
                if thing['type']['key'] == '/type/redirect':
                    print 'following redirect %s => %s' % (edition_key, thing['location'])
                    edition_key = thing['location']
            if try_merge(e1, edition_key, thing):
                print 'add source records:', edition_key, item
                print (edition_key, item)
                print >> add_src_rec, (edition_key, item)
                #add_source_records(edition_key, ia)
                #write_log(ia, when, "found match: " + edition_key)
                match = True
                break
        if not match:
            print full_rec
            print >> new_book, full_rec
            break

sys.exit(0)

########NEW FILE########
__FILENAME__ = jsontest
"""script to compare performance of simplejson and jsonlib.
"""

import sys, time, urllib2
import simplejson
import jsonlib, jsonlib2, yajl

def read():
    for  line in open(sys.argv[1]):
        yield line.strip().split("\t")[-1]


def timeit(label, seq):
    t0 = time.time()
    for x in seq:
        pass
    t1 = time.time()
    print label, "%.2f sec" % (t1-t0)

if False and __name__ == "__main__":
    timeit("read", read())
    timeit("simplejson.load", (simplejson.loads(json) for json in read()))
    timeit("jsonlib.load", (jsonlib.loads(json) for json in read()))
    
    
    timeit("simplejson.load-dump", (simplejson.dumps(simplejson.loads(json)) for json in read()))
    timeit("jsonlib.load-dump", (jsonlib.dumps(jsonlib.loads(json)) for json in read()))


def bench(count, f, *args):
    times = []
    for _ in range(count):
        t0 = time.time()
        f(*args)
        times.append(time.time() - t0)
    times = sorted(times)
    return "avg %.5f med %.5f max %.5f min %.5f" % (
        sum(times) / float(len(times)),
        times[int(len(times) / 2.0)],
        times[-1],
        times[0]
   )

if True and __name__ == "__main__":
    for b in (
        simplejson.dumps({"str": "value", 
                          "num": 1.0, 
                          "strlist": ["a", "b", "c"],
                          "numlist": range(1000)}),
        urllib2.urlopen("http://freebase.com/api/trans/notable_types_2?id=/en/bob_dylan").read(),
        urllib2.urlopen("http://freebase.com/api/trans/popular_topics_by_type_debug?id=/film/actor").read()
    ):
        print "-" * 80
        print "%s char blob" % len(b)
        print "-" * 80
        impls = (simplejson, jsonlib, jsonlib2, yajl)
        print "loads"
        for impl in impls:
            print "%s: %s" % (impl.__name__.ljust(10), bench(100, impl.loads, b))
        print "dumps"
        for impl in impls:
            print "%s: %s" % (impl.__name__.ljust(10), bench(100, impl.dumps, impl.loads(b)))

########NEW FILE########
__FILENAME__ = mark-templates
#! /usr/bin/env python
"""Script to mark the given templates as part of a plugin.

USAGE: python scripts/2010/07/mark-templates.py worksearch /upstream/templates/search/*
"""

import _init_path

import sys
from openlibrary.api import OpenLibrary

def main():
    ol = OpenLibrary()
    ol.autologin()

    plugin = sys.argv[1]

    all_docs = []

    for pattern in sys.argv[2:]:
        docs = ol.query({"key~": pattern, "*": None}, limit=1000)
        all_docs.extend(docs)

    for doc in all_docs:
        doc['plugin'] = plugin

    print ol.save_many(all_docs, comment="Marked as part of %s plugin." % plugin)

if __name__ == "__main__":
    main()
        

########NEW FILE########
__FILENAME__ = nyt_bestsellers_bot
#!/usr/bin/env python

import urllib2, urllib, sys, collections, re, os, site, datetime

local_site = os.path.join(os.path.dirname(__file__), "..", "..", "..")
site.addsitedir(local_site)

from optparse import OptionParser
import simplejson as json
import pprint
from openlibrary.api import OpenLibrary

NYT_BEST_SELLERS_URL = "http://api.nytimes.com/svc/books/v2/lists"

def LOG(level, msg):
    print >> sys.stderr, "%s: %s" % (level, msg.encode('utf-8'))

def _request(request, parser=json.loads):
    request = (urllib2.Request(request, None, headers={"Referrer": "http://www.openlibrary.org"})
               if isinstance(request, basestring)
               else request)
    results = None
    conn = urllib2.urlopen(request)
    try:
        results = conn.read()
        results = unicode(results, 'utf-8')
        results = parser(results)
    except Exception, e:
        LOG("ERROR", "error loading %s: %s results: %s" % (request, e, results))
        raise
    finally:
        conn.close()
    return results

def get_nyt_bestseller_list_names():
    url = "%s/%s.json?%s" % (NYT_BEST_SELLERS_URL, 
                             "names", 
                             urllib.urlencode({"api-key": NYT_API_KEY}))
    results = _request(url)
    assert 'results' in results
    assert len(results['results']) == results['num_results']
    return [r['list_name'] for r in results['results']]

def load_nyt_bestseller_list(list_name):
    url = "%s/%s.json?%s" % (NYT_BEST_SELLERS_URL, 
                             urllib.quote(list_name.replace(' ', '-')), 
                             urllib.urlencode({"api-key": NYT_API_KEY}))
    
    results = _request(url)
    assert 'results' in results

    if len(results['results']) != results['num_results']:
        LOG("ERROR", "expected %s result for %s, got %s" % (
            results['num_results'], len(results['results']), list_name))

    return results['results']

def _do_ol_query(type="/type/edition", **query):
    query.setdefault("type", type)
    return OL.query(query)
    

def reconcile_authors(authors):
    result = set()
    result.update(_do_ol_query(type='/type/author', name=authors.upper()))
    authors = " ".join([a.capitalize() for a in authors.split()])
    result.update(_do_ol_query(type='/type/author', name=authors))
    return result

def reconcile_book(book):
    result = set()
    for isbn10 in (x['isbn10'] for x in book['isbns']):
        for edition in  _do_ol_query(works={"title": None}, isbn_10=isbn10):
            result.add(edition['key'])
            result.update([x['key'] for x in edition['works'] or []])

    if result:
        LOG("INFO", "RECONCILED BY ISBN10: %s" % str(result))
        return result

    for isbn13 in (x['isbn13'] for x in book['isbns']):
        for edition in  _do_ol_query(works={"title": None}, isbn_13=str(isbn13)):
            result.add(edition['key'])
            result.update([x['key'] for x in edition['works'] or []])

    if result:
        LOG("INFO", "RECONCILED BY ISBN13: %s" % str(result))
        return result

    authors = reconcile_authors(book['book_details'][0]['author'])
    if not authors:
        authors = set()
        for a in re.split("(?: (?:and|with) )|(?:,|&)|(?:^edited|others$)", 
                          book['book_details'][0]['author']):
            authors.update(reconcile_authors(a))

    if not authors:
        LOG("INFO", "NO AUTHOR: %s" % pprint.pformat(book['book_details']))
        return []
    
    for a in authors:
        title = book['book_details'][0]['title']
        r = []
        r.extend(_do_ol_query(type="/type/work", authors={"author": {"key": str(a)}}, 
                              title=title))
        title = " ".join([t.capitalize() for t in title.split()])
        r.extend(_do_ol_query(type="/type/work", authors={"author": {"key": str(a)}},
                              title=title))
        if r:
            result.update([x['key'] for x in r])
            LOG("INFO", "RECONCILED BY AUTHOR: %s" % str(result))
            return result
    return result

def _get_first_bestseller_date(nyt):
    bd = nyt['bestsellers_date']
    wol = nyt['weeks_on_list']
    bd = datetime.datetime.strptime(bd, "%Y-%m-%d")
    wol = datetime.timedelta(days=wol * 7)
    result = bd - wol
    return result.date().isoformat()

def write_machine_tags(ln, books):
    key_to_nyt = {}
    for book in books:
        for work in book['ol:works']:
            key_to_nyt[work] = book['nyt']

    works = OL.get_many(list(set(key_to_nyt.keys())))
    write = {}
    for work in works.values():
        nyt = key_to_nyt[work['key']]
        tags = (
            "New York Times bestseller",
            "nyt:%s=%s" % ("_".join([s.lower() for s in ln.split()]),
                           _get_first_bestseller_date(nyt))
        )
        if 'subjects' not in work:
            work['subjects'] = list(tags)
            write[work['key']] = work
        else:
            for tag in tags:
                if tag not in work['subjects']:
                    work['subjects'].append(tag)
                    write[work['key']] = work
        # clean up any broken tags
        work['subjects'] = [s for s in work['subjects']
                            if not s.startswith(("nyt:", "nytimes:")) or s in tags]

        if work['key'] not in write:
            LOG("INFO", "all tags already present, skipping %s: '%s' by %s" % (
                work['key'], 
                nyt['book_details'][0]['title'], nyt['book_details'][0]['author']
            ))
        else:
            LOG("DEBUG", "Adding tags (%s) to %s" % (", ".join(tags), work['key']))
    LOG("INFO", "WRITING MACHINE TAGS FOR %s of %s works" % (
        len(write), len(books)
    ))
    if write:
        OL.save_many(write.values(), comment="Adding tags to New York Times %s bestsellers" % ln)


if __name__ == "__main__":
    op = OptionParser(usage="%prog [-a HOST:PORT] [-k nyt_api_key] -u [bot_username] -p [bot_password]")
    op.add_option("-a", "--api-host", dest="openlibrary_host", 
                  default="openlibrary.org:80",
                  help="The openlibrary API host")
    op.add_option("-k", "--nyt-api-key", dest="nyt_api_key", 
                  help="API key for use with the nyt bestsellers api")
    op.add_option("-u", "--bot-username", dest="username", 
                  default="nyt_bestsellers_bot",
                  help="The bot username for accessing the Open Library API")
    op.add_option("-p", "--bot-password", dest="password", 
                  help="The bot password for accessing the Open Library API")

    options, _ = op.parse_args()

    global NYT_API_KEY
    NYT_API_KEY = options.nyt_api_key
    global OL
    OL = OpenLibrary("http://%s" % options.openlibrary_host)
    OL.login(options.username, options.password)
    results = collections.defaultdict(list)
    for ln in get_nyt_bestseller_list_names():
        LOG("INFO", "processing list %s" % ln)
        for i, book in enumerate(load_nyt_bestseller_list(ln)):
            ol_keys = reconcile_book(book)
            if not ol_keys:
                LOG("WARN", "unable to reconcile '%s' by %s - no OL book found" % (
                    book['book_details'][0]['title'], book['book_details'][0]['author']
                ))
            if not (key for key in ol_keys if key.startswith("/works/")):
                LOG("WARN", "only editions for '%s' by %s: %s" % (
                    book['book_details'][0]['title'], book['book_details'][0]['author'], ol_keys
                ))
            results[ln].append({
                    "nyt": book, 
                    "ol:keys": ol_keys,
                    "ol:works": (key for key in ol_keys if key.startswith("/works/"))
            })
        if results[ln]:
            LOG("INFO", "RECONCILED %s%% of %s" % (int(len([r for r in results[ln] if r['ol:works']]) / 
                                                       float(len(results[ln])) * 100), 
                                             ln))
            write_machine_tags(ln, results[ln])
        else:
            LOG("WARN", "No bestsellers for %s" % ln)

########NEW FILE########
__FILENAME__ = gen_cover_mapping
#! /usr/bin/env python
"""Script to generate <coverid, key, isbns> mapping for each active cover.
"""
import _init_path
from openlibrary.data.dump import read_tsv
import web

import simplejson
import sys

def main(filename):
    for tokens in read_tsv(filename):
        json = tokens[-1]
        doc = simplejson.loads(json)
        
        cover = doc.get('covers') and doc.get('covers')[0]
        isbns = doc.get('isbn_10', []) + doc.get('isbn_13', [])
        key = doc['key']

        key = web.safestr(key)
        isbns = (web.safestr(isbn) for isbn in isbns)
        
        try:
            if cover and cover > 0:
                print "\t".join([str(cover), key, ",".join(isbns)])
        except:
            print >> sys.stderr, doc
            print >> sys.stderr, (key, cover, isbns)
            raise

if __name__ == '__main__':
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = infobase_couchdb_replay
#! /usr/bin/env python
"""Script to reload infobase log to a couchdb database.
"""
import couchdb
import simplejson

import _init_path
from infogami.infobase.logreader import LogFile

class CouchDBReplayer:
    def __init__(self, url):
        self.db = couchdb.Database(url)
        
    def get_offset(self):
        doc = self.db.get("@offset")
        return doc and doc['offset']
        
    def set_offset(self, offset):
        doc = dict(offset=offset)
        self.db['@offset'] = self.prepare_doc(doc, "@offset")
        
    def replay(self, log):
        offset = self.get_offset()
        if offset:
            log.seek(offset)
        print offset, log.tell()
            
        for line in log:
            item = simplejson.loads(line)
            self.replay_item(item)
            
        self.set_offset(log.tell())
        
    def replay_item(self, item):
        action = item.get("action")
        m = getattr(self, "replay_" + action, None)
        return m and m(item)
        
    def replay_save(self, item):
        data = item.get('data')
        doc = data and data.get('doc')
        changeset = data.get('changeset')
        if doc and changeset:
            return self._replay_save([doc], changeset)
        
    def replay_save_many(self, item):
        data = item.get('data')
        docs = data and data.get('docs')
        changeset = data.get('changeset')
        if docs and changeset:
            return self._replay_save(docs, changeset)
        
    def _replay_save(self, docs, changeset):
        print "replaying", changeset['id']
        for doc in docs:
            self.prepare_doc(doc, doc['key'])
        self.prepare_doc(changeset, "@changes/" + changeset['id'])
        
        self.db.update(docs + [changeset])
        
    def prepare_doc(self, doc, id):
        doc['_id'] = id

        old_doc = self.db.get(id)
        if old_doc:
            doc['_rev'] = old_doc['_rev']
        return doc
        
def main(couchdb_db_url, logdir):
    replayer = CouchDBReplayer(couchdb_db_url)
    log = LogFile(logdir)
    replayer.replay(log)
    
if __name__ == '__main__':
    import sys
    main(sys.argv[1], sys.argv[2])

########NEW FILE########
__FILENAME__ = _init_path
../../_init_path.py
########NEW FILE########
__FILENAME__ = crunch_logs
#!/usr/bin/env python

import gzip, re, urlparse, collections, datetime, httplib, csv, os, sys
import lxml.etree

from optparse import OptionParser


LOG_FORMAT = re.compile(r'^(?P<ip>[0-9.]+) (?P<host>[\S]+) (?P<dunno>[\S]+)\ '
                        '\[(?P<date>[^\]]+)\] "(?P<req>.*)" (?P<code>\d{3}) (?P<size>(\d+|-))\ '
                        '"(?P<referrer>.+)"\ "(?P<user_agent>.+)"\ (?P<offset>\d+)$')



def load_user_agent_map():
    conn = httplib.HTTPConnection("www.user-agents.org")
    try:
        conn.putrequest("GET", "/allagents.xml")
        conn.putheader("User-Agent", "OpenlibraryAnalyticsBot/1.0 (http://www.openlibrary.org)")
        conn.endheaders()
        resp = conn.getresponse()
        assert int(resp.status) == 200
        data = lxml.etree.fromstring(resp.read())
    finally:
        conn.close()
    result = {}
    for ua in data.iterchildren():
        t = ua.find("Type").text
        t = tuple(t.split()) if t else tuple()
        result[ua.find("String").text] = t
    return result

class UserAgent:
    """
    Counts user agent categorization as described by www.user-agents.org.
    Useful for distinguishing browser/bot requests.
    """

    name = "ua_type"
    
    def __init__(self):
        self._user_agent_map = load_user_agent_map()

    def map(self, line):
        # determine if the request is being made by a browser
        if line['user_agent'] not in user_agent_map:
            if ('bot' in line['user_agent'] 
                or 'http://' in line['user_agent']
                or line['user_agent'].startswith("Python-urllib")):
                # just make it a bot
                user_agent_map[line['user_agent']] = ('R',)
            else:
                user_agent_map[line['user_agent']] = ('B',)
                
        line['is_bot'] = 'B' not in user_agent_map.get(line['user_agent'])

        return dict((ua_type, 1) for ua_type in user_agent_map.get(line['user_agent'], []))

class ResponseCode:
    """
    Maps a line to its status code.
    """

    name = "response_code"

    def map(self, line):
        return {line['code']: 1}

class RequestType:
    """
    Fuzzy breakdown of request by its type (api, web, media, etc)
    """

    name = "request_type"

    def map(self, line):
        req_type = 'other'
        if line['req']:
            req = line['req'].split()
            if len(req) == 3:
                req = urlparse.urlsplit(req[1]).path
                if req.startswith(("/api", "/query.json")) or req.endswith(".json"):
                    req_type = 'api'
                elif req.startswith(("/css", "/images")):
                    req_type = 'media'
                else:
                    req_type = 'web'

        line['request_type'] = req_type
        return {req_type: 1}

class Referrer:
    """
    Maps to referrer for non bot requests.
    """

    name = "referrer"

    def map(self, line):
        if not line['referrer'] or line.get('is_bot', True) or line.get('request_type') != 'web':
            return None
        line['referrer'] = urlparse.urlsplit(line['referrer']).netloc
        if line['referrer'] in ('localhost', 'openlibrary.org'):
            line['referrer'] = None
        if line['referrer']:
            return {line['referrer']: 1}

class PageType:
    """
    Fuzzy page type breakdown (book, author, work, etc)
    """

    name = "page_type"
    
    def map(self, line):
        if line['code'] != 200:
            return
        if not line['req']:
            return {"unknown": 1}
        req = line['req'].split()
        if len(req) != 3:
            return {"unusual": 1}
        req = urlparse.urlsplit(req[1]).path
        if not req or req[0] != "/":
            print "%s: malformed %s" % (self.name, line['req'])
            return {"malformed": 1}
        if req.startswith(("/api/", "/query")):
            return {'api': 1}
        elif req.startswith(("/css/", "/images/", "/js/", "/static/", "/favicon.ico")):
            return None #{"images/js/css": 1}
        elif req.startswith(("/w/", "/works")):
            return {'work': 1}
        elif req.startswith(("/b/", "/books")):
            return {'books': 1}
        elif req.startswith(("/a/", "/authors")):
            return {'author': 1}
        elif req == "/":
            return {'home': 1}

        result = req.split("/")[1]
        if result.endswith(".json"):
            result = result[:-len(".json")]
        return {result: 1}

class ApiRequestType:
    """
    Fuzzy breakdown of api request type (work, author, book, query, etc)
    """

    name = "api_request_type"

    def map(self, line):
        req_type = 'other'
        if not line['req']:
            return
        req = line['req'].split()
        if len(req) != 3:
            return
        req = urlparse.urlsplit(req[1]).path
        if req.startswith("/query"):
            return {"query": 1}
        if req.endswith(".json"):
            req = req[:-len(".json")]
            return {"%s.json" % {
                    "w": "works", "works": "works",
                    "b": "books", "books": "books",
                    "a": "authors", "authors": "authors"
                    }.get(req.split("/")[1], req.split("/")[1]): 1}
        if req.startswith("/api"):
            req = req.split("/")
            if len(req) < 3:
                return {"unknown.api": 1}
            else:
                return {"%s.api" % req[2]: 1} 
            
        return None

def main():
    """
    Takes a stream of access logs and processes them with each of the processors.
    Example usage:
    # (find . -name 'access.log.gz' -exec zcat {} \; ; find . -name 'access.log' -exec cat {} \; ;) | crunch_logs.py -d /tmp/crunched
    """
    parser = OptionParser(usage="%prog -d [dest]")
    parser.add_option("-d", "--dest", dest="dest",
                      default="/tmp",
                      help="Where to emit tsvs")

    options, args = parser.parse_args()
    user_agent_map = load_user_agent_map()

    if len(args) == 1:
        fn = args[0]
        f = gzip.open(fn) if fn.endswith(".gz") else open(fn)
    else:
        f = sys.stdin

    # collection, thing, date, value
    data = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int)))

    processors = (
        ResponseCode(), RequestType(), UserAgent(), 
        Referrer(), PageType(), ApiRequestType()
    )

    for line in f:
        if not line:
            break
        try:
            line = LOG_FORMAT.match(line.strip()).groupdict()
        except Exception, e:
            print "FAIL: %s" % line
            continue
        if line['referrer'] == '-':
            line['referrer'] = None
        if line['dunno'] == '-':
            line['dunno'] = None

        if line['host'] != 'openlibrary.org':
            continue

        # some minor normalization of the line
        line.update({'code': int(line['code']), 
                     'size': None if line['size'] == '-' else int(line['size']), 
                     'offset': int(line['offset']),
                     'date': datetime.datetime.strptime(line['date'][:len("dd/mm/yyyy") + 1], "%d/%b/%Y").date()})

        for p in processors:
            # for each processor map to some data
            m = p.map(line)
            if not m:
                continue
            for value, count in m.iteritems():
                # the reduce step is implicitly a sum operation right now
                data[p.name][line['date']][value] += count

#    for d, stats in referrers.itermitems():
#        hosts_sorted = sorted(stats.keys(), key=lambda host: stats[host])
#        top_n = dict((host, stats[host]) for hosts_sorted[:100])
    for table_name, table in data.iteritems():
        # do some totals so we can filter down to top100 since more data than that is unmanagable
        # in a csv
        totals = collections.defaultdict(int)
        for day in table.itervalues():
            for value, count in day.iteritems():
                totals[value] += count

        data[table_name]['totals'] = totals

        # filter down to top 100 for each day
        top_100 = sorted(totals.keys(), key=lambda k: totals[k], reverse=True)[:100]
        for date in table.keys():
            table[date] = dict((k, table[date][k]) for k in table[date].iterkeys() if k in top_100)

    for table_name, table in data.iteritems():
        f = open(os.path.join(options.dest, "%s.csv" % table_name), 'w')
        out = csv.writer(f, delimiter='\t')
        for date, day in table.iteritems():
            for value, count in day.iteritems():
                out.writerow([date.isoformat() if isinstance(date, datetime.date) else date, value, count])
        f.close()

if __name__ == "__main__":
    main()

# helpfull shell stuff for debugging

#print('\n'.join(['\t'.join([("%s)" % (i + 1)).ljust(4), h.ljust(40), str(y[h])]) for i, h in enumerate(sh)]))
#grep 2010-09-21 referrers.csv | awk '{printf "%d\t%s\n", $3, $2}' | sort -nr
#(find tmp/logs/ -name '*.gz' -exec bash -c 'gzip -dc {} | head -n 10000'  \; ;) | python src/scratch/01.py -d tmp/y/
# for i in `find . -name *.gz`; do j=`echo $i | sed -e "s/\.\/\([0-9]*\)\/\([0-9]*\)\(.*\)/access_\1_\2.log.gz/g"`; echo $j; scp $i ariel@ia331503:/0/logs/$j; done
# (find . -name 'access.log.gz' -exec zcat {} \; ; find . -name 'access.log' -exec cat {} \; ;)

########NEW FILE########
__FILENAME__ = make_plot
#!/usr/bin/env python

import datetime, csv, os, sys, collections, subprocess, tempfile

from optparse import OptionParser

def main():
    """
    Uses gnuplot to graph a csv produced by crunch_logs.py
    """
    parser = OptionParser(usage="%prog file")
    parser.add_option("-l", "--limit", dest="limit", type="int",
                      default=10,
                      help="The maximum number of lines to emit")
    parser.add_option("-t", "--title", dest="title",
                      default=None,
                      help="The title for the plot")

    options, args = parser.parse_args()

    if len(args) != 1:
        parser.error("file required")

    in_fn = os.path.abspath(args[0])

    input = csv.reader(open(in_fn), delimiter='\t')
    data = collections.defaultdict(dict)
    dates = set()

    totals = {}

    for date, value, count in input:
        if date == "totals":
            totals[value] = count
        else:
            dates.add(date)
            data[value][date] = count

    values = sorted(totals.keys(), key=lambda k: int(totals[k]), reverse=True)[:options.limit]
    data = dict((k, v) for k, v in data.iteritems() if k in values)

    dates = sorted(dates)
    
    dat_f = tempfile.NamedTemporaryFile(prefix="%s.dat" % os.path.splitext(os.path.basename(in_fn))[0], 
                                        delete=False)
    plt_f = tempfile.NamedTemporaryFile(prefix="%s.plt" % os.path.splitext(os.path.basename(in_fn))[0], 
                                        delete=False)

    print "writing to %s, %s" % (plt_f.name, dat_f.name)
    output_fn = os.path.join(os.path.dirname(in_fn), "%s.png" % os.path.splitext(in_fn)[0])
    for line in (#"set terminal x11",
        'set terminal png medium size 1500,1500',
        "set xlabel 'date'",
        'set ylabel "num_requests"',
        "set xdata time",
        'set timefmt "%Y-%m-%d"',
        'set format x "%b %d"',
        'set format y "%-.0f"',
        'set output "%s"\n' % output_fn
    ):
        plt_f.write("%s\n" % line)

    if options.title:
        plt_f.write('set title "%s"\n' % options.title)

    plot_cmd = []
    for i, v in enumerate(values):
        plot_cmd.append('"%s" using 1:%d title "%s" with lines' % (dat_f.name, i + 2, v))
    
    plt_f.write("plot %s\n;" % ", \\\n\t".join(plot_cmd))

    dat_f.write("%s\n" % "\t".join(["#", "date"] + values))

    for date in dates:
        dat_f.write("\t%s\n" % "\t".join([date] + [str(data[v].get(date, 0)) for v in values]))

    #plt_f.write("pause -1\n")

    plt_f.close()
    dat_f.close()

    print "running %s" % " ".join(["gnuplot", plt_f.name])
    assert subprocess.call(["gnuplot", plt_f.name]) == 0
    print "plot written to %s" % output_fn

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = lendable_books
"""Script to find out-of-print books from a list of archive identifiers.

Availability of a book in print is found by checking OL and amazon.com for
latest edition and checking the published date.

This scripts uses [pyaws][1] library for querying amazon.com.

[1]: http://github.com/IanLewis/pyaws
"""
import simplejson
import yaml
import shelve
import urllib2
import re
import optparse
import sys

from xml.dom import minidom
from ConfigParser import ConfigParser

from pyaws import ecs

RE_YEAR = re.compile("(\d\d\d\d)")

class Command:
    def __init__(self):
        self.parser = optparse.OptionParser()
        self.init()
    
    def add_option(self, *a, **kw):
        self.parser.add_option(*a, **kw)
        
    def __call__(self, args):
        args = list(args)
        options, args = self.parser.parse_args(args)
        kwargs = options.__dict__
        self.run(*args, **kwargs)

class Summary(Command):
    """Prints summary statistics.
    """
    name = "summary"
    
    def init(self):
        self.add_option("-d", dest="database", help="name of the data file to use", default="books.db")
        
    def run(self, database):
        db = shelve.open(database)
        
        for k in db:
            d = db[k]
            if d.get('amazon'):
                books = d['amazon']
                latest_edition = self.find_latest_edition(books)
                
                title = d['ia'].get('title')
                
                if latest_edition['PublishedYear']:
                    print "\t".join([k, latest_edition['ASIN'], latest_edition['PublishedYear'], repr(title)])
            
    def find_latest_edition(self, amazon_books):
        """Finds the latest edition from the list of amazon books.
        
        Books which are on sale in amazon market place have ASINs starting
        with 'B' and they are ignored.
        """
        def extract_year(date_string):
            m = RE_YEAR.match(date_string)
            return m and m.group(1)
            
        def get_published_year(book):
            return extract_year(book.get('PublicationDate', '')) or 0
            
        for book in amazon_books:
            book['PublishedYear'] = get_published_year(book)
        
        sorted_books = sorted(amazon_books, key=lambda book: (not book['ASIN'].startswith("B"), book['PublishedYear']))
        return sorted_books[-1]
            
class LoadAmazon(Command):
    """Queries amazon.com to see check book availability.
    """    
    name = "load-amazon"
    
    def init(self):
        self.add_option("-d", dest="database", help="name of the data file to use", default="books.db")
        
    def run(self, database):
        db = shelve.open(database)
        
        self.setup_amazon_keys()
        
        for i, k in enumerate(db):
            d = db[k]
            if 'amazon' not in d and 'ia' in d:
                print >> sys.stderr, i, "querying amazon for", k
                doc = d['ia']

                title = doc.get('title') or ""
                authors = doc.get('authors')
                author = authors and authors[0] or ""

                try:
                    d['amazon'] = self.query_amazon(title, author)
                    db[k] = d
                except Exception:
                    print >> sys.stderr, "Failed to load amazon data for", k

    def query_amazon(self, title, author):
        """Queries amazon.com using its API to find all the books matching
        given title and author.
        """
        # strips dates from author names
        author = re.sub('[0-9-]+', ' ', author).strip()
        
        title = title.encode('utf-8').replace("/", " ")
        author = author.encode('utf-8').replace("/", " ")
        
        try:
            res = ecs.ItemSearch(None, SearchIndex="Books", Title=title, Author=author, ResponseGroup="Large")
        except KeyError, e:
            # Ignore nomathes error
            if 'ECommerceService.NoExactMatches' in str(e):
                return []
            else:
                raise

        def process(x):
            x = x.__dict__

            # remove unwanted data
            unwanted = [
                "ItemLinks",
                "DetailPageURL",
                "BrowseNodes",
                "Offers",
                "CustomerReviews",
                "ImageSets",
                "SmallImage", "MediumImage", "LargeImage", 
            ]
            for key in unwanted:
                x.pop(key, None)

            return x

        return [process(x) for x in take(50, res)]

    def setup_amazon_keys(self):
        config = ConfigParser()
        files = config.read([".amazonrc", "~/.amazonrc"])
        if not files:
            raise Exception("ERROR: Unable to find .amazonrc with access keys.")

        access_key = config.get("default", "access_key")
        secret = config.get("default", "secret")

        ecs.setLicenseKey(access_key)
        ecs.setSecretAccessKey(secret)
            

def load_settings(settings_file):
    return yaml.safe_load(open(settings_file).read())

def jsonget(url):
    json = urllib2.urlopen(url).read()
    return simplejson.loads(json)

def load_ol_data(settings, ia_id):
    url = settings["works_solr"] + "/select?wt=json&q=ia:%s" % ia_id
    response = jsonget(url)
    docs = response['response']['docs']
    if docs:
        return docs[0]
    
def load_ol(settings_file, shelve_file, ia_ids_file):
    settings = load_settings(settings_file)
    sh = shelve.open(shelve_file)

    for ia_id in open(ia_ids_file):
        ia_id = ia_id.strip()

        d = sh.get(ia_id, {})
        
        if not d.get("ol"):
            print "loading ol data for", ia_id, d
            d['ol'] = load_ol_data(settings, ia_id)
            sh[ia_id] = d
            
def load_ia(shelve_file, ia_ids_file):
    sh = shelve.open(shelve_file)

    for i, ia_id in enumerate(open(ia_ids_file)):
        ia_id = ia_id.strip()
        d = sh.get(ia_id, {})
        if not d.get("ia"):
            print i, "loading ia data for", ia_id
    
            try:
                d['ia'] = _load_ia_data(ia_id)
                sh[ia_id] = d        
            except Exception:
                print "ERROR: failed to load ia data for", ia_id
                import traceback
                traceback.print_exc()
            
def _load_ia_data(ia):
    url = "http://www.archive.org/download/%(ia)s/%(ia)s_meta.xml" % locals()
    xml = urllib2.urlopen(url).read()
    
    dom = minidom.parseString(xml)
    
    def get_elements(name):
        return [e.childNodes[0].data for e in dom.getElementsByTagName(name) if e.childNodes]

    def get_element(name):
        try:
            return get_elements(name)[0]
        except IndexError:
            return None
        
    return {
        "title": get_element("title"),
        "authors": get_elements("creator"),
        "addeddate": get_element("addeddate"),
        "collections": get_elements("collections"),
        "publisher": get_element("publisher"),
        "date": get_element("date"),
        "mediatype": get_element("mediatype"),
    }            
    
def _setup_amazon_keys():
    config = ConfigParser()
    files = config.read([".amazonrc", "~/.amazonrc"])
    if not files:
        print >> sys.stderr, "ERROR: Unable to find .amazonrc with access keys."
    
    access_key = config.get("default", "access_key")
    secret = config.get("default", "secret")

    ecs.setLicenseKey(access_key)
    ecs.setSecretAccessKey(secret)
    
def load_amazon(shelve_file):
    _setup_amazon_keys()

    sh = shelve.open(shelve_file)
    for i, k in enumerate(sh):
        d = sh[k]
        if 'amazon' not in d and 'ia' in d:
            print >> sys.stderr, i, "querying amazon for", k
            doc = d['ia']

            title = doc.get('title')
            authors = doc.get('authors')
            author = authors and authors[0] or ""
            
            d['amazon'] = _query_amazon(title, author)
            sh[k] = d

def _query_amazon(title, author):
    # strips dates from author names
    author = re.sub('[0-9-]+', ' ', author).strip()
    
    try:
        res = ecs.ItemSearch(None, SearchIndex="Books", Title=title, Author=author, ResponseGroup="ItemAttributes")
    except KeyError, e:
        # Ignore nomathes error
        if 'ECommerceService.NoExactMatches' in str(e):
            return []
        else:
            raise
            
    def process(x):
        x = x.__dict__
        
        # remove unwanted data
        x.pop("ItemLinks", None)
        x.pop("DetailPageURL", None)
        
        return x
        
    return [process(x) for x in take(50, res)]
    
def take(n, seq):
    """Takes first n elements from the seq."""
    seq = iter(seq)

    for i in range(n):
        yield seq.next()

def print_all(shelve_file):
    sh = shelve.open(shelve_file)
    for k in sh:
        print k + "\t" + simplejson.dumps(sh[k])
        
def debug(shelve_file):
    sh = shelve.open(shelve_file)
    for k in sh:
        d = sh[k]
        if 'amazon' in d:
            doc = d['amazon']
    
            for d in doc:
                if not d['ASIN'].startswith("B"):
                    print d
            break

def help():
    print __doc__

def main(cmd, *args):
    if cmd == "load_ol":
        load_ol(*args)
    elif cmd == "load_ia":
        load_ia(*args)
    elif cmd == "load_amazon":
        LoadAmazon()(args)
    elif cmd == "print":
        print_all(*args)
    elif cmd == "debug":
        debug(*args)
    elif cmd == "summary":
        Summary()(args)
    else:
        help()

if __name__ == "__main__":
    main(*sys.argv[1:])

########NEW FILE########
__FILENAME__ = compare_subject_works
#! /usr/bin/env
"""Script to compare subject info in solr and couchdb.
"""
import sys
import simplejson
import web
import urllib2
import time

couchdb_url = "http://ia331510:5984/works/_design/seeds/_view/seeds"

def wget(url):
    print >> sys.stderr, time.asctime(), "wget", url
    return urllib2.urlopen(url).read()

def jsonget(url):
    return simplejson.loads(wget(url))

def get_couchdb_works(subject):
    url = couchdb_url + "?key=" + simplejson.dumps(subject)
    rows = jsonget(url)['rows']
    return [row['id'] for row in rows]
    
def get_subjects(work):
    def get(name, prefix):
        return [prefix + s.lower().replace(" ", "_") for s in work.get(name, []) if isinstance(s, basestring)]
        
    return get("subjects", "subject:") \
        + get("subject_places", "place:") \
        + get("subject_times", "time:") \
        + get("subject_people", "person:")
        
def get_solr_works(subject):
    subject = web.lstrips(subject, "subject:")
    url = "http://openlibrary.org/subjects/%s.json?limit=10000" % subject
    data = jsonget(url)
    return [w['key'] for w in data['works']]
    
def get_doc(key):
    doc = jsonget("http://openlibrary.org" + key + ".json")
    return doc
    
def compare_works(subject):
    solr_works = get_solr_works(subject)
    couch_works = get_couchdb_works(subject)
    
    works = dict((k, None) for k in solr_works + couch_works)
    for key in works:
        doc = get_doc(key)
        subjects = get_subjects(doc)
        works[key] = {"key": key, "subject": subject in subjects, "solr": key in solr_works, "couch": key in couch_works}
        
    for w in sorted(works.values(), key=lambda w: (w['solr'], w['couch'], w['subject'])):
        print w["key"], w['subject'], w['solr'], w['couch']

if __name__ == "__main__":
    compare_works(sys.argv[1])

########NEW FILE########
__FILENAME__ = fix_redirects
#! /usr/bin/env python
"""Script to fix redirects in OL database.

In the OL database the redirect docs are still using /a/foo and /b/foo 
instead of /authors/foo and /books/foo. This script fixes it.
    
USAGE:

    python fix_redirects.py openlibrary.yml infobase.yml
"""

import sys
import simplejson
import yaml
import memcache

import _init_path
from infogami.infobase.server import parse_db_parameters
from openlibrary.data import db

def read_config(filename):
    return yaml.safe_load(open(filename).read())
    
def setup_mc(ol_config_file):
    config = read_config(ol_config_file)
    servers = config.get("memcache_servers")
    return memcache.Client(servers)

def setup_db(infobase_config_file):
    config = read_config(infobase_config_file)
    db_parameters = parse_db_parameters(config['db_parameters'])
    db.setup_database(**db_parameters)
    
def get_type_redirect():
    return db.db.query("SELECT id FROM thing WHERE key='/type/redirect'")[0].id
    
def longquery(query, vars, callback):
    """Executes a long query using SQL cursors and passing a chunk of rows to the callback function in each iteration.
    """
    for rows in db.longquery(query, vars=vars):
        t = db.db.transaction()
        try:
            callback(rows)
        except:
            t.rollback()
            raise
        else:
            t.commit()
            
def fix_doc(doc):
    if 'location' in doc and (doc['location'].startswith("/a/") or doc['location'].startswith("/b/")):
        doc['location'] = doc['location'].replace('/a/', '/authors/').replace('/b/', '/books/')
    return doc

def fix_json(json):
    doc = simplejson.loads(json)
    doc = fix_doc(doc)
    return simplejson.dumps(doc)
            
def fix_redirects(rows):
    rows = [dict(thing_id=r.thing_id, revision=r.revision, data=fix_json(r.data)) for r in rows]
    
    db.db.query("CREATE TEMP TABLE data_redirects (thing_id int, revision int, data text, UNIQUE(thing_id, revision))")
    db.db.multiple_insert('data_redirects', rows)

def main(ol_config_file, infobase_config_file):
    setup_mc(ol_config_file)
    setup_db(infobase_config_file)
    
    type_redirect = get_type_redirect()
    
    query = "SELECT data.* FROM thing, data WHERE thing.type=$type_redirect AND thing.id=data.thing_id"
    longquery(query, vars=locals(), callback=fix_redirects)
    
def test_fix_doc():
    assert fix_doc({}) == {}
    assert fix_doc({'location': '/a/foo'}) == {'location': '/authors/foo'}
    assert fix_doc({'location': '/b/foo'}) == {'location': '/books/foo'}
    assert fix_doc({'location': '/c/foo'}) == {'location': '/c/foo'}

if __name__ == '__main__':
    main(sys.argv[1], sys.argv[2])
########NEW FILE########
__FILENAME__ = loan_status
#!/usr/bin/env python

# Simple script to check if a book is part of "Lending library" and currently available
# for loan
# 
# $ python scripts/2011/02/loan_status.py /books/OL6244901M && echo yes
# Not available
# 
# $ python scripts/2011/02/loan_status.py /books/OL6390314M && echo yes
# Available
# yes

import urllib2
import simplejson

host = 'http://openlibrary.org'
#host = 'http://mang-dev.us.archive.org:8080'

def get_loan_status(edition_key):
    global host
    status_url = '%s%s/loan_status/_borrow_status' % (host, edition_key)
    try:
        req = urllib2.Request(status_url)
        opener = urllib2.build_opener()
        f = opener.open(req)
        return simplejson.load(f)
    except:
        return None

def main():
    import sys
    if len(sys.argv) < 2:
        print "Usage: loan_status.py /books/OL123M"
        sys.exit(-1)
        
    status = get_loan_status(sys.argv[1])
    # print status
    if status is None:
        print 'Error retrieving status'
        sys.exit(1)
    elif status['loan_available'] and 'Lending library' in status['lending_subjects']:
        print 'Available'
        sys.exit(0)
    else:
        print 'Not available'
        sys.exit(2)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = _init_path
../../_init_path.py
########NEW FILE########
__FILENAME__ = get_titles
import web, psycopg2, sys

conn = psycopg2.connect("dbname='openlibrary' host='ol-db'")
cur = conn.cursor()

cur.execute("select id, key from thing where key='/type/edition'")
edition_id = cur.fetchone()[0]
print edition_id

get_fields = ["'%s'" % i for i in ['title','work_title','work_titles','title_prefix','subtitle']]
sql = "select id, name from property where type=52 and name in (%s)" % ','.join(get_fields)
print sql
cur.execute(sql)
properties = dict(cur.fetchall())
print properties

keys = ', '.join(`k` for k in properties.keys())
#sql = "select key, key_id, value, ordering from edition_str, thing where key_id in (%s) and thing_id=thing.id and type=%d" % (keys, edition_id)
sql = "select thing_id, key_id, value, ordering from edition_str where key_id in (%s)" % (keys,)
print sql
cur.execute(sql)

out = open('/1/labs/titles/list', 'w')
rows = cur.fetchmany()
while rows:
    for key, key_id, value, ordering in rows:
        print >> out, (key, properties[key_id], value, ordering)
    rows = cur.fetchmany()
out.close()

########NEW FILE########
__FILENAME__ = oldump
"""Script to dump OL database from a given date and restore it to another db.

This script must be run on the db node.
"""
import os, sys
import web
import simplejson
from infogami.infobase._dbstore.save import IndexUtil
from openlibrary.core import schema

db = None

engine = "postgres"
dbname = "openlibrary"
        
class RestoreEngine:
    """Engine to update an existing database with new changes from a dump.
    """
    def __init__(self, dirname):
        self.dirname = dirname
        self.index_engine = IndexUtil(db, schema.get_schema())
        
    def path(self, filename):
        return os.path.abspath(os.path.join(self.dirname, filename))
        
    def restore(self):
        self.restore_transactions()
        self.restore_tables()
        self.restore_sequences()
        
    def restore_sequences(self):
        d = simplejson.loads(open(self.path("sequences.txt")).read())
        
        for name, value in d.items():
            db.query("SELECT setval($name, $value)", vars=locals())
        
    def restore_tables(self):
        # some tables can't be restored before some other table is restored because of foreign-key constraints.
        # This dict specified the order. Smaller number must be restored first.
        order = {
            "store": 1,
            "store_index": 2
        }
        
        tables = [f[len("table_"):-len(".txt")] for f in os.listdir(self.dirname) if f.startswith("table_")]
        tables.sort(key=lambda key: order.get(key, 0))
        
        for t in tables[::-1]:
            db.query("DELETE FROM %s" % t)

        for t in tables:
            filename = self.path("table_%s.txt" % t)
            db.query("COPY %s FROM $filename" % t, vars=locals())
            
    def get_doc(self, thing_id, revision):
        d = db.query("SELECT data FROM data WHERE thing_id=$thing_id AND revision=$revision", vars=locals())
        try:
            return simplejson.loads(d[0].data)
        except IndexError:
            return {}
            
    def restore_tx(self, row):
        data = row.pop("_versions")

        tx = db.transaction()
        try:
            old_docs = []
            new_docs = []
            for d in data:
                id = d['thing_id']

                doc = simplejson.loads(d['data'])
                key = doc['key']
                type_id = self.get_thing_id(doc['type']['key'])

                if d['revision'] == 1:
                    db.insert("thing", seqname=False, 
                        id=d['thing_id'], key=key, type=type_id,
                        latest_revision=d['revision'],
                        created=row['created'], last_modified=row['created'])
                else:
                    db.update('thing', where="id=$id", 
                        type=type_id,
                        latest_revision=d['revision'],
                        last_modified=row['created'], 
                        vars=locals())
                    old_docs.append(self.get_doc(d['thing_id'], d['revision']-1))
                new_docs.append(doc)

            db.insert("transaction", seqname=False, **row)

            values = [{"id": d['version_id'], "thing_id": d['thing_id'], "revision": d['revision'], "transaction_id": row['id']} for d in data]
            db.multiple_insert("version", values, seqname=False)

            values = [{"data": d['data'], "thing_id": d['thing_id'], "revision": d['revision']} for d in data]
            db.multiple_insert("data", values, seqname=False)
            
            self.delete_index(old_docs)
            self.insert_index(new_docs)
        except:
            tx.rollback()
            raise
        else:
            tx.commit()
        
    def restore_transactions(self):
        for line in open(self.path("transactions.txt")):
            row = simplejson.loads(line)
            if self.has_transaction(row['id']):
                print "ignoring tx", row['id']
                continue
            else:
                self.restore_tx(row)
                
    def has_transaction(self, txid):
        d = db.query("SELECT id FROM transaction WHERE id=$txid", vars=locals())
        return bool(d)

    def get_thing_id(self, key):
        return db.query("SELECT id FROM thing WHERE key=$key", vars=locals())[0].id
        
    def _process_key(self, key):
        # some data in the database still has /b/ instead /books. 
        # The transaformation is still done in software.
        mapping = (
            "/l/", "/languages/",
            "/a/", "/authors/",
            "/b/", "/books/",
            "/user/", "/people/"
        )

        if "/" in key and key.split("/")[1] in ['a', 'b', 'l', 'user']:
            for old, new in web.group(mapping, 2):
                if key.startswith(old):
                    return new + key[len(old):]
        return key
            
    def delete_index(self, docs):
        all_deletes = {}
        for doc in docs:
            doc = dict(doc, _force_reindex=True)
            dummy_doc = {"key": self._process_key(doc['key']), "type": {"key": "/type/foo"}}
            deletes, _inserts = self.index_engine.diff_index(doc, dummy_doc)
            all_deletes.update(deletes)
            
        all_deletes = self.index_engine.compile_index(all_deletes)
        self.index_engine.delete_index(all_deletes)
        
    def insert_index(self, docs):
        all_inserts = {}
        for doc in docs:
            _deletes, inserts = self.index_engine.diff_index({}, doc)
            all_inserts.update(inserts)
            
        all_inserts = self.index_engine.compile_index(all_inserts)
        self.index_engine.insert_index(all_inserts)
    
    
class DumpEngine:
    def __init__(self, dirname):
        self.dirname = dirname
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        
        # make sure postgres can write to the dir. Required to copy tables.
        os.system("chmod 777 " + dirname)
        
    def path(self, filename):
        return os.path.abspath(os.path.join(self.dirname, filename))
        
    def dump(self, date):
        self.dump_transactions(date)
        self.dump_tables()
        self.dump_sequences()
        
    def dump_transactions(self, date):
        txid = db.query("SELECT id FROM transaction where created >= $date order by id limit 1", vars=locals())[0].id
        txid = txid-1 # include txid also

        f = open(self.path("transactions.txt"), "w")
        for tx in self.read_transactions(txid):
            row = simplejson.dumps(tx) + "\n"
            f.write(row)
        f.close()
            
    def dump_tables(self):
        for t in ["account", "store", "store_index", "seq"]:
            filename = self.path("table_%s.txt" % t)
            db.query("COPY %s TO $filename" % t, vars=locals())
            
    def dump_sequences(self):
        sequences = db.query("SELECT c.relname as name FROM pg_class c WHERE c.relkind = 'S'")
        d = {}
        for seq in sequences:
            d[seq.name] = db.query("SELECT last_value from %s" % seq.name)[0].last_value

        f = open(self.path("sequences.txt"), "w")
        f.write(simplejson.dumps(d) + "\n")
        f.close()
    
    def read_transactions(self, txid):
        """Returns an iterator over transactions in the db starting from the given transaction ID.
        """
        while True:
            rows = db.query("SELECT * FROM transaction where id >= $txid order by id limit 100", vars=locals()).list()
            if not rows:
                return

            for row in rows:
                data = db.query("SELECT version.id as version_id, data.*"
                    + " FROM version, data"
                    + " WHERE version.thing_id=data.thing_id"
                    + " AND version.revision=data.revision"
                    + " AND version.transaction_id=$row.id",
                    vars=locals())
                row._versions = data.list()
                row.created = row.created.isoformat()
                yield row
            txid = row.id+1
        
def main():
    global db
    db = web.database(dbn=engine, db=dbname, user=os.getenv("USER"), pw="")
    
    if "--restore" in sys.argv:
        sys.argv.remove("--restore")
        e = RestoreEngine(sys.argv[1])
        e.restore()
    else:
        dirname = sys.argv[1]
        date = sys.argv[2]
        e = DumpEngine(dirname)
        e.dump(date)
    
if __name__ == '__main__':
    main()    

########NEW FILE########
__FILENAME__ = sample_bulkwrite
"""Sample script to update OL records in bulk.
"""

from openlibrary.api import OpenLibrary
import web
import sys

ol = OpenLibrary()

def read_mapping(filename, chunksize=1000):
    """Reads OLID, OCLC_NUMBER mapping from given file.

    Assumes that the file contains one OLID, OCLC_NUMBER per line, separated by tab.
    """
    for line in open(filename):
        olid, oclc = line.strip().split("\t")
        yield olid, oclc

def add_identifier(doc, id_name, id_value):
    if id_name in ["isbn_10", "isbn_13", "oclc_numbers", "lccn"]:
        ids = doc.setdefault(id_name, [])
    else:
        ids = doc.setdefault("identifiers", {}).setdefault(id_name, [])

    if id_value not in ids:
        ids.append(id_value)

def has_identifier(doc, id_name, id_value):
    if id_name in ["isbn_10", "isbn_13", "oclc_numbers", "lccn"]:
        return id_value in doc.get(id_name, [])
    else:
        return id_value in doc.get("identifiers", {}).get(id_name, [])

def get_docs(keys):
    # ol.get_many returns a dict, taking values() to get the list of docs
    return ol.get_many(keys).values() 

def add_oclc_ids(filename):
    """Adds OCLC Ids to OL records.
    """
    for mapping in web.group(read_mapping(filename), 1000):
        mapping = dict(mapping)

        docs = get_docs(mapping.keys())

        # ignore docs that already have the oclc_number that we are about to set
        docs = [doc for doc in doc if not has_identifier(doc, "oclc_numbers", mapping[doc['key']])

        for doc in docs:
            add_identifier(doc, "oclc_numbers", mapping[doc['key']])

        if docs:
            ol.save_many(docs, comment="Added OCLC numbers.")

def main():
    ol.login("bot-name", "password-here")
    add_oclc_ids(sys.argv[1])

########NEW FILE########
__FILENAME__ = titles_from_couch
from urllib import urlopen
from openlibrary.catalog.utils import mk_norm
import json, re, codecs
#import couchdb, sys
#couch = couchdb.Server('http://ol-couch0.us.archive.org:5984/')
#db = couch['editions']

#for num, couch_id in enumerate(db):
#    print couch_id, couch_id, db[id]

re_parens = re.compile('\s*\((.*)\)\s*')

out = codecs.open('/1/labs/titles/list2', 'w', 'utf-8')

def cat_title(prefix, title):
    if not prefix:
        return mk_norm(title)
    if prefix[-1] != ' ':
        prefix += ' '
    return mk_norm(prefix + title)

def process(title_prefix, title, key):
    print >> out, cat_title(title_prefix, title), key
    if '(' in title and ')' in title:
        t1 = cat_title(title_prefix, re_parens.sub(' ', title))
        print >> out, t1, key
        t2 = cat_title(title_prefix, re_parens.sub(lambda m: ' ' + m.group(1) + ' ', title))
        print >> out, t2, key

def iter_editions():
    url = 'http://ol-couch0.us.archive.org:5984/editions/_all_docs?include_docs=true'
    first_line = True
    for line in urlopen(url):
        if first_line:
            print line
            first_line = False
            continue
        line = line.strip()
        assert line[-1] == ','
        line = line[:-1]
        d = json.loads(line)
        title = d['doc'].get('title')
        key = d['key']
        if not title:
            continue
        title_prefix = d['doc'].get('title_prefix')
        process(None, title, key)
        if title_prefix:
            process(title_prefix, title, key)

if __name__ == '__main__':
    iter_editions()


########NEW FILE########
__FILENAME__ = reindex_transactions
"""Script to reindex specified transactions in the openlibrary database.

Some transcations/changesets in the openlibrary database have been not indexed
because of a bug. This script fixes the old records by reindexing the given
transactions.

USAGE:

    $ python reindex_transactions.py transaction_id [transaction_id, ...]

It is possible to query the database to get the transaction ids and pass them to the script

    $ psql openlibrary -t  -c 'select id from transaction order by id desc limit 5' | xargs python reindex_transactions.py

"""

import logging
import sys
import os

import _init_path

import simplejson
import web
from infogami.infobase._dbstore.save import SaveImpl

logger = logging.getLogger("openlibrary.transactions")

@web.memoize
def get_db():
    # assumes that the name of the db is openlibrary
    return web.database(dbn="postgres", db="openlibrary", user=os.getenv("USER"), pw="")

def reindex_transaction(id):
    logging.info("reindexing %s", id)

    db = get_db()
    impl = SaveImpl(db)

    with db.transaction():
        tx = db.query("SELECT id, data FROM transaction where id=$id", vars=locals())[0]
        db.delete("transaction_index", where="tx_id=$id", vars=locals())

        logging.info("tx.data %s", tx.data)
        if tx.data:
            data = simplejson.loads(tx.data)
            impl._index_transaction_data(id, data)

def verify_index(tx_id):
    logging.info("verifying %s", tx_id)
    db = get_db()
    impl = SaveImpl(db)
    tx = db.query("SELECT id, action, created, data FROM transaction where id=$tx_id", vars=locals())[0]

    if tx.data:
        d = db.query("SELECT * FROM transaction_index WHERE tx_id=$tx_id", vars=locals())
        index = sorted((row.key, row.value) for row in d)

        tx_data = simplejson.loads(tx.data)
        index2 = compute_index(tx_data)
        if index != index2:
            print "\t".join([str(x) for x in [tx.id, tx.action, tx.created.isoformat(), index, index2]])

def compute_index(tx_data):
    d = []
    def index(key, value):
        if isinstance(value, (basestring, int)):
            d.append((key, value))
        elif isinstance(value, list):
            for v in value:
                index(key, v)

    for k, v in tx_data.iteritems():
        index(k, v)
    return sorted(d)

def main():
    logger.info("BEGIN")
    for id in sys.argv[1:]:
        reindex_transaction(id)
    logger.info("END")

def main_verify():
    logger.info("BEGIN")
    for id in sys.argv[1:]:
        verify_index(id)
    logger.info("END")

if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s [%(levelname)s] %(message)s")

    if "--verify" in sys.argv:
        sys.argv.remove("--verify")
        main_verify()
    else:
        main()
########NEW FILE########
__FILENAME__ = smashwords
import csv, httplib, sys, codecs, re
from openlibrary.api import OpenLibrary
from pprint import pprint, pformat

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

h1 = httplib.HTTPConnection('www.archive.org')
h1.connect()

ol = OpenLibrary('http://openlibrary.org/')
#ol.login('EdwardBot', 'As1Wae9b')

input_file = '/home/edward/Documents/smashwords_ia_20110325.csv'
input_file = '/home/edward/Documents/smashwords_ia_20110325-extended-20110406.csv'

#headings = ['Distributor', 'Author Name', 'Author Bio', 'Publisher', 'SWID',
#    'Book Title', 'Price', 'Short Book Description', 'Long Book Description',
#    'ISBN', 'BISAC I', 'BISAC II', 'Where to buy']

# ['Distributor', 'Author Name', 'Author Bio', 'Publisher', 'SWID',
# 'Book Title', 'Pub. Date @ Smashwords', 'Price', 'Short Book Description',
# 'Long Book Description', 'ISBN', 'BISAC I', 'BISAC II', 'Word Count (Approx.)',
# 'Where to buy']

authors = {}
titles = set()
isbn = set()

name_map = {
    'JA Konrath': 'J. A. Konrath'
}

done = set(['978-1-4523-2945-1', '978-1-4523-0368-0', '978-1-4523-4686-1',
            '978-0-981-94368-8', '978-0-987-05532-3', '978-0-987-05533-0',
            '978-0-987-05533-0', '978-1-4523-3743-2', '978-1-4523-4108-8',
            '978-1-4523-1429-7', '978-1-4523-9020-8', '978-1-4523-2207-0',
            '978-1-4523-9091-8', '978-1-4523-8731-4', '978-1-4523-7639-4',
            'SW00000046140', '978-1-4523-4426-3', '978-1-4523-0687-2',
            '978-1-4523-0686-5', '978-1-4523-0684-1', '978-1-4523-0683-4',
            '978-1-4524-0245-1', '978-1-4524-0243-7', '978-1-102-46655-0',
            '978-1-4523-0372-7', '978-1-4523-0321-5', '978-1-4523-6775-0',
            '978-1-4580-7969-5', '978-1-4523-8020-9', '978-1-4581-8282-1',
            '978-1-4523-5564-1', 'SW00000047707', '978-1-4523-5204-6',
            '978-1-4523-1740-3', '978-1-4523-5559-7', '978-1-4523-5601-3',
            '978-1-4581-3793-7', '978-1-4523-8141-1', '978-1-4523-5065-3',
            '978-1-4580-0894-7', '978-1-4580-8419-4', '978-1-4523-2360-2',
            '978-1-4523-1861-5', '978-1-4523-9376-6', '978-1-4581-3742-5',
            '978-1-4523-6574-9', '978-1-4523-2484-5', '978-1-4523-2607-8',
            '978-1-4523-7547-2', '978-1-4523-5034-9', '978-0-976-22351-1',
            '978-1-102-46644-4', '978-1-4523-2826-3', '978-1-102-46945-2',
            '978-1-4523-4192-7', '978-1-4523-0388-8', '978-1-4523-0385-7',
            '978-1-4523-0382-6', '978-1-4523-0376-5', '978-1-4523-0375-8',
            '978-1-4523-0367-3', '978-1-4523-0362-8', '978-1-4523-0359-8',
            '978-1-4523-0353-6', '978-1-4523-0346-8', '978-1-4523-0341-3',
            '978-1-4523-0330-7', '978-1-4523-0317-8', '978-1-4523-0309-3',
            '978-1-4523-0286-7', '978-0-986-59422-9', '978-1-4523-5506-1',
            '978-1-4523-7693-6', '978-1-4523-8473-3', '978-1-4523-2417-3',
            '978-1-4523-5642-6', '978-1-4523-2123-3', '978-1-4523-4620-5',
            '978-1-4523-6227-4', '978-1-4523-3639-8', '978-1-4523-7777-3',
            '978-1-4523-8301-9', '978-1-4523-1636-9', '978-1-4523-7388-1',
            '978-1-4523-7016-3', '978-1-4523-8655-3', '978-1-4523-8749-9',
            '978-1-4523-9632-3'])

print len(done)

re_pub_date = re.compile('^(20\d\d)/(\d\d)/(\d\d) \d\d:\d\d:\d\d$')

headings = None
check_ia = False
for row in csv.reader(open(input_file)):
    if not headings:
        headings = row
        print row
        continue
    book = dict(zip(headings, [s.decode('utf-8') for s in row]))
    if book['Distributor'] != 'Smashwords':
        print book['Distributor']
    assert book['Distributor'] == 'Smashwords'
    assert book['Publisher'] == ''
    assert book['SWID'].isdigit()
    author_name = book['Author Name']
    if author_name in name_map:
        author_name = name_map[author_name]
    author_bio = book['Author Bio']
    isbn = book['ISBN']

    #if isbn:
    #    existing = ol.query({'type':'/type/edition', 'isbn_13': isbn})
    #print existing
    #continue

    title = book['Book Title']
    #print `isbn, title`
    assert isbn == '' or isbn.replace('-', '').isdigit()
    assert title not in titles
    titles.add(title)
    if book['Author Name'] not in authors:
        authors[author_name] = {
            'name': book['Author Name'],
            'bio': author_bio,
            'editions': []
        }
    else:
        assert authors[author_name]['bio'] == author_bio
    author = authors[author_name]

    ia = isbn if isbn else 'SW000000' + book['SWID']

    if ia in done:
        continue

    pprint(book)

    if check_ia:
        h1.request('GET', 'http://www.archive.org/download/' + ia)
        res = h1.getresponse()
        print res.getheader('location')
        res.read()

    m = re_pub_date.match(book['Pub. Date @ Smashwords'])
    date = '-'.join(m.groups())

    edition = { 'title': title, 'ia': ia, 'publish_date': date, }
    if isbn:
        edition['isbn_13'] = isbn.replace('-', '')
    if book['Long Book Description']:
        edition['description'] = book['Long Book Description']
    elif book['Short Book Description']:
        edition['description'] = book['Short Book Description']
    #print edition
    author['editions'].append(edition)

#sys.exit(0)

print len(authors)

authors_done = set([u'Zoe Winters', u'Derek Ciccone', u'Shayne Parkinson', u'Joanna Penn'])
if False:
    for k in [u'Zoe Winters', u'Derek Ciccone', u'Shayne Parkinson', u'Joanna Penn']:
        v = authors[k]
        akey = ol.new({
            'type': '/type/author',
            'name': k,
            'bio': v['bio'],
        })
        print
        print akey, k
        for e in v['editions']:
            wkey = ol.new({
                'type': '/type/work',
                'title': e['title'],
                'authors': [{'author': {'key': akey}}],
                'description': e['description'],
                'subjects': ['Lending library'],
            })
            print wkey, e['title']
            edition = {
                'type': '/type/edition',
                'title': e['title'],
                'authors': [{'key': akey}],
                'works': [{'key': wkey}],
                'ocaid': e['ia'],
                'publishers': ['Smashwords'],
            }
            if 'isbn' in e:
                edition['isbn'] = e['isbn']
            ekey = ol.new(edition)
            print ekey, e['title']
        continue

update = {}
for k, v in authors.items():
    if not v['editions']:
        continue
    authors = ol.query({'type': '/type/author', 'name': k})
    assert authors
    if not authors:
        assert k in authors_done
        continue
        author_done.append(k)
        print {'name': k, 'bio': v['bio'] }
        for e in v['editions']:
            pprint({
                'title': e['title'],
                'authors': {'author': {'key': 'xxx'}},
                'description': e['description'],
            })
        print
        continue
        print 'bio:', `v['bio']`
        print 'editions'
        pprint(v['editions'])
        print
    if authors:
        akey = authors[0]
        #print k, map(str, authors)
        q = {
            'type': '/type/work',
            'authors': {'author': {'key': akey} },
            'title': None,
        }
        works = list(ol.query(q))
        work_titles = [(w['title'].lower(), w) for w in works]
        for e in v['editions']:
            q = {'type': '/type/edition', 'ocaid': e['ia']}
            existing = list(ol.query(q))
            if existing:
                print existing
                print 'skip existing:', str(existing[0]), e['ia']
                continue
            print e
            assert not any(e['title'].lower().startswith(t) for t, w in work_titles)
            if k not in update:
                update[k] = dict((a, b) for a, b in v.items() if a != 'editions')
                update[k]['key'] = str(akey)
                update[k]['editions'] = []
            update[k]['editions'].append(e)
            continue
            for t, w in work_titles:
                if not e['title'].lower().startswith(t):
                    continue
                if k not in update:
                    update[k] = dict((a, b) for a, b in v.items() if a != 'editions')
                    update[k]['key'] = str(akey)
                    update[k]['editions'] = []
                e['work'] = w['key']
                update[k]['editions'].append(e)
                q = {'type': '/type/edition', 'works': w['key'], 'ocaid': None}
                assert all(not e['ocaid'] for e in ol.query(q))
                print dict((a,b) for a, b in v.items() if a != 'editions')
                print 'new:', e
                print 'work:', w
                print
                break

h1.close()

pprint(update.values(), stream=open('update2', 'w'))

########NEW FILE########
__FILENAME__ = smashwords_cover
import csv
from openlibrary.catalog.title_page_img.load import add_cover_image

input_file = 'smashwords_ia_20110325-extended-20110406.csv'

headings = None
for row in csv.reader(open(input_file)):
    if not headings:
        headings = row
        print row
        continue
    book = dict(zip(headings, [s.decode('utf-8') for s in row]))

    isbn = book['ISBN']

    ia = isbn if isbn else 'SW000000' + book['SWID']

    q = {'type':'/type/edition', 'ocaid': ia, 'works': None}
    existing = list(ol.query(q))
    print (existing[0]['key'], ia)
    add_cover_image(existing[0]['key'], ia)

########NEW FILE########
__FILENAME__ = smashwords_load
from openlibrary.api import OpenLibrary

ol = OpenLibrary('http://openlibrary.org/')

data = eval(open('update').read())

done = []

for author in data:
    print (author['key'], author['name'])
    akey = author['key']
    a = ol.get(akey)
    if not a.get('bio') and author['bio']:
        a['bio'] = author['bio']
        ol.save(akey, a, 'Add author bio from Smashwords.')

    for edition in author['editions']:
#        wkey = ol.new({
#            'type': '/type/work',
#            'title': edition['title'],
#            'authors': [{'author': {'key': akey}}],
#            'description': edition['description'],
#            'subjects': ['Lending library'],
#        })
#
        wkey = edition['work']
        w = ol.get(wkey)
        assert edition['description']
        if not w.get('description'):
            w['description'] = edition['description']
        if 'Lending library' not in w.get('subjects', []):
            w.setdefault('subjects', []).append('Lending library')
            ol.save(wkey, w, 'Add lending edition from Smashwords')
        continue

        q = {'type': '/type/edition', 'ocaid': edition['ia']}
        existing = list(ol.query(q))
        if existing:
            print existing
            print 'skip existing:', str(existing[0]), edition['ia']
            continue

        e = {
            'type': '/type/edition',
            'title': edition['title'],
            'authors': [{'key': akey}],
            'works': [{'key': wkey}],
            'ocaid': edition['ia'],
            'publishers': ['Smashwords'],
            'publish_date': edition['publish_date'],
        }
        if 'isbn' in edition:
            e['isbn'] = edition['isbn']
        ekey = ol.new(e, 'Add lending edition from Smashwords')
        print ekey, e['ocaid'], e['title']
        done.append(e['ocaid'])

print done

########NEW FILE########
__FILENAME__ = test_olcompress
"""OL uses a custom compression in storing objects in memcache.

This programs tests the effectiveness of that compression aginst regular compression.
"""
import urllib
import zlib
from openlibrary.utils import olcompress
import random
import simplejson

def wget(url):
    return urllib.urlopen(url).read()

def do_compress(text):
    c1 = zlib.compress(text)

    compress = olcompress.OLCompressor().compress
    c2 = compress(text)

    return len(text), len(c1), len(c2)
    
def test_url(label, url):
    name = url.split("/")[-1]
    text = wget(url)
    x0, x1, x2 = do_compress(text)
    improvement = (x1-x2)/float(x1) * 100
    cols = label, name, x0, x1, x2, improvement
    print "\t".join(str(c) for c in cols)
    return x1, x2
    
def test_random_pattern(label, pattern, max, count):
    return [
        test_url(label, pattern % random.randint(1, max))
        for i in range(count)
    ]
    
def test_recent_lists():
    def get_list_urls():
        json = wget('http://openlibrary.org/recentchanges/lists.json?limit=10')
        d = simplejson.loads(json)

        for change in d:
            key = change.get("data", {}).get("list", {}).get("key")
            if key:
                yield "http://openlibrary.org/" + key + ".json"
    
    return [test_url("list", url) for url in get_list_urls()]

def main():
    d1 = test_random_pattern("book", "http://openlibrary.org/books/OL%dM.json", 2000000, 10)
    d2 = test_random_pattern("work", "http://openlibrary.org/works/OL%dW.json", 1000000, 10)
    d3 = test_random_pattern("author", "http://openlibrary.org/authors/OL%dA.json", 600000, 10)
    d4 = test_recent_lists()
        
    d = d1+d2+d3+d4
    x1 = sum(x[0] for x in d)
    x2 = sum(x[1] for x in d)
    
    improvement = (x1-x2)/float(x1) * 100
    
    print
    print "Overall improvement", improvement
    
    
if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = _init_path
../../_init_path.py
########NEW FILE########
__FILENAME__ = account_v2_migration
"""
Script to migrate accounts to new format.

Usage:
    $ python account_v2_migration.py infobase.yml
"""
from __future__ import with_statement
import _init_path

from infogami.infobase import server
from infogami.infobase._dbstore.store import Store

import sys
import yaml
import web

def migrate_account_table(db):
    def get_status(row):
        if row.get("verified"):
            return "active"
        else:
            return "pending"
            
    with db.transaction():
        store = Store(db)
    
        db.query("DECLARE longquery NO SCROLL CURSOR FOR SELECT thing.key, thing.created, account.* FROM thing, account WHERE thing.id=account.thing_id")
        while True:
            rows = db.query("FETCH FORWARD 100 FROM longquery").list()
            if not rows:
                break
                
            docs = []
            for row in rows:
                # Handle bad rows in the thing table.
                if not row.key.startswith("/people/"):
                    continue
                    
                username = row.key.split("/")[-1]
                doc = {
                    "_key": "account/" + username,
                    "type": "account",
            
                    "status": get_status(row),
                    "created_on": row.created.isoformat(),
            
                    "username": username,
                    "lusername": username.lower(),
            
                    "email": row.email,
                    "enc_password": row.password,
                    "bot": row.get("bot", False),
                }
                email_doc = {
                    "_key": "account-email/" + row.email,
                    "type": "account-email",
                    "username": username
                }
                docs.append(doc)
                docs.append(email_doc)
            store.put_many(docs)
        
def main(configfile):
    server.load_config(configfile)
    db = web.database(**web.config.db_parameters)

    with db.transaction():
        migrate_account_table(db)

if __name__ == '__main__':
    main(sys.argv[1])
########NEW FILE########
__FILENAME__ = load_loan_stats
"""Script to add loan info to couchdb admin database from infobase logs.

USAGE: 

Pass all log entries as input to the script.

   $ cat logs/2011/05/*.log | python scripts/2011/05/load_loan_stats.py openlibrary.yml

Or only pass the store entries to run the script run faster.

   $ grep -h '"action": "store.' logs/2011/05/*.log | python scripts/2011/05/load_loan_stats.py openlibrary.yml
"""
import sys
import simplejson
import yaml
import couchdb
import logging
import re

logger = logging.getLogger('loans')

def get_couchdb_url(configfile):
    d = yaml.safe_load(open(configfile).read())
    return d['admin']['counts_db']
    
def read_log():
    for line in sys.stdin:
        line = line.strip()
        if line:
            try:
                yield simplejson.loads(line)
            except ValueError:
                logger.error("failed to parse log entry: %s", line, exc_info=True)

re_uuid = re.compile('^[0-9a-f]{32}$')
def is_uuid(s):
    return len(s) == 32 and re_uuid.match(s.lower())

def loan_start(db, row):    
    d = row['data']['data']
    key = "loans/" + row['data']['key']
    
    logger.info("log-start %s %s", key, row['timestamp'])  
    
    if key not in db:
        db[key] = {
            "_id": key,
            "book": d['book'],
            "resource_type": d['resource_type'],
            "t_start": row['timestamp'],
            "status": "active"
        }
    else:
        logger.warn("loan alredy present in the db: %s", key)

def loan_end(db, row):
    key = "loans/" + row['data']['key']

    logger.info("log-end %s %s", key, row['timestamp'])  
    
    loan = db.get(key)
    if loan:
        loan["status"] = "completed"
        loan["t_end"] = row['timestamp']
        
        db[key] = loan
    else:
        logger.warn("loan missing in the db: %s", key)

class CachedDB:
    def __init__(self, db):
        self.db = db
        self.cache = {}

    def commit(self):
        self.db.update(self.cache.values())
        self.cache.clear()
    
    def __contains__(self, key):
        return key in self.cache or key in self.db

    def get(self, key, default=None):
        if key in self.cache:
            return self.cache[key]
        else:
            return self.db.get(key, default)

    def __getitem__(self, key):
        if key in self.cache:
            return self.cache[key]
        else:
            return self.db[key]
    
    def __setitem__(self, key, doc):
        doc['_id'] = key
        self.cache[key] = doc
        if len(self.cache) > 100:
            self.commit()

def main(configfile):
    url = get_couchdb_url(configfile)
    db = couchdb.Database(url)
    db = CachedDB(db)
    
    for row in read_log():
        if row.get("action") == 'store.put' and row['data']['data'].get('type') == '/type/loan':
            loan_start(db, row)
        elif row.get("action") == 'store.delete' and is_uuid(row['data']['key']):
            loan_end(db, row)
    db.commit()
    
if __name__ == '__main__':
    FORMAT = "%(asctime)-15s %(levelname)s %(message)s"
    logging.basicConfig(level=logging.INFO, format=FORMAT)
    
    main(sys.argv[1])

########NEW FILE########
__FILENAME__ = verify-works-database
"""Script to makes sure the works coucudb database is up-to-date.
"""
import web
import sys
import logging
import datetime

from infogami import config
import couchdb

from openlibrary.core.lists import engine, updater

LIMIT = 1000

logger = logging.getLogger("openlibrary.script")

@web.memoize
def get_works_db():
    """Returns couchdb databse for storing works.
    """
    db_url = config.get("lists", {}).get("works_db")
    return couchdb.Database(db_url)
    
@web.memoize
def get_editions_db():
    db_url = config.get("lists", {}).get("editions_db")
    return couchdb.Database(db_url)
    
@web.memoize
def get_seeds_engine():
    """Returns updater.SeedsDB instance."""
    db_url = config.get("lists", {}).get("seeds_db")
    db = couchdb.Database(db_url)
    return updater.SeedsDB(db, web.storage(db=get_works_db()))
    
def query(site, q):
    q["limit"] = LIMIT
    keys = site.things(q)
    
    # qeury for remaning if there are still more keys
    if len(keys) == LIMIT:
        q["offset"] = q.get("offset", 0) + LIMIT
        keys += query(q)
    return keys
    
def get_work(site, key):
    """Returns work dict with editions filled in.
    """
    work = site.get(key).dict()
    edition_keys = query(site, {"type": "/type/edition", "works": key})
    work['editions'] = [doc.dict() for doc in site.get_many(edition_keys)]
    return work
    
def _couch_save(db, doc):
    logger.info("Adding %s to %s db", doc['_id'], db.name)
    
    olddoc = db.get(doc['_id'])
    if olddoc:
        doc['_rev'] = olddoc['_rev']
    db.save(doc)
    
def update_work(work):
    db = get_works_db()
    work['_id'] = work['key']
    _couch_save(db, work)    

def update_editions(work):
    """Adds add editions of this work to editions db.
    """
    seeds = engine.get_seeds(work)
    db = get_editions_db()

    for e in work['editions']:
        e = dict(e, seeds=seeds, _id=e['key'])
        _couch_save(db, e)

def update_seeds(work):
    seeds = engine.get_seeds(work)
    logger.info("updating seeds: %s", seeds)
    get_seeds_engine().update_seeds(seeds)
    
def uniq(values):
    """Returns uniq values while preserving the order.
    """
    uniq_values = []
    visited = set()
    
    for v in values:
        if v not in visited:
            visited.add(v)
            uniq_values.append(v)
            
    return uniq_values
    
def get_work_keys(site, keys):
    """Substitutes edition keys with their work keys.
    """
    work_keys = [k for k in keys if k.startswith("/works/")]
    edition_keys = [k for k in keys if k.startswith("/books/")]
    if edition_keys:
        editions = [doc.dict() for doc in site.get_many(edition_keys)]
        for e in editions:
            for w in e.get("works", []):
                work_keys.append(w['key'])
    
    return uniq(work_keys)
    
def update(site, keys):
    """Updates the entries for gives keys in the list databases.
    
    For a work:
        * the work is updated in the works_db
        * all its editions are updated in editions_db
        * all the seeds are updated in seeds_db
    
    Updating an edition is same as updating its work.
    """
    logger.info("BEGIN update")
    keys = get_work_keys(site, keys)
    
    for key in keys:
        work = get_work(site, key)
        update_work(work)
        update_editions(work)
        update_seeds(work)
    logger.info("END update")
    
def verify_doc(doc):
    if doc['type']['key'] == '/type/work':
        db = get_works_db()
    elif doc['type']['key'] == '/type/edition':
        db = get_editions_db()
    else:
        return True
        
    rev = doc['revision']
    doc2 = db.get(doc['key'])
    rev2 = doc2 and doc2['revision'] or 0
    
    if rev2 < rev:
        logger.error("db has stale entry for %s (%s < %s)", doc['key'], rev2, rev)
        return False
    
    return True

def verify(site, keys):
    """Verifies the freshness of the docs in the couchdb and returns the list of stale keys.
    """
    docs = web.ctx.site.get_many(keys)
    
    stale = []
    for doc in docs:
        if not verify_doc(doc):
            stale.append(doc['key'])
            
    return stale
        
def verify_day(site, day, do_update=False):
    logger.info("BEGIN verify_day %s", day)
    changes = get_modifications(site, day)
    changes = (k for k in changes if k.startswith("/works/") or k.startswith("/books/"))
    
    stale_count = 0
    total_count = 0
    
    for keys in web.group(changes, 1000):
        stale = verify(site, keys)
        if do_update and stale:
            logger.info("updating %s stale docs", len(stale))
            update(site, stale)
        else:
            logger.info("found %s/%d stale docs", len(stale), len(keys))
            
        stale_count += len(stale)
        total_count += len(keys)
        
    logger.info("END verify_day. found %d/%d stale docs", stale_count, total_count)
        
def get_modifications(site, day):
    """Returns an iterator over all the modified docs in a given day (as string).
    """
    limit = 100
    
    n = limit
    offset = 0
    
    yyyy, mm, dd = map(int, day.split("-"))
    
    begin_date = datetime.date(yyyy, mm, dd)
    end_date = begin_date + datetime.timedelta(days=1)
    
    while n == limit:
        changes = site.recentchanges({"begin_date": begin_date.isoformat(), "end_date": end_date.isoformat(), "limit": limit, "offset": offset})
        changes = [c.dict() for c in changes]
        n = len(changes)
        offset += n
        for c in changes:
            for x in c['changes']:
                yield x['key']
                
        break
                
def main():
    #update(web.ctx.site, sys.argv[1:])
    verify_day(web.ctx.site, sys.argv[1], "--update" in sys.argv)

if __name__ == '__main__':
    web.config.debug = True
    main()
########NEW FILE########
__FILENAME__ = expire_accounts
import datetime

import web

def delete_old_links():
    for doc in web.ctx.site.store.values(type="account-link"):
        expiry_date = datetime.datetime.strptime(doc['expires_on'], "%Y-%m-%dT%H:%M:%S.%f")
        now = datetime.datetime.utcnow()
        key = doc['_key']
        if expiry_date > now:
            print "Deleting link %s"%(key)
            del web.ctx.site.store[key]
        else:
            print "Retaining link %s"%(key)

def main():
    delete_old_links()
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())

        

########NEW FILE########
__FILENAME__ = inspect-memcache
#! /usr/bin/env python
"""Script to inspect memcache.

Usage:
    python scripts/2011/08/inspect-memcache.py localhost:7060
"""

import sys
import memcache

mc = memcache.Client([sys.argv[1]])

slabs = mc.get_stats("slabs")[0][1]
keys = sorted([key.split(":")[0] for key in slabs if "chunk_size" in key], key=lambda k: int(k))
for k in keys:
    print k
    print "  chunk_size", slabs[k + ":chunk_size"]
    print "  total_chunks", slabs[k + ":total_chunks"]
    dump = mc.get_stats("cachedump %s 5" % k)[0][1]
    for dk, dv in dump.items():
        print "  " + dk, dv

########NEW FILE########
__FILENAME__ = memcache-size
#! /bin/bash
"""Script to findout the amount of memory required to cache all OL data.
"""

import random
import web
import simplejson
from openlibrary.utils import olcompress

works = 15000000
editions = 25000000
authors = 6000000

compressor = olcompress.OLCompressor()

N = 100

def clen(d):
    """Computes the compressed length of given data."""
    return len(compressor.compress(simplejson.dumps(d)))

def get_sizes(label, pattern, max, count):
    # there might not be docs for some numbers. 
    # Considering double keys than required and skipping the None
    keys = [pattern % random.randint(0, max) for i in range(2*count)]
    docs = [doc for doc in web.ctx.site.get_many(keys) if doc][:count]

    overheads = 48 + 20 # item size and key size

    doc_size = overheads + sum(clen(doc.dict()) for doc in docs) / len(docs)
    data_size = overheads + sum(clen(doc._get_d()) for doc in docs) / len(docs)
    M = 1000000

    total_doc_size = doc_size*max/M
    total_data_size = data_size*max/M
    total_size = total_doc_size + total_data_size
    sizes = [doc_size, data_size, total_doc_size, total_data_size, total_size]
    print "\t".join(map(str, [label] + sizes))

def main():
    get_sizes("works", "/works/OL%dW", works, N)
    get_sizes("books", "/books/OL%dM", editions, N)
    get_sizes("authors", "/authors/OL%dA", authors, N)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = oldoc
"""Script to print documentation about all public functions available to templates.

USAGE:
    Get all docs:
        $ ./scripts/openlibary conf/openlibrary.yml runscript scripts/2011/08/oldoc.py
    
    Get docs for one single function:
        $ ./scripts/openlibary conf/openlibrary.yml runscript scripts/2011/08/oldoc.py datestr

"""
import web
import pydoc
import sys
import re

functions = web.template.Template.globals

args = sys.argv[1:]

if args:
    functions = dict((name, value) for (name, value) in functions.items() if name in args)

for name, f in sorted(functions.items()):
    if hasattr(f, "__call__"):
        try:
            doc = pydoc.text.document(f, name)
            # Remove special chars to print text as bold on the terminal
            print re.sub("\x08.", "", doc)
        except Exception:
            # ignore errors
            pass

########NEW FILE########
__FILENAME__ = generate_deworks
"""Script to generate denormalizied works dump.

USAGE:

    python scripts/2011/09/generate_deworks.py ol_dump_latest.txt.gz | gzip -c > ol_dump_deworks.txt.gz


Output Format:

Each row contains a JSON document with the following fields:

* work - The work documents
* editions - List of editions that belong to this work
* authors - All the authors of this work
* ia - IA metadata for all the ia items referenced in the editions as a list
* duplicates - dictionary of duplicates (key -> it's duplicates) of work and edition docs mentioned above

CHANGELOG:

2013-04-26: Changed the format of denormalized documents and included duplicates/redirects.
2012-10-31: include IA metadata in the response
2012-03-15: print the output to stdout so that it can be piped though gzip
2011-10-04: first version

"""
import time
import sys
import gzip
import simplejson
import itertools
import os
import logging
#import bsddb
import zlib
import subprocess
import web
from collections import defaultdict
import re
import shutil

from openlibrary.data import mapreduce
from openlibrary.utils import compress


try:
    import bsddb
except ImportError:
    #import bsddb3 as bsddb
    pass

logger = logging.getLogger(None)

class DenormalizeWorksTask(mapreduce.Task):
    """Map reduce task to generate denormalized works dump from OL dump.
    """
    def __init__(self, ia_metadata):
        mapreduce.Task.__init__(self)
        self.ia = ia_metadata
        self.authors = AuthorsDict()
        self.duplicates = defaultdict(list)

    def get_ia_metadata(self, identifier):
        row = self.ia.get(identifier)
        if row:
            boxid, collection_str = row.split("\t")
            collections = collection_str.split(";")
            return {"boxid": [boxid], "collection": collections}
        else:
            # the requested identifier is not found.
            # returning a fake metadata
            return {"collection":[]}

    def close(self):
        """Removes all the temp files created for running map-reduce.
        """
        shutil.rmtree(self.tmpdir, ignore_errors=True)

    def map(self, key, value):
        """Takes key and json as inputs and emits (work-key, work-json) or (work-key, edition-json)
        for work and edition records respectively.
        """
        doc = simplejson.loads(value)
        type_key = doc['type']['key']
        
        if type_key == '/type/edition':
            if doc.get('works'):
                yield doc['works'][0]['key'], value
            else:
                yield doc['key'], value
        elif type_key == '/type/work':
            yield doc['key'], value
        elif type_key == '/type/author':
            # Store all authors for later use
            self.authors[key] = doc
        elif type_key == '/type/redirect':
            self.duplicates[doc.get("location")].append(key)

    def process_edition(self, e):
        if "ocaid" in e:
            ia = self.get_ia_metadata(e["ocaid"])
            if ia is not None:
                e['_ia_meta'] = ia

    def reduce(self, key, values):
        docs = {}
        for json in values:
            doc = simplejson.loads(json)
            self.process_edition(doc)
            docs[doc['key']] = doc
            
        if key.startswith("/works/"):
            work = docs.pop(key, {})
        else:
            # work-less edition
            work = {}

        doc = {}
        doc['work'] = work
        doc['editions'] = docs.values()
        doc['authors'] = [self.get_author_data(a['author']) for a in work.get('authors', []) if 'author' in a]

        ia_ids = [e['ocaid'] for e in doc['editions'] if e.get('ocaid') and e['ocaid'] in self.ia]
        doc['ia'] = [self.get_ia_metadata(ia_id) for ia_id in ia_ids]

        keys = set()
        keys.add(work.get("key"))
        keys.update(e.get("key") for e in doc['editions'])
        if None in keys:
            keys.remove(None)

        duplicates = [(k, self.get_duplicates(k)) for k in keys]
        duplicates = [(k, dups) for k, dups in duplicates if dups]
        doc['duplicates'] = dict(duplicates)
        return key, simplejson.dumps(doc)

    def get_duplicates(self, key, result=None, level=0):
        if result is None:
            result = set()

        # Avoid infinite recursion in case of mutual redirects
        if level >= 10:
            return result

        for k in self.duplicates[key]:
            result.add(k) # add the duplicate and all it's duplicates
            self.get_duplicates(k, result, level+1)

        return result

    def get_author_data(self, author):
        def get(key):
            return self.authors.get(key, {"key": key})

        if 'key' in author:
            key = author['key']
            return get(key)
        elif 'author' in author and 'key' in author['author']:
            key = author['author']['key']
            return {"author": get(key)}
        else:
            return author

class AuthorsDict:
    """Dictionary for storing author records in memory very efficiently.
    """
    re_author = re.compile("/authors/OL(\d+)A")
    chunk_size = 100

    def __init__(self):
        self.d = defaultdict(lambda: [None] * self.chunk_size)
        self.z = compress.Compressor('{"name": "", "personal_name": ""}')

    def __len__(self):
        return len(d)

    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError:
            return default

    def __getitem__(self, key):
        try:
            m = self.re_author.match(key)
        except TypeError:
            print repr(key)
            raise
        if m:
            index = int(m.group(1))
            index0 = index / self.chunk_size
            index1 = index % self.chunk_size
            try:
                value = self.d[index0][index1]
            except (KeyError, IndexError):
                raise KeyError(key)
            if value is None:
                raise KeyError(key)
        else:
            value = self.d[key]
        doc = self.decode(value)
        doc['key'] = key
        return doc

    def __setitem__(self, key, value):
        m = self.re_author.match(key)
        if m:
            index = int(m.group(1))
            index0 = index / self.chunk_size
            index1 = index % self.chunk_size
            self.d[index0][index1] = self.encode(value)
        else:
            self.d[key] = self.encode(value)

    def encode(self, doc):
        for k in ['type', 'key', 'latest_revision', 'revision', 'last_modified', 'created']:
            doc.pop(k, None)
        return self.z.compress(simplejson.dumps(doc))

    def decode(self, text):
        doc = simplejson.loads(self.z.decompress(text))
        doc['type'] = {'key': '/type/author'}
        return doc
        
def xopen(filename, mode='r'):
    if filename.endswith(".gz"):
        return gzip.open(filename, mode)
    elif filename == "-":
        if mode == "r":
            return sys.stdin
        else:
            return sys.stdout
    else:
        return open(filename, mode)

def read_ia_metadata(filename):
    logger.info("BEGIN reading " + filename)
    t0 = time.time()
    N = 500000
    d = {}
    for i, line in enumerate(xopen(filename)):
        if i % N == 0:
            logger.info("reading line %d" % i)
        id, rest = line.strip("\n").split("\t", 1)
        d[id] = rest
    logger.info("END reading " + filename)
    return d
        
def read_dump(filename):
    t0 = time.time()
    N = 50000
    for i, line in enumerate(xopen(filename)):
        if i % N == 0:
            t1 = time.time()
            dt = t1-t0
            rate = dt and (N/dt)
            t0 = t1
            logger.info("reading %d (%d docs/sec)", i, rate)
        _type, key, _revision, _last_modified, jsondata = line.strip().split("\t")
        yield key, jsondata
    
def mkdir_p(path):
    if not os.path.exists(path):
        os.makedirs(path)
        
def main(dumpfile, ia_dumpfile):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")

    ia_metadata = read_ia_metadata(ia_dumpfile)
    records = read_dump(dumpfile)
    task = DenormalizeWorksTask(ia_metadata)

    for key, json in task.process(records):
        print key + "\t" + json

    task.close()
    
def make_author_db(author_dump_file):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")

    os.system("rm -rf authors.sqlite")
    db = web.database(dbn="sqlite", db="authors.sqlite")
    db.printing = False
    db.query("CREATE TABLE docs (key text, value blog)")
    db.query("PRAGMA cachesize=%d" % (1024*1024*100))
    db.query("PRAGMA synchronous=OFF")
    db.query("PRAGMA journal_mode=OFF")

    for chunk in web.group(read_dump(author_dump_file), 1000):
        t = db.transaction()
        for key, value in chunk:
            db.insert("docs", key=key, value=value)
        t.commit()

    logger.log("BEGIN create index")
    db.query("CREATE UNIQUE INDEX key_index ON docs(key)")
    logger.log("END create index")
    
def make_ia_db(editions_dump_file):
    #logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")
    #global logger
    #logger = logging.getLogger("openlibrary")
    logger.info("BEGIN make_ia_db %s", editions_dump_file)
    
    #db = bsddb.hashopen("ia.db", cachesize=1024*1024*500)
    
    #from openlibrary.core import ia
    
    f = open("ocaids.txt", "w")

    for key, json in read_dump(editions_dump_file):
        if "ocaid" in json:
            doc = simplejson.loads(json)
            ocaid = doc.get('ocaid')
            print >> f, ocaid
            """
            if ocaid:
                metaxml = ia.get_meta_xml(ocaid)
                db[ocaid] = simplejson.dumps(metaxml)
            """
    f.close()
    #db.close()

def test_AuthorsDict():
    d = AuthorsDict()
    
    docs = [
        {"key": "/authors/OL1A", "type": {"key": "/type/author"}, "name": "foo"},
        {"key": "/authors/OL2A", "type": {"key": "/type/author"}, "name": "bar"},
        {"key": "bad-key", "name": "bad doc", "type": {"key": "/type/author"}},
    ]

    for doc in docs:
        d[doc['key']] = dict(doc)

    for doc in docs:
        assert d[doc['key']] == doc
    
if __name__ == '__main__':
    # the --authordb and --iadb options are not in use now.
    if "--authordb" in sys.argv:
        sys.argv.remove("--authordb")
        make_author_db(sys.argv[1])
    elif "--iadb" in sys.argv:
        sys.argv.remove("--iadb")
        make_ia_db(sys.argv[1])
    else:
        main(sys.argv[1], sys.argv[2])

########NEW FILE########
__FILENAME__ = match-smithsonian-artists
"""Script to match Smithsonian artist names with OL.
"""

import web
import csv
import urllib
import simplejson
import logging
import sys

logger = logging.getLogger()

base_url = "http://openlibrary.org/search/authors.json"
base_url = "http://anand.openlibrary.org/search/authors.json"

def query_authors(lastname, firstname, birth_date):
    """Query using the experimental author search API.
    """
    if not birth_date:
        logger.info("%s, no matches found.", (lastname, firstname, birth_date))
        return
    url = base_url + "?" + urllib.urlencode({"q": "%s %s birth_date:%s" % (lastname, firstname, birth_date)}) 
    json = urllib.urlopen(url).read()
    data = simplejson.loads(json)
    n = data['numFound']
    if n == 1:
        logger.info("queried for %s. found exact match", (lastname, firstname, birth_date))
        return '/authors/' + data['docs'][0]['key']
    elif n > 1:
        logger.info("queried for %s, found %s duplicates. %s", (lastname, firstname, birth_date), n, url.replace(".json", ""))
    else:
        logger.info("queried for %s, no matches found.", (lastname, firstname, birth_date))
    
def read(filename):
    rows = csv.reader(open(filename))
    cols = [c.strip() for c in rows.next()]
    logging.info("reading cols %s", cols)

    for row in rows:
        row = [val.strip() for val in row]
        yield web.storage(zip(cols, row))

def match(record):
    key = query_authors(record['aaa_last'], record['aaa_first'], record['aaa_date_born'])
    if key:
        record.openlibrary = "http://openlibrary.org" + key
    else:
        record.openlibrary = ""

def main(filename):
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")
    records = read(filename)

    w = csv.writer(sys.stdout)
    cols =["aaa_last", "aaa_first", "aaa_date_born", "aaa_date_died", "fullurl", "openlibrary"]
    w.writerow(cols)
    for r in records:
        match(r)
        w.writerow([r[c] for c in cols])

if __name__ == "__main__":
    try:
        main(sys.argv[1])
    except:
        sys.stdout.flush()
        raise

########NEW FILE########
__FILENAME__ = detect-bad-author-merge
"""Script to detect bad author merges from infobase write logs.

It is noticed that some author merges reverting some work pages to an older
revision when updating the author info.

https://github.com/internetarchive/openlibrary/issues/89

It can be identified by looking at the revision in `changes` of the changeset
and the input query given. The difference between the revisions should be
exactly 1 if everything is alright. It is a bad-merge if the difference is
more than 1.
"""
import sys
import json

def read():
    for line in sys.stdin:
        try:
            yield json.loads(line.strip())
        except ValueError:
            # bad JSON
            pass

def read_author_merges():
    return (row for row in read() 
                if row.get('action') == 'save_many' 
                and row['data']['changeset']['kind'] == 'merge-authors')

def is_bad_merge(row):
    """A merge is bad if the difference between revision of any modified page
    in the input and after merge is more than 1.
    
    Returns the list of keys of effected docs. It will be empty list if the merge is alright.
    """
    # revisions from the input query
    revs = dict((doc['key'], doc.get('revision')) for doc in row['data']['query'])
    # (key, revision) tuples from changeset changes
    changes = [(c['key'], c['revision']) for c in row['data']['changeset']['changes']]    
    return [key for key, rev in changes if revs[key] is not None and rev - revs[key] > 1]

def main():
    for row in read_author_merges():
        keys = is_bad_merge(row)
        if keys:
            print row['timestamp'], row['data']['changeset']['id'], " ".join(keys)

if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = undo-author-merge
"""Script to undo a bad author-merge.

http://openlibrary.org/recentchanges/2011/05/31/merge-authors/43575781

Usage:
    ./scripts/openlibrary-server openlibrary.yml runscript undo-author-merge.py
"""
import web
import json
import urllib2
import shelve
from infogami.infobase.client import ClientException

change_id = "43575781"


cache = shelve.open("cache.shelve")

def get_doc(key, revision):
    if revision == 0:
        return {
            "key": key,
            "type": {"key": "/type/delete"}
        }
    else:
        kv = "%s@%s" % (key, revision)
        
        if kv not in cache:
            cache[kv] = web.ctx.site.get(key, revision).dict()
        else:
            print kv, "found in cache"
        return cache[kv]
        
def get_work_authors(work):
    authors = work.get('authors') or []
    if authors:
        return [a['author']['key'] for a in authors if 'author' in a]
    else:
        return []
        
def get_work(edition):
    works = edition.get('works')
    return works and works[0]['key'] or None

def main():
    changeset = web.ctx.site.get_change(change_id).dict()
    
    keys = [c['key'] for c in changeset['changes']]
    revs = dict((c['key'], c['revision']) for c in changeset['changes'])
    old_docs = dict((c['key'], get_doc(c['key'], c['revision']-1)) for c in changeset['changes'])
    latest_docs = dict((doc['key'], doc) for doc in web.ctx.site.get_many(keys, raw=True))
    
    undo_docs = {}
    
    def process_keys(keys):
        for key in keys:
            old_doc = old_docs[key]
            new_doc = latest_docs[key]
        
            if new_doc['type']['key'] != old_doc['type']['key']:
                print key, "TYPE CHANGE", old_doc['type']['key'], new_doc['type']['key']
                undo_docs[key] = old_doc
            elif new_doc['type']['key'] == '/type/work' and new_doc.get('authors') != old_doc.get('authors'):
                print key, 'AUTHOR CHANGE', get_work_authors(old_doc), get_work_authors(new_doc)
                doc = dict(new_doc, authors=old_doc.get('authors') or [])
                undo_docs[key] = doc
            elif old_doc['type']['key'] == '/type/edition' and new_doc.get('works') != old_doc.get('works'):
                print key, 'WORK CHANGE', get_work(old_doc), get_work(new_doc)
                doc = dict(new_doc, works=old_doc.get('works') or [])
            
                if doc.get('works'):
                    wkey = doc['works'][0]['key']
                    work = old_docs.get(wkey)
                    if work is not None:
                        doc['authors'] = [{'key': key} for key in get_work_authors(work)]
                        undo_docs[key] = doc
                    else:
                        print key, "IGNORING, WORK NOTFOUND", wkey
                                        
    # process authors, works and books in order
    # XXX: Running all of them together is failing. 
    # Ran the script 3 times enabling just one of the following 3 lines each time.
    process_keys(k for k in keys if k.startswith('/authors/'))
    #process_keys(k for k in keys if k.startswith('/works/'))
    #process_keys(k for k in keys if k.startswith('/books/'))
    
    web.ctx.site.login("AnandBot", "**-change-this-before-running-the-script-**")

    data = {
        "parent_changeset": change_id
    }
    print "saving..."
    web.ctx.ip = '127.0.0.1'
    try:
        web.ctx.site.save_many(undo_docs.values(), action="undo", data=data, comment='Undo merge of "Miguel de Unamuno" and "Miguel de Cervantes Saavedra"') 
    except ClientException, e:
        print 'ERROR', e.json
    
if __name__ == '__main__':
    try:
        main()
    finally:
        cache.close()
########NEW FILE########
__FILENAME__ = delete-cover
#! /usr/bin/env python
"""Script to delete cover permanantly from coverstore.

Used when some assert copyright on an images and demands to remove it from our collection.

USAGE:
    
    python scripts/2012/02/delete-cover.py 5699220

After deleting the item must be uploaded back to archive.org cluster. This can be done using:

    /olsystem/bin/uploaditem.py --u

    
"""
import os
import sys

def download(itemid, filename):
    os.system("mkdir -p " + itemid)
    os.system("wget -nv http://www.archive.org/download/%(itemid)s/%(filename)s -O %(itemid)s/%(filename)s" % locals())

def delete_cover(itemid, zipfile, filename):
    download(itemid, zipfile)
    os.system("7z d %(filename)s %(itemid)s/%(zipfile)s" % locals())

def main():
    coverid = int(sys.argv[1])
    itemid = "olcovers%d" % (coverid/10000) 

    download(itemid, itemid + "_meta.xml")

    for suffix in ["-S", "-M", "-L", ""]:
        zipfile = itemid + suffix + ".zip"
        imgfile = str(coverid) + suffix + ".jpg"
        delete_cover(itemid, zipfile, imgfile)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = dump-ia-items
"""Script to dump TSV of identifier,boxid,collection columns for each
archive.org text item.

All dark items are excluded.

USAGE:

    python dump-ia-items.py --host dbserver --user archive --password secret --database archive
"""

import web
import optparse

def make_parser():
    p = optparse.OptionParser()
    p.add_option("-H", "--host", default="localhost", help="database host")
    p.add_option("-u", "--user", default="archive", help="database user")
    p.add_option("-p", "--password", default="", help="database password")
    p.add_option("-d", "--database", default="", help="database name (mandatory)")
    return p    

def tabjoin(*args):
    return "\t".join((a or "").encode("utf-8") for a in args)

def dump_metadata(db):
    limit = 100000
    offset = 0

    while True:
        rows = db.select("metadata", 
                what="identifier, boxid, collection, curatestate", 
                where='mediatype="texts"', 
                limit=limit, 
                offset=offset)
        if not rows:
            break
        offset += len(rows)
        for row in rows:
            # exclude dark items
            if row.curatestate == "dark":
                continue
            print tabjoin(row.identifier, row.boxid, row.collection)

def main():
    p = make_parser()
    options, args = p.parse_args()

    if not options.database:
        p.error("Please specify a database. Try -h for help.")

    kw = {
        "dbn": "mysql",
        "host": options.host,
        "db": options.database,
        "user": options.user,
        "pw": options.password
    }

    db = web.database(**kw)
    dump_metadata(db)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = find-indexed-works
"""Script find works that are not indexed in solr.

USAGE: zcat ol_dump_works_latest.txt.gz | cut -f2 | python scripts/2013/find-indexed-works.py http://solr-node:8983/solr/works
"""

import sys
import urllib
import json
import time
import web

solr_base_url = sys.argv[1]

def jsonget(url, data=None):
	print >> sys.stderr, time.asctime(), "jsonget", url[:120], data[:50] if data else ""
	return json.load(urllib.urlopen(url, data))

def is_indexed_in_solr(key):
	url = solr_base_url + "/select?wt=json&rows=0&q=key:" + key
	d = jsonget(url)
	return d['response']['numFound'] > 0

def find_not_indexed(keys, chunk_size=1000):
	for chunk in web.group(keys, chunk_size):
		chunk = list(chunk)
		q=" OR ".join("key:" + k for k in chunk)
		params = urllib.urlencode({"q": q, "rows": chunk_size, "wt": "json", "fl": "key"})
		url = solr_base_url + "/select"
		d = jsonget(url, params)
		found = set(doc['key'] for doc in d['response']['docs'])
		for k in chunk:
			if k not in found:
				yield k

def main():
	keys = (line.strip().split("/")[-1] for line in sys.stdin)
	for k in find_not_indexed(keys):
		print k

if __name__ == '__main__':
	main()
########NEW FILE########
__FILENAME__ = commit_inside
#!/usr/bin/python

import httplib

index = 'inside'
solr_host = 'ol-search-inside:8983'

h1 = httplib.HTTPConnection(solr_host)
h1.connect()
url = 'http://%s/solr/%s/update' % (solr_host, index)
h1.request('POST', url, '<commit />', { 'Content-type': 'text/xml;charset=utf-8'})
response = h1.getresponse()
response_body = response.read()
if response.reason != 'OK':
    print response.reason
    print response_body
assert response.reason == 'OK'
h1.close()

########NEW FILE########
__FILENAME__ = copydocs
#! /usr/bin/env python
"""Script to copy docs from one OL instance to another. 
Typically used to copy templates, macros, css and js from 
openlibrary.org to dev instance.

USAGE: 
    ./scripts/copydocs.py --src http://openlibrary.org --dest http://0.0.0.0:8080 /templates/*

This script can also be used to copy books and authors from OL to dev instance. 

    ./scripts/copydocs.py /authors/OL113592A 
    ./scripts/copydocs.py /works/OL1098727W
"""

import _init_path
import sys
import os
import simplejson
import web

from openlibrary.api import OpenLibrary, marshal, unmarshal
from optparse import OptionParser

__version__ = "0.2"

def find(server, prefix):
    q = {'key~': prefix, 'limit': 1000}
    
    # until all properties and backreferences are deleted on production server
    if prefix == '/type':
        q['type'] = '/type/type'
    
    return [unicode(x) for x in server.query(q)]
    
def expand(server, keys):
    if isinstance(server, Disk):
        for k in keys:
            yield k
    else:
        for key in keys:
            if key.endswith('*'):
                for k in find(server, key):
                    yield k
            else:
                yield key


desc = """Script to copy docs from one OL instance to another.
Typically used to copy templates, macros, css and js from
openlibrary.org to dev instance. paths can end with wildcards.
"""
def parse_args():
    parser = OptionParser("usage: %s [options] path1 path2" % sys.argv[0], description=desc, version=__version__)
    parser.add_option("-c", "--comment", dest="comment", default="", help="comment")
    parser.add_option("--src", dest="src", metavar="SOURCE_URL", default="http://openlibrary.org/", help="URL of the source server (default: %default)")
    parser.add_option("--dest", dest="dest", metavar="DEST_URL", default="http://0.0.0.0:8080/", help="URL of the destination server (default: %default)")
    parser.add_option("-r", "--recursive", dest="recursive", action='store_true', default=False, help="Recursively fetch all the referred docs.")
    parser.add_option("-l", "--list", dest="lists", action="append", default=[], help="copy docs from a list.")
    return parser.parse_args()

class Disk:
    def __init__(self, root):
        self.root = root

    def get_many(self, keys):
        def f(k):
            return {
                "key": k, 
                "type": {"key": "/type/template"}, 
                "body": {
                    "type": "/type/text", 
                    "value": open(self.root + k.replace(".tmpl", ".html")).read()
                }
            }
        return dict((k, f(k)) for k in keys)

    def save_many(self, docs, comment=None):
        def write(path, text):
            dir = os.path.dirname(path)
            if not os.path.exists(dir):
                os.makedirs(dir)

            if isinstance(text, dict):
                text = text['value']

            try:
                print "writing", path
                f = open(path, "w")
                f.write(text)
                f.close()
            except IOError:
                print "failed", path

        for doc in marshal(docs):
            path = os.path.join(self.root, doc['key'][1:])
            if doc['type']['key'] == '/type/template':
                path = path.replace(".tmpl", ".html")
                write(path, doc['body'])
            elif doc['type']['key'] == '/type/macro':
                path = path + ".html"
                write(path, doc['macro'])
                
def read_lines(filename):
    try:
        return [line.strip() for line in open(filename)]
    except IOError:
        return []
        
def get_references(doc, result=None):
    if result is None:
        result = []

    if isinstance(doc, list):
        for v in doc:
            get_references(v, result)
    elif isinstance(doc, dict):
        if 'key' in doc and len(doc) == 1:
            result.append(doc['key'])

        for k, v in doc.items():
            get_references(v, result)
    return result
    
def copy(src, dest, keys, comment, recursive=False, saved=None, cache=None):
    if saved is None:
        saved = set()
    if cache is None:
        cache = {}
        
    def get_many(keys):
        docs = marshal(src.get_many(keys).values())
        # work records may contain excepts, which reference the author of the excerpt. 
        # Deleting them to prevent loading the users.
        # Deleting the covers and photos also because they don't show up in the dev instance.
        for doc in docs:
            doc.pop('excerpts', None)
            #doc.pop('covers', None)
            #doc.pop('photos', None)

            # Authors are now with works. We don't need authors at editions.
            if doc['type']['key'] == '/type/edition':
                doc.pop('authors', None)

        return docs
        
    def fetch(keys):
        docs = []
        
        for k in keys:
            if k in cache:
                docs.append(cache[k])
                
        keys = [k for k in keys if k not in cache]
        if keys:
            print "fetching", keys
            docs2 = get_many(keys)
            cache.update((doc['key'], doc) for doc in docs2)
            docs.extend(docs2)
        return docs
        
    keys = [k for k in keys if k not in saved]
    docs = fetch(keys)
    
    if recursive:
        refs = get_references(docs)
        refs = [r for r in list(set(refs)) if not r.startswith("/type/") and not r.startswith("/languages/")]
        if refs:
            print "found references", refs
            copy(src, dest, refs, comment, recursive=True, saved=saved, cache=cache)
    
    docs = [doc for doc in docs if doc['key'] not in saved]
    
    keys = [doc['key'] for doc in docs]
    print "saving", keys
    print dest.save_many(docs, comment=comment)
    saved.update(keys)
    
def copy_list(src, dest, list_key, comment):
    keys = set()
    
    def jsonget(url):
        url = url.encode("utf-8")
        text = src._request(url).read()
        return simplejson.loads(text)
    
    def get(key):
        print "get", key
        return marshal(src.get(list_key))
        
    def query(**q):
        print "query", q
        return [x['key'] for x in marshal(src.query(q))]
        
    def get_list_seeds(list_key):
        d = jsonget(list_key + "/seeds.json")
        return d['entries'] #[x['url'] for x in d['entries']]
        
    def add_seed(seed):
        if seed['type'] == 'edition':
            keys.add(seed['url'])
        elif seed['type'] == 'work':
            keys.add(seed['url'])
        elif seed['type'] == 'subject':
            doc = jsonget(seed['url'] + "/works.json")
            keys.update(w['key'] for w in doc['works'])
        
    seeds = get_list_seeds(list_key)
    for seed in seeds:
        add_seed(seed)
        
    edition_keys = set(k for k in keys if k.startswith("/books/"))
    work_keys = set(k for k in keys if k.startswith("/works/"))
    
    for w in work_keys:
        edition_keys.update(query(type='/type/edition', works=w, limit=500))

    keys = list(edition_keys) + list(work_keys)
    copy(src, dest, keys, comment=comment, recursive=True)
            
def main():
    options, args = parse_args()

    if options.src.startswith("http://"):
        src = OpenLibrary(options.src)
    else:
        src = Disk(options.src)

    if options.dest.startswith("http://"):
        dest = OpenLibrary(options.dest)
        section = "[%s]" % web.lstrips(options.dest, "http://").strip("/")
        if section in read_lines(os.path.expanduser("~/.olrc")):
            dest.autologin()
        else:
            dest.login("admin", "admin123")
    else:
        dest = Disk(options.dest)
        
    for list_key in options.lists:
        copy_list(src, dest, list_key, comment=options.comment)

    keys = args
    keys = list(expand(src, keys))
    
    copy(src, dest, keys, comment=options.comment, recursive=options.recursive)
    
if __name__ == '__main__':
    main()


########NEW FILE########
__FILENAME__ = fake_loan_server
#! /usr/bin/env python
"""
Fake loan status server to make dev instance work with borrowing books.
"""
import web

urls = (
    "/is_loaned_out/(.*)", "is_loaned_out"
)
app = web.application(urls, globals())

class is_loaned_out:
    def GET(self, resource_id):
        web.header("Content-type", "application/json")
        return "[]"

if __name__ == "__main__":
    app.run()


########NEW FILE########
__FILENAME__ = fetchmail
#!/usr/bin/env python

from openlibrary.core import fetchmail

if __name__ == "__main__":
    import sys
    sys.exit(fetchmail.main(sys.argv[1:]))

########NEW FILE########
__FILENAME__ = generate-api-docs
import web
import os
import re
import shutil
from collections import defaultdict

template = """\
$def with (mod, submodules)
$ name = mod.split(".")[-1]

$name
$("=" * len(name))

$if submodules:
    Submodules
    ----------

    .. toctree::
       :maxdepth: 1

       $for m in submodules: $m

    Documentation
    -------------
    
    .. automodule:: $mod
$else:
    .. automodule:: $mod
"""

t = web.template.Template(template)

def docpath(path):
    return "docs/api/" + path.replace(".py", ".rst").replace("__init__", "index")
    
def modname(path):
    return path.replace(".py", "").replace("/__init__", "").replace("/", ".")
    
def write(path, text):
    dirname = os.path.dirname(path)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    
    print "writing", path
    f = open(path, "w")
    f.write(text)
    f.close()
    
def find_python_sources(dir):
    ignores = [
        "openlibrary/catalog.*",
        "openlibrary/solr.*",
        "openlibrary/plugins/recaptcha.*",
        "openlibrary/plugins/akismet.*",
        "openlibrary/plugins/bookrev.*",
        "openlibrary/plugins/copyright.*",
        "openlibrary/plugins/heartbeat.*",
        ".*tests",
        "infogami/plugins.*",
        "infogami.utils.markdown",
    ]
    re_ignore = re.compile("|".join(ignores))

    for dirpath, dirnames, filenames in os.walk(dir):
        if re_ignore.match(dirpath):
            print "ignoring", dirpath
            continue
            
        for f in filenames:
            if f.endswith(".py"):
                yield os.path.join(dirpath, f)
    
def generate_docs(dir):
    shutil.rmtree(docpath(dir), ignore_errors=True)
    
    paths = list(find_python_sources(dir))
    
    submodule_dict = defaultdict(list)
    
    for path in paths:
        dir = os.path.dirname(path)
        
        if path.endswith("__init__.py"):
            dir = os.path.dirname(dir)
            
        submodule_dict[dir].append(path)
    
    for path in paths:
        dirname = os.path.dirname(path)
        if path.endswith("__init__.py"):
            submodules = [web.lstrips(docpath(s), docpath(dirname) + "/") for s in submodule_dict[dirname]]
        else:
            submodules = []
        submodules.sort()
        
        mod = modname(path)
        text = str(t(mod, submodules))
        write(docpath(path), text)
        
        # set the modification time same as the source file
        mtime = os.stat(path).st_mtime
        os.utime(docpath(path), (mtime, mtime))
                
def generate_index():
    filenames = sorted(os.listdir("docs/api"))
    
    f = open("docs/api/index.rst", "w")
    
    f.write("API Documentation\n")
    f.write("=================\n")
    f.write("\n")
    f.write(".. toctree::\n")
    f.write("   :maxdepth: 1\n")
    f.write("\n")
    f.write("\n".join("   " + filename for filename in filenames))

def main():    
    generate_docs("openlibrary")
    generate_docs("infogami")
    #generate_index()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = install_ol
#!/usr/bin/python

'''
Copyright(c)2010 Internet Archive. Software license AGPL version 3.

This script installs a development copy of Open Library.
It has been tested with a clean install of Ubuntu 10.04.

This script is re-runnable, and should be run as root.
'''

import commands
import os
import urllib
import subprocess
import time
import datetime
import json

install_dir = '/home/openlibrary/openlibrary'
install_user = 'openlibrary'
assert 0 == os.getuid()


def cmd(description, command):
    print description
    print "  running " + command
    (ret, out) = commands.getstatusoutput(command)
    print out + "\n"
    assert 0 == ret

def install(package):
    cmd('installing ' + package, 'DEBIAN_FRONTEND=noninteractive apt-get --force-yes -qq install ' + package)

def easyinstall(package):
    cmd('easy-installing ' + package, 'easy_install '+package)

def getjson(url):
    f = urllib.urlopen(url)
    c = f.read()
    f.close()
    obj = json.loads(c) 
    return obj

#install instructions from http://openlibrary.org/dev/docs/setup

### Dependencies

install('postgresql')
install('git-core')
install('openjdk-6-jre')

install('python-setuptools')
cmd('installing version 0.33 of web.py','easy_install web.py==0.33')

easyinstall('Babel')
easyinstall('pyyaml')

install('python-dev') #required for psycopg2
install('libpq-dev')  #required for psycopg2
easyinstall('psycopg2')

easyinstall('simplejson')
easyinstall('python-memcached')

install('libxml2-dev') #required for lxml
install('libxslt-dev') #required for lxml
easyinstall('lxml')

easyinstall('PIL')
easyinstall('pymarc')
easyinstall('genshi')
easyinstall('BeautifulSoup')


### Source code

if not os.path.exists(install_dir):
    cmd('cloning openlibrary repo', '''sudo -u %s git clone git://github.com/openlibrary/openlibrary.git "%s"''' % (install_user, install_dir))
else:
    #directory already exists, so this script is being re-run
    cmd('updating source', '''cd "%s" && sudo -u %s git pull''' % (install_dir, install_user))
 
cmd('run setup.sh to checkout other source code', '''cd "%s" && sudo -u %s ./setup.sh''' % (install_dir, install_user))


### Open Library Web Server

print 'creating postgres user %s and not checking for errors' % install_user
#this command will return an error if the user already exists, just ignore it
commands.getstatusoutput('''sudo -u postgres createuser -d -S -R %s''' % install_user)

print 'creating openlibrary db and not checking for errors\n'
#this command will return an error if the db already exists...
commands.getstatusoutput('''sudo -u %s createdb openlibrary''' % install_user)


cmd('copying configuration file', '''sudo -u %s cat %s/conf/sample_openlibrary.yml |perl -p -e 's/anand/%s/g;' > %s/openlibrary.yml''' % (install_user, install_dir, install_user, install_dir))

cmd('bootstapping', '''cd %s && sudo -u %s ./scripts/openlibrary-server openlibrary.yml install''' % (install_dir, install_user))



#start up an instance of OL on the default port, so we can create users
print 'Starting up an OL instance!'

p = subprocess.Popen(['sudo', '-u', install_user, './scripts/openlibrary-server','openlibrary.yml'], cwd=install_dir)


print "Waiting 5 seconds for OL to start up..."
time.sleep(5)
print "done waiting for OL to start"

#check to see if user openlibrary already exists
obj = getjson('http://0.0.0.0:8080/people/openlibrary.json')

if 'error' in obj:
    print "user openlibrary does not exist. creating it!"

    post_data = '&'.join(('displayname=openlibrary',
                            'username=openlibrary',
                            'password=openlibrary',
                            'email=' + urllib.quote('openlibrary@example.com'),
                            'agreement=yes',
                            'submit=Sign+Up'
                            ))
    
    f = urllib.urlopen('http://0.0.0.0:8080/account/create', post_data)
    c = f.read()
    f.close()

    #print "received this response:"
    #print c

    time.sleep(2) #wait for things to settle

    print "verifying the openlibrary account was created"
    obj = getjson('http://0.0.0.0:8080/people/openlibrary.json')
    assert "error" not in obj

#check to see if openlibrary user is in admin group
obj = getjson('http://0.0.0.0:8080/usergroup/admin.json')
isAdmin = False
for user in obj['members']:
    if '/people/openlibrary' == user['key']:
        isAdmin = True

if not isAdmin:
    post_data = 'type.key=%2Ftype%2Fusergroup&description=Group+of+admin+users.&members%230=%2Fpeople%2Fadmin&members%231=%2Fpeople%2Fopenlibrary&members%232=&members%233=&members%234=&members%235=&members%236=&members%237=&_comment=&_save=Save'

    f = urllib.urlopen('http://0.0.0.0:8080/usergroup/admin?m=edit', post_data)
    c = f.read()
    f.close()
    time.sleep(2)

    #verify openlibrary user is now in admin group
    print 'verifying openlibrary user was added to admin group'
    obj = getjson('http://0.0.0.0:8080/usergroup/admin.json')
    isAdmin = False
    for user in obj['members']:
        if '/people/openlibrary' == user['key']:
            isAdmin = True
    assert isAdmin

else:
    print "openlibrary user is already an admin"


#add openlibrary user to api usergroup, if needed
in_api_group = False
obj = getjson('http://0.0.0.0:8080/usergroup/api.json')
if 'members' in obj:
    for user in obj['members']:
        if '/people/openlibrary' == user['key']:
            in_api_group = True

if not in_api_group:
    post_data = 'type.key=%2Ftype%2Fusergroup&description=&members%230=%2Fpeople%2Fopenlibrary&members%231=&members%232=&members%233=&members%234=&_comment=&_save=Save'

    f = urllib.urlopen('http://0.0.0.0:8080/usergroup/api?m=edit', post_data)
    c = f.read()
    f.close()
    time.sleep(2)

    #verify openlibrary user is now in api group
    print 'verifying openlibrary user was added to api group'
    obj = getjson('http://0.0.0.0:8080/usergroup/api.json')
    in_api_group = False
    for user in obj['members']:
        if '/people/openlibrary' == user['key']:
            in_api_group = True
    assert in_api_group
else:
    print "/people/openlibrary already in api group"


#Copy templates, macros and some config from openlibrary.org website.

cmd('copying upstream config from openlibrary.org',
    '''cd "%s" && sudo -u %s ./scripts/copydocs.py --src http://openlibrary.org/ --dest http://0.0.0.0:8080/ /upstream/*''' % (install_dir, install_user))

cmd('copying edition config from openlibrary.org',
    '''cd "%s" && sudo -u %s ./scripts/copydocs.py --src http://openlibrary.org/ --dest http://0.0.0.0:8080/ /config/edition''' % (install_dir, install_user))

print "creating AccountBot user"
#the /people/AccountBot page exists, but we need to change it's type to 'user'
post_data = 'title=&type.key=%2Ftype%2Fuser&body=&_comment=&_save=Save'
f = urllib.urlopen('http://0.0.0.0:8080/people/AccountBot?m=edit', post_data)
c = f.read()
f.close()


### Infobase Server

cmd('copying infobase conf file', '''sudo -u %s cat %s/conf/sample_infobase.yml |perl -p -e 's/anand/%s/g;' > %s/infobase.yml''' % (install_user, install_dir, install_user, install_dir))

cmd('make coverstore directory', '''sudo -u %s mkdir -p %s/coverstore/localdisk''' % (install_user, install_dir))

print 'starting up infobase'

infobase = subprocess.Popen(['sudo', '-u', install_user, './scripts/infobase-server','infobase.yml', '7000'], cwd=install_dir)
time.sleep(2)

cmd('editing openlibrary.yml to use infobase', """perl -p -i -e 's/#infobase_server/infobase_server/;' %s/openlibrary.yml""" % install_dir)


### Coverstore Web Server

print 'creating coverstore db and not checking for errors\n'
#this command will return an error if the db already exists...
commands.getstatusoutput('''sudo -u %s createdb coverstore''' % install_user)

cmd('adding the coverstore schema', '''sudo -u %s psql coverstore < %s/openlibrary/coverstore/schema.sql''' % (install_user, install_dir))

cmd('copying coverstore conf file', '''sudo -u %s cat %s/conf/sample_coverstore.yml |perl -p -e 's/anand/%s/g;' > %s/coverstore.yml''' % (install_user, install_dir, install_user, install_dir))

print 'starting up coverstore on port 8070'
infobase = subprocess.Popen(['sudo', '-u', install_user, './scripts/coverstore-server','coverstore.yml', '8070'], cwd=install_dir)
time.sleep(2)

print "restarting server!"
p.terminate()
time.sleep(2)
p = subprocess.Popen(['sudo', '-u', install_user, './scripts/openlibrary-server','openlibrary.yml'], cwd=install_dir)
print "Waiting 5 seconds for OL to start up..."
time.sleep(5)
print "done waiting for OL to restart"


### Solr Search Engine

cmd('setting up solr', '''cd %s && sudo -u %s %s/scripts/setup_solr.py''' % (install_dir, install_user, install_dir))

print 'starting solr'
solr = subprocess.Popen(['sudo', '-u', install_user, 'java', '-jar', 'start.jar'], cwd=install_dir+'/vendor/solr')

print 'waiting 30 seconds for solr'
time.sleep(30)

print 'writing ~/.olrc'
f = open(os.path.expanduser('~'+install_user+'/.olrc'), 'w')
f.write('[0.0.0.0:8080]\n')
f.write('username = admin\n')
f.write('password = admin123\n')
f.close()

cmd('making solr update state dir', '''sudo -u %s mkdir -p %s/state''' % (install_user, install_dir))
cmd('writing solr update state', '''sudo -u %s echo %s:0 > %s/state/solr_update''' % (install_user, datetime.date.today().isoformat(), install_dir))

print 'starting solr update script'
solrupdate = subprocess.Popen(['sudo', '-u', install_user, 'scripts/solr_update.py', '--server=0.0.0.0:8080'], cwd=install_dir)


### restart

print "restarting server!"
p.terminate()
time.sleep(2)
p = subprocess.Popen(['sudo', '-u', install_user, './scripts/openlibrary-server','openlibrary.yml'], cwd=install_dir)
print "Waiting 5 seconds for OL to start up..."
time.sleep(5)
print "done waiting for OL to restart"


print 'finished installing openlibrary! please visit http://0.0.0.0:8080'

########NEW FILE########
__FILENAME__ = jsondump
#! /usr/bin/env python
"""Script to generate json dumps.

The script deals with 3 data formats.

1. dump of data table in OL postgres database.
2. rawdump: tab seperated file with key, type and json of page in each row
3. bookdump: dump containing only books with each property expanded. (used for solr import)

To create jsondump:
    
    1. Take dump of data table of the openlibrary database

        $ psql -c 'copy data to stdout' > data.txt

    2. Sort the dump of data table on first key. You may want to do this on a machine other than production db node.

        $ sort -k1 -S 2G data.txt > data_sorted.txt

    3. Create rawdump file.

        $ ./scripts/jsondump.py rawdump data_sorted.txt > rawdump.txt

    4. The rawdump file contains 'key', 'type' and 'json' columns. The json column should be taken to generate jsondump from rawdump.
    
        $ cut -f3 rawdump.txt > jsondump.txt
        
To generate bookdump:

    * Generate rawdump using the above procedure.
    * split types to group the dump by type. This command created type/authors.txt, type/editions.txt etc.
    
        $ ./scripts/jsondump.py split_types rawdump.txt

    * generate bookdump. (This operation is highly memory intensive).
    
        $ ./scripts/jsondump.py bookdump type/edition.txt type/author.txt type/language.txt > bookdump.txt
"""
import sys
import simplejson
import re
import time
import os

commands = {}
def command(f):
    commands[f.__name__] = f
    return f

@command
def rawdump(datafile):
    """Generates a json dump from copy of data table from OL database.
    
    Usage:
    
        $ python jsondump.py rawdump datafile > dumpfile
    """
    write_rawdump(sys.stdout, read_json(read_data_table(datafile)))

@command
def merge(dump, idump):
    """Merges a large dump with increamental dump.

        $ python jsondump.py bigdump.txt dailydump.txt > bigdump2.txt
    """
    def read(path):
        for line in xopen(path):
            key, _ = line.split("\t", 1)[0]
            yield key, line

    def do_merge():
        d = make_dict(read(idump))
        for key, line in read(dump):
            yield d.pop(key, line)
    
    sys.stdout.writelines(do_merge())

@command
def json2rawdump(jsonfile):
    """Converts a file containing json rows to rawdump format.
    """
    write_rawdump(sys.stdout, read_json(jsonfile))

@command
def split_types(rawdump):
    """Write each type of objects in separate files."""
    files = {}
    if not os.path.exists('type'):
        os.mkdir('type')

    for key, type, json in read_rawdump(rawdump):
        if type not in files:
            files[type] = open(type[1:] + '.txt', 'w')
        files[type].write("\t".join([key, type, json]))

    for t in files:
        files[t].close()

@command
def bookdump(editions_file, authors_file, languages_file):
    """Generates bookdump from rawdump.
    """
    def process():
        log("BEGIN read_authors")
        authors = make_dict((key, strip_json(json)) for key, type, json in read_rawdump(authors_file))
        languages = make_dict((key, strip_json(json)) for key, type, json in read_rawdump(languages_file))
        log("END read_authors")
        for key, type, json in read_rawdump(editions_file):
            d = simplejson.loads(strip_json(json))
            d['authors'] = [simplejson.loads(authors.get(a['key']) or '{"key": "%s"}' % a['key']) for a in d.get('authors', []) if isinstance(a, dict)]
            d['languages'] = [simplejson.loads(languages.get(a['key']) or '{"key": "%s"}' % a['key']) for a in d.get('languages', []) if isinstance(a, dict)]
            yield key, type, simplejson.dumps(d) + "\n"

    write_rawdump(sys.stdout, process())

@command
def modified(db, date):
    """Display list of modified keys on a given day.
    
        $ python jsondump.py modified dbname YYYY-MM-DD
    """
    import os
    os.system("""psql %s -t -c "select key from thing where last_modified >= '%s' and last_modified < (date '%s' + interval '1 day')" """ % (db, date, date))

@command
def help(cmd=None):
    """Displays this help."""
    action = cmd and get_action(cmd)
    if action:
        print "python jsondump.py " + cmd
        print 
        print action.__doc__
    else:
        print __doc__
        print "List of commands:"
        print

        for k in sorted(commands.keys()):
            doc = commands[k].__doc__ or " "
            print "  %-10s\t%s" % (k, doc.splitlines()[0])

def get_action(cmd):
    if cmd in commands:
        return commands[cmd]
    else:
        print >> sys.stderr, "No such command:", cmd
        return help

def listget(x, i, default=None):
    try:
        return x[i]
    except IndexError:
        return default
    
def main():
    action = get_action(listget(sys.argv, 1, "help"))
    action(*sys.argv[2:])

#---
def make_sub(d):
    """
        >>> f = make_sub(dict(a='aa', bb='b'))
        >>> f('aabbb')
        'aaaabb'
    """
    def f(a):
        return d[a.group(0)]
    rx = re.compile("|".join(map(re.escape, d.keys())))
    return lambda s: s and rx.sub(f, s)

def invert_dict(d):
    return dict((v, k) for (k, v) in d.items())

_escape_dict = {'\n': r'\n', '\r': r'\r', '\t': r'\t', '\\': r'\\'}

escape = make_sub(_escape_dict)
unescape = make_sub(invert_dict(_escape_dict))

def doctest_escape():
    r"""
        >>> escape("\n\t")
        '\\n\\t'
        >>> unescape('\\n\\t')
        '\n\t'
    """

def read_data_table(path):
    r"""Read dump of postgres data table assuming that it is sorted by first column.
    
        >>> list(read_data_table(['1\t1\tJSON-1-1\n', '1\t2\tJSON-1-2\n', '2\t1\tJSON-2-1\n']))
        ['JSON-1-2\n', 'JSON-2-1\n']
        >>> list(read_data_table(['1\t1\tJSON\\t1-1\n']))
        ['JSON\t1-1\n']
    
    """
    xthing_id = None
    xrev = 0
    xjson = ""

    for line in xopen(path):
        thing_id, rev, json = line.split("\t")
        thing_id = int(thing_id)
        rev = int(rev)
        if not xthing_id:
            xthing_id = thing_id
            xrev = rev
            xjson = json
        if xthing_id == thing_id:
            # take the json with higher rev.
            if rev > xrev:
                xrev = rev
                xjson = json
        else:
            yield unescape(xjson)
            xthing_id = thing_id
            xrev = rev
            xjson = json

    yield unescape(xjson)

def read_rawdump(file):
    r"""
        >>> list(read_rawdump(["/foo\t/type/page\tfoo-json\n", "/bar\t/type/doc\tbar-json\n"]))
        [['/foo', '/type/page', 'foo-json\n'], ['/bar', '/type/doc', 'bar-json\n']]
    """
    return (line.split("\t", 2) for line in xopen(file))

def write_rawdump(file, data):
    # assuming that newline is already present in json (column#3).
    file.writelines("%s\t%s\t%s" % row for row in data)

def read_json(file):
    r"""
        >>> list(read_json(['{"key": "/foo", "type": {"key": "/type/page"}, "title": "foo"}\n']))
        [('/foo', '/type/page', '{"key": "/foo", "type": {"key": "/type/page"}, "title": "foo"}\n')]
    """
    for json in xopen(file):
        d = simplejson.loads(json)        
        ret = (d['key'], d['type']['key'], json)
        if not all(isinstance(i, basestring) for i in ret):
            print 'not all strings:'
            print josn
        yield ret

def xopen(file):
    if isinstance(file, str):
        if file == "-":
            return sys.stdin
        elif file.endswith('.gz'):
            import gzip
            return gzip.open(file)
        else:
            return open(file)
    else:
        return file

def make_dict(items):
    return dict(items)

re_json_strip = re.compile(r', "(latest_revision|revision|id)": \d+|, "(last_modified|type|created)": {[^{}]*}')
def strip_json(json):
    """remove created, last_modified, type, etc from json."""
    return re_json_strip.sub("", json)

def log(*a):
    print >> sys.stderr, time.asctime(), " ".join(map(str, a))

def capture_stdout(f):
    import StringIO
    def g(*a):
        stdout, sys.stdout = sys.stdout, StringIO.StringIO()
        f(*a)
        out, sys.stdout = sys.stdout.getvalue(), stdout
        return out
    return g
        
@command
def test(*args):
    """Test this module.
    """
    sys.argv = args
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = lc_marc_update
#!/usr/bin/env python

from openlibrary.catalog.read_rc import read_rc
from openlibrary import config
from ftplib import FTP
from time import sleep
from lxml import etree
import os, sys, httplib, json, argparse

parser = argparse.ArgumentParser(description='Library of Congress MARC update')
parser.add_argument('--config', default='openlibrary.yml')
args = parser.parse_args()

config_file = args.config
config.load(config_file)
c = config.runtime_config['lc_marc_update']
base_url = 'http://openlibrary.org'
import_api_url = base_url + '/api/import'

def put_file(con, ia, filename, data):
    print 'uploading %s' % filename
    headers = {
        'authorization': "LOW " + c['s3_key'] + ':' + c['s3_secret'],
#        'x-archive-queue-derive': 0,
    }
    url = 'http://s3.us.archive.org/' + ia + '/' + filename
    print url
    for attempt in range(5):
        con.request('PUT', url, data, headers)
        try:
            res = con.getresponse()
        except httplib.BadStatusLine as bad:
            print 'bad status line:', bad.line
            raise
        body = res.read()
        if '<Error>' not in body:
            return
        print 'error'
        print body
        if no_bucket_error not in body and internal_error not in body:
            sys.exit(0)
        print 'retry'
        sleep(5)
    print 'too many failed attempts'


url = 'http://archive.org/download/marc_loc_updates/marc_loc_updates_files.xml'

attempts = 10
wait = 5
for attempt in range(attempts):
    try:
        root = etree.parse(url).getroot()
        break
    except:
        if attempt == attempts-1:
            raise
        print 'error on attempt %d, retrying in %s seconds' % (attempt, wait)
        sleep(wait)

existing = set(f.attrib['name'] for f in root)
#existing.remove("v40.i32.records.utf8") # for testing
#existing.remove("v40.i32.report") # for testing

host = 'rs7.loc.gov'

to_upload = set()

def print_line(f):
    if 'books.test' not in f and f not in existing:
        to_upload.add(f)

def read_block(block):
    global data
    data += block

ftp = FTP(host)
ftp.set_pasv(False)
welcome = ftp.getwelcome()
ftp.login(c['lc_update_user'], c['lc_update_pass'])
ftp.cwd('/emds/books/all')
ftp.retrlines('NLST', print_line)

if to_upload:
    print welcome
else:
    ftp.close()
    sys.exit(0)

bad = open(c['log_location'] + 'lc_marc_bad_import', 'a')

def iter_marc(data):
    pos = 0
    while pos < len(data):
        length = data[pos:pos+5]
        int_length = int(length)
        yield (pos, int_length, data[pos:pos+int_length])
        pos += int_length

def login(h1, password):
    body = json.dumps({'username': 'LCImportBot', 'password': password})
    headers = {'Content-Type': 'application/json'}  
    h1.request('POST', base_url + '/account/login', body, headers)
    print base_url + '/account/login'
    res = h1.getresponse()

    print res.read()
    print 'status:', res.status
    assert res.status == 200
    cookies = res.getheader('set-cookie').split(',')
    cookie =  ';'.join([c.split(';')[0] for c in cookies])
    return cookie


h1 = httplib.HTTPConnection('openlibrary.org')
headers = {
    'Content-type': 'application/marc',
    'Cookie': login(h1, c['ol_bot_pass']),
}
h1.close()

item_id = 'marc_loc_updates'
for f in to_upload:
    data = ''
    print 'downloading', f
    ftp.retrbinary('RETR ' + f, read_block)
    print 'done'
    con = httplib.HTTPConnection('s3.us.archive.org')
    con.connect()
    put_file(con, item_id, f, data)
    con.close()
    if not f.endswith('.records.utf8'):
        continue

    loc_file = item_id + '/' + f
    for p, l, marc_data in iter_marc(data):
        loc = '%s:%d:%d' % (loc_file, p, l)
        headers['x-archive-meta-source-record'] = 'marc:' + loc
        try:
            h1 = httplib.HTTPConnection('openlibrary.org')
            h1.request('POST', import_api_url, marc_data, headers)
            try:
                res = h1.getresponse()
            except httplib.BadStatusLine:
                raise BadImport
            body = res.read()
            if res.status != 200:
                raise BadImport
            else:
                try:
                    reply = json.loads(body)
                except ValueError:
                    print 'not JSON:', `body`
                    raise BadImport
            assert res.status == 200
            print reply
            assert reply['success']
            h1.close()
        except BadImport:
            print >> bad, loc
            bad.flush()

ftp.close()

########NEW FILE########
__FILENAME__ = mail_bad_author_query
#!/usr/bin/python
import web, os, smtplib, sys
from email.mime.text import MIMEText

password = open(os.path.expanduser('~/.openlibrary_db_password')).read()
if password.endswith('\n'):
    password = password[:-1]
db_error = web.database(dbn='postgres', db='ol_errors', host='localhost', user='openlibrary', pw=password)
db_error.printing = False

body = '''When using the Open Library query interface the results should only include
authors. There shouldn't be any redirects or deleted authors in the results.

Below is a list of bad results for author queries from imports that have
run today.
'''

seen = set()
bad_count = 0
for row in db_error.query("select t, query, result from errors where t between 'yesterday' and 'today'"):
    author = row.query
    if author in seen:
        continue
    seen.add(author)
    bad_count += 1
    body += '-' * 60 + '\nAuthor name: ' + author + '\n'
    body += 'http://openlibrary.org/query.json?type=/type/author&name=%s' % web.urlquote(author) + '\n\n'
    body += row.result + '\n'

if bad_count == 0:
    sys.exit(0)

#print body

addr_from = 'edward@archive.org'
addr_to = 'openlibrary@archive.org'
#msg = MIMEText(body, 'plain', 'utf-8')
try:
    msg = MIMEText(body, 'plain', 'iso-8859-15')
except UnicodeEncodeError:
    msg = MIMEText(body, 'plain', 'utf-8')
msg['From'] = addr_from
msg['To'] = addr_to
msg['Subject'] = "import error report: %d bad author queries" % bad_count

s = smtplib.SMTP('mail.archive.org')
s.sendmail(addr_from, [addr_to], msg.as_string())
s.quit()

########NEW FILE########
__FILENAME__ = manage-lists
#! /usr/bin/env python
"""Script to manage lists.

Usage: ./scripts/manage-lists.py command settings.yml args
"""
import _init_path
import sys, os, shutil, tempfile, time, re
import itertools, collections
import optparse
import simplejson
import couchdb
import web
import datetime
import urllib2

from openlibrary.core import formats
from openlibrary.core.lists.updater import Updater

class Command:
    def __init__(self, name=None):
        self.name = name or ""
        self.parser = optparse.OptionParser("%prog " + self.name + " [options]", add_help_option=False)
        self.parser.add_option("-h", "--help", action="store_true", help="display this help message")

        self.init()

    def add_option(self, *a, **kw):
        self.parser.add_option(*a, **kw)

    def add_subcommand(self, name, cmd):
        cmd.name = name
        self.subcommands.append(cmd)
        
    def main(self):
        self(sys.argv[1:])

    def __call__(self, args):
        args = list(args)
        options, args = self.parser.parse_args(args)
        kwargs = options.__dict__
        
        help = kwargs.pop('help', False)
        if help:
            self.help()
        else:
            self.run(*args, **kwargs)
            
    def help(self):
        print self.parser.format_help()

    def init(self):
        pass
        
class MakeEdtions(Command):
    def run(self):
        for line in sys.stdin:
            json = line.strip().split("\t")[-1]
            work = simplejson.loads(json)
            
            seeds = [seed for seed, info in GenerateSeeds().map(work)]
            for e in work['editions']:
                e["seeds"] = seeds
            print "%s\t%s" % (e['key'].encode('utf-8'), simplejson.dumps(e))

class SortWorker:
    def __init__(self, indir, outdir):
        self.indir = indir
        self.outdir = outdir
    
    def __call__(self, filename):
        os.system("sort -S 1G -k1 %s/%s -o %s/%s" % (self.indir, filename, self.outdir, filename))

class GroupWorksWorker:
    """Worker to group edtions of a work."""
    def __init__(self, indir, outdir):
        self.indir = indir
        self.outdir = outdir

    def __call__(self, filename):
        infile = os.path.join(self.indir, filename)
        outfile = os.path.join(self.outdir, filename)
        out = open(outfile, 'w', 50*1024*1024)
        
        for work in self.parse(self.read(open(infile))):
            out.write(work['key'] + "\t" + simplejson.dumps(work) + "\n")
        out.close()

    def read(self, f):
        for line in f:
            key, json = line.strip().split("\t", 1)
            yield key, json

    def parse(self, rows):
        for key, chunk in itertools.groupby(rows, lambda row: row[0]):
            docs = [simplejson.loads(json) for key, json in chunk]

            work = None
            editions = []

            for doc in docs:
                if doc['type']['key'] == '/type/work':
                    work = doc
                else:
                    editions.append(doc)

            if not work:
                work = {
                    "key": editions[0]['works'][0]['key'],
                    "type": {"key": "/type/work"},
                    "dummy_": True
                }

            work['editions'] = editions
            yield work
            
class ProcessChangesets(Command):
    """Command to process changesets.
    
    Changesets are processed and effected seeds are generated for each changeset. This involves accessing infobase for 
    """
    def run(self, configfile):
        pass

class ProcessDump(Command):
    """Processes works and edition dumps and generates documents that can be loaded into couchdb.
    
    The generated files:
        * works.txt
        * editions.txt
    """
    def init(self):
        self.parser.add_option("--no-cleanup", action="store_true", default=False)
        self.parser.add_option("--bucket-size", type="int", default=100000)

    def run(self, works_dump, editons_dump, bucket_size=100000, **options):
        seq = self.read_dumps(works_dump, editons_dump)
        
        tmpdir = tempfile.mkdtemp(prefix="ol-process-dump-")
        
        docs_dir = os.path.join(tmpdir, "docs")
        sorted_dir = os.path.join(tmpdir, "sorted-docs")
        works_dir = os.path.join(tmpdir, "works")
        
        os.mkdir(docs_dir)
        os.mkdir(sorted_dir)
        os.mkdir(works_dir)
        
        self.log("spitting inputs into %s" % docs_dir)
        self.split(seq, docs_dir, bucket_size)
        
        self.log("sorting into %s" % sorted_dir)
        self.sort_files(docs_dir, sorted_dir)

        self.log("grouping works into %s" % works_dir)
        self.group_works(sorted_dir, works_dir)
        
        self.log("merging the sorted works")
        os.system("cat  %s/* > works.txt" % works_dir)

        if not options.get("no_cleanup"):
            self.log("cleaning up...")
            shutil.rmtree(tmpdir)
        self.log("done")
        
    def log(self, msg):
        print >> sys.stderr, time.asctime(), msg
        
    def sort_files(self, dir, outdir):
        for f in os.listdir(dir):
            infile = os.path.join(dir, f)
            outfile = os.path.join(outdir, f)
            self.log("sort %s" % f)
            os.system("sort -S1G -k1 %s -o %s" % (infile, outfile))
        
    def split(self, seq, dir, bucket_size):
        M = 1024*1024
        files = {}
        
        def get_file(index):
            if index not in files:
                files[index] = open(os.path.join(dir, "%04d.txt" % index), "w", 2*M)
            return files[index]
        
        for key, json in seq:
            try:
                index = int(web.numify(key)) / bucket_size
            except Exception:
                print >> sys.stderr, "bad key %s" % key
                continue
                
            get_file(index).write("%s\t%s\n" % (key, json))
        
        for f in files.values():
            f.close()
            
    def read_dumps(self, works_dump, editons_dump):
        return itertools.chain(
            self.read_works_dump(works_dump),
            self.read_editions_dump(editons_dump))

    def read_works_dump(self, works_dump):
        for tokens in self.read_tokens(works_dump):
            key = tokens[1]
            json = tokens[-1]
            yield key, json
            
    def read_editions_dump(self, editons_dump):
        for tokens in self.read_tokens(editons_dump):
            json = tokens[-1]
            doc = simplejson.loads(json)
            if 'works' in doc:
                yield doc['works'][0]['key'], json
                
    def read_tokens(self, filename):
        self.log("reading %s" % filename)
        M=1024*1024
        for i, line in enumerate(xopen(filename, 'r', 100*M)):
            if i % 1000000 == 0:
                self.log(i)
            yield line.strip().split("\t")
                
    def group_works(self, indir, outdir):
        worker = GroupWorksWorker(indir, outdir)
        
        import multiprocessing
        pool = multiprocessing.Pool(4)
        pool.map(worker, os.listdir(indir))
        
MEGABYTE = 1024 * 1024
        
class GenerateEditions(Command):
    """Processes the works.txt, the denormalized works dump, to generate editions.txt with seed info.
    """
    def init(self):
        cmd = GenerateSeeds()
        self.get_subjects = cmd.get_subjects
        self.get_authors = cmd.get_authors
        self.read_works = cmd.read_works
        
    def run(self, works_txt=None):
        works_file = works_txt and xopen(works_txt) or sys.stdin
        works = self.read_works(works_file)
        
        f = open("editions.txt", "w", 10 * MEGABYTE)
        
        for work in works:
            seeds = self.get_seeds(work)
            for e in work.get('editions', []):
                e['seeds'] = [e['key']] + seeds
                f.write("%s\t%s\n" % (e['key'], simplejson.dumps(e)))
        f.close()
        
    def get_seeds(self, work):
        return [work['key']] + \
            [a['key'] for a in self.get_authors(work)] + \
            [s['key'] for s in self.get_subjects(work)]
    
class GenerateSeeds(Command):
    """Processes works.txt and generates seeds.txt.
    
    This command runs a map function over all the docs from works.txt and runs
    a reduce function to generate summary for each seed.
    
    This process involves three steps.
        1. Generate map.txt
        2. Sort map.txt on seed
        3. reduce the map.txt
    """
    def init(self):
        self.re_subject = re.compile("[, _]+")
        self.parser.add_option("--no-cleanup", action="store_true", default=False)
        
    def run(self, works_txt=None, **options):
        works_file = works_txt and xopen(works_txt) or sys.stdin
        works = self.read_works(works_file)
        
        tmpdir = tempfile.mkdtemp(prefix="ol-seeds-")
        
        self.log("tmpdir: %s" % tmpdir)
        
        map_dir = os.path.join(tmpdir, "map")
        sorted_dir = os.path.join(tmpdir, "sorted")
        reduce_dir = os.path.join(tmpdir, "reduce")
        
        os.mkdir(map_dir)
        os.mkdir(sorted_dir)
        os.mkdir(reduce_dir)
        
        files = {}
        for work in works:
            for seed, info in self.map(work):
                self.write(map_dir, files, seed, info)
        for f in files.values():
            f.close()
        
        self.log("sorting...")
        self.sort_files(map_dir, sorted_dir)

        self.log("running reduce")
        self.reduce_files(sorted_dir, reduce_dir)

        self.log("merging the results...")
        os.system("cat %s/* > seeds.txt" % reduce_dir)
        
        if not options.get("no_cleanup"):
            self.log("cleaning up...")
            shutil.rmtree(tmpdir)
        self.log("done")
        
    def reduce_files(self, indir, outdir):
        for f in os.listdir(indir):
            infile = os.path.join(indir, f)
            outfile = os.path.join(outdir, f)
            self.log("reduce %s" % f)
            
            out = open(outfile, "w", 10*1024*1024)
            for key, chunk in itertools.groupby(self.read_kvs(infile), lambda kv: kv[0]):
                values = [v for k, v in chunk]
                val = self.reduce(values)
                out.write("%s\t%s\n" % (key, simplejson.dumps(val)))
            out.close()
                
    def read_kvs(self, filename):
        for line in open(filename):
            key, value = line.strip().split("\t")
            value = simplejson.loads(value)
            yield key, value
        
    def write(self, dir, files, key, value):
        index = self.hash(key)
        if index not in files:
            files[index] = open(os.path.join(dir, "%04d.txt" % index), "w", 1024*1024)
            
        text = "%s\t%s\n" % (key.encode('utf-8'), simplejson.dumps(value))
        files[index].write(text)
        
    def hash(self, key):
        """Returns an integer in the range [0..999] for each key.
        """
        if key.startswith("/authors/"):
            n = int(web.numify(key))
            return 100 + n / 1000000
        elif key.startswith("/books/"):
            n = int(web.numify(key))
            return 200 + n / 1000000
        elif key.startswith("person:"):
            return 400
        elif key.startswith("place:"):
            return 410
        elif key.startswith("subject:"):
            return 420
        elif key.startswith("time:"):
            return 430
        elif key.startswith("/works/"):
            n = int(web.numify(key))
            return 800 + n / 1000000
        else:
            return 900

    def sort_files(self, dir, outdir):
        for f in os.listdir(dir):
            infile = os.path.join(dir, f)
            outfile = os.path.join(outdir, f)
            self.log("sort %s" % f)
            os.system("sort -S1G -k1 %s -o %s" % (infile, outfile))

    def read_works(self, works_file):
        self.log("reading " + works_file.name)
        for i, line in enumerate(works_file):
            if i % 1000000 == 0:
                self.log(i)
            key, json = line.strip().split("\t", 1)
            yield simplejson.loads(json)
            
    def log(self, msg):
        print >> sys.stderr, time.asctime(), msg
            
    def map(self, work):
        """Map function for generating seed info.
        Returns a generator with (seed, seed_info) for each seed of the given work.
        
        Seed_info is of the following format:
        
            {
                "works": 1,
                "editions": 10,
                "subjects": [{"name": "San Francisco", key="place:san_francisco"}],
                "last_modified": "2010-10-11T10:20:30"
            }
        """
        authors = self.get_authors(work)
        subjects = self.get_subjects(work)
        editions = work.get("editions", [])
        ebooks = [e for e in editions if "ocaid" in e]

        docs = [work] + work.get("editions", [])

        try:
            last_modified = max(doc['last_modified']['value'] for doc in docs if 'last_modified' in doc)
        except ValueError:
            last_modified = ""
            
        xwork = {
            "works": 1,
            "subjects": subjects,
            "editions": len(editions),
            "ebooks": len(ebooks),
            "last_modified": last_modified
        }
        
        yield work['key'], xwork        
        for a in authors:
            yield a['key'], xwork
            
        for e in editions:
            yield e['key'], dict(xwork, editions=1)
        
        for s in subjects:
            yield s['key'], dict(xwork, subjects=[s])
            
    def reduce(self, values):
        works = len(values)
        editions = sum(v['editions'] for v in values)
        ebooks = sum(v['ebooks'] for v in values)
        last_update = max(v['last_modified'] for v in values)
        
        subjects = self.process_subjects(s for v in values for s in v['subjects'])
        
        return {
            "works": works,
            "editions": editions,
            "ebooks": ebooks,
            "last_update": last_update,
            "subjects": subjects
        }
                
    def most_used(self, seq):
        d = collections.defaultdict(lambda: 0)
        for x in seq:
            d[x] += 1
        return sorted(d, key=lambda k: d[k], reverse=True)[0] 

    def process_subjects(self, subjects):
        d = collections.defaultdict(list)

        for s in subjects:
            d[s['key']].append(s['name'])

        subjects = [{"key": key, "name": self.most_used(names), "count": len(names)} for key, names in d.items()]
        subjects.sort(key=lambda s: s['count'], reverse=True)
        return subjects

    def get_authors(self, work):
        return [a['author'] for a in work.get('authors', []) if 'author' in a]

    def _get_subject(self, subject, prefix):
        if isinstance(subject, basestring):
            key = prefix + self.re_subject.sub("_", subject.lower()).strip("_")
            return {"key": key, "name": subject}

    def get_subjects(self, work):
        subjects = [self._get_subject(s, "subject:") for s in work.get("subjects", [])]
        places = [self._get_subject(s, "place:") for s in work.get("subject_places", [])]
        people = [self._get_subject(s, "person:") for s in work.get("subject_people", [])]
        times = [self._get_subject(s, "time:") for s in work.get("subject_times", [])]
        return [s for s in subjects + places + people + times if s is not None]
        
class CouchDBImport(Command):
    def run(self, url):
        db = couchdb.Database(url)
        for docs in web.group(self.read(), 1000):
            db.update(docs)
        
    def read(self):
        for line in sys.stdin:
            key, json = line.strip().split("\t")
            doc = simplejson.loads(json)
            doc['_id'] = key
            yield doc
            
class LogReplay(Command):
    """Reads to the log file and updates the works and editions db.
    """
    def init(self):
        self.parser.usage = '%prog log-replay [options] lists_config'
        self.add_option("--offset-file", help="file to store the current log offset.", default="/var/run/openlibrary/list_updater.offset")
    
    def run(self, configfile, offset_file):
        self.offset_file = offset_file
        conf = formats.load_yaml(open(configfile).read())
        updater = Updater(conf.get("lists"))
        
        infobase_log_url = "http://%s/openlibrary.org/log" % conf.get("infobase_server")
        self.read_changesets(infobase_log_url, updater.process_changesets)
    
    def read_log(self, url, callback, chunksize=100):
        offset = self.read_offset(self.offset_file) or self.default_offset()

        while True:
            json = self.wget("%s/%s?limit=%d" % (url, offset, chunksize))
            d = simplejson.loads(json)

            if not d['data']:
                print >> sys.stderr, time.asctime(), "sleeping for 2 sec."
                time.sleep(2)
                continue

            callback(d['data'])

            offset = d['offset']
            self.write(self.offset_file, offset)

    def read_changesets(self, url, callback, chunksize=100):
        def f(rows):
            changesets = [row['data']['changeset'] for row in rows
                            if row.get('data', {}).get('changeset')]

            changesets and callback(changesets)
        self.read_log(url, callback=f, chunksize=chunksize)

    def wget(self, url):
        print >> sys.stderr, time.asctime(), "wget", url
        return urllib2.urlopen(url).read()

    def write(self, filename, text):
        f = open(filename, "w")
        f.write(text)
        f.close()
        
    def read_offset(self, filename):
        try:
            return open(filename).read().strip()
        except IOError:
            return None

    def default_offset(self):
        return datetime.date.today().isoformat() + ":0"

class UpdateSeeds(Command):
    def init(self):
        self.parser.usage = '%prog update-seeds lists_config'
        
    def run(self, configfile):
        conf = self.read_lists_config(configfile)
        updater = Updater(conf)
        
        while True:
            seeds = updater.update_pending_seeds(limit=100)
            if not seeds:
                print >> sys.stderr, time.asctime(), "no pending seeds. sleeping for 2 seconds."
                time.sleep(2)
        
    def read_lists_config(self, configfile):
        conf = formats.load_yaml(open(configfile).read())
        return conf.get("lists")

            
def xopen(filename, mode="r", buffering=1):
    if filename.endswith(".gz"):
        import gzip
        return gzip.open(filename, mode, buffering)
    else:
        return open(filename, mode, buffering)
        
def main(cmd=None, *args):
    commands = {
        "process-dump": ProcessDump(),
        "generate-seeds": GenerateSeeds(),
        "generate-editions": GenerateEditions(),
        "make-editions": MakeEdtions(),
        "couchdb-import": CouchDBImport(),
        "log-replay": LogReplay(),
        "update-seeds": UpdateSeeds()
    }
    if cmd in commands:
        commands[cmd](args)
    elif cmd is None or cmd == "--help" or cmd == "help":
        print __doc__
        
        print "Available commands:"
        print
        for name in sorted(commands):
            if not name.startswith("_"):
                print "  " + name
        print
    else:
        print >> sys.stderr, "Unknown command %r. see '%s --help'." % (cmd, sys.argv[0])
        
if __name__ == "__main__":
    main(*sys.argv[1:])
    

########NEW FILE########
__FILENAME__ = migrate_db
#! /usr/bin/env python
"""Script to migrate the OL database to latest schema.
"""

import itertools
import os
import sys

import simplejson
import web

changelog = """\
2010-04-22: 10 - Created unique index on thing.key
2010-07-01: 11 - Added `seq` table
2010-08-02: 12 - Added `data` column to transaction table.
2010-08-03: 13 - Added `changes` column to transaction table
2010-08-13: 14 - Added `transaction_index` table to index data in the transaction table.
"""

LATEST_VERSION = 14

class Upgrader:
    def upgrade(self, db):
        v = self.get_database_version(db)
        
        print "current db version:", v
        print "latest version:", LATEST_VERSION
        
        t = db.transaction()
        try:
            for i in range(v, LATEST_VERSION):
                print "upgrading to", i+1
                f = getattr(self, "upgrade_%03d" % (i+1))
                f(db)
        except: 
            print
            print "**ERROR**: Failed to complete the upgrade. rolling back..."
            print
            t.rollback()
            raise
        else:
            t.commit()
            print "done"
    
    def upgrade_011(self, db):
        """Add seq table."""
        q = """CREATE TABLE seq (
            id serial primary key,
            name text unique,
            value int default 0
        )"""
        db.query(q)
            
    def upgrade_012(self, db):
        """Add data column to transaction table."""
        db.query("ALTER TABLE transaction ADD COLUMN data text")
    
    def upgrade_013(self, db):
        """Add changes column to transaction table."""
        db.query("ALTER TABLE transaction ADD COLUMN changes text")
        
        # populate changes
        rows = db.query("SELECT thing.key, version.revision, version.transaction_id" 
            + " FROM  thing, version"
            + " WHERE thing.id=version.thing_id"
            + " ORDER BY version.transaction_id")
        
        for tx_id, changes in itertools.groupby(rows, lambda row: row.transaction_id):
            changes = [{"key": row.key, "revision": row.revision} for row in changes]
            db.update("transaction", where="id=$tx_id", changes=simplejson.dumps(changes), vars=locals())
        
    def upgrade_014(self, db):
        """Add transaction_index table."""
        
        q = """
        create table transaction_index (
            tx_id int references transaction,
            key text,
            value text
        );
        create index transaction_index_key_value_idx ON transaction_index(key, value);
        create index transaction_index_tx_id_idx ON transaction_index(tx_id);
        """
        db.query(q)

    def get_database_version(self, db):
        schema = self.read_schema(db)
    
        if 'seq' not in schema:
            return 10
        elif 'data' not in schema['transaction']:
            return 11
        elif 'changes' not in schema['transaction']:
            return 12
        elif 'transaction_index' not in schema:
            return 13
        else:
            return LATEST_VERSION

    def read_schema(self, db):
        rows = db.query("SELECT table_name, column_name,  data_type "
            + " FROM information_schema.columns"
            + " WHERE table_schema = 'public'") 

        schema = web.storage()
        for row in rows:
            t = schema.setdefault(row.table_name, web.storage())
            t[row.column_name] = row
        return schema

def usage():
    print >> sys.stderr
    print >> sys.stderr, "USAGE: %s dbname" % sys.argv[0]
    print >> sys.stderr

def main():
    if len(sys.argv) != 2:
        usage()
        sys.exit(1)
    elif sys.argv[1] in ["-h", "--help"]:
        usage()
    else:
        dbname = sys.argv[1]
        db = web.database(dbn='postgres', db=dbname, user=os.getenv('USER'), pw='')
        Upgrader().upgrade(db)
 
if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = minicron
#!/usr/bin/env python

import os
import sys
import datetime
import logging
import optparse
import _init_path
from openlibrary.core import minicron


def parse_options(args):
    parser = optparse.OptionParser(usage = "%prog [options] crontabfile")
    parser.add_option("-s", "--start-time", dest = "starttime",
                      default = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                      help = "Start time in the form YYYY-MM-DD hh:mm:ss (default is now)")
    parser.add_option("-d", "--debug", action = "store_true", dest = "debug", default = False,
                      help = "Enable debugging output")
    parser.add_option("-i", "--interval", dest="interval", type="int", default = 60,
                      help = "Number of seconds that are equivalent to a minute of the cron's internal timer. This is useful for scaling.(default is 60)")
    opts, args = parser.parse_args(args)
    if not args:
        raise parser.error("No crontab file specified")
    return opts, args


def main():
    opts, args = parse_options(sys.argv[1:])
    if opts.debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(level=level, format = "[%(levelname)s] : %(filename)s:%(lineno)d : %(message)s")
    cron = minicron.Minicron(args[0], datetime.datetime.strptime(opts.starttime, "%Y-%m-%d %H:%M:%S"), opts.interval)
    try:
        cron.run()
    except KeyboardInterrupt:
        logging.info("User initiated shutdown")
        return 0
    return -1
    
    
if __name__ == "__main__":
    sys.exit(main())

        
        
        
        
        

########NEW FILE########
__FILENAME__ = new-solr-updater
"""New script to handle solr updates.

Author: Anand Chitipothu

Changes:
2013-02-25: First version
"""

import _init_path

import yaml
import logging
import json
import urllib, urllib2
import argparse
import datetime
import time
import web
import sys
import re
import socket

from openlibrary.solr import update_work
from openlibrary import config

logger = logging.getLogger("solr-updater")

LOAD_IA_SCANS = False
COMMIT = True

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config')
    parser.add_argument('--state-file', default="solr-update.state")
    parser.add_argument('--ol-url', default="http://openlibrary.org/")
    parser.add_argument('--socket-timeout', type=int, default=10)
    parser.add_argument('--load-ia-scans', dest="load_ia_scans", action="store_true", default=False)
    parser.add_argument('--no-commit', dest="commit", action="store_false", default=True)
    return parser.parse_args()

def load_config(path):
    c = yaml.safe_load(open(path))

    # required for update_work module to work
    config.runtime_config = c
    return c

def read_state_file(path):
    try:
        return open(path).read()
    except IOError:
        logger.error("State file %s is not found. Reading log from the beginning of today", path)
        return datetime.date.today().isoformat() + ":0"

def get_default_offset():
    return datetime.date.today().isoformat() + ":0"


class InfobaseLog:
    def __init__(self, hostname):
        self.base_url = 'http://%s/openlibrary.org/log' % hostname
        self.offset = get_default_offset()

    def tell(self):
        return self.offset

    def seek(self, offset):
        self.offset = offset.strip()

    def read_records(self, max_fetches=10):
        """Reads all the available log records from the server.
        """
        for i in range(max_fetches):
            url = "%s/%s?limit=100" % (self.base_url, self.offset)
            logger.info("Reading log from %s", url)
            try:
                jsontext = urllib2.urlopen(url).read()
            except urllib2.URLError as e:
                logger.error("Failed to open URL %s", url, exc_info=True)
                if e.args and e.args[0].args == (111, 'Connection refused'):
                    logger.error('make sure infogami server is working, connection refused from %s', url)
                    sys.exit(1)
                raise

            try:
                d = json.loads(jsontext)
            except:
                logger.error("Bad JSON: %s", jsontext)
                raise
            data = d['data']
            # no more data is available
            if not data:
                logger.info("no more records found")
                return

            for record in data:
                yield record

            self.offset = d['offset']

def parse_log(records):    
    for rec in records:
        action = rec.get('action')
        if action == 'save':
            key = rec['data'].get('key')
            if key:
                yield key
        elif action == 'save_many':
            changes = rec['data'].get('changeset', {}).get('changes', [])
            for c in changes:
                yield c['key']

        elif action == 'store.put':
            # A sample record looks like this:
            # {
            #   "action": "store.put", 
            #   "timestamp": "2011-12-01T00:00:44.241604", 
            #   "data": {
            #       "data": {"borrowed": "false", "_key": "ebooks/books/OL5854888M", "_rev": "975708", "type": "ebook", "book_key": "/books/OL5854888M"},
            #       "key": "ebooks/books/OL5854888M"
            #   }, 
            #   "site": "openlibrary.org"
            # }
            data = rec.get('data', {}).get("data", {})
            key = data.get("_key", "")
            if data.get("type") == "ebook" and key.startswith("ebooks/books/"):
                edition_key = data.get('book_key')
                if edition_key:
                    yield edition_key
            elif LOAD_IA_SCANS and data.get("type") == "ia-scan" and key.startswith("ia-scan/"):
                identifier = data.get('identifier')
                if identifier and is_allowed_itemid(identifier):
                    yield "/books/ia:" + identifier

            # Hack to force updating something from admin interface
            # The admin interface writes the keys to update to a document named 
            # 'solr-force-update' in the store and whatever keys are written to that 
            # are picked by this script
            elif key == 'solr-force-update':
                keys = data.get('keys')
                for k in keys:
                    yield k

        elif action == 'store.delete':
            key = rec.get("data", {}).get("key")
            # An ia-scan key is deleted when that book is deleted/darked from IA.
            # Delete it from OL solr by updating that key
            if key.startswith("ia-scan/"):
                ol_key = "/works/ia:" + key.split("/")[-1]
                yield ol_key

def is_allowed_itemid(identifier):
    if not re.match("^[a-zA-Z0-9_.-]*$", identifier):
        return False

    # items starts with these prefixes are not books. Ignore them.
    ignore_prefixes = config.runtime_config.get("ia_ignore_prefixes", [])
    for prefix in ignore_prefixes:
        if identifier.startswith(prefix):
            return False
        
    return True            

def update_keys(keys):
    keys = (k for k in keys if k.count("/") == 2 and k.split("/")[1] in ["books", "authors", "works"])

    count = 0
    for chunk in web.group(keys, 100):
        chunk = list(chunk)
        count += len(chunk)
        update_work.update_keys(chunk, commit=False)

    if count:
        logger.info("updated %d documents", count)

    return count

class Solr:
    def __init__(self):
        self.reset()

    def reset(self):
        self.total_docs = 0
        self.t_start = time.time()

    def commit(self, ndocs):
        """Performs solr commit only if there are sufficient number
        of documents or enough time has been passed since last commit.
        """
        self.total_docs += ndocs

        # no documents to commit
        if not self.total_docs:
            return

        dt = time.time() - self.t_start
        if self.total_docs > 100 or dt > 60:
            logger.info("doing solr commit (%d docs updated, last commit was %0.1f seconds ago)", self.total_docs, dt)
            self._solr_commit()
            self.reset()
        else:
            logger.info("skipping solr commit (%d docs updated, last commit was %0.1f seconds ago)", self.total_docs, dt)

    def _solr_commit(self):
        logger.info("BEGIN commit")
        update_work.solr_update(['<commit/>'], index="works")
        logger.info("END commit")

def process_args(args):
    # Sometimes archive.org requests blocks forever. 
    # Setting a timeout will make the request fail instead of waiting forever. 
    socket.setdefaulttimeout(args.socket_timeout)

    global LOAD_IA_SCANS, COMMIT
    LOAD_IA_SCANS = args.load_ia_scans
    COMMIT = args.commit

def main():
    FORMAT = "%(asctime)-15s %(levelname)s %(message)s"
    logging.basicConfig(level=logging.INFO, format=FORMAT)

    logger.info("BEGIN new-solr-updater")

    args = parse_arguments()
    process_args(args)

    # set OL URL when running on a dev-instance
    if args.ol_url:
        host = web.lstrips(args.ol_url, "http://").strip("/")
        update_work.set_query_host(host)

    config = load_config(args.config)

    state_file = args.state_file
    offset = read_state_file(state_file)

    logfile = InfobaseLog(config['infobase_server'])
    logfile.seek(offset)

    solr = Solr()

    while True:
        records = logfile.read_records()
        keys = parse_log(records)
        count = update_keys(keys)

        offset = logfile.tell()
        logger.info("saving offset %s", offset)
        with open(state_file, "w") as f:
            f.write(offset)

        if COMMIT:
            solr.commit(ndocs=count)

        # don't sleep after committing some records. 
        # While the commit was on, some more edits might have happened.
        if count == 0:
            logger.info("No more log records available, sleeping...")
            time.sleep(5)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = oclc_to_marc
"""Find marc record URL from oclc number.

Usage: python oclc_to_marc.py oclc_1 oclc_2
"""
import urllib
import simplejson

root = "http://openlibrary.org"

def wget(path):
    return simplejson.loads(urllib.urlopen(root + path).read())

def find_marc_url(d):
    if d.get('source_records'):
        return d['source_records'][0]

    # some times initial revision is 2 instead of 1. So taking first 3 revisions (in reverse order) 
    # and picking the machine comment from the last one
    result = wget('%s.json?m=history&offset=%d' % (d['key'], d['revision']-3))
    if result:
        return result[-1]['machine_comment'] or ""
    else:
        return ""

def main(oclc):
    query = urllib.urlencode({'type': '/type/edition', 'oclc_numbers': oclc, '*': ''})
    result = wget('/query.json?' + query)

    for d in result:
        print "\t".join([oclc, d['key'], find_marc_url(d)])

if __name__ == "__main__":
    import sys
    if len(sys.argv) == 1 or "-h" in sys.argv or "--help" in sys.argv:
        print >> sys.stderr, __doc__
    else:
        for oclc in sys.argv[1:]:
            main(oclc)

########NEW FILE########
__FILENAME__ = oldump
#! /usr/bin/env python
import _init_path
from openlibrary.data import dump

if __name__ == "__main__":
    import sys
    dump.main(sys.argv[1], sys.argv[2:])

########NEW FILE########
__FILENAME__ = pull-templates
#! /usr/bin/env python
"""Script to pull templates and macros from an openlibrary instance to repository.
"""
import _init_path

import os
import web
from optparse import OptionParser

from openlibrary.api import OpenLibrary, marshal

def parse_options(args=None):
    parser = OptionParser(args)
    parser.add_option("-s", "--server", dest="server", default="http://openlibrary.org/", help="URL of the openlibrary website (default: %default)")
    parser.add_option("--template-root", dest="template_root", default="/upstream", help="Template root (default: %default)")
    parser.add_option("--default-plugin", dest="default_plugin", default="upstream", help="Default plugin (default: %default)")

    options, args = parser.parse_args()

    options.template_root = options.template_root.rstrip("/")    
    return options, args

def write(path, text):
    print "saving", path
    
    dir = os.path.dirname(path)
    if not os.path.exists(dir):
        os.makedirs(dir)

    text = text.replace("\r\n", "\n").replace("\r", "\n")
        
    f = open(path, "w")
    f.write(text.encode("utf-8"))
    f.close()

def delete(path):
    print "deleting", path
    if os.path.exists(path):
        os.remove(path)

def make_path(doc):
    if doc['key'].endswith(".css"):
        return "static/css/" + doc['key'].split("/")[-1]
    elif doc['key'].endswith(".js"):
        return "openlibrary/plugins/openlibrary/js/" + doc['key'].split("/")[-1]
    else:
        key = doc['key'].rsplit(".")[0]
        key = web.lstrips(key, options.template_root)
        
        plugin = doc.get("plugin", options.default_plugin)
        return "openlibrary/plugins/%s%s.html" % (plugin, key)

def get_value(doc, property):
    value = doc.get(property, "")
    if isinstance(value, dict) and "value" in value:
        return value['value']
    else:
        return value

def main():
    global options
    options, args = parse_options()
    ol = OpenLibrary(options.server)

    for pattern in args:
        docs = ol.query({"key~": pattern, "*": None}, limit=1000)
        for doc in marshal(docs):
            # Anand: special care to ignore bad documents in the database.
            if "--duplicate" in doc['key']:
                continue

            if doc['type']['key'] == '/type/template':
                write(make_path(doc), get_value(doc, 'body'))
            elif doc['type']['key'] == '/type/macro':
                write(make_path(doc), get_value(doc, 'macro'))
            elif doc['type']['key'] == '/type/rawtext':
                write(make_path(doc), get_value(doc, 'body'))
            else:
                delete(make_path(doc))

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = solr_author_merge
#!/usr/bin/python

import _init_path

import sys
from openlibrary import config
from time import time, sleep
import argparse, simplejson, re
from urllib2 import URLError, urlopen
from openlibrary.catalog.utils.query import withKey, set_query_host
from openlibrary.solr.update_work import update_author, update_work, get_work_subjects, add_field, solr_update, AuthorRedirect
from collections import defaultdict
from lxml.etree import tostring, Element
from openlibrary.solr.update import subject_count, subject_need_update

parser = argparse.ArgumentParser(description='solr author merge')
parser.add_argument('--config', default='openlibrary.yml')
parser.add_argument('--state_file', default='author_merge')
args = parser.parse_args()

config_file = args.config
config.load(config_file)

base = 'http://%s/openlibrary.org/log/' % config.runtime_config['infobase_server']
set_query_host('openlibrary.org')

state_file = config.runtime_config['state_dir'] + '/' + args.state_file
offset = open(state_file).readline()[:-1]

#out = open('author_merge_logs', 'w')

re_author_key = re.compile(r'^/(?:a|authors)/(OL\d+A)$')

update_times = []

subjects_to_update = set()
authors_to_update = []

def solr_update_authors(authors_to_update):
    for a in authors_to_update:
        try:
            author_updates = ['<delete>' + ''.join('<id>%s</id>' % re_author_key.match(akey).group(1) for akey in a['redirects']) + '</delete>']
        except:
            print 'redirects'
            print a['redirects']
            raise
        author_updates += update_author(a['master_key'], a=a['master'], handle_redirects=False)
    solr_update(author_updates, index='authors', debug=False)
    solr_update(['<commit/>'], index='authors', debug=True)

def solr_update_subjects():
    global subjects_to_update
    print subjects_to_update

    subject_add = Element("add")
    for subject_type, subject_name in subjects_to_update:
        key = subject_type + '/' + subject_name
        count = subject_count(subject_type, subject_name)

        if not subject_need_update(key, count):
            print 'no updated needed:', (subject_type, subject_name, count)
            continue
        print 'updated needed:', (subject_type, subject_name, count)

        doc = Element("doc")
        add_field(doc, 'key', key)
        add_field(doc, 'name', subject_name)
        add_field(doc, 'type', subject_type)
        add_field(doc, 'count', count)
        subject_add.append(doc)

    if len(subject_add):
        print 'updating subjects'
        add_xml = tostring(subject_add).encode('utf-8')
        solr_update([add_xml], debug=False, index='subjects')
        solr_update(['<commit />'], debug=True, index='subjects')

    subjects_to_update = set()

def solr_updates(i):
    global subjects_to_update, authors_to_update
    t0 = time()
    d = i['data']
    changeset = d['changeset']
    print 'author:', d['author']
    try:
        assert len(changeset['data']) == 2 and 'master' in changeset['data'] and 'duplicates' in changeset['data']
    except:
        print d['changeset']
        raise
    master_key = changeset['data']['master']
    dup_keys = changeset['data']['duplicates']
    assert dup_keys
    print 'timestamp:', i['timestamp']
    print 'dups:', dup_keys
    print 'records to update:', len(d['result'])

    master = None
    obj_by_key = {}
    works = []
    editions_by_work = defaultdict(list)
    for obj in d['query']:
        obj_type = obj['type']['key']
        k = obj['key']
        if obj_type == '/type/work':
            works.append(obj['key'])
        elif obj_type == '/type/edition':
            if 'works' not in obj:
                continue
            for w in obj['works']:
                editions_by_work[w['key']].append(obj)
        obj_by_key[k] = obj
    master = obj_by_key.get(master_key)
    #print 'master:', master

    if len(d['result']) == 0:
        print i

    work_updates = []
    for wkey in works:
            #print 'editions_by_work:', editions_by_work
            work = obj_by_key[wkey]
            work['editions'] = editions_by_work[wkey]
            subjects = get_work_subjects(work)
            for subject_type, values in subjects.iteritems():
                subjects_to_update.update((subject_type, v) for v in values)
            try:
                ret = update_work(work, obj_cache=obj_by_key, debug=True)
            except AuthorRedirect:
                work = withKey(wkey)
                work['editions'] = editions_by_work[wkey]
                ret = update_work(work, debug=True, resolve_redirects=True)
            work_updates += ret
    if work_updates:
        solr_update(work_updates, debug=False, index='works')

    authors_to_update.append({ 'redirects': dup_keys, 'master_key': master_key, 'master': master})
    print 'authors to update:', len(authors_to_update)

    t1 = time() - t0
    update_times.append(t1)
    print 'update takes: %d seconds' % t1
    print

while True:
    url = base + offset
#out = open('author_redirects', 'w')
#for url in open('author_merge_logs'):
    print url,

    try:
        data = urlopen(url).read()
    except URLError as inst:
        if inst.args and inst.args[0].args == (111, 'Connection refused'):
            print 'make sure infogami server is working, connection refused from:'
            print url
            sys.exit(0)
        print 'url:', url
        raise
    try:
        ret = simplejson.loads(data)
    except:
        open('bad_data.json', 'w').write(data)
        raise

    offset = ret['offset']
    data_list = ret['data']
    if len(data_list) == 0:
        if authors_to_update:
            print 'commit'
            solr_update(['<commit/>'], debug=True, index='works')
            solr_update_authors(authors_to_update)
            authors_to_update = []
            solr_update_subjects()

        print 'waiting'
        sleep(10)
        continue
    else:
        print
    for i in data_list:
        action = i.pop('action')
        if action != 'save_many':
            continue
        if i['data']['comment'] != 'merge authors':
            continue
        if 'changeset' not in i['data']:
            print i
        if i['data']['changeset']['kind'] == 'edit-book':
            continue
        if i['timestamp'] == '2010-08-05T14:37:25.139418':
            continue # bad author redirect
        if len(i['data']['result']) == 0:
            continue # no change
        solr_updates(i)

        print "average update time: %.1f seconds" % (float(sum(update_times)) / float(len(update_times)))
    print >> open(state_file, 'w'), offset
   
#out.close()

########NEW FILE########
__FILENAME__ = solr_author_merge_work_finder
#!/usr/bin/python

import _init_path

from openlibrary import config
import argparse, simplejson, re
from urllib import urlopen
from time import time, sleep
from openlibrary.catalog.works.find_works import find_title_redirects, find_works, get_books, books_query, update_works

parser = argparse.ArgumentParser(description='solr author merge')
parser.add_argument('--config', default='openlibrary.yml')
parser.add_argument('--state_file', default='author_merge_work_finder')
args = parser.parse_args()

config_file = args.config
config.load(config_file)

update_times = []

state_file = config.runtime_config['state_dir'] + '/' + args.state_file
offset = open(state_file).readline()[:-1]

base = 'http://%s/openlibrary.org/log/' % config.runtime_config['infobase_server']

def run_work_finder(i):
    t0 = time()
    d = i['data']
    print 'timestamp:', i['timestamp']
    print 'author:', d['author']
    print '%d records updated:' % len(d['result'])
    if 'changeset' not in d:
        print 'no changeset in author merge'
        print
        return
    changeset = d['changeset']

    try:
        assert len(changeset['data']) == 2 and 'master' in changeset['data'] and 'duplicates' in changeset['data']
    except:
        print d['changeset']
        raise
    akey = changeset['data']['master']
    dup_keys = changeset['data']['duplicates']
    #print d['changeset']
    print 'dups:', dup_keys

    title_redirects = find_title_redirects(akey)
    works = find_works(get_books(akey, books_query(akey)), existing=title_redirects)
    print 'author:', akey
    print 'works:', works
    updated = update_works(akey, works, do_updates=True)
    print '%d records updated' % len(updated)

    t1 = time() - t0
    update_times.append(t1)
    print 'update takes: %d seconds' % t1
    print

while True:
    url = base + offset
    print url,

    try:
        data = urlopen(url).read()
    except URLError as inst:
        if inst.args and inst.args[0].args == (111, 'Connection refused'):
            print 'make sure infogami server is working, connection refused from:'
            print url
            sys.exit(0)
        print 'url:', url
        raise
    try:
        ret = simplejson.loads(data)
    except:
        open('bad_data.json', 'w').write(data)
        raise

    offset = ret['offset']
    data_list = ret['data']
    if len(data_list) == 0:
        print 'waiting'
        sleep(10)
        continue
    else:
        print
    for i in data_list:
        action = i.pop('action')
        if action != 'save_many':
            continue
        if i['data']['changeset']['kind'] != 'merge-authors':
            continue
        if len(i['data']['result']) == 0:
            continue # no change
        print 'run work finder'
        try:
            run_work_finder(i)
        except:
            print offset
            raise

        if update_times:
            print "average update time: %.1f seconds" % (float(sum(update_times)) / float(len(update_times)))
    print >> open(state_file, 'w'), offset


########NEW FILE########
__FILENAME__ = solr_update
#!/usr/bin/env python

import _init_path

from urllib import urlopen
from urllib2 import URLError

import simplejson, re
from time import time, sleep
from openlibrary.catalog.utils.query import withKey, set_query_host
from openlibrary.solr.update import subject_count, subject_need_update
from openlibrary.solr.update_work import update_work, solr_update, update_author, AuthorRedirect, get_work_subjects, add_field, strip_bad_char
from lxml.etree import tostring, Element
from openlibrary.api import OpenLibrary, Reference
from openlibrary.catalog.read_rc import read_rc
from openlibrary import config

import argparse
from os.path import exists
import sys

parser = argparse.ArgumentParser(description='update solr')
parser.add_argument('--server', default='openlibrary.org')
parser.add_argument('--config', default='openlibrary.yml')
parser.add_argument('--author_limit', default=1000)
parser.add_argument('--work_limit', default=1000)
parser.add_argument('--skip_user', action='append', default=[])
parser.add_argument('--only_user', action='append', default=[])
parser.add_argument('--state_file', default='solr_update')
parser.add_argument('--handle_author_merge', action='store_true')
parser.add_argument('--mins', default=5)
parser.add_argument('--only_author_merge', action='store_true')
parser.add_argument('--skip_author_merge', action='store_true')
parser.add_argument('--no_commit', action='store_true')
parser.add_argument('--no_author_updates', action='store_true')
parser.add_argument('--just_consider_authors', action='store_true')
parser.add_argument('--limit', default=None)

args = parser.parse_args()
handle_author_merge = args.handle_author_merge
only_author_merge = args.only_author_merge
skip_author_merge = args.skip_author_merge

if only_author_merge:
    handle_author_merge = True

if handle_author_merge:
    from openlibrary.catalog.works.find_works import find_title_redirects, find_works, get_books, books_query, update_works

ol = OpenLibrary("http://" + args.server)
set_query_host(args.server)
done_login = False

config_file = args.config
config.load(config_file)

solr_works = config.runtime_config["plugin_worksearch"]["solr"]
solr_subjects = config.runtime_config["plugin_worksearch"]["subject_solr"]

def fix_hardcoded_config():
    from openlibrary.catalog.utils import query 
    query = query_host = args.server
    
    from openlibrary.solr import update
    update.solr_works = solr_works
    update.solr_subjects = solr_subjects

fix_hardcoded_config()

base = 'http://%s/openlibrary.org/log/' % config.runtime_config['infobase_server']

skip_user = set(u.lower() for u in args.skip_user)
only_user = set(u.lower() for u in args.only_user)

if 'state_dir' not in config.runtime_config:
    print 'state_dir missing from ' + config_file
    sys.exit(0)

state_file = config.runtime_config['state_dir'] + '/' + args.state_file

if exists(state_file):
    offset = open(state_file).readline()[:-1]
else:
    offset = "2010-06-01:0"

print 'start:', offset
authors_to_update = set()
works_to_update = set()
last_update = time()
author_limit = int(args.author_limit)
work_limit = int(args.work_limit)
time_limit = 60 * int(args.mins)

def run_update():
    global authors_to_update, works_to_update
    subjects_to_update = set()
    global last_update
    print 'running update: %s works %s authors' % (len(works_to_update), len(authors_to_update))
    if works_to_update:
        requests = []
        num = 0
        total = len(works_to_update)
        for wkey in works_to_update:
            num += 1
            print 'update work: %s %d/%d' % (wkey, num, total)
            if '/' in wkey[7:]:
                print 'bad wkey:', wkey
                continue
            work_to_update = withKey(wkey)
            for attempt in range(5):
                try:
                    requests += update_work(work_to_update)
                except AuthorRedirect:
                    print 'fixing author redirect'
                    w = ol.get(wkey)
                    need_update = False
                    for a in w['authors']:
                        r = ol.get(a['author'])
                        if r['type'] == '/type/redirect':
                            a['author'] = {'key': r['location']}
                            need_update = True
                    if need_update:
                        if not done_login:
                            rc = read_rc()
                            ol.login('EdwardBot', rc['EdwardBot']) 
                        ol.save(w['key'], w, 'avoid author redirect')
            if work_to_update['type']['key'] == '/type/work' and work_to_update.get('title'):
                subjects = get_work_subjects(work_to_update)
                print subjects
                for subject_type, values in subjects.iteritems():
                    subjects_to_update.update((subject_type, v) for v in values)
                if len(requests) >= 100:
                    solr_update(requests, debug=True)
                    requests = []
    #            if num % 1000 == 0:
    #                solr_update(['<commit/>'], debug=True)
        if requests:
            solr_update(requests, debug=True)
        if not args.no_commit:
            solr_update(['<commit/>'], debug=True)
    last_update = time()
    if not args.no_author_updates and authors_to_update:
        requests = []
        for akey in authors_to_update:
            print 'update author:', `akey`
            try:
                request = update_author(akey)
                if request:
                    requests += request
            except AttributeError:
                print 'akey:', `akey`
                raise
        if not args.no_commit:
            solr_update(requests + ['<commit/>'], index='authors', debug=True)
    subject_add = Element("add")
    print subjects_to_update
    for subject_type, subject_name in subjects_to_update:
        key = subject_type + '/' + subject_name
        count = subject_count(subject_type, subject_name)

        if not subject_need_update(key, count):
            print 'no updated needed:', (subject_type, subject_name, count)
            continue
        print 'updated needed:', (subject_type, subject_name, count)

        doc = Element("doc")
        add_field(doc, 'key', key)
        add_field(doc, 'name', subject_name)
        add_field(doc, 'type', subject_type)
        add_field(doc, 'count', count)
        subject_add.append(doc)

    if len(subject_add):
        print 'updating subjects'
        add_xml = tostring(subject_add).encode('utf-8')
        solr_update([add_xml, '<commit />'], debug=True, index='subjects')

    authors_to_update = set()
    works_to_update = set()
    subjects_to_update = set()
    print >> open(state_file, 'w'), offset

def process_save(key, query):
    if query:
        obj_type = query['type']['key'] if isinstance(query['type'], dict) else query['type']
        if obj_type == '/type/delete':
            print key, 'deleted'
    if key.startswith('/authors/') or key.startswith('/a/'):
        authors_to_update.add(key)
        q = {
            'type':'/type/work',
            'authors':{'author':{'key': key}},
            'limit':0,
        }
        works = None
        for attempt in range(5):
            try:
                works = ol.query(q)
                break
            except:
                sleep(10)
        if not works:
            works = ol.query(q)
        works_to_update.update(works)
        return
    elif args.just_consider_authors:
        return
    if key.startswith('/works/'):
        works_to_update.add(key)

        authors = [a['author']['key'] if isinstance(a['author'], dict) else a['author'] for a in (query.get('authors') or []) if a.get('author')]
        if query:
            authors_to_update.update(a for a in authors if a)
        return
    if (key.startswith('/books/') or key.startswith('/b/')) and query and obj_type != '/type/delete':
        if obj_type != '/type/edition':
            print 'bad type for ', key
            return
        works_to_update.update(w['key'] if isinstance(w, dict) else w for w in (query.get('works') or []))
        try:
            authors_to_update.update(a['key'] if isinstance(a, dict) else a for a in (query.get('authors') or []))
        except:
            print query
            raise

while True:
    url = base + offset
    if args.limit:
        url += '?limit=' + args.limit
    print url
    try:
        data = urlopen(url).read()
    except URLError as inst:
        if inst.args and inst.args[0].args == (111, 'Connection refused'):
            print 'make sure infogami server is working, connection refused from:'
            print url
            sys.exit(0)
        print 'url:', url
        raise
    try:
        ret = simplejson.loads(data)
    except:
        open('bad_data.json', 'w').write(data)
        raise

    offset = ret['offset']
    data = ret['data']
    print offset, len(data), '%s works %s authors' % (len(works_to_update), len(authors_to_update))
    if len(data) == 0:
        if authors_to_update or works_to_update:
            run_update()
        sleep(5)
        continue
    for i in data:
        action = i.pop('action')
        if action == 'new_account':
            continue
        author = i['data'].get('author') if 'data' in i else None
        lc_author = None
        if author:
            author = author.split('/')[-1]
            lc_author = author.lower()
            if lc_author in skip_user or (only_user and lc_author not in only_user):
                continue
        if author == 'AccountBot':
            if action not in ('save', 'save_many'):
                print action, author, key, i.keys()
                print i['data']
            assert action in ('save', 'save_many')
            continue
        if i.get('data') and i['data'].get('comment') and 'ia_box_id' in i['data']['comment']:
            continue
        if action == 'save':
            if only_author_merge:
                continue
            key = i['data'].pop('key')
            process_save(key, i['data']['query'])
        elif action == 'save_many':
            author_merge = i['data']['comment'] == 'merge authors'
            if author_merge and skip_author_merge:
                continue
            if author_merge and only_author_merge:
                continue
            if handle_author_merge and not i['data']['author'].endswith('Bot') and author_merge:
                first_redirect = i['data']['query'][0]
                assert first_redirect['type']['key'] == '/type/redirect'
                akey = first_redirect['location']
                if akey.startswith('/authors/'):
                    akey = '/a/' + akey[len('/authors/'):]
                title_redirects = find_title_redirects(akey)
                works = find_works(akey, get_books(akey, books_query(akey)), existing=title_redirects)
                updated = update_works(akey, works, do_updates=True)
                works_to_update.update(w['key'] for w in updated)
            for query in i['data']['query']:
                key = query.pop('key')
                process_save(key, query)
        # store.put gets called when any document is updated in the store. Borrowing/Returning a book triggers one.
        elif action == 'store.put':
            # A sample record looks like this:
            # {
            #   "action": "store.put", 
            #   "timestamp": "2011-12-01T00:00:44.241604", 
            #   "data": {
            #       "data": {"borrowed": "false", "_key": "ebooks/books/OL5854888M", "_rev": "975708", "type": "ebook", "book_key": "/books/OL5854888M"},
            #       "key": "ebooks/books/OL5854888M"
            #   }, 
            #   "site": "openlibrary.org"
            # }
            data = i.get('data', {}).get("data")
            if data.get("type") == "ebook" and data.get("_key", "").startswith("ebooks/books/"):
                edition_key = data['book_key']
                process_save(edition_key, withKey(edition_key))
    since_last_update = time() - last_update
    if len(works_to_update) > work_limit or len(authors_to_update) > author_limit or since_last_update > time_limit:
        run_update()

########NEW FILE########
__FILENAME__ = store_counts
#!/olsystem/bin/olenv python

import sys

import _init_path

from openlibrary.admin import stats

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 5:
        print >>sys.stderr, "Usage : %s infobase_config openlibrary_config coverstore_config number_of_days"
        sys.exit(-1)
    sys.exit(stats.main(*sys.argv[1:]))

########NEW FILE########
__FILENAME__ = ipstats
#!/olsystem/bin/olenv python
"""
Temporary script to store unique IPs in a single day by parsing the
lighttpd log files directly.
"""
import os
import datetime
import subprocess

import couchdb
import yaml

def connect_to_couch(config_file):
    "Connects to the couch databases"
    f = open(config_file)
    config = yaml.load(f)
    f.close()
    admin_db = config["admin"]["counts_db"]
    return couchdb.Database(admin_db)

def store_data(db, data, date):
    uid = date.strftime("counts-%Y-%m-%d")
    print uid
    try:
        vals = db[uid]
        vals.update(data)
    except couchdb.http.ResourceNotFound:
        vals = data
        db[uid] = vals
    print "saving %s"%vals
    db.save(vals)

def run_for_day(d):
    basedir = d.strftime("/var/log/nginx/")
    awk = ["awk", '$2 == "openlibrary.org" { print $1 }']
    sort = ["sort", "-u"]
    count = ["wc", "-l"]
    print "           ", basedir
    zipfile = d.strftime("access.log-%Y%m%d.gz")
    if os.path.exists(basedir + zipfile):
        print "              Using ",  basedir + zipfile
        cmd = subprocess.Popen(["zcat", basedir + zipfile], stdout = subprocess.PIPE)
    elif os.path.exists(basedir + "access.log"):
        cmd = subprocess.Popen(["cat", "%s/access.log"%basedir], stdout = subprocess.PIPE)
        print "              Using ",  basedir + "access.log"
    print "           ", awk
    cmd = subprocess.Popen(awk,   stdin = cmd.stdout, stdout = subprocess.PIPE)
    print "           ", sort
    cmd = subprocess.Popen(sort,  stdin = cmd.stdout, stdout = subprocess.PIPE)
    print "           ", count
    cmd = subprocess.Popen(count, stdin = cmd.stdout, stdout = subprocess.PIPE)
    val = cmd.stdout.read()
    return dict (visitors = int(val))
    
    
def main(config):
    admin_db = connect_to_couch(config)
    current = datetime.datetime.now()
    for i in range(2):
        print current
        d = run_for_day(current)
        store_data(admin_db, d, current)
        current = current - datetime.timedelta(days = 1)

if __name__ == "__main__":
    import sys
    sys.exit(main(sys.argv[1]))

########NEW FILE########
__FILENAME__ = update-loans
#! /usr/bin/env python
"""Script to update loans and waiting loans on regular intervals.

Tasks done:
* delete all waiting-loans that are expired
"""
import sys
import web
from openlibrary.core import waitinglist
from openlibrary.plugins.upstream import borrow

web.config.debug = False

def usage():
    print "python scripts/openlibrary-server openlibrary.yml runscript scripts/update-loans.py [update-loans | update-waitinglists]"

def main():
    try:
        cmd = sys.argv[1]
    except IndexError:
        cmd = "help"

    if cmd == "update-loans":
        borrow.update_all_loan_status()
    elif cmd == "update-waitinglists":
        waitinglist.prune_expired_waitingloans()
    else:
        help()

if __name__ == "__main__":
    main()
    

########NEW FILE########
__FILENAME__ = view_marc
#!/usr/bin/python2.5
from openlibrary.catalog.marc.fast_parse import *
from openlibrary.catalog.get_ia import get_from_archive
import sys, codecs, re

sys.stdout = codecs.getwriter('utf-8')(sys.stdout)

re_subtag = re.compile('\x1f(.)([^\x1f]*)')

def fmt_subfields(line):
    def bold(s):
        return ''.join(c + "\b" + c for c in s)
    assert line[-1] == '\x1e'
    return ''.join(' ' + bold('$' + m.group(1)) + ' ' + translate(m.group(2)) for m in re_subtag.finditer(line[2:-1]))

def show_book(data):
    print 'leader:', data[:24]
    for tag, line in get_all_tag_lines(data):
        if tag.startswith('00'):
            print tag, line[:-1]
        else:
            print tag, line[0:2], fmt_subfields(line)

if __name__ == '__main__':
    source = sys.argv[1]
    if ':' in source:
        data = get_from_archive(source)
    else:
        data = open(source).read()
    show_book(data)

########NEW FILE########
__FILENAME__ = work_finder
#!/usr/bin/python

from openlibrary.catalog.works.find_works import find_title_redirects, find_works, get_books, books_query, update_works
import sys
from pprint import pprint

akey = sys.argv[1]
title_redirects = find_title_redirects(akey)
print 'title_redirects:'
pprint(title_redirects)
print

works = find_works(akey, get_books(akey, books_query(akey)), existing=title_redirects)
works = list(works)
print 'works:'
pprint(works)
print

updated = update_works(akey, works, do_updates=True)
print 'updated works:'
pprint(updated)

########NEW FILE########
__FILENAME__ = z3950_view
import web
from PyZ3950 import zoom
from lxml import etree
import sys, re
from urllib import urlopen
from openlibrary.catalog.marc.html import html_record
from openlibrary.catalog.marc import xml_to_html

tree = etree.parse('/petabox/www/petabox/includes/ztargets.xml')
root = tree.getroot()

targets = {}
for t in root:
    cur = {}
    if not isinstance(t.tag, str):
        continue
    for element in t:
        cur[element.tag] = element.text
    targets[cur['title']] = cur

re_identifier = re.compile('^([^:/]+)(?::(\d+))?(?:/(.+))?$')

def get_marc(target_name, cclquery, result_offset):
    target = targets[target_name]
    m = re_identifier.match(target['identifier'])
    (host, port, db) = m.groups()
    port = int(port) if port else 210
    conn = zoom.Connection (host, port)
    if db:
        conn.databaseName = db
    conn.preferredRecordSyntax = 'USMARC'
    query = zoom.Query ('PQF', cclquery)
    res = conn.search (query)
    offset = 0
    for r in res:
        return r.data
        offset += 1
        if offset == result_offset:
            return r.data

urls = (
    '/z39.50/(.+)', 'z3950_lookup',
    '/z39.50', 'search_page',
    '/', 'index',
)
app = web.application(urls, globals())

class index:
    def GET(self):
        return '''
<html>
<head>
<title>Open Library labs</title>
</head>
<body>
<ul>
<li><a href="z39.50">Z39.50 lookup</a>
</ul>
</body>
<html>'''

class search_page:
    def GET(self):
        i = web.input()
        if 'ia' in i:
            raise web.seeother('/z39.50/' + i.ia.strip())
        return '''
<html>
<head>
<title>Z39.50 search</title>
</head>
<body>
<form>Internet archive identifier: <input name="ia"><input value="go" type="submit"></form>
</body>
<html>'''



class z3950_lookup:
    def GET(self, ia):

        ret = '''
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Marc lookup: %s</title>
</head>
<body>
<form action="/z39.50">Internet archive identifier: <input name="ia" value="%s"><input value="go" type="submit"></form>
<h1>%s</h1>
<ul>
<li><a href="http://www.archive.org/details/%s">Internet archive detail page</a>
<li><a href="http://openlibrary.org/show-records/ia:%s">View current IA MARC record</a>
</ul>
''' % (ia, ia, ia, ia, ia)
        marc_source = 'http://www.archive.org/download/' + ia + '/' + ia + '_metasource.xml'
        marc_xml = 'http://www.archive.org/download/' + ia + '/' + ia + '_marc.xml'
        marc_bin = 'http://www.archive.org/download/' + ia + '/' + ia + '_meta.mrc'

        try:
            from_marc_xml = xml_to_html.html_record(urlopen(marc_xml).read())
        except:
            from_marc_xml = None

        try:
            meta_mrc = urlopen(marc_bin).read()
            from_marc_bin = html_record(meta_mrc)
        except:
            from_marc_bin = None

        root = etree.parse(urlopen(marc_source)).getroot()
        cclquery = root.find('cclquery').text
        target_name = root.find('target').text
        result_offset = root.find('resultOffset').text

        marc = get_marc(target_name, cclquery, result_offset)
        rec = html_record(marc)

        ret += '<h2>From Z39.50</h2>'

        ret += 'leader: <code>' + rec.leader.replace(' ', '&nbsp;') + '</code><br>'
        ret += rec.html() + '<br>\n'

        if from_marc_xml:
            ret += '<h2>From MARC XML on archive.org</h2>'
            
            ret += 'leader: <code>' + from_marc_xml.leader.replace(' ', '&nbsp;') + '</code><br>'
            ret += from_marc_xml.html() + '<br>\n'

        if from_marc_xml:
            ret += '<h2>From MARC binary on archive.org</h2>'
            
            ret += 'record length: ' + `len(meta_mrc)` + ' bytes<br>'
            ret += 'leader: <code>' + from_marc_bin.leader.replace(' ', '&nbsp;') + '</code><br>'
            ret += from_marc_bin.html() + '<br>\n'

        ret += '</body></html>'

        return ret

if __name__ == "__main__":
    app.run()


########NEW FILE########
__FILENAME__ = _init_path
"""Helper to add openlibrary module to sys.path.
"""

import os
from os.path import abspath, realpath, join, dirname, pardir
import sys

path = __file__.replace('.pyc', '.py') 
scripts_root = dirname(realpath(path))

OL_PATH = abspath(join(scripts_root, pardir))
sys.path.insert(0, OL_PATH)

# Add the PWD as the first entry in the path.
# The path we get from __file__ and abspath will have all the links expanded.
# This creates trouble in symlink based deployments. Work-around is to add the
# current directory to path and let the app run from that directory.
sys.path.insert(0, os.getcwd())

########NEW FILE########
