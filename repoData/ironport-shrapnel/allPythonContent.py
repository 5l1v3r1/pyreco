__FILENAME__ = t0
# -*- Mode: Python -*-

from coro.asn1.ber import *
import unittest

# These are mostly positive test cases, need some negative ones as well.
# Though - this code *has* been through the protos c06-ldapv3-enc-r1 test suite,
#   but it's a rather large suite (89MB).  Consider automating a download of
#   the suite here?

class ber_test_case (unittest.TestCase):
    pass

class simple_test (ber_test_case):
    def runTest (self):
        x = SEQUENCE (
            SET (INTEGER(34), INTEGER(19), OCTET_STRING('fishing line')),
            OBJID ([2, 3, 4, 5, 6, 88]),
            OCTET_STRING ("spaghetti"),
            BOOLEAN(True),
            BOOLEAN(False),
        )
        self.assertEqual (
            x, '0.1\x14\x02\x01"\x02\x01\x13\x04\x0cfishing line\x06\x05S\x04\x05\x06X\x04\tspaghetti\x01\x01\xff\x01\x01\x00')  # noqa
        self.assertEqual (
            decode (x), ([[34, 19, 'fishing line'], ('oid', [2, 3, 4, 5, 6, 88]), 'spaghetti', True, False], 48))

# www.google.com cert
google_cert = """-----BEGIN CERTIFICATE-----
MIIDITCCAoqgAwIBAgIQT52W2WawmStUwpV8tBV9TTANBgkqhkiG9w0BAQUFADBM
MQswCQYDVQQGEwJaQTElMCMGA1UEChMcVGhhd3RlIENvbnN1bHRpbmcgKFB0eSkg
THRkLjEWMBQGA1UEAxMNVGhhd3RlIFNHQyBDQTAeFw0xMTEwMjYwMDAwMDBaFw0x
MzA5MzAyMzU5NTlaMGgxCzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlh
MRYwFAYDVQQHFA1Nb3VudGFpbiBWaWV3MRMwEQYDVQQKFApHb29nbGUgSW5jMRcw
FQYDVQQDFA53d3cuZ29vZ2xlLmNvbTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkC
gYEA3rcmQ6aZhc04pxUJuc8PycNVjIjujI0oJyRLKl6g2Bb6YRhLz21ggNM1QDJy
wI8S2OVOj7my9tkVXlqGMaO6hqpryNlxjMzNJxMenUJdOPanrO/6YvMYgdQkRn8B
d3zGKokUmbuYOR2oGfs5AER9G5RqeC1prcB6LPrQ2iASmNMCAwEAAaOB5zCB5DAM
BgNVHRMBAf8EAjAAMDYGA1UdHwQvMC0wK6ApoCeGJWh0dHA6Ly9jcmwudGhhd3Rl
LmNvbS9UaGF3dGVTR0NDQS5jcmwwKAYDVR0lBCEwHwYIKwYBBQUHAwEGCCsGAQUF
BwMCBglghkgBhvhCBAEwcgYIKwYBBQUHAQEEZjBkMCIGCCsGAQUFBzABhhZodHRw
Oi8vb2NzcC50aGF3dGUuY29tMD4GCCsGAQUFBzAChjJodHRwOi8vd3d3LnRoYXd0
ZS5jb20vcmVwb3NpdG9yeS9UaGF3dGVfU0dDX0NBLmNydDANBgkqhkiG9w0BAQUF
AAOBgQAhrNWuyjSJWsKrUtKyNGadeqvu5nzVfsJcKLt0AMkQH0IT/GmKHiSgAgDp
ulvKGQSy068Bsn5fFNum21K5mvMSf3yinDtvmX3qUA12IxL/92ZzKbeVCq3Yi7Le
IOkKcGQRCMha8X2e7GmlpdWC1ycenlbN0nbVeSv3JUMcafC4+Q==
-----END CERTIFICATE-----"""

class x509_test (ber_test_case):

    def runTest (self):
        import base64
        lines = google_cert.split ('\n')
        enc = base64.decodestring (''.join (lines[1:-1]))
        self.assertEqual (
            decode (enc),
            ([[('context', 0, [2]),
               105827261859531100510423749949966875981L,
               [('oid', [1, 2, 840, 113549, 1, 1, 5]), None],
               [[[('oid', [2, 5, 4, 6]), ('PRINTABLE_STRING', 19, 'ZA')]],
                [[('oid', [2, 5, 4, 10]),
                  ('PRINTABLE_STRING', 19, 'Thawte Consulting (Pty) Ltd.')]],
                [[('oid', [2, 5, 4, 3]), ('PRINTABLE_STRING', 19, 'Thawte SGC CA')]]],
               [('UTC_TIME', 23, '111026000000Z'), ('UTC_TIME', 23, '130930235959Z')],
               [[[('oid', [2, 5, 4, 6]), ('PRINTABLE_STRING', 19, 'US')]],
                   [[('oid', [2, 5, 4, 8]), ('PRINTABLE_STRING', 19, 'California')]],
                   [[('oid', [2, 5, 4, 7]), ('T61_STRING', 20, 'Mountain View')]],
                   [[('oid', [2, 5, 4, 10]), ('T61_STRING', 20, 'Google Inc')]],
                   [[('oid', [2, 5, 4, 3]), ('T61_STRING', 20, 'www.google.com')]]],
               [[('oid', [1, 2, 840, 113549, 1, 1, 1]), None],
                ('bitstring',
                 (0,
                  "0\x81\x89\x02\x81\x81\x00\xde\xb7&C\xa6\x99\x85\xcd8\xa7\x15\t\xb9\xcf\x0f"
                  "\xc9\xc3U\x8c\x88\xee\x8c\x8d('$K*^\xa0\xd8\x16\xfaa\x18K\xcfm`\x80\xd35@2r"
                  "\xc0\x8f\x12\xd8\xe5N\x8f\xb9\xb2\xf6\xd9\x15^Z\x861\xa3\xba\x86\xaak\xc8\xd9"
                  "q\x8c\xcc\xcd'\x13\x1e\x9dB]8\xf6\xa7\xac\xef\xfab\xf3\x18\x81\xd4$F\x7f\x01w|"
                  "\xc6*\x89\x14\x99\xbb\x989\x1d\xa8\x19\xfb9\x00D}\x1b\x94jx-i\xad\xc0z,\xfa\xd0"
                  "\xda \x12\x98\xd3\x02\x03\x01\x00\x01"))],
               ('context',
                3,
                [[[('oid', [2, 5, 29, 19]), True, '0\x00'],
                  [('oid', [2, 5, 29, 31]),
                   "0-0+\xa0)\xa0'\x86%http://crl.thawte.com/ThawteSGCCA.crl"],
                  [('oid', [2, 5, 29, 37]),
                   '0\x1f\x06\x08+\x06\x01\x05\x05\x07\x03\x01\x06\x08+\x06\x01\x05\x05\x07\x03'
                   '\x02\x06\t`\x86H\x01\x86\xf8B\x04\x01'],
                  [('oid', [1, 3, 6, 1, 5, 5, 7, 1, 1]),
                   '0d0"\x06\x08+\x06\x01\x05\x05\x070\x01\x86\x16http://ocsp.thawte.com0>\x06'
                   '\x08+\x06\x01\x05\x05\x070\x02\x862http://www.thawte.com/repository/Thawte_SGC_CA.crt']]])],
              [('oid', [1, 2, 840, 113549, 1, 1, 5]), None],
              ('bitstring',
               (0,
                "!\xac\xd5\xae\xca4\x89Z\xc2\xabR\xd2\xb24f\x9dz\xab\xee\xe6|\xd5~\xc2\\("
                "\xbbt\x00\xc9\x10\x1fB\x13\xfci\x8a\x1e$\xa0\x02\x00\xe9\xba[\xca\x19\x04"
                "\xb2\xd3\xaf\x01\xb2~_\x14\xdb\xa6\xdbR\xb9\x9a\xf3\x12\x7f|\xa2\x9c;o\x99"
                "}\xeaP\rv#\x12\xff\xf7fs)\xb7\x95\n\xad\xd8\x8b\xb2\xde \xe9\npd\x11\x08"
                "\xc8Z\xf1}\x9e\xeci\xa5\xa5\xd5\x82\xd7'\x1e\x9eV\xcd\xd2v\xd5y+\xf7%C\x1c"
                "i\xf0\xb8\xf9"))],
                805)
        )
        dec, length = decode (enc)
        public_key = dec[0][6][1][1][1]
        self.assertEqual (
            decode (public_key),
            ([156396091895984667473837837332877995558144703880815901117439532534031286131520903863087599986938779606924811933611903716377206837300122262900786662124968110191717844999183338594373129421417536020806373385428322642107305024162536996222164292639147591878860587271770855626780464602884552232097424473091745159379L, 65537], 140)  # noqa
        )

class bignum_test (ber_test_case):

    def runTest (self):
        self.assertEquals (
            decode ('\x02\x82\x04\xe3\x01' + '\x00' * 1250),
            (1 << 10000, 1255)
        )
        self.assertEquals (
            INTEGER (1 << 10000),
            '\x02\x82\x04\xe3\x01' + '\x00' * 1250,
        )

class bignum_test_2 (ber_test_case):

    def runTest (self):
        for i in range (5):
            n = 1 << (10 ** i)
            self.assertEquals (
                decode (INTEGER (n))[0],
                n
            )

class bignum_test_3 (ber_test_case):

    def runTest (self):
        import random
        n = 1
        for x in range (10000):
            n = n * 10 + random.randint (0, 10)
        print n
        self.assertEquals (decode (INTEGER (n))[0], n)

def suite():
    suite = unittest.TestSuite()
    suite.addTest (simple_test())
    suite.addTest (x509_test())
    suite.addTest (bignum_test())
    suite.addTest (bignum_test_2())
    suite.addTest (bignum_test_3())
    return suite

if __name__ == '__main__':
    unittest.main (defaultTest='suite')

########NEW FILE########
__FILENAME__ = t_real
# -*- Mode: Python -*-

# test asn.1 real encoding

# Note: you'll need version pyasn1-0.1.4+
from pyasn1.codec.ber import decoder
from coro.asn1.ber import *

def go (f):
    d = encode_double (f)
    r = decoder.decode (d)[0]
    f0 = float (r)
    return f, f0, r, d

print go (1.0e9)
print go (3.14159265358979323846)
print go (1e1000)
print go (-1e1000)
print go (1e300)
print go (-1e300)
print go (1e-5)
print go (-1e-5)

########NEW FILE########
__FILENAME__ = backdoor
# -*- Mode: Python -*-

# Copyright 1999, 2000 by eGroups, Inc.
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of
# eGroups not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# EGROUPS DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL EGROUPS BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

"""Backdoor Python access.

This module implements a server that allows one to telnet to a socket and get a
Python prompt in a process.

Simply spawn a thread running the `serve` function to start a backdoor server.
"""

VERSION_STRING = '$Id: //prod/main/ap/shrapnel/coro/backdoor.py#6 $'

import coro
import cStringIO
import fcntl
import sys
import traceback
import os

# Originally, this object implemented the file-output api, and set
# sys.stdout and sys.stderr to 'self'.  However, if any other
# coroutine ran, it would see the captured definition of sys.stdout,
# and would send its output here, instead of the expected place.  Now
# the code captures all output using StringIO.  A little less
# flexible, a little less efficient, but much less surprising!
# [Note: this is exactly the same problem addressed by Scheme's
#  dynamic-wind facility]

class backdoor:

    def __init__ (self, sock, line_separator='\r\n', welcome_message=None, global_dict=None):
        self.sock = sock
        self.buffer = ''
        self.lines = []
        self.multilines = []
        self.line_separator = line_separator
        self.welcome_message = welcome_message
        self.global_dict = global_dict

        # allow the user to change the prompts:
        if 'ps1' not in sys.__dict__:
            sys.ps1 = '>>> '
        if 'ps2' not in sys.__dict__:
            sys.ps2 = '... '

    def send (self, data):
        try:
            self.sock.send (data)
        except:
            pass

    def prompt (self):
        if self.multilines:
            self.send (sys.ps2)
        else:
            self.send (sys.ps1)

    def read_line (self):
        if self.lines:
            l = self.lines[0]
            self.lines = self.lines[1:]
            return l
        else:
            while not self.lines:
                block = self.sock.recv (8100)
                if not block:
                    return None
                elif block == '\004':
                    self.sock.close()
                    return None
                else:
                    self.buffer += block
                    lines = self.buffer.split (self.line_separator)
                    for l in lines[:-1]:
                        self.lines.append (l)
                    self.buffer = lines[-1]
            return self.read_line()

    def send_welcome_message(self):
        self.send ('Python ' + sys.version.replace ('\n', '\r\n') + self.line_separator)
        self.send (sys.copyright.replace ('\n', '\r\n') + self.line_separator)
        if self.welcome_message is not None:
            # make '\n' into the right line separator and terminate with
            # a line separator
            lines = self.welcome_message.split ('\n')
            if lines[-1] != '':
                lines.append('')
            self.send (self.line_separator.join (lines))

    def login (self):
        "override to provide authentication"
        pass

    def read_eval_print_loop (self):
        self.login()
        self.send_welcome_message()
        if self.global_dict is None:
            # this does the equivalent of 'from __main__ import *'
            env = sys.modules['__main__'].__dict__.copy()
        else:
            env = self.global_dict.copy()

        while 1:
            self.prompt()
            try:
                line = self.read_line()
            except EOFError:
                break
            if line is None:
                break
            elif self.multilines:
                self.multilines.append(line)
                if line == '':
                    code = '\n'.join (self.multilines)
                    self.parse(code, env)
                    # we do this after the parsing so parse() knows not to do
                    # a second round of multiline input if it really is an
                    # unexpected EOF
                    self.multilines = []
            else:
                self.parse(line, env)

    def print_result (self, result):
        "override to process the result (e.g., pprint)"
        print result

    def parse (self, line, env):
        save = sys.stdout, sys.stderr
        output = cStringIO.StringIO()
        try:
            try:
                sys.stdout = sys.stderr = output
                co = compile (line, repr(self), 'eval')
                result = eval (co, env)
                if result is not None:
                    self.print_result (result)
                    env['_'] = result
            except SyntaxError:
                try:
                    co = compile (line, repr(self), 'exec')
                    exec co in env
                except SyntaxError, msg:
                    # this is a hack, but it is a righteous hack:
                    if not self.multilines and msg[0] == 'unexpected EOF while parsing':
                        self.multilines.append(line)
                    else:
                        traceback.print_exc()
                except:
                    traceback.print_exc()
            except:
                traceback.print_exc()
        finally:
            sys.stdout, sys.stderr = save
            self.send (output.getvalue())
            del output

def client (conn, addr, welcome_message=None, global_dict=None):
    b = backdoor (conn, welcome_message=welcome_message, global_dict=global_dict)
    b.read_eval_print_loop()

def serve (port=None, ip='', unix_path=None, welcome_message=None, global_dict=None, client_class=None):
    """Backdoor server function.

    This function will listen on the backdoor socket and spawn new threads for
    each connection.

    :Parameters:
        - `port`: The IPv4 port to listen on (defaults to automatically choose
          an unused port between 8023->8033).  May also be a list of ports.
        - `ip`: The IP to listen on.  Defaults to all IP's.
        - `unix_path`: The unix path to listen on.  If this is specified, then
          it will use unix-domain sockets, otherwise it will use IPv4 sockets.
        - `welcome_message`: A welcome message to display when a user logs in.
        - `global_dict`: The global dictionary to use for client sessions.
    """
    import errno
    if unix_path:
        try:
            os.remove (unix_path)
        except OSError, why:
            if why[0] == errno.ENOENT:
                pass
            else:
                raise
        s = coro.make_socket (coro.PF.LOCAL, coro.SOCK.STREAM)
        s.bind (unix_path)
        coro.print_stderr('Backdoor started on unix socket %s\n' % unix_path)
    else:
        s = coro.make_socket (coro.PF.INET, coro.SOCK.STREAM)
        s.set_reuse_addr()
        if port is None:
            ports = xrange(8023, 8033)
        else:
            if type(port) is int:
                ports = [port]
            else:
                ports = port
        for i in ports:
            try:
                s.bind ((ip, i))
                coro.print_stderr('Backdoor started on port %d\n' % i)
                break
            except OSError, why:
                if why[0] != errno.EADDRINUSE:
                    raise OSError(why)
        else:
            raise Exception("couldn't bind a port (try not specifying a port)")

    if client_class is None:
        client_class = client

    flags = fcntl.fcntl(s.fileno(), fcntl.F_GETFD)
    fcntl.fcntl(s.fileno(), fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC)

    s.listen (1024)
    while 1:
        conn, addr = s.accept()
        coro.print_stderr ('incoming backdoor connection from %r\n' % (conn.getpeername(),))
        thread = coro.spawn (client_class, conn, addr, welcome_message, global_dict)
        thread.set_name('backdoor session')

import coro.ssh.transport.server
import coro.ssh.connection.connect
import coro.ssh.l4_transport.coro_socket_transport
import coro.ssh.auth.userauth
import coro.ssh.connection.interactive_session

class ssh_repl (coro.ssh.connection.interactive_session.Interactive_Session_Server):

    def __init__ (self, connection_service):
        coro.ssh.connection.interactive_session.Interactive_Session_Server.__init__ (self, connection_service)
        coro.spawn (self.go)

    def go (self):
        b = backdoor (self, line_separator='\n')
        # this is to avoid getting the banner/copyright/etc mixed in with ssh client pty/x11 warnings
        coro.sleep_relative (0.1)
        b.read_eval_print_loop()
        self.close()
        coro.print_stderr ('closed ssh backdoor from: %r\n' % (self.transport.transport.peer,))

# see coro/ssh/demo/backdoor.py for instructions on setting up an ssh backdoor server.
class ssh_server:
    def __init__ (self, port, addr, server_key, authenticators):
        self.port = port
        self.addr = addr
        self.server_key = server_key
        self.authenticators = authenticators
        coro.spawn (self.serve)

    def serve (self):
        serve (self.port, self.addr, client_class=self.new_connection)

    def new_connection (self, conn, addr, welcome_message, global_dict):
        # debug = coro.ssh.util.debug.Debug()
        # debug.level = coro.ssh.util.debug.DEBUG_3
        transport = coro.ssh.l4_transport.coro_socket_transport.coro_socket_transport(sock=conn)
        server = coro.ssh.transport.server.SSH_Server_Transport (self.server_key)  # , debug=debug)
        authenticator = coro.ssh.auth.userauth.Authenticator (server, self.authenticators)
        server.connect (transport, authenticator)
        service = coro.ssh.connection.connect.Connection_Service (server, ssh_repl)

if __name__ == '__main__':
    thread = coro.spawn (serve, welcome_message='Testing backdoor.py')
    thread.set_name('backdoor server')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = postgres
# -*- Mode: Python -*-

# SMR: I believe this was written by Larry Rosenstein circa 2003?

# Note: this code could use some modernization.
# * latency will be high because of the query/response model

import exceptions
import re
import socket
import string
import struct
import sys
import time
import types

import coro

W = coro.write_stderr
P = coro.print_stderr

MAX_RECONNECT_RETRY   = 10.0
RECONNECT_RETRY_GRAIN = 0.1
DEFAULT_RECV_SIZE     = 0x8000
POSTGRES_HEADER_SIZE     = 0x5  # 1-byte message type; 4-byte length

# large object mode constants

INV_WRITE = 0x00020000
INV_READ = 0x00040000

SEEK_SET = 0
SEEK_CUR = 1
SEEK_END = 2

# ===========================================================================
# Protocol logging; saves data in memory to avoid changing thread
# scheduling behavior, etc.
# ===========================================================================
DATA = None
DATAP = 0
DATAX = 0

def LOG(data, seq=None):
    global DATA, DATAP, DATAX

    if DATA is None:
        DATA = [''] * 5000
    DATAX += 1
    if not seq:
        seq = DATAX

    # coro.print_stderr("%d: DATA[%d] = %s\n" % (DATAX, DATAP, data[:30]))
    DATA[DATAP] = '%d: %s' % (seq, data)
    DATAP += 1
    if DATAP >= len(DATA):
        DATAP = 0

    return seq

def DUMPLOG(n):
    import pprint
    global DATA, DATAP

    f = open(n, 'w')
    reordered = DATA[DATAP:] + DATA[:DATAP]
    pprint.pprint((DATAP, DATAX), f)
    pprint.pprint(reordered, f)
    f.close()

# ===========================================================================
# Exceptions
# ===========================================================================

class PostgresError(exceptions.Exception):
    """Base class of all exceptions raised here."""
    pass

class BackendError(PostgresError):
    """Exception sent by backend; includes error_field dictionary
    that contains details."""

    def __init__(self, msg, error_data=None):
        PostgresError.__init__(self, msg)

        if isinstance(error_data, types.DictType):
            self.error_fields = error_data.copy()
        elif error_data:
            self.error_fields = unpack_error_data(error_data)
        else:
            self.error_fields = {}

    def error_code(self):
        return self.error_fields.get(PG_SQLSTATE_FIELD)

    def __str__(self):
        return "%s (%s %s: %s)" % (
            self.args[0],
            self.error_fields.get(PG_SEVERITY_FIELD, 'UNKNOWN'),
            self.error_fields.get(PG_SQLSTATE_FIELD, '?????'),
            self.error_fields.get(PG_MESSAGE_FIELD, '-----'))

class ConnectError(BackendError):
    pass

class QueryError(BackendError):
    pass

class FunctionError(BackendError):
    pass


class ConnectionClosedError(PostgresError):
    pass

class InternalError(PostgresError):
    """Unexpected condition inside the library."""
    pass


# ===========================================================================
# String Quoting
# ===========================================================================

_std_split_re = re.compile('''([\x00-\x1f\\\\'\x7f-\xff]+)''')
_std_char_quote = {
    '\000': '',
    '\\': '\\\\',
    "'": "\\'",
    '\r': '\\r',
    '\n': '\\n',
    '\b': '\\b',
    '\f': '\\f',
    '\t': '\\t',
}

_array_split_re = re.compile('''([\x00-\x1f\\\\'"\x7f-\xff]+)''')  # add double quote
_array_char_quote = _std_char_quote.copy()
_array_char_quote['\\'] = '\\\\\\\\'
_array_char_quote['"'] = '\\\\"'

_copy_split_re = re.compile('''([\x00-\x1f\\\\\x7f-\xff]+)''')
_copy_char_quote = _std_char_quote.copy()
del _copy_char_quote["'"]

def _std_quote_char(c):
    quoted = _std_char_quote.get(c)
    if quoted is not None:
        return quoted
    elif ord(c) < ord(' '):
        return '\\%03o' % ord(c)
    else:
        return c

def _array_quote_char(c):
    quoted = _array_char_quote.get(c)
    if quoted is not None:
        return quoted
    elif ord(c) < ord(' '):
        return '\\\\\\\\%03o' % ord(c)
    else:
        return c

_copy_quote_char = _std_quote_char

def quote_string(s):
    return _quote_string(s, "'", _std_split_re, _std_quote_char)

def quote_string_array(s):
    return _quote_string(s, '"', _array_split_re, _array_quote_char)

def quote_string_copy(s):
    return _quote_string(s, "", _copy_split_re, _copy_quote_char)

def _quote_string(s, delim, splitter, quoter):
    parts = splitter.split(s)
    for i in xrange(1, len(parts), 2):
        # even elements are OK; odd elements need conversion
        parts[i] = ''.join(map(quoter, parts[i]))

    # reconstitute string with the proper delimiters
    return "%s%s%s" % (delim, ''.join(parts), delim)

_like_split_re = re.compile('''([%_\\\\]+)''')
def _like_quote_char(c):
    return '\\' + c

def escape_like_string(s):
    """Escape characters in an LIKE/ILIKE match string."""
    return _quote_string(s, "", _like_split_re, _like_quote_char)

def _bytea_quote_char(c):
    return r'\\%03o' % ord(c)

def quote_bytea(s):
    return _quote_string(s, "'", _std_split_re, _bytea_quote_char)


# Regex that finds all utf-8 sequences representing characters
# >= U+10000.  (Technically, this finds all 4-byte utf-8 sequences,
# which is more than is actually allowed in utf-8.  Normally, 4-byte
# sequences must start with 0xf0-0xf4.)
#
_4_byte_utf8_re = re.compile(r'[\xf0-\xf7]...')

# Unicode replacement char in utf-8
_replacement_char_utf8 = u'\ufffd'.encode('utf-8')

def scrub_utf8(s):
    """Replace characters >= U+10000 with U+FFFD.

    Related to bug 35006 (and others).  Postgres can't handle Unicode
    characters outside the Basic Multilingual Plane (>= U+10000).
    Replace them # with REPLACEMENT CHARACTER U+FFFD.

    This doesn't do a lot of checks on the original string; it is
    assumed to be valid utf-8.
    """

    return _4_byte_utf8_re.sub(_replacement_char_utf8, s)


# ===========================================================================
# Client class
# ===========================================================================

class postgres_client:
    # connection states
    DISCONNECTED = 'disconnected'
    CONNECTED = 'connected'
    COPYIN = 'copyin'

    DEFAULT_ADDRESS = '/tmp/.s.PGSQL.5432'
    DEFAULT_USER = 'pgsql'

    def __init__ (self, database, username='', password='',
                  address=None):

        self._backend_pid = 0
        self._secret_key = 0  # used for cancellations

        if username:
            self.username = username
        else:
            self.username = self.DEFAULT_USER
        self.password = password
        if address:
            self.address = address
        else:
            self.address = self.DEFAULT_ADDRESS

        self.database  = database
        self.backend_parameters = {}

        self._state = self.DISCONNECTED
        self._socket = None
        self._debug = 0

    def connect(self):
        try:
            if self._state == self.DISCONNECTED:
                try:
                    self._socket = self.connect_socket()
                except (socket.error, OSError, IOError), e:
                    # problem connecting; Postgres probably isn't running
                    # convert this to a common PostgresError exception
                    raise ConnectError((str(e), sys.exc_info()[2]))

                self._startup()
                self._wait_for_backend()

                self._state = self.CONNECTED
        finally:
            # If the state is still DISCONNECTED, then there was problem.
            if self._state == self.DISCONNECTED:
                self._dump_connection()

    def is_connected(self):
        try:
            return (self._socket is not None and
                    self._state != self.DISCONNECTED and
                    self._socket.getpeername() and
                    1)
        except:
            return 0

    def cancel(self):
        """Cancel current operation on this connection.

        This sends a Postgres cancel request packet on a newly-opened
        db connection.  It should not be called directly because
        without external synchronization there's no way to tell what
        operation (if any) is going to be cancelled.

        If the cancel request does something, the thread that
        made the query will get a PostgresError exception with
        code PG_ERROR_QUERY_CANCELLED."""

        if self._state != self.DISCONNECTED:
            socket = self.connect_socket()
            try:
                data = build_message(PG_CANCELREQUEST_MSG,
                                     80877102,
                                     self._backend_pid,
                                     self._secret_key,
                                     )
                socket.send (data)
            finally:
                socket.close()

    def get_pid(self):
        """Return pid of the backend process for this connection.

        Does a database query the first time, and so the caller is
        responsible for ensuring that the connection is not in use."""

        if not self.is_connected():
            self._backend_pid = 0  # throw away cached value
            return 0
        elif not self._backend_pid:
            res = self.query('SELECT pg_backend_pid()')
            if res.ntuples > 0:
                self._backend_pid = res.getvalue(0, 0)

        return self._backend_pid

    def query(self, query):
        return self._simple_query(query)

    def Q (self, query):
        "perform a simple query, return all the results"
        return self._simple_query (query).getallrows()

    def query_timeout(self, query, timeout):
        """Run query with a timeout.

        If the query times out, raise TimeoutError."""

        aq = async_query(self)
        aq.start(query)  # start another thread working

        # Wait for results
        return aq.get_result(timeout)

    def analyze(self, vacuum=True, full=False, table='',
                fake_duplicate_problem=False):
        """Perform an ANALYZE statement.

        This handles the common problem where the pg_statistic table has
        gotten corrupted such that it no longer satisfies the "unique"
        constraint of its index.  This can happen if the database runs
        out of disk space.

        vacuum: whether to turn this into a VACUUM statement
        full: whether to turn this into a VACUUM FULL statement
        table: allows analyzing an individual table
        fake_duplicate_problem: for testing (see below)

        If the SQL statement fails with a violation of a unique
        constraint error, the code removes all the rows from the
        pg_statistic table and tries the command again.

        Since it's not possible to re-create the problem through any
        normal sequence, the caller can simulate the problem by passing
        fake_duplicate_problem=True.
        """

        if full:
            cmd = "VACUUM FULL"
        elif vacuum:
            cmd = "VACUUM"
        else:
            cmd = "ANALYZE"

        # Create the desired command
        sql = """%s %s""" % (cmd, table)
        try:
            if fake_duplicate_problem:
                # Do a simple analyze to ensure there's stat data
                self.query('ANALYZE')

                # Try to insert a duplicate row, which generates the
                # kind of error we're looking for.
                self.query('INSERT INTO pg_statistic SELECT * from pg_statistic LIMIT 1')
                # TODO: raise an exception in case we succeeded?

            self.query(sql)
            return
        except QueryError, e:
            if e.error_code() == PG_ERROR_UNIQUE_VIOLATION:  # Duplicate key
                # This usually means a problem with the pg_statistic table
                # Will retry below...
                P("analyze error (%r)" % (fake_duplicate_problem,))
                pass
            else:
                raise

        # Could find only duplicates, but this table usually is small,
        # and it doesn't look like ANALYZE is any faster if there is
        # existing data.
        try:
            self.query("""DELETE FROM pg_statistic""")
        except PostgresError:
            pass  # don't barf trying to fix pg_statistic

        # Retry.  May still raise an exception, but at this point,
        # there's nothing more to be done about it.
        self.query(sql)

    def putline(self, ln):
        if self._state == self.COPYIN:
            self.send_packet (PG_COPY_DATA_MSG, _ByteN_arg(ln))
        else:
            raise InternalError('Current state (%s) is not %s' % (self._state, self.COPYIN))

    def endcopy(self):
        if self._state == self.COPYIN:
            self.send_packet(PG_COPY_DONE_MSG)

            # now go back to processing backend messages
            self._state = self.CONNECTED
            return self._simple_query(None)
        else:
            pass  # we'll let this slide

    def lo_creat(self, mode):
        fn_oid = self._get_lo_function('lo_creat')
        return self._function_call(fn_oid, mode)

    def lo_open(self, loid, mode):
        fn_oid = self._get_lo_function('lo_open')
        return self._function_call(fn_oid, loid, mode)

    def lo_write(self, fd, data):
        fn_oid = self._get_lo_function('lowrite')
        return self._function_call(fn_oid, fd,
                                   _ByteN_arg(data))

    def lo_read(self, fd, numbytes=0):
        fn_oid = self._get_lo_function('loread')
        if numbytes <= 0:
            # read to end of file
            current_pos = self.lo_tell(fd)
            lo_end = self.lo_lseek(fd, 0, SEEK_END)
            numbytes = lo_end - current_pos

            self.lo_lseek(fd, current_pos, SEEK_SET)

        return self._function_call(fn_oid, fd, numbytes)

    def lo_lseek(self, fd, offset, whence):
        fn_oid = self._get_lo_function('lo_lseek')
        return self._function_call(fn_oid, fd, offset, whence)

    def lo_tell(self, fd):
        fn_oid = self._get_lo_function('lo_tell')
        return self._function_call(fn_oid, fd)

    def lo_close(self, fd):
        fn_oid = self._get_lo_function('lo_close')
        return self._function_call(fn_oid, fd)

    def lo_unlink(self, loid):
        fn_oid = self._get_lo_function('lo_unlink')
        return self._function_call(fn_oid, loid)

    def close(self):
        if self._state != self.DISCONNECTED:
            try:  # best effort at cleanly shutting down
                self.send_packet(PG_TERMINATE_MSG)

                # Make sure the db disconnects, so that if the caller
                # tries to (say) delete the database the system won't
                # report it in use.  (Mostly seen during testing.)
                for unused in xrange(10):
                    if self.is_connected():
                        sleep(0.1)
                    else:
                        break
            except (socket.error, OSError, IOError):
                pass

            self._dump_connection()

    def _dump_connection(self):
        """Throw away the socket, to get to a known state."""

        try:
            if self._socket:
                self._socket.close()
        except (socket.error, OSError, IOError):
            pass

        self._socket = None
        self._backend_pid = 0
        self._state = self.DISCONNECTED

    finish = close  # compatibility with libpq

    def notice_received(self, where, message_fields):
        # print where, message_fields
        pass

# Internal Routines

    def connect_socket(self):
        """Connect to database at self.address.

        Returns socket object and function to send data on the socket.
        Works with TCP and Unix-domain sockets."""

        if isinstance(self.address, type('')):
            sock = coro.unix_sock()
        else:
            sock = coro.tcp_sock()
        sock.connect (self.address)
        return sock

    _debug = 0

    def send_packet (self, message_type, *data_args):
        data = build_message (message_type, *data_args)

        if self._debug:
            print '-->', repr(message_type)
            a, b = dump_hex(data)
            print a
            print b

        self._socket.send (data)

        return None

    def get_header(self):
        header = self._socket.recv_exact (5)
        message_type, length = struct.unpack ('!cL', header)
        return message_type, length - 4  # return length of data only

    def read_packet(self):
        msg, length = self.get_header()
        data = self._socket.recv_exact (length)
        if self._debug:
            print "<--", repr(msg), length, len(data)
            a, b = dump_hex(data)
            print a
            print b
        return msg, data

    def _startup(self):
        """Implement startup phase of Postgres protocol"""

        # send startup packet
        self.send_packet (
            PG_STARTUP_MSG,
            0x00030000,  # protocol version
            'user', self.username,
            'database', self.database,
            ''  # terminate options
        )

        msg, data = self.read_packet()
        if msg == PG_AUTHENTICATION_OK_MSG:
            tag, = struct.unpack ('!L', data[:4])
            if tag == 0:
                pass  # no password needed
            elif tag == 5:
                salt = data[4:8]
                # SMR: thx to postgres-pr for this!
                m = md5_hex (self.password + self.username)
                m = 'md5' + md5_hex (m + salt)
                self.send_packet (PG_PASSWORD_MESSAGE, m)
            else:
                raise NotImplementedError ("authentication type: %d" % (tag,))
        elif msg == PG_ERROR_MSG:
            raise ConnectError(("_startup", data))
        else:
            raise ConnectError("Authentication required (%d)" % msg)

    def _wait_for_backend(self):
        """Wait for the backend to be ready for a query"""

        while 1:
            msg, data = self.read_packet()

            if msg == PG_READY_FOR_QUERY_MSG:
                return

            elif msg == PG_BACKEND_KEY_DATA_MSG:
                self._backend_pid, self._secret_key = unpack_data (data, 'ii')
                # print "pid=%d, secret=%d" % (self._backend_pid, self._secret_key)

            elif msg == PG_PARAMETER_STATUS_MSG:
                k, v = unpack_data(data, 'ss')
                self._set_parameter(k, v)

            elif msg == PG_ERROR_MSG:
                raise ConnectError(("_wait_for_backend", data))

            elif msg == PG_NOTICE_MSG:
                self._notice(BACKEND_START_NOTICE, data)

            else:
                continue  # ignore packet?

    def _simple_query(self, query):
        """Execute a simple query.

        query can be None, which skips sending the query
        packet, and immediately goes to processing backend
        messages."""

        if query is not None:
            self._exit_copy_mode('New query started')
            self.send_packet(PG_QUERY_MSG, query)

        exception = None
        result = None

        while 1:
            msg, data = self.read_packet()

            if msg == PG_READY_FOR_QUERY_MSG:
                break

            elif msg == PG_COMMAND_COMPLETE_MSG:
                if not result:
                    result = query_results()
                result._command_complete(data)

            elif msg == PG_COPY_IN_RESPONSE_MSG:
                self._state = self.COPYIN
                break  # exit loop so we can accept putline calls

            elif msg == PG_COPY_OUT_RESPONSE_MSG:
                if not exception:
                    exception = InternalError('COPY OUT not supported')
                break

            elif msg == PG_ROW_DESCRIPTION_MSG:
                result = query_results()
                result._row_description(data)

            elif msg == PG_DATA_ROW_MSG:
                if result:
                    result._data_row(data)
                else:
                    if not exception:
                        exception = InternalError(
                            'DataRow message before RowDescription?')

            elif msg == PG_EMPTY_QUERY_RESPONSE_MSG:
                # totally empty query (skip it?)
                continue

            elif msg == PG_PARAMETER_STATUS_MSG:
                k, v = unpack_data(data, 'ss')
                self._set_parameter(k, v)

            elif msg == PG_ERROR_MSG:
                if not exception:
                    exception = QueryError('_simple_query', data)
                    # LOG("QueryError: %r" % exception.error_fields)
                    # coro.print_stderr("QueryError: %r\n" % exception.error_fields)

            elif msg == PG_NOTICE_MSG:
                self._notice(QUERY_NOTICE, data)

            else:
                continue  # ignore packet?

        if exception:
            raise exception

        return result

    def _exit_copy_mode(self, msg='_exit_copy_mode'):
        if self._state == self.COPYIN:
            self.send_packet(PG_COPY_FAIL_MSG, msg)

    def _set_parameter(self, key, value):
        self.backend_parameters[key] = value
# print "Parameter: %s=%s" % (k, v)

    def _notice(self, where, data):
        self.notice_received(where, unpack_notice_data(data))

    def _get_lo_function(self, name):
        global _lo_functions

        if not _lo_functions:
            res = self._simple_query(
                """select proname, oid from pg_proc
                   where proname='lo_open' or
                         proname='lo_close' or
                         proname='lo_creat' or
                         proname='lo_unlink' or
                         proname='lo_lseek' or
                         proname='lo_tell' or
                         proname='loread' or
                         proname='lowrite'""")
            fns = {}
            for i in xrange(res.ntuples):
                fns[res.getvalue(i, 0)] = res.getvalue(i, 1)

            _lo_functions = fns

        fn = _lo_functions.get(name)
        if fn:
            return fn
        else:
            raise InternalError('Failed to get %s function oid' % name)

    def _function_call(self, fn_oid, *args):
        """Execute a function call."""

        # Construct the arguments for the function call message

        packet_args = [fn_oid,  # what function
                       _Int16_arg(1),  # only need 1 format type...
                       _Int16_arg(1),  # ...which is binary
                       ]

        # Number of args
        packet_args.append(_Int16_arg(len(args)))

        # Args (length followed by bytes)...
        for a in args:
            if a is None:
                packet_args.append(-1)  # special representation for NULL
            else:
                arg_data = pack_data(a)
                packet_args.extend((len(arg_data), _ByteN_arg(arg_data)))

        # Result type (binary)
        packet_args.append(_Int16_arg(1))

        self.send_packet(PG_FUNCTION_CALL_MSG, *packet_args)

        exception = None
        result = None

        while 1:
            msg, data = self.read_packet()

            if msg == PG_READY_FOR_QUERY_MSG:
                break

            elif msg == PG_FUNCTION_CALL_RESPONSE:
                return_len, data = unpack_data(data, 'i', return_rest=1)

                if return_len == 2:
                    result = struct.unpack('!h', data)[0]
                elif return_len == 4:
                    result = struct.unpack('!i', data)[0]
                else:
                    result = data  # punt on decoding?

            elif msg == PG_ERROR_MSG:
                if not exception:
                    exception = FunctionError('_function_call', data)
                    # LOG("FunctionError: %r" % exception.error_fields)
                    # coro.print_stderr("FunctionError: %r\n" % exception.error_fields)

            elif msg == PG_NOTICE_MSG:
                self._notice(FUNCTION_NOTICE, data)

            else:
                continue  # ignore packet?

        if exception:
            # coro.print_stderr("exception: %r\n" % exception.error_fields)
            raise exception

        return result

_lo_functions = None


# ===========================================================================
# Query Results
# ===========================================================================

# XXX this is where we need some generator goodness.

class query_results:
    def __init__(self):
        self.ntuples = 0
        self.nfields = 0
        self.cmdTuples = 0
        self.oidValue = 0

        self._field_names = []
        self._field_types = []
        self._rows = []

    def fname(self, fidx):
        if fidx >= 0 and fidx < len(self._field_names):
            return self._field_names[fidx]
        else:
            return None

    def fnumber(self, name):
        for i in xrange(len(self._field_names)):
            if self._field_names[i] == name:
                return i

        return -1

    def getvalue(self, row, col):
        r = self.getrow(row)
        if r is not None and col >= 0 and col < len(r):
            return r[col]
        else:
            return None

    def getrow(self, row):
        if row >= 0 and row < len(self._rows):
            return self._rows[row]
        else:
            return None

    def getallrows(self):
        return self._rows

    def getrowdict(self, row):
        """Get a result row as a dictionary {col name: value}"""

        r = self.getrow(row)
        if r is not None and len(r) <= len(self._field_names):
            result = {}
            for i in xrange(len(r)):
                result[self._field_names[i]] = r[i]
            return result
        else:
            return None

    def getallrowdicts(self):
        return map(self.getrowdict, xrange(self.ntuples))

    def getcolumn(self, col):
        """Get a list of all values in a given column."""

        try:
            return map(lambda x: x[col], self._rows)
        except IndexError:
            # col out of range
            return None

    def _row_description(self, data):
        self.nfields, data = unpack_data(data, 'h', return_rest=1)

        for unused in xrange(self.nfields):
            (fname, table_oid,
             colnum, ftype,
             fsize, fmod,
             format, data) = unpack_data(data, 'sihihih', return_rest=1)
            self._field_names.append(fname)
            self._field_types.append(ftype)

    def _data_row(self, data):
        nvalues, data = unpack_data(data, 'h', return_rest=1)

        new_row = [None] * self.nfields
        for i in xrange(min(self.nfields, nvalues)):
            collen, data = unpack_data(data, 'i', return_rest=1)
            if collen >= 0:
                cast = decode_type_map.get(self._field_types[i], str)
                new_row[i] = cast(data[:collen])
                data = data[collen:]

        self._rows.append(new_row)

    def _command_complete(self, data):
        # extract some info from the data
        tag = unpack_data(data, 's')[0]
        parts = tag.split(' ')

        self.oidValue = 0
        self.cmdTuples = 0

        if parts[0] == 'INSERT':
            self.oidValue = int(parts[1])
            self.cmdTuples = int(parts[2])
        elif parts[0] in ('DELETE', 'UPDATE', 'MOVE', 'FETCH'):
            self.cmdTuples = int(parts[1])

        self.ntuples = len(self._rows)


class async_query:
    """Class that encapsulates an asynchronous database query.

    Currently, this is used for implementing a timeout, but in the
    future it could be exposed at the app level, to allow a query
    to be started and the results tested some time later.

    :IVariables:
        - `_client`: instance of postgres_client, representing
          the database connection.
        - `_thread`: the thread running the query
        - `_res`: result of the query; None if not yet completed;
          and exception if the query produced an exception
        - `_cv`: condition_variable to allow a thread waiting for
          the result to be awaken.

    An instance can only run a single query, although the result
    can be read multiple times by multiple threads.
    """

    def __init__(self, client):
        self._client = client

        # It isn't necessary to hang onto the thread, but it might
        # help in debugging.
        self._thread = None

        self._res = None
        self._cv = coro.condition_variable()

    def start(self, sql):
        """Start a thread to run sql as a query."""

        assert self._thread is None
        self._thread = coro.spawn(self._run_query, sql)
        self._thread.set_name('async_query %r' % id(self))

    def _run_query(self, sql):
        """Function run in secondary thread to run db query."""

        try:
            res = self._client.query(sql)
        except PostgresError, e:
            # The query raised an exception, which becomes the "result"
            res = e

        # Set the result attribute & wake threads waiting for the answer.
        self._res = res
        self._cv.wake_all()

    def get_result(self, timeout):
        """Get query result waiting up to timeout seconds.

        If the query completes within the timeout, it will be
        returned.  If the query raises an exception, the same
        exception will be raised by this call.  If the query times
        out, this call will raise coro.TimeoutError.
        """

        # See if the query has completed already; if not
        # wait for a result up to the timeout.
        if self._res is None:
            try:
                coro.with_timeout(timeout, self._cv.wait)
            except coro.TimeoutError:
                # Timeout, cancel the request.
                self._client.cancel()

            # The request may or may not be cancelled.  Either way,
            # wait for the other thread to post a result and finish
            # using the db connection.
            while self._res is None:
                self._cv.wait()

        if isinstance(self._res, Exception):
            if (isinstance(self._res, BackendError) and
                    self._res.error_code() == PG_ERROR_QUERY_CANCELLED):
                raise coro.TimeoutError  # really did time out
            else:
                raise self._res

        # Fall through if we got an actual result
        return self._res


# ======================================================================
# Places in the code that can get notice messages
# ======================================================================

BACKEND_START_NOTICE = "wait for backend"
QUERY_NOTICE = "query"
FUNCTION_NOTICE = "function"


# ======================================================================
# Postgres message types
# ======================================================================

# See http://www.postgresql.org/docs/9.2/static/protocol-message-formats.html

PG_STARTUP_MSG = ''
PG_CANCELREQUEST_MSG = ''
PG_COMMAND_COMPLETE_MSG = 'C'
PG_COPY_DONE_MSG = 'c'
PG_DATA_ROW_MSG = 'D'
PG_COPY_DATA_MSG = 'd'
PG_ERROR_MSG = 'E'
PG_FUNCTION_CALL_MSG = 'F'
PG_COPY_FAIL_MSG = 'f'
PG_COPY_IN_RESPONSE_MSG = 'G'
PG_COPY_OUT_RESPONSE_MSG = 'H'
PG_EMPTY_QUERY_RESPONSE_MSG = 'I'
PG_BACKEND_KEY_DATA_MSG = 'K'
PG_NOTICE_MSG = 'N'
PG_QUERY_MSG = 'Q'
PG_AUTHENTICATION_OK_MSG = 'R'
PG_PARAMETER_STATUS_MSG = 'S'
PG_SYNC_MSG = 'S'
PG_ROW_DESCRIPTION_MSG = 'T'
PG_FUNCTION_CALL_RESPONSE = 'V'
PG_TERMINATE_MSG = 'X'
PG_READY_FOR_QUERY_MSG = 'Z'

# added by SMR 20130401
PG_PASSWORD_MESSAGE = 'p'

# ======================================================================
# Field types for Notice & Error messages
# ======================================================================

PG_SEVERITY_FIELD = 'S'
PG_SQLSTATE_FIELD = 'C'
PG_MESSAGE_FIELD = 'M'
PG_DEFAULT_FIELD = 'D'
PG_HINT_FIELD = 'H'
PG_POSITION_FIELD = 'P'
PG_WHERE_FIELD = 'W'
PG_FILE_FIELD = 'F'
PG_LINE_FIELD = 'L'
PG_ROUTINE_FIELD = 'R'


# ======================================================================
# Common Error Codes
# ======================================================================

PG_ERROR_UNIQUE_VIOLATION               = '23505'
PG_ERROR_INVALID_CATALOG_NAME           = '3D000'
PG_ERROR_DATABASE_EXISTS                = '42P04'
PG_ERROR_OBJECT_IN_USE                  = '55006'
PG_ERROR_QUERY_CANCELLED                = '57014'


# ======================================================================
# Decoding Postgres data types
# ======================================================================

def _simple_array_decode(x, fn):
    """Decode array string where the elements are guaranteed not to
    have any quoting (and there for we can just split on commas).
    Call fn on each element to convert to actual value"""

    if x[0] != '{' or x[-1] != '}':
        raise ValueError('Array not enclosed in {...}')
    x = x[1:-1]
    return map(lambda s, fn=fn: fn(s.strip()), x.split(','))

_simple_str_elt_re = re.compile(r'''([^"][^,]*),?''')
_quoted_str_elt_re = re.compile(r'''"((?:\\\\|\\"|[^"])*)",?''')
_escape_sub_re = re.compile(r'\\(.)')

def _general_array_decode(x):
    """Decode array string where elements may be quoted."""

    if x[0] != '{' or x[-1] != '}':
        raise ValueError('Array not enclosed in {...}')
    x = x[1:-1]

    result = []
    pos = 0
    while pos < len(x):
        # check for unquoted element
        match = _simple_str_elt_re.match(x, pos)
        if match:
            result.append(match.group(1))
            pos = match.end()
            continue

        # check for quoted element
        match = _quoted_str_elt_re.match(x, pos)
        if match:
            # must undo '\' escaping
            part = _escape_sub_re.sub(r'\1', match.group(1))
            result.append(part)
            pos = match.end()
            continue

        # problem
        raise ValueError('Could not parse array (%s)' % x[pos:])

    return result

def decode_bool(x):
    return x == 't' or x == 'T'

def decode_int_array(x):
    return _simple_array_decode(x, int)

def decode_str_array(x):
    return _general_array_decode(x)

# Matches \xxx and \\ where x is an octal character.
# Note that the regexp syntax requires 2 \ characters in a pattern,
# and the raw string eliminates the need to quote each of those.
_quoted_bytea_re = re.compile(r'(\\[0-7][0-7][0-7]|\\\\)')

def decode_bytea(x):
    parts = _quoted_bytea_re.split(x)
    for i in xrange(1, len(parts), 2):
        # even elements are OK; odd elements need conversion
        if parts[i] == '\\\\':  # 2 slashes...
            parts[i] = '\\'  # ...become 1
        else:
            parts[i] = chr(int(parts[i][1:], 8))

    return ''.join(parts)

#
# from src/include/catalog/pg_type.h
#
# by default leave as a string
decode_type_map = {}
decode_type_names = {}

for oid, cast, name in (
    (16, decode_bool, 'bool'),
    (17, decode_bytea, 'bytea'),
    (18, str, 'char'),
    (19, str, 'name'),
    (20, long, 'int8'),
    (21, int, 'int2'),
    (22, decode_int_array, 'int2vector'),
    (23, int, 'int4'),
    (25, str, 'text'),
    (26, int, 'oid'),
    (27, long, 'tid'),
    (28, int, 'xid'),
    (29, int, 'cid'),
    (30, str, 'oidvector'),
    (700, float, 'float4'),
    (701, float, 'float8'),
    (1007, decode_int_array, 'int4[]'),
    (1009, decode_str_array, 'text[]'),
):
    decode_type_map[oid] = cast
    decode_type_names[oid] = name


# ===========================================================================
#                           Packet Protocol
# ===========================================================================

def not_unpack_data(data, formats, return_rest=0):
    """Unpack the data part of a packed according to format:

    i: Int32
    h: Int16
    s: String
    c: Byte1 (returned as 1-character string)

    If return_rest is true, then return the rest of the
    data as the last item in the list.
    """

    pos = 0
    result = []

    for code in formats:
        if code == 'i':
            result.append(struct.unpack('!i', data[pos:pos + 4])[0])
            pos += 4
        elif code == 'h':
            result.append(struct.unpack('!h', data[pos:pos + 2])[0])
            pos += 2
        elif code == 'c':
            result.append(data[pos])
            pos += 1
        elif code == 's':
            i = data.find('\0', pos)
            if i < 0:
                i = len(data)

            result.append(data[pos:pos + i])
            pos += (i + 1)

    if return_rest:
        result.append(data[pos:])

    return result

try:
    # try to pull in cython speedup[s]
    from coro.db.postgres import proto
    unpack_data = proto.unpack_data
except ImportError:
    pass

def unpack_error_data(data):
    """Unpack the dictionary-like structure used in Error and Notice
    responses.  keys are 1 character codes followed by a String.  A
    '\0' key signals the end of the data."""

    pos = 0
    result = {}

    while pos < len(data):
        k = data[pos]
        if k == '\0':
            break
        pos += 1
        if not k:
            break

        i = data.find('\0', pos)
        if i < 0:
            i = len(data)

        result[k] = data[pos:i]
        pos = i + 1

    return result

unpack_notice_data = unpack_error_data

def pack_data(*data_args):
    """Pack data values into an appropriate payload.

    Each element of data_args can be:

    string: encode as String
    int: encode as Int32
    list/tuple: use the first element as a pack string
                applied to rest of the elements
    """

    parts = []
    for d in data_args:
        if isinstance(d, types.StringType):
            parts.extend((d, '\0'))  # null terminated string
        elif isinstance(d, types.IntType):
            parts.append(struct.pack('!i', d))
        elif isinstance(d, types.ListType) or isinstance(d, types.TupleType):
            parts.append(struct.pack(d[0], *d[1:]))

    return ''.join(parts)

def build_message (message_type, *data_args):
    """Build a Postgres message."""

    args = pack_data(*data_args)
    data = ''.join ([message_type, struct.pack('!i', len(args) + 4), args])
    return data

# helpers that can be used for args to pack_data

def _Int16_arg(x):
    if isinstance(x, types.IntType):
        return ('!h', x)
    else:
        return ('!%dh' % len(x),) + tuple(x)

def _Int32_arg(x):
    if isinstance(x, types.IntType):
        return int(x)
    else:
        return ('!%di' % len(x),) + tuple(x)

def _ByteN_arg(x):
    return ('!%ds' % len(x), x)

# used to generate the dumps below
def dump_hex (s):
    r1 = []
    r2 = []

    for ch in s:
        r1.append (' %02x' % ord(ch))
        if (ch in string.letters) or (ch in string.digits):
            r2.append ('  %c' % ch)
        else:
            r2.append ('   ')

    return string.join (r1, ''), string.join (r2, '')


# ===========================================================================
#                           Postgres Utilities
# ===========================================================================

def connect_to_db(database, username='', address=None, schema=None):
    """Utility to connect to the indicated database.

    Return instance of postgres_client if successful or None if there
    is no such database.  If schema is supplied as a string, and the
    database doesn't exist, create the database."""

    database = database.lower()
    db = postgres_client(database=database,
                         username=username,
                         address=address)
    try:
        db.connect()
        return db
    except ConnectError, e:  # doesn't exist
        if e.error_code() == PG_ERROR_INVALID_CATALOG_NAME:
            if schema is not None:
                dbm = database_manager(username=username,
                                       address=address)
                dbm.create_database(database, schema)

                # try again
                db.connect()
                return db
            else:
                return None
        else:
            raise  # some other Postgres error

class database_manager:
    def __init__(self, username='', password='',
                 address=None,
                 debug=False):
        self._db = postgres_client('template1',
                                   username=username,
                                   password=password,
                                   address=address)
        self._debug = debug

    def has_database(self, database):

        res = self._query(
            '''select datname from pg_database
               where datname=%s''' % quote_string(database))
        return res.ntuples

    def create_database(self, database, schema=None):
        delete_db = False

        try:
            self._query('CREATE DATABASE %s' % database)
            if schema:
                # If we can't install the schema, ensure db is dropped
                delete_db = True

                # connect to new database and load it up
                new_db = postgres_client(database,
                                         username=self._db.username,
                                         password=self._db.password,
                                         address=self._db.address)
                new_db.connect()
                try:
                    new_db.query(schema)
                    delete_db = False  # it's done
                finally:
                    new_db.close()
        finally:
            if delete_db:
                try:
                    self.drop_database(database)
                except coro.Interrupted:
                    raise
                except:
                    pass

    def drop_database(self, database):
        try:
            self._query('DROP DATABASE %s' % database)
        except QueryError, e:
            if e.error_code() == PG_ERROR_INVALID_CATALOG_NAME:
                pass  # this is OK
            else:
                raise

    def _query(self, sql):
        """Perform a query against the template1 database.

        create/drop database cannot be done while there are other
        connections to template1, so only maintain the connection for
        as long as necessary.

        Also, this method handles the error that occurs if >1
        connection tries to manipulate databases while there are other
        connections.  (Because there are multiple processes involved,
        it isn't possible to serialize connections to template1.)
        """

        self._db.connect()
        try:
            while True:
                try:
                    return self._db.query(sql)
                except QueryError, e:
                    if e.error_code() == PG_ERROR_OBJECT_IN_USE:
                        self._backoff()
                    else:
                        raise  # some other exception
        finally:
            self._db.close()  # close the connection

    def _backoff(self):
        """Called after finding that template1 is in use.

        Checks to see if the current connection has the lowest pid.
        If not, then closes the connection to allow the lowest
        to proceed.  This should prevent livelock where 2 threads
        continually interfere with one another."""

        my_pid = self._db.get_pid()

        while True:
            # Get smallest pid connected to template1
            res = self._db.query(
                """SELECT procpid FROM pg_stat_activity
                   WHERE datname='template1' ORDER BY procpid""")

            if res.ntuples == 0:
                # For some reason, occasionally there is nothing in
                # the pg_stat_activity table.  Try again.
                sleep(0.5)
                continue
            elif res.getvalue(0, 0) != my_pid:
                if self._debug:
                    P("%d closing" % (my_pid,))
                self._db.close()  # the other connection takes priority
                break
            else:
                # This connection has smallest pid, so remain connected.
                if self._debug:
                    P("%d continuing" % (my_pid,))
                break

        # Allow some time for other connections to finish.
        sleep(2)

        # Must be connected before returning.
        if not self._db.is_connected():
            self._db.connect()

def md5_hex (s):
    import hashlib
    h = hashlib.new ('md5')
    h.update (s)
    return h.hexdigest()

def sleep(x):
    coro.sleep_relative(x)

# def make_db():
# db = postgres_client('lrosenstein', '', 'system_quarantine',
# ('127.0.0.1', 5432))
# db.connect()
# return db

# def test_open(db):
# db.query("BEGIN")
# return db.lo_open(17219, INV_READ)

# def test_read(db):
# db.query("BEGIN")
#    fd = db.lo_open(17219, INV_READ)
# print db.lo_read(fd, 50)
# print db.lo_read(fd)
# db.lo_close(fd)
# db.query("ROLLBACK")

def test_concurrent_dbm(tries):
    dbm = database_manager(debug=True)
    for i in xrange(tries):
        my_tid = coro.current().thread_id()
        db_name = "test_database_%d_%d" % (my_tid, i)
        dbm.create_database(db_name)
        dbm.drop_database(db_name)
        P("done %r" % db_name)

def watcher(thread_ids):
    while True:
        if len(thread_ids) == 0:
            coro.set_exit()
            break
        thread_ids = [x for x in thread_ids if x in coro.all_threads]
        coro.sleep_relative(0.1)

if __name__ == '__main__':

    import backdoor
    coro.spawn (backdoor.serve)

    # thread_ids = []
    #
    # for i in xrange(3):
    #    thread_ids.append(coro.spawn(test_concurrent_dbm, 5).thread_id())
    # coro.spawn(watcher, thread_ids)

    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = tcopy
# -*- Mode: Python -*-

import coro
import coro.db.postgres as PG
import struct
W = coro.write_stderr

# postgres implements the COPY FROM STDIN command inside the protocol,
#  if you emit a COPY command (via SQL), it will put the connection into
#  copyin mode.  Feed it the exact same data you would have from a file,
#  and you have a much faster way of populating a database.

class writer:
    def __init__ (self, forms, fout, chunk_size=16000):
        self.forms = forms
        self.fout = fout
        self.buffer = []
        self.size = 0
        self.chunk_size = chunk_size
        self.append ('PGCOPY\n\xff\r\n\x00')
        self.append (struct.pack ('>L', 0))  # flags
        self.append (struct.pack ('>L', 0))  # ext len
        self.count = 0

    def append (self, data):
        self.buffer.append (data)
        self.size += len (data)
        if self.size > self.chunk_size:
            self.flush()

    def flush (self):
        block, self.buffer = self.buffer, []
        self.size = 0
        block = ''.join (block)
        self.fout (block)

    def write_row (self, row):
        row_data = [struct.pack ('>h', len(row))]
        for i in range (len (self.forms)):
            if row[i] is None:
                row_data.append (struct.pack ('>l', -1))
            else:
                data = struct.pack ('>%s' % (self.forms[i],), row[i])
                row_data.append (struct.pack ('>l', len(data)))
                row_data.append (data)
        self.count += 1
        self.append (''.join (row_data,))

    def done (self):
        self.append (struct.pack ('>h', -1))
        self.flush()

def t0():
    db = PG.postgres_client ('t0', 'foo', 'bar')
    db.connect()
    try:
        db.Q ('drop table squares;')
    except PG.QueryError:
        pass
    db.Q ('create table squares (n int, n2 int);')
    db.query ('copy squares from stdin binary;')
    w = writer (('i', 'i'), db.putline)
    for i in range (1000):
        w.write_row ([i, i * i])
    w.done()
    db.endcopy()

coro.spawn (t0)
coro.event_loop()

########NEW FILE########
__FILENAME__ = cache
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""DNS cache object.

:Variables:
    - `dns_request`: counter.  Incremented for each top-level query
      (not incremented for additional queries required for recursive queries).
    - `net_request`: counter.  A query is being sent over the network.
    - `cache_hit`: counter.  A value was successfully retrieved from
      the cache. This does not include expired entries.
    - `cache_miss`: counter.  A value was not found in the cache. This
      does not include expired entries.
    - `cache_exception`: counter.  A "negative" cache entry was hit.
    - `cache_expired`: counter.  A value was found in the cache, but
      has expired.
"""

# Two kinds of negatively-cached data
CACHE_NXDOMAIN = "NXDOMAIN"
CACHE_NODATA   = "NODATA"

# how long (in seconds) to cache non-rfc2308 negative responses
DEFAULT_NEGATIVE_TTL = 0

# limit on the amount of work we will do for any one query
RUNAWAY = 40

# limit on the amount of gluelessness we will take for any one query
GLUELESSNESS = 3

DEFAULT_MIN_TTL = 1800

# Goals:
# 1) paranoia
# 2) do not sacrifice safety for performance; i.e., send extra queries to be safe.
# 3) performance

# TODO:
# truncation (although very rare)
# pay attention to depth of gluelessness?
# flag lame NS in cache
# Add a placeholder in the cache so that while
#   a query is executing, new threads don't try to do the same query.
# detect servers that are down, and discourage their use (w/eventual rehabilitation)
# support unknown dns data types so djb doesn't embarrass us

# BUGS
# I bet we get confused if we get an empty list of IP's for a nameserver.
#

# QUESTIONS
# how common is it for a nameserver to have more than one address (from the internet)?
# djb doesn't appear to care about the AA bit?

# ===========================================================================
# It'd be nice if there were a set of 'test' domains along with some queries
# that would act as a regression test for correctness.  Another possibility
# would be a test server that would provide canned responses to do the same.
#
#                      *** THIS IS WORTH DOING ***
#
# ===========================================================================

# See rfc2181 for many clarifications to rfc1035.

# section 5.3.3 of rfc1034

#  1) must reply always come from the IP we sent it to? [yes. djb even uses a connected socket]
#  2) can we insist on authoritative data?

# updated Feb 2013
# dig @a.root-servers.net.
raw_hints = """\
a.root-servers.net.	3600000	IN	A	198.41.0.4
a.root-servers.net.	3600000	IN	AAAA	2001:503:ba3e::2:30
b.root-servers.net.	3600000	IN	A	192.228.79.201
c.root-servers.net.	3600000	IN	A	192.33.4.12
d.root-servers.net.	3600000	IN	A	199.7.91.13
d.root-servers.net.	3600000	IN	AAAA	2001:500:2d::d
e.root-servers.net.	3600000	IN	A	192.203.230.10
f.root-servers.net.	3600000	IN	A	192.5.5.241
f.root-servers.net.	3600000	IN	AAAA	2001:500:2f::f
g.root-servers.net.	3600000	IN	A	192.112.36.4
h.root-servers.net.	3600000	IN	A	128.63.2.53
h.root-servers.net.	3600000	IN	AAAA	2001:500:1::803f:235
i.root-servers.net.	3600000	IN	A	192.36.148.17
i.root-servers.net.	3600000	IN	AAAA	2001:7fe::53"""

# filtered below if !ipv6
root_hints = [x.split()[4] for x in raw_hints.split('\n')]

def domain_suffix (a, b):
    # is b a suffix of a?
    if a == '' or a == b:
        return 1
    else:
        la = len(a)
        return b[-la:] == a and b[-(la + 1)] == '.'

class timeouts:

    """Used to control the timeouts used for queries.
    If there is one possible server, it uses a 60-second timeout.
    If there is two servers, then there is a 15-second timeout for the first
    and a 45 second timeout for the second.
    3: 5, 10, 45
    4: 1, 3, 11, 45
    5: 1, 3, 11, 45, 1
    6: 1, 3, 11, 45, 1, 1
    etc.
    """

    def __init__ (self, n):
        self.index = 0
        self.values = self.times (n)

    def times (self, n):
        """Generate the timeout list for n servers."""
        if n == 1:
            return [60]
        elif n == 2:
            return [15, 45]
        elif n == 3:
            return [5, 10, 45]
        elif n >= 4:
            return [1, 3, 11, 45]

    def next (self):
        """Return the next timeout value."""
        if self.index >= len(self.values):
            return 1
        else:
            self.index += 1
            return self.values[self.index - 1]

class Work:

    def __init__ (self):
        self.work = 0
        self.glue = 0

    def indent (self):
        return '  ' * self.work

    # XXX need to decide *exactly* what constitutes work, and where's the
    #     best place in the code to do the increment.  One obvious answer
    #     is in the query() function itself - but a more finessed answer
    #     might be in cache_get().  That would catch CNAME loops that are
    #     entirely in the cache.  Think of any other types of loops that
    #     might not require calls to query() or cache_get().

    def incr (self, qname, qtype, ns_name=''):
        self.work += 1
        if self.work >= RUNAWAY:
            raise DNS_Runaway_Query_Error (qname, qtype, ns_name)

    def incr_glue (self):
        self.glue += 1

class dns_cache:

    # If parent_ns is set, then it should be a dictionary:
    # {domain: ns_list}
    # Domain should be a string such as "ironport.com".
    # ns_list should be a list of (TTL, ns_name) tuples.
    # A domain of the empty string is used if none of the other domains match.
    # More specific domains "win", so "sfo.ironport.com" would get used over
    # "ironport.com" for something like "foo.sfo.ironport.com".
    #
    # Entries in ns_list will be processed in order.
    parent_ns = None

    # Turns on/off debug log calls.
    debug = False

    def __init__ (self, cache_size=10000, negative_ttl=DEFAULT_NEGATIVE_TTL, ttl_min=DEFAULT_MIN_TTL):
        self.cache = lru_with_pin (cache_size)
        # hardcode localhost, TTL=0 means never expire
        self._pin_localhost()
        # The maximum number of parallel queries allowed
        self.max_outstanding = 500
        # used to control the concurrency
        self.outstanding_sem = coro.semaphore(self.max_outstanding)

        self.bootstrap_cv = coro.condition_variable()
        self.source_ip = ''
        self.negative_ttl = negative_ttl
        self.flush_callbacks = []
        self.use_actual_ttl = 0
        self.ttl_min = ttl_min

    def log (self, *args):
        coro.write_stderr (repr(args) + '\n')

    def _pin_localhost(self):
        """_pin_localhost() -> None
        Adds the localhost entries to the pinned cache.
        """
        self.cache.pin (('localhost', 'A'), [(0, '127.0.0.1')])
        self.cache.pin (('localhost', 'AAAA'), [(0, '::1')])
        self.cache.pin (('1.0.0.127.in-addr.arpa', 'PTR'), [(0, 'localhost')])
        self.cache.pin (
            ('1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa.', 'PTR'),
            [(0, 'localhost')]
        )

    def empty(self, pinned_as_well=0):
        """empty(self, pinned_as_well = 0) -> None
        Clears out the cache.
        Be sure to know what you are doing when setting pinned_as_well.
        """
        self.cache.empty(pinned_as_well)
        if pinned_as_well:
            self._pin_localhost()

    def build_request (self, qid, name, qtype, qclass, recursion):
        """build_request (self, qid, name, qtype, qclass, recursion)
        Build a request packet.  Returns the raw packet as a string."""
        m = packet.Packer()
        h = packet.Header()
        h.id = qid
        h.opcode = packet.OPCODE.QUERY
        h.rd = recursion
        h.qdcount = 1
        m.addHeader (h)
        qtype_code = getattr (packet.TYPE, qtype)
        m.addQuestion (name, qtype_code, qclass)
        return m.getbuf()

    def send_request (self, request, use_tcp=0):
        """send_request (self, request)
        Send a request over a socket.  request is a tuple:
        qid, ip, (qname, qtype, qclass, recursion)
        Returns a dns_reply object.
        """
        net_request.inc()
        qid, ip, (qname, qtype, qclass, recursion) = request
        r = self.build_request (qid, qname, qtype, qclass, recursion)
        import struct
        try:
            if ':' in ip:
                sfamily = coro.AF.INET6
            else:
                sfamily = coro.AF.INET
            if use_tcp:
                stype = coro.SOCK.STREAM
            else:
                stype = coro.SOCK.DGRAM
            s = coro.make_socket (sfamily, stype)
        except OSError, why:
            self.log ('COMMON.APP_FAILURE', tb.traceback_string() + ' why: ' + str(why))
            raise DNS_Soft_Error((qname, qtype, ip, str(why)))

        try:
            try:
                if self.source_ip:
                    s.bind((self.source_ip, 0))
                s.connect ((ip, 53))
                while 1:
                    if use_tcp:
                        r = struct.pack ('>h', len(r)) + r
                    n = s.send (r)
                    if n != len(r):
                        self.log('DNS.NETWORK_ERROR', 'send', ip, qname)
                        raise DNS_Soft_Error((qname, qtype, ip, 'Failed to send all data.'))
                    if use_tcp:
                        rlen_bytes = s.recv_exact(2)
                        rlen, = struct.unpack ('>H', rlen_bytes)
                        # rlen is the length of the packet (not including the 2 rlen bytes)
                        data = s.recv_exact(rlen)
                    else:
                        # UDP
                        # 1024 - ~PyStringObject overhead (which is actually 20 bytes)
                        # RFC actually limits it to 512 bytes
                        data = s.recv (1000)
                    try:
                        reply = unpack_reply (data, use_actual_ttl=self.use_actual_ttl, ttl_min=self.ttl_min)
                    except packet.UnpackError:
                        self.log('DNS.RESPONSE_ERROR', repr(data), ip, qname)
                        raise DNS_Soft_Error((qname, qtype, ip, 'Packet was corrupted.'))
                    except:
                        self.log('COMMON.APP_FAILURE', tb.traceback_string() + ' REPR:' + repr(data))
                        raise DNS_Soft_Error((qname, qtype, ip, 'Unknown Error'))
                    if reply.id != qid:
                        # probably a DoS attack...
                        self.log('DNS.RESPONSE_ERROR', 'id(%i)!=qid(%i)' % (reply.id, qid), ip, qname)
                        # ... don't let 'em hog us
                        coro.sleep_relative (1)
                    elif reply.rcode and reply.rcode != packet.RCODE.NXDomain:
                        rcode = packet.RCODE_MAP.get (reply.rcode, str(reply.rcode))
                        self.log('DNS.RESPONSE_ERROR', 'rcode=%s data=%r' % (rcode, repr(data)), ip, qname)
                        raise DNS_Soft_Error((qname, qtype, ip, rcode))
                    else:
                        return reply
            except OSError, why:
                self.log('DNS.NETWORK_ERROR', why, ip, qname)
                raise DNS_Soft_Error((qname, qtype, ip, str(why)))
            except EOFError:
                # recv_exact will raise EOFError if it doesn't get enough data.
                self.log('DNS.NETWORK_ERROR', 'EOF', ip, qname)
                raise DNS_Soft_Error(qname, qtype, ip, 'EOF')
        finally:
            s.close()

    def do_query(self, q, ip):
        """do_query(self, q, ip)
        Execute a request to the given IP.
        q is a query tuple of (qname, qtype, qclass, recursion)
        This will generate the query ID.
        Returns a dns_reply object.
        """
        if self.outstanding_sem.avail <= 0:
            # we've hit a system resource limit
            # we use max_outstanding twice here because the log message wants "current" and "max"
            # but these are always going to be the same in the dns server
            self.log('DNS.LIMIT', self.max_outstanding, self.max_outstanding)
        self.outstanding_sem.acquire(1)
        try:
            qid = dns_random (65536)
            reply = self.send_request ((qid, ip, q))
            if reply.tc:
                # truncated, try tcp
                qid = dns_random (65536)
                return self.send_request ((qid, ip, q), use_tcp=1)
            else:
                return reply
        finally:
            self.outstanding_sem.release(1)

    # ==================================================
    # resolver algorithm, cache management
    # ==================================================

    def bootstrap_cache (self):
        """bootstrap_cache(self)
        Bootstrap the cache.  Seeds the initial root servers
        into the cache starting with root_hints."""
        if self.need_bootstrapping == 2:
            # bootstrapping is already in progress
            # wait for it to finish
            # 60 seconds should be a good timeout to prevent an infinite hang
            try:
                coro.with_timeout (60, self.bootstrap_cv.wait)
            except coro.TimeoutError:
                pass
            if self.need_bootstrapping:
                # the bootstrapping process failed
                self.log('DNS.BOOTSTRAP_FAILED')
                raise DNS_Soft_Error(('', 'NS', 'bootstrapping', 'Failed to bootstrap the DNS cache.'))
        elif self.need_bootstrapping == 1:
            # need to bootstrap the cache
            self.need_bootstrapping = 2
            try:
                try:
                    self._bootstrap_cache()
                except:
                    self.need_bootstrapping = 1
                    raise
            finally:
                self.bootstrap_cv.wake_all()

    def _bootstrap_cache (self):
        if self.parent_ns:
            self.need_bootstrapping = 0
            return

        self.log('DNS.STRAPPING')
        # XXX try them all w/timeout schedule...
        for ip in permute (root_hints):
            try:
                result = self.query_by_ip ('', 'NS', ip, 5.0, Work())
                # Check that the reply is kosher.  We have seen situations
                # where root server responses had an empty reply.
                if not result.an or not result.ar:
                    continue
                d = {}
                for kind, rname, ttl, data in result.an:
                    d[data] = ttl
                records_with_matching_glue = []
                for kind, rname, ttl, data in result.ar:
                    if d.has_key (rname) and kind in ('A', 'AAAA'):
                        # record glue with TTL=0 (never expires)
                        self.encache (rname, kind, [(0, data)], permanent=1)
                        records_with_matching_glue.append(rname)
                if not records_with_matching_glue:
                    # Hm, none of the glue matches the answer.  I've never
                    # seen this, but might as well guard against it.
                    continue
                # Only record entries that had matching glue records.
                # Otherwise, we'd stick glueless NS entries into the cache,
                # and that can cause problems.
                self.encache ('', 'NS', [(0, ns) for ns in records_with_matching_glue], permanent=1)
                self.log('DNS.STRAPPED')
                self.need_bootstrapping = 0
                return
            except DNS_Error:
                pass
            except coro.TimeoutError:
                pass
        self.log('DNS.BOOTSTRAP_FAILED')
        raise DNS_Soft_Error(('', 'NS', 'bootstrapping', 'Failed to bootstrap the DNS cache.'))

    def authoritative (self, name, base):
        "is a reply from server for <base> authoritative for <name>?"
        # 1) if equal, yes
        # 2) if a subdomain of <base>, yes.
        # 3) root servers are authoritative for everything [scary]
        return domain_suffix (base, name)

    def authoritative_ns (self, ns_name, domain):
        "check the cache to see if <ns_name> happens to be authoritative for <domain>"
        while domain:
            for ttl, name in self.cache_get (domain, 'NS', ()):
                if ns_name == name:
                    return 1
            domain = self.up_domain (domain)
        return 0

    def up_domain (self, name):
        if not name:
            raise DNS_Missing_Root_Data_Error
        i = name.find ('.')
        if i == -1:
            return ''
        else:
            return name[i + 1:]

    def merge_answers (self, key, new):
        """merge new data with data from the cache for key=(qname,qtype)"""
        old = self.cache.get (key, None)
        if old is None:
            return new
        else:
            d = {}
            for ttl, data in old:
                if data not in (CACHE_NODATA, CACHE_NXDOMAIN):
                    d[data] = ttl
            for ttl, data in new:
                d[data] = ttl
            return [(ttl, data) for (data, ttl) in d.items()]

    def encache (self, qname, qtype, answers, permanent=0, stomp=0):
        """encache (self, qname, qtype, answers, permanent=0)
        Add qname/qtype with results 'answers' into the cache.
        If permanent, the value is permanent in the cache."""
        if self.debug:
            self.log('DNS.ENCACHE', qname, qtype, answers)
        key = (qname, qtype)
        if permanent:
            self.cache.pin (key, answers)
        else:
            if stomp:
                self.cache[key] = answers
            else:
                self.cache[key] = self.merge_answers (key, answers)

    def _cache_get (self, key, instead):
        # This function acts as a ttl-expiring front end to self.cache.get().
        # If a record exists but has a ttl that has expired, this
        # function will expire it and pretend it never existed.  This
        # simplifies the work in the 'real' cache_get() method.
        probe = self.cache.get (key, instead)
        if probe is instead:
            return instead
        else:
            # check for any expired TTL's.  If we find one, act as if
            # every record has expired. [XXX justify this policy]
            for ttl, data in probe:
                # TTL of zero means it's a permanent entry
                if ttl and (coro.now > ttl):
                    cache_expired.inc()
                    # blow away stale data
                    del self.cache[key]
                    # pretend the record was never there
                    return instead
            else:
                # nothing expired, return the record
                return probe

    def cache_get (self, qname, qtype, instead=None, work=None):
        """cache_get (self, qname, qtype, instead)
        Returns the value in the cache for qname/qtype.
        If it isn't in the cache, it returns 'instead'.
        """
        # coro.print_stderr ('cache_get(%r,%r)\n' % (qname, qtype))
        if work is None:
            work = Work()
        work.incr (qname, qtype)
        cname_probe = self._cache_get ((qname, 'CNAME'), instead)
        if cname_probe is not instead:
            # follow CNAME in the cache as well
            ttl, cname = cname_probe[0]
            return self.cache_get (cname, qtype, instead, work)
        else:
            val = self._cache_get ((qname, qtype), instead)
            if val is instead:
                cache_miss.inc()
                return instead
            else:
                cache_hit.inc()
                ttl0, data0 = val[0]
                if data0 is CACHE_NXDOMAIN:
                    # negative cache
                    cache_exception.inc()
                    raise DNS_Hard_Error((qname, qtype, (packet.RCODE.NXDomain, 'NXDomain')))
                elif data0 is CACHE_NODATA:
                    # negative cache
                    # [XXX in what way does this qualify as an exception?]
                    cache_exception.inc()
                    return []
                else:
                    return val

    # 0 - no
    # 1 - yes
    # 2 - in progress
    need_bootstrapping = 1

    def best_nameserver (self, qname):
        "the best nameserver for <qname> in our cache"
        if self.need_bootstrapping:
            self.bootstrap_cache()
        while 1:
            ns = self.cache_get (qname, 'NS', None)
            if ns is None:
                # will terminate with qname == ''
                qname = self.up_domain (qname)
            else:
                return ns, qname

    def _pick_parent_ns_fast(self, domain):
        # This is a performance hack.  Most people do not override domains in
        # their dns configuration.  No need to spin cycles doing the
        # "up_domain" thing.  This function is dynamically replaced via the
        # configuration management code in PrioritizedIP.py.
        return self.parent_ns['']

    def _pick_parent_ns_slow(self, domain):
        while 1:
            if domain in self.parent_ns:
                return self.parent_ns[domain]
            # This will raise an exception if domain=='' which should never
            # happen because parent_ns should always have a '' entry.
            domain = self.up_domain(domain)

    _pick_parent_ns = _pick_parent_ns_slow

    def query (self, qname, qtype, work=None, no_cache=0, timeout=None):
        """query (self, qname, qtype, work=[0], no_cache=0)
        Return the result [(ttl, value),...] of qname/qtype
        timeout supplies a manual override to generated timeouts based on number of servers
        """
        qname = qname.lower()
        # This function doesn't support the 'ANY' query.  use query_by_{name,ip} instead.
        # Why, you say? because ANY returns *many* types of records,
        #  whereas this method returns a *specific* type of record.  In other words,
        #  query_by_{name,ip}() return a <dns_reply> object, but this method returns
        #  a list of answers (i.e., <dns_reply.an>).
        if work is None:
            dns_request.inc()
            work = Work()
        if self.debug:
            self.log ('DNS.QUERY', 'Q', work.indent(), "%r, %r" % (qname, qtype))
        if not packet.dot_sane (qname):
            raise DNS_Malformed_Qname_Error((qname, qtype, (packet.RCODE.ServFail, '')))
        if not no_cache:
            results = self.cache_get (qname, qtype, None)
            if results is not None:
                return results
        # what's the best NS we have in the cache?
        if self.parent_ns:
            ns_names = self._pick_parent_ns(qname)
            zone = ""
        elif work.glue > GLUELESSNESS:
            # once we go past our tolerance for gluelessness, force
            # things to start back at the root...
            self.log ('DNS.GLUELESSNESS', qname, qtype)
            ns_names, zone = self.best_nameserver ('')
            work.glue = 0
        else:
            ns_names, zone = self.best_nameserver (qname)

        last_dns_soft_error = None
        while 1:
            r = None
            lame_count = 0

            t = timeouts (len(ns_names))
            to = timeout

            if not self.parent_ns:
                # Don't permute when using recursive resolvers since they are
                # ordered in a specific prioritized fashion.
                ns_names = permute (ns_names)
            for ttl, ns_name in ns_names:
                # timeout override check
                if timeout is None:
                    to = t.next()

                try:
                    r = self.query_by_name (qname, qtype, ns_name, to, work)

                except DNS_Soft_Error, why:
                    # try the next nameserver...
                    last_dns_soft_error = why

                except DNS_Hard_Error:
                    # nameserver doesn't exist. dripping in lameness.
                    lame_count += 1

                except DNS_Lame_Error:
                    # all the nameservers for *this* nameserver are lame.
                    lame_count += 1

                else:
                    cname = None
                    soa = None
                    ref_zone = None
                    results = []
                    referrals = []

                    # collect answers...
                    for kind, rname, ttl, data in r.an:
                        if kind == qtype and self.authoritative (rname, zone):
                            results.append ((ttl, data))
                        elif kind == 'CNAME':
                            cname = ttl, data

                    # collect referrals

                    for kind, rname, ttl, data in r.ns:
                        # should we check that all <rname> are equal?
                        if kind == 'NS':
                            if ref_zone is None:
                                # coro.print_stderr ('ref %s %s\n' % (repr(rname), repr(data)))
                                ref_zone = rname
                                referrals.append ((ttl, data))
                            elif ref_zone == rname:
                                referrals.append ((ttl, data))
                            else:
                                self.log('DNS.ODD_AUTHORITY', r.ns)
                        elif kind == 'SOA':
                            # XXX should we cache SOA?
                            soa = rname, ttl, data

                    # scan the additional records for glue...
                    for kind, rname, ttl, data in r.ar:
                        if kind in ('A', 'AAAA'):
                            if self.authoritative (rname, zone):
                                if self.debug:
                                    self.log('DNS.GLUE.ACCEPT', rname, zone, data, ns_name)
                                self.encache (rname, kind, [(ttl, data)])
                            elif self.authoritative_ns (ns_name, rname):
                                if self.debug:
                                    self.log('DNS.GLUE.ACCEPT.LUCKY', rname, zone, data, ns_name)
                                self.encache (rname, kind, [(ttl, data)])
                            else:
                                if self.debug:
                                    self.log('DNS.GLUE.DENY', rname, zone, data, ns_name)

                    # 1.``The query was not answered because the query name is an alias. I need to
                    # change the query name and try again.'' This applies if the answer section of the
                    # response contains a CNAME record for the query name and CNAME does not match the
                    # query type.

                    if cname and qtype != 'CNAME':
                        self.encache (qname, 'CNAME', [cname])
                        if self.debug:
                            self.log('DNS.FOLLOWING_CNAME', qname, cname)
                        return self.query (cname[1], qtype, work)

                    # 2.``The query name has no records answering the query, and is also guaranteed to
                    # have no records of any other type.'' This applies if the response code is NXDOMAIN
                    # and #1 doesn't apply. The amount of time that this information can be cached
                    # depends on the contents of the SOA record in the authority section of the
                    # response, if there is one.

                    elif not results and r.rcode == packet.RCODE.NXDomain:
                        if soa:
                            self.encache (qname, qtype, [(soa[1], CACHE_NXDOMAIN)], stomp=1)
                        elif self.negative_ttl:
                            ttl = (self.negative_ttl * coro.ticks_per_sec) + coro.now
                            self.encache (qname, qtype, [(ttl, CACHE_NXDOMAIN)], stomp=1)
                        if self.debug:
                            self.log('DNS.NXDOMAIN', qname, qtype)
                        raise DNS_Hard_Error((qname, qtype, (r.rcode, packet.RCODE_MAP[r.rcode])))

                    # 3.``The query name has one or more records answering the query.''  This applies if
                    # the answer section of the response contains one or more records under the query
                    # name matching the query type, and #1 doesn't apply, and #2 doesn't apply.

                    elif results:
                        self.encache (qname, qtype, results)
                        return results

                    # 4.``The query was not answered because the server does not have the answer. I need
                    # to contact other servers.'' This applies if the authority section of the response
                    # contains NS records, and the authority section of the response does not contain
                    # SOA records, and #1 doesn't apply, and #2 doesn't apply, and #3 doesn't apply. The
                    # ``other servers'' are named in the NS records in the authority section.

                    elif referrals and not soa:

                        # lame example:
                        # we ask sca02.sec.dns.exodus.net, which is supposed to be an NS for briefcase.com
                        # it replies with a referral to 'com', and gives us the gtld servers.
                        # djb detects this with:
                        # if (dns_domain_equal(referral,control) || !dns_domain_suffix(referral,control)) {...}

                        # 'unlameness'.  sometimes queries will get here and show up as lame when they're
                        # not.  An SOA query will appear lame if the SOA really belongs to a parent of
                        # of qname.  The SOA will appear in the authority section correctly tagged with the
                        # parent domain name.  Special case we're not going to need since we don't normally
                        # do SOA queries.

                        if (zone == ref_zone or not domain_suffix (zone, ref_zone)):
                            lame_count += 1
                            self.log('DNS.LAME_REFERRAL', ns_name, zone, ref_zone, referrals, qname)
                            # now go to the next name server, i.e., continue in for ns_name...
                        else:
                            self.encache (ref_zone, 'NS', referrals)
                            ns_names, zone = referrals, ref_zone
                            # break out of ns_names loop so we can follow the referral
                            break

                    # 5.``The query name has no records answering the query, but it may have records of
                    # another type.'' This applies if #1 doesn't apply, and #2 doesn't apply, and #3
                    # doesn't apply, and #4 doesn't apply. The amount of time that this information can
                    # be cached depends on the contents of the SOA record in the authority section, if
                    # there is one.

                    else:
                        if soa:
                            self.encache (qname, qtype, [(soa[1], CACHE_NODATA)], stomp=1)
                        elif self.negative_ttl:
                            ttl = (self.negative_ttl * coro.ticks_per_sec) + coro.now
                            self.encache (qname, qtype, [(ttl, CACHE_NODATA)], stomp=1)
                        return []
            else:
                if lame_count == len(ns_names):
                    # all servers were lame (either lame delegation or DNS Hard Error)
                    raise DNS_Lame_Error (qname, qtype, ns_names)
                else:
                    # None of the servers worked...at least 1 return DNS Soft Error)
                    if last_dns_soft_error is None:
                        ns_names_list = ', '.join ([x[1] for x in ns_names])
                        raise DNS_Soft_Error (qname, qtype, ns_names_list, 'All nameservers failed.')
                    else:
                        raise last_dns_soft_error

    def query_by_name (self, qname, qtype, ns_name, timeout, work):
        """query_by_name (self, qname, qtype, ns_name, timeout, work)
        Send the query qname/qtype to the nameserver 'ns_name'.
        Returns a dns_reply object.
        """

        if self.debug:
            self.log('DNS.QUERY', 'QN', work.indent(), "%r, %r, %r" % (qname, qtype, ns_name))
        work.incr (qname, qtype)
        # do we have this glue in the cache?
        addrs = self.cache_get (ns_name, 'A', None)

        # XXX IPv6 caching the root servers and/or TLD servers doesn't work
        # right. We end up in infinite recursion trying to find AAAA results
        # for hosts that don't actually have AAAAs. The world eventually
        # rights itself but not after a lot of trying. Need to do something
        # better here.
        # v6_addrs = self.cache_get (ns_name, 'AAAA', None)

        if addrs is None:
            work.incr_glue()
            addrs = self.query (ns_name, 'A', work, no_cache=True)
        # if v6_addrs is None:
            # work.incr_glue()
            # print "up", qname, "but no entry for ns server", ns_name
            # v6_addrs = self.query (ns_name, 'AAAA', work, no_cache=True)
            # print "6 Lookup glue for", ns_name, "got results", repr(v6_addrs)

        if not addrs:
            raise DNS_Soft_Error (qname, qtype, ns_name, 'Empty reply for NS IP.')

        for ttl, ip in permute (addrs):
            # Don't send queries to 127.xx or 0.xx.  On a production system we
            # have our resolver listening on 127.0.0.1.  If a hostname has
            # a nameserver with the IP of 127.0.0.1, this would cause a flurry
            # of queries going to ourself.
            if not ip.startswith('127.') and not ip.startswith('0.'):
                try:
                    return self.query_by_ip (qname, qtype, ip, timeout=timeout, work=work)
                except coro.TimeoutError:
                    pass
                except DNS_Soft_Error:
                    pass

        raise DNS_Soft_Error (qname, qtype, ns_name, 'unable to reach nameserver on any valid IP')

    def query_by_ip (self, qname, qtype, ip, timeout, work):
        """query_by_ip (self, qname, qtype, ip, timeout, work)
        Send the query qname/qtype to the nameserver 'ip'.
        Returns a dns_reply object.
        """
        if self.debug:
            self.log('DNS.QUERY', 'QIP', work.indent(), "%r,%r,%r,%d" % (qname, qtype, ip, int(timeout)))
        if self.parent_ns:
            recursion = 1
        else:
            recursion = 0
        q = (qname, qtype, packet.CLASS.IN, recursion)
        return coro.with_timeout (timeout, self.do_query, q, ip)

    def __delitem__ (self, key):
        del self.cache[key]

def permute (x):
    if len(x) == 1:
        return x
    else:
        x0 = list (x)
        random.shuffle (x0)
        return x0

import coro
import coro.dns
import coro.dns.packet as packet
from coro.dns.surf import dns_random, set_seed
import random
from coro import tb
import coro
import socket
from coro.dns.exceptions import *
from coro.lru import lru_with_pin
from coro.dns.reply import unpack_reply

import os
set_seed (os.urandom (128))

if not coro.has_ipv6():
    root_hints = [x for x in root_hints if ':' not in x]

class resolver:
    def __init__ (self):
        self.cache = dns_cache()

    def gethostbyname (self, name, qtype):
        for ttl, addr in permute (self.cache.query (name, qtype)):
            return addr

    def resolve_ipv4 (self, name):
        return self.gethostbyname (name, 'A')

    def resolve_ipv6 (self, name):
        return self.gethostbyname (name, 'AAAA')

def install():
    "install the builtin resolver into the coro socket layer"
    coro.set_resolver (resolver())

# emulate 'statsmon' module
class StaticCounter:
    def __init__ (self, name):
        self.name = name
        self.val = 0

    def inc (self):
        self.val += 1

    def __repr__ (self):
        return '<counter %r %r>' % (self.name, self.val)

dns_request     = StaticCounter('dns_request')
net_request     = StaticCounter('net_request')
cache_hit       = StaticCounter('cache_hit')
cache_miss      = StaticCounter('cache_miss')
cache_exception = StaticCounter('cache_exception')
cache_expired   = StaticCounter('cache_expired')

if __name__ == '__main__':
    import backdoor
    d = dns_cache()
    d.debug = True
    coro.spawn (backdoor.serve)
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = exceptions
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# dns_exceptions
#
# These are all the excpetions that can be raised by the DNS module.
#

import coro.dns.packet as packet

MAX_DNSRCODE = 65535

class DNS_Error(Exception):
    """Base class for DNS-related errors in this file.
    Do NOT use this directly!
    """
    pass

class DNS_Soft_Error (DNS_Error):
    def __init__(*args):
        """DNS_Soft_Error(qname, qtype, ip, error_string)"""
        DNS_Error.__init__ (*args)
        self, self.qname, self.qtype, self.nameserver, self.error_string = args

    def __str__(self):
        return 'DNS Soft Error looking up %s (%s) while asking %s. Error was: %s' % (
            self.qname, self.qtype, self.nameserver, self.error_string
        )

class DNS_Hard_Error (DNS_Error):
    def __init__(*args):
        """DNS_Hard_Error(qname, qtype, (dnsrcode, error_string))"""
        DNS_Error.__init__ (*args)
        self, self.qname, self.qtype, (self.dnsrcode, self.error_string) = args

    def __str__(self):
        return 'DNS Hard Error looking up %s (%s):  %s' % (
            self.qname, self.qtype, self.error_string
        )

class DNS_Many_Errors (DNS_Error):
    """A container class for multiple DNS errors.

    When looking up MX records we often have to look up A records and AAAA
    records. If one of the queries succeeds and one fails then we use the
    success result and carry on as normal.

    If both queries fail we need a way to communicate to the rest of the
    system (and mainly to log files) that there were two queries attempted and
    they both failed.
    """

    def __init__(self, exceptions):
        """Initialize ourselves.

        :Parameters:
            - `exceptions`: A list of exception instances.
        """
        self.exceptions = exceptions

    attributes_to_join = ('qname', 'qtype', 'error_string')

    def __getattr__(self, attrname):
        """If the attribute being requested is in `attributes_to_join` then
        request that attribute from all of the contained exceptions and
        join them with a newline. For any other attribute, the value
        returned is from the least severe exception."""

        self.exceptions = sorted(self.exceptions, key=self._exception_key_func)
        attr_values = [getattr(e, attrname) for e in self.exceptions if hasattr(e, attrname)]

        if not attr_values:
            raise AttributeError('None of \'%s\' inner exceptions has attribute \'%s\'' %
                                 (self.__class__.__name__, attrname))
        elif attrname in self.attributes_to_join:
            return '\n'.join(attr_values)
        else:
            return attr_values[0]

    def __str__(self):
        """Return all of the exception strings separated by newline."""
        exception_strings = []
        for e in self.exceptions:
            exception_strings.append(str(e))

        return 'Multiple DNS queries were attempted and failed: ' + \
            '\n'.join(exception_strings)

    def _exception_key_func(self, e):
        """This function helps to sort the contained exceptions in DNS_Many_Errors
        based on severity.

        If the given exception is not of type DNS_Hard_Error (no 'dnsrcode'
        attribute'), e.g. DNS_Soft_Error, it returns 0 so that it is treated as
        the least severe type. If the given exception is of type DNS_Hard_Error,
        then the value of the 'dnsrcode' attribute is evaluated. If the value is
        equal to dnsrcode.Refused then it returns MAX_DNSRCODE so that it is
        treated as the most severe type. NXDomain is considered as the next most
        severe type."""

        rcode = 0
        if hasattr(e, 'dnsrcode'):
            rcode = getattr(e, 'dnsrcode')
            if rcode == packet.RCODE.NXDomain:
                rcode = MAX_DNSRCODE - 1
            if rcode == packet.RCODE.Refused:
                rcode = MAX_DNSRCODE

        return rcode

class DNS_Many_Errors_Soft(DNS_Many_Errors, DNS_Soft_Error):
    pass

class DNS_Many_Errors_Hard(DNS_Many_Errors, DNS_Hard_Error):
    pass

class DNS_Lame_Error (DNS_Soft_Error):
    def __init__ (self, qname, qtype, ns_names):
        """DNS_Lame_Error(qname, qtype, ns_names)"""
        self.ns_names = ', '.join(map(lambda x: x[1], ns_names))
        DNS_Soft_Error.__init__ (
            self, qname, qtype, self.ns_names, 'Lame Delegation Error'
        )

    def __str__(self):
        return (
            'DNS Lame Delegation Error looking up %s (%s).'
            '  Nameserver list was: %s.' % (
                self.qname, self.qtype, self.ns_names
            )
        )

class DNS_Runaway_Query_Error(DNS_Soft_Error):
    def __init__(self, qname, qtype, ns_name):
        """DNS_Runaway_Query_Error(qname, qtype, ns_name)"""
        self.ns_name = ns_name
        DNS_Soft_Error.__init__ (
            self, qname, qtype, ns_name, 'Runaway Error'
        )

    def __str__(self):
        return (
            'DNS Runaway Error looking up %s (%s).'
            '  Nameserver was: %s.' % (self.qname, self.qtype, self.ns_name)
        )

class DNS_Malformed_Qname_Error (DNS_Hard_Error):
    def __str__(self):
        return 'DNS Malformed Query Error looking up %s (%s).' % (self.qname, self.qtype)

class DNS_Missing_Root_Data_Error (DNS_Soft_Error):
    def __init__(self):
        DNS_Soft_Error.__init__ (self, '', '', '', 'Missing Root Data Error')

    def __str__(self):
        return 'Failed to get root nameserver.'

class DNS_No_Local_Resolvers (DNS_Soft_Error):
    def __init__(self):
        DNS_Soft_Error.__init__ (self, '', '', '', 'No local DNS resolvers are running')

    def __str__(self):
        return 'No local DNS resolvers are running.'

########NEW FILE########
__FILENAME__ = reply
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro
import coro.dns.packet as packet

# XXX move this into packet.pyx?

class dns_reply:
    """A reply to a DNS query.
    self.rcode -> The rcode of the query.
    self.q     -> List of question tuples (qname, qtype, qclass)
    self.an    -> List of answer tuples (typename, name, ttl, DATA)
    self.ns    -> List of nameserver tuples (typename, name, ttl, DATA)
    self.ar    -> List of additional record tuples (typename, name, ttl, DATA)
    DATA depends on the type of the RR entry.
    """

    def __init__ (self):
        self.rcode = 0
        self.q  = []
        self.an = []
        self.ns = []
        self.ar = []
        self.aa = 0
        self.id = 0
        self.tc = 0

    def __repr__ (self):
        return '<dns_reply rcode=%d q:%s an:%s ns:%s ar:%s>' % (
            self.rcode, self.q, self.an, self.ns, self.ar
        )


def get_rr (u, use_actual_ttl=0, ttl_min=1800):
    """get_rr (u, use_actual_ttl=0)
    Unpack and return an RR entry as
    (typename, name, ttl, DATA)
    Where DATA depends on the type of the RR.
    <use_actual_ttl>, if true, avoids using the minimum TTL.
    <min_ttl>, minimum TTL.
    """
    name, type, klass, ttl, rdlength = u.getRRheader()

    if klass != packet.CLASS.IN:
        return None
    else:
        typename = packet.TYPE_MAP[type]
        mname = 'get%sdata' % typename
        # convert ttl from relative to absolute
        # minimum ttl is <ttl_min> mins

        if not use_actual_ttl:
            ttl = max(ttl, ttl_min)

        ttl = (ttl * coro.ticks_per_sec) + coro.now
        if hasattr (u, mname):
            return (typename, name, ttl, getattr(u, mname)())
        else:
            return (typename, name, ttl, u.getbytes(rdlength))

def unpack_reply (reply, use_actual_ttl=0, ttl_min=1800):
    """unpack_reply (reply)
    Given a reply packet, it returns a dns_reply object."""
    u = packet.Unpacker(reply)
    h = u.getHeader()
    # ID - 16-bit identifier
    # QR - boolean query=0 response=1
    # AA - authoritative answer
    # TC - Truncation
    # RD - Recursion Desired
    # RA - Recursion Available
    # Z  - Reserved
    r = dns_reply()
    r.rcode = h.rcode
    r.aa = h.aa
    r.id = h.id
    r.tc = h.tc
    if h.tc:
        # don't bother trying to unpack the rest of the response, it's
        # likely to be mangled.
        pass
    else:
        __pychecker__ = 'unusednames=x'
        r.q  = [u.getQuestion() for x in range (h.qdcount)]
        r.an = filter (None, [get_rr (u, use_actual_ttl, ttl_min) for x in range (h.ancount)])
        r.ns = filter (None, [get_rr (u, use_actual_ttl, ttl_min) for x in range (h.nscount)])
        r.ar = filter (None, [get_rr (u, use_actual_ttl, ttl_min) for x in range (h.arcount)])
    return r

########NEW FILE########
__FILENAME__ = stub_resolver
# -*- Mode: Python -*-

import coro
import coro.dns
import coro.dns.packet as packet

import random

class QueryFailed (Exception):
    pass

class stub_resolver:

    def __init__ (self, nameservers, inflight=200):
        self.nameservers = nameservers
        self.inflight = coro.semaphore (inflight)
        self.inflight_ids = set()

    def lookup (self, qname, qtype, timeout=10, retries=3):
        m = packet.Packer()
        h = packet.Header()
        while 1:
            qid = random.randrange (65536)
            # avoid collisions
            if qid not in self.inflight_ids:
                break
        h.id = qid
        h.opcode = packet.OPCODE.QUERY
        h.rd = 1
        h.qdcount = 1
        m.addHeader (h)
        m.addQuestion (qname, qtype, packet.CLASS.IN)
        p = m.getbuf()
        for addr in self.nameservers:
            for i in range (retries):
                self.inflight.acquire (1)
                self.inflight_ids.add (qid)
                try:
                    s = coro.udp_sock()
                    s.connect ((addr, 53))
                    s.send (p)
                    try:
                        reply = coro.with_timeout (timeout, s.recv, 1000)
                        u = packet.Unpacker (reply)
                        result = u.unpack()
                        rh = result[0]
                        if rh.id != qid:
                            raise QueryFailed ("bad id in reply")
                        else:
                            return result
                    except coro.TimeoutError:
                        pass
                finally:
                    self.inflight.release (1)
                    self.inflight_ids.remove (qid)

        raise QueryFailed ("no reply from nameservers")

    def gethostbyname (self, name, qtype):
        header, qdl, anl, nsl, arl = self.lookup (name, qtype)
        for answer in anl:
            name, rtype, _, ttl, addr = answer
            if getattr (packet.TYPE, rtype) == qtype:
                return addr
        else:
            raise QueryFailed ("no answer in nameserver reply")

    def resolve_ipv4 (self, name):
        return self.gethostbyname (name, packet.TYPE.A)

    def resolve_ipv6 (self, name):
        return self.gethostbyname (name, packet.TYPE.AAAA)

def install (nameserver_ips):
    "install a stub resolver into the coro socket layer"
    coro.set_resolver (
        stub_resolver (nameserver_ips)
    )

########NEW FILE########
__FILENAME__ = tpack
# -*- Mode: Python -*-

import coro
import coro.dns.packet as dns

def testpacker():
    # See section 4.1.4 of RFC 1035
    p = dns.Packer()
    p.addbytes('*' * 20)
    p.addname('f.ISI.ARPA')
    p.addbytes('*' * 8)
    p.addname('Foo.F.isi.arpa')
    p.addbytes('*' * 18)
    p.addname('arpa')
    p.addbytes('*' * 26)
    p.addname('')
    packet = p.getbuf()
    assert packet == (
        '********************\x01f\x03ISI\x04ARPA\x00'
        '********\x03Foo\xc0\x14******************\xc0\x1a'
        '**************************\x00'
    )
    u = dns.Unpacker (packet)
    res = (
        u.getbytes(20),
        u.getname(),
        u.getbytes(8),
        u.getname(),
        u.getbytes(18),
        u.getname(),
        u.getbytes(26),
        u.getname(),
    )
    assert res == (
        '********************',
        'f.isi.arpa',
        '********',
        'foo.f.isi.arpa',
        '******************',
        'arpa',
        '**************************',
        ''
    )

def test_packer_2 ():
    p = dns.Packer()
    h = dns.Header()
    h.id = 3141
    h.opcode = dns.OPCODE.QUERY
    h.rd = 0
    h.ancount = 1
    h.arcount = 1
    h.qdcount = 1
    p.addHeader (h)
    p.addQuestion ('glerg.org', dns.TYPE.CNAME, dns.CLASS.IN)
    p.addCNAME ('glerg.org', dns.CLASS.IN, 3000, 'blerb.com')
    p.addHINFO ('brodig.com', dns.CLASS.IN, 5000, 'vax', 'vms')
    data = p.getbuf()
    u = dns.Unpacker (data)
    h, qdl, anl, nsl, arl = u.unpack()
    assert qdl == [('glerg.org', 5, 1)]
    assert anl == [('glerg.org', 'CNAME', 'IN', 3000, 'blerb.com')]
    assert arl == [('brodig.com', 'HINFO', 'IN', 5000, ('vax', 'vms'))]
    assert nsl == []
    assert h.id == 3141
    assert h.opcode == dns.OPCODE.QUERY
    assert h.ancount == 1
    assert h.qdcount == 1

def t0 (qname='www.nightmare.com', qtype=dns.TYPE.A):
    m = dns.Packer()
    h = dns.Header()
    h.id = 3141
    h.opcode = dns.OPCODE.QUERY
    h.rd = 1
    h.qdcount = 1
    m.addHeader (h)
    m.addQuestion (qname, qtype, dns.CLASS.IN)
    p = m.getbuf()
    return p

def t1 (qname, qtype):
    p = t0 (qname, getattr (dns.TYPE, qtype))
    s = coro.udp_sock()
    s.connect (('192.168.200.1', 53))
    s.send (p)
    r = s.recv (8192)
    coro.write_stderr ('reply=%r\n' % (r,))
    u = dns.Unpacker (r)
    return u.unpack()

def t2():
    import coro.dns.stub_resolver
    r = coro.dns.stub_resolver.stub_resolver (['192.168.200.1'])
    coro.set_resolver (r)

# XXX make this into a real unit test.
if __name__ == '__main__':
    # import coro.backdoor
    # coro.spawn (coro.backdoor.serve, unix_path='/tmp/xx.bd')
    # coro.event_loop()
    test_packer_2()

########NEW FILE########
__FILENAME__ = socket
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# $Header: //prod/main/ap/shrapnel/coro/emulation/socket.py#1 $

"""Emulation of Python's socket module in a coro app.

See Python's documentation for the socket module for details.
"""

from __future__ import absolute_import

import os
import socket as _socketmodule

import coro

##############################################################################
# Exceptions
##############################################################################
error = _socketmodule.error
herror = _socketmodule.herror
gaierror = _socketmodule.gaierror
timeout = _socketmodule.timeout

##############################################################################
# Constants
##############################################################################
g = globals()
for name in _socketmodule.__all__:
    value = getattr(_socketmodule, name)
    if isinstance(value, (int, long)):
        g[name] = value
del g, name, value

try:
    BDADDR_ANY = _socketmodule.BDADDR_ANY
    BDADDR_LOCAL = _socketmodule.BDADDR_LOCAL
except AttributeError:
    pass

##############################################################################
# Timeout.
##############################################################################
_defaulttimeout = None

def getdefaulttimeout():
    return _defaulttimeout

def setdefaulttimeout(timeout):
    global _defaulttimeout
    if timeout is not None and timeout < 0:
        raise ValueError('Invalid timeout')
    _defaulttimeout = timeout

##############################################################################
# Things not currently emulated.
##############################################################################

getfqdn = _socketmodule.getfqdn
gethostbyname = _socketmodule.gethostbyname
gethostbyname_ex = _socketmodule.gethostbyname_ex
gethostbyaddr = _socketmodule.gethostbyaddr
getaddrinfo = _socketmodule.getaddrinfo
getnameinfo = _socketmodule.getnameinfo

##############################################################################
# Things that don't need emulation.
##############################################################################
getservbyname = _socketmodule.getservbyname
getservbyport = _socketmodule.getservbyport
getprotobyname = _socketmodule.getprotobyname
gethostname = _socketmodule.gethostname
ntohs = _socketmodule.ntohs
ntohl = _socketmodule.ntohl
htons = _socketmodule.htons
htonl = _socketmodule.htonl
inet_aton = _socketmodule.inet_aton
inet_ntoa = _socketmodule.inet_ntoa
inet_pton = _socketmodule.inet_pton
inet_ntop = _socketmodule.inet_ntop

##############################################################################
# Socket object.
##############################################################################
class _ErrorConverter(object):

    def __enter__(self):
        pass

    def __exit__(self, exc_type, exc_value, exc_tb):
        if exc_value is not None:
            if isinstance(exc_value, OSError):
                raise error(exc_value.errno, exc_value.strerror)
            # Otherwise, raise original exception.

_error_converter = _ErrorConverter()

class socket(object):

    def __init__(self, family=AF_INET, type=SOCK_STREAM, proto=0, _sock=None):
        if _sock is None:
            self._sock = coro.sock(family, type, proto)
        else:
            self._sock = _sock
        self.family = family
        self.type = type
        self.proto = proto
        self.timeout = _defaulttimeout

    def _with_timeout(self, func, *args, **kwargs):
        if self.timeout:
            try:
                return coro.with_timeout(self.timeout, func, *args, **kwargs)
            except coro.TimeoutError:
                raise timeout('Timed out')
        else:
            return func(*args, **kwargs)

    def accept(self):
        with _error_converter:
            sock, addr = self._with_timeout(self._sock.accept)
            return socket(sock.domain, sock.stype, 0, _sock=sock), addr

    def bind(self, address):
        with _error_converter:
            self._sock.bind(address)

    def close(self):
        with _error_converter:
            self._sock.close()

    def connect(self, address):
        with _error_converter:
            self._with_timeout(self._sock.connect, address)

    def connect_ex(self, address):
        try:
            self._with_timeout(self._sock.connect, address)
        except OSError, e:
            return e.errno
        else:
            return 0

    def dup(self):
        return socket(self.family, self.type, self.proto, self._sock)

    def fileno(self):
        return self._sock.fileno()

    def getpeername(self):
        with _error_converter:
            return self._sock.getpeername()

    def getsockname(self):
        with _error_converter:
            return self._sock.getsockname()

    def getsockopt(self, level, optname, buflen=0):
        with _error_converter:
            return self._sock.getsockopt(level, optname, buflen)

    def listen(self, backlog):
        with _error_converter:
            return self._sock.listen(backlog)

    def makefile(self, mode='r', bufsize=-1):
        # We use dup() here because we don't use the dummy socket concept
        # to handle file descriptor reference counting (which Python uses as
        # a cross-platform issue).
        return _socketmodule._fileobject(self.dup(), mode, bufsize, True)

    def recv(self, bufsize, flags=0):
        if flags != 0:
            raise AssertionError('Setting flags not yet supported.')
        with _error_converter:
            return self._with_timeout(self._sock.recv, bufsize)

    def recv_into(self, buffer, nbytes=0, flags=0):
        return self._sock.recv_into(buffer, nbytes, flags)

    def recvfrom(self, bufsize, flags=0):
        with _error_converter:
            return self._with_timeout(self._sock.recvfrom, bufsize, flags)

    def recvfrom_into(self, buffer, nbytes=0, flags=0):
        return self._sock.recvfrom_into(buffer, nbytes, flags)

    def send(self, data, flags=0):
        # XXX: Difference: this will call send again if not all data was sent.
        if flags != 0:
            raise AssertionError('Setting flags not yet supported.')
        with _error_converter:
            return self._with_timeout(self._sock.send, data)

    def sendall(self, data, flags=0):
        if flags != 0:
            raise AssertionError('Setting flags not yet supported.')
        with _error_converter:
            return self._with_timeout(self._sock.sendall, data)

    def sendto(self, data, flags_or_address, maybe_address=None):
        # Dumb API
        if maybe_address is None:
            address = flags_or_address
            flags = 0
        else:
            address = maybe_address
            flags = flags_or_address
        with _error_converter:
            return self._with_timeout(self._sock.sendto, data, address, flags)

    def setblocking(self, flag):
        if flag:
            # blocking
            pass
        else:
            # non-blocking
            # self.timeout = 0
            raise AssertionError('Non-blocking mode not yet supported.')

    def settimeout(self, value):
        # XXX: check value for 0 or None when we support non-blocking sockets.
        self.timeout = value

    def gettimeout(self):
        return self.timeout

    def setsockopt(self, level, optname, value):
        with _error_converter:
            self._sock.setsockopt(level, optname, value)

    def shutdown(self, how):
        with _error_converter:
            self._sock.shutdown(how)

SocketType = socket


##############################################################################
# Module-level functions.
##############################################################################

_GLOBAL_DEFAULT_TIMEOUT = object()

def create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT, source_address=None):
    # This is copied directly from Python's implementation.
    msg = "getaddrinfo returns an empty list"
    host, port = address
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
        af, socktype, proto, canonname, sa = res
        sock = None
        try:
            sock = socket(af, socktype, proto)
            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:
                sock.settimeout(timeout)
            if source_address:
                sock.bind(source_address)
            sock.connect(sa)
            return sock

        except error, msg:
            if sock is not None:
                sock.close()

    raise error(msg)

def fromfd(fd, family, type, proto=0):
    with _error_converter:
        fd = os.dup(fd)
        sock = coro.sock(family, type, proto, fd=fd)
    return socket(family, type, proto, _sock=sock)

def socketpair(family=0, type=0, proto=0):
    with _error_converter:
        s1, s2 = coro.socketpair(family, type, proto)
    s1s = socket(family, type, proto, _sock=s1)
    s2s = socket(family, type, proto, _sock=s2)
    return (s1s, s2s)

# Some naughty code (urllib2 for example) use this directly.
_fileobject = _socketmodule._fileobject

########NEW FILE########
__FILENAME__ = thread
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# $Header: //prod/main/ap/shrapnel/coro/emulation/thread.py#1 $

"""Emulation of the Python ``thread`` module.

It is best to reference the Python documentation for the ``thread`` module, but
this module will provide some cursory information.
"""

import coro

class error(Exception):

    def __init__(self, *args):
        self.args = args

def start_new_thread(function, args, kwargs={}):
    """Create a new thread.

    :Parameters:
        - `function`: The function to call in the new thread.
        - `args`: Arguments to call the function with.
        - `kwargs`: Keyword arguments to call the function with.

    :Return:
        Returns the "id" of the thread.
    """
    return coro.spawn(function, *args, **kwargs).id

class LockType(object):

    """A simple mutex lock.

    XXX NOTE: This does not behave the same way as Python's lock does. It
    behaves like the threading.RLock object.  Shrapnel does not have a
    non-reentrant lock.
    """

    def __init__(self):
        self._lock = coro.mutex()

    def acquire(self, waitflag=None):
        """Acquire the lock.

        :Parameters:
            - `waitflag`: If zero, will immediately return with False if it is
              unable to acquire the lock, True if it did.  Otherwise, this will
              block until the lock can be acquired.

        :Return:
            Returns True if it was acquired, False if not (only when waitflag
            is nonzero).
        """
        if waitflag is None or waitflag:
            self._lock.lock()
            return True
        else:
            return not self._lock.trylock()

    def release(self):
        """Release the lock.

        XXX: NOTE: This deviates from the standard Python version. This will
        fail if you try to release a lock acquired by another thread.  Standard
        Python threads allow you to do that.
        """
        self._lock.unlock()

    def locked(self):
        """Determine if the lock is locked.

        :Return:
            Returns a boolean value, True if it is locked.
        """
        return self._lock.locked()

    __enter__ = acquire

    def __exit__(self, exc_type, exc_value, traceback):
        self.release()

def interrupt_main():
    """Shrapnel does not have a concept of a "main" thread.  As a compromise,
    this will exit the process with code 1.
    """
    coro.set_exit(1)

def exit():
    """Exit the current thread by raising the `coro.Shutdown` exception."""
    raise coro.Shutdown

def allocate_lock():
    """Create a new lock.

    The lock is initially unlocked.

    :Return:
        Returns a `LockType` instance.
    """
    return LockType()

def get_ident():
    """Returns the "id" of the current thread."""
    return coro.current().id

def stack_size(size=None):
    """This method does nothing, because coro uses a dynamic stack."""
    return

########NEW FILE########
__FILENAME__ = threading
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# $Header: //prod/main/ap/shrapnel/coro/emulation/threading.py#2 $

"""Emulation of the Python ``threading`` module.

It is best to reference the Python documentation for the ``threading`` module,
but this module will provide some cursory information.
"""

import sys

import coro
import coro.emulation.thread

_trace_hook = None

def settrace(func):
    global _trace_hook
    _trace_hook = func

##############################################################################
# Locking
##############################################################################
Lock = coro.emulation.thread.LockType
RLock = coro.emulation.thread.LockType

class Condition(object):

    def __init__(self, lock=None):
        if lock is None:
            lock = RLock()
        self.__lock = lock
        self.acquire = lock.acquire
        self.release = lock.release
        self.__cv = coro.condition_variable()

    def __enter__(self):
        return self.__lock.__enter__()

    def __exit__(self, *args):
        return self.__lock.__exit(*args)

    def wait(self, timeout=None):
        # This API is a little on the bizarre side.  Probably due to some
        # preemption race conditions we don't have in shrapnel.
        # XXX: This is not emulating the "save/restore" logic of the Python
        # version.  We could probably do that without too much difficulty
        # if self.lock == RLock.
        self.__lock.release()
        try:
            if timeout is None:
                self.__cv.wait()
            else:
                try:
                    coro.with_timeout(timeout, self.__cv.wait)
                except coro.TimeoutError:
                    # Seems dumb not to return an indication it timed out.
                    pass
        finally:
            self.__lock.acquire()

    def notify(self):
        self.__cv.wake_one()

    def notify_all(self):
        self.__cv.wake_all()

    notifyAll = notify_all


class Semaphore(object):

    def __init__(self, value=1):
        self._sem = coro.semaphore(value)

    def acquire(self, blocking=True):
        if blocking:
            self._sem.acquire(1)
            return True
        else:
            if len(self._sem) < 1:
                return False
            else:
                self._sem.acquire(1)
                return True

    def release(self):
        self._sem.release(1)

    __enter__ = acquire

    def __exit__(self, exc_type, exc_value, traceback):
        self.release()

class BoundedSemaphore(Semaphore):

    def __init__(self, value=1):
        Semaphore.__init__(self, value)
        self._initial_value = value

    def release(self):
        if len(self._sem) >= self._initial_value:
            raise ValueError('Semaphore released too many times.')
        return Semaphore.release(self)

class Event(object):

    def __init__(self):
        self.__cond = coro.condition_variable()
        self.__flag = False

    def is_set(self):
        return self.__flag
    isSet = is_set

    def set(self):
        self.__flag = True
        self.__cond.wake_all()

    def clear(self):
        self.__flag = False

    def wait(self, timeout=None):
        if not self.__flag:
            if timeout is None:
                self.__cond.wait()
            else:
                try:
                    coro.with_timeout(timeout, self.__cond.wait)
                except coro.TimeoutError:
                    pass

##############################################################################
# Thread Object
##############################################################################

_active_threads = {}

class Thread(object):

    """Shrapnel emulation of Python Thread.

    XXX: Daemonic threads are not supported (yet).

    """

    daemon = False

    def __init__(self, group=None, target=None, name=None,
                 args=(), kwargs=None, _co=None):
        if group is not None:
            raise AssertionError('group must be None for now')
        if kwargs is None:
            kwargs = {}
        self.__target = target
        self.__args = args
        self.__kwargs = kwargs
        if _co is None:
            self.__co = coro.new(self.__bootstrap)
        else:
            self.__co = _co
        self.ident = self.__co.id
        if name:
            self.__co.set_name(name)

    def __bootstrap(self):
        _active_threads[self.__co.id] = self
        if _trace_hook:
            sys.settrace(_trace_hook)
        try:
            self.run()
        finally:
            del _active_threads[self.__co.id]

    def start(self):
        self.__co.start()

    def run(self):
        try:
            if self.__target:
                self.__target(*self.__args, **self.__kwargs)
        finally:
            del self.__target, self.__args, self.__kwargs

    def join(self, timeout=None):
        if timeout is None:
            self.__co.join()
        else:
            try:
                coro.with_timeout(timeout, self.__co.join)
            except coro.TimeoutError:
                pass

    def is_alive(self):
        return not self.__co.dead
    isAlive = is_alive

    @property
    def name(self):
        return self.__co.name

    @name.setter
    def name(self, name):
        self.__co.set_name(name)

    def getName(self):
        return self.name

    def setName(self, name):
        self.name = name

    def isDaemon(self):
        return self.daemon

    def setDaemon(self, daemonic):
        self.daemon = daemonic

##############################################################################
# Thread-Local Storage
##############################################################################
local = coro.ThreadLocal

##############################################################################
# Timer
##############################################################################

class Timer(Thread):

    def __init__(self, interval, function, args=(), kwargs={}):
        Thread.__init__(self)
        self.interval = interval
        self.function = function
        self.args = args
        self.kwargs = kwargs
        self.finished = Event()

    def cancel(self):
        self.finished.set()

    def run(self):
        self.finished.wait(self.interval)
        if not self.finished.is_set():
            self.function(*self.args, **self.kwargs)
        self.finished.set()

##############################################################################
# Global API
##############################################################################
def current_thread():
    # XXX: Probably doesn't work in "main" thread.
    try:
        return _active_threads[coro.current().id]
    except KeyError:
        # Thread was probably not started by threading but by coro instead.
        # Try creating a wrapper.
        # XXX: This does not install into _active_threads because there isn't
        # a safe way to remove it when the thread dies.  Access a global from
        # __del__ isn't safe.
        return Thread(_co=coro.current())

def active_count():
    return coro.get_live_coros()
activeCount = active_count

def enumerate():
    return _active_threads.values()

##############################################################################
# Not Implemented
##############################################################################

# stack_size
# setprofile

########NEW FILE########
__FILENAME__ = frontdoor
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# -*- Mode: Python -*-

# code is from jesse, thx.

import coro
import termios

# from Python/Lib/tty.py
# Indexes for termios list.
IFLAG = 0
OFLAG = 1
CFLAG = 2
LFLAG = 3
ISPEED = 4
OSPEED = 5
CC = 6

class stdin (coro.sock):

    def __init__ (self):
        coro.sock.__init__ (self, fd=0)
        self.fd = 0
        self.old = termios.tcgetattr (self.fd)
        # print 'old=%r' % (self.old,)
        self.new = termios.tcgetattr (self.fd)
        self.new[LFLAG] &= ~(termios.ICANON | termios.ECHO  | termios.IEXTEN)
        self.new[IFLAG] &= ~(termios.IGNBRK | termios.IXOFF | termios.IXON)
        self.new[CC][termios.VMIN] = 1
        self.new[CC][termios.VTIME] = 0
        self.new[CC][termios.CINTR] = 254  # block ctrl-c?  doesn't work.
        # print 'new=%r' % (self.new,)
        termios.tcsetattr (self.fd, termios.TCSANOW, self.new)

    def __dealloc__ (self):
        self.restore()

    def restore (self):
        # print '[restoring stdin to %r]' % (self.old,)
        termios.tcsetattr (self.fd, termios.TCSAFLUSH, self.old)

    def read (self, size):
        return self.recv (size)

    def write (self, data):
        return self.send (data)

    def writelines (self, list):
        return self.writev (filter (None, list))

# to use:
#
# s = stdin()
# while 1:
#     block = s.recv (1024)
#
# be sure to call restore() or your terminal will be hosed.

########NEW FILE########
__FILENAME__ = client
# -*- Mode: Python -*-

import re
import coro
import coro.read_stream
from protocol import http_file, header_set, latch

W = coro.write_stderr

class HTTP_Protocol_Error (Exception):
    pass

class Bad_Response (HTTP_Protocol_Error):
    pass

# viewed at its core, HTTP is a two-way exchange of messages,
#   some of which may have content associated with them.

# two different usage patterns for pipelined requests:
# 1) requests are made by different threads
# 2) requests are made by a single thread
#
# we accommodate both patterns here.
# for #2, use the lower-level send_request() method, for #1,
#   use GET, PUT, etc...

class request:

    def __init__ (self, method, uri, headers, content=None, force=True):
        self.method = method
        self.uri = uri
        self.qheaders = headers
        self.latch = latch()
        self.force = force
        self.qcontent = content
        self.content = None
        self.response = None
        self.rheader = None
        self.rfile = None

    def wake (self):
        "signal that a reply to this request has been received"
        if self.rfile and self.force:
            self.content = self.rfile.read()
        self.latch.wake_all()
        if self.rfile and not self.force:
            self.rfile.wait()

    def wait (self):
        "wait for the reply to be recieved. (if force=True wait for content as well)"
        return self.latch.wait()

    def abort (self):
        "abort this client request"
        self.latch.wake_all()
        if self.rfile:
            self.rfile.abort()

    def has_body (self):
        # XXX duplicates logic from server.py:http_request
        h = self.rheader
        if h.has_key ('transfer-encoding'):
            return True
        else:
            probe = h.get_one ('content-length')
            if probe:
                try:
                    size = int (probe)
                    if size == 0:
                        return False
                    elif size > 0:
                        return True
                    else:
                        return False
                except ValueError:
                    return False
            elif h.test ('connection', 'close') and self.method == 'GET':
                # XXX unless 204
                return True
            else:
                return False

class client:

    def __init__ (self, host, port=80, conn=None, inflight=100):
        self.host = host
        self.inflight = coro.semaphore (inflight)
        if conn is None:
            self.conn = coro.tcp_sock()
            self.conn.connect ((host, port))
        else:
            self.conn = conn
        self.stream = coro.read_stream.sock_stream (self.conn)
        self.pending = coro.fifo()
        coro.spawn (self.read_thread)

    def read_thread (self):
        while 1:
            req = self.pending.pop()
            if req is None:
                break
            else:
                self._read_message (req)
                if not req.response:
                    break
                else:
                    req.wake()

    def close (self):
        self.pending.push (None)
        self.conn.close()

    response_re = re.compile ('([^ ]+) ([0-9][0-9][0-9]) (.+)')

    def _read_message (self, req):
        line = self.stream.read_line()
        if not line:
            raise HTTP_Protocol_Error ('unexpected close')
        req.response = line[:-2]
        m = self.response_re.match (req.response)
        if not m:
            raise Bad_Response (req.response)
        else:
            req.version, req.reply_code, req.reason = m.groups()
        lines = []
        while 1:
            line = self.stream.read_line()
            if not line:
                raise HTTP_Protocol_Error ('unexpected close')
            elif line == '\r\n':
                break
            else:
                lines.append (line[:-2])
        req.rheader = h = header_set (lines)
        if req.has_body():
            req.rfile = http_file (h, self.stream)

    def send_request (self, method, uri, headers, content=None, force=False):
        try:
            self.inflight.acquire (1)
            req = request (method, uri, headers, content, force)
            self._send_request (method, uri, headers, content)
            self.pending.push (req)
            return req
        finally:
            self.inflight.release (1)

    def _send_request (self, method, uri, headers, content):
        if not headers.has_key ('host'):
            headers['host'] = self.host
        if content:
            if type(content) is str:
                headers['content-length'] = len(content)
            elif not headers.has_key ('content-length'):
                headers['transfer-encoding'] = 'chunked'
        req = (
            '%s %s HTTP/1.1\r\n'
            '%s\r\n' % (method, uri, headers)
        )
        self.conn.send (req)
        # XXX 100 continue
        if content:
            if type(content) is str:
                self.conn.send (content)
            elif headers.has_key ('content-length'):
                clen = int (headers.get_one ('content-length'))
                slen = 0
                for block in content:
                    self.conn.send (block)
                    slen += len(block)
                    if slen > clen:
                        raise HTTP_Protocol_Error ("content larger than declared length", clen, slen)
                else:
                    if slen != clen:
                        raise HTTP_Protocol_Error ("content smaller than declared length", clen, slen)
            else:
                # chunked encoding
                for block in content:
                    if block:
                        self.conn.writev (['%x\r\n' % (len (block),), block])
                self.conn.send ('0\r\n')

    def GET (self, uri, **headers):
        headers = header_set().from_keywords (headers)
        req = self.send_request ('GET', uri, headers, force=True)
        req.wait()
        return req

    def GET_file (self, uri, **headers):
        headers = header_set().from_keywords (headers)
        req = self.send_request ('GET', uri, headers, force=False)
        req.wait()
        return req

    def PUT (self, uri, content, **headers):
        headers = header_set().from_keywords (headers)
        req = self.send_request ('PUT', uri, headers, content, force=True)
        req.wait()
        return req

    def POST (self, uri, content, **headers):
        headers = header_set().from_keywords (headers)
        req = self.send_request ('POST', uri, headers, content, force=True)
        req.wait()
        return req

########NEW FILE########
__FILENAME__ = grid
# -*- Mode: Python -*-

import coro
import coro.http
import coro.backdoor

# toy: move an X through a grid.
# tests: POST data, compression, persistent connections, shared state

import sys
W = sys.stderr.write

class grid_handler:

    def __init__ (self, w, h):
        self.w = w
        self.h = h
        self.grid = [['.' for x in range (w)] for y in range (h)]
        self.pos = [w / 2, h / 2]
        self.grid[self.pos[1]][self.pos[0]] = 'X'

    def match (self, request):
        return request.path.startswith ('/grid')

    def handle_request (self, request):
        if request.path == '/grid/source':
            request['content-type'] = 'text/plain'
            request.set_deflate()
            request.push (open ('grid.py', 'rb').read())
            request.done()
            return
        request['content-type'] = 'text/html'
        request.set_deflate()
        if request.file:
            data = request.file.read()
            pairs = [x.split('=') for x in data.split ('&')]
            for k, v in pairs:
                if k == 'dir':
                    x0, y0 = self.pos
                    x1, y1 = self.pos
                    if v == 'left':
                        x1 = max (x0 - 1, 0)
                    elif v == 'right':
                        x1 = min (x0 + 1, self.w - 1)
                    elif v == 'up':
                        y1 = max (y0 - 1, 0)
                    elif v == 'down':
                        y1 = min (y0 + 1, self.h - 1)
                    else:
                        pass
                    self.grid[y0][x0] = '*'
                    self.grid[y1][x1] = 'X'
                    self.pos = [x1, y1]
        else:
            pass
        l = []
        for y in self.grid:
            l.append (''.join (y))
        request.push ('<pre>')
        request.push ('\n'.join (l))
        request.push ('\n</pre>\n')
        request.push (
            '<form name="input" action="grid" method="post">'
            '<input type="submit" name="dir" value="left" />'
            '<input type="submit" name="dir" value="right" />'
            '<input type="submit" name="dir" value="up" />'
            '<input type="submit" name="dir" value="down" />'
            '</form>'
            '<a href="/grid/source">source for this handler</a>'
        )
        request.done()

server = coro.http.server()
server.push_handler (grid_handler (50, 30))
server.push_handler (coro.http.handlers.coro_status_handler())
server.push_handler (coro.http.handlers.favicon_handler())
coro.spawn (server.start, ('0.0.0.0', 9001))
coro.spawn (coro.backdoor.serve, unix_path='/tmp/httpd.bd')
coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = jsonrpc_client
import coro
from coro.http.json_rpc import json_rpc_remote as JR

def go():
    r = JR ('http://127.0.0.1:9001/jsonrpc')
    print r.invoke ('sum', 1, 2, 3)

coro.spawn (go)
coro.event_loop()

########NEW FILE########
__FILENAME__ = jsonrpc_server
# -*- Mode: Python -*-

import coro
import coro.http
import coro.backdoor
import coro.http.json_rpc
import operator

W = coro.write_stderr

class server_root:

    def handle_json_rpc (self, method, params):
        W ('method=%r params=%r\n' % (method, params))
        if method == 'sum':
            return sum (params)
        else:
            return None

server = coro.http.server()
server.push_handler (coro.http.json_rpc.json_rpc_handler (server_root()))
server.push_handler (coro.http.handlers.coro_status_handler())
server.push_handler (coro.http.handlers.favicon_handler())
coro.spawn (server.start, ('0.0.0.0', 9001))
coro.spawn (coro.backdoor.serve, unix_path='/tmp/httpd.bd')
coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = openssl_server
# -*- Mode: Python -*-

# demo an https server using OpenSSL.

import coro
import coro.http
import coro.backdoor
from coro.ssl import openssl

ctx = coro.ssl.new_ctx (
    cert=openssl.x509 (open('../cert/server.crt').read()),
    key=openssl.pkey (open('../cert/server.key').read(), private=True),
)

server = coro.http.openssl_server (ctx)
server.push_handler (coro.http.handlers.coro_status_handler())
server.push_handler (coro.http.handlers.favicon_handler())
coro.spawn (server.start, ('0.0.0.0', 9443))
coro.spawn (coro.backdoor.serve, unix_path='/tmp/https.bd')
coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = session
# -*- Mode: Python -*-

import coro
import coro.http
import coro.backdoor

# demonstrate the session handler

import sys
W = sys.stderr.write

def session (sid, fifo):
    i = 0
    while 1:
        try:
            # wait a half hour for a new hit
            request = coro.with_timeout (1800, fifo.pop)
        except coro.TimeoutError:
            break
        else:
            request['content-type'] = 'text/html'
            if i == 10:
                request.push (
                    '<html><h1>Session Over!  Bye!</h1>'
                    '<a href="session">start over</a>'
                    '</html>'
                )
                request.done()
                break
            else:
                request.push (
                    '<html><h1>Session Demo</h1><br><h2>Hit=%d</h2>'
                    '<a href="session">hit me!</a>'
                    '</html>' % (i,)
                )
                request.done()
            i += 1

server = coro.http.server()
server.push_handler (coro.http.handlers.coro_status_handler())
server.push_handler (coro.http.session_handler.session_handler ('session', session))
server.push_handler (coro.http.handlers.favicon_handler())
coro.spawn (server.start, ('0.0.0.0', 9001))
coro.spawn (coro.backdoor.serve, unix_path='/tmp/httpd.bd')
coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = spdy_client
# -*- Mode: Python -*-

import coro
import coro.backdoor
import coro.ssl

from coro.http.spdy import spdy_client
from coro.http.protocol import header_set

W = coro.write_stderr

ctx = coro.ssl.new_ctx (
    # cert=coro.ssl.x509 (open ('cert/server.crt').read()),
    # key=coro.ssl.pkey (open ('cert/server.key').read(), '', True),
    next_protos=['spdy/2', 'http/1.1'],
    proto='tlsv1',
)

def t0():
    global ctx, s, c
    # ctx = coro.ssl.new_ctx (proto='tlsv1', next_protos=['spdy/2', 'http/1.1'])
    s = coro.ssl.sock (ctx)
    c = spdy_client ('127.0.0.1', 9443, s)
    W ('negotiated: %r\n' % (s.ssl.get_next_protos_negotiated(),))
    h = header_set()
    req = c.send_request ('GET', '/status', h, content=None, force=True)
    req.wait()
    W ('%s\n' % (req.response,))
    print repr(req.rfile.read())
    return req

if __name__ == '__main__':
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/spdy_client.bd')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = spdy_server
import coro
import coro.ssl
import coro.http.spdy
import coro.backdoor

ctx = coro.ssl.new_ctx (
    cert=coro.ssl.x509 (open ('cert/server.crt').read()),
    key=coro.ssl.pkey (open ('cert/server.key').read(), '', True),
    next_protos=['spdy/3', 'http/1.1'],
    proto='tlsv1',
)

server = coro.http.spdy.spdy_openssl_server (ctx)
server.push_handler (coro.http.handlers.favicon_handler())
server.push_handler (coro.http.handlers.coro_status_handler())
coro.spawn (server.start, ('0.0.0.0', 9443))
coro.spawn (coro.backdoor.serve, unix_path='/tmp/spdys.bd')
coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = tclient
# -*- Mode: Python -*-

import coro
W = coro.write_stderr

from coro.http.client import client as http_client
from coro.http.protocol import header_set

def t0():
    c = http_client ('127.0.0.1', 80)
    h = header_set()
    l = [c.send_request ('GET', '/postgresql/html/', h, content=None, force=True) for x in range (10)]
    for req in l:
        req.wait()
        W ('%s\n' % (req.response,))

def t1():
    c = http_client ('127.0.0.1', 80)
    rl = coro.in_parallel ([(c.GET, ('/postgresql/html/',))] * 10)
    for x in rl:
        W ('%s\n' % (x.response,))
    return rl

if __name__ == '__main__':
    import coro.backdoor
    coro.spawn (t0)
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/xx.bd')
    coro.event_loop()

########NEW FILE########
__FILENAME__ = tlslite_server
# -*- Mode: Python -*-

# demo an https server using the TLSLite package.

import coro
import coro.http
import coro.backdoor

# -----------------------------------------------------------------------
# --- change the location of the chain and key files on the next line ---
# -----------------------------------------------------------------------
server = coro.http.tlslite_server (
    'cert/server.crt',
    'cert/server.key',
)
server.push_handler (coro.http.handlers.coro_status_handler())
server.push_handler (coro.http.handlers.favicon_handler())
coro.spawn (server.start, ('0.0.0.0', 9443))
coro.spawn (coro.backdoor.serve, unix_path='/tmp/httpsd.bd')
coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = chat_server
# -*- Mode: Python -*-

# http://martinsikora.com/nodejs-and-websocket-simple-chat-tutorial

import json
from coro.http.websocket import handler, websocket

import coro
W = coro.write_stderr

class server:

    colors = ['red', 'green', 'blue', 'magenta', 'purple', 'plum', 'orange']

    def __init__ (self):
        self.clients = set()
        self.color_index = 0

    def next_color (self):
        r = self.colors[self.color_index % len (self.colors)]
        self.color_index += 1
        return r

    def new_session (self, *args, **kwargs):
        self.clients.add (connection (self, *args, **kwargs))

    def broadcast (self, name, color, payload):
        to_remove = set()
        for client in self.clients:
            try:
                client.send_message (name, color, payload)
            except:
                to_remove.add (client)
        self.clients.difference_update (to_remove)

class connection (websocket):

    def __init__ (self, server, *args, **kwargs):
        websocket.__init__ (self, *args, **kwargs)
        self.server = server
        self.color = server.next_color()
        self.name = None

    def handle_packet (self, p):
        payload = p.unpack()
        if p.opcode == 0x01:
            if self.name is None:
                reply = json.dumps ({'type': 'color', 'data': self.color})
                self.send_text (reply)
                self.name = payload
            else:
                self.server.broadcast (self.name, self.color, payload)
        return False

    def send_message (self, name, color, message):
        # W ('send_message %r %r %r\n' % (name, color, message))
        self.send_text (
            json.dumps ({
                'type': 'message',
                'data': {
                    'time': int (coro.now_usec / 1000000),
                    'text': message,
                    'author': name,
                    'color': color
                }
            })
        )

if __name__ == '__main__':
    import coro.http
    import coro.backdoor
    import os
    cwd = os.getcwd()

    chat_server = server()

    ih = coro.http.handlers.favicon_handler()
    sh = coro.http.handlers.coro_status_handler()
    fh = coro.http.handlers.file_handler (cwd)
    wh = handler ('/chat', chat_server.new_session)
    handlers = [ih, sh, fh, wh]
    # http_server = coro.http.server (('0.0.0.0', 9001))
    http_server = coro.http.server ()
    for h in handlers:
        http_server.push_handler (h)
    coro.spawn (http_server.start, ('0.0.0.0', 9001))
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/ws_chat.bd')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = echo_server
# -*- Mode: Python -*-

from coro.http.websocket import handler, websocket

import coro
W = coro.write_stderr

class echo_server (websocket):

    def __init__ (self, *args, **kwargs):
        self.pending = []
        websocket.__init__ (self, *args, **kwargs)

    def handle_packet (self, p):
        # W ('packet=%r\n' % (p,))
        self.pending.append (p.unpack())
        if p.fin:
            data, self.pending = self.pending, []
            self.send_text (''.join (data))
        return False

if __name__ == '__main__':
    import coro.http
    import coro.backdoor
    fh = coro.http.handlers.favicon_handler()
    sh = coro.http.handlers.coro_status_handler()
    wh = handler ('/echo', echo_server)
    handlers = [fh, sh, wh]
    # server = coro.http.server (('0.0.0.0', 9001))
    server = coro.http.server ()
    for h in handlers:
        server.push_handler (h)
    # coro.spawn (server.start)
    coro.spawn (server.start, ('0.0.0.0', 9001))
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/ws.bd')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = field
# -*- Mode: Python -*-

from coro.http.websocket import handler, websocket
import math
import pickle
import random
import re

import region
import quadtree

import coro
W = coro.write_stderr

# need a way to declare a dirty region, and send a single
#   redraw command for all the dirtied objects.  So we need to separate
#   object movement from redrawing... probably with a timer of some kind,
#   accumulate dirty rects, then redraw in one swell foop
# another thing to consider: sending deltas rather than the entire list.
#  for example, if the viewport hasn't moved, then the list of rectangles
#  won't be changing.  Can we send a diff?  [this might just be easier with layers]

# for layers see: http://stackoverflow.com/questions/3008635/html5-canvas-element-multiple-layers

colors = ['red', 'green', 'blue', 'magenta', 'purple', 'plum', 'orange']

# sample 'box' object.
class box (quadtree.ob):
    def __init__ (self, color, (l, t, r, b)):
        self.color = color
        self.set_rect (l, t, r, b)

    def move (self, dx, dy):
        x0, y0, x1, y1 = self.get_rect()
        self.set_rect (
            int(x0 + dx), int(y0 + dy), int(x1 + dx), int(y0 + dy)
        )

    def cmd (self, xoff, yoff):
        # command to draw me relative to <xoff,yoff>?
        x0, y0, x1, y1 = self.get_rect()
        return 'B,%s,%d,%d,%d,%d' % (self.color, x0 - xoff, y0 - yoff, x1 - x0, y1 - y0)

    def __repr__ (self):
        return '<box (%d,%d,%d,%d)>' % self.get_rect()

class circle (box):
    def __init__ (self, color, center, radius):
        x, y = center
        r = radius
        self.color = color
        self.center = center
        self.radius = radius
        self.set_rect (*self.get_rect())

    def get_rect (self):
        x, y = self.center
        r = self.radius
        return x - r, y - r, x + r, y + r

    def move (self, dx, dy):
        x, y = self.center
        self.center = int(x + dx), int(y + dy)
        self.set_rect (*self.get_rect())

    def cmd (self, xoff, yoff):
        # command to draw me relative to <xoff,yoff>?
        x, y = self.center
        return 'C,%s,%d,%d,%d' % (self.color, x - xoff, y - yoff, self.radius)

    def __repr__ (self):
        return '<circle (%d,%d) radius=%d)>' % (self.center[0], self.center[1], self.radius)

# should have multiple qt's:
# 1) for the background, immutable
# 2) for the client viewports
# 3) for moving objects

class field:
    def __init__ (self, w=1024 * 20, h=1024 * 20):
        self.w = w
        self.h = h
        self.Q_views = quadtree.quadtree()
        self.Q_back = quadtree.quadtree()
        self.Q_obs = quadtree.quadtree()
        self.generate_random_field()

    def generate_random_field (self):
        for i in range (5000):
            c = random.choice (colors)
            x = random.randint (0, self.w - 100)
            y = random.randint (0, self.h - 100)
            w = random.randint (50, 300)
            h = random.randint (50, 300)
            b = box (c, (x, y, x + w, y + h))
            self.Q_back.insert (b)
        for i in range (1000):
            coro.spawn (self.wanderer)

    def new_conn (self, *args, **kwargs):
        c = field_conn (self, *args, **kwargs)
        self.Q_views.insert (c)

    def new_ob (self, ob):
        self.Q_obs.insert (ob)
        # W ('new ob %r\n' % (self.Q_obs,))
        # self.Q_obs.dump()

    def move_ob (self, ob, dx, dy):
        r0 = ob.get_rect()
        self.Q_obs.delete (ob)
        ob.move (dx, dy)
        r1 = ob.get_rect()
        self.Q_obs.insert (ob)
        # self.Q_obs.dump()
        # notify any viewers
        r2 = region.union (r0, r1)
        for client in self.Q_views.search (r2):
            client.draw_window()

    sleep = 0.1

    def wanderer (self):
        # spawn me!
        x = random.randint (100, self.w - 100)
        y = random.randint (100, self.h - 100)
        c = random.choice (colors)
        ob = circle (c, (x, y), 25)
        self.new_ob (ob)
        while 1:
            # pick a random direction
            heading = random.randint (0, 360) * (math.pi / 180.0)
            speed = (random.random() * 10) + 3
            dx = math.cos (heading) * speed
            dy = math.sin (heading) * speed
            # go that way for 20 steps
            for i in range (20):
                self.move_ob (ob, dx, dy)
                # not working yet...
                # if not ob.range_check (0, 0, self.w, self.h):
                #     W ('%r hit an edge!\n' % (ob,))
                #     dx = - (dx * 5)
                #     dy = - (dy * 5)
                #     self.move_ob (ob, dx, dy)
                coro.sleep_relative (self.sleep)

class field_conn (websocket, quadtree.ob):

    def __init__ (self, field, *args, **kwargs):
        websocket.__init__ (self, *args, **kwargs)
        self.send_mutex = coro.mutex()
        self.field = field
        rx = random.randint (0, self.field.w - 1024)
        ry = random.randint (0, self.field.h - 1024)
        self.set_rect (rx, ry, rx + 1024, ry + 1024)
        self.draw_window()
        self.mouse_down = None, None

    def move_window (self, mx, my):
        self.field.Q_views.delete (self)
        x0, y0, x1, y1 = self.get_rect()
        x0 = x0 + mx
        y0 = y0 + my
        x1 = x1 + mx
        y1 = y1 + my
        if x0 < 0:
            x0 = 0
            x1 = 1024
        if y0 < 0:
            y0 = 0
            y1 = 1024
        if x1 > self.field.w - 1024:
            x1 = self.field.w - 1024
            x0 = x1 - 1024
        if y1 > self.field.h - 1024:
            y1 = self.field.h - 1024
            y0 = y1 - 1024
        self.set_rect (x0, y0, x1, y1)
        self.field.Q_views.insert (self)
        self.draw_window()
        self.send_text ('M pos=%d,%d' % self.get_rect()[:2])

    def draw_qt (self, r, Q):
        px, py = self.get_rect()[:2]
        for ob in Q.search (self.rect):
            r.append (ob.cmd (px, py))

    def draw_window (self):
        r = ['F']
        self.draw_qt (r, self.field.Q_back)
        self.draw_qt (r, self.field.Q_obs)
        try:
            self.send_text ('|'.join (r))
        except coro.ClosedError:
            self.handle_close()

    def send_text (self, payload):
        with self.send_mutex:
            websocket.send_text (self, payload)

    def handle_packet (self, p):
        data = p.unpack()
        event = p.unpack().split (',')
        # W ('packet = %r event=%r\n' % (p, event))
        if event[0] == 'KD':
            ascii = int (event[1])
            if ascii == 87:  # W
                self.move_window (0, -10)
            elif ascii == 65:  # A
                self.move_window (-10, 0)
            elif ascii == 83:  # S
                self.move_window (0, 10)
            elif ascii == 68:  # D
                self.move_window (10, 0)
            elif ascii == 82:  # R
                x0, y0 = self.get_rect()[:2]
                self.move_window (-x0, -y0)
        elif event[0] == 'MD':
            self.on_mouse_down (int (event[1]), int (event[2]))
        elif event[0] == 'MU':
            self.on_mouse_up (int (event[1]), int (event[2]))
        elif event[0] == 'MM':
            self.on_mouse_move (int (event[1]), int (event[2]))
        elif event[0] == 'TS':
            tl = self.unpack_touch_list (event[1:])
            self.last_touch_move = tl[0][0], tl[0][1]
            self.on_mouse_down (tl[0][0], tl[0][1])
        elif event[0] == 'TM':
            tl = self.unpack_touch_list (event[1:])
            self.last_touch_move = tl[0][0], tl[0][1]
            self.on_mouse_move (tl[0][0], tl[0][1])
        elif event[0] == 'TE':
            # emulate mouse up by with saved last touch_move
            x0, y0 = self.last_touch_move
            if x0 is not None:
                self.on_mouse_up (x0, y0)
            self.last_touch_move = None, None
        else:
            W ('unknown event: %r\n' % (event,))
        return False

    def unpack_touch_list (self, tl):
        return [[int(y) for y in x.split('.')] for x in tl]

    def on_mouse_down (self, x, y):
        self.mouse_down = x, y

    def on_mouse_up (self, x1, y1):
        x0, y0 = self.mouse_down
        self.mouse_down = None, None
        if x0 is not None:
            self.move_window (x0 - x1, y0 - y1)
            self.draw_window()

    def on_mouse_move (self, x1, y1):
        x0, y0 = self.mouse_down
        if x0:
            if abs(x1 - x0) > 10 or abs(y1 - y0) > 10:
                # moved enough to redraw
                self.mouse_down = x1, y1
                self.move_window (x0 - x1, y0 - y1)

if __name__ == '__main__':
    import coro.http
    import coro.backdoor
    import os
    cwd = os.getcwd()
    f = field()
    ih = coro.http.handlers.favicon_handler()
    sh = coro.http.handlers.coro_status_handler()
    th = handler ('/field', f.new_conn)
    fh = coro.http.handlers.file_handler (cwd)
    # so you can browse the source
    import mimetypes
    mimetypes.init()
    mimetypes.add_type ('text/plain', '.py')
    handlers = [th, ih, sh, fh]
    # server = coro.http.server (('0.0.0.0', 9001))
    server = coro.http.server()
    for h in handlers:
        server.push_handler (h)
    # coro.spawn (server.start)
    coro.spawn (server.start, ('0.0.0.0', 9001))
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/ws.bd')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = quadtree_py
# -*- Mode: Python; tab-width: 4 -*-

# SMR 2012: originally from dynwin, circa 1995-96.
#   modernized, replace some funs with generators

#
# Quad-Tree.  A 2D spatial data structure.
#
# Used to quickly locate 'objects' within a particular region.  Each
# node in the tree represents a rectangular region, while its children
# represent that region split into four quadrants.  An 'object' is
# stored at a particular level if it is contained within that region,
# but will not fit into any of the individual quadrants inside it.
#
# If an object is inserted into a quadtree that 'overflows' the current
# boundaries, the tree is re-rooted in a larger space.

import region

contains = region.region_contains_region_p
intersects = region.region_intersect_p

# split a rect into four quadrants

def split (rect):
    l, t, r, b = rect
    w2 = ((r - l) / 2) + l
    h2 = ((b - t) / 2) + t
    return (
        (l, t, w2, h2),
        (w2, t, r, h2),
        (l, h2, w2, b),
        (w2, h2, r, b)
    )

# insert an object into the tree.  The object must have a
# 'get_rect()' method in order to support searching.

def insert (tree, tree_rect, ob, ob_rect):
    quads = split(tree_rect)
    # If tree_rect is in quads, then we've shrunk down to a
    # degenerate rectangle, and we will store the object at
    # this level without splitting further.
    if tree_rect not in quads:
        for i in range(4):
            if contains (quads[i], ob_rect):
                if not tree[i]:
                    tree[i] = [None, None, None, None, set()]
                insert (tree[i], quads[i], ob, ob_rect)
                return
    tree[4].add (ob)

# generate all the objects intersecting with <search_rect>
def search_gen (tree, tree_rect, search_rect):
    quads = split (tree_rect)
    # copy the set to avoid 'set changed size during iteration'
    for ob in list (tree[4]):
        if intersects (ob.get_rect(), search_rect):
            yield ob
    for i in range(4):
        if tree[i] and intersects (quads[i], search_rect):
            for ob in search_gen (tree[i], quads[i], search_rect):
                yield ob

# delete a particular object from the tree.

def delete (tree, tree_rect, ob, ob_rect):
    if tree[4]:
        try:
            tree[4].remove (ob)
            return any (tree)
        except KeyError:
            # object not stored here
            pass
    quads = split (tree_rect)
    for i in range(4):
        if tree[i] and intersects (quads[i], ob_rect):
            if not delete (tree[i], quads[i], ob, ob_rect):
                tree[i] = None
                # equivalent to "tree != [None,None,None,None,[]]"
                return any (tree)
    return any (tree)

def gen_all (tree):
    if tree[4]:
        for ob in tree[4]:
            yield ob
    for quad in tree[:4]:
        if quad:
            for x in gen_all (quad):
                yield x

def dump (rect, tree, depth=0):
    print '  ' * depth, rect, tree[4]
    quads = split (rect)
    for i in range (4):
        if tree[i]:
            dump (quads[i], tree[i], depth + 1)

# wrapper for a quadtree, maintains bounds, keeps track of the
# number of objects, etc...

class quadtree:
    def __init__ (self, rect=(0, 0, 16, 16)):
        self.rect = rect
        self.tree = [None, None, None, None, set()]
        self.num_obs = 0
        self.bounds = (0, 0, 0, 0)

    def __repr__ (self):
        return '<quad tree (objects:%d) bounds:%s >' % (
            self.num_obs,
            repr(self.bounds)
        )

    def check_bounds (self, rect):
        l, t, r, b = self.bounds
        L, T, R, B = rect
        if L < l:
            l = L
        if T < t:
            t = T
        if R > r:
            r = R
        if B > b:
            b = B
        self.bounds = l, t, r, b

    def get_bounds (self):
        return self.bounds

    def insert (self, ob):
        rect = ob.get_rect()
        while not contains (self.rect, rect):
            l, t, r, b = self.rect
            w, h = r - l, b - t
            # favor growing right and down
            if (rect[2] > r) or (rect[3] > b):
                # resize, placing original in the upper left
                self.rect = l, t, (r + w), (b + h)
                self.tree = [self.tree, None, None, None, set()]
            elif (rect[0] < l) or (rect[1] < t):
                # resize, placing original in lower right
                self.rect = (l - w, t - h, r, b)
                self.tree = [None, None, None, self.tree, set()]
        # we know the target rect fits in our space
        insert (self.tree, self.rect, ob, rect)
        self.check_bounds (rect)
        self.num_obs += 1

    def gen_all (self):
        for ob in gen_all (self.tree):
            yield ob

    def search_gen (self, rect):
        for ob in search_gen (self.tree, self.rect, rect):
            yield ob

    def delete (self, ob):
        # we ignore the return, because we can't 'forget'
        # the root node.
        delete (self.tree, self.rect, ob, ob.get_rect())
        # XXX this is bad, it assumes the object was deleted
        self.num_obs -= 1

    def dump (self):
        print self
        dump (self.rect, self.tree, 0)

# sample 'box' object.
class box:
    def __init__ (self, rect):
        self.rect = rect

    def get_rect (self):
        return self.rect

    def __repr__ (self):
        return '<box (%d,%d,%d,%d)>' % self.rect

########NEW FILE########
__FILENAME__ = region
# -*- Mode: Python; tab-width: 4 -*-

# SMR 2012: originally from dynwin, circa 1995

###########################################################################
# regions
###########################################################################

#
# do two rectangular regions intersect?
#
# +--->
# |           lb,tb-------+
# |           |           |
# |    la,ta--+-----+     |
# |   |       |     |     |
# |   |       +-----+-rb,bb
# V   |             |
#     +--------ra,ba

# a rect is (left,top,right,bottom)
# top is < bottom, left is < right

# proof: imagine all possible cases in 1 dimension,
# (there are six) and then generalize.  simplify the
# expression and you get this.  (trust me 8^)

def region_intersect_p (a, b):
    return (a[2] >= b[0]) and \
           (b[2] >= a[0]) and \
           (a[3] >= b[1]) and \
           (b[3] >= a[1])

def point_in_region_p (x, y, r):
    return (r[0] <= x <= r[2]) and (r[1] <= y <= r[3])

# does region <a> fully contain region <b>?
def region_contains_region_p (a, b):
    return (a[0] <= b[0]) and \
           (a[2] >= b[2]) and \
           (a[1] <= b[1]) and \
           (a[3] >= b[3])

def union (a, b):
    x0, y0, x1, y1 = a
    x2, y2, x3, y3 = b
    return (
        min (x0, x2),
        min (y0, y2),
        max (x1, x3),
        max (y1, y3)
    )

########NEW FILE########
__FILENAME__ = simple
# -*- Mode: Python -*-

from coro.http.websocket import handler, websocket
import pickle
import re

import coro
W = coro.write_stderr

def timestamp():
    return coro.now / 1000000

drawing_re = re.compile ('drawing_([0-9]+).bin')
draw_re = re.compile ('D,([0-9]+),([0-9]+),([0-9]+),([0-9]+)')

class server:

    def __init__ (self):
        self.clients = set()
        self.dead = set()
        self.drawing = []
        self.timestamp = timestamp()
        all = self.get_all_drawings()
        if len (all):
            self.set_drawing (all[0])

    def new_session (self, *args, **kwargs):
        client = sketch_conn (self, *args, **kwargs)
        self.clients.add (client)
        for payload in self.drawing:
            client.send_text (payload)

    def clear_drawing (self):
        self.save_drawing()
        self.broadcast ('CD', False)

    def get_all_drawings (self):
        import os
        files = os.listdir ('.')
        r = []
        for path in files:
            m = drawing_re.match (path)
            if m:
                stamp = int (m.group(1))
                r.append (stamp)
        r.sort()
        return r

    def get_path (self, stamp):
        return 'drawing_%d.bin' % (stamp,)

    def save_drawing (self):
        if len(self.drawing):
            W ('saving %r\n' % (self.timestamp,))
            f = open (self.get_path (self.timestamp), 'wb')
            drawing, self.drawing = self.drawing, []
            self.timestamp = timestamp()
            pickle.dump (drawing, f)
            f.close()
        else:
            W ('empty drawing [no save]\n')

    def next_drawing (self):
        all = self.get_all_drawings()
        for t in all:
            if t > self.timestamp:
                self.set_drawing (t)
                return

    def prev_drawing (self):
        all = self.get_all_drawings()
        all.reverse()
        for t in all:
            if t < self.timestamp:
                self.set_drawing (t)
                return

    def set_drawing (self, stamp):
        self.save_drawing()
        self.timestamp = stamp
        self.drawing = pickle.load (open (self.get_path (stamp)))
        self.broadcast ('CD', False)
        W ('set drawing %d [%d lines]\n' % (stamp, len(self.drawing)))
        for payload in self.drawing:
            self.broadcast (payload, False)
        W ('done\n')

    def broadcast (self, payload, save=True):
        if save:
            self.drawing.append (payload)
        # copy to avoid "Set changed size during iteration"
        for client in list (self.clients):
            try:
                client.send_text (payload)
            except:
                self.dead.add (client)
                tb = coro.compact_traceback()
                W ('error: tb=%r' % (tb,))
        self.clients.difference_update (self.dead)
        self.dead = set()

    def undo (self):
        if len (self.drawing):
            last = self.drawing.pop()
            m = draw_re.match (last)
            if m:
                self.broadcast ('E,%s,%s,%s,%s' % m.groups(), save=False)

class sketch_conn (websocket):

    def __init__ (self, server, *args, **kwargs):
        self.send_mutex = coro.mutex()
        self.server = server
        self.mouse_down = False
        self.line_start = None, None
        websocket.__init__ (self, *args, **kwargs)

    def handle_close (self):
        self.server.dead.add (self)

    def send_text (self, payload):
        with self.send_mutex:
            websocket.send_text (self, payload)

    def handle_packet (self, p):
        event = p.unpack().split (',')
        # W ('packet = %r event=%r\n' % (p, event))
        if event[0] == 'MD':
            self.mouse_down = True
            self.line_start = int (event[1]), int (event[2])
        elif event[0] == 'MU':
            self.mouse_down = False
        elif event[0] == 'MM':
            if self.mouse_down:
                x1, y1 = int (event[1]), int (event[2])
                self.server.broadcast (
                    'D,%d,%d,%s,%s' % (
                        self.line_start[0], self.line_start[1], x1, y1
                    )
                )
                self.line_start = x1, y1
        elif event[0] == 'CD':
            self.server.clear_drawing()
        elif event[0] == 'ND':
            self.server.next_drawing()
        elif event[0] == 'PD':
            self.server.prev_drawing()
        elif event[0] == 'KD':
            if event[1] == '85':  # 'U'
                self.server.undo()
            elif event[1] == '82':  # 'R'
                self.server.set_drawing (self.server.timestamp)
        else:
            W ('unknown event: %r\n' % (event,))
        return False

if __name__ == '__main__':
    import coro.http
    import coro.backdoor
    import os
    cwd = os.getcwd()
    sketch_server = server()
    ih = coro.http.handlers.favicon_handler()
    sh = coro.http.handlers.coro_status_handler()
    wh = handler ('/sketch', sketch_server.new_session)
    fh = coro.http.handlers.file_handler (cwd)
    handlers = [wh, ih, sh, fh]
    # server = coro.http.server (('0.0.0.0', 9001))
    server = coro.http.server()
    for h in handlers:
        server.push_handler (h)
    # coro.spawn (server.start)
    coro.spawn (server.start, ('0.0.0.0', 9001))
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/ws.bd')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = term
# -*- Mode: Python -*-

from coro.http.websocket import handler, websocket
import pickle
import pprint
import re

import coro
W = coro.write_stderr

# python3 includes an 'html' module that does this, but it'd be nice
#   if we could just stream this transform somehow... maybe using the
#   string StreamWriter stuff?

def escape (s):
    return s.replace ('&', '&amp;').replace ('<', '&lt;').replace ('>', '&gt;')

class terminal (websocket):

    def __init__ (self, *args, **kwargs):
        websocket.__init__ (self, *args, **kwargs)
        self.send_mutex = coro.mutex()
        self.repl = repl (self)
        self.history = []
        self.history_index = 0
        self.line = []
        self.repl.read_eval_print_loop()

    def handle_close (self):
        pass

    def send_text (self, payload):
        with self.send_mutex:
            websocket.send_text (self, payload)

    def handle_packet (self, p):
        data = p.unpack()
        event = p.unpack().split (',')
        # W ('packet = %r event=%r\n' % (p, event))
        if event[0] == 'K':
            ascii = int (event[1])
            # W ('ascii=%d\n' % (ascii,))
            if ascii in (10, 13):  # lf cr
                ascii = 10
                line = ''.join (self.line)
                self.history.append (line)
                self.line = []
                self.send_text ('C')
                self.send_text ('D' + escape (line) + '\n')
                self.repl.inlines.push (line)
            elif ascii == 4:  # ctrl-d
                self.repl.inlines.push (None)
            elif ascii in (16, 14):  # ctrl-p, ctrl-n
                if ascii == 16:
                    self.history_index = (self.history_index + 1) % len(self.history)
                else:
                    self.history_index = (self.history_index - 1) % len(self.history)
                line = self.history[0 - self.history_index]
                # turn into a list of chars...
                self.line = [x for x in line]
                self.send_text ('B' + escape (line))
            else:
                self.line.append (chr (ascii))
                self.send_text ('I' + escape (chr (ascii)))
        elif event[0] == 'B':
            if len(self.line):
                del self.line[-1]
                self.send_text ('B' + ''.join (self.line))
        else:
            W ('unknown event: %r\n' % (event,))
        return False

from coro.backdoor import backdoor

class NotAuthorized (Exception):
    pass

class repl (backdoor):

    def __init__ (self, term):
        backdoor.__init__ (self, term, '\n')
        self.inlines = coro.fifo()

    def login (self):
        self.send ('Username: ')
        u = self.read_line()
        self.send ('Password: ')
        p = self.read_line()
        # XXX self.sock should really be called self.conn
        if self.sock.handler.auth_dict.get (u, None) != p:
            coro.sleep_relative (3)
            self.send ('Sorry, Charlie\n')
            self.sock.conn.close()
            raise NotAuthorized (u)

    def print_result (self, result):
        pprint.pprint (result)

    def read_line (self):
        line = self.inlines.pop()
        if line is None:
            self.sock.send_text ('D' + escape ('goodbye!\n'))
            self.sock.conn.close()
        else:
            return line

    def send (self, data):
        # Note: sock is really a terminal object
        self.sock.send_text ('D' + escape (data))

if __name__ == '__main__':
    import coro.http
    import coro.backdoor
    import os
    ih = coro.http.handlers.favicon_handler()
    sh = coro.http.handlers.coro_status_handler()
    th = handler ('/term', terminal)
    th.auth_dict = {'foo': 'bar'}
    # serve files out of this directory
    fh = coro.http.handlers.file_handler (os.getcwd())
    handlers = [th, ih, sh, fh]
    # server = coro.http.server()
    # server = coro.http.tlslite_server (
    # should point to the test cert in coro/http/cert/
    #    '../../../cert/server.crt',
    #    '../../../cert/server.key',
    #    )
    import coro.ssl
    from coro.ssl import openssl
    ctx = coro.ssl.new_ctx (
        cert=openssl.x509 (open('../../../cert/server.crt').read()),
        key=openssl.pkey (open('../../../cert/server.key').read(), private=True),
    )
    server = coro.http.openssl_server (ctx)
    for h in handlers:
        server.push_handler (h)
    # coro.spawn (server.start)
    coro.spawn (server.start, ('0.0.0.0', 9001))
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/ws.bd')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = handlers
# -*- Mode: Python -*-

import coro
import mimetypes
import os
import re
import stat
import sys
import time
import zlib

from coro.http.http_date import build_http_date, parse_http_date

W = sys.stderr.write

# these two aren't real handlers, they're more like templates
#  to give you an idea how to write one.
class post_handler:

    def match (self, request):
        # override to do a better job of matching
        return request._method == 'post'

    def handle_request (self, request):
        data = request.file.read()
        W ('post handler, data=%r\n' % (data,))
        request.done()

class put_handler:

    def match (self, request):
        # override to do a better job of matching
        return request.method == 'put'

    def handle_request (self, request):
        fp = request.file
        while 1:
            line = fp.readline()
            if not line:
                W ('line: DONE!\n')
                break
            else:
                W ('line: %r\n' % (line,))
        request.done()

class coro_status_handler:

    def match (self, request):
        return request.path.split ('/')[1] == 'status'

    def clean (self, s):
        s = s.replace ('<', '&lt;')
        s = s.replace ('>', '&gt;')
        return s

    def handle_request (self, request):
        request['content-type'] = 'text/html; charset=utf-8'
        request.set_deflate()
        request.push (
            '<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" '
            '"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">'
            '<html xmlns="http://www.w3.org/1999/xhtml">\r\n'
        )
        request.push ('<head><title>status</title></head><body>\r\n')
        request.push ('<p>Listening on\r\n')
        request.push (repr (request.server.addr))
        request.push ('</p>\r\n')
        request.push ('<table border="1">\r\n')
        all_threads = ((x, coro.where(x)) for x in coro.all_threads.values())
        for thread, traceback in all_threads:
            request.push ('<tr><td>%s\r\n' % self.clean (repr(thread)))
            request.push ('<pre>\r\n')
            # traceback format seems to have changed
            for level in traceback[1:-1].split ('] ['):
                [file, fun] = level.split (' ')
                fun, line = fun.split ('|')
                request.push ('<b>%20s</b>:%3d %s\r\n' % (self.clean (fun), int(line), self.clean (file)))
            request.push ('</pre></td></tr>')
        request.push ('</table>\r\n')
        request.push ('<p><a href="status">Update</a></p>')
        request.push ('</body></html>')
        request.done()

class file_handler:

    block_size = 16000

    def __init__ (self, doc_root):
        self.doc_root = doc_root

    def match (self, request):
        path = request.path
        filename = os.path.join (self.doc_root, path[1:])
        return os.path.exists (filename)

    crack_if_modified_since = re.compile ('([^;]+)(; length=([0-9]+))?$', re.IGNORECASE)

    def handle_request (self, request):
        path = request.path
        filename = os.path.join (self.doc_root, path[1:])

        if request.method not in ('get', 'head'):
            request.error (405)
            return

        if os.path.isdir (filename):
            filename = os.path.join (filename, 'index.html')

        if not os.path.isfile (filename):
            request.error (404)
        else:
            stat_info = os.stat (filename)
            mtime = stat_info[stat.ST_MTIME]
            file_length = stat_info[stat.ST_SIZE]

            ims = request['if-modified-since']
            if ims:
                length_match = 1
                m = self.crack_if_modified_since.match (ims)
                if m:
                    length = m.group (3)
                    if length:
                        if int(length) != file_length:
                            length_match = 0

                ims_date = parse_http_date (m.group(1))

                if length_match and ims_date:
                    if mtime <= ims_date:
                        request.error (304)
                        return

            ftype, fencoding = mimetypes.guess_type (filename)
            request['Content-Type'] = ftype or 'text/plain'
            request['Last-Modified'] = build_http_date (mtime)

            # Note: these are blocking file operations.
            if request.method == 'get':
                f = open (filename, 'rb')
                block = f.read (self.block_size)
                if not block:
                    request.error (204)  # no content
                else:
                    while 1:
                        request.push (block)
                        block = f.read (self.block_size)
                        if not block:
                            break
                    request.done()
            elif request.method == 'head':
                pass
                request.done()
            else:
                # should be impossible
                request.error (405)

sample = (
    'AAABAAEAICAQAAEABADoAgAAFgAAACgAAAAgAAAAQAAAAAEABAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'
    'AAAAAAAAAAAAAAD/+K///8AH//+iI///QAH//r4g//x3AH//Z6J//UABP/ovgD/458Ef+u+wv/Tn'
    '0R/+79if9OXZH/6gCJ/2BwAf/u/8n/h33R/7Z7kf/ReQH/+qu7//BUW//7vrv//RR3//7r///80d'
    '///pq///8EP//+rH///d9///6j///9Af/w=='
).decode ('base64')

zsample = zlib.compress (sample, 9)

last_modified = build_http_date (time.time())

class favicon_handler:

    def __init__ (self, data=None):
        if data is None:
            self.data = zsample
        else:
            self.data = data

    def match (self, request):
        return request.path == '/favicon.ico'

    def handle_request (self, request):
        if request['if-modified-since']:
            # if we cared, we could check that timestamp.
            request.error (304)
        else:
            request['content-type'] = 'image/x-icon'
            request['last-modified'] = last_modified
            # are there browsers that don't accept deflate?
            request['content-encoding'] = 'deflate'
            request.push (self.data)
            request.done()

# [based on ancient medusa code]
# This is a 'handler' that wraps an authorization method
# around access to the resources normally served up by
# another handler.

# does anyone support digest authentication? (rfc2069)

import hashlib

class auth_handler:

    def __init__ (self, dict, handler, realm='default'):
        self.dict = dict
        self.handler = handler
        self.realm = realm
        self.pass_count = 0
        self.fail_count = 0

    def match (self, request):
        # by default, use the given handler's matcher
        return self.handler.match (request)

    def parse_authorization (self, h):
        parts = h.split()
        kind = parts[0].lower()
        if kind != 'digest':
            return {}
        else:
            # split on comma
            parts = h.split (',')
            # strip off 'digest '
            parts[0] = parts[0][7:]
            # trim extra whitespace
            parts = [x.strip() for x in parts]
            d = {}
            for part in parts:
                i = part.find ('=')
                if i == -1:
                    return {}
                else:
                    key = part[:i]
                    val = part[i + 1:]
                    # strip quotes
                    val = val.replace ('"', ' ').strip()
                    d[key.lower()] = val
            return d

    def check_response (self, request, d):
        username = d['username']
        passwd = self.dict.get (username, None)
        if passwd is None:
            return False
        else:
            hash = lambda x: hashlib.md5(x).hexdigest()
            s1 = ':'.join ((username, d['realm'], passwd))
            s2 = ':'.join ((request.method.upper(), request.uri))
            ha1 = hash (s1)
            ha2 = hash (s2)
            s3 = ':'.join ((ha1, d['nonce'], ha2))
            ha3 = hash (s3)
            if ha3 == d['response']:
                return True
            else:
                return False

    def handle_request (self, request):
        # authorize a request before handling it...
        h = request['authorization']
        if h:
            d = self.parse_authorization (request['authorization'])
            if d and self.check_response (request, d):
                return self.handler.handle_request (request)
            else:
                self.handle_unauthorized (request)
        else:
            self.handle_unauthorized (request)

    def get_nonce (self):
        return hashlib.sha1 (os.urandom(16)).hexdigest()[:16]

    def handle_unauthorized (self, request):
        # We are now going to receive data that we want to ignore.
        # to ignore the file data we're not interested in.
        self.fail_count += 1
        nonce = self.get_nonce()
        request['WWW-Authenticate'] = ','.join ([
            'Digest realm="%s"' % self.realm,
            'nonce="%s"' % (nonce,),
        ])
        request.error (401)

########NEW FILE########
__FILENAME__ = http_date
# -*- Mode: Python; tab-width: 4 -*-

import re
import string
import time

def concat (*args):
    return ''.join (args)

def join (seq, field=' '):
    return field.join (seq)

def group (s):
    return '(' + s + ')'

short_days = ['sun', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat']
long_days = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']

short_day_reg = group (join (short_days, '|'))
long_day_reg = group (join (long_days, '|'))

daymap = {}
for i in range(7):
    daymap[short_days[i]] = i
    daymap[long_days[i]] = i

hms_reg = join (3 * [group('[0-9][0-9]')], ':')

months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']

monmap = {}
for i in range(12):
    monmap[months[i]] = i + 1

months_reg = group (join (months, '|'))

# From draft-ietf-http-v11-spec-07.txt/3.3.1
#       Sun, 06 Nov 1994 08:49:37 GMT  ; RFC 822, updated by RFC 1123
#       Sunday, 06-Nov-94 08:49:37 GMT ; RFC 850, obsoleted by RFC 1036
#       Sun Nov  6 08:49:37 1994       ; ANSI C's asctime() format

# rfc822 format
rfc822_date = join (
    [concat (short_day_reg, ','),    # day
     group('[0-9][0-9]?'),          # date
     months_reg,                    # month
     group('[0-9]+'),               # year
     hms_reg,                       # hour minute second
     'gmt'
     ],
    ' '
)

rfc822_reg = re.compile (rfc822_date)

def unpack_rfc822 (m):
    g = m.group
    a = string.atoi
    return (
        a(g(4)),        # year
        monmap[g(3)],   # month
        a(g(2)),        # day
        a(g(5)),        # hour
        a(g(6)),        # minute
        a(g(7)),        # second
        0,
        0,
        0
    )

# rfc850 format
rfc850_date = join (
    [concat (long_day_reg, ','),
     join (
         [group ('[0-9][0-9]?'),
          months_reg,
          group ('[0-9]+')
          ],
         '-'
    ),
        hms_reg,
        'gmt'
    ],
    ' '
)

rfc850_reg = re.compile (rfc850_date)
# they actually unpack the same way
def unpack_rfc850 (m):
    g = m.group
    a = string.atoi
    return (
        a(g(4)),        # year
        monmap[g(3)],   # month
        a(g(2)),        # day
        a(g(5)),        # hour
        a(g(6)),        # minute
        a(g(7)),        # second
        0,
        0,
        0
    )

# parsdate.parsedate    - ~700/sec.
# parse_http_date       - ~1333/sec.

weekdayname = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
monthname = [None, 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
             'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

def build_http_date(when):
    year, month, day, hh, mm, ss, wd, y, z = time.gmtime(when)
    return "%s, %02d %3s %4d %02d:%02d:%02d GMT" % (
        weekdayname[wd],
        day, monthname[month], year,
        hh, mm, ss)

def parse_http_date (d):
    d = string.lower (d)
    tz = time.timezone
    m = rfc850_reg.match (d)
    try:
        if m and m.end() == len(d):
            retval = int (time.mktime (unpack_rfc850(m)) - tz)
        else:
            m = rfc822_reg.match (d)
            if m and m.end() == len(d):
                retval = int (time.mktime (unpack_rfc822(m)) - tz)
            else:
                return 0
        return retval
    except:
        # problem in unpack or mktime failed
        return 0

########NEW FILE########
__FILENAME__ = json_rpc
# -*- Mode: Python -*-

import json
import urlparse
import coro
import base64
from coro.http.client import client as http_client
from coro.http.protocol import header_set

class json_rpc_handler:

    def __init__ (self, root):
        self.root = root

    def match (self, request):
        return request.method == 'post' and request.path == '/jsonrpc'

    def handle_request (self, request):
        data = request.file.read()
        qd = json.loads (data)
        v2 = qd.has_key ('jsonrpc')
        result = self.root.handle_json_rpc (qd['method'], qd['params'])
        request['content-type'] = 'application/json'
        if v2:
            rd = {'jsonrpc': '2.0', 'result': result, 'id': qd['id']}
        else:
            rd = {'result': result, 'error': None, 'id': qd['id']}
        request.push (json.dumps (rd))
        request.done()

class Error (Exception):
    pass

class proxy:

    def __init__ (self, remote, name):
        self.remote = remote
        self.name = name

    def __call__ (self, *args, **kwargs):
        return self.remote.invoke (self.name, args, kwargs)

class json_rpc_remote:

    def __init__ (self, url, auth_info=None):
        self.url = url
        self.url_ob = urlparse.urlparse (url)
        assert (self.url_ob.scheme == 'http')
        self.counter = 0
        if auth_info:
            # basic only
            self.auth = base64.b64encode ('%s:%s' % auth_info)
        else:
            self.auth = None
        self.conn = None

    def invoke (self, name, *args, **kwargs):
        if self.conn is None:
            self.conn = http_client (self.url_ob.hostname, self.url_ob.port)
        if kwargs:
            assert (not args)  # no way to mix positional & named args
            params = kwargs
        else:
            params = list (args)
        jreq = json.dumps ({'method': name, 'params': params, 'id': self.counter})
        self.counter += 1
        if self.auth:
            req = self.conn.POST (self.url_ob.path, jreq, Authorization='Basic %s' % (self.auth,))
        else:
            req = self.conn.POST (self.url_ob.path, jreq)
        if req.reply_code == '200':
            jrep = json.loads (req.content)
            return jrep['result']
        else:
            raise Error ((req.reply_code, req.content))

    def close (self):
        if self.conn is not None:
            self.conn.close()
            self.conn = None

    def __getattr__ (self, name):
        return proxy (self, name)

########NEW FILE########
__FILENAME__ = protocol
# -*- Mode: Python -*-

import coro
from coro import read_stream

W = coro.write_stderr

class HTTP_Protocol_Error (Exception):
    pass

class HTTP_Upgrade (Exception):
    "indicates a connection has left the HTTP universe"
    pass

# candidate for sync.pyx?
class latch:

    "Like a CV, except without the race - if the event has already fired then wait() will return immediately."

    def __init__ (self):
        self.cv = coro.condition_variable()
        self.done = False

    def wake_all (self, args=()):
        self.done = True
        self.args = args
        return self.cv.wake_all (args)

    def wait (self):
        if not self.done:
            return self.cv.wait()
        else:
            return self.args

class http_file:

    "HTTP message content, as a file-like object."

    buffer_size = 8000

    def __init__ (self, headers, stream):
        self.streami = stream
        self.done_cv = latch()
        if headers.get_one ('transfer-encoding') == 'chunked':
            self.streamo = read_stream.buffered_stream (self._gen_read_chunked().next)
        else:
            content_length = headers.get_one ('content-length')
            if content_length:
                self.content_length = int (content_length)
                self.streamo = read_stream.buffered_stream (self._gen_read_fixed().next)
            elif headers.test ('connection', 'close'):
                self.streamo = read_stream.buffered_stream (self._gen_read_all().next)
            else:
                raise HTTP_Protocol_Error ("no way to determine length of HTTP data")

    def _gen_read_chunked (self):
        "generator: decodes chunked transfer-encoding."
        s = self.streami
        while 1:
            chunk_size = int (s.read_line()[:-2], 16)
            if chunk_size == 0:
                assert (s.read_exact (2) == '\r\n')
                self.done_cv.wake_all()
                return
            else:
                remain = chunk_size
                while remain:
                    ask = min (remain, self.buffer_size)
                    yield s.read_exact (ask)
                    remain -= ask
                assert (s.read_exact (2) == '\r\n')

    def _gen_read_fixed (self):
        "generate fixed-size blocks of content."
        s = self.streami
        remain = self.content_length
        while remain:
            ask = min (remain, self.buffer_size)
            block = s.read_exact (ask)
            remain -= ask
            yield block
        self.done_cv.wake_all()
        return

    def _gen_read_all (self):
        "generate content from all remaining data from socket"
        s = self.streami
        while 1:
            block = s.read_exact (self.buffer_size)
            if not block:
                self.done_cv.wake_all()
                return
            else:
                yield block

    def read (self, size=0, join=True):
        "read from the file.  join=False returns a generator, join=True returns a string."
        if size == 0:
            r = (x for x in self.streamo.read_all())
            if join:
                return ''.join (r)
            else:
                return r
        else:
            # ignore join argument
            return self.streamo.read_exact (size)

    def readline (self):
        "read a newline-delimited line."
        if self.done_cv.done:
            return ''
        else:
            return self.streamo.read_until ('\n')

    def wait (self):
        "wait until all the content has been read."
        self.done_cv.wait()

    def abort (self, info='aborted'):
        self.done_cv.wake_all (info)

class header_set:

    def __init__ (self, headers=()):
        self.headers = {}
        for h in headers:
            self.crack (h)

    def from_keywords (self, kwds):
        """Populate this header set from a dictionary of keyword arguments
           (e.g., 'content_length' becomes 'content-length')"""
        r = []
        for k, v in kwds.items():
            k = k.replace ('_', '-')
            self[k] = v
        return self

    def crack (self, h):
        "Crack one header line."
        # deliberately ignoring 822 crap like continuation lines.
        try:
            i = h.index (': ')
            name, value = h[:i], h[i + 2:]
            self[name] = value
        except ValueError:
            coro.write_stderr ('dropping bogus header %r\n' % (h,))
            pass

    def get_one (self, key):
        """Get the value of a header expected to have at most one value.
           If not present, return None.  If more than one, raise ValueError."""
        r = self.headers.get (key, None)
        if r is None:
            return r
        elif isinstance (r, list) and len (r) > 1:
            raise ValueError ("expected only one %s header, got %r" % (key, r))
        else:
            return r[0]

    def has_key (self, key):
        "Is this header present?"
        return self.headers.has_key (key.lower())

    def test (self, key, value):
        "Is this header present with this value?"
        for x in self.headers.get (key.lower(), []):
            if x == value:
                return True
        else:
            return False

    def __getitem__ (self, key):
        "Returns the list of values for this header, or None."
        return self.headers.get (key, None)

    def __setitem__ (self, name, value):
        "Add a value to the header <name>."
        name = name.lower()
        probe = self.headers.get (name)
        if probe is None:
            self.headers[name] = [value]
        else:
            probe.append (value)

    def __delitem__ (self, name):
        "Remove a header."
        del self.headers[name]

    def remove (self, name):
        "remove a header [if present]"
        if self.headers.has_key (name):
            del self.headers[name]

    def __str__ (self):
        "Render the set of headers."
        r = []
        for k, vl in self.headers.iteritems():
            for v in vl:
                r.append ('%s: %s\r\n' % (k, v))
        return ''.join (r)

    def copy (self):
        "Return a copy of this header set"
        h = header_set()
        h.headers = self.headers.copy()
        return h

########NEW FILE########
__FILENAME__ = server
# -*- Mode: Python -*-

# history: this code traces all the way back to medusa, through egroups, then ironport, and into shrapnel.
#  Very Rewritten in Feb 2012.

import coro
import errno
import http_date
import mimetypes
import os
import re
from coro import read_stream
import socket
import stat
import sys
import time
import zlib

from protocol import latch, http_file, header_set, HTTP_Upgrade

W = sys.stderr.write

__version__ = '0.1'

class request_stream:

    def __init__ (self, conn, stream):
        self.timeout = conn.server.client_timeout
        self.conn = conn
        self.stream = stream

    def get_request (self):
        request_line = self.stream.read_line()
        if not request_line:
            raise StopIteration
        else:
            # read header
            lines = []
            while 1:
                line = self.stream.read_line()
                # XXX handle continuation lines
                if line == '':
                    raise StopIteration
                elif line == '\r\n':
                    break
                else:
                    lines.append (line[:-2])
        return http_request (self.conn, request_line[:-2], header_set (lines))

    def gen_requests (self):
        # read HTTP requests on this stream
        while 1:
            try:
                request = coro.with_timeout (self.timeout, self.get_request)
            except coro.TimeoutError:
                return
            else:
                yield request
                # can't read another request until we finish reading this one
                # [it might have a body]
                request.wait_until_read()

class connection:

    def __init__ (self, server, conn, addr):
        self.server = server
        self.stream = None
        self.conn = conn
        self.peer = addr

    def run (self):
        self.stream = read_stream.sock_stream (self.conn)
        upgrade = False
        try:
            try:
                for request in request_stream (self, self.stream).gen_requests():
                    if request.bad:
                        # bad request
                        request.error (400)
                    else:
                        try:
                            handler = self.pick_handler (request)
                            if handler:
                                # XXX with_timeout() ?
                                handler.handle_request (request)
                            else:
                                request.error (404)
                            request.wait_until_done()
                        except (coro.TimeoutError, coro.Interrupted):
                            raise
                        except HTTP_Upgrade:
                            upgrade = True
                            break
                        # XXX use Exception here, avoid catch/raise of coro.TimeoutError/Interrupted?
                        except:
                            tb = coro.compact_traceback()
                            self.server.log ('error: %r request=%r tb=%r' % (self.peer, request, tb))
                            request.error (500, tb)
            except (OSError, coro.TimeoutError, coro.ClosedError):
                pass
        finally:
            if not upgrade:
                self.conn.close()

    def log (self, msg):
        self.server.log (msg)

    def pick_handler (self, request):
        for handler in self.server.handlers:
            if handler.match (request):
                return handler
        return None

    def send (self, data):
        return self.conn.send (data)

    def close (self):
        self.conn.close()


class http_request:
    request_count = 0
    # <path>;<params>?<query>#<fragment>
    path_re = re.compile ('(/[^;?#]*)(;[^?#]*)?(\?[^#]*)?(#.*)?')
    # <method> <uri> HTTP/<version>
    request_re = re.compile ('([^ ]+) ([^ ]+) *(HTTP/([0-9.]+))?')
    # shadowed instance variables
    chunking     = False
    close        = False
    is_done      = False
    sent_headers = False
    bad          = False
    body_done    = False
    file         = None

    def __init__ (self, client, request, headers):
        self.reply_headers = header_set()
        self.reply_code = 200
        http_request.request_count = http_request.request_count + 1
        self.request_number = http_request.request_count
        self.request = request
        self.request_headers = headers
        self.client = client
        self.server = client.server
        self.tstart = time.time()  # XXX use coro.now
        self.peer = client.peer
        self.output = buffered_output (self.client.conn)
        self.done_cv = latch()
        self.deflate = None
        m = http_request.request_re.match (request)
        if m:
            (self.method, self.uri, ver, self.version) = m.groups()
            self.method = self.method.lower()
            if not self.version:
                self.version = "0.9"
            m = http_request.path_re.match (self.uri)
            if m:
                (self.path, self.params, self.query, self.frag) = m.groups()
            else:
                self.bad = True
        else:
            self.version = "1.0"
            self.bad = True
        if self.has_body():
            self.file = http_file (headers, client.stream)

    def __repr__ (self):
        return '<http request from %r : %r>' % (self.peer, self.request,)

    def wait_until_read (self):
        "wait until this entire request body has been read"
        if self.file:
            self.file.done_cv.wait()

    def wait_until_done (self):
        "wait until this request is done (i.e, the response has been sent)"
        if not self.is_done:
            self.done_cv.wait()

    def has_body (self):
        if self.request_headers.has_key ('transfer-encoding'):
            # 4.4 ignore any content-length
            return True
        else:
            probe = self.request_headers.get_one ('content-length')
            if probe:
                try:
                    size = int (probe)
                    if size == 0:
                        return False
                    elif size > 0:
                        return True
                    else:
                        return False
                except ValueError:
                    return False

    def can_deflate (self):
        acc_enc = self.request_headers.get_one ('accept-encoding')
        if acc_enc:
            for kind in acc_enc.split (','):
                if kind.strip().lower() == 'deflate':
                    return True
        return False

    def set_deflate (self):
        "set this request for on-the-fly compression (via zlib DEFLATE)"
        if self.can_deflate():
            self.deflate = zlib.compressobj()
            self['content-encoding'] = 'deflate'
            # http://zoompf.com/blog/2012/02/lose-the-wait-http-compression
            # Note: chrome,firefox,safari,opera all handle the header.  Not MSIE, sigh.  Discard it.
            assert (self.deflate.compress ('') == '\x78\x9c')
            return self.deflate

    def push (self, data, flush=False):
        "push output data for this request.  buffered, maybe chunked, maybe compressed"
        if not self.sent_headers:
            self.sent_headers = 1
            self.output.write (self.get_headers())
            if self.chunking:
                self.output.set_chunk()
        if self.deflate:
            if data:
                data = self.deflate.compress (data)
            if flush:
                data += self.deflate.flush()
        if data:
            self.output.write (data)

    def done (self):
        if self.is_done:
            W ('done called twice?\n')
            return
        if not self.sent_headers:
            self.push ('')
        if self.deflate:
            self.push ('', flush=True)
        self.output.flush()
        if self.close:
            self.client.close()
        self.is_done = True
        self.client.server.log (self.log_line())
        self.done_cv.wake_all()

    # note: the difference of meaning between getitem/setitem
    def __getitem__ (self, key):
        # fetch a request header
        # use this only when you expect at most one of this header.
        return self.request_headers.get_one (key)

    def __setitem__ (self, key, val):
        # set a reply header
        self.reply_headers[key] = val

    def get_headers (self):
        chunked = False
        # here is were we decide things like keep-alive, 1.0 vs 1.1, chunking, etc.
        hi = self.request_headers
        ho = self.reply_headers
        connection = hi.get_one('connection')
        if connection:
            connection_tokens = [x.strip() for x in connection.split(',')]
        else:
            connection_tokens = ()
        close_it = False
        if self.version == '1.1':
            if 'close' in connection_tokens:
                close_it = True
            elif not ho.get_one ('content-length'):
                ho['transfer-encoding'] = 'chunked'
                chunked = True
        elif self.version == '1.0':
            if 'keep-alive' in connection_tokens:
                if not ho.get_one ('content-length'):
                    close_it = True
                else:
                    ho['connection'] = 'keep-alive'
            else:
                close_it = True
        elif self.version == '0.9':
            close_it = True

        if close_it:
            ho['connection'] = 'close'

        self.chunking = chunked
        self.close = close_it

        ho['server'] = 'shrapnel httpd/%s' % __version__
        ho['date'] = http_date.build_http_date (coro.now_usec / coro.microseconds)

        return self.response (self.reply_code) + '\r\n' + str (self.reply_headers) + '\r\n'

    def response (self, code=200):
        message = self.responses[code]
        self.reply_code = code
        return 'HTTP/%s %d %s' % (self.version, code, message)

    def error (self, code, reason=None):
        self.reply_code = code
        message = self.responses[code]
        s = self.DEFAULT_ERROR_MESSAGE % {
            'code': code, 'message': message, 'reason': reason
        }
        self['content-length'] = str(len(s))
        self['content-type'] = 'text/html'
        self.push (s, flush=True)
        self.done()

    def log_line (self):
        now = time.time()
        # somewhere between common log format and squid, avoid the
        # expense of formatting time
        return '%.03f %s "%s" %d %d %0.2f' % (
            now,
            '%s:%d' % self.peer,
            self.request,
            self.reply_code,
            self.output.sent,
            now - self.tstart,
        )

    responses = {
        100: "Continue",
        101: "Switching Protocols",
        200: "OK",
        201: "Created",
        202: "Accepted",
        203: "Non-Authoritative Information",
        204: "No Content",
        205: "Reset Content",
        206: "Partial Content",
        300: "Multiple Choices",
        301: "Moved Permanently",
        302: "Moved Temporarily",
        303: "See Other",
        304: "Not Modified",
        305: "Use Proxy",
        400: "Bad Request",
        401: "Unauthorized",
        402: "Payment Required",
        403: "Forbidden",
        404: "Not Found",
        405: "Method Not Allowed",
        406: "Not Acceptable",
        407: "Proxy Authentication Required",
        408: "Request Time-out",
        409: "Conflict",
        410: "Gone",
        411: "Length Required",
        412: "Precondition Failed",
        413: "Request Entity Too Large",
        414: "Request-URI Too Large",
        415: "Unsupported Media Type",
        500: "Internal Server Error",
        501: "Not Implemented",
        502: "Bad Gateway",
        503: "Service Unavailable",
        504: "Gateway Time-out",
        505: "HTTP Version not supported"
    }

    # Default error message
    DEFAULT_ERROR_MESSAGE = '\r\n'.join ([
        '<html>',
        '<head>',
        '<title>Error response</title>',
        '</head>',
        '<body>',
        '<h1>Error response</h1>',
        '<p>Error code %(code)d.',
        '<p>Message: %(message)s.',
        '<p>Reason: %(reason)s.',
        '</body>',
        '</html>',
        ''
    ])

# chunking works thus:
#    <data>
# becomes:
#    <hex-length><CRLF>
#    <data><CRLF>
# when done, signal with
#    0<CRLF><CRLF>

class buffered_output:

    "Buffer HTTP output data; handle the 'chunked' transfer-encoding"

    def __init__ (self, conn, size=8000):
        self.conn = conn
        self.size = size
        self.buffer = []
        self.len = 0
        self.sent = 0
        self.chunk_index = -1
        self.chunk_len = 0

    # at this point *exactly*, we want to start chunking the output.
    # this is called immediately after the headers are pushed.
    def set_chunk (self):
        "start chunking here, exactly."
        self.chunk_index = len (self.buffer)

    def get_data (self):
        "get data to send. may chunk."
        data, self.buffer = self.buffer, []
        if self.chunk_index >= 0:
            # chunkify (the post-header portion of) our output list
            data.insert (self.chunk_index, '%x\r\n' % (self.chunk_len,))
            data.append ('\r\n')
            self.chunk_len = 0
            self.chunk_index = 0
        self.len = 0
        return data

    def write (self, data):
        "Push data to the buffer. If the accumulated data goes over the buffer size, send it."
        self.buffer.append (data)
        self.len += len (data)
        if self.chunk_index >= 0:
            self.chunk_len += len (data)
        if self.len >= self.size:
            self.send (self.get_data())

    def flush (self):
        "Flush the data from this buffer."
        data = self.get_data()
        if self.chunk_index >= 0:
            data.append ('0\r\n\r\n')
        self.send (data)

    def send (self, data):
        try:
            self.sent += self.conn.writev (data)
        except AttributeError:
            # underlying socket may not support writev (e.g., tlslite)
            self.sent += self.conn.send (''.join (data))

class server:

    client_timeout = 30

    def __init__ (self):
        self.handlers = []
        self.shutdown_flag = 0
        self.thread_id = None
        self.addr = ()
        self.sock = None

    def log (self, line):
        sys.stderr.write ('http %s:%d: %s\n' % (self.addr[0], self.addr[1], line))

    def push_handler (self, handler):
        self.handlers.append (handler)

    def start (self, addr, retries=5):
        """Start the web server listening on addr in a new coroutine.

        Try up to <retries> time to bind to that address.
        Raises an exception if the bind fails."""

        self.addr = addr
        self.sock = self.create_sock()
        self.sock.set_reuse_addr()
        done = 0
        save_errno = 0
        while not done:
            for x in xrange (retries):
                try:
                    self.sock.bind (addr)
                except OSError, why:
                    if why.errno not in (errno.EADDRNOTAVAIL, errno.EADDRINUSE):
                        raise
                    else:
                        save_errno = 0
                        if why.errno == errno.EADDRINUSE:
                            was_eaddrinuse = 1
                else:
                    done = 1
                    break
                coro.sleep_relative (1)
            else:
                self.log ('cannot bind to %s:%d after 5 attempts, errno = %d' % (addr[0], addr[1], save_errno))
                coro.sleep_relative (15)

        self.sock.listen (1024)
        c = coro.spawn (self.run)
        c.set_name ('%s (%s:%d)' % (self.__class__.__name__, addr[0], addr[1]))

    def run (self):
        self.thread_id = coro.current().thread_id()
        while not self.shutdown_flag:
            try:
                conn, addr = self.accept()
                client = self.create_connection (conn, addr)
                c = coro.spawn (client.run)
                c.set_name ('%s connection on %r' % (self.__class__.__name__, addr,))
            except coro.Shutdown:
                break
            except:
                self.log ('error: %r' % (coro.compact_traceback(),))
                coro.sleep_relative (0.25)
                continue
        self.sock.close()

    def accept (self):
        return self.sock.accept()

    def create_sock (self):
        # the assumption here is that you would never run an HTTP server
        #   on a unix socket, if you need that then override this method.
        if ':' in self.addr[0]:
            return coro.tcp6_sock()
        else:
            return coro.tcp_sock()

    def create_connection (self, conn, addr):
        return connection (self, conn, addr)

    def shutdown (self):
        self.shutdown_flag = 1
        try:
            # XXX SMR is this really necessary?
            thread = coro.get_thread_by_id (self.thread_id)
            thread.shutdown()
        except KeyError:
            return  # already exited

class tlslite_server (server):

    "https server using the tlslite package"

    def __init__ (self, cert_path, key_path, **handshake_args):
        server.__init__ (self)
        self.handshake_args = handshake_args
        self.cert_path = cert_path
        self.key_path = key_path
        self.read_chain()
        self.read_private()

    def accept (self):
        import tlslite
        while 1:
            conn0, addr = server.accept (self)
            conn = tlslite.TLSConnection (conn0)
            conn.ignoreAbruptClose = True
            conn.handshakeServer (certChain=self.chain, privateKey=self.private, **self.handshake_args)
            return conn, addr

    def read_chain (self):
        "cert chain is all in one file, in LEAF -> ROOT order"
        import tlslite
        delim = '-----END CERTIFICATE-----\n'
        data = open (self.cert_path).read()
        certs = data.split (delim)
        chain = []
        for cert in certs:
            if cert:
                x = tlslite.X509()
                x.parse (cert + delim)
                chain.append (x)
        self.chain = tlslite.X509CertChain (chain)

    def read_private (self):
        import tlslite
        self.private = tlslite.parsePEMKey (
            open (self.key_path).read(),
            private=True
        )

class openssl_server (server):

    def __init__ (self, ctx, verify=False):
        self.ctx = ctx
        # XXX do something with verify
        self.verify = verify
        server.__init__ (self)

    def create_sock (self):
        import coro.ssl
        import socket
        if ':' in self.addr[0]:
            domain = socket.AF_INET6
        else:
            domain = socket.AF_INET
        return coro.ssl.sock (self.ctx, domain=domain)

########NEW FILE########
__FILENAME__ = session_handler
# -*- Mode: Python -*-

import coro
import time
import uuid

import sys
W = sys.stderr.write

# See: http://en.wikipedia.org/wiki/HTTP_cookie#Session_cookie

def extract_session (cookie):
    parts = cookie.split (';')
    for part in parts:
        pair = [x.strip() for x in part.split ('=')]
        if len (pair) == 2:
            if pair[0] == 'session':
                return pair[1]
    return None

class session_handler:

    def __init__ (self, name, function):
        self.name = name
        self.function = function
        self.sessions = {}

    def match (self, request):
        path = request.path.split ('/')
        if (len(path) > 1) and (path[1] == self.name):
            return 1
        else:
            return 0

    def find_session (self, request):
        # XXX does http allow more than one cookie header?
        cookie = request['cookie']
        if cookie:
            sid = extract_session (cookie)
            return sid, self.sessions.get (sid, None)
        else:
            return None, None

    def gen_session_id (self):
        return str (uuid.uuid4())

    def handle_request (self, request):
        sid, fifo = self.find_session (request)
        if fifo is None:
            # login
            fifo = coro.fifo()
            fifo.push (request)
            sid = self.gen_session_id()
            request['set-cookie'] = 'session=%s' % (sid,)
            self.sessions[sid] = fifo
            coro.spawn (self.wrap, sid, fifo)
        else:
            fifo.push (request)

    def wrap (self, sid, fifo):
        try:
            self.function (sid, fifo)
        finally:
            del self.sessions[sid]

########NEW FILE########
__FILENAME__ = spdy
# -*- Mode: Python -*-

import struct
import coro
import sys

from coro.http import connection, tlslite_server, openssl_server, http_request
from coro.http.protocol import header_set, http_file
from coro.http.zspdy import inflator, deflator, unpack_control_frame, pack_control_frame
from coro.http.zspdy import pack_data_frame, pack_http_header, unpack_http_header

W = coro.write_stderr

# tricky bits:
#
# It's important to use one zlib compression object per connection,
#   the protocol assumes it and won't work if you try to create a
#   new context per request/stream
#
# The protocol looks like it supports a generic 'stream' facility, but it does not.
#   Each 'stream' is really a single request/reply, and the HTTP headers are part of
#   SYN_STREAM/SYN_REPLY.  In other words, SPDY is very HTTP-centric.

# When a reply is large (say >1MB) we still get a form of head-blocking behavior
#   unless we chop it up into bits.  Think about an architecture that would
#   automatically do that.  [i.e., a configurable max size for data frames]

class spdy_file (http_file):

    # override http_file's content generator (which is a 'pull' generator)
    #   with this coro.fifo-based 'push' generator.

    def get_content_gen (self, headers):
        self.content_fifo = coro.fifo()
        return self._gen_spdy()

    def _gen_spdy (self):
        while 1:
            block = self.content_fifo.pop()
            if block is None:
                # W ('gen_spdy: end of content\n')
                self.done_cv.wake_all()
                break
            else:
                yield block

FLAG_FIN = 0x01
FLAG_UNIDIRECTIONAL = 0x02

class spdy_server_request (http_request):

    def __init__ (self, flags, stream_id, client, headers):
        self.fin_sent = False
        self.flags = flags
        self.stream_id = stream_id
        self.pending_data_frame = None
        method = headers.get_one (':method')
        scheme = headers.get_one (':scheme')
        host   = headers.get_one (':host')
        path   = headers.get_one (':path')
        version = headers.get_one (':version')
        # XXX proxy
        # url = '%s://%s/%s' % (scheme, host, path)
        url = path
        # XXX consider changing the api to take these as separate arguments
        request = '%s %s %s' % (method, url, version)
        # XXX consider removing method/url/version?
        http_request.__init__ (self, client, request, headers)

    def can_deflate (self):
        return True

    def has_body (self):
        return not (self.flags & FLAG_FIN)

    def make_content_file (self):
        # XXX probably untested...
        self.file = spdy_file (self.request_headers, self.client.stream)

    def push_syn_reply (self, has_data=False):
        reason = self.responses[self.reply_code]
        self.reply_headers[':status'] = '%d %s' % (self.reply_code, reason)
        self.reply_headers[':version'] = 'HTTP/1.1'
        self.client.push_syn_reply (self, has_data)
        self.sent_headers = True

    def push_data (self, data, last=False):
        # we hold back one frame in order to be able to set FLAG_FIN on the last one.
        if self.pending_data_frame is None:
            self.pending_data_frame = data
        else:
            self.pending_data_frame, data = data, self.pending_data_frame
            self.client.push_data_frame (self, data, last)

    def push (self, data, flush=False):
        "push output data for this request."
        if not self.sent_headers:
            self.push_syn_reply (has_data=data)
        if self.deflate:
            if data:
                data = self.deflate.compress (data)
        if data:
            self.push_data (data)

    def done (self):
        if not self.sent_headers:
            self.push_syn_reply (has_data=False)
        else:
            if self.deflate:
                self.push_data (self.deflate.flush())
            self.push_data (None, last=True)
        http_request.done (self)

# this is a mixin class used for both server and client.

class spdy_protocol:

    frame_types = {
        1: 'syn_stream',
        2: 'syn_reply',
        3: 'rst_stream',
        4: 'settings',
        # removed in draft3
        5: 'noop',
        6: 'ping',
        7: 'goaway',
        8: 'headers',
        9: 'window_update',
    }

    status_codes = {
        1: 'protocol_error',
        2: 'invalid_stream',
        3: 'refused_stream',
        4: 'unsupported_version',
        5: 'cancel',
        6: 'internal_error',
        7: 'flow_control_error',
    }

    protocol = 'spdy/3'

    def read_exact (self, size):
        try:
            return self.conn.read_exact (size)
        except AttributeError:
            left = size
            r = []
            while left:
                block = self.conn.recv (left)
                if not block:
                    break
                else:
                    r.append (block)
                    left -= len (block)
            return ''.join (r)

    def read_frames (self):
        while 1:
            head = self.read_exact (8)
            if not head:
                break
            elif ord(head[0]) & 0x80:
                self.read_control_frame (head)
            else:
                self.read_data_frame (head)

    def read_control_frame (self, head):
        fversion, ftype, flags, length = unpack_control_frame (head)
        data = self.read_exact (length)
        # W ('control: version=%d type=%d flags=%x length=%d\n' % (fversion, ftype, flags, length, ))
        assert (fversion == 3)
        method_name = 'frame_%s' % (self.frame_types.get (ftype, ''),)
        if method_name == 'frame_':
            self.log ('unknown SPDY frame type: %d\n' % (ftype,))
        else:
            method = getattr (self, method_name)
            method (flags, data)

    def read_data_frame (self, head):
        stream_id, flags, length = unpack_data_frame (head)
        data = self.read_exact (length)
        # W ('data: stream_id=%d flags=%x length=%d\n' % (stream_id, flags, length))
        self.handle_data_frame (stream_id, flags, data)

    spdy_version = 3

    def unpack_http_header (self, data):
        hs = header_set()
        hs.headers = unpack_http_header (self.inflate (data))
        return hs

    def pack_http_header (self, hset):
        return self.deflate (pack_http_header (hset.headers))

    def pack_control_frame (self, ftype, flags, data):
        return pack_control_frame (self.spdy_version, ftype, flags, data)

    def pack_data_frame (self, stream_id, flags, data):
        return pack_data_frame (self.spdy_version, stream_id, flags, data)

# --------------------------------------------------------------------------------
#                             spdy server
# --------------------------------------------------------------------------------

# XXX not a fan of multiple inheritance, but this seems to be the cleanest way to share
# XXX the code between server and client...

class spdy_connection (spdy_protocol, connection):

    # default to 400K buffered output
    output_buffer_size = 400 * 1024

    def run (self):
        self.streams = {}
        self.deflate = deflator()
        self.inflate = inflator()
        self.ofifo = coro.fifo()
        self.obuf = coro.semaphore (self.output_buffer_size)
        coro.spawn (self.send_thread)
        try:
            self.read_frames()
        finally:
            self.ofifo.push (None)

    def close (self):
        self.ofifo.push (None)
        self.conn.close()

    def send_thread (self):
        while 1:
            block = self.ofifo.pop()
            if block is None:
                break
            else:
                self.conn.send (block)
                self.obuf.release (len(block))

    def send_frame (self, frame):
        # self.conn.send (frame)
        self.obuf.acquire (len(frame))
        self.ofifo.push (frame)

    def push_data_frame (self, req, data, last):
        if last:
            flags = FLAG_FIN
        else:
            flags = 0
        # it'd be nice to use writev here, but the layers of either tlslite or openssl
        #  preclude it...
        frame = self.pack_data_frame (req.stream_id, flags, data)
        self.send_frame (frame)
        req.output.sent += len (frame)

    def push_syn_reply (self, req, has_data):
        if not has_data:
            flags = 0x01
        else:
            flags = 0x00
        # W ('req.reply_headers=%r\n' % (str(req.reply_headers),))
        name_vals = self.pack_http_header (req.reply_headers)
        # W ('compressed name_vals=%r\n' % (name_vals,))
        frame = self.pack_control_frame (
            0x02, flags,
            ''.join ([struct.pack ('>L', req.stream_id), name_vals])
        )
        self.send_frame (frame)
        req.output.sent += len (frame)

    def frame_syn_stream (self, flags, data):
        sid, asid, pri = struct.unpack ('>LLH', data[:10])
        # XXX do something with priority
        sid  &= 0x7fffffff
        asid &= 0x7fffffff
        # W ('syn_stream: sid=%d asid=%d pri=%x ' % (sid, asid, pri))
        headers = self.unpack_http_header (data[10:])
        req = spdy_server_request (flags, sid, self, headers)
        # W ('%s\n' % req.request,)
        self.streams[sid] = req
        coro.spawn (self.handle_request, req)

    def handle_data_frame (self, stream_id, flags, data):
        probe = self.streams.get (stream_id, None)
        if probe is not None:
            probe.file.content_fifo.push (data)
            if flags & FLAG_FIN:
                probe.file.content_fifo.push (None)
                del self.streams[stream_id]
        else:
            self.log ('orphaned data frame [%d bytes] for stream %d\n' % (length, stream_id))

    def handle_request (self, req):
        try:
            handler = self.pick_handler (req)
            if handler:
                # XXX with_timeout()
                handler.handle_request (req)
            else:
                req.error (404)
        except:
            tb = coro.compact_traceback()
            req.error (500, tb)
            self.log ('error: %r request=%r tb=%r' % (self.peer, req, tb))

    def frame_rst_stream (self, flags, data):
        stream_id, status_code = struct.unpack ('>LL', data)
        # W ('reset: %x status=%d %s\n' % (stream_id, status_code, self.status_codes.get (status_code, 'unknown')))
        del self.streams[stream_id]

    def frame_goaway (self, flags, data):
        last_stream_id, = struct.unpack ('>L', data)
        # W ('goaway last_stream_id=%d\n' % (last_stream_id,))
        # XXX arrange for the connection to close
        self.close()

    def frame_ping (self, flags, data):
        ping_id, = struct.unpack ('>L', data)
        # W ('ping_id=%x\n' % (ping_id,))
        self.send_frame (self.pack_control_frame (6, 0, data))

    def frame_settings (self, flags, data):
        self.log ('SPDY settings frame received [ignored]')

    def frame_headers (self, flags, data):
        self.log ('SPDY headers frame received [ignored]')

class spdy_tlslite_server (tlslite_server):

    def __init__ (self, addr, cert_path, key_path, settings=None):
        tlslite_server.__init__ (self, addr, cert_path, key_path, nextProtos=['spdy/3', 'http/1.1'], settings=settings)

    def create_connection (self, conn, addr):
        if conn.next_proto == b'spdy/3':
            return spdy_connection (self, conn, addr)
        else:
            return connection (self, conn, addr)

class spdy_openssl_server (openssl_server):

    def create_connection (self, conn, addr):
        # ensure that negotiation finishes...
        if conn.ssl.get_next_protos_negotiated() == b'spdy/3':
            return spdy_connection (self, conn, addr)
        else:
            return connection (self, conn, addr)

# --------------------------------------------------------------------------------
#                             spdy client
# --------------------------------------------------------------------------------

from coro.http import client as http_client

class spdy_client_request (http_client.request):

    _has_body = False

    def wake (self):
        if self.rfile and self.force:
            self.content = self.rfile.read()
        self.latch.wake_all()
        if self.rfile and not self.force:
            self.rfile.wait()

    def wait (self):
        pass

    def has_body (self):
        return self._has_body

class spdy_client (spdy_protocol, http_client.client):

    def __init__ (self, host, port=443, conn=None, inflight=100):
        self.counter = 1
        self.deflate = deflator()
        self.inflate = inflator()
        self.send_mutex = coro.mutex()
        http_client.client.__init__ (self, host, port, conn, inflight)
        # replace the fifo with a dictionary (spdy is not serialized)
        self.pending = {}

    def read_thread (self):
        try:
            self.read_frames()
        except coro.ClosedError as err:
            for sid, req in self.pending.iteritems():
                req.set_error (err)

    def close (self):
        self.conn.close()

    def push_syn_stream (self, headers, has_data):
        sid = self.counter
        self.counter += 1
        if not has_data:
            flags = 0x01
        else:
            flags = 0x00
        asid, pri = 0, 0
        name_vals = self.pack_http_header (headers)
        frame = self.pack_control_frame (
            0x01, flags,
            ''.join ([struct.pack ('>LLH', sid, asid, pri), name_vals])
        )
        with self.send_mutex:
            self.send_frame (frame)
        return sid

    def send_frame (self, frame):
        with self.send_mutex:
            return self.conn.send (frame)

    def push_data_frame (self, stream_id, data, last):
        if last:
            flags = FLAG_FIN
        else:
            flags = 0
        self.send_frame (self.pack_data_frame (stream_id, flags, data))

    def frame_syn_reply (self, flags, data):
        sid, _ = struct.unpack ('>LH', data[:6])
        hs = self.unpack_http_header (data[6:])
        req = self.pending.get (sid, None)
        if req is None:
            self.log ('orphaned syn_reply, sid=%r' % (sid,))
        else:
            req.version = hs.get_one ('version')
            req.reply_code, req.reason = hs.get_one ('status').split(' ', 1)
            req.rheader = hs
            if not flags & FLAG_FIN:
                # we have content...
                req.rfile = spdy_file (hs, None)
                req._has_body = True
            else:
                del self.pending[sid]
                req._has_body = False

    def handle_data_frame (self, sid, flags, data):
        req = self.pending.get (sid, None)
        if req is None:
            self.log ('orphaned data frame sid=%r' % (sid,))
        else:
            req.rfile.content_fifo.push (data)
            if flags & FLAG_FIN:
                req.rfile.content_fifo.push (None)
                del self.pending[sid]

    def send_request (self, method, uri, headers, content=None, force=False):
        try:
            self.inflight.acquire (1)
            req = spdy_client_request (method.upper(), uri, headers, content, force)
            sid = self._send_request (method, uri, headers, content)
            self.pending[sid] = req
            return req
        finally:
            self.inflight.release (1)

    def _send_request (self, method, uri, headers, content):
        if not headers.has_key ('host'):
            headers['host'] = self.host
        if content:
            has_data = True
        else:
            has_data = False
        headers.set_one (':method', method)
        headers.set_one (':scheme', 'https')
        headers.set_one (':path', uri)
        headers.set_one (':version', 'HTTP/1.1')
        sid = self.push_syn_stream (headers, has_data)
        if content:
            # tricky, hold one block back
            last = None
            for block in content:
                if last:
                    self.push_data_frame (sid, block, False)
                last = block
            self.push_data_frame (sid, last, True)
        return sid

########NEW FILE########
__FILENAME__ = websocket
# -*- Mode: Python -*-

import base64
import struct
import coro
import os
import sys
import hashlib

W = coro.write_stderr

from coro.http.protocol import HTTP_Upgrade
from coro import read_stream

# RFC 6455

class WebSocketError (Exception):
    pass

class TooMuchData (WebSocketError):
    pass

class UnknownOpcode (WebSocketError):
    pass

def do_mask (data, mask):
    n = len (data)
    r = bytearray (n)
    i = 0
    while i < len (data):
        r[i] = chr (ord (data[i]) ^ mask[i % 4])
        i += 1
    return bytes (r)

class ws_packet:
    fin = 0
    opcode = 0
    mask = 0
    plen = 0
    masking = []
    payload = ''

    def __repr__ (self):
        return '<fin=%r opcode=%r mask=%r plen=%r masking=%r payload=%d bytes>' % (
            self.fin,
            self.opcode,
            self.mask,
            self.plen,
            self.masking,
            len (self.payload),
        )

    def unpack (self):
        if self.mask:
            return do_mask (self.payload, self.masking)
        else:
            return self.payload

class handler:

    magic = "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"

    def __init__ (self, path, factory):
        self.path = path
        self.factory = factory

    def match (self, request):
        # try to catch both versions of the protocol
        return (
            request.path == self.path
            and request.method == 'get'
            and request['upgrade']
            and request['upgrade'].lower() == 'websocket'
        )

    def h76_frob (self, key):
        digits = int (''.join ([x for x in key if x in '0123456789']))
        spaces = key.count (' ')
        return digits / spaces

    def handle_request (self, request):
        rh = request.request_headers
        key = rh.get_one ('sec-websocket-key')
        conn = request.client.conn
        if key:
            d = hashlib.new ('sha1')
            d.update (key + self.magic)
            reply = base64.encodestring (d.digest()).strip()
            r = [
                'HTTP/1.1 101 Switching Protocols',
                'Upgrade: websocket',
                'Connection: Upgrade',
                'Sec-WebSocket-Accept: %s' % (reply,),
            ]
            if rh.has_key ('sec-websocket-protocol'):
                # XXX verify this
                r.append (
                    'Sec-WebSocket-Protocol: %s' % (
                        rh.get_one ('sec-websocket-protocol')
                    )
                )
            conn.send ('\r\n'.join (r) + '\r\n\r\n')
            protocol = 'rfc6455'
        else:
            # for Safari, this implements the obsolete hixie-76 protocol
            # http://tools.ietf.org/html/draft-hixie-thewebsocketprotocol-76
            key1 = self.h76_frob (rh.get_one ('sec-websocket-key1'))
            key2 = self.h76_frob (rh.get_one ('sec-websocket-key2'))
            tail = request.client.stream.read_exact (8)
            key = struct.pack ('>L', key1) + struct.pack ('>L', key2) + tail
            d = hashlib.new ('md5')
            d.update (key)
            reply = d.digest()
            host = rh.get_one ('host')
            r = [
                'HTTP/1.1 101 WebSocket Protocol Handshake',
                'Upgrade: WebSocket',
                'Connection: Upgrade',
                'Sec-WebSocket-Origin: http://%s' % (host,),
                'Sec-WebSocket-Location: ws://%s%s' % (host, request.uri),
            ]
            all = '\r\n'.join (r) + '\r\n\r\n' + reply
            conn.send (all)
            protocol = 'hixie_76'
        # pass this websocket off to its new life...
        self.factory (protocol, request, self)
        raise HTTP_Upgrade

class websocket:

    def __init__ (self, proto, http_request, handler):
        self.request = http_request
        self.handler = handler
        self.stream = http_request.client.stream
        self.conn = http_request.client.conn
        # tlslite has a deeply buried "except: shutdown()" clause
        #  that breaks coro timeouts.
        self.tlslite = hasattr (self.conn, 'ignoreAbruptClose')
        self.proto = proto
        if proto == 'rfc6455':
            coro.spawn (self.read_thread)
        else:
            coro.spawn (self.read_thread_hixie_76)

    # ------------ RFC 6455 ------------
    def read_thread (self):
        close_it = False
        try:
            while 1:
                try:
                    if not self.tlslite:
                        close_it = coro.with_timeout (10, self.read_packet)
                    else:
                        close_it = self.read_packet()
                except coro.TimeoutError:
                    self.send_pong ('bleep')
                except coro.ClosedError:
                    break
                if close_it:
                    break
        finally:
            self.handle_close()
            self.conn.close()

    def read_packet (self):
        head = self.stream.read_exact (2)
        if not head:
            return True
        head, = struct.unpack ('>H', head)
        p = ws_packet()
        p.fin    = (head & 0x8000) >> 15
        p.opcode = (head & 0x0f00) >> 8
        p.mask   = (head & 0x0080) >> 7
        plen     = (head & 0x007f) >> 0
        if plen < 126:
            pass
        elif plen == 126:
            plen, = struct.unpack ('>H', self.stream.read_exact (2))
        else:  # plen == 127:
            plen, = struct.unpack ('>Q', self.stream.read_exact (8))
        p.plen = plen
        if plen > 1 << 20:
            raise TooMuchData (plen)
        if p.mask:
            p.masking = struct.unpack ('>BBBB', self.stream.read_exact (4))
        else:
            p.masking = None
        p.payload = self.stream.read_exact (plen)
        if p.opcode in (0, 1, 2):
            return self.handle_packet (p)
        elif p.opcode == 8:
            # close
            return True
        elif p.opcode == 9:
            # ping
            assert (p.fin)  # probably up to no good...
            self.send_pong (self, p.payload)
            return False
        else:
            raise UnknownOpcode (p)

    # ----------- hixie-76 -------------
    def read_thread_hixie_76 (self):
        self.stream = self.request.client.stream
        close_it = False
        try:
            while 1:
                try:
                    close_it = self.read_packet_hixie_76()
                except coro.ClosedError:
                    break
                if close_it:
                    break
        finally:
            self.conn.close()

    def read_packet_hixie_76 (self):
        ftype = self.stream.read_exact (1)
        if not ftype:
            return True
        ftype = ord (ftype)
        if ftype & 0x80:
            length = 0
            while 1:
                b = ord (self.stream.read_exact (1))
                length = (length << 7) | (b & 0x7f)
                if not b & 0x80:
                    break
                if length > 1 << 20:
                    raise TooMuchData (length)
            if length:
                payload = self.stream.read_exact (length)
            if ftype == 0xff:
                return True
        else:
            data = self.stream.read_until (b'\xff')
            if ftype == 0x00:
                p = ws_packet()
                p.fin = 1
                p.opcode = 0x01
                p.mask = None
                p.payload = data[:-1]
                self.handle_packet (p)

    # ---

    def handle_packet (self, p):
        # abstract method, override to implement your own logic
        return False

    def handle_close (self):
        # abstract method
        pass

    def send_text (self, data, fin=True):
        return self.send_packet (0x01, data, fin)

    def send_binary (self, data, fin=True):
        return self.send_packet (0x02, data, fin)

    def send_pong (self, data):
        return self.send_packet (0x0a, data, True)

    def send_packet (self, opcode, data, fin=True):
        if self.proto == 'rfc6455':
            head = 0
            if fin:
                head |= 0x8000
            assert opcode in (0, 1, 2, 8, 9, 10)
            head |= opcode << 8
            ld = len (data)
            if ld < 126:
                head |= ld
                p = [struct.pack ('>H', head), data]
            elif ld < 1 << 16:
                head |= 126
                p = [struct.pack ('>HH', head, ld), data]
            elif ld < 1 << 32:
                head |= 127
                p = [struct.pack ('>HQ', head, ld), data]
            else:
                raise TooMuchData (ld)
            # RFC6455: A server MUST NOT mask any frames that it sends to the client.
            self.writev (p)
        else:
            self.writev (['\x00', data, '\xff'])

    # for socket wrapping layers like tlslite
    def writev (self, data):
        try:
            return self.conn.writev (data)
        except AttributeError:
            return self.conn.write (''.join (data))

########NEW FILE########
__FILENAME__ = client
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# pull in visible bits of the low-level pyrex module
import coro
from coro.asn1.ber import *
from coro.ldap.query import *
import re

W = coro.write_stderr

re_dn = re.compile(r'\s*([,=])\s*')
re_dn_attr = re.compile(r'^([^,]+)(=[^,]+)(,.*)?$')

class ProtocolError (Exception):
    """An LDAP Protocol Error occurred"""
    pass

class Exit_Recv_Thread (Exception):
    "oob signal the ldap client recv thread to exit"
    pass

class LDAP:
    BindRequest                 = 0
    BindResponse                = 1
    UnbindRequest               = 2
    SearchRequest               = 3
    SearchResultEntry           = 4
    SearchResultDone            = 5
    SearchResultReference       = 19  # <--- NOT IN SEQUENCE
    ModifyRequest               = 6
    ModifyResponse              = 7
    AddRequest                  = 8
    AddResponse                 = 9
    DelRequest                  = 10
    DelResponse                 = 11
    ModifyDNRequest             = 12
    ModifyDNResponse            = 13
    CompareRequest              = 14
    CompareResponse             = 15
    AbandonRequest              = 16
    ExtendedRequest             = 23  # <--- NOT IN SEQUENCE
    ExtendedResponse            = 24

class SCOPE:
    BASE      = 0
    ONELEVEL  = 1
    SUBTREE   = 2

class DEREF:
    NEVER     = 0
    SEARCHING = 1
    FINDING   = 2
    ALWAYS    = 3

def encode_search_request (
    base_object,
    scope,
    deref_aliases,
    size_limit,
    time_limit,
    types_only,
    filter,
    which_attrs=None,
    compatibility={}
):
    if scope is None:
        scope = compatibility.get('scope', SCOPE.SUBTREE)
    if which_attrs is None:
        which_attrs = SEQUENCE()
    elif len(which_attrs) == 0:
        # Per section 4.5.1 of rfc 2251, if you really mean the empty
        # list, you can't pass the empty list because the empty list means
        # something else. You need to pass a list consisting of the OID 1.1,
        # which really (see sections 4.1.2, 4.1.4, and 4.1.5) isn't an OID
        # at all. Except some servers (Exchange 5.5) require something
        # different here, hence the lookup in the compatibility dict.
        which_attrs = SEQUENCE (
            OCTET_STRING (compatibility.get ('no_attr_attr', '1.1'))
        )
    else:
        which_attrs = SEQUENCE (*[OCTET_STRING (x) for x in which_attrs])
    return TLV (
        APPLICATION (LDAP.SearchRequest),
        OCTET_STRING (base_object),
        ENUMERATED (scope),
        ENUMERATED (deref_aliases),
        INTEGER (size_limit),
        INTEGER (time_limit),
        BOOLEAN (types_only),
        parse_query (filter),
        which_attrs,
    )

class AUTH:
    # 1 and 2 are reserved
    simple      = 0x00
    sasl        = 0x03

class RESULT:
    success                      = 0
    operationsError              = 1
    protocolError                = 2
    timeLimitExceeded            = 3
    sizeLimitExceeded            = 4
    compareFalse                 = 5
    compareTrue                  = 6
    authMethodNotSupported       = 7
    strongAuthRequired           = 8
    referral                     = 10
    adminLimitExceeded           = 11
    unavailableCriticalExtension = 12
    confidentialityRequired      = 13
    saslBindInProgress           = 14
    noSuchAttribute              = 16
    undefinedAttributeType       = 17
    inappropriateMatching        = 18
    constraintViolation          = 19
    attributeOrValueExists       = 20
    invalidAttributeSyntax       = 21
    noSuchObject                 = 32
    aliasProblem                 = 33
    invalidDNSyntax              = 34
    aliasDereferencingProblem    = 36
    inappropriateAuthentication  = 48
    invalidCredentials           = 49
    insufficientAccessRights     = 50
    busy                         = 51
    unavailable                  = 52
    unwillingToPerform           = 53
    loopDetect                   = 54
    namingViolation              = 64
    objectClassViolation         = 65
    notAllowedOnNonLeaf          = 66
    notAllowedOnRDN              = 67
    entryAlreadyExists           = 68
    objectClassModsProhibited    = 69
    affectsMultipleDSAs          = 71
    other                        = 80

class Error (Exception):

    def __init__ (self, answer):
        Exception.__init__ (self)
        self.code = answer[0]
        self.answer = answer
        self.error_string = result_string (answer[0])

    def __str__ (self):
        if len(self.answer) == 3:
            # We know how to parse it if it's length 3. Second element is
            # the "got DN", and third element is the error message. See
            # section 4 of RFC 1777.

            if self.answer[2]:
                parenthesize_got_dn = 1
                err_msg = " %r" % (self.answer[2],)
            else:
                parenthesize_got_dn = 0
                err_msg = ""

            if self.answer[1]:
                err_msg += " "
                if parenthesize_got_dn:
                    err_msg += "("
                err_msg += "Failed after successfully matching partial DN: %r" \
                           % (self.answer[1],)
                if parenthesize_got_dn:
                    err_msg += ")"
        else:
            err_msg = " %r" % (self.answer,)

        return '<LDAP Error "%s" [0x%x]%s>' % (self.error_string, self.code,
                                               err_msg)
    __repr__ = __str__

RESULT._reverse_map = r = {}
for attr in dir(RESULT):
    value = getattr (RESULT, attr)
    if isinstance(value, type(0)):
        r[value] = attr

def result_string (result):
    try:
        return RESULT._reverse_map[result]
    except KeyError:
        return "unknown error %r" % (result,)

def encode_bind_request (version, name, auth_data):
    assert (1 <= version <= 127)
    return TLV (
        APPLICATION (LDAP.BindRequest),
        INTEGER (version),
        OCTET_STRING (name),
        auth_data
    )

def encode_simple_bind (version, name, login):
    return encode_bind_request (
        version,
        name,
        TLV (
            CHOICE (AUTH.simple, 0),
            login
        )
    )

def encode_sasl_bind (version, name, mechanism, credentials=''):
    if credentials:
        cred = OCTET_STRING (credentials)
    else:
        cred = ''
    return encode_bind_request (
        version,
        name,
        TLV (
            CHOICE (AUTH.sasl),
            OCTET_STRING (mechanism),
            cred
        )
    )

def encode_starttls ():
    # encode STARTTLS request: RFC 2830, 2.1
    return TLV (
        APPLICATION (LDAP.ExtendedRequest),
        TLV (CHOICE (0, 0), '1.3.6.1.4.1.1466.20037')
    )

class client:

    # Note: default port is 389
    def __init__ (self, addr):
        self.msgid = 1
        self.addr = addr
        if isinstance (addr, tuple):
            self.sock = coro.tcp_sock()
        else:
            self.sock = coro.unix_sock()
        self.sock.connect (addr)
        self.pending = {}
        self.recv_thread_ob = coro.spawn (self.recv_thread)

    def recv_exact (self, size):
        try:
            return self.sock.recv_exact (size)
        except AttributeError:
            # tlslite has no recv_exact
            left = size
            r = []
            while left:
                block = self.sock.recv (left)
                if not block:
                    break
                else:
                    r.append (block)
                    left -= len (block)
            return ''.join (r)

    # XXX the ironport code had a simple buffering layer here, might want
    #  to reinstate that...
    def _recv_packet (self):
        # All received packets must be BER SEQUENCE. We can tell from
        # the header how much data we need to complete the packet.
        # ensure we have the sequence header - I'm inlining the (type,
        # length) detection here to get good buffering behavior
        tl = self.recv_exact (2)
        if not tl:
            return [None, None]
        tag = tl[0]
        if tag != '0':  # SEQUENCE | STRUCTURED
            raise ProtocolError ('bad tag byte: %r' % (tag,))
        l = ord (tl[1])
        p = [tl]
        if l & 0x80:
            # <l> tells us how many bytes of actual length
            ll = l & 0x7f
            len_bytes = self.recv_exact (ll)
            p.append (len_bytes)
            # fetch length
            n = 0
            for i in xrange (ll):
                n = (n << 8) | ord(len_bytes[i])
            if (n < 0) or (n > 1000000):
                # let's be reasonable, folks
                raise ProtocolError ('invalid packet length: %d' % (n,))
            need = n
        else:
            # <l> is the length of the sequence
            need = l
        # fetch the rest of the packet...
        p.append (self.recv_exact (need))
        packet = ''.join (p)
        reply, plen = decode (packet)
        return reply

    def recv_thread (self):
        while not self.exit_recv_thread:
            [msgid, reply] = self._recv_packet()
            if msgid is None:
                break
            else:
                probe = self.pending.get (msgid, None)
                if probe is None:
                    raise ProtocolError ('unknown message id in reply: %d' % (msgid,))
                else:
                    probe.schedule (reply)

    default_timeout = 10

    def send_message (self, msg):
        msgid = self.msgid
        self.msgid += 1
        self.sock.send (SEQUENCE (INTEGER (msgid), msg))
        try:
            self.pending[msgid] = me = coro.current()
            reply = coro.with_timeout (self.default_timeout, me._yield)
            return reply
        finally:
            del self.pending[msgid]

    # server replies NO:
    # starttls decoded=[1, ('application', 24, [2, '', 'unsupported extended operation'])]
    # server replies YES:
    # starttls decoded=[1, ('application', 24, [0, '', ''])]

    exit_recv_thread = False

    def starttls (self, *future_cert_params):
        import tlslite
        self.exit_recv_thread = True
        reply = self.send_message (encode_starttls())
        if reply[2] == 0:
            conn = tlslite.TLSConnection (self.sock)
            # does ldap allow client-cert authentication?
            conn.handshakeClientCert()
            self.osock = self.sock
            self.sock = conn
        # restart recv thread (maybe) with TLS socket wrapper
        self.exit_recv_thread = False
        self.recv_thread_ob = coro.spawn (self.recv_thread)
        return reply

    ldap_protocol_version = 3

    def simple_bind (self, name, login):
        return self.send_message (encode_simple_bind (self.ldap_protocol_version, name, login))

    def sasl_bind (self, name, mechanism, credentials):
        return self.send_message (encode_sasl_bind (self.ldap_protocol_version, name, mechanism, credentials))

def t0():
    sample = encode_message (
        3141,
        encode_search_request (
            'dc=nightmare,dc=com',
            SCOPE.SUBTREE,
            DEREF.NEVER,
            0,
            0,
            0,
            '(&(objectclass=inetorgperson)(userid=srushing))',
            # '(&(objectclass=inetorgperson)(userid=newton))',
            # ask for these specific attributes only
            ['mailAlternateAddress', 'rfc822ForwardingMailbox']
        )
    )

    import pprint
    import socket
    s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
    s.connect (('127.0.0.1', 389))
    s.send (sample)
    pprint.pprint (decode (s.recv (8192)))

def t1():
    c = client (('127.0.0.1', 389))
    c.bind_simple (3, 'cn=manager,dc=nightmare,dc=com', 'fnord')
    return c

if __name__ == '__main__':
    import coro.backdoor
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/ldap.bd')
    coro.event_loop()

########NEW FILE########
__FILENAME__ = t0
# -*- Mode: Python -*-

import unittest
import sys
from coro.asn1.ber import *
from coro.ldap.query import *

C = 'context'

pq_tests = [
    # simple equality
    ('(xxx=yyy)',
     ((C, 3, ['xxx', 'yyy']),
      12)),
    # simple expression, plus 'present'
    ('(|(xx=y)(zz=*))',
     ((C, 1, [(C, 3, ['xx', 'y']), (C, 7, 'zz')]),
      15)),
    # nary expressions
    ('(|(a=b)(b=c)(c=d)(e=f)(f=g)(h=i))',
     ((C, 1, [(C, 3, ['a', 'b']), (C, 3, ['b', 'c']), (C, 3, ['c', 'd']), (C, 3, ['e', 'f']), (C, 3, ['f', 'g']), (C, 3, ['h', 'i'])]),  # noqa
      50)),
    ('(|(!(a=*))(&(b=c)(d=e))(x<=y))',
     ((C, 1, [(C, 2, [(C, 7, 'a')]), (C, 0, [(C, 3, ['b', 'c']), (C, 3, ['d', 'e'])]), (C, 6, ['x', 'y'])]),
      33)),
    # approximate match
    ('(zz~=yy)', ((C, 8, ['zz', 'yy']), 10)),
    # substring
    ('(a=ins*tiga*tor)', ((C, 4, ['a', [(C, 0, 'ins'), (C, 1, 'tiga'), (C, 2, 'tor')]]), 23)),
    ('(a=*y)', ((C, 4, ['a', [(C, 2, 'y')]]), 10)),
    ('(a=y*)', ((C, 4, ['a', [(C, 0, 'y')]]), 10)),
    ('(a=*y*)', ((C, 4, ['a', [(C, 1, 'y')]]), 10)),
    ('(a=*x*y)', ((C, 4, ['a', [(C, 1, 'x'), (C, 2, 'y')]]), 13)),
    ('(a=*x*y*)', ((C, 4, ['a', [(C, 1, 'x'), (C, 1, 'y')]]), 13)),
    ('(a=*x*y*z)', ((C, 4, ['a', [(C, 1, 'x'), (C, 1, 'y'), (C, 2, 'z')]]), 16)),
    # syntax errors
    ('(a=', QuerySyntaxError),
    ('(a<b)', QuerySyntaxError),
    # good hex escape
    ('(a=some\\AAthing)', ((C, 3, ['a', 'some\252thing']), 17)),
    # bad hex escape
    ('(a=some\\AZthing)', QuerySyntaxError),
    # upper/lower case hex escape
    ('(a=xy\\Aaz)', ((C, 3, ['a', 'xy\252z']), 11)),
    # escaped splat
    ('(a=x*y\\2az)', ((C, 4, ['a', [(C, 0, 'x'), (C, 2, 'y*z')]]), 15)),
    # illegal splat
    ('(a~=sam*son)', QuerySyntaxError),
    # junk/illegal
    ('junk', QuerySyntaxError),
    # lots of parens
    (('(' * 100), QuerySyntaxError),
    # expression too complex
    (('(!' * 55) + '(x=y)' + (')' * 55), QuerySyntaxError),
    # expression not too complex
    (('(!' * 10) + '(x=y)' + (')' * 10),
     ((C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 3, ['x', 'y'])])])])])])])])])])]),  # noqa
      28)),
]

class parse_query_test (unittest.TestCase):
    def runTest (self):
        for q, e in pq_tests:
            try:
                self.assertEqual (decode (parse_query (q)), e)
            except AssertionError:
                raise
            except:
                self.assertEqual (sys.exc_info()[0], e)

def suite():
    suite = unittest.TestSuite()
    suite.addTest (parse_query_test())
    return suite

if __name__ == '__main__':
    unittest.main (defaultTest='suite')

########NEW FILE########
__FILENAME__ = optional
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# $Header: //prod/main/ap/shrapnel/coro/optional.py#3 $

"""Functions that can run in or out of coro.

This module provides emulation for some functions to run whether or not the
coro event loop is running.
"""

import coro
import signal
import time

class _shutdown_sigalrm_exc (Exception):
    pass

def _shutdown_sigalrm_handler(unused_signum, unused_frame):
    raise _shutdown_sigalrm_exc

def with_timeout(timeout, function, *args, **kwargs):
    """Call a function with a timeout.

    This version supports running even if the coro event loop isn't running by
    using SIGALRM.

    See `coro._coro.sched.with_timeout` for more detail.

    :Parameters:
        - `timeout`: The number of seconds to wait before raising the timeout.
          May be a floating point number.
        - `function`: The function to call.

    :Return:
        Returns the return value of the function.

    :Exceptions:
        - `coro.TimeoutError`: The timeout expired.
    """
    if coro.coro_is_running():
        return coro.with_timeout(timeout, function, *args, **kwargs)
    else:
        # Use sigalarm to do the magic.
        old_sigalrm_handler = signal.signal(signal.SIGALRM, _shutdown_sigalrm_handler)
        try:
            try:
                signal.alarm(timeout)
                return function(*args, **kwargs)
            except _shutdown_sigalrm_exc:
                raise coro.TimeoutError
        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_sigalrm_handler)

def sleep_relative(delta):
    """Sleep for a period of time.

    :Parameters:
        - `delta`: The number of seconds to sleep.
    """
    time.sleep(delta)

########NEW FILE########
__FILENAME__ = print_profile
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# $Header: /cvsroot/ap/shrapnel/coro/print_profile.py,v 1.1 2006/11/30 21:58:41 ehuss Exp $

"""Display profile data as HTML.

TODO
====
- The javascript sorting code has a weird behavior.  If you click on a column
  to sort it, then click on a different column, the click on the previous
  column, the sort order changes.  I would prefer it it just went with the
  previous sort order.  However, fixing it is not obvious to me.

"""

import urllib
import cgi
import sys
import coro.profiler
import cPickle
import time

PER_COLUMNS = ('ticks', 'utime', 'stime')

# from http://kryogenix.org/code/browser/sorttable/
# Note: I changed the default sort direction to 'down'
sortable_js = """addEvent(window, "load", sortables_init);

var SORT_COLUMN_INDEX;

function sortables_init() {
    // Find all tables with class sortable and make them sortable
    if (!document.getElementsByTagName) return;
    tbls = document.getElementsByTagName("table");
    for (ti=0;ti<tbls.length;ti++) {
        thisTbl = tbls[ti];
        if (((' '+thisTbl.className+' ').indexOf("sortable") != -1) && (thisTbl.id)) {
            //initTable(thisTbl.id);
            ts_makeSortable(thisTbl);
        }
    }
}

function ts_makeSortable(table) {
    if (table.rows && table.rows.length > 0) {
        var firstRow = table.rows[0];
    }
    if (!firstRow) return;

    // We have a first row: assume it's the header, and make its contents clickable links
    for (var i=0;i<firstRow.cells.length;i++) {
        var cell = firstRow.cells[i];
        var txt = ts_getInnerText(cell);
        cell.innerHTML = '<a href="#" class="sortheader" '+
        'onclick="ts_resortTable(this, '+i+');return false;">' +
        txt+'<span class="sortarrow">&nbsp;&nbsp;&nbsp;</span></a>';
    }
}

function ts_getInnerText(el) {
    if (typeof el == "string") return el;
    if (typeof el == "undefined") { return el };
    if (el.innerText) return el.innerText;  //Not needed but it is faster
    var str = "";

    var cs = el.childNodes;
    var l = cs.length;
    for (var i = 0; i < l; i++) {
        switch (cs[i].nodeType) {
            case 1: //ELEMENT_NODE
                str += ts_getInnerText(cs[i]);
                break;
            case 3: //TEXT_NODE
                str += cs[i].nodeValue;
                break;
        }
    }
    return str;
}

function ts_resortTable(lnk,clid) {
    // get the span
    var span;
    for (var ci=0;ci<lnk.childNodes.length;ci++) {
        if (lnk.childNodes[ci].tagName && lnk.childNodes[ci].tagName.toLowerCase() == 'span') span = lnk.childNodes[ci];
    }
    var spantext = ts_getInnerText(span);
    var td = lnk.parentNode;
    var column = clid || td.cellIndex;
    var table = getParent(td,'TABLE');

    // Work out a type for the column
    if (table.rows.length <= 1) return;
    var itm = ts_getInnerText(table.rows[1].cells[column]);
    //sortfn = ts_sort_caseinsensitive;
    //if (itm.match(/^\d\d[\/-]\d\d[\/-]\d\d\d\d$/)) sortfn = ts_sort_date;
    //if (itm.match(/^\d\d[\/-]\d\d[\/-]\d\d$/)) sortfn = ts_sort_date;
    //if (itm.match(/^[\ufffd$]/)) sortfn = ts_sort_currency;
    //if (itm.match(/^[\d\.]+$/)) sortfn = ts_sort_numeric;
    sortfn = ts_sort_numeric;
    SORT_COLUMN_INDEX = column;
    var firstRow = new Array();
    var newRows = new Array();
    for (i=0;i<table.rows[0].length;i++) { firstRow[i] = table.rows[0][i]; }
    for (j=1;j<table.rows.length;j++) { newRows[j-1] = table.rows[j]; }

    newRows.sort(sortfn);

    if (span.getAttribute("sortdir") == 'up') {
        ARROW = '&nbsp;&nbsp;&darr;';
        span.setAttribute('sortdir','down');
    } else {
        ARROW = '&nbsp;&nbsp;&uarr;';
        newRows.reverse();
        span.setAttribute('sortdir','up');
    }

    // We appendChild rows that already exist to the tbody, so it moves them rather than creating new ones
    // don't do sortbottom rows
    for (i=0;i<newRows.length;i++) {
        if (!newRows[i].className || (newRows[i].className && (newRows[i].className.indexOf('sortbottom') == -1)))
             table.tBodies[0].appendChild(newRows[i]);
    }
    // do sortbottom rows only
    for (i=0;i<newRows.length;i++) {
        if (newRows[i].className && (newRows[i].className.indexOf('sortbottom') != -1))
            table.tBodies[0].appendChild(newRows[i]);
    }

    // Delete any other arrows there may be showing
    var allspans = document.getElementsByTagName("span");
    for (var ci=0;ci<allspans.length;ci++) {
        if (allspans[ci].className == 'sortarrow') {
            if (getParent(allspans[ci],"table") == getParent(lnk,"table")) { // in the same table as us?
                allspans[ci].innerHTML = '&nbsp;&nbsp;&nbsp;';
            }
        }
    }

    span.innerHTML = ARROW;
}

function getParent(el, pTagName) {
    if (el == null) return null;
    else if (el.nodeType == 1 && el.tagName.toLowerCase() == pTagName.toLowerCase())
        // Gecko bug, supposed to be uppercase
        return el;
    else
        return getParent(el.parentNode, pTagName);
}
function ts_sort_date(a,b) {
    // y2k notes: two digit years less than 50 are treated as 20XX, greater than 50 are treated as 19XX
    aa = ts_getInnerText(a.cells[SORT_COLUMN_INDEX]);
    bb = ts_getInnerText(b.cells[SORT_COLUMN_INDEX]);
    if (aa.length == 10) {
        dt1 = aa.substr(6,4)+aa.substr(3,2)+aa.substr(0,2);
    } else {
        yr = aa.substr(6,2);
        if (parseInt(yr) < 50) { yr = '20'+yr; } else { yr = '19'+yr; }
        dt1 = yr+aa.substr(3,2)+aa.substr(0,2);
    }
    if (bb.length == 10) {
        dt2 = bb.substr(6,4)+bb.substr(3,2)+bb.substr(0,2);
    } else {
        yr = bb.substr(6,2);
        if (parseInt(yr) < 50) { yr = '20'+yr; } else { yr = '19'+yr; }
        dt2 = yr+bb.substr(3,2)+bb.substr(0,2);
    }
    if (dt1==dt2) return 0;
    if (dt1<dt2) return -1;
    return 1;
}

function ts_sort_currency(a,b) {
    aa = ts_getInnerText(a.cells[SORT_COLUMN_INDEX]).replace(/[^0-9.]/g,'');
    bb = ts_getInnerText(b.cells[SORT_COLUMN_INDEX]).replace(/[^0-9.]/g,'');
    return parseFloat(aa) - parseFloat(bb);
}

function ts_sort_numeric(a,b) {
    aa = parseFloat(ts_getInnerText(a.cells[SORT_COLUMN_INDEX]));
    if (isNaN(aa)) aa = 0;
    bb = parseFloat(ts_getInnerText(b.cells[SORT_COLUMN_INDEX]));
    if (isNaN(bb)) bb = 0;
    return aa-bb;
}

function ts_sort_caseinsensitive(a,b) {
    aa = ts_getInnerText(a.cells[SORT_COLUMN_INDEX]).toLowerCase();
    bb = ts_getInnerText(b.cells[SORT_COLUMN_INDEX]).toLowerCase();
    if (aa==bb) return 0;
    if (aa<bb) return -1;
    return 1;
}

function ts_sort_default(a,b) {
    aa = ts_getInnerText(a.cells[SORT_COLUMN_INDEX]);
    bb = ts_getInnerText(b.cells[SORT_COLUMN_INDEX]);
    if (aa==bb) return 0;
    if (aa<bb) return -1;
    return 1;
}


function addEvent(elm, evType, fn, useCapture)
// addEvent and removeEvent
// cross-browser event handling for IE5+,  NS6 and Mozilla
// By Scott Andrew
{
  if (elm.addEventListener){
    elm.addEventListener(evType, fn, useCapture);
    return true;
  } else if (elm.attachEvent){
    var r = elm.attachEvent("on"+evType, fn);
    return r;
  } else {
    alert("Handler could not be removed");
  }
}
"""

def _mapfuns(d1, d2):
    """
    Given two dicts, d1 (k1 -> v1) and d2 (k2 -> v2), returns a dict which has
    the mapping (k1 -> k2) such that _name(k1) == _name(k2).
    """
    def _name(fn):
        """
        Strips the line number at the end if any.
        Eg. 'foo.py:23' -> 'foo.py', 'foo:bar.py' -> 'foo:bar.py' etc.
        """
        parts = fn.rsplit(':', 1)
        try:
            int(parts[1])
            return parts[0]
        except Exception:
            return fn

    m1 = [(_name(f), f) for f in d1]
    m2 = dict([(_name(f), f) for f in d2])
    return dict([(v, m2[k]) for k, v in m1 if k in m2])


class profile_data:

    """

    profile_data = {function_string: data_tuple}

    data_tuple[0] is always calls

    call_data = {caller_string: [(callee_string, call_count),...])
                }

    """

    def __init__(self, filename):
        self._load(filename)

    def _load(self, filename):
        f = open(filename)
        header = f.read(len(coro.profiler.MAGIC))
        if header != coro.profiler.MAGIC:
            err('Header not valid.')
        self.bench_type, self.headings, self.time = cPickle.load(f)
        self.profile_data = cPickle.load(f)
        self.call_data = cPickle.load(f)

    def process(self, other_profile):
        self._print_header()
        self._print_timings(False, other_profile)
        print '<hr>'
        self._print_timings(True, other_profile)
        self._print_call_graph()
        self._print_footer()

    def _print_timings(self, aggregate, other_profile):
        if aggregate:
            print '<h2>Aggregate Timings</h2>'
        else:
            print '<h2>Non-Aggregate Timings</h2>'

        # Find any columns that have all zeros and skip them.
        has_nonzero = {}
        # Also determine the sum for the column.
        column_sums = [0] * len(self.headings)
        empty_cols = [0] * len(self.headings)

        for heading in self.headings:
            has_nonzero[heading] = False
        for function_string, (calls, data_tuple, aggregate_data_tuple) in self.profile_data.iteritems():
            if function_string != '<wait>':
                for i, heading in enumerate(self.headings):
                    if aggregate:
                        data_item = aggregate_data_tuple[i]
                    else:
                        data_item = data_tuple[i]
                    if data_item:
                        has_nonzero[heading] = True
                        column_sums[i] += data_item
        skip_headings = []
        for heading, nonzero in has_nonzero.items():
            if not nonzero:
                skip_headings.append(heading)

        print '<table id="t1" class="sortable" border=1 cellpadding=2 cellspacing=0>'
        print '  <tr>'
        print '    <th>calls</th>'
        for heading in self.headings:
            if heading not in skip_headings:
                print '    <th>%s</th>' % (heading,)
                if heading in PER_COLUMNS:
                    print '    <th>%s/call</th>' % (heading,)
        print '    <th>Function</th>'
        print '  </tr>'

        m = _mapfuns(self.profile_data, other_profile)
        for function_string, (calls, data_tuple, aggregate_data_tuple) in self.profile_data.iteritems():
            try:
                calls2, data_tuple2, aggregate_data_tuple2 = other_profile[m[function_string]]
            except KeyError:
                calls2, data_tuple2, aggregate_data_tuple2 = 0, empty_cols, empty_cols
            print '  <tr align=right>'
            print '    <td>%s</td>' % (calls - calls2, )
            for i, heading in enumerate(self.headings):
                if heading not in skip_headings:
                    if aggregate:
                        data_item = aggregate_data_tuple[i] - aggregate_data_tuple2[i]
                    else:
                        data_item = data_tuple[i] - data_tuple2[i]
                    if isinstance(data_item, float):
                        value = '%.6f' % (data_item,)
                    else:
                        value = data_item
                    if data_item and function_string != '<wait>':
                        pct = ' (%.2f%%)' % ((float(data_item) / column_sums[i]) * 100,)
                    else:
                        pct = ''
                    print '    <td>%s%s</td>' % (value, pct)
                    if heading in PER_COLUMNS:
                        if calls == 0:
                            per = data_item
                        else:
                            if isinstance(data_item, float):
                                per = '%.6f' % (data_item / calls,)
                            else:
                                per = data_item / calls
                        print '    <td>%s</td>' % (per, )
            print '    <td align=left><a name="tt_%s"></a><a href="#cg_%s">%s</a></td>' % (
                urllib.quote_plus(function_string),
                urllib.quote_plus(function_string),
                cgi.escape(function_string, quote=True)
            )
            print '  </tr>'
        print '</table>'
        print '<p><tt>/call</tt> columns represent the time spent in that function per call <b>on average</b>.'
        print '<p>Columns with all zeros are not displayed.'

    def _print_call_graph(self):
        # self.call_data is caller->callee, make a reverse graph of callee->caller
        rg = {}
        for caller_string, callees in self.call_data.iteritems():
            for callee_string, call_count in callees:
                if callee_string in rg:
                    rg[callee_string].append((caller_string, call_count))
                else:
                    rg[callee_string] = [(caller_string, call_count)]

        functions = self.profile_data.items()
        functions.sort()

        for function_string, (calls, data_tuple, aggregate_data_tuple) in functions:
            print '<hr>'
            print '<tt><a name="cg_%s">%s</a> -- ' % (
                urllib.quote_plus(function_string),
                cgi.escape(function_string, quote=True)
            )
            for (data_item, heading) in zip(data_tuple, self.headings):
                if data_item != 0:
                    print '%s=%s' % (heading, data_item)
            print '</tt>'
            print '<pre>'
            # Print callers.
            if function_string in rg:
                l = []
                for caller, count in rg[function_string]:
                    l.append((caller, count))
                l.sort(lambda a, b: cmp(a[1], b[1]))
                for caller, count in l:
                    print '%10i/%-10i (%04.1f%%) <a href="#tt_%s">%s</a>' % (
                        count,
                        calls,
                        (float(count) / calls) * 100,
                        urllib.quote_plus(caller),
                        cgi.escape(caller, quote=True)
                    )

            print '%15i           <b>%s</b>' % (calls, function_string)

            # Print callees.
            callees2 = []
            callees = self.call_data.get(function_string, ())
            for callee_string, call_count in callees:
                callee_calls = self.profile_data.get(callee_string, [1])[0]
                callees2.append((callee_string, call_count, callee_calls))
            callees2.sort(lambda a, b: cmp(a[1], b[1]))
            for callee_string, call_count, callee_calls in callees2:
                print '%10i/%-10i (%04.1f%%) <a href="#tt_%s">%s</a>' % (
                    call_count,
                    callee_calls,
                    (float(call_count) / callee_calls) * 100,
                    urllib.quote_plus(callee_string),
                    cgi.escape(callee_string, quote=True)
                )
            print '</pre>'

        print '<hr>'
        print 'Description of output:'
        print '<pre>'
        print 'filename:funcname:lineno -- prof_data'
        print
        print '   caller_x/caller_y   (%) caller'
        print '    total_calls           <b>function</b>'
        print '   callee_x/callee_y   (%) callee'
        print
        print 'caller_x is the number of times caller made a call to this function.'
        print 'caller_y is the total number of calls to the function from all callers combined.'
        print
        print 'callee_x is the total number of times the function called this callee.'
        print 'callee_y is the total number of calls to this callee from all functions in the program.'
        print
        print 'Profile data values of 0 are not displayed.'

    def _print_header(self):
        print '<html><head><title>Shrapnel Profile</title></head><body bgcolor="#ffffff">'
        print '<script type="text/javascript"><!--'
        print sortable_js
        print '// -->'
        print '</script>'
        print '<h1>Shrapnel Profile Results</h1>'
        print '<hr>'

    def _print_footer(self):
        print '<hr>'
        print 'Profile data collected at %s<br>' % (time.ctime(self.time),)
        print 'Output generated at %s<br>' % (time.ctime(),)
        print '</body></html>'

def err(msg):
    sys.stderr.write(msg + '\n')
    sys.exit(1)

def usage():
    print 'Usage: print_profile.py profile_filename [other_profile]'
    print
    print ' print_profile.py filename             -> Convert Profile data to HTML'
    print ' print_profile.py filename1, filename2 -> Compare timings between profile results in file1 and file2'
    print
    print 'Output HTML is sent to stdout'

def main(baseline, otherfile=None):
    data = profile_data(baseline)
    other_profile = {}
    if otherfile:
        data2 = profile_data(otherfile)
        if data.bench_type != data2.bench_type:
            print 'Cannot Compare. Bench types are different.'
            return
        other_profile = data2.profile_data
    data.process(other_profile)

if __name__ == '__main__':
    if len(sys.argv) not in (2, 3):
        usage()
        sys.exit(1)

    baseline = sys.argv[1]
    otherfile = sys.argv[2] if len(sys.argv) == 3 else None
    main(baseline, otherfile)

########NEW FILE########
__FILENAME__ = profiler
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# $Header: //prod/main/ap/shrapnel/coro/profiler.py#9 $

"""Coro code profiler.

Introduction
============
This profiler is coro-aware.  It produces output to a binary file on disk. You
then use the :mod:`coro.print_profile` module to convert it to an HTML file.

Using The Profiler
==================
There are two ways to run the profiler.  One is to use the
:func:`coro.profiler.go` function where you give it a python function to run.
Profiling will start and call the function, and then the profiler will
automatically stop when the function exits.

The other method is to call :func:`coro.profiler.start` to start the profiler
and :func:`coro.profiler.stop` when you want to stop profiling.  This can be
conveniently done from the backdoor.

Rendering Output
================
Once you have profiler output, you must use the ``print_profile`` module
to convert it to HTML.  The typical method for doing this is::

    python -m coro.print_profile /tmp/coro_profile.bin > my_profile.html

Then view the profile output in your web browser.

Profiler Types
==============
The profiler supports different ways of gathering statistics.  This is done by
specifying the "bench" object to use (see :func:`go` and :func:`start`).  They
default to the "rusage" method of gathering statistics about every function
call (see the getrusage man page for more detail).  If you want a higher
performance profile, you can use the :class:`coro.bench` object instead which
simply records TSC values for every function call.  If you want to define your
own method of gathering statistics, subclass :class:`coro.bench` and implement
your own techniques.

"""

import coro
import cPickle
import time

MAGIC = 'SHRAP_PROF1'

def _dump(p, filename):
    f = open(filename, 'w')
    f.write(MAGIC)
    data = (p.bench_class.__name__, p.bench_class().get_headings(), time.time())
    cPickle.dump(data, f, -1)

    # Dump profile data.
    data = {}
    for func, bench in p.charges.iteritems():
        data[func.as_str()] = bench.get_data()
    cPickle.dump(data, f, -1)

    # Dump call data.
    data = {}
    for caller, cc in p.call_counts.iteritems():
        count_data = cc.get()
        result = []
        for callee, count in count_data:
            element = (callee.as_str(), count)
            result.append(element)
        data[caller.as_str()] = result
    cPickle.dump(data, f, -1)
    f.close()

def go (fun, *args, **kwargs):
    """Start the profiler on a function.

    This will start the profiler, and then call the provided function.
    The profiler will shut down once the function returns.

    Additional arguments provided are passed to the function.

    This will display the results to stdout after the function is finished.

    :param fun: The function to call.

    :keyword profile_filename: The name of the file to save the profile data.
          Defaults to '/tmp/coro_profile.bin'.
    :keyword profile_bench: The bench object type to use.  Defaults to
          :class:`coro.rusage_bench`.
    """
    if 'profile_filename' in kwargs:
        profile_filename = kwargs['profile_filename']
        del kwargs['profile_filename']
    else:
        profile_filename = '/tmp/coro_profile.bin'

    if 'profile_bench' in kwargs:
        profile_bench = kwargs['profile_bench']
        del kwargs['profile_bench']
    else:
        profile_bench = coro.rusage_bench

    p = coro.new_profiler (profile_bench)
    p.start()
    try:
        return fun (*args, **kwargs)
    finally:
        total_ticks = p.stop()
        user_ticks = _dump (p, profile_filename)

def start(profile_bench=coro.rusage_bench):
    """Start the profiler.

    :Parameters:
        - `profile_bench`: The profiler type to use.
    """
    p = coro.new_profiler(profile_bench)
    p.start()

def stop(filename='/tmp/coro_profile.bin'):
    """Stop the profiler.

    :Parameters:
        - `filename`: The filename to use for the profile output.
    """
    p = coro.get_the_profiler()
    p.stop()
    _dump(p, filename)

def tak1 (x, y, z):
    if y >= x:
        return z
    else:
        return tak1 (
            tak1 (x - 1, y, z),
            tak2 (y - 1, z, x),
            tak2 (z - 1, x, y)
        )

def tak2 (x, y, z):
    if y >= x:
        return z
    else:
        return tak2 (
            tak2 (x - 1, y, z),
            tak1 (y - 1, z, x),
            tak1 (z - 1, x, y)
        )

if __name__ == '__main__':
    go (tak2, 18, 12, 6)
    print 'now run print_profile.py ...'

########NEW FILE########
__FILENAME__ = read_stream
# -*- Mode: Python -*-

class socket_producer:
    def __init__ (self, conn, buffer_size=8000):
        self.conn = conn
        self.buffer_size = buffer_size

    def next (self):
        return self.conn.recv (self.buffer_size)

def sock_stream (sock):
    return buffered_stream (socket_producer (sock).next)

class buffered_stream:

    def __init__ (self, producer):
        self.producer = producer
        self.buffer = ''

    def gen_read_until (self, delim):
        "generate pieces of input up to and including <delim>, then StopIteration"
        ld = len(delim)
        m = 0
        while 1:
            if not self.buffer:
                self.buffer = self.producer()
                if not self.buffer:
                    # eof
                    yield ''
                    return
            i = 0
            while i < len (self.buffer):
                if self.buffer[i] == delim[m]:
                    m += 1
                    if m == ld:
                        result, self.buffer = self.buffer[:i + 1], self.buffer[i + 1:]
                        yield result
                        return
                else:
                    m = 0
                i += 1
            block, self.buffer = self.buffer, ''
            yield block

    def gen_read_until_dfa (self, dfa):
        "generate pieces of input up to and including a match on <dfa>, then StopIteration"
        m = 0
        while 1:
            if not self.buffer:
                self.buffer = self.producer()
                if not self.buffer:
                    # eof
                    yield ''
                    return
            i = 0
            while i < len (self.buffer):
                if dfa.consume (self.buffer[i]):
                    result, self.buffer = self.buffer[:i + 1], self.buffer[i + 1:]
                    yield result
                    return
                i += 1
            block, self.buffer = self.buffer, ''
            yield block

    def gen_read_exact (self, size):
        "generate pieces of input up to <size> bytes, then StopIteration"
        remain = size
        while remain:
            if len (self.buffer) >= remain:
                result, self.buffer = self.buffer[:remain], self.buffer[remain:]
                yield result
                return
            else:
                piece, self.buffer = self.buffer, self.producer()
                remain -= len (piece)
                yield piece
                if not self.buffer:
                    # eof
                    yield ''
                    return

    def read_until (self, delim, join=True):
        "read until <delim>.  return a list of parts unless <join> is True"
        result = (x for x in self.gen_read_until (delim))
        if join:
            return ''.join (result)
        else:
            return result

    def read_exact (self, size, join=True):
        "read exactly <size> bytes.  return a list of parts unless <join> is True"
        result = (x for x in self.gen_read_exact (size))
        if join:
            return ''.join (result)
        else:
            return result

    def flush (self):
        "flush this stream's buffer"
        result, self.buffer = self.buffer, ''
        return result

    def read_line (self, delim='\r\n'):
        "read a CRLF-delimited line from this stream"
        return self.read_until (delim)

    def read_all (self):
        "read from self.producer until the stream terminates"
        if self.buffer:
            yield self.flush()
        while 1:
            block = self.producer()
            if not block:
                return
            else:
                yield block

########NEW FILE########
__FILENAME__ = signal_handler
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# $Header: //prod/main/ap/shrapnel/coro/signal_handler.py#8 $

"""Signal handler."""

import coro
import signal
import os

UNAME = os.uname()[0]

def register (signum, handler, once_only=False):
    """Register a signal handler.

    :Parameters:
        - `signum`: The signal number to register.
        - `handler`: A callable object that takes one parameter which is the
          signal number that was triggered.
        - `once_only`: If True, will only trigger once, and then disable the
          signal handler.  Defaults to False.

    :Exceptions:
        - `coro.SimultaneousError`: Another handler is already registered for
          this signal.
    """

    if UNAME in ('FreeBSD', 'Darwin'):
        # first, turn *off* normal signal handling...
        signal.signal (signum, signal.SIG_IGN)
        # register with kqueue
        flags = coro.EV.ADD
        if once_only:
            flags |= coro.EV.ONESHOT
        k_handler = lambda x: handler(x.ident)
        coro.set_handler ((signum, coro.EVFILT.SIGNAL), k_handler, flags)
    else:
        signal.signal(signum, handler)


# alias for backward compatibility
register_signal_handler = register

########NEW FILE########
__FILENAME__ = userauth
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# ssh_userauth
#
# This implements the ssh-userauth service for authenticating a user.
#

import os

from coro.ssh.util.packet import unpack_payload, pack_payload, BYTE, STRING, BOOLEAN, NAME_LIST
from coro.ssh.util import debug as ssh_debug
from coro.ssh.util.password import get_password
from coro.ssh.util import safe_string
from coro.ssh.auth import Authentication_Error, Authentication_System

import coro

class Userauth_Method_Not_Allowed_Error(Exception):
    """Userauth_Method_Not_Allowed_Error
    This is raised when it is determined that the method is not supported.
    """

    def __init__(self, auths_that_can_continue):
        Exception.__init__(self, auths_that_can_continue)
        self.auths_that_can_continue = auths_that_can_continue

class Userauth_Authentication_Method:

    """Userauth_Authentication_Method

    This is the base class for various different types of authentication
    methods supported by the userauth service.
    """

    name = ''

    def __init__(self, transport):
        self.transport = transport

    def authenticate(self, username, service_name):
        """authenticate(self, username, service_name) -> None
        Try to authenticate with the remote side.

        Raises Authentication_Error if it fails.
        """
        raise NotImplementedError

    def msg_userauth_failure(self, packet):
        self.transport.debug.write(ssh_debug.DEBUG_1, '%s Auth: Userauth failure.', (self.name,))
        msg, auths_that_can_continue, partial_success = unpack_payload(PAYLOAD_MSG_USERAUTH_FAILURE, packet)
        # XXX: How to handle partial_success?
        if self.name not in auths_that_can_continue:
            self.transport.debug.write(
                ssh_debug.DEBUG_1, '%s Auth: Not in the list of auths that can continue', (self.name,))
            raise Userauth_Method_Not_Allowed_Error(auths_that_can_continue)

class Publickey(Userauth_Authentication_Method):
    name = 'publickey'

    def authenticate(self, username, service_name):
        local_username = os.getlogin()
        for key_storage in self.transport.supported_key_storages:
            self.transport.debug.write(
                ssh_debug.DEBUG_1, 'Publickey Auth: Trying to load keytype "%s" for user "%s".',
                (key_storage.__class__.__name__, local_username))
            loaded_keys = key_storage.load_keys(username=local_username)
            if loaded_keys:
                for loaded_key in loaded_keys:
                    # Test this key type.
                    self.transport.debug.write(
                        ssh_debug.DEBUG_1, 'Publickey Auth: Sending PK test for keytype "%s".', (loaded_key.name,))
                    packet = pack_payload(PAYLOAD_MSG_USERAUTH_REQUEST_PK_TEST,
                                          (SSH_MSG_USERAUTH_REQUEST,
                                           username,
                                           service_name,
                                           'publickey',
                                           0,
                                           loaded_key.name,
                                           loaded_key.get_public_key_blob()
                                           ))
                    self.transport.send_packet(packet)
                    message_type, packet = self.transport.receive_message((SSH_MSG_USERAUTH_PK_OK,
                                                                           SSH_MSG_USERAUTH_FAILURE,
                                                                           ))
                    if message_type == SSH_MSG_USERAUTH_PK_OK:
                        # This public key is ok to try.
                        try:
                            self._try_auth(packet, loaded_key, username, service_name)
                        except Authentication_Error:
                            # Nope, didn't work.  Loop and try next.
                            pass
                        else:
                            # Done!
                            return
                    elif message_type == SSH_MSG_USERAUTH_FAILURE:
                        # Key type not allowed.
                        self.msg_userauth_failure(packet)
                        # Loop through and try next key.
                    else:
                        # Should never happen.
                        raise ValueError(message_type)
                else:
                    self.transport.debug.write(ssh_debug.DEBUG_1, 'Publickey Auth: No more key storage types left.')
            else:
                self.transport.debug.write(ssh_debug.DEBUG_1, 'Publickey Auth: No keys found of this key storage type.')
        else:
            self.transport.debug.write(ssh_debug.DEBUG_1, 'Publickey Auth: No more storage key types left to try.')
            raise Authentication_Error

    def _try_auth(self, packet, loaded_key, username, service_name):
        self.transport.debug.write(ssh_debug.DEBUG_1, 'Publickey Auth: Got OK for this key type.')
        msg, key_algorithm_name, key_blob = unpack_payload(PAYLOAD_MSG_USERAUTH_PK_OK, packet)
        assert (key_algorithm_name == loaded_key.name)
        # XXX: Check key_blob, too?
        # Send the actual request.
        # Compute signature.
        session_id = self.transport.key_exchange.session_id
        sig_data = pack_payload(PAYLOAD_USERAUTH_REQUEST_PK_SIGNATURE,
                                (session_id,
                                 SSH_MSG_USERAUTH_REQUEST,
                                 username,
                                 service_name,
                                 'publickey',
                                 1,
                                 loaded_key.name,
                                 loaded_key.get_public_key_blob()
                                 ))
        signature = loaded_key.sign(sig_data)

        self.transport.debug.write(ssh_debug.DEBUG_1, 'Publickey Auth: Sending userauth request.')
        packet = pack_payload(PAYLOAD_MSG_USERAUTH_REQUEST_PK,
                              (SSH_MSG_USERAUTH_REQUEST,
                               username,
                               service_name,
                               'publickey',
                               1,
                               loaded_key.name,
                               loaded_key.get_public_key_blob(),
                               signature))
        self.transport.send_packet(packet)
        message_type, packet = self.transport.receive_message((SSH_MSG_USERAUTH_SUCCESS,
                                                               SSH_MSG_USERAUTH_FAILURE))
        if message_type == SSH_MSG_USERAUTH_SUCCESS:
            # Success.
            return
        elif message_type == SSH_MSG_USERAUTH_FAILURE:
            self.msg_userauth_failure(packet)
            raise Authentication_Error
        else:
            # Should never happen.
            raise ValueError(message_type)

class Password(Userauth_Authentication_Method):
    name = 'password'

    def authenticate(self, username, service_name):
        password = self.get_password(username)
        packet = pack_payload(PAYLOAD_MSG_USERAUTH_REQUEST_PASSWORD,
                              (SSH_MSG_USERAUTH_REQUEST,
                               username,
                               service_name,
                               'password',
                               0,
                               password
                               ))
        self.transport.send_packet(packet)
        # While loop in case we get a CHANGEREQ packet.
        while 1:
            try:
                message_type, packet = self.transport.receive_message((
                    SSH_MSG_USERAUTH_SUCCESS, SSH_MSG_USERAUTH_FAILURE,
                    SSH_MSG_USERAUTH_PASSWD_CHANGEREQ))
            except EOFError:
                # In case of an expired user, an EOFError is raised
                # Expired accounts are also considered as authentication errors
                raise Authentication_Error
            if message_type == SSH_MSG_USERAUTH_SUCCESS:
                # Success!
                return
            elif message_type == SSH_MSG_USERAUTH_FAILURE:
                self.msg_userauth_failure(packet)
                # XXX: Could ask for user's password again?
                # XXX: Should handle partial_success flag for CHANGEREQ response.
                raise Authentication_Error
            elif message_type == SSH_MSG_USERAUTH_PASSWD_CHANGEREQ:
                self.msg_userauth_passwd_changereq(packet, username, service_name)
            else:
                # Should never happen.
                raise ValueError(message_type)

    def get_password(self, username, prompt=None):
        if prompt is None:
            prompt = '%s\'s password> ' % username
        return get_password(prompt)

    def msg_userauth_passwd_changereq(self, packet, username, service_name):
        # User's password has expired.  Allow the user to enter a new password.
        msg, prompt, language = unpack_payload(PAYLOAD_MSG_USERAUTH_PASSWD_CHANGEREQ, packet)
        print safe_string(prompt)
        old_password = self.get_password('%s\'s old password> ' % username)
        while 1:
            new_password = self.get_password('%s\'s new password> ' % username)
            new_password2 = self.get_password('Retype new password> ')
            if new_password != new_password2:
                print 'Passwords did not match!  Try again.'
            else:
                break
        packet = pack_payload(PAYLOAD_MSG_USERAUTH_REQUEST_CHANGE_PASSWD,
                              (SSH_MSG_USERAUTH_REQUEST,
                               username,
                               service_name,
                               'password',
                               1,
                               old_password,
                               new_password))
        self.transport.send_packet(packet)

# Not implemented, yet.
# class Hostbased(Userauth_Authentication_Method):
#    name = 'hostbased'
#
#    def get_userauth_request(self, username, service_name):
#        pass

class Userauth(Authentication_System):

    name = 'ssh-userauth'
    methods = None

    def __init__(self, transport, username=None):
        if username is None:
            self.username = os.getlogin()
        else:
            self.username = username
        self.transport = transport
        # Instantiate methods.
        self.methods = [Publickey(transport), Password(transport)]

    def authenticate(self, service_name):
        """authenticate(self, service_name) -> None
        Attempts to authenticate with the remote side.
        Assumes you have already confirmed that ssh-userauth is OK to use
        by sending a SSH_MSG_SERVICE_REQUEST packet.  This will try the
        authentication methods listed in self.methods in order until one of
        them works.

        Raises Authentication_Error if none of the authentication methods work.

        <service_name>: The name of the service that you want to use after
                        authenticating.  Typically 'ssh-connection'.
        """
        # Assume that all of our auths can continue.
        # Userauth_Method_Not_Allowed_Error will update this list if we
        # ever receive an error.
        auths_that_can_continue = [method.name for method in self.methods]
        callbacks = {SSH_MSG_USERAUTH_BANNER: self.msg_userauth_banner}
        self.transport.register_callbacks(self.name, callbacks)
        try:
            for method in self.methods:
                if method.name in auths_that_can_continue:
                    self.transport.debug.write(ssh_debug.DEBUG_1, 'Trying authentication method "%s".', (method.name,))
                    try:
                        method.authenticate(self.username, service_name)
                    except Authentication_Error:
                        self.transport.debug.write(
                            ssh_debug.DEBUG_1, 'Authentication method "%s" failed.', (method.name,))
                    except Userauth_Method_Not_Allowed_Error, why:
                        auths_that_can_continue = why.auths_that_can_continue
                    else:
                        # Authentication success.
                        return
            else:
                raise Authentication_Error
        finally:
            self.transport.unregister_callbacks(self.name)

    def msg_userauth_banner(self, packet):
        msg, message, language = unpack_payload(PAYLOAD_MSG_USERAUTH_BANNER, packet)
        print safe_string(message)

# ----------------------------------------------------------------------------------------------------
# server side authentication
# ----------------------------------------------------------------------------------------------------

class Authenticator:

    # maximum number of auth attempts per session
    max_tries = 5
    # amount of time to pause between attempts
    sleep_time = 0.1

    def __init__ (self, transport, methods):
        self.transport = transport
        self.methods = methods
        # XXX this assumes only one of each method type?  e,g, might a user want multiple entries under 'publickey'?
        self.by_name = dict((method.name, method) for method in self.methods)

    def send (self, format, values):
        self.transport.send_packet (pack_payload (format, values))

    def send_failure (self):
        self.send (PAYLOAD_MSG_USERAUTH_FAILURE, (SSH_MSG_USERAUTH_FAILURE, self.by_name.keys(), 0))

    def authenticate (self, service_name):
        session_id = self.transport.key_exchange.session_id
        tries = self.max_tries
        while tries:
            # this is relatively stateless, because the RFC says the client can send requests without bothering
            #  to see if the server even wants them.
            # XXX allow each method to be tried, then removed from the list of
            # possibles? [think about combinations of service/user/etc... though]
            message_type, packet = self.transport.receive_message ((SSH_MSG_USERAUTH_REQUEST,))
            msg, username, service, method = unpack_payload (PAYLOAD_MSG_USERAUTH_REQUEST, packet)
            if method == 'none':
                self.send_failure()
            elif method == 'publickey':
                mprobe = self.by_name.get ('publickey')
                if not mprobe:
                    self.send_failure()
                else:
                    (_, _, _, _, btest, alg, blob) = unpack_payload (PAYLOAD_MSG_USERAUTH_REQUEST_PK_TEST, packet)
                    if not btest:
                        # XXX: ask method if it's ok with serv+user+alg+blob
                        self.send (PAYLOAD_MSG_USERAUTH_PK_OK, (SSH_MSG_USERAUTH_PK_OK, alg, blob))
                    else:
                        (_, _, _, _, _, _, key, sig) = unpack_payload (PAYLOAD_MSG_USERAUTH_REQUEST_PK, packet)
                        self.transport.debug.write (
                            ssh_debug.DEBUG_1, 'Trying Public Key Authentication: %r' % ((service, username, alg),))
                        if mprobe.authenticate (session_id, service, username, alg, key, sig):
                            self.send (PAYLOAD_MSG_USERAUTH_SUCCESS, (SSH_MSG_USERAUTH_SUCCESS,))
                            return True
                        else:
                            tries -= 1
                            coro.sleep_relative (self.sleep_time)
                            self.send_failure()
            elif method == 'password':
                mprobe = self.by_name.get ('password')
                if not mprobe:
                    self.send_failure()
                else:
                    (_, _, _, _, btest, password) = unpack_payload (PAYLOAD_MSG_USERAUTH_REQUEST_PASSWORD, packet)
                    self.transport.debug.write (
                        ssh_debug.DEBUG_1, 'Trying Password Authentication: %r' % ((service, username,),))
                    if btest:
                        self.transport.debug.write (
                            ssh_debug.DEBUG_1, 'Client side trying to change password: Not Yet Implemented')
                        return send_failure()
                    elif mprobe.authenticate (service, username, password):
                        self.send (PAYLOAD_MSG_USERAUTH_SUCCESS, (SSH_MSG_USERAUTH_SUCCESS,))
                        return True
                    else:
                        tries -= 1
                        coro.sleep_relative (self.sleep_time)
                        self.send_failure()
            else:
                tries -= 1
                coro.sleep_relative (self.sleep_time)
                self.send_failure()

class Public_Key_Authenticator:

    name = 'publickey'

    # { <user> : { <service>: [key0, key1, ...], ...}, ... }

    def __init__ (self, keys):
        self.keys = keys

    def authenticate (self, session_id, serv, user, alg, blob, sig):
        # build the signable data
        to_sign = pack_payload (
            PAYLOAD_USERAUTH_REQUEST_PK_SIGNATURE, (
                session_id, SSH_MSG_USERAUTH_REQUEST, user, serv, 'publickey', 1, alg, blob
            )
        )
        try:
            keys = self.keys[user][serv]
            for key in keys:
                if key.name == alg and key.get_public_key_blob() == blob and key.verify (to_sign, sig):
                    return True
            return False
        except KeyError:
            return False

class Password_Authenticator:

    name = 'password'

    # { <user> : { <service>: <pwd>, ...}, ... }

    def __init__ (self, pwds):
        self.pwds = pwds

    def authenticate (self, service, username, password):
        try:
            return self.pwds[username][service] == password
        except KeyError:
            return False

# ----------------------------------------------------------------------------------------------------

SSH_MSG_USERAUTH_REQUEST    = 50
SSH_MSG_USERAUTH_FAILURE    = 51
SSH_MSG_USERAUTH_SUCCESS    = 52
SSH_MSG_USERAUTH_BANNER     = 53

SSH_MSG_USERAUTH_PK_OK      = 60
SSH_MSG_USERAUTH_PASSWD_CHANGEREQ = 60

# this is the 'generic header' of all auth requests
PAYLOAD_MSG_USERAUTH_REQUEST = (
    BYTE,    # SSH_MSG_USERAUTH_REQUEST
    STRING,  # username
    STRING,  # service
    STRING,  # method
)

PAYLOAD_MSG_USERAUTH_FAILURE = (BYTE,      # SSH_MSG_USERAUTH_FAILURE
                                NAME_LIST,  # authentications that can continue
                                BOOLEAN)   # partial success

PAYLOAD_MSG_USERAUTH_SUCCESS = (BYTE,)   # SSH_MSG_USERAUTH_SUCCESS

PAYLOAD_MSG_USERAUTH_BANNER = (BYTE,     # SSH_MSG_USERAUTH_BANNER
                               STRING,   # message
                               STRING)   # language tag

PAYLOAD_MSG_USERAUTH_REQUEST_PK_TEST = (BYTE,    # SSH_MSG_USERAUTH_REQUEST
                                        STRING,  # username
                                        STRING,  # service
                                        STRING,  # "publickey"
                                        BOOLEAN,  # FALSE
                                        STRING,  # public key algorithm name
                                        STRING)  # public key blob

PAYLOAD_MSG_USERAUTH_REQUEST_PK = (BYTE,    # SSH_MSG_USERAUTH_REQUEST
                                   STRING,  # username
                                   STRING,  # service
                                   STRING,  # "publickey"
                                   BOOLEAN,  # TRUE
                                   STRING,  # public key algorithm name
                                   STRING,  # public key blob
                                   STRING)  # signature

PAYLOAD_USERAUTH_REQUEST_PK_SIGNATURE = (STRING,  # session identifier
                                         BYTE,    # SSH_MSG_USERAUTH_REQUEST
                                         STRING,  # username
                                         STRING,  # service
                                         STRING,  # "publickey"
                                         BOOLEAN,  # TRUE
                                         STRING,  # public key algorithm name
                                         STRING)  # public key to be used for authentication

PAYLOAD_MSG_USERAUTH_PK_OK = (BYTE,      # SSH_MSG_USERAUTH_PK_OK
                              STRING,    # public key algorithm name from the request
                              STRING)    # public key blob from the request

PAYLOAD_MSG_USERAUTH_REQUEST_PASSWORD = (BYTE,   # SSH_MSG_USERAUTH_REQUEST
                                         STRING,  # username
                                         STRING,  # service
                                         STRING,  # "password"
                                         BOOLEAN,  # FALSE
                                         STRING)  # plaintext password

PAYLOAD_MSG_USERAUTH_PASSWD_CHANGEREQ = (BYTE,   # SSH_MSG_USERAUTH_PASSWD_CHANGEREQ
                                         STRING,  # prompt
                                         STRING)  # language tag

PAYLOAD_MSG_USERAUTH_REQUEST_CHANGE_PASSWD = (BYTE,      # SSH_MSG_USERAUTH_REQUEST
                                              STRING,    # username
                                              STRING,    # service
                                              STRING,    # "password"
                                              BOOLEAN,   # TRUE
                                              STRING,    # plaintext old password
                                              STRING)    # plaintext new password

PAYLOAD_MSG_USERAUTH_REQUEST_HOSTBASED = (BYTE,      # SSH_MSG_USERAUTH_REQUEST
                                          STRING,    # username
                                          STRING,    # service
                                          STRING,    # "hostbased"
                                          STRING,    # public key algorithm for host key
                                          STRING,    # public host key and certificates for client host
                                          STRING,    # client host name
                                          STRING,    # username on the client host
                                          STRING)    # signature

PAYLOAD_USERAUTH_REQUEST_HOSTBASED_SIGNATURE = (STRING,  # session identifier
                                                BYTE,    # SSH_MSG_USERAUTH_REQUEST
                                                STRING,  # username
                                                STRING,  # service
                                                STRING,  # "hostbased"
                                                STRING,  # public key algorithm for host key
                                                STRING,  # public host key and certificates for client host
                                                STRING,  # client host name
                                                STRING)  # username on the client host

########NEW FILE########
__FILENAME__ = blowfish_cbc
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.cipher.blowfish_cbc
#
# Implements the Blowfish cipher in CBC mode.
#

from coro.ssh.cipher import SSH_Cipher_Method
from Crypto.Cipher import Blowfish

class Blowfish_CBC(SSH_Cipher_Method):

    name = 'blowfish-cbc'
    block_size = 8
    key_size = 16
    iv_size = 8
    cipher = None

    def encrypt(self, data):
        return self.cipher.encrypt(data)

    def descrypt(self, data):
        return self.cipher.decrypt(data)

    def set_encryption_key_and_iv(self, key, IV):
        self.key = key
        self.IV = IV
        self.cipher = Blowfish.new(key, Blowfish.MODE_CBC, IV)

########NEW FILE########
__FILENAME__ = des3_cbc
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.cipher.des3_cbc
#
# Implements the Triple DES cipher in CBC mode.
#

from coro.ssh.cipher import SSH_Cipher_Method
from Crypto.Cipher import DES3

class Triple_DES_CBC(SSH_Cipher_Method):

    name = '3des-cbc'
    block_size = 8
    key_size = 24
    iv_size = 8
    cipher = None

    def encrypt(self, data):
        return self.cipher.encrypt(data)

    def decrypt(self, data):
        return self.cipher.decrypt(data)

    def set_encryption_key_and_iv(self, key, IV):
        self.key = key
        self.IV = IV
        self.cipher = DES3.new(key, DES3.MODE_CBC, IV)

########NEW FILE########
__FILENAME__ = none
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.cipher.none
#
# This is the 'none' cipher.  It passes the data through without encryption.
#

from coro.ssh.cipher import SSH_Cipher_Method

class Cipher_None(SSH_Cipher_Method):

    def encrypt(self, data):
        return data

    def decrypt(self, data):
        return data

########NEW FILE########
__FILENAME__ = none
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.compression.none
#
# This is the 'none' compression method.
# It passes the data through uncompressed.
#

from coro.ssh.compression import SSH_Compression_Method

class Compression_None(SSH_Compression_Method):

    def compress(self, data):
        return data

########NEW FILE########
__FILENAME__ = channel
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.connection.channel
#
# This is the base class for developing channels
# used by the ssh_connect service.
#

import coro

from coro.ssh.connection.data_buffer import Buffer
from coro.ssh.connection.constants import *
from coro.ssh.util import packet, debug

class Channel_Request_Failure(Exception):
    pass

class Channel_Open_Error(Exception):
    def __init__(self, channel_id, reason_code, reason_text, language):
        Exception.__init__(self, channel_id, reason_code, reason_text, language)
        self.channel_id = channel_id
        self.reason_code = reason_code
        self.reason_text = reason_text
        self.language = language

    def __str__(self):
        if self.reason_code in channel_open_error_strings:
            reason_code_str = ' (%s)' % channel_open_error_strings[self.reason_code]
        else:
            reason_code_str = ''
        return 'Channel ID %i Open Error: %i%s: %r' % \
               (self.channel_id, self.reason_code, reason_code_str, self.reason_text)

class Channel_Closed_Error(Exception):
    pass

# List of channel open error codes.
SSH_OPEN_ADMINISTRATIVELY_PROHIBITED    = 1
SSH_OPEN_CONNECT_FAILED                 = 2
SSH_OPEN_UNKNOWN_CHANNEL_TYPE           = 3
SSH_OPEN_RESOURCE_SHORTAGE              = 4

channel_open_error_strings = {
    SSH_OPEN_ADMINISTRATIVELY_PROHIBITED: 'administratively prohibited',
    SSH_OPEN_CONNECT_FAILED: 'connect failed',
    SSH_OPEN_UNKNOWN_CHANNEL_TYPE: 'unknown channel type',
    SSH_OPEN_RESOURCE_SHORTAGE: 'resource shortage',
}

class Channel:
    name = ''
    channel_id = 0
    window_size = 131072  # 128k
    max_packet_size = 131072  # 128k
    # Additional ssh.util.packet data types used in the CHANNEL_OPEN message.
    additional_packet_data_types = ()

    # This is a flag you can change if you want to handle extended data
    # differently.
    treat_extended_data_as_regular = 1

    # This is how many bytes the remote side can send.
    # Once it hits zero, I start ignoring data.
    # The current algorithm is to increase the window size back to the
    # initial size whenever this number drops below one half.
    window_data_left = window_size

    # Condition variable triggered whenever the window is updated.
    window_data_added_cv = None

    closed = 1
    eof = 1

    # This is the instance that is created for data flowing in from the other
    # side.
    remote_channel = None

    # This is a buffer of data received.  It is a Buffer instance.
    # A value of '' in the buffer indicates EOF.
    recv_buffer = None
    # This is a buffer of extended data received.
    # The key is the data_type_code, and the value is a Buffer instance.
    extended_recv_buffer = None

    # This is a condition variable triggered when channel request responses
    # are received.  The thread is awoken with a boolean value that indicates
    # whether or not the request succeeded.
    # XXX: There is a problem that this doesn't handle concurrent requests.
    channel_request_cv = None

    # This is a condition variable triggered when open success or failure
    # is received.
    channel_open_cv = None

    def __init__(self, connection_service):
        self.connection_service = connection_service
        # Local reference for convenience.
        self.transport = connection_service.transport
        self.recv_buffer = Buffer()
        self.extended_recv_buffer = {}
        self.remote_channel = Remote_Channel()
        self.window_data_added_cv = coro.condition_variable()
        self.channel_request_cv = coro.condition_variable()
        self.channel_open_cv = coro.condition_variable()

    def __str__(self):
        return '<Channel %s ID:%i>' % (self.name, self.channel_id)

    def get_additional_open_data(self):
        """get_additional_open_data(self) -> data
        Returns the additional information used for opening a channel.
        <data> is a tuple of the actual data that is appended to the packet.
        """
        # No additional data by default.
        return ()

    def set_additional_open_data(self, data):
        """set_additional_open_data(self, data) -> None
        Sets the additional data information.
        <data> is a tuple of the data elements specific to this channel type.
        """
        # By default ignore any open data.
        return None

    def send_channel_request(self, request_type, payload, data, want_reply=1, default_reply_handler=True):
        """send_channel_request(self, request_type, payload, data, want_reply=1, default_reply_handler=True) -> None
        This is a generic mechanism for sending a channel request packet.

        <request_type>: Request type to send.
        <payload>: The ssh_packet format definition.
        <data>: The data to go with the payload.
        <want_reply>: The want_reply flag.
        <default_reply_handler>: If true and want_reply is True, then the
                default reply handler will be used.  The default reply
                handler is pretty simple.  Will just return if
                CHANNEL_REQUEST_SUCCESS is received.  Will raise
                Channel_Request_Failure if FAILURE is received.
        """
        # XXX: There is a problem with the spec.  It does not indicate how to
        #      match requests with responses.  IIRC, they talked about it on
        #      the mailing list and updated the spec.  Need to investigate if
        #      they have clarified the spec on how to handle this.
        # XXX: Or maybe concurrent channel requests on the same channel are
        #      not allowed?
        if self.remote_channel.closed:
            raise Channel_Closed_Error
        pkt = packet.pack_payload(SSH_MSG_CHANNEL_REQUEST_PAYLOAD,
                                  (SSH_MSG_CHANNEL_REQUEST,
                                   self.remote_channel.channel_id,
                                   request_type,
                                   int(want_reply)))
        pkt_data = packet.pack_payload(payload, data)
        self.transport.send_packet(pkt + pkt_data)
        if want_reply and default_reply_handler:
            # Wait for response.
            assert len(self.channel_request_cv) == 0, 'Concurrent channel requests not supported!'
            if not self.channel_request_cv.wait():
                raise Channel_Request_Failure

    def append_data_received(self, data):
        """append_data_received(self, data) -> None
        Indicates that the given data was received.
        """
        if data:
            self.recv_buffer.write(data)

    def append_extended_data_received(self, data_type_code, data):
        """append_extended_data_received(self, data_type_code, data) -> None
        Indicates that the given extended data was received.
        """
        if data:
            if self.treat_extended_data_as_regular:
                self.append_data_received(data)
            else:
                if data_type_code in self.extended_recv_buffer:
                    self.extended_recv_buffer[data_type_code].write(data)
                else:
                    b = Buffer()
                    b.write(data)
                    self.extended_recv_buffer[data_type_code] = b

    def set_eof(self):
        """set_eof(self) -> None
        Indicate that there is no more data on this channel.
        """
        self.eof = 1
        self.recv_buffer.write('')
        for b in self.extended_recv_buffer.values():
            b.write('')

    def handle_request(self, request_type, want_reply, type_specific_packet_data):
        # Default is always to fail.  Specific channel types override this method.
        if want_reply:
            pkt = packet.pack_payload(SSH_MSG_CHANNEL_FAILURE_PAYLOAD,
                                      (SSH_MSG_CHANNEL_FAILURE,
                                       self.remote_channel.channel_id,))
            self.transport.send_packet(pkt)

    def open(self):
        """open(self) -> None
        Opens the channel to the remote side.
        """
        assert self.closed
        self.connection_service.register_channel(self)
        self.transport.debug.write(debug.DEBUG_2, 'sending channel open request channel ID %i', (self.channel_id,))

        # Send the open request.
        additional_data = self.get_additional_open_data()
        packet_payload = SSH_MSG_CHANNEL_OPEN_PAYLOAD + self.additional_packet_data_types
        packet_data = (SSH_MSG_CHANNEL_OPEN,
                       self.name,
                       self.channel_id,
                       self.window_size,
                       self.max_packet_size) + additional_data
        pkt = packet.pack_payload(packet_payload, packet_data)
        self.transport.send_packet(pkt)
        success, data = self.channel_open_cv.wait()
        if success:
            self.set_additional_open_data(data)
        else:
            reason_code, reason_text, language = data
            raise Channel_Open_Error(self.channel_id, reason_code, reason_text, language)

    def close(self):
        """close(self) -> None
        Tell remote side to close its channel.
        Our side is not considered "closed" until after we receive
        SSH_MSG_CHANNEL_CLOSE from the remote side.
        """
        if not self.remote_channel.closed:
            self.remote_channel.closed = 1
            pkt = packet.pack_payload(SSH_MSG_CHANNEL_CLOSE_PAYLOAD,
                                      (SSH_MSG_CHANNEL_CLOSE,
                                       self.remote_channel.channel_id))
            self.transport.send_packet(pkt)

            # We need to cause any threads that were trying to write on
            # this channel to stop trying to write. If they were asleep
            # waiting for one of the three condition variables, we need to
            # wake them up. They will notice that self.remote_channel.closed
            # is now true, and will do the right thing.
            self.window_data_added_cv.wake_all()
            self.channel_request_cv.wake_all(False)
            self.channel_open_cv.wake_all((False, (SSH_OPEN_CONNECT_FAILED,
                                                   'Channel has been closed',
                                                   None)))

    def send_window_adjustment(self, bytes_to_add):
        self.transport.debug.write(debug.DEBUG_2, 'sending window adjustment to add %i bytes', (bytes_to_add,))
        pkt = packet.pack_payload(SSH_MSG_CHANNEL_WINDOW_ADJUST_PAYLOAD,
                                  (SSH_MSG_CHANNEL_WINDOW_ADJUST,
                                   self.remote_channel.channel_id,
                                   bytes_to_add))
        self.transport.send_packet(pkt)
        self.window_data_left += bytes_to_add

    def has_data_to_read(self, extended=None):
        """has_data_to_read(self, extended=None) -> boolean
        Returns whether or not there is data available to read.

        <extended>: data_type_code of extended data type to read.
                    Set to None for normal data.
        """
        if extended is None:
            b = self.recv_buffer
        else:
            if extended in self.extended_recv_buffer:
                b = self.extended_recv_buffer[extended]
            else:
                return False
        if b and b.fifo.peek() != '':
            return True
        else:
            return False

    def _check_window_adjust(self):
        if self.window_data_left < self.window_size / 2:
            # Increase the window so that the other side may send more data.
            self.send_window_adjustment(self.window_size - self.window_data_left)

    def read(self, bytes, extended=None):
        """read(self, bytes, extended=None) -> data
        Read data off the channel.
        Reads at most <bytes> bytes.  It may return less than <bytes> even
        if there is more data in the buffer.

        <bytes>: Number of bytes to read.
        <extended>: data_type_code of extended data type to read.
                    Set to None to read normal data.
        """
        if extended is not None:
            if extended not in self.extended_recv_buffer:
                self.extended_recv_buffer[extended] = Buffer()
            b = self.extended_recv_buffer[extended]
        else:
            b = self.recv_buffer
        result = b.read_at_most(bytes)
        # Only adjust the window when the buffer is clear.
        if not b:
            self._check_window_adjust()
        return result

    def read_exact(self, bytes, extended=None):
        """read_exact(self, bytes, extended=None) -> data
        Read exactly <bytes> number of bytes off the channel.
        May return less than <bytes> bytes if EOF is reached.

        <bytes>: Number of bytes to read.  Blocks until enough data is
                 available.
        <extended>: data_type_code of extended data type to read.
                    Set to None to read normal data.
        """
        if extended is not None:
            if extended not in self.extended_recv_buffer:
                self.extended_recv_buffer[extended] = Buffer()
            b = self.extended_recv_buffer[extended]
        else:
            b = self.recv_buffer

        result = []
        bytes_left = bytes
        while bytes_left > 0:
            data = b.read_at_most(bytes_left)
            if not data:
                if result:
                    return ''.join(result)
                else:
                    raise EOFError
            result.append(data)
            bytes_left -= len(data)
            # Only adjust the window when the buffer is clear.
            if not b:
                self._check_window_adjust()
        return ''.join(result)

    # Make an alias for convenience.
    recv = read

    def send(self, data):
        """send(self, data) -> None
        Send the given data string.
        """
        data_start = 0
        while data_start < len(data):
            if self.remote_channel.closed:
                raise Channel_Closed_Error
            while self.remote_channel.window_data_left == 0:
                # Currently waiting for window update.
                self.window_data_added_cv.wait()
                # check again inside loop since if we're closed, the window
                # might never update
                if self.remote_channel.closed:
                    raise Channel_Closed_Error
            # Send what we can.
            max_size = min(self.remote_channel.window_data_left, self.remote_channel.max_packet_size)
            data_to_send = data[data_start:data_start + max_size]
            data_start += max_size

            pkt = packet.pack_payload(SSH_MSG_CHANNEL_DATA_PAYLOAD,
                                      (SSH_MSG_CHANNEL_DATA,
                                       self.remote_channel.channel_id,
                                       data_to_send))
            self.transport.debug.write(debug.DEBUG_3, 'channel %i window lowered by %i to %i',
                                       (self.remote_channel.channel_id, len(data_to_send),
                                        self.remote_channel.window_data_left))
            self.remote_channel.window_data_left -= len(data_to_send)
            self.transport.send_packet(pkt)

    def send_extended(self, data, data_type_code):
        """send_extended(self, data, data_type_code) -> None
        Send the given data string as extended data with the given data_type_code.
        """
        data_start = 0

        while data_start < len(data):
            if self.remote_channel.closed:
                raise Channel_Closed_Error
            while self.remote_channel.window_data_left == 0:
                # Currently waiting for window update.
                self.window_data_added_cv.wait()
                # check again inside loop since if we're closed, the window
                # might never update
                if self.remote_channel.closed:
                    raise Channel_Closed_Error

            # Send what we can.
            max_size = min(self.remote_channel.window_data_left, self.remote_channel.max_packet_size)
            data_to_send = data[data_start:data_start + max_size]
            data_start += max_size

            pkt = packet.pack_payload(SSH_MSG_CHANNEL_EXTENDED_DATA_PAYLOAD,
                                      (SSH_MSG_CHANNEL_EXTENDED_DATA,
                                       self.remote_channel.channel_id,
                                       data_type_code,
                                       data_to_send))
            self.remote_channel.window_data_left -= len(data_to_send)
            self.transport.send_packet(pkt)

    def channel_request_success(self):
        """channel_request_success(self) -> None
        This is called whenever a CHANNEL_SUCCESS message is received.
        """
        self.channel_request_cv.wake_one(args=True)

    def send_channel_request_success (self):
        self.transport.send (SSH_MSG_CHANNEL_SUCCESS_PAYLOAD, (SSH_MSG_CHANNEL_SUCCESS, self.remote_channel.channel_id))

    def channel_request_failure(self):
        """channel_request_success(self) -> None
        This is called whenever a CHANNEL_FAILURE message is received.
        """
        self.channel_request_cv.wake_one(args=False)

    def send_channel_request_failure (self):
        self.transport.send (SSH_MSG_CHANNEL_FAILURE_PAYLOAD, (SSH_MSG_CHANNEL_FAILURE, self.remote_channel.channel_id))

    def channel_open_success(self, data):
        """channel_open_success(self, data) -> None
        Indicates the channel is opened.

        <data> is a tuple of the data elements specific to this channel type.
        """
        # Default is to ignore any extra data.
        assert len(self.channel_open_cv) == 1
        self.channel_open_cv.wake_one((True, data))

    def channel_open_failure(self, reason_code, reason_text, language):
        """channel_open_failure(self, reason_code, reason_text, language) -> None
        This is called when opening a channel fails.
        """
        assert len(self.channel_open_cv) == 1
        self.channel_open_cv.wake_one((False, (reason_code, reason_text, language)))

class Remote_Channel:
    channel_id = 0
    window_size = 131072  # 128k
    max_packet_size = 131072  # 128k

    # This is how many bytes I can send to the remote side.
    # Once it hits zero, I start buffering data.
    window_data_left = window_size

    closed = 1
    eof = 1

########NEW FILE########
__FILENAME__ = connect
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.connection.connect
#
# This implements the SSH Connect service.  This service can run interactive
# login sessions, remote execution of comments, forwarded TCP/IP connections,
# and forwarded X11 connections.  These channels can be multiplexed into a
# single encrypted tunnel.
#

from coro.ssh.transport import SSH_Service
from coro.ssh.util import packet as ssh_packet
from coro.ssh.util import debug as ssh_debug
from coro.ssh.connection.channel import Channel
from constants import *

class Connection_Service(SSH_Service):
    name = 'ssh-connection'

    # This is a counter used to assign local channel ID's.
    next_channel_id = 0

    # This is a dictionary of channels.
    # The key is the channel ID, the value is the Channel object.
    local_channels = None
    # This is a dictionary of remote channels.
    # The key is the remote channel ID, the value is a Remote_Channel object.
    remote_channels = None

    def __init__(self, transport, new_channel_class=None):
        self.transport = transport
        self.local_channels = {}
        self.remote_channels = {}
        self.new_channel_class = new_channel_class

        callbacks = {SSH_MSG_GLOBAL_REQUEST: self.msg_global_request,
                     SSH_MSG_CHANNEL_WINDOW_ADJUST: self.msg_channel_window_adjust,
                     SSH_MSG_CHANNEL_DATA: self.msg_channel_data,
                     SSH_MSG_CHANNEL_EXTENDED_DATA: self.msg_channel_extended_data,
                     SSH_MSG_CHANNEL_EOF: self.msg_channel_eof,
                     SSH_MSG_CHANNEL_CLOSE: self.msg_channel_close,
                     SSH_MSG_CHANNEL_REQUEST: self.msg_channel_request,
                     SSH_MSG_CHANNEL_SUCCESS: self.msg_channel_success,
                     SSH_MSG_CHANNEL_FAILURE: self.msg_channel_failure,
                     SSH_MSG_CHANNEL_OPEN_CONFIRMATION: self.msg_channel_open_confirmation,
                     SSH_MSG_CHANNEL_OPEN_FAILURE: self.msg_channel_open_failure,
                     # server side
                     SSH_MSG_CHANNEL_OPEN: self.msg_channel_open,
                     }
        self.transport.register_callbacks('ssh-connection', callbacks)

    def register_channel(self, channel):
        """register_channel(self, channel) -> None
        When opening a channel, this function is called to add it to the
        local_channels dictionary and to set the channel id.
        """
        channel.channel_id = self.next_channel_id
        assert channel.channel_id not in self.local_channels
        self.next_channel_id += 1               # XXX: Overflow?
        self.local_channels[channel.channel_id] = channel

    def msg_global_request(self, pkt):
        # XXX: finish this (it's a server thing)
        data, offset = ssh_packet.unpack_payload_get_offset(SSH_MSG_GLOBAL_REQUEST_PAYLOAD, pkt)
        msg, request_name, want_reply = data
        raise NotImplementedError

    def msg_channel_window_adjust(self, pkt):
        msg, channel_id, bytes_to_add = ssh_packet.unpack_payload(SSH_MSG_CHANNEL_WINDOW_ADJUST_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        channel.remote_channel.window_data_left += bytes_to_add
        self.transport.debug.write(ssh_debug.DEBUG_3, 'channel %i window increased by %i to %i',
                                   (channel.remote_channel.channel_id, bytes_to_add,
                                    channel.remote_channel.window_data_left))
        channel.window_data_added_cv.wake_all()

    def msg_channel_data(self, pkt):
        msg, channel_id, data = ssh_packet.unpack_payload(SSH_MSG_CHANNEL_DATA_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        # XXX: In theory, we should verify that len(data) <= channel.max_packet_size
        if len(data) > channel.window_data_left:
            self.transport.debug.write(ssh_debug.WARNING, 'channel %i %i bytes overflowed window of %i',
                                       (channel.channel_id, len(data), channel.remote_channel.window_data_left))
            # Data is ignored.
        else:
            channel.window_data_left -= len(data)
            channel.append_data_received(data)

    def msg_channel_extended_data(self, pkt):
        msg, channel_id, data_type_code, data = ssh_packet.unpack_payload(SSH_MSG_CHANNEL_EXTENDED_DATA_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        if len(data) > channel.window_data_left:
            self.transport.debug.write(ssh_debug.WARNING, 'channel %i %i bytes overflowed window of %i',
                                       (channel.channel_id, len(data), channel.remote_channel.window_data_left))
            # Data is ignored.
        else:
            channel.window_data_left -= len(data)
            channel.append_extended_data_received(data_type_code, data)

    def msg_channel_eof(self, pkt):
        msg, channel_id = ssh_packet.unpack_payload(SSH_MSG_CHANNEL_EOF_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        # assert it is not already closed?
        channel.set_eof()

    def msg_channel_close(self, pkt):
        msg, channel_id = ssh_packet.unpack_payload(SSH_MSG_CHANNEL_CLOSE_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        del self.local_channels[channel_id]
        del self.remote_channels[channel.remote_channel.channel_id]
        # assert it is not already closed?
        channel.closed = 1
        if not channel.remote_channel.closed:
            # Close the other side.
            channel.close()
        channel.set_eof()

    def msg_channel_request(self, pkt):
        data, offset = ssh_packet.unpack_payload_get_offset(SSH_MSG_CHANNEL_REQUEST_PAYLOAD, pkt)
        msg, channel_id, request_type, want_reply = data
        channel = self.local_channels[channel_id]
        channel.handle_request(request_type, want_reply, pkt[offset:])

    def msg_channel_success(self, pkt):
        msg, channel_id = ssh_packet.unpack_payload(SSH_MSG_CHANNEL_SUCCESS_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        channel.channel_request_success()

    def msg_channel_failure(self, pkt):
        msg, channel_id = ssh_packet.unpack_payload(SSH_MSG_CHANNEL_FAILURE_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        channel.channel_request_failure()

    def msg_channel_open_confirmation(self, pkt):
        data, offset = ssh_packet.unpack_payload_get_offset(SSH_MSG_CHANNEL_OPEN_CONFIRMATION_PAYLOAD, pkt)
        msg, recipient_channel, sender_channel, window_size, max_packet_size = data
        self.transport.debug.write(
            ssh_debug.DEBUG_1,
            'channel %i open confirmation sender_channel=%i window_size=%i max_packet_size=%i',
            (recipient_channel, sender_channel, window_size, max_packet_size))
        channel = self.local_channels[recipient_channel]
        # XXX: Assert that the channel is not already open?
        channel.closed = 0
        channel.eof = 0
        channel.remote_channel.closed = 0
        channel.remote_channel.channel_id = sender_channel
        assert sender_channel not in self.remote_channels
        self.remote_channels[sender_channel] = channel.remote_channel
        channel.remote_channel.window_size = window_size
        channel.remote_channel.window_data_left = window_size
        channel.remote_channel.max_packet_size = max_packet_size
        additional_data = ssh_packet.unpack_payload(channel.additional_packet_data_types, pkt, offset)
        channel.channel_open_success(additional_data)

    def msg_channel_open_failure(self, pkt):
        msg, channel_id, reason_code, reason_text, language = ssh_packet.unpack_payload(
            SSH_MSG_CHANNEL_OPEN_FAILURE_PAYLOAD, pkt)
        channel = self.local_channels[channel_id]
        # XXX: Assert that the channel is not already open?
        channel.channel_open_failure(reason_code, reason_text, language)

    # --------------------------------------------------------------------------------
    # server side
    # --------------------------------------------------------------------------------
    def msg_channel_open (self, pkt):
        _, channel_type, remote_id, initial_window, max_packet_size = ssh_packet.unpack_payload (
            SSH_MSG_CHANNEL_OPEN_PAYLOAD, pkt)
        channel = self.new_channel_class (self)
        self.register_channel (channel)
        channel.remote_channel.closed = 0
        self.transport.send (
            SSH_MSG_CHANNEL_OPEN_CONFIRMATION_PAYLOAD, (
                SSH_MSG_CHANNEL_OPEN_CONFIRMATION,
                remote_id,
                channel.channel_id,
                initial_window,
                max_packet_size,
            )
        )
        self.transport.debug.write (
            ssh_debug.DEBUG_1,
            'channel %i open confirmation sender_channel=%i window_size=%i max_packet_size=%i', (
                remote_id, channel.channel_id, initial_window, max_packet_size
            )
        )

########NEW FILE########
__FILENAME__ = constants
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.connect.constants
#
# Constants used in the connect protocol.

from coro.ssh.util import packet

SSH_MSG_GLOBAL_REQUEST                 = 80
SSH_MSG_GLOBAL_REQUEST_SUCCESS         = 81
SSH_MSG_GLOBAL_REQUEST_FAILURE         = 82
SSH_MSG_CHANNEL_OPEN                   = 90
SSH_MSG_CHANNEL_OPEN_CONFIRMATION      = 91
SSH_MSG_CHANNEL_OPEN_FAILURE           = 92
SSH_MSG_CHANNEL_WINDOW_ADJUST          = 93
SSH_MSG_CHANNEL_DATA                   = 94
SSH_MSG_CHANNEL_EXTENDED_DATA          = 95
SSH_MSG_CHANNEL_EOF                    = 96
SSH_MSG_CHANNEL_CLOSE                  = 97
SSH_MSG_CHANNEL_REQUEST                = 98
SSH_MSG_CHANNEL_SUCCESS                = 99
SSH_MSG_CHANNEL_FAILURE                = 100

# This is the basic open payload.  Different session types may add
# additional information to this.
SSH_MSG_CHANNEL_OPEN_PAYLOAD = (packet.BYTE,        # SSH_MSG_CHANNEL_OPEN
                                packet.STRING,      # channel type
                                packet.UINT32,      # sender channel
                                packet.UINT32,      # initial window size
                                packet.UINT32)      # maximum packet size

# This is the basic confirmation payload.  Different session types may add
# addition information to this.
SSH_MSG_CHANNEL_OPEN_CONFIRMATION_PAYLOAD = (packet.BYTE,   # SSH_MSG_CHANNEL_OPEN_CONFIRMATION,
                                             packet.UINT32,  # recipient channel
                                             packet.UINT32,  # sender channel
                                             packet.UINT32,  # initial window size
                                             packet.UINT32)  # maximum packet size

SSH_MSG_CHANNEL_OPEN_FAILURE_PAYLOAD = (packet.BYTE,    # SSH_MSG_CHANNEL_OPEN_FAILURE
                                        packet.UINT32,  # recipient channel
                                        packet.UINT32,  # reason_code
                                        packet.STRING,  # reason_text
                                        packet.STRING)  # language

SSH_MSG_CHANNEL_CLOSE_PAYLOAD = (packet.BYTE,       # SSH_MSG_CHANNEL_CLOSE
                                 packet.UINT32)     # recipient_channel


# This may contain additional request-specific data.
SSH_MSG_GLOBAL_REQUEST_PAYLOAD = (packet.BYTE,      # SSH_MSG_GLOBAL_REQUEST
                                  packet.STRING,    # request name
                                  packet.BOOLEAN)   # want reply

# This may contain additional request-specific data.
SSH_MSG_GLOBAL_REQUEST_SUCCESS_PAYLOAD = (packet.BYTE)     # SSH_MSG_GLOBAL_REQUEST_SUCCESS

SSH_MSG_GLOBAL_REQUEST_FAILURE_PAYLOAD = (packet.BYTE)     # SSH_MSG_GLOBAL_REQUEST_FAILURE

SSH_MSG_CHANNEL_WINDOW_ADJUST_PAYLOAD = (packet.BYTE,   # SSH_MSG_CHANNEL_WINDOW_ADJUST
                                         packet.UINT32,  # recipient channel
                                         packet.UINT32)  # bytes to add

SSH_MSG_CHANNEL_DATA_PAYLOAD = (packet.BYTE,        # SSH_MSG_CHANNEL_DATA
                                packet.UINT32,      # recipient channel
                                packet.STRING)      # data

SSH_MSG_CHANNEL_EXTENDED_DATA_PAYLOAD = (packet.BYTE,   # SSH_MSG_CHANNEL_EXTENDED_DATA
                                         packet.UINT32,  # recipient channel
                                         packet.UINT32,  # data_type_code
                                         packet.STRING)  # data

SSH_EXTENDED_DATA_STDERR = 1

SSH_MSG_CHANNEL_EOF_PAYLOAD = (packet.BYTE,     # SSH_MSG_CHANNEL_EOF
                               packet.UINT32)   # recipient channel

# This may contain additional request-specific data.
SSH_MSG_CHANNEL_REQUEST_PAYLOAD = (packet.BYTE,     # SSH_MSG_CHANNEL_REQUEST
                                   packet.UINT32,   # recipient_channel
                                   packet.STRING,   # request type
                                   packet.BOOLEAN)  # want reply

SSH_MSG_CHANNEL_FAILURE_PAYLOAD = (packet.BYTE,     # SSH_MSG_CHANNEL_FAILURE
                                   packet.UINT32)   # recipient_channel

SSH_MSG_CHANNEL_SUCCESS_PAYLOAD = (packet.BYTE,     # SSH_MSG_CHANNEL_SUCCESS
                                   packet.UINT32)   # recipient_channel

########NEW FILE########
__FILENAME__ = data_buffer
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.connection.data_buffer
#
# This implements a simple buffer class that works like a FIFO.
# It is time-efficient since it tries to avoid string copies whenever
# possible.

import coro

class Buffer:

    def __init__(self):
        self.fifo = coro.fifo()

    def __len__(self):
        return len(self.fifo)

    def write(self, data):
        """write(self, data) -> None
        Writes data to the buffer.
        """
        self.fifo.push(data)

    def pop(self):
        """pop(self) -> str
        Pops the first string from the buffer.
        """
        return self.fifo.pop()

    def read_at_most(self, bytes):
        """read_at_most(self, bytes) -> str
        Reads at most <bytes>.
        May return less than <bytes> even if there is more data in the buffer.
        Returns the empty string when the buffer is empty.
        """
        while 1:
            try:
                data = self.fifo.top()
            except IndexError:
                # Buffer empty.
                self.fifo.cv.wait()
            else:
                break
        if not data:
            raise EOFError
        if len(data) > bytes:
            result, left = data[:bytes], data[bytes:]
            self.fifo.pop()
            self.fifo.push_front (left)
            return result
        else:
            return self.fifo.pop()

########NEW FILE########
__FILENAME__ = interactive_session
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.connect.interactive_session
#
# This implements the "session" channel of the ssh_connect service.
#

import channel
from coro.ssh.util import packet as ssh_packet
from connect import *

from coro import write_stderr as W

class Interactive_Session(channel.Channel):
    name = 'session'

    def send_environment_variable(self, name, value):
        self.send_channel_request('env', ENV_CHANNEL_REQUEST_PAYLOAD,
                                  (name,
                                   value))

class Interactive_Session_Client(Interactive_Session):

    def open_pty(self, term='', width_char=0, height_char=0, width_pixels=0, height_pixels=0, modes=''):
        self.send_channel_request('pty-req', PTY_CHANNEL_REQUEST_PAYLOAD,
                                  (term,
                                   width_char,
                                   height_char,
                                   width_pixels,
                                   height_pixels,
                                   modes))

    def open_shell(self):
        self.send_channel_request('shell', (), ())

    def exec_command(self, command):
        self.send_channel_request('exec', EXEC_CHANNEL_REQUEST_PAYLOAD, (command,))

    def handle_request(self, request_type, want_reply, type_specific_packet_data):
        # W ('interactive_session_client: handle_request %r %r %r\n' %
        #    (request_type, want_reply, type_specific_packet_data))
        if request_type in self.request_handlers:
            self.request_handlers[request_type] (self, want_reply, type_specific_packet_data)
        elif want_reply:
            packet = ssh_packet.pack_payload(SSH_MSG_CHANNEL_FAILURE_PAYLOAD, (self.remote_channel.channel_id,))
            self.transport.send_packet(packet)

    exit_status = None

    def handle_exit_status(self, want_reply, type_specific_packet_data):
        self.exit_status = ssh_packet.unpack_payload(EXIT_STATUS_PAYLOAD, type_specific_packet_data)

    exit_signal = None

    def handle_exit_signal(self, want_reply, type_specific_packet_data):
        self.exit_signal = ssh_packet.unpack_payload(EXIT_SIGNAL_PAYLOAD, type_specific_packet_data)

    request_handlers = {
        'exit-status': handle_exit_status,
        'exit-signal': handle_exit_signal,
    }


# I think to do anything useful here we'll need full terminal emulation like
# http://liftoff.github.io/GateOne/Developer/terminal.html from RFC4254 section 8
pty_modes = {
    0: ('TTY_OP_END', "Indicates end of options."),
    1: ('VINTR', ("Interrupt character; 255 if none.  Similarly for the other characters."
                  " Not all of these characters are supported on all systems.")),
    2: ('VQUIT', "The quit character (sends SIGQUIT signal on POSIX systems)."),
    3: ('VERASE', "Erase the character to left of the cursor."),
    4: ('VKILL', "Kill the current input line."),
    5: ('VEOF', "End-of-file character (sends EOF from the terminal)."),
    6: ('VEOL', "End-of-line character in addition to carriage return and/or linefeed."),
    7: ('VEOL2', "Additional end-of-line character."),
    8: ('VSTART', "Continues paused output (normally control-Q)."),
    9: ('VSTOP', "Pauses output (normally control-S)."),
    10: ('VSUSP', "Suspends the current program."),
    11: ('VDSUSP', "Another suspend character."),
    12: ('VREPRINT', "Reprints the current input line."),
    13: ('VWERASE', "Erases a word left of cursor."),
    14: ('VLNEXT', "Enter the next character typed literally, even if it is a special character"),
    15: ('VFLUSH', "Character to flush output."),
    16: ('VSWTCH', "Switch to a different shell layer."),
    17: ('VSTATUS', "Prints system status line (load, command, pid, etc)."),
    18: ('VDISCARD', "Toggles the flushing of terminal output."),
    30: ('IGNPAR', "The ignore parity flag.  The parameter SHOULD be 0 if this flag is FALSE, and 1 if it is TRUE."),
    31: ('PARMRK', "Mark parity and framing errors."),
    32: ('INPCK', "Enable checking of parity errors."),
    33: ('ISTRIP', "Strip 8th bit off characters."),
    34: ('INLCR', "Map NL into CR on input."),
    35: ('IGNCR', "Ignore CR on input."),
    36: ('ICRNL', "Map CR to NL on input."),
    37: ('IUCLC', "Translate uppercase characters to lowercase."),
    38: ('IXON', "Enable output flow control."),
    39: ('IXANY', "Any char will restart after stop."),
    40: ('IXOFF', "Enable input flow control."),
    41: ('IMAXBEL', "Ring bell on input queue full."),
    50: ('ISIG', "Enable signals INTR, QUIT, [D]SUSP."),
    51: ('ICANON', "Canonicalize input lines."),
    52: ('XCASE', "Enable input and output of uppercase characters by preceding their lowercase equivalents '\'."),
    53: ('ECHO', "Enable echoing."),
    54: ('ECHOE', "Visually erase chars."),
    55: ('ECHOK', "Kill character discards current line."),
    56: ('ECHONL', "Echo NL even if ECHO is off."),
    57: ('NOFLSH', "Don't flush after interrupt."),
    58: ('TOSTOP', "Stop background jobs from output."),
    59: ('IEXTEN', "Enable extensions."),
    60: ('ECHOCTL', "Echo control characters as ^(Char)."),
    61: ('ECHOKE', "Visual erase for line kill."),
    62: ('PENDIN', "Retype pending input."),
    70: ('OPOST', "Enable output processing."),
    71: ('OLCUC', "Convert lowercase to uppercase."),
    72: ('ONLCR', "Map NL to CR-NL."),
    73: ('OCRNL', "Translate carriage return to newline (output)."),
    74: ('ONOCR', "Translate newline to carriage return-newline (output)."),
    75: ('ONLRET', "Newline performs a carriage return (output)."),
    90: ('CS7', "7 bit mode."),
    91: ('CS8', "8 bit mode."),
    92: ('PARENB', "Parity enable."),
    93: ('PARODD', "Odd parity, else even."),
    128: ('TTY_OP_ISPEED', "Specifies the input baud rate in bits per second."),
    129: ('TTY_OP_OSPEED', "Specifies the output baud rate in bits per second."),
}

class PTY:
    def __init__ (self, settings):
        self.term, self.width_char, self.height_char, self.width_pixels, self.height_pixels, self.modes = settings
        # self.unpack_modes()

    def unpack_modes (self):
        import struct
        i = 0
        modes = self.modes
        while i < len (modes):
            opcode = ord (modes[i])
            i += 1
            if opcode >= 160:
                break
            elif pty_modes.has_key (opcode):
                name, description = pty_modes[opcode]
                value, = struct.unpack ('>L', modes[i:i + 4])
                setattr (self, name, value)
                i += 4
                # W ('PTY setting: %r = %r\n' % (name, value))
            else:
                # W ('unknown PTY setting: %r = %r\n' % (opcode, struct.unpack ('>L', modes[i:i+4])))
                i += 4

class Interactive_Session_Server(Interactive_Session):

    def handle_request(self, request_type, want_reply, type_specific_packet_data):
        # W ('interactive_session_server: handle_request %r %r %r\n' %
        #    (request_type, want_reply, type_specific_packet_data))
        if request_type in self.request_handlers:
            self.request_handlers[request_type] (self, want_reply, type_specific_packet_data)
        elif want_reply:
            packet = ssh_packet.pack_payload(SSH_MSG_CHANNEL_FAILURE_PAYLOAD, (self.remote_channel.channel_id,))
            self.transport.send_packet(packet)

    # by default, refuse PTY requests, a lot of terminal smarts are needed to do it right.
    accept_pty = False

    def handle_pty_request(self, want_reply, type_specific_packet_data):
        self.pty = PTY (ssh_packet.unpack_payload(PTY_CHANNEL_REQUEST_PAYLOAD, type_specific_packet_data))
        if want_reply:
            if self.accept_pty:
                self.send_channel_request_success()
            else:
                self.send_channel_request_failure()

    def handle_x11_request(self, want_reply, type_specific_packet_data):
        single_connection, auth_protocol, auth_cookie, screen_number = ssh_packet.unpack_payload(
            X11_CHANNEL_REQUEST_PAYLOAD, type_specific_packet_data)
        # XXX fantasize about doing X11 forwarding here?  I think not.
        if want_reply:
            self.send_channel_request_failure()

    def handle_shell_request(self, want_reply, type_specific_packet_data):
        # XXX whatever, sure, it worked.
        if want_reply:
            self.send_channel_request_success()

    request_handlers = {
        'pty-req': handle_pty_request,
        'x11-req': handle_x11_request,
        'shell': handle_shell_request,
        # env :
        # exec :
        # subsystem :
    }

PTY_CHANNEL_REQUEST_PAYLOAD = (
    ssh_packet.STRING,         # TERM environment variable value (e.g., vt100)
    ssh_packet.UINT32,         # terminal width, characters (e.g., 80)
    ssh_packet.UINT32,         # terminal height, rows (e.g., 24)
    ssh_packet.UINT32,         # terminal width, pixels (e.g., 640)
    ssh_packet.UINT32,         # terminal height, pixels (e.g., 480)
    ssh_packet.STRING,         # encoded terminal modes
)

X11_CHANNEL_REQUEST_PAYLOAD = (
    ssh_packet.BOOLEAN,         # single connection
    ssh_packet.STRING,          # x11 authentication protocol
    ssh_packet.STRING,          # x11 authentication cookie
    ssh_packet.UINT32,          # x11 screen number
)

ENV_CHANNEL_REQUEST_PAYLOAD = (
    ssh_packet.STRING,          # variable name
    ssh_packet.STRING,          # variable value
)

EXEC_CHANNEL_REQUEST_PAYLOAD = (
    ssh_packet.STRING,          # command
)

EXIT_STATUS_PAYLOAD = (
    ssh_packet.UINT32,          # exit_status
)

EXIT_SIGNAL_PAYLOAD = (
    ssh_packet.STRING,          # signal name
    ssh_packet.BOOLEAN,         # core dumped
    ssh_packet.STRING,          # error message
    ssh_packet.STRING,          # language tag
)

########NEW FILE########
__FILENAME__ = tty_modes
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_tty_modes
#
# List of TTY modes used for SSH Interactive Sessions.
#

import struct

class Term_Mode_Builder:

    def __init__(self):
        self.ops = []

    def set_mode(self, opcode, value):
        self.ops.append(chr(opcode) + struct.pack('>I', value))

    def get_mode(self):
        return ''.join(self.ops)


TTY_OP_END = 0   # Indicates end of options.
VINTR      = 1   # Interrupt character; 255 if none.  Similarly for the
# other characters. Not all of these characters are
# supported on all systems.
VQUIT      = 2   # The quit character (sends SIGQUIT signal on POSIX
# systems).
VERASE     = 3   # Erase the character to left of the cursor.
VKILL      = 4   # Kill the current input line.
VEOF       = 5   # End-of-file character (sends EOF from the terminal).
VEOL       = 6   # End-of-line character in addition to carriage return
# and/or linefeed.
VEOL2      = 7   # Additional end-of-line character.
VSTART     = 8   # Continues paused output (normally control-Q).
VSTOP      = 9   # Pauses output (normally control-S).
VSUSP      = 10  # Suspends the current program.
VDSUSP     = 11  # Another suspend character.
VREPRINT   = 12  # Reprints the current input line.
VWERASE    = 13  # Erases a word left of cursor.
VLNEXT     = 14  # Enter the next character typed literally, even if it
# is a special character
VFLUSH     = 15  # Character to flush output.
VSWTCH     = 16  # Switch to a different shell layer.
VSTATUS    = 17  # Prints system status line (load, command, pid etc).
VDISCARD   = 18  # Toggles the flushing of terminal output.
IGNPAR     = 30  # The ignore parity flag.  The parameter SHOULD be 0 if
# this flag is FALSE set, and 1 if it is TRUE.
PARMRK     = 31  # Mark parity and framing errors.
INPCK      = 32  # Enable checking of parity errors.
ISTRIP     = 33  # Strip 8th bit off characters.
INLCR      = 34  # Map NL into CR on input.
IGNCR      = 35  # Ignore CR on input.
ICRNL      = 36  # Map CR to NL on input.
IUCLC      = 37  # Translate uppercase characters to lowercase.
IXON       = 38  # Enable output flow control.
IXANY      = 39  # Any char will restart after stop.
IXOFF      = 40  # Enable input flow control.
IMAXBEL    = 41  # Ring bell on input queue full.
ISIG       = 50  # Enable signals INTR, QUIT, [D]SUSP.
ICANON     = 51  # Canonicalize input lines.
XCASE      = 52  # Enable input and output of uppercase characters by
# preceding their lowercase equivalents with `\'.
ECHO       = 53  # Enable echoing.
ECHOE      = 54  # Visually erase chars.
ECHOK      = 55  # Kill character discards current line.
ECHONL     = 56  # Echo NL even if ECHO is off.
NOFLSH     = 57  # Don't flush after interrupt.
TOSTOP     = 58  # Stop background jobs from output.
IEXTEN     = 59  # Enable extensions.
ECHOCTL    = 60  # Echo control characters as ^(Char).
ECHOKE     = 61  # Visual erase for line kill.
PENDIN     = 62  # Retype pending input.
OPOST      = 70  # Enable output processing.
OLCUC      = 71  # Convert lowercase to uppercase.
ONLCR      = 72  # Map NL to CR-NL.
OCRNL      = 73  # Translate carriage return to newline (output).
ONOCR      = 74  # Translate newline to carriage return-newline
ONLRET     = 75  # Newline performs a carriage return (output).
CS7        = 90  # 7 bit mode.
CS8        = 91  # 8 bit mode.
PARENB     = 92  # Parity enable.
PARODD     = 93  # Odd parity, else even.

TTY_OP_ISPEED = 128  # Specifies the input baud rate in bits per second.
TTY_OP_OSPEED = 129  # Specifies the output baud rate in bits per second.

########NEW FILE########
__FILENAME__ = backdoor
# -*- Mode: Python -*-

import coro.ssh.transport.server
import coro.ssh.connection.connect
import coro.ssh.l4_transport.coro_socket_transport
import coro.ssh.auth.userauth
import coro.ssh.connection.interactive_session

import coro
import coro.backdoor

import getopt
import socket
import sys


from coro.ssh.keys.openssh_key_storage import OpenSSH_Key_Storage

server_key_pri = """-----BEGIN DSA PRIVATE KEY-----
MIIBuwIBAAKBgQDTfwvvQo0WnUmZpnUFmqF/TXSXFaJ1NKbBLQXPh8dhHgTN1uFO
ZibFXMKpDHLCGCdGRm5eHansB9hu2+nNoaFf3oLDHc8ctuE7xRHT8x174D2AxcnX
r0Fw3BnZHj58lLlhayDJ4S6W77yefGEOuo/wKUEPjAUBCrvxKq3bKAeVUQIVAPpR
bJO1QQZPlj4w+MXmRTgW7wGfAoGAVUkBIX+RLrh9guyiAadi9xGk8S7n5w2PbcsP
KTG8x/ttCDEuaBp6El6qt86cA+M2GPvXjuMGR5BQT8IOaWS7Aw2+J1IamLCsrPfq
oiQvz3cqxOAutuIuorzbIAgVo0hiAyovZE4u2zzKeci7OtfD8pRThSby4Dgbkeix
FQFhW08CgYBSxcduHDSqJTCjFK4hwTlNck4h2hC1E4xuMfxYsUZkLrBAsD3nzU2W
jNoZppTz3W8XC7YnTxonncXNWxCWsDWpvs0b2zGj7uUvGRtlyxtQpybyN3LZ0flo
DssTygy7t0KlS7T2a1IhqiVDbrSUoGXz+Wp/z66lCpSLTlPsGpLeLwIVAMQldwwH
OekNfzzIBr6QkMvmIOuL
-----END DSA PRIVATE KEY-----
"""

ks = OpenSSH_Key_Storage()
server_key_ob = ks.parse_private_key (server_key_pri)

# will authentication user 'foo' with password 'bar' for the
# 'ssh-connection' service [the only service currently supported]
pwd_auth = coro.ssh.auth.userauth.Password_Authenticator ({'foo': {'ssh-connection': 'bar'}})

# how to add public-key authentication:
#
#   user_key_pub = """ssh-dss AAAAB...Stc= username@hostname.domain\n"""
#   user_key_ob = ks.parse_public_key (user_key_pub)
#   pubkey_auth = coro.ssh.auth.userauth.Public_Key_Authenticator ({'luser': { 'ssh-connection' : [user_key_ob]}})
#
# add/replace <pubkey_auth> to the list "[pwd_auth]" below...

def usage():
    print 'backdoor.py [-p port]'

def main():

    login_username = None
    ip = None
    port = 8022

    try:
        optlist, args = getopt.getopt(sys.argv[1:], 'p:')
    except getopt.GetoptError, why:
        print str(why)
        usage()
        sys.exit(1)

    for option, value in optlist:
        if option == '-p':
            port = int (value)

    coro.spawn (coro.backdoor.ssh_server, port, '', server_key_ob, [pwd_auth])
    coro.event_loop()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = remote_exec
# -*- Mode: Python; insert-tabs-mode: nil -*-

from coro.ssh.transport.client import SSH_Client_Transport
from coro.ssh.l4_transport.coro_socket_transport import coro_socket_transport
from coro.ssh.auth.userauth import Userauth
from coro.ssh.connection.interactive_session import Interactive_Session_Client
from coro.ssh.connection.connect import Connection_Service

import sys
import coro
# avoid full-blown dns resolver
coro.set_resolver (coro.dummy_resolver())

class client:

    # this is to avoid a PTR lookup, its value is not important
    hostname = 'host'

    def __init__ (self, ip, username, port=22):
        self.ip = ip
        self.port = port
        self.username = username

    def open (self):
        client = SSH_Client_Transport()
        transport = coro_socket_transport (self.ip, port=self.port, hostname=self.hostname)
        client.connect (transport)
        auth_method = Userauth (client, self.username)
        service = Connection_Service (client)
        client.authenticate (auth_method, service.name)
        channel = Interactive_Session_Client (service)
        channel.open()
        return channel

    def read_all (self, channel):
        while 1:
            try:
                yield channel.read (1000)
            except EOFError:
                break

    def command (self, cmd, output=sys.stdout):
        channel = self.open()
        channel.exec_command (cmd)
        for block in self.read_all (channel):
            output.write (block)
        channel.close()

def go (ip, username, cmd):
    c = client (ip, username)
    c.command (cmd)
    coro.set_exit()

# try: python remote_exec.py 10.1.1.3 bubba "ls -l"
if __name__ == '__main__':
    import sys
    if len(sys.argv) < 3:
        sys.stderr.write ('Usage: %s <ip> <username> <cmd>\n' % (sys.argv[0],))
        sys.stderr.write ('Example: python %s 10.1.1.3 bubba "ls -l"\n' % (sys.argv[0],))
    else:
        coro.spawn (go, *sys.argv[1:])
        coro.event_loop()

########NEW FILE########
__FILENAME__ = dss
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.dss
#
# Encapsulates the DSS key.

import public_private_key
import hashlib
from coro.ssh.util import packet, random
from Crypto.PublicKey import DSA
from Crypto.Util import number

class SSH_DSS(public_private_key.SSH_Public_Private_Key):

    # Features of this key type.
    supports_signature = 1
    supports_encryption = 0
    name = 'ssh-dss'
    # why not store these as attributes?
    # p, q, g, y
    private_key = (0L, 0L, 0L, 0L, 0L)
    # p, q, g, y, x
    public_key = (0L, 0L, 0L, 0L)

    def set_public_key(self, public_key):
        dss, p, q, g, y = packet.unpack_payload(DSS_PUBLIC_KEY_PAYLOAD, public_key)
        if dss != 'ssh-dss':
            raise ValueError(dss)
        self.public_key = (p, q, g, y)

    set_public_key.__doc__ = public_private_key.SSH_Public_Private_Key.set_public_key.__doc__

    def set_private_key(self, private_key):
        dss, p, q, g, y, x = packet.unpack_payload(DSS_PRIVATE_KEY_PAYLOAD, private_key)
        if dss != 'ssh-dss':
            raise ValueError(dss)
        self.private_key = (p, q, g, y, x)

    set_private_key.__doc__ = public_private_key.SSH_Public_Private_Key.set_private_key.__doc__

    def get_public_key_blob(self):
        if self.public_key != (0, 0, 0, 0):
            p, q, g, y = self.public_key
        else:
            p, q, g, y, x = self.private_key
        return packet.pack_payload(DSS_PUBLIC_KEY_PAYLOAD,
                                   ('ssh-dss',
                                    p, q, g, y))

    get_public_key_blob.__doc__ = public_private_key.SSH_Public_Private_Key.get_public_key_blob.__doc__

    def get_private_key_blob(self):
        p, q, g, y, x = self.private_key
        return packet.pack_payload(DSS_PRIVATE_KEY_PAYLOAD,
                                   ('ssh-dss',
                                    p, q, g, y, x))

    get_private_key_blob.__doc__ = public_private_key.SSH_Public_Private_Key.get_private_key_blob.__doc__

    def create (self, size=1024):
        key = DSA.generate (size)
        self.private_key = (key.p, key.q, key.g, key.y, key.x)
        self.public_key = (key.p, key.q, key.g, key.y)

    def sign(self, message):
        p, q, g, y, x = self.private_key
        dsa_obj = DSA.construct((y, g, p, q, x))
        message_hash = hashlib.sha1(message).digest()
        # Get a random number that is greater than 2 and less than q.
        random_number = random.get_random_number_from_range(2, q)
        random_data = number.long_to_bytes(random_number)
        r, s = dsa_obj.sign(message_hash, random_data)
        signature = number.long_to_bytes(r, 20) + number.long_to_bytes(s, 20)
        return packet.pack_payload(DSS_SIG_PAYLOAD,
                                   ('ssh-dss',
                                    signature))

    sign.__doc__ = public_private_key.SSH_Public_Private_Key.sign.__doc__

    def verify(self, message, signature):
        p, q, g, y = self.public_key
        dss, blob = packet.unpack_payload(DSS_SIG_PAYLOAD, signature)
        if dss != 'ssh-dss':
            raise ValueError(dss)
        # blob is the concatenation of r and s
        # r and s are 160-bit (20-byte) integers in network-byte-order
        assert(len(blob) == 40)
        r = number.bytes_to_long(blob[:20])
        s = number.bytes_to_long(blob[20:])
        dsa_obj = DSA.construct((y, g, p, q))
        hash_of_message = hashlib.sha1(message).digest()
        return dsa_obj.verify(hash_of_message, (r, s))

    verify.__doc__ = public_private_key.SSH_Public_Private_Key.verify.__doc__

DSS_PUBLIC_KEY_PAYLOAD = (packet.STRING,  # "ssh-dss"
                          packet.MPINT,   # p
                          packet.MPINT,   # q
                          packet.MPINT,   # g
                          packet.MPINT    # y
                          )

DSS_PRIVATE_KEY_PAYLOAD = (packet.STRING,  # "ssh-dss"
                           packet.MPINT,   # p
                           packet.MPINT,   # q
                           packet.MPINT,   # g
                           packet.MPINT,   # y
                           packet.MPINT,   # x
                           )


DSS_SIG_PAYLOAD = (packet.STRING,  # "ssh-dss"
                   packet.STRING   # signature_key_blob
                   )

########NEW FILE########
__FILENAME__ = key_storage
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.key_storage
#
# This module loads and saves various types of SSH public/private keys.
#

class Invalid_Server_Public_Host_Key(Exception):

    """Invalid_Server_Public_Host_Key(host_id, public_host_key)
    This exception is raised when we have no knowledge of the server's
    public key.  Normally what happens is the user is asked if the fingerprint
    of the public key is OK.

    <host_id>: A Remote_Host_ID instance.
    <public_host_key>: A SSH_Public_Private_Key instance.
    """

    def __init__(self, host_id, public_host_key):
        self.host_id = host_id
        self.public_host_key = public_host_key
        Exception.__init__(self, host_id, public_host_key)

    def __str__(self):
        return '<Invalid_Server_Public_host_Key host_id=%s>' % self.host_id

# XXX: include the filename and line number of the conflict.
class Host_Key_Changed_Error(Exception):

    """Host_Key_Changed_Error(host_id, location)
    This exception is raised when the server's public host key does not match
    our database.

    <host_id>: A Remote_Host_ID instance
    <location>: A string to direct the user to how to find the offending key
                in their local database.  May be the empty string if it is
                not relevant.
    """

    def __init__(self, host_id, location):
        self.host_id = host_id
        self.location = location
        Exception.__init__(self, host_id, location)

    def __str__(self):
        return '<Host_Key_Changed_Error host_id=%s location=%s>' % (self.host_id, self.location)

class SSH_Key_Storage:

    def load_keys(self, username=None, **kwargs):
        """load_keys(self, username=None, **kwargs) -> [private_public_key_obj, ...]
        Loads the public and private keys.

        <username> defaults to the current user.

        Different key storage classes take different arguments.

        Returns a list of SSH_Public_Private_Key objects.
        Returns an empty list if the key is not available.
        """
        raise NotImplementedError

    def load_private_keys(self, username=None, **kwargs):
        """load_private_keys(self, username=None, **kwargs) -> [private_key_obj, ...]
        Loads the private keys.

        <username> defaults to the current user.

        Different key storage classes take different arguments.

        Returns a list of SSH_Public_Private_Key objects.
        Returns an empty list if the key is not available.
        """
        raise NotImplementedError

    def load_public_keys(self, username=None, **kwargs):
        """load_public_keys(self, username=None, **kwargs) -> [public_key_obj, ...]
        Loads the public keys.

        <username> defaults to the current user.

        Different key storage classes take different arguments.

        Returns a list of SSH_Public_Private_Key objects.
        Returns an empty list if the key is not available.
        """
        raise NotImplementedError

    def verify(self, host_id, server_key_types, public_host_key, username=None):
        """verify(self, host_id, server_key_types, public_host_key) -> boolean
        This verifies that the given public host key is known.
        Returns true if it is OK.

        <username>: defaults to the current user.
        <server_key_types>: A list of SSH_Public_Private_Key objects that we support.
        <public_host_key>: A SSH_Public_Private_Key instance.
        <host_id>: Remote_Host_ID instance.
        """
        raise NotImplementedError

    def update_known_hosts(self, host, public_host_key, username=None):
        """update_known_hosts(self, host, public_host_key, username=None) -> None
        Updates the known hosts database for the given user.

        <host>: The host string.
        <public_host_key>: A SSH_Public_Private_Key instance.
        <username>: Defaults to the current user.
        """
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = openssh_authorized_keys
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.openssh_authorized_keys
#
# This module handles the authorized_keys file.
#

import os
import re
from rebuild import *

class DuplicateKeyError(Exception):
    pass

# Keys are in two formats:
# SSH1:
#   [options] bits exponent modulus comment
# SSH2:
#   [options] keytype base64_key comment
#
# The options are optional. They never start with a number, nor do they
# contain spaces. It is a comma-separated list of values.
#
# The SSH1 key format is as follows. The bits is a number, typically
# something like 1024. The exponent is also a number, typically something
# like 35. The modulus is a very long string of numbers. The comment can be
# anything, but it is typically username@hostname.
#
# The SSH2 key format is as follows. The keytype is either ssh-dss or ssh-rsa.
# The key is a long string encoded in base-64. The comment can be anything,
# but it is typically username@hostname.

SPACE = '[ \t]'

OPTION_START = '[^ \t0-9]'
ATOM = '[^ \t]'
QUOTED_ATOM = '"[^"]*"'
WORD = OR(ATOM, QUOTED_ATOM)
OPTIONS = NAME('options', OPTION_START + SPLAT(WORD))

BITS = NAME('bits', '\d+')
EXPONENT = NAME('exponent', '\d+')
MODULUS = NAME('modulus', '\d+')
COMMENT = NAME('comment', '.*')

ssh1_key = CONCAT('^',
                  SPLAT(SPACE),
                  BITS, PLUS(SPACE),
                  EXPONENT, PLUS(SPACE),
                  MODULUS, SPLAT(SPACE),
                  COMMENT
                  )

ssh1_key_w_options = CONCAT('^',
                            SPLAT(SPACE),
                            OPTIONS, PLUS(SPACE),
                            BITS, PLUS(SPACE),
                            EXPONENT, PLUS(SPACE),
                            MODULUS, SPLAT(SPACE),
                            COMMENT
                            )


# The man page for OpenSSH specifies that "ssh-dss" and "ssh-rsa" are the only
# valid types, but the code actually checks for this list of types.  Let's
# try to be as flexible as possible.
KEYTYPE = NAME('keytype', OR('ssh-dss', 'ssh-rsa', 'rsa1', 'rsa', 'dsa'))
# Not a very exact base64 regex, but should be good enough.
# OpenSSH seems to ignore spaces anywhere.  Also, this doesn't check for
# a "partial" or truncated base64 string.
BASE64_KEY = NAME('base64_key', PLUS('[a-zA-Z0-9+/=]'))

ssh2_key = CONCAT('^',
                  SPLAT(SPACE),
                  KEYTYPE, PLUS(SPACE),
                  BASE64_KEY, SPLAT(SPACE),
                  COMMENT
                  )

ssh2_key_w_options = CONCAT('^',
                            SPLAT(SPACE),
                            OPTIONS, PLUS(SPACE),
                            KEYTYPE, PLUS(SPACE),
                            BASE64_KEY, SPLAT(SPACE),
                            COMMENT
                            )

ssh1_key_re = re.compile(ssh1_key)
ssh2_key_re = re.compile(ssh2_key)
ssh1_key_w_options_re = re.compile(ssh1_key_w_options)
ssh2_key_w_options_re = re.compile(ssh2_key_w_options)

class OpenSSH_Authorized_Keys:

    """OpenSSH_Authorized_Keys(filename)

    This is a class that will represent an SSH authorized_keys file.
    """

    def __init__(self, filename):
        self.filename = filename
        # This is a list of dictionary objects.
        self.keys = []
        self.read()

    def read(self):
        """read() -> None
        Reads the contents of the keyfile into memory.
        If the file does not exist, then it does nothing.
        """
        if os.path.exists(self.filename):
            lines = open(self.filename).readlines()
        else:
            lines = []
        for line in lines:
            line = line.strip()
            # ignore comment lines
            if line and line[0] != '#':
                try:
                    self.add_key(line)
                except (DuplicateKeyError, ValueError):
                    # Ignore this entry.
                    # Maybe we should print an error or something?
                    pass

    def add_key(self, key):
        """add_key(key) -> None
        Adds the given key to the object.
        <key> is a string.
        Raises DuplicateKeyError if the key already exists.
        Raises ValueError if the key does not appear to be a valid format.
        """
        key = key.strip()
        m = ssh1_key_re.match(key)
        if not m:
            m = ssh2_key_re.match(key)
            if not m:
                m = ssh1_key_w_options_re.match(key)
                if not m:
                    m = ssh2_key_w_options_re.match(key)
                    if not m:
                        raise ValueError(key)

        values = m.groupdict()
        if (('keytype' in values and not values['keytype']) or
                ('base64_key' in values and not values['base64_key']) or
                ('bits' in values and not values['bits']) or
                ('exponent' in values and not values['exponent']) or
                ('modulus' in values and not values['modulus'])):
            raise ValueError(key)
        self._value_strip(values)
        if 'options' not in values or not values['options']:
            # If it doesn't exist, or it exists as None, set it to the empty string.
            values['options'] = ''
        if not values['comment']:
            values['comment'] = ''
        self._duplicate_check(values)
        self.keys.append(values)

    def _value_strip(self, d):
        """_value_strip(d) -> None
        Takes d, which is a dict, and calls strip() on all its values.
        """
        for key, value in d.items():
            if value:
                d[key] = value.strip()

    def _duplicate_check(self, key):
        """_duplicate_check(key) -> None
        Checks if key (which is dict-format) is a duplicate.
        Raises DuplicateKeyError if it is.
        """
        if 'bits' in key:
            # SSH1
            for x in self.keys:
                if ('bits' in x and
                        x['bits'] == key['bits'] and
                        x['exponent'] == key['exponent'] and
                        x['modulus'] == key['modulus']):
                    raise DuplicateKeyError
        else:
            # SSH2
            for x in self.keys:
                if ('keytype' in x and
                        x['keytype'] == key['keytype'] and
                        x['base64_key'] == key['base64_key']):
                    raise DuplicateKeyError

    def write(self):
        """write() -> None
        Writes the keyfile to disk, safely overwriting the keyfile that
        already exists.
        """
        # Avoid concurrent races here?
        tmp_filename = self.filename + '.tmp'
        fd = os.open(tmp_filename, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0644)
        write = lambda x, y=fd: os.write(y, x)
        map(write, map(self.keydict_to_string, self.keys))
        os.close(fd)
        os.rename(tmp_filename, self.filename)

    def keydict_to_string(self, key, short_output=0):
        """keydict_to_string(key, short_output=0) -> string
        Converts an SSH dict-format key into a string.
        <short_output> - Set to true if you want to exclude options and comment.
        """
        if short_output:
            options = ''
            comment = ''
        else:
            options = key['options']
            comment = key['comment']
        if 'bits' in key:
            # SSH1
            bits = key['bits']
            exponent = key['exponent']
            modulus = key['modulus']
            result = ' '.join([options, bits, exponent, modulus, comment])
        else:
            # SSH2
            keytype = key['keytype']
            base64_key = key['base64_key']
            result = ' '.join([options, keytype, base64_key, comment])
        return result.strip() + '\n'

########NEW FILE########
__FILENAME__ = openssh_key_formats
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.openssh_key_formats
#
# This module contains expressions for parsing and matching keys.
#

import re
from rebuild import *

SPACE = '[ \t]'
NUMBER = '\d'
LIST_OF_HOSTS = NAME('list_of_hosts', PLUS('[^ \t]'))
COMMENT = OPTIONAL(
    PLUS(SPACE),
    NAME('comment', PLUS('.'))
)

ssh1_key = re.compile(
    CONCAT('^',
           LIST_OF_HOSTS,
           PLUS(SPACE),
           NAME('number_of_bits', PLUS(NUMBER)),
           PLUS(SPACE),
           NAME('exponent', PLUS(NUMBER)),
           PLUS(SPACE),
           NAME('modulus', PLUS(NUMBER)),
           COMMENT
           )
)

# The man page for OpenSSH specifies that "ssh-dss" and "ssh-rsa" are the only
# valid types, but the code actually checks for this list of types.  Let's
# try to be as flexible as possible.
KEYTYPE = NAME('keytype', OR('ssh-dss', 'ssh-rsa', 'rsa1', 'rsa', 'dsa'))
# Not a very exact base64 regex, but should be good enough.
# OpenSSH seems to ignore spaces anywhere.  Also, this doesn't check for
# a "partial" or truncated base64 string.
BASE64_KEY = NAME('base64_key', PLUS('[a-zA-Z0-9+/=]'))

ssh2_key = re.compile(
    CONCAT('^',
           KEYTYPE,
           PLUS(SPACE),
           BASE64_KEY,
           COMMENT
           )
)

ssh2_known_hosts_entry = re.compile(
    CONCAT('^',
           LIST_OF_HOSTS,
           PLUS(SPACE),
           KEYTYPE,
           PLUS(SPACE),
           BASE64_KEY,
           COMMENT
           )
)

########NEW FILE########
__FILENAME__ = openssh_key_storage
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.openssh_key_storage
#
# This module is capable of loading key files that are generated by OpenSSH.
#

# XXX: Make key parse error exception.  Replace asserts.

import key_storage
import binascii
import hashlib
import os
import re
import rebuild
import dss
import rsa

from coro.asn1.ber import decode as ber_decode
from coro.ssh.keys import openssh_key_formats
from coro.ssh.util import str_xor
from coro.ssh.util.password import get_password

from Crypto.Cipher import DES
import openssh_known_hosts
import remote_host

class OpenSSH_Key_Storage(key_storage.SSH_Key_Storage):

    header = re.compile(
        rebuild.CONCAT(
            rebuild.NAME('name', '[^:]*'),
            ':',
            rebuild.SPLAT('[ \t]'),
            rebuild.NAME('value', '.*')
        )
    )

    key_types = ('dsa', 'rsa')

    def get_private_key_filenames(self, username, private_key_filename):
        """get_private_key_filenames(self, username, private_key_filename) -> [filename, ...]
        Gets the filenames of the private keys.

        <username> - Look into the home directory of this user for the key.
                     If None, uses the current user.
        <private_key_filename> - If this is set, then this is the value to
                     return.  Otherwise the filename is computed from the
                     username's home directory.  This is provided as a
                     convenience to handle the situation where the filename
                     is forced to a specific value.
        """
        if private_key_filename is None:
            if username is None:
                username = os.getlogin()
            home_dir = os.path.expanduser('~' + username)
            result = []
            for key_type in self.key_types:
                private_key_filename = os.path.join(home_dir, '.ssh', 'id_%s' % key_type)
                result.append(private_key_filename)
            return result
        else:
            return [private_key_filename]

    get_private_key_filenames = classmethod(get_private_key_filenames)

    def get_public_key_filenames(self, username, public_key_filename):
        """get_public_key_filenames(self, username, public_key_filename) -> [filename, ...]
        Gets the filenames of the public keys.

        <username> - Look into the home directory of this user for the key.
                     If None, uses the current user.
        <private_key_filename> - If this is set, then this is the value to
                     return.  Otherwise the filename is computed from the
                     username's home directory.  This is provided as a
                     convenience to handle the situation where the filename
                     is forced to a specific value.
        """
        if public_key_filename is None:
            result = self.get_private_key_filenames(username, None)
            result = map(lambda x: x + '.pub', result)
            return result
        else:
            return [public_key_filename]

    get_public_key_filenames = classmethod(get_public_key_filenames)

    def load_keys(self, username=None, private_key_filename=None, public_key_filename=None):
        """load_keys(self, username=None, private_key_filename=None, public_key_filename=None) -> [public_private_key_obj, ...]
        Loads both the private and public keys.  Returns a list of
        SSH_Public_Private_Key object.  Returns an empty list if both the
        public and private keys are not available.

        <private_key_filename> - defaults to $HOME/.ssh/id_dsa or id_rsa.
        <public_key_filename>  - If set to None, then it will assume the
                                 filename is the same as
                                 <private_key_filename> with a .pub extension.
        <username> - Look into the home directory of this user for the key.
                     If None, uses the current user.
        """
        private_key_filenames = self.get_private_key_filenames(username, private_key_filename)
        result = []
        for filename in private_key_filenames:
            private_keys = self.load_private_keys(private_key_filename=filename)
            if private_keys:
                assert (len(private_keys) == 1)
                if public_key_filename is None:
                    pub_filename = filename + '.pub'
                else:
                    pub_filename = public_key_filename
                public_keys = self.load_public_keys(public_key_filename=pub_filename)
                if public_keys:
                    assert (len(public_keys) == 1)
                    # Join the two keys into one.
                    key = private_keys[0]
                    key.public_key = public_keys[0].public_key
                    result.append(key)
        return result

    def load_private_keys(self, username=None, private_key_filename=None):
        """load_private_keys(self, username=None, private_key_filename=None) -> [key_obj, ...]
        Loads the private keys with the given filename.
        Defaults to $HOME/.ssh/id_dsa or id_rsa
        Returns a list SSH_Public_Private_Key object.
        Returns an empty list if the key is not available.
        """
        private_key_filenames = self.get_private_key_filenames(username, private_key_filename)
        result = []
        for filename in private_key_filenames:
            try:
                data = open(filename).read()
            except IOError:
                pass
            else:
                result.append(self.parse_private_key(data))
        return result

    load_private_keys = classmethod(load_private_keys)

    def load_public_keys(self, username=None, public_key_filename=None):
        """load_public_keys(self, username=None, public_key_filename=None) -> [key_obj, ...]
        Loads the public keys with the given filename.
        Defaults to $HOME/.ssh/id_dsa.pub
        Returns a list of SSH_Public_Private_Key object.
        Returns an empty list if the key is not available.
        """
        public_key_filenames = self.get_public_key_filenames(username, public_key_filename)
        result = []
        for filename in public_key_filenames:
            try:
                data = open(filename).read()
            except IOError:
                pass
            else:
                result.append(self.parse_public_key(data))
        return result

    load_public_keys = classmethod(load_public_keys)

    def parse_private_key(self, private_key):
        """parse_private_key(self, private_key) -> key_obj
        Parses the given string into an SSH_Public_Private_Key object.
        """
        # Format (PEM which is RFC 1421):
        # -----BEGIN DSA PRIVATE KEY-----
        # RFC 822 headers.
        # keydata_base64
        # -----END DSA PRIVATE KEY-----
        # keydata is BER-encoded
        data = private_key.split('\n')
        self._strip_empty_surrounding_lines(data)
        for key_type in self.key_types:
            if (data[0] == '-----BEGIN %s PRIVATE KEY-----' % (key_type.upper(),) and
                    data[-1] == '-----END %s PRIVATE KEY-----' % (key_type.upper(),)):
                break
        else:
            raise ValueError('Corrupt key header/footer format: %s %s' % (data[0], data[-1]))

        key_data = []
        # XXX: Does not support multiple headers with the same name.
        headers = {}
        current_line = 1
        if ':' in data[current_line]:
            # starts with RFC 822 headers
            continuation = 0    # Flag to follow continuation line
            current_value = []
            name = ''   # pychecker squelch
            while 1:
                line = data[current_line]
                if line.startswith(' ') or line.startswith('\t') or continuation:
                    if line.endswith('\\'):
                        # Strip trailing slash.
                        line = line[:-1]
                        continuation = 1
                    else:
                        continuation = 0
                    current_value.append(line.lstrip())
                    current_line += 1
                    continue
                else:
                    if current_value:
                        headers[name] = ''.join(current_value)
                        current_value = []

                if not line:
                    # end of headers
                    break

                match = self.header.match(line)
                assert (match is not None), 'Invalid header value in private key: %r' % line
                d = match.groupdict()
                name = d['name']
                value = d['value']
                if line.endswith('\\'):
                    # Continuation (see ietf-secsh-publickeyfile)
                    continuation = 1
                current_value.append(value)
                current_line += 1

        # Parse the key
        while 1:
            if data[current_line].startswith('-----'):
                break
            if data[current_line]:
                key_data.append(data[current_line])
            current_line += 1
        key_data = ''.join(key_data)
        key_data = binascii.a2b_base64(key_data)
        if 'Proc-Type' in headers:
            proc_type = headers['Proc-Type'].split(',')
            proc_type = map(lambda x: x.strip(), proc_type)
            if len(proc_type) == 2 and proc_type[0] == '4' and proc_type[1] == 'ENCRYPTED':
                # Key is encrypted.
                assert 'DEK-Info' in headers, 'Private key missing DEK-Info field.'
                dek_info = headers['DEK-Info'].split(',')
                dek_info = map(lambda x: x.strip(), dek_info)
                assert (len(dek_info) == 2), 'Expected two values in DEK-Info field: %r' % dek_info
                # XXX: Do we need to support more encryption types?
                assert (dek_info[0] == 'DES-EDE3-CBC'), 'Can only handle DES-EDE3-CBC encryption: %r' % dek_info[0]
                iv = binascii.a2b_hex(dek_info[1])
                passphrase = self.ask_for_passphrase()
                # Convert passphrase to a key.
                a = hashlib.md5(passphrase + iv).digest()
                b = hashlib.md5(a + passphrase + iv).digest()
                passkey = (a + b)[:24]        # Only need first 24 characters.
                key_data = self.des_ede3_cbc_decrypt(key_data, iv, passkey)

        key_data, _ = ber_decode (key_data)
        # Crypto.RSA requires that all are PyLong
        key_data = [long(x) for x in key_data]
        # key_data[0] is always 0???
        if key_type not in keytype_map:
            return None
        key_obj = keytype_map[key_type]()

        # Just so happens both dsa and rsa keys have 5 numbers.
        key_obj.private_key = tuple(key_data[1:6])
        return key_obj

    parse_private_key = classmethod(parse_private_key)

    def ask_for_passphrase():
        return get_password('Enter passphrase> ')

    ask_for_passphrase = staticmethod(ask_for_passphrase)

    def des_ede3_cbc_decrypt(data, iv, key):
        assert (len(data) % 8 == 0), 'Data block must be a multiple of 8: %i' % len(data)
        key1 = DES.new(key[0:8], DES.MODE_ECB)
        key2 = DES.new(key[8:16], DES.MODE_ECB)
        key3 = DES.new(key[16:24], DES.MODE_ECB)
        # Outer-CBC Mode
        # 8-byte blocks
        result = []
        prev = iv
        for i in xrange(0, len(data), 8):
            block = data[i:i + 8]
            value = key1.decrypt(
                key2.encrypt(
                    key3.decrypt(block)))
            result.append(str_xor(prev, value))
            prev = block

        return ''.join(result)

    des_ede3_cbc_decrypt = staticmethod(des_ede3_cbc_decrypt)

    def parse_public_key(public_key):
        """parse_public_key(public_key) -> key_obj
        Parses the given string into an SSH_Public_Private_Key object.
        Returns None if parsing fails.

        <public_key>: The public key as a base64 string.
        """
        # Format:
        # keytype SPACE+ base64_string [ SPACE+ comment ]
        key_match = openssh_key_formats.ssh2_key.match(public_key)
        if not key_match:
            return None
        keytype = key_match.group('keytype')
        if keytype not in keytype_map:
            return None
        try:
            key = binascii.a2b_base64(key_match.group('base64_key'))
        except binascii.Error:
            return None
        key_obj = keytype_map[keytype]()
        key_obj.set_public_key(key)
        return key_obj

    parse_public_key = staticmethod(parse_public_key)

    def _strip_empty_surrounding_lines(data):
        while 1:
            if not data[0]:
                del data[0]
            else:
                break
        while 1:
            if not data[-1]:
                del data[-1]
            else:
                break

    _strip_empty_surrounding_lines = staticmethod(_strip_empty_surrounding_lines)

    def get_authorized_keys_filename(username, authorized_keys_filename=None):
        if authorized_keys_filename is None:
            if username is None:
                username = os.getlogin()
            home_dir = os.path.expanduser('~' + username)
            authorized_keys_filename = os.path.join(home_dir, '.ssh', 'authorized_keys')
        return authorized_keys_filename

    get_authorized_keys_filename = staticmethod(get_authorized_keys_filename)

    def verify(self, host_id, server_key_types, public_host_key, username=None, port=22):
        for key in server_key_types:
            if public_host_key.name == key.name:
                # This is a supported key type.
                if self._verify_contains(host_id, public_host_key, username, port):
                    return 1
        return 0

    verify.__doc__ = key_storage.SSH_Key_Storage.verify.__doc__

    verify = classmethod(verify)

    def _verify_contains(host_id, key, username, port):
        """_verify_contains(host_id, key, username) -> boolean
        Checks whether <key> is in the known_hosts file.
        """
        # Currently only supported IPv4
        if not isinstance(host_id, remote_host.IPv4_Remote_Host_ID):
            return 0
        hostfile = openssh_known_hosts.OpenSSH_Known_Hosts()
        return hostfile.check_for_host(host_id, key, username, port)

    _verify_contains = staticmethod(_verify_contains)

    def update_known_hosts(host, public_host_key, username=None):
        hostfile = openssh_known_hosts.OpenSSH_Known_Hosts()
        hostfile.update_known_hosts(host, public_host_key, username)

    update_known_hosts.__doc__ = key_storage.SSH_Key_Storage.update_known_hosts.__doc__

    update_known_hosts = staticmethod(update_known_hosts)


keytype_map = {'ssh-dss': dss.SSH_DSS,
               'dss': dss.SSH_DSS,
               'dsa': dss.SSH_DSS,
               'ssh-rsa': rsa.SSH_RSA,
               'rsa': rsa.SSH_RSA,
               #               'rsa1': None
               }

import unittest

class ssh_key_storage_test_case(unittest.TestCase):
    pass

class load_dsa_test_case(ssh_key_storage_test_case):

    def runTest(self):
        public_key = 'ssh-dss AAAAB3NzaC1kc3MAAACBAM46u7kMaoOESTiF3fwqvKry2YSYwlgcl2fRtw5IBgLyeS5SLy/M18ZeGLBokFSAFN110B4X6mUK05VMn3KGo0xKnu35+s4g20vOn9ubjXzUkt4EORJZ+MPPaQOllW22m5fjutND3SzahUOx9Z/PaTSbRLGovpTA7NjlliUVt32rAAAAFQCgqkv3v9z16r0z36InixKZTeWcIQAAAIB7qsZKumVthTLCzj/nAgOvdehLm8PbpWAYe8g1QyAGhbyB0MTwak0TvtBrxCq1nbCkYuFdPVtAWw7Q6fk4nf+3vNiKIl55lVMmUpJ2KkGBJDuEuMUWPRiiJZwW+KxKUyB7pKY5gwJt4DLGlfVjQW4q+b0qm83k/XUoW3VW/L4TIAAAAIAQbMUcClGzedoL7bIf4vh7DiQedlMaTM66EL8awJAQBNfAc9au84J0yMz84/6Dub2h+XwP6Ip5E+QjD32grBgj2MV3orjeXa3GKEbmLV9+3asZKma+gzfQurz0rfR767vp5p4ZScODAp/u64FrMQeiMLD0TePAOhDX7Y6ON5AOlw== admin@test04.god\n'  # noqa
        public_key_value = (144819228510396375480510966045726324197234443151241728654670685625305230385467763734653299992854300412367868856607501321634131298084648429649714452472261648519166487595581105734370788168033696455943547609540069712392591019911289209306656760054646817215504894551439102079913490941604156000063251698742214491563L,  # noqa
                            917236267741783881593757959752012731596818193441L,
                            86841982599782731711680786695115998714268381589063264016290973272276727186587704621026934440034006184167355751218909451695519359588918346763818149790632054843515968619897385312275286742098283669267848018249714288204339152924266843441212538369167733167339150986744361013446636969482251895247384926570829189920L,  # noqa
                            11533944838210201987952882615702205024058326377484185868096298195008186074503068022497845512065175251915059614398323642650025427428897101740618300183253025704503580499048317459491367465314627384976328186431247669440664650252937901083193085980371291330875135658464646502031914168659603641006946305696513134231L)  # noqa
        private_key = """-----BEGIN DSA PRIVATE KEY-----
MIIBuwIBAAKBgQDOOru5DGqDhEk4hd38Kryq8tmEmMJYHJdn0bcOSAYC8nkuUi8v
zNfGXhiwaJBUgBTdddAeF+plCtOVTJ9yhqNMSp7t+frOINtLzp/bm4181JLeBDkS
WfjDz2kDpZVttpuX47rTQ90s2oVDsfWfz2k0m0SxqL6UwOzY5ZYlFbd9qwIVAKCq
S/e/3PXqvTPfoieLEplN5ZwhAoGAe6rGSrplbYUyws4/5wIDr3XoS5vD26VgGHvI
NUMgBoW8gdDE8GpNE77Qa8QqtZ2wpGLhXT1bQFsO0On5OJ3/t7zYiiJeeZVTJlKS
dipBgSQ7hLjFFj0YoiWcFvisSlMge6SmOYMCbeAyxpX1Y0FuKvm9KpvN5P11KFt1
Vvy+EyACgYAQbMUcClGzedoL7bIf4vh7DiQedlMaTM66EL8awJAQBNfAc9au84J0
yMz84/6Dub2h+XwP6Ip5E+QjD32grBgj2MV3orjeXa3GKEbmLV9+3asZKma+gzfQ
urz0rfR767vp5p4ZScODAp/u64FrMQeiMLD0TePAOhDX7Y6ON5AOlwIVAIJE+2W3
jbdJzPIVCZV/ns8QD/HE
-----END DSA PRIVATE KEY-----"""
        private_key_value = (144819228510396375480510966045726324197234443151241728654670685625305230385467763734653299992854300412367868856607501321634131298084648429649714452472261648519166487595581105734370788168033696455943547609540069712392591019911289209306656760054646817215504894551439102079913490941604156000063251698742214491563L,  # noqa
                             917236267741783881593757959752012731596818193441L,
                             86841982599782731711680786695115998714268381589063264016290973272276727186587704621026934440034006184167355751218909451695519359588918346763818149790632054843515968619897385312275286742098283669267848018249714288204339152924266843441212538369167733167339150986744361013446636969482251895247384926570829189920L,  # noqa
                             11533944838210201987952882615702205024058326377484185868096298195008186074503068022497845512065175251915059614398323642650025427428897101740618300183253025704503580499048317459491367465314627384976328186431247669440664650252937901083193085980371291330875135658464646502031914168659603641006946305696513134231L,  # noqa
                             743707150676871705974360193988282239149564490180L)

        encrypted_private_key = """-----BEGIN DSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: DES-EDE3-CBC,4D88293673F5EB5A

z9/jHRxmgWErlTqA4+BsGWLnFFYJAUSDKLv2mbB7vsTkdHggJGUFm370hXR494R2
kV8zTcdl8xNaD5O9wv2TxAa+1XAK9PMvoZ22EicJkJfdVZB1WxOEo6gYafcbUwn5
jCw2WtOdmFL1LfBTKJUQsN3+s/Z/8xIFjHyZ1AqBEsbtvyT5x6gTCb5gqd9aLmAy
U1MSAS69G83XS0vwGjUBu6hIY+NqH97MYaYuUxYRHZ4kwx6a+AZA05VAWuqCNG/i
REnwjW62umnal6bAb/P0ShV/5Q1N+jOgfbAVeSbwOBNi6l0a3R2vsACowBsQGIOs
AI0LkOljn0SQ9EbiVFt0X3EmDDqXJ4pyUQiQWWVqk/NdlcOXlVHxW1LAH178prSh
9lynvklH9ddxf1ogZzoklnwbHFOQL82VP3OgHzLe4zHEZb7/7n04Hsn65tE1IDPN
BZCMWNmWn5b4XlpvM9qmrqw7OSHXJmo3pUuobgMDJY8ivajqqgLnKobNUyRqIIXe
K+HsnOddon8EQ7paJXiQtIoduGkNprkteopuVCTPVnJ7iPH7nlZlklRzMd0Nf6HX
uUG2MBh5S6IgJq3XFEqkLfnz1kLZTEqa
-----END DSA PRIVATE KEY-----"""

        encrypted_private_key_value = (164075852029082894163234846758911180897102424744594692831708895466836370115659341783053385481002813494629146363751701674649183075680471787603685864085093058760568072639047095709834084191969571447062510635065846311615327171352818023633056996941311207910795837452601272338087606069040250703385973352638645677247L,  # noqa
                                       1382042759715880151069055791721895992148320772021L,
                                       153926732596083235894968118340186482799172595631686494839668449588513699006316353000942531708584009247814149841973530532258546565044160391136120217307693710508485285457287378935130848066293114662648349996271228116599598513247673155335538020265820918414007609749609438690078201904948848717674434559570846935983L,  # noqa
                                       72384903337992313747768521223976137917296105324177373979721513786321354786021736609201334497124282719580897296769614546413169191983285786187571827959205578548773194544237603360013637593100703429306046520179789796054440223880857876564434231726404752178503857092191315752338544897832163988229715776540342317876L,  # noqa
                                       863501795884281323360678431361598105234793720895L)

        a = OpenSSH_Key_Storage()
        dss_obj = a.parse_public_key(public_key)
        self.assertEqual(dss_obj.public_key, public_key_value)

        dss_obj = a.parse_private_key(private_key)
        self.assertEqual(dss_obj.private_key, private_key_value)

        # Try an encrypted private key.
        class fixed_passphrase_OpenSSH_Key_Storage(OpenSSH_Key_Storage):
            def ask_for_passphrase():
                return 'foobar'
            ask_for_passphrase = staticmethod(ask_for_passphrase)

        a = fixed_passphrase_OpenSSH_Key_Storage()
        dss_obj = a.parse_private_key(encrypted_private_key)
        self.assertEqual(dss_obj.private_key, encrypted_private_key_value)


class load_rsa_test_case(ssh_key_storage_test_case):

    def runTest(self):
        public_key = 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAIEAqmjoccK1YhAhSC4TzycQ/61EtlbbeoeJDdaK03523oNbLdxw5snrsu1cpiR6xlPo5hBsaVBOvBeoQt+SrGwTy12P76i0LCyj6G+ylyHelWG5AsH4PyOETZonEaQiGozCHFXjZ4s/fQ0JjJ55zmhGgqpQNnz2SmqpjZjCipJX70c= admin@test04.god\n'  # noqa
        public_key_value = (35L,
                            119665828850037267028328680471142741797655500899213447140209093165296153783302091227178836947120698323025616662599207205000714031772597163041046686901539652734222225828489682382176199509894978250917122539502090949162883932641842735933478535393127460177054018760505497941735313342238826050943434409531493838663L)  # noqa
        private_key = """-----BEGIN RSA PRIVATE KEY-----
MIICWgIBAAKBgQCqaOhxwrViECFILhPPJxD/rUS2Vtt6h4kN1orTfnbeg1st3HDm
yeuy7VymJHrGU+jmEGxpUE68F6hC35KsbBPLXY/vqLQsLKPob7KXId6VYbkCwfg/
I4RNmicRpCIajMIcVeNniz99DQmMnnnOaEaCqlA2fPZKaqmNmMKKklfvRwIBIwKB
gD9LiYlW8unou+ecFfx8OYOJf+v0YCYyV3orHZ8DFjVj/UuMZHL6ir7NMQqClACF
kQT+yS5uSSFKnZUueE6rzNXmW+TSXbkYx+Ews60gC1gkbKpV45oKZhg1yfRErwOy
JF8VPQzstIZdf0iWU3uQ+6T64CrPLz0c40b5I47ux5mbAkEA3r2aeIQsftWedjgx
+mXwtCJb1+3oOcbz3tPoZGTqvLWReuQ5ZmZIj06Lg3cy3sNH5+pjDCmiQlH8Eo7E
fFM8jwJBAMPa7SEzquE9/2KqV/iVLYaZedtqfRyDGIz4XIbSTT80dyuplTFpVJWz
Fks1ie97Q4FswqV03D0HXEnnu2vD7ckCQHjqla8jLhj3nyo7w1wLdAoEBfjgPDyf
M+3+AdAZhr42rg+DNRpUyE3LjZCCiVRbYYyGjYpCfKeo2UvnGjTcunkCQBDJn0v9
HUaBqC0HSV5zL8m1Yjdg5icD7ClXHd+roDieGNfbVe5K211J3VbnVPdFFGot5Mxa
eUcPQmy8F2ECKlMCQQDX7sV4IqQrPXGC3TpHXNMJfRy+RuGNFH3mf5j3iAhgnSc5
Sk5Qdbor2yjwE/GHz2ycGtkjRulOUTv4TYX5+O6u
-----END RSA PRIVATE KEY-----"""
        private_key_value = (119665828850037267028328680471142741797655500899213447140209093165296153783302091227178836947120698323025616662599207205000714031772597163041046686901539652734222225828489682382176199509894978250917122539502090949162883932641842735933478535393127460177054018760505497941735313342238826050943434409531493838663L,  # noqa
                             35L,
                             44447307858585270610522081317853018381986328905422137509220520318538571405226491027237853723216259377123800474679705533285979497515536089129531626563429005729644097293794274690875731684502848992888319583322382841862406829655518405505486920254346175585047037778915301016757980191172721050366334657821864860059L,  # noqa
                             11665873813839313890672267141763021801562202172556393999332152934535676204166852333822550370230102963227543021845798110514178180132425544663462167708908687L,  # noqa
                             10257768150044345052157551318596288583246323320335701676273534238701739242861950987619850286757345117997197327072483122544803209060028258639176304917999049L)  # noqa

        encrypted_private_key = """-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: DES-EDE3-CBC,09F6C5E0A7DE5062

12K8ifZGdXBydKq73mrFhrZ3MQvesgCHnSF5DAFt6xka89S7LC8fGxyxKtUNxewF
SZ+9m6A+/ZvD5vsI8xSicdEToFKDIzjBdPg6/2c2PZ5qQn46fQUjoRh3HVxMwe3j
y2c8s9z62aprf6P+QrllcI2h1dSoGpiS2v/CaeZSAo+cDnULrX7ICraVLx7TMdsz
vXuaf4Qa+CcunYOl9fSMAATUZD3LiRAICLZxcsT7MaqVLerWquZ54kOnOLAXMMDb
bth0+DlErt7Er2zjCWz+GSZY5QG663FTBVVVhgQGrj3D9T9VgvMVOneqJHGgkHB8
OYP5Lzw3ukNxlxP8L5Is22S7dGqHNYAoqecS8l6kPkrChRosVHcyl0WViKKuUacm
Oeeh7bYiQNtWy7yXWfwbA/qV46rQqb9jvoK0X0poL1QxLnayZtE2Py5AwGT17MLE
Fgcf5aXRk8BlEhS7Cxx7bTFfgC4reV/SL6D+bWYU91YuWx+Ivr1W/WF7JGBhY+PC
PuYX4L0U1btWcj5Y35ZZQX3iLM5Qo+39gL8YJ8Ee+F51MEu89yuSJrxpbanec55r
iawSKr0WOyY44GA3sfRGKbr6EN56QoR938S0nVAwYCPqwJz00+7ElpLNgH4Utjwj
68pP4jQTVGgI7K4gxN2jDvlol/dTprmjXmyHykW7s7s5Ew4wrN+qMFpgeyIz9/qc
5L0OtjBAbjyLFdE0Xngg0Lmn2bIlvL8jrMPGwaxqu2T0ulrLN8Z2G+1iAafj8Kqh
VaPIN0x5mSV39WpxGz4SmIOVZdIlUL9dOJEv4K4qOHQ=
-----END RSA PRIVATE KEY-----"""

        encrypted_private_key_value = (132188032201840059483513934225114077156533953079152741406965569217951447670952039954408688762434667064008713729945007603706264832329500562212083089065564763658016112303811971922789074398850943636984349987490511238526103841681580491336332634066855256218075929727422563972932130562723997143531042508709389080193L,  # noqa
                                       35L,
                                       86866421161209181946309156776503536417150883452014658638863088343225237040911340541468566901028495499205726165392433568149831175530814655167940315671656829536913792735395949882296110745152030053580749430717046323948708357601049256155974132066189771161275225056736507873625291084649019736200813189569105066383L,  # noqa
                                       12385928121051751494808912011530512750393016832123879182480209182965942204322814452624601155140075558662348021403304814310741397421628588564328911043663179L,  # noqa
                                       10672436567524284031640817763908324234466779420903824519134916712505476909909154093286756280899157968011858280387816948898605556328090289066804367098576867L)  # noqa

        a = OpenSSH_Key_Storage()
        rsa_obj = a.parse_public_key(public_key)
        self.assertEqual(rsa_obj.public_key, public_key_value)

        rsa_obj = a.parse_private_key(private_key)
        self.assertEqual(rsa_obj.private_key, private_key_value)

        # Try an encrypted private key.
        class fixed_passphrase_OpenSSH_Key_Storage(OpenSSH_Key_Storage):
            def ask_for_passphrase():
                return 'foobar'
            ask_for_passphrase = staticmethod(ask_for_passphrase)

        a = fixed_passphrase_OpenSSH_Key_Storage()
        rsa_obj = a.parse_private_key(encrypted_private_key)
        self.assertEqual(rsa_obj.private_key, encrypted_private_key_value)

def suite():
    suite = unittest.TestSuite()
    suite.addTest(load_dsa_test_case())
    suite.addTest(load_rsa_test_case())
    return suite

if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = openssh_known_hosts
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.openssh_known_hosts
#
# This module handles the known_hosts file.
#

# The known_hosts file has the following format:
# Each line contains a key with the following fields (space separated):
# SSH2:
# list_of_hosts, keytype, key, comment
# SSH1:
# list_of_hosts, number_of_bits, exponent, modulus, comment
#
# hosts is a comma-separated list of hosts.
# '*' and '?' are allowed wildcards.
# A hostname starting with '!' means negation.  If a hostname matches a
# negated pattern, it is not accepted (by that line) even if it matched
# another pattern on the line.
#
# Lines starting with '#' are comments.

# The known_hosts file is found in $HOME/.ssh/known_hosts or
# in $SSHDIR/ssh_known_hosts.
#
# As a historical note, OpenSSH used to have files called known_hosts2
# (or ssh_known_hosts2 for the system-wide version).  This implementation
# does not try to load these copies, since that technique is quite antiquated.

import base64
import binascii
import errno
import os
import re

from coro.ssh.keys import dss, rsa
from coro.ssh.keys import openssh_key_formats
from coro.ssh.keys.key_storage import Host_Key_Changed_Error
from coro.ssh.keys.remote_host import IPv4_Remote_Host_ID

class OpenSSH_Known_Hosts:

    def __init__(self):
        pass

    def get_known_hosts_filenames(self, username):
        # XXX: Support system-wide copy.
        return [self.get_users_known_hosts_filename(username)]

    def get_users_known_hosts_filename(self, username):
        if username is None:
            username = os.getlogin()
        home_dir = os.path.expanduser('~' + username)
        user_known_hosts_filename = os.path.join(home_dir, '.ssh', 'known_hosts')
        return user_known_hosts_filename

    def check_for_host(self, host_id, key, username=None, port=22):
        """check_for_host(self, host_id, key, username=None) -> boolean
        Checks if the given key is in the known_hosts file.
        Returns true if it is, otherwise returns false.
        If the host was found, but the key did not match, it raises a
        Host_Key_Changed_Error exception.

        <username> - May be None to use the current user.

        <host_id> - A IPv4_Remote_Host_ID instance.

        <key> - A SSH_Public_Private_Key instance.
        """

        if not isinstance(host_id, IPv4_Remote_Host_ID):
            raise TypeError(host_id)

        if host_id.hostname is not None:
            hosts = [host_id.ip, host_id.hostname]
        else:
            hosts = [host_id.ip]

        # Changed is a variable to detect a Host_Key_Changed_Error.
        # We store away that the error has occurred so that we can allow
        # other files to potentially have a correct copy.
        changed = None
        for filename in self.get_known_hosts_filenames(username):
            for host in hosts:
                try:
                    if self._check_for_host(filename, host_id, host, port, key):
                        return 1
                except Host_Key_Changed_Error, e:
                    changed = e

        if changed is None:
            return 0
        else:
            raise changed

    def _check_for_host(self, filename, host_id, host, port, key):
        try:
            f = open(filename)
        except IOError:
            return 0

        changed = None
        line_number = 0
        for line in f.readlines():
            line_number += 1
            line = line.strip()
            if len(line) == 0 or line[0] == '#':
                continue
            m = openssh_key_formats.ssh2_known_hosts_entry.match(line)
            if m:
                if key.name == m.group('keytype'):
                    if self._match_host(host, port, m.group('list_of_hosts')):
                        if self._match_key(key, m.group('base64_key')):
                            return 1
                        else:
                            # Found a conflicting key.
                            changed = Host_Key_Changed_Error(host_id, '%s:%i' % (filename, line_number))
            else:
                # Currently not supporting SSH1 style.
                # m = openssh_key_formats.ssh1_key.match(line)
                continue

        if changed is None:
            return 0
        else:
            raise changed

    def _match_host(self, host, port, pattern):
        patterns = pattern.split(',')
        # Negated_Pattern is used to terminate the checks.
        try:
            for p in patterns:
                if self._match_pattern(host, port, p):
                    return 1
        except OpenSSH_Known_Hosts.Negated_Pattern:
            return 0
        return 0

    class Negated_Pattern(Exception):
        pass

    host_with_port = re.compile ('^\\[([^\\]]+)\\]:([0-9]+)')

    def _match_pattern(self, host, port, pattern):
        # XXX: OpenSSH does not do any special work to check IP addresses.
        # It just assumes that it will match character-for-character.
        # Thus, 001.002.003.004 != 1.2.3.4 even though those are technically
        # the same IP.
        if pattern and pattern[0] == '!':
            negate = 1
            pattern = pattern[1:]
        else:
            negate = 0
        if host == pattern:
            if negate:
                raise OpenSSH_Known_Hosts.Negated_Pattern
            else:
                return 1
        # check for host port
        port_probe = self.host_with_port.match (pattern)
        if port_probe:
            # host with port
            host0, port0 = port_probe.groups()
            port0 = int (port0)
            if host == host0 and port == port0:
                if negate:
                    raise OpenSSH_Known_Hosts.Negated_Pattern
                else:
                    return 1
        # Check for wildcards.
        # XXX: Lazy
        # XXX: We could potentially escape other RE-special characters.
        pattern = pattern.replace('.', '[.]')
        # Convert * and ? wildcards into RE wildcards.
        pattern = pattern.replace('*', '.*')
        pattern = pattern.replace('?', '.')
        pattern = pattern + '$'
        r = re.compile(pattern, re.IGNORECASE)
        if r.match(host):
            if negate:
                raise OpenSSH_Known_Hosts.Negated_Pattern
            else:
                return 1
        else:
            return 0

    def _match_key(self, key_obj, base64_key):
        key = key_obj.name + ' ' + base64_key
        # XXX: static or class method would make this instantiation not necessary.
        #      Too bad the syntax sucks.
        from coro.ssh.keys.openssh_key_storage import OpenSSH_Key_Storage
        x = OpenSSH_Key_Storage()
        parsed_key = x.parse_public_key(key)
        if parsed_key.public_key == key_obj.public_key:
            return 1
        else:
            return 0

    def update_known_hosts(self, host, public_host_key, username=None):
        # XXX: Locking
        filename = self.get_users_known_hosts_filename(username)
        tmp_filename = filename + '.tmp'
        try:
            f = open(filename)
        except IOError, why:
            if why.errno == errno.ENOENT:
                f = None
            else:
                raise
        f_tmp = open(tmp_filename, 'w')

        # This is a flag used to indicate that we made the update.
        # If, after parsing through the original known_hosts file, and we
        # have not done the update, then we will just append the new key to
        # the file.
        updated = 0
        if f:
            for line in f.readlines():
                line = line.strip()
                new_line = line
                if len(line) != 0 and line[0] != '#':
                    m = openssh_key_formats.ssh2_known_hosts_entry.match(line)
                    if m:
                        if public_host_key.name == m.group('keytype'):
                            # Same keytype..See if we need to update.
                            # If the key is the same, then just update the host list.
                            # XXX: This code needs to be refactored.
                            base64_key = m.group('base64_key')
                            binary_key = binascii.a2b_base64(base64_key)
                            if public_host_key.name == 'ssh-dss':
                                key_obj = dss.SSH_DSS()
                            elif public_host_key.name == 'ssh-rsa':
                                key_obj = dss.SSH_RSA()
                            else:
                                # This should never happen.
                                raise ValueError(public_host_key.name)
                            host_list = m.group('list_of_hosts')
                            host_list = host_list.split(',')
                            key_obj.set_public_key(binary_key)
                            if key_obj.get_public_key_blob() == public_host_key.get_public_key_blob():
                                # Same key.
                                # Add this host to the list if it is not already there.
                                tmp_host_list = [x.lower() for x in host_list]
                                if host.lower() not in tmp_host_list:
                                    host_list.append(host)
                                comment = m.group('comment')
                                if comment is None:
                                    comment = ''
                                new_line = ','.join(host_list) + ' ' + m.group('keytype') + \
                                    ' ' + m.group('base64_key') + comment
                                updated = 1
                            else:
                                # Keys differ...Remove this host from the list if it was in there.
                                new_host_list = filter(lambda x, y=host.lower(): x.lower() != y, host_list)
                                comment = m.group('comment')
                                if comment is None:
                                    comment = ''
                                new_line = ','.join(new_host_list) + ' ' + m.group('keytype') + \
                                    ' ' + m.group('base64_key') + comment
                    else:
                        # XXX: Support SSH1 keys.
                        pass
                f_tmp.write(new_line + '\n')

        if not updated:
            # Append to the end.
            base64_key = base64.encodestring(public_host_key.get_public_key_blob())
            # Strip the newlines that the base64 module inserts.
            base64_key = base64_key.replace('\n', '')
            f_tmp.write(host + ' ' + public_host_key.name + ' ' + base64_key + '\n')

        if f:
            f.close()
        f_tmp.close()
        # XXX: Permissions??
        os.rename(tmp_filename, filename)


import unittest

class openssh_known_hosts_test_case(unittest.TestCase):
    pass

class check_for_host_test_case(openssh_known_hosts_test_case):

    def runTest(self):
        # Build a sample known_hosts test file.
        tmp_filename = os.tempnam()
        f = open(tmp_filename, 'w')
        f.write("""# Example known hosts file.
10.1.1.108 ssh-dss AAAAB3NzaC1kc3MAAACBAOdTJwlIDyxIKAaCoGr/XsySV8AzJDU2fAePcO7CBURUYyUHS9uKsgsjZw7qBkdnkWT/Yx2Z8k9j+HuJ2L3mI6y9+cknen9ycze6g/UwmYa2+forEz7NXiEWi3lTHXnAXjEpmCWZAB6++HPM9rq+wQluOcN8cks57sttzxzqQG6RAAAAFQCgQ/edsYFn4jEBiJhoj97GElWM0QAAAIAz5hffnGq9zK5rtrNpKVdz6KIEyeXlGd16f6NwotVwpNd2zEXBC3+z13TyMiX35H9m7fWYQen7O7RNekJl5Gz7V6UA7lipNFrhmg/eO6rnXetrrgjdiHF5mSx3O8uBQOU5tK+IyAINtBhDqM6GNEqEkFa9yT6POYjGA8ihSaUUOQAAAIEAvvDYfg+KrBZUlJGMK/g1muBbBu5o+UbppgRTOsEfAMKRovV0vsZc4AIaeh/uGVKS+zXqQHh7btHgTMQ47hxF3tPVFWIgO6vDtsQX90e9xaCfmKQY2EV0Wrq1XUKxOycTNRZ5kCxYkq4qRhs5QnqB/Ov71g7HHxsJ8pnjSDusiNo=
172.16.1.11 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAIEAvUNY7kd1sDujt9HhdT6VWtf8yVRAw2Ib+M6ptWTuWWnPGR6TP/ZwumSs/rAguyxWrNRbw7Eainr/BTEFATpJRYKUDPZKGHLT3ixtOy7scUVRyaJD7F3L7BujkhHLWOyFJGtoZmJEdQmddGDwq+16gLD06GA8/N8kkQFRR6vwlRs=
64.70.20.70,64.70.44.3 1024 35 162807158017859311401243513535320968370503503816817576276599779420791975206320054411137858395244854129122865069311130487158120563446636918588974972115213166463069362091898230386572857193095086738994217228848073927343769936543295334648942676920084374567042307974866766193585693129128286570059425685457486987781
lists.ironport.com,10.1.1.109 ssh-dss AAAAB3NzaC1kc3MAAACBAOfO0s6KFDk8lU7hJyLWevjEIi9drfn8wJYFvYAc+apN4+Qlq4DtFXMDH8U5pQWpZsj705ywi5cex8aEaeepfeQBe6NQCmJci47cTTnaiy/IR7d2hZkB0LmJJX6JxYWWtk2kFyL4xbPEfXbBpNprTfNzgi32YeeIKak3T3amYo8dAAAAFQDInSP36WJZ7WnH13qBXZM+5USftwAAAIAQ7CHz/hwxpmYNind6Zm7vmFC8JkRdkTjNIfyuHszfgHI3+imhSJJxjaSwdvLNi+2P2cdoTrL45ITPFT0+YSq1VIXclqa0k0kjETFbayGbq9DE3w7S6WBiiewTcllu7NzO9EvaNt3XJUQ7SpvNBoLhv+XAHkdhX0ouwwtyeElT6gAAAIAEcOOSQClq9CIcYEjwtfDBANaJ7o2WfYJMqto+ibjnl+1YFtGw9ofD5gi5gtEIGtSb6mO88ooX9sfmkaAY+1L/gTdb3Fxc3zuL2PymBt1ruNTgVzEjV35h94lgC3+F4mPz0jQpnpsbxhm/uDn/i1BeRBlzMhyWOAHfLknna9WCmg==
!outlaw.qa,*.qa,172.17.0.201 ssh-dss AAAAB3NzaC1kc3MAAACBAPaQAeia7kiuORu9425IyZRKlRPkom9mjEVERjN3Lw5R93rBZSwbl8wiT1PEeBN2047SZD7ucHaAUqAU39l//JVA0Q/RHXczad1niqC7Y7YKSpu3XfI7vpgMd91XIlxNhnhvNLtWfmwuWuX1FFiByKUY7fsHVeKTYwnvRPiv89IBAAAAFQCryy7v2z5Olv1Z0bSoQLDemiSzywAAAIEA3pmx1n0YRuw3hY4RfXbQCUxtu19bldG4XlNnmeIE8cb4tdGHBgLnLrMpSLsA4aMOWAzzDvB/Gk9AlgyNuYp2NaCFStE5yYiK9c+wTNpChCsDx/BqWMtPYTKQDZmhmSp94noIQd429OIJhQt1qL/7vHD1Tac/2V33TsADYW4aS+4AAACBANC4tVdIkB5vyLm2BrjK+P7uS8SaUSfKaAd83XahVz2q8cIeiFHrXvfLRFeks99vgxSPq6mqxC5zpcDGFWBm1UJY4PxyG+t6AhgYEPefD+ofXAvTHLPIRJbNv2BDP6vHOKRfAYtWGbQf6sXw4VwS9mAR6JHlGoHMLnRewMcq49jE
*.com,test04.god ssh-dss AAAAB3NzaC1kc3MAAACBAK3p8k1i9I/m0no3LAS4etFGsommDJcBQfsuP/nn42O0VyDXltcfjLvWxABZow6iKJHiHZ8FN/FxOX+jZUlIplrs6oRYbKeWegq3NcvelEderWhIyOKrDZHgO9HprwamSMWFxDG5kUSJ/em/G5N+rGv8K7dJfCus42ynh0+a/Q1dAAAAFQD1/X/izKQrZs//Q5HgVVOfEqK6+wAAAIBQw1TWAHQHiihsCbbMbGuzm/7Rq9YTvGNyzmBgAP/fbmv/Vi3lZwmTilKSkebEFvrWeAT1hI9KufzjeRhkUCZGzCmCt7A614/brJRIznOAvWaTRsy/wzw7kdARljdQRTcnSXnpc81jEzMyt2SzcifZOvyNfIhAtFXX6yXeFg1dpgAAAIBoJZa1MTGEWJ43BcFftRGbnf/EK5+SDlYgrSiJZeGAUURvrdJPPtCSRtQU7ldiGfKiPcD/6U0XcC9o09/sDSfFOEtTFnawe74pqcQVT3x2hQ5Zs1W82M2arNXaoYBo21RAE4oy1u010a4hjxPoSrAVyQXVwL2Sv8B5vDu99sIu1w==

""")  # noqa
        f.close()

        # Make a subclass so we can control which file it loads.
        class custom_known_hosts(OpenSSH_Known_Hosts):
            def __init__(self, tmp_filename):
                self.tmp_filename = tmp_filename

            def get_known_hosts_filenames(self, username):
                return [self.tmp_filename]

        try:
            from coro.ssh.keys.openssh_key_storage import OpenSSH_Key_Storage
            keystore = OpenSSH_Key_Storage()
            x = custom_known_hosts(tmp_filename)
            # Make some keys to test against.
            # 10.1.1.108
            k1 = keystore.parse_public_key(
                'ssh-dss AAAAB3NzaC1kc3MAAACBAOdTJwlIDyxIKAaCoGr/XsySV8AzJDU2fAePcO7CBURUYyUHS9uKsgsjZw7qBkdnkWT/Yx2Z8k9j+HuJ2L3mI6y9+cknen9ycze6g/UwmYa2+forEz7NXiEWi3lTHXnAXjEpmCWZAB6++HPM9rq+wQluOcN8cks57sttzxzqQG6RAAAAFQCgQ/edsYFn4jEBiJhoj97GElWM0QAAAIAz5hffnGq9zK5rtrNpKVdz6KIEyeXlGd16f6NwotVwpNd2zEXBC3+z13TyMiX35H9m7fWYQen7O7RNekJl5Gz7V6UA7lipNFrhmg/eO6rnXetrrgjdiHF5mSx3O8uBQOU5tK+IyAINtBhDqM6GNEqEkFa9yT6POYjGA8ihSaUUOQAAAIEAvvDYfg+KrBZUlJGMK/g1muBbBu5o+UbppgRTOsEfAMKRovV0vsZc4AIaeh/uGVKS+zXqQHh7btHgTMQ47hxF3tPVFWIgO6vDtsQX90e9xaCfmKQY2EV0Wrq1XUKxOycTNRZ5kCxYkq4qRhs5QnqB/Ov71g7HHxsJ8pnjSDusiNo=')  # noqa
            # lists.ironport.com
            k2 = keystore.parse_public_key(
                'ssh-dss AAAAB3NzaC1kc3MAAACBAOfO0s6KFDk8lU7hJyLWevjEIi9drfn8wJYFvYAc+apN4+Qlq4DtFXMDH8U5pQWpZsj705ywi5cex8aEaeepfeQBe6NQCmJci47cTTnaiy/IR7d2hZkB0LmJJX6JxYWWtk2kFyL4xbPEfXbBpNprTfNzgi32YeeIKak3T3amYo8dAAAAFQDInSP36WJZ7WnH13qBXZM+5USftwAAAIAQ7CHz/hwxpmYNind6Zm7vmFC8JkRdkTjNIfyuHszfgHI3+imhSJJxjaSwdvLNi+2P2cdoTrL45ITPFT0+YSq1VIXclqa0k0kjETFbayGbq9DE3w7S6WBiiewTcllu7NzO9EvaNt3XJUQ7SpvNBoLhv+XAHkdhX0ouwwtyeElT6gAAAIAEcOOSQClq9CIcYEjwtfDBANaJ7o2WfYJMqto+ibjnl+1YFtGw9ofD5gi5gtEIGtSb6mO88ooX9sfmkaAY+1L/gTdb3Fxc3zuL2PymBt1ruNTgVzEjV35h94lgC3+F4mPz0jQpnpsbxhm/uDn/i1BeRBlzMhyWOAHfLknna9WCmg==')  # noqa
            # 172.17.0.201
            k3 = keystore.parse_public_key(
                'ssh-dss AAAAB3NzaC1kc3MAAACBAPaQAeia7kiuORu9425IyZRKlRPkom9mjEVERjN3Lw5R93rBZSwbl8wiT1PEeBN2047SZD7ucHaAUqAU39l//JVA0Q/RHXczad1niqC7Y7YKSpu3XfI7vpgMd91XIlxNhnhvNLtWfmwuWuX1FFiByKUY7fsHVeKTYwnvRPiv89IBAAAAFQCryy7v2z5Olv1Z0bSoQLDemiSzywAAAIEA3pmx1n0YRuw3hY4RfXbQCUxtu19bldG4XlNnmeIE8cb4tdGHBgLnLrMpSLsA4aMOWAzzDvB/Gk9AlgyNuYp2NaCFStE5yYiK9c+wTNpChCsDx/BqWMtPYTKQDZmhmSp94noIQd429OIJhQt1qL/7vHD1Tac/2V33TsADYW4aS+4AAACBANC4tVdIkB5vyLm2BrjK+P7uS8SaUSfKaAd83XahVz2q8cIeiFHrXvfLRFeks99vgxSPq6mqxC5zpcDGFWBm1UJY4PxyG+t6AhgYEPefD+ofXAvTHLPIRJbNv2BDP6vHOKRfAYtWGbQf6sXw4VwS9mAR6JHlGoHMLnRewMcq49jE')  # noqa
            # test04.god
            k4 = keystore.parse_public_key(
                'ssh-dss AAAAB3NzaC1kc3MAAACBAK3p8k1i9I/m0no3LAS4etFGsommDJcBQfsuP/nn42O0VyDXltcfjLvWxABZow6iKJHiHZ8FN/FxOX+jZUlIplrs6oRYbKeWegq3NcvelEderWhIyOKrDZHgO9HprwamSMWFxDG5kUSJ/em/G5N+rGv8K7dJfCus42ynh0+a/Q1dAAAAFQD1/X/izKQrZs//Q5HgVVOfEqK6+wAAAIBQw1TWAHQHiihsCbbMbGuzm/7Rq9YTvGNyzmBgAP/fbmv/Vi3lZwmTilKSkebEFvrWeAT1hI9KufzjeRhkUCZGzCmCt7A614/brJRIznOAvWaTRsy/wzw7kdARljdQRTcnSXnpc81jEzMyt2SzcifZOvyNfIhAtFXX6yXeFg1dpgAAAIBoJZa1MTGEWJ43BcFftRGbnf/EK5+SDlYgrSiJZeGAUURvrdJPPtCSRtQU7ldiGfKiPcD/6U0XcC9o09/sDSfFOEtTFnawe74pqcQVT3x2hQ5Zs1W82M2arNXaoYBo21RAE4oy1u010a4hjxPoSrAVyQXVwL2Sv8B5vDu99sIu1w==')  # noqa
            # Make a key that doesn't exist in the known hosts file.
            unknown_key = keystore.parse_public_key(
                'ssh-dss AAAAB3NzaC1kc3MAAACBAJSc17NO4rxvhUfwjMzJMG9On9umzzlbwlN0wBv5riYetE1flTyySOUPa8YvpNYmMs5GSz0CzO/FI/EM/rgYvpvA+KKpV/9oL+XoT/O36t6Q8MZIGXwj75lxP8X9NSZxO0b5E7CRDyW5rsl6xfa3YaQrWqZRKhOeGASWRYtUZcpVAAAAFQCazkzFpIwqEpAbn0jZlkUHKbpwuQAAAIA4AGcL/OMIDtxC7T1smSPVk0VEr5i+IfL4xPLRSQCw6/Jr4OLzBH/TiTAjyp7NZszIu586J85t1nO3kOx/fKI8Ik2jDvJOmdUtDvMZnbZK1rvFiw3dCxEERGVW1LjyAnxtebl/pOJ6CpO4Pfh87mx+iH9m90oZSCDz602DXUz50wAAAIA0mmctzgavC8ApEsbKI69MhaYhkyxvEaucTarkGPAPvXPurfVJ8ZwtK3dYckLgn3a5WPHWqIZVfmtSbnkwld+t3BIl8IX6bKa2WaffUeU6k50ssUV6IvW+IHd0JJ/mwE6f9caNS7x0pC0+DQujp553IP5cr9NskQTK4j/Iwwlkrw==')  # noqa
            # 172.16.1.11
            k5 = keystore.parse_public_key(
                'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAIEAvUNY7kd1sDujt9HhdT6VWtf8yVRAw2Ib+M6ptWTuWWnPGR6TP/ZwumSs/rAguyxWrNRbw7Eainr/BTEFATpJRYKUDPZKGHLT3ixtOy7scUVRyaJD7F3L7BujkhHLWOyFJGtoZmJEdQmddGDwq+16gLD06GA8/N8kkQFRR6vwlRs=')  # noqa

            # Do the tests.
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('10.1.1.108', ''), k1), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('1.2.3.4', ''), k1), 0)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('0.0.0.0', 'lists.ironport.com'), k2), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('lists.ironport.com', '10.1.1.109'), k2), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('10.1.1.109', ''), k2), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('0.0.0.0', 'outlaw.qa'), k3), 0)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('0.0.0.0', 'foo.qa'), k3), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('172.17.0.201', ''), k3), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('0.0.0.0', 'foo.com'), k4), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('0.0.0.0', 'test04.god'), k4), 1)
            self.assertRaises(Host_Key_Changed_Error, x.check_for_host, IPv4_Remote_Host_ID('10.1.1.108', ''), k2)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('lists.ironport.com', '10.1.1.108'), k1), 1)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('0.0.0.0', 'unknown.dom'), k1), 0)
            self.assertRaises(
                Host_Key_Changed_Error, x.check_for_host, IPv4_Remote_Host_ID('10.1.1.108', ''), unknown_key)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('172.16.1.11', ''), unknown_key), 0)
            self.assertEqual(x.check_for_host(IPv4_Remote_Host_ID('172.16.1.11', ''), k5), 1)
        finally:
            os.unlink(tmp_filename)


def suite():
    suite = unittest.TestSuite()
    suite.addTest(check_for_host_test_case())
    return suite

if __name__ == '__main__':
    unittest.main(defaultTest='suite', module='openssh_known_hosts')

########NEW FILE########
__FILENAME__ = public_private_key
# $Header: //prod/main/ap/ssh/ssh/keys/public_private_key.py#1 $

"""ssh.keys.public_private_key

This is the base public/private key object.
Specific key types subclass this for their implementation.
"""

import hashlib

class SSH_Public_Private_Key:
    """SSH_Public_Private_Key

    Base class for any type of public/private key.
    """
    name = 'none'

    # Features of this key type.
    supports_signature = 0
    supports_encryption = 0

    # Keys are encoded according to the implementation.
    private_key = None
    public_key = None

    def set_public_key(self, public_key):
        """set_public_key(self, public_key) -> None
        Sets the public key.  public_key is a string encoded according
        to the algorithm.
        """
        raise NotImplementedError

    def set_private_key(self, private_key):
        """set_private_key(self, private_key) -> None
        Sets the private key.  private_key is a string encoded according
        to the algorithm.
        """
        raise NotImplementedError

    def get_public_key_blob(self):
        raise NotImplementedError

    def get_private_key_blob(self):
        raise NotImplementedError

    def sign(self, message):
        """sign(self, message, ) -> signature
        Signs a message with the given private_key.
        <message> is a string of bytes.

        The resulting signature is encoded as a payload of (string, bytes)
        where string is the signature format identifier and byes is the
        signature blob.
        """
        raise NotImplementedError

    def verify(self, message, signature):
        """verify(self, message, signature, public_key) -> boolean
        Returns true or false if the signature is a match for the
        signature of <message>.
        <message> is a string of bytes.
        <signature> is a payload encoded as (string, bytes).
        """
        raise NotImplementedError

    def public_key_fingerprint(self):
        """public_key_fingerprint(self) -> fingerprint string
        Returns a fingerprint of the public key.
        """
        m = hashlib.md5(self.get_public_key_blob())
        # hexdigest returns lowercase already, but I just wanted to be careful.
        fingerprint = m.hexdigest().lower()
        pieces = [fingerprint[x] + fingerprint[x + 1] for x in xrange(0, len(fingerprint), 2)]
        return ':'.join(pieces)

    # XXX: encrypt functions...

########NEW FILE########
__FILENAME__ = rebuild
# -*- Mode: Python -*-

# utility functions for building regular expressions

def OR (*args):
    return '(?:' + '|'.join (args) + ')'

def CONCAT (*args):
    return '(?:' + ''.join (args) + ')'

def NTIMES (arg, l, h):
    return '(?:' + arg + '){%d,%d}' % (l, h)

def OPTIONAL (*args):
    return '(?:' + CONCAT(*args) + ')?'

def PLUS (*args):
    return '(?:' + CONCAT(*args) + ')+'

def SPLAT (*args):
    return '(?:' + CONCAT(*args) + ')*'

def NAME (name, arg):
    return '(?P<%s>%s)' % (name, arg)

########NEW FILE########
__FILENAME__ = remote_host
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# remote_host
#
# This module is the class that abstracts the ID of a host.
# Typically the ID is based on the IP or hostname of the remote host, but this
# allows you to use non-IP configurations.
#

class Remote_Host_ID:
    pass

class IPv4_Remote_Host_ID(Remote_Host_ID):

    """IPv4_Remote_Host_ID

    Represents the ID of the remote host.

    <ip> is required.
    <hostname> is optional.
    """

    ip = ''
    hostname = None

    def __init__(self, ip, hostname):
        self.ip = ip
        self.hostname = hostname

    def __repr__(self):
        return '<IPv4_Remote_Host_ID instance ip=%r hostname=%r>' % (self.ip, self.hostname)

########NEW FILE########
__FILENAME__ = rsa
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.rsa
#
# Encapsulates the RSA key.

import hashlib
import public_private_key
from coro.ssh.util import packet
from Crypto.PublicKey import RSA
from Crypto.Util import number

# This is the DER encoding of the SHA1 identifier.
SHA1_Digest_Info = '\x30\x21\x30\x09\x06\x05\x2b\x0e\x03\x02\x1a\x05\x00\x04\x14'

class SSH_RSA(public_private_key.SSH_Public_Private_Key):

    # Features of this key type.
    supports_signature = 1
    supports_encryption = 0
    name = 'ssh-rsa'
    private_key = (0L, 0L, 0L, 0L, 0L)  # n, e, d, p, q
    public_key = (0L, 0L)       # e, n

    def set_public_key(self, public_key):
        rsa, e, n = packet.unpack_payload(RSA_PUBLIC_KEY_PAYLOAD, public_key)
        if rsa != 'ssh-rsa':
            raise ValueError(rsa)
        self.public_key = (e, n)

    def set_private_key(self, private_key):
        rsa, n, e, d, p, q = packet.unpack_payload(RSA_PRIVATE_KEY_PAYLOAD, private_key)
        if rsa != 'ssh-rsa':
            raise ValueError(rsa)
        self.public_key = (n, e, d, p, q)

    def get_public_key_blob(self):
        e, n = self.public_key
        return packet.pack_payload(RSA_PUBLIC_KEY_PAYLOAD,
                                   ('ssh-rsa',
                                    e, n))

    def get_private_key_blob(self):
        n, e, d, p, q = self.public_key
        return packet.pack_payload(RSA_PRIVATE_KEY_PAYLOAD,
                                   ('ssh-rsa',
                                    n, e, d, p, q))

    def emsa_pkcs1_v1_5_encode(self, message, n_len):
        """emsa_pkcs1_v1_5_encode(self, message, n_len) -> encoded_message
        Encodes the given string via the EMSA PKCS#1 version 1.5 method.

        <message> - The string to encode.
        <n_len> - The length (in octets) of the RSA modulus n.
        """
        hash = hashlib.sha1(message).digest()
        T = SHA1_Digest_Info + hash
        if __debug__:
            assert n_len >= len(T) + 11
        # PKCS spec says that it's -3...I do not understand why that doesn't work.
        PS = '\xff' * (n_len - len(T) - 2)
        if __debug__:
            assert len(PS) >= 8
        return '\x00\x01' + PS + '\x00' + T

    def sign(self, message):
        n, e, d, p, q = self.private_key
        rsa_obj = RSA.construct((n, e, d, p, q))
        modulus_n_length_in_octets = rsa_obj.size() / 8
        encoded_message = self.emsa_pkcs1_v1_5_encode(message, modulus_n_length_in_octets)
        signature = rsa_obj.sign(encoded_message, '')[0]    # Returns tuple of 1 element.
        signature = number.long_to_bytes(signature)
        return packet.pack_payload(RSA_SIG_PAYLOAD,
                                   ('ssh-rsa',
                                    signature))

    def verify(self, message, signature):
        e, n = self.public_key
        rsa, blob = packet.unpack_payload(RSA_SIG_PAYLOAD, signature)
        if rsa != 'ssh-rsa':
            raise ValueError(rsa)
        s = number.bytes_to_long(blob)
        rsa_obj = RSA.construct((n, e))
        modulus_n_length_in_octets = rsa_obj.size() / 8
        encoded_message = self.emsa_pkcs1_v1_5_encode(message, modulus_n_length_in_octets)
        return rsa_obj.verify(encoded_message, (s,))

RSA_PUBLIC_KEY_PAYLOAD = (packet.STRING,  # "ssh-rsa"
                          packet.MPINT,   # e
                          packet.MPINT    # n
                          )

RSA_PRIVATE_KEY_PAYLOAD = (packet.STRING,  # "ssh-rsa"
                           packet.MPINT,   # n
                           packet.MPINT,   # e
                           packet.MPINT,   # d
                           packet.MPINT,   # p
                           packet.MPINT,   # q
                           )

RSA_SIG_PAYLOAD = (packet.STRING,  # "ssh-rsa"
                   packet.STRING   # signature_key_blob
                   )

########NEW FILE########
__FILENAME__ = static_key_storage
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.keys.static_key_storage
#
# This module is a key storage type where they keys are retained in memory.
#

import os

import key_storage
import remote_host
import openssh_key_storage

class Static_Key_Storage(key_storage.SSH_Key_Storage):

    """Static_Key_Storage

    A key storage mechanism where all keys are maintained in memory.

    Note that this isn't terribly secure.
    """

    key_types = ('dsa', 'rsa')

    def __init__(self):
        # The key is the username, the value is the key object.
        self.public_key = {}
        self.private_key = {}
        # List of (hosts, (SSH_Public_Private_Key,...)) where hosts is a list of strings.
        # Does not support any fancy meta-matching, sorry.
        # Does not support user-specific known hosts.
        self.known_hosts = []

    def set_private_host_key(self, username, key_obj):
        self.private_key[username] = key_obj

    def set_public_host_key(self, username, key_obj):
        self.public_key[username] = key_obj

    def add_known_hosts(self, hosts, keystrings):
        """add_known_hosts(self, hosts, keystrings) -> None
        Add a known host.

        <hosts>: List of host strings.
        <keystrings>: List of strings of the host's public key.
                      Must be in OpenSSH's standard format.
        """
        key_storage = openssh_key_storage.OpenSSH_Key_Storage
        key_objs = []
        for keystring in keystrings:
            key_obj = key_storage.parse_public_key(keystring)
            key_objs.append(key_obj)
        # XXX: Could merge host entries.
        self.known_hosts.append((hosts, key_objs))

    def load_keys(self, username=None):
        if username not in self.public_key or username not in self.private_key:
            return None
        if username is None:
            username = os.getlogin()
        key_obj = self.private_key[username]
        public_key = self.public_key[username]
        key_obj.public_key = public_key.public_key
        return [key_obj]

    load_keys.__doc__ = key_storage.SSH_Key_Storage.load_keys.__doc__

    def load_private_keys(self, username=None):
        if username not in self.private_key:
            return []
        if username is None:
            username = os.getlogin()
        return [self.private_key[username]]

    load_private_keys.__doc__ = key_storage.SSH_Key_Storage.load_private_keys.__doc__

    def load_public_keys(self, username=None):
        if username not in self.public_key:
            return []
        if username is None:
            username = os.getlogin()
        return [self.public_key[username]]

    load_public_keys.__doc__ = key_storage.SSH_Key_Storage.load_public_keys.__doc__

    def verify(self, host_id, server_key_types, public_host_key, username=None):
        if username is None:
            username = os.getlogin()
        for key in server_key_types:
            if public_host_key.name == key.name:
                # This is a supported key type.
                if self._verify_contains(host_id, public_host_key, username):
                    return 1
        return 0

    verify.__doc__ = key_storage.SSH_Key_Storage.verify.__doc__

    def _verify_contains(self, host_id, public_host_key, username):
        __pychecker__ = 'unusednames=username'
        # Currently only supported IPv4
        if not isinstance(host_id, remote_host.IPv4_Remote_Host_ID):
            return 0
        for hosts, key_objs in self.known_hosts:
            for key_obj in key_objs:
                if key_obj.name == public_host_key.name:
                    for host in hosts:
                        if host == host_id.ip or host == host_id.hostname:
                            if key_obj.public_key == public_host_key.public_key:
                                return 1
        return 0

    def update_known_hosts(self, host, public_host_key, username=None):
        __pychecker__ = 'unusednames=username'
        self.known_hosts.append(([host], public_host_key))

    update_known_hosts.__doc__ = key_storage.SSH_Key_Storage.update_known_hosts.__doc__

########NEW FILE########
__FILENAME__ = diffie_hellman
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.key_exchange.diffie_hellman
#
# This module implements the Diffie Hellman Group 1 SHA1 key exchange.
#

import hashlib

from coro import write_stderr as W

from coro.ssh.util import packet as ssh_packet
from coro.ssh.util import debug as ssh_debug
from coro.ssh.util import random as ssh_random

from coro.ssh.key_exchange import SSH_Key_Exchange
from coro.ssh.transport import constants
from coro.ssh.keys import parse_public_key

from coro.ssh.util.mpint import pack_mpint

# 2**1024 - 2**960 - 1 + 2**64 * floor( 2**894 Pi + 129093 )
DH_PRIME = 179769313486231590770839156793787453197860296048756011706444423684197180216158519368947833795864925541502180565485980503646440548199239100050792877003355816639229553136239076508735759914822574862575007425302077447712589550957937778424442426617334727629299387668709205606050270810842907692932019128194467627007L  # noqa
DH_GENERATOR = 2L

SSH_MSG_KEXDH_INIT      = 30
SSH_MSG_KEXDH_REPLY     = 31

def hexdump(src, length=16):
    FILTER = ''.join([(len(repr(chr(x))) == 3) and chr(x) or '.' for x in range(256)])
    lines = []
    for c in xrange(0, len(src), length):
        chars = src[c:c + length]
        hex = ' '.join(["%02x" % ord(x) for x in chars])
        printable = ''.join(["%s" % ((ord(x) <= 127 and FILTER[ord(x)]) or '.') for x in chars])
        lines.append("%04x  %-*s  %s\n" % (c, length * 3, hex, printable))
    return ''.join(lines)

class Diffie_Hellman_Group1_SHA1(SSH_Key_Exchange):

    name = 'diffie-hellman-group1-sha1'

    # What type of host key features this kex algorithm wants.
    wants_signature_host_key = 1
    wants_encryption_host_key = 0

    client_random_value = ''    # x
    client_exchange_value = 0L  # e
    server_public_host_key = None  # k_s

    server_random_value = ''    # y
    server_exchange_value = 0L  # f

    def get_initial_client_kex_packet(self):
        self.transport.debug.write(ssh_debug.DEBUG_3, 'get_initial_kex_packet()')
        # Send initial key.
        # This is x.
        self.client_random_value = ssh_random.get_random_number(512)
        # p is large safe prime (DH_PRIME)
        # g is a generator for a subgroup of GF(p) (DH_GENERATOR)
        # compute e=g**x mod p
        self.client_exchange_value = pow(DH_GENERATOR, self.client_random_value, DH_PRIME)
        return ssh_packet.pack_payload(KEXDH_INIT_PAYLOAD,
                                       (SSH_MSG_KEXDH_INIT,
                                        self.client_exchange_value)
                                       )

    def get_initial_server_kex_packet(self):
        return None

    def _get_hash_object(self):
        """_get_hash_object(self) -> hash_object
        Return a raw hash object (see get_hash_object).
        """
        return hashlib.sha1()

    def register_client_callbacks(self):
        callbacks = {SSH_MSG_KEXDH_REPLY: self.msg_kexdh_reply}
        self.transport.register_callbacks(self.name, callbacks)

    def register_server_callbacks(self):
        callbacks = {SSH_MSG_KEXDH_INIT: self.msg_kexdh_init}
        self.transport.register_callbacks(self.name, callbacks)

    def msg_kexdh_init (self, packet):
        # mpint     e
        msg, self.client_exchange_value = ssh_packet.unpack_payload (KEXDH_INIT_PAYLOAD, packet)
        # XXX make sure e is a valid number
        # This is y.
        self.server_random_value = ssh_random.get_random_number(512)
        # p is large safe prime (DH_PRIME)
        # g is a generator for a subgroup of GF(p) (DH_GENERATOR)
        # compute f=g**y mod p
        self.server_exchange_value = pow(DH_GENERATOR, self.server_random_value, DH_PRIME)
        self.shared_secret = pow (self.client_exchange_value, self.server_random_value, DH_PRIME)
        K_S = self.transport.server_key.get_public_key_blob()
        payload_inputs = (
            self.c2s_version_string,
            self.s2c_version_string,
            self.c2s_kexinit_packet,
            self.s2c_kexinit_packet,
            K_S,
            self.client_exchange_value,
            self.server_exchange_value,
            self.shared_secret
        )
        H = ssh_packet.pack_payload (KEXDH_HASH_PAYLOAD, payload_inputs)
        self.exchange_hash = hashlib.sha1(H).digest()
        if self.session_id is None:
            # The session id is the first exchange hash.
            self.session_id = self.exchange_hash
        H_sig = self.transport.server_key.sign (self.exchange_hash)
        packet = ssh_packet.pack_payload (
            KEXDH_REPLY_PAYLOAD, (
                SSH_MSG_KEXDH_REPLY,
                K_S,
                self.server_exchange_value,
                H_sig
            )
        )
        self.transport.send_packet (packet)

    def msg_kexdh_reply(self, packet):
        # string    server public host key and certificates (K_S)
        # mpint     f
        # string    signature of H
        msg, public_host_key, server_exchange_value, signature_of_h = ssh_packet.unpack_payload(
            KEXDH_REPLY_PAYLOAD, packet)

        # Create a SSH_Public_Private_Key instance from the packed string.
        self.server_public_host_key = parse_public_key(public_host_key)

        # Verify that this is a known host key.
        self.transport.verify_public_host_key(self.server_public_host_key)

        # Make sure f is a valid number
        if server_exchange_value <= 1 or server_exchange_value >= DH_PRIME - 1:
            self.transport.send_disconnect(
                constants.SSH_DISCONNECT_KEY_EXCHANGE_FAILED,
                'Key exchange did not succeed: Server exchange value not valid.')

        # K = f**x mod p
        self.shared_secret = pow(server_exchange_value, self.client_random_value, DH_PRIME)
        # Verify hash.
        # string    V_C, the client's version string (CR and NL excluded)
        # string    V_S, the server's version string (CR and NL excluded)
        # string    I_C, the payload of the client's SSH_MSG_KEXINIT
        # string    I_S, the payload of the server's SSH_MSG_KEXINIT
        # string    K_S, the host key
        # mpint     e, exchange value sent by the client
        # mpint     f, exchange value sent by the server
        # mpint     K, the shared secret
        H = ssh_packet.pack_payload(KEXDH_HASH_PAYLOAD,
                                    (self.c2s_version_string,
                                     self.s2c_version_string,
                                     self.c2s_kexinit_packet,
                                     self.s2c_kexinit_packet,
                                     public_host_key,
                                     self.client_exchange_value,
                                     server_exchange_value,
                                     self.shared_secret))
        # Double check that the signature from the server matches our signature.
        hash = hashlib.sha1(H)
        self.exchange_hash = hash.digest()
        if self.session_id is None:
            # The session id is the first exchange hash.
            self.session_id = self.exchange_hash

        if not self.server_public_host_key.verify(self.exchange_hash, signature_of_h):
            self.transport.send_disconnect(
                constants.SSH_DISCONNECT_KEY_EXCHANGE_FAILED, 'Key exchange did not succeed:  Signature did not match.')

        # Finished...
        # self.transport.send_newkeys()

KEXDH_REPLY_PAYLOAD = (ssh_packet.BYTE,
                       ssh_packet.STRING,      # public host key and certificates (K_S)
                       ssh_packet.MPINT,       # f
                       ssh_packet.STRING       # signature of H
                       )

KEXDH_INIT_PAYLOAD = (ssh_packet.BYTE,
                      ssh_packet.MPINT    # e
                      )

KEXDH_HASH_PAYLOAD = (ssh_packet.STRING,  # V_C, the client's version string (CR and NL excluded)
                      ssh_packet.STRING,  # V_S, the server's version string (CR and NL excluded)
                      ssh_packet.STRING,  # I_C, the payload of the client's SSH_MSG_KEXINIT
                      ssh_packet.STRING,  # I_S, the payload of the server's SSH_MSG_KEXINIT
                      ssh_packet.STRING,  # K_S, the host key
                      ssh_packet.MPINT,  # e, exchange value sent by the client
                      ssh_packet.MPINT,  # f, exchange value sent by the server
                      ssh_packet.MPINT   # K, the shared secret
                      )

########NEW FILE########
__FILENAME__ = coro_socket_transport
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.l4_transport.coro_socket_transport
#
# Socket transport used by SSH.
#

import coro
from coro.dns.exceptions import DNS_Error
from coro.oserrors import raise_oserror
import errno
import socket
from coro.ssh import l4_transport
from coro.ssh.keys import remote_host

class coro_socket_transport(l4_transport.Transport):

    # The socket
    s = None
    # peer
    peer = None

    def __init__(self, ip=None, port=22, bind_ip=None, hostname=None, sock=None):
        self.ip = ip
        self.port = port
        self.bind_ip = bind_ip
        self.hostname = hostname
        if sock is None:
            if ':' in ip:
                self.s = coro.tcp6_sock()
            else:
                self.s = coro.tcp_sock()
        else:
            self.s = sock
            self.peer = self.s.getpeername()

    def connect(self):
        if self.bind_ip is not None:
            self.s.bind((self.bind_ip, 0))
        if '%' in self.ip:
            # link local address, need 4-tuple
            ai = socket.getaddrinfo (self.ip, self.port)
            address = ai[0][4]
            ip, port, flowinfo, scope_id = address
            ip, intf = ip.split ('%')
            self.s.connect ((ip, port, flowinfo, scope_id))
        else:
            self.s.connect((self.ip, self.port))

    def read(self, bytes):
        # XXX: This could be made more efficient.
        count = bytes
        result = []
        while count > 0:
            try:
                chunk = self.s.recv(count)
            except OSError, why:
                if why.errno == errno.EBADF:
                    raise EOFError
                else:
                    raise
            except coro.ClosedError:
                raise EOFError
            if len(chunk) == 0:
                raise EOFError
            count -= len(chunk)
            result.append(chunk)
        return ''.join(result)

    def write(self, bytes):
        try:
            return self.s.send(bytes)
        except OSError, why:
            if why.errno == errno.EBADF:
                raise_oserror(errno.EPIPE)
            else:
                raise
        except coro.ClosedError:
            raise_oserror(errno.EPIPE)

    def read_line(self):
        # XXX: This should be made more efficient with buffering.
        # However, the complexity and overhead of adding buffering just
        # to support reading the line at the beginning of the protocol
        # negotiation seems kinda silly.
        result = []
        while 1:
            try:
                it = self.s.recv(1)
            except OSError, why:
                if why.errno == errno.EBADF:
                    raise EOFError
                else:
                    raise
            except coro.ClosedError:
                raise EOFError
            if not it:
                raise EOFError

            if it == '\r':
                # This is a part of CR LF line ending.  Skip it.
                pass

            elif it == '\n':
                break

            else:
                result.append(it)

        return ''.join(result)

    def close(self):
        self.s.close()

    def get_hostname(self):
        if self.hostname is None:
            resolver = coro.get_resolver()
            try:
                in_addr = to_in_addr_arpa (self.ip)
                self.hostname = resolver.cache.query (in_addr, 'PTR')[0][1]
            except (DNS_Error, IndexError):
                # XXX: Log debug message.
                pass
        return self.hostname

    def get_host_id(self):
        return remote_host.IPv4_Remote_Host_ID(self.ip, self.get_hostname())

    def get_port(self):
        return self.port

# obviously ipv4 only
def to_in_addr_arpa (ip):
    octets = ip.split ('.')
    octets.reverse()
    return '%s.in-addr.arpa' % ('.'.join (octets))

########NEW FILE########
__FILENAME__ = hmac
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_hmac
#
# This implements the base HMAC hashing algorithm from RFC 2104.
#

from coro.ssh.mac import SSH_MAC_Method
from coro.ssh.util import str_xor
import struct

class SSH_HMAC(SSH_MAC_Method):
    """SSH_HMac

    Base class of other HMAC algorithms.
    See RFC 2104.
    """

    def get_hash_object(self):
        raise NotImplementedError

    def set_key(self, key):
        if __debug__:
            # Insecure.
            assert len(key) >= self.digest_size
        if len(key) > self.block_size:
            # Key is too big.  Hash it and use the result as the key.
            # This really isn't necessary because we're certain that the key
            # is going to be the correct size.  However, I put it in here
            # for completeness with the HMAC spec.
            import sys
            sys.stderr.write('WARNING: Unexecpted HMAC key size!!!\n')
            h = self.get_hash_object()
            self.key = h.update(key).digest()
        else:
            self.key = key

        ipad = '\x36' * self.block_size
        opad = '\x5C' * self.block_size
        padded_key = self.key + '\0' * (self.block_size - len(self.key))

        self._enc_ipad = str_xor(padded_key, ipad)
        self._enc_opad = str_xor(padded_key, opad)

    def digest(self, sequence_number, data):
        sequence_number = struct.pack('>L', sequence_number)
        return self.hmac(sequence_number + data)

    def hmac(self, data):
        # H(K XOR opad, H(K XOR ipad, text))
        hash = self.get_hash_object()
        hash.update(self._enc_ipad)
        hash.update(data)
        b = hash.digest()
        hash = self.get_hash_object()
        hash.update(self._enc_opad)
        hash.update(b)
        return hash.digest()

########NEW FILE########
__FILENAME__ = hmac_md5
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_hmac_md5
#
# Implements the hmac-md5 SSH MAC algorithm.

from hmac import SSH_HMAC
import hashlib

class HMAC_MD5(SSH_HMAC):

    name = 'hmac-md5'
    block_size = 64
    digest_size = 16
    key_size = 16

    def get_hash_object(self):
        return hashlib.md5()

import unittest

class ssh_hmac_md5_test_case(unittest.TestCase):
    pass

class hmac_md5_test_case(ssh_hmac_md5_test_case):

    def runTest(self):
        # From RFC2104
        a = HMAC_MD5()
        a.set_key('\x0b' * 16)
        self.assertEqual(a.hmac('Hi There'), '\x92\x94\x72\x7a\x36\x38\xbb\x1c\x13\xf4\x8e\xf8\x15\x8b\xfc\x9d')

        a = HMAC_MD5()
        a.set_key('Jefe' + '\0' * 12)
        self.assertEqual(a.hmac('what do ya want for nothing?'),
                         '\x75\x0c\x78\x3e\x6a\xb0\xb5\x03\xea\xa8\x6e\x31\x0a\x5d\xb7\x38')

        a = HMAC_MD5()
        a.set_key('\xAA' * 16)
        self.assertEqual(a.hmac('\xDD' * 50), '\x56\xbe\x34\x52\x1d\x14\x4c\x88\xdb\xb8\xc7\x33\xf0\xe8\xb3\xf6')

def suite():
    suite = unittest.TestSuite()
    suite.addTest(hmac_md5_test_case())
    return suite

if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = hmac_sha1
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_hmac_sha1
#
# Implements the hmac-sha1 SSH MAC algorithm.

from hmac import SSH_HMAC
import hashlib

class HMAC_SHA1(SSH_HMAC):

    name = 'hmac-sha1'
    block_size = 64
    digest_size = 20
    key_size = 20

    def get_hash_object(self):
        return hashlib.sha1()

########NEW FILE########
__FILENAME__ = none
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.mac.none
#
# This is the 'none' mac that returns an empty value.
#

from coro.ssh.mac import SSH_MAC_Method

class MAC_None(SSH_MAC_Method):

    name = 'none'

    def digest(self, sequence_number, data):
        return ''

########NEW FILE########
__FILENAME__ = cli
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""CLI interface.

This is the interface for running SCP and a command-line process.
"""

import optparse
import sys

from coro.ssh.scp import client, core

usage = """\
scp [options] file1 file2
scp [options] file1 ... directory

Each file or directory argument is either:

    - A remote filename or directory in the format of
      ``username@hostname:pathname`` or ``hostname:pathname``.
    - A local filename or directory.  A local filename can not have any colons
      before a slash.

If more than one source file is specified, the destination must be a directory
that exists.

The following options are available:

-p      Preserve modification time, access time, and mode of the original file.
-r      If any source filename is a directory, then transfer the directory and
        all subfiles and directories to the remote host.  The destination must
        be a directory.
-v      Produce verbose output.  Up to 3 -v commands for more verbosity.

Note on file ownership and mode:  If the destination is a file, and that file
already exists, then the mode and owner for the destination file is preserved.
If the destination is a directory or a filename that does not exist, then the
file will be created with the ownership of the remote user and the mode of the
original file modified by the umask of the remote user.
"""

class CLI:

    def main(self, args=None):
        """The main entry point for the SCP cli program.

        Calls sys.exit when finished.

        :Parameters:
            - `args`: The command line arguments as a list of strings.  If
              None, will use sys.argv.
        """
        parser = optparse.OptionParser(usage=usage, add_help_option=False)
        parser.add_option('-p', dest='preserve', action='store_true')
        parser.add_option('-r', dest='recursive', action='store_true')
        parser.add_option('-v', dest='verbosity', action='count')
        parser.add_option('-t', dest='action_to', action='store_true')
        parser.add_option('-f', dest='action_from', action='store_true')
        parser.add_option('-d', dest='target_should_be_dir', action='store_true')
        parser.add_option('--help', dest='help', action='store_true')
        if args is None:
            args = sys.argv[1:]
        options, arguments = parser.parse_args(args)
        if options.help:
            print usage
            sys.exit(0)

        if options.action_from:
            scp = self._get_scp()
            scp.verbosity = options.verbosity
            scp.debug(core.DEBUG_EXTRA, 'options: %r', args)
            scp.read_response()
            scp.send(options.preserve,
                     options.recursive,
                     arguments
                     )
            sys.exit(int(scp.had_errors))

        elif options.action_to:
            scp = self._get_scp()
            scp.verbosity = options.verbosity
            scp.debug(core.DEBUG_EXTRA, 'options: %r', args)
            if len(arguments) != 1:
                scp.hard_error('Must specify 1 target.')
            scp.receive(options.preserve,
                        options.recursive,
                        options.target_should_be_dir,
                        arguments[0]
                        )
            sys.exit(int(scp.had_errors))
        else:
            print 'Function unavailable.'
            sys.exit(1)
            client = self._get_client()
            client.main(options.preserve,
                        options.recursive,
                        options.verbosity,
                        arguments
                        )

    def _get_scp(self):
        return core.SCP()

    def _get_client(self):
        return client.Client()

if __name__ == '__main__':
    cli = CLI()
    cli.main()

########NEW FILE########
__FILENAME__ = client
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""SCP client.

This is the client side of the SCP process.  It uses the SSH library to spawn
connections.
"""

from coro.ssh.scp.core import SCP

class Client(SCP):

    def main(self, preserve, recursive, verbose, pathnames):
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = core
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Core SCP code.

This implements the core code of the SCP program.
"""

import glob
import os
import stat
import sys
import time

DEBUG_NORMAL = 1
DEBUG_EXTRA  = 2
DEBUG_FULL   = 3

class RootEscapeError(Exception):

    """Raised when a user tries to access files outside of the root directory."""


class SCP:

    """Core SCP implementation.

    This implements the core transfer functions for SCP.  It does not involve
    any of the SSH code, so it is more similar to RCP.

    :IVariables:
        - `output`: A file-like object for sending data.
        - `input`: A file-like object for receiving data.
        - `verbosity`: Level of verbosity.
        - `had_errors`: Flag to keep track if there were any errors.
        - `blocksize`: Size of blocks to read and write.
        - `path_filter`: A function to filter pathnames to determine whether or
          not they are accessible.  It should take one parameter, the full
          pathname, and return True if it is ok, otherwise False. May be None
          to indicate no filter.
        - `filter_recursive_only`: If True, will only apply the filter for
          recursive sends.  The filter is always appled for receiving.
        - `root`: The root of the filesystem.  By setting this, you can
          transparently reset the root (similar to chroot).  Setting this will
          implicitly change the current working directory to this directory.
          May be None to indicate normal operation.
        - `root_slash`: Same as ``root`` with a trailing slash.
        - `shell_globbing`: If true, will do shell-style globbing on the file
          arguments when sending files.
    """

    output = None
    input = None
    had_errors = False
    verbosity = 0
    blocksize = 4096
    max_line_size = 4096
    root = None
    root_slash = None

    def __init__(self, input=None,
                 output=None,
                 path_filter=None,
                 filter_recursive_only=False,
                 root=None,
                 shell_globbing=False
                 ):
        if input:
            self.input = input
        else:
            self.input = sys.stdin
        if output:
            self.output = output
        else:
            self.output = sys.stdout
        self.path_filter = path_filter
        # Make sure there are no trailing slashes.
        if root:
            self.root = root.rstrip('/')
            self.root_slash = root + '/'
        self.filter_recursive_only = filter_recursive_only
        self.shell_globbing = shell_globbing

    def _get_relative_root(self, path):
        if self.root:
            if path.startswith('/'):
                path = path.lstrip('/')
            path = os.path.join(self.root, path)

        path = os.path.realpath(path)

        if self.root:
            if not path.startswith(self.root_slash) and path != self.root:
                # A symlink or .. broke us out of the root.
                # Do not allow this.
                raise RootEscapeError

        return path

    def _remove_root(self, path):
        if self.root:
            if path.startswith(self.root):
                path = path[len(self.root):]
        return path

    def _filter(self, path):
        if self.path_filter:
            return not self.path_filter(path)
        else:
            return False

    def receive(self, preserve, recursive, target_should_be_dir, target):
        """Receive file(s).

        This is the -t flag.

        :Parameters:
            - `preserve`: If true, attempts to preserve modification time,
              access time, and mode of the original file.
            - `recursive`: If true, will recursively send files.
            - `target_should_be_dir`: If true, the target should be a directory.
            - `target`: The target for the file(s).
        """
        if preserve:
            # Clear the mask so that the mode can be properly preserved.
            os.umask(0)
        if '\n' in target:
            self.soft_error('target (%r) contains a newline' % (target,))
            return

        try:
            relative_target = self._get_relative_root(target)
        except RootEscapeError:
            self.soft_error('%s: target not available.' % (target,))
            return

        if self._filter(relative_target):
            self.soft_error('%s: target not available.' % (target,))
            return

        target_is_dir = os.path.isdir(relative_target)
        if target_should_be_dir and not target_is_dir:
            self.soft_error('Expected target %s to be a directory.' % (target,))
            return

        self._respond_success()

        got_timestamp = False
        mtime_sec = mtime_usec = atime_sec = atime_usec = 0
        while 1:
            result = []
            if not self._read_line(result):
                return
            result = ''.join(result)
            self.debug(DEBUG_FULL, 'Executing command: %r', result)
            try:
                code = result[0]
            except IndexError:
                self.hard_error('Expected command character.')
            result = result[1:]
            if code == '\01':
                self._report_error(result[1:])
                self.had_errors = True
            elif code == '\02':
                self._report_error(result[1:])
                sys.exit(1)
            elif code == 'E':
                self._respond_success()
                return
            elif code == 'T':
                parts = result.split()
                if len(parts) != 4:
                    self.hard_error('Timestamp command format error (%r)' % (result,))
                try:
                    mtime_sec = int(parts[0])
                    mtime_usec = int(parts[1])
                    atime_sec = int(parts[2])
                    atime_usec = int(parts[3])
                except ValueError:
                    self.hard_error('Invalid timestamp value (%r)' % (result,))
                else:
                    got_timestamp = True
                    self._respond_success()
            elif code == 'C' or code == 'D':
                if len(result) < 8:
                    # Minimum for 4 bytes (mode), space, length (min 1 byte),
                    # space, name (min 1 byte).
                    self.hard_error('Command length too short (%r)' % (result,))
                try:
                    mode = int(result[:4], 8)
                except ValueError:
                    self.hard_error('Invalid file mode (%r)' % (result,))
                if result[4] != ' ':
                    self.hard_error('Command not properly delimited (%r)' % (result,))
                try:
                    end = result.index(' ', 5)
                except ValueError:
                    self.hard_error('Command not properly delimited (%r)' % (result,))
                try:
                    size = int(result[5:end])
                except ValueError:
                    self.hard_error('Invalid size (%r)' % (result,))
                filename = result[end + 1:]
                if not filename:
                    self.hard_error('Filename not specified (%r)' % (result,))
                if '/' in filename or filename == '..':
                    self.hard_error('Invalid filename (%r)' % (result,))
                if target_is_dir:
                    # Store the file in the target directory.
                    relative_pathname = os.path.join(target, filename)
                else:
                    # Store the file as the target.
                    relative_pathname = target
                try:
                    absolute_pathname = self._get_relative_root(relative_pathname)
                except RootEscapeError:
                    self.soft_error('%s: target not available.' % (relative_pathname,))
                    return

                if self._filter(absolute_pathname):
                    self.soft_error('%s: target not available.' % (relative_pathname,))
                    return

                self.debug(DEBUG_EXTRA, 'Receiving file %r mode %o size %i in dir %r' % (filename, mode, size, target))
                if code == 'D':
                    # Creating a directory.
                    if not recursive:
                        self.hard_error('received directory without -r')
                    if os.path.exists(absolute_pathname):
                        if not os.path.isdir(absolute_pathname):
                            self.soft_error('Target (%r) exists, but is not a directory.' % (relative_pathname,))
                            continue
                        if preserve:
                            try:
                                os.chmod(absolute_pathname, mode)
                            except OSError, e:
                                self.debug(
                                    DEBUG_NORMAL, 'Failed to chmod %r to %o: %s', relative_pathname, mode, e.strerror)
                                # Continue, this is not critical.
                    else:
                        try:
                            os.mkdir(absolute_pathname, mode)
                        except OSError, e:
                            self.soft_error('Unable to make directory (%r): %s' % (relative_pathname, e.strerror))
                            continue
                    self.receive(preserve, recursive, target_should_be_dir, relative_pathname)
                    if got_timestamp:
                        got_timestamp = False
                        timestamp = (mtime_sec, atime_sec)
                        try:
                            os.utime(absolute_pathname, timestamp)
                        except OSError, e:
                            self.soft_error('Failed to set timestamp (%r) on %r: %s' %
                                            (timestamp, relative_pathname, e.strerror))
                            continue
                else:
                    # code == 'C'
                    # Creating a file.
                    try:
                        fd = os.open(absolute_pathname, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, mode)
                    except OSError, e:
                        self.soft_error('Failed to create %r: %s' % (relative_pathname, e.strerror))
                        continue
                    try:
                        self.debug(DEBUG_FULL, 'Send response before start.')
                        self._respond_success()
                        bytes_left = size
                        error = None
                        while bytes_left > 0:
                            block = self.input.read(min(bytes_left, self.blocksize))
                            if not block:
                                self.hard_error('End of file, but expected more data while ready %r.' %
                                                (relative_pathname,))
                            bytes_left -= len(block)
                            if not error:
                                try:
                                    os.write(fd, block)
                                except OSError, e:
                                    error = e
                    finally:
                        os.close(fd)
                    self.read_response()
                    if got_timestamp:
                        got_timestamp = False
                        timestamp = (atime_sec, mtime_sec)
                        self.debug(DEBUG_EXTRA, 'Setting timestamp of %r to %r.', relative_pathname, timestamp)
                        try:
                            os.utime(absolute_pathname, timestamp)
                        except OSError, e:
                            self.soft_error('Failed to set timestamp (%r) on %r: %s' %
                                            (timestamp, relative_pathname, e.strerror))
                            continue
                    if error:
                        self.soft_error('Error while writing %r: %s' % (relative_pathname, error.strerror))
                    else:
                        self._respond_success()

            else:
                # Unknown command.
                self.hard_error('Invalid command: %r' % (result,))

    def send(self, preserve, recursive, pathnames):
        """Send file(s).

        This is the -f flag.  Make sure you call read_response before calling
        this the first time.

        :Parameters:
            - `preserve`: If true, attempts to preserve modification time,
              access time, and mode of the original file.
            - `recursive`: If true, will recursively send files.
            - `pathnames`: List of pathnames to send.
        """
        for relative_pathname in pathnames:
            if '\n' in relative_pathname:
                self.soft_error('skipping, filename (%r) contains a newline' % (relative_pathname,))
                continue
            # Remove any trailing slashes.
            relative_pathname = relative_pathname.rstrip('/')
            try:
                absolute_pathname = self._get_relative_root(relative_pathname)
            except RootEscapeError:
                self.soft_error('%s: Invalid pathname.' % (relative_pathname,))
                return

            # Potentially do shell expansion.
            if self.shell_globbing:
                more_absolute_pathnames = glob.glob(absolute_pathname)
                if not more_absolute_pathnames:
                    self.soft_error('%s: No such file or directory.' % (relative_pathname,))
                    continue
            else:
                more_absolute_pathnames = [absolute_pathname]

            for more_absolute_pathname in more_absolute_pathnames:
                more_relative_pathname = self._remove_root(more_absolute_pathname)
                # If the original pathname provided was relative, leave the
                # globbed versions as relative.
                if not relative_pathname.startswith('/'):
                    more_relative_pathname = more_relative_pathname.lstrip('/')
                self._send(preserve,
                           recursive,
                           more_relative_pathname,
                           more_absolute_pathname
                           )

    def _send(self, preserve, recursive, relative_pathname, absolute_pathname):

        if not self.filter_recursive_only and self._filter(absolute_pathname):
            self.soft_error('%s: No such file or directory' % (relative_pathname,))
            return

        try:
            st = os.stat(absolute_pathname)
        except OSError, e:
            self.soft_error('%s: %s' % (relative_pathname, e.strerror))
            return
        if stat.S_ISDIR(st.st_mode):
            if self.filter_recursive_only and self._filter(absolute_pathname):
                self.soft_error('%s: No such file or directory' % (relative_pathname,))
                return

            if recursive:
                self._send_recursive(preserve, relative_pathname, absolute_pathname, st)
            else:
                self.soft_error('%s: skipping, is a directory and -r not specified' % (relative_pathname,))
        elif stat.S_ISREG(st.st_mode):
            self._send_file(preserve, relative_pathname, absolute_pathname, st)
        else:
            self.soft_error('%s: skipping, not a regular file' % (relative_pathname,))

    def _send_file(self, preserve, relative_pathname, absolute_pathname, st):
        # pathname should already have been filtered.
        if preserve:
            if not self._send_preserve_timestamp(st):
                return
        base = os.path.basename(relative_pathname)
        self.debug(DEBUG_NORMAL, 'Sending file %r', relative_pathname)
        self.output.write('C%04o %i %s\n' % (stat.S_IMODE(st.st_mode),
                                             st.st_size,
                                             base
                                             )
                          )
        self.output.flush()
        if not self.read_response():
            return
        try:
            fd = os.open(absolute_pathname, os.O_RDONLY)
        except OSError, e:
            self.soft_error('%s: %s' % (relative_pathname, e.strerror))
            return
        try:
            bytes_left = st.st_size
            error = None
            while bytes_left > 0:
                if bytes_left < self.blocksize:
                    to_read = bytes_left
                else:
                    to_read = self.blocksize
                block = os.read(fd, to_read)
                if not block:
                    error = '%s: File shrunk while reading.' % (relative_pathname,)
                    # Keep writing to stay in sync.
                    dummy_data = '\0' * self.blocksize
                    while bytes_left > 0:
                        if bytes_left < len(dummy_data):
                            dummy_data = dummy_data[:bytes_left]
                        self.output.write(dummy_data)
                        bytes_left -= len(dummy_data)
                    break
                self.output.write(block)
                bytes_left -= len(block)
        finally:
            os.close(fd)
        self.output.flush()
        if error:
            self.soft_error(error)
        else:
            self.debug(DEBUG_FULL, 'File send complete, sending success, waiting for response.')
            self._respond_success()
        self.read_response()

    def _send_recursive(self, preserve, relative_pathname, absolute_pathname, st):
        self.debug(DEBUG_EXTRA, 'Send recursive %r', relative_pathname)
        try:
            files = os.listdir(absolute_pathname)
        except OSError, e:
            self.soft_error('%s: %s' % (relative_pathname, e.strerror))
            return
        if preserve:
            if not self._send_preserve_timestamp(st):
                return
        base = os.path.basename(relative_pathname)

        self.debug(DEBUG_NORMAL, 'Sending directory %r', relative_pathname)
        self.output.write('D%04o %i %s\n' % (stat.S_IMODE(st.st_mode),
                                             0,
                                             base
                                             )
                          )
        self.output.flush()
        if not self.read_response():
            return
        for filename in files:
            recursive_relative_pathname = os.path.join(relative_pathname, filename)
            try:
                recursive_absolute_pathname = self._get_relative_root(recursive_relative_pathname)
            except RootEscapeError:
                continue

            if self._filter(recursive_absolute_pathname):
                continue
            self.send(preserve, True, [recursive_relative_pathname])

        self.debug(DEBUG_FULL, 'Sending end of transmission.')
        self.output.write('E\n')
        self.output.flush()
        self.read_response()
        self.debug(DEBUG_FULL, 'End of transmission response read.')

    def _send_preserve_timestamp(self, st):
        self.debug(DEBUG_EXTRA, 'Sending preserve timestamp %i %i', st.st_mtime, st.st_atime)
        self.output.write('T%i 0 %i 0\n' % (st.st_mtime, st.st_atime))
        self.output.flush()
        if self.read_response():
            return True
        else:
            return False

    def _respond_success(self):
        self.output.write('\0')
        self.output.flush()

    def soft_error(self, error):
        """Sends a soft-error response.

        :Parameters:
            - `error`: The error string to send.  This should not have any
              newline characters.
        """
        self.debug(DEBUG_NORMAL, 'Soft error: %s' % (error,))
        self.output.write('\01scp: %s\n' % (error,))
        self.output.flush()
        self._report_error(error)
        self.had_errors = True

    def hard_error(self, error):
        """Sends a hard-error response.

        This function calls sys.exit.

        :Parameters:
            - `error`: The error string to send.  This should not have any
              newline characters.
        """
        self.debug(DEBUG_NORMAL, 'Hard error: %s' % (error,))
        self.output.write('\02scp: %s\n' % (error,))
        self.output.flush()
        self._report_error(error)
        sys.exit(1)

    def read_response(self):
        """Read a response.

        This function calls sys.exit on fatal errors.

        :Return:
            Returns True on a successful response, False otherwise.
        """
        code = self.input.read(1)
        self.debug(DEBUG_FULL, 'Got response %r' % (code,))
        if code == '\0':
            return True
        else:
            if code == '\1' or code == '\2':
                result = []
            else:
                result = [code]
            self._read_line(result)
            result = ''.join(result)
            self._report_error(result)
            self.had_errors = True
            if code == '\1':
                return False
            else:
                sys.exit(1)

    def _read_line(self, result):
        for unused in xrange(self.max_line_size):
            char = self.input.read(1)
            if not char:
                if not result:
                    return False
                else:
                    self.hard_error('End of line, expected \\n')
            if char == '\n':
                break
            result.append(char)
        else:
            # Never saw a \n.
            self.hard_error('Command line too long (%i)' % (self.max_line_size,))
        return True

    def _report_error(self, message):
        """Report an error message.

        By default, this does nothing (since the base code is intended for use
        on the server side). The client side should send the message to stderr
        or its equivalent.

        :Parameters:
            - `message`: The message to print (should not have a newline).
        """
        pass

    def debug(self, level, format, *args):
        """Send a debug message.

        :Parameters:
            - `level`: The level of the message.  May be DEBUG_NORMAL,
              DEBUG_EXTRA, or DEBUG_FULL.
            - `format`: The string to write.  Will be applied with the Python
              format operator with the rest of the arguments.
        """
        if level <= self.verbosity:
            msg = format % args
            print >>sys.stderr, '%s %i:%s' % (time.ctime(), level, msg)

########NEW FILE########
__FILENAME__ = test_coro_client
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
# NOTE: THIS DOES NOT WORK.
# There is a problem with reading from stdin via kqueue.
# I'm not sure (yet) what exactly is wrong.
#
# SMR 2013: seems to kinda work on OSX

import coro.ssh.transport.client
import coro.ssh.connection.connect
import coro.ssh.l4_transport.coro_socket_transport
import coro.ssh.auth.userauth
import coro.ssh.connection.interactive_session
import getopt
import sys
import termios
import fcntl
import os
import coro.ssh.util.debug
import socket
import coro

def usage():
    print 'test_coro_client [-l login_name] [-p port] hostname | user@hostname'

def is_ip (s):
    if s.count ('%') == 1:
        s, intf = s.split('%')
    try:
        socket.inet_pton (coro.AF.INET6, s)
        return True
    except socket.error:
        try:
            socket.inet_pton (coro.AF.INET, s)
            return True
        except:
            return False

oldterm = None
oldflags = None

def set_stdin_unbuffered():
    global oldterm, oldflags

    oldterm = termios.tcgetattr(0)
    newattr = termios.tcgetattr(0)
    newattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO
    termios.tcsetattr(0, termios.TCSANOW, newattr)

    oldflags = fcntl.fcntl(0, fcntl.F_GETFL)
    fcntl.fcntl(0, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)

def input_thread(channel):
    stdin = coro.fd_sock(0)
    while 1:
        data = stdin.recv(100)
        channel.send(data)

def transport_thread(channel):
    stdout = coro.fd_sock (1)
    while not channel.eof and not channel.closed:
        try:
            data = channel.read(1024)
            if data:
                stdout.send (data)
                # os.write(1, data)
        except EOFError:
            break
    coro.set_exit()

def doit (ip, port, username):
    if not is_ip (ip):
        ip = coro.get_resolver().resolve_ipv4 (ip)
    debug = coro.ssh.util.debug.Debug()
    debug.level = coro.ssh.util.debug.DEBUG_3
    client = coro.ssh.transport.client.SSH_Client_Transport(debug=debug)
    transport = coro.ssh.l4_transport.coro_socket_transport.coro_socket_transport(ip, port=port)
    client.connect(transport)
    auth_method = coro.ssh.auth.userauth.Userauth(client, username)
    service = coro.ssh.connection.connect.Connection_Service(client)
    client.authenticate(auth_method, service.name)
    channel = coro.ssh.connection.interactive_session.Interactive_Session_Client(service)
    channel.open()
    channel.open_pty()
    channel.open_shell()
    set_stdin_unbuffered()
    coro.spawn(input_thread, channel)
    coro.spawn(transport_thread, channel)

def main():

    login_username = None
    ip = None
    port = 22

    try:
        optlist, args = getopt.getopt(sys.argv[1:], 'l:p:')
    except getopt.GetoptError, why:
        print str(why)
        usage()
        sys.exit(1)

    for option, value in optlist:
        if option == '-l':
            login_username = value
        elif option == '-p':
            port = int (value)

    if len(args) != 1:
        usage()
        sys.exit(1)

    ip = args[0]
    if '@' in ip:
        login_username, ip = ip.split('@', 1)

    coro.spawn(doit, ip, port, login_username)
    try:
        coro.event_loop()
    finally:
        if oldterm:
            termios.tcsetattr(0, termios.TCSAFLUSH, oldterm)
            fcntl.fcntl(0, fcntl.F_SETFL, oldflags)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_coro_server
# -*- Mode: Python -*-

import coro.ssh.transport.server
import coro.ssh.connection.connect
import coro.ssh.l4_transport.coro_socket_transport
import coro.ssh.auth.userauth
import coro.ssh.connection.interactive_session
import getopt
import sys
import termios
import fcntl
import os
import coro.ssh.util.debug
import socket
import coro

from coro.ssh.keys.openssh_key_storage import OpenSSH_Key_Storage

W = coro.write_stderr

server_key_pri = """-----BEGIN DSA PRIVATE KEY-----
MIIBuwIBAAKBgQDTfwvvQo0WnUmZpnUFmqF/TXSXFaJ1NKbBLQXPh8dhHgTN1uFO
ZibFXMKpDHLCGCdGRm5eHansB9hu2+nNoaFf3oLDHc8ctuE7xRHT8x174D2AxcnX
r0Fw3BnZHj58lLlhayDJ4S6W77yefGEOuo/wKUEPjAUBCrvxKq3bKAeVUQIVAPpR
bJO1QQZPlj4w+MXmRTgW7wGfAoGAVUkBIX+RLrh9guyiAadi9xGk8S7n5w2PbcsP
KTG8x/ttCDEuaBp6El6qt86cA+M2GPvXjuMGR5BQT8IOaWS7Aw2+J1IamLCsrPfq
oiQvz3cqxOAutuIuorzbIAgVo0hiAyovZE4u2zzKeci7OtfD8pRThSby4Dgbkeix
FQFhW08CgYBSxcduHDSqJTCjFK4hwTlNck4h2hC1E4xuMfxYsUZkLrBAsD3nzU2W
jNoZppTz3W8XC7YnTxonncXNWxCWsDWpvs0b2zGj7uUvGRtlyxtQpybyN3LZ0flo
DssTygy7t0KlS7T2a1IhqiVDbrSUoGXz+Wp/z66lCpSLTlPsGpLeLwIVAMQldwwH
OekNfzzIBr6QkMvmIOuL
-----END DSA PRIVATE KEY-----
"""

user_key_pub = """ssh-dss AAAAB3NzaC1kc3MAAACBAPawYoOY758V46mBep5i3pRQSuXnmYLiwBWH06NMXfMKkncZE4eWIVVoDqZmeMfCSHP8uY2gS+QDfdMCGtqu9sX8noPx5SG6gzUnadhFKU2+o7tbJ9WkQX7TPHB2GLBk5SNn6MFfLlLwlLv+OFnO0jcBD81fkCZp19BoZt1CCMGLAAAAFQCCSKBZHEoXw7Y1jiT0GFuqGgPMaQAAAIB2EjHBcrMa6jvmNI1DLYEHrYlQ30cDvnYYIyunMsp6SybE1sLN2W3UqGLjqB2i3FgWh7o1yUVWdImBvFz4kdYVhlEcYUeTgu8IWH2YNFcr7/Q4IpF9h20pu/ASuR9aK/D8sA4s7JqVfkS/mIaOZ8W2aZiOSaqvJXQPee9tiKgLDAAAAIEA6jTTFwh0wBlLdzALSaxf+A4IPGwE3mlmVmzt+A+a+EqL2ZRmAZ2puQH3NKckqrAlHDY7gGuF5XlHUTiTbVanuv6vCRlPwHWCPNNZhYFqGLpMEqRNPV2cMlU0gaPn69DMZwbDNCJghZI6C2uejoh3agHvHq8jgm9q4e3X3nEjStc= rushing@beast.local\n"""  # noqa

ks = OpenSSH_Key_Storage()
server_key_ob = ks.parse_private_key (server_key_pri)
user_key_ob = ks.parse_public_key (user_key_pub)

def usage():
    print 'test_coro_server [-p port]'

def serve (port):
    s = coro.tcp_sock()
    s.bind (('', port))
    s.listen (5)
    while 1:
        conn, addr = s.accept()
        coro.spawn (go, conn, addr)

class echo_server (coro.ssh.connection.interactive_session.Interactive_Session_Server):
    def __init__ (self, connection_service):
        coro.ssh.connection.interactive_session.Interactive_Session_Server.__init__ (self, connection_service)
        coro.spawn (self.go)

    def go (self):
        self.send ('Welcome to the echo server.\n')
        while 1:
            try:
                block = self.read (1000)
                self.send (block)
            except EOFError:
                break
        self.close()

def go (conn, addr):
    debug = coro.ssh.util.debug.Debug()
    debug.level = coro.ssh.util.debug.DEBUG_3
    transport = coro.ssh.l4_transport.coro_socket_transport.coro_socket_transport(sock=conn)
    server = coro.ssh.transport.server.SSH_Server_Transport (server_key_ob, debug=debug)
    pubkey_auth = coro.ssh.auth.userauth.Public_Key_Authenticator ({'rushing': {'ssh-connection': [user_key_ob]}})
    pwd_auth = coro.ssh.auth.userauth.Password_Authenticator ({'foo': {'ssh-connection': 'bar'}})
    authenticator = coro.ssh.auth.userauth.Authenticator (server, [pubkey_auth, pwd_auth])
    server.connect (transport, authenticator)
    service = coro.ssh.connection.connect.Connection_Service (server, echo_server)

def main():

    login_username = None
    ip = None
    port = 22

    try:
        optlist, args = getopt.getopt(sys.argv[1:], 'p:')
    except getopt.GetoptError, why:
        print str(why)
        usage()
        sys.exit(1)

    for option, value in optlist:
        if option == '-p':
            port = int (value)

    coro.spawn (serve, port)
    coro.event_loop()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_ssh_transport
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# test_ssh_transport
#
# Test routines for ssh_transport.
#
# Broken into a separate file because it's quite large.
#

import coro.ssh.l4_transport
import coro.ssh.util.packet
import coro.ssh.transport
import coro.ssh.key_exchange
import coro.ssh.transport.transport
import coro.ssh.transport.client
from coro.ssh.keys.public_private_key import SSH_Public_Private_Key
from coro.ssh.transport.transport import One_Way_SSH_Transport
from coro.ssh.transport.constants import *

import unittest

class ssh_transport_test_case(unittest.TestCase):
    pass

class Null_Transport(coro.ssh.l4_transport.Transport):

    def connect(self):
        return

    def read(self, bytes):
        return ''

    def write(self, data):
        return len(data)

    def read_line(self):
        return ''

    def close(self):
        return

    def get_host_id(self):
        return None

class kexinit_test_case(ssh_transport_test_case):

    def runTest(self):
        # Simple Test

        # Can't instantiate SSH_Transport directly.
        a = coro.ssh.transport.client.SSH_Client_Transport()
        a.transport = Null_Transport()
        # Prepare kexinit packet.
        a._send_kexinit()
        # Create a fake packet.
        fake_kexinit_packet = coro.ssh.util.packet.pack_payload(
            coro.ssh.util.packet.PAYLOAD_MSG_KEXINIT, (
                SSH_MSG_KEXINIT,
                'A' * 16,  # cookie
                ['diffie-hellman-group1-sha1'],
                ['ssh-dss'],
                ['3des-cbc'],
                ['3des-cbc'],
                ['hmac-sha1'],
                ['hmac-sha1'],
                ['none'],
                ['none'],
                [],
                [],
                0,  # first_kex_packet_follows
                0   # reserved
            )
        )
        a.msg_kexinit(fake_kexinit_packet)
        # Set preferred algorithms.
        a.send_newkeys()
        # Make sure algorithms were set properly.
        self.assertEqual(a.key_exchange.name, 'diffie-hellman-group1-sha1')
        self.assertEqual(a.server_key.name, 'ssh-dss')
        self.assertEqual(a.c2s.cipher.name, '3des-cbc')
        self.assertEqual(a.s2c.cipher.name, '3des-cbc')
        self.assertEqual(a.c2s.mac.name, 'hmac-sha1')
        self.assertEqual(a.s2c.mac.name, 'hmac-sha1')
        self.assertEqual(a.c2s.compression.name, 'none')
        self.assertEqual(a.s2c.compression.name, 'none')

        # Complex Test 1
        a = ssh_client.SSH_Client_Transport()
        a.transport = Null_Transport()
        # Prepare kexinit packet.
        a._send_kexinit()
        # Create a fake packet.
        fake_kexinit_packet = coro.ssh.util.packet.pack_payload(
            coro.ssh.util.packet.PAYLOAD_MSG_KEXINIT, (
                SSH_MSG_KEXINIT,
                'A' * 16,  # cookie
                ['foobar', 'diffie-hellman-group1-sha1'],
                ['fake-server-key-type', 'ssh-dss'],
                ['encrypt-this', '3des-cbc'],
                ['xor', '3des-cbc'],
                ['none', 'hmac-sha1'],
                ['mac-daddy', 'hmac-sha1'],
                ['zzz', 'none'],
                ['aaa', 'none'],
                ['pig-latin'],
                [],
                0,  # first_kex_packet_follows
                0   # reserved
            )
        )
        a.msg_kexinit(fake_kexinit_packet)
        # Set preferred algorithms.
        a.send_newkeys()
        # Make sure algorithms were set properly.
        self.assertEqual(a.key_exchange.name, 'diffie-hellman-group1-sha1')
        self.assertEqual(a.server_key.name, 'ssh-dss')
        self.assertEqual(a.c2s.cipher.name, '3des-cbc')
        self.assertEqual(a.s2c.cipher.name, '3des-cbc')
        self.assertEqual(a.c2s.mac.name, 'hmac-sha1')
        self.assertEqual(a.s2c.mac.name, 'hmac-sha1')
        self.assertEqual(a.c2s.compression.name, 'none')
        self.assertEqual(a.s2c.compression.name, 'none')

        # Complex Test 2
        class funky_one_way(One_Way_SSH_Transport):
            def __init__(self, transport):
                self.supported_macs.reverse()
                One_Way_SSH_Transport.__init__(self, transport)
        a = ssh_client.SSH_Client_Transport(client_transport=funky_one_way())
        a.transport = Null_Transport()
        # Prepare kexinit packet.
        a._send_kexinit()
        # Create a fake packet.
        fake_kexinit_packet = coro.ssh.util.packet.pack_payload(
            coro.ssh.util.packet.PAYLOAD_MSG_KEXINIT, (
                SSH_MSG_KEXINIT,
                'A' * 16,  # cookie
                ['foobar', 'diffie-hellman-group1-sha1'],
                ['fake-server-key-type', 'ssh-dss'],
                ['encrypt-this', '3des-cbc'],
                ['xor', '3des-cbc'],
                ['hmac-sha1', 'none'],
                ['mac-daddy', 'hmac-sha1'],
                ['zzz', 'none'],
                ['aaa', 'none'],
                ['pig-latin'],
                [],
                0,  # first_kex_packet_follows
                0   # reserved
            )
        )
        a.msg_kexinit(fake_kexinit_packet)
        # Set preferred algorithms.
        a.send_newkeys()
        # Make sure algorithms were set properly.
        self.assertEqual(a.key_exchange.name, 'diffie-hellman-group1-sha1')
        self.assertEqual(a.server_key.name, 'ssh-dss')
        self.assertEqual(a.c2s.cipher.name, '3des-cbc')
        self.assertEqual(a.s2c.cipher.name, '3des-cbc')
        self.assertEqual(a.c2s.mac.name, 'none')
        self.assertEqual(a.s2c.mac.name, 'hmac-sha1')
        self.assertEqual(a.c2s.compression.name, 'none')
        self.assertEqual(a.s2c.compression.name, 'none')

        # Mismatch Test1
        class bogus_ssh_server_key(SSH_Public_Private_Key):
            supports_signature = 0
            supports_encryption = 0
            name = 'bogus'

        class funky_one_way2(One_Way_SSH_Transport):
            supported_server_keys = [bogus_ssh_server_key]

        a = ssh_client.SSH_Client_Transport(client_transport=funky_one_way2())
        a.transport = Null_Transport()
        # Prepare kexinit packet.
        a._send_kexinit()
        # Create a fake packet.
        fake_kexinit_packet = coro.ssh.util.packet.pack_payload(
            coro.ssh.util.packet.PAYLOAD_MSG_KEXINIT, (
                SSH_MSG_KEXINIT,
                'A' * 16,  # cookie
                ['foobar', 'diffie-hellman-group1-sha1'],
                ['fake-server-key-type', 'ssh-dss'],
                ['encrypt-this', '3des-cbc'],
                ['xor', '3des-cbc'],
                ['hmac-sha1', 'none'],
                ['mac-daddy', 'hmac-sha1'],
                ['zzz', 'none'],
                ['aaa', 'none'],
                ['pig-latin'],
                [],
                0,  # first_kex_packet_follows
                0   # reserved
            )
        )
        self.assertRaises(coro.ssh.transport.SSH_Protocol_Error, a.msg_kexinit, fake_kexinit_packet)

        # Mismatch Test2
        self.test_matchup_kex_and_key(['one', 'two'],
                                      ['two', 'one'],
                                      ['a', 'b'],
                                      ['b', 'c'],
                                      {'one': {'wants_enc': 1,
                                               'wants_sig': 1},
                                       'two': {'wants_enc': 1,
                                               'wants_sig': 1},
                                       },
                                      {'a': {'enc': 0,
                                             'sig': 1},
                                       'b': {'enc': 1,
                                             'sig': 0},
                                       'c': {'enc': 1,
                                             'sig': 1}
                                       },
                                      1,    # Expects exception.
                                      None, None)

    def test_matchup_kex_and_key(self, c2s_kex_supported, s2c_kex_supported,
                                 c2s_key_supported, s2c_key_supported,
                                 kex_features, key_features,
                                 expected_exception,
                                 expected_kex,
                                 expected_key):
        import new
        c2s_kex = []
        for kex in c2s_kex_supported:
            f = new.classobj(kex, (coro.ssh.key_exchange.SSH_Key_Exchange,), {})
            f.name = kex
            f.wants_signature_host_key = kex_features[kex]['wants_sig']
            f.wants_encryption_host_key = kex_features[kex]['wants_enc']
            c2s_kex.append(f)
        s2c_kex = []
        for kex in s2c_kex_supported:
            f = new.classobj(kex, (coro.ssh.key_exchange.SSH_Key_Exchange,), {})
            f.name = kex
            f.wants_signature_host_key = kex_features[kex]['wants_sig']
            f.wants_encryption_host_key = kex_features[kex]['wants_enc']
            s2c_kex.append(f)
        c2s_key = []
        for key in c2s_key_supported:
            k = new.classobj(key, (SSH_Public_Private_Key,), {})
            k.name = key
            k.supports_signature = key_features[key]['sig']
            k.supports_encryption = key_features[key]['enc']
            c2s_key.append(k)
        s2c_key = []
        for key in s2c_key_supported:
            k = new.classobj(key, (SSH_Public_Private_Key,), {})
            k.name = key
            k.supports_signature = key_features[key]['sig']
            k.supports_encryption = key_features[key]['enc']
            s2c_key.append(k)

        class client_transport(One_Way_SSH_Transport):
            supported_key_exchanges = c2s_kex
            supported_server_keys = c2s_key

        class server_transport(One_Way_SSH_Transport):
            supported_key_exchanges = s2c_key
            supported_server_keys = s2c_key

        import ssh_client
        a = ssh_client.SSH_Client_Transport(client_transport=client_transport(), server_transport=server_transport())
        a.transport = Null_Transport()
        # Prepare kexinit packet.
        a._send_kexinit()
        # Create a fake packet.
        fake_kexinit_packet = coro.ssh.util.packet.pack_payload(
            coro.ssh.util.packet.PAYLOAD_MSG_KEXINIT, (
                SSH_MSG_KEXINIT,
                'A' * 16,  # cookie
                s2c_kex_supported,
                s2c_key_supported,
                ['3des-cbc'],
                ['3des-cbc'],
                ['hmac-sha1'],
                ['hmac-sha1'],
                ['none'],
                ['none'],
                [],
                [],
                0,  # first_kex_packet_follows
                0   # reserved
            )
        )
        if expected_exception:
            self.assertRaises(coro.ssh.transport.SSH_Protocol_Error, a.msg_kexinit, fake_kexinit_packet)
        else:
            a.msg_kexinit(fake_kexinit_packet)
            # Set preferred algorithms.
            a.send_newkeys()
            # Make sure algorithms were set properly.
            self.assertEqual(a.key_exchange.name, expected_kex)
            self.assertEqual(a.server_key.name, expected_key)

def suite():
    suite = unittest.TestSuite()
    suite.addTest(kexinit_test_case())
    return suite

if __name__ == '__main__':
    unittest.main(module='test_ssh_transport', defaultTest='suite')

########NEW FILE########
__FILENAME__ = client
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.transport.client
#
# This implements the client functionality of the SSH transport layer.
#

from coro.ssh.util import debug
from coro.ssh.util import packet as ssh_packet
from coro.ssh.transport import transport
from coro.ssh.keys import key_storage
from coro.ssh.transport.constants import *

class SSH_Client_Transport(transport.SSH_Transport):

    def __init__(self, client_transport=None, server_transport=None, debug=None):
        transport.SSH_Transport.__init__(self, client_transport, server_transport, debug)
        self.self2remote = self.c2s
        self.remote2self = self.s2c

    def connect(self, transport):
        """connect(self, transport) -> None
        Connect to the remote host.
        """
        try:
            self._connect(transport)
        except:
            # Any exception is fatal.
            self.disconnect()
            raise

    def _connect(self, transport):
        self.transport = transport
        transport.connect()

        # Send identification string.
        if self.c2s.comments:
            comments = ' ' + self.c2s.comments
        else:
            comments = ''
        self.c2s.version_string = 'SSH-' + self.c2s.protocol_version + '-' + self.c2s.software_version + comments
        transport.write(self.c2s.version_string + '\r\n')
        # Receive server's identification string.
        while 1:
            # We might receive lines before we get the version.  Just ignore
            # them.
            # Note that the SSH spec says that clients MUST be able to receive
            # a version string that does not end in CRLF.  It appears that not
            # even OpenSSH does this.  Plus, the spec does not indicate how to
            # determine the length of the identification string.  We're not
            # going to bother supporting that here.  It's for backwards
            # compatibility.
            line = transport.read_line()
            if line.startswith('SSH-'):
                # Got the identification string.
                self.s2c.version_string = line
                # See if there are any optional comments.
                i = line.find(' ')
                if i != -1:
                    self.s2c.comments = line[i + 1:]
                    line = line[:i]
                # Break up the identification string into its parts.
                parts = line.split('-')
                if len(parts) != 3:
                    self.send_disconnect(
                        transport.SSH_DISCONNECT_PROTOCOL_ERROR, 'server identification invalid: %r' % line)
                self.s2c.protocol_version = parts[1]
                self.s2c.software_version = parts[2]
                if self.s2c.protocol_version not in ('1.99', '2.0'):
                    self.send_disconnect(transport.SSH_DISCONNECT_PROTOCOL_VERSION_NOT_SUPPORTED,
                                         'protocol version not supported: %r' % self.s2c.protocol_version)
                break

        self.send_kexinit()

        if self.self2remote.proactive_kex:
            # Go ahead and send our kex packet with our preferred algorithm.
            # This will assume the server side supports the algorithm.
            self.c2s.set_preferred('key_exchange')
            self.c2s.set_preferred('server_key')
            self.set_key_exchange()
            packet = self.c2s.key_exchange.get_initial_client_kex_packet()
            self.send_packet(packet)

        # Start the receive thread.
        # This depends on coro behavior that the thread won't start until we go to sleep
        # (which will happen in _process_kexinit).
        self.start_receive_thread()
        # Receive server kexinit
        self._process_kexinit()
        self.debug.write(debug.DEBUG_3, 'key exchange: got kexinit')
        if not self.self2remote.proactive_kex:
            packet = self.key_exchange.get_initial_client_kex_packet()
            if packet:
                # It is possible for a key exchange algorithm to not have
                # an initial packet to send on the client side.
                self.send_packet(packet)
        # Let the key exchange finish.
        # XXX: Need to lock down to prevent any non-key exchange packets from being transferred.
        # Key exchange finished and SSH_MSG_NEWKEYS sent, wait for the remote side to send NEWKEYS.
        message_type, packet = self.receive_message((SSH_MSG_NEWKEYS,))
        self.msg_newkeys(packet)
        # XXX: Unlock key exchange lockdown for c2s.

    def authenticate(self, authentication_method, service_name):
        """authenticate(self, authentication_method, service) -> None
        Authenticate with the remote side.

        <authentication_method>:
        <service_name>: The name of the service that you want to use after
                        authenticating.  Typically 'ssh-connection'.
        """
        # Ask the remote side if it is OK to use this authentication service.
        self.debug.write(debug.DEBUG_3, 'authenticate: sending service request (%s)', (authentication_method.name,))
        service_request_packet = ssh_packet.pack_payload(ssh_packet.PAYLOAD_MSG_SERVICE_REQUEST,
                                                         (transport.SSH_MSG_SERVICE_REQUEST,
                                                          authentication_method.name))
        self.send_packet(service_request_packet)
        # Server will disconnect if it doesn't like our service request.
        self.debug.write(debug.DEBUG_3, 'authenticate: waiting for SERVICE_ACCEPT')
        message_type, packet = self.receive_message((transport.SSH_MSG_SERVICE_ACCEPT,))
        msg, accepted_service_name = ssh_packet.unpack_payload(ssh_packet.PAYLOAD_MSG_SERVICE_ACCEPT, packet)
        self.debug.write(debug.DEBUG_3, 'authenticate: got SERVICE_ACCEPT')
        if accepted_service_name != authentication_method.name:
            self.send_disconnect(transport.SSH_DISCONNECT_PROTOCOL_ERROR,
                                 'accepted service does not match requested service "%s"!="%s"' %
                                 (authentication_method.name, accepted_service_name))
        # This authetnication service is OK, try to authenticate.
        authentication_method.authenticate(service_name)

    def request_service(self, service_instance):
        """request_service(self, service_instance) -> None
        Requests to run this service over the transport.
        If the service is not available, then a disconnect will be sent.
        <service_instance> is a SSH_Service class instance.
        """
        # SSH_MSG_SERVICE_REQUEST
        pass

    def msg_service_request_response(self, packet):
        pass

    def verify_public_host_key(self, public_host_key, username=None):
        """verify_public_host_key(self, public_host_key, username=None) -> None
        Verifies that the given public host key is the correct public key for
        the current remote host.
        <public_host_key>: A SSH_Public_Private_Key instance.

        Raises Invalid_Server_Public_Host_Key exception if it does not match.
        """
        host_id = self.transport.get_host_id()
        port = self.transport.get_port()
        for storage in self.supported_key_storages:
            if storage.verify(host_id, self.c2s.supported_server_keys, public_host_key, username, port):
                return
        raise key_storage.Invalid_Server_Public_Host_Key(host_id, public_host_key)

########NEW FILE########
__FILENAME__ = constants
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.transport.constants
#
# Constants used in the transport modules.
#

SSH_MSG_DISCONNECT      = 1
SSH_MSG_IGNORE          = 2
SSH_MSG_UNIMPLEMENTED   = 3
SSH_MSG_DEBUG           = 4
SSH_MSG_SERVICE_REQUEST = 5
SSH_MSG_SERVICE_ACCEPT  = 6
SSH_MSG_KEXINIT         = 20
SSH_MSG_NEWKEYS         = 21

SSH_DISCONNECT_HOST_NOT_ALLOWED_TO_CONNECT    = 1
SSH_DISCONNECT_PROTOCOL_ERROR                 = 2
SSH_DISCONNECT_KEY_EXCHANGE_FAILED            = 3
SSH_DISCONNECT_RESERVED                       = 4
SSH_DISCONNECT_MAC_ERROR                      = 5
SSH_DISCONNECT_COMPRESSION_ERROR              = 6
SSH_DISCONNECT_SERVICE_NOT_AVAILABLE          = 7
SSH_DISCONNECT_PROTOCOL_VERSION_NOT_SUPPORTED = 8
SSH_DISCONNECT_HOST_KEY_NOT_VERIFIABLE        = 9
SSH_DISCONNECT_CONNECTION_LOST                = 10
SSH_DISCONNECT_BY_APPLICATION                 = 11
SSH_DISCONNECT_TOO_MANY_CONNECTIONS           = 12
SSH_DISCONNECT_AUTH_CANCELLED_BY_USER         = 13
SSH_DISCONNECT_NO_MORE_AUTH_METHODS_AVAILABLE = 14
SSH_DISCONNECT_ILLEGAL_USER_NAME              = 15

########NEW FILE########
__FILENAME__ = server
# -*- Mode: Python -*-

#
# ssh.transport.server
#
# This implements the server functionality of the SSH transport layer.
#

from coro.ssh.util import debug
from coro.ssh.util import packet as ssh_packet
from coro.ssh.transport import transport as ssh_transport
from coro.ssh.keys import key_storage
from coro.ssh.transport.constants import *

from coro.ssh.auth import userauth

from coro import write_stderr as W

class SSH_Server_Transport (ssh_transport.SSH_Transport):

    def __init__(self, server_key, client_transport=None, server_transport=None, debug=None):
        ssh_transport.SSH_Transport.__init__(self, client_transport, server_transport, debug)
        self.self2remote = self.s2c
        self.remote2self = self.c2s
        self.is_server = True
        # XXX should accept a list of keys...
        self.s2c.supported_server_keys = [server_key]

    def connect (self, transport, authenticator):
        """connect(self, transport) -> None
        Connect to the remote host.
        """
        try:
            self._connect(transport, authenticator)
        except:
            # Any exception is fatal.
            self.disconnect()
            raise

    def _connect(self, transport, authenticator):
        # transport is already connected
        # Send identification string.
        self.transport = transport
        if self.s2c.comments:
            comments = ' ' + self.s2c.comments
        else:
            comments = ''
        self.s2c.version_string = 'SSH-' + self.s2c.protocol_version + '-' + self.s2c.software_version + comments
        transport.write(self.s2c.version_string + '\r\n')
        # Receive client's identification string.
        while 1:
            line = transport.read_line()
            if line.startswith('SSH-'):
                # Got the identification string.
                self.c2s.version_string = line
                # See if there are any optional comments.
                i = line.find(' ')
                if i != -1:
                    self.c2s.comments = line[i + 1:]
                    line = line[:i]
                # Break up the identification string into its parts.
                parts = line.split('-')
                if len(parts) != 3:
                    self.send_disconnect (
                        ssh_transport.SSH_DISCONNECT_PROTOCOL_ERROR,
                        'server identification invalid: %r' % line
                    )
                self.c2s.protocol_version = parts[1]
                self.c2s.software_version = parts[2]
                if self.c2s.protocol_version not in ('1.99', '2.0'):
                    self.send_disconnect (
                        ssh_transport.SSH_DISCONNECT_PROTOCOL_VERSION_NOT_SUPPORTED,
                        'protocol version not supported: %r' % self.c2s.protocol_version
                    )
                break

        self.send_kexinit()
        self.s2c.set_preferred('key_exchange')
        self.s2c.set_preferred('server_key')

        if self.self2remote.proactive_kex:
            # Go ahead and send our kex packet with our preferred algorithm.
            # This will assume the client side supports the algorithm.
            self.s2c.set_preferred('key_exchange')
            self.s2c.set_preferred('server_key')
            self.set_key_exchange()
            self.debug.write(debug.DEBUG_3, 'key exchange: sending proactive server kex packet')
            packet = self.c2s.key_exchange.get_initial_server_kex_packet()
            self.send_packet(packet)

        self.start_receive_thread()
        # Receive server kexinit
        self._process_kexinit()
        self.debug.write(debug.DEBUG_3, 'key exchange: got kexinit')
        self.debug.write(debug.DEBUG_3, 'key exchange: self.server_key=%r' % (self.server_key,))
        if not self.self2remote.proactive_kex:
            self.debug.write(debug.DEBUG_3, 'key exchange: sending initial server kex packet')
            packet = self.key_exchange.get_initial_server_kex_packet()
            if packet:
                # It is possible for a key exchange algorithm to not have
                # an initial packet to send on the client side.
                self.send_packet(packet)

        message_type, packet = self.receive_message ((SSH_MSG_SERVICE_REQUEST,))
        msg, service_name = ssh_packet.unpack_payload (ssh_packet.PAYLOAD_MSG_SERVICE_REQUEST, packet)
        self.debug.write (debug.DEBUG_1, 'service_request: %r' % (service_name,))
        # XXX consider other possibilities
        if service_name != 'ssh-userauth':
            self.send_disconnect (SSH_DISCONNECT_SERVICE_NOT_AVAILABLE, "not today, zurg")
        else:
            self.send_packet (
                ssh_packet.pack_payload (
                    ssh_packet.PAYLOAD_MSG_SERVICE_ACCEPT, (
                        ssh_transport.SSH_MSG_SERVICE_ACCEPT,
                        'ssh-userauth',
                    )
                )
            )
            authenticator.authenticate (service_name)

########NEW FILE########
__FILENAME__ = transport
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_transport
#
# This module implements the SSH Transport layer.
# This is the lowest-level layer of the SSH Protocol.  It does NOT implement
# authentication or anything else.
#
# This implements the SSH2 protocol ONLY.
#

import coro
import struct
import sys

from coro.ssh.util import random, pick_from_list
from coro.ssh.util import debug as ssh_debug
from coro.ssh.util import packet as ssh_packet
from coro.ssh.transport import SSH_Protocol_Error

from coro.ssh.transport.constants import *
from coro.ssh.key_exchange.diffie_hellman import Diffie_Hellman_Group1_SHA1
from coro.ssh.keys.dss import SSH_DSS
from coro.ssh.keys.rsa import SSH_RSA
from coro.ssh.cipher.des3_cbc import Triple_DES_CBC
from coro.ssh.cipher.blowfish_cbc import Blowfish_CBC
from coro.ssh.mac.hmac_sha1 import HMAC_SHA1
from coro.ssh.mac.hmac_md5 import HMAC_MD5
from coro.ssh.mac.none import MAC_None
from coro.ssh.cipher.none import Cipher_None
from coro.ssh.compression.none import Compression_None
from coro.ssh.keys.openssh_key_storage import OpenSSH_Key_Storage

from coro import write_stderr as W

class SSH_Transport:

    # The low-level OS transport abstraction.
    transport = None

    # These are references to the in-use entry from one of the
    # supported_xxx lists.
    # These two values are not set until after we have received the remote
    # side's kexinit packet.  (Thus, you can not rely on them when making
    # a proactive guess before the kexinit packet has arrived.)
    key_exchange = None
    server_key = None

    # Set this to true if we failed to guess the correct kex algorithm.
    ignore_first_packet = False

    # Normally One_Way_SSH_Transport instances.
    # You can subclass this class to use a different
    # transport that supports different algorithms.
    c2s = None  # client to server ssh transport
    s2c = None  # server to client ssh transport

    # These are references to the appropriate c2s or s2c object.
    self2remote = None
    remote2self = None

    # List of key_storage instances.
    supported_key_storages = None

    # Boolean whether or not we are the server.
    is_server = False

    # Thread object for reading from the socket.
    _receive_thread = None

    # Flag to indicate whether or not this connection is closed.
    closed = True

    def __init__(self, client_transport=None, server_transport=None, debug=None):
        self.tmc = Thread_Message_Callbacks()
        self.send_mutex = coro.mutex()
        # This is the registry of modules that want to receive certain messages.
        # The key is the module name, the value is a dictionary of message number
        # to callback function.  The function takes 1 parameter (the packet).
        self.message_callback_registry = {}
        # This is a mapping of SSH message numbers to the function to call when
        # that message is received.  It is an optimized version computed from
        # message_callback_registry.
        self.message_callbacks = {}

        if debug is None:
            self.debug = ssh_debug.Debug()
        else:
            self.debug = debug
        if client_transport is None:
            self.c2s = One_Way_SSH_Transport(self)
        else:
            self.c2s = client_transport
        if server_transport is None:
            self.s2c = One_Way_SSH_Transport(self)
        else:
            self.s2c = server_transport
        self.supported_key_storages = [OpenSSH_Key_Storage()]
        # XXX who/what sets self.is_server?  can we use self.is_server
        #     to decide which callbacks to register?  Or should that be done
        #     by the subclass?
        self.register_callbacks('__base__',
                                {SSH_MSG_IGNORE: self.msg_ignore,
                                 SSH_MSG_DEBUG: self.msg_debug,
                                 SSH_MSG_DISCONNECT: self.msg_disconnect,
                                 SSH_MSG_UNIMPLEMENTED: self.msg_unimplemented,
                                 # SSH_MSG_KEXINIT:self.msg_kexinit,
                                 SSH_MSG_NEWKEYS: self.msg_newkeys,
                                 }
                                )

    def unregister_callbacks(self, module_name):
        """unregister_callbacks(self, module_name) -> None
        Remove the given module from the registry.
        """
        try:
            del self.message_callback_registry[module_name]
        except KeyError:
            pass
        self._recompile_callback_registry()

    def register_callbacks(self, module_name, callback_dict):
        """register_callbacks(self, module_name, callback_dict) -> None
        Registers the given module_name (a string) the given callbacks.
        <callback_dict> is a dictionary of message number to callback function.

        If there were callbacks previously registered under the same name,
        this will clear the previous values.

        If more than one module is listening for the same message numbers,
        it is not deterministic which one will receive the message.
        In other words, don't do that!
        Also, beware that some message numbers can be the same even if they
        are referenced with different names.  An example is
        SSH_MSG_KEX_DH_GEX_GROUP and SSH_MSG_KEXDH_REPLY both are the
        number 31.
        """
        self.debug.write(ssh_debug.DEBUG_3, 'register_callbacks(module_name=%s, callback_dict=...)', (module_name,))
        self.message_callback_registry[module_name] = callback_dict
        self._recompile_callback_registry()

    def _recompile_callback_registry(self):
        # Recompile message_callbacks
        self.message_callbacks = {}
        for dictn in self.message_callback_registry.values():
            self.message_callbacks.update(dictn)

    def disconnect(self):
        """disconnect(self) -> None
        Closes the connection.
        """
        if not self.closed:
            self.stop_receive_thread()
            self.closed = True

    # Make an alias.
    close = disconnect

    def send_disconnect(self, reason_code, description):
        """send_disconnect(self, reason_code, description) -> None
        """
        self.debug.write(
            ssh_debug.DEBUG_3, 'send_disconnect(reason_code=%r, description=%r)', (reason_code, description))
        # Language tag currently set to the empty string.
        language_tag = ''
        self.send_packet(
            ssh_packet.pack_payload (
                ssh_packet.PAYLOAD_MSG_DISCONNECT,
                (SSH_MSG_DISCONNECT,
                 reason_code,
                 description,
                 language_tag)
            )
        )
        self.disconnect()
        raise SSH_Protocol_Error(reason_code, description)

    def send (self, format, values):
        self.send_packet (ssh_packet.pack_payload (format, values))

    def send_packet(self, data):
        """send_packet(self, data) -> None
        Sends the given packet data.
        <data>: A string.
        """
        self.send_mutex.lock()
        try:
            try:
                self._send_packet(data)
            except:
                # Any error is fatal.
                self.disconnect()
                raise
        finally:
            self.send_mutex.unlock()

    def _send_packet(self, data):
        # Packet is:
        # uint32 packet_length
        # byte padding_length
        # byte[n1] payload
        # byte[n2] random padding
        # byte[m] MAC
        self.debug.write(ssh_debug.DEBUG_3, 'send_packet( len(data)=%i )', (len(data),))
        data = self.self2remote.compression.compress(data)

        # packet_len + padding_length + payload + random_padding
        # must be multiple of cipher block size.
        block_size = max(8, self.self2remote.cipher.block_size)
        padding_length = block_size - ((5 + len(data)) % block_size)
        if padding_length < 4:
            padding_length += block_size
        # Total packet size must also be at least 16 bytes.
        base_size = 5 + len(data) + padding_length
        minimum_size = max(16, block_size)
        if base_size < minimum_size:
            self.debug.write(ssh_debug.DEBUG_2, 'send_packet: base size too small')
            # Add enough padding to make it big enough.
            # Make a first guess of the padding length.
            padding_length_guess = minimum_size - base_size
            # See how much larger it should be to make it a multiple of the
            # block size.
            additional_padding_length = block_size - ((5 + len(data) + padding_length_guess) % block_size)
            padding_length = padding_length_guess + additional_padding_length

        self.debug.write(ssh_debug.DEBUG_2, 'send_packet: padding_length=%i', (padding_length,))
        self.debug.write(ssh_debug.DEBUG_2, 'send_packet: cipher=%s', (self.self2remote.cipher.name,))

        packet_length = 1 + len(data) + padding_length
        self.debug.write(ssh_debug.DEBUG_2, 'send_packet: packet_length=%i', (packet_length,))

        random_padding = random.get_random_data(padding_length)
        chunk = struct.pack('>Ic', packet_length, chr(padding_length)) + data + random_padding
        self.debug.write(ssh_debug.DEBUG_2, 'send_packet: chunk_length=%i', (len(chunk),))

        mac = self.self2remote.mac.digest(self.self2remote.packet_sequence_number, chunk)
        self.self2remote.inc_packet_sequence_number()
        self.debug.write(ssh_debug.DEBUG_2, 'send_packet: mac=%r', (mac,))

        # self.debug.write(ssh_debug.DEBUG_2, 'send_packet: chunk=%r', (chunk,))
        encrypted_chunk = self.self2remote.cipher.encrypt(chunk)
        # self.debug.write(ssh_debug.DEBUG_2, 'send_packet: encrypted_chunk=%r', (encrypted_chunk,))

        self.transport.write(encrypted_chunk + mac)

    def receive_message(self, wait_till):
        """receive_message(self, wait_till) -> message_type, packet
        Read a message off the stream.

        <wait_till>: List of message types that you are looking for.
        """
        if not self._receive_thread:
            raise SSH_Protocol_Error('receive thread not running')
        self.tmc.add(coro.current(), wait_till)
        try:
            return coro._yield()
        except:
            self.tmc.remove(coro.current())
            raise

    def start_receive_thread(self):
        """start_receive_thread(self) -> None
        Spawns the receive thread.
        """
        self.closed = False
        self._receive_thread = coro.spawn(self.receive_loop)

    def stop_receive_thread(self):
        """stop_receive_thread(self) -> None
        Stops the receive thread.
        """
        # If the receive loop calls a handler that calls disconnect
        # then we don't want to raise an exception on ourself.
        if self._receive_thread and self._receive_thread is not coro.current():
            self._receive_thread.raise_exception(Stop_Receiving_Exception)
            self._receive_thread = None

    def receive_loop(self):
        """receive_loop(self) -> None
        This is the receive thread.  It runs forever processing messages off
        the socket.
        """
        exc_type = exc_data = exc_tb = None
        while True:
            try:
                packet, sequence_number = self._receive_packet()
            except Stop_Receiving_Exception:
                break
            except:
                exc_type, exc_data, exc_tb = sys.exc_info()
                break
            if self.ignore_first_packet:
                # This can only happen during the beginning of the
                # connection, so we don't need to worry about multiple
                # threads since there can be only 1.
                assert(len(self.tmc.processing_threads) <= 1)
                self.debug.write(ssh_debug.DEBUG_1, 'receive_thread: ignoring first packet')
                self.ignore_first_packet = False
                continue

            message_type = ord(packet[0])
            self.debug.write(ssh_debug.DEBUG_2, 'receive_thread: message_type=%i', (message_type,))
            try:
                self._handle_packet(message_type, packet, sequence_number)
            except Stop_Receiving_Exception:
                break
            except:
                exc_type, exc_data, exc_tb = sys.exc_info()
                break
            # XXX: We should check here for SSH_MSG_KEXINIT.
            #      If we see it, then we should lock down and prevent any
            #      other messages other than those for the key exchange.
            # if message_type == SSH_MSG_KEXINIT:

            # Wake up anyone waiting for their message.
            if message_type in self.tmc.processing_messages:
                thread = self.tmc.processing_messages[message_type]
                try:
                    self.debug.write(ssh_debug.DEBUG_2, 'receive_thread: waiting thread waking up')
                    coro.schedule(thread, (message_type, packet))
                except coro.ScheduleError:
                    # Coro already scheduled.
                    pass
                self.tmc.remove(thread)
        self.closed = True
        self.transport.close()
        self._receive_thread = None
        if exc_data is None:
            exc_data = SSH_Protocol_Error('Receive thread shut down.')

        # Wake up anyone still waiting for messages.
        for thread in self.tmc.processing_messages.values():
            try:
                # XXX: Too bad can't pass in traceback.
                thread.raise_exception(exc_data, force=False)
            except coro.ScheduleError:
                # Coro already scheduled.
                pass
        self.tmc.clear()

    def _handle_packet(self, message_type, packet, sequence_number):
        if message_type not in self.tmc.processing_messages:
            if message_type in self.message_callbacks:
                f = self.message_callbacks[message_type]
                self.debug.write(ssh_debug.DEBUG_2, 'receive_thread: calling registered function %s', (f.__name__,))
                f(packet)
            else:
                self.debug.write(ssh_debug.DEBUG_2, 'receive_thread: unimplemented message type (%i)', (message_type,))
                self.send_unimplemented(sequence_number)

    def prepare_keys(self):
        self.c2s.cipher.set_encryption_key_and_iv(self.key_exchange.get_encryption_key('C', self.c2s.cipher.key_size),
                                                  self.key_exchange.get_encryption_key('A', self.c2s.cipher.iv_size))
        self.s2c.cipher.set_encryption_key_and_iv(self.key_exchange.get_encryption_key('D', self.s2c.cipher.key_size),
                                                  self.key_exchange.get_encryption_key('B', self.s2c.cipher.iv_size))
        self.c2s.mac.set_key(self.key_exchange.get_encryption_key('E', self.c2s.mac.key_size))
        self.s2c.mac.set_key(self.key_exchange.get_encryption_key('F', self.s2c.mac.key_size))

    def send_newkeys(self):
        self.debug.write(ssh_debug.DEBUG_3, 'send_newkeys()')
        packet = ssh_packet.pack_payload(ssh_packet.PAYLOAD_MSG_NEWKEYS, (SSH_MSG_NEWKEYS,))
        self.send_packet(packet)
        # XXX: Unlock key exchange lockdown for self2remote.

    def send_unimplemented(self, sequence_number):
        self.debug.write(ssh_debug.DEBUG_3, 'send_unimplemented(sequence_number=%i)', (sequence_number,))
        self.send_packet(
            ssh_packet.pack_payload(ssh_packet.PAYLOAD_MSG_UNIMPLEMENTED,
                                    (SSH_MSG_UNIMPLEMENTED,
                                     sequence_number)
                                    )
        )

    def _receive_packet(self):
        """_receive_packet(self) -> payload, sequence_number
        Reads a packet off the l4 transport.
        """
        self.debug.write(ssh_debug.DEBUG_3, 'receive_packet()')
        first_chunk = self.transport.read(max(8, self.remote2self.cipher.block_size))
        self.debug.write(ssh_debug.DEBUG_3, 'receive_packet: first_chunk=%r', (first_chunk,))
        first_chunk = self.remote2self.cipher.decrypt(first_chunk)
        self.debug.write(ssh_debug.DEBUG_3, 'receive_packet: post decrypt: %r', (first_chunk,))
        self.debug.write(ssh_debug.DEBUG_3, 'receive_packet: cipher=%s', (self.remote2self.cipher.name,))

        packet_length = struct.unpack('>I', first_chunk[:4])[0]
        min_packet_length = max(16, self.remote2self.cipher.block_size)
        # +4 to include the length field.
        if packet_length + 4 < min_packet_length:
            self.debug.write(
                ssh_debug.WARNING, 'receive_packet: packet length too small (len=%i)', (packet_length + 4,))
            self.send_disconnect(SSH_DISCONNECT_PROTOCOL_ERROR, 'packet length too small: %i' % packet_length)
        if packet_length + 4 > 1048576:  # 1 megabyte
            self.debug.write(ssh_debug.WARNING, 'receive_packet: packet length too big (len=%i)', (packet_length + 4,))
            self.send_disconnect(SSH_DISCONNECT_PROTOCOL_ERROR, 'packet length too big: %i' % packet_length)

        self.debug.write(
            ssh_debug.DEBUG_3, 'receive_packet: reading rest of packet (packet_length=%i)', (packet_length,))
        rest_of_packet = self.transport.read(packet_length - len(first_chunk) + 4 + self.remote2self.mac.digest_size)
        if self.remote2self.mac.digest_size == 0:
            mac = ''
        else:
            mac = rest_of_packet[-self.remote2self.mac.digest_size:]
            rest_of_packet = rest_of_packet[:-self.remote2self.mac.digest_size]
        rest_of_packet = self.remote2self.cipher.decrypt(rest_of_packet)
        packet = first_chunk + rest_of_packet

        padding_len = ord(packet[4])
        self.debug.write(ssh_debug.DEBUG_3, 'receive_packet: padding_length=%i', (padding_len,))
        payload = packet[5:packet_length + 4 - padding_len]

        packet_sequence_number = self.remote2self.packet_sequence_number
        self.debug.write(ssh_debug.DEBUG_3, 'receive_packet: packet=%r', (packet,))
        self.debug.write(ssh_debug.DEBUG_3, 'receive_packet: packet_sequence_number=%i', (packet_sequence_number,))
        computed_mac = self.remote2self.mac.digest(packet_sequence_number, packet)
        self.remote2self.inc_packet_sequence_number()

        if computed_mac != mac:
            self.debug.write(
                ssh_debug.WARNING, 'receive_packet: mac did not match: computed=%r actual=%r', (computed_mac, mac))
            self.send_disconnect(SSH_DISCONNECT_MAC_ERROR, 'mac did not match')

        return payload, packet_sequence_number

    def msg_disconnect(self, packet):
        msg, reason_code, description, language = ssh_packet.unpack_payload (ssh_packet.PAYLOAD_MSG_DISCONNECT, packet)
        self.disconnect()
        raise SSH_Protocol_Error(reason_code, description)

    def msg_ignore(self, packet):
        # msg, data = ssh_packet.unpack_payload(ssh_packet.PAYLOAD_MSG_IGNORE, packet)
        pass

    def msg_debug(self, packet):
        msg, always_display, message, language = ssh_packet.unpack_payload(ssh_packet.PAYLOAD_MSG_DEBUG, packet)
        self.debug.write(ssh_debug.DEBUG_1, 'SSH_MSG_DEBUG: %s', message)

    def msg_unimplemented(self, packet):
        msg, seq_number = ssh_packet.unpack_payload(ssh_packet.PAYLOAD_MSG_UNIMPLEMENTED, packet)
        self.debug.write(ssh_debug.DEBUG_1, 'SSH_MSG_UNIMPLEMENTED: %i', seq_number)

    def msg_kexinit(self, packet):
        self.remote2self.kexinit_packet = packet
        msg, cookie, kex_algorithms, server_host_key_algorithms, encryption_algorithms_c2s, \
            encryption_algorithms_s2c, mac_algorithms_c2s, mac_algorithms_s2c, \
            compression_algorithms_c2s, compression_algorithms_s2c, \
            languages_c2s, languages_s2c, first_kex_packet_follows, pad = ssh_packet.unpack_payload(
                ssh_packet.PAYLOAD_MSG_KEXINIT, packet)

        self.remote2self.proactive_kex = first_kex_packet_follows

        self.c2s.set_supported(kex_algorithms,
                               server_host_key_algorithms,
                               encryption_algorithms_c2s,
                               mac_algorithms_c2s,
                               compression_algorithms_c2s,
                               languages_c2s,
                               1)  # Prefer client's list.
        self.s2c.set_supported(kex_algorithms,
                               server_host_key_algorithms,
                               encryption_algorithms_s2c,
                               mac_algorithms_s2c,
                               compression_algorithms_s2c,
                               languages_s2c,
                               0)  # Prefer client's list.

        # The algorithm that we use is the first item that is on the client's
        # list that is also on the server's list.
        self._matchup_kex_and_key()
        self._matchup('cipher')
        self._matchup('mac')
        self._matchup('compression')
        # XXX: lang not supported

        # See if we guessed the kex properly.
        if self.remote2self.proactive_kex and \
                self.remote2self.key_exchange.name != self.key_exchange.name:
            # Remote side sent an incorrect initial kex packet...ignore it.
            self.ignore_first_packet = True

        if self.self2remote.proactive_kex and \
                self.self2remote.key_exchange.name != self.key_exchange.name:
            # We sent an invalid initial kex packet.
            # Resend proper kex packet.
            self.debug.write(ssh_debug.DEBUG_1, 'msg_kexinit: Resending initial kex packet due to incorrect guess')
            if self.is_server:
                packet = self.key_exchange.get_initial_server_kex_packet()
            else:
                packet = self.key_exchange.get_initial_client_kex_packet()
            # packet should never be None because if proactive_kex is set,
            # then that means we sent the first packet.
            assert (packet is not None)
            self.send_packet(packet)

        # Sync up.
        self.remote2self.key_exchange = self.key_exchange
        self.self2remote.key_exchange = self.key_exchange
        self.remote2self.server_key = self.server_key
        self.self2remote.server_key = self.server_key

        # Make sure kex algorithm has the information it needs.
        self.key_exchange.set_info(self.c2s.version_string, self.s2c.version_string,
                                   self.c2s.kexinit_packet, self.s2c.kexinit_packet, self.s2c.supported_server_keys)

    def _matchup(self, what):
        if getattr(self.remote2self, what) is None:
            self.send_disconnect(
                SSH_DISCONNECT_KEY_EXCHANGE_FAILED, 'We do not support any of the remote side\'s %ss.' % what)
        if getattr(self.self2remote, what) is None:
            self.send_disconnect(
                SSH_DISCONNECT_KEY_EXCHANGE_FAILED, 'The remote side does not support any of our %ss.' % what)

    def _matchup_kex_and_key(self):
        """_matchup_kex_and_key(self) -> None
        This sets self.key_exchange and self.server_key to the appropriate
        value.  It checks that both us and the remote end support the same
        key exchange and we both support a key type that has the appropriate
        features required by the key exchange algorithm.
        """
        self.remote2self.set_preferred('key_exchange')
        self.remote2self.set_preferred('server_key')
        self.self2remote.set_preferred('key_exchange')
        self.self2remote.set_preferred('server_key')

        if self.remote2self.key_exchange is None or self.self2remote.key_exchange is None:
            self.send_disconnect(SSH_DISCONNECT_KEY_EXCHANGE_FAILED, 'Could not find matching key exchange algorithm.')
        if self.remote2self.server_key is None or self.self2remote.server_key is None:
            self.send_disconnect(SSH_DISCONNECT_KEY_EXCHANGE_FAILED, 'Could not find matching server key type.')

        if self.remote2self.key_exchange.name != self.self2remote.key_exchange.name:
            # iterate over client's kex algorithms,
            # one at a time.  Choose the first algorithm that satisfies
            # the following conditions:
            # +  the server also supports the algorithm,
            # +  if the algorithm requires an encryption-capable host key,
            #    there is an encryption-capable algorithm on the server's
            #    server_host_key_algorithms that is also supported by the
            #    client, and
            # +  if the algorithm requires a signature-capable host key,
            #    there is a signature-capable algorithm on the server's
            #    server_host_key_algorithms that is also supported by the
            #    client.
            # +  If no algorithm satisfying all these conditions can be
            #    found, the connection fails, and both sides MUST
            #    disconnect.
            for client_kex_algorithm in self.c2s.supported_key_exchanges:
                server_kex_algorithm = pick_from_list(client_kex_algorithm.name, self.s2c.supported_key_exchanges)
                if server_kex_algorithm is not None:
                    # We both support this kex algorithm.
                    # See if we both have key types that match the requirements of this kex algorithm.
                    for server_host_key_type in self.s2c.supported_server_keys:
                        if (server_kex_algorithm.wants_encryption_host_key and not server_host_key_type.supports_encryption) or \
                           (server_kex_algorithm.wants_signature_host_key and not server_host_key_type.supports_signature):  # noqa
                            # This host key is not appropriate.
                            continue
                        else:
                            # This key meets our requirements.
                            break
                    else:
                        # None of the host key types worked, try next kex algorithm.
                        continue

                    # If we got here, then this is the kex to use.
                    self.set_key_exchange(client_kex_algorithm.name, server_host_key_type.name)
                    break
            else:
                # None of the kex algorithms worked.
                self.send_disconnect(
                    SSH_DISCONNECT_KEY_EXCHANGE_FAILED, 'Could not find matching key exchange algorithm.')
        else:
            # We have agreement on the kex algorithm to use.
            # See if we have agreement on the server host key type.
            self.debug.write (ssh_debug.DEBUG_3, 'msg_kexinit: agreement on %r' % (self.self2remote.key_exchange.name,))
            if self.remote2self.server_key.name != self.self2remote.server_key.name:
                # See if we share a server host key type that also works with our chosen kex algorithm.
                for client_server_key_type in self.c2s.supported_server_keys:
                    server_server_key_type = pick_from_list(client_server_key_type.name, self.s2c.supported_server_keys)
                    if server_server_key_type is not None:
                        # We both support this server key algorithm.
                        # See if it matches our kex algorithm requirements.
                        if (self.remote2self.key_exchange.wants_encryption_host_key and not server_server_key_type.supports_encryption) or \
                           (self.remote2self.key_exchange.wants_signature_host_key and not server_server_key_type.supports_signature):  # noqa
                            # This server key type is not appropriate.
                            continue
                        else:
                            # This meets our requirements.
                            break
                else:
                    # None of the server key types worked.
                    self.send_disconnect(
                        SSH_DISCONNECT_KEY_EXCHANGE_FAILED,
                        'Could not find matching server key type for %s key exchange.' %
                        self.remote2self.key_exchange.name)
            self.debug.write (ssh_debug.DEBUG_3, 'msg_kexinit: set_key_exchange: %r' %
                              (self.remote2self.server_key.name,))
            self.set_key_exchange(self.remote2self.key_exchange.name, self.remote2self.server_key.name)

    def set_key_exchange(self, key_exchange=None, server_host_key_type=None):
        """set_key_exchange(self, key_exchange=None, server_host_key_type=None) -> None
        Sets the key exchange algorithm to use.

        <key_exchange> - A string.  The key exchange algorithm to use.
                         Must be one of those in supported_key_exchanges.
                         Set to None to default to the preferred algorithm.
        <server_host_key_type> - A string.  The server host key type to use.
                         Must be one of thos in supported_server_keys.
                         Set to None to default to the preferred algorithm.
                         The key_exchange algorithm MUST support this type
                         of key.
        """
        kex = pick_from_list(key_exchange, self.self2remote.supported_key_exchanges)
        if kex is None:
            raise ValueError('Unknown key exchange algorithm: %s' % key_exchange)
        key = pick_from_list(server_host_key_type, self.self2remote.supported_server_keys)
        if key is None:
            raise ValueError('Unknown server key type: %s' % server_host_key_type)
        self.key_exchange = kex
        self.server_key = key
        if self.is_server:
            self.key_exchange.register_server_callbacks()
        else:
            self.key_exchange.register_client_callbacks()

    def _process_kexinit(self):
        """_process_kexinit(self) -> None
        This processes the key exchange.
        """
        message_type, packet = self.receive_message((SSH_MSG_KEXINIT,))
        self.msg_kexinit(packet)

    def send_kexinit(self):
        """send_kexinit(self) -> None
        Start the key exchange.
        """
        # Tell the remote side what features we support.
        self.debug.write(ssh_debug.DEBUG_3, 'send_kexinit()')
        packet = self._send_kexinit()
        self.send_packet(packet)

    def _send_kexinit(self):
        """_send_kexinit(self) -> None
        Sets self2remote.kexinit_packet.
        Separate function to help with unittests.
        """
        cookie = random.get_random_data(16)
        server_keys = [x.name for x in self.self2remote.supported_server_keys]
        server_keys.reverse()
        packet = ssh_packet.pack_payload(ssh_packet.PAYLOAD_MSG_KEXINIT,
                                         (SSH_MSG_KEXINIT,
                                          cookie,
                                          [x.name for x in self.self2remote.supported_key_exchanges],
                                             # [x.name for x in self.self2remote.supported_server_keys],
                                             server_keys,
                                             [x.name for x in self.c2s.supported_ciphers],
                                             [x.name for x in self.s2c.supported_ciphers],
                                             [x.name for x in self.c2s.supported_macs],
                                             [x.name for x in self.s2c.supported_macs],
                                             [x.name for x in self.c2s.supported_compressions],
                                             [x.name for x in self.s2c.supported_compressions],
                                             [x.name for x in self.c2s.supported_languages],
                                             [x.name for x in self.s2c.supported_languages],
                                             self.self2remote.proactive_kex,  # first_kex_packet_follows
                                             0  # reserved
                                          )
                                         )
        self.self2remote.kexinit_packet = packet
        return packet

    def msg_newkeys(self, packet):
        self.send_newkeys()
        self.debug.write(ssh_debug.DEBUG_3, 'msg_newkeys(packet=...)')
        # Switch to using new algorithms.
        self.remote2self.set_preferred()
        self.self2remote.set_preferred()
        # Set the keys to use for encryption and MAC.
        self.prepare_keys()
        self.debug.write(ssh_debug.DEBUG_3, 'msg_newkeys: keys have been prepared')

class One_Way_SSH_Transport:

    # These are references to the in-use entry from one of the
    # supported_xxx lists.
    key_exchange = None
    server_key = None
    compression = None
    cipher = None
    mac = None
    language = None

    protocol_version = '2.0'
    software_version = 'Shrapnel_1.0'
    comments = ''
    version_string = ''
    kexinit_packet = ''

    # Whether or not we sent our first kex packet with the assumption that
    # the remote side supports our preferred algorithms.
    proactive_kex = 0

    packet_sequence_number = 0

    def __init__(self, transport):
        # Instantiate all components.
        # An assumption is made that the first (preferred) key exchange algorithm
        # supports the first (preferred) server key type.
        self.supported_key_exchanges = [Diffie_Hellman_Group1_SHA1(transport)]
        self.supported_server_keys = [SSH_DSS(), SSH_RSA()]
        self.supported_compressions = [Compression_None()]
        self.supported_ciphers = [Triple_DES_CBC(),
                                  Blowfish_CBC(),
                                  Cipher_None(),
                                  ]
        self.supported_macs = [HMAC_SHA1(),
                               MAC_None(),
                               ]
        self.supported_languages = []

        self.set_none()

    def inc_packet_sequence_number(self):
        """inc_packet_sequence_number(self) -> None
        Raises the packet sequence number by one.
        """
        self.packet_sequence_number += 1
        if self.packet_sequence_number == 4294967296:
            self.packet_sequence_number = 0

    def set_none(self):
        """set_none(self) -> None
        Sets the in-use settings to that which is suitable for the beginning
        of a connection.
        """
        self.key_exchange = None
        self.server_key = None
        self.compression = Compression_None()
        self.cipher = Cipher_None()
        self.mac = MAC_None()
        self.language = None

    def set_preferred(self, what=None):
        """set_preferred(self, what = None) -> None
        Sets the "preferred" pointers to the first element of the appropriate
        lists.

        <what> - Can be a string to indicate which element to set.
                 Set to None to set all elements.
        """
        def get(list):
            try:
                return list[0]
            except IndexError:
                return None

        if what is None:
            self.key_exchange = get(self.supported_key_exchanges)
            self.server_key = get(self.supported_server_keys)
            self.compression = get(self.supported_compressions)
            self.cipher = get(self.supported_ciphers)
            self.mac = get(self.supported_macs)
            self.language = get(self.supported_languages)
        else:
            supported = getattr(self, 'supported_%ss' % what)
            setattr(self, what, get(supported))

    def set_supported(self, kex, key, encrypt, mac, compress, lang, prefer_self):
        """set_supported(self, kex, key, encrypt, mac, compress, lang, prefer_self) -> None
        Sets the supported feature lists.
        Each argument is a list of strings.
        <prefer_self> - boolean.  If true, prefers the order of self.supported_xxx.
                                  If false, prefers the order of the given lists.
        """
        def _filter(feature_list, algorithm_list, prefer_self):
            algorithm_list[:] = filter(lambda x, y=feature_list: x.name in y, algorithm_list)
            if not prefer_self:
                # Change the order to match that of <feature_list>
                new_list = []
                for feature in feature_list:
                    for self_alg in algorithm_list:
                        if self_alg.name == feature:
                            new_list.append(self_alg)

                if __debug__:
                    assert(len(algorithm_list) == len(new_list))
                algorithm_list[:] = new_list

        _filter(kex, self.supported_key_exchanges, prefer_self)
        _filter(key, self.supported_server_keys, prefer_self)
        _filter(encrypt, self.supported_ciphers, prefer_self)
        _filter(mac, self.supported_macs, prefer_self)
        _filter(compress, self.supported_compressions, prefer_self)
        _filter(lang, self.supported_languages, prefer_self)

class Thread_Message_Callbacks:

    """Thread_Message_Callbacks()

    This is a simple wrapper around the transport's process messages mechanism.
    """

    def __init__(self):
        # processing_messages: Dictionary of {message: thread}
        # Indicates which thread to wake up for each message.
        self.processing_messages = {}
        # processing_threads: Dictionary of {thread_id:wait_till}
        # Reverse of processing_messages.
        self.processing_threads = {}

    def clear(self):
        """clear(self) -> None
        Clear all threads/messages.
        """
        self.processing_messages = {}
        self.processing_threads = {}

    def remove(self, coro_object):
        """remove(self, coro_object) -> None
        Remove a thread that is being tracked.
        """
        thread_id = coro_object.thread_id()
        if thread_id in self.processing_threads:
            messages = self.processing_threads[thread_id]
            for m in messages:
                assert(self.processing_messages[m] == coro_object)
                del self.processing_messages[m]
            del self.processing_threads[thread_id]

    def add(self, coro_object, messages_waiting_for):
        """add(self, coro_object, messages_waiting_for) -> None
        Add a thread to the list of processing.

        <coro_object>: The thread object.
        <messages_waiting_for>: List of messages the thread is waiting for.
        """
        self.processing_threads[coro_object.thread_id()] = messages_waiting_for
        for m in messages_waiting_for:
            if m in self.processing_messages:
                raise AssertionError('Can\'t register message %i with multiple threads.' % m)
            self.processing_messages[m] = coro_object

class Stop_Receiving_Exception(Exception):
    pass

########NEW FILE########
__FILENAME__ = debug
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_debug
#
# This module implements the debugging facilities.
#
# Debug information is represented at different levels:
# ERROR - Fatal error.
# WARNING - Warning about a non-fatal problem.
# INFO - General information.
# DEBUG_1 - Action-level information.
# DEBUG_2 - Packet-level information.
# DEBUG_3 - Low-level function tracing.
#
# Setting the log level includes all levels above it.
# Default level is WARNING.
#

ERROR   = 0
WARNING = 1
INFO    = 2
DEBUG_1 = 3
DEBUG_2 = 4
DEBUG_3 = 5

level_text = {ERROR: 'Error',
              WARNING: 'Warning',
              INFO: 'Info',
              DEBUG_1: 'Debug 1',
              DEBUG_2: 'Debug 2',
              DEBUG_3: 'Debug 3',
              }

import sys

class Debug:

    level = WARNING

    def write(self, level, message, args=None):
        if level <= self.level:
            if args is not None:
                message = message % args
            try:
                sys.stderr.write('[%s] %s\n' % (level_text[level], message))
            except IOError:
                pass

########NEW FILE########
__FILENAME__ = mpint
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_mpint
#
# Routines to handle SSH multiple-precision integers.
#

import struct

def pack_mpint(n):
    """pack_mpint(n) -> string
    Multiple-Precision Integer
    Packs a python long into a big endian string.
    """
    # Per the spec:
    # - two's compliment
    # - big-endian
    # - negative numbers have MSB of the first byte set
    # - if MSB would be set in a positive number, then preceed with a zero byte
    # - Unnecessary leading bytes with the value 0 or 255 MUST NOT be included.
    # - Zero stored as four 0 bytes.

    # This is very inefficient.
    s = []
    x = long(n)
    if x == 0:
        return ''
    elif x < 0:
        while 1:
            s.append(struct.pack('>L', x & 0xffffffffL))
            if x == -1:
                break
            x = x >> 32
    else:
        while x > 0:
            s.append(struct.pack('>L', x & 0xffffffffL))
            x = x >> 32
    s.reverse()
    s = ''.join(s)

    if s[0] == '\0':
        # Remove extra leading zeros.
        # This is a positive number.
        count = 0
        for i in s:
            if i != '\0':
                break
            count += 1
        # If the MSB is set, pad with a zero byte.
        if ord(s[count]) & 128:
            s = s[count - 1:]
        else:
            s = s[count:]
    elif s[0] == '\377' and n < 0:
        # Remove extra leading ones.
        # This is a negative number.
        for x in xrange(len(s)):
            i = s[x]
            if i != '\377':
                break
        # If the MSB is not set, then we need to sign-extend and make sure
        # there is another byte of all ones.
        if ord(s[x]) & 128:
            s = s[x:]
        else:
            s = s[x - 1:]

    # If the MSB is set and this is a positive number, pad with a zero.
    if n > 0 and ord(s[0]) & 128:
        s = '\0' + s
    return s

def unpack_mpint(mpint):
    """unpack_mpint(mpint) -> long
    Unpacks a multiple-precision string.
    """
    if len(mpint) % 4:
        # Need to pad it so that it is a multiple of 4.
        pad = 4 - (len(mpint) % 4)
        if ord(mpint[0]) & 128:
            # Negative number
            mpint = '\377' * pad + mpint
            struct_format = '>i'
        else:
            # Positive number
            mpint = '\0' * pad + mpint
            struct_format = '>I'
    else:
        if mpint and ord(mpint[0]) & 128:
            # Negative
            struct_format = '>i'
        else:
            # Positive
            struct_format = '>I'
    result = 0L
    for x in xrange(0, len(mpint), 4):
        result = (result << 32) | struct.unpack(struct_format, mpint[x: x + 4])[0]
    return result

import unittest

class ssh_packet_test_case(unittest.TestCase):
    pass

class mpint_test_case(ssh_packet_test_case):

    def runTest(self):
        self.check(0, '')
        self.check(0x9a378f9b2e332a7L, '\x09\xa3\x78\xf9\xb2\xe3\x32\xa7')
        self.check(0x80L, '\0\x80')
        self.check(-0x1234L, '\xed\xcc')
        self.check(-0xdeadbeefL, '\xff\x21\x52\x41\x11')
        self.check(0xffffffffL, '\0\xff\xff\xff\xff')
        self.check(-0xffffffffL, '\xff\0\0\0\x01')
        self.check(-1L, '\377')

    def check(self, num, string):
        self.assertEqual(pack_mpint(num), string)
        self.assertEqual(unpack_mpint(string), num)

def suite():
    suite = unittest.TestSuite()
    suite.addTest(mpint_test_case())
    return suite

if __name__ == '__main__':
    unittest.main(defaultTest='suite')

########NEW FILE########
__FILENAME__ = packet
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_packet
#
# This module implements features to pack and unpack SSH packets.

# Format Codes
BYTE = 'byte'
BOOLEAN = 'boolean'
UINT32 = 'uint32'
UINT64 = 'uint64'
STRING = 'string'
MPINT = 'mpint'
NAME_LIST = 'name-list'
FIXED_STRING = 'fixed-string'

import struct
import mpint
import types

def unpack_payload(format, payload, offset=0):
    """unpack_payload(format, payload, offset=0) -> items
    Unpacks an SSH payload.

    <format> is a list of Format Codes.
    <payload> is the SSH payload.
    <offset> is the character offset into <payload> to start.
    """
    return unpack_payload_get_offset(format, payload, offset)[0]

def unpack_payload_get_offset(format, payload, offset=0):
    """unpack_payload_get_offset(format, payload, offset=0) -> items, index_where_scanning_stopped
    Unpacks an SSH payload.

    <format> is a list of Format Codes.
    <payload> is the SSH payload.
    <offset> is the character offset into <payload> to start.
    """
    i = offset   # Index into payload
    result = []
    for value_type in format:
        if value_type is BYTE:
            result.append(payload[i])
            i += 1
        elif value_type is BOOLEAN:
            result.append(ord(payload[i]) and 1 or 0)
            i += 1
        elif value_type is UINT32:
            result.append(struct.unpack('>I', payload[i:i + 4])[0])
            i += 4
        elif value_type is UINT64:
            result.append(struct.unpack('>Q', payload[i:i + 8])[0])
            i += 8
        elif value_type is STRING:
            str_len = struct.unpack('>I', payload[i:i + 4])[0]
            i += 4
            result.append(payload[i:i + str_len])
            i += str_len
        elif value_type is MPINT:
            mpint_len = struct.unpack('>I', payload[i:i + 4])[0]
            i += 4
            value = payload[i:i + mpint_len]
            i += mpint_len
            result.append(mpint.unpack_mpint(value))
        elif value_type is NAME_LIST:
            list_len = struct.unpack('>I', payload[i:i + 4])[0]
            i += 4
            result.append(payload[i:i + list_len].split(','))
            i += list_len
        elif isinstance(value_type, types.TupleType):
            if value_type[0] is FIXED_STRING:
                str_len = value_type[1]
                result.append(payload[i:i + str_len])
                i += str_len
        else:
            raise ValueError(value_type)
    return result, i

def pack_payload(format, values):
    """pack_payload(format, values) -> packet_str
    Creates an SSH payload.

    <format> is a list Format Codes.
    <values> is a tuple of values to use.
    """
    packet = [''] * len(format)
    if __debug__:
        assert(len(values) == len(format))
    i = 0
    for value_type in format:
        if value_type is BYTE:
            if isinstance(values[i], types.StringType):
                if __debug__:
                    assert(len(values[i]) == 1)
                packet[i] = values[i]
            else:
                packet[i] = chr(values[i])
        elif value_type is BOOLEAN:
            packet[i] = chr(values[i] and 1 or 0)
        elif value_type is UINT32:
            packet[i] = struct.pack('>I', values[i])
        elif value_type is UINT64:
            packet[i] = struct.pack('>Q', values[i])
        elif value_type is STRING:
            packet[i] = struct.pack('>I', len(values[i])) + values[i]
        elif value_type is MPINT:
            n = mpint.pack_mpint(values[i])
            packet[i] = struct.pack('>I', len(n)) + n
        elif value_type is NAME_LIST:
            # We could potentially check for validity here.
            # Names should be at least 1 byte long and should not
            # contain commas.
            s = ','.join(values[i])
            packet[i] = struct.pack('>I', len(s)) + s
        elif isinstance(value_type, types.TupleType) and value_type[0] is FIXED_STRING:
            packet[i] = values[i]
        else:
            raise ValueError(value_type)
        i += 1
    return ''.join(packet)

# Packet format definitions.
PAYLOAD_MSG_DISCONNECT = (
    BYTE,
    UINT32,   # reason code
    STRING,   # description
    STRING    # language tag
)

PAYLOAD_MSG_IGNORE = (
    BYTE,
    STRING    # data
)

PAYLOAD_MSG_UNIMPLEMENTED = (
    BYTE,
    UINT32     # packet sequence number of rejected message
)

PAYLOAD_MSG_DEBUG = (
    BYTE,
    BOOLEAN,   # always_display
    STRING,    # message
    STRING     # language tag
)

PAYLOAD_MSG_KEXINIT = (
    BYTE,
    (FIXED_STRING, 16),  # cookie
    NAME_LIST,        # kex_algorithms
    NAME_LIST,        # server_host_key_algorithms
    NAME_LIST,        # encryption_algorithms_client_to_server
    NAME_LIST,        # encryption_algorithms_server_to_client
    NAME_LIST,        # mac_algorithms_client_to_server
    NAME_LIST,        # mac_algorithms_server_to_client
    NAME_LIST,        # compression_algorithms_client_to_server
    NAME_LIST,        # compression_algorithms_server_to_client
    NAME_LIST,        # languages_client_to_server
    NAME_LIST,        # languages_server_to_client
    BOOLEAN,          # first_kex_packet_follows
    UINT32            # 0 (reserved for future extension)
)

PAYLOAD_MSG_NEWKEYS = (BYTE,)
PAYLOAD_MSG_SERVICE_REQUEST = (BYTE, STRING)     # service name
PAYLOAD_MSG_SERVICE_ACCEPT = (BYTE, STRING)     # service_name

import unittest

class ssh_packet_test_case(unittest.TestCase):
    pass

class unpack_test_case(ssh_packet_test_case):

    def runTest(self):
        # KEXINIT packet grabbed from my OpenSSH server.
        sample_packet = '\024\212a\330\261\300}.\252b%~\006j\242\356\367\000\000\000=diffie-hellman-group-exchange-sha1,diffie-hellman-group1-sha1\000\000\000\007ssh-dss\000\000\000\207aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,arcfour,aes192-cbc,aes256-cbc,rijndael-cbc@lysator.liu.se,aes128-ctr,aes192-ctr,aes256-ctr\000\000\000\207aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,arcfour,aes192-cbc,aes256-cbc,rijndael-cbc@lysator.liu.se,aes128-ctr,aes192-ctr,aes256-ctr\000\000\000Uhmac-md5,hmac-sha1,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96\000\000\000Uhmac-md5,hmac-sha1,hmac-ripemd160,hmac-ripemd160@openssh.com,hmac-sha1-96,hmac-md5-96\000\000\000\011none,zlib\000\000\000\011none,zlib\000\000\000\000\000\000\000\000\000\000\000\000\000'  # noqa
        msg, cookie, kex_algorithms, server_host_key_algorithms, encryption_algorithms_c2s, \
            encryption_algorithms_s2c, mac_algorithms_c2s, mac_algorithms_s2c, \
            compression_algorithms_c2s, compression_algorithms_s2c, \
            languages_c2s, languages_s2c, first_kex_packet_follows, reserved = unpack_payload(
                PAYLOAD_MSG_KEXINIT, sample_packet)

def suite():
    suite = unittest.TestSuite()
    suite.addTest(unpack_test_case())
    return suite

if __name__ == '__main__':
    unittest.main(module='ssh_packet', defaultTest='suite')

########NEW FILE########
__FILENAME__ = password
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.util.password
#
# Simple code to input a password.
#

import termios
import sys

def get_password(prompt):
    print prompt,
    # Turn off echo.
    term_settings = termios.tcgetattr(0)        # 0 is stdin
    term_settings[3] &= ~termios.ECHO
    termios.tcsetattr(0, termios.TCSANOW, term_settings)
    try:
        result = sys.stdin.readline()
    finally:
        # Restore ECHO
        term_settings[3] |= termios.ECHO
        termios.tcsetattr(0, termios.TCSANOW, term_settings)
    if not result:
        raise EOFError
    return result[:-1]  # Strip trailing newline

########NEW FILE########
__FILENAME__ = random
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh_random
#
# Consolidated location for getting random data.
#

import Crypto.Util.number
import Crypto.Util.randpool
import math

# XXX: Should we stir the pool occasionally?
random_pool = Crypto.Util.randpool.RandomPool()

def get_random_data(bytes):
    """get_random_data(bytes) -> str
    Gets <bytes> number of bytes of random data.
    """
    return random_pool.get_bytes(bytes)

def get_random_number(bits):
    """get_random_number(bits) -> python long
    Return a random number.
    <bits> is the number of bits in the number.
    """
    return Crypto.Util.number.getRandomNumber(bits, random_pool.get_bytes)

def get_random_number_from_range(low, high):
    """get_random_number_from_range(low, high) -> python long
    Returns a random number that is greater than <low> and less than <high>.
    """
    bits = highest_bit(high)
    while 1:
        x = get_random_number(bits)
        if x > low and x < high:
            break
    return x

def highest_bit(num):
    """highest_bit(num) -> n
    Determines the bit position of the highest bit in the given number.
    Example: 0x800000 returns 24.
    """
    try:
        return int(math.floor(math.log10(num) / math.log10(2))) + 1
    except OverflowError:
        assert(num == 0)
        return 0

# This was an alternate version of highest_bit, but the
# logarithm version seems to be a little faster.

highest_bit_map = {
    '0': 0,
    '1': 1,
    '2': 2,
    '3': 2,
    '4': 3,
    '5': 3,
    '6': 3,
    '7': 3,
    '8': 4,
    '9': 4,
    'A': 4,
    'B': 4,
    'C': 4,
    'D': 4,
    'E': 4,
    'F': 4,
}

def highest_bit2(num):
    """highest_bit(num) -> n
    Determines the bit position of the highest bit in the given number.
    Example: 0x800000 returns 24.
    """
    # Use a table for the high nibble, then count the number
    # of nibbles up to the highest one.  This algorithm is significantly faster
    # than shifting the number until it is zero.

    # Convert to long so that we don't have to test if it is an int or long
    # in order to accomodate for L at the end.
    h = hex(long(num))
    # Subtract 4 to accomodate for first two characters (0x) the last
    # character (L) and the high nibble character.
    return (len(h) - 4) * 4 + highest_bit_map[h[2]]

import unittest

class ssh_random_test_case(unittest.TestCase):
    pass

class highest_bit_test_case(ssh_random_test_case):

    def runTest(self):
        for x in xrange(300):
            self.assertEqual(highest_bit(x), highest_bit2(x))
        x = 144819228510396375480510966045726324197234443151241728654670685625305230385467763734653299992854300412367868856607501321634131298084648429649714452472261648519166487595581105734370788168033696455943547609540069712392591019911289209306656760054646817215504894551439102079913490941604156000063251698742214491563L  # noqa
        self.assertEqual(highest_bit(x), highest_bit2(x))
        while x > 0:
            x = x / 2
            self.assertEqual(highest_bit(x), highest_bit2(x))

def suite():
    suite = unittest.TestSuite()
    suite.addTest(highest_bit_test_case())
    return suite

if __name__ == '__main__':
    unittest.main(module='ssh_random', defaultTest='suite')

########NEW FILE########
__FILENAME__ = coro_interactive_ssh_wrapper
# Copyright (c) 2002-2012 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# ssh.wrapper.coro_interactive_ssh_wrapper
#
# This is a easy-to-use wrapper that uses coro_ssh.
#
# It has only a few features, but many may be added in the future.

import dnsqr
import inet_utils
import coro.ssh.transport.client
import coro.ssh.connection.connect
import coro.ssh.l4_transport.coro_socket_transport
import coro.ssh.auth.userauth
import coro.ssh.connection.interactive_session
import coro.ssh.util.debug

DISABLE_PASSWORD = '__DISABLE_PASSWORD__'

class Coro_Interactive_SSH_Wrapper:

    client = None
    transport = None
    service = None
    channel = None

    def __init__(self):
        pass

    def connect(self, username, remote_address, remote_port=22, password=None,
                command=None, debug_level=coro.ssh.util.debug.WARNING):
        """connect(self, username, remote_address, remote_port=22, password=None,
                   command=None, debug_level=coro.ssh.util.debug.WARNING) -> None
        The opens a connection to the remote side and authenticates.

        <username> - The remote username to log into.
        <remote_address> - The remote address to connect to.
        <remote_port> - The remote port to connect to.
        <password> - The password to use when connecting to the remote side.
                     If None, and there are no authorized_keys configured,
                     then it will ask for the password on stdout/stdin.
                     If DISABLE_PASSWORD, will disable password auth.
        <command> - The command to run on the remote side.
                    If no command is given, then it will open a pty and shell.
        <debug_level> - Level a debuging to print to stderr.
        """

        self.client = coro.ssh.transport.client.SSH_Client_Transport()
        if inet_utils.is_ip(remote_address):
            remote_ip = remote_address
            hostname = None
        else:
            dns_query_result = dnsqr.query(remote_address, 'A')
            remote_ip = dns_query_result[0][-1]
            hostname = remote_address
        coro_socket_transport = coro.ssh.l4_transport.coro_socket_transport
        self.transport = coro_socket_transport.coro_socket_transport(
            remote_ip, remote_port, hostname=hostname)
        self.client.connect(self.transport)
        self.client.debug.level = debug_level
        self.service = coro.ssh.connection.connect.Connection_Service(self.client)
        self._authenticate(username, password)
        self.channel = coro.ssh.connection.interactive_session.Interactive_Session_Client(self.service)
        self.channel.open()
        if command is not None:
            self.channel.exec_command(command)
        else:
            self.channel.open_pty()
            self.channel.open_shell()

    def _authenticate(self, username, password=None):
        auth_method = coro.ssh.auth.userauth.Userauth(self.client)
        auth_method.username = username
        if password is not None:
            for x in xrange(len(auth_method.methods)):
                if auth_method.methods[x].name == 'password':
                    break
            else:
                # This should never happen.
                raise ValueError('Expected password auth method in Userauth class')
            if password is DISABLE_PASSWORD:
                del auth_method.methods[x]
            else:
                password_method = Fixed_Password_Auth(self.client)
                password_method.password = password
                auth_method.methods[x] = password_method
        self.client.authenticate(auth_method, self.service.name)

    def disconnect(self):
        if self.channel is not None:
            self.channel.close()
        if self.client is not None:
            self.client.disconnect()

    close = disconnect

    def read(self, bytes):
        return self.channel.read(bytes)

    recv = read

    def read_exact(self, bytes):
        return self.channel.read_exact(bytes)

    def write(self, data):
        self.channel.send(data)

    send = write

class Fixed_Password_Auth(coro.ssh.auth.userauth.Password):

    password = None

    def get_password(self, username, prompt=None):
        return self.password

########NEW FILE########
__FILENAME__ = t_server
# -*- Mode: Python -*-

import coro
from coro.ssl import openssl
import coro.ssl
import coro.backdoor

W = coro.write_stderr

ctx = openssl.ssl_ctx()
ctx.use_cert (openssl.x509 (open ('../../http/cert/server.crt').read()))
ctx.use_key (openssl.pkey (open ('../../http/cert/server.key').read(), '', True))
ctx.set_ciphers ('RC4-SHA:RC4-MD5:ALL')
# ctx.set_tmp_dh (openssl.dh_param (open ('../../http/cert/dh_param_1024.pem').read()))
# ctx.set_next_protos (['spdy/2', 'http/1.1'])

def session (conn, addr):
    conn.ssl.set_fd (conn.fd)
    try:
        print 'conn=', conn, conn.__class__
        while 1:
            block = conn.recv (1000)
            if not block:
                break
            else:
                conn.send (block)
    finally:
        coro.sleep_relative (1000)
        W ('why for I exit?\n')

all_conns = []

def serve (port=9000):
    s = coro.ssl.sock (ctx)
    s.bind (('0.0.0.0', port))
    s.listen (50)
    print 's=', s
    while 1:
        conn, addr = s.accept()
        print 'conn, addr=', conn, addr
        all_conns.append (conn)
        coro.spawn (session, conn, addr)

if __name__ == '__main__':
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/xx.bd')
    coro.spawn (serve)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = tb
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Compact stack trace functions.

This module provides some functions to get a stack trace string.

These stack traces are much more compact than the ones generated by the
traceback module.
"""

import os
import sys

def _get_module_name(n):
    try:
        path, filename = os.path.split(n)
        path, directory = os.path.split(path)
        # name, ext = os.path.splitext(filename)
        if directory:
            return '/'.join((directory, filename))
        else:
            return filename
    except:
        return '???'

def stack_string(f=None):
    """Return a compact string representing the current call stack.

    :Parameters:
        - `f`: Frame object.  If not specified, will use the current call
          position.

    :Return:
        Returns a string of the current stack trace.
    """
    if f is None:
        try:
            raise ZeroDivisionError
        except ZeroDivisionError:
            f = sys.exc_info()[2].tb_frame.f_back
    stack = []
    while f is not None:
        stack.append(
            _get_module_name(f.f_code.co_filename) + ' ' +
            f.f_code.co_name + '|' +
            str(f.f_lineno)
        )
        f = f.f_back
    return '[' + ('] ['.join(stack)) + ']'

def traceback_string(t=None, v=None, tb=None):
    """Returns a compact string representing the current exception.

    If an exception is not provided as an argument, then it will get the
    current exception from `sys.exc_info`.

    :Parameters:
        - `t`: Exception type.
        - `v`: Exception value.
        - `tb`: Traceback object.

    :Return:
        Returns a string of the current exception and stack trace.
    """
    if t is None:
        t, v, tb = sys.exc_info()
    tbinfo = []
    if tb is None:
        # this should never happen, but then again, lots of things
        # should never happen but do
        return 'traceback is None!!!'
    while tb is not None:
        tbinfo.append (
            _get_module_name (tb.tb_frame.f_code.co_filename) + ' ' +
            tb.tb_frame.f_code.co_name + '|' +
            str(tb.tb_lineno)
        )
        tb = tb.tb_next

    # just to be safe
    del tb
    first = tbinfo[-1]
    info = '[' + ('] ['.join (tbinfo)) + ']'
    return repr(((first), str(t), str(v), info))

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Shrapnel documentation build configuration file, created by
# sphinx-quickstart on Fri Apr 13 18:44:49 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import os
import sys

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
# sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
# source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Shrapnel'
copyright = u'2012, Sam Rushing'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.0.2'
# The full version, including alpha/beta/rc tags.
release = '1.0.2'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
# language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
# Else, today_fmt is used as the format for a strftime call.
# today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
# default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
# modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {'collapsiblesidebar': True}

# Add any paths that contain custom themes here, relative to this directory.
# html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
# html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
# html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
# html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
# html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
# html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
# html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {}

# If false, no module index is generated.
# html_domain_indices = True

# If false, no index is generated.
# html_use_index = True

# If true, the index is split into individual pages for each letter.
# html_split_index = False

# If true, links to the reST sources are added to the pages.
# html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
# html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
# html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
# html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Shrapneldoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    # 'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    # 'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    # 'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
    ('index', 'Shrapnel.tex', u'Shrapnel Documentation', u'Sam Rushing', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
# latex_use_parts = False

# If true, show page references after internal links.
# latex_show_pagerefs = False

# If true, show URL addresses after external links.
# latex_show_urls = False

# Documents to append as an appendix to all manuals.
# latex_appendices = []

# If false, no module index is generated.
# latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'shrapnel', u'Shrapnel Documentation',
     [u'Sam Rushing'], 1)
]

# If true, show URL addresses after external links.
# man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    ('index', 'Shrapnel', u'Shrapnel Documentation',
     u'Sam Rushing', 'Shrapnel', 'One line description of project.',
     'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
# texinfo_appendices = []

# If false, no module index is generated.
# texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
# texinfo_show_urls = 'footnote'

autodoc_default_flags = ['members', 'show-inheritance']

########NEW FILE########
__FILENAME__ = proxy
# -*- Mode: Python -*-

import coro
W = coro.write_stderr

class session:
    counter = 0

    def __init__ (self, conn, addr, saddr):
        self.conn = conn
        self.addr = addr
        self.saddr = saddr
        self.id = session.counter
        session.counter += 1
        self.proxy = coro.tcp_sock()
        self.proxy.connect (saddr)
        coro.spawn (self.feed, self.conn, self.proxy, '<==')
        coro.spawn (self.feed, self.proxy, self.conn, '==>')

    def feed (self, c0, c1, dir):
        try:
            while 1:
                block = c0.recv (1000)
                W ('%s %d %r\n' % (dir, self.id, block))
                if not block:
                    break
                else:
                    c1.send (block)
        finally:
            c0.close()

def serve (saddr):
    ip, port = saddr
    s = coro.tcp_sock()
    s.bind (('0.0.0.0', port + 9000))
    s.listen (5)
    while 1:
        conn, caddr = s.accept()
        coro.spawn (session, conn, caddr, saddr)

if __name__ == '__main__':
    import sys
    if len (sys.argv) < 3:
        print 'Usage: %s <server-host> <server-port>' % sys.argv[0]
    else:
        coro.spawn (serve, (sys.argv[1], int (sys.argv[2])))
        coro.event_loop()

########NEW FILE########
__FILENAME__ = t0
import coro
import coro.backdoor
coro.spawn (coro.backdoor.serve, unix_path='/tmp/xx.bd')
coro.event_loop()

########NEW FILE########
__FILENAME__ = t1
# -*- Mode: Python -*-

import coro
import coro.backdoor

def session (conn, addr):
    while 1:
        block = conn.recv (1000)
        if not block:
            break
        else:
            conn.send (block)

def serve (port=9000):
    s = coro.tcp_sock()
    s.bind (('', port))
    s.listen (50)
    while 1:
        conn, addr = s.accept()
        coro.spawn (session, conn, addr)

if __name__ == '__main__':
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/xx.bd')
    coro.spawn (serve)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = t2
# -*- Mode: Python -*-

import coro

def client (ip='127.0.0.1', port=9000):
    global alive
    alive += 1
    try:
        s = coro.tcp_sock()
        s.connect ((ip, port))
        for i in range (10):
            s.send ('howdy there\r\n')
            assert (s.recv_exact (13) == 'howdy there\r\n')
        coro.write_stderr ('.')
        s.close()
    finally:
        alive -= 1
        if alive == 0:
            coro.write_stderr ('\ndone.\n')
            coro.set_exit()

if __name__ == '__main__':
    alive = 0
    for i in range (100):
        coro.spawn (client)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = t3
# -*- Mode: Python -*-

import coro
import coro.backdoor
import sys
import curses

import pyximport
pyximport.install()
sys.path.append ('/Users/rushing/src/cython')
import newterm

def session (conn, addr):
    while 1:
        block = conn.recv (1000)
        if not block:
            break
        else:
            conn.send (block)

def serve (port=9000):
    s = coro.tcp_sock()
    s.bind (('', port))
    s.listen (50)
    while 1:
        conn, addr = s.accept()
        coro.spawn (session, conn, addr)

if __name__ == '__main__':
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/xx.bd')
    coro.spawn (serve)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = worms
# -*- Mode: Python -*-

# http://en.wikipedia.org/wiki/ANSI_escape_code

import array
import coro
import math

W = coro.write_stderr

CSI = '\x1B['

at_format = CSI + '%d;%dH%s'  # + CSI + '0m'

def at (x, y, ch):
    return at_format % (y, x, ch)

import random

class arena:
    def __init__ (self, w=150, h=53):
        self.w = w
        self.h = h
        self.data = [array.array ('c', " " * w) for y in range (h)]
        # put some walls around the outside
        for i in range (w):
            self.data[0][i] = '='
            self.data[-1][i] = '='
        for j in range (h):
            self.data[j][0] = '|'
            self.data[j][-1] = '|'
        self.data[0][0] = '+'
        self.data[0][-1] = '+'
        self.data[-1][0] = '+'
        self.data[-1][-1] = '+'
        self.worms = []
        self.listeners = []

    def random_pos (self):
        while 1:
            x = random.randrange (1, self.w - 1)
            y = random.randrange (1, self.h - 1)
            if self[x, y] == ' ':
                break
        return x, y

    def render (self):
        return '\n'.join (x.tostring() for x in self.data)

    def __getitem__ (self, (x, y)):
        return self.data[y][x]

    def draw (self, pos, chr='*'):
        x, y = pos
        if self.data[y][x] in "=|+":
            import pdb
            pdb.set_trace()
        self.data[y][x] = chr
        for lx in self.listeners:
            lx.draw (pos, chr)

    def erase (self, pos):
        x, y = pos
        self.data[y][x] = ' '
        for lx in self.listeners:
            lx.erase (pos)

    def populate (self, n=5):
        for i in range (n):
            x, y = self.random_pos()
            self.worms.append (worm (self, x, y))

    def cull (self, hoffa=False):
        removed = []
        for worm in self.worms:
            if worm.stunned:
                W ('culling worm %r\n' % (worm.chr,))
                worm.kill (hoffa)
                removed.append (worm)
        for r in removed:
            self.worms.remove (r)

class worm:

    counter = 0

    # up down left right
    movex = [0, 0, -1, 1]
    movey = [-1, 1, 0, 0]

    def __init__ (self, arena, x, y, length=10):
        self.arena = arena
        self.head = x, y
        self.tail = []
        self.dir = random.randrange (0, 4)
        self.length = length
        worm.counter += 1
        self.chr = '0123456789abcdefghijklmnopqrstuvwxyz'[worm.counter % 36]
        self.speed = random.randrange (200, 400)
        self.exit = False
        self.stunned = False
        W ('new worm %r @ %d, %d speed=%d\n' % (self.chr, x, y, self.speed))
        coro.spawn (self.go)

    def go (self):
        while not self.exit:
            coro.sleep_relative (self.speed / 10000.0)
            if random.randrange (0, 20) == 10:
                if not self.turn():
                    return
            else:
                nx, ny = self.update()
                while self.arena[(nx, ny)] != ' ':
                    if not self.turn():
                        return
                    nx, ny = self.update()
                self.move ((nx, ny))

    def update (self):
        x, y = self.head
        return (
            x + self.movex[self.dir],
            y + self.movey[self.dir]
        )

    def turn (self):
        while not self.exit:
            x, y = self.head
            a = self.arena
            choices = []
            for i in range (4):
                nx = x + self.movex[i]
                ny = y + self.movey[i]
                if a[nx, ny] == ' ':
                    choices.append (i)
            if not choices:
                return self.stun()
            else:
                self.dir = random.choice (choices)
                return True

    def stun (self):
        self.stunned = True
        for pos in self.tail:
            self.arena.draw (pos, '*')
        coro.sleep_relative (5)
        self.stunned = False
        return not self.exit

    def move (self, pos):
        self.tail.append (self.head)
        self.head = pos
        self.arena.draw (pos, self.chr)
        if len (self.tail) > self.length:
            tpos = self.tail.pop (0)
            self.arena.erase (tpos)

    def kill (self, hoffa=True):
        self.arena.erase (self.head)
        for pos in self.tail:
            if hoffa:
                self.arena.draw (pos, '#')
            else:
                self.arena.erase (pos)
        self.exit = True
        self.head = (None, None)
        self.tail = []

class terminal:
    def __init__ (self, conn):
        self.conn = conn
        self.conn.send (
            '\xff\xfc\x01'  # IAC WONT ECHO
            '\xff\xfb\x03'  # IAC WILL SUPPRESS_GO_AHEAD
            '\xff\xfc"'     # IAC WONT LINEMODE
        )
        # turn off the cursor
        self.conn.send (CSI + '?25l')
        self.fifo = coro.fifo()
        self.redraw()
        self.t0 = coro.spawn (self.listen)
        self.t1 = coro.spawn (self.writer)
        self.exit = False

    def redraw (self):
        self.fifo.push (''.join ([
            # clear the screen
            CSI + '2J',
            # move to home
            CSI + '1;1H',
            # draw the arena
            the_arena.render(),
            '\n keys: [q]uit [r]edraw [n]ew [c]ull [l]engthen [h]offa\n',
        ]))

    def draw (self, (x, y), chr):
        chr = (
            CSI
            + '%dm' % (40 + ord(chr) % 8,)
            + CSI
            + '%dm' % (30 + ord(chr) % 7,)
            + chr
            + CSI
            + '0m'
        )
        self.fifo.push (at (x + 1, y + 1, chr))

    def erase (self, (x, y)):
        self.fifo.push (at (x + 1, y + 1, ' '))

    def writer (self):
        while not self.exit:
            data = self.fifo.pop()
            if data is None:
                break
            else:
                self.conn.send (data)

    def listen (self):
        while not self.exit:
            byte = self.conn.recv (1)
            if byte == '\xff':
                # telnet stuff, dump it
                self.conn.recv (2)
            elif byte == 'r':
                self.redraw()
            elif byte == 'n':
                the_arena.populate (1)
            elif byte == 'c':
                the_arena.cull()
            elif byte == 'l':
                for worm in the_arena.worms:
                    worm.length += 1
            elif byte == 'h':
                the_arena.cull (hoffa=True)
            elif byte == 'q':
                self.exit = True
                # turn the cursor back on...
                self.conn.send (CSI + '?25h')
        the_arena.listeners.remove (self)
        self.conn.close()
        self.fifo.push (None)

def status():
    while 1:
        coro.sleep_relative (2)
        coro.write_stderr ('%5d worms %5d threads\n' % (len(the_arena.worms), len(coro.all_threads)))

def serve():
    s = coro.tcp_sock()
    s.bind (('', 9001))
    s.listen (5)
    coro.write_stderr ('Try "telnet localhost 9001" to watch the worms!\n')
    while 1:
        c, a = s.accept()
        t = terminal (c)
        the_arena.listeners.append (t)

if __name__ == '__main__':
    import coro.backdoor
    the_arena = arena()
    the_arena.populate (10)
    coro.spawn (status)
    coro.spawn (serve)
    coro.spawn (coro.backdoor.serve, unix_path='/tmp/xx.bd')
    # import coro.profiler
    # coro.profiler.go (coro.event_loop)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = backdoor
# -*- Mode: Python -*-

# Copyright 1999, 2000 by eGroups, Inc.
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of
# eGroups not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# EGROUPS DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL EGROUPS BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

VERSION_STRING = '$Id$'

import coro

import socket
import string
import cStringIO
import sys
import traceback
import types
import os

# Originally, this object implemented the file-output api, and set
# sys.stdout and sys.stderr to 'self'.  However, if any other
# coroutine ran, it would see the captured definition of sys.stdout,
# and would send its output here, instead of the expected place.  Now
# the code captures all output using StringIO.  A little less
# flexible, a little less efficient, but much less surprising!
# [Note: this is exactly the same problem addressed by Scheme's
#  dynamic-wind facility]

class backdoor:

    def __init__ (self, socket, line_separator='\r\n', welcome_message=None, global_dict=None):
        self.socket = socket
        self.buffer = ''
        self.lines = []
        self.multilines = []
        self.line_separator = line_separator
        self.welcome_message = welcome_message
        self.global_dict = global_dict

        # allow the user to change the prompts:
        if 'ps1' not in sys.__dict__:
            sys.ps1 = '>>> '
        if 'ps2' not in sys.__dict__:
            sys.ps2 = '... '

    def send (self, data):
        try:
            self.socket.send (data)
        except:
            pass

    def prompt (self):
        if self.multilines:
            self.send (sys.ps2)
        else:
            self.send (sys.ps1)

    def read_line (self):
        if self.lines:
            l = self.lines[0]
            self.lines = self.lines[1:]
            return l
        else:
            while not self.lines:
                block = self.socket.recv (8192)
                if not block:
                    return None
                elif block == '\004':
                    self.socket.close()
                    return None
                else:
                    self.buffer = self.buffer + block
                    lines = string.split (self.buffer, self.line_separator)
                    for l in lines[:-1]:
                        self.lines.append (l)
                    self.buffer = lines[-1]
            return self.read_line()

    def send_welcome_message(self):
        self.send ('Python ' + sys.version + self.line_separator)
        self.send (sys.copyright + self.line_separator)
        if self.welcome_message is not None:
            # make '\n' into the right line separator and terminate with
            # a line separator
            lines = string.split(self.welcome_message, '\n')
            if lines[-1] != '':
                lines.append('')
            self.send(string.join(lines, self.line_separator))

    def read_eval_print_loop (self):
        self.send_welcome_message()

        if self.global_dict is None:
            # this does the equivalent of 'from __main__ import *'
            env = sys.modules['__main__'].__dict__.copy()
        else:
            env = self.global_dict.copy()

        # Some of Python's special values (such as __ispkg__)
        # can cause problems when inherited from __main__.
        # To be on the safe side, we'll only include the things that we care
        # about.
        for name, value in env.items():
            if (name.startswith('__') and
                    name.endswith('__') and
                    name not in ('__builtins__',)):
                del env[name]
        env['__name__'] = '__backdoor__'

        while True:
            self.prompt()
            line = self.read_line()
            if line is None:
                break
            elif self.multilines:
                self.multilines.append(line)
                if line == '':
                    code = string.join(self.multilines, '\n')
                    self.parse(code, env)
                    # we do this after the parsing so parse() knows not to do
                    # a second round of multiline input if it really is an
                    # unexpected EOF
                    self.multilines = []
            else:
                self.parse(line, env)

    def parse(self, line, env):
        save = sys.stdout, sys.stderr
        output = cStringIO.StringIO()
        try:
            try:
                sys.stdout = sys.stderr = output
                co = compile (line, repr(self), 'eval')
                result = eval (co, env)
                if result is not None:
                    print repr(result)
                    env['_'] = result
            except SyntaxError:
                try:
                    co = compile (line, repr(self), 'exec')
                    exec co in env
                except SyntaxError as msg:
                    # this is a hack, but it is a righteous hack:
                    if not self.multilines and msg[0] == 'unexpected EOF while parsing':
                        self.multilines.append(line)
                    else:
                        traceback.print_exc()
                except:
                    traceback.print_exc()
            except:
                traceback.print_exc()
        finally:
            sys.stdout, sys.stderr = save
            self.send (output.getvalue())
            del output

def client (conn, addr, welcome_message=None, global_dict=None):
    b = backdoor (conn, welcome_message=welcome_message, global_dict=global_dict)
    b.read_eval_print_loop()

def serve (port=None, ip='127.0.0.1', unix_path=None, welcome_message=None, global_dict=None):
    import errno
    if unix_path:
        try:
            os.remove(unix_path)
        except OSError as why:
            if why[0] == errno.ENOENT:
                pass
            else:
                raise
        try:
            s = coro.make_socket (socket.AF_UNIX, socket.SOCK_STREAM)
            s.bind(unix_path)
        except OSError:
            coro.print_stderr('Error starting up backdoor on unix socket %s\n' % unix_path)
            raise
        coro.print_stderr('Backdoor started on unix socket %s\n' % unix_path)
    else:
        s = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
        s.set_reuse_addr()
        if port is None:
            ports = xrange(8023, 8033)
        else:
            if isinstance(port, types.IntType):
                ports = [port]
            else:
                ports = port
        bound = 0
        for i in ports:
            try:
                s.bind ((ip, i))
                bound = 1
                break
            except (OSError, socket.error) as why:
                if why[0] != errno.EADDRINUSE:
                    raise OSError(why)
        if not bound:
            raise Exception("couldn't bind a port (try not specifying a port)")
        coro.print_stderr('Backdoor started on port %d\n' % i)
    s.listen (1024)
    while True:
        conn, addr = s.accept()
        coro.print_stderr ('incoming connection from %r\n' % (conn.getsockname(),))
        thread = coro.spawn (client, conn, addr, welcome_message, global_dict)
        thread.set_name('backdoor session')

if __name__ == '__main__':
    thread = coro.spawn (serve, welcome_message='Testing backdoor.py')
    thread.set_name('backdoor')
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = bdc
# telnet client
#
# Usage : telnetclient.py hostname portnumber
#
# Programmed by Gang Seong Lee
# 2000.3.20

from telnetlib import Telnet
import time
import sys
from threading import *
# import readline
import editline
editline.parse_and_bind ('bind -e')
import os

host = ''
port = 23

class ReaderThread(Thread):
    def __init__(self, telnet):
        self.telnet = telnet
        Thread.__init__(self)

    def run(self):
        while True:
            str = self.telnet.read_some()
            if str == '':
                break
            sys.stdout.write(str)
            sys.stdout.flush()

def main(host, port):
    telnet = Telnet()
    telnet.open(host, port)

    reader = ReaderThread(telnet)
    reader.start()

    while True:
        if not reader.isAlive():
            print 'giving up'
            break
        try:
            line = raw_input()
        except:
            break
        telnet.write(line + '\r\n')
    print '\n'
    os._exit(0)
#        telnet.close()

if __name__ == '__main__':
    try:
        host = sys.argv[1]
    except:
        pass
    try:
        port = int(sys.argv[2])
    except:
        pass
    try:
        main(host, port)
    except:
        pass
    sys.exit(0)

########NEW FILE########
__FILENAME__ = check_limits
import os
import resource

def verify_limit(name, minval):
    current_soft_value, current_hard_value = resource.getrlimit(name)
    if current_soft_value < minval:
        if current_hard_value >= minval:
            # reset it
            resource.setrlimit(name, (minval, current_hard_value))
        else:
            # root can raise the hard limit...let's try
            try:
                resource.setrlimit(name, (minval, minval))
            except ValueError:
                raise SystemError(
                    ("The ulimit '%i' has value '%d', which is less than the required '
                     "minimum value of '%d'...tried to raise but failed.") %
                    (name, current_soft_value, minval))
        # check again, just to make sure changes took
        current_soft_value, current_hard_value = resource.getrlimit(name)
        if current_soft_value < minval:
            raise SystemError(
                "The ulimit '%i' has value '%d', which is less than the required minimum value of '%d'" %
                (name, current_soft_value, minval))


# The (soft limit) values that we expect
resource_minimums = {
    resource.RLIMIT_CPU: resource.RLIM_INFINITY,
    resource.RLIMIT_FSIZE: resource.RLIM_INFINITY,
    resource.RLIMIT_DATA: 2093056,                    # 2 gigs of memory
    resource.RLIMIT_STACK: 65536,
    # we can run just fine with a corelimit...but we may want to enable this
    # or we may not, if in the future we want to set the value to 0
    #    resource.RLIMIT_CORE:   resource.RLIM_INFINITY,
    # this may not be necessary because we (AFAIK) never lock any memory
    resource.RLIMIT_MEMLOCK: resource.RLIM_INFINITY,
    resource.RLIMIT_NOFILE: 16000,
    # this may not be necessary because we do not fork very many processes
    resource.RLIMIT_NPROC: 500,
    resource.RLIMIT_RSS: resource.RLIM_INFINITY,
    resource.RLIMIT_SBSIZE: resource.RLIM_INFINITY,
}

def verify():
    if not os.environ.get("BUILDING"):
        for name, minval in resource_minimums.items():
            verify_limit (name, minval)

if __name__ == '__main__':
    verify()

########NEW FILE########
__FILENAME__ = check_sysctls
# -*- Mode: Python -*-

"""Verify that system resource values are set appropriately for heavy-duty benchmarking.
   Some of these values can be set (using 'sysctl') on a live system.  Some can be set
   in /boot/loader.conf.  Some (but not all!) can be changed in the kernel config as well.

   Our position is going to be this:
     1) prefer /boot/loader.conf over kernel config
        [this is because at least one value (TCBHASHSIZE) can't be set in the kernel
         config but *can* be set from loader.conf]
     2) prefer /etc/sysctl.conf to changing things manually

"""

import os
import sysctl
import tb

def verify_resource (name, minval):
    try:
        x = sysctl.sysctl (name, 1)
    except:
        raise SystemError("Failed to query sysctl MIB '%s': %s" % (name, tb.traceback_string()))
    else:
        if x < minval:
            raise SystemError(
                "The sysctl MIB '%s' has value '%d', which is less than the required minimum value of '%d'" %
                (name, x, minval))

resource_minimums = {
    "kern.maxfiles": 16384,        # /etc/sysctl.conf
    "kern.maxfilesperproc": 16000,        # /etc/sysctl.conf
    "kern.ipc.somaxconn": 8192,        # /etc/sysctl.conf
    "kern.ipc.nmbufs": 65536,        # /boot/loader.conf
    "kern.ipc.nmbclusters": 16384,        # /boot/loader.conf
    "net.inet.ip.portrange.last": 49151,        # /etc/sysctl.conf
    "net.inet.tcp.tcbhashsize": 16384,        # /boot/loader.conf
    "net.inet.ip.intr_queue_maxlen": 200,        # /etc/sysctl.conf
    # "machdep.tsc_freq"              :     0,        # see kernel config and rdtsc.h
}

def verify():
    if not os.environ.get("BUILDING"):
        for name, minval in resource_minimums.items():
            verify_resource (name, minval)

if __name__ == '__main__':
    verify()

########NEW FILE########
__FILENAME__ = coro_bench
# -*- Mode: Python -*-

import getrusage
import string

def format_rusage (l):
    times = map (lambda x: "%ld %ld" % x, l[:3])
    nums = (
        "maxrss:%d "
        "ixrss:%d "
        "idrss:%d "
        "isrss:%d "
        "minflt:%d "
        "majflt:%d "
        "nswap:%d "
        "inblock:%d "
        "oublock:%d "
        "msgsnd:%d "
        "msgrcv:%d "
        "nsignals:%d "
        "nvcsw:%d "
        "nivcsw:%d" % tuple(l[3:])
    )
    return string.join (times + [nums], '|')

def diff_timeval (a, b):
    r = [0, 0]
    r[0] = b[0] - a[0]
    r[1] = b[1] - a[1]
    if (r[1] < 0):
        r[0] -= 1
        r[1] += 1000000
    return tuple(r)

def diff_rusage (a, b):
    rrt, rut, rst = map (diff_timeval, a[:3], b[:3])
    return (
        rrt,
        rut,
        rst,
        b[3],           # maxrss
        b[4],           # ixrss
        b[5],           # idrss
        b[6],           # isrss
        b[7] - a[7],    # minflt
        b[8] - a[8],    # majflt
        b[9] - a[9],    # nswap
        b[10] - a[10],    # inblock
        b[11] - a[11],  # oublock
        b[12] - a[12],  # msgsnd
        b[13] - a[13],  # msgrcv
        b[14] - a[14],  # nsignals
        b[15] - a[15],  # nvcsw
        b[16] - a[16],  # nivcsw
    )

class real_timer:
    def __init__ (self):
        self.start = getrusage.getrusage()

    def mark (self):
        self.start = getrusage.getrusage()

    def bench (self):
        now = getrusage.getrusage()
        return diff_rusage (self.start, now)

def dump_stats_by_line():
    import sys
    d = sys.statistical_profiling_data.items()
    d.sort (lambda a, b: cmp(b[1], a[1]))
    for (co, line), count in d[:100]:
        print '%6d %s:%s:%s' % (count, co.co_filename, co.co_name, line)

def dump_stats_by_fun():
    import sys
    d = sys.statistical_profiling_data.items()
    d2 = {}
    for (co, line), count in d:
        n = d2.get (co, 0)
        d2[co] = count + n
    d2 = d2.items()
    d2.sort (lambda a, b: cmp(b[1], a[1]))
    for co, count in d2[:100]:
        print '%6d %s:%s' % (count, co.co_filename, co.co_name)

########NEW FILE########
__FILENAME__ = coro_fifo
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import copy
import coro

class circular_fifo:

    """circular_fifo
    This is a circular fifo.  It is just a regular FIFO with much better
    performance than a python list using append() and pop(0).
    If the FIFO becomes full, then it will automatically double the
    size to allow more data to be added.
    """

    def __init__(self, default_size=1024, data=None):
        """circular_fifo(default_size = 1024, data = None) -> circular_fifo_object
        Default size is the number of slots you want to start with.
        data can be a list of elements to seed the fifo.
        """
        self.cv = coro.condition_variable()
        if data:
            # Note that we need an extra element because
            # we need somewhere for tail to point to.
            if len(data) <= default_size:
                default_size = len(data) + 1
            self._fifo = copy.copy(data)
            self._fifo.extend([None] * (default_size - len(data)))
            self._tail = len(data)
            self._head = 0
            self._fifo_size = default_size
        else:
            self._fifo = [None] * default_size
            self._tail = 0
            self._head = 0
            self._fifo_size = default_size

    def remove(self, entry):
        """remove(entry) -> None
        Removes the given entry from the fifo.  This is an actual value
        comparison, NOT an index comparison.

        Only removes the first instance (to match list behaviour).

        Everything is shifted to fit (thus performance can be poor if
        the item is not at the head or the tail of the queue).
        Raises ValueError if it is not in the queue.
        """
        if self._tail >= self._head:
            # normal list
            end = self._tail
        else:
            # wrapping list
            end = self._fifo_size
        for x in xrange(self._head, end):
            if self._fifo[x] == entry:
                # this is the entry to remove
                self._remove(x)
                # In theory, if we want to make it remove EVERY instance,
                # then we just remove this return and put it after the for loop
                # (same for the return in the code below)
                return None
        else:
            # see if we need to wrap around
            if self._tail < self._head:
                # yes, wrap around
                for x in xrange(self._tail):
                    if self._fifo[x] == entry:
                        # this is the entry to remove
                        self._remove(x)
                        return None
        raise ValueError("x not in list")

    def _remove(self, index):
        """_remove(index) -> None
        Removes the value at the given index
        (actual index in the list, not the queue).
        Assumes you've given a valid index.
        """
        if self._tail == self._head:
            # Sanity check
            return ValueError, "fifo is empty"
        if self._tail >= self._head:
            # normal list
            del self._fifo[index]
            self._fifo.append(None)
            self._tail -= 1
            return None
        else:
            # wrapping list
            if index >= self._head:
                # towards the end
                del self._fifo[index]
                self._fifo.insert(self._head, None)
                self._head += 1
            else:
                # towards the beginning
                del self._fifo[index]
                self._fifo.insert(self._tail, None)
                self._tail -= 1

    def enqueue(self, data):
        """enqueue(data) -> None
        Adds the given element to the fifo.
        """
        self._fifo[self._tail] = data
        self._tail += 1
        if self._tail == self._fifo_size:
            self._tail = 0
        if self._tail == self._head:
            # fifo is full...expand it
            self._fifo[self._tail:self._tail] = [None] * self._fifo_size
            self._head += self._fifo_size
            self._fifo_size *= 2
        self.cv.wake_all()

    def put_on_front(self, data):
        """put_on_front(self, data) -> None
        Puts the given data on the front of the FIFO.
        """
        if self._head == 0:
            # Wrap around.
            self._head = self._fifo_size - 1
        else:
            self._head -= 1
        self._fifo[self._head] = data
        if self._head == self._tail:
            # fifo is full...expand it
            self._fifo[self._tail:self._tail] = [None] * self._fifo_size
            self._head += self._fifo_size
            self._fifo_size *= 2
        self.cv.wake_all()

    def dequeue(self):
        """dequeue() -> data
        Returns the next element in the fifo.
        Blocks if the fifo is empty until someone adds data.
        """
        if self._tail == self._head:
            self.cv.wait()
        data = self._fifo[self._head]
        self._fifo[self._head] = None
        if self._head == self._fifo_size - 1:
            self._head = 0
        else:
            self._head += 1
        return data

    def peek(self):
        """peek() -> data
        Returns the next element in the fifo without actually removing it.
        Raises IndexError if the queue is empty.
        """
        if self._tail == self._head:
            raise IndexError('peek from an empty fifo')
        return self._fifo[self._head]

    def poke(self, data):
        """poke(self, data) -> None
        Replaces the data at head of the fifo.
        """
        if self._tail == self._head:
            raise IndexError('poke on an empty fifo')
        self._fifo[self._head] = data
        self.cv.wake_all()

    def count(self, entry):
        """count(entry) -> #
        Returns the number of times entry appears in the fifo.
        """
        c = 0
        for x in self:
            if x == entry:
                c += 1
        return c

    def __len__(self):
        if self._tail >= self._head:
            return self._tail - self._head
        else:
            return (self._fifo_size - self._head) + self._tail

    def __getitem__(self, key):
        if self._tail >= self._head:
            if self._head + key >= self._tail:
                raise IndexError(key)
            return self._fifo[self._head + key]
        else:
            if self._head + key >= self._fifo_size:
                key -= (self._fifo_size - self._head)
                if key >= self._tail:
                    raise IndexError(key)
                return self._fifo[key]
            return self._fifo[self._head + key]

    def __repr__(self):
        if self._tail >= self._head:
            return str(self._fifo[self._head:self._tail])
        else:
            return str(self._fifo[self._head:self._fifo_size] + self._fifo[:self._tail])

    def as_list(self):
        """as_list() -> list
        Returns the fifo as a list (the first element is the head of
        the fifo, the last element is the tail).
        """
        if self._tail >= self._head:
            return self._fifo[self._head:self._tail]
        else:
            return self._fifo[self._head:self._fifo_size] + self._fifo[:self._tail]

    def clear(self):
        """clear(self) -> None
        Empties the fifo.
        """
        self._head = 0
        self._tail = 0

if __name__ == '__main__':

    import coro
    import random
    import sys

    q = circular_fifo()
    finished = 0
    size = 10000

    def push_pop_test():

        def gets(i):
            for x in i:
                print x

        bob = circular_fifo(default_size=10)
        bob.enqueue(1)
        bob.enqueue(2)
        bob.enqueue(3)
        bob.enqueue(4)
        bob.enqueue(5)
        bob.enqueue(6)
        print '%i:%r' % (len(bob), bob)
        gets(bob)
        bob.dequeue()
        bob.dequeue()
        print '%i:%r' % (len(bob), bob)
        gets(bob)
        bob.enqueue(7)
        bob.enqueue(8)
        bob.enqueue(9)
        print '%i:%r' % (len(bob), bob)
        gets(bob)
        bob.enqueue(10)
        print '%i:%r' % (len(bob), bob)
        gets(bob)
        bob.enqueue(11)
        print '%i:%r' % (len(bob), bob)
        gets(bob)
        bob.enqueue(12)
        print '%i:%r' % (len(bob), bob)
        gets(bob)
        bob.enqueue(13)
        print '%i:%r' % (len(bob), bob)
        gets(bob)
        bob.enqueue(14)
        print '%i:%r' % (len(bob), bob)
        gets(bob)

        # sys.exit(0)

        coro.spawn(pusher)
        coro.spawn(popper)

    def pusher():
        global finished, q, size
        total = size
        while True:
            if total == 1:
                to_do = 1
            else:
                to_do = random.randint(1, total / 2)
            total -= to_do
            for x in xrange(to_do):
                q.enqueue('some_data')
            if total == 0:
                break
            coro.sleep_relative(0)
        finished = 1

    def popper():
        global finished, q, size
        for x in xrange(size):
            while True:
                data = q.dequeue()
                if data is not None:
                    break
                if finished:
                    break
                coro.sleep_relative(0)
            coro.sleep_relative(0)

    def remove_test():
        global q, finished, size
        for x in xrange(size):
            q.enqueue(random.randint(0, 1000))
        for x in xrange(1000):
            c = q.count(x)
            while c:
                # If this raises an error, then we know something is broken
                l = len(q)
                q.remove(x)
                if len(q) != l - 1:
                    raise Exception('didn\'t adjust size properly')
                c -= 1
            if q.count(x) != 0:
                raise Exception('failed to remove all entries for %i in %r' % (x, q))
        finished = 1

    def make_remove_test2(to_remove):
        global q, finished, size
        q = circular_fifo(10)
        for x in xrange(1, 9):
            q.enqueue(x)
        q.dequeue()
        q.dequeue()
        q.dequeue()
        q.enqueue(10)
        q.enqueue(11)
        q.enqueue(12)
        print 'before: ', to_remove, q._fifo, repr(q)
        q.remove(to_remove)
        print ' after: ', to_remove, q._fifo, repr(q)
        q.enqueue(13)
        print '    13: ', to_remove, q._fifo, repr(q)

    def remove_test2():
        global finished
        make_remove_test2(4)
        make_remove_test2(5)
        make_remove_test2(11)
        make_remove_test2(12)
        finished = 1

    def do_test(test_func):
        global q, finished
        q = circular_fifo()
        finished = 0
        coro.spawn(test_func)
        while not finished:
            coro.sleep_relative(1)

    def do_tests():
        global q, finished
        do_test(push_pop_test)
        do_test(remove_test)
        do_test(remove_test2)
        coro.set_exit()

    coro.spawn(do_tests)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = coro_mysql
# -*- Mode: Python -*-

# Copyright 1999 by eGroups, Inc.
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of
# eGroups not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# EGROUPS DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL EGROUPS BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

VERSION_STRING = '$Id'

import exceptions
import math
import socket
import string
import sys

import coro

W = sys.stderr.write

MAX_RECONNECT_RETRY   = 10.0
RECONNECT_RETRY_GRAIN = 0.1
DEFAULT_RECV_SIZE     = 0x8000
MYSQL_HEADER_SIZE     = 0x4
MYSQL_END = chr(0xfe)

class InternalError (exceptions.Exception):
    pass

class error (exceptions.Exception):
    pass

# ===========================================================================
#                            Authentication
# ===========================================================================

# Note: I have ignored the stuff to support an older version of the protocol.
#
# The code is based on the file mysql-3.21.33/client/password.c
#
# The auth scheme is challenge/response.  Upon connection the server
# sends an 8-byte challenge message.  This is hashed with the password
# to produce an 8-byte response.  The server side performs an identical
# hash to verify the password is correct.

class random_state:

    def __init__ (self, seed, seed2):
        self.max_value = 0x3FFFFFFF
        self.seed = seed % self.max_value
        self.seed2 = seed2 % self.max_value
        return None

    def rnd (self):
        self.seed = (self.seed * 3 + self.seed2) % self.max_value
        self.seed2 = (self.seed + self.seed2 + 33) % self.max_value
        return float(self.seed) / float(self.max_value)

def hash_password (password):
    nr = 1345345333
    nr2 = 0x12345671
    add = 7

    for ch in password:
        if (ch == ' ') or (ch == '\t'):
            continue
        tmp = ord(ch)
        nr = nr ^ (((nr & 63) + add) * tmp) + (nr << 8)
        nr2 = nr2 + ((nr2 << 8) ^ nr)
        add = add + tmp

    return (nr & ((1 << 31) - 1), nr2 & ((1 << 31) - 1))

def scramble (message, password):
    hash_pass = hash_password (password)
    hash_mess = hash_password (message)

    r = random_state (
        hash_pass[0] ^ hash_mess[0],
        hash_pass[1] ^ hash_mess[1]
    )
    to = []

    for ch in message:
        to.append (int (math.floor ((r.rnd() * 31) + 64)))

    extra = int (math.floor (r.rnd() * 31))
    for i in range(len(to)):
        to[i] = to[i] ^ extra

    return to

# ===========================================================================
#                           Packet Protocol
# ===========================================================================

def unpacket (p):
    # 3-byte length, one-byte packet number, followed by packet data
    a, b, c, s = map (ord, p[:4])
    l = a | (b << 8) | (c << 16)

    # s is a sequence number
    return l, s

def packet (data, s=0):
    l = len(data)
    a, b, c = l & 0xff, (l >> 8) & 0xff, (l >> 16) & 0xff
    h = map (chr, [a, b, c, s])

    return string.join (h, '') + data

def n_byte_num (data, n, pos=0):
    result = 0
    for i in range(n):
        result = result | (ord(data[pos + i]) << (8 * i))

    return result

def decode_length (data, pos=0):
    n = ord(data[pos])

    if n < 251:
        return n, 1
    elif n == 251:
        return 0, 1
    elif n == 252:
        return n_byte_num (data, 2, pos + 1), 3
    elif n == 253:
        return n_byte_num (data, 3, pos + 1), 4
    else:
        # libmysql adds 6, why?
        return n_byte_num (data, 4, pos + 1), 5

# used to generate the dumps below
def dump_hex (s):
    r1 = []
    r2 = []

    for ch in s:
        r1.append (' %02x' % ord(ch))
        if (ch in string.letters) or (ch in string.digits):
            r2.append ('  %c' % ch)
        else:
            r2.append ('   ')

    return string.join (r1, ''), string.join (r2, '')

# ===========================================================================
# generic utils
# ===========================================================================

class mysql_client:

    def __init__ (self, username, password, address=('127.0.0.1', 3306),
                  debug = 0, timeout=None, connect_timeout=None):

        # remember this for reconnect
        self.username = username
        self.password = password
        self.address = address

        self._database  = None

        self._connected = 0

        self._recv_buffer = ''
        self._recv_length = 0

        self._lock = 0
        self._debug = debug
        self._timeout = timeout
        self._connect_timeout = connect_timeout

    def make_socket(self, *args, **kwargs):
        if self._debug:
            return socket.socket (*args, **kwargs)
        else:
            return coro.make_socket (*args, **kwargs)

    def connect (self):
        if (isinstance(self.address, type(''))):
            self.socket = self.make_socket(socket.AF_UNIX, socket.SOCK_STREAM)
        else:
            self.socket = self.make_socket(socket.AF_INET, socket.SOCK_STREAM)
        self.socket.connect (self.address)
        self._recv_buffer = ''
        self._recv_length = 0
        return None

    def recv(self):
        data = self.socket.recv (DEFAULT_RECV_SIZE)
        if not data:
            raise InternalError("Lost connection to MySQL server during query")
        else:
            self._recv_buffer = self._recv_buffer + data
            self._recv_length = self._recv_length + len(data)
        return None

    def write (self, data):
        ln = len(data)
        while data:
            n = self.socket.send (data)
            if not n:
                raise InternalError("Lost connection to MySQL server during query")
            else:
                data = data[n:]
        return ln

    debug = 0

    def send_packet (self, data, sequence=0):

        if self.debug:
            print '--> %03d' % sequence
            a, b = dump_hex (data)
            print a
            print b

        self.write (packet (data, sequence))

        return None

    def get_header(self):

        if self._recv_length < MYSQL_HEADER_SIZE:
            return None, None
        else:
            # 3-byte length, one-byte packet number, followed by packet data
            a, b, c, seq = map (ord, self._recv_buffer[:MYSQL_HEADER_SIZE])
            length = a | (b << 8) | (c << 16)

        return length, seq

    def read_packet (self):

        packet_len, seq = self.get_header()

        while (MYSQL_HEADER_SIZE > self._recv_length
               or packet_len + MYSQL_HEADER_SIZE > self._recv_length):

            self.recv()

            if packet_len is None:
                packet_len, seq = self.get_header()
        #
        # now we have at least one packet
        #
        data = self._recv_buffer[MYSQL_HEADER_SIZE:MYSQL_HEADER_SIZE + packet_len]
        self._recv_buffer = self._recv_buffer[MYSQL_HEADER_SIZE + packet_len:]
        self._recv_length = self._recv_length - (MYSQL_HEADER_SIZE + packet_len)

        return seq, data

    def login (self):

        seq, data = self.read_packet()
        # unpack the greeting
        protocol_version = ord(data[0])
        eos = string.find (data, '\000')
        mysql_version = data[1:eos]
        # thread_id = n_byte_num (data[eos+1:eos+5], 4, eos)
        thread_id = n_byte_num (data, 4, eos + 1)
        challenge = data[eos + 5:eos + 13]

        auth = (protocol_version, mysql_version, thread_id, challenge)

        lp = self.build_login_packet (challenge)
        # seems to require a sequence number of one
        self.send_packet (lp, 1)
        #
        # read the response, which will check for errors
        #
        response_tuple = self.read_reply_header()

        if response_tuple != (0, 0, 0):
            raise InternalError('unknown header response: <%r>' % (response_tuple,))

        #
        # mark that we are now connected
        #
        return None

    def check_connection(self):

        if not self._connected:

            self.connect()
            self.login()

            if self._database is not None:

                self.cmd_use(self._database)

            self._connected = 1

        return None

    def build_login_packet (self, challenge):
        auth = string.join (map (chr, scramble (challenge, self.password)), '')
        # 2 bytes of client_capability
        # 3 bytes of max_allowed_packet
        # the '5' == (LONG_PASSWORD | LONG_FLAG)
        return '\005\000\000\000\020' + self.username + '\000' + auth

    def command (self, command_type, command):
        q = chr(decode_db_cmds[command_type]) + command
        self.send_packet (q, 0)
        return None

    def unpack_data (self, d):
        r = []
        i = 0
        while i < len(d):
            fl = ord(d[i])
            if fl > 250:
                fl, scoot = decode_length (d, i)
                i = i + scoot
            else:
                i = i + 1
            r.append (d[i:i + fl])
            i = i + fl
        return r

    def unpack_int(self, data_str):
        if len(data_str) > 4:
            raise TypeError('data too long to be an int32: <%d>' % len(data_str))
        value = 0
        while len(data_str):
            i = ord(data_str[len(data_str) - 1])
            data_str = data_str[:len(data_str) - 1]
            value = value + (i << (8 * len(data_str)))
        return value

    def read_reply_header(self):
        #
        # read in the reply header and return the results.
        #
        seq, data = self.read_packet()

        rows_in_set = 0
        affected_rows = 0
        insert_id = 0

        if data[0] == chr(0xff):
            error_num = ord(data[1]) + (ord(data[2]) << 8)
            error_msg = data[3:]

            raise InternalError('ERROR %d: %s' % (error_num, error_msg))

        elif data[0] == MYSQL_END:
            raise InternalError('unknown header <%s>' % (repr(data)))
        else:

            rows_in_set, move     = decode_length(data, 0)
            data = data[move:]

            if len(data):

                affected_rows, move = decode_length(data, 0)
                data = data[move:]
                insert_id, move     = decode_length(data, 0)
                data = data[move:]

            msg = data

        return rows_in_set, affected_rows, insert_id
    #
    # Internal mysql client requests to get raw data from db (cmd_*)
    #

    def cmd_use (self, database):
        self.command ('init_db', database)

        rows, affected, insert_id = self.read_reply_header()

        if rows != 0 or affected != 0 or insert_id != 0:
            msg = 'unexpected header: <%d> <%d> <%d>' % (rows, affected, insert_id)
            raise InternalError(msg)

        self._database = database

        return None

    def cmd_query (self, query, callback=None):
        # print 'coro mysql query: "%s"' % (repr(query))
        self.command ('query', query)
        #
        # read in the header
        #
        nfields, affected, insert_id = self.read_reply_header()

        if not nfields:
            return statement([], [], affected, insert_id)

        decoders = range(nfields)
        fields = []
        i = 0

        while True:
            seq, data = self.read_packet()

            if data == MYSQL_END:
                break
            else:
                field = self.unpack_data (data)
                decoders[i] = decode_type_map[ord(field[3])]
                fields.append (field)

            i = i + 1

        if len(fields) != nfields:
            raise InternalError("number of fields did not match")

        # read rows
        rows = []
        field_range = range(nfields)

        while True:
            seq, data = self.read_packet()

            if data == MYSQL_END:
                break
            else:
                row = self.unpack_data (data)
                # apply decoders
                for i in field_range:
                    try:
                        row[i] = decoders[i](row[i])
                    except exceptions.ValueError:
                        # bob HACK.  string reps of large unsigned values
                        #     will throw a valueerror exception.
                        try:
                            row[i] = long(row[i])
                        except ValueError:
                            row[i] = None

                if callback is None:
                    rows.append(row)
                else:
                    callback(row)

        return statement(fields, rows)

    def cmd_quit (self):
        self.command ('quit', '')
        #
        # no reply!
        #
        return None

    def cmd_shutdown (self):
        self.command ('shutdown', '')
        seq, data = self.read_packet()
        return None

    def cmd_drop (self, db_name):
        self.command ('drop_db', db_name)
        nfields, affected, insert_id = self.read_reply_header()
        return None

    def cmd_listfields(self, cmd):
        self.command ('field_list', cmd)

        rows = []
        #
        # read data line until we get 255 which is error or 254 which is
        # end of data ( I think :-)
        #
        while True:

            seq, data = self.read_packet()
            #
            # terminal cases.
            #
            if data[0] == chr(0xff):
                raise InternalError(data[3:])

            elif data[0] == MYSQL_END:

                return rows
            else:

                row = self.unpack_data(data)

                table_name = row[0]
                field_name = row[1]
                field_size = self.unpack_int(row[2])
                field_type = decode_type_names[ord(row[3])]
                field_flag = self.unpack_int(row[4])
                field_val  = row[5]

                flag_value = ''

                if field_flag & decode_flag_value['pri_key']:
                    flag_value = flag_value + decode_flag_name['pri_key']

                if field_flag & decode_flag_value['not_null']:
                    flag_value = flag_value + ' ' + decode_flag_name['not_null']

                if field_flag & decode_flag_value['unique_key']:
                    flag_value = flag_value + ' ' + decode_flag_name['unique_key']

                if field_flag & decode_flag_value['multiple_key']:
                    flag_value = flag_value + ' ' + decode_flag_name['multiple_key']

                if field_flag & decode_flag_value['auto']:
                    flag_value = flag_value + ' ' + decode_flag_name['auto']
                #
                # for some reason we do not pass back the default value (row[5])
                #
                rows.append(
                    [field_name, table_name, field_type, field_size, flag_value]
                )

        return None

    def cmd_create(self, name):
        self.command ('create_db', name)
        nfields, affected, insert_id = self.read_reply_header()
        return None

    #
    # MySQL module compatibility, properly wraps raw client requests,
    # to format the return types.
    #
    def selectdb(self, database):
        return self.cmd_use (database,)

    def query (self, q):
        return self.cmd_query(q)

    def listtables (self, wildcard=None):
        if wildcard is None:
            cmd = "show tables"
        else:
            cmd = "show tables like '%s'" % (wildcard)
        return self.cmd_query(cmd).fetchrows()

    def listfields (self, table_name, wildcard=None):
        if wildcard is None:
            cmd = "%s\000\000" % (table_name)
        else:
            cmd = "%s\000%s\000" % (table_name, wildcard)
        return self.cmd_listfields (cmd)

    def drop(self, database):
        return self.cmd_drop (database)

    def create(self, db_name):
        return self.cmd_create (db_name)

    def close(self):
        return self.cmd_quit()

# compatibility layer, avoid it if you can by using cmd_query directly.
# incomplete and hackish.  perhaps a better solution would be to implement
# the DB API ourselves rather than using Mysqldb.py

class statement:

    def __init__ (self, fields, rows, affected_rows=-1, insert_id=0):
        self._fields = fields
        self._rows = rows

        if affected_rows < 0:
            self._affected_rows = len(rows)
        else:
            self._affected_rows = affected_rows

        self._index = 0
        self._insert_id = insert_id

        return None
    # =======================================================================
    # internal methods
    # =======================================================================

    def _fetchone (self):
        if self._index < len(self._rows):
            result = self._rows[self._index]
            self._index = self._index + 1
        else:
            result = []

        return result

    def _fetchmany (self, size):
        result = self._rows[self._index:self._index + size]
        self._index = self._index + len(result)

        return result

    def _fetchall (self):
        result = self._rows[self._index:]
        self._index = self._index + len(result)

        return result
    # =======================================================================
    # external methods
    # =======================================================================

    def affectedrows (self):
        return self._affected_rows

    def numrows (self):
        return len(self._rows)

    def numfields(self):
        return len(self._fields)

    def fields (self):
        # raw format:
        # table, fieldname, ??? (flags?), datatype
        # ['groupmap', 'gid', '\013\000\000', '\003', '\013B\000']
        # MySQL returns
        # ['gid', 'groupmap', 'long', 11, 'pri notnull auto_inc mkey']
        return map (lambda x: (x[1],
                               x[0],
                               decode_type_names[ord(x[3])],
                               ord(x[4][0])),
                    self._fields)

    def fetchrows(self, size=0):
        if size:
            return self._fetchmany(size)
        else:
            return self._fetchall()

    # [{'groupmap.podid': 2,
    #   'groupmap.listname': 'medusa',
    #   'groupmap.active': 'y',
    #   'groupmap.gid': 116225,
    #   'groupmap.locked': 'n'}]
    def fetchdict (self, size=0, options=0):
        if options & OPTION_SHORT:
            keys = map (lambda x: x[1], self._fields)
        else:
            keys = map (lambda x: "%s.%s" % (x[0], x[1]), self._fields)
        range_len_keys = range(len(keys))
        result = []
        for row in self.fetchrows(size):
            d = {}
            for j in range_len_keys:
                d[keys[j]] = row[j]
            result.append(d)
        return result

    def insert_id (self):
        # i have no idea what this is
        return self._insert_id

OPTION_SHORT = 1
# ======================================================================
# decoding MySQL data types
#
# from mysql-3.21.33/include/mysql_com.h.in
#
# by default leave as a string
decode_type_map = [str] * 256
decode_type_names = ['unknown'] * 256

# Many of these are not correct!  Note especially
# the time/date types... If you want to write a real decoder
# for any of these, just replace 'str' with your function.

for code, cast, name in (
    (0, float, 'decimal'),
    (1, int, 'tiny'),
    (2, int, 'short'),
    (3, int, 'long'),
    (4, float, 'float'),
    (5, float, 'double'),
    (6, str, 'null'),
    (7, str, 'timestamp'),
    # (8,                long,   'longlong'),
    (8, str, 'unhandled'),  # Mysqldb expects unhandled.  strange.
    (9, int, 'int24'),
    (10, str, 'date'),  # looks like YYYY-MM-DD ??
    (11, str, 'time'),  # looks like HH:MM:SS
    (12, str, 'datetime'),
    (13, str, 'year'),
    (14, str, 'newdate'),
    (247, str, 'enum'),
    (248, str, 'set'),
    (249, str, 'tiny_blob'),
    (250, str, 'medium_blob'),
    (251, str, 'long_blob'),
    (252, str, 'blob'),
    (253, str, 'varchar'),  # in the C code it is VAR_STRING
    (254, str, 'string')
):
    decode_type_map[code] = cast
    decode_type_names[code] = name
#
# we need flag mappings also
#
decode_flag_value = {}
decode_flag_name  = {}

for value, flag, name in (
    (1, 'not_null', 'notnull'),  # Field can not be NULL
    (2, 'pri_key', 'pri'),      # Field is part of a primary key
    (4, 'unique_key', 'ukey'),     # Field is part of a unique key
    (8, 'multiple_key', 'mkey'),     # Field is part of a key
    (16, 'blob', 'unused'),   # Field is a blob
    (32, 'unsigned', 'unused'),   # Field is unsigned
    (64, 'zerofill', 'unused'),   # Field is zerofill
    (128, 'binary', 'unused'),
    (256, 'enum', 'unused'),   # field is an enum
    (512, 'auto', 'auto_inc'),  # field is a autoincrement field
    (1024, 'timestamp', 'unused'),   # Field is a timestamp
    (2048, 'set', 'unused'),   # field is a set
    (16384, 'part_key', 'unused'),   # Intern; Part of some key
    (32768, 'group', 'unused'),   # Intern: Group field
    (65536, 'unique', 'unused')    # Intern: Used by sql_yacc
):
    decode_flag_value[flag] = value
    decode_flag_name[flag]  = name
#
# database commands
#
decode_db_cmds = {}

for value, name in (
    (0, 'sleep'),
    (1, 'quit'),
    (2, 'init_db'),
    (3, 'query'),
    (4, 'field_list'),
    (5, 'create_db'),
    (6, 'drop_db'),
    (7, 'refresh'),
    (8, 'shutdown'),
    (9, 'statistics'),
    (10, 'process_info'),
    (11, 'connect'),
    (12, 'process_kill'),
    (13, 'debug')
):
    decode_db_cmds[name] = value

# ======================================================================
##
# SMR - borrowed from daGADFLY.py, moved dict 'constant' out of
# function definition.
#
# quote_for_escape = {'\0': '\\0', "'": "''", '"': '""', '\\': '\\\\'}
# martinb - changed to match the behaviour of MySQL:
quote_for_escape = {'\0': '\\0', "'": "\\'", '"': '\\"', '\\': '\\\\'}

import types

def escape(s):
    quote = quote_for_escape
    if isinstance(s, types.IntType):
        return str(s)
    elif s is None:
        return ""
    elif isinstance(s, types.StringType):
        r = range(len(s))
        r.reverse()              # iterate backwards, so as not to destroy indexing

        for i in r:
            if s[i] in quote:
                s = s[:i] + quote[s[i]] + s[i + 1:]

        return s

    else:
        log(s)
        log (type(s))
        raise MySQLError

def test ():
    c = mysql_client ('rushing', 'fnord', ('10.1.1.55', 3306))
    print 'connecting...'
    c.connect()
    print 'logging in...'
    c.login()
    print c
    c.cmd_use ('mysql')
    for row in c.cmd_query ('select * from user').fetchrows():
        print row
    c.cmd_quit()

if __name__ == '__main__':

    import backdoor
    coro.spawn (backdoor.serve)

    for i in range(1):
        coro.spawn (test)

    coro.event_loop (30.0)
#
# - mysql_client is analogous to DBH in MySQLmodule.c, and statment is
#   analogous to STH in MySQLmodule.c
# - DBH is the database handler, and STH is the statment handler,
# - Here are the methods that the MySQLmodule.c implements, and if they
#   are at least attempted here in coromysql
#
# DBH:
#
#    "selectdb"       - yes
#    "do"             - no
#    "query"          - yes
#    "listdbs"        - no
#    "listtables"     - yes
#    "listfields"     - yes
#    "listprocesses"  - no
#    "create"         - yes
#    "stat"           - no
#    "clientinfo"     - no
#    "hostinfo"       - no
#    "serverinfo"     - no
#    "protoinfo"      - no
#    "drop"           - yes
#    "reload"         - no
#    "insert_id"      - no
#    "close"          - yes
#    "shutdown"       - no
#
# STH:
#
#    "fields"         - yes
#    "fetchrows"      - yes
#    "fetchdict"      - yes
#    "seek"           - no
#    "numrows"        - yes
#    "numfields"      - yes
#    "eof"            - no
#    "affectedrows"   - yes
#    "insert_id"      - yes

########NEW FILE########
__FILENAME__ = coro_postgres
# -*- Mode: Python -*-

VERSION_STRING = '$Id'

try:
    import coro
    GOT_CORO = 1
except:
    GOT_CORO = 0

import exceptions
import re
import socket
import string
import struct
import sys
import time
import types

if GOT_CORO:
    W = coro.print_stderr
else:
    W = sys.stderr.write

MAX_RECONNECT_RETRY   = 10.0
RECONNECT_RETRY_GRAIN = 0.1
DEFAULT_RECV_SIZE     = 0x8000
POSTGRES_HEADER_SIZE     = 0x5  # 1-byte message type; 4-byte length

# large object mode constants

INV_WRITE = 0x00020000
INV_READ = 0x00040000

SEEK_SET = 0
SEEK_CUR = 1
SEEK_END = 2

# ===========================================================================
# Protocol logging; saves data in memory to avoid changing thread
# scheduling behavior, etc.
# ===========================================================================
DATA = None
DATAP = 0
DATAX = 0

def LOG(data, seq=None):
    global DATA, DATAP, DATAX

    if DATA is None:
        DATA = [''] * 5000
    DATAX += 1
    if not seq:
        seq = DATAX

    # coro.print_stderr("%d: DATA[%d] = %s\n" % (DATAX, DATAP, data[:30]))
    DATA[DATAP] = '%d: %s' % (seq, data)
    DATAP += 1
    if DATAP >= len(DATA):
        DATAP = 0

    return seq

def DUMPLOG(n):
    import pprint
    global DATA, DATAP

    f = open(n, 'w')
    reordered = DATA[DATAP:] + DATA[:DATAP]
    pprint.pprint((DATAP, DATAX), f)
    pprint.pprint(reordered, f)
    f.close()

# ===========================================================================
# Exceptions
# ===========================================================================

class PostgresError(exceptions.Exception):
    """Base class of all exceptions raised here."""
    pass

class BackendError(PostgresError):
    """Exception sent by backend; includes error_field dictionary
    that contains details."""

    def __init__(self, msg, error_data=None):
        PostgresError.__init__(self, msg)

        if isinstance(error_data, types.DictType):
            self.error_fields = error_data.copy()
        elif error_data:
            self.error_fields = unpack_error_data(error_data)
        else:
            self.error_fields = {}

    def error_code(self):
        return self.error_fields.get(PG_SQLSTATE_FIELD)

    def __str__(self):
        return "%s (%s %s: %s)" % (
            self.args[0],
            self.error_fields.get(PG_SEVERITY_FIELD, 'UNKNOWN'),
            self.error_fields.get(PG_SQLSTATE_FIELD, '?????'),
            self.error_fields.get(PG_MESSAGE_FIELD, '-----'))

class ConnectError(BackendError):
    pass

class QueryError(BackendError):
    pass

class FunctionError(BackendError):
    pass


class ConnectionClosedError(PostgresError):
    pass

class InternalError(PostgresError):
    """Unexpected condition inside the library."""
    pass


# ===========================================================================
# String Quoting
# ===========================================================================

_std_split_re = re.compile('''([\x00-\x1f\\\\'\x7f-\xff]+)''')
_std_char_quote = {
    '\000': '',
    '\\': '\\\\',
    "'": "\\'",
    '\r': '\\r',
    '\n': '\\n',
    '\b': '\\b',
    '\f': '\\f',
    '\t': '\\t',
}

_array_split_re = re.compile('''([\x00-\x1f\\\\'"\x7f-\xff]+)''')  # add double quote
_array_char_quote = _std_char_quote.copy()
_array_char_quote['\\'] = '\\\\\\\\'
_array_char_quote['"'] = '\\\\"'

_copy_split_re = re.compile('''([\x00-\x1f\\\\\x7f-\xff]+)''')
_copy_char_quote = _std_char_quote.copy()
del _copy_char_quote["'"]

def _std_quote_char(c):
    quoted = _std_char_quote.get(c)
    if quoted is not None:
        return quoted
    elif ord(c) < ord(' '):
        return '\\%03o' % ord(c)
    else:
        return c

def _array_quote_char(c):
    quoted = _array_char_quote.get(c)
    if quoted is not None:
        return quoted
    elif ord(c) < ord(' '):
        return '\\\\\\\\%03o' % ord(c)
    else:
        return c

_copy_quote_char = _std_quote_char

def quote_string(s):
    return _quote_string(s, "'", _std_split_re, _std_quote_char)

def quote_string_array(s):
    return _quote_string(s, '"', _array_split_re, _array_quote_char)

def quote_string_copy(s):
    return _quote_string(s, "", _copy_split_re, _copy_quote_char)

def _quote_string(s, delim, splitter, quoter):
    parts = splitter.split(s)
    for i in xrange(1, len(parts), 2):
        # even elements are OK; odd elements need conversion
        parts[i] = ''.join(map(quoter, parts[i]))

    # reconstitute string with the proper delimiters
    return "%s%s%s" % (delim, ''.join(parts), delim)

_like_split_re = re.compile('''([%_\\\\]+)''')
def _like_quote_char(c):
    return '\\' + c

def escape_like_string(s):
    """Escape characters in an LIKE/ILIKE match string."""
    return _quote_string(s, "", _like_split_re, _like_quote_char)

def _bytea_quote_char(c):
    return r'\\%03o' % ord(c)

def quote_bytea(s):
    return _quote_string(s, "'", _std_split_re, _bytea_quote_char)


# Regex that finds all utf-8 sequences representing characters
# >= U+10000.  (Technically, this finds all 4-byte utf-8 sequences,
# which is more than is actually allowed in utf-8.  Normally, 4-byte
# sequences must start with 0xf0-0xf4.)
#
_4_byte_utf8_re = re.compile(r'[\xf0-\xf7]...')

# Unicode replacement char in utf-8
_replacement_char_utf8 = u'\ufffd'.encode('utf-8')

def scrub_utf8(s):
    """Replace characters >= U+10000 with U+FFFD.

    Related to bug 35006 (and others).  Postgres can't handle Unicode
    characters outside the Basic Multilingual Plane (>= U+10000).
    Replace them # with REPLACEMENT CHARACTER U+FFFD.

    This doesn't do a lot of checks on the original string; it is
    assumed to be valid utf-8.
    """

    return _4_byte_utf8_re.sub(_replacement_char_utf8, s)


# ===========================================================================
# Client class
# ===========================================================================

class postgres_client:
    # connection states
    DISCONNECTED = 'disconnected'
    CONNECTED = 'connected'
    COPYIN = 'copyin'

    DEFAULT_ADDRESS = '/tmp/.s.PGSQL.5432'
    DEFAULT_USER = 'pgsql'

    def __init__ (self, database, username='', password='',
                  address=None):

        self._backend_pid = 0
        self._secret_key = 0  # used for cancellations

        if username:
            self.username = username
        else:
            self.username = self.DEFAULT_USER
        self.password = password
        if address:
            self.address = address
        else:
            self.address = self.DEFAULT_ADDRESS

        self.database  = database
        self.backend_parameters = {}

        self._state = self.DISCONNECTED

        self._socket = None

        # Function to use when sending data; differs between "normal" and
        # coro sockets. This is intended to send all the data and raise
        # and exception if there was an error.
        self._sock_send = None

        self._recv_buffer = ''
        self._recv_length = 0

        self._debug = 0

    def connect(self):
        try:
            if self._state == self.DISCONNECTED:
                try:
                    self._socket, self._sock_send = self.connect_socket()
                except (socket.error, OSError, IOError) as e:
                    # problem connecting; Postgres probably isn't running
                    # convert this to a common PostgresError exception
                    raise ConnectError(str(e), sys.exc_info()[2])

                self._recv_buffer = ''
                self._recv_length = 0

                self._startup()
                self._wait_for_backend()

                self._state = self.CONNECTED
        finally:
            # If the state is still DISCONNECTED, then there was problem.
            if self._state == self.DISCONNECTED:
                self._dump_connection()

    def is_connected(self):
        try:
            return (self._socket is not None and
                    self._state != self.DISCONNECTED and
                    self._socket.getpeername() and
                    1)
        except:
            return 0

    def cancel(self):
        """Cancel current operation on this connection.

        This sends a Postgres cancel request packet on a newly-opened
        db connection.  It should not be called directly because
        without external synchronization there's no way to tell what
        operation (if any) is going to be cancelled.

        If the cancel request does something, the thread that
        made the query will get a PostgresError exception with
        code PG_ERROR_QUERY_CANCELLED."""

        if self._state != self.DISCONNECTED:
            socket, sock_send = self.connect_socket()
            try:
                data = build_message(PG_CANCELREQUEST_MSG,
                                     80877102,
                                     self._backend_pid,
                                     self._secret_key,
                                     )
                sock_send(data)
            finally:
                socket.close()

    def get_pid(self):
        """Return pid of the backend process for this connection.

        Does a database query the first time, and so the caller is
        responsible for ensuring that the connection is not in use."""

        if not self.is_connected():
            self._backend_pid = 0  # throw away cached value
            return 0
        elif not self._backend_pid:
            res = self.query('SELECT pg_backend_pid()')
            if res.ntuples > 0:
                self._backend_pid = res.getvalue(0, 0)

        return self._backend_pid

    def query(self, query):
        return self._simple_query(query)

    def query_timeout(self, query, timeout):
        """Run query with a timeout.

        If the query times out, raise TimeoutError."""

        aq = async_query(self)
        aq.start(query)  # start another thread working

        # Wait for results
        return aq.get_result(timeout)

    def analyze(self, vacuum=True, full=False, table='',
                fake_duplicate_problem=False):
        """Perform an ANALYZE statement.

        This handles the common problem where the pg_statistic table has
        gotten corrupted such that it no longer satisfies the "unique"
        constraint of its index.  This can happen if the database runs
        out of disk space.

        vacuum: whether to turn this into a VACUUM statement
        full: whether to turn this into a VACUUM FULL statement
        table: allows analyzing an individual table
        fake_duplicate_problem: for testing (see below)

        If the SQL statement fails with a violation of a unique
        constraint error, the code removes all the rows from the
        pg_statistic table and tries the command again.

        Since it's not possible to re-create the problem through any
        normal sequence, the caller can simulate the problem by passing
        fake_duplicate_problem=True.
        """

        if full:
            cmd = "VACUUM FULL"
        elif vacuum:
            cmd = "VACUUM"
        else:
            cmd = "ANALYZE"

        # Create the desired command
        sql = """%s %s""" % (cmd, table)
        try:
            if fake_duplicate_problem:
                # Do a simple analyze to ensure there's stat data
                self.query('ANALYZE')

                # Try to insert a duplicate row, which generates the
                # kind of error we're looking for.
                self.query('INSERT INTO pg_statistic SELECT * from pg_statistic LIMIT 1')
                # TODO: raise an exception in case we succeeded?

            self.query(sql)
            return
        except QueryError as e:
            if e.error_code() == PG_ERROR_UNIQUE_VIOLATION:  # Duplicate key
                # This usually means a problem with the pg_statistic table
                # Will retry below...
                P("analyze error (%r)" % (fake_duplicate_problem,))
                pass
            else:
                raise

        # Could find only duplicates, but this table usually is small,
        # and it doesn't look like ANALYZE is any faster if there is
        # existing data.
        try:
            self.query("""DELETE FROM pg_statistic""")
        except PostgresError:
            pass  # don't barf trying to fix pg_statistic

        # Retry.  May still raise an exception, but at this point,
        # there's nothing more to be done about it.
        self.query(sql)

    def putline(self, ln):
        if self._state == self.COPYIN:
            self.send_packet(PG_COPY_DATA_MSG,
                             _ByteN_arg(ln))
        else:
            raise InternalError(
                'Current state (%s) is not %s' % (self._state, self.COPYIN))

    def endcopy(self):
        if self._state == self.COPYIN:
            self.send_packet(PG_COPY_DONE_MSG)

            # now go back to processing backend messages
            self._state = self.CONNECTED
            return self._simple_query(None)
        else:
            pass  # we'll let this slide

    def lo_creat(self, mode):
        fn_oid = self._get_lo_function('lo_creat')
        return self._function_call(fn_oid, mode)

    def lo_open(self, loid, mode):
        fn_oid = self._get_lo_function('lo_open')
        return self._function_call(fn_oid, loid, mode)

    def lo_write(self, fd, data):
        fn_oid = self._get_lo_function('lowrite')
        return self._function_call(fn_oid, fd,
                                   _ByteN_arg(data))

    def lo_read(self, fd, numbytes=0):
        fn_oid = self._get_lo_function('loread')
        if numbytes <= 0:
            # read to end of file
            current_pos = self.lo_tell(fd)
            lo_end = self.lo_lseek(fd, 0, SEEK_END)
            numbytes = lo_end - current_pos

            self.lo_lseek(fd, current_pos, SEEK_SET)

        return self._function_call(fn_oid, fd, numbytes)

    def lo_lseek(self, fd, offset, whence):
        fn_oid = self._get_lo_function('lo_lseek')
        return self._function_call(fn_oid, fd, offset, whence)

    def lo_tell(self, fd):
        fn_oid = self._get_lo_function('lo_tell')
        return self._function_call(fn_oid, fd)

    def lo_close(self, fd):
        fn_oid = self._get_lo_function('lo_close')
        return self._function_call(fn_oid, fd)

    def lo_unlink(self, loid):
        fn_oid = self._get_lo_function('lo_unlink')
        return self._function_call(fn_oid, loid)

    def close(self):
        if self._state != self.DISCONNECTED:
            try:  # best effort at cleanly shutting down
                self.send_packet(PG_TERMINATE_MSG)

                # Make sure the db disconnects, so that if the caller
                # tries to (say) delete the database the system won't
                # report it in use.  (Mostly seen during testing.)
                for unused in xrange(10):
                    if self.is_connected():
                        sleep(0.1)
                    else:
                        break
            except (socket.error, OSError, IOError):
                pass

            self._dump_connection()

    def _dump_connection(self):
        """Throw away the socket, to get to a known state."""

        try:
            if self._socket:
                self._socket.close()
        except (socket.error, OSError, IOError):
            pass

        self._socket = None
        self._backend_pid = 0
        self._state = self.DISCONNECTED

    finish = close  # compatibility with libpq

    def notice_received(self, where, message_fields):
        # print where, message_fields
        pass

# Internal Routines

    def connect_socket(self):
        """Connect to database at self.address.

        Returns socket object and function to send data on the socket.
        Works in both coro and non-coro environments, and with TCP and
        Unix-domain sockets."""

        if (isinstance(self.address, type(''))):
            sock, sock_send = self.make_socket(socket.AF_UNIX,
                                               socket.SOCK_STREAM)
        else:
            sock, sock_send = self.make_socket(socket.AF_INET,
                                               socket.SOCK_STREAM)
        sock.connect (self.address)
        return sock, sock_send

    def make_socket(self, *args, **kwargs):
        """Make socket in both coro and non-coro environments.

        Returns socket, send function; the latter accounts for
        different behavior between socket.send and coro_socket.send
        by using self.sendall instead."""

        if GOT_CORO and coro.coro_is_running():
            sock = coro.make_socket(*args, **kwargs)
            sender = sock.send
        else:
            sock = socket.socket(*args, **kwargs)
            sender = self.sendall

        return (sock, sender)

    def recv(self):
        # SEQ = LOG('do recv')
        try:
            data = self._socket.recv (DEFAULT_RECV_SIZE)
        except (socket.error, OSError, IOError):
            data = None
        # LOG(data, SEQ)

        if not data:
            raise ConnectionClosedError
        else:
            self._recv_buffer = self._recv_buffer + data
            self._recv_length = self._recv_length + len(data)
        return None

    def write (self, data):
        try:
            # SEQ = LOG(data)
            self._sock_send(data)
            # LOG('done send', SEQ)
        except (socket.error, OSError, IOError):
            raise ConnectionClosedError

    def sendall(self, data):
        """Send all the data over a (normal) socket.

        This mirrors the behavior of socket.send for coro_sockets."""

        while data:
            n = self._socket.send(data)
            if n:
                data = data[n:]
            else:
                raise ConnectionClosedError

    debug = 0

    def send_packet (self, message_type, *data_args):
        data = build_message(message_type, *data_args)

        if self.debug:
            print '-->', message_type
            a, b = dump_hex(data)
            print a
            print b

        self.write(data)

        return None

    def get_header(self):
        if self._recv_length < POSTGRES_HEADER_SIZE:
            return None, None
        else:
            # 1-byte message type, 4-byte length
            message_type = self._recv_buffer[0]
            length = struct.unpack('!i',
                                   self._recv_buffer[1:POSTGRES_HEADER_SIZE])[0]

            return message_type, length - 4  # return length of data only

    def read_packet(self):
        msg, length = self.get_header()

        while (POSTGRES_HEADER_SIZE > self._recv_length
               or length + POSTGRES_HEADER_SIZE > self._recv_length):

            self.recv()

            if length is None:
                msg, length = self.get_header()

        #
        # now we have at least one packet
        #
        data = self._recv_buffer[POSTGRES_HEADER_SIZE:POSTGRES_HEADER_SIZE + length]
        self._recv_buffer = self._recv_buffer[POSTGRES_HEADER_SIZE + length:]
        self._recv_length = self._recv_length - (POSTGRES_HEADER_SIZE + length)

        if self.debug:
            print "<--", msg, length, len(data)
            a, b = dump_hex(data)
            print a
            print b

        return msg, data

    def _startup(self):
        """Implement startup phase of Postgres protocol"""

        # send startup packet
        self.send_packet(PG_STARTUP_MSG,
                         0x00030000,  # protocol version
                         'user', self.username,
                         'database', self.database,
                         ''  # terminate options
                         )

        msg, data = self.read_packet()
        if msg == PG_AUTHENTICATION_OK_MSG:
            pass  # no password needed
        elif msg == PG_ERROR_MSG:
            raise ConnectError("_startup", data)
        else:
            raise ConnectError("Authentication required (%d)" % msg)

    def _wait_for_backend(self):
        """Wait for the backend to be ready for a query"""

        while True:
            msg, data = self.read_packet()

            if msg == PG_READY_FOR_QUERY_MSG:
                return

            elif msg == PG_BACKEND_KEY_DATA_MSG:
                self._backend_pid, self._secret_key = unpack_data(data,
                                                                  'ii')
# print "pid=%d, secret=%d" % (self._backend_pid,
# self._secret_key)

            elif msg == PG_PARAMETER_STATUS_MSG:
                k, v = unpack_data(data, 'ss')
                self._set_parameter(k, v)

            elif msg == PG_ERROR_MSG:
                raise ConnectError("_wait_for_backend", data)

            elif msg == PG_NOTICE_MSG:
                self._notice(BACKEND_START_NOTICE, data)

            else:
                continue  # ignore packet?

    def _simple_query(self, query):
        """Execute a simple query.

        query can be None, which skips sending the query
        packet, and immediately goes to processing backend
        messages."""

        if query is not None:
            self._exit_copy_mode('New query started')
            self.send_packet(PG_QUERY_MSG, query)

        exception = None
        result = None

        while True:
            msg, data = self.read_packet()

            if msg == PG_READY_FOR_QUERY_MSG:
                break

            elif msg == PG_COMMAND_COMPLETE_MSG:
                if not result:
                    result = query_results()
                result._command_complete(data)

            elif msg == PG_COPY_IN_RESPONSE_MSG:
                self._state = self.COPYIN
                break  # exit loop so we can accept putline calls

            elif msg == PG_COPY_OUT_RESPONSE_MSG:
                if not exception:
                    exception = InternalError('COPY OUT not supported')
                break

            elif msg == PG_ROW_DESCRIPTION_MSG:
                result = query_results()
                result._row_description(data)

            elif msg == PG_DATA_ROW_MSG:
                if result:
                    result._data_row(data)
                else:
                    if not exception:
                        exception = InternalError(
                            'DataRow message before RowDescription?')

            elif msg == PG_EMPTY_QUERY_RESPONSE_MSG:
                # totally empty query (skip it?)
                continue

            elif msg == PG_PARAMETER_STATUS_MSG:
                k, v = unpack_data(data, 'ss')
                self._set_parameter(k, v)

            elif msg == PG_ERROR_MSG:
                if not exception:
                    exception = QueryError('_simple_query', data)
                    # LOG("QueryError: %r" % exception.error_fields)
                    # coro.print_stderr("QueryError: %r\n" % exception.error_fields)

            elif msg == PG_NOTICE_MSG:
                self._notice(QUERY_NOTICE, data)

            else:
                continue  # ignore packet?

        if exception:
            raise exception

        return result

    def _exit_copy_mode(self, msg='_exit_copy_mode'):
        if self._state == self.COPYIN:
            self.send_packet(PG_COPY_FAIL_MSG, msg)

    def _set_parameter(self, key, value):
        self.backend_parameters[key] = value
# print "Parameter: %s=%s" % (k, v)

    def _notice(self, where, data):
        self.notice_received(where, unpack_notice_data(data))

    def _get_lo_function(self, name):
        global _lo_functions

        if not _lo_functions:
            res = self._simple_query(
                """select proname, oid from pg_proc
                   where proname='lo_open' or
                         proname='lo_close' or
                         proname='lo_creat' or
                         proname='lo_unlink' or
                         proname='lo_lseek' or
                         proname='lo_tell' or
                         proname='loread' or
                         proname='lowrite'""")
            fns = {}
            for i in xrange(res.ntuples):
                fns[res.getvalue(i, 0)] = res.getvalue(i, 1)

            _lo_functions = fns

        fn = _lo_functions.get(name)
        if fn:
            return fn
        else:
            raise InternalError('Failed to get %s function oid' % name)

    def _function_call(self, fn_oid, *args):
        """Execute a function call."""

        # Construct the arguments for the function call message

        packet_args = [fn_oid,  # what function
                       _Int16_arg(1),  # only need 1 format type...
                       _Int16_arg(1),  # ...which is binary
                       ]

        # Number of args
        packet_args.append(_Int16_arg(len(args)))

        # Args (length followed by bytes)...
        for a in args:
            if a is None:
                packet_args.append(-1)  # special representation for NULL
            else:
                arg_data = pack_data(a)
                packet_args.extend((len(arg_data), _ByteN_arg(arg_data)))

        # Result type (binary)
        packet_args.append(_Int16_arg(1))

        self.send_packet(PG_FUNCTION_CALL_MSG, *packet_args)

        exception = None
        result = None

        while True:
            msg, data = self.read_packet()

            if msg == PG_READY_FOR_QUERY_MSG:
                break

            elif msg == PG_FUNCTION_CALL_RESPONSE:
                return_len, data = unpack_data(data, 'i', return_rest=1)

                if return_len == 2:
                    result = struct.unpack('!h', data)[0]
                elif return_len == 4:
                    result = struct.unpack('!i', data)[0]
                else:
                    result = data  # punt on decoding?

            elif msg == PG_ERROR_MSG:
                if not exception:
                    exception = FunctionError('_function_call', data)
                    # LOG("FunctionError: %r" % exception.error_fields)
                    # coro.print_stderr("FunctionError: %r\n" % exception.error_fields)

            elif msg == PG_NOTICE_MSG:
                self._notice(FUNCTION_NOTICE, data)

            else:
                continue  # ignore packet?

        if exception:
            # coro.print_stderr("exception: %r\n" % exception.error_fields)
            raise exception

        return result

_lo_functions = None


# ===========================================================================
# Query Results
# ===========================================================================

class query_results:
    def __init__(self):
        self.ntuples = 0
        self.nfields = 0
        self.cmdTuples = 0
        self.oidValue = 0

        self._field_names = []
        self._field_types = []
        self._rows = []

    def fname(self, fidx):
        if fidx >= 0 and fidx < len(self._field_names):
            return self._field_names[fidx]
        else:
            return None

    def fnumber(self, name):
        for i in xrange(len(self._field_names)):
            if self._field_names[i] == name:
                return i

        return -1

    def getvalue(self, row, col):
        r = self.getrow(row)
        if r is not None and col >= 0 and col < len(r):
            return r[col]
        else:
            return None

    def getrow(self, row):
        if row >= 0 and row < len(self._rows):
            return self._rows[row]
        else:
            return None

    def getallrows(self):
        return self._rows

    def getrowdict(self, row):
        """Get a result row as a dictionary {col name: value}"""

        r = self.getrow(row)
        if r is not None and len(r) <= len(self._field_names):
            result = {}
            for i in xrange(len(r)):
                result[self._field_names[i]] = r[i]
            return result
        else:
            return None

    def getallrowdicts(self):
        return map(self.getrowdict, xrange(self.ntuples))

    def getcolumn(self, col):
        """Get a list of all values in a given column."""

        try:
            return map(lambda x: x[col], self._rows)
        except IndexError:
            # col out of range
            return None

    def _row_description(self, data):
        self.nfields, data = unpack_data(data, 'h', return_rest=1)

        for unused in xrange(self.nfields):
            (fname, table_oid,
             colnum, ftype,
             fsize, fmod,
             format, data) = unpack_data(data, 'sihihih', return_rest=1)
            self._field_names.append(fname)
            self._field_types.append(ftype)

    def _data_row(self, data):
        nvalues, data = unpack_data(data, 'h', return_rest=1)

        new_row = [None] * self.nfields
        for i in xrange(min(self.nfields, nvalues)):
            collen, data = unpack_data(data, 'i', return_rest=1)
            if collen >= 0:
                cast = decode_type_map.get(self._field_types[i], str)
                new_row[i] = cast(data[:collen])
                data = data[collen:]

        self._rows.append(new_row)

    def _command_complete(self, data):
        # extract some info from the data
        tag = unpack_data(data, 's')[0]
        parts = tag.split(' ')

        self.oidValue = 0
        self.cmdTuples = 0

        if parts[0] == 'INSERT':
            self.oidValue = int(parts[1])
            self.cmdTuples = int(parts[2])
        elif parts[0] in ('DELETE', 'UPDATE', 'MOVE', 'FETCH'):
            self.cmdTuples = int(parts[1])

        self.ntuples = len(self._rows)


class async_query:
    """Class that encapsulates an asynchronous database query.

    Currently, this is used for implementing a timeout, but in the
    future it could be exposed at the app level, to allow a query
    to be started and the results tested some time later.

    :IVariables:
        - `_client`: instance of postgres_client, representing
          the database connection.
        - `_thread`: the thread running the query
        - `_res`: result of the query; None if not yet completed;
          and exception if the query produced an exception
        - `_cv`: condition_variable to allow a thread waiting for
          the result to be awaken.

    An instance can only run a single query, although the result
    can be read multiple times by multiple threads.
    """

    def __init__(self, client):
        self._client = client

        # It isn't necessary to hang onto the thread, but it might
        # help in debugging.
        self._thread = None

        self._res = None
        self._cv = coro.condition_variable()

    def start(self, sql):
        """Start a thread to run sql as a query."""

        assert self._thread is None
        self._thread = coro.spawn(self._run_query, sql)
        self._thread.set_name('async_query %r' % id(self))

    def _run_query(self, sql):
        """Function run in secondary thread to run db query."""

        try:
            res = self._client.query(sql)
        except PostgresError as e:
            # The query raised an exception, which becomes the "result"
            res = e

        # Set the result attribute & wake threads waiting for the answer.
        self._res = res
        self._cv.wake_all()

    def get_result(self, timeout):
        """Get query result waiting up to timeout seconds.

        If the query completes within the timeout, it will be
        returned.  If the query raises an exception, the same
        exception will be raised by this call.  If the query times
        out, this call will raise coro.TimeoutError.
        """

        # See if the query has completed already; if not
        # wait for a result up to the timeout.
        if self._res is None:
            try:
                coro.with_timeout(timeout, self._cv.wait)
            except coro.TimeoutError:
                # Timeout, cancel the request.
                self._client.cancel()

            # The request may or may not be cancelled.  Either way,
            # wait for the other thread to post a result and finish
            # using the db connection.
            while self._res is None:
                self._cv.wait()

        if isinstance(self._res, Exception):
            if (isinstance(self._res, BackendError) and
                    self._res.error_code() == PG_ERROR_QUERY_CANCELLED):
                raise coro.TimeoutError  # really did time out
            else:
                raise self._res

        # Fall through if we got an actual result
        return self._res


# ======================================================================
# Places in the code that can get notice messages
# ======================================================================

BACKEND_START_NOTICE = "wait for backend"
QUERY_NOTICE = "query"
FUNCTION_NOTICE = "function"


# ======================================================================
# Postgres message types
# ======================================================================

PG_STARTUP_MSG = ''
PG_CANCELREQUEST_MSG = ''
PG_COMMAND_COMPLETE_MSG = 'C'
PG_COPY_DONE_MSG = 'c'
PG_DATA_ROW_MSG = 'D'
PG_COPY_DATA_MSG = 'd'
PG_ERROR_MSG = 'E'
PG_FUNCTION_CALL_MSG = 'F'
PG_COPY_FAIL_MSG = 'f'
PG_COPY_IN_RESPONSE_MSG = 'G'
PG_COPY_OUT_RESPONSE_MSG = 'H'
PG_EMPTY_QUERY_RESPONSE_MSG = 'I'
PG_BACKEND_KEY_DATA_MSG = 'K'
PG_NOTICE_MSG = 'N'
PG_QUERY_MSG = 'Q'
PG_AUTHENTICATION_OK_MSG = 'R'
PG_PARAMETER_STATUS_MSG = 'S'
PG_SYNC_MSG = 'S'
PG_ROW_DESCRIPTION_MSG = 'T'
PG_FUNCTION_CALL_RESPONSE = 'V'
PG_TERMINATE_MSG = 'X'
PG_READY_FOR_QUERY_MSG = 'Z'


# ======================================================================
# Field types for Notice & Error messages
# ======================================================================

PG_SEVERITY_FIELD = 'S'
PG_SQLSTATE_FIELD = 'C'
PG_MESSAGE_FIELD = 'M'
PG_DEFAULT_FIELD = 'D'
PG_HINT_FIELD = 'H'
PG_POSITION_FIELD = 'P'
PG_WHERE_FIELD = 'W'
PG_FILE_FIELD = 'F'
PG_LINE_FIELD = 'L'
PG_ROUTINE_FIELD = 'R'


# ======================================================================
# Common Error Codes
# ======================================================================

PG_ERROR_UNIQUE_VIOLATION               = '23505'
PG_ERROR_INVALID_CATALOG_NAME           = '3D000'
PG_ERROR_DATABASE_EXISTS                = '42P04'
PG_ERROR_OBJECT_IN_USE                  = '55006'
PG_ERROR_QUERY_CANCELLED                = '57014'


# ======================================================================
# Decoding Postgres data types
# ======================================================================

def _simple_array_decode(x, fn):
    """Decode array string where the elements are guaranteed not to
    have any quoting (and there for we can just split on commas).
    Call fn on each element to convert to actual value"""

    if x[0] != '{' or x[-1] != '}':
        raise ValueError('Array not enclosed in {...}')
    x = x[1:-1]
    return map(lambda s, fn=fn: fn(s.strip()), x.split(','))

_simple_str_elt_re = re.compile(r'''([^"][^,]*),?''')
_quoted_str_elt_re = re.compile(r'''"((?:\\\\|\\"|[^"])*)",?''')
_escape_sub_re = re.compile(r'\\(.)')

def _general_array_decode(x):
    """Decode array string where elements may be quoted."""

    if x[0] != '{' or x[-1] != '}':
        raise ValueError('Array not enclosed in {...}')
    x = x[1:-1]

    result = []
    pos = 0
    while pos < len(x):
        # check for unquoted element
        match = _simple_str_elt_re.match(x, pos)
        if match:
            result.append(match.group(1))
            pos = match.end()
            continue

        # check for quoted element
        match = _quoted_str_elt_re.match(x, pos)
        if match:
            # must undo '\' escaping
            part = _escape_sub_re.sub(r'\1', match.group(1))
            result.append(part)
            pos = match.end()
            continue

        # problem
        raise ValueError('Could not parse array (%s)' % x[pos:])

    return result

def decode_bool(x):
    return x == 't' or x == 'T'

def decode_int_array(x):
    return _simple_array_decode(x, int)

def decode_str_array(x):
    return _general_array_decode(x)

# Matches \xxx and \\ where x is an octal character.
# Note that the regexp syntax requires 2 \ characters in a pattern,
# and the raw string eliminates the need to quote each of those.
_quoted_bytea_re = re.compile(r'(\\[0-7][0-7][0-7]|\\\\)')

def decode_bytea(x):
    parts = _quoted_bytea_re.split(x)
    for i in xrange(1, len(parts), 2):
        # even elements are OK; odd elements need conversion
        if parts[i] == '\\\\':  # 2 slashes...
            parts[i] = '\\'  # ...become 1
        else:
            parts[i] = chr(int(parts[i][1:], 8))

    return ''.join(parts)

#
# from src/include/catalog/pg_type.h
#
# by default leave as a string
decode_type_map = {}
decode_type_names = {}

for oid, cast, name in (
    (16, decode_bool, 'bool'),
    (17, decode_bytea, 'bytea'),
    (18, str, 'char'),
    (19, str, 'name'),
    (20, long, 'int8'),
    (21, int, 'int2'),
    (22, decode_int_array, 'int2vector'),
    (23, int, 'int4'),
    (25, str, 'text'),
    (26, int, 'oid'),
    (27, long, 'tid'),
    (28, int, 'xid'),
    (29, int, 'cid'),
    (30, str, 'oidvector'),
    (700, float, 'float4'),
    (701, float, 'float8'),
    (1007, decode_int_array, 'int4[]'),
    (1009, decode_str_array, 'text[]'),
):
    decode_type_map[oid] = cast
    decode_type_names[oid] = name


# ===========================================================================
#                           Packet Protocol
# ===========================================================================

def unpack_data(data, formats, return_rest=0):
    """Unpack the data part of a packed according to format:

    i: Int32
    h: Int16
    s: String
    c: Byte1 (returned as 1-character string)

    If return_rest is true, then return the rest of the
    data as the last item in the list.
    """

    pos = 0
    result = []

    for code in formats:
        if code == 'i':
            result.append(struct.unpack('!i', data[pos:pos + 4])[0])
            pos += 4
        elif code == 'h':
            result.append(struct.unpack('!h', data[pos:pos + 2])[0])
            pos += 2
        elif code == 'c':
            result.append(data[pos])
            pos += 1
        elif code == 's':
            i = data.find('\0', pos)
            if i < 0:
                i = len(data)

            result.append(data[pos:pos + i])
            pos += (i + 1)

    if return_rest:
        result.append(data[pos:])

    return result

def unpack_error_data(data):
    """Unpack the dictionary-like structure used in Error and Notice
    responses.  keys are 1 character codes followed by a String.  A
    '\0' key signals the end of the data."""

    pos = 0
    result = {}

    while pos < len(data):
        k = data[pos]
        if k == '\0':
            break
        pos += 1
        if not k:
            break

        i = data.find('\0', pos)
        if i < 0:
            i = len(data)

        result[k] = data[pos:i]
        pos = i + 1

    return result

unpack_notice_data = unpack_error_data

def pack_data(*data_args):
    """Pack data values into an appropriate payload.

    Each element of data_args can be:

    string: encode as String
    int: encode as Int32
    list/tuple: use the first element as a pack string
                applied to rest of the elements
    """

    parts = []
    for d in data_args:
        if isinstance(d, types.StringType):
            parts.extend((d, '\0'))  # null terminated string
        elif isinstance(d, types.IntType):
            parts.append(struct.pack('!i', d))
        elif isinstance(d, types.ListType) or isinstance(d, types.TupleType):
            parts.append(struct.pack(d[0], *d[1:]))

    return ''.join(parts)

def build_message (message_type, *data_args):
    """Build a Postgres message."""

    args = pack_data(*data_args)
    data = '%s%s%s' % (message_type,
                       struct.pack('!i', len(args) + 4),
                       args)
    return data

# helpers that can be used for args to pack_data

def _Int16_arg(x):
    if isinstance(x, types.IntType):
        return ('!h', x)
    else:
        return ('!%dh' % len(x),) + tuple(x)

def _Int32_arg(x):
    if isinstance(x, types.IntType):
        return int(x)
    else:
        return ('!%di' % len(x),) + tuple(x)

def _ByteN_arg(x):
    return ('!%ds' % len(x), x)

# used to generate the dumps below
def dump_hex (s):
    r1 = []
    r2 = []

    for ch in s:
        r1.append (' %02x' % ord(ch))
        if (ch in string.letters) or (ch in string.digits):
            r2.append ('  %c' % ch)
        else:
            r2.append ('   ')

    return string.join (r1, ''), string.join (r2, '')


# ===========================================================================
#                           Postgres Utilities
# ===========================================================================

def connect_to_db(database, username='', address=None, schema=None):
    """Utility to connect to the indicated database.

    Return instance of postgres_client if successful or None if there
    is no such database.  If schema is supplied as a string, and the
    database doesn't exist, create the database."""

    database = database.lower()
    db = postgres_client(database=database,
                         username=username,
                         address=address)
    try:
        db.connect()
        return db
    except ConnectError as e:  # doesn't exist
        if e.error_code() == PG_ERROR_INVALID_CATALOG_NAME:
            if schema is not None:
                dbm = database_manager(username=username,
                                       address=address)
                dbm.create_database(database, schema)

                # try again
                db.connect()
                return db
            else:
                return None
        else:
            raise  # some other Postgres error

class database_manager:
    def __init__(self, username='', password='',
                 address=None,
                 debug=False):
        self._db = postgres_client('template1',
                                   username=username,
                                   password=password,
                                   address=address)
        self._debug = debug

    def has_database(self, database):

        res = self._query(
            '''select datname from pg_database
               where datname=%s''' % quote_string(database))
        return res.ntuples

    def create_database(self, database, schema=None):
        delete_db = False

        try:
            self._query('CREATE DATABASE %s' % database)
            if schema:
                # If we can't install the schema, ensure db is dropped
                delete_db = True

                # connect to new database and load it up
                new_db = postgres_client(database,
                                         username=self._db.username,
                                         password=self._db.password,
                                         address=self._db.address)
                new_db.connect()
                try:
                    new_db.query(schema)
                    delete_db = False  # it's done
                finally:
                    new_db.close()
        finally:
            if delete_db:
                try:
                    self.drop_database(database)
                except coro.Interrupted:
                    raise
                except:
                    pass

    def drop_database(self, database):
        try:
            self._query('DROP DATABASE %s' % database)
        except QueryError as e:
            if e.error_code() == PG_ERROR_INVALID_CATALOG_NAME:
                pass  # this is OK
            else:
                raise

    def _query(self, sql):
        """Perform a query against the template1 database.

        create/drop database cannot be done while there are other
        connections to template1, so only maintain the connection for
        as long as necessary.

        Also, this method handles the error that occurs if >1
        connection tries to manipulate databases while there are other
        connections.  (Because there are multiple processes involved,
        it isn't possible to serialize connections to template1.)
        """

        self._db.connect()
        try:
            while True:
                try:
                    return self._db.query(sql)
                except QueryError as e:
                    if e.error_code() == PG_ERROR_OBJECT_IN_USE:
                        self._backoff()
                    else:
                        raise  # some other exception
        finally:
            self._db.close()  # close the connection

    def _backoff(self):
        """Called after finding that template1 is in use.

        Checks to see if the current connection has the lowest pid.
        If not, then closes the connection to allow the lowest
        to proceed.  This should prevent livelock where 2 threads
        continually interfere with one another."""

        my_pid = self._db.get_pid()

        while True:
            # Get smallest pid connected to template1
            res = self._db.query(
                """SELECT procpid FROM pg_stat_activity
                   WHERE datname='template1' ORDER BY procpid""")

            if res.ntuples == 0:
                # For some reason, occasionally there is nothing in
                # the pg_stat_activity table.  Try again.
                sleep(0.5)
                continue
            elif res.getvalue(0, 0) != my_pid:
                if self._debug:
                    P("%d closing" % (my_pid,))
                self._db.close()  # the other connection takes priority
                break
            else:
                # This connection has smallest pid, so remain connected.
                if self._debug:
                    P("%d continuing" % (my_pid,))
                break

        # Allow some time for other connections to finish.
        sleep(2)

        # Must be connected before returning.
        if not self._db.is_connected():
            self._db.connect()

def P(x):
    """Uses coro.print_stderr in a coro environment, else plain print."""

    if GOT_CORO and coro.coro_is_running():
        coro.print_stderr("%s\n" % x)
    else:
        print x

def sleep(x):
    """Sleep that works in both coro and non-coro environments."""

    if GOT_CORO and coro.coro_is_running():
        coro.sleep_relative(x)
    else:
        time.sleep(x)

# def test ():
#    c = mysql_client ('rushing', 'fnord', ('10.1.1.55', 3306))
# print 'connecting...'
# c.connect()
# print 'logging in...'
# c.login()
# print c
#    c.cmd_use ('mysql')
# for row in c.cmd_query ('select * from user').fetchrows():
# print row
# c.cmd_quit()

# def test_mgr(munge=0):
#    import quarantine_schema

#    dbm = database_manager()
#    sq = 'system_quarantine'

# if munge:
# schema = quarantine_schema.quarantine_schema.replace(
# 'INTO ', 'INTO zzz', 1)
# else:
#        schema = quarantine_schema.quarantine_schema
# print schema[:500]

# try:
# print dbm.has_database(sq)
# dbm.drop_database(sq)
#        dbm.create_database(sq, schema)
# except BackendError, e:
# print e.error_fields
# if 0:
#            import traceback
# traceback.print_exc()
# else:
# raise


# def make_db():
# db = postgres_client('lrosenstein', '', 'system_quarantine',
# ('127.0.0.1', 5432))
# db.connect()
# return db

# def test_open(db):
# db.query("BEGIN")
# return db.lo_open(17219, INV_READ)

# def test_read(db):
# db.query("BEGIN")
#    fd = db.lo_open(17219, INV_READ)
# print db.lo_read(fd, 50)
# print db.lo_read(fd)
# db.lo_close(fd)
# db.query("ROLLBACK")

def test_concurrent_dbm(tries):
    dbm = database_manager(debug=True)
    for i in xrange(tries):
        my_tid = coro.current().thread_id()
        db_name = "test_database_%d_%d" % (my_tid, i)
        dbm.create_database(db_name)
        dbm.drop_database(db_name)
        P("done %r" % db_name)

def watcher(thread_ids):
    while True:
        if len(thread_ids) == 0:
            coro.set_exit()
            break
        thread_ids = [x for x in thread_ids if x in coro.all_threads]
        coro.sleep_relative(0.1)

if __name__ == '__main__':

    import backdoor
    coro.spawn (backdoor.serve)

    thread_ids = []

    for i in xrange(3):
        thread_ids.append(coro.spawn(test_concurrent_dbm, 5).thread_id())
    coro.spawn(watcher, thread_ids)

    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = coro_process
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Process management for a coroutine environment.

This module provides methods for dealing with processes that will use the
kqueue facility from coro to cooperate with other coro threads.

All functions that may raise OSError will raise an errno-specific instance from
the `oserrors` module.
"""

import errno
import os
import signal

import _process
import coro
import process
from _process import DEV_NULL, PIPE, STDOUT
from process import AbnormalExit

class ProcessTimeout(Exception):

    """Process did not finish in time.

    :IVariables:
        - `stdout_output`: The output received so far.
        - `stderr_output`: The output received so far.  (None for `capture`.)
    """

    def __init__(self, stdout_output, stderr_output):
        self.stdout_output = stdout_output
        self.stderr_output = stderr_output
        Exception.__init__(self)


def capture(command, tie_out_err=True, cwd=None, env=None, timeout=0, pgrp=0):
    """Run a program in the background and capture its output.

    :Parameters:
        - `command`: The command to execute.  If it is a string, it will be
          parsed for command-line arguments.  Otherwise it assumes it is a
          sequence of arguments, with the first element being the command to
          execute.

          If the command does not contain a slash (/) it will search the PATH
          environment for the executable.
        - `tie_out_err`: If true, it will also capture output to stderr. If
          False, stderr output will go to ``/dev/null``.
        - `cwd`: Change the working directory to this path if specified before
          executing the program.
        - `env`: The environment to use.  If None, the environment is not
          changed.  May be a dictionary or a list of 'NAME=VALUE' strings.
        - `timeout`: If specified, will use a coro timeout to ensure that the
          process returns within the specified length of time.  If it does not,
          it is forcefully killed (with SIGKILL) and `ProcessTimeout` is
          raised.
        - `pgrp`: Set to -1 to keep process group unchanged, 0 to create a new
          job (default) and >0 to set process group to pgrp

    :Return:
        Returns a tuple ``(status, output)``.  Status is a
        `process.ExitStatus` instance.  Output is a string.

    :Exceptions:
        - `OSError`: Generic system error.
        - `ValueError`: The command value is invalid.
        - `ProcessTimeout`: The process did not return within `timeout`
          seconds.
    """
    if tie_out_err:
        stderr = STDOUT
    else:
        stderr = DEV_NULL
    p = spawn_job_bg(command, stdin=DEV_NULL, stdout=PIPE, stderr=stderr, cwd=cwd, env=env, pgrp=pgrp)
    status = None
    result = []

    def do_read():
        while True:
            block = p.stdout.read(1024)
            if block:
                result.append(block)
            else:
                break
        return p.wait()

    try:
        if timeout:
            status = coro.with_timeout(timeout, do_read)
        else:
            status = do_read()
    except BaseException as e:
        try:
            p.killpg(signal.SIGKILL)
        except OSError as kill_exc:
            if kill_exc.errno != errno.ESRCH:
                raise
        # Make sure we clean up the zombie.
        coro.spawn(p.wait)
        if isinstance(e, coro.TimeoutError):
            raise ProcessTimeout(''.join(result), None)
        else:
            raise

    return status, ''.join(result)

def capture_with_stderr(command, cwd=None, env=None, timeout=0, pgrp=0):
    """Run a program in the background and capture its output.

    stdout and stderr are captured independently.

    :Parameters:
        - `command`: The command to execute.  If it is a string, it will be
          parsed for command-line arguments.  Otherwise it assumes it is a
          sequence of arguments, with the first element being the command to
          execute.

          If the command does not contain a slash (/) it will search the PATH
          environment for the executable.
        - `cwd`: Change the working directory to this path if specified before
          executing the program.
        - `env`: The environment to use.  If None, the environment is not
          changed.  May be a dictionary or a list of 'NAME=VALUE' strings.
        - `timeout`: If specified, will use a coro timeout to ensure that the
          process returns within the specified length of time.  If it does not,
          it is forcefully killed (with SIGKILL) and `ProcessTimeout` is
          raised.
        - `pgrp`: Set to -1 to keep process group unchanged, 0 to create a new
          job (default) and >0 to set process group to pgrp

    :Return:
        Returns a tuple ``(status, stdout_output, stderr_output)``.  Status is an
        `ExitStatus` instance.  The outputs are strings.

    :Exceptions:
        - `OSError`: Generic system error.
        - `ValueError`: The command value is invalid.
        - `ProcessTimeout`: The process did not return within `timeout`
          seconds.
    """
    status = None
    stdout_result = []
    stderr_result = []

    finished_sem = coro.inverted_semaphore(2)

    p = spawn_job_bg(command, stdin=DEV_NULL, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env, pgrp=pgrp)

    def do_read(s, result):
        while True:
            block = s.read(1024)
            if block:
                result.append(block)
            else:
                break
        finished_sem.release()

    def do_work():
        finished_sem.block_till_zero()
        return p.wait()

    try:
        coro.spawn(do_read, p.stdout, stdout_result)
        coro.spawn(do_read, p.stderr, stderr_result)
        if timeout:
            status = coro.with_timeout(timeout, do_work)
        else:
            status = do_work()
    except BaseException as e:
        try:
            p.killpg(signal.SIGKILL)
        except OSError as kill_exc:
            if kill_exc.errno != errno.ESRCH:
                raise
        # Make sure we clean up the zombie.
        coro.spawn(p.wait)
        if isinstance(e, coro.TimeoutError):
            raise ProcessTimeout(''.join(stdout_result), ''.join(stderr_result))
        else:
            raise

    return status, ''.join(stdout_result), ''.join(stderr_result)


def spawn_job_bg(command, stdin=DEV_NULL, stdout=DEV_NULL, stderr=DEV_NULL, fd_except=None, cwd=None, env=None, pgrp=0):
    """Spawn a job into the background.

    :Parameters:
        - `command`: The command to execute.  If it is a string, it will be
          parsed for command-line arguments.  Otherwise it assumes it is a
          sequence of arguments, with the first element being the command to
          execute.

          If the command does not contain a slash (/) it will search the PATH
          environment for the executable.
        - `stdin`: Either `DEV_NULL` or `PIPE`.
        - `stdout`: Either `DEV_NULL` or `PIPE`.
        - `stderr`: Either `DEV_NULL`, `PIPE`, or `STDOUT`.
        - `fd_except`: A list of file descriptors to NOT close.  By default all
          file descriptors (except for stdin/stdout/stderr are closed).
        - `cwd`: The working directory to use for the child process (the
          default is to leave it alone).
        - `env`: The environment to use.  If None, the environment is not
          changed.  May be a dictionary or a list of 'NAME=VALUE' strings.
        - `pgrp`: Set to -1 to keep process group unchanged, 0 to create a new
          job (default) and >0 to set process group to pgrp

    :Return:
        Returns a `CoroProcess` instance.

    :Exceptions:
        - `OSError`: General low-level error.
        - `ValueError`: The command value is invalid.
    """
    pid, in_fd, out_fd, err_fd = _process.spawn_job_bg(command, stdin, stdout, stderr, fd_except, cwd, env, pgrp)
    if in_fd != -1:
        in_file = coro.fd_sock(in_fd)
    else:
        in_file = None
    if out_fd != -1:
        out_file = coro.fd_sock(out_fd)
    else:
        out_file = None
    if err_fd == out_fd:
        err_file = out_file
    elif err_fd != -1:
        err_file = coro.fd_sock(err_fd)
    else:
        err_file = None

    return CoroProcess(command, pid, in_file, out_file, err_file)

class CoroProcess(process.Process):

    def _waitpid(self, options):
        if options and options & ~os.WNOHANG:
            # Currently kqueue does not appear to support job-control notices.
            # Anyone know how to fix that?
            raise AssertionError('No options besides WNOHANG are supported for coro processes.')
        if options & os.WNOHANG:
            return os.waitpid(self.pid, os.WNOHANG)
        else:
            return coro.waitpid(self.pid)

########NEW FILE########
__FILENAME__ = coro_ssl
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Coroutine wrapper for SSL sockets.

import coro
import datafile
import socket
import sslip

init_defaults = 1
try:
    CERT = datafile.get_file('coro_ssl_data', 'demo-cert.txt')
    KEY = datafile.get_file('coro_ssl_data', 'demo-key.txt')
    KEY_PASS = datafile.get_file('coro_ssl_data', 'demo-pass.txt').strip()
    DH_PARAM_512 = datafile.get_file('coro_ssl_data', 'dh_512.pem')
    DH_PARAM_1024 = datafile.get_file('coro_ssl_data', 'dh_1024.pem')
except IOError:
    # ignore IOErrors here ... they SHOULD only occur when building a
    # frozen upgrade binary
    init_defaults = 0
    pass


if init_defaults:
    default_cert = sslip.read_pem_cert (CERT)
    default_key = sslip.read_pem_key (KEY, KEY_PASS)

    default_ctx = sslip.ssl_ctx (sslip.SSLV23_METHOD)
    default_ctx.use_cert (default_cert)
    default_ctx.use_key  (default_key)
    # diffie-hellman parameters
    default_ctx.set_tmp_dh (DH_PARAM_512)
    default_ctx.set_options (default_ctx.get_options() | sslip.SSL_OP_SINGLE_DH_USE)
    # put these two RC4 ciphers up front, they use much less CPU than 3DES
    # default_ctx.set_ciphers ('RC4-SHA:RC4-MD5:ALL')
else:
    default_cert = None
    default_key = None
    default_ctx = None

# Helper code
ssl_default_op = sslip.SSL_OP_SINGLE_DH_USE
ssl_op_map = {
    "sslv2": ssl_default_op | sslip.SSL_OP_NO_SSLv3 | sslip.SSL_OP_NO_TLSv1,
    "sslv3": ssl_default_op | sslip.SSL_OP_NO_SSLv2 | sslip.SSL_OP_NO_TLSv1,
    "tlsv1": ssl_default_op | sslip.SSL_OP_NO_SSLv2 | sslip.SSL_OP_NO_SSLv3,
    "sslv2sslv3": ssl_default_op | sslip.SSL_OP_NO_TLSv1,
    "sslv3tlsv1": ssl_default_op | sslip.SSL_OP_NO_SSLv2,
    "sslv2sslv3tlsv1": ssl_default_op,
}

def new_ssl_ctx(protocol, method, ciphers):
    new_ctx = ssl_ctx(protocol)
    new_ctx.set_options (ssl_op_map[method])
    new_ctx.set_ciphers(ciphers)
    new_ctx.set_tmp_dh (DH_PARAM_1024)
    return new_ctx

# this is a candidate for using an SSL_CTX
def check_key (cert, key, passwd='', chain=()):
    """check_key() -> cert key
    Returns 1 if key can sign certificate.
    """
    try:
        ctx = sslip.ssl_ctx (sslip.TLSV1_CLIENT_METHOD)

        cert_obj = sslip.read_pem_cert(cert)

        # Convert chain certs to cert objects
        cert_chain = []
        for c in chain:
            cert_chain.append(sslip.read_pem_cert(c))

        ctx.use_cert(cert_obj, tuple(cert_chain))

        key_obj = sslip.read_pem_key(key, passwd)
        ctx.use_key(key_obj)

        if ctx.check_key():
            return 1
        else:
            return 0
    except:
        return 0

ssl_ctx = sslip.ssl_ctx

class ssl_sock (object):

    cert = default_cert
    priv = default_key
    ctx  = default_ctx

    def __init__ (self, ctx=default_ctx):
        self.debug = 0
        self.thread_id = 0
        self.ctx = ctx

    def create (self, sock=None, verify=None):
        self.ssl = self.ctx.ssl()

        if sock is None:
            self.sock = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
        else:
            self.sock = sock

        if verify:
            self.ssl.set_verify (sslip.SSL_VERIFY_PEER)

        self.ssl.set_fd (self.sock.fileno())

    def _non_blocking_retry (self, fun, *args):
        while True:
            try:
                return fun (*args)
            except sslip.WantRead:
                self.sock.wait_for_read()
            except sslip.WantWrite:
                self.sock.wait_for_write()
        # mollify pychecker, ugh.
        return None

    def set_reuse_addr (self):
        self.sock.set_reuse_addr()

    def bind (self, addr):
        self.sock.set_reuse_addr()
        return self.sock.bind (addr)

    def listen (self, backlog):
        self.ssl.set_accept_state()
        return self.sock.listen (backlog)

    def accept (self, verify=None):
        """ ssl_sock -> ssl_sock, addr

        Protocol, unspecified, is inherited from the accepting socket's
        protocol field.  Otherwise, the supplied protocol is used for
        the new connection.
        """
        conn, addr = self.sock.accept()

        try:
            new = self.__class__(self.ctx)
            new.create (sock=conn, verify=verify)
            new.ssl.set_accept_state()
            return new, addr
        except:
            # close connection
            conn.close()
            raise

    def set_accept_state (self):
        return self.ssl.set_accept_state()

    def set_connect_state (self):
        return self.ssl.set_connect_state()

    def ssl_accept (self):
        self._non_blocking_retry (self.ssl.accept)

    def connect (self, addr):
        self.sock.connect (addr)
        self.ssl.set_connect_state()

    def ssl_connect (self):
        self._non_blocking_retry (self.ssl.connect)

    def recv (self, block_size):
        return self._non_blocking_retry (self.ssl.read, block_size)

    def recvfrom (self, block_size, timeout=30):
        raise SystemError("recvfrom not supported for SSL sockets")

    def send (self, data):
        return self._non_blocking_retry (self.ssl.write, data)

    # SSL_write() makes this guarantee.
    sendall = send

    def sendto (self, data, addr):
        raise SystemError("sendto not supported for SSL sockets")

    def writev (self, list_of_data):
        _sum = 0
        for data in list_of_data:
            _sum += self._non_blocking_retry (self.ssl.write, data)
        return _sum

    def shutdown (self):
        # TODO: this should be changed to have a 'how' argument to
        # match non-SSL sockets.
        return self._non_blocking_retry (self.ssl.shutdown)

    def close (self):
        try:
            try:
                self.shutdown()
            except coro.TimeoutError:
                pass
        finally:
            self.sock.close()

    def getCipher (self):
        return self.ssl.get_cipher()

    # The following are taken from #defines in /usr/include/openssl/*.h
    _protocol_str_map = {0x0002: 'SSLv2',  # SSL2_VERSION
                         0x0300: 'SSLv3',  # SSL3_VERSION
                         0x0301: 'TLSv1',  # TLS1_VERSION
                         }

    def getProtocol (self):
        prot_id = self.ssl.get_protocol()
        try:
            return self._protocol_str_map[prot_id]
        except KeyError:
            return '(UNKNOWN:%x)' % (prot_id,)

    # forward these other methods to the ssl socket
    def getpeername (self, *args):
        return self.sock.getpeername (*args)

    def getsockname (self, *args):
        return self.sock.getsockname (*args)

    def getsockopt (self, *args):
        return self.sock.getsockopt (*args)

    def setsockopt (self, *args):
        return self.sock.setsockopt (*args)

    def setblocking (self, flag):
        if flag:
            raise SystemError("cannot set coro socket to blocking-mode")
        else:
            # coro sockets are always in non-blocking mode.
            pass

    def settimeout (self, value):
        raise SystemError("use coro.with_timeout() rather than sock.settimeout()")

    def gettimeout (self):
        return None

    def makefile(self, mode='r', bufsize=-1):
        return socket._fileobject(self, mode, bufsize)

########NEW FILE########
__FILENAME__ = devstat
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import sysctl
import struct

# see /usr/include/sys/devicestat.h

devstat_format = (
    # struct devstat {
    'II'  # STAILQ_ENTRY(devstat)   dev_links;
    'I'   # u_int32_t               device_number;       // Devstat device number.
    '16s'  # char                    device_name[DEVSTAT_NAME_LEN];
    'i'   # int                     unit_number;
    'q'   # u_int64_t               bytes_read;          //  Total bytes read from a device.
    'q'   # u_int64_t               bytes_written;       //  Total bytes written to a device.
    'q'   # u_int64_t               bytes_freed;         //  Total bytes freed from a device.
    'q'   # u_int64_t               num_reads;           //  Total number of read requests to the device.
    'q'   # u_int64_t               num_writes;          //  Total number of write requests to the device.
    'q'   # u_int64_t               num_frees;           //  Total number of free requests to the device.
    'q'   # u_int64_t               num_other;           //  Total number of transactions that don't read or write data.
    'i'   # int32_t                 busy_count;          //  Total number of transactions outstanding for the device.
    'I'   # u_int32_t               block_size;          //  Block size, bytes
    # u_int64_t               tag_types[3];        //  The number of simple, ordered, and head of queue tags sent.
    'qqq'
    'II'  # struct timeval          dev_creation_time;   //  Time the device was created.
    'II'  # struct timeval          busy_time;           //  Total amount of time drive has spent processing requests.
    # struct timeval          start_time;          //  The time when
    # busy_count was last == 0.  Or, the start of the latest busy period.
    'II'
    'II'  # struct timeval          last_comp_time;      //  Last time a transaction was completed.
    '8s'
    #       devstat_support_flags   flags;               //  Which statistics are supported by a given device.
    #       devstat_type_flags      device_type;         //  Device type
    #       devstat_priority        priority;            //  Controls list pos.
    # };
)

def fix_zt_string (s):
    return s[:s.index('\000')]

devstat_size = struct.calcsize (devstat_format)

class devstat_struct:

    def cleanup (self):
        self.device_name = fix_zt_string (self.device_name)

    def pprint (self):
        import pprint
        pprint.pprint (self.__dict__)

def devstat():
    if sysctl.sysctl ('kern.devstat.version', 1) != 4:
        raise SystemError("devstat(9) version has changed")
    data = sysctl.sysctl ('kern.devstat.all', 0)
    devices = {}
    for i in range (0, len(data), devstat_size):
        sub = data[i:i + devstat_size]
        if len(sub) != devstat_size:
            # there are an extra four bytes tacked on to the end.
            # don't know what they're about.  padding?  They always
            # seem to be ' \000\000\000'
            break
        else:
            info = struct.unpack (devstat_format, sub)
            ds = devstat_struct()
            (ds.dev_links0, ds.dev_links1,
             device_number, ds.device_name, ds.unit_number,
             ds.bytes_read, ds.bytes_written, ds.bytes_freed,
             ds.num_reads, ds.num_writes, ds.num_frees, ds.num_other,
             ds.busy_count, ds.block_size,
             ds.tag_types0, ds.tag_types1, ds.tag_types2,
             ds.dev_creation_time0, ds.dev_creation_time1,
             ds.busy_time0, ds.busy_time1,
             ds.start_time0, ds.start_time1,
             ds.last_comp_time0, ds.last_comp_time1,
             enums
             ) = info
            ds.cleanup()
            devices[ds.device_name] = ds
    return devices

def devstat_print_all():
    d = sorted(devstat().items())
    for k, v in d:
        print '---', k, '---'
        v.pprint()

if __name__ == '__main__':
    devstat_print_all()

########NEW FILE########
__FILENAME__ = echo_server
# -*- Mode: Python -*-

# a clone of coro_in_c's 't2.c'

import coro
import coro_bench
import socket

the_timer = coro_bench.real_timer()

def service_client (conn, addr):
    while True:
        try:
            data = coro.with_timeout (10, conn.recv, 8192)
        except coro.TimeoutError:
            conn.send ('too slow, moe.  good-bye!\r\n')
            data = None
        if not data:
            conn.close()
            break
        else:
            if data[0] == '!':
                # a command
                if data == '!quit\r\n':
                    conn.send ('ok\r\n')
                    conn.close()
                    break
                elif data == '!shutdown\r\n':
                    coro.set_exit()
                    conn.send ('ok\r\n')
                    conn.close()
                    break
                elif data == '!mark\r\n':
                    the_timer.mark()
                    conn.send ('ok\r\n')
                elif data == '!bench\r\n':
                    conn.send (
                        coro_bench.format_rusage (
                            the_timer.bench()
                        ) + '\r\n\000'
                    )
                elif data == '!stats\r\n':
                    conn.send ('ok\r\n')
                    coro_bench.dump_stats()
                else:
                    conn.send ('huh?\r\n')
            else:
                conn.send (data)

def serve (port):
    s = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
    s.set_reuse_addr()
    s.bind (('', port))
    s.listen (8192)
    while True:
        conn, addr = s.accept()
        coro.spawn (service_client, conn, addr)

if __name__ == '__main__':
    import backdoor
    coro.spawn (backdoor.serve)
    coro.spawn (serve, 9001)
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = file_watcher
# -*- Mode: Python -*-

import coro
from kqueue_events import *

def register (fd, callback, what=NOTE_DELETE, once_only=1):
    # register with kqueue
    # without EV_CLEAR the event triggers repeatedly for a single write
    flags = EV_ADD | EV_CLEAR
    if once_only:
        flags |= EV_ONESHOT

    coro.set_handler ((fd, EVFILT_VNODE), callback, flags, what)

########NEW FILE########
__FILENAME__ = filesys
# -*- Mode: Python; tab-width: 4 -*-
#   $Id$
#   Author: Sam Rushing <rushing@nightmare.com>
#
# Generic filesystem interface.
#

class TempFileCreateError (Exception):
    pass

# We want to provide a complete wrapper around any and all
# filesystem operations.

# this class is really just for documentation,
# identifying the API for a filesystem object.

# opening files for reading, and listing directories, should
# return a producer.

class abstract_filesystem:
    def __init__ (self):
        pass

    def current_directory (self):
        "Return a string representing the current directory."
        pass

    def listdir (self, path, long=0, ls_filter=None):
        """Return a listing of the directory at 'path' The empty string
        indicates the current directory.  If 'long' is set, instead
        return a list of (name, stat_info) tuples
        """
        pass

    def open (self, path, mode):
        "Return an open file object"
        pass

    def tmp_create (self, path, mode):
        """tmp_create(self, path, mode) -> (filename, File Object)
        Creates a temporary file in the current directory open for writing.
        """
        pass

    def stat (self, path):
        "Return the equivalent of os.stat() on the given path."
        pass

    def isdir (self, path):
        "Does the path represent a directory?"
        pass

    def isfile (self, path):
        "Does the path represent a plain file?"
        pass

    def cwd (self, path):
        "Change the working directory."
        pass

    def cdup (self):
        "Change to the parent of the current directory."
        pass

    def longify (self, path):
        """Return a 'long' representation of the filename
        [for the output of the LIST command]"""
        pass

# standard wrapper around a unix-like filesystem, with a 'false root'
# capability.

# security considerations: can symbolic links be used to 'escape' the
# root?  should we allow it?  if not, then we could scan the
# filesystem on startup, but that would not help if they were added
# later.  We will probably need to check for symlinks in the cwd method.

# what to do if wd is an invalid directory?

import grp
import grp2
import os
import pwd
import re
import stat
import string

import external_auth

def safe_stat (path):
    try:
        return (strip_leading_directory_string(path), os.stat (path))
    except:
        return None

import glob

def collapse_double_slashes(filename_string):
    while True:
        double_slash = filename_string.count('//')
        if double_slash > 0:
            filename_string = string.replace(filename_string, '//', '/')
        else:
            return filename_string

def strip_leading_directory_string(filename_string):
    last_slash_index = filename_string.rfind('/')
    if last_slash_index > 0:
        return filename_string[last_slash_index + 1:]
    else:
        return filename_string

def get_leading_directory_string(filename_string):
    last_slash_index = filename_string.rfind('/')
    if last_slash_index > 0:
        return filename_string[0:(last_slash_index)]
    else:
        return ''


# This class can be enhanced in future to implement a full-fledged access
# control list The method check_access() is the only intended public interface
# of the class Right now this is used only by os_filesystem.rmdir(). To extend
# this class, define a series of functions of the form access_xxx(resource)
# where xxx = the type of operation intended (for example delete or list).  The
# user then invokes the method AccessChecker.check_access(resource, [ list of
# access_xxx functions ]) to check access on his resource. In future we can also
# consider using the current 'persona' while performing access checks.
#
# Ideally, the functionality of AccessChecker and ls_filter module should be
# combined. However, I am not tinkering with the ls_filter code at present.
class AccessChecker:

    _acl_delete = {'configuration': True}

    def _check_exists(self, list, resource):
        return (resource in list)

    def check_access(self, resource, access_func_list):
        """check_access(resource, <list if access control functions>) -> Boolean
        This method runs the specified resource through the list of specified
        access control functions and returns the logical AND of the result.
        The access control functions are intended to be members of this class.
        Each checks the resource for a specific type of access. Each access
        function takes the resource as its argument and returns a boolean
        depending on whether the specific type of access is allowed
        """
        for access_func in access_func_list:
            if not access_func(resource):
                return False
        return True

    def access_delete(self, resource):
        return not self._check_exists(self._acl_delete, resource)


class os_filesystem:
    path_module = os.path

    # set this to zero if you want to disable pathname globbing.
    # [we currently don't glob, anyway]
    do_globbing = 1

    def __init__ (self, root, wd='/', access_checker=None):
        self.root = root
        self.wd = wd
        if access_checker is None:
            self.access_checker = AccessChecker()
        else:
            self.access_checker = access_checker

    def current_directory (self):
        return self.wd

    def isfile (self, path):
        p = self.normalize (self.path_module.join (self.wd, path))
        return self.path_module.isfile (self.translate(p))

    def isdir (self, path):
        p = self.normalize (self.path_module.join (self.wd, path))
        return self.path_module.isdir (self.translate(p))

    def cwd (self, path):
        p = self.normalize (self.path_module.join (self.wd, path))
        translated_path = self.translate(p)
        if not self.path_module.isdir (translated_path):
            return 0
        else:
            old_dir = os.getcwd()
            # temporarily change to that directory, in order
            # to see if we have permission to do so.
            try:
                can = 0
                try:
                    os.chdir (translated_path)
                    can = 1
                    self.wd = p
                except:
                    pass
            finally:
                if can:
                    os.chdir (old_dir)
            return can

    def cdup (self):
        return self.cwd ('..')

    def listdir (self, path, long=0, ls_filter=None):
        old_dir = os.getcwd()
        translated_path = self.translate(path)
        try:
            # It was suggested that we glob...  and that we glob
            # in the current directory only.  So it shall be.
            os.chdir(self.root + self.wd)
            # Glob path
            try:
                if os.path.isdir(translated_path):
                    ld = glob.glob(translated_path + '/*')
                else:
                    ld = glob.glob(translated_path)
            except Exception:
                raise OSError('Invalid pathname')
            if ls_filter:
                ld = filter(ls_filter, ld)
            # Return the appropriate list
            if long:
                # if os.stat fails we ignore that file.
                ld = filter(None, map (safe_stat, ld))
                return list_producer(ld, 1, self.longify)
            else:
                # This list should contain only 'files'.
                ld = filter(os.path.isfile, ld)
                if (os.path.isdir(translated_path)):
                    prefix = collapse_double_slashes(path + '/')
                elif (path == '.') or (get_leading_directory_string(path) == ''):
                    prefix = ''
                else:
                    prefix = collapse_double_slashes(get_leading_directory_string(path) + '/')
                ld = map(lambda filename, prefix=prefix: prefix + strip_leading_directory_string(filename), ld)
                return list_producer(ld, 0, None)
        finally:
            os.chdir (old_dir)

    # TODO: implement a cache w/timeout for stat()
    def stat (self, path):
        p = self.translate (path)
        return os.stat (p)

    def open (self, path, mode):
        p = self.translate (path)
        return open (p, mode)

    def tmp_create (self, path, mode):
        """tmp_create(self, path, mode) -> File Object
        Creates a temporary file in the current directory.
        """
        # Create a unique temporary filename in the same directory
        tmp_path = path + '.tmp.%d' % (time.time(),)
        p = self.translate(tmp_path)
        # I'm not going to worry about filename clashes...chances are very
        # unlikely that it will happen, and if it does, the person shouldn't
        # be updating the same file multiple times in one second and this
        # provides some sort of error message to let them know they are
        # behaving poorly.

        # Do one very very limited check to catch one of the possible cases
        # where the temp file could be written but could not be renamed back
        # to the target. Even if this check falls through we still have
        # exception handling in the higher layers, after the file has already
        # been fully sent, when the rename() is actually performed. So the
        # only reason to do a preliminary, non-authoritative check here as
        # well is to prevent the file from even getting sent if we can tell
        # right away that the rename is going to fail.
        try:
            stat_info = self.stat(path)
            if stat.S_ISDIR(stat_info[stat.ST_MODE]):
                raise TempFileCreateError
        except OSError:
            # this is probably ENOENT, which would be a Good Thing
            pass

        fd = os.open(p, os.O_RDWR | os.O_CREAT | os.O_EXCL, 0o640)
        return (tmp_path, os.fdopen(fd, mode))

    def rename(self, from_path, to_path):
        from_path_t = self.translate (from_path)
        to_path_t = self.translate (to_path)
        return os.rename(from_path_t, to_path_t)

    def unlink (self, path):
        p = self.translate (path)
        return os.unlink (p)

    def mkdir (self, path):
        p = self.translate (path)
        return os.mkdir (p)

    def rmdir (self, path):
        p = self.translate (path)
        if self.access_checker:
            # Checking for basename(p) and not the entire path.
            # I am making this similar to the behavior of ls_filter
            if not self.access_checker.check_access(
                os.path.basename(p),
                [self.access_checker.access_delete]
            ):
                raise OSError
        return os.rmdir (p)

    # utility methods
    def normalize (self, path):
        # watch for the ever-sneaky '/+' path element
        path = re.sub ('/+', '/', path)
        p = self.path_module.normpath (path)
        # remove 'dangling' cdup's.
        if len(p) > 2 and p[:3] == '/..':
            p = '/'
        return p

    def translate (self, path):
        # we need to join together three separate
        # path components, and do it safely.
        # <real_root>/<current_directory>/<path>
        # use the operating system's path separator.
        path = string.join (string.split (path, '/'), os.sep)
        p = self.normalize (self.path_module.join (self.wd, path))
        p = self.normalize (self.path_module.join (self.root, p[1:]))
        return p

    def longify (self, xxx_todo_changeme):
        (path, stat_info) = xxx_todo_changeme
        return unix_longify (path, stat_info)

    def __repr__ (self):
        return '<unix-style fs root:%s wd:%s>' % (
            self.root,
            self.wd
        )

if os.name == 'posix':

    class unix_filesystem (os_filesystem):
        pass

    class schiz_wrapper:

        def __init__ (self, fun, schiz):
            self.fun = fun
            self.schiz = schiz

        def __call__ (self, *args, **kwargs):
            try:
                self.schiz.become_persona()
                return self.fun(*args, **kwargs)
            finally:
                self.schiz.become_nobody()

    class schizophrenic_unix_filesystem:
        PROCESS_UID     = os.getuid()
        PROCESS_EUID    = os.geteuid()
        PROCESS_GID     = os.getgid()
        PROCESS_EGID    = os.getegid()
        PROCESS_GROUPS  = grp2.getgroups()

        def __init__ (self, root, wd='/', persona=(None, None, None)):
            self.fs = os_filesystem(root, wd)
            self.persona = persona

        def become_persona (self):
            if self.persona is not (None, None, None):
                username, uid, gid = self.persona
                # Set the group access list.
                grp2.setgroups(external_auth.getgrouplist(username, gid))
                # don't change if it has already been done
                # the order of these is important!
                if os.getegid() != gid:
                    os.setegid (gid)
                if os.geteuid() != uid:
                    os.seteuid (uid)

        def become_nobody (self):
            if self.persona is not (None, None, None):
                os.seteuid (self.PROCESS_UID)
                os.setegid (self.PROCESS_GID)
                grp2.setgroups(self.PROCESS_GROUPS)

        def __getattr__(self, name):
            return schiz_wrapper (getattr (self.fs, name), self)

# This hasn't been very reliable across different platforms.
# maybe think about a separate 'directory server'.
#
#   import posixpath
#   import fcntl
#   import FCNTL
#   import select
#   import asyncore
#
# pipes /bin/ls for directory listings.
#   class unix_filesystem (os_filesystem):
#       pass
#       path_module = posixpath
#
#       def listdir (self, path, long=0):
#           p = self.translate (path)
#           if not long:
#               return list_producer (os.listdir (p), 0, None)
#           else:
#               command = '/bin/ls -l %s' % p
#               print 'opening pipe to "%s"' % command
#               fd = os.popen (command, 'rt')
#               return pipe_channel (fd)
#
# this is both a dispatcher, _and_ a producer
#   class pipe_channel (asyncore.file_dispatcher):
#       buffer_size = 4096
#
#       def __init__ (self, fd):
#           asyncore.file_dispatcher.__init__ (self, fd)
#           self.fd = fd
#           self.done = 0
#           self.data = ''
#
#       def handle_read (self):
#           if len (self.data) < self.buffer_size:
#               self.data = self.data + self.fd.read (self.buffer_size)
# print '%s.handle_read() => len(self.data) == %d' % (self, len(self.data))
#
#       def handle_expt (self):
# print '%s.handle_expt()' % self
#           self.done = 1
#
#       def ready (self):
# print '%s.ready() => %d' % (self, len(self.data))
#           return ((len (self.data) > 0) or self.done)
#
#       def more (self):
#           if self.data:
#               r = self.data
#               self.data = ''
#           elif self.done:
#               self.close()
#               self.downstream.finished()
#               r = ''
#           else:
#               r = None
# print '%s.more() => %s' % (self, (r and len(r)))
#           return r

# For the 'real' root, we could obtain a list of drives, and then
# use that.  Doesn't win32 provide such a 'real' filesystem?
# [yes, I think something like this "\\.\c\windows"]

class msdos_filesystem (os_filesystem):
    def longify (self, xxx_todo_changeme1):
        (path, stat_info) = xxx_todo_changeme1
        return msdos_longify (path, stat_info)

# A merged filesystem will let you plug other filesystems together.
# We really need the equivalent of a 'mount' capability - this seems
# to be the most general idea.  So you'd use a 'mount' method to place
# another filesystem somewhere in the hierarchy.

# Note: this is most likely how I will handle ~user directories
# with the http server.

class merged_filesystem:
    def __init__ (self, *fsys):
        pass

# this matches the output of NT's ftp server (when in
# MSDOS mode) exactly.

def msdos_longify (file, stat_info):
    if stat.S_ISDIR (stat_info[stat.ST_MODE]):
        dir = '<DIR>'
    else:
        dir = '     '
    date = msdos_date (stat_info[stat.ST_MTIME])
    return '%s       %s %8d %s' % (
        date,
        dir,
        stat_info[stat.ST_SIZE],
        file
    )

def msdos_date (t):
    try:
        info = time.localtime (t)
    except:
        info = time.localtime (0)
    # year, month, day, hour, minute, second, ...
    if info[3] > 11:
        merid = 'PM'
        info[3] = info[3] - 12
    else:
        merid = 'AM'
    return '%02d-%02d-%02d  %02d:%02d%s' % (
        info[1],
        info[2],
        info[0] % 100,
        info[3],
        info[4],
        merid
    )

months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

mode_table = {
    '0': '---',
    '1': '--x',
    '2': '-w-',
    '3': '-wx',
    '4': 'r--',
    '5': 'r-x',
    '6': 'rw-',
    '7': 'rwx'
}

import time

def unix_longify (file, stat_info):
    # for now, only pay attention to the lower bits
    mode = ('%o' % stat_info[stat.ST_MODE])[-3:]
    mode = string.join (map (lambda x: mode_table[x], mode), '')
    if stat.S_ISDIR (stat_info[stat.ST_MODE]):
        dirchar = 'd'
    else:
        dirchar = '-'
    date = ls_date (long(time.time()), stat_info[stat.ST_MTIME])
    try:
        username = pwd.getpwuid(stat_info[stat.ST_UID])[0]
    except KeyError:
        username = str(stat_info[stat.ST_UID])
    try:
        groupname = grp.getgrgid(stat_info[stat.ST_GID])[0]
    except KeyError:
        groupname = str(stat_info[stat.ST_GID])
    return '%s%s %3d %-8s %-8s %8d %s %s' % (
        dirchar,
        mode,
        stat_info[stat.ST_NLINK],
        username,
        groupname,
        stat_info[stat.ST_SIZE],
        date,
        file
    )

# Emulate the unix 'ls' command's date field.
# it has two formats - if the date is more than 180
# days in the past, then it's like this:
# Oct 19  1995
# otherwise, it looks like this:
# Oct 19 17:33

def ls_date (now, t):
    try:
        info = time.localtime (t)
    except:
        info = time.localtime (0)
    # 15,600,000 == 86,400 * 180
    if (now - t) > 15600000:
        return '%s %2d  %d' % (
            months[info[1] - 1],
            info[2],
            info[0]
        )
    else:
        return '%s %2d %02d:%02d' % (
            months[info[1] - 1],
            info[2],
            info[3],
            info[4]
        )

# ===========================================================================
# Producers
# ===========================================================================

class list_producer:
    def __init__ (self, file_list, long, longify):
        self.file_list = file_list
        self.long = long
        self.longify = longify
        self.done = 0

    def ready (self):
        if len(self.file_list):
            return 1
        else:
            if not self.done:
                self.done = 1
            return 0
        return (len(self.file_list) > 0)

    # this should do a pushd/popd
    def more (self):
        if not self.file_list:
            return ''
        else:
            # do a few at a time
            bunch = self.file_list[:50]
            if self.long:
                bunch = map (self.longify, bunch)
            self.file_list = self.file_list[50:]
            return string.joinfields (bunch, '\r\n') + '\r\n'

########NEW FILE########
__FILENAME__ = ftp_server
# -*- Mode: Python; tab-width: 4 -*-

#   Author: Sam Rushing <rushing@nightmare.com>
#   Copyright 1996-2000 by Sam Rushing
#                        All Rights Reserved.
#

RCS_ID = '$Id$

# An extensible, configurable, asynchronous FTP server.
#
# All socket I/O is non-blocking, however file I/O is currently
# blocking.  Eventually file I/O may be made non-blocking, too, if it
# seems necessary.  Currently the only CPU-intensive operation is
# getting and formatting a directory listing.  [this could be moved
# into another process/directory server, or another thread?]
#
# Only a subset of RFC 959 is implemented, but much of that RFC is
# vestigial anyway.  I've attempted to include the most commonly-used
# commands, using the feature set of wu-ftpd as a guide.

import coro
import crypt
import errno
import pwd
import qlog
import re
import socket
import stat
import string
import sys
import tags
import tb
import time
import read_stream

from counter import counter

# TODO: implement a directory listing cache.  On very-high-load
# servers this could save a lot of disk abuse, and possibly the
# work of computing emulated unix ls output.

# Potential security problem with the FTP protocol?  I don't think
# there's any verification of the origin of a data connection.  Not
# really a problem for the server (since it doesn't send the port
# command, except when in PASV mode) But I think a data connection
# could be spoofed by a program with access to a sniffer - it could
# watch for a PORT command to go over a command channel, and then
# connect to that port before the server does.

# Unix user id's:
# In order to support assuming the id of a particular user,
# it seems there are two options:
# 1) fork, and seteuid in the child
# 2) carefully control the effective uid around filesystem accessing
#    methods, using try/finally. [this seems to work]

class ftp_channel:

    # defaults for a reliable __repr__
    addr = ('unknown', '0')

    # unset this in a derived class in order
    # to enable the commands in 'self.write_commands'
    read_only = 1
    write_commands = ['appe', 'dele', 'mkd', 'rmd', 'rnfr', 'rnto', 'stor', 'stou']

    restart_position = 0

    # comply with (possibly troublesome) RFC959 requirements
    # This is necessary to correctly run an active data connection
    # through a firewall that triggers on the source port (expected
    # to be 'L-1', or 20 in the normal case).
    bind_local_minus_one = 0

    shutdown_flag = 0

    recv_timeout = 900
    send_timeout = 900

    # Function that can filter files from the ls commands
    # Given 1 parameter that is the name of the file
    ls_filter = None

    def __init__ (self, server, conn, addr, session_id):
        self.server = server
        self.current_mode = 'a'
        self.addr = addr
        self.conn = conn
        self.session_id = session_id
        self.thread_id = None
        # client data port.  Defaults to 'the same as the control connection'.
        self.client_addr = (addr[0], 21)
        self.in_buffer = ''
        self.closing = 0
        self.passive_acceptor = None
        self.filesystem = None
        self.authorized = 0
        self.stream = read_stream.stream_reader (self.read)
        self.user = None

    # __repr__ for app-failure logging
    def __repr__ (self):
        return (" server: %r current_mode: %s addr: %r conn: %r "
                "session_id: %r thread_id: %r closing: %r authorized: %r "
                "user: %r" % (self.server, self.current_mode, self.addr,
                              self.conn, self.session_id, self.thread_id, self.closing,
                              self.authorized, self.user))

    def read (self, size):
        return self.conn.recv (size)

    def send_file (self, s, f):
        while 1:
            # use a big block size, since this is likely to be on a fast network
            block = f.read (32768)
            if block:
                coro.with_timeout(self.send_timeout, s.send, block)
            else:
                break

    def send_with_producer (self, s, p):
        while 1:
            block = p.more()
            if block:
                coro.with_timeout (self.send_timeout, s.send, block)
            else:
                break

    def read_line_timeout (self):
        return coro.with_timeout (self.recv_timeout, self.stream.read_line)

    def send (self, data):
        return coro.with_timeout (self.send_timeout, self.conn.send, data)

    def writev (self, data):
        return coro.with_timeout (self.send_timeout, self.conn.writev, data)

    # --------------------------------------------------

    def shutdown(self, rudely=0):
        """shutdown(rudely=0) -> None
        Shuts down this session.
        Set rudely to immediately shut it down.
        """
        # TODO: Hmm...the user will just spontaneously be disconnected
        # Can we give them a disconnected error?
        self.shutdown_flag = 1
        if rudely:
            try:
                my_thread = coro.get_thread_by_id (self.thread_id)
            except KeyError:
                # thread already exited
                return
            my_thread.shutdown()

    def run (self):
        try:
            try:
                self._run()
            except coro.Shutdown:
                # We've been asked to shutdown
                return
            except:
                qlog.write('COMMON.APP_FAILURE', tb.traceback_string() + repr(self))
        finally:
            if self.user:
                qlog.write('FTPD.LOGOUT', self.session_id, self.user)
            self.close()
            self.server.session_done(self)
            # remove cycle
            del self.stream

    def get_version(self):
        return tags.version()

    def send_greeting(self):
        self.respond (
            '220 %s IronPort FTP server (V%s) ready.' % (
                self.server.hostname,
                self.get_version()
            )
        )

    def _run (self):
        self.thread_id = coro.current().thread_id()
        try:
            # send the greeting
            self.send_greeting()

            while not self.shutdown_flag:
                line, eof = self.read_line_timeout()
                if eof:
                    break
                line = orig_line = line.lstrip()

                parts = line.split()
                if len (parts) < 1:
                    self.command_not_understood ('')
                    continue
                command = parts[0].lower()
                if len(parts) > 1:
                    args = ' '.join (parts[1:])
                    # If command arguments include null character, python path parsing
                    # function will complain. Remove the null characters.
                    line = [command, args.replace('\0', '')]
                else:
                    line = [command]

                # watch especially for 'urgent' abort commands.
                if command.find ('abor') != -1:
                    # strip off telnet sync chars and the like...
                    while command and command[0] not in string.letters:
                        command = command[1:]
                fun_name = 'cmd_%s' % command
                if command != 'pass':
                    qlog.write('FTPD.RECV', self.session_id, repr(orig_line)[1:-1])
                else:
                    qlog.write('FTPD.RECV', self.session_id, line[0] + ' <password>')
                self.in_buffer = ''
                if not hasattr (self, fun_name):
                    self.command_not_understood (line[0])
                    continue
                fun = getattr (self, fun_name)
                if (not self.authorized) and (command not in ('user', 'pass', 'help', 'quit')):
                    self.respond ('530 Please log in with USER and PASS')
                elif (not self.check_command_authorization (self.user, command)):
                    self.command_not_authorized (command)
                else:
                    if hasattr (self, '_syntax_%s' % command):
                        r = getattr (self, '_syntax_%s' % command)
                        m = re.match (r, orig_line, re.IGNORECASE)
                        if m is None:
                            self.respond ('501 Syntax error in parameters or arguments')
                            continue
                    try:
                        result = apply (fun, (line,))
                    except OSError, why:
                        if why[0] in self.disconnect_errors:
                            # log it & ignore
                            qlog.write ('FTPD.DISCONNECT', self.session_id, why.strerror)
                            break
                        else:
                            raise
                    except coro.TimeoutError:
                        qlog.write('FTPD.DISCONNECT', self.session_id, 'Remote side timed out')
                        break
                    except coro.Interrupted:
                        raise
                    except:
                        self.server.total_exceptions.increment()
                        qlog.write('COMMON.APP_FAILURE', tb.traceback_string() +
                                   ' fun: ' + repr(fun) + ' line: ' + repr(line))
                        self.respond ('451 Server Error')
                        self.close_passive_acceptor()
                    else:
                        if result == 'quit':
                            break
        except read_stream.BufferOverflow:
            try:
                self.respond ('500 line too long.  good-bye')
            except coro.TimeoutError:
                pass
            except OSError:
                pass
        except coro.TimeoutError:
            try:
                self.respond ('421 timeout.  good-bye')
            except coro.TimeoutError:
                pass
            except OSError:
                pass
        except OSError, why:
            if why[0] in self.disconnect_errors:
                # log it & ignore
                qlog.write('FTPD.DISCONNECT', self.session_id, why.strerror)
            else:
                # Unknown error.  If it's something like EBADF, then there is
                # something seriously wrong.
                raise

    # the set of errors that indicate a connection problem
    disconnect_errors = (
        errno.ECONNRESET,
        errno.EHOSTUNREACH,
        errno.ECONNREFUSED,
        errno.EHOSTDOWN,
        errno.EPIPE,
        errno.ETIMEDOUT
    )

    def close_passive_acceptor (self):
        if self.passive_acceptor:
            self.passive_acceptor.close()
            self.passive_acceptor = None

    def close (self):
        self.close_passive_acceptor()
        if self.conn:
            self.conn.close()
            self.conn = None
            self.server.closed_sessions.increment()

    # --------------------------------------------------
    # filesystem interface functions.
    # override these to provide access control or perform
    # other functions.
    # --------------------------------------------------

    def cwd (self, line):
        return self.filesystem.cwd (line[1])

    def cdup (self, line):
        return self.filesystem.cdup()

    def open (self, path, mode):
        return self.filesystem.open (path, mode)

    def tmp_create(self, path, mode):
        return self.filesystem.tmp_create(path, mode)

    # returns a producer
    def listdir (self, path, long=0, ls_filter=None):
        return self.filesystem.listdir (path, long, ls_filter)

    def get_dir_list (self, line, long=0):
        # we need to scan the command line for arguments to '/bin/ls'...
        args = line[1:]
        path_args = []
        for arg in args:
            if arg[0] != '-':
                path_args.append (arg)
            else:
                # ignore arguments
                pass
        if len(path_args) < 1:
            dir = '.'
        else:
            dir = path_args[0]
        return self.listdir (dir, long, self.ls_filter)

    # --------------------------------------------------
    # authorization methods
    # --------------------------------------------------

    def check_command_authorization (self, username, command):
        try:
            return self.server.authorizer.check_command_authorization(username, command)
        except AttributeError:
            if command in self.write_commands and self.read_only:
                return 0
            else:
                return 1

    # --------------------------------------------------
    # utility methods
    # --------------------------------------------------

    def respond (self, resp):
        qlog.write('FTPD.SEND', self.session_id, resp)
        self.send (resp + '\r\n')

    def command_not_understood (self, command):
        self.respond ("500 '%s': command not understood." % command)

    def command_not_authorized (self, command):
        self.respond (
            "530 You are not authorized to perform the '%s' command" % (
                command
            )
        )

    def make_data_channel (self):
        # In PASV mode, the connection may or may _not_ have been made
        # yet.  [although in most cases it is... FTP Explorer being
        # the only exception I've yet seen].  This gets somewhat confusing
        # because things may happen in any order...
        pa = self.passive_acceptor
        if pa:
            conn, addr = pa.accept()
            self.close_passive_acceptor()
            return conn, addr
        else:
            # not in PASV mode.
            ip, port = self.client_addr
            cdc = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
            if self.bind_local_minus_one:
                cdc.bind ((self.server.ip, self.server.port - 1))
            else:
                # using random port number
                cdc.bind ((self.server.ip, 0))
            try:
                cdc.connect (self.client_addr)
            except OSError, why:
                cdc.close()
                cdc = None
                self.respond ("425 Can't build data connection: %s" % why.strerror)
            return cdc, self.client_addr

    type_map = {
        'a': 'ASCII',
        'i': 'Binary',
        'e': 'EBCDIC',
        'l': 'Binary'
    }

    type_mode_map = {
        'a': 't',
        'i': 'b',
        'e': 'b',
        'l': 'b'
    }

    # --------------------------------------------------
    # command methods
    # --------------------------------------------------

    _help_type = 'specify data transfer type'
    _syntax_type = 'type (a|i|l)$'

    def cmd_type (self, line):
        # ascii, ebcdic, image, local <byte size>
        t = string.lower (line[1])
        # no support for EBCDIC
        # if t not in ['a','e','i','l']:
        if t not in ['a', 'i', 'l']:
            self.command_not_understood (string.join (line))
        elif t == 'l' and (len(line) > 2 and line[2] != '8'):
            self.respond ('504 Byte size must be 8')
        else:
            self.current_mode = t
            self.respond ('200 Type set to %s.' % self.type_map[t])

    _help_quit = 'terminate session'
    _syntax_quit = 'quit$'

    def cmd_quit (self, line):
        self.respond ('221 Goodbye.')
        return 'quit'

    _help_port = 'specify data connection port'
    _syntax_port = 'port ([0-9]{1,3},){5}[0-9]{1,3}$'

    def cmd_port (self, line):
        info = line[1].split (',')
        ip = '.'.join (info[:4])
        port = (int (info[4]) * 256) + int (info[5])
        # TODO: we should (optionally) verify that the
        # ip number belongs to the client.  [wu-ftpd does this?]
        self.client_addr = (ip, port)
        self.respond ('200 PORT command successful.')

    _help_pasv = 'prepare for server-to-server transfer'
    _syntax_pasv = 'pasv$'

    def cmd_pasv (self, line):
        # careful to close one that might already be there...
        self.close_passive_acceptor()
        ps = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.passive_acceptor = ps
        ps.bind ((self.conn.getsockname()[0], 0))
        ps.listen (1)
        (ip, port) = ps.getsockname()
        self.respond (
            '227 Entering Passive Mode (%s,%d,%d)' % (
                ','.join (ip.split ('.')),
                port / 256,
                port % 256
            )
        )

    _help_nlst = 'give name list of files in directory'
    _syntax_nlst = 'nlst( \S+)?'

    def cmd_nlst (self, line):
        # ncftp adds the -FC argument for the user-visible 'nlist'
        # command.  We could try to emulate ls flags, but not just yet.
        if '-FC' in line:
            line.remove ('-FC')
        try:
            dir_list_producer = self.get_dir_list (line, 0)
        except OSError, why:
            self.respond ('550 Could not list directory: %r' % why[0])
            return
        self.respond (
            '150 Opening %s mode data connection for file list' % (
                self.type_map[self.current_mode]
            )
        )
        conn, addr = self.make_data_channel()
        if conn:
            try:
                self.send_with_producer (conn, dir_list_producer)
                self.respond ('226 Transfer Complete')
            finally:
                conn.close()

    _help_list = 'give list files in a directory'
    _syntax_list = 'list( \S+)?'

    def cmd_list (self, line):
        try:
            dir_list_producer = self.get_dir_list (line, 1)
        except OSError, why:
            self.respond ('550 Could not list directory: %r' % why[0])
            return
        self.respond (
            '150 Opening %s mode data connection for file list' % (
                self.type_map[self.current_mode]
            )
        )
        conn, addr = self.make_data_channel()
        if conn:
            try:
                self.send_with_producer (conn, dir_list_producer)
                self.respond ('226 Transfer Complete')
            finally:
                conn.close()

    _help_cwd = 'change working directory'
    _syntax_cwd = 'cwd \S.*$'

    def cmd_cwd (self, line):
        if self.cwd (line):
            self.respond ('250 CWD command successful.')
        else:
            self.respond ('550 No such directory.')

    _help_cdup = 'change to parent of current working directory'
    _syntax_cdup = 'cdup$'

    def cmd_cdup (self, line):
        if self.cdup(line):
            self.respond ('250 CDUP command successful.')
        else:
            self.respond ('550 No such directory.')

    _help_pwd = 'print the current working directory'
    _syntax_pwd = 'pwd$'

    def cmd_pwd (self, line):
        self.respond (
            '257 "%s" is the current directory.' % (
                self.filesystem.current_directory()
            )
        )

    # modification time
    # example output:
    # 213 19960301204320
    _help_mdtm = 'show last modification time of file'
    _syntax_mdtm = 'mdtm \S+'

    def cmd_mdtm (self, line):
        filename = line[1]
        if not self.filesystem.isfile (filename):
            self.respond ('550 "%s" is not a file' % filename)
        else:
            mtime = time.gmtime(self.filesystem.stat(filename)[stat.ST_MTIME])
            self.respond (
                '213 %4d%02d%02d%02d%02d%02d' % (
                    mtime[0],
                    mtime[1],
                    mtime[2],
                    mtime[3],
                    mtime[4],
                    mtime[5]
                )
            )

    _help_noop = 'do nothing'
    _syntax_noop = 'noop$'

    def cmd_noop (self, line):
        self.respond ('200 NOOP command successful.')

    _help_size = 'return size of file'
    _syntax_size = 'size \S+'

    def cmd_size (self, line):
        filename = line[1]
        if not self.filesystem.isfile (filename):
            self.respond ('550 "%s" is not a file' % filename)
        else:
            self.respond (
                '213 %d' % (self.filesystem.stat(filename)[stat.ST_SIZE])
            )

    _help_retr = 'retrieve a file'
    _syntax_retr = 'retr \S+'

    def cmd_retr (self, line):
        if len(line) < 2:
            self.command_not_understood (string.join (line))
        else:
            file = line[1]
            if not self.filesystem.isfile (file):
                self.respond ('550 No such file')
            else:
                try:
                    # FIXME: for some reason, 'rt' isn't working on win95
                    mode = 'r' + self.type_mode_map[self.current_mode]
                    fd = self.open (file, mode)
                except (OSError, IOError), why:
                    self.respond ('553 could not open file for reading: %r' % why[0])
                    return
                try:
                    self.respond (
                        "150 Opening %s mode data connection for file '%s'" % (
                            self.type_map[self.current_mode],
                            file
                        )
                    )
                    conn, addr = self.make_data_channel()
                    if conn:
                        try:
                            fd.seek(0, 2)
                            filesize = fd.tell()
                            fd.seek(0, 0)
                            qlog.write('FTPD.DOWNLOAD', self.session_id, file, filesize)
                            if self.restart_position:
                                # try to position the file as requested, but
                                # give up silently on failure (the 'file object'
                                # may not support seek())
                                try:
                                    fd.seek (self.restart_position)
                                except:
                                    pass
                                self.restart_position = 0
                            try:
                                self.send_file (conn, fd)
                            except OSError, why:
                                self.respond ('451 Transfer aborted. %s' % (str(why)))
                            except coro.TimeoutError:
                                self.respond ('421 Transfer timed out.')
                            except coro.Shutdown:
                                self.respond ('451 Transfer aborted.  FTP service shutting down.')
                            else:
                                self.respond ('226 Transfer Complete')
                        finally:
                            conn.close()
                finally:
                    fd.close()

    # Whether or not to use a temporary file to make atomic updates
    # when writing over a file that already exists.
    use_atomic_store = 1

    _help_stor = 'store a file'
    _syntax_stor = 'stor \S+'

    def cmd_stor (self, line, mode='wb'):
        # don't use 'atomic store' when in 'append' mode (see cmd_appe())
        atomic_flag = self.use_atomic_store and ('a' not in mode)
        if len (line) < 2:
            self.command_not_understood (string.join (line))
            return
        if self.restart_position:
            self.restart_position = 0
            self.respond ('553 restart on STOR not yet supported')
            return
        file = line[1]
        # todo: handle that type flag
        try:
            if atomic_flag:
                tmp_filename, fd = self.tmp_create(file, mode)
            else:
                fd = self.open (file, mode)
        except (OSError, IOError), why:
            self.respond ('553 could not open file for writing: %r' % why[0])
            return
        except filesys.TempFileCreateError:
            self.respond ('553 could not open temporary file. Target may be a directory.')
            return
        try:
            self.respond (
                '150 Opening %s connection for %s' % (
                    self.type_map[self.current_mode],
                    file
                )
            )
            conn, addr = self.make_data_channel ()
            if conn:
                xfer_success = 1
                try:
                    bytes = 0
                    while 1:
                        block = coro.with_timeout (self.recv_timeout, conn.recv, 8192)
                        if block:
                            bytes += len(block)
                            try:
                                fd.write (block)
                            except (OSError, IOError), e:
                                xfer_success = 0
                                qlog.write('FTPD.UPLOAD_FAILURE',
                                           self.session_id, file, e.strerror)
                                self.respond ("452 Store failed: " + e.strerror)
                        else:
                            break
                    fd.close()
                    fd = None
                    if atomic_flag:
                        # atomic rename
                        try:
                            self.filesystem.rename (tmp_filename, file)
                        except OSError, e:
                            xfer_success = 0
                            qlog.write('FTPD.UPLOAD_FAILURE',
                                       self.session_id, file, e.strerror)
                            self.respond ("553 Store failed: " + e.strerror)
                finally:
                    conn.close()
                if xfer_success:
                    qlog.write('FTPD.UPLOAD', self.session_id, file, bytes)
                    self.respond ('226 Transfer Complete')
        finally:
            if fd:
                fd.close()
            if atomic_flag:
                try:
                    self.filesystem.unlink(tmp_filename)
                except OSError:
                    # normally ENOENT
                    pass

    _help_abor = 'abort operation'
    _syntax_abor = 'abor$'

    def cmd_abor (self, line):
        self.respond ('226 ABOR command successful.')

    _help_appe = 'append to a file'
    _syntax_appe = 'appe \S+'

    def cmd_appe (self, line):
        return self.cmd_stor (line, 'ab')

    _help_dele = 'delete file'
    _syntax_dele = 'dele \S+'

    def cmd_dele (self, line):
        if len (line) != 2:
            self.command_not_understood (string.join (line))
        else:
            file = line[1]
            if self.filesystem.isfile (file):
                try:
                    self.filesystem.unlink (file)
                    self.respond ('250 DELE command successful.')
                except OSError:
                    self.respond ('550 error deleting file.')
            else:
                self.respond ('550 %s: No such file.' % file)

    _help_mkd = 'make a directory'
    _syntax_mkd = 'mkd \S+'

    def cmd_mkd (self, line):
        if len (line) != 2:
            self.command_not_understood (string.join (line))
        else:
            path = line[1]
            try:
                self.filesystem.mkdir (path)
                self.respond ('257 MKD command successful.')
            except OSError:
                self.respond ('550 error creating directory.')

    _help_rmd = 'remove a directory'
    _syntax_rmd = 'rmd \S+'

    def cmd_rmd (self, line):
        if len (line) != 2:
            self.command_not_understood (string.join (line))
        else:
            path = line[1]
            try:
                self.filesystem.rmdir (path)
                self.respond ('250 RMD command successful.')
            except OSError:
                self.respond ('550 error removing directory.')

    _help_user = 'specify user name'
    _syntax_user = 'user \S+'

    def cmd_user (self, line):
        if len(line) > 1:
            self.user = line[1]
            self.respond ('331 Password required.')
        else:
            self.command_not_understood (string.join (line))

    _help_pass = 'specify password'
    _syntax_pass = 'pass \S+'

    def cmd_pass (self, line):
        if len(line) < 2:
            pw = ''
        else:
            pw = line[1]
        try:
            result, message, fs = self.server.authorizer.authorize (self, self.user, pw)
        except SystemError:
            result = None
            message = 'User %s access denied' % self.user

        if result:
            self.respond ('230 %s' % message)
            self.filesystem = fs
            self.authorized = 1
            qlog.write('FTPD.LOGIN', self.session_id, self.user)
        else:
            qlog.write('FTPD.LOGIN_FAILED', self.session_id, self.user)
            self.respond ('530 %s' % message)

    _help_rest = 'restart incomplete transfer'
    _syntax_rest = 'rest [0-9]+$'

    def cmd_rest (self, line):
        try:
            pos = string.atoi (line[1])
        except ValueError:
            self.command_not_understood (string.join (line))
        self.restart_position = pos
        self.respond (
            '350 Restarting at %d. Send STORE or RETRIEVE to initiate transfer.' % pos
        )

    _help_stru = 'obsolete - set file transfer structure'
    _syntax_stru = 'stru (f|r|p)$'

    def cmd_stru (self, line):
        if line[1] in ('f', 'F'):
            # f == 'file'
            self.respond ('200 STRU F Ok')
        else:
            self.respond ('504 Unimplemented STRU type')

    _help_mode = 'obsolete - set file transfer mode'
    _syntax_mode = 'mode (s|b|c)$'

    def cmd_mode (self, line):
        if line[1] in ('s', 'S'):
            # f == 'file'
            self.respond ('200 MODE S Ok')
        else:
            self.respond ('502 Unimplemented MODE type')

# The stat command has two personalities.  Normally it returns status
# information about the current connection.  But if given an argument,
# it is equivalent to the LIST command, with the data sent over the
# control connection.  Strange.  But wuftpd, ftpd, and nt's ftp server
# all support it.
#
#
#  _help_stat = 'return status of server'
# def cmd_stat (self, line):
# pass

    _help_syst = 'show operating system type of server system'
    _syntax_syst = 'syst$'

    def cmd_syst (self, line):
        # Replying to this command is of questionable utility, because
        # this server does not behave in a predictable way w.r.t. the
        # output of the LIST command.  We emulate Unix ls output, but
        # on win32 the pathname can contain drive information at the front
        # Currently, the combination of ensuring that os.sep == '/'
        # and removing the leading slash when necessary seems to work.
        # [cd'ing to another drive also works]
        #
        # This is how wuftpd responds, and is probably
        # the most expected.  The main purpose of this reply is so that
        # the client knows to expect Unix ls-style LIST output.
        self.respond ('215 UNIX Type: L8')
        # one disadvantage to this is that some client programs
        # assume they can pass args to /bin/ls.
        # a few typical responses:
        # 215 UNIX Type: L8 (wuftpd)
        # 215 Windows_NT version 3.51
        # 215 VMS MultiNet V3.3
        # 500 'SYST': command not understood. (SVR4)

    _help_help = 'give help information'
    _syntax_help = 'help( .*)?$'

    def cmd_help (self, line):
        # find all the methods that match 'cmd_xxxx',
        # use their docstrings for the help response.
        attrs = dir(ftp_channel)
        help_lines = []
        for attr in attrs:
            if attr[:6] == '_help_':
                cmd = attr.split ('_')[2]
                help_lines.append ('\t%s\t%s' % (cmd, getattr (self, attr)))
        if help_lines:
            self.writev ([
                '214-The following commands are recognized\r\n214-',
                '\r\n214-'.join (help_lines[:-1]),
                '\r\n214 ',
                help_lines[-1],
                '\r\n'
            ])
        else:
            self.send ('214      Help Unavailable\r\n')

class ftp_server:

    def __init__ (self, authorizer, channel=ftp_channel, hostname=None, ip='0.0.0.0', port=21):
        self.ip = ip
        self.port = port
        self.authorizer = authorizer
        self.channel = channel
        self.thread_id = None
        # Used to signal when all the clients have exited
        self.shutdown_cv = coro.condition_variable()
        # list of ftp_channel instances
        self.clients = []
        self.session_id = 1

        if hostname is None:
            self.hostname = socket.gethostname()
        else:
            self.hostname = hostname

        # statistics
        self.total_sessions = counter()
        self.closed_sessions = counter()
        self.total_files_out = counter()
        self.total_files_in = counter()
        self.total_bytes_out = counter()
        self.total_bytes_in = counter()
        self.total_exceptions = counter()

    def session_done(self, client):
        """session_done(client) -> None
        Indicates that the given session is done.
        Client is a ftp_channel instance.
        """
        self.clients.remove(client)
        if len(self.clients) == 0:
            self.shutdown_cv.wake_one()

    def shutdown(self, timeout):
        """shutdown(timeout) -> None
        Shuts down the server and all the children within timeout seconds.
        Rudely terminates sessions if they don't exit within timeout
        seconds.
        """
        # Shut down the main accept loop
        if self.thread_id:
            try:
                my_thread = coro.get_thread_by_id (self.thread_id)
            except KeyError:
                # thread already exited
                return
            my_thread.shutdown()
        # Shut down all the children
        if self.clients:
            for c in self.clients:
                c.shutdown()
            # wait for all the children to finish
            try:
                coro.with_timeout(timeout, self.shutdown_cv.wait)
            except coro.TimeoutError:
                # kill hard
                for c in self.clients:
                    c.shutdown(rudely=1)

    def run (self):
        try:
            self._run()
        except coro.Shutdown:
            # We've been asked to shutdown
            return

    def _run (self):
        """Listens on the FTP port accepting connections and spawning sessions."""
        self.thread_id = coro.current().thread_id()
        s = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
        try:
            s.set_reuse_addr()
            done = 0
            while not done:
                for x in xrange (5):
                    try:
                        was_eaddrinuse = 0
                        s.bind ((self.ip, self.port))
                    except OSError, why:
                        if why[0] == errno.EACCES:
                            coro.print_stderr(
                                'Access denied binding to %s:%i.  Are you running as root?\n' % (self.ip, self.port))
                            return
                        elif why[0] == errno.EADDRINUSE:
                            was_eaddrinuse = 1
                        elif why[0] != errno.EADDRNOTAVAIL:
                            raise
                    else:
                        done = 1
                        break
                    coro.sleep_relative (1)
                else:
                    coro.print_stderr ("cannot bind to %s:%d after 5 attempts\n" % (self.ip, self.port))
                    if was_eaddrinuse:
                        qlog.write('FTPD.PORT_IN_USE',
                                   self.ip, str(self.port))
                    coro.sleep_relative (15)
            s.listen (1024)
            while 1:
                conn_list = s.accept_many()
                for conn, addr in conn_list:
                    qlog.write('FTPD.CONNECTION', self.session_id, addr[0], self.ip)
                    session = self.channel (self, conn, addr, self.session_id)
                    self.session_id += 1
                    thread = coro.spawn (session.run)
                    thread.set_name (
                        "%s_%d" % (
                            session.__class__.__name__,
                            thread.thread_id()
                        )
                    )
                    self.clients.append(session)
        finally:
            s.close()

import filesys

# not much of a doorman! 8^)
class dummy_authorizer:
    def __init__ (self, root='/'):
        self.root = root

    def authorize (self, channel, username, password):
        channel.persona = -1, -1
        channel.read_only = 1
        return 1, 'Ok.', filesys.os_filesystem (self.root)

class anon_authorizer:
    def __init__ (self, root='/'):
        self.root = root

    def authorize (self, channel, username, password):
        if username in ('ftp', 'anonymous'):
            channel.persona = -1, -1
            channel.read_only = 1
            return 1, 'Ok.', filesys.os_filesystem (self.root)
        else:
            return 0, 'Password invalid.', None

class unix_authorizer:

    def __init__(self, root=None, wd=None):
        """unix_authorizer(root=None, wd=None) -> unix_authorizer instance
        Creates a unix authorizer instance.
        root may be a callable function.  It is given one argument, the name of the user.
        It is supposed to return the root directory for this user.  By default this is '/'.
        wd may be a callable function.  It is given one argument, the name of the user.
        It is supposed to return the working directory that the user first starts in.
        By default this is the user's home directory.
        """
        self.root = root
        self.wd = wd

    # return a trio of (success, reply_string, filesystem)
    def authorize (self, channel, username, password):
        try:
            pw_name, pw_passwd, pw_uid, pw_gid, pw_gecos, pw_dir, pw_shell = pwd.getpwnam (username)
        except (KeyError, TypeError):
            return 0, 'No such user.', None
        if pw_passwd == '*':
            raise SystemError("unable to fetch encrypted password. not running as root?")
        else:
            if crypt.crypt (password, pw_passwd) == pw_passwd:
                # XXX think about this
                # channel.read_only = 0
                # channel.read_only = 1
                if self.root:
                    root = self.root(pw_name)
                else:
                    root = '/'
                if self.wd:
                    wd = self.wd(pw_name)
                else:
                    wd = pw_dir
                fs = filesys.schizophrenic_unix_filesystem (
                    root,
                    wd,
                    persona=(pw_name, pw_uid, pw_gid)
                )
                return 1, 'Login successful.', fs
            else:
                return 0, 'Password invalid.', None

    def __repr__ (self):
        return '<standard unix authorizer>'

def test (port='8021'):
    fs = ftp_server (dummy_authorizer(), port=int(port))
    # fs = ftp_server (unix_authorizer(), port=int(port))
    qlog.disable()
    coro.spawn (fs.run)
    coro.event_loop()

if __name__ == '__main__':
    test (sys.argv[1])

########NEW FILE########
__FILENAME__ = ftp_client
# -*- Mode: Python -*-

# simple ftp client

import dnsqr
import dns_exceptions
import read_stream
import coro
import re
import socket

send_timeout = 800
recv_timeout = 800
connect_timeout = 800

class ftp_error (Exception):
    pass

class ftp_client:

    pasv_mode = 1
    ftp_port = 21
    debug = 0

    def __init__ (self, local_ip=None):
        """__init__(local_ip=None) -> ftp_client
        Creates a new ftp_client.  Binds to the local_ip if it is given.
        NOTE: You MUST del the stream object when you are done, or this will leak.
        """
        self.local_ip = local_ip
        self.s = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
        if local_ip:
            self.s.bind((local_ip, 0))
        self.stream = read_stream.stream_reader (self.recv)

    def recv (self, size):
        global recv_timeout
        return coro.with_timeout (recv_timeout, self.s.recv, size)

    def send (self, data):
        global send_timeout
        return coro.with_timeout (send_timeout, self.s.send, data)

    def debug_line (self, line):
        coro.print_stderr (line + '\n')

    ip_re = re.compile ('[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')

    def connect (self, host):
        global connect_timeout
        if not self.ip_re.match (host):
            ip = dnsqr.query(host, 'A')
            if ip:
                # pull out the ip of the first entry
                ip = ip[0][1]
            else:
                # no results found for this entry
                raise dns_exceptions.DNS_Hard_Error
        else:
            ip = host
        coro.with_timeout (connect_timeout, self.s.connect, (ip, self.ftp_port))
        self.read_response ('2')

    def _read_response (self):
        multiline = 0
        response = []
        while True:
            line, eof = self.stream.read_line()
            if eof:
                raise EOFError
            if self.debug:
                self.debug_line ('<= %s' % line)
            cont = line[3:4]
            if multiline and line[0:4] == '    ':
                response.append (line[4:])
            elif cont == ' ':
                response.append (line[4:])
                return line[0:3], response
            elif cont == '-':
                response.append (line[4:])
                multiline = 1
            else:
                raise ftp_error(line)

    def read_response (self, expect):
        code, response = self._read_response()
        if code[0] != expect:
            raise ftp_error(code, response)
        else:
            return response

    def command (self, command, expect):
        "send a synchronous FTP command; expecting to receive reply code <expect>"
        if self.debug:
            self.debug_line ('=> %s' % command)
        self.send (command + '\r\n')
        return self.read_response (expect)

    def cmd_user (self, username):
        self.command ('USER %s' % username, '3')

    def cmd_pass (self, password):
        self.command ('PASS %s' % password, '2')

    pasv_re = re.compile ('.*\(([0-9]{1,3},[0-9]{1,3},[0-9]{1,3},[0-9]{1,3},[0-9]{1,3},[0-9]{1,3})\)')

    def parse_pasv_reply (self, reply):
        m = self.pasv_re.match (reply[-1])
        if not m:
            raise ftp_error("unable to parse PASV reply: %r" % reply)
        else:
            nums = m.groups()[0].split (',')
            ip = nums[:4]
            ip = '.'.join (nums[:4])
            port = map (int, nums[4:6])
            port = port[0] << 8 | port[1]
            return ip, port

    def make_data_channel (self):
        global connect_timeout
        if self.pasv_mode:
            reply = self.command ('PASV', '2')
            ip, port = self.parse_pasv_reply (reply)
            dc = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
            if self.local_ip:
                dc.bind((self.local_ip, 0))
            if self.debug:
                coro.print_stderr ('connecting to %s:%s\n' % (ip, port))
            coro.with_timeout (connect_timeout, dc.connect, (ip, port))
            return dc
        else:
            raise ftp_error("non-pasv transfers not yet implemented")

    def read_from_data_channel (self, dc, block_reader):
        global recv_timeout
        while True:
            block = coro.with_timeout (recv_timeout, dc.recv, 8192)
            if not block:
                break
            else:
                block_reader (block)

    def write_to_data_channel (self, dc, block_writer):
        global send_timeout
        try:
            while True:
                block = block_writer()
                if not block:
                    break
                else:
                    coro.with_timeout (send_timeout, dc.send, block)
        finally:
            dc.close()

    def cmd_retr (self, filename, block_reader):
        conn = self.make_data_channel()
        self.command ('RETR %s' % filename, '1')
        self.read_from_data_channel (conn, block_reader)
        self.read_response ('2')

    def cmd_list (self, block_reader):
        conn = self.make_data_channel()
        self.command ('LIST', '1')
        self.read_from_data_channel (conn, block_reader)
        self.read_response ('2')

    def cmd_stor (self, filename, block_writer):
        conn = self.make_data_channel()
        self.command ('STOR %s' % filename, '1')
        self.write_to_data_channel (conn, block_writer)
        self.read_response ('2')

    def cmd_quit (self):
        self.command ('QUIT', '2')
        self.s.close()

    def cmd_cwd (self, dir):
        self.command ('CWD %s' % dir, '2')

    def cmd_pwd (self):
        return self.command ('PWD', '2')

    def cmd_type (self, type='I'):
        return self.command ('TYPE %s' % type, '2')

def test1():
    coro.print_stderr ("waiting 5 seconds...\n")
    coro.sleep_relative (5)
    f = ftp_client()
    coro.print_stderr ("connecting...\n")
    f.connect ('squirl.nightmare.com')
    coro.print_stderr ("logging in...\n")
    f.cmd_user ('anonymous')
    f.cmd_pass ('rushing@')
    blocks = []
    coro.print_stderr ("retrieving directory listing...\n")
    f.cmd_list (blocks.append)
    coro.print_stderr ("done.\n")
    f.cmd_quit()
    coro.print_stderr ('-' * 20 + '\n')
    coro.print_stderr (''.join (blocks))
    coro.print_stderr ('-' * 20 + '\n')

def test2():
    coro.sleep_relative (5)
    f = ftp_client()
    coro.print_stderr ("connecting...\n")
    f.connect ('10.1.1.209')
    coro.print_stderr ("logging in...\n")
    f.cmd_user ('ftpguy')
    f.cmd_pass ('ftpguy')
    f.cmd_type ('I')
    coro.print_stderr ("sending file...\n")
    file = open ('ftp_client.py', 'rb')
    thunk = (lambda f=file: f.read (8192))
    f.cmd_stor ('a_file.txt', thunk)
    coro.print_stderr ("done.\n")
    f.cmd_quit()

if __name__ == '__main__':
    import backdoor
    coro.spawn (backdoor.serve)
    coro.spawn (test2)
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = gc_monitor
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro
import gc
import types

monitor_sleep_interval = 60 * 60

def safe_repr (x):
    if isinstance (x, types.InstanceType):
        return '<%s object at %x>' % (x.__class__.__name__, id(x))
    else:
        return '<%s object at %x>' % (str(type(x)), id(x))

def monitor_thread():
    while True:
        if gc.garbage:
            coro.print_stderr (
                "Warning: possible memory leak: len(gc.garbage)=%d\n" % (len(gc.garbage),)
            )
            coro.print_stderr (
                "\tFirst %d objects in gc.garbage:\n" % (len(gc.garbage[:5]))
            )
            for x in gc.garbage[:5]:
                coro.print_stderr ("\t  %s\n" % (safe_repr (x),))
        coro.sleep_relative (monitor_sleep_interval)

########NEW FILE########
__FILENAME__ = kqueue_events
# Generated by h2py from /usr/include/sys/event.h
EVFILT_READ = (-1)
EVFILT_WRITE = (-2)
EVFILT_AIO = (-3)
EVFILT_VNODE = (-4)
EVFILT_PROC = (-5)
EVFILT_SIGNAL = (-6)
EVFILT_SYSCOUNT = 6
EV_ADD = 0x0001
EV_DELETE = 0x0002
EV_ENABLE = 0x0004
EV_DISABLE = 0x0008
EV_ONESHOT = 0x0010
EV_CLEAR = 0x0020
EV_SYSFLAGS = 0xF000
EV_FLAG1 = 0x2000
EV_EOF = 0x8000
EV_ERROR = 0x4000
NOTE_DELETE = 0x0001
NOTE_WRITE = 0x0002
NOTE_EXTEND = 0x0004
NOTE_ATTRIB = 0x0008
NOTE_LINK = 0x0010
NOTE_RENAME = 0x0020
NOTE_EXIT = 0x80000000
NOTE_FORK = 0x40000000
NOTE_EXEC = 0x20000000
NOTE_PCTRLMASK = 0xf0000000
NOTE_PDATAMASK = 0x000fffff
NOTE_TRACK = 0x00000001
NOTE_TRACKERR = 0x00000002
NOTE_CHILD = 0x00000004

########NEW FILE########
__FILENAME__ = ber
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

#
# Basic Encoding Rules (BER) for SMI data types
#
# Keith Dart <kdart@cosinecom.com>, 2001
# Some functions partially derived from Simon Leinen's <simon@switch.ch>
# BER PERL module (which was actually taken from the pysnmp module).

# XXX stolen from PyNMS by Sam Rushing <srushing@ironport.com>
# XXX there's a lot of stuff in here that I'm not using - clean it up!

class UnknownTag (Exception):
    pass

TRUE = 0xFF
FALSE = 0

CONTEXT = 'context'
APPLICATION = 'application'
UNKNOWN = 'unknown'

# flags for BER tags
FLAGS = {
    'UNIVERSAL': 0x00,
    'STRUCTURED': 0x20,
    'APPLICATION': 0x40,
    'CONTEXT': 0x80,
    'PRIVATE': 0xC0,
}

# universal BER tags
TAGS = {
    'INTEGER': 0x02,
    'OCTET_STRING': 0x04,
    'OBJID': 0x06,
    'NULL': 0x05,
    'SEQUENCE': 0x10 | FLAGS['STRUCTURED'],
    'BOOLEAN': 0x01,
    'BITSTRING': 0x03,
    'SET': 0x11,
    'SETOF': 0x31,
    'Enumerated': 0x0a,
}

# pre-compute tag encodings
TAG_ENCODINGS = {}
for _tn in TAGS.keys():
    TAG_ENCODINGS[_tn] = chr(TAGS[_tn])
del _tn

# invert TAG map to speed decoding
REVERSE_TAGS = {}
for _name, _value in TAGS.items():
    REVERSE_TAGS[_value] = _name
del _name, _value

# BER encoders / decoders
#
# BER HEADER ENCODERS / DECODERS

def encode_length(length):
    """encode BER length"""
    # if given length fits one byte
    if length < 0x80:
        return '%c' % length
    # two bytes required
    elif length < 0xFF:
        return '%c%c' % (0x81, length)
    # three bytes required
    else:
        return '%c%c%c' % (0x82, (length >> 8) & 0xFF, length & 0xFF)

# encode a native signed 32bit integer
def encode_an_integer_type (arg, ber_tag):
    # this would actually be easier in C!
    if arg == 0:
        return '%c%c%c' % (ber_tag, 1, 0)
    s = []
    shiftcount = 0
    topval = arg & 0xff800000
    while topval == 0 or topval == 0xff800000:
        arg = arg << 8
        topval = arg & 0xff800000
        shiftcount = shiftcount + 1
    __pychecker__ = 'unusednames=i'
    for i in xrange(4 - shiftcount):
        s.append(((arg & 0xff000000) >> 24) & 0xff)
        arg = arg << 8
    result = ''.join(map(chr, s))
    return ber_tag + encode_length(len(result)) + result


# encode an unsigned 32 bit value (which is actually a python long)
def encode_an_unsigned(arg, ber_tag):
    s = []
    while arg > 0:
        s.insert(0, chr(arg & 0xff))
        arg = arg >> 8
    if s and (ord(s[0]) & 0x80):
        s.insert(0, chr(0))
    result = ''.join(s)
    return ber_tag + encode_length(len(result)) + result


def encode_tag (name):
    """encode ASN.1 data type tag"""
    # lookup the tag ID by name
    try:
        return TAG_ENCODINGS[name]
    except KeyError:
        raise UnknownTag(name)

def decode_tag (tag):
    """decode ASN.1 data type tag"""
    try:
        return REVERSE_TAGS[tag]
    except KeyError:
        raise UnknownTag(tag)

def encode_boolean(value):
    if value:
        return TAG_ENCODINGS['BOOLEAN'] + '%c%c' % (1, TRUE)
    else:
        return TAG_ENCODINGS['BOOLEAN'] + '%c%c' % (1, FALSE)

# encode an octets string
def encode_string(string):
    """encode ASN.1 string"""
    return TAG_ENCODINGS['OCTET_STRING'] + encode_length(len(string)) + string

def encode_a_pdu(ber_tag, *args):
    encodings = []
    for arg in args:
        if arg is None:
            encodings.append(encode_null())
        else:
            encodings.append(arg.encode())
    res = ''.join(encodings)
    return ber_tag + encode_length(len(res)) + res

# encode null
def encode_null():
    """encode ASN.1 NULL"""
    return '%c%c' % (TAGS['NULL'], 0)

def encode_integer(value):
    return encode_an_integer_type(value, TAG_ENCODINGS['INTEGER'])

# create dictionary that maps ber_tag encodings to methods
ENCODE_METHODS = {}
ENCODE_METHODS[None] = encode_null
ENCODE_METHODS[chr(TAGS['BOOLEAN'])] = encode_boolean
ENCODE_METHODS[chr(TAGS['INTEGER'])] = encode_integer
ENCODE_METHODS[chr(TAGS['OCTET_STRING'])] = encode_string
ENCODE_METHODS[chr(TAGS['NULL'])] = encode_null
ENCODE_METHODS[chr(TAGS['Enumerated'])] = encode_integer

#
# ASN.1 DATA TYPES DECODERS

class TLV:
    def __init__(self, tag, length, value, lol):
        self.tag = tag
        self.length = length
        self.value = value
        self.lol = lol  # length of length - needed for sequence decoding - kludgy

    def __str__(self):
        return '<TLV: tag=0x%x, length=%d, value=%s>' % (self.tag, self.length, repr(self.value))

    def __repr__(self):
        return '%s(0x%x, %d, %s)' % (self.__class__.__name__, self.tag, self.length, repr(self.value))

    def decode(self):
        try:
            return DECODE_METHODS[self.tag](self.length, self.value)
        except KeyError:
            # rather than barf, return CONTEXT and APPLICATION types as AST's.
            # we can check for them explicitly like this:
            #     "if x[0] is ber.APPLICATION:"
            # XXX can both be set?  Doesn't seem likely.
            if self.tag & FLAGS['CONTEXT']:
                kind = CONTEXT
            elif self.tag & FLAGS['APPLICATION']:
                kind = APPLICATION
            else:
                kind = None
            if kind:
                if self.tag & FLAGS['STRUCTURED']:
                    return (kind, self.tag & 0x1f, decode_structured (len(self.value), self.value))
                else:
                    return (kind, self.tag & 0x1f, self.value)
            else:
                return (UNKNOWN, self.tag, self.value)

def get_tlv(message):
    if len(message) > 1:
        tag = ord(message[0])
        length, inc = _decode_message_length(message)
        value = message[inc + 1:length + inc + 1]
        return TLV(tag, length, value, inc)
    else:
        raise ValueError('ber.get_tlv: message too small')

def decode (message):
    return get_tlv (message).decode()

def _decode_message_length(message):
    # message[0] is the tag
    fb = ord(message[1])
    msb = fb & 0x80
    val = fb & 0x7f
    if not msb:
        return val, 1
    else:
        return _decode_an_integer (val, message[2:2 + val], signed=0), 1 + val

# A SEQUENCE is implicitly a tuple
def decode_sequence(length, message):
    assert length == len(message)
    sequence = []
    index = 0
    if length:
        while index < length:
            newtlv = get_tlv(message[index:])
            index = index + newtlv.length + newtlv.lol + 1
            sequence.append(newtlv.decode())
    return tuple(sequence)

def decode_structured (length, message):
    assert length == len(message)
    sequence = []
    index = 0
    if length:
        while index < length:
            newtlv = get_tlv(message[index:])
            index = index + newtlv.length + newtlv.lol + 1
            item = newtlv.decode()
            sequence.append (item)
    return tuple(sequence)

def decode_boolean(length, message):
    __pychecker__ = 'unusednames=length'
    return ord(message[0])

# this decodes any basic integer type, and returns a long to handle
# unsigned types.
def _decode_an_integer (length, message, signed=1):
    __pychecker__ = 'unusednames=length'
    val = ord (message[0])
    if val & 0x80 and signed:
        val = val - 256
    for c in message[1:]:
        val = val << 8 | ord(c)
    return val

def _decode_an_unsigned(length, message):
    val = 0
    for i in xrange(length):
        val = (val << 8) + ord(message[i])
    return val

def decode_integer(length, message):
    return _decode_an_integer(length, message)

def decode_string(length, message):
    __pychecker__ = 'unusednames=length'
    return message

def decode_null(length, message):
    return None

def decode_exception(length, message):
    return None

DECODE_METHODS = {}
DECODE_METHODS[TAGS['BOOLEAN']] = decode_boolean
DECODE_METHODS[TAGS['INTEGER']] = decode_integer
DECODE_METHODS[TAGS['OCTET_STRING']] = decode_string
DECODE_METHODS[TAGS['NULL']] = decode_null
DECODE_METHODS[TAGS['SEQUENCE']] = decode_sequence
DECODE_METHODS[TAGS['SETOF']] = decode_structured
DECODE_METHODS[TAGS['Enumerated']] = decode_integer

########NEW FILE########
__FILENAME__ = ldap
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# pull in visible bits of the low-level pyrex module
from _ldap import *
import re

re_dn = re.compile(r'\s*([,=])\s*')
re_dn_attr = re.compile(r'^([^,]+)(=[^,]+)(,.*)?$')

class ProtocolError (Exception):
    """An LDAP Protocol Error occurred"""
    pass

class LDAP:
    BindRequest                 = 0
    BindResponse                = 1
    UnbindRequest               = 2
    SearchRequest               = 3
    SearchResultEntry           = 4
    SearchResultDone            = 5
    SearchResultReference       = 19  # <--- NOT IN SEQUENCE
    ModifyRequest               = 6
    ModifyResponse              = 7
    AddRequest                  = 8
    AddResponse                 = 9
    DelRequest                  = 10
    DelResponse                 = 11
    ModifyDNRequest             = 12
    ModifyDNResponse            = 13
    CompareRequest              = 14
    CompareResponse             = 15
    AbandonRequest              = 16
    ExtendedRequest             = 23  # <--- NOT IN SEQUENCE
    ExtendedResponse            = 24

class SCOPE:
    BASE      = 0
    ONELEVEL  = 1
    SUBTREE   = 2

class DEREF:
    NEVER     = 0
    SEARCHING = 1
    FINDING   = 2
    ALWAYS    = 3

def encode_search_request (
    base_object,
    scope,
    deref_aliases,
    size_limit,
    time_limit,
    types_only,
    filter,
    which_attrs=None,
    compatibility={}
):
    if scope is None:
        scope = compatibility.get('scope', SCOPE.SUBTREE)
    if which_attrs is None:
        which_attrs = SEQUENCE()
    elif len(which_attrs) == 0:
        # Per section 4.5.1 of rfc 2251, if you really mean the empty
        # list, you can't pass the empty list because the empty list means
        # something else. You need to pass a list consisting of the OID 1.1,
        # which really (see sections 4.1.2, 4.1.4, and 4.1.5) isn't an OID
        # at all. Except some servers (Exchange 5.5) require something
        # different here, hence the lookup in the compatibility dict.
        which_attrs = SEQUENCE (
            OCTET_STRING (compatibility.get ('no_attr_attr', '1.1'))
        )
    else:
        which_attrs = SEQUENCE (*[OCTET_STRING (x) for x in which_attrs])
    return TLV (
        APPLICATION (LDAP.SearchRequest),
        OCTET_STRING (base_object),
        ENUMERATED (scope),
        ENUMERATED (deref_aliases),
        INTEGER (size_limit),
        INTEGER (time_limit),
        BOOLEAN (types_only),
        parse_query (filter),
        which_attrs,
    )

class AUTH:
    # 1 and 2 are reserved
    simple      = 0x00
    sasl        = 0x03

class RESULT:
    success                      = 0
    operationsError              = 1
    protocolError                = 2
    timeLimitExceeded            = 3
    sizeLimitExceeded            = 4
    compareFalse                 = 5
    compareTrue                  = 6
    authMethodNotSupported       = 7
    strongAuthRequired           = 8
    referral                     = 10
    adminLimitExceeded           = 11
    unavailableCriticalExtension = 12
    confidentialityRequired      = 13
    saslBindInProgress           = 14
    noSuchAttribute              = 16
    undefinedAttributeType       = 17
    inappropriateMatching        = 18
    constraintViolation          = 19
    attributeOrValueExists       = 20
    invalidAttributeSyntax       = 21
    noSuchObject                 = 32
    aliasProblem                 = 33
    invalidDNSyntax              = 34
    aliasDereferencingProblem    = 36
    inappropriateAuthentication  = 48
    invalidCredentials           = 49
    insufficientAccessRights     = 50
    busy                         = 51
    unavailable                  = 52
    unwillingToPerform           = 53
    loopDetect                   = 54
    namingViolation              = 64
    objectClassViolation         = 65
    notAllowedOnNonLeaf          = 66
    notAllowedOnRDN              = 67
    entryAlreadyExists           = 68
    objectClassModsProhibited    = 69
    affectsMultipleDSAs          = 71
    other                        = 80

class Error (Exception):

    def __init__ (self, answer):
        Exception.__init__ (self)
        self.code = answer[0]
        self.answer = answer
        self.error_string = result_string (answer[0])

    def __str__ (self):
        if len(self.answer) == 3:
            # We know how to parse it if it's length 3. Second element is
            # the "got DN", and third element is the error message. See
            # section 4 of RFC 1777.

            if self.answer[2]:
                parenthesize_got_dn = 1
                err_msg = " %r" % (self.answer[2],)
            else:
                parenthesize_got_dn = 0
                err_msg = ""

            if self.answer[1]:
                err_msg += " "
                if parenthesize_got_dn:
                    err_msg += "("
                err_msg += "Failed after successfully matching partial DN: %r" \
                           % (self.answer[1],)
                if parenthesize_got_dn:
                    err_msg += ")"
        else:
            err_msg = " %r" % (self.answer,)

        return '<LDAP Error "%s" [0x%x]%s>' % (self.error_string, self.code,
                                               err_msg)
    __repr__ = __str__

RESULT._reverse_map = r = {}
for attr in dir(RESULT):
    value = getattr (RESULT, attr)
    if (isinstance(value, type(0))):
        r[value] = attr

def result_string (result):
    try:
        return RESULT._reverse_map[result]
    except KeyError:
        return "unknown error %r" % (result,)

def encode_bind_request (version, name, auth_data):
    assert (1 <= version <= 127)
    return TLV (
        APPLICATION (LDAP.BindRequest),
        INTEGER (version),
        OCTET_STRING (name),
        auth_data
    )

def encode_simple_bind (version, name, login):
    return encode_bind_request (
        version,
        name,
        TLV (
            CHOICE (AUTH.simple, 0),
            login
        )
    )

def encode_sasl_bind (version, name, mechanism, credentials=''):
    if credentials:
        cred = OCTET_STRING (credentials)
    else:
        cred = ''
    return encode_bind_request (
        version,
        name,
        TLV (
            CHOICE (AUTH.sasl),
            OCTET_STRING (mechanism),
            cred
        )
    )

def encode_starttls ():
    # encode STARTTLS request: RFC 2830, 2.1

    # SEQUENCE(OCTET_STRING('1.3.6.1.4.1.1466.20037'), )
    # SEQUENCE('1.3.6.1.4.1.1466.20037')
    return TLV (
        APPLICATION(LDAP.ExtendedRequest),
        TLV (CHOICE (0, 0), '1.3.6.1.4.1.1466.20037')
    )

def encode_abandon(message_id):
    return TLV(APPLICATION(LDAP.AbandonRequest),
               INTEGER(message_id))

def encode_message (message_id, data):
    "encode/wrap an LDAP protocol message"
    # controls = SEQUENCE() # controls NYI, optional
    # return SEQUENCE (INTEGER (message_id), data, controls)
    return SEQUENCE (INTEGER (message_id), data)

def match_leaf(path, scope, leaf, base):
    """Match a  combination of leaf and base against a path
    based on scope.  path is the full DN returned by LDAP server.
    leaf and base form the match pattern.

    :Parameters:
        - `path`: Value of DN returned by the server(For example,
          the 'memberOf' attribute returned by AD server)
        - 'scope': Depth from the base DN to which search is performed.
          Can be one of SCOPE.* values
        - 'leaf': Leftmost part of the match pattern.  Usually it is
           CN=<groupname> for AD group matching.
        - 'base':  Rightmost part of the match pattern.  It is the base DN
          of LDAP server configured for external authentication with unnecessary
          spaces stripped.

          It is assumed that all DNs returnd by AD server does not have any
          spaces between values and attributes.  However, admin can configure
          baseDN with spaces between values and attributes. Before matching,
          such spaces are removed.
          For Ex: cn     =    admin_group,   cn  =   Users
                  is stripped to
                  cn=admin_group,cn=Users

    :Return:
        True if the leaf and base match the path in the given scope.
        Else False.
    """
    base = re_dn.sub(r'\1', base).strip()
    if scope == SCOPE.ONELEVEL:
        if path == ('%s,%s' % (leaf, base)):
            return True
    if scope == SCOPE.SUBTREE:
        if path.startswith(leaf) and path.endswith(base):
            return True
    return False

def normalize_dn(dn):
    """The first "cn" is assumed to be the Group Name and is case-sensitive.
    While the rest of the string is the base dn, which is supposed to be
    case-insensitive. Normalization is done accordingly.
    Ex: cn=admin_group,cn=Users,dc=ad1,dc=ibqa normalizes to
        CN=admin_group,CN=USERS,DC=AD1,DC=IBQA
    """
    dn = re_dn_attr.match(dn)
    return dn.group(1).upper() + dn.group(2) + dn.group(3).upper()

if __name__ == '__main__':
    sample = encode_message (
        3141,
        encode_search_request (
            'dc=ironport,dc=com',
            SCOPE.SUBTREE,
            DEREF.NEVER,
            0,
            0,
            0,
            '(&(objectclass=inetorgperson)(userid=srushing))',
            # '(&(objectclass=inetorgperson)(userid=newton))',
            # ask for these specific attributes only
            ['mailAlternateAddress', 'rfc822ForwardingMailbox']
        )
    )

    import pprint
    import socket
    s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
    s.connect (('printers.ironport.com', 389))
    s.send (sample)
    pprint.pprint (decode (s.recv (8192)))

########NEW FILE########
__FILENAME__ = ldapurl
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
ldapurl - handling of LDAP URLs as described in RFC 2255
written by Michael Stroeder <michael@stroeder.com>

See http://python-ldap.sourceforge.net for details.

\$Id: ldapurl.py,v 1.1 2007/04/07 06:30:26 tdraegen Exp $

Python compability note:
This module only works with Python 2.0+ since
1. string methods are used instead of module string and
2. list comprehensions are used.
"""

__version__ = '0.5.2'

__all__ = [
    # constants
    'SEARCH_SCOPE', 'SEARCH_SCOPE_STR',
    'LDAP_SCOPE_BASE', 'LDAP_SCOPE_ONELEVEL', 'LDAP_SCOPE_SUBTREE',
    # functions
    'isLDAPUrl',
    # classes
    'LDAPUrlExtension', 'LDAPUrlExtensions', 'LDAPUrl'
]

import UserDict

from urllib import quote, unquote

LDAP_SCOPE_BASE = 0
LDAP_SCOPE_ONELEVEL = 1
LDAP_SCOPE_SUBTREE = 2

SEARCH_SCOPE_STR = {None: '', 0: 'base', 1: 'one', 2: 'sub'}

SEARCH_SCOPE = {
    '': None,
    # the search scope strings defined in RFC2255
    'base': LDAP_SCOPE_BASE,
    'one': LDAP_SCOPE_ONELEVEL,
    'sub': LDAP_SCOPE_SUBTREE,
}

# Some widely used types
StringType = type('')
TupleType = type(())


def isLDAPUrl(s):
    """
    Returns 1 if s is a LDAP URL, 0 else
    """
    s_lower = s.lower()
    return \
        s_lower.startswith('ldap://') or \
        s_lower.startswith('ldaps://') or \
        s_lower.startswith('ldapi://')


def ldapUrlEscape(s):
    """Returns URL encoding of string s"""
    return quote(s).replace(',', '%2C').replace('/', '%2F')


class LDAPUrlExtension:
    """
    Class for parsing and unparsing LDAP URL extensions
    as described in RFC 2255.

    BNF definition of LDAP URL extensions:

         extensions = extension *("," extension)
         extension  = ["!"] extype ["=" exvalue]
         extype     = token / xtoken
         exvalue    = LDAPString from section 4.1.2 of [2]
         token      = oid from section 4.1 of [3]
         xtoken     = ("X-" / "x-") token

    Usable class attributes:
    critical
          Boolean integer marking the extension as critical
    extype
          Type of extension
    exvalue
          Value of extension
    """

    def __init__(self, extensionStr=None, critical=0, extype=None, exvalue=None):
        self.critical = critical
        self.extype = extype
        self.exvalue = exvalue
        if extensionStr:
            self._parse(extensionStr)

    def _parse(self, extension):
        extension = extension.strip()
        if not extension:
            # Don't parse empty strings
            self.extype, self.exvalue = None, None
            return
        self.critical = extension[0] == '!'
        if extension[0] == '!':
            extension = extension[1:].strip()
        self.extype, self.exvalue = extension.split('=', 1)
        self.extype = self.extype.strip()
        self.exvalue = unquote(self.exvalue.strip())

    def unparse(self):
        return '%s%s=%s' % (
            '!' * (self.critical > 0),
            self.extype, (self.exvalue or '').replace(',', r'%2C')
        )

    def __str__(self):
        return self.unparse()

    def __repr__(self):
        return '<%s.%s instance at %s: %s>' % (
            self.__class__.__module__,
            self.__class__.__name__,
            hex(id(self)),
            self.__dict__
        )

    def __eq__(self, other):
        return \
            (self.critical == other.critical) and \
            (self.extype == other.extype) and \
            (self.exvalue == other.exvalue)

    def __ne__(self, other):
        return not self.__eq__(other)


class LDAPUrlExtensions(UserDict.UserDict):
    """
    Models a collection of LDAP URL extensions as
    dictionary type
    """

    def __init__(self, default=None):
        UserDict.UserDict.__init__(self)
        for k, v in (default or {}).items():
            self[k] = v

    def __setitem__(self, name, value):
        """
        value
            Either LDAPUrlExtension instance, (critical,exvalue)
            or string'ed exvalue
        """
        assert isinstance(value, LDAPUrlExtension)
        assert name == value.extype
        self.data[name] = value

    def values(self):
        return [
            self[k]
            for k in self.keys()
        ]

    def __str__(self):
        return ','.join(map(str, self.values()))

    def __repr__(self):
        return '<%s.%s instance at %s: %s>' % (
            self.__class__.__module__,
            self.__class__.__name__,
            hex(id(self)),
            self.data
        )

    def __eq__(self, other):
        assert isinstance(other, self.__class__), TypeError(
            "other has to be instance of %s" % (self.__class__)
        )
        return self.data == other.data

    def parse(self, extListStr):
        for extension_str in extListStr.strip().split(','):
            e = LDAPUrlExtension(extension_str)
            self[e.extype] = e

    def unparse(self):
        return ','.join([v.unparse() for v in self.values()])


class LDAPUrl:
    """
    Class for parsing and unparsing LDAP URLs
    as described in RFC 2255.

    BNF definition of LDAP URL:

      hostport     host:port
      dn           distinguished name
      attributes   list with attributes
      scope        search scope string
      filter       LDAP search filter
      ldapurl    = scheme "://" [hostport] ["/"
                       [dn ["?" [attrs] ["?" [scope]
                       ["?" [filter] ["?" extensions]]]]]]

    Usable class attributes:
      urlscheme
          URL scheme (either ldap, ldaps or ldapi)
      hostport
          LDAP host (default '')
      dn
          String holding distinguished name (default '')
      attrs
          list of attribute types (default None)
      scope
          integer search scope for ldap-module
      filterstr
          String representation of LDAP Search Filters
          (see RFC 2254)
      extensions
          Dictionary used as extensions store
      who
          Maps automagically to bindname LDAP URL extension
      cred
          Maps automagically to X-BINDPW LDAP URL extension
    """

    attr2extype = {'who': 'bindname', 'cred': 'X-BINDPW'}

    def __init__(
        self,
        ldapUrl=None,
        urlscheme='ldap',
        hostport='', dn='', attrs=None, scope=None, filterstr=None,
        extensions=None,
        who=None, cred=None
    ):
        self.urlscheme = urlscheme
        self.hostport = hostport
        self.dn = dn
        self.attrs = attrs
        self.scope = scope
        self.filterstr = filterstr
        self.extensions = (extensions or LDAPUrlExtensions({}))
        if ldapUrl is not None:
            self._parse(ldapUrl)
        if who is not None:
            self.who = who
        if cred is not None:
            self.cred = cred

    def __eq__(self, other):
        return \
            self.urlscheme == other.urlscheme and \
            self.hostport == other.hostport and \
            self.dn == other.dn and \
            self.attrs == other.attrs and \
            self.scope == other.scope and \
            self.filterstr == other.filterstr and \
            self.extensions == other.extensions

    def __ne__(self, other):
        return not self.__eq__(other)

    def _parse(self, ldap_url):
        """
        parse a LDAP URL and set the class attributes
        urlscheme,host,dn,attrs,scope,filterstr,extensions
        """
        if not isLDAPUrl(ldap_url):
            raise ValueError('Parameter ldap_url does not seem to be a LDAP URL.')
        scheme, rest = ldap_url.split('://', 1)
        self.urlscheme = scheme.strip()
        if self.urlscheme not in ['ldap', 'ldaps', 'ldapi']:
            raise ValueError('LDAP URL contains unsupported URL scheme %s.' % (self.urlscheme))
        slash_pos = rest.find('/')
        qemark_pos = rest.find('?')
        if (slash_pos == -1) and (qemark_pos == -1):
            # No / and ? found at all
            self.hostport = unquote(rest)
            self.dn = ''
            return
        else:
            if slash_pos != -1 and (qemark_pos == -1 or (slash_pos < qemark_pos)):
                # Slash separates DN from hostport
                self.hostport = unquote(rest[:slash_pos])
                # Eat the slash from rest
                rest = rest[slash_pos + 1:]
            elif qemark_pos != 1 and (slash_pos == -1 or (slash_pos > qemark_pos)):
                # Question mark separates hostport from rest, DN is assumed to be empty
                self.hostport = unquote(rest[:qemark_pos])
                # Do not eat question mark
                rest = rest[qemark_pos:]
            else:
                raise ValueError('Something completely weird happened!')
        paramlist = rest.split('?')
        paramlist_len = len(paramlist)
        if paramlist_len >= 1:
            self.dn = unquote(paramlist[0]).strip()
        if (paramlist_len >= 2) and (paramlist[1]):
            self.attrs = unquote(paramlist[1].strip()).split(',')
        if paramlist_len >= 3:
            scope = paramlist[2].strip()
            try:
                self.scope = SEARCH_SCOPE[scope]
            except KeyError:
                raise ValueError(
                    "Search scope must be either one of base, one or sub. LDAP URL contained %s" %
                    (repr(scope)))
        if paramlist_len >= 4:
            filterstr = paramlist[3].strip()
            if not filterstr:
                self.filterstr = None
            else:
                self.filterstr = unquote(filterstr)
        if paramlist_len >= 5:
            self.extensions = LDAPUrlExtensions()
            self.extensions.parse(paramlist[4])
        return

    def applyDefaults(self, defaults):
        """
        Apply defaults to all class attributes which are None.

        defaults
            Dictionary containing a mapping from class attributes
            to default values
        """
        for k in defaults.keys():
            if getattr(self, k) is None:
                setattr(self, k, defaults[k])

    def initializeUrl(self):
        """
        Returns LDAP URL suitable to be passed to ldap.initialize()
        """
        if self.urlscheme == 'ldapi':
            # hostport part might contain slashes when ldapi:// is used
            hostport = ldapUrlEscape(self.hostport)
        else:
            hostport = self.hostport
        return '%s://%s' % (self.urlscheme, hostport)

    def unparse(self):
        """
        Returns LDAP URL depending on class attributes set.
        """
        if self.attrs is None:
            attrs_str = ''
        else:
            attrs_str = ','.join(self.attrs)
        scope_str = SEARCH_SCOPE_STR[self.scope]
        if self.filterstr is None:
            filterstr = ''
        else:
            filterstr = ldapUrlEscape(self.filterstr)
        dn = ldapUrlEscape(self.dn)
        if self.urlscheme == 'ldapi':
            # hostport part might contain slashes when ldapi:// is used
            hostport = ldapUrlEscape(self.hostport)
        else:
            hostport = self.hostport
        ldap_url = '%s://%s/%s?%s?%s?%s' % (
            self.urlscheme,
            hostport, dn, attrs_str, scope_str, filterstr
        )
        if self.extensions:
            ldap_url = ldap_url + '?' + self.extensions.unparse()
        return ldap_url

    def htmlHREF(self, urlPrefix='', hrefText=None, hrefTarget=None):
        """Complete """
        assert isinstance(urlPrefix, StringType), "urlPrefix must be StringType"
        if hrefText is None:
            hrefText = self.unparse()
        assert isinstance(hrefText, StringType), "hrefText must be StringType"
        if hrefTarget is None:
            target = ''
        else:
            assert isinstance(hrefTarget, StringType), "hrefTarget must be StringType"
            target = ' target="%s"' % hrefTarget
        return '<a%s href="%s%s">%s</a>' % (
            target, urlPrefix, self.unparse(), hrefText
        )

    def __str__(self):
        return self.unparse()

    def __repr__(self):
        return '<%s.%s instance at %s: %s>' % (
            self.__class__.__module__,
            self.__class__.__name__,
            hex(id(self)),
            self.__dict__
        )

    def __getattr__(self, name):
        if name in self.attr2extype:
            extype = self.attr2extype[name]
            if extype in self.extensions:
                result = unquote(self.extensions[extype].exvalue)
            else:
                return None
        else:
            raise AttributeError("%s has no attribute %s" % (
                self.__class__.__name__, name
            ))
        return result  # __getattr__()

    def __setattr__(self, name, value):
        if name in self.attr2extype:
            extype = self.attr2extype[name]
            if value is None:
                # A value of None means that extension is deleted
                delattr(self, name)
            elif value is not None:
                # Add appropriate extension
                self.extensions[extype] = LDAPUrlExtension(
                    extype=extype, exvalue=unquote(value)
                )
        else:
            self.__dict__[name] = value

    def __delattr__(self, name):
        if name in self.attr2extype:
            extype = self.attr2extype[name]
            try:
                del self.extensions[extype]
            except KeyError:
                pass
        else:
            del self.__dict__[name]

########NEW FILE########
__FILENAME__ = ldap_client
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# TODO:
#
# - most of the client API is done; what's not:
#       add "max_inflight_ldap_operations" value
#       Make this dynamic, based on round-trip time for a gold star.
# - BIND handling:
#   - bug 28550
# - Put a bound on the size of the server-group's unattached_inquiries list
#   Existing code has no bound.
# - Handle abandoning of inquiries (eg, if search time takes too long (read_timeout))

import copy
import coro
import coro_ssl
import dnsqr
import dnsrcode
import dns_exceptions
import inet_utils
import ldap
import ldapurl
import ldap_api
import ldap_cmd
import lru
import math
import sslip
import sys
import tb

PROTOCOL_VERSION = 3

TRANSPORT_PLAINTEXT = 0
TRANSPORT_SSL = 1
TRANSPORT_STARTTLS = 2

DEFAULT_COMPATIBILITY = {}
DEFAULT_INQUIRY_TIMEOUT = 120
DEFAULT_MAX_CONNS = 1
DEFAULT_MAX_TIME_PER_CONN = 21600
DEFAULT_MAX_REQUESTS_PER_CONN = 10000

CONNECT_BEHAVIOR_LOAD_BALANCE = 0
CONNECT_BEHAVIOR_FAILOVER = 1

DEFAULT_CACHE_SIZE = 10000
DEFAULT_CACHE_TTL = 0

DEFAULT_FAILOVER_TIMEOUT = 120
DEFAULT_READ_TIMEOUT = 30

MAX_REFERRAL_DEPTH = 10

PORT_LDAP = 389
PORT_LDAPS = 636

# Special processing for the error
INQUIRY_TIMED_OUT = 'inquiry timed out'

######################################################################
# INQUIRIES
#
# Inquiries encapsulate an individual LDAP Operation.  Inquiries contain
# a conditional variable on which routines can wait until resolution has
# been reached.
#
# Every supported LDAP Operation is implemented by a derived Inquiry
# class.  Each subclass then implements Operation specific result
# handling.
#
# Referral handling is a special case.  In order to follow a referral,
# an inquiry can request the ldap_client to follow the referral until it
# terminates.  In this regard, as much as possible inquiries are kept
# separate from the underlying transport.
#
# There is a twist wrt referral handling.  Referrals can include
# operation-modifying parameters.  This means that chains of referrals
# can incrementally change the original parameters; therefore, referrals
# themselves are a tuple of (parameters, URI).  By combining the two
# elements, a new operation is capable of progressing.
######################################################################
class Inquiry (object):

    __slots__ = \
        ('birth',              # Time this inquiry was created, in coro ticks.
         'timeout',            # Time this inquiry should live, in coro ticks.
         'state',              # UNATTACHED, ATTACHED, or DONE
         'rendered_request',   # Either None or a packed LDAP message, as string
         'cv',                 # None or a condition variable
         'result',             # None or a (success, result) tuple
         'referrals',          # List of referrals to follow
         'done_referrals',     # List of already followed referrals
         'resolver',           # The ldap server "(host:port)" services this inquiry.
         # taken from the ldap_connection where this inquiry
         # is resolved
         )
    STATE_UNATTACHED = 0
    STATE_ATTACHED = 1
    STATE_DONE = 2

    def __init__(self, server_group, rendered_request, timeout):
        self.server_group = server_group
        self.birth = coro.now
        self.timeout = timeout
        self.state = self.STATE_UNATTACHED
        self.rendered_request = rendered_request
        self.cv = coro.condition_variable()
        self.result = None
        self.referrals = []
        self.done_referrals = []
        self.resolver = None

    def state_to_timeout(self):
        self.state = self.STATE_DONE
        self.rendered_request = None
        self.result = (False, INQUIRY_TIMED_OUT)
        cv = self.cv
        if cv:
            self.cv = None
            cv.wake_all()

    def state_to_attached(self):
        assert(self.rendered_request is not None)
        self.state = self.STATE_ATTACHED
        return self.rendered_request

    def state_to_unattached(self):
        self.state = self.STATE_UNATTACHED

    def value_to_referral_list(self, value):
        """value_to_referral_list(value)
        Convert a fresh-from-BER-decoding referral into the list of
        parameter + URIs.

        Value is (result, base, search, ('context', 3, [uri list]))."""
        try:
            uri_list = value[3][2]
            for uri in uri_list:
                self.referrals.append((self.get_parameters(), uri))
        except IndexError:
            pass

    def handle_referrals(self):
        if self.referrals and not self.server_group:
            log.write('LDAP.DEBUG', "Could not follow referral: referral returned for transport-layer LDAP operation")
            return False

        while self.referrals and \
                (len(self.done_referrals) < MAX_REFERRAL_DEPTH):

            # Referrals are FIFO
            params, ref = self.referrals.pop(0)
            if ref in self.done_referrals:
                continue
            self.done_referrals.append(ref)

            # Tell the client to follow this referral
            if self.server_group.inquiry_referral(self, params, ref):
                log.write('LDAP.DEBUG', "Following referral: %s" % (ref,))
                return True
            else:
                log.write('LDAP.DEBUG', "Could not follow referral: %s" % (ref,))

        return False

    def handle_continuations(self):
        """handle_continuations()
        Search continuation handling hook.  Only Search inquiries need
        to override."""
        return False

    def state_to_done(self, success, result, resolver=None):
        if self.state is self.STATE_DONE:
            return

        assert(self.cv is not None)

        # Handle referrals before allowing completion
        if self.handle_referrals():
            return

        # Search continuation hook
        if self.handle_continuations():
            return

        cv = self.cv

        if self.state is self.STATE_UNATTACHED:
            self.rendered_request = None

        self.resolver = resolver
        self.state = self.STATE_DONE
        self.cv = None

        # Fix up results of (True, None).  These are referrals that
        # end without being resolved (no servers around to follow).
        # So, if result is (True, None) and we've followed some
        # referrals without actually getting data, return the right
        # thing.
        if (success is True) and (result is None) and \
           len(self.done_referrals) and (self.result is None):
            self.result = (False, "Referral following yielded no result.")
        else:
            self.result = (success, result)

        if cv:
            cv.wake_all()

    def wait_on_cv(self):
        # See bug 33294.  If cv doesn't exist, the inquiry has been
        # resolved before a cache-context has had a chance to update.
        if self.cv is not None:
            self.cv.wait()

    def get_rendered_operation(self):
        return self.rendered_request

    def get_result(self):
        return self.result

    def get_birth(self):
        return self.birth

    def get_parameters(self):
        raise Exception("Implement get_parameters()")
        return None

    def process_response(self, response):
        """Common processing."""
        try:
            kind, op, value = response
        except (ValueError, TypeError):
            return (False, 'Cannot find kind/code/answer: %s' % (response,))
        if kind != ldap.kind_application:
            return (False, '"kind" not application: %s' % (response,))

        # Note: any LDAPResult can return a result code of "referral".
        # At this point in processing we don't know if this response is
        # in fact an LDAPResult -- it could be an intermediary search
        # result.

        return self._inquiry_specific_processing(op, value)

    def _inquiry_specific_processing(self, op, value):
        """Derived classes must implement this."""
        __pychecker__ = 'unusednames=op,value'
        raise Exception("Implement process_response()")

    def update_from_referral(self, params, lurl):
        """update_from_referral(parameters, LDALUrl)
        Update this Inquiry with referral information.
        See RFC 4511 section 4.1.10
        """
        __pychecker__ = 'unusednames=params,lurl'
        raise Exception("Implement update_from_referral()")

    def get_time_to_live(self):
        """Return how much time left (in seconds) before this inquiry should
        be timed out if not resolved
        """
        time_to_live = (self.birth + self.timeout - coro.now) / float(coro.ticks_per_sec)
        if time_to_live > 0.0:
            return time_to_live
        else:
            return 0.0


class Inquiry_StartTLS(Inquiry):
    """Inquiry_StartTLS - object to implement StartTLS handling."""

    def __init__(self, server_group, timeout):
        Inquiry.__init__(self, server_group, ldap.encode_starttls(), timeout)

    def get_parameters(self):
        return None

    def _inquiry_specific_processing(self, op, value):
        # StartTLS specific handling
        if op == ldap.LDAP.ExtendedResponse:
            try:
                if value[0] == ldap.RESULT.success:
                    return (True, None)
                elif value[0] == ldap.RESULT.referral:
                    self.value_to_referral_list(value)
                    return (True, None)
                else:
                    # bug 22139, paranoid check for protocol error from
                    # the server
                    return (False, 'Got error response from starttls, '
                            'but cannot parse the error: %r' % value)
            except (IndexError, TypeError):
                return (False, 'Cannot find reponse to starttls: %r' % value)
        else:
            return (False, 'Unexpected response code: %r' % op)

    def update_from_referral(self, params, lurl):
        """update_from_referral(parameters, LDALUrl)
        Update this Inquiry with referral information.
        See RFC 4511 section 4.1.10

        StartTLS inquiries are parameterless, therefore referrals do not
        affect the previously encoded Operation.
        """
        __pychecker__ = 'unusednames=params,lurl'
        pass

class Inquiry_Simple_Bind(Inquiry):
    """Inquiry_Simple_Bind - object to encapsulate BindRequest (with
    "simple" authentication) handling."""

    def __init__(self, server_group, user, pword, timeout):
        Inquiry.__init__(self, server_group,
                         ldap.encode_simple_bind(PROTOCOL_VERSION, user, pword),
                         timeout)
        self.user = user
        self.pword = pword

    def get_parameters(self):
        return (self.user, self.pword)

    def _inquiry_specific_processing(self, op, value):
        # BIND specific handling
        if op == ldap.LDAP.BindResponse:
            try:
                if value[0] == ldap.RESULT.success:
                    return (True, True)
                elif value[0] == ldap.RESULT.referral:
                    self.value_to_referral_list(value)
                    return (True, None)
                else:
                    return (True, False)
            except (IndexError, TypeError):
                return (False, 'Cannot find response to bind: %r' % (value,))
        else:
            return (False, 'Unexpected response code: %r' % (value,))

    def update_from_referral(self, params, lurl):
        """update_from_referral(parameters, LDALUrl)
        Update this Inquiry with referral information.
        See RFC 4511 section 4.1.10

        Except for username and password, simple BIND inquiries are
        parameterless, therefore referrals do not affect the previously
        encoded Operation.
        """
        __pychecker__ = 'unusednames=params,lurl'
        pass

class Inquiry_Search(Inquiry):
    """Inquiry_Search - object to implement LDAP Search handling."""

    def __init__(self, server_group, base, query_str, look_for_attrs,
                 compatibility, timeout):
        encoded_req = ldap.encode_search_request(base, None,
                                                 ldap.DEREF.NEVER, 0, 0,
                                                 0, query_str,
                                                 look_for_attrs,
                                                 compatibility)
        Inquiry.__init__(self, server_group, encoded_req, timeout)
        self.base = base
        self.query_string = query_str
        self.look_for_attrs = look_for_attrs
        self.compatibility = compatibility
        self.search_results = []
        self.continuations = []  # search-specific form of referral
        self.explored_continuations = []

    def get_parameters(self):
        return (self.base, self.query_string, self.look_for_attrs, self.compatibility)

    def _inquiry_specific_processing(self, op, value):
        # Search specific handling
        #
        # Includes continuation handling -- aka search-based referrals
        if op == ldap.LDAP.SearchResultEntry:
            try:
                result_dn, attrs = value
                self.search_results.append((result_dn, dict(attrs)))
            except (ValueError, TypeError):
                return (False, 'Cannot decode DN and attributes/values: %r' % (value,))
            return (None, 'continue')
        elif op == ldap.LDAP.SearchResultDone:
            if value[0] == ldap.RESULT.success:
                return (True, self.search_results)
            elif value[0] == ldap.RESULT.referral:
                self.value_to_referral_list(value)
                return (True, None)
            elif value[0] == ldap.RESULT.noSuchObject:
                # if the searched object does not exist and we are done
                # searching, we should treat it as not-found, instead of
                # server-error (return status of False indicate server error)
                log.write('LDAP.DEBUG', value[2])  # log the error message
                return (True, [])
            else:
                return (False, value)
        elif op == ldap.LDAP.SearchResultReference:
            if not isinstance(value, list):
                value = [value]
            for val in value:
                self.continuations.append((self.get_parameters(), val))
            return (None, 'continue')
        else:
            return (False, 'Unexpected response code: %r' % (value,))

    def handle_continuations(self):
        """handle_continuations()
        Search continuation handling hook.  Class Inquiry_Search
        overrides."""
        if self.continuations and not self.server_group:
            log.write(
                'LDAP.DEBUG',
                "Could not follow continuations: continuation returned for transport-layer LDAP operation")
            return False

        while self.continuations and \
                (len(self.explored_continuations) < MAX_REFERRAL_DEPTH):

            # Continuations are FIFO
            params, c = self.continuations.pop(0)
            if c in self.explored_continuations:
                continue
            self.explored_continuations.append(c)

            # Tell the client to follow this continuation
            if self.server_group.inquiry_continuation(self, params, c):
                log.write('LDAP.DEBUG', "Query %s following continuation: %s" % (self.get_query_string(), c))
                return True
            else:
                log.write('LDAP.DEBUG', "Query %s could not follow continuation: %s" % (self.get_query_string(), c))

        return False

    def get_query_string(self):
        """get_query_string()
        Return query string for logging use."""
        return self.query_string

    def update_from_referral(self, params, lurl):
        """update_from_referral(parameters, LDALUrl)
        Update this Inquiry with referral information.
        See RFC 4511 section 4.1.10

        Search inquiries are affected by DN, filter, scope, "other"
        modifications.  For now we only care about DN and filter.
        """
        self.base, self.query_string, self.look_for_attrs, self.compatibility = params
        if lurl.dn:
            self.base = lurl.dn
        if lurl.filterstr:
            self.query_string = lurl.filterstr
        encoded_req = ldap.encode_search_request(self.base, None,
                                                 ldap.DEREF.NEVER, 0, 0,
                                                 0, self.query_string,
                                                 self.look_for_attrs,
                                                 self.compatibility)
        self.rendered_request = encoded_req

######################################################################
# CACHE ENTRIES
#
# This class encapsulates an LDAP response.  If no response exists, this
# class provides the interface between the calling thread and the inner
# workings of the ldap_client.
######################################################################
class ldap_cache_entry:

    CACHE_STATE_NEW = 0
    CACHE_STATE_INPROGRESS = 1
    CACHE_STATE_RESOLVED = 2

    def __init__(self, key):
        self.key = key
        self.when = 0
        self.data = ((False, None), None)
        self.inquiry = None
        self.state = self.CACHE_STATE_NEW
        self.resolver = None

    def abort(self, sg_ctx):
        self._set_data((False, []))
        self._state_to_resolved(sg_ctx)

    def _state_to_resolved(self, sg_ctx):
        # Delete this entry from the nascent cache list
        try:
            del sg_ctx.new_cache[self.key]
        except KeyError:
            pass

        # Results will stick around for cache-ttl time
        self._set_time(coro.now)

        # Update state
        self.state = self.CACHE_STATE_RESOLVED

    def _set_resolved(self, sg_ctx):
        """_set_resolved(sg_ctx)
        Resolve a cache-entry.  Supplied server-group object is required
        to place the cache-entry into the correct cache."""
        if not self._is_resolved():
            self._set_data(self.inquiry.get_result(), self.inquiry.resolver)
            self._state_to_resolved(sg_ctx)

            # Free up the inquiry
            self.inquiry = None

            # Place this ldap_cache_entry into the appropriate
            # cache (positive or negative).
            (success, result), resolver = self.data
            if sg_ctx.cache_size:

                # A positive cache hit is considered to be an inquiry
                # that both successfully resolved *and* contains
                # non-empty information.
                if success:
                    if result:
                        if not sg_ctx.pos_cache:
                            sg_ctx.pos_cache = lru.lru(sg_ctx.cache_size)
                        sg_ctx.pos_cache[self.key] = self
                    elif sg_ctx.is_ready():
                        # Only add to the neg cache if the server is operational
                        # ie. do not cache transport, or server failure results
                        if not sg_ctx.neg_cache:
                            sg_ctx.neg_cache = lru.lru(sg_ctx.cache_size)
                        sg_ctx.neg_cache[self.key] = self

    def _is_resolved(self):
        return (self.state == self.CACHE_STATE_RESOLVED)

    def _set_inprogress(self):
        self.state = self.CACHE_STATE_INPROGRESS

    def _set_data(self, data, resolver=None):
        """Store the ldap query result for this Inquiry

        :Parameters:
            - 'data' : result from ldap query resolution as a tuple
                       (<server status>, <ldap result>)
            - 'resolver' : the ldap server where the result is obtained
        """
        try:
            status, result = data
            self.data = ((status, result), resolver)
        except (TypeError, ValueError):
            self.data = ((False, []), resolver)
        except:
            self.data = ((False, []), resolver)
            raise

    def get_data(self, sg_ctx):
        """Return the query's result.  This call might block if the inquiry
        has not been resolved.

        :Parameters:
            - 'sg_ctx' : the server group handing this query

        :Return:
            ((server_status, results), resolver)
        """
        if self._is_resolved():
            return self.data

        def __wait_on_inquiry(ce, sg):
            ce.inquiry.wait_on_cv()
            ce._set_resolved(sg)
            return ce.data

        if not sg_ctx.is_down():
            # Wait for query resolve if the server is not down
            try:
                return coro.with_timeout(self.inquiry.get_time_to_live(), __wait_on_inquiry, self, sg_ctx)
            except coro.TimeoutError:
                # At this point, the inquiry has passed its timeout
                if self.inquiry:
                    self.inquiry.state_to_timeout()
                    self._set_resolved(sg_ctx)
                return self.data
        else:
            if self.check_ttl_expired(sg_ctx):
                return self.data
            else:
                return ((False, "LDAP server misconfigured or unreachable"), None)

    def check_ttl_expired(self, sg_ctx):
        """ This function checks if this entry has inquiry that is older than
        the inquiry time-to-live. If it is older, it sets the inquiry state to
        timeout and deletes it from the new_cache.

        :Returns:
            - True - if the entry was stale
            - False - Otherwise
        """
        if self.inquiry.get_time_to_live() <= 0:
            self.inquiry.state_to_timeout()
            self._set_resolved(sg_ctx)
            log.write('LDAP.DEBUG', "Cleared stale entry: %s" % (self.key,))
            return True

        return False

    def _set_time(self, time):
        self.when = time

    def set_inquiry(self, inquiry):
        self._set_inprogress()
        self.inquiry = inquiry

######################################################################
# CLIENT
#
# The ldap_client class is the global context.  It contains the
# one-per-process library-like initialization data.  It also acts
# as a container for server-groups.
######################################################################
class ldap_client:
    def __init__(self):
        self.server_groups = {}

    # server group API
    def create_server_group(self, name, behavior=CONNECT_BEHAVIOR_FAILOVER,
                            bind_ip=None):
        """ Create and return an empty server-group. """
        if name in self.server_groups:
            return None
        self.server_groups[name] = ldap_server_group(self, name, behavior,
                                                     bind_ip)
        return self.server_groups[name]

    def get_server_group(self, name):
        """ Given a server-group name, return an object. """
        return self.server_groups.get(name, None)

    def remove_server_group(self, name):
        """ Remove an existing server-group. """
        try:
            self.server_groups[name].shutdown()
            del self.server_groups[name]
        except KeyError:
            return

    def destroy_client(self):
        for sg in self.server_groups.values():
            sg.shutdown()

    # cache API
    def clear_all_caches(self):
        """ Clear all caches. """
        for sg in self.server_groups.values():
            sg.clear_cache(True)

    # Referral helper
    def find_server(self, host, port):
        """ Find a server. """
        for sg in self.server_groups.values():
            for server in sg.servers:
                if server.hostname == host and \
                   int(server.port) == int(port):
                    return server
        return None

######################################################################
# SERVER GROUPS
#
# The server-group class is a collection of servers.  A collection can
# be in either a "failover" or a "round-robin" configuration.
#
# This class also maintains the inquiry cache, which provides an
# interface between the application (which is essentially doing a simple
# query) and the mechanics of resolving LDAP operations.
#
# This class exposes the query API to the application.  To the
# application, only "searching" and "binding" are currently exposed.
#
# Finally, this class utilizes a single worker thread to manage servers,
# deal with configuration updates, and to manage the resolution of
# inquiries.
######################################################################
def LDAPUrl_to_host_port(lurl):
    host = None
    port = None
    try:
        colon = lurl.hostport.index(':')
    except ValueError:
        host = lurl.hostport
        if lurl.urlscheme == 'ldaps':
            port = PORT_LDAPS
        else:
            port = PORT_LDAP
    else:
        host = lurl.hostport[:colon]
        port = lurl.hostport[colon + 1:]
    return(host, port)

class ldap_server_group:    # server container

    def __init__(self, client, name, behavior, bind_ip):
        self.client = client
        self.name = name
        self.sg_thread = None
        self.servers = []
        self.server_pos = 0
        self.connection_behavior = behavior
        self.bind_ip = bind_ip
        self.new_cache = {}     # Nascent cache entries
        self.pos_cache = None   # LRU-based cache for positive results
        self.neg_cache = None   # LRU-based cache for negative results
        self.unattached_inquiries = []
        self.unattached_bind_inquiries = []
        self.pending_inquiries_event = False
        self.pending_bind_inquiries_event = False
        self.worker_fifo = coro.fifo()      # For immediate consumption
        self.event_list = []                # For describing future events
        self._shutting_down = 0             # 0 - not shutting down
        # 1 - shutting down servers
        # 2 - dead
        self.cache_size = DEFAULT_CACHE_SIZE
        self.cache_ttl = DEFAULT_CACHE_TTL
        self.compatibility = DEFAULT_COMPATIBILITY
        self.inquiry_timeout = DEFAULT_INQUIRY_TIMEOUT * coro.ticks_per_sec
        self.max_conns = DEFAULT_MAX_CONNS
        self.max_time_per_conn = DEFAULT_MAX_TIME_PER_CONN
        self.max_requests_per_conn = DEFAULT_MAX_REQUESTS_PER_CONN
        self.failover_timeout = DEFAULT_FAILOVER_TIMEOUT
        self.operation_timeout = DEFAULT_READ_TIMEOUT

        # These commands are used most often, so use one insteads of
        # creating anew everytime.
        self.cmd_inquiries = ldap_cmd.CmdInquiries()
        self.cmd_bind_inquiries = ldap_cmd.CmdBindInquiries()

    def start(self):
        """ Kick off the server-group by spawning a worker. """
        if not self.sg_thread and not self._shutting_down:
            self.sg_thread = coro.spawn(self.sg_thread_bootstrap)

    def shutdown(self):
        self.worker_fifo.push(ldap_cmd.CmdShutdown())

    def add_server(self, hostname, port=None, authtype='anonymous',
                   authdata={}, transport=TRANSPORT_PLAINTEXT):
        """ Add a server to this server-group.
        The following must be updated afer the server """
        if self._shutting_down:
            return
        new_server = ldap_server(self, hostname, port, self.bind_ip, authtype,
                                 authdata, transport)
        self.servers.append(new_server)
        self.worker_fifo.push(ldap_cmd.CmdServerState(new_server))

    # Server group accessors
    def get_name(self):
        """get_name() -> name"""
        return self.name

    def get_number_of_servers(self):
        """get_number_of_servers() -> number of servers"""
        return len(self.servers)

    # Configuration updates.  All others require a new server-group.
    def set_compatibility(self, compatibility):
        assert(isinstance(compatibility, dict))
        self.worker_fifo.push(ldap_cmd.CmdSetCompat(copy.deepcopy(compatibility)))

    def set_inquiry_timeout(self, timeout):
        assert(isinstance(timeout, int))
        self.worker_fifo.push(ldap_cmd.CmdSetInqTimeout(timeout))

    def set_max_connections(self, max_connections):
        assert(isinstance(max_connections, int))
        self.worker_fifo.push(ldap_cmd.CmdSetMaxConns(max_connections))

    def set_max_requests_per_connection(self, max_requests_per_connection):
        assert(isinstance(max_requests_per_connection, int))
        self.worker_fifo.push(ldap_cmd.CmdSetMaxRequestsPerConn(max_requests_per_connection))

    def set_max_time_per_connection(self, max_time_per_connection):
        assert(isinstance(max_time_per_connection, int))
        self.worker_fifo.push(ldap_cmd.CmdSetMaxTimePerConn(max_time_per_connection))

    def set_cache_size(self, cache_size):
        self.worker_fifo.push(ldap_cmd.CmdSetCacheSize(cache_size))

    def set_cache_ttl(self, cache_ttl):
        self.worker_fifo.push(ldap_cmd.CmdSetCacheTtl(cache_ttl))

    # Server group knobs when using client as diagnostic tool
    def set_operation_timeout(self, timeout):
        """set_operation_timeout(timeout)
        Change the LDAP operation timeout to <timeout> seconds.  This is
        used to limit the amount of time an operation is allowed to
        resolve against individual servers."""
        self.worker_fifo.push(ldap_cmd.CmdSetOperationTimeout(timeout))

    def set_failover_timeout(self, timeout):
        """set_failover_timeout()
        Change the amount of time a server can be DOWN before a failover
        event is noted."""
        self.worker_fifo.push(ldap_cmd.CmdSetFailoverTimeout(timeout))

    # Cache API
    def clear_cache(self, refresh_ip_list=False):
        """ Clear the server-group cache.  Only clear if the group is
        actually running."""
        if self.sg_thread and not self._shutting_down:
            log.write('LDAP.DEBUG',
                      'Clearing LDAP server-group "%s" cache' % self.name)
            self.pos_cache = None
            self.neg_cache = None

            # Reset the query time to expire IPs in ip list
            if refresh_ip_list:
                for srv in self.servers:
                    srv.ip_list = [(0, ip) for ttl, ip in srv.ip_list]

    def get_cache_context(self, key):
        """get_cache_context(key) -> cache context
        Retrieve the cache context if it exists.  If the context has
        been resolved, caller will pull results from it.  If the context
        references an inquiry that is still resolving, then the caller
        waits (using the context).

        The cache exists all the time, regardless of cache-size. If the
        cache-size is zero, entries are not preserved; however,
        concurrent requests are still serviced by a single LDAP
        transaction."""

        # Figure out if we already have a cache entry for this
        cache_to_use = None
        if key in self.new_cache:
            entry = self.new_cache[key]
            # If the entry is stale (pending more than inquiry_timeout) then do not
            # use this inquiry as it will fail this query immediately
            if not entry.check_ttl_expired(self):
                # new_cache is always not resolved yet, simply return it
                return (False, entry)
        # Check the positive cache
        elif self.pos_cache and key in self.pos_cache:
            cache_to_use = self.pos_cache
        # Check the negative cache
        elif self.neg_cache and key in self.neg_cache:
            cache_to_use = self.neg_cache

        if cache_to_use:
            entry = cache_to_use[key]
            if (coro.now - entry.when) < (self.cache_ttl * coro.ticks_per_sec):
                # Cache hit is still valid
                return (False, entry)

        # Create a new entry and add to to the nascent cache list
        ce = ldap_cache_entry(key)
        self.new_cache[key] = ce

        return (True, ce)

    def clear_cache_entry(self, query_string):
        """clear_cache_entry(query_string) -> Bool
        Clear caches of results for specific query_string.
        Returns whether any caches have been cleared."""
        # This interface is used by ldap_api to implement a remote_cmd
        # call.  ldap_api needs to change to pass the query_string,
        # instead of (queryname, address, group_or_envelop_sender)

        # qta - It is very inefficient that we have to do a
        # table scan to clear the cache.  Restructure the
        # cache to avoid this problem!

        def __clear_cache_entry(cache):
            cache_entry_found = False
            if cache:
                for (k, v) in cache.items():
                    if k[0] == query_string:
                        del cache[k]
                        cache_entry_found = True
                        log.write('LDAP.DEBUG', "Cache cleared: %s" % (query_string,))
            return cache_entry_found

        if __clear_cache_entry(self.pos_cache):
            return True
        if __clear_cache_entry(self.neg_cache):
            return True

        log.write('LDAP.DEBUG', "Cache not found: %s" % (query_string,))
        return False

    # query API
    def enqueue_operation(self, operation):
        """ Place the operation & context onto the server's
        "execute me" queue. """
        # Place operation/Inquiry on ops-to-execute queue
        self.unattached_inquiries.append(operation)
        # Only service inquiries if the server group is ready.
        if not self.pending_inquiries_event:
            self.pending_inquiries_event = True
            self.worker_fifo.push(self.cmd_inquiries)

    def enqueue_bind_operation(self, operation):
        """ Place the BIND operation & context onto the server's
        BIND "execute me" queue. """
        # Place operation/Inquiry on ops-to-execute queue
        self.unattached_bind_inquiries.append(operation)
        # Only service inquiries if the server group is ready.
        if not self.pending_bind_inquiries_event:
            self.pending_bind_inquiries_event = True
            self.worker_fifo.push(self.cmd_bind_inquiries)

    def search_query(self, query_string, base, look_for_attrs):  # maybe timeout &
        """search_query(query_string, base, look_for_attrs)
        XXX previously known as "query()" to ldap_api.  App doesn't care
        which of the underlying servers executes the "query" (LDAPSearch
        operation)."""
        if self._shutting_down:
            return None

        # Grab a cache/query context (may or may not be resolved)
        nascent, ctx = self.get_cache_context((query_string, base, "%s" % (look_for_attrs,)))

        if nascent and ctx.inquiry is None:
            try:
                # Get ourselves an appropriate Inquiry
                the_inc = Inquiry_Search(self, base, query_string, look_for_attrs,
                                         self.compatibility, self.inquiry_timeout)
            except ldap.QuerySyntaxError as e:
                # Abort this operation
                ctx.abort(self)
                return ((False, str(e)), None)

            ctx.set_inquiry(the_inc)
            # Enqueue this operation for future processing by the client
            self.enqueue_operation(the_inc)

        elif ctx._is_resolved():
            log.write('LDAP.DEBUG', "Query %s resolved via cache hit" % (query_string,))

        return ctx.get_data(self)

    def simple_bind_query(self, username, password):
        """simple_bind_query(username, password)
        BIND API (using 'simple' authentication)."""

        # Explicitly disallow 'unauthenticated bind'
        if username and not password.strip():
            return (
                (False, "Failed binding with user '%s' and supplied password: Invalid credentials" % username), None)

        if self._shutting_down:
            return None

        # Grab a cache/query context (may or may not be resolved)
        nascent, ctx = self.get_cache_context((username, password))

        if nascent:
            # Get ourselves an appropriate Inquiry
            the_inc = Inquiry_Simple_Bind(self, username, password,
                                          self.inquiry_timeout)
            ctx.set_inquiry(the_inc)

            # Enqueue this operation for future processing by the client
            self.enqueue_bind_operation(the_inc)

        return ctx.get_data(self)

    def is_down(self):
        """ Determine if the server-group is down: all servers in this
        server-group is is down
        """
        for srv in self.servers:
            if not srv.is_down():
                return False
        return True

    def is_ready(self):
        """ Determine if the server-group is ready to service request,
        in other words, if all servers are not marked DOWN.
        """
        for srv in self.servers:
            if srv.is_connected():
                return True
        return False

    def get_server_for_inquiry(self):
        """ Return a server that is ready to accept a new query.
            This is where failover/round-robin handling occurs.
        """
        # If only one server configured:
        if len(self.servers) == 1:
            if self.servers[0].is_connected():
                return self.servers[0]
            else:
                return None

        # Failover
        #
        # Published documentation says to stick to original if it
        # exists.  Otherwise, if we're running on a non-primary server,
        # check every few minutes to see if primary comes online. When
        # it does, use it.
        #
        if self.connection_behavior == CONNECT_BEHAVIOR_FAILOVER:

            if self.server_pos != 0 and self.servers[0].is_connected():
                return self.servers[0]
            if self.servers[self.server_pos].is_connected():
                return self.servers[self.server_pos]
            else:
                index = self.server_pos + 1
                count = len(self.servers)
                for offset in xrange(count):
                    next_index = (index + offset) % count
                    if self.servers[next_index].is_connected():
                        self.server_pos = next_index
                        return self.servers[next_index]

            # No servers are available
            return None

        # Load balance
        #
        # Load balancing behavior means to round-robin across existing
        # (working) servers.  This behavior is similar to simple DNS-based
        # round-robining (more complicated as we track which servers are
        # down).
        elif self.connection_behavior == CONNECT_BEHAVIOR_LOAD_BALANCE:

            count = len(self.servers)
            for offset in xrange(count):
                next_index = (self.server_pos + offset) % count
                if self.servers[next_index].is_connected():
                    self.server_pos = next_index + 1
                    return self.servers[next_index]

            # No available server
            return None

        else:
            # Unknown behavior
            log.write('LDAP.DEBUG', "Unknown connection behavior type: %d"
                      % (self.connection_behavior))
            return None

    def find_referral_server(self, host, port):
        """find_referral_server(host, port)"""
        # If host/port points to this server-group, we're good
        for server in self.servers:
            if server.hostname == host and \
               int(server.port) == int(port):
                return server
        # Otherwise find a different server_group to pass this off to
        return self.client.find_server(host, port)

    def inquiry_referral(self, inquiry, params, referral):
        """inquiry_referral(inquiry, params, referral)
        An inquiry is in need of referral-following."""
        # Extract host/port from referral
        lurl = ldapurl.LDAPUrl(referral)
        host, port = LDAPUrl_to_host_port(lurl)

        # If host/port points to this server-group, we're good
        referral_server = self.find_referral_server(host, port)
        if referral_server:

            # Update inquiry with new parameters
            inquiry.update_from_referral(params, lurl)

            if isinstance(inquiry, Inquiry_Simple_Bind):
                referral_server.resolve_bind_inquiry(inquiry)
            else:
                referral_server.resolve_inquiry(inquiry)
            return True

        log.write('LDAP.DEBUG', "Could not find a server to follow referral: %s" % (referral,))

        return False

    def inquiry_continuation(self, inquiry, params, continuation):
        """inquiry_continuation(inquiry, params, continuation)
        An inquiry is in need of continuation-following.

        For now this is pretty much identical to inquiry_referral(), but
        in the future continuation handling will involve parsing and
        dealing with all sorts of extras like filter modifications and
        base changes."""
        # Extract host/port from continuation
        lurl = ldapurl.LDAPUrl(continuation)
        host, port = LDAPUrl_to_host_port(lurl)

        # If host/port points to this server-group, we're good
        continuation_server = self.find_referral_server(host, port)
        if continuation_server:

            # Update inquiry with new parameters
            # See RFC 4511 section 4.5.3
            # For now, only Search cares about referrals and
            # continuations, and for now the modifications are the
            # same.
            inquiry.update_from_referral(params, lurl)

            continuation_server.resolve_inquiry(inquiry)
            return True

        log.write('LDAP.DEBUG', "Could not find a server to follow continuation: %s" % (continuation,))

        return False

    def sg_thread_bootstrap(self):
        """ the work-thread """
        while True:
            try:
                self.sg_thread_worker()
            except coro.Interrupted:
                raise
            except ldap_cmd.ServerGroupShutdown:
                # Shutdown raised up by the worker thread
                assert(self._shutting_down == 2)
                break
            except:
                log.write('COMMON.APP_FAILURE', tb.traceback_string())
                coro.sleep_relative(10)
            else:
                # If worker returns with no exception, that means we're
                # shut down, but let's do an assert to make sure.
                assert(self._shutting_down == 2)
                break

    def _add_event(self, server, wait_time, event):
        for (t, e) in self.event_list:
            if e == event:
                return
        if wait_time is None:
            wait_time = coro.now + (server.normalized_delay_time() * coro.ticks_per_sec)
        else:
            wait_time = coro.now + (wait_time * coro.ticks_per_sec)
        self.event_list.append((wait_time, event))

    def _add_bind_event(self, server, wait_time=None):
        for conn in server.bind_connections.values():
            if conn['conn_obj'].is_connected():
                return
        self._add_event(server, wait_time, ldap_cmd.CmdBindConn(server, True))

    def _get_due_events(self):
        time_to_sleep = DEFAULT_READ_TIMEOUT
        due_events = []

        if self.event_list:
            time_now = coro.now
            ticks_per_sec = coro.ticks_per_sec
            events = []
            for entry in self.event_list:
                event_time, event = entry
                if time_now >= event_time:
                    due_events.append(event)
                else:
                    time_to_sleep = min(time_to_sleep,
                                        int(math.ceil(float(event_time - time_now) / ticks_per_sec)))
                    events.append(entry)
            self.event_list = events

        if due_events:
            # if there are event to process, we will not wait around for the fifo queue
            return (0, due_events)
        else:
            return (time_to_sleep, [])

    def check_auth_failure(self):
        """Check if all servers in this server group has failed to connect
        due to authentication error

        :Return:
            - True if all server is down due to auth failure, False otherwise.
                   if there is no server in the server group, return False
                   (i.e., there is no auth failure)
        """
        for srv in self.servers:
            if not srv.is_auth_failure():
                return False
                break
        return (len(self.servers) > 0)

    def _inquiry_timeout_event(self, wake_time):
        self.event_list.append((wake_time, ldap_cmd.CmdInquiryTimeout()))

    def _inquiry_timeout_request(self):
        self.worker_fifo.push(ldap_cmd.CmdInquiryTimeout())

    def sg_thread_worker(self):
        self.worker_fifo.push(ldap_cmd.CmdBootstrap())   # Bootstrap

        def list_iter(*ell):
            for el in ell:
                for e in el:
                    yield e

        while True:
            requests = []
            due_events = []

            # pop_all() will wait() until there is something to do
            try:
                time_to_sleep, due_events = self._get_due_events()
                requests = coro.with_timeout(time_to_sleep, self.worker_fifo.pop_all)
            except coro.TimeoutError:
                # If we got timeout, go check inquiries and bind_inquiries, there
                # might be left over from previous run.
                if self.unattached_bind_inquiries:
                    requests.append(self.cmd_bind_inquiries)
                if self.unattached_inquiries:
                    requests.append(self.cmd_inquiries)

            # Service any requests
            for req in list_iter(due_events, requests):
                req.run(self)

######################################################################
# SERVERS
#
# The server class is the glue between the server-group and individual
# connections.  This class is basically two parts:
#
#   - Top half - manipulated by the server-group's worker thread.
#   - Bottom half - manipulated by the server's connections.
#
# This class does not maintain any execution contexts itself; it simply
# keeps the mechanics of the tranports layer separate from the logic of
# inquiry resolution.
#
# The top half interfaces with the server-group by placing operands on
# the server-group's work-FIFO.  The server-group's worker then services
# the operand during it's next work loop.
#
# The bottom half consists of creating and managing connections. As
# connections come and go, connections use callbacks to inform the
# server of changes.
######################################################################
class ldap_server:
    """ldap_server -- the glue between the server-group and individual
    connections.  This abstraction is basically two parts.  The top half
    is manipulated by the server-group's worker thread.  The bottom half
    is manipulated by the server's connections.  The ldap_server doesn't
    actually do anything but act as a state for the upper and lower
    execution contexts.
    """

    SRV_STATE_NEW = 0
    SRV_STATE_CONNECTING = 1
    SRV_STATE_CONNECTED = 2
    SRV_STATE_DOWN = 3

    reconnect_delay = 30
    recentness_threshold = 120

    def __init__(self, sg_context, hostname, port, bind_ip, authtype,
                 authdata, transport):
        self.sg_context = sg_context
        self.hostname = hostname
        self.port = port
        self.bind_ip = bind_ip
        self.authtype = authtype
        self.authdata = authdata
        self.transport = transport

        self.max_conns = sg_context.max_conns
        self.max_conn_time = sg_context.max_time_per_conn
        self.max_conn_requests = sg_context.max_requests_per_conn
        self.ldap_op_timeout = sg_context.operation_timeout
        self.failover_if_err_in_last = sg_context.failover_timeout

        self.connections = {}
        self.bind_connections = {}
        self.next_connection_id = 1
        self.next_conn_die_time = 0
        self.most_recent_cid = 0   # Used to round-robin connections
        # when dispatching inquiries
        self.ip_list = []
        self.ip_pos = 0

        self.state = self.SRV_STATE_NEW
        self.shutdown_flag = False
        self.auth_failure = False

        # Start with the assumption that this server has successfully
        # connected.  This acts as the starting point from which the
        # "how long until this server gets marked as down" calculation
        # can be made.
        self.last_conn_read_time = coro.now
        self.last_conn_success_time = coro.now
        self.last_conn_err = None
        self.last_conn_err_time = None
        self.last_spawned_connection_time = None
        self.last_spawned_bind_time = None

        self.inquiries_to_send = []
        self.binds_to_send = []

        # Tracking if a connection spawner is active
        self.connection_spawner_thread = None

    ##################################################################
    # TOP HALF
    ##################################################################
    def resolve_inquiry(self, inquiry):
        """resolve_inquiry(inquiry) -> Bool
        Give this inquiry to the server to resolve.  If no connections
        are CONNECTED, inquiry is placed onto self.inquiries_to_send."""

        # If the inquiry has previously timed out, let it be taken off
        # the inquiry list.  If rendered_request is None, this inquiry
        # has been timed out, we won't try it again until it is retired
        # from the cache

        if inquiry and inquiry.rendered_request:
            resolving_conn_obj = self.get_conn_obj_for_inquiry()
            if resolving_conn_obj:
                resolving_conn_obj.send_inquiry(inquiry)
            else:
                self.inquiries_to_send.append(inquiry)

    def resolve_bind_inquiry(self, inquiry):
        """resolve_bind_inquiry(inquiry) -> Bool
        Give this inquiry to the server to resolve. If no connections
        can be made, inquiry is placed on self.binds_to_send."""
        assert(self.state is not self.SRV_STATE_DOWN)
        # Find an existing connection:
        try:
            binder_conn_obj = self.get_conn_obj_for_bind()
        except dns_exceptions.DNS_Error as e:
            log.write('LDAP.ERROR', 'DNS', e)
            self.state = self.SRV_STATE_DOWN
            binder_conn_obj = None
            # Retry later
            self.sg_context._add_bind_event(self)
            # Fall through to allow inquiry to find self.binds_to_send

        if inquiry and inquiry.rendered_request:
            if binder_conn_obj is None:
                self.binds_to_send.append(inquiry)
            else:
                binder_conn_obj.send_inquiry(inquiry)

    def is_down(self):
        """is_down() -> Bool
        Determine if this server is down."""
        return self.state is self.SRV_STATE_DOWN

    def is_connected(self):
        """is_connected() -> Bool
        Determine if this server has any active connection.
        """
        if self.connections:
            return True
        else:
            return False

    def is_auth_failure(self):
        """is_auth_failure() -> Bool
        Determine if this server failed to authenticate."""
        return self.auth_failure

    def shutdown(self):
        """shutdown()
        Gracefully disconnect everything.  Setting the shutdown_flag
        will prevent new connections.
        """
        self.shutdown_flag = True

        # Kill existing connections.  Doing this causes all outstanding
        # inquiries to be finished.
        for conn in self.connections.keys():
            self.connections[conn]['conn_obj'].teardown_connection(with_error=False)

        # Kill existing bind_connections.
        for bind_conn in self.bind_connections.keys():
            self.bind_connections[bind_conn]['conn_obj'].teardown_connection(with_error=False)

        # Shutdown any queued inquiries
        while (self.inquiries_to_send):
            inq = self.inquiries_to_send.pop()
            inq.state_to_unattached()
        while (self.binds_to_send):
            inq = self.binds_to_send.pop()
            inq.state_to_unattached()

    def state_transition(self):
        """Deal with server state transitions."""
        # If NEW, we're only NEW once.  Transition to CONNECTING.
        if self.state == self.SRV_STATE_NEW:
            self.spawn_connections(bootstrap=True)
            # Move to CONNECTING once the start() thread starts
            # We're CONNECTED when all connections have finished

    def resolve_hostname_lookup(self):
        """Refresh this server's IP address list.
        Resolve self.hostname into a list of IPs: [ip, ..]."""
        if self.shutdown_flag:
            return

        # Did they give us an IP addres?
        try:
            inet_utils.atoh(self.hostname)
        except ValueError:
            # They gave us a hostname
            pass
        else:
            # They gave us an IP.  Fake it up.
            self.ip_list = [(0, self.hostname)]
            return

        # Query for a result
        dns_results = dnsqr.query(self.hostname, 'A')
        if not dns_results:
            # Allow this to propagate
            raise dns_exceptions.DNS_Hard_Error(self.hostname, 'A',
                                                (dnsrcode.ServFail,
                                                 'Empty result set'))
        # Result is a list of (ttl, value) tuples
        self.ip_list = dns_results

        # Sanity check
        if len(self.ip_list) == 0:
            raise dns_exceptions.DNS_Malformed_Qname_Error('', 'A',
                                                           (dnsrcode.ServFail,
                                                            ''))

    def get_ip(self):
        """Return an ip address to use.  Round-robin across the available
        IPs.  Track entries as they expire."""
        list_length = len(self.ip_list)
        if list_length == 0:
            return None

        # Check to see if a TTL has expired.  If any TTL has expired,
        # we'll requery.
        ttl_list = sorted([ttl for ttl, value in self.ip_list])
        if coro.now > ttl_list[0]:
            try:
                self.resolve_hostname_lookup()
            except dns_exceptions.DNS_Error as e:
                log.write('LDAP.ERROR', 'DNS', e)
                return None

            # Recalculate in case things changed
            list_length = len(self.ip_list)
            if list_length == 0:
                return None

        self.ip_pos = (self.ip_pos + 1) % list_length
        return self.ip_list[self.ip_pos][1]

    def get_port(self):
        """Return a port to use."""
        if self.port is not None:
            return self.port
        else:
            # TRANSPORT_SSL is special.  Other start off on standard port.
            if self.transport == TRANSPORT_SSL:
                return PORT_LDAPS
            else:
                return PORT_LDAP

    def calculate_next_conn_timeout(self):
        """calculate_next_conn_timeout()
        Determine when the sg-worker needs to timeout old connections."""
        max_ctime_ticks = self.max_conn_time * coro.ticks_per_sec
        key_list = self.connections.keys()
        next_time = self.next_conn_die_time
        for i in key_list:
            if next_time <= 0 or \
               ((self.connections[i]['birth'] + max_ctime_ticks) < next_time):
                next_time = self.connections[i]['birth'] + max_ctime_ticks
        if self.next_conn_die_time > next_time:
            self.next_conn_die_time = next_time
            # CmdConnTimeout is self-generating, so we only need to have one copy of it
            self.sg_context.event_list = filter((lambda t_e: not isinstance(t_e[1], ldap_cmd.CmdConnTimeout)),
                                                self.sg_context.event_list)
            self.sg_context.event_list.append((self.next_conn_die_time, ldap_cmd.CmdConnTimeout(self)))

    def new_connection(self):
        # Increment the connection ID
        conn_id = self.next_connection_id
        self.next_connection_id += 1

        # Construct connection-dictionary.  Like a light-weight class.
        conn_dict = {
            'server_context': self,
            'birth': coro.now,
            'conn_obj': None,  # can't fill in now because of circular reference
            'conn_id': conn_id,
            'bind_ip': self.bind_ip,
            'max_requests': self.max_conn_requests,
            'waiting_for_flight_inquiries': [],
            'use_ssl': self.transport == TRANSPORT_SSL,
            'authtype': self.authtype,
            'authdata': self.authdata,
            'transport': self.transport,
        }

        # Create a connection and wrap it with all the trim'ns
        conn_dict['conn_obj'] = ldap_connection(conn_dict,
                                                self.sg_context.get_name(),
                                                self.hostname,
                                                self.get_ip(),
                                                self.get_port(),
                                                self.ldap_op_timeout)

        # Check for shutdown condition
        if self.shutdown_flag:
            return

        conn_dict['conn_obj'].start()
        self.last_spawned_connection_time = coro.now

        return conn_dict

    def new_bind_connection(self):
        """new_bind_connection()
        Very similar to new_connection() above, but don't interfere
        with the machinary of maintaining connections."""
        # Increment the connection ID
        conn_id = self.next_connection_id
        self.next_connection_id += 1

        # Construct connection-dictionary.
        bind_conn_dict = {
            'server_context': self,
            'birth': coro.now,
            'conn_obj': None,  # can't fill in now because of circular reference
            'conn_id': conn_id,
            'bind_ip': self.bind_ip,
            'max_requests': self.max_conn_requests,
            'waiting_for_flight_inquiries': [],
            'use_ssl': self.transport == TRANSPORT_SSL,
            'transport': self.transport,
            'bind_inquiry': None,   # XXX fill this in later
        }

        # Create a connection and wrap it with all the trim'ns
        bind_conn_dict['conn_obj'] = ldap_binder_connection(bind_conn_dict,
                                                            self.sg_context.get_name(),
                                                            self.hostname,
                                                            self.get_ip(),
                                                            self.get_port(),
                                                            self.ldap_op_timeout)

        # Check for shutdown condition
        if self.shutdown_flag:
            return None

        bind_conn_dict['conn_obj'].start()

        self.last_spawned_bind_time = coro.now

        return bind_conn_dict

    def spawn_connections(self, bootstrap=False):
        """spawn_connections()
        Spawn a thread to do the work of creating connections.  Thread is
        spawned to allow DNS resolution to block.  Pass "bootstrap" to
        indicate if this is a first-time through to forces initial DNS
        lookup."""
        if not self.connection_spawner_thread:
            self.connection_spawner_thread = \
                coro.spawn(self.connection_spawner, bootstrap)

    def connection_spawner(self, bootstrap):
        """connection_spawner()
        Encapsulate the kicking off of new connections in a thread to
        allow DNS resolution to block."""
        try:
            if self.state == self.SRV_STATE_NEW:
                self.state = self.SRV_STATE_CONNECTING

            wait_time = 1
            while not self.shutdown_flag and \
                    (len(self.connections) < self.max_conns):

                if bootstrap or not self.ip_list:
                    try:
                        self.resolve_hostname_lookup()
                        if self.ip_list:
                            bootstrap = False
                    except dns_exceptions.DNS_Soft_Error as e:
                        log.write('LDAP.ERROR', 'DNS', e)
                        coro.sleep_relative(wait_time)
                        continue
                    except dns_exceptions.DNS_Error as e:
                        log.write('LDAP.ERROR', 'DNS', e)
                        self.state = self.SRV_STATE_DOWN
                        coro.sleep_relative(self.reconnect_delay)
                        continue

                # Create a connection
                conn_dict = self.new_connection()
                if conn_dict is None:   # server shutdown
                    break

                # Wait around until this connection is completed before
                # spawn the next one to avoid spawning too many failed
                # connection attempts.  If the connect attempt fails, we
                # increase the wait time upto 30 sec (the current default)
                # and reset the wait time if connect is successful.
                conn = conn_dict['conn_obj']
                while not (self.shutdown_flag or conn.is_connected()):

                    # If authentication fails, then delay retry the whole
                    # default reconnect delay
                    if self.auth_failure:
                        wait_time = self.reconnect_delay
                        break

                    if conn.is_dead():
                        if wait_time < self.reconnect_delay:
                            wait_time += 1
                        break
                    coro.sleep_relative(0.2)

                if conn.is_connected():
                    wait_time = 1
                else:
                    coro.sleep_relative(wait_time)

        finally:
            self.connection_spawner_thread = None

    def normalized_delay_time(self):
        """normalized_delay_time()
        Normalize the delay-before-reconnect-time.  Either use the
        default reconnect-delay-time, or 1/2 of the failover time,
        whichever is less."""
        half_failover = int(math.ceil(self.failover_if_err_in_last / 2))
        return min(self.reconnect_delay, half_failover)

    def wait_time_until_spawn(self, last_spawn_time):
        """wait_time_until_spawn(last_spawn_time) -> time
        Returns if this server should insert a delay before spawning the
        next connection."""
        # If we've never received a transport-layer error, don't wait
        if self.last_conn_err_time is None:
            return None
        delay_time = self.normalized_delay_time()
        ticks_since_last_conn_err = coro.now - self.last_conn_err_time
        if ticks_since_last_conn_err > (self.recentness_threshold
                                        * coro.ticks_per_sec):
            return None

        if last_spawn_time is None:
            return None
        next_can_spawn_connection = last_spawn_time \
            + (delay_time * coro.ticks_per_sec)
        if coro.now >= next_can_spawn_connection:
            return None
        else:
            # Avoid feeding coro.with_timeout() an int(zero) timeout value.
            return int(math.ceil(float(next_can_spawn_connection - coro.now)
                                 / float(coro.ticks_per_sec)))

    def update_connections(self):
        """Called by connection event handler in response to connection event
        generated by sg-worker to update this server's connection pool.  It
        cleans up any dead connection, and create addition connection if the
        number of active connection is less than the pool size.
        """
        # Reap dead connections.  If they contain any inquiries, pluck 'em.
        delete_list = []
        for conn_id in self.connections:
            if self.connections[conn_id]['conn_obj'].is_dead():
                self.connections[conn_id]['conn_obj'] = None
                delete_list.append(conn_id)
        if delete_list:
            self.state = self.SRV_STATE_CONNECTING
            for del_item in delete_list:
                del self.connections[del_item]

        # If we're not fully connected, figure out if we should be
        # spawning new connections.
        if not self.shutdown_flag and self.state != self.SRV_STATE_CONNECTED:
            self.spawn_connections()

    def update_bind_connections(self):
        """update_bind_connections()
        Called by sg-worker to update this server's bind connections."""
        # Reap dead connections.  If they contain any inquiries, pluck 'em.
        delete_list = []
        for conn_id in self.bind_connections:
            if self.bind_connections[conn_id]['conn_obj'].is_dead():
                self.bind_connections[conn_id]['conn_obj'] = None
                delete_list.append(conn_id)
        if delete_list:
            self.state = self.SRV_STATE_CONNECTING
            for del_item in delete_list:
                del self.bind_connections[del_item]

        # Are new connections in need of spawning?
        if self.binds_to_send:
            # Do we wait until later?
            wait_time = self.wait_time_until_spawn(self.last_spawned_bind_time)
            if wait_time is not None:
                log.write('LDAP.DEBUG', 'Waiting period until next '
                          'bind connection attempt is %d seconds' % wait_time)
                # Convert from "wait X secs" to absolute time
                wake_time = (wait_time * coro.ticks_per_sec) + coro.now

                # We only need 1 outstanding event
                self.sg_context._add_bind_event(self, wake_time)
            else:
                local_bind_list = self.binds_to_send
                self.binds_to_send = []
                while(local_bind_list):
                    self.resolve_bind_inquiry(local_bind_list.pop(0))

    def timeout_old_connections(self, now_time):
        """timeout_old_connections()
        Called by sg-worker to kill off too-old connections."""
        # Check connections
        stopped_something = False
        for conn_id in self.connections.keys():
            if (self.connections[conn_id]['birth'] +
                    (self.max_conn_time * coro.ticks_per_sec) < now_time):
                self.connections[conn_id]['conn_obj'].teardown_connection(with_error=False)
                stopped_something = True
        if stopped_something:
            self.sg_context.worker_fifo.push(ldap_cmd.CmdConn(self))

        # Check BIND connections
        stopped_something = False
        for conn_id in self.bind_connections.keys():
            if (self.bind_connections[conn_id]['birth'] +
                    (self.max_conn_time * coro.ticks_per_sec) < now_time):
                self.bind_connections[conn_id]['conn_obj'].teardown_connection(with_error=False)
                stopped_something = True
        if stopped_something:
            self.sg_context.worker_fifo.push(ldap_cmd.CmdBindConn(self))

    def update_max_connections(self, max_connections):
        """update_max_connections()"""
        old_max = self.max_conns
        self.max_conns = max_connections

        if max_connections < old_max:

            # Close the delta via graceful-disconnection
            num_to_close = old_max - max_connections
            key_list = self.connections.keys()
            for i in key_list:
                # Have we killed enough?
                if not num_to_close:
                    break
                num_to_close -= 1
                # Kill!
                self.connections[i]['conn_obj'].teardown_connection(with_error=False)

        elif max_connections > old_max:
            # Only spawn more if we're doing that sort of thing
            if self.state == self.SRV_STATE_CONNECTING or \
               self.state == self.SRV_STATE_CONNECTED:
                self.spawn_connections()

    def update_max_time_per_connection(self, mtpc):
        """update_max_time_per_connection()
        :Parameters:
            - 'mtpc' : The maximum time allowed for each connection
        Determine if the new max connection time will cause some connections
        to expire, if so kill and restart them anew, other wise update the
        conn timeout event"""
        # Safety measure - bogus value could send LDAP spinning
        if mtpc <= 60:
            mtpc = DEFAULT_MAX_TIME_PER_CONN
        old_time = self.max_conn_time
        self.max_conn_time = mtpc
        if mtpc < old_time:
            inform_new_connections = False
            mtpc_ticks = mtpc * coro.ticks_per_sec
            key_list = self.connections.keys()
            for i in key_list:
                if self.connections[i]['birth'] + mtpc_ticks <= coro.now:
                    self.connections[i]['conn_obj'].teardown_connection(with_error=False)
                    inform_new_connections = True
            if inform_new_connections:
                self.spawn_connections()
            else:
                self.calculate_next_conn_timeout()

    def update_max_requests_per_connection(self, mrpc):
        """update_max_requests_per_connection()
        Go through connections to determine if any should be stopped."""
        old_reqs = self.max_conn_requests
        self.max_conn_requests = mrpc

        # Update all existing connection's 'max_requests' item.  While
        # walking the list, discontinue any connections that are over
        # the new maximum.
        key_list = self.connections.keys()
        inform_new_connections = False
        for i in key_list:
            self.connections[i]['max_requests'] = mrpc
            if mrpc < old_reqs:
                if self.connections[i]['conn_obj'].request_count() > mrpc:
                    self.connections[i]['conn_obj'].teardown_connection(with_error=False)
                    inform_new_connections = True
        if inform_new_connections:
            self.spawn_connections()

    def update_operation_timeout(self, timeout):
        """update_operation_timeout(timeout)
        Update the operation timeout.  Timeout is essentially the read
        timeout allowed for an LDAP Operation to resolve."""
        self.ldap_op_timeout = timeout

    def update_failover_timeout(self, timeout):
        """update_failover_timeout(timeout)
        Update the failover timeout."""
        self.failover_if_err_in_last = timeout

    def get_conn_obj_for_inquiry(self):
        """get_conn_obj_for_inquiry() -> connection
        Discover a connection to transport an inquiry.  Attempt to
        round-robin across connections, skipping non-connected
        connections.

        Return None if no fully-connected connections are available."""

        next_cid = self.most_recent_cid + 1
        conn_count = len(self.connections)

        for count in xrange(conn_count):
            next_cid = (next_cid + count) % conn_count
            conn_obj = self.connections.values()[next_cid]['conn_obj']
            if conn_obj and conn_obj.is_connected() and \
                    conn_obj.request_count() < self.max_conn_requests:
                self.most_recent_cid = next_cid
                return conn_obj

        return None

    def get_conn_obj_for_bind(self):
        """get_conn_obj_for_bind() -> connection or None
        Discover a connection to complete a BIND.  Use an existing
        connection if possible, otherwise create a new connection up
        until the max-connections limit is hit."""
        conn_obj = None

        # Can we use an existing connection?
        cid_list = self.bind_connections.keys()
        list_len = len(cid_list)
        for i in xrange(list_len):
            conn_obj = self.bind_connections[cid_list[i]]['conn_obj']
            if conn_obj and conn_obj.is_connected():
                if not conn_obj.inquiry_count():
                    # Connected with Zero requests, use this
                    return conn_obj

        # If no existing connection, can a new one be created?
        if len(self.bind_connections) < self.max_conns:
            new_conn = self.new_bind_connection()
            if new_conn:
                self.bind_connections[new_conn['conn_id']] = new_conn
                return new_conn['conn_obj']
            else:
                self.sg_context._add_bind_event(self)

        # Still no connection?  Return None
        return None

    def give_back_inquiries(self):
        """give_back_inquiries()
        Give any queued inquiries back to the server-group."""
        if self.inquiries_to_send:
            self.sg_context.worker_fifo.push(ldap_cmd.CmdInquiryList(self.inquiries_to_send))
            self.inquiries_to_send = []
        if self.binds_to_send:
            self.sg_context.worker_fifo.push(ldap_cmd.CmdBindInquiryList(self.binds_to_send))
            self.binds_to_send = []

    def build_log(self, msg):
        """build_log(msg)
        Dress up an error message with server info."""
        return '%s:%s(%s:%d) %s' % \
            (self.sg_context.get_name(), self.hostname,
             self.get_ip(), self.get_port(), msg)

    ##################################################################
    # BOTTOM HALF
    ##################################################################

    def note_connection_read(self):
        """note_connection_read()
        Note that a connection has successful read.  This updates this
        amount of time that must pass before a server can be marked as
        down."""
        self.last_conn_read_time = coro.now

    def _set_srv_state(self, with_error, auth_error):
        # A dead connection means we're no longer fully connected
        if self.state == self.SRV_STATE_CONNECTED:
            self.state = self.SRV_STATE_CONNECTING

        if auth_error:
            self.auth_failure = True
            self.state = self.SRV_STATE_DOWN

        # Track if this is due to a transport error
        if with_error:
            self.last_conn_err_time = coro.now

            if self.state != self.SRV_STATE_DOWN:

                # This server is down if we haven't successfully
                # connected in at least the past 3 minutes.  Hueristic
                # loosely imported from old code.
                last_traffic_time = max(self.last_conn_success_time,
                                        self.last_conn_read_time)

                if (self.last_conn_err_time >= (last_traffic_time +
                                                (self.failover_if_err_in_last *
                                                 coro.ticks_per_sec))):
                    # Yes, it's been at least 3 minutes
                    log.write('LDAP.DEBUG',
                              self.build_log('this server marked DOWN'))
                    self.state = self.SRV_STATE_DOWN

    def dead_connection(self, conn, with_error=False, auth_error=False):
        """dead_connection()
        Dead connection callback."""
        self._set_srv_state(with_error, auth_error)

        # Return our inquiries back to server-group
        self.give_back_inquiries()

        self.sg_context.worker_fifo.push(ldap_cmd.CmdShutdownConn(self, conn))

    def dead_bind_connection(self, conn, with_error=False):
        """dead_bind_connection()
        Dead BIND connection callback."""
        self._set_srv_state(with_error, False)

        # Return our inquiries back to server-group
        self.give_back_inquiries()

        self.sg_context.worker_fifo.push(ldap_cmd.CmdShutdownBindConn(self, conn))

    def server_responsive(self):
        """server_responsive(conn_id)
        Connection has negotiated transport-layer successfully.  If
        server is DOWN, it should move to CONNECTING."""
        if self.state == self.SRV_STATE_DOWN:
            self.state = self.SRV_STATE_CONNECTING

    def completed_connection(self, conn_id, conn):
        """completed_connection()
        Live connection callback."""
        self.last_conn_success_time = coro.now

        # Add self to connection list
        self.connections[conn_id] = conn.cookie

        # Update the ConnTimeout event
        self.calculate_next_conn_timeout()

        # If server is down, it's now connecting
        if self.state == self.SRV_STATE_DOWN:
            self.state = self.SRV_STATE_CONNECTING

        # If server is connecting, it might now be fully connected
        if self.state == self.SRV_STATE_CONNECTING and \
           self.max_conns == len(self.connections):
            # Count how many connections have connected
            total_connected = 0
            for conn_id in self.connections:
                if self.connections[conn_id]['conn_obj'].is_connected():
                    total_connected += 1
            if total_connected == self.max_conns:
                self.state = self.SRV_STATE_CONNECTED

        # Update auth state
        self.auth_failure = False

        # Inform the server-group
        self.sg_context.worker_fifo.push(ldap_cmd.CmdServerConnected(self))

    def connection_inquiry_give_back(self, inquiry_list):
        """connection_inquiry_give_back(inquiry_list)
        Connection callback to give back a list of inquiries that could
        not be completed."""
        self.sg_context.worker_fifo.push(ldap_cmd.CmdInquiryList(inquiry_list))

    def connection_bind_inquiry_give_back(self, inquiry_list):
        """connection_bind_inquiry_give_back(inquiry_list)
        Connection callback to give back a list of BIND inquiries that
        could not be completed."""
        if self.shutdown_flag:
            for inqr in inquiry_list:
                inqr.state_to_unattached()
            return
        self.binds_to_send.extend(inquiry_list)

setup_connection_timeout = 60

######################################################################
# CONNECTIONS
#
# The connection class wraps sockets with reader and writer threads. In
# order to become connected, this class implements LDAP-specific
# connectivity logic such as authentication and SSL negotiation.
#
# Other LDAPism are maintained, such as tracking per-connection message
# identifiers.
######################################################################
class ldap_connection:

    # State descriptions
    # NEW - Connection has been newly created.
    # CONNECTING - A thread has been spawned off to take care of all things
    #              connection-related up until the point where LDAP operations
    #              can be executed.
    # CONNECTED - Stable connection w/ 2 service threads - reader and writer.
    # DEAD - Connection is in need of clean up.

    CONN_NEW = 0
    CONN_CONNECTING = 1
    CONN_AUTHENTICATING = 2
    CONN_CONNECTED = 3
    CONN_DEAD = 4

    def __init__(self, cookie, sg_name, hostname, ip, port, read_timeout):
        self.sg_name = sg_name
        self.hostname = hostname
        self.ip = ip
        self.port = port
        self.cookie = cookie
        self.read_timeout = read_timeout
        self.state = self.CONN_NEW
        self.inflight_inquiries = {}
        self.out_fifo = coro.fifo()
        self.next_ldap_msg_id = 1
        self.buffer = ''
        self.last_err = ''
        self.completed_requests = 0
        self.last_write = 0
        self.auth_failed = False
        self.s = None

        self.connect_thread = None
        self.reader_thread = None
        self.writer_thread = None

        self.shutdown_flag = False
        self.connection_name = "(%s:%s)" % (self.hostname, self.port)

    def start(self):
        """start()
        Kick off this connection.  A "setup_connection_thread" thread is
        spawned to allow the calling thread to continue on. The spawned
        thread wraps the connection-operation with a time-out wrapper.
        """
        self.connect_thread = coro.spawn(self.setup_connection_thread)

    def _get_drain_list(self):
        """_get_drain_list()
        Generate a list of to-be-drained-inquiries."""
        inquiry_list = []

        # Drain any inquiries found in in-flight bucket
        if self.inflight_inquiries:
            for inflight in self.inflight_inquiries.keys():
                inflight_inqr = self.inflight_inquiries.pop(inflight)
                inflight_inqr.state_to_unattached()
                inquiry_list.append(inflight_inqr)
            self.inflight_inquiries = {}

        # Drain any inquiries found in out_fifo
        if len(self.out_fifo):
            op_list = self.out_fifo.pop_all()
            for mid, inqr in op_list:
                inqr.state_to_unattached()
                inquiry_list.append(inqr)

        return inquiry_list

    def drain_inquiries(self):
        """drain_inquiries()
        Return inquiries back to server."""
        drain_list = self._get_drain_list()
        if drain_list:
            sc = self.cookie['server_context']
            sc.connection_inquiry_give_back(drain_list)

    def is_connected(self):
        """is_connected() -> Bool"""
        return self.state == self.CONN_CONNECTED

    def is_dead(self):
        """is_dead() -> Bool"""
        return self.state == self.CONN_DEAD

    def request_count(self):
        """request_count() -> number of completed, outstanding, and
        inflight requests.
        NOTE: authenticate operations are not counted.

        Server calls this to determine if this connection can take
        any more requests."""
        return self.completed_requests + \
            len(self.out_fifo) + \
            len(self.inflight_inquiries)

    def setup_connection_thread(self):
        """Method to be called as thread to construct/connect socket.
        Doesn't do much except wrap everything in timeout code."""
        if self.shutdown_flag:
            return
        # log.write('LDAP.DEBUG', self.build_last_err('creating a new connection'))
        try:
            coro.with_timeout(setup_connection_timeout, self.setup_connection)

        except coro.TimeoutError as e:
            # Log this timeout
            self.last_err = self.build_last_err(
                'Timeout attempting to connect: %s' % e)
            log.write('LDAP.DEBUG', self.last_err)

            # Mark this connection as dead
            self.teardown_connection()

        except coro.Interrupted:
            raise

        except:
            # Mark this connection as dead after logging important bits
            exc_info = sys.exc_info()
            self.last_err = tb.traceback_string(*exc_info)
            log.write('COMMON.APP_FAILURE', self.last_err)
            self.teardown_connection()

    def setup_connection(self):
        """setup_connection(); connect to a socket.  Once a socket has
        been connected, reader and writer threads are spawned to handle
        LDAP operations."""
        log.write('LDAP.DEBUG', self.build_last_err("connecting to server"))
        try:
            self.state = self.CONN_CONNECTING
            if self.cookie['use_ssl']:
                sock = coro_ssl.ssl_sock(ldap_api.ssl_ctx)
                sock.create()
            else:
                sock = coro.tcp_sock()

            if self.cookie['bind_ip'] is not None:
                sock.bind((self.cookie['bind_ip'], 0))

            sock.connect((self.ip, self.port))
            if self.cookie['use_ssl']:
                sock.ssl_connect()

            self.s = sock

        except sslip.Error as e:
            self.last_err = self.build_last_err('SSL Error: %s' % e)
            log.write('LDAP.DEBUG', self.last_err)
            self.teardown_connection()

        except OSError as e:
            self.last_err = self.build_last_err('Connection Error: %s' % e)
            log.write('LDAP.DEBUG', self.last_err)
            self.teardown_connection()

        else:
            # Server is responsive
            self.cookie['server_context'].server_responsive()

            # Deal with StartTLS
            if not self.start_starttls():
                self.teardown_connection()
                return

            # Deal with authentication
            if not self.authenticate_connection():
                self.teardown_connection()
                return

            # If we get this far, we should already have a servicable
            # connection, set the state as connected and start the
            # reader and writer threads
            self.state = self.CONN_CONNECTED

            # Spawn readers and writers
            self.writer_thread = coro.spawn(self.writer)
            self.reader_thread = coro.spawn(self.reader)

            log.write('LDAP.DEBUG', self.build_last_err('connected to server'))

            # Tell the server we're connected
            sc = self.cookie['server_context']
            sc.completed_connection(self.cookie['conn_id'], self)

    def upgrade_connection_to_ssl(self):
        """upgrade_connection_to_ssl()
        Upgrade a connection's existing non-SSL socket with an SSL wrapper.
        This must occur before the connection's reader and writer threads
        have been spawned.
        """
        sock = coro_ssl.ssl_sock(ldap_api.ssl_ctx)
        sock.create(sock=self.s)
        sock.ssl_connect()
        self.s = sock

    def start_starttls(self):
        """start_starttls()
        Deal with transport type of TRANSPORT_STARTTLS."""
        if self.state == self.CONN_CONNECTING or \
           self.state == self.CONN_CONNECTED:

            if self.cookie['transport'] == TRANSPORT_STARTTLS:

                # Create LDAP StartTLS operation
                starttls_inquiry = Inquiry_StartTLS(None, DEFAULT_INQUIRY_TIMEOUT * coro.ticks_per_sec)

                # Send the StartTLS operation
                response = self.send_inquiry_and_wait(starttls_inquiry)

                # Decode response, if any.  A "None" response means the
                # connection has failed.  self.last_err contains the error.
                if response:
                    # Process inquiry and result to yield a (code, value) tuple
                    success, result = starttls_inquiry.process_response(response)
                    if success:
                        # If successful, "upgrade" the underlying socket into an
                        # SSL socket.
                        try:
                            self.upgrade_connection_to_ssl()

                        except sslip.Error as e:
                            self.last_err = self.build_last_err('SSL Error: %s' % e)
                            log.write('LDAP.DEBUG', self.last_err)
                            return False

                        except OSError as e:
                            self.last_err = self.build_last_err('Connection Error: %s' % e)
                            log.write('LDAP.DEBUG', self.last_err)
                            return False

                        except EOFError:
                            self.last_err = self.build_last_err("Connection closed (EOF)")
                            log.write('LDAP.DEBUG', self.last_err)
                            return False

                    else:
                        self.last_err = self.build_last_err(result)
                        return False
                else:
                    # Failed to StartTLS
                    log.write('LDAP.DEBUG', self.last_err)
                    return False
        return True

    def authenticate_connection(self):
        """authenticate_connection()
        Authenticate a connection via 'anonymous' or 'password'."""
        if self.state != self.CONN_CONNECTING and \
                self.state != self.CONN_CONNECTED:
            return False

        # Mark as authenticating
        self.state = self.CONN_AUTHENTICATING
        if self.cookie['authtype'] == 'anonymous':
            # No auth needed
            pass
        elif self.cookie['authtype'] == 'password':
            auth_inquiry = Inquiry_Simple_Bind(None,
                                               self.cookie['authdata']['user'],
                                               self.cookie['authdata']['password'],
                                               DEFAULT_INQUIRY_TIMEOUT * coro.ticks_per_sec)

            response = self.send_inquiry_and_wait(auth_inquiry)

            # If 'response' is None, the connection has gone away.
            if response is None:
                log.write('LDAP.DEBUG', self.last_err)
                return False
            else:
                success, result = auth_inquiry.process_response(response)
                if not success or (success is True and result is False):
                    if not success:
                        self.last_err = self.build_last_err(result)
                    else:
                        self.last_err = self.build_last_err('auth failed')
                        self.auth_failed = True
                    log.write('LDAP.DEBUG', self.last_err)
                    return False
        # --> support SASL here <---
        else:
            self.last_err = self.build_last_err('unknown auth type %r' %
                                                (self.cookie['authtype'],))
            return False

        return True

    def send_inquiry(self, inquiry):
        """send_inquiry()
        Send an inquiry over this connection."""
        # Get a message ID for this inquiry
        new_msgid = self.next_ldap_msgid()

        # Place data on "send me" queue
        self.out_fifo.push((new_msgid, inquiry))
        inquiry.state_to_attached()

    def send_inquiry_and_wait(self, inquiry):
        """send_inquiry_and_wait(inquiry)
        Send out an inquiry and wait for a result.  This should only be
        called before the reader and writer threads are spawned.  That
        is, before the connection reaches "CONNECTED" state.

        Returns a result or None if the connection has gone or needs to
        go away.
        """

        # Get a message ID for this inquiry
        new_msgid = self.next_ldap_msgid()

        rendered_request = inquiry.state_to_attached()

        packet_to_send = self._render_packet(new_msgid, rendered_request)

        try:
            # Send packet
            self.s.send(packet_to_send)

            # Wait for a response
            try:
                message_id, response = self._recv_packet()
            except coro.TimeoutError:
                self.last_err = self.build_last_err("read timeout")
                return None

            if message_id != new_msgid:
                self.last_err = self.build_last_err(
                    'Received unexpected LDAP msg ID: %s, buffer: %s' %
                    (message_id, self.buffer))
                return None

            # Return to caller.  Caller must process the response.
            return response

        except OSError as e:
            self.last_err = self.build_last_err("Connection Error: %s" % e)
            return None
        except coro.ClosedError:
            self.last_err = self.build_last_err("Connection closed")
            return None
        except sslip.Error as e:
            self.last_err = self.build_last_err("SSL Error: %s" % e)
            return None
        except EOFError:
            self.last_err = self.build_last_err("Connection closed (EOF)")
            return None
        except coro.Interrupted:
            raise
        except:
            exc_info = sys.exc_info()
            self.last_err = self.build_last_err(tb.traceback_string(*exc_info))
            log.write('COMMON.APP_FAILURE', self.last_err)

            # Raise this generic error
            raise

    def has_read_timeout(self):
        """has_read_timeout()
        Determine if connection has waited self.read_timeout time for an
        inquiry to be processed."""
        if self.last_write and (coro.now >
                                (self.last_write + (self.read_timeout * coro.ticks_per_sec))):
            return True
        return False

    def reader(self):
        """reader() - thread/method that reads from network"""
        try:
            while self.state == self.CONN_CONNECTED:
                try:
                    # Receive a packet
                    message_id, response = self._recv_packet()

                    # _recv_packet() returns (None, None) if conn dies
                    if message_id is None:
                        # self.last_err already populated
                        log.write('LDAP.DEBUG', self.last_err)
                        break

                    # Lookup in-flight operation
                    try:
                        inquiry = self.inflight_inquiries[message_id]
                    except KeyError:
                        self.last_err = self.build_last_err('unexpected msg id: %s' % message_id)
                        log.write('LDAP.DEBUG', self.last_err)
                        break

                    # A successful read means this server is not dead
                    sc = self.cookie['server_context']
                    sc.note_connection_read()

                    # Process this response.  If the operation is still
                    # outstanding (eg, an on-going search),
                    # process_response returns (success==None,
                    # result=='continue').
                    success, result = inquiry.process_response(response)
                    if success is None and result == 'continue':
                        continue

                    # No longer in flight
                    del self.inflight_inquiries[message_id]

                    inquiry.state_to_done(success, result, self.connection_name)

                    # Update self.last_write if nothing else is on the wire
                    if not self.inflight_inquiries:
                        self.last_write = 0

                    # Increment completed request counter
                    self.completed_requests += 1

                    # Shutdown if max-requests-per-conn exceeded
                    if self.completed_requests >= self.cookie['max_requests']:
                        break

                except OSError as e:
                    self.last_err = self.build_last_err("Connection Error: %s" % e)
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.ClosedError:
                    self.last_err = self.build_last_err("Connection closed")
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.TimeoutError:
                    # This is only an error if we've been waiting for
                    # self.read_timeout in response to inflight requests.
                    # Otherwise the connection is simply idle.
                    if self.has_read_timeout():
                        self.last_err = self.build_last_err("read timeout")
                        log.write('LDAP.DEBUG', self.last_err)
                        break
                    else:
                        # Idle connection, keep reading
                        pass
                except sslip.Error as e:
                    self.last_err = self.build_last_err("SSL Error: %s" % e)
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except EOFError:
                    self.last_err = self.build_last_err("Connection closed (EOF)")
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.Interrupted:
                    log.write('LDAP.DEBUG',
                              self.build_last_err("Connection interrupted (reader)"))
                    break
                except:
                    exc_info = sys.exc_info()
                    self.last_err = self.build_last_err(tb.traceback_string(*exc_info))
                    log.write('COMMON.APP_FAILURE', self.last_err)

                    # Raise this generic error
                    raise

        finally:
            self.teardown_connection()

    def writer(self):
        """writer() - thread/method that writes to network"""
        try:
            while self.state == self.CONN_CONNECTED:
                try:
                    mid, inqr = self.out_fifo.pop()

                    try:
                        rendered_packet = self._rendered_packet_from_inquiry((mid, inqr))
                    except TypeError:
                        # If we have problem render the packet, then return it as an error
                        exc_info = sys.exc_info()
                        self.last_err = self.build_last_err(tb.traceback_string(*exc_info))
                        log.write('COMMON.APP_FAILURE', self.last_err)

                        inqr.state_to_done(False, self.last_err, None)
                        continue

                    try:
                        self.inflight_inquiries[mid] = inqr
                        self.s.send(rendered_packet)
                        self.last_write = coro.now
                    except:
                        # If problem while sending it over the wire, then requeue
                        if mid in self.inflight_inquiries:
                            del self.inflight_inquiries[mid]
                        self.out_fifo.push((mid, inqr))
                        raise

                except OSError as e:
                    self.last_err = self.build_last_err("Connection Error: %s" % e)
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.ClosedError:
                    self.last_err = self.build_last_err("Connection closed")
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except sslip.Error as e:
                    self.last_err = self.build_last_err("SSL Error: %s" % e)
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except EOFError:
                    self.last_err = self.build_last_err("Connection closed (EOF)")
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.Interrupted:
                    log.write('LDAP.DEBUG',
                              self.build_last_err("Connection interrupted (writer)"))
                    break
                except:
                    exc_info = sys.exc_info()
                    self.last_err = self.build_last_err(tb.traceback_string(*exc_info))
                    log.write('COMMON.APP_FAILURE', self.last_err)

                    # Raise this generic error
                    raise

        finally:
            self.teardown_connection()

    def _teardown_connection(self):
        if self.state == self.CONN_DEAD:
            return False

        # Mark as DEAD
        self.state = self.CONN_DEAD

        self.shutdown_flag = True

        # Return inquiries to server
        self.drain_inquiries()

        return True

    def teardown_connection(self, with_error=True):
        """teardown_connection()
        A connection has died due to a transport problem, or a
        connection has died because too many requests have been
        completed.  In short, teardown_connection() is called by the
        transport handling threads (reader or writer).

        Do everything necessary to move a connection into the DEAD
        state. DEAD connections are reaped by the sg_worker thread."""

        if self._teardown_connection():
            # Inform the parent
            sc = self.cookie['server_context']
            sc.dead_connection(self, with_error, auth_error=self.auth_failed)

    def next_ldap_msgid(self):
        """next_ldap_msgid()
        Utility to return next msg ID for this connection."""
        ldap_msgid = self.next_ldap_msg_id
        while True:
            next_id = ldap_msgid + 1
            # Detect ID roll-over
            if next_id >= 0x40000000:
                next_id = 1
            if ldap_msgid not in self.inflight_inquiries:
                break
            ldap_msgid = next_id
        self.next_ldap_msg_id = next_id
        return ldap_msgid

    def build_last_err(self, msg):
        """build_last_err(msg)
        Dress up an error message with connection info."""
        return '%s:%s(%s:%d) (%d) %s' % \
            (self.sg_name, self.hostname, self.ip, self.port,
             self.cookie['conn_id'], msg)

    def _render_packet(self, message_id, request):
        return ldap.encode_message(message_id, request)

    def _rendered_packet_from_inquiry(self, xxx_todo_changeme):
        """_render_operation_list()
        Convert a list of (id, inquiry) into a """
        (message_id, inquiry) = xxx_todo_changeme
        return ldap.encode_message(message_id, inquiry.get_rendered_operation())

    def _need(self, n):
        "Ensure at least <n> bytes in self.buffer"
        while len(self.buffer) < n:
            # Let caller deal with timeout
            block = coro.with_timeout(self.read_timeout, self.s.recv, 8192)
            if not block:
                raise EOFError
            # tdraegen XXX - um, string concatenation?
            self.buffer += block

    def _recv_packet(self):
        # All received packets must be BER SEQUENCE. We can tell from
        # the header how much data we need to complete the packet.
        # ensure we have the sequence header - I'm inlining the (type,
        # length) detection here to get good buffering behavior
        self._need(2)
        tag = self.buffer[0]
        if tag != '0':  # SEQUENCE | STRUCTURED
            self.last_err = self.build_last_err(
                'Received invalid LDAP packet: invalid starting tag: %s' %
                self.buffer)
            return (None, None)
        l = ord(self.buffer[1])
        if l & 0x80:
            # <l> tells us how many bytes of actual length
            ll = l & 0x7f
            self._need(2 + ll)
            # fetch length
            n = 0
            for i in xrange (ll):
                n = (n << 8) | ord(self.buffer[2 + i])
            if (n < 0) or (n > 1000000):
                # let's be reasonable, folks
                self.last_err = self.build_last_err(
                    'Received invalid LDAP packet: '
                    'invalid packet length: %d, buffer: %s' %
                    (n, self.buffer))
                return (None, None)
            need = n + 2 + ll
        else:
            # <l> is the length of the sequence
            need = l + 2
        # fetch the rest of the packet...
        self._need(need)
        # this will probably empty self.buffer
        packet, self.buffer = self.buffer[:need], self.buffer[need:]
        try:
            # We could do: return ldap.decode(packet)[0]
            # but then we'd lose debugging info related to bug 12719.
            try:
                (message_id, answer), unused = ldap.decode(packet)
            except ValueError:
                self.last_err = self.build_last_err(
                    'Received invalid LDAP packet: Top-level LDAP packet '
                    'is composed of something other than message ID and '
                    'response: %r' % (packet,))
                return (None, None)
            else:
                return (message_id, answer)
        except ldap.DecodeError as e:
            self.last_err = self.build_last_err(
                'Received invalid LDAP packet: %s, %r' % (str(e), packet))
            return (None, None)

class ldap_binder_connection(ldap_connection):
    """Derived class to override BIND-specific connection behavior."""

    def drain_inquiries(self):
        """drain_inquiries()
        Return inquiries back to server."""
        drain_list = self._get_drain_list()
        if drain_list:
            sc = self.cookie['server_context']
            sc.connection_bind_inquiry_give_back(drain_list)

    def inquiry_count(self):
        """inquiry_count() -> number of outstanding, and inflight
        requests.

        Server calls this to determine if this connection is currently
        devoid of inquiries.
        """
        return len(self.out_fifo) + len(self.inflight_inquiries)

    def setup_connection(self):
        """setup_connection(); connect to a socket.  Once a socket has
        been connected, reader and writer threads are spawned to handle
        LDAP operations."""
        log.write('LDAP.DEBUG', self.build_last_err("connecting to server"))
        try:
            self.state = self.CONN_CONNECTING
            if self.cookie['use_ssl']:
                sock = coro_ssl.ssl_sock(ldap_api.ssl_ctx)
                sock.create()
            else:
                sock = coro.tcp_sock()

            if self.cookie['bind_ip'] is not None:
                sock.bind((self.cookie['bind_ip'], 0))

            sock.connect((self.ip, self.port))
            if self.cookie['use_ssl']:
                sock.ssl_connect()

            self.s = sock

        except sslip.Error as e:
            self.last_err = self.build_last_err('SSL Error: %s' % e)
            log.write('LDAP.DEBUG', self.last_err)
            self.teardown_connection()

        except OSError as e:
            self.last_err = self.build_last_err('Connection Error: %s' % e)
            log.write('LDAP.DEBUG', self.last_err)
            self.teardown_connection()

        else:
            # Server is responsive
            self.cookie['server_context'].server_responsive()

            # Deal with StartTLS
            if not self.start_starttls():
                self.teardown_connection()
                return

            # Spawn readers and writers
            if self.state == self.CONN_CONNECTING:
                self.state = self.CONN_CONNECTED
                self.writer_thread = coro.spawn(self.writer)
                self.reader_thread = coro.spawn(self.reader)

    def reader(self):
        """reader() - thread/method that reads from network"""
        try:
            while self.state == self.CONN_CONNECTED:
                try:
                    # Receive a packet
                    message_id, response = self._recv_packet()

                    # _recv_packet() returns (None, None) if conn dies
                    if message_id is None:
                        # self.last_err already populated
                        log.write('LDAP.DEBUG', self.last_err)
                        break

                    # Lookup in-flight operation
                    try:
                        inquiry = self.inflight_inquiries[message_id]
                    except KeyError:
                        self.last_err = self.build_last_err('unexpected msg id: %s' % message_id)
                        log.write('LDAP.DEBUG', self.last_err)
                        break

                    # A successful read means this server is not dead
                    sc = self.cookie['server_context']
                    sc.note_connection_read()

                    # Process this response.
                    success, result = inquiry.process_response(response)

                    # No longer in flight
                    del self.inflight_inquiries[message_id]

                    inquiry.state_to_done(success, result, self.connection_name)

                    # Update self.last_write if nothing else is on the wire
                    if not self.inflight_inquiries:
                        self.last_write = 0

                    # Increment completed request counter
                    self.completed_requests += 1

                    # Shutdown if max-requests-per-conn exceeded
                    if self.completed_requests >= self.cookie['max_requests']:
                        break

                except OSError as e:
                    self.last_err = self.build_last_err("Connection Error: %s" % e)
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.ClosedError:
                    self.last_err = self.build_last_err("Connection closed")
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.TimeoutError:
                    # This is only an error if we've been waiting for
                    # self.read_timeout in response to inflight requests.
                    # Otherwise the connection is simply idle.
                    if self.has_read_timeout():
                        self.last_err = self.build_last_err("read timeout")
                        log.write('LDAP.DEBUG', self.last_err)
                        break
                    else:
                        # Idle connection, keep reading
                        pass
                except sslip.Error as e:
                    self.last_err = self.build_last_err("SSL Error: %s" % e)
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except EOFError:
                    self.last_err = self.build_last_err("Connection closed (EOF)")
                    log.write('LDAP.DEBUG', self.last_err)
                    break
                except coro.Interrupted:
                    log.write('LDAP.DEBUG',
                              self.build_last_err("Connection interrupted (reader)"))
                    break
                except:
                    exc_info = sys.exc_info()
                    self.last_err = self.build_last_err(tb.traceback_string(*exc_info))
                    log.write('COMMON.APP_FAILURE', self.last_err)

                    # Raise this generic error
                    raise

        finally:
            self.teardown_connection()

    def teardown_connection(self, with_error=True):
        """teardown_connection()
        A connection has died due to a transport problem, or a
        connection has died because too many requests have been
        completed.  In short, tearddown_connection() is called by the
        transport handling threads (reader or writer).

        Do everything necessary to move a connection into the DEAD
        state. DEAD connections are reaped by the sg_worker thread."""

        if self._teardown_connection():
            # Inform the parent
            sc = self.cookie['server_context']
            sc.dead_bind_connection(self, with_error)

class QlogWrapper:
    def __init__(self):
        self._thread = None
        self._fifo = coro.fifo()

    def _run(self):
        import qlog
        while True:
            try:
                while True:
                    msgs = self._fifo.pop_all()
                    for message, args, kwargs in msgs:
                        qlog.write(message, *args, **kwargs)
            except coro.Interrupted:
                raise
            except:
                qlog.write('COMMON.APP_FAILURE', tb.traceback_string())
                coro.sleep_relative(10)

    def write(self, message, *args, **kwargs):
        if self._thread is None:
            self._thread = coro.spawn(self._run)
        self._fifo.push((message, args[:], copy.copy(kwargs)))

# Fake one for bootstrapping/development
class QlogWrapperFake:
    def __init__(self):
        self._thread = None
        self._fifo = coro.fifo()

    def _run(self):
        while True:
            try:
                while True:
                    msgs = self._fifo.pop_all()
                    for message, args, kwargs in msgs:
                        print message + ' ' + args[0]
            except coro.Interrupted:
                raise
            except:
                print 'COMMON.APP_FAILURE ' + tb.traceback_string()
                coro.sleep_relative(10)

    def write(self, message, *args, **kwargs):
        if self._thread is None:
            self._thread = coro.spawn(self._run)
        self._fifo.push((message, args[:], copy.copy(kwargs)))

log = QlogWrapper()

# __main__

if __name__ == '__main__':
    import backdoor
    import comm_path
    import os
    import service

    log = QlogWrapperFake()

    bd_path = comm_path.mk_backdoor_path('ldap')
    coro.spawn (backdoor.serve, unix_path=bd_path, global_dict=service.__dict__, thread_name='backdoor')

    # Make a client context
    client = ldap_client()

    # Make a server-group
    sg = client.create_server_group("my_test_group")

    # Add a server
    sg.add_server("localhost", 1111, 'password',
                  {'user': 'fakie', 'password': 'mypass'},
                  TRANSPORT_PLAINTEXT)

    sg.add_server("localhost", 1112, 'anonymous',
                  {'user': 'fakie', 'password': 'mypass'},
                  TRANSPORT_PLAINTEXT)

    # XXX modify max number of connections
    sg.set_max_connections(10)

    # Start the sg
    sg.start()

    try:
        coro.event_loop(30.0)
    finally:
        try:
            os.unlink(bd_path)
        except:
            pass

########NEW FILE########
__FILENAME__ = ldap_cmd
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro

class ServerGroupShutdown(Exception):
    def __init__(self, sg_ctx):
        self.sg_ctx = sg_ctx


class Cmd(object):
    """The base command class for controlling the worker thread main loop """

    def __eq__(self, other):
        return (self is other)

    def run(self, sg_ctx):
        """Run the command object
        All subclass should override this method
        :Parameters:
            - `sg_ctx` : the LDAP server group context
        """
        raise Exception("Implement run()")


class CmdBootstrap(Cmd):
    """This command is initiated at the start of the worker thread.  Its
    purpose is to do any needed initialization.
    """

    def run(self, sg_ctx):
        # Populate the periodic 'inquiry_timeout_event'
        sg_ctx._inquiry_timeout_event(sg_ctx.inquiry_timeout + coro.now)


class CmdConn(Cmd):
    def __init__(self, server, via_event=False):
        self.server = server
        self.via_event = via_event

    def __eq__(self, other):
        return (self.__class__ == other.__class__ and
                self.server == other.server and
                self.via_event == other.via_event)

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self.server)

    def run(self, sg_ctx):
        if self.server in sg_ctx.servers:
            self.server.update_connections()


class CmdBindConn(CmdConn):
    def __init__(self, server, via_event=False):
        self.server = server
        self.via_event = via_event

    def run(self, sg_ctx):
        if self.server in sg_ctx.servers:
            self.server.update_bind_connections()


class CmdConnTimeout(CmdConn):
    def run(self, sg_ctx):
        # Get the server that needs attention
        if self.server in sg_ctx.servers:
            self.server.timeout_old_connections(coro.now)
            # Add another 'conn_timeout' event
            self.server.calculate_next_conn_timeout()


class CmdServerState(Cmd):
    def __init__(self, server):
        self.server = server

    def run(self, sg_ctx):
        if self.server in sg_ctx.servers:
            self.server.state_transition()


class CmdShutdown(Cmd):
    def run(self, sg_ctx):
        # Tell servers to cease all connections
        if sg_ctx._shutting_down == 0:
            sg_ctx._shutting_down = 1
            for srv in sg_ctx.servers:
                srv.shutdown()

        # Clear out the event list
        sg_ctx.event_list = []

        # The server is now ready to be shutdown, this command
        # will terminate the sg worker's main loop.  It will be
        # the last thing in this server fifo queue.
        sg_ctx.worker_fifo.push(CmdShutdownSgWorker())


class CmdShutdownSgWorker(Cmd):
    def run(self, sg_ctx):
        if sg_ctx.unattached_inquiries:
            try:
                # Transfer the pending inquiries to the new server if possible
                new_sg_ctx = sg_ctx.client.server_groups[sg_ctx.name]
                new_sg_ctx.unattached_inquiries.extend(sg_ctx.unattached_inquiries)
            except KeyError:
                # Not possible to transfer, return failure
                for inq in sg_ctx.unattached_inquiries:
                    inq.state_to_done(False, "service shutdown")
        sg_ctx._shutting_down = 2
        # This will terminate the sg worker thread's main loop, and thus
        # terminate the server
        raise ServerGroupShutdown(sg_ctx)


class CmdShutdownConn(Cmd):
    def __init__(self, srv, conn):
        self.srv = srv
        self.conn = conn

    def __repr__(self):
        return '<CmdShutdownConn: %s>' % (self.conn,)

    def shutdown_connection(self, sg_ctx):
        assert(self.conn.state == self.conn.CONN_DEAD)
        assert(len(self.conn.inflight_inquiries) == 0)
        assert(len(self.conn.out_fifo) == 0)
        # shutdown the reader
        if self.conn.reader_thread:
            self.conn.reader_thread.shutdown()
            self.conn.reader_thread.join()
            self.conn.reader_thread = None
        # shutdown the writer
        if self.conn.writer_thread:
            self.conn.writer_thread.shutdown()
            self.conn.writer_thread.join()
            self.conn.writer_thread = None
        # close the socket
        if self.conn.s:
            self.conn.s.close()
            self.conn.s = None

    def run(self, sg_ctx):
        self.shutdown_connection(sg_ctx)

        # update connections
        sg_ctx.worker_fifo.push(CmdConn(self.srv))

class CmdShutdownBindConn(CmdShutdownConn):
    def run(self, sg_ctx):
        self.shutdown_connection(sg_ctx)

        # update bind connections
        sg_ctx.worker_fifo.push(CmdBindConn(self.srv))

class CmdSetCompat(Cmd):
    def __init__(self, compatibility):
        self.compatibility = compatibility

    def run(self, sg_ctx):
        sg_ctx.compatibility = self.compatibility


class CmdSetInqTimeout(Cmd):
    def __init__(self, inquiry_timeout):
        self.inquiry_timeout = inquiry_timeout

    def run(self, sg_ctx):
        sg_ctx.inquiry_timeout = self.inquiry_timeout * coro.ticks_per_sec
        # Update timeout by forcing 'inquiry_timeout_event'
        sg_ctx._inquiry_timeout_request()


class CmdSetMaxConns(Cmd):
    def __init__(self, max_conns):
        self.max_conns = max_conns

    def run(self, sg_ctx):
        if sg_ctx.max_conns != self.max_conns:
            sg_ctx.max_conns = self.max_conns
            for srv in sg_ctx.servers:
                srv.update_max_connections(self.max_conns)


class CmdSetMaxTimePerConn(Cmd):
    def __init__(self, max_time_per_conn):
        self.max_time_per_conn = max_time_per_conn

    def run(self, sg_ctx):
        if sg_ctx.max_time_per_conn != self.max_time_per_conn:
            sg_ctx.max_time_per_conn = self.max_time_per_conn
            for srv in sg_ctx.servers:
                srv.update_max_time_per_connection(self.max_time_per_conn)


class CmdSetMaxRequestsPerConn(Cmd):
    def __init__(self, max_requests_per_conn):
        self.max_requests_per_conn = max_requests_per_conn

    def run(self, sg_ctx):
        if sg_ctx.max_requests_per_conn != self.max_requests_per_conn:
            sg_ctx.max_requests_per_conn = self.max_requests_per_conn
            for srv in sg_ctx.servers:
                srv.update_max_requests_per_connection(self.max_requests_per_conn)


class CmdSetCacheSize(Cmd):
    def __init__(self, cache_size):
        self.cache_size = cache_size

    def run(self, sg_ctx):
        sg_ctx.cache_size = self.cache_size
        sg_ctx.clear_cache()


class CmdSetCacheTtl(Cmd):
    def __init__(self, cache_ttl):
        self.cache_ttl = cache_ttl

    def run(self, sg_ctx):
        sg_ctx.cache_ttl = self.cache_ttl
        sg_ctx.clear_cache()


class CmdSetOperationTimeout(Cmd):
    def __init__(self, operation_timeout):
        self.operation_timeout = operation_timeout

    def run(self, sg_ctx):
        if sg_ctx.operation_timeout != self.operation_timeout:
            sg_ctx.operation_timeout = self.operation_timeout
            for srv in sg_ctx.servers:
                srv.update_operation_timeout(self.operation_timeout)


class CmdSetFailoverTimeout(Cmd):
    def __init__(self, failover_timeout):
        self.failover_timeout = failover_timeout

    def run(self, sg_ctx):
        if sg_ctx.failover_timeout != self.failover_timeout:
            sg_ctx.failover_timeout = self.failover_timeout
            for srv in sg_ctx.servers:
                srv.update_failover_timeout(self.failover_timeout)


class CmdInquiries(Cmd):
    def run(self, sg_ctx):
        sg_ctx.pending_inquiries_event = False
        for index in xrange(len(sg_ctx.unattached_inquiries)):
            inq = sg_ctx.unattached_inquiries.pop(0)
            q_server = sg_ctx.get_server_for_inquiry()
            if q_server:
                q_server.resolve_inquiry(inq)
            else:
                sg_ctx.unattached_inquiries.append(inq)

        # Check to see if all servers are failed because
        # of bad-authentication
        if sg_ctx.check_auth_failure() and sg_ctx.unattached_inquiries:
            while sg_ctx.unattached_inquiries:
                inq = sg_ctx.unattached_inquiries.pop(0)
                inq.state_to_done(False, "server authentication failed")


class CmdInquiryList(Cmd):
    def __init__(self, inquiries):
        self.inquiries = inquiries

    def run(self, sg_ctx):
        if isinstance(self.inquiries, list) and self.inquiries:
            sg_ctx.unattached_inquiries.extend(self.inquiries)
            if not sg_ctx.pending_inquiries_event:
                sg_ctx.pending_inquiries_event = True
                sg_ctx.worker_fifo.push(CmdInquiries())


class CmdBindInquiries(Cmd):
    def run(self, sg_ctx):
        sg_ctx.pending_bind_inquiries_event = False
        attachment_failures = []
        while (sg_ctx.unattached_bind_inquiries):
            inq = sg_ctx.unattached_bind_inquiries.pop(0)
            q_server = sg_ctx.get_server_for_inquiry()
            if q_server:
                q_server.resolve_bind_inquiry(inq)
            else:
                attachment_failures.append(inq)
        if attachment_failures:
            # Some inquiries couldn't be attached; this is
            # only possible if no servers are available.
            sg_ctx.unattached_bind_inquiries = attachment_failures


class CmdBindInquiryList(Cmd):
    def __init__(self, inquiries):
        self.inquiries = inquiries

    def run(self, sg_ctx):
        # Put list of inquiries back on unattached_bind_inquiries
        if isinstance(self.inquiries, list) and self.inquiries:
            sg_ctx.unattached_bind_inquiries.extend(self.inquiries)
            if not sg_ctx.pending_bind_inquiries_event:
                sg_ctx.pending_bind_inquiries_event = True
                sg_ctx.worker_fifo.push(CmdBindInquiries())


class CmdServerConnected(Cmd):
    def __init__(self, server):
        self.server = server

    def __repr__(self):
        return '<CmdServerConnected: %s>' % (self.server,)

    def run(self, sg_ctx):
        # A server has completely connected
        if not sg_ctx.pending_inquiries_event and \
                sg_ctx.unattached_inquiries:
            sg_ctx.pending_inquiries_event = True
            sg_ctx.worker_fifo.push(CmdInquiries())
        if not sg_ctx.pending_bind_inquiries_event and \
                sg_ctx.unattached_bind_inquiries:
            sg_ctx.pending_bind_inquiries_event = True
            sg_ctx.worker_fifo.push(CmdBindInquiries())


class CmdInquiryTimeout(Cmd):
    def run(self, sg_ctx):
        # It's time to expire some inquiries, maybe
        valid_list = []
        valid_birth = None
        while (sg_ctx.unattached_inquiries):
            inq = sg_ctx.unattached_inquiries.pop(0)

            if (inq.get_birth() + sg_ctx.inquiry_timeout) > coro.now:
                valid_list.append(inq)
                if not valid_birth or (inq.get_birth() < valid_birth):
                    valid_birth = inq.get_birth()

        if valid_list:
            sg_ctx.unattached_inquiries = valid_list
            sg_ctx.pending_inquiries_event = True
            sg_ctx.worker_fifo.push(CmdInquiries())

        # Check the BIND list
        valid_bind_list = []
        valid_bind_birth = None
        while (sg_ctx.unattached_bind_inquiries):
            inq = sg_ctx.unattached_bind_inquiries.pop(0)

            if (inq.get_birth() + sg_ctx.inquiry_timeout) > coro.now:
                valid_bind_list.append(inq)
                if not valid_bind_birth or (inq.get_birth() < valid_bind_birth):
                    valid_bind_birth = inq.get_birth()

        if valid_bind_list:
            sg_ctx.unattached_bind_inquiries = valid_bind_list
            sg_ctx.pending_bind_inquiries_event = True
            sg_ctx.worker_fifo.push(CmdBindInquiries())

        # Add a new 'inquiry_timeout_event' event
        if valid_birth and valid_bind_birth:
            wake_time = min(valid_birth, valid_bind_birth) + sg_ctx.inquiry_timeout
        elif valid_birth:
            wake_time = valid_birth + sg_ctx.inquiry_timeout
        elif valid_bind_birth:
            wake_time = valid_bind_birth + sg_ctx.inquiry_timeout
        else:
            wake_time = sg_ctx.inquiry_timeout + coro.now
        # Only add one
        add_event = True
        for wt, event in sg_ctx.event_list:
            if isinstance(event, CmdInquiryTimeout):
                add_event = False
                if wt > wake_time:
                    # Replace 'wt' with 'wake_time'
                    rindex = sg_ctx.event_list.index((wt, event))
                    sg_ctx.event_list[rindex] = (wake_time, event)
        if add_event:
            sg_ctx._inquiry_timeout_event(wake_time)

########NEW FILE########
__FILENAME__ = test__ldap
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# -*- Mode: Python -*-

from _ldap import *
import sys
import tb
import pprint
import unittest

class _ldap_test_case (unittest.TestCase):
    pass

class protos_test (_ldap_test_case):

    # you need the protos c06-ldapv3-enc-r1 test suite
    # to run this test...

    def runTest (self):
        i = 0
        while True:
            try:
                data = open (('%08d' % n), 'rb').read()
                try:
                    pprint.pprint (decode (data))
                except:
                    sys.stderr.write ('%4d %r\n' % (n, tb.traceback_string()))
            except IOError:
                break
            else:
                i += 1

class integer_test (_ldap_test_case):

    def runTest (self):
        # test small integers
        for i in range (-1000, 1000):
            self.assertEqual (decode (INTEGER (i))[0], i)
        # test larger integers
        for i in range (1000000, 2000000, 50):
            self.assertEqual (decode (INTEGER (i))[0], i)
        # test larger integers
        for i in range (-2000000, -1000000, 50):
            self.assertEqual (decode (INTEGER (i))[0], i)
        # test long integers
        for i in range (1000000, 2000000, 50):
            self.assertEqual (decode (INTEGER (i))[0], i)
        big = 2038490283059834505983450695834059639085793847509834752039485034967489769487694856
        self.assertEqual (decode (INTEGER (big))[0], big)

C = 'context'

pq_tests = [
    # simple equality
    ('(xxx=yyy)',
     ((C, 3, ['xxx', 'yyy']),
      12)),
    # simple expression, plus 'present'
    ('(|(xx=y)(zz=*))',
     ((C, 1, [(C, 3, ['xx', 'y']), (C, 7, 'zz')]),
      15)),
    # nary expressions
    ('(|(a=b)(b=c)(c=d)(e=f)(f=g)(h=i))',
     ((C, 1, [(C, 3, ['a', 'b']), (C, 3, ['b', 'c']), (C, 3, ['c', 'd']), (C, 3, ['e', 'f']),
      (C, 3, ['f', 'g']), (C, 3, ['h', 'i'])]), 50)),
    ('(|(!(a=*))(&(b=c)(d=e))(x<=y))',
     ((C, 1, [(C, 2, [(C, 7, 'a')]), (C, 0, [(C, 3, ['b', 'c']), (C, 3, ['d', 'e'])]), (C, 6, ['x', 'y'])]),
      33)),
    # approximate match
    ('(zz~=yy)', ((C, 8, ['zz', 'yy']), 10)),
    # substring
    ('(a=ins*tiga*tor)', ((C, 4, ['a', [(C, 0, 'ins'), (C, 1, 'tiga'), (C, 2, 'tor')]]), 23)),
    ('(a=*y)', ((C, 4, ['a', [(C, 2, 'y')]]), 10)),
    ('(a=y*)', ((C, 4, ['a', [(C, 0, 'y')]]), 10)),
    ('(a=*y*)', ((C, 4, ['a', [(C, 1, 'y')]]), 10)),
    ('(a=*x*y)', ((C, 4, ['a', [(C, 1, 'x'), (C, 2, 'y')]]), 13)),
    ('(a=*x*y*)', ((C, 4, ['a', [(C, 1, 'x'), (C, 1, 'y')]]), 13)),
    ('(a=*x*y*z)', ((C, 4, ['a', [(C, 1, 'x'), (C, 1, 'y'), (C, 2, 'z')]]), 16)),
    # syntax errors
    ('(a=', QuerySyntaxError),
    ('(a<b)', QuerySyntaxError),
    # good hex escape
    ('(a=some\\AAthing)', ((C, 3, ['a', 'some\252thing']), 17)),
    # bad hex escape
    ('(a=some\\AZthing)', QuerySyntaxError),
    # upper/lower case hex escape
    ('(a=xy\\Aaz)', ((C, 3, ['a', 'xy\252z']), 11)),
    # escaped splat
    ('(a=x*y\\2az)', ((C, 4, ['a', [(C, 0, 'x'), (C, 2, 'y*z')]]), 15)),
    # illegal splat
    ('(a~=sam*son)', QuerySyntaxError),
    # junk/illegal
    ('junk', QuerySyntaxError),
    # lots of parens
    (('(' * 100), QuerySyntaxError),
    # expression too complex
    (('(!' * 55) + '(x=y)' + (')' * 55), QuerySyntaxError),
    # expression not too complex
    (('(!' * 10) + '(x=y)' + (')' * 10),
     ((C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2, [(C, 2,
       [(C, 2, [(C, 2, [(C, 3, ['x', 'y'])])])])])])])])])])]), 28)),
]

class parse_query_test (_ldap_test_case):
    def runTest (self):
        for q, e in pq_tests:
            try:
                self.assertEqual (decode (parse_query (q)), e)
            except AssertionError:
                raise
            except:
                self.assertEqual (sys.exc_info()[0], e)

def suite():
    suite = unittest.TestSuite()
    suite.addTest (integer_test())
    suite.addTest (parse_query_test())
    return suite

if __name__ == '__main__':
    unittest.main (defaultTest='suite')

########NEW FILE########
__FILENAME__ = locksmith
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""locksmith.py

This is a class that manages a set of locks.
"""

import coro
import random
import tb

class DeadlockError(Exception):
    """DeadlockError

    Raised when an attempt to call lock() causes a deadlock.
    """
    pass

class LockSmith:

    """LockSmith()

    This class manages a set of locks.
    Locks are associated with a key.
    A separate thread handles detecting deadlocks.
    """

    # How often the manager thread checks for deadlocks in seconds.
    deadlock_check_period = 60

    # The coroutine object that is the thread manager.
    thread = None

    def __init__(self):
        self.locks = {}

    def __len__(self):
        return len(self.locks)

    def check_for_deadlocks(self):
        """check_for_deadlocks(self) -> None
        This will check for deadlocks.
        Deadlocks will be broken by picking one of the deadlocked threads and
        raising DeadlockError on it.
        """
        # In order to detect deadlock, we build a wait-for-graph.
        # Each node in the graph is a thread.  An edge (t1, t2) indicates
        # that thread t1 is waiting for a lock that is currently held by t2.
        # We compute the "in-degree" value for each node.  That is the number
        # of edges leading into the node.  We do a topological-order traversal
        # of the graph.  If we do not successfully visit a node, then that
        # indicates a cycle which implies a deadlock.
        if len(self.locks) <= 1:
            return
        g = self._build_graph()
        thread_ids = self._find_threads_to_break(g)
        for thread_id in thread_ids:
            thread = coro.all_threads[thread_id]
            thread.raise_exception(DeadlockError)
        if __debug__:
            # Make sure that there are no more cycles.
            for k, v in g.items():
                if v in thread_ids:
                    del g[k]
            cycle_g = self._find_cycles(g)
            assert (len(cycle_g) == 0)

    def _build_graph(self):
        """_build_graph(self) -> g
        Builds a wait-for-graph based on the threads in the locks.
        """
        raise NotImplementedError

    def _in_degree(self, g):
        """_in_degree (self, g) -> indegree_dict
        Computes the in-degree for each vertex and returns a dictionary where
        the key is the vertex and the value is its in-degree.

        <g>: A graph.  Dictionary with keys as nodes and value is a list of
        nodes pointing to it.
        """
        r = {}
        # set every node's in-degree to zero
        for k, v in g.iteritems():
            r[k] = 0
            r[v] = 0
        # every time we find an edge, increment the destination
        for v in g.itervalues():
            r[v] += 1
        return r

    def _find_threads_to_break(self, g):
        """_find_threads_to_break(self, g) -> thread_id_list
        Find threads to interrupt with DeadlockError in order to break any
        cycles that exist.  In a cycle, it picks a thread at random to break
        the cycle.  There can be more than 1 thread returned if there are
        multiple cycles.
        """
        to_break = []
        cycle_g = self._find_cycles(g)
        while cycle_g:
            # Randomly pick a node to break.
            thread = random.choice(cycle_g.keys())
            del cycle_g[thread]
            to_break.append(thread)
            cycle_g = self._find_cycles(cycle_g)
        return to_break

    def _find_cycles(self, g):
        """_find_cycles(g) -> cycle_g
        Finds cycles in the given graph.

        <g>: A graph.  Dictionary with keys as nodes and value the node it is
        pointing to.

        Returns <g> with all nodes not involved in a cycle removed.
        """
        g = g.copy()
        while True:
            in_d = self._in_degree(g)
            found = False
            for k, v in in_d.iteritems():
                if v == 0:
                    # found a node with in-degree of zero.
                    del g[k]
                    found = True
            if not found:
                return g

    def start_manager_thread(self):
        """start_manager_thread(self) -> None
        This will spawn a thread that continually monitors the locks looking
        for deadlocks.
        """
        assert (self.thread is None)
        self.thread = coro.spawn(self.thread_manager, thread_name='locksmith_cycle_manager')

    def thread_manager(self):
        """thread_manager(self) -> None
        This is the deadlock detection thread.
        """
        while True:
            try:
                coro.sleep_relative(self.deadlock_check_period)
                self.check_for_deadlocks()
            except coro.Interrupted:
                # Exiting.
                self.thread = None
                return
            except:
                coro.print_stderr(tb.traceback_string() + '\n')

    def stop_manager_thread(self):
        """stop_manager_thread(self) -> None
        This will stop the manager thread.
        """
        if self.thread is not None:
            self.thread.shutdown()


class MutexLockSmith(LockSmith):

    # Rules of the global lock:
    # To get the global lock, no other threads may have any locks.
    #   - If other threads have locks, wait for them to unlock.
    #     - During this time, all new threads attempting to lock a new var will block.
    #     - Wait for all current threads to finish, then get global lock.
    # While someone has the global lock, you are not allowed to lock anything.

    global_lock_name = '__global__'

    def __init__(self, *args):
        LockSmith.__init__(self, *args)
        # Threads waiting to acquire the global lock wait on global_cv.
        self.global_cv = coro.condition_variable()
        # Threads waiting for the global lock to be released wait on global_finished_cv.
        self.global_finished_cv = coro.condition_variable()

    def lock_global(self):
        """lock_global(self) -> None
        Acquires the global lock.
        """
        # Check if the rules are being met to acquire the global lock.
        while True:
            if not self.locks:
                # Simple case, no other locks held.
                self._lock(self.global_lock_name)
                return

            # Check if I am the sole owner of all locks.
            for lock in self.locks.values():
                if not lock.has_lock() or lock:
                    # Don't own this lock or there are waiters for it.
                    break
            else:
                # I own all the locks, nobody waiting for them.
                self._lock(self.global_lock_name)
                return
            # Must wait for all current threads to release their locks.
            self.global_cv.wait()
            # Loop again.

    def try_lock_global(self):
        """try_lock_global(self) -> boolean
        Attempts to acquire the global lock, without blocking. Returns true
        on failure and false on success, mirroring the behavior of
        coro.mutex.trylock().
        """
        # Check if the rules are being met to acquire the global lock.
        if not self.locks:
            # Simple case, no other locks held. This should always succeed.
            rv = self._trylock(self.global_lock_name)
            assert(not rv)
            return False

        # Check if I am the sole owner of all locks.
        for lock in self.locks.values():
            if not lock.has_lock() or lock:
                # Don't own this lock or there are waiters for it.
                break
        else:
            # I own all the locks, nobody waiting for them. This should always
            # succeed.
            return self._trylock(self.global_lock_name)
            assert(not rv)
            return False

        # Must wait for all current threads to release their locks.
        return True

    def unlock_global(self):
        """unlock_global(self) -> None
        Unlock the global lock.
        """
        self._unlock(self.global_lock_name)
        # If the global lock is completely released.
        if self.global_lock_name not in self.locks:
            # See if anyone was waiting for it.
            if self.global_cv:
                # Wake them up.
                self.global_cv.wake_all()
            # People waiting for me to finish.
            elif self.global_finished_cv:
                # Wake them up.
                self.global_finished_cv.wake_all()
        else:
            # The global lock is still held.
            if __debug__:
                # Assertion check.
                lock = self.locks[self.global_lock_name]
                assert (lock.has_lock())

    def lock(self, key):
        """lock(self, key) -> None
        Aquires a lock for the given key.
        """
        # To lock the global lock, you need to use lock_global
        assert(key != self.global_lock_name)
        # Check if the rules for the global lock are met before getting a var lock.
        while True:
            # If global lock is held.
            if self.global_lock_name in self.locks:
                lock = self.locks[self.global_lock_name]
                # And I don't hold it.
                if not lock.has_lock():
                    # Then I must wait.
                    self.global_finished_cv.wait()
                else:
                    # I own the global lock.  It is okay for me to lock other locks.
                    break
            # If someone is waiting for global lock.
            elif self.global_cv:
                # If I am a new thread.
                for lock in self.locks.values():
                    if lock.has_lock():
                        # Not a new thread.
                        break
                else:
                    # Then I must wait for the global lock to be released.
                    self.global_finished_cv.wait()
                    # Loop again.
                    continue
                # I am not a new thread, okay to lock.
                break
            else:
                # No global lock contention.
                break

        self._lock(key)

    def try_lock(self, key):
        """try_lock(self, key) -> boolean
        Attempts to acquire a lock for the given key, without blocking. Returns
        true on failure and false on success, mirroring the behavior of
        coro.mutex.trylock().
        """
        # To lock the global lock, you need to use lock_global
        assert(key != self.global_lock_name)

        # If global lock is held.
        if self.global_lock_name in self.locks:
            lock = self.locks[self.global_lock_name]
            # And I don't hold it.
            if not lock.has_lock():
                # Then I would block.
                return True
        # If someone is waiting for global lock.
        elif self.global_cv:
            # If I am a new thread.
            for lock in self.locks.values():
                if lock.has_lock():
                    # Not a new thread.
                    break
            else:
                # I am a new thread. I would have to wait for the global lock
                # to be released.
                return True

        return self._trylock(key)

    def _lock(self, key):
        # Could use setdefault, but that would mean the default argument
        # would cause a mutex object to be created every time and then thrown
        # away.
        if key in self.locks:
            lock = self.locks[key]
        else:
            lock = self.locks[key] = coro.mutex()
        lock.lock()

    def _trylock(self, key):
        # Could use setdefault, but that would mean the default argument
        # would cause a mutex object to be created every time and then thrown
        # away.
        if key in self.locks:
            lock = self.locks[key]
        else:
            lock = self.locks[key] = coro.mutex()
        return lock.trylock()

    def unlock(self, key):
        """unlock(self, key) -> None
        Releases a lock for the given key.

        Raises KeyError if no lock is being held.
        """
        # To unlock the global lock, you need to use unlock_global
        assert(key != self.global_lock_name)
        self._unlock(key)
        if not self.locks or self._global_waiter_has_all_locks():
            self.global_cv.wake_all()

    def _unlock(self, key):
        lock = self.locks[key]
        lock.unlock()
        if not lock.locked() and not lock:
            # Nobody waiting on this lock, get rid of it.
            del self.locks[key]

    def _global_waiter_has_all_locks(self):
        """_global_waiter_has_all_locks(self) -> boolean
        Returns whether or not one of the threads waiting for the global lock
        owns all locks.
        """
        for t in self.global_cv._waiting:
            for lock in self.locks.values():
                if not lock.has_lock(t):
                    # No, somebody else has this lock.
                    break
            else:
                # t owns all the current locks.
                return True
        # None of the global waiters has all the locks.
        return False

    def trylock(self, key):
        """trylock(self, key) -> boolean
        Attempts to lock the mutex.  If it is already locked, then it
        returns 1.  If it successfully acquires the lock, it returns 0.
        """
        if key in self.locks:
            lock = self.locks[key]
        else:
            lock = self.locks[key] = coro.mutex()
        return lock.trylock()

    def _build_graph(self):
        """_build_graph(self) -> g
        Builds a wait-for-graph based on the threads in the locks.
        """
        g = {}
        for lock in self.locks.values():
            owner_id = lock._owner.thread_id()
            for t in lock._waiting:
                thread_id = t.thread_id()
                # Not possible for a thread to be waiting for more than one
                # lock.
                assert (thread_id not in g)
                g[thread_id] = owner_id
        if self.global_cv:
            # Threads waiting to acquire the global lock.
            for t in self.global_cv._waiting:
                thread_id = t.thread_id()
                for lock in self.locks.values():
                    lock_owner_thread_id = lock._owner.thread_id()
                    # Do not include situation where I own an ordinary lock
                    # AND am waiting for the global lock.
                    if thread_id != lock_owner_thread_id:
                        assert (thread_id not in g)
                        g[thread_id] = lock_owner_thread_id

        if self.global_finished_cv:
            # Threads waiting for global lock to be released.
            # Rare case where we ran in between one thread unlocking the
            # global lock and another locking it can cause the global lock
            # to not exist.
            if self.global_lock_name in self.locks:
                lock = self.locks[self.global_lock_name]
                owner_thread_id = lock._owner.thread_id()
                for t in self.global_finished_cv._waiting:
                    thread_id = t.thread_id()
                    assert (thread_id not in g)
                    g[thread_id] = owner_thread_id
            # In theory, these threads are also waiting for anything
            # in self.global_cv._waiting, but that would break our
            # model a thread can only be blocked by 1 other thread.
            # This is ok, since it can't cause a deadlock, anyways.

        return g

########NEW FILE########
__FILENAME__ = mbuf_statistics
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import struct
import operator
import sysctl
from functools import reduce

def get_current_mbufs():
    data = sysctl.sysctl ('kern.ipc.mbtypes')
    nelems = len(data) / 4
    nums = struct.unpack (nelems * 'l', data)
    free = nums[0]
    return free, reduce (operator.add, nums[1:])

def get():
    (peak_mbufs, clusters, spare, clfree,
     drops, wait, drain, mcfail, mpfail,
     msize, mclbytes, minclsize, mlen, mhlen
     ) = struct.unpack (14 * 'l', sysctl.sysctl ('kern.ipc.mbstat'))
    (max_mbufs,) = struct.unpack ('l', sysctl.sysctl ('kern.ipc.nmbufs'))
    (max_mbclusters,) = struct.unpack ('l', sysctl.sysctl ('kern.ipc.nmbclusters'))
    free_mbufs, current_mbufs = get_current_mbufs()
    return locals()

########NEW FILE########
__FILENAME__ = null_server
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


import coro
import signal

if __name__ == '__main__':
    signal.signal(signal.SIGUSR1, signal.SIG_IGN)
    signal.signal(signal.SIGUSR2, signal.SIG_IGN)
    coro.set_print_exit_string(False)
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = smtp_source
# -*- Mode: Python -*-

# smtp load generator

import coro
import re
import socket
import string

#
usage_message = """
python %s server
    [-h <host>]
    [-p <port>]
    [-c <nconns>]
    [-m <nmessages>]
    [-i <nidle>]
    [-r <nrcpts>]
"""

def usage ():
    sys.stderr.write (usage_message % sys.argv[0])

class smtp_client:

    buffer_size = 8192
    response_re = re.compile ('([0-9][0-9][0-9])([ -])(.*)')

    def __init__ (self):
        self.s = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.buffer = ''
        self.lines = []

    def connect (self, address):
        self.s.connect (address)

    def read_line (self):
        if self.lines:
            l = self.lines[0]
            self.lines = self.lines[1:]
            return l
        else:
            while not self.lines:
                buffer = self.s.recv (self.buffer_size)
                lines = buffer.split ('\r\n')
                for l in lines[:-1]:
                    self.lines.append (l)
                self.buffer = lines[-1]
            return self.read_line()

    def get_response (self):
        result = ''
        while True:
            line = self.read_line()
            m = self.response_re.match (line)
            if not m:
                raise ProtocolError(repr(line))
            else:
                code, cont, text = m.groups()
                result = result + text + '\n'
                if cont == ' ':
                    return code, result

    def command (self, command):
        self.s.send (command + '\r\n')
        return self.get_response()

    def send (self, data):
        return self.s.send (data)

def smtp_session (ip, port, nm, nr):
    s = smtp_client()
    s.connect ((ip, port))
    s.get_response()
    sys.stderr.write ('+')
    for x in range(nm):
        code, text = s.command ('MAIL FROM:<fred@hell.org>')
        for y in range(nr):
            code, text = s.command ('RCPT TO:<fred%d@hell.org>' % y)
        # we need to check the reply codes...
        code, text = s.command ('DATA')
        s.send (
            "Subject: testing\r\n\r\n"
            "This is message #%d\r\n" % x +
            "BCNU\r\n"
            "\r\n.\r\n"
        )
        code, text = s.get_response()
        sys.stderr.write ('m')
    s.command ('QUIT')
    sys.stderr.write('-')
    global count
    count = count - 1
    # print 'count =',count
    if count == 0:
        coro.set_exit()

count = None

def go (ip, port, nc, nm, ni, nr):
    global count
    count = nc
    for i in range (nc):
        coro.spawn (smtp_session, ip, port, nm, nr)

if __name__ == '__main__':
    import sys
    import getopt

    nc = 10
    nm = 100
    ni = 0
    nr = 1
    port = 25
    host = None
    backdoor = 0

    opts, args = getopt.getopt (sys.argv[1:], "h:c:m:i:p:r:b")

    for o, a in opts:
        if o == '-h':
            host = a
        elif o == '-c':
            nc = int(a)
        elif o == '-m':
            nm = int(a)
        elif o == '-i':
            ni = int(a)
        elif o == '-p':
            port = int(a)
        elif o == '-r':
            nr = int(a)
        elif o == '-b':
            backdoor = 1

    if not host:
        usage()
    else:

        if backdoor:
            import backdoor
            coro.spawn (backdoor.serve, 8023)

        ip = socket.gethostbyname (host)
        go (ip, port, nc, nm, ni, nr)
        coro.event_loop()
        import os
        os._exit(1)

########NEW FILE########
__FILENAME__ = bintree
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

def split (xxx_todo_changeme):
    (lo, hi) = xxx_todo_changeme
    w2 = (hi - lo) / 2
    return ((lo, lo + w2), (lo + w2, hi))

def contains (xxx_todo_changeme1, xxx_todo_changeme2):
    (la, ra) = xxx_todo_changeme1
    (lb, rb) = xxx_todo_changeme2
    return la <= lb and ra >= rb

def intersects (xxx_todo_changeme3, xxx_todo_changeme4):
    (la, ra) = xxx_todo_changeme3
    (lb, rb) = xxx_todo_changeme4
    return ra >= lb and rb >= la

# L,T,R,B

class node (object):

    __slots__ = ('l', 'r', 'objects')

    def __init__ (self):
        self.l = None
        self.r = None
        self.objects = None

    def get (self, i):
        if i == 0:
            if self.l is None:
                self.l = node()
            return self.l
        else:
            if self.r is None:
                self.r = node()
            return self.r

    def insert (self, seg, needle):
        segs = split (seg)
        for i in (0, 1):
            s = segs[i]
            if contains (s, needle):
                return self.get(i).insert (s, needle)
        if self.objects is None:
            self.objects = [needle]
        else:
            self.objects.append (needle)

    def delete (self, seg, needle):
        segs = split (seg)
        for i in (0, 1):
            s = segs[i]
            if contains (s, needle):
                return self.get(i).delete (s, needle)
        try:
            self.objects.remove (needle)
            return True
        except ValueError:
            pass

    def search_apply (self, seg, needle, fun):
        if self.objects is not None:
            for ob in self.objects:
                if intersects (ob, needle):
                    fun (ob)
        segs = split (seg)
        for i in (0, 1):
            s = segs[i]
            if contains (s, needle):
                return self.get(i).search_apply (s, needle, fun)

    def dump (self, line, depth):
        print '  ' * depth, line,
        if self.objects:
            print self.objects
        else:
            print
        l, r = split (line)
        if self.l:
            self.l.dump (l, depth + 1)
        if self.r:
            self.r.dump (r, depth + 1)

class bintree:

    def __init__ (self, line=(0, 1024)):
        self.tree = node()
        self.line = line
        self.size = 0

    def __repr__ (self):
        return '<bintree tree (objects:%d) line:%r >' % (
            self.size,
            self.line
        )

    def dump (self):
        self.tree.dump (self.line, 0)

    def insert (self, line):
        while True:
            if contains (self.line, line):
                self.tree.insert (self.line, line)
                break
            else:
                n = node()
                ll, lr = line
                sl, sr = self.line
                w = sr - sl
                if ll < sl:
                    # outside to the left
                    self.line = sl - w, sr
                    n.r, self.tree = self.tree, n
                else:
                    self.line = sl, sr + w
                    n.l, self.tree = self.tree, n
        self.size += 1

    def delete (self, needle):
        return self.tree.delete (self.line, needle)

    def search (self, needle):
        r = []
        self.tree.search_apply (self.line, needle, r.append)
        return r

    def search_apply (self, needle, fun):
        self.tree.search_apply (self.line, needle, fun)

def t0():
    t = bintree()
    t.insert ((0, 200))
    t.insert ((180, 280))
    t.insert ((3000, 4000))
    t.insert ((3050, 3060))
    t.insert ((1000, 1500))
    t.insert ((1400, 1800))
    t.dump()
    t.delete ((3050, 3060))
    t.delete ((180, 280))
    t.dump()
    return t

########NEW FILE########
__FILENAME__ = test_aio_lio
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Test program for AIO.

This exercises AIO functionality in coro.  Run with --help for more detail.
"""

# XXX TODO
# - convert to real unittest?

import backdoor
import bintree
import comm_path
import coro
import optparse
import os
import random
import signal
import sysctl
import t_aio
import tempfile
from comma_group import comma_group

usage = """test_aio.py [options] test_path

The test_path is a path to a file or device where you want to test AIO.
BEWARE!  THIS WILL DESTROY ANYTHING ON THAT DEVICE OR FILE!
"""

USING_LISTIO = 0
MAX_LIO = 0

try:
    import lio_listio
except ImportError:
    if hasattr(coro, 'lio_read'):
        USING_LISTIO = 1
    else:
        USING_LISTIO = 0

if USING_LISTIO:
    MAX_LIO = sysctl.sysctl('p1003_1b.aio_listio_max', 1)
    if MAX_LIO:
        USING_LISTIO = 1
    else:
        USING_LISTIO = 0

# 512 bytes
DISK_BLOCK_SIZE = (1 << 9)
DISK_BLOCK_MASK = ((1 << 9) - 1)

# fall 512 bytes shy of a full 64K, this will avoid wasting an entire
# 32K malloc block to hold the extra 24 bytes of Python object
# overhead.
MAX_LIO_SIZE = ((64 * 1024) - DISK_BLOCK_SIZE)

assert ((MAX_LIO_SIZE % DISK_BLOCK_SIZE) == 0)

def get_random(nbytes):
    return open('/dev/urandom').read(nbytes)

def shutdown(signum):
    # Unfortunately if the code gets in a tight loop, this doesn't run because
    # it is being delievered by kqueue.
    coro.set_exit(1)
    os._exit(1)

class TestAIO:

    def __init__(self):
        self._worker_semaphore = coro.inverted_semaphore()
        self._writes_finished = 0
        self._reads_finished = 0
        self._bytes_written = 0
        self._bytes_read = 0
        self._start_time = 0
        self._fd = -1
        self._written = []
        self._size = 0
        self._num_live_writers = 0

        self._write_cancel_success = 0
        self._write_cancel_fail = 0
        self._read_cancel_success = 0
        self._read_cancel_fail = 0
        self._assertion_errors = 0
        self._write_locks = None
        self._read_locks = None

        self.main_thread_state = 'Not started.'
        self.writer_status = {}
        self.reader_status = {}
        self.lio_status = {}

    def main(self):
        try:
            try:
                self._main()
            except SystemExit as e:
                if e.code is not None:
                    if not isinstance(e.code, int):
                        print e.code
                coro.set_exit()
        finally:
            coro.set_exit()

    def _main(self):
        parser = optparse.OptionParser(usage=usage)
        parser.add_option('-v', '--verbose', action='count',
                          help='Verbose output.  Specify multiple times for more verbosity.'
                          )
        parser.add_option('--blocks', type='int', action='store', default=1048576,
                          metavar=repr(1048576),
                          help='The size of the file in blocks.'
                          )
        parser.add_option('--num-writers', type='int', action='store', default=50,
                          metavar=repr(50),
                          help='The number of writer threads.'
                          )
        parser.add_option('--num-readers', type='int', action='store', default=50,
                          metavar=repr(50),
                          help='The number of reader threads.'
                          )
        parser.add_option('--max-num-blocks', type='int', action='store', default=100 * 1024,
                          metavar=repr(100 * 1024),
                          help='The maximum number of blocks to write.'
                          )
        parser.add_option('--block-size', type='int', action='store', default=1,
                          metavar=repr(1),
                          help='The size of a block.'
                          )
        parser.add_option('--duration', type='int', action='store', default=60,
                          metavar=repr(60),
                          help='How long to run the test in seconds.'
                          )
        parser.add_option('--reader-delay', type='int', action='store', default=10000,
                          metavar=repr(10000),
                          help='How many writes to wait to finish before starting the readers.'
                          )
        parser.add_option('--greediness', type='int', action='store', default=10,
                          metavar=repr(10),
                          help='Number of consecutive reads or writes to perform before yielding.'
                          )
        parser.add_option('--cancel-percent', type='int', action='store', default=10,
                          metavar=repr(10),
                          help='The percent of operations to try to cancel.'
                          )
        parser.add_option('--lio', action='store_true',
                          help='Use LIO instead of AIO.'
                          )
        parser.add_option('--min-lio', type='int', action='store', default=1,
                          metavar=repr(1),
                          help='The minimum number of events per LIO submit.'
                          )
        parser.add_option('--max-lio', type='int', action='store', default=MAX_LIO,
                          metavar=repr(MAX_LIO),
                          help='The maximum number of events per LIO submit.'
                          )
        parser.add_option('--max-lio-size', type='int', action='store', default=MAX_LIO_SIZE,
                          metavar=repr(MAX_LIO_SIZE),
                          help=('The maximum size of a block of data in a LIO request. '
                                'Do not change unless you know what you are doing. '
                                'This is used instead of --max-num-blocks when using LIO.')
                          )
        parser.add_option('--num-lio-workers', type='int', action='store', default=50,
                          metavar=repr(50),
                          help='The number of workers to use for LIO (used instead of --num-readers and --num-writers).'
                          )
        # blocked
        # interrupt percentage
        self.options, arguments = parser.parse_args()
        if len(arguments) != 1:
            parser.error('Must specify 1 argument.')

        # Check for valid settings.
        if self.options.lio:
            if not USING_LISTIO:
                parser.error((
                    'Unable to use LIO.  Either lio_listio is not compiled, '
                    'or the sysctl p1003_1b.aio_listio_max is not set.'))
            if self.options.max_lio > MAX_LIO:
                parser.error((
                    'Maximum number of LIO events cannot be set above the p1003_1b.aio_listio_max '
                    'sysctl value (currently %i).') % (MAX_LIO,))
            if self.options.min_lio > self.options.max_lio:
                parser.error('--min-lio cannot be set above --max-lio')
            if self.options.max_lio_size % self.options.block_size:
                parser.error('--max-lio-size is not a multiple of --block-size')
        else:
            if self.options.max_num_blocks > self.options.blocks / 2:
                parser.error('max_num_blocks cannot be greater than the file size divided by 2.')

        self._size = self.options.blocks * self.options.block_size

        self._write_locks = bintree.bintree((0, self._size))
        self._read_locks = bintree.bintree((0, self._size))

        self.path = arguments[0]
        self.main_thread_state = 'Creating file.'
        self.create()
        self.main_thread_state = 'Preparing file.'
        self.prep()
        if self.options.lio:
            self.start_lio()
        else:
            self.start_aio()
        print 'Write cancel success: %i' % (self._write_cancel_success,)
        print 'Write cancel failure: %i' % (self._write_cancel_fail,)
        print 'Read cancel success: %i' % (self._read_cancel_success,)
        print 'Read cancel failure: %i' % (self._read_cancel_fail,)
        print 'Total writes: %s (%s bytes)' % (comma_group(self._writes_finished), comma_group(self._bytes_written))
        print 'Total reads: %s (%s bytes)' % (comma_group(self._reads_finished), comma_group(self._bytes_read))
        print 'Assertion errors: %i' % (self._assertion_errors,)

    def log(self, level, message, *args):
        if level <= self.options.verbose:
            if args:
                message = message % args
            coro.print_stderr(message + '\n')

    def create(self):
        if not os.path.exists(self.path) or os.path.isfile(self.path):
            self.log(0, 'Creating %r', self.path)
            fd = os.open(self.path, os.O_RDWR | os.O_CREAT | os.O_TRUNC)
            try:
                size = self.options.blocks * self.options.block_size
                os.lseek(fd, size - 1, 0)
                os.write(fd, '\0')
            finally:
                os.close(fd)
        else:
            self.log(0, '%s is not a regular file, assuming block device', self.path)

    def start_aio(self):
        self._start_time = coro.get_now()
        self.main_thread_state = 'Starting writers.'
        self.log(1, 'Starting %i writers.', self.options.num_writers)
        for x in xrange(self.options.num_writers):
            self._worker_semaphore.acquire()
            coro.spawn(self._writer, x)

        while True:
            # Spin lock.
            status = 'Waiting for writers to ramp up %i/%i.' % (self._writes_finished, self.options.reader_delay)
            self.main_thread_state = status
            self.log(2, status)
            if int(self._worker_semaphore) == 0:
                self.log(0, 'EEP!  All writers exited too fast.')
                self.main_thread_state = 'Aborted.'
                return
            if self._writes_finished < self.options.reader_delay:
                coro.sleep_relative(0.5)
            else:
                break

        self.main_thread_state = 'Starting readers.'
        self.log(1, 'Starting %i readers.', self.options.num_readers)
        for x in xrange(self.options.num_readers):
            self._worker_semaphore.acquire()
            coro.spawn(self._reader, x)

        self.main_thread_state = 'Waiting for all threads to finish.'
        self.log(1, 'Waiting till all threads finished.')
        self._worker_semaphore.block_till_zero()
        self.main_thread_state = 'Done.'

    def start_lio(self):
        self._start_time = coro.get_now()
        self.main_thread_state = 'Starting LIO workers.'
        self.log(1, 'Starting %i lio workers.', self.options.num_lio_workers)
        for x in xrange(self.options.num_lio_workers):
            self._worker_semaphore.acquire()
            coro.spawn(self._lio, x)

        self.main_thread_state = 'Waiting for all threads to finish.'
        self.log(1, 'Waiting till all threads finished.')
        self._worker_semaphore.block_till_zero()
        self.main_thread_state = 'Done.'

    def _get_to_write(self):
        attempts = 0
        while True:
            # Pick a random location to read from.
            attempts += 1
            if not attempts % 3:
                self.log(2, 'Failed to pick write location 3 times.')
                coro.sleep_relative(0)
            block_pos = random.randint(1, self.options.blocks) - 1
            pos = block_pos * self.options.block_size

            if self.options.lio:
                max_lio_blocks = self.options.max_lio_size / self.options.block_size
                max_num_blocks = min(max_lio_blocks,
                                     self.options.blocks - block_pos)
            else:
                max_num_blocks = min(self.options.max_num_blocks,
                                     self.options.blocks - block_pos)
            num_blocks = random.randint(1, max_num_blocks)
            size = num_blocks * self.options.block_size

            area = (pos, pos + size)

            if (self._write_locks.search(area) or
                    self._read_locks.search(area)):
                # Someone is currently writing to this position.
                self.log(2, 'writer skipping lock %r', area)
            else:
                break

        self._write_locks.insert(area)
        return pos, size, area

    def _get_to_read(self):
        attempts = 0
        while True:
            # Pick a random location that has been written.
            # Don't pick anything that overlaps with stuff being written.
            attempts += 1
            if not attempts % 3:
                self.log(2, 'Failed to pick read location 3 times.')
                coro.sleep_relative(0)
            block_index = random.randint(0, len(self._written) - 1)
            pos, size = self._written[block_index]
            area = (pos, pos + size)
            if self._write_locks.search(area):
                self.log(2, 'reader skipping write lock %r', area)
            else:
                self._written.pop(block_index)
                break

        self._read_locks.insert(area)
        return pos, size, area

    def _writer(self, writer_num):
        selfish_acts = 1
        self._num_live_writers += 1
        self.writer_status[writer_num] = 'Starting.'
        try:
            while True:
                if coro.get_now() > self._start_time + self.options.duration * coro.ticks_per_sec:
                    self.writer_status[writer_num] = 'Finished.'
                    return
                if not selfish_acts % self.options.greediness:
                    self.writer_status[writer_num] = 'Greediness sleep.'
                    coro.sleep_relative(0)
                self.writer_status[writer_num] = 'Getting area to write.'
                pos, size, area = self._get_to_write()

                self.log(3, '%i: write(%i, %i)', writer_num, size, pos)
                data = t_aio.make_data(pos, size)
                try:
                    if random.random() < self.options.cancel_percent / 100.0:
                        try:
                            self.writer_status[writer_num] = 'Writing with cancel.'
                            num_written = coro.with_timeout(0, coro.aio_write, self._fd, data, long(pos))
                        except coro.TimeoutError:
                            self._write_cancel_success += 1
                        else:
                            self._written.append((pos, size))
                            self._write_cancel_fail += 1
                    else:
                        self.writer_status[writer_num] = 'Writing.'
                        num_written = coro.aio_write(self._fd, data, long(pos))
                        if num_written != size:
                            self.log(0, 'ERROR: Failed to write %i bytes (%i written).' % (size, num_written))
                            self._assertion_errors += 1
                        else:
                            self._written.append((pos, size))
                finally:
                    self._write_locks.delete(area)
                selfish_acts += 1
                # print len(self._written)
                self._writes_finished += 1
                self._bytes_written += size
        finally:
            self._worker_semaphore.release()
            self._num_live_writers -= 1

    def _have_blocks_to_read(self):
        if len(self._written) == 0:
            self.log(3, 'Reader has nothing to do.')
            return False
        # Because we can read faster than we can write, we do not want
        # the amount of available data to fall too low.
        if len(self._written) < self.options.reader_delay / 2 and self._num_live_writers:
            self.log(3, 'Reader is backing off.')
            return False
        return True

    def _reader(self, reader_num):
        selfish_acts = 1
        self.reader_status[reader_num] = 'Starting.'
        try:
            while True:
                if coro.get_now() > self._start_time + self.options.duration * coro.ticks_per_sec:
                    self.reader_status[reader_num] = 'Finished.'
                    return
                if not selfish_acts % self.options.greediness:
                    self.reader_status[reader_num] = 'Greediness sleep.'
                    coro.sleep_relative(0)
                if not self._have_blocks_to_read():
                    self.reader_status[reader_num] = 'Sleeping, waiting for writers to catch up.'
                    coro.sleep_relative(0.1)
                    continue
                pos, size, area = self._get_to_read()

                self.log(3, '%i: read(%i, %i)', reader_num, size, pos)
                try:
                    if random.random() < self.options.cancel_percent / 100.0:
                        try:
                            self.reader_status[reader_num] = 'Reading with cancel.'
                            data = coro.with_timeout(0, coro.aio_read, self._fd, size, long(pos))
                        except coro.TimeoutError:
                            self._read_cancel_success += 1
                        else:
                            self._read_cancel_fail += 1
                    else:
                        self.reader_status[reader_num] = 'Reading.'
                        data = coro.aio_read(self._fd, size, long(pos))
                        expected_data = t_aio.make_data(pos, size)
                        if data != expected_data:
                            self._assertion_errors += 1
                            self.log(0, 'ERROR: data read=%i expected=%i pos=%i', len(data), size, pos)
                            fname = tempfile.mktemp()
                            f = open(fname, 'w')
                            f.write(data)
                            f.close()
                            self.log(0, 'Wrote temp file %s', fname)
                finally:
                    self._read_locks.delete(area)
                selfish_acts += 1
                self._reads_finished += 1
                self._bytes_read += size
        finally:
            self._worker_semaphore.release()

    def _lio(self, worker_num):
        selfish_acts = 1
        self._num_live_writers += 1
        self.lio_status[worker_num] = 'Starting.'
        try:
            while True:
                if coro.get_now() > self._start_time + self.options.duration * coro.ticks_per_sec:
                    self.lio_status[worker_num] = 'Finished.'
                    return
                if not selfish_acts % self.options.greediness:
                    self.lio_status[worker_num] = 'Greediness sleep.'
                    coro.sleep_relative(0)
                num_events = random.randint(self.options.min_lio, self.options.max_lio)
                requests = []
                reads_to_unlock = []
                writes_to_unlock = []
                self.log(3, '%i: lio(%i)', worker_num, num_events)
                if self._have_blocks_to_read():
                    do_read = random.randint(0, 1)
                else:
                    do_read = False
                if do_read:
                    expected_result = []
                else:
                    expected_result = 0

                for unused in xrange(num_events):
                    if do_read:
                        if not self._have_blocks_to_read():
                            # Skip the rest of the reads.
                            continue
                        self.lio_status[worker_num] = 'Getting area to read.'
                        pos, size, area = self._get_to_read()
                        requests.append((self._fd, pos, size))
                        data = t_aio.make_data(pos, size)
                        expected_result.append(data)
                        self.log(3, '%i: lio_read(%i, %i)', worker_num, size, pos)
                        reads_to_unlock.append(area)
                        lio_op = coro.lio_read
                    else:
                        self.lio_status[worker_num] = 'Getting area to write.'
                        pos, size, area = self._get_to_write()
                        data = t_aio.make_data(pos, size)
                        requests.append((self._fd, pos, data))
                        expected_result += size
                        self.log(3, '%i: lio_write(%i, %i)', worker_num, size, pos)
                        writes_to_unlock.append(area)
                        lio_op = coro.lio_write

                try:
                    if random.random() < self.options.cancel_percent / 100.0:
                        try:
                            if do_read:
                                self.lio_status[worker_num] = 'Doing LIO READ with cancel.'
                            else:
                                self.lio_status[worker_num] = 'Doing LIO WRITE with cancel.'
                            result = coro.with_timeout(0, lio_op, requests)
                        except coro.TimeoutError:
                            self._write_cancel_success += 1
                            continue
                        else:
                            self._write_cancel_fail += 1
                    else:
                        if do_read:
                            self.lio_status[worker_num] = 'Doing LIO READ.'
                        else:
                            self.lio_status[worker_num] = 'Doing LIO WRITE.'
                        result = lio_op(requests)
                finally:
                    for area in reads_to_unlock:
                        self._read_locks.delete(area)
                    for area in writes_to_unlock:
                        self._write_locks.delete(area)

                if do_read:
                    if len(result) != len(expected_result):
                        self.log(0, 'ERROR: Length of result (%i) is not expected (%i).',
                                 len(result), len(expected_result))
                        continue

                    for result_value, expected_value in zip(result, expected_result):
                        if result_value != expected_value:
                            self.log(0, 'ERROR: Expected read of %i bytes, got %i bytes not equal.',
                                     len(expected_value), len(result_value))
                        else:
                            self._reads_finished += 1
                            self._bytes_read += len(expected_value)
                else:
                    # doing a write, return value is just an integer.
                    if result != expected_result:
                        self.log(0, 'ERROR: Result from write (%i) not expected (%i).', result, expected_result)
                        continue
                    else:
                        self._writes_finished += 1
                        self._bytes_written += expected_result
                        self._written.append((pos, size))

        finally:
            self._num_live_writers -= 1
            self._worker_semaphore.release()

    def prep(self):
        size = self.options.blocks * self.options.block_size
        self.log(1, 'Writing out %i bytes.', size)
        # Not sure if we should have a command-line flag to try with or without
        # O_DIRECT.  Not sure if it makes a difference.
        # XXX: Add O_FSYNC for testing.
        self._fd = os.open(self.path, os.O_RDWR | os.O_DIRECT | os.O_FSYNC)
        block = '\0' * 1024 * 1024
        num_blocks = size / len(block)
        for unused in xrange(num_blocks):
            os.write(self._fd, block)
        # Write any partial data.
        partial_size = size % len(block)
        if partial_size:
            os.write(self._fd, block[:partial_size])

    def print_status(self, unused=None):
        print 'Main thread status: %s' % (self.main_thread_state,)
        print 'Current read pool size: %i' % (len(self._written),)
        if self.writer_status:
            print 'Writer status:'
            items = sorted(self.writer_status.items())
            for i, status in items:
                print '%i: %s' % (i, status)
        if self.reader_status:
            print 'Reader status:'
            items = sorted(self.reader_status.items())
            for i, status in items:
                print '%i: %s' % (i, status)
        if self.lio_status:
            print 'LIO status:'
            items = sorted(self.lio_status.items())
            for i, status in items:
                print '%i: %s' % (i, status)

if __name__ == '__main__':
    t = TestAIO()
    coro.set_print_exit_string(False)
    coro.install_signal_handlers = 0
    bd_path = comm_path.mk_backdoor_path('test_aio_lio')
    coro.spawn(backdoor.serve, unix_path=bd_path).set_name('backdoor')
    coro.signal_handler.register_signal_handler (signal.SIGINT, shutdown)
    coro.signal_handler.register_signal_handler (signal.SIGTERM, shutdown)
    coro.signal_handler.register_signal_handler (signal.SIGHUP, t.print_status)
    coro.spawn(t.main)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = test_condition
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro

wait1 = coro.condition_variable()
wait2 = coro.condition_variable()

def func1():
    print "func1 entering into wait."
    wait1.wait()
    print "func1 broke out of wait."

def func1_2():
    print "func1_2 entering into wait."
    wait1.wait()
    print "func1_2 broke out of wait."

def func2():
    print "func2 entering into wait."
    wait2.wait()
    print "func2 broke out of wait."

def func2_2():
    print "func2_2 entering into wait."
    wait2.wait()
    print "func2_2 broke out of wait."

def func3():
    wait1.wake_one()
    wait2.wake_all()

coro.spawn(func1)
coro.spawn(func1_2)
coro.spawn(func2)
coro.spawn(func2_2)
coro.spawn(func3)
coro.event_loop(30.0)

########NEW FILE########
__FILENAME__ = test_coro
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# test_coro
#
# This file tries to excersize all the functionality of the coroutine system.
# It will try to make sure things function properly, and don't leak.

# To Test:
# - sleep_absolute/relative
# - with_timeout
# - wait_for_kevent
# - register_other_kevent
# - aio_read/write
# - selfish operations
# - yield/resume
# - socket functions
# - interrupt
#
# Tests:
# - functional
# - memory leaks

from comma_group import comma_group
import coro
import GetArg
import mstats
import sys
import types

##############################################################################

def test_make_coro():
    test_func = lambda(x, y): (x, y)
    for x in xrange(1000):
        a = coro.new(test_func, 0, 1)
        # Unspawned threads aren't able to clean up their all_threads reference
        del coro.all_threads[a.thread_id()]
        del a
    coros = map(lambda x, test_func=test_func: coro.new(test_func, x, 1), range(1000))
    for x in coros:
        # Unspawned threads aren't able to clean up their all_threads reference
        del coro.all_threads[x.thread_id()]
    del coros

##############################################################################

def test_sleep():
    coro.print_stderr('    This pause should be 1 second.\n')
    coro.sleep_relative(1)
    coro.print_stderr('    This pause should be 1 second.\n')
    coro.sleep_relative(1.0)
    coro.print_stderr('    This pause should be 1 second.\n')
    coro.sleep_relative(1L * coro.ticks_per_sec)
    coro.print_stderr('    This pause should be 1 second.\n')
    coro.sleep_absolute(coro.now + 1L * coro.ticks_per_sec)

##############################################################################

def test_sleep_interrupt():
    coro.spawn(_test_sleep_interrupt, coro.current())
    # The other thread will interrupt me
    coro.print_stderr('There should be no pause here:\n')
    try:
        coro.sleep_relative(10)
    except coro.Interrupted, why:
        if why != 'foobar':
            raise ValueError('Incorrect interrupt value.')

def _test_sleep_interrupt(c):
    c.interrupt('foobar')

##############################################################################

def test_multiple_sleep_interrupt():
    # The first interrupt will work just fine
    coro.spawn(_test_multiple_sleep_interrupt_ok, coro.current())
    # Since it has already been interrupted, you can't interrupt it ever again.
    # The following threads will fail.  This is to be expected.
    # Someday we need to redesign how interrupts work so that these won't fail.
    # But that will be very tricky.
    coro.spawn(_test_multiple_sleep_interrupt_fail, coro.current())
    coro.spawn(_test_multiple_sleep_interrupt_fail, coro.current())
    coro.spawn(_test_multiple_sleep_interrupt_fail, coro.current())
    # The other thread will interrupt me
    coro.print_stderr('    There should be no pause here:\n')
    try:
        coro.sleep_relative(10)
    except coro.Interrupted, why:
        if why != 'foobar':
            raise ValueError('Incorrect interrupt value.')

def _test_multiple_sleep_interrupt_ok(c):
    c.interrupt('foobar')

def _test_multiple_sleep_interrupt_fail(c):
    try:
        c.interrupt('foobar')
    except SystemError, why:
        coro.print_stderr('    Expected exception: %s\n' % (why,))
    else:
        raise ValueError('SystemError didn\'t happen as expected!')

##############################################################################

def test_with_timeout():
    try:
        coro.print_stderr('    Should be a 1 second pause:\n')
        coro.with_timeout(1, coro.sleep_relative, 5)
    except coro.TimeoutError:
        pass
    else:
        raise ValueError('Timeout didn\'t happen as expected!')

def test_with_timeout_with_interrupt():
    coro.spawn(_test_with_timeout_with_interrupt, coro.current())
    try:
        coro.print_stderr('    Should be no pause:\n')
        coro.with_timeout(1, coro.sleep_relative, 5)
    except coro.Interrupted, why:
        if why != 'foobar':
            raise ValueError('Interrupt value is not foobar!')
    else:
        raise ValueError('Interrupt didn\'t happen as expected!')

def _test_with_timeout_with_interrupt(c):
    c.interrupt('foobar')

##############################################################################

def test_resume():
    # Resume myself.  This should work without problems.
    coro.spawn(_test_resume, coro.current())
    result = coro.yield()
    if result != 'yoyo':
        raise ValueError('Resume with wrong value!')

def _test_resume(c):
    c.resume('yoyo')

##############################################################################

def test_interrupt():
    # Resume myself.  This should work without problems.
    coro.spawn(_test_interrupt_resume, coro.current())
    # Interrupt, this should be latent
    coro.spawn(_test_interrupt_latent, coro.current())
    # Interrupt, this should fail
    coro.spawn(_test_interrupt_fail, coro.current())
    result = coro.yield()
    coro.print_stderr('resuming with result %r\n' % (result,))
    if result != 'yoyo':
        raise ValueError('Resume with wrong value!')
    # Go back to sleep to catch the latent interrupt
    try:
        result = coro.yield()
    except coro.Interrupted, why:
        if why != 'foo':
            raise ValueError('Wrong why %s' % why)

def _test_interrupt_resume(c):
    coro.print_stderr('resume running\n')
    coro.coro_sched.the_scheduler.schedule(c, 'yoyo')

def _test_interrupt_latent(c):
    coro.print_stderr('interrupter running\n')
    result = c.interrupt('foo')
    if result != 1:
        raise ValueError('Not latent? %i' % result)

def _test_interrupt_fail(c):
    try:
        c.interrupt('bar')
    except SystemError:
        pass
    else:
        raise ValueError('Second latent interrupt didn\'t fail?')


##############################################################################

def test_raise():
    # Resume myself.  This should work without problems.
    coro.spawn(_test_raise, coro.current())
    try:
        result = coro.yield()
    except ZeroDivisionError, why:
        if why[0] != 12345:
            raise ValueError('Why is wrong %s' % (why,))
        pass
    else:
        raise ValueError('Failed to raise!')

def _test_raise(c):
    c.resume_with_exc(ZeroDivisionError, 12345)

##############################################################################
##############################################################################
##############################################################################

def do_tests(tests):
    for x in xrange(5):
        for func_name in tests:
            f = globals()[func_name]
            if isinstance(f, types.FunctionType):
                if f.func_name.startswith('test_'):
                    coro.print_stderr('Running test %s...\n' % f.func_name)
                    start_ram = mstats.get_malloc_stats()['allocated_bytes']
                    apply(f, ())
                    end_ram = mstats.get_malloc_stats()['allocated_bytes']
                    coro.print_stderr('RAM difference: %s\n' % comma_group(end_ram - start_ram))
    coro._exit = 1

if __name__ == '__main__':
    args = GetArg.GetArg()
    args.process(sys.argv[1:])
    if args.arguments:
        tests = args.arguments
    else:
        tests = globals().keys()
    coro.spawn(do_tests, tests)
    coro.event_loop()

########NEW FILE########
__FILENAME__ = test_coro_ssl
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# -*- Mode: Python -*-

# first stab at coro SSL sockets.

import sslip
import coro
import socket
import coro_ssl
import string

# there are some instructions here:
#    http://members.netscapeonline.co.uk/jeremyalansmith/ssltutorial/
# that tell you how to quickly and easily generate some cert and key files.
# It's also pretty easy to generate them directly using the POW module.
# look in POW/test/utest.py for lots and lots of examples...
#
# also, for quick testing, you can't beat mini_httpd, by Poskanzer.
# The source tarball is 30k!  Compile it thus:
# 1) uncomment & edit the four SSL lines at the top of Makefile
# 2) gmake
# 3) run it: 'mini_httpd -S -p 443'

# You don't need a certificate to make a client connection.
# But: the server side can decide not to talk to you unless. [see ssl.setVerifyMode()]
# Netscape for example, does not use a certificate when making an https connection.


def http_client():
    "fetch a web page from an https server"
    client = coro_ssl.ssl_sock()
    client.create()
    client.connect (('192.168.200.44', 8080))
    client.send ('GET / HTTP/1.1\r\nConnection: close\r\n\r\n')
    data = ''
    while True:
        block = client.recv (8192)
        if block:
            data += block
        else:
            break
    print data
    client.close()

def client():
    "test echo client"
    client = coro_ssl.ssl_sock()
    client.create()
    client.connect (('127.0.0.1', 9090))
    print client.recv (8192)
    for i in range (10):
        client.send ('Hello There (%d)\r\n' % i)
        print client.recv (8192)
    client.close()

def channel (conn, addr):
    "echo server client thread"
    peer_cert = conn.ssl.get_peer_cert()
    if peer_cert:
        print 'peer CN=%r' % (peer_cert.get_cn(),)
    else:
        print 'no peer cert'
    conn.send ('Hi there %r\n' % (addr,))
    print 'cipher=', conn.ssl.get_cipher()
    while True:
        line = conn.recv (8192)
        if not line:
            break
        else:
            conn.send (line)

def server (port=9090):
    "echo server"
    server = coro_ssl.ssl_sock()
    server.create()
    server.bind (('', port))
    server.listen (1024)
    while True:
        conn, addr = server.accept()
        coro.spawn (channel, conn, addr)

def smtp_tls_server_session (conn):
    oconn = conn
    conn.send ('200 howdy\r\n')
    while True:
        cmd = conn.recv (1024)
        coro.print_stderr ('got %r\r\n' % (cmd,))
        cmd = cmd.lower()
        if cmd.startswith ('starttls'):
            conn.send ('220 ready for tls\r\n')
            ctx = coro_ssl.ssl_ctx (sslip.SSLV23_SERVER_METHOD)
            try:
                sconn = coro_ssl.ssl_sock (ctx)
                sconn.create (sock=conn)
                sconn.ssl_accept()
                conn = sconn
            except sslip.Error:
                # conn.send ('454 TLS negotiation failed\r\n')
                pass
        elif cmd.startswith ('data'):
            conn.send ('354 go ahead\r\n')
            while True:
                block = conn.recv (8192)
                if block.endswith ('\r\n.\r\n'):
                    break
            conn.send ('250 Ok.\r\n')
        elif cmd.startswith ('quit'):
            conn.send ('221 byte\r\n')
            conn.close()
            break
        elif cmd.startswith ('ehlo'):
            conn.send (
                '250-loki.ironport.com\r\n'
                '250-PIPELINING\r\n'
                '250-SIZE 10240000\r\n'
                '250-STARTTLS\r\n'
                '250 8BITMIME\r\n'
            )
        else:
            conn.send ('200 ok\r\n')

def smtp_tls_server():
    sock = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
    sock.bind (('0.0.0.0', 25))
    sock.listen (5)
    while True:
        conn, addr = sock.accept()
        coro.spawn (smtp_tls_server_session, conn)

# an easy way to test this is to install the qmail-tls port.
# [tricky, had to rename /var/qmail/control/cert.pem to servercert.pem]
def smtp_tls_session (
    host,
    port='25',
    fromaddr='rushing@nightmare.com',
    to='rushing@nightmare.com'
):
    print "smtp tls test client"
    sock = coro.make_socket (socket.AF_INET, socket.SOCK_STREAM)
    print "host=%r port=%r" % (host, port)
    port = string.atoi(port)
    sock.connect ((host, port))
    print sock.recv (8192)
    sock.send ('EHLO fang\r\n')
    print sock.recv (8192)
    sock.send ('STARTTLS\r\n')
    print sock.recv (8192)
    ctx = coro_ssl.ssl_ctx (sslip.SSLV2_CLIENT_METHOD)
    client = coro_ssl.ssl_sock (ctx)
    client.create (sock=sock)
    # client.ssl.set_connect_state()
    try:
        coro.print_stderr ('calling ssl_connect()\n')
        client.ssl_connect()
        coro.print_stderr ('ssl_connect done()\n')
    except sslip.Error:
        coro.print_stderr ("TLS negotiation failed\n")
        coro.print_stderr ("hit <return> to attempt fallback\n")
        client.shutdown()
        raw_input()
    else:
        sock = client
    print "ssl_connect() finished"
    sock.send ('HELP\r\n')
    print sock.recv (8192)
    sock.send ('MAIL FROM:<' + fromaddr + '>\r\n')
    print sock.recv (8192)
    sock.send ('RCPT TO:<' + to + '>\r\n')
    print sock.recv (8192)
    sock.send ('DATA\r\n')
    print sock.recv (8192)
    sock.send ('From: ' + fromaddr + '\r\nSubject: testing STARTTLS\r\n\r\nHi there.  I was encrypted\r\n.\r\n')
    print sock.recv (8192)
    sock.send ('QUIT\r\n')
    print sock.recv (8192)
    sock.close()
    coro._exit = 1

if __name__ == '__main__':
    import backdoor
    import sys
    fromaddr = 'srushing@ironport.com'
    to = 'srushing@ironport.com'
    host = '192.168.200.6'
    port = '25'
    coro.spawn (backdoor.serve)
    if '-s' in sys.argv:
        coro.spawn (server)
    elif '--smtp-tls-server' in sys.argv:
        coro.spawn (smtp_tls_server)
    elif '--smtp-tls' in sys.argv:
        if '--host' in sys.argv:
            i = 1 + sys.argv.index('--host')
            host = sys.argv[i]
        if '--port' in sys.argv:
            i = 1 + sys.argv.index('--port')
            port = sys.argv[i]
        if '--from' in sys.argv:
            i = 1 + sys.argv.index('--from')
            fromaddr = sys.argv[i]
        if '--to' in sys.argv:
            i = 1 + sys.argv.index('--to')
            to = sys.argv[i]
        coro.spawn (smtp_tls_session, host, port, fromaddr, to)
    else:
        coro.spawn (client)
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = test_frame_lock
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


import coro
import operator
import sys
from functools import reduce

W = coro.write_stderr

# These tests need to be cleaned up a bit so they make sense to
# someone other than me.  The general structure of these is to uncover
# unexpected recursive invocations of the Python VM by getting them to
# stack up; and then unwind out of order.  Several of the tests below
# do this by arranging a series of functions to wait a certain amount
# of time before returning.  If we arrange for the first-called to be
# the first to return, we'll get them out of order.

class thing:

    def __init__ (self, sleep):
        self.sleep = sleep

    def __call__ (self, *args):
        W ('%r.__call__(), depth=%d\n' % (self, sys.get_dispatcher_depth()))
        coro.sleep_relative (self.sleep)
        return reduce (operator.add, args)

class thing2:

    def __init__ (self, sleep):
        coro.sleep_relative (sleep)
        W ('%r.__init__(), depth=%d\n' % (self, sys.get_dispatcher_depth()))

def fun0 (sleep, arg0, arg1):
    fun_42(sleep)
    return arg0 + arg1

def fun_42 (sleep):
    coro.sleep_relative (sleep)

def fun1 (sleep, arg0, arg1):
    W ('fun1() sleep=%d\n' % sleep)
    result = fun0(*(sleep, arg0, arg1))
    W ('fun1() sleep=%d, result=%r\n' % (sleep, result))

# tests switching in __call__
def go (t):
    W ('before %r {%d}\n' % (t, sys.get_dispatcher_depth()))
    result = t(3, 4, 5)
    W ('after %r {%d}\n' % (t, sys.get_dispatcher_depth()))
    return result

# tests switching in __init__
def go2():
    for x in range (5):
        coro.spawn (go3, x)

# tests switching in apply()
def go4():
    for x in range (5):
        coro.spawn (fun1, x, 3, 4)

def fun2 (sleep):
    W ("fun2() sleep=%d\n" % (sleep,))
    x = thing2(*(sleep,))
    W ("fun2() sleep=%d, result=%r\n" % (sleep, x))

def go5():
    for x in range (5):
        coro.spawn (fun2, x)

all_things = []

def go3 (x):
    global all_things
    W ('before __init__\n')
    t = thing2 (x)
    W ('appending %r\n' % (t,))
    all_things.append (t)

class Plonk (Exception):
    def __init__ (*args):
        Exception.__init__ (*args)
        W ('args=%r\n' % (args,))

def fun3():
    coro.sleep_relative (5)
    raise Plonk(1, 2, 3)

def fun4():
    for i in range (50):
        x = operator.add(*(3, 4))
        assert (x == 7)
        W ('x=7 [%d]\n' % (sys.getrefcount(7)))

def fun5():
    for i in range (50):
        x = thing(*(i,))
        W ('None [%d]\n' % (sys.getrefcount(None)))

# ================================================================================
# test switching in call of unbound method
# ================================================================================

class my_fifo (coro.fifo):

    def pop (self):
        return coro.fifo.pop (self)

the_fifo = my_fifo()

def popper (n):
    global live
    W ('>%d>' % (n,))
    while True:
        x = the_fifo.pop()
        W ('[%d.%d]' % (n, x))
        coro.sleep_relative (0)
    W ('<%d<' % (n,))

def pusher():
    i = 0
    while i < 100:
        the_fifo.push (i)
        coro.sleep_relative (.005)
        i += 1
    coro._exit = 1

def fun6():
    coro.spawn (pusher)
    for i in xrange (13):
        coro.spawn (popper, i)

# ================================================================================

def fun7():
    import os
    v = os.environ.get ("HOME")
    W ('v=%r\n' % (v,))
    W ('os.environ=%r\n' % (os.environ,))

# ================================================================================

import _coro

class X:
    def m (self, arg):
        _coro.breakpoint()
        return arg + 42

class Y (X):
    def m (self, arg):
        return X.m (self, arg) + 19

def fun8():
    y = Y()
    W ('y.m(34)=%r\n' % (y.m(34),))

# ================================================================================

class A:
    def m (self, t, *args):
        W ('>> %r.m(), t=%d depth=%d\n' % (self, t, sys.get_dispatcher_depth()))
        coro.sleep_relative (t)
        W ('<< %r.m(), t=%d depth=%d\n' % (self, t, sys.get_dispatcher_depth()))
        return args + (42,)

def go9 (t):
    a = A()
    return a.m (t, 3, 4, 5)

# tests switching in fancy-args method (ext_do_call)
def fun9():
    for x in range (5):
        coro.spawn (go9, x)

# ================================================================================

if 0:
    # tests switching in __call__
    for x in range (5):
        t = thing (x)
        coro.spawn (go, t)
elif 0:
    # tests switching in __init__
    coro.spawn (go2)
elif 0:
    # tests switching in apply()
    coro.spawn (go4)
elif 0:
    # tests switching in apply(<class>, <args>)
    coro.spawn (go5)
elif 0:
    # tests exception.__init__
    coro.spawn (fun3)
elif 0:
    # tests apply (<builtin>, <args>)
    coro.spawn (fun4)
elif 0:
    # leak test __init__
    coro.spawn (fun5)
elif 0:
    # tests switching in call of unbound method
    coro.spawn (fun6)
elif 0:
    # ensure override of __repr__ doesn't get UnwindToken
    coro.spawn (fun7)
elif 0:
    # tests some other fucked-up thing
    coro.spawn (fun8)
elif 1:
    # tests switching in fancy-args method
    coro.spawn (fun9)

coro.event_loop()

########NEW FILE########
__FILENAME__ = test_inverted_semaphore
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro

l = coro.inverted_semaphore()

def worker(sleep):
    global l
    print 'working starting'
    l.acquire()
    coro.sleep_relative(sleep)
    print 'worker done, releasing'
    l.release()

def waiter():
    global l
    print 'waiter'
    l.block_till_zero()
    print 'waiter awoke'
    coro._exit = 1

coro.spawn(worker, 5)
coro.spawn(worker, 10)
coro.spawn(waiter)
coro.event_loop()

########NEW FILE########
__FILENAME__ = test_in_parallel
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import os
import coro
from coro import aio_read

def t1():
    print 'expect [1024] * 10...'
    fd = os.open ('/kernel', os.O_RDONLY)
    # 10 1KB read requests
    requests = [
        (aio_read, (fd, 1024, int64(i * 102400)))
        for i in range (10)
    ]
    results = coro.in_parallel (requests)
    print [len(x) for x in results]
    print 'expect one OSError at the end...'
    fd = os.open ('/kernel', os.O_RDONLY)
    # 10 1KB read requests
    requests = [
        (aio_read, (fd, 1024, int64(i * 102400)))
        for i in range (10)
    ]
    requests.append (
        (aio_read, (-1, 1024, 1024))
    )
    try:
        coro.in_parallel (requests)
    except coro.InParallelError as e:
        for i in xrange (len(e.partial_results)):
            status, result = e.partial_results[i]
            if status is coro.SUCCESS:
                print 'job %2d:' % i, status, len(result)
            else:
                print 'job %2d:' % i, status, result
    else:
        raise SystemError("expected an exception")

if __name__ == '__main__':
    import backdoor
    coro.spawn (backdoor.serve)
    coro.spawn (t1)
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = test_multiple_interrupts
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# This script will trigger multiple interrupts to test the "latent interrupt"
# code of coro_sched.c.

import coro
import sys

exitval = -1
msg = "failure message was not set"

def simple_thread():
    global exitval, msg
    count = 0
    try:
        while count < 10:
            # print "count is %d" % (count,)
            coro.sleep_relative (1.0)
            count += 1
        msg = "thread was never interrupted"
        exitval = 1
    except coro.Interrupted as why:
        if why == []:
            msg = "multiple interrupts policy succeeded"
            exitval = 0
        elif why == [{}]:
            msg = "multiple interrupts policy reversed - it should use the first interrupt's value"
            exitval = 1
        else:
            msg = "interrupts out of order: interrupt value is %r" % (why,)
            exitval = 1
    except:
        msg = "multiple interrupts test failed: exception is %r" % tb.traceback_string()
        exitval = 1
    coro._exit = 1

def tripwire(sleep_val):
    thread = coro.spawn (simple_thread)
    # let simple_thread run for a while:
    coro.sleep_relative (sleep_val)
    # use these values to try to trigger refcount bugs:
    thread.interrupt([])
    thread.interrupt({})
    thread.interrupt({'': []})
    thread.interrupt([{}])

def main(verbose=0):
    global exitval, msg
    default_exitval = exitval
    default_msg = msg
    reports = []
    failures = 0

    # 3.0 is a multiple of 1.0, which timing allows a latent interrupt.
    for sleep_val in [2.5, 3.0]:
        coro.spawn (tripwire, sleep_val)
        coro.event_loop ()
        reports.append ((sleep_val, msg, exitval))
        if exitval != 0:
            failures += 1

        # reset values:
        coro._exit = 0
        exitval = default_exitval
        msg = default_msg

    if failures:
        if verbose:
            print "%d failures" % (failures,)
            for sleep_val, msg, exitval in reports:
                print "sleep val %s: %s" % (sleep_val, msg)

        sys.exit (1)
    else:
        if verbose:
            print "All tests passed."
        sys.exit (0)

if __name__ == '__main__':
    main(verbose=1)

########NEW FILE########
__FILENAME__ = test_mutex
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro

m = coro.mutex()

def test_mutex():
    m.lock()
    coro.spawn(locker, 1)
    coro.spawn(locker, 2)
    coro.yield_and_schedule()
    m.unlock()
    coro.yield_and_schedule()
    coro._exit = 1

def locker(n):
    print 'locker %i' % n
    m.lock()
    print 'locker %i got it' % n
    m.unlock()


coro.spawn(test_mutex)
coro.event_loop()

########NEW FILE########
__FILENAME__ = test_other_kevent
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro
import kqueue_events
import process
import os

def p():
    while True:
        print 'hi'
        coro.sleep_relative(.5)

def main():
    pid, fi, fo, fe = process.spawn_job_bg('/bin/sleep 3')
    coro.wait_for_kevent (pid, kqueue_events.EV_ADD | kqueue_events.EV_ONESHOT,
                          kqueue_events.EVFILT_PROC, kqueue_events.NOTE_EXIT)
    print 'wait done'
    os.waitpid(pid, 0)
    coro.sleep_relative(5)
    coro._exit = 1

coro.spawn(p)
coro.spawn(main)
coro.event_loop()

########NEW FILE########
__FILENAME__ = test_rtsig_scheduler
# -*- Mode: Python; tab-width: 4 -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import sys
import socket

d = 'asdf' * 1024
s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
s.connect (('localhost', 10001))

i = 0
while True:
    ignore = s.send (d)
    ignore = s.recv (4096)
    i = i + 1
    if (i % 100) == 0:
        sys.stderr.write ('.')

########NEW FILE########
__FILENAME__ = test_rw_lock
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro

rw_lock = coro.rw_lock()

def test_rw_lock():
    rw_lock.read_lock()
    rw_lock.read_lock()
    # since write_lock will block, let's get someone to unlock the readers
    coro.spawn(unlocker)
    rw_lock.write_lock()
    coro.spawn(reader)
    coro.spawn(writer)
    coro.yield_and_schedule()
    rw_lock.write_unlock()
    coro._exit = 1

def writer():
    rw_lock.write_lock()
    rw_lock.write_unlock()

def reader():
    print 'reader locking'
    rw_lock.read_lock()
    print 'reader got lock'
    rw_lock.read_unlock()
    print 'reader done'

def unlocker():
    print 'unlocker'
    rw_lock.read_unlock()
    rw_lock.read_unlock()

coro.spawn(test_rw_lock)
coro.event_loop()

########NEW FILE########
__FILENAME__ = test_semaphore
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro

oranges_left = coro.semaphore(10)

def func1():
    print "func1 getting some oranges."
    oranges_left.acquire(9)
    print "func1 done"

def func2():
    print "func2 getting some oranges."
    oranges_left.acquire(3)
    print "func2 done"
    coro._exit = 1

def func3():
    print "func3 releasing oranges from func1"
    oranges_left.release(9)
    print "func3 done"

coro.spawn(func1)
coro.spawn(func2)
coro.spawn(func3)
coro.event_loop()

########NEW FILE########
__FILENAME__ = test_sendfile
# -*- Mode: Python; tab-width: 4 -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import coro
import os
import socket
import string

def client (conn, addr):
    # get request line
    request = conn.recv (8192)
    lines = string.split (request, '\r\n')
    filename = lines[0]
    if os.path.isfile (filename):
        fd = os.open (filename, os.O_RDONLY)
        length = os.lseek (fd, 0, 2)
        os.lseek (fd, 0, 0)
        conn.sendfile (fd, 0, length)
        conn.send ('bye.\r\n')
        os.close (fd)
    else:
        conn.send ('no such file!\r\n')
    conn.close()

def server (port=18080):
    s = coro.coroutine_socket()
    s.create_socket (socket.AF_INET, socket.SOCK_STREAM)
    s.set_reuse_addr()
    s.bind (('', port))
    s.listen (1024)
    while True:
        conn, addr = s.accept()
        coro.spawn (client, conn, addr)

if __name__ == '__main__':
    coro.spawn (server)
    coro.event_loop (30.0)

########NEW FILE########
__FILENAME__ = who_calls
# -*- Mode: Python -*-
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import sys

class whoCallsError (Exception):
    pass

import os

def get_module_name (n):
    try:
        return os.path.split (n)[-1].split('.')[0]
    except:
        return '???'

def who_calls_helper():
    tinfo = []
    exc_info = sys.exc_info()

    f = exc_info[2].tb_frame.f_back
    while f:
        tinfo.append ((
            get_module_name (f.f_code.co_filename),
            f.f_code.co_name,
            str (f.f_lineno)
        ))
        f = f.f_back

    del exc_info
    tinfo.reverse()
    return '[' + ('] ['.join (map ('|'.join, tinfo))) + ']'

def who_calls():
    try:
        raise whoCallsError
    except whoCallsError:
        tinfo = who_calls_helper()
    return tinfo

########NEW FILE########
__FILENAME__ = coro_unittest
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""coro unittest helper.

This code will help you start up a unittest running in a coro environment.

In your unittest, add test case classes:

    class SomeTest(unittest.TestCase):

        def test_foo(self):
            \"\"\"Do the foo test.\"\"\"
            ...

At the bottom of the file, put this:

    if __name__ == '__main__':
        coro_unittest.run_tests()

This will automatically find any TestCase class in your file and run all
methods starting with "test_" in it.

This will also set up a test case that will assert that no threads crashed
while running.
"""

import coro
import signal
import sys
from coro import tb
import unittest

exc_str_list = []

def exception_notifier():
    exc_str = tb.traceback_string()
    coro.default_exception_notifier()
    exc_str_list.append(exc_str)

class ThreadFailedTest(unittest.TestCase):

    def test_threads_failed(self):
        """Checking for threads that crashed."""
        coro.sleep_relative(0)
        if len(exc_str_list) != 0:
            self.fail('Threads have crashed: %r' % (exc_str_list,))

exit_code = 0

def main():
    global exit_code
    try:
        try:
            # p = unittest.TestProgram(runNow=False)
            p = unittest.TestProgram()
            # This should always be the last test case run.
            p.test.addTest(ThreadFailedTest('test_threads_failed'))
            p.runTests()
        except SystemExit as e:
            exit_code = e.code
    finally:
        coro.set_exit(exit_code)

main_thread = None

def sigterm_handler(unused):
    main_thread.raise_exception(KeyboardInterrupt)

def run_tests():
    global main_thread
    coro.install_signal_handlers = 0
    coro.signal_handler.register (signal.SIGTERM, sigterm_handler)
    coro.signal_handler.register (signal.SIGINT, sigterm_handler)
    coro.set_exception_notifier(exception_notifier)
    main_thread = coro.spawn(main)
    coro.set_print_exit_string(False)
    coro.event_loop()
    sys.exit(exit_code)

########NEW FILE########
__FILENAME__ = test_accept_many
# -*- Mode: Python -*-

"""Unittest for socket.accept_many() call."""

import socket
import sys
import unittest

import coro
import coro_unittest

do_sleeps = False

class TestServer:

    def serve (self, family, address):
        self.s = coro.make_socket (family, socket.SOCK_STREAM)
        self.s.bind ((address, 0))
        self.port = self.s.getsockname()[1]
        self.s.set_reuse_addr()
        self.s.listen (5)
        while True:
            try:
                coro.write_stderr ('accepting...\n')
                conns = self.s.accept_many (5)
                coro.write_stderr ('...after: conns=%r\n' % (conns,))
            except coro.Shutdown:
                break
            for s, addr in conns:
                session = TestSession (s, addr)
                coro.spawn (session.run)

class TestSession:

    def __init__ (self, s, addr):
        self.s = s
        self.addr = addr

    def run (self):
        self.s.send ('howdy!\r\n')
        self.s.close()

class Test (unittest.TestCase):

    def test_accept_many (self):
        global count
        server = TestServer()
        coro.spawn (server.serve, coro.AF.INET, '127.0.0.1')
        coro.yield_slice()

        def connect():
            s = coro.make_socket (coro.AF.INET, socket.SOCK_STREAM)
            coro.with_timeout (5, s.connect, ('127.0.0.1', server.port))
            howdy = coro.with_timeout (5, s.recv, 100)
            self.assertEqual (howdy, 'howdy!\r\n')
            count -= 1
            if count == 0:
                server_thread.raise_exception(coro.Shutdown)

        coro.spawn (connect)
        coro.spawn (connect)
        coro.spawn (connect)
        count = 3


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_aio
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for AIO.

"""

from coro import oserrors
import coro
import coro_unittest
import os
import unittest
import random
import resource

UNAME = os.uname()[0]

class Test(unittest.TestCase):

    FLAG = os.O_RDWR | os.O_CREAT | os.O_TRUNC
    if UNAME == 'Linux':
        FLAG |= os.O_DIRECT

    def tearDown(self):
        if os.path.exists('test_aio_file'):
            os.unlink('test_aio_file')

    def _read_write(self, data):
        n = coro.aio_write(self.fd, data, 0)
        self.assertEqual(n, len(data))
        a = coro.aio_read(self.fd, len(data), 0)
        self.assertEqual(a, data)
        os.ftruncate(self.fd, 0)

    def test_read_write(self):
        """Test read/write."""
        self.fd = os.open('test_lio_file', Test.FLAG)

        # Simple 1-byte test.
        self._read_write('a')

        # Try something really large.
        data = os.urandom(5 * 1024 * 1024)
        self._read_write(data)

        # Test offset read/write.
        orig_data = os.urandom(512 * 1024)
        filesize = len(orig_data)
        coro.aio_write(self.fd, orig_data, 0)
        for x in xrange(100):
            size = random.randint(1, filesize)
            offset = random.randint(0, filesize - size)
            data = coro.aio_read(self.fd, size, offset)
            self.assertEqual(data, orig_data[offset:offset + size])

        os.close(self.fd)

    def test_leak(self):
        """Test map leak."""
        # There was a bug where we were leaking events in the event map.
        self.fd = os.open('test_lio_file', Test.FLAG)

        event_size = len(coro.event_map)

        filesize = 512 * 1024
        orig_data = os.urandom(filesize)
        coro.aio_write(self.fd, orig_data, 0)
        for x in xrange(100):
            size = random.randint(1, filesize)
            offset = random.randint(0, filesize - size)
            data = coro.aio_read(self.fd, size, offset)
            self.assertEqual(data, orig_data[offset:offset + size])

        self.assertEqual(event_size, len(coro.event_map))

        # Try error path.
        os.close(self.fd)

        for x in xrange(100):
            size = random.randint(1, filesize)
            offset = random.randint(0, filesize - size)
            self.assertRaises(OSError, coro.aio_read, self.fd, size, offset)

        self.assertEqual(event_size, len(coro.event_map))

    def test_error(self):
        """Test error return."""
        fd = os.open('test_aio_file', Test.FLAG)
        data = os.urandom(1024 * 1024)
        r = coro.aio_write(fd, data, 0)
        self.assertEqual(r, len(data))
        self.assertEqual(coro.aio_read(fd, len(data), 0), data)

        # Rip away the file descriptor.
        os.close(fd)
        # Verify it fails.
        self.assertRaises(OSError, coro.aio_read, fd, len(data), 0)

        # Try a test that will fail from aio_return.
        # (NOTE: On FreeBSD before 7, this would actually show up as an
        # error immediately from aio_error, but in FreeBSD 7 it now appears to
        # go through the kqueue code path.)
        soft, hard = resource.getrlimit(resource.RLIMIT_FSIZE)
        if soft >= 0:
            fd = os.open('test_aio_file', Test.FLAG)
            self.assertRaises(oserrors.EFBIG, coro.aio_write, fd, data, soft)
            os.close(fd)


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_condition_variable
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for condition variable.

XXX: This needs additional tests for:
- wake_one
- wake_n
- raise_all
"""

import coro
import coro_unittest
import unittest

class Test(unittest.TestCase):

    def test_cond_interrupt_schedule(self):
        """Test interrupt then schedule on condition variable."""
        c = coro.condition_variable()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._cond_block, c))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._cond_block, c)
        coro.yield_slice()
        # Cause an interrupt on these threads.
        for t in threads:
            t.shutdown()
        # Now try to get the non-interrupted thread to run.
        c.wake_all()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def _cond_block(self, c):
        c.wait()
        self._resume_count += 1

    def test_cond_schedule_interrupt(self):
        """Test schedule then interrupt on condition variable."""
        c = coro.condition_variable()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._cond_block, c))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._cond_block, c)
        coro.yield_slice()
        # Schedule all of the threads (except the no interrupt thread).
        c.wake_all()
        # Now interrupt them.
        for t in threads:
            t.shutdown()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_event_queue
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for event queue wrapper."""

import unittest

import coro
import coro_unittest

class Test(unittest.TestCase):

    def setUp(self):
        self.q = coro.event_queue()

    def test_insert(self):
        data = [(3, "3"), (2, "21"), (1, "1"), (2, "22")]
        res  = ["1", "21", "22", "3"]

        for i in data:
            self.q.insert(*i)
        self.assertEquals(len(data), len(self.q))
        for j in res:
            self.assertEquals(self.q.top(), j)
            self.assertEquals(self.q.pop(), j)

    def test_remove(self):
        data = [(3, "3"), (2, "21"), (1, "1"), (2, "22")]

        for i in data:
            self.q.insert(*i)
        self.assertRaises(IndexError, self.q.remove, 1, "2")
        self.assertRaises(IndexError, self.q.remove, 10, "2")
        for i in data:
            self.q.remove(*i)
        self.assertEquals(0, len(self.q))

    def test_empty(self):
        self.assertRaises(IndexError, self.q.top)
        self.assertRaises(IndexError, self.q.pop)
        self.assertRaises(IndexError, self.q.remove, 1, "2")

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_interrupt
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for interrupting coroutines."""

import coro
import coro_unittest
import unittest

class TestException(Exception):
    pass

class Test(unittest.TestCase):

    def test_scheduled_staging_interrupt(self):
        """Test interrupting a thread that is scheduled and in the staging list."""
        t = coro.get_now() + coro.ticks_per_sec * 3
        exception_raised = [False]

        def foo():
            self.assertFalse(exception_raised[0])
            try:
                coro.sleep_absolute(t)
            except TestException:
                exception_raised[0] = True

        c = coro.spawn(foo)
        coro.sleep_absolute(t)
        c.raise_exception(TestException)
        coro.yield_slice()
        self.assertTrue(exception_raised[0])

    def test_scheduled_pending_interrupt(self):
        """Test interrupting a thread that is scheduled and in the pending list."""
        exception_raised = [False]

        def foo():
            self.assertFalse(exception_raised[0])
            try:
                coro._yield()
            except TestException:
                exception_raised[0] = True

        c = coro.spawn(foo)
        coro.yield_slice()
        c.schedule()
        c.raise_exception(TestException)
        coro.yield_slice()
        self.assertTrue(exception_raised[0])

    def test_interrupt_sleeping_coro(self):
        """Test interrupting a thread in a sleep call."""
        exception_raised = [False]

        def foo():
            self.assertFalse(exception_raised[0])
            try:
                coro.sleep_relative(3)
            except TestException:
                exception_raised[0] = True

        c = coro.spawn(foo)
        coro.yield_slice()
        c.raise_exception(TestException)
        coro.yield_slice()
        self.assertTrue(exception_raised[0])

    def test_not_started_interrupt(self):
        """Test interrupting a thread that has not started."""
        def foo():
            pass

        c = coro.spawn(foo)
        self.assertRaises(coro.NotStartedError, c.raise_exception, TestException)
        self.assertTrue(c.scheduled)
        self.assertFalse(c.dead)
        self.assertFalse(c.started)
        c.raise_exception(TestException, cancel_start=True)
        self.assertFalse(c.scheduled)
        self.assertTrue(c.dead)
        self.assertFalse(c.started)
        # Try again and see what happens.
        self.assertRaises(coro.DeadCoroutine, c.raise_exception, TestException, cancel_start=True)
        self.assertFalse(c.scheduled)
        self.assertTrue(c.dead)
        self.assertFalse(c.started)
        # Shutdown shouldn't ever raise an exception.
        c.shutdown()
        self.assertFalse(c.scheduled)
        self.assertTrue(c.dead)
        self.assertFalse(c.started)

    def test_interrupt_sleeping(self):
        """Test interrupting a sleeping thread at the exact same time the sleep
        expires.
        """
        self.assertRaises(coro.TimeoutError,
                          coro.with_timeout, 0, coro.sleep_relative, 0)


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_in_parallel
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for coro.in_parallel."""

import coro
import coro_unittest
import unittest

class Test(unittest.TestCase):

    def test_timeout(self):
        """Test in_parallel with timeout (interrupted)."""
        def sleeper(num):
            coro.sleep_relative(5)
            return num

        self.assertRaises(coro.TimeoutError,
                          coro.with_timeout,
                          2,
                          coro.in_parallel,
                          [(sleeper, (1,)),
                           (sleeper, (2,)),
                              (sleeper, (3,)),
                           ]
                          )

        results = coro.with_timeout(7, coro.in_parallel,
                                    [(sleeper, (4,)),
                                     (sleeper, (5,)),
                                        (sleeper, (6,)),
                                     ]
                                    )
        self.assertEqual(results, [4, 5, 6])

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_isemaphore
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for inverted semaphore."""

import coro
import coro_unittest
import unittest

class Test(unittest.TestCase):

    def test_isem_interrupt_schedule(self):
        """Test interrupt then schedule on inverted semaphore."""
        s = coro.inverted_semaphore()
        s.acquire(1)
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._isem_block, s))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._isem_block, s)
        coro.yield_slice()
        # Cause an interrupt on these threads.
        for t in threads:
            t.shutdown()
        # Now try to get the non-interrupted thread to run.
        s.release(1)
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def _isem_block(self, s):
        s.block_till_zero()
        self._resume_count += 1

    def test_isem_schedule_interrupt(self):
        """Test schedule then interrupt on inverted semaphore."""
        s = coro.inverted_semaphore()
        s.acquire(1)
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._isem_block, s))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._isem_block, s)
        coro.yield_slice()
        # Schedule all of the threads.
        s.release(1)
        # Now interrupt them.
        for t in threads:
            t.shutdown()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_lio
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for LIO.

XXX TODO:
- Test pack_blocks_for_write.
"""

import coro
import coro_unittest
import operator
import os
import tempfile
import unittest
from functools import reduce

class Test(unittest.TestCase):

    def tearDown(self):
        if os.path.exists('test_lio_file'):
            os.unlink('test_lio_file')

    def _read_write(self, data):
        total_bytes = reduce(operator.add, map(len, data))
        n = coro.many_lio_writes(self.fd, 0, data)
        self.assertEqual(n, total_bytes)
        a = coro.many_lio_reads(self.fd, 0, total_bytes)
        self.assertEqual(a, data)
        os.ftruncate(self.fd, 0)

    def test_read_write(self):
        """Test read/write."""
        self.fd = os.open('test_lio_file', os.O_RDWR | os.O_CREAT | os.O_TRUNC)

        # Simple 1-byte test.
        self._read_write(['a'])

        # Test 3 blocks in 1 batch.
        data = [os.urandom(coro.MAX_AIO_SIZE),
                os.urandom(coro.MAX_AIO_SIZE),
                os.urandom(int(coro.MAX_AIO_SIZE * 0.6)),
                ]
        self._read_write(data)

        # Test various numbers of full batches.
        for n in xrange(1, 5):
            data = [os.urandom(coro.MAX_AIO_SIZE) for x in xrange(coro.MAX_LIO * n)]
            self._read_write(data)

        # Test offset read/write.
        data = [os.urandom(coro.MAX_AIO_SIZE) for x in xrange(3)]
        total_bytes = reduce(operator.add, map(len, data))
        coro.many_lio_writes(self.fd, 0, data)
        a = coro.many_lio_reads(self.fd, 512, total_bytes - 512)
        expected_data = ''.join(data)[512:]
        actual_data = ''.join(a)
        self.assertEqual(actual_data, expected_data)

        coro.many_lio_writes(self.fd, 1024, ['hi there'])
        a = coro.many_lio_reads(self.fd, 1024, 8)
        self.assertEqual(a, ['hi there'])


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_local
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for thread-local storage."""

import unittest

import coro
import coro_unittest

class Test(unittest.TestCase):

    shared = None
    t1_cv = None
    t2_cv = None

    def t1(self):
        self.assertFalse(hasattr(self.shared, 'x'))
        self.assertRaises(AttributeError, lambda: self.shared.x)
        # Wait for main thread.
        self.t1_cv.wait()
        self.assertFalse(hasattr(self.shared, 'x'))
        self.assertRaises(AttributeError, lambda: self.shared.x)
        self.shared.x = 2
        self.t1_cv.wait()
        self.assertEqual(self.shared.x, 2)

    def t2(self):
        self.assertFalse(hasattr(self.shared, 'x'))
        self.assertRaises(AttributeError, lambda: self.shared.x)
        # Wait for main thread.
        self.t2_cv.wait()
        self.assertFalse(hasattr(self.shared, 'x'))
        self.assertRaises(AttributeError, lambda: self.shared.x)
        self.shared.x = 3
        self.t2_cv.wait()
        self.assertEqual(self.shared.x, 3)

    def test_local(self):
        """Test thread-local storage."""
        self.t1_cv = coro.condition_variable()
        self.t2_cv = coro.condition_variable()
        self.shared = coro.ThreadLocal()
        self.shared.x = 1
        t1 = coro.spawn(self.t1)
        t2 = coro.spawn(self.t2)

        # Let them run.
        coro.yield_slice()
        self.t1_cv.wake_one()
        # Let t1 run.
        coro.yield_slice()
        self.assertEqual(self.shared.x, 1)

        self.t2_cv.wake_one()
        # Let t2 run.
        coro.yield_slice()
        self.assertEqual(self.shared.x, 1)

        self.t1_cv.wake_one()
        self.t2_cv.wake_one()
        coro.yield_slice()

        t1.join()
        t2.join()
        self.assertEqual(self.shared.x, 1)
        del self.shared

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_lru
# -*- Mode: Python -*-

from coro.lru import lru
import unittest

class Test (unittest.TestCase):

    def test_0 (self):
        d = lru (4)

        for x in range (4):
            d[x] = repr(x)

        # they're in the LRU in the reverse order they were inserted;
        # i.e., '3' is the last one inserted, and is thus at the head.
        self.assertEqual (list(d), [(3, '3'), (2, '2'), (1, '1'), (0, '0')])
        # '1' is at the tail.  Let's refer to it and put it at the head.
        x = d[1]
        self.assertEqual (list(d), [(1, '1'), (3, '3'), (2, '2'), (0, '0')])
        # same thing, with '0'.
        y = d[0]
        self.assertEqual (list(d), [(0, '0'), (1, '1'), (3, '3'), (2, '2')])
        # if we insert a new node now, it should push out '2'.
        d[5] = '5'
        self.assertEqual (list(d), [(5, '5'), (0, '0'), (1, '1'), (3, '3')])

    def test_1 (self):
        # test delete-to-empty
        d = lru (4)
        d[0] = 1
        d[1] = 2
        d[2] = 3
        d[3] = 4
        # print 'del[2]'
        del d[2]
        # print 'del[1]'
        del d[1]
        # print 'del[3]'
        del d[3]
        # print 'del[0]'
        del d[0]
        self.assertEqual (len(d), 0)

    def test_2 (self):
        # test delete-to-empty
        d = lru (4)
        d[0] = 1
        del d[0]
        self.assertEqual (len(d), 0)
        return d

    def test_3 (self):
        # test size limitation
        d = lru (100)
        import random
        for i in range (10000):
            k = random.random()
            v = str(k)
            d[k] = v
        self.assertEqual (len(d), 100)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_mutex
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for mutex."""

import coro
import coro_unittest
import unittest

class Test(unittest.TestCase):

    def test_mutex_interrupt_schedule(self):
        """Test interrupt then schedule on mutex."""
        m = coro.mutex()
        m.lock()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._mutex_block, m))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._mutex_block, m)
        coro.yield_slice()
        # Cause an interrupt on these threads.
        for t in threads:
            t.shutdown()
        # Now try to get the non-interrupted thread to run.
        m.unlock()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def _mutex_block(self, m):
        m.lock()
        self._resume_count += 1
        m.unlock()

    def test_mutex_schedule_interrupt(self):
        """Test schedule then interrupt on mutex."""
        m = coro.mutex()
        m.lock()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._mutex_block, m))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._mutex_block, m)
        coro.yield_slice()
        # Schedule all of the threads.
        m.unlock()
        # Now interrupt them.
        for t in threads:
            t.shutdown()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def test_ownership(self):
        """Test mutex ownership."""
        m = coro.mutex()
        m.lock()
        t = coro.spawn(self._test_ownership, m)
        coro.yield_slice()
        self.assertTrue(m.has_lock())
        self.assertFalse(m.has_lock(t))
        self.assertTrue(m.unlock())
        coro.yield_slice()
        self.assertFalse(m.has_lock())
        self.assertTrue(m.has_lock(t))
        coro.yield_slice()

    def _test_ownership(self, m):
        # This trylock "fails".
        self.assertTrue(m.trylock())
        # This one blocks.
        self.assertTrue(m.lock())
        # Bounce back to other thread.
        coro.yield_slice()

    def test_multiple_lock(self):
        """Test locking mutex multiple times."""
        m = coro.mutex()
        self.assertFalse(m.locked())
        self.assertFalse(m.has_lock())
        self.assertFalse(m.lock())
        self.assertTrue(m.locked())
        self.assertTrue(m.has_lock())
        self.assertFalse(m.lock())
        self.assertTrue(m.locked())
        self.assertTrue(m.has_lock())
        self.assertFalse(m.unlock())
        self.assertTrue(m.locked())
        self.assertTrue(m.has_lock())
        self.assertFalse(m.unlock())
        self.assertFalse(m.locked())
        self.assertFalse(m.has_lock())

    def test_unlock_resume(self):
        """Test that unlock resume."""
        m = coro.mutex()
        coro.spawn(self._test_unlock_resume, m)
        coro.yield_slice()
        # This will block, bounce over to other thread.
        self.assertTrue(m.lock())
        self.assertTrue(m.has_lock())
        self.assertFalse(m.unlock())
        self.assertFalse(m.has_lock())

    def _test_unlock_resume(self, m):
        self.assertFalse(m.lock())
        # Bounce back to other thread.
        coro.yield_slice()
        self.assertTrue(m.unlock())

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_notify_of_close
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for the notify_of_close functionality."""

import coro
import coro_unittest
import os
import unittest
from coro import oserrors

class ForceSend(coro.Interrupted):
    pass

class Test(unittest.TestCase):

    _dummy_thread = None
    _blocker_socket = None
    _echo_thread = None
    _echo_socket = None
    port = 0

    def setUp(self):
        self._start_listener()

    def tearDown(self):
        if self._dummy_thread:
            self._dummy_thread.shutdown()
            self._dummy_thread.join()

    def _echo(self, sock):
        while True:
            try:
                try:
                    data = sock.recv(1024)
                except oserrors.ECONNRESET:
                    return
            except ForceSend:
                sock.send('hi there\n')
            else:
                if not data:
                    return
                sock.send(data)

    def _dummy_listener(self, s):
        while True:
            sock, addr = s.accept()
            self._echo_socket = sock
            self._echo_thread = coro.spawn(self._echo, sock)

    def _start_listener(self):
        s = coro.tcp_sock()
        s.bind(('127.0.0.1', 0))
        s.listen(5)
        addr = s.getsockname()
        self.port = addr[1]
        self._dummy_thread = coro.spawn(self._dummy_listener, s)
        coro.yield_slice()

    def _blocker_thread(self):
        self._blocker_socket = coro.tcp_sock()
        self._blocker_socket.connect(('127.0.0.1', self.port))
        while True:
            coro.print_stderr('reading')
            try:
                self._blocker_socket.read(1024)
            except coro.ClosedError:
                coro.print_stderr('it was closed')
                return

    def test_submitted_shutdown_close(self):
        t = coro.spawn(self._blocker_thread)
        coro.sleep_relative(1)
        t.shutdown()
        self._blocker_socket.close()
        t.join()

    def test_submitted_close_shutdown(self):
        t = coro.spawn(self._blocker_thread)
        coro.sleep_relative(1)
        self._blocker_socket.close()
        t.shutdown()
        t.join()

    def _shutdown_close(self, t):
        t.shutdown()
        self._blocker_socket.close()

    def test_new_shutdown_close(self):
        t = coro.spawn(self._blocker_thread)
        t2 = coro.spawn(self._shutdown_close, t)
        t.join()
        t2.join()

    def _close_shutdown(self, t):
        self._blocker_socket.close()
        t.shutdown()

    def test_new_close_shutdown(self):
        t = coro.spawn(self._blocker_thread)
        t2 = coro.spawn(self._close_shutdown, t)
        t.join()
        t2.join()

    def _fired_blocker(self):
        self.assertRaises(coro.ClosedError, self._fired_blocker_socket.read, 1024)
        return

    def _fired_closer(self, event):
        self._fired_blocker_socket.close()

    _fired_blocker_socket = None

    def test_fired(self):
        s = coro.tcp_sock()
        s.connect(('127.0.0.1', self.port))
        self._fired_blocker_socket = s
        # We need to somehow schedule two threads to both wake up on kevent at
        # the same time in a particular order.  The first one will call close
        # on the socket of the second one.
        f = open('test_fire', 'w')
        coro.set_handler((f.fileno(), coro.EVFILT.VNODE),
                         self._fired_closer,
                         fflags=coro.NOTE.DELETE
                         )

        t2 = coro.spawn(self._fired_blocker)
        # t2.set_max_selfish_acts(1)
        # Yield to allow fired blocker to block.
        coro.yield_slice()
        # Now, cause threads blocked on kevents to get scheduled in a specific
        # order.
        os.unlink('test_fire')
        s.send('force send')
        # Let those threads run.
        coro.yield_slice()

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_poller
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for the poller."""

import coro
import coro_process
import coro_unittest
import os
import unittest

class Test(unittest.TestCase):

    def test_wait_for_interrupt_new(self):
        # Test KEVENT_STATUS_NEW
        proc = coro_process.spawn_job_bg('sleep 30')

        def waiter():
            proc.wait()
        waiter_thread = coro.spawn(waiter)

        def killer():
            waiter_thread.shutdown()
        coro.spawn(killer)
        # Waiter should be scheduled to run.
        # It will add kevent to changelist, then yield.
        # Then killer should be scheduled to run next.  It
        # will reschedule waiter with an exception.
        # Waiter should then wake up and hit the
        # KEVENT_STATUS_NEW cleanup code path.
        coro.yield_slice()
        # If we reach here, good.
        # Even better, after the unittest process exits, if there are no
        # core dumps, things are good.

    def test_wait_for_interrupt_submitted(self):
        # Test KEVENT_STATUS_SUBMITTED
        proc = coro_process.spawn_job_bg('sleep 30')

        def waiter():
            proc.wait()
        waiter_thread = coro.spawn(waiter)
        coro.yield_slice()
        # Waiter has submitted its kevent.
        # Interrupt it before it fires.
        waiter_thread.shutdown()
        coro.yield_slice()
        # If we reach here, good.
        # Even better if the process doesn't crash.

    def test_wait_for_interrupted_fired(self):
        # Test KEVENT_STATUS_FIRED
        # This is tricky, need two coroutines to be scheduled at the same time,
        # the first one a normal one and the second one as a result of a kevent
        # firing (in that specific order).
        read_fd1, write_fd1 = os.pipe()
        read_fd2, write_fd2 = os.pipe()
        try:
            read_sock1 = coro.sock(fd=read_fd1)
            read_sock2 = coro.sock(fd=read_fd2)
            # We're going to have two threads.  Since we can't guarantee
            # which one will show up in kqueue first, we'll just have it
            # interrupt the other one, and verify that only one ran.
            sleeper1_thread = None
            sleeper2_thread = None
            sleeper1_data = []
            sleeper2_data = []

            def sleeper1():
                data = read_sock1.read(10)
                sleeper1_data.append(data)
                sleeper2_thread.shutdown()

            def sleeper2():
                data = read_sock2.read(10)
                sleeper2_data.append(data)
                sleeper1_thread.shutdown()

            sleeper1_thread = coro.spawn(sleeper1)
            sleeper2_thread = coro.spawn(sleeper2)

            coro.yield_slice()
            # Both are waiting, wake them both up.
            os.write(write_fd1, 'sleeper1')
            os.write(write_fd2, 'sleeper2')
            coro.yield_slice()
            # Yield again to ensure the shutdown runs.
            coro.yield_slice()

            self.assertTrue(len(sleeper1_data) == 1 or
                            len(sleeper2_data) == 1)
            self.assertTrue(sleeper1_thread.dead)
            self.assertTrue(sleeper2_thread.dead)
        finally:
            os.close(read_fd1)
            os.close(write_fd1)
            os.close(read_fd2)
            os.close(write_fd2)


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_profile
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for coro profiler."""

import cStringIO
import os
import sys
import unittest

import coro
import coro.profiler
import coro.print_profile
import coro_unittest

# Sam's favorite profile function.
def tak1 (x, y, z):
    coro.yield_slice()
    if y >= x:
        return z
    else:
        return tak1 (
            tak1 (x - 1, y, z),
            tak2 (y - 1, z, x),
            tak2 (z - 1, x, y)
        )

def tak2 (x, y, z):
    coro.yield_slice()
    if y >= x:
        return z
    else:
        return tak2 (
            tak2 (x - 1, y, z),
            tak1 (y - 1, z, x),
            tak1 (z - 1, x, y)
        )

def multi_test():
    t1 = coro.spawn(tak2, 18, 12, 6)
    t2 = coro.spawn(tak2, 18, 12, 6)
    t3 = coro.spawn(tak2, 18, 12, 6)
    t1.join()
    t2.join()
    t3.join()

class Test(unittest.TestCase):

    def test_profile(self):
        prof_filename = 'test_profile.bin'
        # Mainly this just checks that it doesn't raise any exceptions.
        try:
            coro.profiler.go(multi_test, profile_filename=prof_filename)
            output = cStringIO.StringIO()
            real_stdout = sys.stdout
            sys.stdout = output
            try:
                coro.print_profile.main(prof_filename)
            finally:
                sys.stdout = real_stdout
        finally:
            if os.path.exists(prof_filename):
                os.unlink(prof_filename)


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_readv
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for readv socket call."""

import socket
import sys
import unittest

import coro
import coro_unittest

do_sleeps = False

class TestServer:

    block_sends = ()

    def serve(self, family, address):
        self.s = coro.make_socket(family, socket.SOCK_STREAM)
        self.s.bind((address, 0))
        self.port = self.s.getsockname()[1]
        self.s.set_reuse_addr()
        self.s.listen(5)
        while True:
            try:
                s, addr = self.s.accept()
            except coro.Shutdown:
                break
            session = TestSession(s, addr, self.block_sends)
            coro.spawn(session.run)

big_block = '0123456789' * 1024 * 100

class TestSession:

    def __init__(self, s, addr, block_sends):
        self.s = s
        self.addr = addr
        self.block_sends = block_sends

    def run(self):
        for block_size in self.block_sends:
            if do_sleeps:
                coro.sleep_relative(0.1)
            self.s.send(big_block[:block_size])
        self.s.close()


class Test(unittest.TestCase):

    def test_readv(self):
        """Test readv."""
        global do_sleeps

        def testit(family, address, block_sends, block_receives, expected_results):
            s = coro.make_socket(family, socket.SOCK_STREAM)
            server.block_sends = block_sends
            coro.with_timeout(5, s.connect, (address, server.port))
            blocks = coro.with_timeout(5, s.readv, block_receives)
            self.assertEqual(len(blocks), len(expected_results))
            for block, expected_block in zip(blocks, expected_results):
                self.assertEqual(block, expected_block)

        to_test = [(socket.AF_INET, '127.0.0.1')]
        if coro.has_ipv6():
            to_test.append((socket.AF_INET6, '::1'))
        else:
            sys.stderr.write('Warning: No IPv6 support; skipping tests\n')
        for family, address in to_test:
            server = TestServer()
            server_thread = coro.spawn(server.serve, family, address)
            # Give the server a chance to start.
            coro.yield_slice()

            # Different levels of greediness.
            for greediness in (1024, 1):
                coro.current().set_max_selfish_acts(greediness)
                # Do it once without sleeps, once with sleeps.
                for sleep in (False, True):
                    do_sleeps = sleep
                    testit(
                        family,
                        address,
                        (5,
                         19,
                         3,
                         8),
                        (5,
                         19,
                         3,
                         8),
                        ('01234',
                         '0123456789012345678',
                         '012',
                         '01234567'))
                    testit(family, address, (5, 19, 3, 8), (24, 3, 8), ('012340123456789012345678', '012', '01234567'))
                    testit(
                        family,
                        address,
                        (5,
                         19,
                         3,
                         8),
                        (2,
                         3,
                         19,
                         3,
                         8),
                        ('01',
                         '234',
                         '0123456789012345678',
                         '012',
                         '01234567'))
                    testit(
                        family,
                        address,
                        (5,
                         5),
                        (1,
                         1,
                         1,
                         1,
                         1,
                         1,
                         1,
                         1,
                         1,
                         1),
                        ('0',
                         '1',
                         '2',
                         '3',
                         '4',
                         '0',
                         '1',
                         '2',
                         '3',
                         '4'))
                    testit(family, address, (1, 1, 1, 1, 1, 1, 1, 1, 1, 1), (5, 5), ('00000', '00000'))
                    testit(family, address, (10,), (5,), ('01234',))
                    testit(family, address, (5,), (10,), ('01234',))
                    testit(family, address, (), (), ())
                    testit(family, address, (), (5, 2, 8), ())
                    testit(family, address, (5, 9), (5, 10), ('01234', '012345678'))
                    testit(family, address, (5, 9), (5, 5, 3, 7), ('01234', '01234', '567', '8'))
                    testit(family, address, (5, 5), (5, 5, 10), ('01234', '01234'))
                    testit(family, address, (5,), (6,), ('01234',))
                    testit(family, address, (512 * 1024,), (512 * 1024,), (big_block[:512 * 1024],))

            server_thread.raise_exception(coro.Shutdown)


if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_read_stream
# -*- Mode: Python -*-

import sys
import unittest
W = sys.stderr.write

# exhaustively test the new generator-based read_stream

# test-driving a new buffered_stream idea here...
class buffered_stream:

    def __init__ (self, producer):
        self.producer = producer
        self.buffer = ''
        self.pos = 0

    # This version avoids repeatedly slicing/copying self.buffer in
    #   the case where a large buffer holds many 'lines'.  The final
    #   speedup is only ~25%, though, so it may not be worth folding
    #   in.  Note: I've also made a Cython version of this.
    def gen_read_until (self, delim):
        "generate pieces of input up to and including <delim>, then StopIteration"
        ld = len(delim)
        m = 0
        while True:
            if self.pos == len(self.buffer):
                self.buffer = self.producer()
                self.pos = 0
                if not self.buffer:
                    # eof
                    yield ''
                    return
            i = self.pos
            lb = len(self.buffer)
            while i < lb:
                if self.buffer[i] == delim[m]:
                    m += 1
                    if m == ld:
                        yield self.buffer[self.pos:i + 1]
                        self.pos = i + 1
                        return
                else:
                    m = 0
                i += 1
            block, self.buffer = self.buffer[self.pos:], ''
            self.pos = 0
            yield block

    def read_until (self, delim, join=True):
        "read until <delim>.  return a list of parts unless <join> is True"
        result = (x for x in self.gen_read_until (delim))
        if join:
            return ''.join (result)
        else:
            return result

    def flush (self):
        "flush this stream's buffer"
        result, self.buffer = self.buffer, ''
        self.pos = 0
        return result

    def read_line (self, delim='\r\n'):
        "read a CRLF-delimited line from this stream"
        return self.read_until (delim)

import string
from random import randrange as R

def make_delim (n):
    return string.letters[:n]

# simulate a series of 'lines'
def make_line_data (delim_size, count=10000):
    delim = make_delim (delim_size)
    r = []
    for i in range (count):
        r.append ('0' * R (20, 150))
        r.append (delim)
    return ''.join (r)

# simulate an smtp stream
# XXX write tests against this!
def make_smtp_data (count=1000):
    delim0 = '\r\n'
    delim1 = '\r\n.\r\n'
    r = []
    # count mail messages
    for i in range (count):
        # 5 to 100 header lines/commands
        for i in R (5, 100):
            # each 15 to 200 chars long
            r.append ('0' * R (15, 200))
            r.append (delim0)
        # followed by message content of 300-15k
        r.append ('1' * R (300, 15000))
        r.append (delim1)
    return ''.join (r)

def str_prod (s, lo, hi=None):
    # string producer, make chunks of size in the range lo, hi (or just <lo>)
    i = 0
    ls = len(s)
    while i < ls:
        if hi is None:
            size = lo
        else:
            size = R (lo, hi)
        yield s[i:i + size]
        i += size

import re
data_re = re.compile ('^0+$')

def read_lines (g, delim):
    r = []
    ld = len(delim)
    while True:
        line = g.read_line (delim)
        if not line:
            break
        else:
            assert line[-ld:] == delim
            assert data_re.match (line[:-ld])
            r.append (line[:-ld])
    # emulate str.split()
    r.append ('')
    return r

class Test (unittest.TestCase):

    def t_line_0 (self, lines0, delim, lo, hi):
        s = buffered_stream (str_prod (lines0, lo, hi).next)
        lines1 = read_lines (s, delim)
        lines2 = lines0.split (delim)
        self.assertEqual (lines1, lines2)

    def t_line_1 (self, delim, lines0):
        t0 = self.t_line_0
        print ' fixed size, small'
        for i in range (1, 20):
            t0 (lines0, delim, i, i + 1)
        print ' random size, small'
        for i in range (1, 20):
            t0 (lines0, delim, i, i + 20)
        print ' 1000 byte buffer'
        t0 (lines0, delim, 1000, None)
        print ' 10000 byte buffer'
        t0 (lines0, delim, 10000, None)

    def test_4 (self):
        print 'delim_size == 4'
        delim = make_delim (4)
        lines0 = make_line_data (4)
        self.t_line_1 (delim, lines0)

    def test_1 (self):
        print 'delim_size == 1'
        delim = make_delim (1)
        lines0 = make_line_data (1)
        self.t_line_1 (delim, lines0)

    def test_20 (self):
        print 'delim_size == 20'
        delim = make_delim (20)
        lines0 = make_line_data (20)
        self.t_line_1 (delim, lines0)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_rw_lock
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for read-write lock."""

import coro
import coro_unittest
import unittest

class Test(unittest.TestCase):

    def test_write_block_interrupt_schedule(self):
        """Test write block interrupt then schedule on rw_lock."""
        lock = coro.rw_lock()
        lock.read_lock()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._write_block, lock))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._write_block, lock)
        coro.yield_slice()
        # Cause an interrupt on these threads.
        for t in threads:
            t.shutdown()
        # Now try to get the non-interrupted thread to run.
        lock.read_unlock()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def _write_block(self, lock):
        lock.write_lock()
        self._resume_count += 1
        lock.write_unlock()

    def _read_block(self, lock):
        lock.read_lock()
        self._resume_count += 1
        lock.read_unlock()

    def test_write_block_schedule_interrupt(self):
        """Test write block schedule then interrupt on rw_lock."""
        lock = coro.rw_lock()
        lock.read_lock()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._write_block, lock))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._write_block, lock)
        coro.yield_slice()
        # Schedule all of the threads.
        lock.read_unlock()
        # Now interrupt them.
        for t in threads:
            t.shutdown()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def test_read_block_interrupt_schedule(self):
        """Test read block interrupt then schedule on rw_lock."""
        lock = coro.rw_lock()
        lock.write_lock()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._read_block, lock))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._read_block, lock)
        coro.yield_slice()
        # Cause an interrupt on these threads.
        for t in threads:
            t.shutdown()
        # Now try to get the non-interrupted thread to run.
        lock.write_unlock()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def test_read_block_schedule_interrupt(self):
        """Test read block schedule then interrupt on rw_lock."""
        lock = coro.rw_lock()
        lock.write_lock()
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._read_block, lock))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._read_block, lock)
        coro.yield_slice()
        # Schedule all of the threads.
        lock.write_unlock()
        # Now interrupt them.
        for t in threads:
            t.shutdown()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_semaphore
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for semaphore."""

import coro
import coro_unittest
import unittest

class Test(unittest.TestCase):

    def test_sem_interrupt_schedule(self):
        """Test interrupt then schedule on semaphore."""
        s = coro.semaphore(1)
        s.acquire(1)
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._sem_block, s))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._sem_block, s)
        coro.yield_slice()
        # Cause an interrupt on these threads.
        for t in threads:
            t.shutdown()
        # Now try to get the non-interrupted thread to run.
        s.release(1)
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def _sem_block(self, s, count=1):
        s.acquire(count)
        self._resume_count += 1
        s.release(count)

    def test_sem_schedule_interrupt(self):
        """Test schedule then interrupt on semaphore."""
        s = coro.semaphore(5)
        s.acquire(5)
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._sem_block, s))
        # Spawn a thread that we will not interrupt.
        no_interrupt_thread = coro.spawn(self._sem_block, s)
        coro.yield_slice()
        # Schedule all of the threads (except the no interrupt thread).
        s.release(5)
        # Now interrupt them.
        for t in threads:
            t.shutdown()
        coro.yield_slice()
        # Verify that it ran.
        self.assertEqual(self._resume_count, 1)

    def test_sem_buildup(self):
        """Test semaphore waiting buildup."""
        # There was a bad bug once where the _waiting list got really big,
        # and a lot of interrupts was causing a lot of thrashing of the
        # _waiting list.
        s = coro.semaphore(1)
        s.acquire(1)
        self._resume_count = 0
        threads = []
        # Spawn some threads that will block and be interrupted.
        for unused in xrange(5):
            threads.append(coro.spawn(self._sem_block, s))
        coro.yield_slice()
        self.assertEqual(len(s._waiting), 5)
        # Now interrupt them.
        for t in threads:
            t.shutdown()
        self.assertEqual(len(s._waiting), 5)
        # Now try to release.
        s.release(1)
        self.assertEqual(len(s._waiting), 0)
        coro.yield_slice()
        self.assertEqual(self._resume_count, 0)

    def test_exception_stomp(self):
        # Pyrex had a bug where if it raised an exception it would stomp on
        # the "current" exception on the Python stack.
        s = coro.semaphore(0)

        def blocker():
            s.acquire(1)
        t1 = coro.spawn(blocker)
        coro.yield_slice()
        # Mark the thread as scheduled.
        t1.shutdown()

        def raiser():
            try:
                raise ValueError(3)
            except ValueError:
                # This will attempt to schedule t1 which will result in a
                # ScheduleError getting raised and caught within the release
                # code.
                s.release(1)
                # This should re-raise ValueError.  But with the bug, it was
                # re-raising ScheduleError.
                raise
        self.assertRaises(ValueError, raiser)

    def test_exception_stomp2(self):
        # Pyrex had a bug where calling a function within an exception handler,
        # and that function raised and caught an exception, it would stomp on
        # the current exception, so re-raising would raise the wrong exception.
        s = coro.semaphore(0)

        def blocker():
            s.acquire(1)
        t1 = coro.spawn(blocker)
        t2 = coro.spawn(blocker)
        coro.yield_slice()
        # Make t1 scheduled.
        s.release(1)
        # Interrupt t1, it will try to schedule t2, but that will fail.
        t1.shutdown()
        # Cause t2 to be scheduled.
        t2.shutdown()
        coro.yield_slice()

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_socket
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Basic unittests for sockets including IPv6."""

import socket
import sys
import unittest

import coro
import coro_unittest

class TestServer:

    accepted_from = None

    def serve(self, address, family):
        self.s = coro.make_socket(family, socket.SOCK_STREAM)
        self.s.bind((address, 0))
        self.bound_ip, self.port = self.s.getsockname()
        self.s.set_reuse_addr()
        self.s.listen(5)

        s, addr = self.s.accept()
        session = TestSession(s, addr)
        coro.spawn(session.run)
        self.accepted_from = s.getsockname()[0]

class TestSession:

    def __init__(self, s, addr):
        self.s = s
        self.addr = addr

    def run(self):
        current_buffer = ''
        while True:
            block = self.s.recv(1024)
            if not block:
                break
            current_buffer = current_buffer + block

        self.s.send(current_buffer)
        self.s.close()

class Test(unittest.TestCase):
    """Test aims to make sure that the changes to parse_address and
    unparse_address in socket.pyx didn't break anything. The tests are
    far from exhaustive but they are a nice smoke test."""

    test_string = 'Hello world'

    def do_work(self, connected_sock):
        connected_sock.send(self.test_string)
        connected_sock.shutdown(socket.SHUT_WR)
        reply = connected_sock.recv(len(self.test_string) + 1)
        self.assertEqual(reply, self.test_string)

    def _test(self, address, family):
        server = TestServer()
        server_thread = coro.spawn(server.serve, address, family)
        # Give the server a chance to start.
        coro.yield_slice()
        self.assertEqual(server.bound_ip, address)

        sock = coro.make_socket(family, socket.SOCK_STREAM)
        sock.connect((address, server.port))

        coro.yield_slice()

        # Double checking that everyone thinks they're connected
        # to the same peer
        self.assertEqual(server.accepted_from, address)
        self.assertEqual(sock.getpeername()[0], server.accepted_from)

        self.do_work(sock)

    def test_v4(self):
        self._test('127.0.0.1', socket.AF_INET)

    def test_v6(self):
        if coro.has_ipv6():
            self._test('::1', socket.AF_INET6)
        else:
            sys.stderr.write('Warning: No IPv6 support; skipping tests\n')

    def test_invalid_ip(self):
        sock = coro.make_socket(socket.AF_INET, socket.SOCK_STREAM)
        self.assertRaises(ValueError, sock.connect, ('123', 80),)
        self.assertRaises(TypeError, sock.connect, (123, 80),)

    def test_bind_empty_ip_v4(self):
        sock = coro.make_socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("", 5010))
        self.assertEquals(sock.domain, socket.AF_INET)

    def test_bind_empty_ip_v6(self):
        sock = coro.make_socket(socket.AF_INET6, socket.SOCK_STREAM)
        sock.bind(("", 5010))
        self.assertEquals(sock.domain, socket.AF_INET6)

    def test_bind_wrong_af_4to6(self):
        sock = coro.make_socket(socket.AF_INET, socket.SOCK_STREAM)
        self.assertEquals(sock.domain, socket.AF_INET)
        self.assertRaises(ValueError, sock.bind, ("2001::1", 5010),)

    def test_bind_wrong_af_6to4(self):
        sock = coro.make_socket(socket.AF_INET6, socket.SOCK_STREAM)
        self.assertEquals(sock.domain, socket.AF_INET6)
        self.assertRaises(ValueError, sock.bind, ("1.1.1.1", 5010),)

    def test_bind_af_unspec(self):
        sock = coro.make_socket(socket.AF_INET6, socket.SOCK_STREAM)
        sock.domain = socket.AF_UNSPEC
        self.assertRaises(ValueError, sock.bind, ("1.1.1.1", 5010),)
        self.assertRaises(ValueError, sock.bind, ("", 5010),)

    def test_getsockname_v4(self):
        sock = coro.make_socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("", 5555))
        sn = sock.getsockname()
        self.assertEquals(sn[0], "0.0.0.0")
        self.assertEquals(sn[1], 5555)

    def test_getsockname_v6(self):
        sock = coro.make_socket(socket.AF_INET6, socket.SOCK_STREAM)
        sock.bind(("", 5555))
        sn = sock.getsockname()
        self.assertEquals(sn[0], "::")
        self.assertEquals(sn[1], 5555)

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
__FILENAME__ = test_tsc_time
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for the tsc_time module."""

import struct
# import sysctl
import time
import unittest
from coro.clocks import tsc_time

class Test(unittest.TestCase):

    # Stub test until we have a working sysctl
    # def test_ticks_per_sec(self):
    #    freq = struct.unpack('I', sysctl.sysctl('machdep.tsc_freq'))[0]
    #    self.assertEqual(freq,
    #                     tsc_time.ticks_per_sec
    #                    )
    #    self.assertEqual(freq / 1000000,
    #                     tsc_time.ticks_per_usec
    #                    )

    def _assert_close(self, a, b, diff):
        low = a - diff
        high = a + diff

        self.assert_(low <= b <= high, '%r %r ~ %r' % (a, b, diff))

    def test_raw_conversion(self):
        now_t = tsc_time.now_tsc().tsc
        now_up = tsc_time.get_kernel_usec()
        now_p = now_up / 1000000
        now_fp = float(now_up) / 1000000

        # Conversion looses some accuracy.
        ticks_close = (tsc_time.ticks_per_sec / 10.0)
        p_close = 0
        up_close = 100000
        fp_close = 0.1

        self._assert_close(now_t,
                           tsc_time.usec_to_ticks(now_up),
                           ticks_close
                           )
        self._assert_close(now_up,
                           tsc_time.ticks_to_usec(now_t),
                           up_close
                           )
        self._assert_close(now_t,
                           tsc_time.usec_to_ticks_safe(now_up),
                           ticks_close
                           )
        self._assert_close(now_up,
                           tsc_time.ticks_to_usec_safe(now_t),
                           up_close
                           )
        self._assert_close(now_t,
                           tsc_time.sec_to_ticks(now_p),
                           tsc_time.ticks_per_sec
                           )
        self._assert_close(now_p,
                           tsc_time.ticks_to_sec(now_t),
                           p_close
                           )
        self._assert_close(now_fp,
                           tsc_time.ticks_to_fsec(now_t),
                           fp_close
                           )
        self._assert_close(now_t,
                           tsc_time.fsec_to_ticks(now_fp),
                           ticks_close
                           )

    def test_time_methods(self):
        t = tsc_time.now_tsc()
        now_up = tsc_time.get_kernel_usec()
        now_fp = float(now_up) / 1000000

        self.assertEqual(time.ctime(now_fp), t.ctime())
        self.assertEqual(time.localtime(now_fp), t.localtime())
        self.assertEqual(time.gmtime(now_fp), t.gmtime())
        self.assertEqual(time.strftime('%a %b %d %H:%M:%S %Y', time.localtime(now_fp)),
                         t.mkstr_local('%a %b %d %H:%M:%S %Y'))
        self.assertEqual(time.strftime('%a %b %d %H:%M:%S %Y', time.gmtime(now_fp)),
                         t.mkstr_utc('%a %b %d %H:%M:%S %Y'))

    def test_comparison(self):
        t = tsc_time.now_tsc()
        t2 = tsc_time.now_tsc()

        self.assert_(t2 > t)
        self.assert_(t < t2)
        self.assert_(t.tsc + 1 > t)
        self.assert_(t.tsc - 1 < t)
        self.assert_(t.tsc + 1 > t)
        self.assert_(t.tsc - 1 < t)
        # Floating point uses larger numbers because of a loss in precision
        # when converting to floating point, ex:
        # >>> float(28536304964998994L)
        # 28536304964998992.0
        self.assert_(t.tsc + 1000.0 > t)
        self.assert_(t.tsc - 1000.0 < t)

        t = tsc_time.now_posix_sec()
        time.sleep(2)
        t2 = tsc_time.now_posix_sec()

        self.assert_(t2 > t)
        self.assert_(t < t2)
        self.assert_(int(t) + 1 > t)
        self.assert_(int(t) - 1 < t)
        self.assert_(int(t) + 1 > t)
        self.assert_(int(t) - 1 < t)
        self.assert_(int(t) + 1.0 > t)
        self.assert_(int(t) - 1.0 < t)

        t = tsc_time.now_posix_usec()
        time.sleep(0.1)
        t2 = tsc_time.now_posix_usec()

        self.assert_(t2 > t)
        self.assert_(t < t2)
        self.assert_(int(t) + 1 > t)
        self.assert_(int(t) - 1 < t)
        self.assert_(int(t) + 1 > t)
        self.assert_(int(t) - 1 < t)
        self.assert_(int(t) + 1.0 > t)
        self.assert_(int(t) - 1.0 < t)

        t = tsc_time.now_posix_fsec()
        time.sleep(0.1)
        t2 = tsc_time.now_posix_fsec()

        self.assert_(t2 > t)
        self.assert_(t < t2)
        self.assert_(int(t) + 1 > t)
        self.assert_(int(t) - 1 < t)
        self.assert_(int(t) + 1 > t)
        self.assert_(int(t) - 1 < t)
        self.assert_(int(t) + 1.0 > t)
        self.assert_(int(t) - 1.0 < t)

    def test_math(self):
        t = tsc_time.now_tsc()

        self.assertEqual(t + 1, t.tsc + 1)
        self.assertEqual(t - 1, t.tsc - 1)
        self.assertEqual(t + 1, t.tsc + 1)
        self.assertEqual(t - 1, t.tsc - 1)
        # Removing floating point comparison because large floating point
        # numbers loose precision, ex:
        # >>> float(28536304964998994L)
        # 28536304964998992.0
        # self.assertEqual(t + 1.0, t.tsc + 1)
        # self.assertEqual(t - 1.0, t.tsc - 1)
        self.assertRaises(TypeError, lambda: t + 'hi')
        self.assertRaises(TypeError, lambda: t - 'hi')

        t = tsc_time.now_posix_sec()

        self.assertEqual(t + 1, t.as_posix_sec() + 1)
        self.assertEqual(t - 1, t.as_posix_sec() - 1)
        self.assertEqual(t + 1, t.as_posix_sec() + 1)
        self.assertEqual(t - 1, t.as_posix_sec() - 1)
        self.assertEqual(t + 1.0, t.as_posix_sec() + 1)
        self.assertEqual(t - 1.0, t.as_posix_sec() - 1)
        self.assertRaises(TypeError, lambda: t + 'hi')
        self.assertRaises(TypeError, lambda: t - 'hi')

        t = tsc_time.now_posix_usec()

        self.assertEqual(t + 1, t.as_posix_usec() + 1)
        self.assertEqual(t - 1, t.as_posix_usec() - 1)
        self.assertEqual(t + 1, t.as_posix_usec() + 1)
        self.assertEqual(t - 1, t.as_posix_usec() - 1)
        self.assertEqual(t + 1.0, t.as_posix_usec() + 1)
        self.assertEqual(t - 1.0, t.as_posix_usec() - 1)
        self.assertRaises(TypeError, lambda: t + 'hi')
        self.assertRaises(TypeError, lambda: t - 'hi')

        t = tsc_time.now_posix_fsec()

        # We lose some precision with double conversions done in C versus
        # Python's conversions.
        self._assert_close(t + 1, t.as_posix_fsec() + 1, 0.001)
        self._assert_close(t - 1, t.as_posix_fsec() - 1, 0.001)
        self._assert_close(t + 1, t.as_posix_fsec() + 1, 0.001)
        self._assert_close(t - 1, t.as_posix_fsec() - 1, 0.001)
        self._assert_close(t + 1.0, t.as_posix_fsec() + 1, 0.001)
        self._assert_close(t - 1.0, t.as_posix_fsec() - 1, 0.001)
        self.assertRaises(TypeError, lambda: t + 'hi')
        self.assertRaises(TypeError, lambda: t - 'hi')

    def test_types(self):
        t = tsc_time.now_tsc()

        self.assertEqual(int(t), t.tsc)
        self.assertEqual(long(t), t.tsc)
        self.assertEqual(float(t), float(t.tsc))

        t = tsc_time.now_posix_sec()

        self.assertEqual(int(t), t.as_posix_sec())
        self.assertEqual(long(t), t.as_posix_sec())
        self.assertEqual(float(t), float(t.as_posix_sec()))

        t = tsc_time.now_posix_usec()

        self.assertEqual(int(t), t.as_posix_usec())
        self.assertEqual(long(t), t.as_posix_usec())
        self.assertEqual(float(t), float(t.as_posix_usec()))

        t = tsc_time.now_posix_fsec()

        self.assertEqual(int(t), t.as_posix_sec())
        self.assertEqual(long(t), t.as_posix_sec())
        self.assertEqual(float(t), t.as_posix_fsec())

    def test_mktime(self):
        for mktime_type in (tsc_time.mktime_tsc,
                            tsc_time.mktime_posix_sec,
                            tsc_time.mktime_posix_usec,
                            tsc_time.mktime_posix_fsec):
            tt = time.localtime()
            t = mktime_type(tt)
            self.assertEqual(t.localtime(), tt)

    def test_from(self):
        now_tsc = tsc_time.now_tsc()

        self.assertEqual(now_tsc.tsc,
                         tsc_time.TSC_from_ticks(now_tsc.tsc).tsc)
        self.assertEqual(tsc_time.sec_to_ticks(now_tsc.as_posix_sec()),
                         tsc_time.TSC_from_posix_sec(now_tsc.as_posix_sec()).tsc)
        self.assertEqual(tsc_time.usec_to_ticks(now_tsc.as_posix_usec()),
                         tsc_time.TSC_from_posix_usec(now_tsc.as_posix_usec()).tsc)
        self.assertEqual(tsc_time.fsec_to_ticks(now_tsc.as_posix_fsec()),
                         tsc_time.TSC_from_posix_fsec(now_tsc.as_posix_fsec()).tsc)

        self.assertEqual(now_tsc.tsc,
                         tsc_time.Posix_from_ticks(now_tsc.tsc).tsc)
        self.assertEqual(tsc_time.sec_to_ticks(now_tsc.as_posix_sec()),
                         tsc_time.Posix_from_posix_sec(now_tsc.as_posix_sec()).tsc)
        self.assertEqual(tsc_time.usec_to_ticks(now_tsc.as_posix_usec()),
                         tsc_time.Posix_from_posix_usec(now_tsc.as_posix_usec()).tsc)
        self.assertEqual(tsc_time.fsec_to_ticks(now_tsc.as_posix_fsec()),
                         tsc_time.Posix_from_posix_fsec(now_tsc.as_posix_fsec()).tsc)

        self.assertEqual(now_tsc.tsc,
                         tsc_time.uPosix_from_ticks(now_tsc.tsc).tsc)
        self.assertEqual(tsc_time.sec_to_ticks(now_tsc.as_posix_sec()),
                         tsc_time.uPosix_from_posix_sec(now_tsc.as_posix_sec()).tsc)
        self.assertEqual(tsc_time.usec_to_ticks(now_tsc.as_posix_usec()),
                         tsc_time.uPosix_from_posix_usec(now_tsc.as_posix_usec()).tsc)
        self.assertEqual(tsc_time.fsec_to_ticks(now_tsc.as_posix_fsec()),
                         tsc_time.uPosix_from_posix_fsec(now_tsc.as_posix_fsec()).tsc)

        self.assertEqual(now_tsc.tsc,
                         tsc_time.fPosix_from_ticks(now_tsc.tsc).tsc)
        self.assertEqual(tsc_time.sec_to_ticks(now_tsc.as_posix_sec()),
                         tsc_time.fPosix_from_posix_sec(now_tsc.as_posix_sec()).tsc)
        self.assertEqual(tsc_time.usec_to_ticks(now_tsc.as_posix_usec()),
                         tsc_time.fPosix_from_posix_usec(now_tsc.as_posix_usec()).tsc)
        self.assertEqual(tsc_time.fsec_to_ticks(now_tsc.as_posix_fsec()),
                         tsc_time.fPosix_from_posix_fsec(now_tsc.as_posix_fsec()).tsc)

    def test_negative_time(self):
        now_tsc = tsc_time.now_tsc()

        diff = 10 * 60 * tsc_time.ticks_per_sec
        ago = now_tsc - diff

        self._assert_close(ago.as_posix_sec(),
                           now_tsc.as_posix_sec() - 10 * 60,
                           1
                           )
        self._assert_close(ago.as_posix_usec(),
                           now_tsc.as_posix_usec() - 10 * 60 * 1000000,
                           1000000
                           )
        self._assert_close(ago.as_posix_fsec(),
                           now_tsc.as_posix_fsec() - 10 * 60,
                           0.3
                           )

        # Microseconds in 1 year.
        diff = long(365 * 24 * 60 * 60 * 1000000)
        now_usec = tsc_time.now_posix_usec()
        ago = now_usec - diff

        self._assert_close(now_usec.as_posix_usec() - diff,
                           ago.as_posix_usec(),
                           10
                           )

        year_ago = now_usec.as_posix_usec() - diff
        year_ago_tsc = tsc_time.TSC_from_posix_usec(year_ago)

        self._assert_close(year_ago_tsc.as_posix_usec(),
                           year_ago,
                           10
                           )


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_writev
# Copyright (c) 2002-2011 IronPort Systems and Cisco Systems
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Unittests for writev socket call."""

import socket
import sys
import unittest

import coro
import coro_unittest

current_buffer = ''
finished = None

send_buffer_size = 32768
recv_buffer_size = 32768

class TestServer:

    def serve(self, family, address):
        self.s = coro.make_socket(family, socket.SOCK_STREAM)
        self.s.setsockopt (socket.SOL_SOCKET, socket.SO_RCVBUF, recv_buffer_size)
        self.s.bind((address, 0))
        self.port = self.s.getsockname()[1]
        self.s.set_reuse_addr()
        self.s.listen(5)
        while True:
            try:
                s, addr = self.s.accept()
            except coro.Shutdown:
                break
            session = TestSession(s, addr)
            coro.spawn(session.run)

class TestSession:

    def __init__(self, s, addr):
        self.s = s
        self.addr = addr

    def run(self):
        global current_buffer, finished
        current_buffer = ''
        received = 0
        while True:
            block = self.s.recv(1024)
            if not block:
                break
            current_buffer = current_buffer + block
        self.s.close()
        finished.wake_all()
        finished = None

class Test(unittest.TestCase):

    def test_writev(self):
        """Test writev."""
        global send_buffer_size, recv_buffer_size

        big_block = '0123456789' * 1024 * 100

        def testit(family, address, block_sends, expected_buffer_result, expected_return):
            global finished
            finished = coro.condition_variable()
            s = coro.make_socket(family, socket.SOCK_STREAM)
            s.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, send_buffer_size)
            coro.with_timeout(5, s.connect, (address, server.port))
            blocks = [big_block[:size] for size in block_sends]
            rc = coro.with_timeout(5, s.writev, blocks)
            s.close()
            if finished is not None:
                coro.with_timeout(5, finished.wait)
            self.assertEqual(expected_buffer_result, current_buffer)
            self.assertEqual(expected_return, rc)

        # Setting the send/recv buffer size to 1 causes writev to indicate it
        # was only able to send 1 byte before blocking.  This allows us to test
        # the "partial" buffer sent code path.
        to_test = [(socket.AF_INET, '127.0.0.1')]
        if coro.has_ipv6():
            to_test.append((socket.AF_INET6, '::1'))
        else:
            sys.stderr.write('Warning: No IPv6 support; skipping tests\n')
        for family, address in to_test:
            for bufsize in (32768, 1):
                send_buffer_size = bufsize
                recv_buffer_size = bufsize

                server = TestServer()
                server_thread = coro.spawn(server.serve, family, address)
                # Give the server a chance to start.
                coro.yield_slice()

                for greediness in (1024, 1):
                    coro.current().set_max_selfish_acts(greediness)
                    testit(family, address, (), '', 0)
                    testit(family, address, (5, 3, 7, 8), '01234012012345601234567', 23)
                    # bufsize==1 is too slow and not necessary
                    if bufsize != 1:
                        testit(family, address, (512 * 1024,), big_block[:512 * 1024], 512 * 1024)

                server_thread.raise_exception(coro.Shutdown)

if __name__ == '__main__':
    coro_unittest.run_tests()

########NEW FILE########
