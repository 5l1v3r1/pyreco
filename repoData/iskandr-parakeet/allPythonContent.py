__FILENAME__ = 2d_convolution
import parakeet
import numpy as np 
from timer import compare_perf




def clamp(i, offset, maxval):
    j = max(0, i + offset)
    return min(j, maxval)


def reflect(pos, offset, bound):
    idx = pos+offset
    return min(2*(bound-1)-idx,max(idx,-idx))
 

def conv(x, weights, mode=clamp):
    sx = x.shape
    sw = weights.shape
    result = np.zeros_like(x)
    for i in xrange(sx[0]):
        for j in xrange(sx[1]):
            for ii in xrange(sw[0]):
                for jj in xrange(sw[1]):
                    idx = mode(i,ii-sw[0]/2,sw[0]), mode(j,jj-sw[0]/2,sw[0])
                    result[i,j] += x[idx] * weights[ii,jj] 
    return result


xsize = (300,300)
x = np.random.randn(*xsize)
wsize = (5,5)
w = np.random.randn(*wsize)

compare_perf(conv, [x,w])

########NEW FILE########
__FILENAME__ = allpairs_distances
import numpy as np

def sqr_dists(X,Y):
  return np.array([[np.sum( (x-y) ** 2) for x in X] for y in Y])

def sqr_dists_loops(X,Y):
  m,n = X.shape[0], Y.shape[0]
  D = np.zeros((m,n), dtype=X.dtype)
  for i in xrange(m):
    for j in xrange(n):
      D[i,j] = np.sum( (X[i, :] -Y[j, :]) ** 2)
  return D

ndims = 10
nsamples = 10**4
nclusters = 50
X = np.random.randn(nsamples, ndims)
Y = np.random.randn(nclusters, ndims)




from timer import timer 

print 
print "Computing distances between %d and %d %s vectors of length %d" % \
  (nsamples, nclusters, X.dtype, ndims)

# 
# Parakeet 
# 

import parakeet

parakeet_dists = parakeet.jit(sqr_dists)

with timer('Parakeet (comprehensions) #1'):
  parakeet_dists(X,Y)

with timer('Parakeet (comprehensions) #2'):
  parakeet_dists(X,Y)

parakeet_dists_loops = parakeet.jit(sqr_dists_loops)

with timer('Parakeet (loops) #1'):
  parakeet_dists_loops(X,Y)

with timer('Parakeet (loops) #2'):
  parakeet_dists_loops(X,Y)



#
# Pure Python 
# 
from timer import timer

with timer('Python (comprehensions)'):
  sqr_dists(X,Y)

with timer('Python (loops)'):
  sqr_dists_loops(X,Y)



#
# Numba 
#

import numba

#
# Numba's @autojit just like Parakeet's @jit
#
numba_dists = numba.autojit(sqr_dists)

with timer('Numba (comprehensions) #1'):
  numba_dists(X,Y)

with timer('Numba (comprehensions) #2'):
  numba_dists(X,Y)

numba_dists_loops = numba.autojit(sqr_dists_loops)

with timer('Numba (loops) #1'):
  numba_dists_loops(X,Y)

with timer('Numba (loops) #2'):
  numba_dists_loops(X,Y)


########NEW FILE########
__FILENAME__ = arc_distance
from math import *

def arc_distance_python_nested_for_loops(a, b):
    """
    Calculates the pairwise arc distance between all points in vector a and b.
    """
    a_nrows = a.shape[0]
    b_nrows = b.shape[0]

    distance_matrix = np.zeros((a_nrows, b_nrows))

    for i in range(a_nrows):
        theta_1 = a[i, 0]
        phi_1 = a[i, 1]
        for j in range(b_nrows):
            theta_2 = b[j, 0]
            phi_2 = b[j, 1]
            temp = (pow(sin((theta_2 - theta_1) / 2), 2)
                    +
                    cos(theta_1) * cos(theta_2)
                    * pow(sin((phi_2 - phi_1) / 2), 2))
            distance_matrix[i, j] = 2 * (atan2(sqrt(temp), sqrt(1 - temp)))
    return distance_matrix


def arc_distance_numpy_tile(a, b):
    """
    Calculates the pairwise arc distance between all points in vector a and b.
    """
    theta_1 = np.tile(a[:, 0], (b.shape[0], 1)).T
    phi_1 = np.tile(a[:, 1], (b.shape[0], 1)).T

    theta_2 = np.tile(b[:, 0], (a.shape[0], 1))
    phi_2 = np.tile(b[:, 1], (a.shape[0], 1))

    temp = (np.sin((theta_2 - theta_1) / 2)**2
            +
            np.cos(theta_1) * np.cos(theta_2)
            * np.sin((phi_2 - phi_1) / 2)**2)
    distance_matrix = 2 * (np.arctan2(np.sqrt(temp), np.sqrt(1 - temp)))

    return distance_matrix


def arc_distance_numpy_broadcast(a, b):
    """
    Calculates the pairwise arc distance between all points in vector a and b.
    """
    theta_1 = a[:, 0][:, None]
    theta_2 = b[:, 0][None, :]
    phi_1 = a[:, 1][:, None]
    phi_2 = b[:, 1][None, :]

    temp = (np.sin((theta_2 - theta_1) / 2)**2
            +
            np.cos(theta_1) * np.cos(theta_2)
            * np.sin((phi_2 - phi_1) / 2)**2)
    distance_matrix = 2 * (np.arctan2(np.sqrt(temp), np.sqrt(1 - temp)))
    return distance_matrix

from compare_perf import compare_perf 

n = 1000 
import numpy as np
a = np.random.rand(n, 2)
b = np.random.rand(n, 2)

compare_perf(arc_distance_python_nested_for_loops, [a,b])
compare_perf(arc_distance_numpy_broadcast, [a,b])
compare_perf(arc_distance_numpy_tile, [a,b])

########NEW FILE########
__FILENAME__ = collatz
#
# Longest hailstone sequence from http://www.mit.edu/~mtikekar/posts/stream-fusion.html
#
import sys

def collatzLen(a0):
    a = a0
    length = 0
    while a != 1:
        a = (a if a%2 == 0 else 3*a+1) / 2
        length += 1
    return length

def maxLen(max_a0):
    max_length = 0
    longest = 0
    for a0 in xrange(1, max_a0 + 1):
        length = collatzLen(a0)
        if length > max_length:
            max_length = length
            longest = a0
    return max_length, longest

from compare_perf import compare_perf

compare_perf(maxLen, [1000000])


########NEW FILE########
__FILENAME__ = compare_perf

from parakeet import jit
import numpy as np 

from timer import timer 


def compare_perf(fn, args, numba= True, cpython = True, 
                 extra = {}, 
                 backends = ('c', 'openmp', 'cuda'), 
                 suppress_output = False,
                 propagate_exceptions = False):

  
  parakeet_fn = jit(fn)
  name = fn.__name__
  parakeet_result = None
  numba_result = None 
  cpython_result = None 
  kwargs = {'suppress_stdout': suppress_output, 
            'suppress_stderr':suppress_output,
            'propagate_exceptions' : propagate_exceptions
           }
  backend = None 
  for backend in backends:
    with timer('Parakeet (backend = %s) #1 -- %s' % (backend, name), **kwargs):
      parakeet_result = parakeet_fn(*args, _backend = backend)

    with timer('Parakeet (backend = %s) #2 -- %s' % (backend, name), **kwargs):
      parakeet_result = parakeet_fn(*args, _backend = backend)

  if numba:
    from numba import autojit, config 
    numba_fn = autojit(fn)
    with timer('Numba #1 -- %s' % name, **kwargs):
      numba_result = numba_fn(*args)

    with timer('Numba #2 -- %s' % name, **kwargs):
      numba_result = numba_fn(*args)

  if cpython:
    with timer('Python -- %s' % name, **kwargs):
      cpython_result = fn(*args)
  
  
  for name, impl in extra.iteritems():
    with timer("%s #1" % name, **kwargs):
      impl(*args)
    with timer("%s #2" % name, **kwargs):
      extra_result = impl(*args)


    if python_result is not None:
      diffs = np.abs(parakeet_result - extra_result)
      assert np.allclose(parakeet_result, extra_result, atol = atol, rtol = rtol), \
        "Max elt difference between Parakeet and %s = %s (median = %s, min = %s)" % \
        (name, np.max(diffs), np.median(diffs), np.min(diffs))

  rtol = 0.0001
  if backend in ('cuda', 'gpu'):
    atol = 0.001
  else:
    atol = 0.00001
  if parakeet_result is not None and cpython_result is not None:
      diffs = np.abs(cpython_result - parakeet_result)
      assert np.allclose(cpython_result, parakeet_result, atol = atol, rtol = rtol), \
        "Max elt difference between Parakeet and CPython = %s (median = %s, min = %s)" % \
        (np.max(diffs), np.median(diffs), np.min(diffs))
  
  if numba_result is not None and cpython_result is not None:
      diffs = np.abs(cpython_result - numba_result)
      assert np.allclose(cpython_result, numba_result, atol = atol, rtol = rtol), \
        "Max elt difference between Numba and CPython = %s (median = %s, min = %s)" % \
        (np.max(diffs), np.median(diffs), np.min(diffs))



########NEW FILE########
__FILENAME__ = diffuse
#
# Code taken from Numba's documentation at http://numba.pydata.org/numba-doc/0.11/arrays.html
#

import numpy as np 

mu = 0.1
Lx, Ly = 101, 101
N = 1000
import parakeet
import parakeet.c_backend
#parakeet.c_backend.config.print_module_source = True


def diffuse_loops(iter_num):
    u = np.zeros((Lx, Ly), dtype=np.float64)
    temp_u = np.zeros_like(u)
    temp_u[Lx / 2, Ly / 2] = 1000.0

    for n in range(iter_num):
        for i in range(1, Lx - 1):
            for j in range(1, Ly - 1):
                u[i, j] = mu * (temp_u[i + 1, j] + temp_u[i - 1, j] +
                                temp_u[i, j + 1] + temp_u[i, j - 1] -
                                4 * temp_u[i, j])

        temp = u
        u = temp_u
        temp_u = temp

    return u

def diffuse_array_expressions(iter_num):
    u = np.zeros((Lx, Ly), dtype=np.float64)
    temp_u = np.zeros_like(u)
    temp_u[Lx / 2, Ly / 2] = 1000.0

    for i in range(iter_num):
        u[1:-1, 1:-1] = mu * (temp_u[2:, 1:-1] + temp_u[:-2, 1:-1] +
                              temp_u[1:-1, 2:] + temp_u[1:-1, :-2] -
                              4 * temp_u[1:-1, 1:-1])

        temp = u
        u = temp_u
        temp_u = temp
    return u


from compare_perf import compare_perf 

compare_perf(diffuse_loops, [N], numba=True)
compare_perf( diffuse_array_expressions, [N], numba =True)

########NEW FILE########
__FILENAME__ = finite-difference
import numpy as np 

def fdtd(input_grid, steps):
    grid = input_grid.copy()
    old_grid = np.zeros_like(input_grid)
    previous_grid = np.zeros_like(input_grid)

    l_x = grid.shape[0]
    l_y = grid.shape[1]

    for i in range(steps):
        previous_grid[:, :] = old_grid
        old_grid[:, :] = grid 
        for x in range(l_x):
            for y in range(l_y):
                grid[x,y] = 0.0
                if x + 1 < l_x:
                    grid[x,y] += old_grid[x+1,y]
                if 0 < x-1 and x - 1 < l_x:
                    grid[x,y] += old_grid[x-1,y]
                if y+1 < l_y:
                    grid[x,y] += old_grid[x,y+1]
                if 0 < y-1 and y-1 < l_y:
                    grid[x,y] += old_grid[x,y-1]
                grid[x,y] /= 2.0
                grid[x,y] -= previous_grid[x,y]
    return grid

N = 1000
steps = 20 
input_grid = np.random.randn(N,N).astype('float64')

import parakeet
parakeet.config.print_generated_code = True 

from compare_perf import compare_perf 
compare_perf(fdtd, [input_grid, steps], backends = ('c', 'openmp', 'cuda'))



########NEW FILE########
__FILENAME__ = growcut

# Authors: Nathan Faggian, Stefan van der Walt, Aron Ahmadia, Olivier Grisel
# https://github.com/stefanv/growcut_py

import numpy as np

def growcut_python(image, state, state_next, window_radius):
    changes = 0
    height = image.shape[0]
    width = image.shape[1]
    for j in xrange(width):
        for i in xrange(height):
            winning_colony = state[i, j, 0]
            defense_strength = state[i, j, 1]
            for jj in xrange(max(j-window_radius,0), min(j+window_radius+1, width)):
                for ii in xrange(max(i-window_radius, 0), min(i+window_radius+1, height)):
                    if ii != i or jj != j:
                        d = image[i, j, :] - image[ii, jj, :]
                        s = np.sum(d**2) 
                        gval = 1.0 - np.sqrt(s) / np.sqrt(3)
                        attack_strength = gval * state[ii, jj, 1]
                        if attack_strength > defense_strength:
                            defense_strength = attack_strength
                            winning_colony = state[ii, jj, 0]
                            changes += 1
            state_next[i, j, 0] = winning_colony
            state_next[i, j, 1] = defense_strength
    return changes
    
N = 50
dtype = np.double
image = np.zeros((N, N, 3), dtype=dtype)
state = np.zeros((N, N, 2), dtype=dtype)
state_next = np.empty_like(state)

# colony 1 is strength 1 at position 0,0
# colony 0 is strength 0 at all other positions
state[0, 0, 0] = 1
state[0, 0, 1] = 1

window_radius = 10

import parakeet 
def growcut_par(image, state, window_radius):
    height = image.shape[0]
    width = image.shape[1]
    def attack((i,j)):
            winning_colony = state[i, j, 0]
            defense_strength = state[i, j, 1]
            for jj in xrange(max(j-window_radius,0), min(j+window_radius+1, width)):
                for ii in xrange(max(i-window_radius, 0), min(i+window_radius+1, height)):
                    if ii != i or jj != j:
                        d = image[i, j, :] - image[ii, jj, :]
                        s = np.sum(d**2) 
                        gval = 1.0 - np.sqrt(s) / np.sqrt(3)
                        attack_strength = gval * state[ii, jj, 1]
                        if attack_strength > defense_strength:
                            defense_strength = attack_strength
                            winning_colony = state[ii, jj, 0]
            return [winning_colony, defense_strength]
    return parakeet.imap(attack, (height, width))

from compare_perf import compare_perf 

import time 
t = time.time()
growcut_python(image, state, state_next, window_radius)
t2 = time.time()
print "Python time", t2 - t
compare_perf(growcut_par, [image, state, window_radius], suppress_output = False, propagate_exceptions = True)



########NEW FILE########
__FILENAME__ = harris
import numpy as np



def harris(I):
  m,n = I.shape
  dx = (I[1:, :] - I[:m-1, :])[:, 1:]
  dy = (I[:, 1:] - I[:, :n-1])[1:, :]

  #
  #   At each point we build a matrix
  #   of derivative products
  #   M =
  #   | A = dx^2     C = dx * dy |
  #   | C = dy * dx  B = dy * dy |
  #
  #   and the score at that point is:
  #      det(M) - k*trace(M)^2
  #
  A = dx * dx
  B = dy * dy
  C = dx * dy
  tr = A + B
  det = A * B - C * C
  k = np.float32(0.05)
  return det - k * tr * tr

from compare_perf import compare_perf 
m,n = 2400, 2400
dtype = 'float32' 
I = (np.random.randn(m,n) ** 2).astype(dtype)

compare_perf(harris, [I], propagate_exceptions=True)

########NEW FILE########
__FILENAME__ = julia
import numpy as np 
from parakeet import testing_helpers

def kernel(zr, zi, cr, ci, lim, cutoff):
    ''' Computes the number of iterations `n` such that 
        |z_n| > `lim`, where `z_n = z_{n-1}**2 + c`.
    '''
    count = np.uint32(0)
    while ((zr*zr + zi*zi) < (lim*lim)) and count < cutoff:
        zr, zi = zr * zr - zi * zi + cr, 2 * zr * zi + ci
        count += np.uint32(1)
    return count

def julia_loops(cr, ci, N, bound=1.5, lim=1000., cutoff=1e6):
    ''' Pure Python calculation of the Julia set for a given `c`.  No NumPy
        array operations are used.
    '''
    julia = np.empty((N, N), dtype=np.uint32)
    grid_x = np.linspace(-bound, bound, N)
    for i, x in enumerate(grid_x):
        for j, y in enumerate(grid_x):
            julia[i,j] = kernel(x, y, cr, ci, lim, cutoff=cutoff)
    return julia

def julia(cr, ci, N, bound=1.5, lim=1000., cutoff=1e6):
   grid_x = np.linspace(-bound, bound, N)
   return np.array([[kernel(x,y,cr,ci,lim,cutoff=cutoff) 
                     for x in grid_x] 
                     for y in grid_x])
                    
from compare_perf import compare_perf 
cr=0.285
ci=0.01
N=1200
bound = 1.5 
lim = 1000
cutoff = 1e6 

extra = {}
try:
  from numba import autojit 
  extra['numba'] = autojit(julia_loops)
except:
  print "Failed to import Numba" 

compare_perf(julia, [cr, ci, N, bound, lim, cutoff], numba = False, extra = extra)


########NEW FILE########
__FILENAME__ = kmeans
import parakeet
import numpy as np 


def dist(x,y):
  return ((x-y)**2).sum()

def kmeans_comprehensions(X, k, niters = 10):
  C = X[:k, :]
  for _ in xrange(niters):
    A = np.array([np.argmin([dist(x,c) for c in C]) for x in X])
    C = np.array([np.mean(X[A == i, :], axis = 0) for i in xrange(k)])
  return C

def kmeans_loops(X, k, niters = 10):
  C = X[:k, :]
  n,ndims = X.shape
  A = np.zeros(n, dtype=int)
  for _ in xrange(niters):
    # assign data points to nearest centroid
    for i in xrange(n):
      x = X[i,:]
      min_dist = dist(x, C[0, :]) 
      min_idx = 0 
      for cidx in xrange(1,k):
        centroid = C[cidx,:]
        curr_dist = 0.0
        for xidx in xrange(ndims):
          curr_dist += (x[xidx] - centroid[xidx])**2
        if curr_dist < min_dist:
          min_dist = curr_dist
          min_idx = cidx
      A[i] = min_idx
    # recompute the clusters by averaging data points 
    # assigned to them 
    for cidx in xrange(k):
      # reset centroids
      for dim_idx in xrange(ndims):
        C[cidx, dim_idx] = 0
      # add each data point only to its assigned centroid
      cluster_count = 0
      for i in xrange(n):
        if A[i] == cidx:
          C[cidx, :] += X[i, :]
          cluster_count += 1
      C[cidx, :] /= cluster_count 
  return C      

n, d = 10**4, 50
X = np.random.randn(n,d)
k = 25

from compare_perf import compare_perf

compare_perf(kmeans_comprehensions, [X, k, 5],cpython=False)

compare_perf(kmeans_loops, [X, k, 5], cpython=True)

########NEW FILE########
__FILENAME__ = matmult
import parakeet
import numpy as np 
def matmult_high_level(X,Y):
  return np.array([[np.dot(x,y) for y in Y.T] for x in X])


#
#n, d = 2500, 20
#m = 2500

n,d,m = 1200,1200,1200
dtype = 'float32'
X = np.random.randn(m,d).astype(dtype)
Y = np.random.randn(d,n).astype(dtype)

from compare_perf import compare_perf

extra = {'BLAS (np.dot)' : np.dot}
try:
  import pycuda
  import pycuda.gpuarray 
  import pycuda.autoinit

  import scikits.cuda.linalg
  import scikits.cuda.cublas 
  cublas_context = scikits.cuda.cublas.cublasCreate()
  def gpu_dot(X,Y):
    X_gpu = pycuda.gpuarray.to_gpu(X)
    Y_gpu = pycuda.gpuarray.to_gpu(Y)
    Z_gpu = scikits.cuda.linalg.dot(X_gpu, Y_gpu, 'N', 'N', handle = cublas_context)
    return Z_gpu.get()
  extra['cuBLAS'] = gpu_dot 
except:
  print "Failed to import PyCUDA + scikits.cuda" 

try:
  import numba 

  @numba.autojit 
  def matmult_loops(X,Y,Z):
    m, d = X.shape
    n = Y.shape[1]
    for i in xrange(m):
      for j in xrange(n):
        total = X[i,0] * Y[0,j] 
        for k in xrange(1,d):
          total += X[i,k] * Y[k,j]
        Z[i,j] = total 
  
  def call_numba(X,Y):
    Z = np.zeros((X.shape[0],Y.shape[1])).astype(dtype)
    matmult_loops(X,Y,Z)
    return Z 

  extra['numba'] = call_numba 

except:
  print "Failed to import Numba" 
  pass 

compare_perf(matmult_high_level, [X,Y],
             cpython=True,
             # numba can't run the nested comprehensions so we use
             # a special loopy version instead 
             numba=False,
             extra = extra, 
             suppress_output = False,
             propagate_exceptions = False)


########NEW FILE########
__FILENAME__ = matmult_tropical
import parakeet
import numpy as np 


def dot(x,y):
    return np.min(x+y)

def matmult_high_level(X,Y):
  return np.array([[dot(x,y) for y in Y.T] for x in X])

def matmult_loops(X,Y,Z):
  m, d = X.shape
  n = Y.shape[1]
  for i in xrange(m):
    for j in xrange(n):
      total = X[i,0] + Y[0,j] 
      for k in xrange(1,d):
        total = min(total, X[i,k] + Y[k,j])
      Z[i,j] = total 
  return Z

n, d = 500, 500
m = 500
X = np.random.randn(m,d)
Y = np.random.randn(d,n)
Z = np.zeros((m,n))
from compare_perf import compare_perf

compare_perf(matmult_high_level, [X,Y], cpython=True, numba=False)
compare_perf(matmult_loops, [X, Y, Z], cpython=False)


########NEW FILE########
__FILENAME__ = morphology

import platform 
impl = platform.python_implementation()

from timer import timer 

print "Running under", impl
running_pypy = impl == 'PyPy'

if running_pypy:
  import numpypy as np 
  # crazy that NumPyPy doesn't support this
  def allclose(x,y):
    if x.shape != y.shape:
      return False
    err = ((x-y)**2).mean()
    return err < 0.000001
  def empty_like(x):
    return np.empty(x.shape, dtype=x.dtype)
else:
  import numpy as np 
  allclose = np.allclose
  empty_like = np.empty_like 
import time 

  
k = 7
width, height = 1024,768
if running_pypy:
  from random import randint
  image = np.array([[randint(0,256) for _ in xrange(width)] for _ in xrange(height)])
else:
  image = np.random.randint(0, 256,  (width, height)) / 256.0

def run(fn, name, imshow=False):
  print 
  print "---", name 
  for (prefix, wrapper) in [('parakeet-', jit), ('numba-', autojit)]:
    try:
      wrapped_fn = wrapper(fn)
      with timer(prefix + name + '-compile', True):
        wrapped_fn(image[:1, :1], k)
      with timer(prefix + name, False):
        result = wrapped_fn(image, k)
      if imshow:
        import pylab
        pylab.imshow(image)
        pylab.figure()
        pylab.imshow(result)
        pylab.figure()
        if scipy_result is not None:
          pylab.imshow(scipy_result)
          pylab.show()
      if not running_pypy and scipy_result is not None:
        assert allclose(result, scipy_result)

    except KeyboardInterrupt:
      raise   
    except:
      print "%s failed" % (prefix+name)
      import sys 
      print sys.exc_info()[1]

def dilate_naive(x, k):
  m,n = x.shape
  y = empty_like(x)
  for i in xrange(m):
    for j in xrange(n):
      currmax = x[i,j]
      for ii in xrange(max(0, i-k/2), min(m, i+k/2+1)):
        for jj in xrange(max(0, j-k/2), min(n, j+k/2+1)):
          elt = x[ii,jj]
          if elt > currmax:
            currmax = elt
      y[i,j] = currmax
  return y  

# Numba doesn't yet support min/max so try using inline if expressions
def dilate_naive_inline(x,k):
  m,n = x.shape
  y = empty_like(x)
  for i in xrange(m):
    start_i = i - k/2
    stop_i = i + k/2 + 1
    for j in xrange(n):
      currmax = x[i,j]
      start_j = j - k/2
      stop_j = j + k/2 + 1
      for ii in xrange(start_i if start_i >= 0 else 0, stop_i if stop_i <= m else m):
        for jj in xrange(start_j if start_j >= 0 else 0, stop_j if stop_j <=n else n):
          elt = x[ii,jj]
          if elt > currmax:
            currmax = elt
      y[i,j] = currmax
  return y  

 
def dilate_decompose_loops(x, k):
  m,n = x.shape
  y = empty_like(x)
  for i in xrange(m):
    for j in xrange(n):
      left_idx = max(0, i-k/2)
      right_idx = min(m, i+k/2+1) 
      currmax = x[left_idx, j]
      for ii in xrange(left_idx+1, right_idx):
        elt = x[ii, j]
        if elt > currmax:
          currmax = elt 
      y[i, j] = currmax 
  z = empty_like(x)
  for i in xrange(m):
    for j in xrange(n):
      left_idx = max(0, j-k/2)
      right_idx = min(n, j+k/2+1)
      currmax = y[i,left_idx]  
      for jj in xrange(left_idx+1, right_idx):
        elt = y[i,jj]
        if elt > currmax:
          currmax = elt
      z[i,j] = currmax
  return z 

def dilate_decompose_loops_inline(x, k):
  m,n = x.shape
  y = empty_like(x)
  for i in xrange(m):
    start_i = i-k/2
    stop_i = i+k/2+1 
    for j in xrange(n):
      left_idx = start_i if start_i >= 0 else 0
      right_idx = stop_i if stop_i <= m else m 
      currmax = x[left_idx, j]
      for ii in xrange(left_idx+1, right_idx):
        elt = x[ii, j]
        if elt > currmax:
          currmax = elt 
      y[i, j] = currmax 
  z = empty_like(x)
  for i in xrange(m):
    for j in xrange(n):
      start_j = j-k/2
      stop_j = j+k/2+1
      left_idx = start_j if start_j >= 0 else 0
      right_idx = stop_j if stop_j <= n else n
      currmax = y[i,left_idx]  
      for jj in xrange(left_idx+1, right_idx):
        elt = y[i,jj]
        if elt > currmax:
            currmax = elt
      z[i,j] = currmax
  return z 


def dilate_1d_naive(x_strip,  k):
  """
  Given a 1-dimensional input and 1-dimensional output, 
  fill output with 1d dilation of input 
  """
  nelts = len(x_strip)
  y_strip = empty_like(x_strip)
  half = k / 2 
  for idx in xrange(nelts):
    left_idx = max(idx-half,0)
    right_idx = min(idx+half+1, nelts)
    currmax = x_strip[left_idx]
    for j in xrange(left_idx+1, right_idx):
      elt = x_strip[j]
      if elt > currmax:
        currmax = elt
    y_strip[idx] = currmax 
  return y_strip

def dilate_decompose(x, k): 
  m,n = x.shape
  y = np.array([dilate_1d_naive(x[row_idx, :], k) for row_idx in xrange(m)])
  return np.array([dilate_1d_naive(y[:, col_idx], k) for col_idx in xrange(n)]).T

def dilate_1d_interior(x_strip, k):
  
  nelts = len(x_strip)
  y_strip = empty_like(x_strip)
  half = k / 2 
  
  interior_start = half+1
  interior_stop = max(nelts-half, interior_start)
  
  # left boundary
  for i in xrange(min(half+1, nelts)):
    left_idx = max(i-half,0)
    right_idx = min(i+half+1, nelts)
    currmax = x_strip[left_idx]
    for j in xrange(left_idx+1, right_idx):
      elt = x_strip[j]
      if elt > currmax:
        currmax = elt
    y_strip[i] = currmax 
    
  #interior 
  for i in xrange(interior_start, interior_stop):
    left_idx = i-half
    right_idx = i+half+1
    currmax = x_strip[left_idx]
    for j in xrange(left_idx+1, right_idx):
      elt = x_strip[j]
      if elt > currmax:
        currmax = elt
    y_strip[i] = currmax 
  
  # right boundary
  for i in xrange(interior_stop, nelts):
    left_idx = max(i-half, 0)
    right_idx = nelts
    currmax = x_strip[left_idx]
    for j in xrange(left_idx+1, right_idx):
      elt = x_strip[j]
      if elt > currmax:
        currmax = elt
    y_strip[i] = currmax 
  return y_strip 

def dilate_decompose_interior(x, k): 
  m,n = x.shape
  y = np.array([dilate_1d_interior(x[row_idx, :],k) for row_idx in xrange(m)])
  return np.array([dilate_1d_interior(y[:, col_idx],k) for col_idx in xrange(n)]).T
 
if __name__ == '__main__':
  if not running_pypy: 
    scipy_result = None 
    import scipy.ndimage
    with timer('scipy'):
      scipy_result = scipy.ndimage.grey_dilation(image, k, mode='nearest')
    from numba import autojit 
    from parakeet import jit 
    jit(dilate_naive)(image, k)

    run(dilate_naive, 'naive', imshow=False)
    run(dilate_naive_inline, 'naive-inline')
    run(dilate_decompose_loops, 'decompose-loops')    
    run(dilate_decompose_loops_inline, 'decompose-loops-inline')
    run(dilate_decompose, 'decompose-slices' )
    run(dilate_decompose_interior, 'decompose-interior')

  with timer('cpython-naive'):
    dilate_naive(image, k,)
  with timer('cpython-naive-inline'):
    dilate_naive_inline(image, k)
  with timer('cpython-decompose-loops'):
    dilate_decompose_loops(image, k)
  with timer('cpython-decompose-loops-inline'):
    dilate_decompose_loops_inline(image, k)

########NEW FILE########
__FILENAME__ = nd_local_maxima
import numpy as np 
import parakeet 

def wrap(pos, offset, bound):
    return ( pos + offset ) % bound

def clamp(pos, offset, bound):
    return min(bound-1,max(0,pos+offset))

def reflect(pos, offset, bound):
    idx = pos+offset
    return min(2*(bound-1)-idx,max(idx,-idx))


def local_maxima(data, wsize, mode=wrap):
  result = np.ones(shape=data.shape,dtype=bool)
  for pos in np.ndindex(data.shape):
    myval = data[pos]  
    for offset in np.ndindex(wsize):
      neighbor_idx = tuple(mode(p, o-w/2, w) for (p, o, w) in zip(pos, offset, wsize))
      result[pos] &= (data[neighbor_idx] <= myval)
  return result 


@parakeet.jit 
def parakeet_local_maxima(data, wsize, mode=wrap):
  def is_max(pos):
    def is_smaller_neighbor(offset):
      neighbor_idx = tuple(mode(p, o-w/2, w) for (p, o, w) in zip(pos, offset, wsize))
      return data[neighbor_idx] <= data[pos]
    return np.all(parakeet.imap(is_smaller_neighbor, wsize))
  return parakeet.imap(is_max, data.shape)
  

# not sure how to get numba to auto-jit size generic code
# get error: "FAILED with KeyError 'sized_pointer(npy_intp, 4)'"
#import numba
#numba_local_maxima = numba.autojit(python_local_maxima) 

from compare_perf import compare_perf 

shape = (30,30,20,12)
x = np.random.randn(*shape)
compare_perf(local_maxima, [x, shape]) 

########NEW FILE########
__FILENAME__ = periodic_dist
from parakeet import jit, config 
import numpy as np 

def dist(x, y, z, L, periodicX, periodicY, periodicZ):
    " ""Computes distances between all particles and places the result in a matrix such that the ij th matrix entry corresponds to the distance between particle i and j"" "
    N = len(x)
    xtemp = np.tile(x,(N,1))
    dx = xtemp - xtemp.T
    ytemp = np.tile(y,(N,1))
    dy = ytemp - ytemp.T
    ztemp = np.tile(z,(N,1))
    dz = ztemp - ztemp.T

    # Particles 'feel' each other across the periodic boundaries
    if periodicX:
        dx[dx>L/2]=dx[dx > L/2]-L
        dx[dx<-L/2]=dx[dx < -L/2]+L
    if periodicY:
        dy[dy>L/2]=dy[dy>L/2]-L
        dy[dy<-L/2]=dy[dy<-L/2]+L
    if periodicZ:
        dz[dz>L/2]=dz[dz>L/2]-L
        dz[dz<-L/2]=dz[dz<-L/2]+L

    # Total Distances
    d = np.sqrt(dx**2+dy**2+dz**2)

    # Mark zero entries with negative 1 to avoid divergences
    d[d==0] = -1

    return d, dx, dy, dz

@jit 
def parakeet_dist(x, y, z, L, periodicX, periodicY, periodicZ):
  N = len(x)
  def periodic_diff(x1, x2, periodic):
    diff = x1 - x2 
    if periodic:
      if diff > (L / 2):
        diff -= L
      if diff < (-L/2):
        diff += L
    return diff
  dx = np.array([[periodic_diff(x1, x2, periodicX) for x1 in x] for x2 in x])
  dy = np.array([[periodic_diff(y1, y2, periodicY) for y1 in y] for y2 in y])
  dz = np.array([[periodic_diff(z1, z2, periodicZ) for z1 in z] for z2 in z])
  d = dx**2 + dy**2 + dz**2
  for i in xrange(N):
    for j in xrange(N):
      if d[i,j] == 0: d[i,j] = -1 
      else: d[i,j] = np.sqrt(d[i,j])
  return d, dx, dy, dz 

def periodic_diff(x1, x2, L, periodic):
  diff = x1 - x2 
  if periodic:
    if diff > (L / 2):
      diff -= L
    if diff < (-L/2):
      diff += L
  return diff

@jit 
def loopy_dist(x, y, z, L, periodicX, periodicY, periodicZ):
  N = len(x)
  dx = np.zeros((N,N))
  dy = np.zeros( (N,N) )
  dz = np.zeros( (N,N) )
  d = np.zeros( (N,N) )
  for i in xrange(N):
    for j in xrange(N):
      dx[i,j] = periodic_diff(x[j], x[i], L, periodicX)
      dy[i,j] = periodic_diff(y[j], y[i], L, periodicY)
      dz[i,j] = periodic_diff(z[j], z[i], L, periodicZ)
      d[i,j] = dx[i,j] ** 2 + dy[i,j] ** 2 + dz[i,j] ** 2 
      if d[i,j] == 0:
          d[i,j] = -1
      else:
          d[i,j] = np.sqrt(d[i,j])
  return d, dx, dy, dz 
from timer import timer

N = 2000 
x = y = z = np.random.rand(N)
L = 4
periodic = True
with timer("Python #1"):
  d, dx, dy, dz = dist(x, x, x, L,periodic, periodic, periodic)

with timer("Python #2"):
  d, dx, dy, dz = dist(x, x, x, L, periodic, periodic, periodic)

with timer("Parakeet Dist #1", suppress_stdout = False, suppress_stderr = False):
  pd, pdx, pdy, pdz = parakeet_dist(x,x,x,L, periodic, periodic, periodic)

assert np.allclose(pdx, dx), (pdx-dx, np.max(np.abs(pdx - dx)))
assert np.allclose(pdy, dy), np.max(np.abs(pdy - dy))
assert np.allclose(pdz, dz), np.max(np.abs(pdz - dz))
assert np.allclose(pd, d), (np.max(np.abs(pd - d)), pd -d)


with timer("Parakeet Dist #2"):
  pd, pdx, pdy, pdz = parakeet_dist(x,x,x,L, periodic, periodic, periodic)


with timer("Parakeet Loop Dist #1"):
  ld, ldx, ldy, ldz = loopy_dist(x,x,x,L, periodic, periodic, periodic)

assert np.allclose(ldx, dx), (ldx-dx, np.max(np.abs(ldx - dx)))
assert np.allclose(ldy, dy), np.max(np.abs(ldy - dy))
assert np.allclose(ldz, dz), np.max(np.abs(ldz - dz))
assert np.allclose(ld, d), (np.max(np.abs(ld - d)), ld -d)

with timer("Parakeet Loop Dist #2"):
  ld, ldx, ldy, ldz = loopy_dist(x,x,x,L, periodic, periodic, periodic)


########NEW FILE########
__FILENAME__ = pulseprop
"""
NMR Pulse propagation 
From http://themodernscientist.com/posts/2013/2013-06-09-simulation_of_nmr_shaped_pulses/
"""

import numpy as np 

pulseLength = 1000. # in microseconds
offset = [-5000., 5000.] # in hertz
n_freq = 500
inputMagnetization = 'Mz' # 'Mx', 'My', or 'Mz'


deltaomega = np.abs(offset[1]-offset[0])/n_freq
relativeomega = np.arange(np.min(offset),np.max(offset),deltaomega)

n_pulse = 1000 # number of points in the pulse, set by user
totalRotation = 180. # in degrees, set by user

fourierCoeffA = np.array([0.49, -1.02, 1.11, -1.57, 0.83, -0.42, 0.26, -0.16, 0.10, -0.07, 0.04, -0.03, 0.01, -0.02, 0.0, -0.01])
x = np.linspace(1,n_pulse,n_pulse)/n_pulse*2.*np.pi
nCosCoef = np.arange(1,len(fourierCoeffA))
cosMat = np.cos(nCosCoef[np.newaxis,:]*x[:,np.newaxis])
cosMat = np.append(np.ones((n_pulse,1)),cosMat,axis=1)*fourierCoeffA
sumMat = np.sum(cosMat,axis=1)

pulseShapeArray = np.zeros((n_pulse,2))
pulseShapeArray[:,0] = np.abs(sumMat)
pulseShapeArray[sumMat<0,1] = 180.

pulseShapeInten = pulseShapeArray[:,0] / np.max(np.abs(pulseShapeArray[:,0]))
pulseShapePhase = pulseShapeArray[:,1] * np.pi/180

xPulseShape = pulseShapeInten * np.cos(pulseShapePhase)
yPulseShape = pulseShapeInten * np.sin(pulseShapePhase)

scalingFactor = np.sum(xPulseShape)/n_pulse
gammaB1max = 1./(pulseLength * 360./totalRotation)/scalingFactor * 1e6
nu1maxdt = 2*np.pi*1e-6*gammaB1max*pulseLength/n_pulse

inputVector = np.array([[0],[0],[1]])
inputMagnetizationDict = {'mx':np.array([[1],[0],[0]]), 'my':np.array([[0],[1],[0]]), 'mz':np.array([[0],[0],[1]]) }
if inputMagnetization.lower() in inputMagnetizationDict.keys():
        inputVector = inputMagnetizationDict[inputMagnetization.lower()]
vectorComponent = inputVector.argmax()

def pulseprop(relativeomega, pulseShapeInten, pulseShapePhase, gammaB1max, nu1maxdt, inputVector, n_pulse, n_freq):
    # Functions for the y and z-rotations and the the function for a generic rotation
    def yrotation(beta):
      return np.array([[np.cos(beta), 0, np.sin(beta)], [0, 1, 0], [-np.sin(beta), 0, np.cos(beta)]])
    def zrotation(beta):
      return np.array([[np.cos(beta), -np.sin(beta), 0], [np.sin(beta), np.cos(beta), 0], [0, 0, 1]])
    def grotation(alpha,theta,phi):
      return np.dot(zrotation(phi),
                    np.dot(yrotation(theta),
                           np.dot(zrotation(alpha), 
                                  np.dot(yrotation(-theta),zrotation(-phi)))))
    
    
    xyzdata = np.zeros((3,len(relativeomega)))
    phi = pulseShapePhase
    
    # Loop through the entire frequency range calculating the rotation matrix (r) at each frequency
    for ind in range(len(relativeomega)):
    
        theta = np.arctan2(pulseShapeInten, relativeomega[ind]/gammaB1max)
        alpha = nu1maxdt * np.sqrt(pulseShapeInten**2+(relativeomega[ind]/gammaB1max)**2)
        
        prop = np.eye(3)
        # The rotation matrix is a recursive loop through each step of the shaped pulse
        for pulseindex in range(n_pulse):
            r = grotation(alpha[pulseindex],theta[pulseindex],phi[pulseindex])
            prop = np.dot(r,prop)
        res = np.dot(prop, inputVector)
        xyzdata[:,ind] = np.dot(prop,inputVector)[:, 0]
        
    return xyzdata

from compare_perf import compare_perf 
from parakeet import jit 
f = jit(pulseprop)
f(relativeomega, pulseShapeInten, pulseShapePhase, gammaB1max, nu1maxdt, inputVector, n_pulse, n_freq)

########NEW FILE########
__FILENAME__ = rosenbrock

# Rosenbrock function derivative
# 
# Copied from https://github.com/numfocus/python-benchmarks/blob/master/rosen_der/rosen_der_python.py
# Original authors: Travis Oliphant (NumPy version) & Serge Guelton (loops version)
#

import numpy as np 
from parakeet import jit 

def rosen_der_np(x):
  der = np.empty_like(x)
  der[1:-1] = (+ 200 * (x[1:-1] - x[:-2] ** 2)
               - 400 * (x[2:] - x[1:-1] ** 2) * x[1:-1]
               - 2 * (1 - x[1:-1]))
  der[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0])
  der[-1] = 200 * (x[-1] - x[-2] ** 2)
  return der

def rosen_der_loops(x):
  n = x.shape[0]
  der = np.empty_like(x)

  for i in range(1, n - 1):
    der[i] = (+ 200 * (x[i] - x[i - 1] ** 2)
              - 400 * (x[i + 1]
              - x[i] ** 2) * x[i]
              - 2 * (1 - x[i]))
  der[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0])
  der[-1] = 200 * (x[-1] - x[-2] ** 2)
  return der

if __name__ == '__main__':
  N = 10**7
  x = np.arange(N) / float(N)
  jit(rosen_der_np)(x) 
  from compare_perf import compare_perf
  # numba still crashes on negative indexing
  compare_perf(rosen_der_np, [x.copy()], numba=False)
  compare_perf(rosen_der_loops, [x.copy()], numba=False)

########NEW FILE########
__FILENAME__ = simple_conv
import numpy as np 
from compare_perf import compare_perf

# Simple convolution of 3x3 patches from a given array x
# by a 3x3 array of filter weights
 
def conv_3x3_trim(x, weights):
  return np.array([[(x[i-1:i+2, j-1:j+2]*weights).sum() 
                    for j in xrange(1, x.shape[1] -2)]
                    for i in xrange(1, x.shape[0] -2)])
 
x = np.random.randn(1500,1500).astype('float32')
w = np.random.randn(3,3).astype('float32')
#compare_perf(conv_3x3_trim, [x,w])

w = np.random.randn(3,3).astype('float32')
# Simple convolution of 5x5 patches from a given array x
# by a 5x5 array of filter weights
 
def conv_3x3_trim_loops(image, weights):
  result = np.zeros_like(image)
  for i in xrange(1,x.shape[0]-1):
    for j in xrange(1,x.shape[1]-1):
      for ii in xrange(3): 
        for jj in xrange(3):
          result[i,j] += image[i-ii+1, j-jj+1] * weights[ii, jj] 
  return result

compare_perf(conv_3x3_trim_loops, [x,w])

import parakeet 
def conv_3x3_imap(image, weights):
  def compute((i,j)):
      total = np.float32(0.0)
      for ii in xrange(3):
          for jj in xrange(3):
            total += image[i+ii-1, j + jj - 1] * weights[ii, jj]
      return total 
  w,h = image.shape
  return parakeet.imap(compute, (w-2,h-2))
compare_perf(conv_3x3_imap, [x,w], backends=('openmp', 'cuda',))

########NEW FILE########
__FILENAME__ = simple_regression
from parakeet import jit, config, c_backend 
 


def covariance(x,y):
  return ((x-x.mean()) * (y-y.mean())).mean()

def fit_simple_regression(x,y):
  slope = covariance(x,y) / covariance(x,x)
  offset = y.mean() - slope * x.mean() 
  return slope, offset

import numpy as np 

N = 2*10**7
x = np.random.randn(N).astype('float64')
slope = 903.29
offset = 102.1
y = slope * x + offset



from compare_perf import compare_perf 
compare_perf(fit_simple_regression, (x,y), numba=True)

########NEW FILE########
__FILENAME__ = smoothing
import numpy as np 


def smooth(x, alpha):
  s = x.copy()
  for i in xrange(1, len(x)):
      s[i] = alpha * x[i] + (1-alpha)*s[i-1]
  return s 

n = 10**6
alpha = 0.01
X = np.random.randn(n).astype('float32')

from compare_perf import compare_perf
compare_perf(smooth, [X, alpha])


########NEW FILE########
__FILENAME__ = sph_render
"""
SPH renderer, modified from Numba version at:
https://gist.github.com/rokroskar/bdcf6c6b210ff0efc738#file-gistfile1-txt-L55
"""
 
 
import numpy as np
from numpy import int32
 
def kernel_func(d, h) : 
    if d < 1 : 
        f = 1.-(3./2)*d**2 + (3./4.)*d**3
    elif d<2 :
        f = 0.25*(2.-d)**3
    else :
        f = 0
    return f/(np.pi*h**3)
 
def distance(x,y,z) : 
    return np.sqrt(x**2+y**2+z**2)
 
def physical_to_pixel(xpos,xmin,dx) : 
    return int32((xpos-xmin)/dx)
 
def pixel_to_physical(xpix,x_start,dx) : 
    return dx*xpix+x_start
 
def render_image(xs, ys, zs, hs, qts, mass, rhos, nx, ny, 
                  xmin = 0.0, xmax = 1.0, ymin = 0.0, ymax = 1.0): 
    MAX_D_OVER_H = 2.0
 
    image = np.zeros((nx,ny))
 
    dx = (xmax-xmin)/nx
    dy = (ymax-ymin)/ny
 
    x_start = xmin+dx/2
    y_start = ymin+dy/2
    zplane = 0.0
 
    # set up the kernel values
    kernel_samples = np.arange(0, 2.01, .01)
    kernel_vals = np.array([kernel_func(x,1.0) for x in kernel_samples])
    for i, (x,y,z,h) in enumerate(zip(xs,ys,zs,hs)):
        qt = qts[i] * mass[i] / rhos[i]

        # is the particle in the frame?
        if ((x > xmin-2*h) and (x < xmax+2*h) and 
            (y > ymin-2*h) and (y < ymax+2*h) and 
            (np.abs(z-zplane) < 2*h)) : 
        
                    
            if (MAX_D_OVER_H*h/dx < 1 ) and (MAX_D_OVER_H*h/dy < 1) : 
                # pixel coordinates 
                xpos = physical_to_pixel(x,xmin,dx)
                ypos = physical_to_pixel(y,ymin,dy)
                # physical coordinates of pixel
                xpixel = pixel_to_physical(xpos,x_start,dx)
                ypixel = pixel_to_physical(ypos,y_start,dy)
                zpixel = zplane
 
                dxpix, dypix, dzpix = [x-xpixel,y-ypixel,z-zpixel]
                d = distance(dxpix,dypix,dzpix)
                if (xpos > 0) and (xpos < nx) and (ypos > 0) and (ypos < ny) and (d/h < 2) : 
                    kernel_val = kernel_vals[int(d/(.01*h))]/(h*h*h)
                    image[xpos,ypos] += qt*kernel_val
 
            else :
                # bottom left of pixels the particle will contribute to
                x_pix_start = int32(physical_to_pixel(x-MAX_D_OVER_H*h,xmin,dx))
                x_pix_stop  = int32(physical_to_pixel(x+MAX_D_OVER_H*h,xmin,dx))
                y_pix_start = int32(physical_to_pixel(y-MAX_D_OVER_H*h,ymin,dy))
                y_pix_stop  = int32(physical_to_pixel(y+MAX_D_OVER_H*h,ymin,dy))
            
                if(x_pix_start < 0):  x_pix_start = 0
                if(x_pix_stop  > nx): x_pix_stop  = int32(nx-1)
                if(y_pix_start < 0):  y_pix_start = 0
                if(y_pix_stop  > ny): y_pix_stop  = int32(ny-1)
    
                
                for xpix in range(x_pix_start, x_pix_stop) : 
                    for ypix in range(y_pix_start, y_pix_stop) : 
                        # physical coordinates of pixel
                        xpixel = pixel_to_physical(xpix,x_start,dx)
                        ypixel = pixel_to_physical(ypix,y_start,dy)
                        zpixel = zplane
 
                        dxpix, dypix, dzpix = [x-xpixel,y-ypixel,z-zpixel]
                        d = distance(dxpix,dypix,dzpix)
                        if (d/h < 2) : 
                            kernel_val = kernel_vals[int(d/(.01*h))]/(h*h*h)
                            image[xpix,ypix] += qt*kernel_val
    
 
    return image

from compare_perf import compare_perf 

N = 160
x = y = z = hs= qts = mass = rhos = np.random.rand(N)
nx=ny=80
args = (x,y,z,hs,qts,mass,rhos,nx,ny, 0.0, 1.0, 0.0, 1.0)
compare_perf(render_image, args, numba = True, backends= ('c',))

########NEW FILE########
__FILENAME__ = summation
     
import numpy as np 
def summation(pos, weights, points):
  n_points = len(points)
  n_weights = len(weights)
  sum_array = np.zeros(n_points)
  sum_array3d = np.zeros((n_points,3))
  def compute(i):
    pxi = points[i, 0]
    pyi = points[i, 1]
    pzi = points[i, 2]
    total = 0.0
    for j in xrange(n_weights):
      weight_j = weights[j]
      xj = pos[j,0]
      yj = pos[j,1]
      zj = pos[j,2]
      dx = pxi - pos[j, 0]
      dy = pyi - pos[j, 1]
      dz = pzi - pos[j, 2]
      dr = 1.0/np.sqrt(dx*dx + dy*dy + dz*dz)
      total += weight_j * dr
      sum_array3d[i,0] += weight_j * dx
      sum_array3d[i,1] += weight_j * dy
      sum_array3d[i,2] += weight_j * dz
    return total 
  sum_array = np.array([compute(i) for i in xrange(n_points)])
  return sum_array, sum_array3d

n_points = 200
n_weights = 400
pos = np.random.randn(n_weights, 3)
weights = np.random.randn(n_weights)
points = np.random.randn(n_points, 3)

from compare_perf import compare_perf 

compare_perf(summation, [pos, weights, points])

########NEW FILE########
__FILENAME__ = template_sph_render


import time 
import numpy as np 
from numpy import sqrt, int32, mod, double 
 
def calculate_distance(template, dx, dy) : 
    side_length = template.shape[0]
    # where is the center position
    cen = side_length/2
    
    for i in range(side_length) : 
        for j in range(side_length) : 
            template[i,j] = sqrt(((i-cen)*dx)**2 + ((j-cen)*dy)**2)




def kernel_loops(ds, h):
  fs = np.zeros_like(ds)
  denom = (np.pi*h**3)
  for i in xrange(ds.shape[0]):
    for j in xrange(ds.shape[1]):
      d = ds[i,j] 
      if d < 1 : 
        fs[i,j] = (1.-(3./2)*d**2 + (3./4.)*d**3) / denom 
      elif d <= 2.0 :
        fs[i,j] = 0.25*(2.-d)**3 / denom  
  return fs 
   


def physical_to_pixel(xpos,xmin,dx) : 
    return int32((xpos-xmin)/dx)

"""

def template_render_image(s,nx,ny,xmin,xmax,ymin,ymax,qty='rho',timing = False,two_d=0):
    
    time_init = time.clock()
    
    xs,ys,zs,hs,qts,mass,rhos = [s[arr] for arr in ['x','y','z','smooth',qty,'mass','rho']]

    # ----------------------
    # setup the global image
    # ----------------------
    image = np.zeros((nx,ny))
    
    dx = (xmax-xmin)/nx
    dy = (ymax-ymin)/ny
    
    x_start = xmin+dx/2
    y_start = ymin+dy/2

    zplane = 0.0

    # ------------------------------------
    # trim particles based on image limits
    # ------------------------------------
    start = time.clock()
    ind = np.where((xs + 2*hs > xmin) & (xs - 2*hs < xmax) & 
                   (ys + 2*hs > ymin) & (ys - 2*hs < ymax) &
                   (np.abs(zs-zplane)*(1-two_d) < 2*hs))[0]

    xs,ys,zs,hs,qts,mass,rhos = (xs[ind],ys[ind],zs[ind],hs[ind],qts[ind],mass[ind],rhos[ind])
    if timing: print '<<< Initial particle selection took %f s'%(time.clock()-start)

    # set the render quantity 
    qts *= mass/rhos

    #
    # bin particles by how many pixels they need in their kernel
    #
    start = time.clock()
    npix = 2.0*hs/dx
    dbin = np.digitize(npix,np.arange(1,npix.max()))
    dbin_sortind = dbin.argsort()
    dbin_sorted = dbin[dbin_sortind]
    xs,ys,zs,hs,qts = (xs[dbin_sortind],ys[dbin_sortind],zs[dbin_sortind],hs[dbin_sortind],qts[dbin_sortind])
    if timing: print '<<< Bin sort done in %f'%(time.clock()-start)

    # ---------------------
    # process the particles 
    # ---------------------
    start = time.clock()
    image = template_kernel_cpu(xs,ys,qts,hs,nx,ny,xmin,xmax,ymin,ymax,two_d)
    if timing: print '<<< Rendering %d particles took %f s'%(len(xs),
                                                             time.clock()-start)
    
    if timing: print '<<< Total time: %f s'%(time.clock()-time_init)

    return image

"""

def template_kernel_cpu(xs,ys,qts,hs,nx,ny,xmin,xmax,ymin,ymax,two_d) : 
    # ------------------
    # create local image 
    # ------------------
    image = np.zeros((nx,ny),dtype=np.float)

    Npart = len(hs) 
    dx = (xmax-xmin)/nx
    dy = (ymax-ymin)/ny

       
    # ------------------------------
    # start the loop through kernels
    # ------------------------------
    kmax = int(np.ceil(hs.max()*2.0/dx*2.0))
    kmin = int(np.floor(hs.min()*2.0/dx*2.0))
    # make sure kmin and kmax are odd
    
    kmax += (1 - mod(kmax,2))
    kmin += (1 - mod(kmin,2))
    
    kmin = max(1,kmin)
    
    kernel_base = np.ones((kmax,kmax))
    kernel = np.ones((kmax,kmax))
    calculate_distance(kernel_base,dx,dy)
    
    max_d_curr = 0.0
    start_ind = 0
    end_ind = 0
    for k in xrange(kmin,kmax+2,2) : 
        # ---------------------------------
        # the max. distance for this kernel
        # ---------------------------------
        max_d_curr = dx*np.floor(k/2.0)
        if max_d_curr < dx/2.0 : 
          max_d_curr = dx/2.0

        i_max_d = double(1./max_d_curr)
        # -------------------------------------------------
        # find the chunk of particles that need this kernel
        # -------------------------------------------------
        end_ind = 0 
        for i in xrange(start_ind,Npart): 
            if 2*hs[end_ind] < max_d_curr:
              end_ind = i
            
        
        Nper_kernel = end_ind-start_ind
        
        # -------------------------------------------------------------------------
        # only continue with kernel generation if there are particles that need it!
        # -------------------------------------------------------------------------
        if Nper_kernel > 0 : 
            kernel = kernel_base[kmax/2-k/2:kmax/2+k/2+1,
                                 kmax/2-k/2:kmax/2+k/2+1]
            kernel = kernel_loops(kernel*i_max_d*2.0,1.0)
            kernel *= 8*i_max_d*i_max_d*i_max_d # kernel / h**3
        
            # --------------------------------
            # paint each particle on the image
            # --------------------------------
            for pind in xrange(start_ind,end_ind) : 
                x,y,h,qt = [xs[pind],ys[pind],hs[pind],qts[pind]]
                
                # set the minimum h to be equal to half pixel width
                #                h = max_d_curr*.5
                #h = max(h,0.55*dx)
                
                # particle pixel center
                xpos = physical_to_pixel(x,xmin,dx)
                ypos = physical_to_pixel(y,ymin,dy)
    
                left  = xpos-k/2
                upper = ypos-k/2

                for i in xrange(0,k) : 
                    for j in xrange(0,k): 
                        if ((i+left>=0) and (i+left < nx) and (j+upper >=0) and (j+upper<ny)) : 
                            image[(i+left),(j+upper)] += kernel[i,j]*qt


            start_ind = end_ind

    return image
  
N = 20
x = y = z = hs= qts = mass = rhos = np.random.rand(N)
nx=ny=100
args = (x, y, qts,hs, nx, ny, 0.0, 1.0, 0.0, 1.0,1)

template_kernel_cpu(*args)
from compare_perf import compare_perf        

compare_perf(template_kernel_cpu, args)

########NEW FILE########
__FILENAME__ = tensor_rotation
import numpy as np 

#
# Tensor rotation
# from Peter Mortensen's stackoverflow question 
# @ http://stackoverflow.com/questions/4962606/fast-tensor-rotation-with-numpy/18301915
#

n = 9 
def rotT_loops(T, g):
    Tprime = np.zeros((n,n,n,n))
    for i in range(n):
        for j in range(n):
            for k in range(n):
                for l in range(n):
                    for ii in range(n):
                        for jj in range(n):
                            for kk in range(n):
                                for ll in range(n):
                                    gg = g[ii,i]*g[jj,j]*g[kk,k]*g[ll,l]
                                    Tprime[i,j,k,l] = Tprime[i,j,k,l] + gg*T[ii,jj,kk,ll]
    return Tprime

def rotT_numpy(T, g):
  """
  Accepted response on stack overflow by phillip
  """
  gg = np.outer(g, g)
  gggg = np.outer(gg, gg).reshape(4 * g.shape)
  axes = ((0, 2, 4, 6), (0, 1, 2, 3))
  return np.tensordot(gggg, T, axes)

T = np.random.randn(n,n,n,n)
g = np.random.randn(n,n)

from compare_perf import compare_perf 
compare_perf(rotT_loops, [T, g], extra = {'numpy_tensordot': rotT_numpy}, numba= False, backends=('c', 'openmp'), cpython=True)


def rotT_par(T, g):
    def compute_elt(i,j,k,l):
      total = 0.0
      for ii in range(n):
        for jj in range(n):
          for kk in range(n):
            for ll in range(n):
              gg = g[ii,i]*g[jj,j]*g[kk,k]*g[ll,l]
              total += gg*T[ii,jj,kk,ll]
      return total 
    return np.array([[[[compute_elt(i,j,k,l) 
	                for k in xrange(n)] 
			for l in xrange(n)] 
			for j in xrange(n)]
			for i in xrange(n)])
compare_perf(rotT_par, [T, g], extra = {'numpy_tensordot': rotT_numpy}, numba= False, backends=('c', 'openmp'), cpython = True)

########NEW FILE########
__FILENAME__ = timer

import cStringIO
import os  
import sys 
import time
import tempfile 

class timer(object):
  def __init__(self, name = None, newline = True, 
               suppress_stdout = True, 
               suppress_stderr = True,
               propagate_exceptions = False):
    self.name = name 
    self.start_t = time.time()
    self.newline = newline
    self.suppress_stdout = suppress_stdout
    self.suppress_stderr = suppress_stderr
    self.propagate_exceptions = propagate_exceptions 
  def __enter__(self):
    if self.suppress_stdout:
      stdout_newfile = tempfile.NamedTemporaryFile()
      self.prev_stdout_fd = os.dup(sys.stdout.fileno())
      os.dup2(stdout_newfile.fileno(), sys.stdout.fileno())
      self.prev_stdout = sys.stdout
  
    if self.suppress_stderr:
      stderr_newfile = tempfile.NamedTemporaryFile()
      self.prev_stderr_fd = os.dup(sys.stderr.fileno())
      os.dup2(stderr_newfile.fileno(), sys.stderr.fileno())
      self.prev_stderr = sys.stderr 
    
    self.start_t = time.time()

  
  def elapsed(self):
    return time.time() - self.start_t
  
  def __exit__(self, exc_type, exc_value, traceback):
    t = self.elapsed()
    if self.suppress_stdout:
      os.dup2(self.prev_stdout_fd, self.prev_stdout.fileno())
    if self.suppress_stderr:
      os.dup2(self.prev_stderr_fd, self.prev_stderr.fileno())
    if self.newline:
      print 
    s = "Elapsed time: " if self.name is None else "%s : " % self.name 
    if exc_type is None:
      s += "%0.4f" % t
    else:
      name = str(exc_type) if exc_type.__name__ is None else exc_type.__name__
      s += "FAILED with %s '%s'" % (name, exc_value)
    print s 
    # don't raise exceptions
    if self.propagate_exceptions:
      return False 
    else:
      return exc_type is not KeyboardInterrupt
    


########NEW FILE########
__FILENAME__ = wald
from parakeet import jit, config 
import time 
import numpy as np 
def wald(v, a, rands, u_rands, sigma, accum):
    mu = a / v
    lam = a**2 / sigma**2
    y = rands[:, accum]**2
    x = mu + (mu**2. * y) / (2.*lam) - mu / (2.*lam) * np.sqrt(4.*mu*lam*y + mu**2. * y**2.)
    z = u_rands[:, accum]
    return x


def rep(f, n = 1000, d = 10000):
  rands = np.random.randn(d, 1)
  urands = np.random.rand(d, 1)
  for i in xrange(n):
    f(1,2,rands,urands,1,0)

n = 1000

t = time.time()
rep(wald, n)
py_t = time.time() - t 


waldjit = jit(wald)
#warmup
rep(waldjit, 1)
t = time.time()
rep(waldjit, n)
par_t = time.time() - t 

config.value_specialization = False 
rep(waldjit, 1)
t = time.time()
rep(waldjit, n)
par_t_no_specialization = time.time() - t 

print "Python time per call:", 1000 * (py_t / n), "ms"
print "Parakeet time w/ value specialization:",  1000 * (par_t  / n), "ms"
print "Parakeet time w/out value specialization", 1000 * (par_t_no_specialization  / n), "ms"


########NEW FILE########
__FILENAME__ = convolution
import numpy as np 
from parakeet import jit 
# Simple convolution of 3x3 patches from a given array x
# by a 3x3 array of filter weights
 
@jit
def conv_3x3_trim(x, weights):
  def compute_pixel(i,j):
    total = 0
    for ii in xrange(3):
        for jj in xrange(3):
          total += weights[ii,jj] * x[i-ii+1, j-jj+1]
    return total 

  return np.array([[compute_pixel(i,j)
                    for j in xrange(1, x.shape[1] -2)]
                    for i in xrange(1, x.shape[0] -2)])
 

x = np.random.randn(50,50)
w = np.random.randn(3,3)

print "Input", x
print "Convolved output", conv_3x3_trim(x,w)



########NEW FILE########
__FILENAME__ = finite-difference
import numpy as np 
from parakeet import jit 

@jit 
def fdtd(input_grid, steps):
    grid = input_grid.copy()
    old_grid = np.zeros_like(input_grid)
    previous_grid = np.zeros_like(input_grid)

    l_x = grid.shape[0]
    l_y = grid.shape[1]

    for i in range(steps):
        previous_grid[:, :] = old_grid
        old_grid[:, :] = grid 
        for x in range(l_x):
            for y in range(l_y):
                grid[x,y] = 0.0
                if 0 < x+1 and x + 1 < l_x:
                    grid[x,y] += old_grid[x+1,y]
                if 0 < x-1 and x- 1 < l_x:
                    grid[x,y] += old_grid[x-1,y]
                if 0 < y+1 and y+1< l_y:
                    grid[x,y] += old_grid[x,y+1]
                if 0 < y-1 and y-1< l_y:
                    grid[x,y] += old_grid[x,y-1]

                grid[x,y] /= 2.0
                grid[x,y] -= previous_grid[x,y]

    return grid

N = 5
steps = 10 
input_grid = np.random.randn(N,N).astype('float32')

fdtd(input_grid, steps)

########NEW FILE########
__FILENAME__ = growcut

# Original authors: Nathan Faggian, Stefan van der Walt, Aron Ahmadia, Olivier Grisel
# https://github.com/stefanv/growcut_py
# ...simplified and made more compact for Parakeet 

import numpy as np
import parakeet 

@parakeet.jit 
def growcut(image, state, window_radius):
    height = image.shape[0]
    width = image.shape[1]
    def attack_cell(i,j):
        winning_colony = state[i, j, 0]
        defense_strength = state[i, j, 1]
        for jj in xrange(max(0, j -  window_radius), 
                         min(width, j+window_radius+1)):
            for ii in xrange(max(0, i - window_radius), 
                             min(height, i + window_radius + 1)):
                if ii != i or jj != j:
                    ssd = np.sum( (image[i,j,:] - image[ii, jj, :]) ** 2)
                    gval = 1.0 - np.sqrt(ssd) / np.sqrt(3.0)
                    attack_strength = gval * state[ii, jj, 1]
    
                    if attack_strength > defense_strength:
                        defense_strength = attack_strength
                        winning_colony = state[ii, jj, 0]
        return np.array([winning_colony, defense_strength])
    return np.array([[attack_cell(i, j) 
                      for i in xrange(height)] 
                      for j in xrange(width)])

    
N = 50
dtype = np.double
image = np.zeros((N, N, 3), dtype=dtype)
state = np.zeros((N, N, 2), dtype=dtype)

# colony 1 is strength 1 at position 0,0
# colony 0 is strength 0 at all other positions
state[0, 0, 0] = 1
state[0, 0, 1] = 1

window_radius = 10

state_next = growcut(image, state, window_radius)


########NEW FILE########
__FILENAME__ = kmeans
from parakeet import jit 
import numpy as np 


def dist(x,y):
  return ((x-y)**2).sum()

@jit 
def kmeans(X, k, niters = 10):
  C = X[:k, :]
  for _ in xrange(niters):
    A = np.array([np.argmin([dist(x,c) for c in C]) for x in X])
    C = np.array([np.mean(X[A == i, :], axis = 0) for i in xrange(k)])
  return C


n, d = 10**3, 100
X = np.random.randn(n,d)
k = 5

C = kmeans(X, k, niters=20) 
print "Clusters:", C

########NEW FILE########
__FILENAME__ = matmult
import parakeet
import numpy as np 

@parakeet.jit
def matmult(X,Y):
  return np.array([[np.dot(x,y) for y in Y.T] for x in X])


n, d = 120, 120
m = 120
dtype = 'float64'
X = np.random.randn(m,d).astype(dtype)
Y = np.random.randn(d,n).astype(dtype)
Z = matmult(X,Y)

########NEW FILE########
__FILENAME__ = rosenbrock

# Rosenbrock function derivative
# 
# Copied from https://github.com/numfocus/python-benchmarks/blob/master/rosen_der/rosen_der_python.py
# Original authors: Travis Oliphant (NumPy version) & Serge Guelton (loops version)
#

import numpy as np 
from parakeet import jit 

@jit 
def rosenbrock_derivative(x):
  der = np.empty_like(x)
  der[1:-1] = (+ 200 * (x[1:-1] - x[:-2] ** 2)
               - 400 * (x[2:] - x[1:-1] ** 2) * x[1:-1]
               - 2 * (1 - x[1:-1]))
  der[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0])
  der[-1] = 200 * (x[-1] - x[-2] ** 2)
  return der

N = 12
x = np.arange(N) / float(N)
print "Input: ", x
print "Deriv(Rosenbrock):", rosenbrock_derivative(x)


########NEW FILE########
__FILENAME__ = simple_avg
from parakeet import jit  

@jit 
def avg(x,y,z):
  """
  Return the average of three scalars or arrays
  (gets optimized into single traversal)
  """
  return (x + y + z) / 3.0

import numpy as np 

N = 20
x = np.random.randn(N)
y = np.random.randn(N)
z = np.random.randn(N)

assert np.allclose(avg(x,y,z), (x+y+z)/3.0)




########NEW FILE########
__FILENAME__ = simple_regression
from parakeet import jit 

def covariance(x,y):
  return ((x-x.mean()) * (y-y.mean())).mean()

@jit 
def fit_simple_regression(x,y):
  slope = covariance(x,y) / covariance(x,x)
  offset = y.mean() - slope * x.mean() 
  return slope, offset

import numpy as np 

N = 10**4
x = np.random.randn(N).astype('float64')
slope = 903.29
offset = 102.1
y = slope * x + offset

slope_estimate, offset_estimate = fit_simple_regression(x,y)

print "Expected slope =", slope, "offset =", offset
print "Parakeet slope =", slope_estimate, "offset =", offset_estimate

########NEW FILE########
__FILENAME__ = sum_loop

from parakeet import jit 

@jit
def sum_loop(xs):
    total = 0
    for x in xs:
        total += x
    return total 

xs = [1,2,3,4,5]
print "Python result", sum(xs)
print "Parakeet result", sum_loop(xs)

########NEW FILE########
__FILENAME__ = tensor_rotation
import numpy as np 

#
# Tensor rotation
# from Peter Mortensen's stackoverflow question 
# @ http://stackoverflow.com/questions/4962606/fast-tensor-rotation-with-numpy/18301915
#


from parakeet import jit 

@jit 
def rotT_loops(T, g):
    def compute_elt(i,j,k,l):
      total = 0
      for ii in range(3):
        for jj in range(3):
          for kk in range(3):
            for ll in range(3):
              gg = g[ii,i]*g[jj,j]*g[kk,k]*g[ll,l]
              total += gg*T[ii,jj,kk,ll]
      return total 
    return np.array([[[[compute_elt(i,j,k,l) 
	                for i in xrange(3)] 
			for j in xrange(3)]
			for k in xrange(3)]
			for l in xrange(3)])

T = np.random.randn(3,3,3,3)
g = np.random.randn(3,3)

#parakeet.config.print_specialized_function = True 
#parakeet.config.print_indexified_function = True 
#parakeet.config.print_loopy_function = True
rotT_loops(T,g)

########NEW FILE########
__FILENAME__ = array_write_analysis

from collect_vars import collect_var_names, collect_var_names_from_exprs
from escape_analysis import escape_analysis
from syntax_visitor import SyntaxVisitor


class ArrayWriteAnalysis(SyntaxVisitor):
  def __init__(self, fn, fresh_alloc_args = set([])):
    self.fn = fn 
    self.fresh_alloc_args = fresh_alloc_args
    
  def visit_fn(self, fn):
    escape_info = escape_analysis(fn, self.fresh_alloc_args)
    self.may_alias = escape_info.may_alias 
    SyntaxVisitor.visit_fn(self, fn)
    self.writes = set([])
    
  def visit_Assign(self, stmt):
    if stmt.lhs.__class__ is Tuple:
      for elt in stmt.lhs.elts:
        self.visit_Assign(elt)
    elif stmt.lhs.__class__ is Index:
      for name in collect_var_names(stmt.lhs.value):
        self.writes.append(name)
        for alias_name in self.may_alias[name]:
          self.writes.append(alias_name)
      
    
  def visit_Index(self):
    pass 
   

########NEW FILE########
__FILENAME__ = collect_vars
from .. syntax import Var, Tuple
from syntax_visitor import SyntaxVisitor

class SetCollector(SyntaxVisitor):
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.var_names = set([])

  def visit_Var(self, expr):
    self.var_names.add(expr.name)

def collect_var_names(expr):
  collector = SetCollector()
  collector.visit_expr(expr)
  return collector.var_names

def collect_var_names_from_exprs(exprs):
  collector = SetCollector()
  collector.visit_expr_list(exprs)
  return collector.var_names

class ListCollector(SyntaxVisitor):
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.var_names = []

  def visit_Var(self, expr):
    self.var_names.append(expr.name)

def collect_var_names_list(expr):
  collector = ListCollector()
  collector.visit_expr(expr)
  return collector.var_names



def collect_binding_names(lhs):
  lhs_class = lhs.__class__
  if lhs_class is Var:
    return [lhs.name]
  elif lhs.__class__ is Tuple:
    combined = []
    for elt in lhs.elts:
      combined.extend(collect_binding_names(elt))
    return combined
  else:
    return []

class CollectBindings(SyntaxVisitor):
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.bindings = {}

  def bind(self, lhs, rhs):
    if lhs.__class__ is Var:
      self.bindings[lhs.name] = rhs
    elif lhs.__class__ is Tuple:
      for elt in lhs.elts:
        self.bind(elt, rhs)

  def visit_Assign(self, stmt):
    self.bind(stmt.lhs, stmt.rhs)

def collect_bindings(fn):
  return CollectBindings().visit_fn(fn)

########NEW FILE########
__FILENAME__ = contains
from ..ndtypes import ScalarT, PtrT, NoneT, TupleT, FnT, ClosureT 
from .. syntax import Adverb, ParFor, Closure, UntypedFn, TypedFn, ArrayExpr  

from syntax_visitor import SyntaxVisitor

def memoize(analyzer):
  _cache = {}
  def memoized(fn_arg): 
    key = analyzer, fn_arg.cache_key
    if key in _cache:
      return _cache[key]
    result = analyzer(fn_arg)
    _cache[key] = result 
    return result 
  return memoized  
  

@memoize 
def contains_structs(fn):
  for t in fn.type_env.itervalues():
    if not isinstance(t, (ScalarT, PtrT, NoneT)):
      return True
  return False

class Yes(Exception):
  pass 

class ContainsSlices(SyntaxVisitor):
  def visit_Index(self, expr):
    if isinstance(expr.index.type, ScalarT):
      idx_types = [expr.index.type]
    elif isinstance(expr.index.type, TupleT):
      idx_types = expr.index.type.elt_types
    else: 
      raise Yes()
    if not all(isinstance(t, ScalarT) for t in idx_types):
      raise Yes()

@memoize 
def contains_slices(fn):
  try:
    ContainsSlices().visit_fn(fn)
    return False 
  except Yes:
    return True
  
class ContainsLoops(SyntaxVisitor):
  def visit_ForLoop(self, expr):
    raise Yes()
  
  def visit_While(self, expr):
    raise Yes()

def contains_loops(fn):
  try:
    ContainsLoops().visit_fn(fn)
    return False 
  except Yes:
    return True
  

class ContainsCalls(SyntaxVisitor):
  def visit_Call(self, expr):
    raise Yes()
  
  def visit_TypedFn(self, expr):
    if contains_calls(expr):
      raise Yes()
  
@memoize 
def contains_calls(fn):
  try: 
    ContainsCalls().visit_fn(fn)
    return False 
  except:
    return True 

class ContainsAdverbs(SyntaxVisitor):
  def visit_expr(self, expr):
    if isinstance(expr, Adverb):
      raise Yes()
    SyntaxVisitor.visit_expr(self, expr)
  
  def visit_TypedFn(self, expr):
    if contains_adverbs(expr):
      raise Yes()
  
@memoize 
def contains_adverbs(fn):
  try:
    ContainsAdverbs().visit_fn(fn)
    return False
  except Yes:
    return True

class ContainsParFor(SyntaxVisitor):
  def visit_stmt(self, stmt):
    if stmt.__class__ is ParFor:
      raise Yes()
    SyntaxVisitor.visit_stmt(self, stmt)
  
  
  def visit_TypedFn(self, expr):
    if contains_parfor(expr):
      raise Yes()

@memoize 
def contains_parfor(fn):
  try:
    ContainsParFor().visit_fn(fn)
    return False 
  except Yes:
    return True 

class ContainsFunctions(SyntaxVisitor):
  def visit_expr(self, expr):
    if isinstance(expr, (UntypedFn, TypedFn, Closure)) or isinstance(expr.type, (FnT, ClosureT)):
      raise Yes()
    SyntaxVisitor.visit_expr(expr)

@memoize 
def contains_functions(fn):
  try:
    ContainsFunctions().visit_fn(fn)
    return False 
  except Yes:
    return True 

class ContainsAlloc(SyntaxVisitor):
  def visit_Alloc(self, _):
    raise Yes()
  
  def visit_AllocArray(self, _):
    raise Yes()
  
  def visit_TypedFn(self, fn):
    if contains_alloc(fn):
      raise Yes()  
      
@memoize 
def contains_alloc(fn):
  try:
    ContainsAlloc().visit_fn(fn) 
    return False 
  except Yes:
    return True

class ContainsArrayOperators(SyntaxVisitor):
  def visit_expr(self, expr):
    """
    We're assuming that array expressions only occur on RHS of assignments
    and aren't nested. It's up to Simplify to de-nest them
    """
    if isinstance(expr, ArrayExpr):
      raise Yes()
  
@memoize 
def contains_array_operators(fn):
  try:
    ContainsArrayOperators().visit_fn(fn) 
    return False 
  except Yes:
    return True        
########NEW FILE########
__FILENAME__ = escape_analysis

from .. import config
from .. ndtypes import ScalarT, ArrayT, PtrT, TupleT, ClosureT, FnT, SliceT, NoneT
from .. syntax import Var, Attribute, Tuple 
from syntax_visitor import SyntaxVisitor

empty = set([])

def collect_nonscalar_names(expr):
  if expr is None or isinstance(expr.type, ScalarT):
    return []
  elif expr.__class__ is Var:
    return [expr.name]
  else:
    return collect_nonscalar_names_from_list(expr.children())

def collect_nonscalar_names_from_list(exprs):
  result = []
  for expr in exprs:
    result.extend(collect_nonscalar_names(expr))
  return result

class EscapeAnalysis(SyntaxVisitor):
  """
  A very imprecise combined escape and alias analysis. 
  Rough idea: whenever you assign some 
     x = expr(y,z)
  then take all the non-scalar values and unify them into 
  a single alias set along with all the previous variable
  names in the alias sets of y and z.  
  """
  
  def __init__(self, fresh_alloc_args = set([])):
    self.fresh_alloc_args = fresh_alloc_args
    self.immutable = set([])
    self.may_alias = {}
    self.may_escape = set([])
    self.may_return = set([])
    
  
  def nested_mutable_types(self, t):
    if isinstance(t, (PtrT, ArrayT)):
      return set([t])
    elif isinstance(t, TupleT):
      result = set([])
      for elt_t in t.elt_types:
        result.update(self.nested_mutable_types(elt_t))
      return result 
    elif isinstance(t, ClosureT):
      result = set([])
      for elt_t in t.arg_types:
        result.update(self.nested_mutable_types(elt_t))
      return result
    else:
      return set([])
    
  def immutable_type(self, t):
    return isinstance(t, (ScalarT, NoneT, SliceT, FnT)) or len(self.nested_mutable_types(t)) == 0
  
  def immutable_name(self, name):
    if name in self.immutable:
      return True 
    return self.immutable_type(self.type_env.get(name))
  
  def visit_fn(self, fn):

    all_scalars = True 
    self.type_env = fn.type_env 
    # every name at least aliases it self
    for (name,t) in fn.type_env.iteritems():

      if self.immutable_type(t):
        self.immutable.add(name)
      else:
        self.may_alias[name] = set([name])
        all_scalars = False

    if all_scalars:
      return  
    
    # every arg also aliases at least all the other input of the same type 
    # unless we were told it's a freshly allocated input
    
      
    reverse_type_mapping = {}
      
    for name in fn.arg_names:
      if name not in self.fresh_alloc_args:
        t = fn.type_env[name]
        for nested_t in self.nested_mutable_types(t):
          reverse_type_mapping.setdefault(nested_t, set([])).add(name)

      
    for name in fn.arg_names:
      # 
      # Every input argument should alias the other inputs of the same type 
      # if they are both arrays or tuples which contain arrays 
      # We must exclude, however, arguments which we're told explicitly were
      # freshly allocated before we entered the function 
      if name not in self.fresh_alloc_args:
        t = fn.type_env[name]
        for nested_t in self.nested_mutable_types(t):
          self.may_alias[name].update(reverse_type_mapping[nested_t])

    self.visit_block(fn.body)
    
    # once we've accumulated all the aliases
    # anyone who points into the input data 
    # is also considered to be escaping 
    for name in fn.arg_names:
      if name not in self.immutable and name not in self.fresh_alloc_args:
        self.may_escape.update(self.may_alias[name])
    
    if config.print_escape_analysis: 
      print "[EscapeAnalysis] In function %s" % (fn.cache_key, )
      print "-------"
      print fn 
      print 
      print "aliases"
      print "-------"
      for (k,aliases) in sorted(self.may_alias.items(), key = lambda (k,_): k):
        print "  %s => %s" % (k, aliases)
      print 
      print "escape set"
      print "----------"
      print sorted(self.may_escape)

  def mark_escape(self, name):
    if name not in self.immutable:
      for alias in self.may_alias[name]:
        self.may_escape.add(alias)
  
  def mark_escape_list(self, names):
    for name in names:
        self.mark_escape(name)
  
  
  def mark_return(self, name):
    if name not in self.immutable:
      for alias in self.may_alias[name]:
        self.may_return.add(alias)
  
  
  def mark_return_list(self, names):
    for name in names:
        self.mark_return(name)
  
  def update_aliases(self, lhs_name, rhs_names):
    """
    Once we've gotten all the RHS names being assigned to the 
    LHS var, we do a recursively lookup into may_alias since
    we may have code like:
      a = tuple(b,c)
      d = tuple(a,e) 
    and we want the alias set of d to be {a,e,b,c}
    """
    if lhs_name not in self.immutable:
      combined_set = self.may_alias[lhs_name]
      for rhs_name in rhs_names:
        if not self.immutable_name(rhs_name):
          combined_set.update(self.may_alias[rhs_name])
      for alias in combined_set:
        self.may_alias[alias] = combined_set
      return combined_set  
  
  def update_escaped(self, lhs_name, rhs_alias_set):
    if lhs_name not in self.immutable and \
       any(alias in self.may_escape for alias in rhs_alias_set):
        self.may_escape.update(rhs_alias_set.difference(self.immutable))
        self.may_escape.add(lhs_name)
        
  def update_return(self, lhs_name, rhs_alias_set):
    if lhs_name not in self.immutable and \
       any(alias in self.may_return for alias in rhs_alias_set):
        self.may_return.update(rhs_alias_set.difference(self.immutable))
        self.may_return.add(lhs_name)
    
  
  def visit_Call(self, expr):
    self.visit_expr(expr.fn)
    self.mark_escape_list(collect_nonscalar_names_from_list(expr.args))

  def visit_Map(self, expr):
    self.visit_expr(expr.fn)
    self.visit_if_expr(expr.axis)
    self.mark_escape_list(collect_nonscalar_names_from_list(expr.args))

  def visit_Reduce(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.combine)
    self.visit_if_expr(expr.axis)
    self.visit_if_expr(expr.init)
    self.mark_escape_list(collect_nonscalar_names_from_list(expr.args))

  def visit_Scan(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.combine)
    self.visit_if_expr(expr.axis)
    self.visit_if_expr(expr.init)
    self.mark_escape_list(collect_nonscalar_names_from_list(expr.args))
  
  def visit_OuterMap(self, expr):
    self.visit_expr(expr.fn)
    self.visit_if_expr(expr.axis)
    self.mark_escape_list(collect_nonscalar_names_from_list(expr.args))

  def collect_lhs_names(self, expr):
    if expr.__class__ is Var:
      return [expr.name]
    elif expr.__class__ is Attribute:
      return self.collect_lhs_names(expr.value)
    elif expr.__class__ is Tuple:
      combined = []
      for elt in expr.elts:
        combined.extend(self.collect_lhs_names(elt))
    else:
      return []
  
  def visit_Closure(self, expr):
    self.mark_escape_list(collect_nonscalar_names_from_list(expr.args))
  
  def visit_Assign(self, stmt):
    lhs_names = set(self.collect_lhs_names(stmt.lhs))
    rhs_names = set(collect_nonscalar_names(stmt.rhs))
 
    for lhs_name in lhs_names:
      self.update_aliases(lhs_name, rhs_names)

  def visit_Return(self, stmt):
    arrays = collect_nonscalar_names(stmt.value)
    self.mark_escape_list(arrays)
    self.mark_return_list(arrays)
 
  def visit_merge(self, merge):
    for (name, (l,r)) in merge.iteritems():
      if l.__class__ is Var: 
        left_aliases = self.may_alias.get(l.name, empty)
      else:
        left_aliases = empty 
      if r.__class__ is Var: 
        right_aliases = self.may_alias.get(r.name, empty)
      else:
        right_aliases = empty
      combined_set = self.update_aliases(name, left_aliases.union(right_aliases))
      self.update_escaped(name, combined_set)
      self.update_return(name, combined_set)

_cache = {}
def escape_analysis(fundef, fresh_alloc_args = set([])):
  key = fundef.cache_key, frozenset(fresh_alloc_args)
  if key in _cache:
    return _cache[key]
  else: 
    analysis = EscapeAnalysis(fresh_alloc_args = fresh_alloc_args)
    analysis.visit_fn(fundef)
    _cache[key] = analysis
    return analysis

def may_alias(fundef):
  return escape_analysis(fundef).may_alias 

def may_escape(fundef):
  return escape_analysis(fundef).may_escape


# TODO: 
# actually generate all this info! 
from collections import namedtuple 
FunctionInfo = namedtuple("FunctionInfo", 
  ("pure", # nothing in this function will ever write to any array's data
   "allocates",        # does this function allocate new arrays?
   "unused", # which local variables never get used?
   
   "may_alias",       # alias relationships between local variables
   "may_escape",      # which variables may alias a returned array's data?
    
   "may_read_arrays",  # which arrays may have their data read in this function? 
   "may_write_arrays",   # which arrays may have their data written to in this function?
   "always_returns_fresh_array", # is the array returned always something locally allocated?
  ))


########NEW FILE########
__FILENAME__ = find_local_arrays
from .. ndtypes import ArrayT, PtrT 
from .. syntax import (Var, Alloc, ArrayView, Array, Struct, AllocArray, 
                       Map, IndexMap, OuterMap, Scan, IndexScan, 
                       ConstArray, ConstArrayLike
                       )
from syntax_visitor import SyntaxVisitor
 
# from syntax import Adverb   

array_alloc_classes = (AllocArray, Array, 
                       Map, IndexMap, OuterMap, 
                       Scan, IndexScan, 
                       ConstArray, ConstArrayLike)

class FindLocalArrays(SyntaxVisitor):
  def __init__(self):
    # hash table mapping from variable names to
    # statements allocating space
    self.local_allocs = {}

    # hash table mapping from variable names to
    # places where we create array views containing
    # locally allocated data pointers
    self.local_arrays = {}
    
    # associate each local array 
    # with its data allocation 
    self.array_to_alloc = {}
  
  def visit_merge(self, merge):
    """
    If both sides of a flow merge are a locally created 
    array originating on the same statement, then 
    also mark the new phi-bound variable as also being 
    that local array. 
    
    Similarly, if both sides are the same local pointer 
    allocation, then propagate that information to the 
    new variable being created by a phi-node
    """
    for (new_name, (left, right)) in merge.iteritems():
      if left.__class__ is Var and right.__class__ is Var:
        left_name = left.name 
        right_name = right.name 
        if left.type.__class__ is ArrayT and \
           left_name in self.local_arrays and \
           right_name in self.local_arrays:
          left_stmt = self.local_arrays[left_name]
          right_stmt = self.local_arrays[right_name]
          if left_stmt == right_stmt:
            self.local_arrays[new_name] = left_stmt 
            if left.name in self.array_to_alloc_name:
              self.array_to_alloc[new_name] = \
                  self.array_to_alloc[left.name]
        elif left.type.__class__ is PtrT and \
             left_name in self.local_allocs and \
             right_name in self.local_allocs:
          left_stmt = self.local_allocs[left_name]
          right_stmt = self.local_allocs[right_name]
          if left_stmt == right_stmt:
            self.local_allocs[new_name] = left_stmt
  
  def visit_Assign(self, stmt):
    if stmt.lhs.__class__ is Var:
      lhs_name = stmt.lhs.name 
      rhs_class = stmt.rhs.__class__
      if rhs_class is Alloc:
        self.local_allocs[lhs_name] = stmt
      elif rhs_class is ArrayView and \
          stmt.rhs.data.__class__ is Var:
        data_name = stmt.rhs.data.name 
        if data_name in self.local_allocs:
          self.local_arrays[lhs_name] = stmt
          self.array_to_alloc[lhs_name] = data_name  
      elif rhs_class is Struct and \
          stmt.rhs.type.__class__ is ArrayT and \
          stmt.rhs.args[0].__class__ is Var:
        data_name = stmt.rhs.args[0].name
        if data_name in self.local_allocs:
          self.local_arrays[lhs_name] = stmt
          self.array_to_alloc[lhs_name] = data_name 
      elif rhs_class in array_alloc_classes:
        self.local_arrays[stmt.lhs.name] = stmt

########NEW FILE########
__FILENAME__ = index_elim_analysis
from dsltools import Node 

from .. ndtypes import Float32, Float64, Int32, Int64
from .. syntax import Var, Range, ConstArray, ConstArrayLike, IndexMap
from collect_vars import collect_var_names 
from syntax_visitor import SyntaxVisitor 


class AbstractValue(Node):
  _members = []

class ConstValue(AbstractValue):
  """
  All elements are the same constant 
  """
  _members = ['value', 'type']

zeros_float32 = ConstValue(0.0, Float32)
zeros_float64 = ConstValue(0.0, Float64)
zeros_int32 = ConstValue(0, Int32)
zeros_int64 = ConstValue(0, Int64)


class ConstElts(AbstractValue):
  """
  All elements known, represented as a numpy array 
  """ 
  _members = ['array']

class RangeArray(AbstractValue):
  """
  Result of a range expression 
  """
  _members = ['start', 'step', 'type']

class IndexMapResult(AbstractValue):
  _members = ['fn']

class Unknown(AbstractValue):
  _members = []
  
unknown = Unknown()

class IndexElimAnalysis(SyntaxVisitor):
    def __init__(self):
      # map variables to (start, stop, step)  
      self.index_vars = {}
      
      # map array names to abstract values 
      self.array_values = {}
      
      # map each array name to a set of names which share the same 
      # memory 
      self.linked_arrays = {}
      
    def visit_Assign(self, stmt):
      lhs = stmt.lhs
      rhs = stmt.rhs 
      
      if lhs.__class__ is Var:
        lhs_name = stmt.lhs.name
        
        rhs_class = rhs.__class__ 
        if rhs_class is Var:
          linked_set = self.linked_arrays.get(lhs_name, set([lhs_name]))
          rhs_name = stmt.rhs.name
          linked_set.add(rhs_name)
          self.linked_arrays[lhs_name] = linked_set 
          self.linked_arrays[rhs_name] = linked_set
          if lhs_name in self.array_values:
            self.array_values[rhs_name] = self.array_values[lhs_name]
        elif rhs_class is Range:
          self.array_values[lhs_name] = RangeArray(rhs.start, rhs.step, rhs.type.elt_type)
        elif rhs_class in (ConstArray, ConstArrayLike):
          self.array_values[lhs_name] = ConstValue(value = rhs.value, type = rhs.type) 
        elif rhs_class is IndexMap:
          self.array_values[lhs_name] = IndexMapResult(fn = rhs.fn)
      else:
        # if not a var, might be an index expression 
        for tainted_lhs_name in collect_var_names(lhs):
          for linked_name in self.linked_arrays.get(tainted_lhs_name, set([tainted_lhs_name])):
            self.array_values[linked_name] = unknown 
########NEW FILE########
__FILENAME__ = inline_allowed

from ..syntax import Assign, ExprStmt, If, While, ForLoop, Comment, Return, ParFor  

def all_branches_return(stmt):
  if isinstance(stmt, Return):
    return True 
  elif isinstance(stmt, If):
    return len(stmt.true) > 0 and \
      all_branches_return(stmt.true[-1]) and \
      len(stmt.false) > 0 and \
      all_branches_return(stmt.false[-1])

def can_inline_block(stmts, outer = False):
  for stmt in stmts:
    stmt_class = stmt.__class__
    if stmt_class in (Assign, ExprStmt, ParFor):
      pass
    elif stmt_class is If:
      # if every branch leading from here ends up 
      # returning a value, then it's OK to replace those
      # with variable assignments  
      # ...but only once the Inliner knows how to deal with 
      # control flow...      
      #if outer and all_branches_return(stmt):
      #  return True
      if not can_inline_block(stmt.true, outer = False):
        return False
      if not can_inline_block(stmt.false, outer=False):
        return False 
          
    elif stmt_class in (While, ForLoop):
      if not can_inline_block(stmt.body):
        return False
    elif stmt_class is Comment:
      continue
    
    else:
      assert stmt_class is Return, "Unexpected statement: %s" % stmt
      if not outer:
        return False 
  return True

def can_inline(fundef):
  return can_inline_block(fundef.body, outer = True)
########NEW FILE########
__FILENAME__ = mutability_analysis
from syntax_visitor import SyntaxVisitor

from .. ndtypes  import TupleT, StructT, Type, PtrT, ClosureT, FnT

class TypeBasedMutabilityAnalysis(SyntaxVisitor):
  """
  The cheapest approximation to discovering which 
  fields are mutable is to do an insensitive analysis that
  marks all types which are on the RHS of a field assignment
  or are passed to a function (since other functions might 
  perform arbitrary mutable actions on fields). 
  
  Enter into the analysis via visit_fn, which returns 
  a set of struct types which may be mutable. 
  """
  
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.mutable_types = set([])
  
  def _mark_type(self, t):
    if self._has_mutable_fields(t):
      self.mutable_types.add(t)
      self._mark_children(t)
    elif isinstance(t, PtrT):
      self.mutable_types.add(t)
      

  def _has_mutable_fields(self, t):
    if isinstance(t, StructT):
      return not isinstance(t, (TupleT, ClosureT, FnT))
    
  def _mark_children(self, t):
    """
    For any type other than a Tuple or Closure, try 
    marking all its children 
    """
    for elt_t in t.children():
      self._mark_type(elt_t)
 
  
  """    
  def _mark(self, obj):
    if isinstance(obj, Type):
      self._mark_type(obj)
    elif isinstance(obj, (tuple, list)):
      for child in obj:
        self._mark(child)
  """   
  def visit_merge(self, phi_nodes):
    pass 
    
  def visit_generic_expr(self, expr):
    pass 
  
  def visit_lhs_Tuple(self, expr):
    for e in expr.elts:
      self.visit_lhs(e)
  
  def visit_lhs_Attribute(self, expr):
    assert False, "Considering making attributes immutable"
    self._mark_type(expr.value.type)
  
  def visit_lhs_Index(self, expr):
 
    self._mark_type(expr.value.type)
  
  def visit_Call(self, expr):
    for arg in expr.args:
      """
      The fields of an argument type might change, 
      but since we pass by value the argument itself
      isn't made mutable
      """
      self._mark_type(arg.type)
    
  def visit_fn(self, fn):
    self.mutable_types.clear()
    self.visit_block(fn.body)
    return self.mutable_types

def find_mutable_types(fn):
  return TypeBasedMutabilityAnalysis().visit_fn(fn) 

########NEW FILE########
__FILENAME__ = offset_analysis
from .. import  prims 
from .. syntax import Var, PrimCall, Const
from syntax_visitor import SyntaxVisitor

class OffsetAnalysis(SyntaxVisitor):
  """Determine static offset relationships between variables"""
  
  def __init__(self):
    # map from variable 'x' to list of variable offset pairs
    # [ ('y', 4), ('z', -1), ...] 
    self.known_offsets = {}
    
  def update(self, x, y, k):
    if x == y:
      assert k == 0, \
         "Impossible %s = %s + %d" % (x,y,k)
    
    if x in self.known_offsets:
      x_offsets = self.known_offsets[x]
    else:
      x_offsets = set([])
      
    x_offsets.add( (y,k) )

    if y in self.known_offsets:
      y_offsets = self.known_offsets[y]
    else:
      y_offsets = set([])
      
    for (z, k2) in y_offsets:
      x_offsets.add( (z, k2 + k) )
      
    self.known_offsets[x] = x_offsets
    self.known_offsets[y] = y_offsets
    
  def visit_merge(self, merge):
    for (k, (l,r)) in merge.iteritems():
      if l.__class__ is Var and \
         r.__class__ is Var and \
         l.name in self.known_offsets and \
         r.name in self.known_offsets:
        left = self.known_offsets[l.name]
        right = self.known_offsets[r.name]
        self.known_offsets[k] = left.intersection(right)  
  
  def visit_PrimCall(self, expr):
    if expr.prim is prims.add:
      x, y = expr.args
      if x.__class__ is Var and y.__class__ is Const:
        return (x.name, int(y.value))
      elif y.__class__ is Var and x.__class__ is Const:
        return (y.name, int(x.value))
    elif expr.prim is prims.subtract:
      x, y = expr.args
      if x.__class__ is Var and y.__class__ is Const:
        return (x.name, -int(y.value))
    return None
  
  def visit_Assign(self, stmt):
    if stmt.lhs.__class__ is Var:
      if stmt.rhs.__class__ is PrimCall:
        rhs = self.visit_PrimCall(stmt.rhs)
        if rhs is not None:
          x = stmt.lhs.name
          (y, offset) = rhs
          self.update(x, y, offset)
          self.update(x, y, offset)
          self.update(y, x, -offset)
      elif stmt.rhs.__class__ is Var:
        x = stmt.lhs.name 
        y = stmt.rhs.name
        self.update(x, y, 0)
        self.update(y, x, 0)
  
  def visit_fn(self, fn):
    SyntaxVisitor.visit_fn(self, fn)
    return self.known_offsets


########NEW FILE########
__FILENAME__ = syntax_visitor
from .. syntax import (Expr, Assign, ExprStmt, ForLoop, If, Return, While, Comment, ParFor, 
                       TypedFn, UntypedFn,  Closure, ClosureElt, Select,  
                       Attribute, Const, Index, PrimCall, Tuple, Var, 
                       Alloc, Array, Call, Struct, Shape, Strides, Range, Ravel, Transpose,
                       AllocArray, ArrayView, Cast, Slice, TupleProj, TypeValue,  
                       Map, Reduce, Scan, OuterMap, IndexMap, IndexReduce, IndexScan )

class SyntaxVisitor(object):
  """
  Traverse the statement structure of a syntax block, optionally collecting
  values
  """

  def visit_if_expr(self, expr):
    if isinstance(expr, Expr):
      self.visit_expr(expr)
      
  def visit_Var(self, expr):
    pass

  def visit_Const(self, expr):
    pass
  
  def visit_Fn(self, expr):
    pass 
  
  def visit_ClosureElt(self, expr):
    self.visit_expr(expr.closure)
  
  def visit_Closure(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr_list(expr.args)
    
    
  def visit_Tuple(self, expr):
    for elt in expr.elts:
      self.visit_expr(elt)

  def visit_PrimCall(self, expr):
    for arg in expr.args:
      self.visit_expr(arg)

  def visit_Attribute(self, expr):
    self.visit_expr(expr.value)

  def visit_Index(self, expr):
    self.visit_expr(expr.value)
    self.visit_expr(expr.index)

  def visit_UntypedFn(self, expr):
    pass 
  
  def visit_TypedFn(self, expr):
    pass

  def visit_Alloc(self, expr):
    self.visit_expr(expr.count)

  def visit_Struct(self, expr):
    for arg in expr.args:
      self.visit_expr(arg)

  def visit_Array(self, expr):
    for elt in expr.elts:
      self.visit_expr(elt)
      
  def visit_ArrayView(self, expr):
    self.visit_expr(expr.data)
    self.visit_expr(expr.shape)
    self.visit_expr(expr.strides)
    self.visit_expr(expr.offset)
    self.visit_expr(expr.size)

  def visit_AllocArray(self, expr):
    self.visit_expr(expr.shape)

  def visit_Ravel(self, expr):
    self.visit_expr(expr.array)

  def visit_Reshape(self, expr):
    self.visit_expr(expr.array)
    self.visit_expr(expr.shape)

  def visit_Shape(self, expr):
    self.visit_expr(expr.array)
  
  def visit_ConstArray(self, expr):
    self.visit_expr(expr.shape)
    self.visit_expr(expr.value)
    
  def visit_ConstArrayLike(self, expr):
    self.visit_expr(expr.array)
    self.visit_expr(expr.value)
  
  def visit_Slice(self, expr):
    self.visit_expr(expr.start)
    self.visit_expr(expr.stop)
    self.visit_expr(expr.step)

  def visit_Transpose(self, expr):
    self.visit_expr(expr.array)
    
  def visit_IndexMap(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.shape)

  def visit_IndexReduce(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.combine)
    self.visit_expr(expr.shape)
    self.visit_expr(expr.init)

  def visit_IndexScan(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.combine)
    self.visit_expr(expr.shape)
    self.visit_expr(expr.init)

  def visit_Map(self, expr):
    self.visit_expr(expr.fn)
    self.visit_if_expr(expr.axis)
    for arg in expr.args:
      self.visit_expr(arg)

  def visit_OuterMap(self, expr):
    self.visit_expr(expr.fn)
    self.visit_if_expr(expr.axis)
    for arg in expr.args:
      self.visit_expr(arg)

  def visit_Reduce(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.combine)
    self.visit_if_expr(expr.axis)
    self.visit_if_expr(expr.init)
    for arg in expr.args:
      self.visit_expr(arg)

  def visit_Scan(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.combine)
    self.visit_if_expr(expr.axis)
    self.visit_if_expr(expr.init)
    for arg in expr.args:
      self.visit_expr(arg)
      
  def visit_TupleProj(self, expr):
    return self.visit_expr(expr.tuple)

  def visit_Call(self, expr):
    self.visit_expr(expr.fn)
    for arg in expr.args:
      self.visit_expr(arg)

  def visit_Cast(self, expr):
    return self.visit_expr(expr.value)

  def visit_Range(self, expr):
    self.visit_expr(expr.start)
    self.visit_expr(expr.stop)
    self.visit_expr(expr.step)
  
  def visit_Select(self, expr):
    self.visit_expr(expr.cond)
    self.visit_expr(expr.true_value)
    self.visit_expr(expr.false_value)
    
  def visit_TypeValue(self, expr):
    pass 
  
  def visit_generic_expr(self, expr):
    for v in expr.children():
      self.visit_expr(v)

  _expr_method_names = {
    Var : 'visit_Var', 
    Const : 'visit_Const', 
    PrimCall : 'visit_PrimCall', 
    Attribute : 'visit_Attribute', 
    Index : 'visit_Index', 
    Tuple : 'visit_Tuple', 
    TupleProj : 'visit_TupleProj', 
    Slice : 'visit_Slice', 
    Struct : 'visit_Struct', 
    AllocArray : 'visit_AllocArray', 
    ArrayView : 'visit_ArrayView', 
    Array : 'visit_Array',
    Range : 'visit_Range',
    Ravel : 'visit_Ravel',   
    Transpose : 'visit_Transpose', 
    Shape : 'visit_Shape', 
    Strides : 'visit_Strides', 
    Alloc : 'visit_Alloc', 
    Cast : 'visit_Cast', 
    Call : 'visit_Call', 
    Select : 'visit_Select', 
    Map : 'visit_Map',
    OuterMap : 'visit_OuterMap',
    IndexMap : 'visit_IndexMap', 
    Reduce : 'visit_Reduce',
    IndexReduce : 'visit_IndexReduce',
    Scan : 'visit_Scan', 
    IndexScan : 'visit_IndexScan',
    Closure : 'visit_Closure', 
    ClosureElt : 'visit_ClosureElt', 
    UntypedFn : 'visit_UntypedFn',  
    TypedFn : 'visit_TypedFn', 
    TypeValue : 'visit_TypeValue', 
  }
  
  def visit_expr(self, expr):   
    c = expr.__class__
    if c is Var:  
      return self.visit_Var(expr)
    elif c is Const: 
      return self.visit_Const(expr)
    elif c is PrimCall:
      return self.visit_PrimCall(expr)
    elif c is Tuple:
      return self.visit_Tuple(expr)
    elif c is TupleProj:
      return self.visit_TupleProj(expr) 
    elif c is Index: 
      return self.visit_Index(expr)
    
    # try looking up the method in fast-path dictionary
    method_name = self._expr_method_names.get(c)
    if not method_name:
      # ...otherwise, just construct a new string following the visit_Expr pattern 
      method_name = "visit_" + c.__name__
    
    # if we found a method, call it 
    if method_name:
      method = getattr(self, method_name)
      
    if method:
      return method(expr)

    for child in expr.children():
      self.visit_expr(child)
      
  def visit_expr_list(self, exprs):
    return [self.visit_expr(expr) for expr in exprs]

  def visit_lhs_Var(self, lhs):
    self.visit_Var(lhs)

  def visit_lhs_Tuple(self, lhs):
    self.visit_Tuple(lhs)

  def visit_lhs_Index(self, lhs):
    self.visit_Index(lhs)

  def visit_lhs_Attribute(self, lhs):
    self.visit_Attribute(lhs)

  def visit_lhs(self, lhs):
    c = lhs.__class__
    if c is Var:
      return self.visit_lhs_Var(lhs)
    elif c is Tuple:
      return self.visit_lhs_Tuple(lhs)
    elif c is Index:
      return self.visit_lhs_Index(lhs)
    elif c is Attribute:
      return self.visit_lhs_Attribute(lhs)
    else:
      assert False, "LHS not implemented: %s" % (lhs,)

  def visit_block(self, stmts):
    for s in stmts:
      self.visit_stmt(s)

  def visit_Assign(self, stmt):
    self.visit_lhs(stmt.lhs)
    self.visit_expr(stmt.rhs)

  def visit_merge(self, phi_nodes):
    for (_, (l,r)) in phi_nodes.iteritems():
      self.visit_expr(l)
      self.visit_expr(r)

  def visit_merge_if(self, phi_nodes):
    self.visit_merge(phi_nodes)

  def visit_If(self, stmt):
    self.visit_expr(stmt.cond)
    self.visit_block(stmt.true)
    self.visit_block(stmt.false)
    self.visit_merge_if(stmt.merge)

  def visit_ExprStmt(self, stmt):
    self.visit_expr(stmt.value)

  def visit_Return(self, stmt):
    self.visit_expr(stmt.value)

  def visit_merge_loop_start(self, phi_nodes):
    pass

  def visit_merge_loop_repeat(self, phi_nodes):
    self.visit_merge(phi_nodes)

  def visit_While(self, stmt):
    self.visit_merge_loop_start(stmt.merge)
    self.visit_expr(stmt.cond)
    self.visit_block(stmt.body)
    self.visit_merge_loop_repeat(stmt.merge)
    
    
  def visit_ForLoop(self, stmt):
    self.visit_lhs(stmt.var)
    self.visit_expr(stmt.start)
    self.visit_merge_loop_start(stmt.merge)
    self.visit_expr(stmt.stop)
    self.visit_block(stmt.body)
    self.visit_expr(stmt.step)
    self.visit_merge_loop_repeat(stmt.merge)

  def visit_Comment(self, stmt):
    pass

  def visit_ParFor(self, expr):
    self.visit_expr(expr.fn)
    self.visit_expr(expr.bounds)
  
  _stmt_method_names = {
    Assign : 'visit_Assign', 
    Return : 'visit_Return', 
    While : 'visit_While', 
    ForLoop : 'visit_ForLoop', 
    If : 'visit_If', 
    ExprStmt : 'visit_ExprStmt', 
    ParFor : 'visit_ParFor', 
    Comment : 'visit_Comment',                    
  }
  
  def visit_stmt(self, stmt):
    c = stmt.__class__
    if c is Assign: 
      self.visit_Assign(stmt)
    else:
      getattr(self, self._stmt_method_names[c])(stmt)
    

  def visit_fn(self, fn):
    self.visit_block(fn.body)

########NEW FILE########
__FILENAME__ = usedef
from .. syntax import Var, Tuple, Index, Attribute  
from syntax_visitor import SyntaxVisitor

class StmtPath(object):
  def __init__(self, elts = ()):
    self.elts = tuple(elts) 
  
  def __str__(self):
    return "{%s}" % ", ".join(str(e) for e in self.elts)
  
  def __len__(self):
    return len(self.elts)

  def __getitem__(self, i):
    return self.elts[i]
  
  def __iter__(self):
    return iter(self.elts)
  
  def __lt__(self, other):
    if other.__class__ is not StmtPath:
      other = StmtPath([other])
    n = len(other)
    for (i,c1) in enumerate(self.elts):
      if n <= i:
        return False 
      c2 = other[i]
      if type(c1) is int and type(c2) is int:
        if c1 < c2:
          return True 
        elif c1 > c2:
          return False 
      # if you're not on the same branch of control flow
      else:
        assert type(c1) is bool and type(c2) is bool 
        if c1 != c2:
          return False
    return False
  
  
  def __eq__(self, other):
    if other.__class__ is not StmtPath:
      other = StmtPath([other])
      
    if len(self) != len(other):
      return False
    else:
      for (i,c) in enumerate(self.elts):
        if c != other[i]:
          return False 
      return True 
  
  def __ne__(self, other):
    return not (self == other)
  
  def __le__(self, other):
    return self == other or self < other
  
  def __lte__(self, other):
    return (self < other) or (self == other)
  
  def __gt__(self, other):
    return not (self <= other) 
  
  def __gte__(self, other):
    return not (self < other)

class UseDefAnalysis(SyntaxVisitor):
  """
  Number all the statements and track the creation as well as first and last
  uses of variables
  """
  def __init__(self):
    # map from pointers of statement objects to
    # nested sequential numbering
    self.stmt_paths = {}

    # statement count per scope 
    self.stmt_counters = [0]
    
    # the scopes leading up to the current one 
    self.path_prefix = []
    
    
    # map from variable names to counter number of their
    # first/last usages
    self.first_use = {}
    self.last_use = {}
    self.first_read = {}
    self.last_read = {}
    self.first_write = {}
    self.last_write = {}
    self.created_on = {}
  
  def counter(self):
    return self.stmt_counters[-1]
  
  def inc_counter(self):
    self.stmt_counters[-1] += 1
  
  def push_counter(self):
    self.stmt_counters.append(0)
    
  def pop_counter(self):
    return self.stmt_counters.pop()
  
  def push_scope(self, c = None):
    if c is None:
      c = self.counter()
    self.path_prefix.append(c) 
  
  def pop_scope(self):
    self.path_prefix.pop()
  
  def curr_path(self):
    return StmtPath(self.path_prefix + [self.counter()])

  def precedes_stmt_id(self, stmt_id1, stmt_id2):
    return self.stmt_paths[stmt_id1] < self.stmt_paths[stmt_id2] 
    
  def precedes_stmt(self, stmt1, stmt2):
    return self.stmt_paths[id(stmt1)] < self.stmt_paths[id(stmt2)] 
    
  def visit_fn(self, fn):
    for name in fn.arg_names:
      self.created_on[name] = 0
    SyntaxVisitor.visit_fn(self, fn)

  def visit_lhs(self, expr):
    if expr.__class__ is Var:
      self.created_on[expr.name] = self.curr_path()
    elif expr.__class__ is Tuple:
      for elt in expr.elts:
        self.visit_lhs(elt)
    else:
      assert expr.__class__ in (Attribute, Index)
      assert expr.value.__class__ is Var
      value_name = expr.value.name 
      p = self.curr_path()
      if value_name not in self.first_write:
        self.first_write[value_name] = p 
      self.last_write[value_name] = p

  def vist_merge(self, merge):
    p = self.curr_path()
    for (name, (l,r)) in merge.iteritems():
      self.visit_expr(l)
      self.visit_expr(r)
      self.created_on[name] = p
  
  def visit_Var(self, expr):
    name = expr.name
    p = self.curr_path()
    if name not in self.first_use:
      self.first_use[name] = p
    self.last_use[name]= p

  def visit_block(self, stmts, branch_label = None):
    self.push_scope()
    if branch_label is not None:
      self.push_scope(branch_label)
    SyntaxVisitor.visit_block(self, stmts)
    if branch_label is not None:
      self.pop_scope()
    self.pop_scope()
    
  def visit_If(self, stmt):
    self.visit_expr(stmt.cond)
    self.visit_block(stmt.true, True)
    self.visit_block(stmt.false, False)
    self.visit_merge(stmt.merge)
    
  def visit_stmt(self, stmt):
    self.inc_counter()
    self.stmt_paths[id(stmt)] = self.curr_path()
    SyntaxVisitor.visit_stmt(self, stmt)

########NEW FILE########
__FILENAME__ = use_analysis
 
from .. syntax import Var, Tuple 
from syntax_visitor import SyntaxVisitor 

class FindLiveVars(SyntaxVisitor):
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.live_vars = set([])
    
  def visit_Var(self, expr):
    self.live_vars.add(expr.name)
    
  def visit_lhs(self, expr):
    expr_class = expr.__class__ 
    if expr_class is Var:
      pass 
    elif expr_class is Tuple:
      for elt in expr.elts:
        self.visit_lhs(elt)
    else:
      self.visit_expr(expr)
      
  def visit_fn(self, fn):
    self.live_vars.clear()
    for name in fn.arg_names:
      self.live_vars.add(name)
    self.visit_block(fn.body)
    return self.live_vars

def find_live_vars(fn):
  return FindLiveVars().visit_fn(fn)

class VarUseCount(SyntaxVisitor):
  
  def __init__(self):
    
    SyntaxVisitor.__init__(self)
    self.counts = {}
    
  def visit_Var(self, expr):
    old_count = self.counts.get(expr.name, 0)
    self.counts[expr.name] = old_count + 1 
  
  def visit_ForLoop(self, stmt):
    self.visit_Var(stmt.var)
    SyntaxVisitor.visit_ForLoop(self, stmt)
  
  def visit_lhs(self, expr):
    expr_class = expr.__class__ 
    if expr_class is Var:
      pass 
    elif expr_class is Tuple:
      for elt in expr.elts:
        self.visit_lhs(elt)
    else:
      self.visit_expr(expr)
      
  def visit_fn(self, fn):
    self.counts.clear()
    for name in fn.arg_names:
      self.counts[name] = 1
    self.visit_block(fn.body)
    return self.counts 

def use_count(fn):
  return VarUseCount().visit_fn(fn)
########NEW FILE########
__FILENAME__ = value_range_analysis
from dsltools import  ScopedDict
 

from .. import prims  
from ..ndtypes import NoneT, NoneType, get_rank 
from ..syntax import Var, Const, PrimCall, Tuple, Slice, TupleProj, Attribute, Shape 
 
import numpy as np 
from ..syntax.helpers import unwrap_constant
from syntax_visitor import SyntaxVisitor



class AbstractValue(object):
  def __eq__(self, other):
    return False    

  def __ne__(self, other):
    return not self == other
  
  def __repr__(self):
    return str(self)
  
  def combine(self, other):
    if self == other:
      return self 
    else:
      return any_value 
  
  def widen(self, other):
    if self == other:
      return self 
    else:
      assert False, "widening not implemented for (%s, %)" % (self, other) 
    
  
class Unknown(AbstractValue):
  """
  Top of the lattice 
  """
  
  def __str__(self):
    return "<unknown>"
  
  def combine(self, other):
    return other
  
  def widen(self, other):
    return other  
  
  
unknown_value = Unknown()

class AnyValue(AbstractValue):
  """
  Bottom of the lattice 
  """
  
  def __str__(self):
    return "<any>"
  
  def combine(self, other):
    return any_value 
  
  def widen(self, other):
    return any_value 
  
  def __eq__(self, other):
    return other.__class__ is AnyValue 
  
any_value = AnyValue()

class NoneValue(AbstractValue):
  def __str__(self):
    return "None"
  
  
  def __eq__(self, other):
    return other.__class__ is NoneValue
  
  def combine(self, other):
    if self == other:
      return self 
    else:
      return any_value 
    
  def widen(self, other):
    return self.combine(other)

class Interval(AbstractValue):
  def __init__(self, lower, upper):
    self.lower = lower 
    self.upper = upper 
  
  def combine(self, other):
    if other.__class__ is not Interval:
      return any_value 
    
    lower = min(self.lower, other.lower)
    upper = max(self.upper, other.upper)
    return Interval(lower,upper)
    
  def __eq__(self, other):
    return other.__class__ is Interval and self.lower == other.lower and self.upper == other.upper
  
  def widen(self, other):
    
    """
    Like 'combine' but not symmetric-- the other value
    is a more recent version from within a loop, 
    so try to follow the 'trend' of its changes. 
    """
    
    if other.__class__ is not Interval:
      return any_value 
    lower_diff = other.lower - self.lower
    upper_diff = other.upper - self.upper
    

    if lower_diff < 0:
      lower = -np.inf
    else:
      lower = self.lower 
    
    
    
    if upper_diff > 0:
      upper = np.inf 
    else:
      upper = self.upper 
    
    if lower == self.lower and upper == self.upper:
      return self 
    else:
      return Interval(lower, upper)     
    
  def __str__(self):
    return "[%s,%s]" % (self.lower, self.upper)
    
const_zero = Interval(0,0)
const_one = Interval(1,1)
const_none = NoneValue()
positive_interval = Interval(0, np.inf)
negative_interval = Interval(-np.inf, 0)
  
class TupleOfIntervals(AbstractValue):
  def __init__(self, elts):
    assert all(isinstance(elt, AbstractValue) for elt in elts)
    self.elts = elts 
    
  def combine(self, other):
    if other.__class__ is not TupleOfIntervals:
      return any_value 
    elif len(other.elts) != len(self.elts):
      return any_value  
    combined_elts = [e1.combine(e2) for e1,e2 in zip(self.elts, other.elts)]
    return TupleOfIntervals(combined_elts)

  def widen(self, other):
    if other.__class__ is not TupleOfIntervals or len(other.elts) != len(self.elts):
      return any_value 
    return mk_tuple([e1.widen(e2) for e1,e2 in zip(self.elts, other.elts)])
  
  def __str__(self):
    return "tuple(%s)" % ", ".join(str(elt) for elt in self.elts)

  def __eq__(self, other):
    return other.__class__ is TupleOfIntervals and len(other.elts) == len(self.elts) and \
      all(e1 == e2 for e1,e2 in zip(self.elts,other.elts))

def mk_tuple(elts):
  if all(elt is None or elt is any_value for elt in elts):
    return any_value 
  else:
    return TupleOfIntervals(elts)  

class SliceOfIntervals(AbstractValue):
  def __init__(self, start, stop, step):
    self.start = start 
    self.stop = stop 
    self.step = step 
    
  def combine(self, other):
    if other.__class__ is not SliceOfIntervals:
      return any_value
    start = self.start.combine(other.start)
    stop = self.stop.combine(other.stop)
    step = self.step.combine(other.step)
    if start == self.start and stop == self.stop and step == self.stop:
      return self 
    return mk_slice(start,stop,step)

  def __eq__(self, other):
    return other.__class__ is SliceOfIntervals and \
      self.start == other.start and \
      self.stop == other.stop and \
      self.step == other.step

  def widen(self, other):
    if other.__class__ is not SliceOfIntervals:
      return any_value 
    start = self.start.widen(other.start)
    stop = self.stop.widen(other.stop)
    step = self.step.widen(other.step)
    if start == self.start and stop == self.stop and step == self.step:
      return self 
    return mk_slice(start,stop,step)
        
  def __str__(self):
    return "slice(start=%s, stop=%s, step=%s)" % (self.start, self.stop, self.step)
    
def mk_slice(start, stop, step):
  if (start is None or start is any_value) and \
     (stop is None or stop is any_value)  and \
     (step is None or step is any_value):
    return any_value 
  return SliceOfIntervals(start, stop, step) 



class ValueRangeAnalyis(SyntaxVisitor):
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.ranges = {} 
    self.old_values = ScopedDict()
    self.old_values.push()

  def get(self, expr):
    c = expr.__class__ 
    
    if expr.type.__class__ is NoneT:
      return const_none 
    elif c is Const:
      return Interval(expr.value, expr.value)
    
    elif c is Var and expr.name in self.ranges:
      return self.ranges[expr.name]
    
    elif c is Tuple:
      elt_values = [self.get(elt) for elt in expr.elts]
      return mk_tuple(elt_values)
    
    elif c is TupleProj:
      tup = self.get(expr.tuple)
      idx = unwrap_constant(expr.index) 
      if tup.__class__ is TupleOfIntervals:
        return tup.elts[idx]
      
    elif c is Slice:
      start = self.get(expr.start)
      stop = self.get(expr.stop)
      if expr.step.type ==  NoneType:
        step = const_one   
      else:
        step = self.get(expr.step)
      return mk_slice(start, stop, step)
    
    elif c is Shape:
      ndims = get_rank(expr.array.type)   
      return mk_tuple([positive_interval] * ndims)
    
    elif c is Attribute:
      if expr.name == 'shape':
        ndims = get_rank(expr.value.type)   
        return mk_tuple([positive_interval] * ndims)
      elif expr.name == 'start':
        sliceval = self.get(expr.value)
        if isinstance(sliceval, SliceOfIntervals):
          return sliceval.start
      elif expr.name == 'stop':
        sliceval = self.get(expr.value)
        if isinstance(sliceval, SliceOfIntervals):
          return sliceval.stop
      elif expr.name == 'step':
        sliceval = self.get(expr.value)
        if isinstance(sliceval, SliceOfIntervals):
          return sliceval.step
    elif c is PrimCall:
      p = expr.prim 
      if p.nin == 2:
        x = self.get(expr.args[0])
        y = self.get(expr.args[1])
        if p == prims.add:

          return self.add_range(x,y)
        elif p == prims.subtract:
          return self.sub_range(x,y)
        elif p == prims.multiply:
          return self.mul_range(x,y)
      elif p.nin == 1:
        x = self.get(expr.args[0])
        if p == prims.negative:
          return self.neg_range(x)
    return any_value 
  
  def set(self, name, val):
    if val is not None and val is not unknown_value:
      old_value = self.ranges.get(name, unknown_value)
      self.ranges[name] = val
      if old_value != val and old_value is not unknown_value:
        self.old_values[name] = old_value 

      
  def add_range(self, x, y):
    if not isinstance(x, Interval) or not isinstance(y, Interval):
      return any_value 
    return Interval (x.lower + y.lower, x.upper + y.upper)
  
  def sub_range(self, x, y):
    if not isinstance(x, Interval) or not isinstance(y, Interval):
      return any_value
    return Interval(x.lower - y.upper, x.upper - y.lower)
  
  def mul_range(self, x, y):
    if not isinstance(x, Interval) or not isinstance(y, Interval):
      return any_value
    xl, xu = x.lower, x.upper 
    yl, yu = y.lower, y.upper 
    products = (xl * yl, xl * yu, xu * yl, xu * yu)
    lower = min(products)
    upper = max(products)
    return Interval(lower, upper)
  
  def neg_range(self, x):
    if not isinstance(x, Interval):
      return any_value 
    return Interval(-x.upper, -x.lower) 
  
  def visit_Assign(self, stmt):
    if stmt.lhs.__class__ is Var:
      name = stmt.lhs.name 
      v = self.get(stmt.rhs)
      self.set(name, v)
  
  def visit_merge_left(self, phi_nodes):
    for (k, (left, _)) in phi_nodes.iteritems():
      left_val = self.get(left)
      self.set(k, left_val)
      
      
  def visit_merge(self, phi_nodes):    
    for (k, (left,right)) in phi_nodes.iteritems():
      left_val = self.get(left)
      right_val = self.get(right)
      self.set(k, left_val.combine(right_val))

  def visit_Select(self, expr):
    return self.get(expr.true_value).combine(self.get(expr.false_value))
  
  def always_positive(self, x, inclusive = True):
    if not isinstance(x, Interval):
      return False 
    elif inclusive:
      return x.lower >= 0
    else:
      return x.lower > 0
  
  def always_negative(self, x, inclusive = True):
    if not isinstance(x, Interval):
      return False 
    elif inclusive:
      return x.upper <= 0
    else:
      return x.upper < 0
    
  
  def widen(self, old_values):
    for (k, oldv) in old_values.iteritems():
      newv = self.ranges[k]
      if oldv != newv:
        self.ranges[k] = oldv.widen(newv)
          
  def run_loop(self, body, merge):
    
    
    # run loop for the first time 
    self.old_values.push()
    self.visit_merge_left(merge)
    self.visit_block(body)
    self.visit_merge(merge)
    
    #run loop for the second time 
    self.visit_block(body)
    self.visit_merge(merge)
    
    old_values = self.old_values.pop()
    self.widen(old_values)
    
    # TODO: verify that it's safe not to run loop with widened values
    #self.visit_block(body)
    #self.visit_merge(merge)
    
    
  def visit_While(self, stmt):
    self.run_loop(stmt.body, stmt.merge)
  
  
  def visit_ForLoop(self, stmt):    

    start = self.get(stmt.start)
    stop = self.get(stmt.stop)
    step = self.get(stmt.step)
    name = stmt.var.name
    iterator_range = any_value  
    if isinstance(start, Interval) and isinstance(stop, Interval):
      lower = min (start.lower, start.upper, stop.lower, stop.upper)
      upper = max (start.lower, start.upper, stop.lower, stop.upper)
      iterator_range = Interval(lower,upper)
      
    elif isinstance(start, Interval) and isinstance(step, Interval):
      if self.always_positive(step):
        iterator_range = Interval(start.lower, np.inf)
      elif self.always_negative(step, inclusive = False):
        iterator_range = Interval(-np.inf, start.upper)
    
    elif isinstance(stop, Interval) and isinstance(step, Interval):
      if self.always_positive(step):
        iterator_range = Interval(-np.inf, stop.upper)
      elif self.always_negative(step, inclusive = False):
        iterator_range = Interval(stop.lower, np.inf)  
    self.set(name, iterator_range)
    self.run_loop(stmt.body, stmt.merge)

      
    
########NEW FILE########
__FILENAME__ = verify
from .. ndtypes import ArrayT, NoneT, NoneType, ScalarT, ClosureT, TupleT, FnT, Type, SliceT, PtrT
from .. ndtypes import lower_rank 

from .. syntax import Expr, Tuple, Var, Index, Closure, TypedFn 
from .. syntax.helpers import get_types, get_elt_types 
from collect_vars import collect_binding_names
from syntax_visitor import SyntaxVisitor

class Verify(SyntaxVisitor):
  call_stack = []
  def __init__(self, fn):
    SyntaxVisitor.__init__(self)
    
    self.fn = fn
    self.call_stack.append(fn)
    self.bound = set(fn.arg_names)
    self.seen_return = False 

  def __del__(self):
    self.call_stack.pop()
    
  def bind_var(self, name):
    assert name not in self.bound, \
        "Error: variable %s already bound" % name
    self.bound.add(name)
    assert name in self.fn.type_env, \
        "Assigned variable %s has no entry in type dictionary" % name

  def phi_value_ok(self, lhs_name, v):
    assert isinstance(v, Expr), \
       "Invalid expression in phi node: %s" % (v,)
    self.visit_expr(v)
    assert v.type is not None, \
       "Error in phi node: value %s has no type annotation" % v
    t = self.fn.type_env[lhs_name]
    assert v.type == t, \
       "Invalid type annotation on %v, expected %s but got %s" % (v,t,v.type)

  def visit_merge_loop_start(self, phi_nodes):
    for (k, (v,_)) in phi_nodes.iteritems():
      self.bind_var(k)
      self.phi_value_ok(k,v)

  def visit_merge_loop_repeat(self, phi_nodes):
    for (k, (left_value, right_value)) in phi_nodes.iteritems():
      self.phi_value_ok(k, left_value)
      self.phi_value_ok(k, right_value)

  def visit_merge_if(self, phi_nodes):
    for (k, (left_value, right_value)) in phi_nodes.iteritems():
      self.bind_var(k)
      self.phi_value_ok(k, left_value)
      self.phi_value_ok(k, right_value)

  def visit_Var(self, expr):
    assert expr.name in self.bound, \
        "Variable %s used before assignment" % expr.name
    assert expr.name in self.fn.type_env, \
        "Variable %s has no entry in the type dictionary" % expr.name
    assert expr.type == self.fn.type_env[expr.name], \
        "Variable %s in fn %s should have type %s but annotated with %s" % \
        (expr.name, self.fn, self.fn.type_env[expr.name], expr.type)

  def visit_PrimCall(self, expr):
    self.visit_expr_list(expr.args)

    for arg in expr.args:
      assert arg.type is not None, \
          "Expected type annotation for %s" % (arg, )
      assert isinstance(arg.type, ScalarT), \
          "Can't call primitive %s with argument %s of non-scalar type %s" % \
          (expr.prim, arg, arg.type)

  def visit_expr(self, expr):
    assert expr is not None, "Expression missing, must be a compiler bug"
    assert expr.type is not None, \
      "Missing type annotation on %s" % expr 
    SyntaxVisitor.visit_expr(self, expr)

  def visit_Const(self, expr):
    assert isinstance(expr.type, (ScalarT, NoneT)), \
      "Invalid constant %s : %s" % (expr.value, expr.type)
       
  def visit_ExprStmt(self, stmt):
    self.visit_expr(stmt.value)
    assert stmt.value.type and stmt.value.type.__class__ is NoneT, \
      "Expected effectful expression %s to have type %s but instead got %s" % \
      (stmt.value, NoneType, stmt.value.type)
  
  def visit_Index(self, expr):
    arr = expr.value
    arr_t = arr.type  
    idx = expr.index 
    idx_t = idx.type 
    assert arr.type is not None, \
      "Error in indexing expression %s: %s lacks type" % (expr, arr) 
    assert idx.type is not None, \
      "Error in indexing expression %s: %s lacks type" % (expr, idx)
    assert isinstance(arr.type, (TupleT, ArrayT, PtrT)), \
      "Expected %s to be an array, pointer, or tuple" % arr
    assert isinstance(idx.type, (TupleT, SliceT, NoneT, ScalarT)), \
      "Expected index %s to be scalar or tuple" % idx
     
    if isinstance(arr_t, ArrayT):
      rank  = arr_t.rank
      if isinstance(idx_t, TupleT):
        n_indices = len(idx_t.elt_types)
      else: 
        n_indices = 1 
      assert rank >= n_indices, \
        "Expected at most %d indices but found %d in expression %s" % (rank, n_indices, expr)
  
  def check_fn_args(self, fn, args = None, arg_types = None):
    if arg_types is None: 
      assert args is not None, "Function args missing" 
      arg_types = get_types(args)
    n_given = len(arg_types)
    n_expected = len(fn.input_types)
    assert n_given == n_expected, \
        "Arity mismatch when calling %s, expected %d but got %d" % \
        (fn.name, n_expected, n_given)
    for (i, arg_name) in enumerate(fn.arg_names):
      given_t = arg_types[i]
      expected_t = fn.type_env[arg_name]
      signature_t = fn.input_types[i] 
      assert expected_t == signature_t, \
          "Function %s has inconsistent types %s and %s for arg %s" % \
          (fn.name, expected_t, signature_t, arg_name)
      assert given_t == expected_t, \
          "Given argument %s%s doesn't matched expected '%s' : %s in %s" % \
          ("'%s' : " %  args[i] if args is not None else "", 
           given_t, 
           arg_name, 
           expected_t, 
           fn.name)  
  
  
  def get_fn_and_closure(self, fn):
    if fn.__class__ is Closure: 
      return fn.fn, tuple(get_types(fn.args)) 
    elif fn.__class__ is TypedFn:
      return fn, ()
    elif fn.type.__class__ is FnT:
      return  fn, ()
    else:
      assert isinstance(fn.type, ClosureT), "Unexpected function %s : %s" % (fn, fn.type)
  
  
  def verify_call(self, fn, args):
    fn, closure_arg_types = self.get_fn_and_closure(fn)
    arg_types = []
    for arg in args:
      if isinstance(arg, Type):
        arg_types.append(arg)
      else:
        assert isinstance(arg, Expr), \
          "Expected call argument to be expression but got %s" % arg 
        arg_types.append(arg.type)
    arg_types = tuple(closure_arg_types) + tuple(arg_types)
    try:
      self.check_fn_args(fn, arg_types = arg_types)
    except:
      print "[verify] Errors in function call %s(%s)" % (fn.name, args) 
      raise 
  
      
  def visit_Call(self, expr):
    self.verify_call(expr.fn, expr.args)
  
  def is_none(self, expr):
    return expr is None or (hasattr(expr, 'type') and isinstance(expr.type, NoneT))
  
  def visit_Map(self, expr):
    assert len(expr.args) > 0, "Can't have Map without array arguments"
    if expr.fn.__class__ is Closure: 
      closure_elts = tuple(expr.fn.args) 
      fn = expr.fn.fn 
    else:
      fn = expr.fn
      closure_elts = ()
    verify(fn)
    if self.is_none(expr.axis):
      elt_types = get_elt_types(expr.args)
    else:
      elt_types = [lower_rank(arg.type, 1) for arg in expr.args]
    arg_types = tuple(get_types(closure_elts)) + tuple(elt_types)
    args = tuple(closure_elts) + tuple(expr.args)                      
    self.check_fn_args(fn, args, arg_types)
  
  def visit_Return(self, stmt):
    self.visit_expr(stmt.value)
    assert stmt.value.type is not None and stmt.value.type == self.fn.return_type, \
        "Incorrect return type in %s: ret value %s, expected %s but got %s" % \
        (self.fn.name, stmt.value, self.fn.return_type, stmt.value.type)
    self.seen_return = True 
    
  def visit_lhs(self, lhs):
    c = lhs.__class__
    assert c in (Var, Tuple, Index), "Invalid LHS of assignment: %s" % lhs 
    if c is Tuple:
      for elt in lhs.elts:
        self.visit_lhs(elt)

  def visit_Assign(self, stmt):
    assert stmt.lhs.type is not None, \
        "Missing LHS type for assignment %s" % stmt
    assert stmt.rhs.type is not None, \
        "Missing RHS type for assignment %s" % stmt
    self.visit_lhs(stmt.lhs)
    lhs_names = collect_binding_names(stmt.lhs)
    for lhs_name in lhs_names:
      self.bind_var(lhs_name)
    self.visit_expr(stmt.rhs)
    rhs_t = stmt.rhs.type 
    if stmt.lhs.__class__ is Index:
      array_t = stmt.lhs.value.type 
      assert stmt.lhs.type == rhs_t or rhs_t == array_t.elt_type, \
        "Mismatch between LHS type %s and RHS %s in '%s'" % \
        (stmt.lhs.type, stmt.rhs.type, stmt)
    else:
      assert stmt.lhs.type == stmt.rhs.type, \
        "Mismatch between LHS type %s and RHS %s in '%s'" % \
        (stmt.lhs.type, stmt.rhs.type, stmt)

  def visit_ForLoop(self, stmt):
    assert stmt.var.__class__ is Var, "Expected loop variable, got: %s" % stmt.var 
    self.bind_var(stmt.var.name)
    self.visit_expr(stmt.var)

    assert stmt.start.type == stmt.var.type, \
      "Loop variable type %s : %s doesn't match start value %s : %s" % \
      (stmt.var, stmt.var.type, stmt.start, stmt.start.type)
    self.visit_expr(stmt.start)
    self.visit_merge_loop_start(stmt.merge)

    assert stmt.stop.type == stmt.var.type, \
      "Loop variable type %s : %s doesn't match stop value %s : %s" % \
      (stmt.var, stmt.var.type, stmt.stop, stmt.stop.type)
    self.visit_expr(stmt.stop)
    self.visit_block(stmt.body)
    
    assert stmt.step.type == stmt.var.type, \
     "Loop variable type %s : %s doesn't match stop value %s : %s" % \
      (stmt.var, stmt.var.type, stmt.stop, stmt.stop.type)
    self.visit_expr(stmt.step)
    self.visit_merge_loop_repeat(stmt.merge)
    
  def visit_ParFor(self, stmt):
    """
    Have to be annoyingly flexible with ParFor since we might encounter
    ordinary or flattened functions as arguments and the latter don't 
    take tuples of indices but have to take each index as a separate parameter
    """
    self.visit_expr(stmt.bounds)
    fn, closure_arg_types = self.get_fn_and_closure(stmt.fn)
    verify(fn)
    bounds_t = stmt.bounds.type
    assert len(fn.input_types) > 0, \
      "Problem with ParFor, can't call function %s which accepts 0 args" % fn.name 
       
    last_input_type = fn.input_types[-1]
    if isinstance(last_input_type, TupleT):
      assert bounds_t == last_input_type, \
        "ParFor function expects index type %s but bounds of type %s" % (last_input_type, bounds_t)
      arg_types = (bounds_t,)
    elif isinstance(bounds_t, TupleT):
      arg_types = bounds_t.elt_types
    else:
      arg_types = (stmt.bounds.type,)
    self.verify_call(fn, tuple(closure_arg_types) + tuple(arg_types))
    
  def visit_stmt(self, stmt):
    assert stmt is not None, "Statement missing, must be a compiler bug"
    SyntaxVisitor.visit_stmt(self, stmt)

  def visit_TypedFn(self, fn):
    return verify(fn)

def verify(fn):
  n_input_types = len(fn.input_types)
  n_arg_names = len(fn.arg_names)
  assert n_input_types == n_arg_names, \
     "Function %s has %d input types and %d input names" % \
     (fn.name, n_input_types, n_arg_names)
  for arg_name, input_t in zip(fn.arg_names, fn.input_types):
    assert arg_name in fn.type_env, \
        "Input %s has no entry in %s's type environment" % \
        (arg_name, fn.name)
    t = fn.type_env[arg_name]
    assert input_t == t, \
        "Mismatch between %s's input type %s and the arg %s's type %s" % \
        (fn.name, input_t, arg_name, t)
  try:    
    verifier = Verify(fn) 
    verifier.visit_block(fn.body)
    assert verifier.seen_return, "Never encountered Return statement"
  except:
    if fn.name == Verify.call_stack[-1].name:
      print "Error in function"
      print "---------------------"
    else:
      print "...called from:"
    print fn
    print 
    raise 

########NEW FILE########
__FILENAME__ = adverb_builder

from ..ndtypes import TupleT, ScalarT
from ..syntax import Expr 
from ..syntax.adverb_helpers import max_rank_arg, max_rank, unwrap_constant 
from ..syntax.helpers import unwrap_constant, wrap_if_constant 
 
from loop_builder import LoopBuilder

class AdverbBuilder(LoopBuilder):

  def list_none_axes(self, args, axis):
    axes = self.normalize_expr_axes(args, axis)
    return [self.is_none(axis) for axis in axes]
  
  def normalize_expr_axes(self, args, axis):
    """
    Given a list of arguments to an adverb and some value 
    or expression representing the axis/axes of iteration, 
    return a tuple of axis expressions, one per argument
    """
    axis = wrap_if_constant(axis)
    assert isinstance(axis, Expr), "Unexpected axis %s" % axis
    
    if isinstance(axis.type, TupleT):
      axes = self.tuple_elts(axis)
    else:
      assert isinstance(axis.type, ScalarT), "Unexpected axis %s : %s" % (axis, axis.type)
      axes = (axis,) * len(args)
    
    assert len(axes) == len(args), "Wrong number of axes (%d) for %d args" % (len(axes), len(args))
    
    if self.rank(max_rank_arg(args)) < 2:
        # if we don't actually have any multidimensional arguments, 
        # might as well make the axes just 0  
      axes = tuple(self.int(0) if self.is_none(axis) else axis for axis in axes)
    return axes 
    
  def normalize_axes(self, args, axis):
    """
    Given a list of arguments to an adverb and some value 
    or expression representing the axis/axes of iteration, 
    return a tuple of axis values, one per argument
    """

    if isinstance(axis, Expr) and isinstance(axis.type, TupleT):
      axis = self.tuple_elts(axis)
          
      
    # unpack the axis argument into a tuple,  
    # if only one axis was given, then repeat it as many times as we have args 
    if isinstance(axis, (list, tuple)):
      axes = tuple([unwrap_constant(elt) for elt in axis])
    elif isinstance(axis, Expr):
      axes = (unwrap_constant(axis),) * len(args) 
    else:
      assert axis is None or isinstance(axis, (int,long)), "Invalid axis %s" % axis
      axes = (axis,) * len(args)
    
    assert len(axes) == len(args), "Wrong number of axes (%d) for %d args" % (len(axes), len(args))
    
    if self.rank(max_rank_arg(args)) < 2:
        # if we don't actually have any multidimensional arguments, 
        # might as well make the axes just 0  
      axes = tuple(0 if axis is None else axis for axis in axes)
    return axes 
  
  def iter_bounds(self, args, axes, cartesian_product = False):
    """
    Given the arguments to an adverb (higher order array operator) and 
    the axes of the adverb's iteration, 
    given back a scalar or tuple representing the iteration space bounds
    the adverb will traverse
    """
    axes = self.normalize_axes(args, axes)
    assert len(args) == len(axes), "Mismatch between args %s and axes %s" % (args, axes) 
 
    
    if cartesian_product:
      bounds = []
      for arg, axis in zip(args, axes):
        if self.rank(arg) == 0:
          continue 
        if axis is None:
          bounds.extend(self.tuple_elts(self.shape(arg)))
        else:
          bounds.append(self.shape(arg, axis))
      return self.tuple(bounds)
    
    #...otherwise, we're doing an elementwise operation across
    #possibly multiple arguments 
    if any(axis is None for axis in axes):
      # if any of the axes are None then just find the highest rank argument 
      # which is going to be fully traversed and use its shape as the bounds
      # for the generated ParFor
      best_rank = -1 
      best_arg = None 
      for curr_arg, curr_axis in zip(args,axes):
        r = self.rank(curr_arg)
        if curr_axis is None and r > best_rank:
          best_rank = r
          best_arg = curr_arg 
      return self.shape(best_arg) 
       
    else:
      # if all axes are integer values, then keep the one with highest rank, 
      # it's bad that we're not doing any error checking here to make sure that 
      # all the non-scalar arguments have compatible shapes 
      best_rank = -1  
      best_arg = None
      best_axis = None 
      for curr_arg, curr_axis in zip(args,axes):
        r = self.rank(curr_arg)
        if r > best_rank:
          best_arg = curr_arg 
          best_axis = curr_axis
          best_rank = r 
      return self.shape(best_arg, best_axis)
########NEW FILE########
__FILENAME__ = arith_builder
import numpy as np 
from .. import prims 
from ..syntax import Const, PrimCall, Var, If, Select 
from ..syntax.helpers import (zero, one, 
                              is_zero, is_one, const_bool, 
                              wrap_constants, get_types, wrap_if_constant)

from core_builder import CoreBuilder 

class ArithBuilder(CoreBuilder):
  def prim(self, prim_fn, args, name=None):
    args = wrap_constants(args)
    arg_types = get_types(args)
    upcast_types = prim_fn.expected_input_types(arg_types)
    result_type = prim_fn.result_type(upcast_types)
    upcast_args = [self.cast(x, t) for (x,t) in zip(args, upcast_types)]
    prim_call = PrimCall(prim_fn, upcast_args, type=result_type)
    if name:
      return self.assign_name(prim_call, name)
    else:
      return prim_call

  def pick_first(self, x, y):
    """Return x but first cast it to the common type of both args"""

    return self.cast(x, x.type.combine(y.type))

  def pick_second(self, x, y):
    """Return y but first cast it to the common type of both args"""

    return self.cast(y, x.type.combine(y.type))

  def pick_const(self, x, y, c):
    """Return a constant cast to the common type of both args"""

    return self.cast(wrap_if_constant(c), x.type.combine(y.type))

  def add(self, x, y, name = None):
    if is_zero(x):
      return self.pick_second(x,y)
    elif is_zero(y):
      return self.pick_first(x,y)
    elif x.__class__ is Const and y.__class__ is Const:
      return self.pick_const(x, y, x.value + y.value)
    if name is None: name = "add_result"
    return self.prim(prims.add, [x,y], name)

  def sub(self, x, y, name = None):
    if is_zero(y):
      return self.pick_first(x,y)
    elif x.__class__ is Const and y.__class__ is Const:
      return self.pick_const(x, y, x.value - y.value)
    
    if name is None: name = "sub_result"
    return self.prim(prims.subtract, [x,y], name)

  def mul(self, x, y, name = None):
    if is_one(x):
      return self.pick_second(x,y)
    elif is_one(y):
      return self.pick_first(x,y)
    elif is_zero(x) or is_zero(y):
      return self.pick_const(x, y, 0)
    
    if name is None: name = "mul_result"
    return self.prim(prims.multiply, [x,y], name)

  def div(self, x, y, name = None):
    if is_one(y):
      return self.pick_first(x,y)
    elif x.__class__ is Const and y.__class__ is Const:
      return self.pick_const(x, y, x.value / y.value)
    else:
      return self.prim(prims.divide, [x,y], name)


  
  
  def mod(self, x, y, name = None):
    if is_one(y):
      return self.pick_const(x, y, 0)
    elif x.__class__ is Const and y.__class__ is Const:
      return self.pick_const(x, y, x.value % y.value)
    if name is None: name = "mod_result"
    return self.prim(prims.mod, [x,y], name)

  def fmod(self, x, y, name = None):
    if is_one(y):
      return self.pick_const(x, y, 0)
    elif x.__class__ is Const and y.__class__ is Const:
      return self.pick_const(x, y, np.fmod(x,y))
    else:
      return self.prim(prims.fmod, [x,y], name)
      
  def div_round_up(self, x, y, name = None):
    div_toward_zero = self.div(x,y,"div_toward_zero")
    m = self.fmod(x, y)
    has_remainder = self.neq(m, zero(m.type), "has_remainder") 
    prod = self.mul(x, y, "prod")
    prod_is_positive = self.is_positive(prod, "prod_is_positive")
    was_rounded_down = self.and_(has_remainder, prod_is_positive)
    div_plus_one = self.add(div_toward_zero, one(div_toward_zero.type), "div_plus_one")
    return self.select(was_rounded_down, div_plus_one, div_toward_zero, name = name)
  
  def safediv(self, x, y, name = None):
    """
    Like div_round_up but only safe for positive integers
    """
    top = self.add(x, y)
    top = self.sub(top, one(top.type))
    return self.div(top, y, name = name)
  
  
  def ceil(self, x, name = None):
    return self.prim(prims.ceil, [x], name)
  
  def floor(self, x, name = None):
    return self.prim(prims.floor, [x], name)
  
  def round(self, x, name = None):
    return self.prim(prims.round, [x], name)
  
  def lt(self, x, y, name = None):
    if isinstance(x, (Var, Const)) and x == y:
      return const_bool(False)
    if name is None: name = "lt_result"
    return self.prim(prims.less, [x,y], name)

  def lte(self, x, y, name = None):
    if isinstance(x, (Var, Const)) and x == y:
      return const_bool(True)
    
    if name is None: name = "lte_result"
    return self.prim(prims.less_equal, [x,y], name)

  
  def gt(self, x, y, name = None):
    if isinstance(x, (Var, Const)) and x == y:
      return const_bool(False)
    if name is None: name = "gt_result"
    return self.prim(prims.greater, [x,y], name)

  def is_positive(self, x, name = "is_positive"):
    return self.gt(x, zero(x.type), name)
  
  def gte(self, x, y, name = None):
    if isinstance(x, (Var, Const)) and x == y:
      return const_bool(True)
    if name is None: name = "gte_result"
    return self.prim(prims.greater_equal, [x,y], name)

  def eq(self, x, y, name = None):
    if isinstance(x, (Var, Const)) and x == y:
      return const_bool(True)
    if name is None: name = "eq_result"
    return self.prim(prims.equal, [x,y], name)

  def neq(self, x, y, name = None):
    if isinstance(x, (Var, Const)) and x == y:
      return const_bool(False)
    if name is None: name = "neq_result"
    return self.prim(prims.not_equal, [x,y], name)

  def min(self, x, y, name = None):
    assert x.type == y.type, \
        "Type mismatch between %s and %s" % (x, y)
    if x.__class__ is Const and y.__class__ is Const:
      return x if x.value < y.value else y 
    cond = self.lte(x, y)
    expr = Select(cond, x, y, type = x.type)
    if name is None: return expr 
    else: return self.assign_name(expr, name)
    
  def max(self, x, y, name = None):
    assert x.type == y.type, \
        "Type mismatch between %s and %s" % (x, y)
    if x.__class__ is Const and y.__class__ is Const:
      return x if x.value > y.value else y 
    cond = self.gte(x, y)
    expr = Select(cond, x, y, type = x.type)
    if name is None: return expr 
    else: self.assign_name(expr, name)
    
  def or_(self, x, y, name = None):
    if x.__class__ is Const and x.value:
      return x
    elif y.__class__ is Const and y.value:
      return y
    if name is None:
      name = "or_result"
    return self.prim(prims.logical_or, [x,y], name)
  
    
  def and_(self, x, y, name = None):
    if x.__class__ is Const and x.value and y.__class__ is Const and y.value:
      return x 
    if name is None:
      name = "and_result"
    return self.prim(prims.logical_and, [x,y], name)
########NEW FILE########
__FILENAME__ = array_builder

from ..ndtypes import (make_slice_type, make_array_type, ptr_type, 
                       ArrayT, TupleT, ScalarT, Type, PtrT, Int64, IntT, Float64)
from ..syntax import (Alloc, AllocArray, ArrayView, Const, Index, Slice, Struct, Var, Select, Expr)
from ..syntax.helpers import (const, zero_i64, wrap_if_constant, slice_none, unwrap_constant)
from arith_builder import ArithBuilder

class ArrayBuilder(ArithBuilder):
  """
  Builder for constructing arrays and getting their properties
  """
  
  def elt_type(self, x):
    if isinstance(x, Type):
      try:
        return x.elt_type
      except:
        return x
    elif self.is_array(x):
      return x.type.elt_type
    else:
      return x.type
  
  def alloc_array(self, elt_t, dims, name = "array", 
                   explicit_struct = False, 
                   array_view = False, 
                   order = "C"):
    """
    Given an element type and sequence of expressions denoting each dimension
    size, generate code to allocate an array and its shape/strides metadata. For
    now I'm assuming that all arrays are in row-major, eventually we should make
    the layout an option.
    """
    
    assert order == "C", "Only row-major layout supported so far, not %s" % order 

    if self.is_tuple(dims):
      shape = dims
      dims = self.tuple_elts(shape)
    else:
      if not isinstance(dims, (list, tuple)):
        dims = [dims]
      shape = self.tuple(dims, "shape", explicit_struct = explicit_struct)
    rank = len(dims)
    array_t = make_array_type(elt_t, rank)
    if explicit_struct or array_view:
      nelts = self.prod(dims, name = "nelts")
      ptr_t = ptr_type(elt_t)

      ptr_var = self.assign_name(Alloc(elt_t, nelts, type = ptr_t), "data_ptr")
      stride_elts = [const(1)]

      # assume row-major for now!

      for d in reversed(dims[1:]):
        next_stride = self.mul(stride_elts[0], d, "dim")
        stride_elts = [next_stride] + stride_elts
      strides = self.tuple(stride_elts, "strides", explicit_struct = explicit_struct)
      if explicit_struct:
        array = Struct([ptr_var, shape, strides, zero_i64, nelts], type = array_t)
      else:
        array = ArrayView(data = ptr_var, 
                          shape = shape, 
                          strides = strides,
                          offset = zero_i64,
                          size = nelts, 
                          type = array_t)
    else:
      array = AllocArray(shape, elt_type = elt_t, type = array_t)
    if name is None: 
      return array 
    return self.assign_name(array, name)
  
  def len(self, array):
    return self.shape(array, 0)
  
  def nelts(self, array, explicit_struct = False):
    shape_elts = self.tuple_elts(self.shape(array), explicit_struct = explicit_struct)
    return self.prod(shape_elts, name = "nelts") 

  def rank(self, value):
    if self.is_array(value):
      return value.type.rank
    else:
      return 0

  def shape(self, array, dim = None, explicit_struct = False, temp = True):
    if array.type.__class__ is ArrayT:
      shape = self.attr(array, "shape", temp = temp)
      if dim is None:
        return shape
      if isinstance(dim, Expr):
        dim = unwrap_constant(dim)
      assert isinstance(dim, (int, long)), "Expected array dimension to be an int, got %s" % dim 
      dim_value = self.tuple_proj(shape, dim, explicit_struct = explicit_struct)
      if temp:
        return self.assign_name(dim_value, "dim%d" % dim)
      else:
        return dim_value 
    else:
      return self.tuple([])

  def strides(self, array, dim = None, explicit_struct = False):
    assert array.type.__class__ is ArrayT
    strides = self.attr(array, "strides")
    if dim is None:
      return strides
    else:
      elt_value = self.tuple_proj(strides, dim, explicit_struct = explicit_struct)
      return self.assign_name(elt_value, "stride%d" % dim)

  def slice_value(self, start, stop, step):
    slice_t = make_slice_type(start.type, stop.type, step.type)
    return Slice(start, stop, step, type = slice_t)

  def build_slice_indices(self, rank, axis, idx):
    """
    Build index tuple to pull out the 'idx' element along the given axis
    """
    if rank == 1:
      assert axis == 0
      return idx
    
    indices = []
    for i in xrange(rank):
      if i == axis:
        indices.append(idx)
      else:
        s = self.slice_value(self.none, self.none, self.int(1))
        indices.append(s)
    return self.tuple(indices)

  def elts_in_slice(self, start, stop, step):
    start_minus_start = self.sub(stop, start, name = "stop_minus_start")
    nelts = self.div(self.cast(start_minus_start, Float64), step, name = "nelts")
    ceil = self.ceil(nelts)
    nelts = self.cast(ceil, Int64)
    return self.max(nelts, self.zero(nelts.type))

  def slice_along_axis(self, arr, axis, idx):
    """
    Pull out a slice if the array has the given axis, 
    otherwise just return the array 
    """
    r = self.rank(arr)
    
    if isinstance(axis, Expr):
      axis = unwrap_constant(axis)
    idx = wrap_if_constant(idx)
    
    if r == 1 and (axis is None or axis == 0):
      return self.index(arr, idx)
    elif axis is None:
      if isinstance(idx.type, ScalarT):
        idx = self.tuple((idx,) * r)
      return self.index(arr, idx)
    elif r > axis:
      index_tuple = self.build_slice_indices(r, axis, idx)
      return self.index(arr, index_tuple)
    else:
      return arr

  def output_slice(self, output, axis, idx):
    """
    Create an expression which acts as an LHS output location 
    for a slice throught the variable 'output' along the given axis
    """
    r = self.rank(output)
    if r > 1:
      output_indices = self.build_slice_indices(r, axis, idx)
    elif r == 1:
      output_idx = self.slice_value(idx, self.none, self.int(1))
      output_indices = self.tuple([output_idx])
    else:
      output_idx = self.slice_value(self.none, self.none, self.none)
      output_indices = self.tuple([output_idx])
    return self.index(output, output_indices)
  

  def size_along_axis(self, value, axis):
    return self.shape(value, axis)

  def check_equal_sizes(self, sizes):
    pass
  
  def index(self, arr, idx, temp = False, name = None):
    """Index into array or tuple differently depending on the type"""
    
    temp = temp or name is not None
    
    arr_t = arr.type

    if isinstance(arr_t, ScalarT):
      # even though it's not correct externally, it's
      # often more convenient to treat indexing
      # into scalars as the identity function.
      # Just be sure to catch this as an error in
      # the user's code earlier in the pipeline.
      return arr
    
    if isinstance(arr_t, TupleT):
      if isinstance(idx, Const):
        idx = idx.value

      assert isinstance(idx, int), \
          "Index into tuple must be an integer, got %s" % idx
      if isinstance(idx, Const):
        idx = idx.value
      proj = self.tuple_proj(arr, idx)
      if temp:
        return self.assign_name(proj, "tuple_elt%d" % idx if name is None else name)
      else:
        return proj

    if self.is_tuple(idx):
      indices = self.tuple_elts(idx)
    elif isinstance(idx, (list,tuple)) or hasattr(idx, '__iter__'):
      indices = tuple(map(wrap_if_constant,idx))
    else:
      indices = (wrap_if_constant(idx),)

    n_required = arr_t.rank
    n_indices = len(indices)
    if n_indices < n_required:
      # all unspecified dimensions are considered fully sliced
      extra = (slice_none,) * (n_required - n_indices)
      indices = indices + extra

    if len(indices) > 1:
      idx = self.tuple(indices, name = name)
    else:
      idx = indices[0]

    t = arr_t.index_type(idx.type)
    idx_expr = Index(arr, idx, type=t)
    if temp:
      return self.assign_name(idx_expr, "array_elt" if name is None else name)
    else:
      return idx_expr

  def index_along_axis(self, arr, axis, idx, name=None):
    if arr.type.__class__ is not ArrayT:
      return arr
    assert isinstance(axis, int), \
        "Axis must be a known constant int, got: " + str(axis)
    indices = []
    for i in xrange(arr.type.rank):
      if i == axis:
        indices.append(wrap_if_constant(idx))
      else:
        indices.append(slice_none)

    index_tuple = self.tuple(indices)


    result_t = arr.type.index_type(index_tuple.type)
    idx_expr = Index(arr, index_tuple, type=result_t)
    if name:
      return self.assign_name(idx_expr, name)
    else:
      return idx_expr

  def setidx(self, arr, idx, v):
    self.assign(self.index(arr, idx, temp=False), v)


  def array_view(self, data, shape, strides, offset, nelts):
    assert isinstance(data.type, PtrT), \
        "Data field of array must be a pointer, got %s" % data.type
    if data.__class__ is not Var:
      data = self.assign_name(data, "data_ptr")
      
    if isinstance(shape.type, ScalarT):
      shape = self.tuple([shape])
    assert isinstance(shape.type, TupleT), \
      "Shape of array must be a tuple, got: %s" % shape.type
      
    if isinstance(strides.type, ScalarT):
      strides = self.tuple(strides)
      
    assert isinstance(strides.type, TupleT), \
      "Strides of array must be a tuple, got: %s" % strides.type

    ndims = len(strides.type.elt_types)
    assert ndims == len(shape.type.elt_types)

    elt_t = data.type.elt_type
    array_t = make_array_type(elt_t, ndims)
    return ArrayView(data = data, shape = shape, strides = strides, 
                     offset = offset, size = nelts, 
                     type = array_t)

  
    
########NEW FILE########
__FILENAME__ = builder
from ..ndtypes import make_array_type, TupleT, IntT, FnT, ClosureT, increase_rank
from ..syntax import ArrayView, Struct, Expr, ParFor, IndexMap, UntypedFn, TypedFn 
from ..syntax.helpers import zero_i64, get_types, one 

from call_builder import CallBuilder
from adverb_builder import AdverbBuilder

class Builder(AdverbBuilder, CallBuilder):
  

  
  def create_output_array(self, fn, inner_args, 
                          outer_shape = (), 
                          name = "output"):
    if isinstance(outer_shape, (list, tuple)):
      outer_shape = self.tuple(outer_shape)
      
    try:
      inner_shape_tuple = self.call_shape(fn, inner_args)
    except:
      print "Shape inference failed when calling %s with %s" % (fn, inner_args)
      import sys
      print "Error %s ==> %s" % (sys.exc_info()[:2])
      raise

    shape = self.concat_tuples(outer_shape, inner_shape_tuple)
    closure_args = self.closure_elts(fn)
    fn = self.get_fn(fn)
    if isinstance(fn, UntypedFn):
      from .. type_inference import infer_return_type 
      arg_types = get_types(tuple(closure_args) + tuple(inner_args))
      return_type = infer_return_type(fn , arg_types)
      
    else:
      assert isinstance(fn, TypedFn), "Unexpected function %s" % fn
      return_type = self.return_type(fn)
    elt_t = self.elt_type(return_type)
    if len(shape.type.elt_types) > 0:
      return self.alloc_array(elt_t, shape, name)
    else:
      return self.fresh_var(elt_t, name) 
  
  def any_eq(self, tup, target_elt):
    elts = self.tuple_elts(tup)
    is_eq = self.eq(elts[0], target_elt)
    for elt in elts[1:]:
      is_eq = self.or_(is_eq, self.eq(elt, target_elt))
    return is_eq 
  
    
  def parfor(self, fn, bounds):
    assert isinstance(bounds, Expr)
    assert isinstance(bounds.type, (TupleT, IntT))
    assert isinstance(fn, Expr)
    assert isinstance(fn.type, (FnT, ClosureT))
    self.blocks += [ParFor(fn = fn, bounds = bounds)]
  
  def imap(self, fn, bounds):

    assert isinstance(bounds, Expr), "Expected imap bounds to be expression, got %s" % bounds
    if isinstance(bounds.type, TupleT):
      tup = bounds 
      ndims = len(bounds.type.elt_types) 
    else:
      assert isinstance(bounds.type, IntT), \
        "Expected imap bounds to be tuple or int, got %s : %s" % (bounds, bounds.type)
      tup = self.tuple([bounds])
      ndims = 1 
    assert isinstance(fn, Expr), "Expected imap function to be expression, got %s" % (fn,)
    assert isinstance(fn.type, (FnT, ClosureT)), \
      "Expected imap function to have a function type but got %s : %s" % (fn, fn.type)
    elt_type = self.return_type(fn) 
    result_type = increase_rank(elt_type, ndims)
    return IndexMap(fn = fn, shape = tup, type = result_type)
    
    
  def ravel(self, x, explicit_struct = False):
    # TODO: Check the strides to see if any element is equal to 1
    # otherwise do an array_copy
    assert self.is_array(x)
    if x.type.rank == 1:
      return x
    
    nelts = self.nelts(x, explicit_struct = explicit_struct)
    assert isinstance(nelts, Expr)
    shape = self.tuple((nelts,), 'shape', explicit_struct = explicit_struct)
    strides = self.tuple((self.int(1),), "strides", explicit_struct = explicit_struct)
    data = self.attr(x, 'data', 'data_ptr')
    offset = self.attr(x, 'offset')
    t = make_array_type(x.type.elt_type, 1)
    if explicit_struct: 
      return Struct(args = (data, shape, strides, offset, nelts), type = t)
    else:
      return ArrayView(data, shape, strides, offset, nelts, type = t)
  
########NEW FILE########
__FILENAME__ = build_fn
from dsltools import NestedBlocks

from .. import names, syntax
from ..ndtypes import NoneType 
from ..syntax import TypedFn
from ..builder import Builder 


def fresh_builder(fn):
  blocks = NestedBlocks()
  blocks.push(fn.body)
  return Builder(type_env = fn.type_env, blocks = blocks)
  
def build_fn(input_types, 
             return_type = NoneType, 
             name = None, 
             input_names = None):
  n_inputs = len(input_types)
  if input_names is None:
    input_names = syntax.gen_data_arg_names(n_inputs)
  assert len(input_names) == n_inputs 
  if name is None:
    name = 'f'
  name = names.refresh(name)
 
  f = TypedFn(name = name, 
              arg_names = input_names, 
              body = [], 
              type_env = dict(zip(input_names, input_types)),
              input_types = input_types, 
              return_type = return_type) 
              
  builder = fresh_builder(f)
  input_vars = builder.input_vars(f) 
  return f, builder, input_vars  


_identity_fn_cache = {}
def mk_identity_fn(t):
  if t in _identity_fn_cache:
    return _identity_fn_cache[t]
  f, b, (x,) = build_fn([t], t, name = "ident")
  b.return_(x)
  _identity_fn_cache[t] = f
  return f

_cast_fn_cache = {}
def mk_cast_fn(from_type, to_type):
  key = (from_type, to_type)
  if key in _cast_fn_cache:
    return _cast_fn_cache[key]
  f, b, (x,) = build_fn([from_type], to_type, name = "cast_%s_%s" % (from_type, to_type))
  b.return_(b.cast(x, to_type))
  _cast_fn_cache[key] = f
  return f

_prim_fn_cache = {}
def mk_prim_fn(prim, arg_types):
  key = prim, tuple(arg_types)
  if key in _prim_fn_cache:
    return _prim_fn_cache[key]
  
  upcast_types = prim.expected_input_types(arg_types)
  result_type = prim.result_type(upcast_types)
  f, b, args = build_fn(arg_types, result_type, name =  prim.name)
  # builder will auto-cast argument vars to appropriate types for the given primitive
  b.return_(b.prim(prim, args))
  _prim_fn_cache[key] = f 
  return f 
  

########NEW FILE########
__FILENAME__ = call_builder

from ..ndtypes import FnT, ClosureT, Type, make_closure_type, NoneType 
from ..syntax import UntypedFn, TypedFn, Var, Call, Closure, ClosureElt, Return, FormalArgs   
from ..syntax.helpers import get_types, zero_i64, none 
from ..syntax.adverb_helpers import max_rank

from core_builder import CoreBuilder
from parakeet.syntax.stmt import ExprStmt

class CallBuilder(CoreBuilder):
  
  """
  Builder for all things related to calling functions
  """
  
  def closure_elt(self, clos, idx, name = None):
    assert isinstance(idx, (int, long))

    if isinstance(clos, Closure):
      result = clos.args[idx]
    else:
      result = ClosureElt(clos, idx, type = clos.type.arg_types[idx])
    if name is None:
      return result
    return self.assign_name(result, name)
  
  def closure_elts(self, clos):
    if clos.__class__ is TypedFn:
      return ()
    return tuple(self.closure_elt(clos, i)
                 for i in xrange(len(clos.type.arg_types)))

  def get_fn(self, maybe_clos):
    if maybe_clos.__class__ is Closure:
      return maybe_clos.fn
    elif maybe_clos.type.__class__ is ClosureT:
      return maybe_clos.type.fn
    else:
      return maybe_clos

  def closure(self, maybe_fn, extra_args, name = None):
    fn = self.get_fn(maybe_fn)
    old_closure_elts = self.closure_elts(maybe_fn)
    closure_elts = old_closure_elts + tuple(extra_args)
    if len(closure_elts) == 0:
      return fn 
    closure_elt_types = [elt.type for elt in closure_elts]
    closure_t = make_closure_type(fn, closure_elt_types)
    result = Closure(fn, closure_elts, type = closure_t)
    if name:
      return self.assign_name(result, name)
    else:
      return result
  

  def return_type(self, fn):
    if isinstance(fn, TypedFn):
      return fn.return_type
    else:
      assert isinstance(fn.type, ClosureT), \
          "Unexpected fn type: %s" % fn.type
      assert isinstance(fn.type.fn, TypedFn), \
          "Unexpected fn: %s" % fn.type.fn
      return fn.type.fn.return_type


  def input_vars(self, fn):
    assert isinstance(fn, TypedFn), "Expected TypedFn, got %s" % fn 
    
    return [Var(arg_name, t) 
            for arg_name, t in 
            zip(fn.arg_names, fn.input_types)]
  
  def input_types(self, closure):
    fn = self.get_fn(closure)
    closure_args = self.closure_elts(closure)
    return fn.input_types[len(closure_args):]
  
  def invoke_type(self, closure, args):
    from .. type_inference import invoke_result_type 
    closure_t = closure.type
    arg_types = get_types(args)
    assert all(isinstance(t, Type) for t in arg_types), \
        "Invalid types: %s" % (arg_types, )
    return invoke_result_type(closure_t, arg_types)

  def is_identity_fn(self, fn):
    if fn.__class__ is TypedFn and len(fn.arg_names) == 1:
      input_name = fn.arg_names[0]
    elif fn.__class__ is UntypedFn:
      args = fn.args 
      if isinstance(args, (list, tuple)):
        input_name = args[0]
      else:
        assert isinstance(args, FormalArgs), "Unexpected args %s" % (args,)
        if args.n_args == 1 and len(args.positional) == 1:
          input_name = args.positional[0]  
        else:
          return False 
    else:
      return False 
    
    if isinstance(input_name, Var):
      input_name = input_name.name
    else:
      assert isinstance(input_name, str), "Unexpected input %s" % (input_name,)    
    if len(fn.body) == 1:
      stmt = fn.body[0]
      if stmt.__class__ is Return:
        expr = stmt.value 
        if expr.__class__ is Var:
          return expr.name == input_name
    return False 
    
  def invoke(self, fn, args, loopify = False, lower = False, name = None):
    

    if isinstance(fn, UntypedFn) or isinstance(fn.type, FnT):
      closure_args = [] 
    else: 
      assert isinstance(fn.type, ClosureT), "Unexpected function %s with type: %s" % (fn, fn.type)
      closure_args = self.closure_elts(fn)
      fn = self.get_fn(fn)
      
    
    args =  tuple(closure_args) + tuple(args)
    
    if isinstance(fn, UntypedFn):
      arg_types = get_types(args)
      from .. import type_inference
      fn = type_inference.specialize(fn, arg_types)
      
    if loopify or lower: 
      from  ..transforms import pipeline
      if loopify: 
        fn = pipeline.loopify(fn)
      if lower: 
        fn = pipeline.lowering(fn)
    
    # don't generate Call nodes for identity function 
    if self.is_identity_fn(fn):
      assert len(args) == 1
      return args[0]
    
  
    call = Call(fn, args, type = fn.return_type)

    if fn.return_type == NoneType:
      self.insert_stmt(ExprStmt(call))
      return none
    else:
      if name is None:
        name = "call_result"
      return self.assign_name(call, name)

  def call(self, fn, args, name = None):
    return self.invoke(fn, args, name = name) 

  def call_shape(self, maybe_clos, args):
    from ..shape_inference import call_shape_expr, shape_codegen
    fn = self.get_fn(maybe_clos)
    closure_args = self.closure_elts(maybe_clos)
    combined_args = tuple(closure_args) + tuple(args)
    if isinstance(fn, UntypedFn):
      # if we're given an untyped function, first specialize it
      from ..type_inference import specialize       
      fn = specialize(fn, get_types(combined_args))
      from ..transforms import pipeline 
      fn = pipeline.high_level_optimizations(fn)
    abstract_shape = call_shape_expr(fn)
    return shape_codegen.make_shape_expr(self, abstract_shape, combined_args)


########NEW FILE########
__FILENAME__ = core_builder
from dsltools import NestedBlocks

from .. import syntax, names 
from ..ndtypes import (make_tuple_type, 
                       Int32, Int64, SliceT, TupleT, ScalarT, StructT, NoneT, 
                       ArrayT, FnT, ClosureT, NoneType) 
from ..syntax import (Assign, Return, If,    
                      ArrayView, Attribute, Cast, Const, Closure,  Comment, Expr, 
                      Index, PrimCall,   Struct, Slice, Tuple, TupleProj, TypedFn, Var, 
                      AllocArray, ArrayExpr, Adverb, Select)

from ..syntax.helpers import (const_bool, const_int, get_types,  
                              one_i64, zero, none)

class CoreBuilder(object):
  none = syntax.none
  null_slice = syntax.slice_none
  
  def __init__(self, type_env = None, blocks = None):
    if type_env is None:
      type_env = {}
    self.type_env = type_env 
    if blocks is None: 
      blocks = NestedBlocks()
      
    self.blocks = blocks 

    # cut down the number of created nodes by
    # remembering which tuple variables we've created
    # and looking up their elements directly
    self.tuple_elt_cache = {}

  def insert_stmt(self, stmt):
    self.blocks.append_to_current(stmt)

  def append(self, stmt):
    self.blocks.append_to_current(stmt)
  
  def comment(self, text):
    self.blocks.append(Comment(text))
  
  def assign(self, lhs, rhs):
    self.insert_stmt(Assign(lhs, rhs))
  
  def return_(self, value):
    self.blocks += [Return(value)]  
  
  def return_none(self):
    self.return_(none)
  
  def return_tuple(self, values):
    self.return_(self.tuple(values, name = None))
    
  def fresh_var(self, t, prefix = "temp"):
    assert prefix is not None
    assert t is not None, "Type required for new variable %s" % prefix
    prefix = names.original(prefix)
    ssa_id = names.fresh(prefix)
    self.type_env[ssa_id] = t
    return Var(ssa_id, type = t)


  def if_(self, cond, true_thunk, false_thunk, result_vars = ()):
    """
    Args:
      cond - the condition expression 
      true_thunk : given some variables, construct the true branch
      false_thunk : given other variables, construct the false branch
      result_vars : the merged version of the values from the two branches
    """
    merge = {}
    assert all(expr.__class__ is Var for expr in result_vars)
    left_vars = []
    right_vars = []
    for result_var in result_vars:
      name = result_var.name 
      t = result_var.type
      left_var = self.fresh_var(t, prefix = "if_true_" + names.original(name) )
      left_vars.append(left_var)
      right_var = self.fresh_var(t, prefix = "if_false_" + names.original(name) )
      right_vars.append(right_var)
      merge[result_var.name] = (left_var, right_var)
    
    self.blocks.push()
    true_thunk(*left_vars)
    true_block = self.blocks.pop()
    
    self.blocks.push()
    false_thunk(*right_vars)
    false_block = self.blocks.pop()
     
    stmt = If(cond, true_block, false_block, merge = merge)
    self.insert_stmt(stmt)
    return stmt 

  def fresh_i32(self, prefix = "temp"):
    return self.fresh_var(Int32, prefix)

  def fresh_i64(self, prefix = "temp"):
    return self.fresh_var(Int64, prefix)

  
  def temp_name(self, expr):
    c = expr.__class__
    if c is PrimCall:
      return expr.prim.name
    elif c is Attribute:
      if expr.value.__class__ is Var:
        return names.original(expr.value.name) + "_" + expr.name
      else:
        return expr.name
    elif c is Attribute:
      if expr.value.__class__ is Var:
        return "%s_%s" % (expr.value.name, expr.name)
      else:
        return expr.name
    elif c is Index:
      idx_t = expr.index.type
      if isinstance(idx_t, SliceT) or \
         (isinstance(idx_t, TupleT) and \
          any(isinstance(elt_t, SliceT) for elt_t in idx_t.elt_types)):
        return "slice"
      else:
        return "elt"
    elif c is TupleProj:
      if expr.tuple.__class__ is Var:
        original = names.original(expr.tuple.name)
        return "%s%d_elt%d" % (original, names.versions[original], expr.index)
      else:
        return "tuple_elt%d" % expr.index
    elif c is Var:
      return names.refresh(expr.name) 
    else:
      return "temp"

  def is_simple(self, expr):
    c = expr.__class__
    return c is Var or \
           c is Const or \
           (c is Tuple and len(expr.elts) == 0) or \
           (c is Struct and len(expr.args) == 0) or \
           (c is Closure and len(expr.args) == 0)


  def is_pure(self, expr):
    c = expr.__class__
    return c is Var or \
           c is Const or \
           c is Tuple or \
           c is Struct or \
           c is Closure or \
           c is AllocArray or \
           c is ArrayView or \
           c is PrimCall or \
           isinstance(expr, ArrayExpr) or \
           isinstance(expr, Adverb)
           
    



  def assign_name(self, expr, name = None):
    #if self.is_simple(expr):
    #  return expr
    if name is None:
      name = self.temp_name(expr)
    
    var = self.fresh_var(expr.type, names.refresh(name))
    self.assign(var, expr)
    return var

  def int(self, x, name = None):
    if name is None: 
      return const_int(x)
    else:
      return self.assign_name(const_int(x), name)

  def bool(self, x, name = None):
    if name is None: 
      return const_bool(x)
    else:
      return self.assign_name(const_bool(x), name)

  def zero(self, t = Int32, name = None):
    if name is None:
      return zero(t)
    else:
      return self.assign_name(zero(t), name)

  def zero_i32(self, name = None):
      return self.zero(t = Int32, name = name)

  def zero_i64(self, name = None):
    return self.zero(t = Int64, name = name)

  def cast(self, expr, t):
    if expr.type == t:
      return expr
    assert isinstance(t, ScalarT), \
        "Can't cast %s : %s to non-scalar type %s" % (expr, expr.type, t)
    return self.assign_name(Cast(expr, type = t), "cast_%s" % t)
  


  def attr(self, obj, field, name = None, temp = True):
    if temp and name is None:
      name = field
    obj_t = obj.type
    c = obj.__class__ 
    if c is Struct:
      pos = obj_t.field_pos(name)
      result =  obj.args[pos]
    elif c in (ArrayView, Slice):
      result = getattr(obj, field)
    else:
      assert isinstance(obj_t, StructT), \
        "Can't get attribute '%s' from type %s" % (field, obj_t)
      field_t = obj.type.field_type(field)
      if field_t == NoneType:
        result = none 
      else:
        result = Attribute(obj, field, type = field_t)
    if name and result.__class__ not in (Var, Const):
      return self.assign_name(result, name)
    else:
      return result

  def is_none(self, x):
    return x is None or (isinstance(x, Expr) and x.type.__class__ is NoneT)


  def tuple(self, elts, name = None, explicit_struct = False):
    if not isinstance(elts, (list, tuple)):
      elts = [elts]
    tuple_t = make_tuple_type(get_types(elts))
    if explicit_struct:
      tuple_expr = Struct(elts, type = tuple_t)
    else:
      tuple_expr = Tuple(elts, type = tuple_t)
    if name:
      result_var = self.assign_name(tuple_expr, name)
      # cache the simple elements so we can look them up directly
      for (i, elt) in enumerate(elts):
        if self.is_simple(elt):
          self.tuple_elt_cache[(result_var.name, i)] = elt
      return result_var
    else:
      return tuple_expr

  def is_tuple(self, x):
    try:
      return x.type.__class__ is TupleT
    except:
      return False
    
  def is_array(self, x):
    return x.type.__class__ is  ArrayT
  
  def is_fn(self, x):
    return x.__class__ in (TypedFn, Closure) or isinstance(x.type, (FnT, ClosureT)) 
  
  def num_tuple_elts(self, x):
    assert self.is_tuple(x)
    return len(x.type.elt_types)
  
  def concat_tuples(self, x, y, name = "concat_tuple"):
    
    x_is_tuple = self.is_tuple(x)
    y_is_tuple = self.is_tuple(y)
    
    if x_is_tuple and y_is_tuple:
      if self.num_tuple_elts(x) == 0:
        return y 
      elif self.num_tuple_elts(y) == 0:
        return x 
      
    if x_is_tuple:
      x_elts = self.tuple_elts(x)
    else:
      x_elts = (x,)
    if y_is_tuple:
      y_elts = self.tuple_elts(y)
    else:
      y_elts = (y,)

    elts = []
    elts.extend(x_elts)
    elts.extend(y_elts)
    return self.tuple(elts, name = name)

  def tuple_proj(self, tup, idx, explicit_struct = False):
    assert isinstance(idx, (int, long))
    if isinstance(tup, Tuple):
      return tup.elts[idx]
    elif isinstance(tup, tuple):
      return tup[idx]
    elif tup.__class__ is Var and (tup.name, idx) in self.tuple_elt_cache:
      return self.tuple_elt_cache[(tup.name, idx)]
    elif explicit_struct:
      return Attribute(tup, "elt%d" % idx, type = tup.type.elt_types[idx])
    else:
      return TupleProj(tup, idx, type = tup.type.elt_types[idx])

  def tuple_elts(self, tup, explicit_struct = False):
    if not isinstance(tup.type, TupleT):
      return [tup]
    nelts = len(tup.type.elt_types)
    return tuple([self.tuple_proj(tup, i, explicit_struct = explicit_struct)
                  for i in xrange(nelts)])

  def prod(self, elts, name = None):
    if self.is_tuple(elts):
      elts = self.tuple_elts(elts)
    if len(elts) == 0:
      return one_i64
    else:
      result = elts[0]
      for e in elts[1:]:
        result = self.mul(result, e, name = name)
      return result
    
  def select(self, cond, true_value, false_value, name = None):
    assert true_value.type == false_value.type
    expr = Select(cond, true_value, false_value, type = true_value.type)
    if name is None:
      return expr
    else:
      return self.assign_name(expr, name)
########NEW FILE########
__FILENAME__ = loop_builder

from .. import syntax 
from ..ndtypes import ScalarT, TupleT 
from ..syntax import Expr, While, ForLoop, Const 
from ..syntax.helpers import zero, one, zero_i32, zero_i64, wrap_if_constant

from array_builder import ArrayBuilder 

class LoopBuilder(ArrayBuilder):
  
  """
  Builder for loops and things that use loops
  """
  
  
  
  def loop_var(self, name = "i", start_val = syntax.zero_i64):
    """
    Generate three SSA variables to use as the before/during/after values
    of a loop counter throughout some loop.

    By default initialize the counter to zero, but optionally start at different
    values using the 'start_val' keyword.
    """

    start_val = wrap_if_constant(start_val)
    counter_type = start_val.type

    counter_before = self.assign_name(start_val, name + "_before")
    counter = self.fresh_var(counter_type, name)
    counter_after = self.fresh_var(counter_type, name + "_after")
    merge = {counter.name:(counter_before, counter_after)}
    return counter, counter_after, merge

  def loop(self, 
           start, 
           niters, 
           loop_body,
           return_stmt = False,
           while_loop = False,
           step = None, 
           merge = None):
    
    if isinstance(start, (int,long)):
      start = self.int(start)
      
    assert isinstance(start, Expr)
    
    if isinstance(niters, (int,long)):
      niters = self.int(niters)
      
    assert isinstance(niters, Expr)
    
    if step is None:
      step = one(start.type)
    elif isinstance(step, (int, long)):
      step = self.int(step)
    
    assert isinstance(step, Expr)
    
    if merge is None:
      merge = {}
      
    if while_loop:
      i, i_after, i_merge = self.loop_var("i", start)
      merge.update(i_merge)
      cond = self.lt(i, niters)
      self.blocks.push()
      loop_body(i)
      self.assign(i_after, self.add(i, step))
      body = self.blocks.pop()
      loop_stmt = While(cond, body, merge)
    else:
      var = self.fresh_var(start.type, "i")
      self.blocks.push()
      loop_body(var)
      body = self.blocks.pop()
      loop_stmt = ForLoop(var, start, niters, step, body, merge)

    if return_stmt:
      return loop_stmt
    else:
      self.blocks += loop_stmt


  class Accumulator:
    def __init__(self, acc_type, fresh_var, assign):
      self.acc_type = acc_type
      self.fresh_var = fresh_var
      self.assign = assign
      self.start_var = fresh_var(acc_type, "acc")
      self.curr_var = self.start_var

    def get(self):
      return self.curr_var

    def update(self, new_value):
      new_var = self.fresh_var(self.acc_type, "acc")
      self.assign(new_var, new_value)
      self.curr_var = new_var

  def mk_acc(self, init):
    return self.Accumulator(init.type, self.fresh_var, self.assign)
    
  def accumulate_loop(self, start, stop, loop_body, init, return_stmt = False):
    acc = self.mk_acc(init)
    def loop_body_with_acc(i):
      loop_body(acc, i)
    loop_stmt = self.loop(start, stop, loop_body_with_acc, return_stmt = True)
    loop_stmt.merge[acc.start_var.name] = (init, acc.curr_var)
    if return_stmt:
      return loop_stmt, acc.start_var
    else:
      self.blocks += loop_stmt
      return acc.start_var 
    
  def array_copy(self, src, dest, return_stmt = False):
    assert self.is_array(dest)
    shape = self.shape(dest)
    dims = self.tuple_elts(shape)
    rank = len(dims)
    index_vars = []
    def create_loops():
      i = len(index_vars)
      def loop_body(index_var):
        index_vars.append(index_var)
        if i+1 == rank:
          index_tuple = self.tuple(index_vars, "idx")
          lhs = self.index(dest, index_tuple, temp=False)
          rhs = self.index(src, index_tuple, temp=True)
          self.assign(lhs, rhs)
        else:
          self.blocks += create_loops()
      start = syntax.zero_i64
      stop = dims[i]
      if i > 0 or return_stmt:
        return self.loop(start, stop, loop_body, True)
      else:
        return self.loop(start, stop, loop_body, return_stmt)

    return create_loops()
  
  def _to_list(self, bounds):
    if isinstance(bounds, Expr):
      if isinstance(bounds.type, ScalarT):
        return [bounds]
      else:
        assert isinstance(bounds.type, TupleT), \
          "Expected tuple but got %s : %s" % (bounds, bounds.type) 
        return self.tuple_elts(bounds)
    elif isinstance(bounds, (int,long)):
      return [bounds]
    else:
      assert isinstance(bounds, (tuple,list))
      return bounds 
  
  
  def nested_loops(self, 
                     upper_bounds, 
                     loop_body, 
                     lower_bounds = None, 
                     step_sizes = None, 
                     index_vars_as_list = False):
    upper_bounds = self._to_list(upper_bounds)
    
    n_loops = len(upper_bounds)
    assert lower_bounds is None or len(lower_bounds) == n_loops
    assert step_sizes is None or len(step_sizes) == n_loops 
    
    if lower_bounds is None:
      lower_bounds = [self.int(0) for _ in upper_bounds]
    else:
      lower_bounds = self._to_list(lower_bounds)
    
    if step_sizes is None:
      step_sizes = [self.int(1) for _ in upper_bounds]
    else:
      step_sizes = self._to_list(step_sizes)
    
    
    def build_loops(index_vars = ()):
      n_indices = len(index_vars)
      if n_indices == n_loops:
        if isinstance(loop_body, Expr):
          input_types = self.input_types(loop_body)
          if len(input_types) == len(index_vars):
            result = self.call(loop_body, index_vars)
          else:
            result = self.call(loop_body, [self.tuple(index_vars)])
        else:
          if index_vars_as_list:
            idx_tuple = list(index_vars)
          elif n_indices > 1:
            idx_tuple = self.tuple(index_vars)
          else:
            idx_tuple = index_vars[0]
          assert hasattr(loop_body, '__call__'), "Expected callable value, got %s" % (loop_body,)
          result = loop_body(idx_tuple)
        assert self.is_none(result), "Expected loop body to return None, not %s" % (result,)
      else:
        def inner_loop_body(idx):
          build_loops(index_vars + (idx,))
        lower = lower_bounds[n_indices]
        upper = upper_bounds[n_indices]  
        step = step_sizes[n_indices]
        # sanity check the bounds 
        if lower.__class__ is Const and upper.__class__ is Const and step.__class__ is Const:
          if step.value < 0:
            assert lower.value > upper.value, \
              "Attempting to build invalid loop from %s to %s by %s" % (lower, upper, step)
          elif step.value > 0:
            assert upper.value > lower.value, \
              "Attempting to build invalid loop from %s to %s by %s" % (lower, upper, step) 
        self.loop(lower, upper, inner_loop_body, step=step)
    build_loops()
########NEW FILE########
__FILENAME__ = config
  ######################################
#          BACKEND SELECTION         #
######################################
#
#  'auto'  : use either 'c' or 'openmp' depending on availability of OpenMP
#  'c': sequential, use gcc or clang to compile
#  'openmp': multi-threaded execution for array operations, requires gcc 4.4+
#  'llvm': deprecated
#  'interp': interpreter, will be dreadfully slow
#  'cuda': experimental GPU support
#

from system_info import openmp_available
backend = 'openmp' if openmp_available else 'c'

######################################
#        PARAKEET OPTIMIZATIONS      #
######################################
  
    
opt_inline = True

opt_fusion = True
opt_combine_nested_maps = True

opt_specialize_fn_args = True 

opt_index_elimination = True
opt_range_propagation = True

opt_licm = True
opt_redundant_load_elimination = True
opt_stack_allocation = True
opt_shape_elim = True 

# replace 
#   a = alloc
#   ...
#   b[i:j] = a
#
#   with 
#     a = b[i:j]  
# TODO: fix it, currently breaks rule30
opt_copy_elimination = False

# may dramatically increase compile time
opt_loop_unrolling = False

# suspiciously complex optimizations may introduce bugs 
# TODO: comb through carefully 
opt_scalar_replacement = False

# experimental!
opt_simplify_array_operators = False
    
# run verifier after each transformation 
opt_verify = False

# recompile functions for distinct patterns of unit strides and 0 or 1 input values 
value_specialization = True 



#####################################
#            DEBUG OUTPUT           #
#####################################

# show untyped IR after it's translated from Python?
print_untyped_function = False

# show the higher level typed function after specialization?
print_specialized_function = False 

# show function after all data adverbs like Map/Reduce/Scan have been 
# lowered to use indexing explicitly into their inputs 
print_indexified_function = False

# print function after all adverbs have been turned to loops
print_loopy_function = False

# show lower level typed function before
# it gets translated to LLVM?
print_lowered_function = False

# before starting function specialization, print the fn name and input types 
print_before_specialization = False

# show the input function to each transformation?
print_functions_before_transforms =  []
                                        
# show the function produced by each transformation?
print_functions_after_transforms =   []

# show aliases and escape sets
print_escape_analysis = False

# how long did each transform take?
print_transform_timings = False

# print each transform's name when it runs
print_transform_names = False

# at exit, print the names of all specialized functions
print_specialized_function_names = False

# tell the backend to print whatever code it generates, 
# whether it's C, CUDA, or LLVM 
print_generated_code = False 

#####################################
#         DESPERATE MEASURES        #
#####################################

testing_find_broken_transform = False 


########NEW FILE########
__FILENAME__ = config
threads_per_block_dim = 16
blocks_per_sm = 128
compute_capability = (1,3)
arch = "sm_%d%d" % compute_capability
########NEW FILE########
__FILENAME__ = cuda_compiler
import numpy as np 

from .. import names
from ..builder import build_fn
# from ..c_backend import FnCompiler
from ..ndtypes import TupleT,  Int32, FnT, ScalarT, SliceT, NoneT, ArrayT, ClosureT
from ..c_backend import PyModuleCompiler
from ..openmp_backend import MulticoreCompiler
from ..syntax import PrintString, SourceExpr
from ..syntax.helpers import get_types, get_fn, get_closure_args, const_int, zero_i32, one_i32

import config 
import device_info 
from cuda_syntax import threadIdx, blockIdx, blockDim 


class CudaCompiler(MulticoreCompiler):
  
  def __init__(self, *args, **kwargs):
    # keep track of which kernels we've already compiled 
    # and map the name of the nested function to its kernel name
    self._kernel_cache = {}
    if 'gpu_depth' in kwargs:
      self.gpu_depth = kwargs['gpu_depth'] 
      del kwargs['gpu_depth']
    else:
      self.gpu_depth = 0
    
    self.device = device_info.best_cuda_device()
    assert self.device, "No GPU found"
    
    self.use_constant_memory_for_args = True
    MulticoreCompiler.__init__(self, 
                               compiler_cmd = ['nvcc', '-arch=%s' % config.arch],
                               extra_compile_flags = ['-fopenmp'],
                               extra_link_flags = ['-lcudart', '-lgomp'], 
                               src_extension = '.cu',  
                               compiler_flag_prefix = '-Xcompiler',
                               linker_flag_prefix = '-Xlinker', 
                               *args, **kwargs)
  
  @property 
  def cache_key(self):
    return self.__class__, self.depth > 0, max(self.gpu_depth, 2) 
  
  def enter_module_body(self):
    self.append('cudaSetDevice(%d);' % device_info.device_id(self.device))
  
  def build_kernel(self, clos, bounds, write_only = None):
    n_indices = len(bounds)
    fn = get_fn(clos)
    outer_closure_exprs = get_closure_args(clos)
    closure_arg_types = get_types(outer_closure_exprs)
    host_closure_args = self.visit_expr_list(outer_closure_exprs)
    
    self.comment("Copying data from closure arguments to the GPU")
    
    if write_only is None:
      write_only = [False] * len(host_closure_args)
    assert len(write_only) >= len(host_closure_args) 
    
    gpu_closure_args = self.args_to_gpu(host_closure_args, closure_arg_types, write_only)
    input_types = fn.input_types 

    kernel_name = names.fresh("kernel_" + fn.name)
    
    if isinstance(input_types[-1], TupleT):
      index_types = input_types[-1].elt_types   
      index_as_tuple = True
      # if function takes a tuple of 
      closure_arg_types = input_types[:-1]
    else:
      index_types = input_types[-n_indices:]
      index_as_tuple = False
      closure_arg_types = input_types[:-n_indices]
      
    n_closure_args = len(closure_arg_types)
    assert len(outer_closure_exprs) == n_closure_args, \
      "Mismatch between closure formal args %s and given %s" % (", ".join(closure_arg_types),
                                                                ", ".join(outer_closure_exprs))
    bound_types = (Int32,) * n_indices
    
    if self.use_constant_memory_for_args:
      outer_input_types = tuple(bound_types)
    else:
      outer_input_types = tuple(closure_arg_types) + bound_types
      
    
        
    
    parakeet_kernel, builder, input_vars  = build_fn(outer_input_types, name = kernel_name)
    
    if self.use_constant_memory_for_args:
      inner_closure_vars = []
      for i, t in enumerate(closure_arg_types):
        raw_name = fn.arg_names[i]
        self.comment("Moving GPU arg #%d %s : %s to constant memory" % (i, raw_name, t))
        const_name = self.fresh_name("gpu_arg_"  + raw_name)
        typename = self.to_ctype(t)
        self.add_decl("__constant__ %s %s" % (typename, const_name))
        inner_closure_vars.append(SourceExpr(const_name, type=t))
        gpu_arg = gpu_closure_args[i]
        # in case this is a constant, should assign to a variable
        first_char = gpu_arg[0]
        if not first_char.isalpha():
          gpu_arg = self.fresh_var(typename, "src_" + raw_name, gpu_arg)
        self.append("cudaMemcpyToSymbolAsync(%s, &%s, sizeof(%s));" % (const_name, gpu_arg, typename))
      inner_closure_vars = tuple(inner_closure_vars)
      # need to do a cudaMemcpyToSymbol for each gpu arg   
      bounds_vars = input_vars
  
    else:
      # TODO: use these to compute indices when n_indices > 3 or 
      # number of threads per block > 1  
      inner_closure_vars = input_vars[:n_closure_args]
      bounds_vars = input_vars[n_closure_args:(n_closure_args + n_indices)]


    THREADS_PER_DIM = config.threads_per_block_dim 

    
    if n_indices == 1:
      elts_per_block = builder.div(bounds_vars[0],  
                                   const_int(THREADS_PER_DIM**2, Int32), 
                                   "elts_per_block")
      elts_per_block = builder.add(elts_per_block, one_i32, "elts_per_block")
      base_idx = builder.mul(elts_per_block, blockIdx.x, "base_x")
      start_idx = builder.add(base_idx, threadIdx.x, "start_x")  
      
      start_indices = [start_idx]
    
      stop_x = builder.mul(builder.add(one_i32, blockIdx.x, "next_base_x"), 
                           elts_per_block, "stop_x")
      stop_x = builder.min(stop_x, bounds_vars[0], "stop_x") 
      stop_indices = [stop_x]
      step_sizes = [const_int(THREADS_PER_DIM**2, Int32)]
       
    else:
      elts_per_block = builder.div(bounds_vars[0],   
                                   const_int(THREADS_PER_DIM, Int32),
                                    "elts_per_block")
      elts_per_block = builder.add(elts_per_block, one_i32, "elts_per_block")
      base_x = builder.mul(elts_per_block, blockIdx.x, "base_x")
      start_x = builder.add(base_x, threadIdx.x, "start_x")  
      start_y = builder.add(builder.mul(elts_per_block, blockIdx.y, "base_y"),
                                threadIdx.y, "start_y")
      
      start_indices = [start_x, start_y]
      
      stop_x = builder.mul(builder.add(one_i32, blockIdx.x, "next_base_x"), 
                           elts_per_block, "stop_x")
      stop_x = builder.min(stop_x, bounds_vars[0], "stop_x") 
      stop_indices = [stop_x]
      stop_y = builder.mul(builder.add(one_i32, blockIdx.y, "next_base_y"),
                           elts_per_block, "stop_y")
      stop_y = builder.min(stop_y, bounds_vars[1], "stop_y")
       
      stop_indices = [stop_x, stop_y]
      
      step_sizes = [const_int(THREADS_PER_DIM, Int32), const_int(THREADS_PER_DIM, Int32)]
      for i in xrange(2, n_indices):
        start_indices.append(zero_i32)
        step_sizes.append(one_i32)
        stop_indices.append(bounds_vars[i])
    
    
    
    def loop_body(index_vars):
      if not isinstance(index_vars, (list,tuple)):
        pass
      indices = [builder.cast(idx, t) for idx, t in zip(index_vars,index_types)]
      if index_as_tuple:
        index_args = (builder.tuple(indices),)
      else:
        index_args = indices
      inner_args = tuple(inner_closure_vars) + tuple(index_args)
      builder.call(fn, inner_args)
      
    builder.nested_loops(stop_indices, loop_body, start_indices, step_sizes, index_vars_as_list = True)
    
    self.enter_kernel()
    c_kernel_name = self.get_fn_name(parakeet_kernel, 
                                     attributes = ["__global__"], 
                                     inline = False)
    self.exit_kernel()
    
    # set cache preference of the kernel we just built 
    self.append("cudaFuncSetCacheConfig(%s, cudaFuncCachePreferL1);" % c_kernel_name)
    # self._kernel_cache[key] = c_kernel_name
    return c_kernel_name, gpu_closure_args, host_closure_args, closure_arg_types
  
  def launch_kernel(self, bounds, params, kernel_name):
    self.synchronize("Done copying arguments to GPU, prepare for kernel launch")
    
    n_bounds = len(bounds)
    sm_count = device_info.num_multiprocessors(self.device)
    n_blocks = sm_count * config.blocks_per_sm 
    
    THREADS_PER_DIM = config.threads_per_block_dim
    
    if n_bounds == 1:
      grid_dims = [n_blocks, 1, 1]
      block_dims = [THREADS_PER_DIM**2, 1, 1]
    else:
      blocks_per_axis = int(np.ceil(np.sqrt(n_blocks)))
      grid_dims = [blocks_per_axis, blocks_per_axis, 1]
      block_dims = [THREADS_PER_DIM,THREADS_PER_DIM,1]

    grid_dims_str = "dim3(%s)" % ", ".join( str(d) for d in grid_dims)
    block_dims_str = "dim3(%s)" % ", ".join( str(d) for d in block_dims)
    
    self.comment("kernel launch")
    
    kernel_args_str = ", ".join(params)
    self.append("%s<<<%s, %s>>>(%s);" % (kernel_name, grid_dims_str, block_dims_str, kernel_args_str))
  
    self.comment("After launching kernel, synchronize to make sure the computation is done")
    self.synchronize("Finished kernel launch")
    
    
  
  def visit_ParFor(self, stmt):
    bounds = self.tuple_to_var_list(stmt.bounds)

    n_indices = len(bounds)
    if n_indices > 5 or not self.in_host():
      return MulticoreCompiler.visit_ParFor(self, stmt)

    # rely on annotation to tell us which arguments don't 
    # have to be moved to/from the GPU
    write_only = getattr(stmt, 'write_only')
    kernel_name, gpu_closure_args, host_closure_args, closure_arg_types  = \
      self.build_kernel(stmt.fn, bounds, write_only)
    
    if self.use_constant_memory_for_args:
      params = bounds 
    else:
      params = tuple(gpu_closure_args) + tuple(bounds)
      
    self.launch_kernel(bounds, params, kernel_name)
    
    self.comment("copy arguments back from the GPU to the host")
    
    read_only = getattr(stmt, 'read_only')
    self.list_to_host(host_closure_args, gpu_closure_args, closure_arg_types, read_only)
    return "/* done with ParFor */"
  
  
  def in_host(self):
    return self.gpu_depth == 0
  
  def in_block(self):
    return self.gpu_depth == 1
  
  def in_gpu(self):
    return self.gpu_depth > 0
  
  def get_fn_name(self, fn_expr, attributes = [], inline = True):
    if self.in_gpu() and not attributes:
      attributes = ["__device__"] 
    kwargs = {'depth':self.depth, 'gpu_depth':self.gpu_depth}
    return PyModuleCompiler.get_fn_name(self, fn_expr, 
                                        compiler_kwargs = kwargs,
                                        attributes = attributes, 
                                        inline = inline)
    

  
  def enter_kernel(self):
    """
    Keep a stack of adverb contexts so we know when we're global vs. block vs. thread
    """
    self.depth += 1
    self.gpu_depth += 1  
  
  def exit_kernel(self):
    self.depth -= 1
    self.gpu_depth -= 1 
  
  
  def in_thread(self):
    return self.depth > 1
  
  def pass_by_value(self, t):
    if isinstance(t, (ScalarT, NoneT, SliceT, FnT)):
      return True 
    elif isinstance(t, TupleT):
      return all(self.pass_by_value(elt_t) for elt_t in t.elt_types)
    elif isinstance(t, ClosureT):
      return all(self.pass_by_value(elt_t) for elt_t in t.arg_types)
    return False 
  
  
  def check_gpu_error(self, context = None, error_code_var = None):
    if error_code_var is None:
      error_code_var = self.fresh_name("cuda_err")
      self.append("cudaError %s = cudaGetLastError();" % error_code_var)
    if context is None:
      context = "\"Generated CUDA source at \" __FILE__ \" : \" __LINE__"
    self.append("""
      if ( cudaSuccess != %s ) {
        printf( "Error after %s: %%s\\n",  cudaGetErrorString(%s) );
      }
    """ % (error_code_var, context, error_code_var))
    
    if self.module_entry:
      self.append("""
        /* TODO: use setjmp/longjmp or C++ exceptions to propagate exceptions 
           outside of the entry function 
        */ 
        if (cudaSuccess != %s) {
          PyErr_Format(PyExc_SystemError, 
                      "Parakeet GPU Error after %%s: %%s", 
                      "%s", 
                      cudaGetErrorString(%s)); 
          return NULL; 
        }
      """ % (error_code_var, context, error_code_var))
          
  
  def synchronize(self, context = None):
    error_code_var = self.fresh_name("cuda_err")
    self.append("cudaError %s = cudaDeviceSynchronize();" % error_code_var)
    self.check_gpu_error(context, error_code_var)
    
  
  def _to_gpu(self, c_expr, t, gpu_array_dict, memcpy = True):
    if self.pass_by_value(t):
      return c_expr
    elif isinstance(t, ArrayT):
      
      ptr_t = "%s*" % self.to_ctype(t.elt_type)
      bytes_per_elt = t.elt_type.dtype.itemsize
      
      dst = self.fresh_var(ptr_t, "gpu_ptr")
      nelts = "%s.size" % c_expr
      nbytes = self.fresh_var("int64_t", "nbytes", "%s * %d" % (nelts, bytes_per_elt))
      
      if memcpy:
        src = "%s.data.raw_ptr" % c_expr   
        memcpy_stmt = "cudaMemcpyAsync(%s, %s, %s, cudaMemcpyHostToDevice);" % (dst, src, nbytes)
      else:
        memcpy_stmt = "/* no memcpy for this %s : %s */" % (c_expr, t)
      
      # allocate the destination pointer on the GPU
      malloc = "cudaMalloc( (void**) &%s, %s);" % (dst, nbytes)
      alloc_and_copy_block = "%s\n%s" % (malloc, memcpy_stmt)

      """
      TODO: do this to cut down on allocations but also
      need to track which arrays have been allocated so we 
      can delete them later 
      
      if t in gpu_array_dict:
        # if we've allocated some other args of the same type, 
        # try reusing their pointers before allocating new memory 
        for i, (prev_host, prev_gpu) in enumerate(gpu_array_dict[t]):
          cond = "%s.data.raw_ptr == %s.data.raw_ptr && %s.size == %s.size" % \
            (c_expr, prev_host, c_expr, prev_host)
          self.append("%sif (%s) {%s = %s.data.raw_ptr;}" % \
                        ("else " if i > 0 else "", cond, dst, prev_gpu))
        self.append("else {%s}" % alloc_and_copy_block)
      else:
      """
      self.append(alloc_and_copy_block)
      
      self.check_gpu_error("cudaMalloc for %s : %s" % (c_expr, t))
      
      # make an identical array descriptor but change its data pointer to the GPU location
      gpu_descriptor = self.fresh_var(self.to_ctype(t), "gpu_array", c_expr)
      self.append("%s.data.raw_ptr = %s;" % (gpu_descriptor, dst))    
      gpu_array_dict.setdefault(t, []).append( (c_expr, gpu_descriptor) )
      return gpu_descriptor
    
    elif isinstance(t, (ClosureT, TupleT)):
      # copy contents of the host tuple into another struct
      gpu_tuple = self.fresh_var(self.to_ctype(t), "gpu_tuple", c_expr)
      for i, elt_t in enumerate(t.elt_types):
        host_elt = "%s.elt%d" % (c_expr, i)
        gpu_elt = self._to_gpu(host_elt, elt_t, gpu_array_dict)
        self.append("%s.elt%d = %s;" % (c_expr, i, gpu_elt))
      return gpu_tuple 
    else:
      assert False, "Unsupported type in CUDA backend %s" % t 
  
  def args_to_gpu(self, host_values, types, write_only):
    # keep track of arrays we've already moved to the GPU, 
    # indexed by their type  
    gpu_array_dict = {}
    
    gpu_args = []
    for i,t in enumerate(types):
      gpu_arg = self._to_gpu(host_values[i], t, gpu_array_dict, memcpy = not write_only[i])
      gpu_args.append(gpu_arg)
    return gpu_args 
  
  
  def to_host(self, host_value, gpu_value, t):
    if self.pass_by_value(t):
      return
    elif isinstance(t, ArrayT):
      dst = "%s.data.raw_ptr" % host_value 
      src = "%s.data.raw_ptr"  % gpu_value 
      nelts = "%s.size" % gpu_value
      nbytes = "%s * %d" % (nelts, t.elt_type.dtype.itemsize)  
      self.append("cudaMemcpy(%s, %s, %s, cudaMemcpyDeviceToHost);" % (dst, src, nbytes) )
      self.append("cudaFree(%s);" % src) 

    elif isinstance(t, (ClosureT, TupleT)):
      for i, elt_t in enumerate(t.elt_types):
        host_elt = "%s.elt%d" % (host_value, i)
        gpu_elt = "%s.elt%d" % (gpu_value, i)
        self.to_host(host_elt, gpu_elt, elt_t)
    else:
      assert False, "Unsupported type in CUDA backend %s" % t 
      
  def list_to_host(self, host_values, gpu_values, types, skip = None):
    for i, t in enumerate(types):
      if skip is None or not skip[i]:
        host_value = host_values[i]
        gpu_value = gpu_values[i]
        self.to_host(host_value, gpu_value, t)
    
  def visit_Alloc(self, expr):
    assert self.in_host(), "Can't dynamically allocate memory in GPU code"
    return MulticoreCompiler.visit_Alloc(self, expr)
  
  def visit_AllocArray(self, expr):
    assert self.in_host(), "Can't dynamically allocate memory in GPU code"
    return MulticoreCompiler.visit_AllocArray(self, expr)


  def use_closure(self, clos):
    fn = get_fn(clos)
    closure_exprs = get_closure_args(clos)
    closure_arg_types = get_types(closure_exprs)
    host_closure_args = self.visit_expr_list(closure_exprs)
    gpu_closure_args = self.args_to_gpu(host_closure_args, closure_arg_types)
    return fn, closure_exprs, closure_arg_types, host_closure_args, gpu_closure_args 
  
  def build_1d_reduce_kernel(self, map_clos, combine_clos, count):
    n_indices = 1
    map_fn, map_closure_exprs, map_closure_arg_types, \
      map_host_closure_args, map_gpu_closure_args = self.use_closure(map_clos)
    combine_fn, combine_closure_exprs, combine_closure_arg_types, \
      combine_host_closure_args, combine_gpu_closure_args = self.use_closure(combine_clos)
    
    self.comment("Copying data from closure arguments to the GPU")
   
    map_input_types = map_fn.input_types
    combine_input_types = combine_fn.input_types 

    kernel_name = names.fresh("reduce_kernel_" + map_fn.name + "_" + combine_fn.name)
    
    if isinstance(map_input_types[-1], TupleT):
      index_types = map_input_types[-1].elt_types   
      index_as_tuple = True
      # if function takes a tuple of 
      map_closure_arg_types = map_input_types[:-1]
    else:
      index_types = map_input_types[-n_indices:]
      index_as_tuple = False
      closure_arg_types = map_input_types[:-n_indices]
      
    n_closure_args = len(closure_arg_types)
    assert len(outer_closure_exprs) == n_closure_args, \
      "Mismatch between closure formal args %s and given %s" % (", ".join(closure_arg_types),
                                                                ", ".join(outer_closure_exprs))
    bound_types = (Int32,) * n_indices
    
    if self.use_constant_memory_for_args:
      outer_input_types = tuple(bound_types)
    else:
      outer_input_types = tuple(closure_arg_types) + bound_types
      
    
        
    
    parakeet_kernel, builder, input_vars  = build_fn(outer_input_types, name = kernel_name)
    
    if self.use_constant_memory_for_args:
      inner_closure_vars = []
      for i, t in enumerate(closure_arg_types):
        raw_name = fn.arg_names[i]
        self.comment("Moving GPU arg #%d %s : %s to constant memory" % (i, raw_name, t))
        const_name = self.fresh_name("gpu_arg_"  + raw_name)
        typename = self.to_ctype(t)
        self.add_decl("__constant__ %s %s" % (typename, const_name))
        inner_closure_vars.append(SourceExpr(const_name, type=t))
        gpu_arg = gpu_closure_args[i]
        # in case this is a constant, should assign to a variable
        first_char = gpu_arg[0]
        if not first_char.isalpha():
          gpu_arg = self.fresh_var(typename, "src_" + raw_name, gpu_arg)
        self.append("cudaMemcpyToSymbolAsync(%s, &%s, sizeof(%s));" % (const_name, gpu_arg, typename))
      inner_closure_vars = tuple(inner_closure_vars)
      # need to do a cudaMemcpyToSymbol for each gpu arg   
      bounds_vars = input_vars
  
    else:
      # TODO: use these to compute indices when n_indices > 3 or 
      # number of threads per block > 1  
      inner_closure_vars = input_vars[:n_closure_args]
      bounds_vars = input_vars[n_closure_args:(n_closure_args + n_indices)]


    THREADS_PER_DIM = config.threads_per_block_dim 

    
    if n_indices == 1:
      elts_per_block = builder.div(bounds_vars[0],  
                                   const_int(THREADS_PER_DIM**2, Int32), 
                                   "elts_per_block")
      elts_per_block = builder.add(elts_per_block, one_i32, "elts_per_block")
      base_idx = builder.mul(elts_per_block, blockIdx.x, "base_x")
      start_idx = builder.add(base_idx, threadIdx.x, "start_x")  
      
      start_indices = [start_idx]
    
      stop_x = builder.mul(builder.add(one_i32, blockIdx.x, "next_base_x"), 
                           elts_per_block, "stop_x")
      stop_x = builder.min(stop_x, bounds_vars[0], "stop_x") 
      stop_indices = [stop_x]
      step_sizes = [const_int(THREADS_PER_DIM**2, Int32)]
       
    else:
      elts_per_block = builder.div(bounds_vars[0],   
                                   const_int(THREADS_PER_DIM, Int32),
                                    "elts_per_block")
      elts_per_block = builder.add(elts_per_block, one_i32, "elts_per_block")
      base_x = builder.mul(elts_per_block, blockIdx.x, "base_x")
      start_x = builder.add(base_x, threadIdx.x, "start_x")  
      start_y = builder.add(builder.mul(elts_per_block, blockIdx.y, "base_y"),
                                threadIdx.y, "start_y")
      
      start_indices = [start_x, start_y]
      
      stop_x = builder.mul(builder.add(one_i32, blockIdx.x, "next_base_x"), 
                           elts_per_block, "stop_x")
      stop_x = builder.min(stop_x, bounds_vars[0], "stop_x") 
      stop_indices = [stop_x]
      stop_y = builder.mul(builder.add(one_i32, blockIdx.y, "next_base_y"),
                           elts_per_block, "stop_y")
      stop_y = builder.min(stop_y, bounds_vars[1], "stop_y")
       
      stop_indices = [stop_x, stop_y]
      
      step_sizes = [const_int(THREADS_PER_DIM, Int32), const_int(THREADS_PER_DIM, Int32)]
      for i in xrange(2, n_indices):
        start_indices.append(zero_i32)
        step_sizes.append(one_i32)
        stop_indices.append(bounds_vars[i])
    
    
    
    def loop_body(index_vars):
      if not isinstance(index_vars, (list,tuple)):
        pass
      indices = [builder.cast(idx, t) for idx, t in zip(index_vars,index_types)]
      if index_as_tuple:
        index_args = (builder.tuple(indices),)
      else:
        index_args = indices
      inner_args = tuple(inner_closure_vars) + tuple(index_args)
      builder.call(fn, inner_args)
      
    builder.nested_loops(stop_indices, loop_body, start_indices, step_sizes, index_vars_as_list = True)
    
    self.enter_kernel()
    c_kernel_name = self.get_fn_name(parakeet_kernel, 
                                     attributes = ["__global__"], 
                                     inline = False)
    self.exit_kernel()
    
    # set cache preference of the kernel we just built 
    self.append("cudaFuncSetCacheConfig(%s, cudaFuncCachePreferL1);" % c_kernel_name)
    # self._kernel_cache[key] = c_kernel_name
    return c_kernel_name, gpu_closure_args, host_closure_args, closure_arg_types
  
  def visit_IndexReduce(self, expr):
    # for now only supporting scalar reductions on the GPU
    bounds = self.tuple_to_var_list(expr.shape)
    if isinstance(expr.type, ScalarT) and len(bounds) == 1 and False:
      n = bounds[0]
      if expr.init is None:
        init = None
      else:
        init = self.visit_expr(expr.init)
      
      # launch map & first round of reductions 
      n = self.div(n, )
      # loop while n > 1
    
      kernel_name, gpu_closure_args, host_closure_args, closure_arg_types  = \
        self.build_kernel(expr.fn, bounds)
    
      if self.use_constant_memory_for_args:
        params = bounds 
      else:
        params = tuple(gpu_closure_args) + tuple(bounds)
      
      self.launch_kernel(bounds, params, kernel_name)
      return "/* done with ParFor */"  
    else:
      return MulticoreCompiler.visit_IndexReduce(self, expr) 


         

########NEW FILE########
__FILENAME__ = cuda_syntax
from ..ndtypes import Int32 
from ..syntax import SourceExpr, SourceStmt

class dim3(object):
  def __init__(self, x, y, z):
    self.x = x 
    self.y = y 
    self.z = z 
  
  def __iter__(self):
    yield self.x 
    yield self.y 
    yield self.z
    
  def __getitem__(self, idx):
    if idx == 0:
      return self.x 
    elif idx == 1:
      return self.y 
    else:
      assert idx == 2, "Unexpected index %s to %s" % (idx, self)
      return self.z
    
  def __str__(self):
    return "dim3(x = %s, y = %s, z = %s)" % (self.x, self.y, self.z) 
 
# pasted literally into the C source 
blockIdx_x = SourceExpr("blockIdx.x", type=Int32)
blockIdx_y = SourceExpr("blockIdx.y", type=Int32)
blockIdx_z = SourceExpr("blockIdx.z", type=Int32)
blockIdx = dim3(blockIdx_x, blockIdx_y, blockIdx_z)

threadIdx_x = SourceExpr("threadIdx.x", type=Int32)
threadIdx_y = SourceExpr("threadIdx.y", type=Int32)
threadIdx_z = SourceExpr("threadIdx.z", type=Int32)
threadIdx = dim3(threadIdx_x, threadIdx_y, threadIdx_z)

blockDim_x = SourceExpr("blockDim.x", type = Int32)
blockDim_y = SourceExpr("blockDim.y", type = Int32)
blockDim_z = SourceExpr("blockDim.z", type = Int32)
blockDim = dim3(blockDim_x, blockDim_y, blockDim_z)

gridDim_x = SourceExpr("gridDim.x", type = Int32)
gridDim_y = SourceExpr("gridDim.y", type = Int32)
gridDim_z = SourceExpr("gridDim.z", type = Int32)
gridDim = dim3(gridDim_x, gridDim_y, gridDim_z)

warpSize = SourceExpr("wrapSize", type=Int32)
__syncthreads = SourceStmt("__syncthreads;")


########NEW FILE########
__FILENAME__ = device_info


import config 
def get_cuda_devices(compute_capability = config.compute_capability):
  try:
    import pycuda.autoinit
    import pycuda.driver  
  except:
    return []
  devices = [pycuda.driver.Device(i) for i in xrange(pycuda.driver.Device.count())]
  return [d for d in devices if d.compute_capability() >= compute_capability]

def device_id(cuda_device):
  import pycuda.driver 
  return cuda_device.get_attribute(pycuda.driver.device_attribute.PCI_DEVICE_ID)

def display_attached(cuda_device):
  import pycuda.driver 
  return cuda_device.get_attribute(pycuda.driver.device_attribute.KERNEL_EXEC_TIMEOUT) == 1  
  
def num_multiprocessors(cuda_device):
  import pycuda.driver
  return cuda_device.get_attribute(pycuda.driver.device_attribute.MULTIPROCESSOR_COUNT)
  

def best_cuda_device(compute_capability = config.compute_capability):
  devices = get_cuda_devices(compute_capability)
  if len(devices) == 0:
    return None
  
  best_device = devices[0]
  for d in devices[1:]:
    if display_attached(best_device) and not display_attached(d):
      best_device = d 
    elif num_multiprocessors(d) > num_multiprocessors(best_device):
      best_device = d
    elif d.total_memory() > best_device.total_memory():
      best_device = d
  return best_device    

def has_gpu():
  return best_cuda_device() is not None

    
  
  
  
########NEW FILE########
__FILENAME__ = run_function
from ..c_backend.prepare_args import prepare_args 
from ..config import value_specialization 
from ..transforms.pipeline import (lower_to_adverbs, ) 
from ..value_specialization import specialize


from cuda_compiler import CudaCompiler 

def run(fn, args):
  args = prepare_args(args, fn.input_types)

  fn = lower_to_adverbs.apply(fn)

  if value_specialization:
    fn = specialize(fn, python_values = args)
  compiled_fn = CudaCompiler().compile_entry(fn)
  assert len(args) == len(fn.input_types)
  result = compiled_fn.c_fn(*args)
  return result
  
########NEW FILE########
__FILENAME__ = base_compiler
from .. import names 
from ..ndtypes import BoolT, IntT 
import type_mappings 
from reserved_names import is_reserved


class BaseCompiler(object):
  
  def __init__(self, extra_link_flags = None, extra_compile_flags = None):
    self.blocks = []
    self.name_versions = {}
    self.name_mappings = {}
    self.extra_link_flags = extra_link_flags if extra_link_flags else []
    self.extra_compile_flags = extra_compile_flags if extra_compile_flags else []
    
  def add_compile_flag(self, flag):
    if flag not in self.extra_compile_flags:
      self.extra_compile_flags.append(flag)
  
  def add_link_flag(self, flag):
    if flag not in self.extra_link_flags:
      self.extra_link_flags.append(flag)
        
  
  def visit_expr(self, expr):
    expr_class_name = expr.__class__.__name__
    method_name = "visit_" + expr_class_name
    assert hasattr(self, method_name), "Unsupported expression %s" % expr_class_name  
    result = getattr(self, method_name)(expr)
    assert result is not None, \
      "Compilation method for expression %s returned None, expected code string" % expr_class_name
    return result 
      
  def visit_expr_list(self, exprs):
    return [self.visit_expr(e) for e in exprs]
  
  def breakpoint(self):
    self.append("raise(SIGINT);")
    
  def visit_stmt(self, stmt):
    stmt_class_name = stmt.__class__.__name__
    method_name = "visit_" + stmt_class_name
    assert hasattr(self, method_name), \
      "Statement %s not supported by %s" % (stmt_class_name, self.__class__.__name__)  
    result = getattr(self, method_name)(stmt)
    assert result is not None, "Compilation method for statement %s return None" % stmt_class_name
    return result 
  
  def push(self):
    self.blocks.append([])
  
  def pop(self):
    stmts = self.blocks.pop()
    return "  " + self.indent("\n".join("  " + stmt for stmt in stmts))
  
  def indent(self, block_str):
    return block_str.replace("\n", "\n  ")
  
  def append(self, stmt):
    stripped = stmt.strip()
    
    assert len(stripped) == 0 or \
      ";" in stripped or \
      stripped.startswith("//") or \
      stripped.startswith("/*"), "Invalid statement: %s" % stmt
    self.blocks[-1].append(stmt)
  
  def newline(self):
    self.append("\n")
    
  def comment(self, text):
    self.append("// %s" % text)
  
  def printf(self, fmt, *args):
    
    result = 'printf("%s\\n"' % fmt
    if len(args) > 0:
      result = result + ", " + ", ".join(str(arg) for arg in args)
    self.append( result + ");" )
  
  def fresh_name(self, prefix):
    prefix = names.original(prefix)
    
    prefix = prefix.replace(".", "")
  
    version = self.name_versions.get(prefix, 1)
    self.name_versions[prefix] = version + 1
    
    # not valid chars!
    if not any(c.isalpha() for c in prefix):
      prefix = self.fresh_name("temp" + prefix) 
      
    if version == 1 and not is_reserved(prefix):
      return prefix 
    elif prefix[-1] != "_":
      return "%s_%d" % (prefix, version)
    else:
      return prefix + str(version)
    
  def to_ctype(self, t):
    """
    Convert Parakeet type to string representing its C type.
    The base class implementation only handles scalars, 
    support for Tuples, Slices, and Arrays is in the overload FlatFnCompiler.to_ctype
    """
    return type_mappings.to_ctype(t)
  
  def fresh_var(self, t, prefix = None, init = None):
    if prefix is None:
      prefix = "temp"
    name = self.fresh_name(prefix)
    if isinstance(t, str):
      t_str = t
    else:
      t_str = self.to_ctype(t)
    if init is None:
      self.append("%s %s;" % (t_str, name))
    else:
      self.append("%s %s = %s;" % (t_str, name, init))
    return name
  
  def fresh_array_var(self, t, n, prefix = None):
    if prefix is None:
      prefix = "temp"
    name = self.fresh_name(prefix)
    if isinstance(t, str):
      t_str = t
    else:
      t_str = self.to_ctype(t)
    self.append("%s %s[%d];" % (t_str, name, n))
    return name
  
  def assign(self, name, rhs):
    self.append("%s = %s;" % (name, rhs))
  
  def name(self, ssa_name, overwrite = False):
    """
    Convert from ssa names, which might have large version numbers and contain 
    syntactically invalid characters to valid local C names
    """
    if ssa_name in self.name_mappings and not overwrite:
      return self.name_mappings[ssa_name]
    prefix = names.original(ssa_name)
    prefix = prefix.replace(".", "")

    name = self.fresh_name(prefix)
    self.name_mappings[ssa_name] = name 
    return name
   
  def return_if_null(self, obj):
    self.append("if (!%s) { return NULL; }" % obj)
    
  def not_(self, x):
    if x == "1":
      return "0"
    elif x == "0":
      return "1"
    return "!%s" % x
  
  def and_(self, x, y):
    if x == "0" or y == "0":
      return "0"
    elif x == "1" and y == "1":
      return "1"
    elif x == "1":
      return y 
    elif y == "1":
      return x
    return "%s && %s" % (x,y) 
  
  def or_(self, x, y):
    if x == "1" or y == "1":
      return "1"
    elif x == "0":
      return y
    elif y == "0":
      return x 
    return "%s || %s" % (x,y) 
  
  def gt(self, x, y, t):
    if isinstance(t, (BoolT, IntT)) and x == y:
      return "0"
    return "%s > %s" % (x, y)
  
  def gte(self, x, y, t):
    if isinstance(t, (BoolT, IntT)) and x == y:
      return "1"
    return "%s >= %s" % (x,y) 
  
  def lt(self, x, y, t):
    if isinstance(t, (BoolT, IntT)) and x == y:
      return "0"
    return "%s < %s" % (x,y)
  
  def lte(self, x, y, t):
    if isinstance(t, (BoolT, IntT)) and x == y:
      return "1"
    return "%s <= %s" % (x, y) 
  
  def neq(self, x, y, t):
    if isinstance(t, (BoolT, IntT)) and x == y:
      return "0"
    return "%s != %s" % (x, y) 
  
  def eq(self, x, y, t):
    if isinstance(t, (BoolT, IntT)) and x == y:
      return "1"
    return "%s == %s" % (x, y)
  
  
  def add(self, x, y):
    if x == "0":
      return y
    elif y == "0":
      return x
    return "%s + %s" % (x,y)
  
  def sub(self, x, y):
    if x == "0":
      return "-(%s)" % y 
    elif y == "0":
      return x 
    return "%s - %s" % (x,y)
  
  def mul(self, x, y):
    if x == "1":
      return y 
    elif y == "1":
      return x 
    elif x == 0 or y == 0:
      return "0"
    return "%s * %s" % (x,y)
  
  def div(self, x, y):
    if x == y:
      return "1"
    elif x == "0":
      return "0"
    elif y == "1":
      return x
    else:
      return "%s / %s" % (x,y)
    
########NEW FILE########
__FILENAME__ = compile_util
import collections
import hashlib
import imp
import os

from tempfile import NamedTemporaryFile

from .. import config as root_config 
import config 
  
from system_info import (python_lib_dir,  
                         windows, 
                         get_source_extension, object_extension, shared_extension,  
                         get_compiler, 
                         include_dirs)
from flags import get_compiler_flags, get_linker_flags
from shell_command import CommandFailed, run_cmd 

CompiledPyFn = collections.namedtuple(
  "CompiledPyFn",
  (
     "c_fn", 
     "src_filename", 
     "module", 
     "shared_filename", 
     "src", 
     "fn_name",
     "fn_signature"
  )
)

CompiledObject = collections.namedtuple(
  "CompiledObject", 
  (
    "src_filename",  
    "object_filename", 
    "fn_name",
  )
)

  
c_headers = ["stdint.h",  "math.h",  "signal.h"]
core_python_headers = ["Python.h"]
numpy_headers = ['numpy/arrayobject.h', 'numpy/arrayscalars.h']

python_headers = core_python_headers + numpy_headers 

# went to some annoying effort to clean up all the array->flags, &c that have been 
# replaced with PyArray_FLAGS in NumPy 1.7 
global_preprocessor_defs = ["#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"]


def create_module_source(raw_src, fn_name, 
                            extra_headers = [], 
                            declarations = [], 
                            extra_function_sources = [], 
                            print_source = None):
    # when compiling with NVCC, other headers get implicitly included 
  # and cause warnings since Python redefines this constant
  src_lines = list(global_preprocessor_defs) 
  if config.undef_posix_c_source:
    src_lines.append("#undef _XOPEN_SOURCE")
    src_lines.append("#undef _POSIX_C_SOURCE")
  
  for header in extra_headers + c_headers:
    src_lines.append("#include <%s>" % header)
  
  for decl in declarations:
    decl = decl.strip()
    if not decl.endswith(";"):
      decl += ";"
    src_lines.append(decl)
    
  src_lines.extend(extra_function_sources)
  
  src_lines.append(raw_src)
  module_init = """
    \n\n
    static PyMethodDef %(fn_name)sMethods[] = {
      {"%(fn_name)s",  %(fn_name)s, METH_VARARGS,
       "%(fn_name)s"},

      {NULL, NULL, 0, NULL}        /* Sentinel */
    };
  
    PyMODINIT_FUNC
    init%(fn_name)s(void)
    {
      //Py_Initialize();
      Py_InitModule("%(fn_name)s", %(fn_name)sMethods);
      import_array();
    }
  """ % locals()
  src_lines.append(module_init)
  full_src =  "\n".join(src_lines)
  
  if print_source is None: print_source = root_config.print_generated_code  
  if print_source:
    for i, line in enumerate(full_src.splitlines()):
        if config.print_line_numbers: print i+1, " ", line
        else: print line
  return full_src    

def create_source_file(src, 
                         fn_name = None, 
                         src_filename = None, 
                         src_extension = None):
  if fn_name is None: prefix = "parakeet_"
  else: prefix = "parakeet_%s_" % fn_name

  if src_extension is None: src_extension = get_source_extension()
  if src_filename is None:
    src_file = NamedTemporaryFile(suffix = src_extension,  
                                  prefix =  prefix, 
                                  delete = False,
                                  mode = 'w', 
                                  )
    src_filename = src_file.name 
  else:
    src_dir = os.path.dirname(src_filename)
    if not os.path.exists(src_dir):
      os.makedirs(src_dir)

    src_file = open(src_filename, 'w')
  
  src_file.write(src)  
  src_file.close()
  return src_file 

def compile_object(
      src_filename, 
      fn_name = None,  
      src_extension = None, 
      declarations = [],
      extra_objects = [], 
      extra_compile_flags = [], 
      print_commands = None, 
      compiler = None, 
      compiler_flag_prefix = None):
  
  
  if print_commands is None:  print_commands = config.print_commands
  if src_extension is None: src_extension = get_source_extension()
  if compiler is None: compiler = get_compiler()
    
  object_name = src_filename.replace(src_extension, object_extension)
  compiler_flags = get_compiler_flags(extra_compile_flags, compiler_flag_prefix)
  
  if isinstance(compiler, (list,tuple)):
    compiler_cmd = list(compiler)
  else:
    compiler_cmd = [compiler]
    
  compiler_cmd += compiler_flags 
  compiler_cmd += ['-c', src_filename, '-o', object_name]
  run_cmd(compiler_cmd, label = "Compile source")
  
  return CompiledObject(src_filename = src_filename, 
                        object_filename = object_name, 
                        fn_name = fn_name)

def link_module(
      compiler, object_name, shared_name, 
      extra_objects = [], 
      extra_link_flags = [], 
      linker_flag_prefix = None):
  linker_flags = get_linker_flags(extra_link_flags, linker_flag_prefix) 
  
  if isinstance(compiler, (list,tuple)):
    linker_cmd = list(compiler)
  else:
    linker_cmd = [compiler]
  linker_cmd += [object_name] 
  linker_cmd += linker_flags 
  linker_cmd += list(extra_objects) 
  linker_cmd += ['-o', shared_name]

  env = os.environ.copy()
  if not windows:
    env["LD_LIBRARY_PATH"] = python_lib_dir
  run_cmd(linker_cmd, env = env, label = "Linking")

def compile_with_distutils(extension_name, 
                              src_filename,
                              extra_objects = [], 
                              extra_compiler_flags = [],
                              extra_link_flags = [],   
                              print_commands = False):

    # copied largely from pyxbuild 
    from distutils.dist import Distribution
    from distutils.extension import Extension
    
    compiler_flags = get_compiler_flags(extra_compiler_flags)
    # don't need -shared in the flags since the default CC on Mac OS 
    # might specify -bundle instead and the two are mutually exclusive
    linker_flags = get_linker_flags(extra_link_flags, shared=False)
    
    ext = Extension(name=extension_name, 
                    sources=[src_filename],
                    include_dirs = include_dirs,  
                    extra_objects=extra_objects,
                    extra_compile_args=compiler_flags,
                    extra_link_args=linker_flags)
  
    script_args = ['build_ext', '--quiet']
    setup_args = {"script_name": None,
                  "script_args": script_args, 
                  }
    dist = Distribution(setup_args)
    if not dist.ext_modules: dist.ext_modules = []
    dist.ext_modules.append(ext)
    # I have no idea how distutils works or why I have to do any of this 
    config_files = dist.find_config_files()
    try: config_files.remove('setup.cfg')
    except ValueError: pass
    dist.parse_config_files(config_files)
    dist.parse_command_line()
    obj_build_ext = dist.get_command_obj("build_ext")
    
    dist.run_commands()
    shared_name = obj_build_ext.get_outputs()[0]
    return shared_name
  
def compiler_is_gnu(compiler):
  if isinstance(compiler, (list,tuple)): compiler = compiler[0]
  return (compiler.endswith("gcc") or
          compiler.endswith("gcc.exe") or 
          compiler.endswith("g++") or 
          compiler.endswith("g++.exe"))

def compile_module_from_source(
      partial_src, 
      fn_name,
      fn_signature = None,  
      src_filename = None,
      src_extension = None, 
      declarations = [],
      extra_function_sources = [], 
      extra_headers = [],  
      extra_objects = [],
      extra_compile_flags = [], 
      extra_link_flags = [], 
      print_source = None, 
      print_commands = None, 
      compiler = None, 
      compiler_flag_prefix = None, 
      linker_flag_prefix = None):
  
  if print_source is None: print_source = root_config.print_generated_code 
  if print_commands is None: print_commands = config.print_commands
  if src_extension is None: src_extension = get_source_extension()
  
  full_src = create_module_source(partial_src, fn_name, 
                                 extra_headers = python_headers + extra_headers, 
                                 declarations = declarations,  
                                 extra_function_sources = extra_function_sources, 
                                 print_source = print_source)


  digest = hashlib.sha224(full_src).hexdigest()
  
  if config.cache_dir:
    cached_name = os.path.join(config.cache_dir, fn_name + "_" + digest + shared_extension)
    have_cached_version = os.path.exists(cached_name)
  else:
    cached_name = None
    have_cached_version = False

  if have_cached_version:
    shared_name = cached_name
  else:
    src_file = create_source_file(full_src,
                                  fn_name = fn_name,
                                  src_filename = src_filename,
                                  src_extension = src_extension)
    src_filename = src_file.name

    if compiler is None: compiler = get_compiler()

    try:
      compiled_object = compile_object(src_filename,
                                       fn_name = fn_name,
                                       src_extension = src_extension,
                                       extra_objects = extra_objects,
                                       extra_compile_flags = extra_compile_flags,
                                       print_commands = print_commands,
                                       compiler = compiler,
                                       compiler_flag_prefix = compiler_flag_prefix)

      object_name = compiled_object.object_filename
      shared_name = src_filename.replace(src_extension, shared_extension)
      link_module(compiler, object_name, shared_name,
                  extra_objects = extra_objects,
                  extra_link_flags = extra_link_flags,
                  linker_flag_prefix = linker_flag_prefix)

      if config.delete_temp_files:
        os.remove(object_name)
    except CommandFailed:
      # if normal compilation fails, try distutils instead
      if not compiler_is_gnu(compiler):
        raise
      if compiler_flag_prefix or linker_flag_prefix:
        raise

      shared_name = compile_with_distutils(fn_name + "_" + digest,
                                           src_filename,
                                           extra_objects,
                                           extra_compile_flags,
                                           extra_link_flags,
                                           print_commands)

  # if we're caching generated modules, move our output
  # over to the cache directory before loading it up.
  if config.cache_dir and not have_cached_version:
    if print_commands:
      print 'Caching... %s -> %s' % (shared_name, cached_name)
    if not os.path.exists(config.cache_dir):
      os.makedirs(config.cache_dir)
    os.rename(shared_name, cached_name)
    shared_name = cached_name

  if print_commands:
    print "Loading newly compiled extension module %s..." % shared_name
  module =  imp.load_dynamic(fn_name, shared_name)
  #on a UNIX-style filesystem it should be OK to delete a file while it's open
  #since the inode will just float untethered from any name
  #If we ever support windows we should find some other way to delete the .dll 
  c_fn = getattr(module,fn_name)
  
  if config.delete_temp_files and src_filename is not None:
    os.remove(src_filename)
    if not config.cache_dir:
      # window's can't just untether inodes like a UNIX
      # ...have to eventually think of a plan to clean these things up
      if not windows: os.remove(shared_name)
    
  compiled_fn = CompiledPyFn(c_fn = c_fn, 
                             module = module, 
                             shared_filename =  shared_name,
                             src = full_src, 
                             src_filename = src_filename,
                             fn_name = fn_name, 
                             fn_signature = fn_signature)
  return compiled_fn


########NEW FILE########
__FILENAME__ = config
##########################
#  Performance Options   #
##########################
fast_math = True 
sse2 = True 
opt_level = '-O2'
# overload the default compiler path  
compiler_path = None

##########################
# Insert Debugging Code  #
##########################
debug = False
check_pyobj_types = False 

#########################
#  Verbose Printing     #
#########################
print_input_ir = False

print_line_numbers = False
 
print_function_source = False

print_commands = False
print_command_elapsed_time = False


# Generate a .c file or a .cpp? 
pure_c = True

# Throw away .c and .o files
delete_temp_files = True

# Directory to use for caching generated modules.
# Set to None to disable caching.
from appdirs import user_cache_dir
cache_dir = user_cache_dir('parakeet')

# if compiling C or OpenMP we can skip some of the craziness and 
# have distutils figure out the system config and compiler for us 
use_distutils = True

# show all the warnings? w
suppress_compiler_output = False 

# when compiling with NVCC, other headers get implicitly included 
# and cause warnings since Python redefines _POSIX_C_SOURCE  
# we can undefine it before including Python.h to get rid of those warnings 
undef_posix_c_source = True 
########NEW FILE########
__FILENAME__ = c_prims
from .. import prims
from ..ndtypes import FloatT, Float32, Float64 


# names used in math library 
_float_fn_names = {
  prims.tan : 'tan', 
  prims.tanh : 'tanh',
  prims.arctan : 'atan', 
  prims.arctanh : 'atanh',
  prims.arctan2 : 'atan2',  
   
  prims.cos : 'cos', 
  prims.cosh : 'cosh', 
  prims.arccos : 'acos', 
  prims.arccosh : 'acosh', 
  
  prims.sin : 'sin', 
  prims.sinh : 'sinh',
  prims.arcsin : 'asin',
  prims.arcsinh : 'asinh', 
  
  prims.log : 'log',
  prims.log2 : 'log2', 
  prims.log10 : 'log10',
  prims.log1p : 'log1p', 
   
   
  prims.exp : 'exp',  
  prims.exp2 : 'exp2', 
  prims.power : 'pow', 
  prims.expm1 : 'expm1', 
  prims.sqrt : 'sqrt', 
  
  prims.abs : 'fabs', 
  
  prims.ceil : 'ceil', 
  prims.floor : 'floor', 
  prims.round : 'round', 
  
} 
 
def float_prim(p, t):
  assert p in _float_fn_names
  assert isinstance(t, FloatT)
  name = _float_fn_names[p]
  if t == Float32:
    return name + "f" 
  else:
    return name 
  
########NEW FILE########
__FILENAME__ = flags
import distutils
import os 

from . system_info import include_dirs, windows, mac_os 
from . import config 

def get_opt_flags():
  opt_flags = [config.opt_level] 
  if config.sse2:
    opt_flags.append('-msse2')
  if config.fast_math:
    opt_flags.append('-ffast-math')
  return opt_flags 

def get_compiler_flags(extra_flags = [], compiler_flag_prefix = None):
  compiler_flags = ['-I%s' % path for path in include_dirs]
  
  def add_flag(flag):
    if compiler_flag_prefix is not None:
      compiler_flags.append(compiler_flag_prefix)
    compiler_flags.append(flag)

  add_flag('-fPIC')
  
  if config.debug:
    # nvcc understands debug mode flags
    compiler_flags.extend(['-g', '-O0'])
  else:
    for flag in get_opt_flags():
      add_flag(flag)

  if not config.pure_c: 
    add_flag('-fpermissive')

  for flag in extra_flags:
    add_flag(flag)
    
  return compiler_flags   


def get_linker_flags(extra_flags = [], 
                     linker_flag_prefix = None, 
                     shared = True):
  if shared:
    # for whatever reason nvcc is OK with the -shared linker flag
    # but not with the -fPIC compiler flag
    linker_flags = ['-shared']
  else:
    linker_flags = []
  
  def add_flag(flag):
    if linker_flag_prefix is not None:
      linker_flags.append(linker_flag_prefix)
    linker_flags.append(flag)
    
  add_flag('-lm')
  if windows:
    # crazy stupid hack for exposing Python symbols
    # even though we're compiling a shared library, why does Windows care?
    # why does windows even exist? 
    inc_dir = distutils.sysconfig.get_python_inc()
    base = os.path.split(inc_dir)[0]
    lib_dir = base + "\libs"
    add_flag('-L' + lib_dir)
    libname = 'python' + distutils.sysconfig.get_python_version().replace('.', '')
    add_flag('-l' + libname)
    
  # add_flag('-Wl,-dynamic')
  if mac_os:
    add_flag("-Wl,-undefined")
    add_flag("-Wl,dynamic_lookup")
    
  for flag in extra_flags:
    add_flag(flag)
  return linker_flags 


########NEW FILE########
__FILENAME__ = fn_compiler
from collections import namedtuple
import numpy as np 

from .. import names, prims  
from ..ndtypes import (IntT, FloatT, TupleT, FnT, Type, BoolT, NoneT, Float32, Float64, Bool, 
                       ClosureT, ScalarT, PtrT, NoneType, ArrayT, SliceT, TypeValueT)    
from ..syntax import (Const, Var,  PrimCall, Attribute, TupleProj, Tuple, ArrayView,
                      Expr, Closure, TypedFn)
# from ..syntax.helpers import get_types   
import type_mappings
from base_compiler import BaseCompiler


CompiledFlatFn = namedtuple("CompiledFlatFn", 
                            ("name", "sig", "src",
                             "extra_objects",
                             "extra_functions",
                             "extra_function_signatures", 
                             "declarations"))


# mapping from (field_types, struct_name, field_names) to type names 
_struct_type_names = {}

# mapping from struct name to decl 
_struct_type_decls = {}

class FnCompiler(BaseCompiler):
  
  def __init__(self,  
               module_entry = False, 
               struct_type_cache = None, 
               **kwargs):
    BaseCompiler.__init__(self, **kwargs)
    
    
    self.declarations = []
    
    # depends on these .o files
    self.extra_objects = set([]) 
    
    # to avoid adding the same function's source twice 
    # we use its signature as a key  
    self.extra_functions = {}
    self.extra_function_signatures = []
    
      
    # are we actually compiling the entry-point into a Python module?
    # if so, expect some of the methods like visit_Return to be overloaded 
    # to return PyObjects
    self.module_entry = module_entry
     
  def add_decl(self, decl):
    if decl not in self.declarations:
      self.declarations.append(decl)
  
  def ptr_struct_type(self, elt_t):
    # need both an actual data pointer 
    # and an optional PyObject base
    field_types = ["%s*" % self.to_ctype(elt_t), "PyObject*"]
    return self.struct_type_from_fields(field_types, 
                                        struct_name = "%s_ptr_type" % elt_t,
                                        field_names = ["raw_ptr", "base"])
  
  def array_struct_type(self, elt_t, rank):
    ptr_field_t = self.ptr_struct_type(elt_t)
    field_types = [ptr_field_t, "npy_intp", "npy_intp", "int64_t", "int64_t"]
    field_names = ["data", "shape", "strides", "offset", "size"]
    field_repeats = {}
    field_repeats["shape"] = rank 
    field_repeats["strides"] = rank
    return self.struct_type_from_fields(field_types, "array_type", field_names, field_repeats)
  
  def slice_struct_type(self, start_t = "int64_t", stop_t = "int64_t", step_t = "int64_t"):
    field_types = [start_t, stop_t, step_t]
    field_names = ["start", "stop", "step"]
    return self.struct_type_from_fields(field_types, "slice_type", field_names)
       
  def struct_type_from_fields(self, 
                                 field_types, 
                                 struct_name = "tuple_type", 
                                 field_names = None, 
                                 field_repeats = {}):
    
    if any(not isinstance(t, str) for t in field_types):
      field_types = tuple(self.to_ctype(t) if isinstance(t, Type) else t 
                          for t in field_types)
    else:
      field_types = tuple(field_types)
    
    if field_names is None:
      field_names = tuple("elt%d" % i for i in xrange(len(field_types)))
    else:
      assert len(field_names) == len(field_types), \
        "Mismatching number of types %d and field names %d" % (len(field_types), len(field_names))
      field_names = tuple(field_names)
    
    repeat_set =  frozenset(sorted(field_repeats.items()))
    key = field_types, struct_name, field_names, repeat_set
     
    if key in _struct_type_names:
      typename = _struct_type_names[key]
      decl = _struct_type_decls[typename]
      if decl not in self.declarations:
        self.declarations.append(decl)
      return typename 
    
    typename = names.fresh(struct_name).replace(".", "")
    
    field_decls = []
    for t, field_name in zip(field_types, field_names):
      if field_name in field_repeats:
        field_decl = "  %s %s[%d];" % (t, field_name, field_repeats[field_name])
      else:
        field_decl = "  %s %s;" % (t, field_name)
      field_decls.append(field_decl)
    decl = "typedef struct %s {\n%s\n} %s;" % (typename, "\n".join(field_decls), typename)
    
    _struct_type_names[key] = typename
    _struct_type_decls[typename] = decl
    self.add_decl(decl)
    return typename 
  

  def to_ctype(self, parakeet_type):
    if isinstance(parakeet_type, (NoneT, ScalarT)):
      return type_mappings.to_ctype(parakeet_type)
    
    elif isinstance(parakeet_type, TupleT):
      return self.struct_type_from_fields(parakeet_type.elt_types)
    elif isinstance(parakeet_type, PtrT):
      return self.ptr_struct_type(parakeet_type.elt_type)
    elif isinstance(parakeet_type, ArrayT):
      elt_t = parakeet_type.elt_type 
      rank = parakeet_type.rank 
      return self.array_struct_type(elt_t, rank)
    
    elif isinstance(parakeet_type, SliceT):
      return self.slice_struct_type()
    elif isinstance(parakeet_type, ClosureT):
      return self.struct_type_from_fields(parakeet_type.arg_types)
    elif isinstance(parakeet_type, TypeValueT):
      return "int"
    else:
      assert False, "Don't know how to make C type for %s" % parakeet_type
    
  
  def to_ctypes(self, ts):
    return tuple(self.to_ctype(t) for t in ts)
  
  
  
  def visit_Slice(self, expr):
    typename = self.to_ctype(expr.type)
    start = self.visit_expr(expr.start)
    stop = self.visit_expr(expr.stop)
    step = self.visit_expr(expr.step)
    return self.fresh_var(typename, "slice", "{%s, %s, %s}" % (start,stop,step))
    
  def visit_Alloc(self, expr):
    elt_t =  expr.elt_type
    nelts = self.fresh_var("npy_intp", "nelts", self.visit_expr(expr.count))
    bytes_per_elt = elt_t.nbytes
    nbytes = self.mul(nelts, bytes_per_elt)#"%s * %d" % (nelts, bytes_per_elt)
    raw_ptr = "(%s) malloc(%s)" % (type_mappings.to_ctype(expr.type), nbytes)
    struct_type = self.to_ctype(expr.type)
    return self.fresh_var(struct_type, "new_ptr", "{%s, NULL}" % raw_ptr)
    
  def visit_Const(self, expr):
    t = expr.type 
    c = t.__class__ 
    if c == BoolT:
      return "1" if expr.value else "0"
    elif c == NoneT:
      return "0"
    
    assert isinstance(t, ScalarT), "Don't know how to translate Const %s : %s" % (expr,t)
    v = expr.value 
    if np.isinf(v):
      return "INFINITY"
    elif np.isnan(v):
      return "NAN"
    return "%s" % expr.value 
  
  def visit_Var(self, expr):
    return self.name(expr.name)
  
  def visit_Cast(self, expr):
    x = self.visit_expr(expr.value)
    ct = self.to_ctype(expr.type)
    if isinstance(expr, (Const, Var)):
      return "(%s) %s" % (ct, x)
    else:
      return "((%s) (%s))" % (ct, x)
  
  

  
  
  def visit_PrimCall(self, expr):
    t = expr.type
    args = self.visit_expr_list(expr.args)
    
    # parenthesize any compound expressions 
    for i, arg_expr in enumerate(expr.args):
      if not isinstance(arg_expr, (Var, Const)):
        args[i] = "(" + args[i] + ")"
        
    p = expr.prim 
    if p == prims.add:
      #return "%s + %s" % (args[0], args[1])
      return self.add(args[0], args[1])
    if p == prims.subtract:
      #return "%s - %s" % (args[0], args[1])
      return self.sub(args[0],args[1])
    elif p == prims.multiply:
      return self.mul(args[0], args[1])
      # return "%s * %s" % (args[0], args[1])
    elif p == prims.divide:
      return self.div(args[0], args[1])
      # return "%s / %s" % (args[0], args[1])
    elif p == prims.negative:
      if t == Bool:
        return "1 - %s" % args[0]
      else:
        return "-%s" % args[0]
    elif p == prims.abs:
      x  = args[0]
      return " %s >= 0 ? %s  : -%s" % (x,x,x)
    
    elif p == prims.bitwise_and:
      return "%s & %s" % (args[0], args[1])
    elif p == prims.bitwise_or:
      return "%s | %s" % (args[0], args[1])
    elif p == prims.bitwise_not:
      return "~%s" % args[0]
    
    elif p == prims.logical_and:
      return self.and_(args[0], args[1])
      
    elif p == prims.logical_or:
      return self.or_(args[0], args[1])
    
    elif p == prims.logical_not:
      return self.not_(args[0])
      
    elif p == prims.equal:
      return self.eq(args[0], args[1], t)
    
    elif p == prims.not_equal:
      return self.neq(args[0], args[1], t)
    
    elif p == prims.greater:
      return self.gt(args[0], args[1], t)
      
    elif p == prims.greater_equal:
      return self.gte(args[0], args[1], t)
    
    elif p == prims.less:
      return self.lt(args[0], args[1], t)
    
    elif p == prims.less_equal:
      return self.lte(args[0], args[1], t)
    
    elif p == prims.remainder:
      x,y = args
      if t == Float32: return "fmod(%s, %s)" % (x,y)
      elif t == Float64: return "fmod(%s, %s)" % (x,y)
      assert isinstance(t, (BoolT, IntT)), "Modulo not implemented for %s" % t
      rem = self.fresh_var(t, "rem", "%s %% %s" % (x,y))
      y_is_negative = self.fresh_var(t, "y_is_negative", "%s < 0" % y)
      rem_is_negative = self.fresh_var(t, "rem_is_negative", "%s < 0" % rem)
      y_nonzero = self.fresh_var(t, "y_nonzero", "%s != 0" % y)
      rem_nonzero = self.fresh_var(t, "rem_nonzero", "%s != 0" % rem)
      neither_zero = self.fresh_var(t, "neither_zero", "%s && %s" % (y_nonzero, rem_nonzero))
      diff_signs = self.fresh_var(t, "diff_signs", "%s ^ %s" % (y_is_negative, rem_is_negative))
      should_flip = self.fresh_var(t, "should_flip", "%s && %s" % (neither_zero, diff_signs))
      flipped_rem = self.fresh_var(t, "flipped_rem", "%s + %s" % (y, rem))
      return "%s ? %s : %s" % (should_flip, flipped_rem, rem)
    elif p == prims.fmod:
      if t == Float32: return "fmodf(%s, %s)" % (args[0], args[1])
      elif t == Float64: return "fmod(%s, %s)" % (args[0], args[1])
      return "%s %% %s" % (args[0], args[1])
    elif p == prims.maximum:
      x,y = args
      return "(%s > %s) ? %s : %s" % (x,y,x,y)
    elif p == prims.minimum:
      x,y = args
      return "(%s < %s) ? %s : %s" % (x,y,x,y)
    
    elif p == prims.power:
      if t == Float32: 
        return "powf(%s, %s)" % (args[0], args[1])
      else:
        return "pow(%s, %s)" % (args[0], args[1])
    
    elif isinstance(t, FloatT):
      # many float prims implemented using the same name in math.h
      name = p.name
      if name.startswith("arc"):
        # arccos -> acos
        name = "a" + name[3:]
      if t == Float32: name = name + "f" 
      if len(args) == 1:
        return "%s(%s)" % (name, args[0])
      else:
        assert len(args) == 2, "Unexpected prim %s with %d args (%s)" % (p, len(args), args)
        return "%s(%s, %s)" % (name, args[0], args[1])
  
    else:
      assert False, "Prim not yet implemented: %s" % p
  
  def visit_Index(self, expr):
    arr = self.visit_expr(expr.value)
    if isinstance(expr.index.type, ScalarT):
      index_exprs = [expr.index]
    else:
      assert isinstance(expr.index.type, TupleT), \
        "Unexpected index %s : %s" % (expr.index, expr.index.type)
      if isinstance(expr.index, Tuple):
        index_exprs = expr.index.elts 
      else:
        index_exprs = [TupleProj(expr.index, i, type = t) 
                       for i, t in enumerate(expr.index.type.elt_types) ]
    assert all(isinstance(idx_expr.type, ScalarT) for idx_expr in index_exprs), \
      "Expected all indices to be scalars but got %s" % (index_exprs,)
    indices = [self.visit_expr(idx_expr) for idx_expr in index_exprs]
    if isinstance(expr.value.type, PtrT):
      assert len (indices) == 1, \
        "Can't index into pointer using %d indices (%s)" % (len(indices), index_exprs)
      raw_ptr = "%s.raw_ptr" % arr
      offset = indices[0]
    else:
      assert isinstance(expr.value.type, ArrayT)
      offset = self.fresh_var("int64_t", "offset", "%s.offset" % arr)
      for i, idx in enumerate(indices):
        stride = "%s.strides[%d]" % (arr, i)
        self.append("%s += %s * %s;" % (offset, idx, stride))
      raw_ptr = "%s.data.raw_ptr" % arr 

    return "%s[%s]" % (raw_ptr, offset)
  
  def visit_Call(self, expr):
    fn_name = self.get_fn_name(expr.fn)
    closure_args = self.get_closure_args(expr.fn)
    args = self.visit_expr_list(expr.args)
    return "%s(%s)" % (fn_name, ", ".join(tuple(closure_args) + tuple(args)))
  
  def visit_Select(self, expr):
    cond = self.visit_expr(expr.cond)
    true = self.visit_expr(expr.true_value)
    false = self.visit_expr(expr.false_value)
    return "%s ? %s : %s" % (cond, true, false) 
  
  def is_pure(self, expr):
    return expr.__class__ in (Var, Const, PrimCall, Attribute, TupleProj, Tuple, ArrayView)
  
  def visit_Assign(self, stmt):
    rhs = self.visit_expr(stmt.rhs)

    if stmt.lhs.__class__ is Var:
      lhs = self.visit_expr(stmt.lhs)
      return "%s %s = %s;" % (self.to_ctype(stmt.lhs.type), lhs, rhs)
    elif stmt.lhs.__class__ is Tuple:
      struct_value = self.fresh_var(self.to_ctype(stmt.lhs.type), "lhs_tuple")
      self.assign(struct_value, rhs)
      
      for i, lhs_var in enumerate(stmt.lhs.elts):
        assert isinstance(lhs_var, Var), "Expected LHS variable, got %s" % lhs_var
        c_name = self.visit_expr(lhs_var)
        self.append("%s %s = %s.elt%d;" % (self.to_ctype(lhs_var.type), c_name, struct_value, i ))
      return "" 
    else:
      lhs = self.visit_expr(stmt.lhs)
      return "%s = %s;" % (lhs, rhs)
  
  def declare(self, parakeet_name, parakeet_type, init_value = None):
    c_name = self.name(parakeet_name)
    t = self.to_ctype(parakeet_type)
    if init_value is None:
      self.append("%s %s;" % (t, c_name))
    else: 
      self.append("%s %s = %s;" % (t, c_name, init_value))
  
  def declare_merge_vars(self, merge):
    """ 
    Declare but don't initialize
    """
    for (name, (left, _)) in merge.iteritems():
      self.declare(name, left.type)
      
  def visit_merge_left(self, merge, fresh_vars = True):
    
    if len(merge) == 0:
      return ""
    
    self.push()
    self.comment("Merge Phi Nodes (left side) " + str(merge))
    for (name, (left, _)) in merge.iteritems():
      c_left = self.visit_expr(left)
      if fresh_vars:
        self.declare(name, left.type, c_left)
      else:
        c_name = self.name(name)
        self.append("%s = %s;" % (c_name, c_left))
        
    return self.pop()
  
  def visit_merge_right(self, merge):
    
    if len(merge) == 0:
      return ""
    self.push()
    self.comment("Merge Phi Nodes (right side) " + str(merge))
    
    for (name, (_, right)) in merge.iteritems():
      c_right = self.visit_expr(right)
     
      self.append("%s = %s;"  % (self.name(name), c_right))
    return self.pop()
  
  def visit_NumCores(self, expr):
    # by default we're running sequentially 
    return "1"
  
  def visit_Comment(self, stmt):
    return "// " + stmt.text
    
  def visit_PrintString(self, stmt):
    self.printf(stmt.text)
    return "// done with printf"
    
  def visit_SourceExpr(self, expr):
    return expr.text 
  
  def visit_SourceStmt(self, stmt):
    return stmt.text 
  
  def visit_If(self, stmt):
    self.declare_merge_vars(stmt.merge)
    cond = self.visit_expr(stmt.cond)
    true = self.visit_block(stmt.true) + self.visit_merge_left(stmt.merge, fresh_vars = False)
    false = self.visit_block(stmt.false) + self.visit_merge_right(stmt.merge)
    return self.indent("if(%s) {\n%s\n} else {\n%s\n}" % (cond, self.indent(true), self.indent(false))) 
  
  def visit_While(self, stmt):
    decls = self.visit_merge_left(stmt.merge, fresh_vars = True)
    cond = self.visit_expr(stmt.cond)
    body = self.visit_block(stmt.body) + self.visit_merge_right(stmt.merge)
    return decls + "while (%s) {%s}" % (cond, body)
  
  def visit_ExprStmt(self, stmt):
    return self.visit_expr(stmt.value) + ";"
  
  def visit_ForLoop(self, stmt):
    s = self.visit_merge_left(stmt.merge, fresh_vars = True)
    start = self.visit_expr(stmt.start)
    stop = self.visit_expr(stmt.stop)
    step = self.visit_expr(stmt.step)
    var = self.visit_expr(stmt.var)
    t = self.to_ctype(stmt.var.type)
    body =  self.visit_block(stmt.body)
    body += self.visit_merge_right(stmt.merge)
    body = self.indent("\n" + body) 
    s += "\n %(t)s %(var)s;"
    up_loop = \
        "\nfor (%(var)s = %(start)s; %(var)s < %(stop)s; %(var)s += %(step)s) {%(body)s}"
    down_loop = \
        "\nfor (%(var)s = %(start)s; %(var)s > %(stop)s; %(var)s += %(step)s) {%(body)s}"
      
    if stmt.step.__class__ is Const:
      if stmt.step.value >= 0:
        s += up_loop
      else:
        s += down_loop
    else:
      s += "if(%(step)s >= 0) {\n"
      s += up_loop
      s += "\n} else {\n"
      s += down_loop
      s += "\n}"
    return s % locals()

  def visit_Return(self, stmt):
    assert not self.return_by_ref, "Returning multiple values by ref not yet implemented: %s" % stmt
    if self.return_void:
      return "return;"
    elif isinstance(stmt.value, Tuple):
      # if not returning multiple values by reference, then make a struct for them
      struct_type = self.to_ctype(stmt.value.type)
      result_elts = ", ".join(self.visit_expr(elt) for elt in stmt.value.elts)
      result_value = "{" + result_elts + "}"
      result = self.fresh_var(struct_type, "result", result_value)
      return "return %s;" % result 
    else:
      v = self.visit_expr(stmt.value)
      return "return %s;" % v
  

  
  def visit_block(self, stmts, push = True):
    if push: self.push()
    for stmt in stmts:
      s = self.visit_stmt(stmt)
      self.append(s)
    self.append("\n")
    return self.indent("\n" + self.pop())
      
  
  # TODO: set inline=True
  # currently causes shared lib loading errors since symbol for 
  # inlined function gets stripped out of the .so 
  def get_fn_name(self, expr, compiler_kwargs = {}, attributes = [], inline = True):
    if expr.__class__ is  TypedFn:
      fn = expr 
    elif expr.__class__ is Closure:
      fn = expr.fn 
    else:
      assert isinstance(expr.type, (FnT, ClosureT)), \
        "Expected function or closure, got %s : %s" % (expr, expr.type)
      fn = expr.type.fn

    compiler = self.__class__(module_entry = False, **compiler_kwargs)
    compiled = compiler.compile_flat_source(fn, attributes = attributes, inline = inline)
    
    if compiled.sig not in self.extra_function_signatures:
      # add any declarations it depends on 
      for decl in compiled.declarations:
        self.add_decl(decl)
      
      #add any external objects it wants to be linked against 
      self.extra_objects.update(compiled.extra_objects)
      
      # first add the new function's dependencies
      for extra_sig in compiled.extra_function_signatures:
        if extra_sig not in self.extra_function_signatures:
          self.extra_function_signatures.append(extra_sig)
          self.extra_functions[extra_sig] = compiled.extra_functions[extra_sig] 
      # now add the function itself 
      self.extra_function_signatures.append(compiled.sig)
      self.extra_functions[compiled.sig] = compiled.src
      
    
    for link_flag in compiler.extra_link_flags:
      if link_flag not in self.extra_link_flags:
        self.extra_link_flags.append(link_flag)
    
    for compile_flag in compiler.extra_compile_flags:
      if compile_flag not in self.extra_compile_flags:
        self.extra_compile_flags.append(compile_flag)
  
    return compiled.name

  def get_closure_args(self, fn):
    if isinstance(fn.type, FnT):
      return []
    else:
      assert isinstance(fn, Closure), "Expected closure, got %s : %s" % (fn, fn.type)
      return self.visit_expr_list(fn.args)
      
  def build_loops(self, loop_vars, bounds, body):
    if len(loop_vars) == 0:
      return body
    var = loop_vars[0]
    bound = bounds[0]
    nested = self.build_loops(loop_vars[1:], bounds[1:], body)
    return """
    for (%s = 0; %s < %s; ++%s) {
      %s
    }""" % (var, var, bound, var, nested )
  
  def visit_TypedFn(self, expr):
    return self.get_fn_name(expr)

  def visit_UntypedFn(self, expr):
    return "{}"
  
  def return_types(self, fn):
    if isinstance(fn.return_type, TupleT):
      return fn.return_type.elt_types
    elif isinstance(fn.return_type, NoneT):
      return []
    else:
      # assert isinstance(fn.return_type, (PtrT, ScalarT)), "Unexpected return type %s" % fn.return_type
      return [fn.return_type]
    
  
  def visit_flat_fn(self, fn, return_by_ref = False, attributes = None, inline = True):
    if attributes is None:
      attributes = []
    
    c_fn_name = names.refresh(fn.name).replace(".", "_")
    arg_types = [self.to_ctype(t) for t in fn.input_types]
    arg_names = [self.name(old_arg) for old_arg in fn.arg_names]

    return_types = self.return_types(fn)
    n_return = len(return_types)
    
    if n_return == 1:
      return_type = self.to_ctype(return_types[0])
      self.return_void = (return_type == NoneType)
      self.return_by_ref = False
    elif n_return == 0:
      return_type = "void"
      self.return_void = True
      self.return_by_ref = False
    elif return_by_ref:
      return_type = "void"
      self.return_void = True
      self.return_by_ref = True
      self.return_var_types = [self.to_ctype(t) for t in return_types]
      self.return_var_names = [self.fresh_name("return_value%d" % i) for i in xrange(n_return)]
      arg_types = arg_types + ["%s*" % t for t in self.return_var_types] 
      arg_names = arg_names + self.return_var_names
    else:
      return_type = self.struct_type_from_fields(return_types)
      self.return_void = False 
      self.return_by_ref = False 
    args_str = ", ".join("%s %s" % (t, name) for (t,name) in zip(arg_types,arg_names))
    
    body_str = self.visit_block(fn.body) 
    
    if inline:
      attributes = attributes + ["static inline"]
    attr_str = " ".join(attributes)
    sig = "%s %s(%s)" % (return_type, c_fn_name, args_str)
    src = "%s %s {\n\n%s}" % (attr_str, sig, body_str) 
    return c_fn_name, sig, src
  
  @property 
  def cache_key(self):
    """
    If we ever need to differentiate compiled function by *how* they were compiled,
    we can use this cache key to track the class of the compiler or other
    relevant meta-data
    """ 
    return self.__class__ 
  
  _flat_compile_cache = {}
  def compile_flat_source(self, parakeet_fn, attributes = [], inline = True):
      
    # make sure compiled source uses consistent names for tuple and array types, 
    # which both need declarations for their C struct representations  
    struct_types = set(t for t in parakeet_fn.type_env.itervalues() 
                         if isinstance(t, (ArrayT, TupleT)))
    
    # include your own class in the cache key so that we get distinct code 
    # for derived compilers like OpenMP and CUDA 
    key = parakeet_fn.cache_key, frozenset(struct_types), self.cache_key, tuple(attributes)
    
    if key in self._flat_compile_cache:
      return self._flat_compile_cache[key]
    
    name, sig, src = self.visit_flat_fn(parakeet_fn, attributes = attributes, inline = inline)
    
    result = CompiledFlatFn(
      name = name, 
      sig = sig, 
      src = src,
      extra_objects = self.extra_objects, 
      extra_functions = self.extra_functions,
      extra_function_signatures = self.extra_function_signatures,
      declarations = self.declarations)
    self._flat_compile_cache[key] = result
    return result

########NEW FILE########
__FILENAME__ = prepare_args
from itertools import izip 
import numpy as np
from ..ndtypes import (type_conv, ScalarT, ArrayT, FnT, ClosureT, SliceT, NoneT, TupleT, TypeValueT)
from ..syntax import TypedFn, UntypedFn


def prepare_closure_args(untyped_fn):
  closure_args = untyped_fn.python_nonlocals()
  closure_arg_types = [type_conv.typeof(v) for v in closure_args]
  return prepare_args(closure_args, closure_arg_types)
      

def prepare_arg(arg, t):
  if isinstance(t, ScalarT):
    return t.dtype.type(arg)
  elif isinstance(t, ArrayT):
    return np.asarray(arg)
  elif isinstance(t, TupleT):
    arg = tuple(arg)
    assert len(arg) == len(t.elt_types)
    return prepare_args(arg, t.elt_types)
  elif isinstance(t, (NoneT, SliceT)):
    return arg
  elif isinstance(t, TypeValueT):
    return ()
  elif isinstance(t, (FnT, ClosureT)):
    if isinstance(arg, TypedFn):
      return ()
    elif isinstance(arg, UntypedFn):
      return prepare_closure_args(arg)
    else:
      from ..frontend import ast_conversion
      untyped = ast_conversion.translate_function_value(arg)
      return prepare_closure_args(untyped)
  assert False, "Can't call compiled C code with argument %s (Python type = %s, Parakeet type = %s)" % \
    (arg, type(arg), t)
  
def prepare_args(args, arg_types):
  return tuple(prepare_arg(arg, t) for arg, t in izip(args, arg_types))
  


########NEW FILE########
__FILENAME__ = pymodule_compiler
from ..analysis import use_count
from ..syntax import Tuple,  Expr
 
from ..ndtypes import (
  TupleT,  ArrayT,  NoneT, elt_type, ScalarT, FloatT, BoolT,  
  IntT,  Int64, SignedT, PtrT, ClosureT, SliceT, ptr_type
)

import type_mappings
from fn_compiler import FnCompiler
from compile_util import compile_module_from_source
from .. import config as root_config 
import config 

def attr_from_kwargs(obj, kwargs, attr, value = None):
  """
  If an attribute is in the kwargs dictionary, then assign it to the
  given object and remove it from the dictionary, otherwise assign 
  the default value
  """
  if attr in kwargs:
    setattr(obj, attr, kwargs[attr])
    del kwargs[attr]
  else:
    setattr(obj, attr, value)


class PyModuleCompiler(FnCompiler):
  """
  Compile a Parakeet function into a Python module with an 
  entry-point that unboxes all the PyObject inputs, 
  runs a flattened computations, and boxes the result as PyObjects
  """
  def __init__(self, module_entry = True, *args, **kwargs):
    attr_from_kwargs(self, kwargs, 'compiler_cmd')    
    attr_from_kwargs(self, kwargs, 'compiler_flag_prefix')
    attr_from_kwargs(self, kwargs, 'linker_flag_prefix')  
    attr_from_kwargs(self, kwargs, 'src_extension')
    FnCompiler.__init__(self, module_entry = module_entry, *args, **kwargs)
    
  def unbox_scalar(self, x, t, target = None):
    assert isinstance(t, ScalarT), "Expected scalar type, got %s" % t
    if target is None:
      target = "scalar_value"
      
    result = self.fresh_var(t, target)
    if isinstance(t, IntT):
      check = "PyInt_Check"
      if isinstance(t, SignedT):
        get = "PyInt_AsLong"
      else:
        get = "PyInt_AsUnsignedLongMask"
    elif isinstance(t, FloatT):
      check = "PyFloat_Check"
      get = "PyFloat_AsDouble"
    else:
      assert isinstance(t, BoolT), "Unexpected type %s" % t 
      check = "PyBool_Check"
      get = "PyObject_IsTrue"
    
    self.append("""
      if (%(check)s(%(x)s)) { %(result)s = %(get)s(%(x)s); }
      else { PyArray_ScalarAsCtype(%(x)s, &%(result)s); }
    """ % locals())
    return result 
  
  def unbox_array(self, boxed_array, elt_type, ndims, target = "array_value"):
    shape_ptr = self.fresh_var("npy_intp*", "shape_ptr", "PyArray_DIMS((PyArrayObject*)%s)" % boxed_array)
    strides_bytes = self.fresh_var("npy_intp*", "strides_bytes", 
                                   "PyArray_STRIDES( (PyArrayObject*) %s)" % boxed_array)

    #strides_elts = self.fresh_name("strides_elts")
    #self.append("npy_intp %s[%d];" % (strides_elts, ndims))
    bytes_per_elt = elt_type.dtype.itemsize 
    typename = self.array_struct_type(elt_type, ndims)
    result = self.fresh_var(typename, "unboxed_array")
    ptr = self.get_boxed_array_ptr(boxed_array, ptr_type(elt_type))
    self.setfield(result, 'data', ptr)
    for i in xrange(ndims):
      if config.debug:
        self.printf("converting strides %s[%d] = %%ld to %%ld" % (strides_bytes, i), 
                    "%s[%d]" % (strides_bytes, i), "%s[%d] / %d" % (strides_bytes, i, bytes_per_elt))   
      self.setidx("%s.strides" % result, i, "%s[%d] / %d" % (strides_bytes, i, bytes_per_elt))
      self.setidx("%s.shape" % result, i, "%s[%d]" % (shape_ptr, i))
    self.setfield(result, "offset", "0")
    self.setfield(result, "size", "PyArray_Size(%s)" % boxed_array)
    return result 
  
  def unbox_tuple(self, boxed_tuple, tuple_t, target = "tuple_value"):
    if isinstance(tuple_t, TupleT):
      elt_types = tuple(tuple_t.elt_types)
    elif isinstance(tuple_t, ClosureT):
      elt_types = tuple(tuple_t.arg_types)
    else:
      assert False, "Expected tuple type, got %s" % tuple_t
     
    c_struct_t = self.struct_type_from_fields(elt_types)
    unboxed_elts = []
    for i in xrange(len(elt_types)):
      elt_t = elt_types[i]
      elt_typename = self.to_ctype(elt_t)
      elt = self.fresh_var(elt_typename, "unboxed_elt%d" % i, 
                           self.tuple_elt(boxed_tuple, i, elt_types[i], boxed=True))
      unboxed_elts.append(elt)
    elts_str = ", ".join(unboxed_elts)
    return self.fresh_var(c_struct_t, target, "{" + elts_str + "}")
      
  def unbox_slice(self, boxed, t, target = None):
    if target is None: target = "slice"
    typename = self.to_ctype(t)
    start = self.unbox("((PySliceObject*)%s)->start" % boxed, t.start_type, "start")
    stop = self.unbox("((PySliceObject*)%s)->stop" % boxed, t.stop_type, "stop")
    step = self.unbox("((PySliceObject*)%s)->step" % boxed, t.step_type, "step")
    return self.fresh_var(typename, target, "{%s,%s,%s}" % (start,stop,step) )
    
  def unbox(self, boxed, t, target = None):
    if isinstance(t, NoneT):
      return "0"
    elif isinstance(t, PtrT):
      assert False, "Unexpected raw pointer passed as argument from Python %s : %s" % (boxed, t)
    if isinstance(t, ScalarT):
      return self.unbox_scalar(boxed, t, target = target)
    elif isinstance(t, (ClosureT, TupleT)):
      return self.unbox_tuple(boxed, t, target = target)
    elif isinstance(t, SliceT):
      return self.unbox_slice(boxed, t, target = target)
    elif isinstance(t, ArrayT):
      return self.unbox_array(boxed, 
                              elt_type = t.elt_type, 
                              ndims = t.rank, target = target)
    else:
      assert False, "Don't know how to unbox %s : %s" % (boxed, t)
        
  def box_none(self):
    self.append("Py_INCREF(Py_None);")
    return "Py_None"
  
  def box_scalar(self, x, t):  
    if isinstance(t, BoolT):
      return "PyBool_FromLong(%s)" % x
    if x.replace("_", "").isalpha():
      scalar = x
    else:
      scalar = self.fresh_name("scalar");
      self.append("%s %s = %s;" % (self.to_ctype(t), scalar, x))
    return "PyArray_Scalar(&%s, PyArray_DescrFromType(%s), NULL)" % (scalar, type_mappings.to_dtype(t) )
  
  def box_tuple(self, x, t):
    if isinstance(t, ClosureT):
      elt_types = t.arg_types 
    else:
      assert isinstance(t, TupleT)
      elt_types = t.elt_types
    unboxed_elts = self.tuple_elts(x, elt_types)
    boxed_elts = [self.box(elt, elt_t) for elt, elt_t in zip(unboxed_elts, elt_types)]
    n = len(boxed_elts)
    if n == 0:
      return "PyTuple_Pack(0)"
    else:
      return "PyTuple_Pack(%d, %s)" % (n, ", ".join(boxed_elts))
  
  def box_slice(self, x, t):
    start = self.box("%s.start" % x, t.start_type)
    stop = self.box("%s.stop" % x, t.stop_type)
    step = self.box("%s.step" % x, t.step_type)
    return "PySlice_New(%s, %s, %s)" % (start, stop, step)
  
  def box(self, x, t):
    if isinstance(t, ScalarT):
      return self.box_scalar(x, t)
    elif isinstance(t, NoneT):
      return self.box_none()
    elif isinstance(t, (ClosureT, TupleT)):
      return self.box_tuple(x, t)
    elif isinstance(t, SliceT):
      return self.box_sclie(x, t)
    elif isinstance(t, ArrayT):
      return self.box_array(x, t)
    else:
      # all other types already boxed 
      return x
      
      
  def as_pyobj(self, expr):
    """
    Compile the expression and if necessary box it up as a PyObject
    """
    x = self.visit_expr(expr)
    if isinstance(expr.type, (NoneT, ScalarT, TupleT, ClosureT, ArrayT, SliceT)):
      return self.box(x, expr.type)
    else:
      return x
  
  def as_pyobj_list(self, exprs):
    return [self.as_pyobj(expr) for expr in exprs]
  
  def c_str(self, obj):
    return "PyString_AsString(PyObject_Str(%s))" % obj
  
  def c_type_str(self, obj):
    return self.c_str("PyObject_Type((PyObject*) %s)" % obj)
   
  def print_pyobj(self, obj, text = ""):
    text = '"%s"' % text
    self.append('printf("%%s%%s\\n", %s, %s);' % (text, self.c_str(obj)))
  
  def print_pyobj_type(self, obj, text=""):
    text = '"%s"' % text
    self.append('printf("%%s%%s\\n", %s, %s);' % (text, self.c_type_str(obj)))

    
  def setfield(self, base, fieldname, value):
    self.append("%s.%s = %s;" % (base, fieldname, value))
  
  def getidx(self, arr, idx, full_array = False):
    """
    Given either the compiled string representation or Parakeet IR
    for an array and index, construct a C indexing expression 
    """

    if isinstance(arr, Expr):
      if isinstance(arr.type, ArrayT):
        arr = self.visit_expr(arr)
        ptr = "%s.data.raw_ptr" % arr
      else:
        assert isinstance(arr.type, PtrT), \
          "Expected array or pointer but got %s : %s" % (arr, arr.type)
        arr = None
        ptr = self.visit_expr(arr)
    else:    
      assert isinstance(arr, str), "Expected string repr of array but got %s" % arr
      if full_array:
        ptr = "%s.data.raw_ptr" % arr 
      else:
        ptr = arr 
        arr = None  
        
    if isinstance(idx, Expr):
      if isinstance(idx.type, TupleT):
        nelts = len(idx.type.elt_types)
        tuple_value = self.visit_expr(idx)
        idx = ["%s.elt%d" % (tuple_value,i) for i in xrange(nelts)]
      else:
        assert isinstance(idx.type, ScalarT), "Expected index %s to be scalar or tuple" % idx
        offset = idx 
        
    if isinstance(idx, str):
      offset = idx 
    elif isinstance(idx, (int,long)):
      offset = "%s" % idx
    elif isinstance(idx, (list,tuple)):
      assert arr, "Expected full array but only pointer %s is available" % ptr 
      offset = "0"
      for i,idx_elt in enumerate(idx):
        if isinstance(idx_elt, Expr):
          idx_elt = self.visit_expr(idx_elt)
        offset = self.add(offset, self.mul("%s.strides[%d]" % (arr,i), idx_elt))
      
    return "%s[%s]" % (ptr, offset)
  
  def setidx(self, arr, idx, value, full_array = False, return_stmt = False):
    stmt = "%s = %s;" % (self.getidx(arr, idx, full_array = full_array), value)
    if return_stmt:
      return stmt 
    else:
      self.append(stmt)

     
  def tuple_to_stack_array(self, expr, name = "array_from_tuple", elt_type = None):
    t0 = expr.type.elt_types[0]
    
    assert expr.type.__class__ is TupleT 
    assert all(t == t0 for t in expr.type.elt_types[1:])
    
    if expr.__class__ is Tuple:
      elts = [self.visit_expr(elt_expr) for elt_expr in expr.elts]
    else:
      tup = self.visit_expr(expr)
      self.check_tuple(tup)
      elts = self.tuple_elts(tup, expr.type.elt_types)
    
    array_name = self.fresh_name(name)
    n = len(expr.type.elt_types)
    if elt_type is None:
      elt_type = self.to_ctype(t0)
    self.append("%s %s[%d];" % (elt_type, array_name, n))
    for i, elt in enumerate(elts):
      self.append("%s[%d] = %s;" % (array_name, i, elt))
    return array_name
    
  def array_to_tuple(self, arr, n, elt_t, boxed = False):
    raw_elts = ["%s[%d]" % (arr,i) for i in xrange(n)]
    if boxed:
      if n == 0: 
        return "PyTuple_Pack(0);"
      else:
        boxed_elts = [self.box_scalar(raw_elt, elt_t) for raw_elt in raw_elts]
        elt_str = ", ".join(boxed_elts)
        return "PyTuple_Pack(%d, %s)" % (n, elt_str)
    else:
      field_types = (self.to_ctype(elt_t),) * n 
      tuple_struct_t = self.struct_type_from_fields(field_types)
      init = "{%s}" % ", ".join(raw_elts)
      return self.fresh_var(tuple_struct_t, "tuple_value", init)
      
  
  def tuple_elts(self, tup, ts, boxed = False):
    result = []
    for i,t in enumerate(ts):
      result.append(self.tuple_elt(tup, i, t, boxed = boxed))
    return result
  
  def mk_tuple(self, elts, boxed = False):
    n = len(elts)
    if boxed:
      if n == 0: 
        return "PyTuple_Pack(0);"
      else:
        elt_str = ", ".join(self.as_pyobj_list(elts)) 
        return "PyTuple_Pack(%d, %s)" % (n, elt_str)
    else:
      field_types = tuple(elt.type for elt in elts)
      tuple_struct_t = self.struct_type_from_fields(field_types)
      elts_str = ", ".join(self.visit_expr(elt) for elt in elts)
      init = "{" + elts_str +  "}"
      return self.fresh_var(tuple_struct_t, "tuple_value", init)
    
  
  def check_tuple(self, tup):
    if not config.check_pyobj_types: return 
    self.newline()
    self.comment("Checking tuple type for %s" % tup)
    self.append("""
      if (!PyTuple_Check(%s)) { 
        PyErr_Format(PyExc_AssertionError, 
                    "Expected %s to be tuple, got %%s", 
                    %s); 
        return NULL;
      }""" % (tup, tup, self.c_type_str(tup)))
 
  def check_slice(self, obj):
    if not config.check_pyobj_types: return 
    self.newline()
    self.comment("Checking slice type for %s" % obj)
    self.append("""
      if (!PySlice_Check(%s)) { 
        PyErr_Format(PyExc_AssertionError, 
                    "Expected %s to be slice, got %%s", 
                    %s); 
        return NULL;
      }""" % (obj, obj, self.c_type_str(obj)))
  
  def check_array(self, arr):
    if not config.check_pyobj_types: return 
    self.newline()
    self.comment("Checking array type for %s" % arr)
    self.append("""
      if (!PyArray_Check(%s)) { 
        PyErr_Format(PyExc_AssertionError, 
                    "Expected %s to be array, got %%s : %%s", 
                    %s, %s); 
        return NULL;
      }""" % (arr, arr, self.c_str(arr), self.c_type_str(arr)))
  
  
  def check_bool(self, x):
    if not config.check_pyobj_types: return 
    self.newline()
    self.comment("Checking bool type for %s" % x)
    self.append("""
      if (!PyArray_IsScalar(%s, Bool)) { 
        PyErr_Format(PyExc_AssertionError, 
                     "Expected %s to be bool, got %%s", 
                     %s); 
        return NULL;
      }""" % (x, x, self.c_type_str(x)))
  
  def check_int(self, x):
    if not config.check_pyobj_types: return 
    self.newline()
    self.comment("Checking int type for %s" % x)
    self.append("""
      if (!PyArray_IsIntegerScalar(%s)) { 
        PyErr_Format(PyExc_AssertionError, 
                     "Expected %s to be int, got %%s", 
                     %s); 
        return NULL;
      }""" % (x, x, self.c_type_str(x)))
  
  def check_type(self, v, t):
    if not config.check_pyobj_types: return 
    if isinstance(t, (ClosureT, TupleT)):
      self.check_tuple(v)
    elif isinstance(t, BoolT):
      self.check_bool(v)
    elif isinstance(t, IntT):
      self.check_int(v)
    elif isinstance(t, ArrayT):
      self.check_array(v)
      
  def tuple_elt(self, tup, idx, t, boxed = False):
    if boxed: 
      self.check_tuple(tup)
      proj_str = "PyTuple_GetItem(%s, %d)" % (tup, idx)
      elt_obj = self.fresh_var("PyObject*", "%s_elt" % tup, proj_str)
      result = self.unbox(elt_obj, t)
      if config.debug and t == Int64:
        self.append(""" printf("tupleproj %s[%d] = %%" PRId64 "\\n", %s);""" % (tup, idx, result))
      return result
    else:
      return "%s.elt%d" % (tup, idx)  
 
    
  def decref(self, obj):
    self.append("Py_DECREF(%s);" % obj)
  
  def get_boxed_array_ptr(self, v, parakeet_ptr_t):
    self.check_array(v)
    c_struct_type = self.to_ctype(parakeet_ptr_t)
    c_ptr_type = type_mappings.to_ctype(parakeet_ptr_t) 
    data_field = "(%s) PyArray_DATA(((PyArrayObject*) %s))" % (c_ptr_type, v) 
    # get the data field but also fill the base object 
    return self.fresh_var(c_struct_type, "data", "{%s, %s}" % (data_field, v))
 
  def attribute(self, v, attr, t, boxed = False):
    if attr == "data":
      if boxed:
        self.check_array(v)
        struct_type = self.to_ctype(t)
        ptr_type = type_mappings.to_ctype(t) 
        data_field = "(%s) PyArray_DATA(((PyArrayObject*) %s))" % (ptr_type, v) 
        # get the data field but also fill the base object 
        return self.fresh_var(struct_type, "data", "{%s, %s}" % (data_field, v))
      else:
        return "%s.data" % v  
    elif attr == "shape":
      shape_name = self.fresh_name("shape")
      elt_types = t.elt_types
      n = len(elt_types)
      elt_t = elt_types[0]  
      if boxed:
        self.check_array(v)
        
        assert all(t == elt_t for t in elt_types)

        shape_array = "PyArray_DIMS( (PyArrayObject*) %s)" % v
        self.append("npy_intp* %s = %s;" % (shape_name, shape_array))
      else:
        self.append("npy_intp* %s = %s.shape;" % (shape_name, v))
      return self.array_to_tuple(shape_name, n, elt_t)
      
    elif attr == "strides":
      elt_types = t.elt_types
      n = len(elt_types)  
      elt_t = elt_types[0]
      if boxed:
        assert False, "Can't directly use NumPy strides without dividing by itemsize"
      else:
        strides_name = self.fresh_var("npy_intp*", "strides", "%s.strides" % v)
      return self.array_to_tuple(strides_name, n, elt_t)
    elif attr == 'offset':
      if boxed:
        return "0"
      else:
        return "%s.offset" % v
    elif attr in ('size', 'nelts'):
      if boxed:
        return "PyArray_Size(%s)" % v
      else:
        return "%s.size" % v
       
    elif attr in ('start', 'stop', 'step') or attr.startswith("elt"):
      if boxed:
        self.check_slice(v)
        obj = "((PySliceObject*)%s)->%s" % (v, attr)
        return self.unbox_scalar(obj, t, attr)
      else:
        return "%s.%s" % (v, attr)
    else:
      assert False, "Unsupported attribute %s" % attr 
   
    
  
  def alloc_array(self, array_t, shape_expr):
    if isinstance(shape_expr.type, ScalarT):
      dim = self.visit_expr(shape_expr)
      nelts = dim 
      shape_elts = [dim]
      ndims = 1 
    else:
      shape = self.visit_expr(shape_expr)
      ndims = array_t.rank
      shape_elts = ["%s.elt%d" % (shape, i) for i in xrange(ndims)]
      nelts = self.fresh_var("int64_t", "nelts", " * ".join(shape_elts))
    bytes_per_elt = array_t.elt_type.dtype.itemsize
    typename = self.to_ctype(array_t)
    result = self.fresh_var(typename, "new_array")
    raw_ptr_t = self.to_ctype(array_t.elt_type) + "*"
    self.setfield(result, "data.raw_ptr", "(%s) malloc(%s * %s)" % (raw_ptr_t, nelts, bytes_per_elt) )
    self.setfield(result, "data.base", "(PyObject*) NULL")
    self.setfield(result, "offset", "0")
    self.setfield(result, "size", nelts)
    # assume C-order layout
    strides_elts = [" * ".join(["1"] + shape_elts[(i+1):]) for i in xrange(ndims)]
    for i in xrange(ndims):
      self.setidx("%s.shape" % result, i, shape_elts[i])
      # assume c major layout for now
      self.setidx("%s.strides" % result, i, strides_elts[i])
    if config.debug:
      self.printf("[Debug] Done allocating array")
    return result 
    
  
  def visit_AllocArray(self, expr, boxed=False):
    if boxed:
      shape = self.tuple_to_stack_array(expr.shape)
      t = type_mappings.to_dtype(elt_type(expr.type))
      return "(PyArrayObject*) PyArray_SimpleNew(%d, %s, %s)" % (expr.type.rank, shape, t)
    
    if config.debug:
      print "[Debug] Allocating array : %s " % expr.type  
    return self.alloc_array(expr.type, expr.shape)
     
  def visit_Tuple(self, expr):
    return self.mk_tuple(expr.elts, boxed = False)
  
  def visit_Closure(self, expr):
    return self.mk_tuple(expr.args)
  
  def visit_TupleProj(self, expr):
    tup = self.visit_expr(expr.tuple)
    result = self.tuple_elt(tup, expr.index, expr.type)
    return result
  
  def visit_ClosureElt(self, expr):
    clos = self.visit_expr(expr.closure)
    return self.tuple_elt(clos, expr.index, expr.type)
  
  def visit_ArrayView(self, expr):
    typename = self.to_ctype(expr.type)
    data = self.visit_expr(expr.data)
    ndims = expr.type.rank 
    offset = self.visit_expr(expr.offset)
    count = self.visit_expr(expr.size)
    shape = self.visit_expr(expr.shape)
    strides = self.visit_expr(expr.strides)
  
    strides_elts = ["%s.elt%d" % (strides, i) for i in xrange(ndims)]
    shape_elts = ["%s.elt%d" % (shape, i) for i in xrange(ndims)]

    result = self.fresh_var(typename, "array_result")
    self.setfield(result, "data", data)
    self.setfield(result, "offset", offset)
    self.setfield(result, 'size', count)
    for i in xrange(ndims):
      self.setidx("%s.shape" % result, i, shape_elts[i])
      self.setidx("%s.strides" % result, i, strides_elts[i])
    return result 
  
  """
  def box_ArrayView(self, expr):
    data = self.visit_expr(expr.data)
    ndims = expr.type.rank 
    offset = self.visit_expr(expr.offset)
    count = self.visit_expr(expr.size)
    shape = self.visit_expr(expr.shape)
    strides = self.visit_expr(expr.strides)
  
    strides_elts = ["%s.elt%d" % (strides, i) for i in xrange(ndims)]
    shape_elts = ["%s.elt%d" % (shape, i) for i in xrange(ndims)]
    shape_array = None 
    strides_array = None 
    elt_type = expr.type.elt_type 
    return self.make_boxed_array(elt_type, ndims, data, strides_array, shape_array, offset, count)
  """
  
  def box_array(self, arr, t):
    elt_t = t.elt_type
    ndims = t.rank 
    data_ptr = "%s.data.raw_ptr" % arr 
    base = "%s.data.base" % arr
    strides_array = "%s.strides" % arr 
    shape_array = "%s.shape" % arr 
    offset = "%s.offset" % arr 
    size = "%s.size" % arr 
    return self.make_boxed_array(elt_t, ndims, data_ptr, strides_array, shape_array, offset, size, base)
  
  def make_boxed_array(self, elt_type, ndims, data_ptr, strides_array, shape_array, offset, size, base = "NULL"):
    
    
    # PyObject* PyArray_SimpleNewFromData(int nd, npy_intp* dims, int typenum, void* data)
    typenum = type_mappings.to_dtype(elt_type)
    array_alloc = \
      "(PyArrayObject*) PyArray_SimpleNewFromData(%d, %s, %s, &%s[%s])" % \
        (ndims, shape_array, typenum, data_ptr, offset)
    vec = self.fresh_var("PyArrayObject*", "fresh_array", array_alloc) 
    self.return_if_null(vec)
    
    # if the pointer had a PyObject reference, 
    # set that as the new array's base 
    if base not in ("0", "NULL"):
      self.append("""
        if (%s) { 
          PyArray_SetBaseObject(%s, %s); 
          Py_INCREF(%s);  
          PyArray_CLEARFLAGS(%s, NPY_ARRAY_OWNDATA); 
        }""" % (base, vec, base, base, vec))
        
    numpy_strides = self.fresh_var("npy_intp*", "numpy_strides")
    self.append("%s = PyArray_STRIDES(  (PyArrayObject*) %s);" % (numpy_strides, vec))
    bytes_per_elt = elt_type.dtype.itemsize
    
    for i in xrange(ndims):
      self.append("%s[%d] = %s[%d] * %d;" % (numpy_strides, i, strides_array, i, bytes_per_elt) )
      
    
    self.append("""
      // clear both fortran and c layout flags 
      PyArray_CLEARFLAGS((PyArrayObject*) %(vec)s, NPY_ARRAY_F_CONTIGUOUS);  
      PyArray_CLEARFLAGS((PyArrayObject*) %(vec)s, NPY_ARRAY_C_CONTIGUOUS);
    """ % locals())
    
    f_layout_strides = ["1"]
    for i in xrange(1, ndims):
      shape_elt = "%s[%d]" % (shape_array, i)
      f_layout_strides.append(f_layout_strides[-1] + " * " + shape_elt)
    
    c_layout_strides = ["1"]
    for i in xrange(ndims-1,0,-1):
      shape_elt = "%s[%d]" % (shape_array, i)
      c_layout_strides = [c_layout_strides[-1] + " * " + shape_elt] + c_layout_strides
    
    
    strides_elts = ["%s[%d]" % (strides_array, i) for i in xrange(ndims)]
    is_c_layout = "&& ".join(self.eq(actual, ideal, Int64) 
                             for actual, ideal 
                             in zip(strides_elts, c_layout_strides))
    is_f_layout = " && ".join(self.eq(actual, ideal, Int64) 
                             for actual, ideal 
                             in zip(strides_elts, f_layout_strides))
    
       
    # make sure the contiguity flags are set correctly 
    self.append("""
      // it's possible that *neither* of the above flags should be on
      // which is why we enable them separately here 
      if (%(is_f_layout)s) { PyArray_ENABLEFLAGS((PyArrayObject*)%(vec)s, NPY_ARRAY_F_CONTIGUOUS); }
      if (%(is_c_layout)s) { PyArray_ENABLEFLAGS((PyArrayObject*)%(vec)s, NPY_ARRAY_C_CONTIGUOUS); }
    """ % locals())
    return vec
  
    
  def visit_Attribute(self, expr):
    attr = expr.name
    v = self.visit_expr(expr.value) 
    return self.attribute(v, attr, expr.type)
  
  def visit_Return(self, stmt):
    if self.module_entry:
      v = self.as_pyobj(stmt.value)
      if config.debug: 
        self.print_pyobj_type(v, "Return type: ")
        self.print_pyobj(v, "Return value: ")
      return "return (PyObject*) %s;" % v
    else:
      return FnCompiler.visit_Return(self, stmt)
  
  def visit_block(self, stmts, push = True):
    if push: self.push()
    for stmt in stmts:
      s = self.visit_stmt(stmt)
      self.append(s)
    self.append("\n")
    return self.pop()
  
  
  def enter_module_body(self):
    """
    Some derived compiler classes might want to use this hook
    to generate special code when entering the function body of the module entry
    """
    pass 
  
  def exit_module_body(self):
    pass 
  
  def visit_fn(self, fn):
    if config.print_input_ir:
      print "=== Compiling to C with %s (entry function) ===" % self.__class__.__name__ 
      print fn
    c_fn_name = self.fresh_name(fn.name)
    uses = use_count(fn)
    self.push()
    
    
    dummy = self.fresh_name("dummy")
    args = self.fresh_name("args")
    
    if config.debug: 
      self.newline()
      self.printf("\\nStarting %s : %s..." % (c_fn_name, fn.type))
      
    for i, argname in enumerate(fn.arg_names):
      assert argname in uses, "Couldn't find arg %s in use-counts" % argname
      if uses[argname] <= 1:
        self.comment("Skipping unused argument %s" % argname)
        continue
      self.comment("Unpacking argument %s"  % argname)
      c_name = self.name(argname)
      t = fn.type_env[argname]
      self.comment("Getting arg #%d: %s : %s => %s" % (i+1, argname,  t, c_name) )
      self.append("PyObject* %s = PyTuple_GetItem(%s, %d);" % (c_name, args, i))
      
      self.check_type(c_name, t)
      if config.debug:
        self.printf("Printing arg #%d %s" % (i,c_name))
        self.print_pyobj_type(c_name, text = "Type: ")
        self.print_pyobj(c_name, text = "Value: ")
      
      if isinstance(t, (TupleT, NoneT, PtrT, ClosureT, ScalarT, ArrayT, SliceT)):
        #new_name = self.name(argname, overwrite = True)
        self.comment("Unboxing %s : %s" % (argname, t))
        var = self.unbox(c_name, t, target = argname)

        self.name_mappings[argname] = var
      

    self.enter_module_body()
    c_body = self.visit_block(fn.body, push=False)
    self.exit_module_body()
    c_body = self.indent(c_body )
    c_args = "PyObject* %s, PyObject* %s" % (dummy, args) #", ".join("PyObject* %s" % self.name(n) for n in fn.arg_names)
    c_sig = "PyObject* %(c_fn_name)s (%(c_args)s)" % locals() 
    fndef = "%s {\n\n %s}" % (c_sig, c_body)
    return c_fn_name, c_sig, fndef 
  
  _entry_compile_cache = {} 
  def compile_entry(self, parakeet_fn):  
    # we include the compiler's class as part of the key
    # since this function might get reused by descendant backends like OpenMP and CUDA
    key = parakeet_fn.cache_key, self.__class__ 
    compiled_fn = self._entry_compile_cache.get(key)
    if compiled_fn: return compiled_fn 
    
    name, sig, src = self.visit_fn(parakeet_fn)
    
    if config.print_function_source: 
      print "Generated C source for %s: %s" %(name, src)
    ordered_function_sources = [self.extra_functions[extra_sig] for 
                                extra_sig in self.extra_function_signatures]

    compiled_fn = compile_module_from_source(
      src, 
      fn_name = name,
      fn_signature = sig, 
      src_extension = self.src_extension,
      extra_objects = set(self.extra_objects),
      extra_function_sources = ordered_function_sources, 
      declarations =  self.declarations, 
      extra_compile_flags = self.extra_compile_flags, 
      extra_link_flags = self.extra_link_flags, 
      print_source = root_config.print_generated_code, 
      compiler = self.compiler_cmd, 
      compiler_flag_prefix = self.compiler_flag_prefix, 
      linker_flag_prefix = self.linker_flag_prefix)
    self._entry_compile_cache[key]  = compiled_fn
    return compiled_fn


########NEW FILE########
__FILENAME__ = reserved_names
keywords = set([
  "auto", 
  "break",
  "case",
  "char",
  "const",
  "continue", 
  "default",
  "do",
  "double",
  "else",
  "enum",
  "extern", 
  "float",
  "for",
  "goto",
  "if", 
  "int",
  "long",
  "register",
  "return"
  "short"
  "signed"
  "sizeof"
  "static"
  "struct",
  "switch"
  "typedef", 
  "union", 
  "unsigned",
  "void", 
  "volatile",
  "while"])

macro_names = set(["assert"])
util_names = set(["printf", "malloc", "free", ])

import math 

base_math_names = [name for name in dir(math) if not name.startswith("__")]

float32_math_names = [name+'f' for name in base_math_names]
long_math_names = [name+"l" for name in base_math_names]
extra_math_names = [
  "atof", "cbrt", "cbrtf", "cbrtl",
  "div", 
  "exp2", "exp2f", "exp2l", 
  "j0", "j1", "jn",
  "log2", "log2f", "log2l", 
  "logb", "logbf", "logbl", 
  "lrintf", "lrintl", 
  "lround", "lroundf", "lroundl", 
  "remainder", "remainderf", "remainderl", 
  "remquo", "remquof", "remquol", 
  "rint", "rintf", "rintl", 
  "round", "roundf", "roundl",
  "scalb", "scalbln", "scalblnf", "scalblnl", "scalbn", "scalbnf", "scalbnl",  
  "signgam", 
  "tgamma", "tgammaf", "tgammal"  
  "y0", "y1", "yn",              
]
math_names = set(base_math_names+float32_math_names+long_math_names+extra_math_names)
all_reserved_names = keywords.union(macro_names).union(util_names).union(math_names)
 
def is_reserved(name):
  return name in all_reserved_names or name.startswith("Py") 
########NEW FILE########
__FILENAME__ = run_function
from prepare_args import prepare_args
from ..transforms.pipeline  import lower_to_loops
from ..value_specialization import specialize
from ..config import value_specialization
from pymodule_compiler import PyModuleCompiler 



_cache = {}
def run(fn, args):
  args = prepare_args(args, fn.input_types)

  fn = lower_to_loops(fn)
  
  if value_specialization: 
    fn = specialize(fn, args)

  key = fn.cache_key
  if key in _cache:
    return _cache[key](*args)
  compiled_fn = PyModuleCompiler().compile_entry(fn)
  c_fn = compiled_fn.c_fn 
  _cache[key] = c_fn 
  return c_fn(*args)
  
########NEW FILE########
__FILENAME__ = shell_command
import os 
import subprocess
import time

import config 

class CommandFailed(Exception):
  def __init__(self, cmd, env, label):
    self.cmd = cmd 
    self.env = env 
    self.label = label 
    
    
  def __str__(self):
    return "CommandFailed(%s)" % self.cmd 
  
  def __repr__(self):
    return "CommandFailed(cmd=%s, env=%s, label=%s)" % (" ".join(self.cmd), self.env, self.label)
  
def run_cmd(cmd, env = None, label = ""):
  if config.print_commands: 
    print " ".join(cmd)
  if config.print_command_elapsed_time: 
    t = time.time()
  
  # first compile silently
  # if you encounter an error, then recompile with output printing
  try:
    if config.suppress_compiler_output: 
      with open(os.devnull, "w") as fnull:
        subprocess.check_call(cmd, stdout = fnull, stderr = fnull, env = env)
    else:
      subprocess.check_call(cmd, env = env)
  except:
    raise CommandFailed(cmd, env, label)
    
  if config.print_command_elapsed_time: 
    if label:
      print "%s, elapsed time: %0.4f" % (label, time.time() - t)
    else:
      print "Elapsed time:", time.time() - t 

########NEW FILE########
__FILENAME__ = system_info
import distutils

import numpy.distutils as npdist 
import numpy.distutils.system_info as np_sysinfo 

from ..system_info import windows, mac_os, openmp_available
import config 
  
config_vars = distutils.sysconfig.get_config_vars()

def get_compiler(_cache = {}):
  if config.compiler_path:
    return config.compiler_path
  if config.pure_c in _cache:
    return _cache[config.pure_c]
  for compiler in [('gcc' if config.pure_c else 'g++'), 
                   'icc']:
    path = distutils.spawn.find_executable(compiler)
    if path:
      _cache[config.pure_c] = path
      return path 
  assert False, "No compiler found!"
    

def get_source_extension():
  return ".c" if config.pure_c else ".cpp"

object_extension = ".o"
shared_extension = np_sysinfo.get_shared_lib_extension(True)




python_include_dirs = [distutils.sysconfig.get_python_inc()]
numpy_include_dirs = npdist.misc_util.get_numpy_include_dirs()
include_dirs = numpy_include_dirs + python_include_dirs 

python_lib_dir = distutils.sysconfig.get_python_lib() + "/../../"
python_version = distutils.sysconfig.get_python_version()


########NEW FILE########
__FILENAME__ = type_mappings

from ..ndtypes import (UInt8, UInt16, UInt32, UInt64, 
                       Int8, Int16, Int32, Int64, Float32, Float64, NoneType,  
                       Bool, ArrayT, ClosureT, TupleT, SliceT, NoneT, PtrT)

_dtype_mappings = {
  Bool : "NPY_BOOL",
  Int8 : "NPY_INT8", 
  Int16 : "NPY_INT16",
  Int32 : "NPY_INT32",
  Int64 : "NPY_INT64",
  UInt8 : "NPY_UINT8",
  UInt16 : "NPY_UINT16",
  UInt32 : "NPY_UINT32",
  UInt64 : "NPY_UINT64",
  Float32 : "NPY_FLOAT32",
  Float64 : "NPY_FLOAT64"
}
def to_dtype(t):
  if t in _dtype_mappings:
    return _dtype_mappings[t]
  assert False, "Unsupported element type %s" % t
  
_ctype_mappings = {
  Int8:  "int8_t",
  UInt8:  "uint8_t",
  UInt16:  "uint16_t",
  UInt32:  "uint32_t",
  UInt64:  "uint64_t",
  Int16:  "int16_t",
  Int32:  "int32_t",
  Int64:  "int64_t",
  Float32:  "float",
  Float64:  "double",
  NoneType:  "int",
  Bool:  "int8_t",
}

def to_ctype(t):
  if t in _ctype_mappings:
    return _ctype_mappings[t]
  elif isinstance(t, PtrT):
    return "%s*" % to_ctype(t.elt_type)
  elif isinstance(t, (ArrayT, ClosureT, TupleT, SliceT, NoneT)):
    return "PyObject*"
  else:
    assert False, "Unsupported type %s" % t
    
########NEW FILE########
__FILENAME__ = ast_conversion
import __builtin__
import ast
import inspect
import types
from collections import OrderedDict

import numpy as np
from dsltools import NestedBlocks, ScopedDict
 
from .. import config, names, prims, syntax

from ..names import NameNotFound
from ..ndtypes import Type
from ..prims import Prim 
from ..syntax import (Expr, 
                      Assign, If, ForLoop, Return,  
                      Var, PrimCall, Cast,  Select, 
                      Map, Reduce, IndexMap, 
                      Enumerate, Zip, Len, Range,    
                      Slice,  Tuple, Array,
                      Const, Call, Index, 
                      FormalArgs, ActualArgs, 
                      UntypedFn, Closure, 
                      SourceInfo)
 
from ..syntax.helpers import (none, true, false, one_i64, zero_i64, zero_i24,  
                              is_python_constant, const)
from ..syntax.wrappers import build_untyped_prim_fn, build_untyped_expr_fn, build_untyped_cast_fn


 
from ..transforms import subst_expr, subst_stmt_list

from decorators import jit, macro  
from python_ref import GlobalValueRef,  ClosureCellRef


class UnsupportedSyntax(Exception):
  def __init__(self, node, function_name = None, filename = None):
    self.function_name = function_name 
    self.filename = filename 
    self.node = node
    
  def __str__(self):
    if self.function_name is not None and \
       self.filename is not None and \
       self.node.lineno is not None:
      return "Parakeet doesn't support %s from function '%s' in file %s on line %d" % \
        (self.node.__class__.__name__, self.function_name, self.filename, self.node.lineno)
    elif self.function_name is not None:
      return "Parakeet doesn't support %s in '%s'" % \
        (self.node.__class__.__name__, self.function_name)
    else:
      return "Parakeet doesn't support %s" % self.node.__class__.__name__


  
class ExternalValue(object):
  """
  Wrap up references global values with this class
  """ 
  def __init__(self, python_value):
    self.value = python_value
    
  def __str__(self):
    return "ExternalValue(%s)" % self.value


def mk_reduce_call(fn, positional, init = None):
  init = none if init is None else init
  axis = zero_i64
  from .. import lib 
  return Reduce(fn = translate_function_value(lib.identity), 
                combine = fn, 
                args = positional, 
                axis = axis, 
                init = init)

def mk_simple_fn(mk_body, input_name = "x", fn_name = "cast"):
  unique_arg_name = names.fresh(input_name)
  unique_fn_name = names.fresh(fn_name)
  var = Var(unique_arg_name)
  formals = FormalArgs()
  formals.add_positional(unique_arg_name, input_name)
  body = mk_body(var)
  return UntypedFn(unique_fn_name, formals, body)
  
       
def is_hashable(x):
  try:
    hash(x)
    return True
  except:
    return False 

def is_prim(v):
  return isinstance(v, (Prim)) or (is_hashable(v) and v in prims.prim_lookup_by_value) 

def is_builtin_function(v):
  return isinstance(v, (types.TypeType, types.BuiltinFunctionType))
  
def is_user_function(v):
  return isinstance(v, (types.FunctionType, jit, macro))

def is_function_value(v):
  return is_user_function(v) or is_builtin_function(v) or is_prim(v) 
    
def is_static_value(v):
  return is_python_constant(v) or \
         type(v) is np.dtype  or \
         is_function_value(v)

def value_to_syntax(v):
  if isinstance(v, Expr):
    return v 
  elif is_python_constant(v):
    return const(v)
  else:
    return translate_function_value(v)
    
class AST_Translator(ast.NodeVisitor):
  def __init__(self, 
               globals_dict=None, 
               closure_cell_dict=None,
               parent=None, 
               function_name = None, 
               filename = None):
    # assignments which need to get prepended at the beginning of the
    # function
    self.globals = globals_dict
    self.blocks = NestedBlocks()

    self.parent = parent 
    self.scopes = ScopedDict()
    
    self.globals_dict = globals_dict 
    self.closure_cell_dict = closure_cell_dict 
    
    # mapping from names/paths to either a closure cell reference or a 
    # global value 
    self.python_refs = OrderedDict()
    
    self.original_outer_names = []
    self.localized_outer_names = []
    self.filename = filename 
    self.function_name = function_name 
    self.push()

  def push(self, scope = None, block = None):
    if scope is None:
      scope = {}
    if block is None:
      block = []
    self.scopes.push(scope)
    self.blocks.push(block)


  def pop(self):
    scope = self.scopes.pop()
    block = self.blocks.pop()
    return scope, block
  
  def fresh_name(self, original_name):
    fresh_name = names.fresh(original_name)
    self.scopes[original_name] = fresh_name
    return fresh_name
    
  def fresh_names(self, original_names):
    return map(self.fresh_name, original_names)

  def fresh_var(self, name):
    return Var(self.fresh_name(name))

  def fresh_vars(self, original_names):
    return map(self.fresh_var, original_names)
  
  def current_block(self):
    return self.blocks.top()

  def current_scope(self):
    return self.scopes.top()

  def ast_to_value(self, expr):
    if isinstance(expr, ast.Num):
      return expr.n
    elif isinstance(expr, ast.Tuple):
      return tuple(self.ast_to_value(elt) for elt in expr.elts)
    elif isinstance(expr, ast.Name):
      return self.lookup_global(expr.id) 
    elif isinstance(expr, ast.Attribute):
      left = self.ast_to_value(expr.value)
      if isinstance(left, ExternalValue):
        left = left.value 
      return getattr(left, expr.attr) 

  def lookup_global(self, key):
    if isinstance(key, (list, tuple)):
      assert len(key) == 1
      key = key[0]
    else:
      assert isinstance(key, str), "Invalid global key: %s" % (key,)

    if self.globals:
      if key in self.globals:
        return self.globals[key]
      elif key in __builtin__.__dict__:

        return __builtin__.__dict__[key]
      else:
        assert False, "Couldn't find global name %s" % key
    else:
      assert self.parent is not None
      return self.parent.lookup_global(key)
    
  def is_global(self, key):
    if isinstance(key, (list, tuple)):
      key = key[0]
    if key in self.scopes:
      return False
    elif self.closure_cell_dict and key in self.closure_cell_dict:
      return False
    if self.globals:
      return key in self.globals or key in __builtins__  
    assert self.parent is not None 
    return self.parent.is_global(key)
  
  
  def local_ref_name(self, ref, python_name):
    for (local_name, other_ref) in self.python_refs.iteritems():
      if ref == other_ref:
        return Var(local_name)
    local_name = names.fresh(python_name)
    self.scopes[python_name] = local_name
    self.original_outer_names.append(python_name)
    self.localized_outer_names.append(local_name)
    self.python_refs[local_name] = ref
    return Var(local_name)
  
  def is_visible_name(self, name):
    if name in self.scopes:
      return True 
    if self.parent:
      return self.parent.is_visible_name(name)
    else:
      return self.is_global(name)
      
  def lookup(self, name):
    #if name in reserved_names:
    #  return reserved_names[name]
    if name in self.scopes:
      return Var(self.scopes[name])
    elif self.parent and self.parent.is_visible_name(name):
      # don't actually keep the outer binding name, we just
      # need to check that it's possible and tell the outer scope
      # to register any necessary python refs
      local_name = names.fresh(name)
      self.scopes[name] = local_name
      self.original_outer_names.append(name)
      self.localized_outer_names.append(local_name)
      return Var(local_name)
    elif self.closure_cell_dict and name in self.closure_cell_dict:
      ref = ClosureCellRef(self.closure_cell_dict[name], name)
      return self.local_ref_name(ref, name)
    elif self.is_global(name):
      value = self.lookup_global(name)
      if is_static_value(value):
        return value_to_syntax(value)
      elif isinstance(value, np.ndarray): 
        ref = GlobalValueRef(value)
        return self.local_ref_name(ref, name)
      else:
        # assume that this is a module or object which will have some 
        # statically convertible value pulled out of it 
        return ExternalValue(value)
      #else:
      #  assert False, "Can't use global value %s" % value
    else:
      raise NameNotFound(name)
      
      
  def visit_list(self, nodes):
    return map(self.visit, nodes)


  def tuple_arg_assignments(self, elts, var):
    """
    Recursively decompose a nested tuple argument like
      def f((x,(y,z))):
        ...
    into a single name and a series of assignments:
      def f(tuple_arg):
        x = tuple_arg[0]
        tuple_arg_elt = tuple_arg[1]
        y = tuple_arg_elt[0]
        z = tuple_arg_elt[1]
    """

    assignments = []
    for (i, sub_arg) in enumerate(elts):
      if isinstance(sub_arg, ast.Tuple):
        name = "tuple_arg_elt"
      else:
        assert isinstance(sub_arg, ast.Name)
        name = sub_arg.id
      lhs = self.fresh_var(name)
      stmt = Assign(lhs, Index(var, Const(i)))
      assignments.append(stmt)
      if isinstance(sub_arg, ast.Tuple):
        more_stmts = self.tuple_arg_assignments(sub_arg.elts, lhs)
        assignments.extend(more_stmts)
    return assignments

  def translate_args(self, args):
    assert not args.kwarg
    formals = FormalArgs()
    assignments = []
    for arg in args.args:
      if isinstance(arg, ast.Name):
        visible_name = arg.id
        local_name = self.fresh_name(visible_name)
        formals.add_positional(local_name, visible_name)
      else:
        assert isinstance(arg, ast.Tuple)
        arg_name = self.fresh_name("tuple_arg")
        formals.add_positional(arg_name)
        var = Var(arg_name)
        stmts = self.tuple_arg_assignments(arg.elts, var)
        assignments.extend(stmts)

    n_defaults = len(args.defaults)
    if n_defaults > 0:
      local_names = formals.positional[-n_defaults:]
      for (k,expr) in zip(local_names, args.defaults):
        v = self.ast_to_value(expr)
        
        # for now we're putting literal python 
        # values in the defaults dictionary of
        # a function's formal arguments
        formals.defaults[k] = v 

    if args.vararg:
      assert isinstance(args.vararg, str)
      formals.starargs = self.fresh_name(args.vararg)

    return formals, assignments


  def visit_Name(self, expr):
    assert isinstance(expr, ast.Name), "Expected AST Name object: %s" % expr
    return self.lookup(expr.id)

  def create_phi_nodes(self, left_scope, right_scope, new_names = {}):
    """
    Phi nodes make explicit the possible sources of each variable's values and
    are needed when either two branches merge or when one was optionally taken.
    """
    merge = {}
    for (name, ssa_name) in left_scope.iteritems():
      left = Var(ssa_name)
      if name in right_scope:
        right = Var(right_scope[name])
      else:
        try:
          right = self.lookup(name)
        except NameNotFound:
          continue 
          
      if name in new_names:
        new_name = new_names[name]
      else:
        new_name = self.fresh_name(name)
      merge[new_name] = (left, right)

    for (name, ssa_name) in right_scope.iteritems():
      if name not in left_scope:
        try:
          left = self.lookup(name)
          right = Var(ssa_name)

          if name in new_names:
            new_name = new_names[name]
          else:
            new_name = self.fresh_name(name)
          merge[new_name] = (left, right)
        except names.NameNotFound:
          # for now skip over variables which weren't defined before
          # a control flow split, which means that loop-local variables
          # can't be used after the loop.
          # TODO: Fix this. Maybe with 'undef' nodes?
          pass
    return merge


  def visit_Index(self, expr):
    return self.visit(expr.value)

  def visit_Ellipsis(self, expr):
    raise RuntimeError("Ellipsis operator unsupported")

  def visit_Slice(self, expr):
    """
    x[l:u:s]
    Optional fields
      expr.lower
      expr.upper
      expr.step
    """

    start = self.visit(expr.lower) if expr.lower else none
    stop = self.visit(expr.upper) if expr.upper else none
    step = self.visit(expr.step) if expr.step else none
    return Slice(start, stop, step)

  def visit_ExtSlice(self, expr):
    slice_elts = map(self.visit, expr.dims)
    if len(slice_elts) > 1:
      return Tuple(slice_elts)
    else:
      return slice_elts[0]

  def visit_UnaryOp(self, expr):
    ssa_val = self.visit(expr.operand)
    # UAdd doesn't do anything!
    
    if expr.op.__class__.__name__ == 'UAdd':
      return ssa_val 
    prim = prims.find_ast_op(expr.op)
    return PrimCall(prim, [ssa_val])

  def visit_BinOp(self, expr):
    ssa_left = self.visit(expr.left)
    ssa_right = self.visit(expr.right)
    prim = prims.find_ast_op(expr.op)
    return PrimCall(prim, [ssa_left, ssa_right])

  def visit_BoolOp(self, expr):
    values = map(self.visit, expr.values)
    prim = prims.find_ast_op(expr.op)
    # Python, strangely, allows more than two arguments to
    # Boolean operators
    result = values[0]
    for v in values[1:]:
      result = PrimCall(prim, [result, v])
    return result

  def visit_Compare(self, expr):
    lhs = self.visit(expr.left)
    assert len(expr.ops) == 1
    prim = prims.find_ast_op(expr.ops[0])
    assert len(expr.comparators) == 1
    rhs = self.visit(expr.comparators[0])
    return PrimCall(prim, [lhs, rhs])

  def visit_Subscript(self, expr):
    value = self.visit(expr.value)
    index = self.visit(expr.slice)
    return Index(value, index)

  def generic_visit(self, expr):
    raise UnsupportedSyntax(expr, 
                            function_name = self.function_name, 
                            filename = self.filename)
  
  def visit(self, node):
    res = ast.NodeVisitor.visit(self, node)
    source_info = SourceInfo(filename = self.filename, 
                             line = getattr(node, 'lineno', None),
                             col = getattr(node, 'e.col_offset', None), 
                             function = self.function_name, )
    res.source_info = source_info 
    return res 
    
  def translate_value_call(self, value, positional, keywords_dict= {}, starargs_expr = None):
    if value is sum:
      return mk_reduce_call(build_untyped_prim_fn(prims.add), positional, zero_i24)
    
    elif value is max:
      if len(positional) == 1:
        return mk_reduce_call(build_untyped_prim_fn(prims.maximum), positional)
      else:
        assert len(positional) == 2
        return PrimCall(prims.maximum, positional)
    
    elif value is min:
      if len(positional) == 1:
        return mk_reduce_call(build_untyped_prim_fn(prims.minimum), positional)
      else:
        assert len(positional) == 2
        return PrimCall(prims.minimum, positional)
    
    elif value is map:
      assert len(keywords_dict) == 0
      assert len(positional) > 1
      axis = keywords_dict.get("axis", None)
      return Map(fn = positional[0], args = positional[1:], axis = axis)
    
    elif value is enumerate:
      assert len(positional) == 1, "Wrong number of args for 'enumerate': %s" % positional 
      assert len(keywords_dict) == 0, \
        "Didn't expect keyword arguments for 'enumerate': %s" % keywords_dict
      return Enumerate(positional[0])
    
    elif value is len:
      assert len(positional) == 1, "Wrong number of args for 'len': %s" % positional 
      assert len(keywords_dict) == 0, \
        "Didn't expect keyword arguments for 'len': %s" % keywords_dict
      return self.len(positional[0])
    
    elif value is zip:
      assert len(positional) > 1, "Wrong number of args for 'zip': %s" % positional 
      assert len(keywords_dict) == 0, \
        "Didn't expect keyword arguments for 'zip': %s" % keywords_dict
      return Zip(values = positional)
    
    from ..mappings import function_mappings
    if value in function_mappings:
      value = function_mappings[value]
    
    if isinstance(value, macro):
      return value.transform(positional, keywords_dict)
      
    fn = translate_function_value(value)
    return Call(fn, ActualArgs(positional, keywords_dict, starargs_expr))
    
  def visit_Call(self, expr):
    """
    TODO: 
    The logic here is broken and haphazard, eventually try to handle nested
    scopes correctly, along with globals, cell refs, etc..
    """
    
    fn, args, keywords_list, starargs, kwargs = \
        expr.func, expr.args, expr.keywords, expr.starargs, expr.kwargs
    assert kwargs is None, "Dictionary of keyword args not supported"

    positional = self.visit_list(args)

    keywords_dict = {}
    for kwd in keywords_list:
      keywords_dict[kwd.arg] = self.visit(kwd.value)

    if starargs:
      starargs_expr = self.visit(starargs)
    else:
      starargs_expr = None
    
    def is_attr_chain(expr):
      return isinstance(expr, ast.Name) or \
             (isinstance(expr, ast.Attribute) and is_attr_chain(expr.value))
    def extract_attr_chain(expr):
      if isinstance(expr, ast.Name):
        return [expr.id]
      else:
        base = extract_attr_chain(expr.value)
        base.append(expr.attr)
        return base
    
    def lookup_attr_chain(names):
      value = self.lookup_global(names[0])
    
      for name in names[1:]:
        if hasattr(value, name):
          value = getattr(value, name)
        else:
          try:
            value = value[name]
          except:
            assert False, "Couldn't find global name %s" % ('.'.join(names))
      return value
          
    if is_attr_chain(fn):
      names = extract_attr_chain(fn)
      
      if self.is_global(names):
        return self.translate_value_call(lookup_attr_chain(names), 
                                         positional, keywords_dict, starargs_expr)
    fn_node = self.visit(fn)    
    if isinstance(fn_node, syntax.Expr):
      actuals = ActualArgs(positional, keywords_dict, starargs_expr)
      return Call(fn_node, actuals)
    else:
      assert isinstance(fn_node, ExternalValue)
      return self.translate_value_call(fn_node.value, 
                                       positional, keywords_dict, starargs_expr)

  def visit_List(self, expr):
    return Array(self.visit_list(expr.elts))
    
  def visit_Expr(self, expr):
    # dummy assignment to allow for side effects on RHS
    lhs = self.fresh_var("dummy")
    if isinstance(expr.value, ast.Str):
      return Assign(lhs, zero_i64)
      # return syntax.Comment(expr.value.s.strip().replace('\n', ''))
    else:
      rhs = self.visit(expr.value)
      return syntax.Assign(lhs, rhs)

  def visit_GeneratorExp(self, expr):
    return self.visit_ListComp(expr)
  
  def visit_ListComp(self, expr):
    gens = expr.generators
    assert len(gens) == 1
    gen = gens[0]
    target = gen.target
    if target.__class__ is ast.Name:
      arg_vars = [target]
    else:
      assert target.__class__ is ast.Tuple and \
        all(e.__class__ is ast.Name for e in target.elts),\
        "Expected comprehension target to be variable or tuple of variables, got %s" % \
        ast.dump(target)
      arg_vars = [ast.Tuple(elts = target.elts)]
    # build a lambda as a Python ast representing 
    # what we do to each element 
    
    args = ast.arguments(args = arg_vars, 
                         vararg = None,  
                         kwarg = None,  
                         defaults = ())
    
    fn = translate_function_ast(name = "comprehension_map", 
                                args = args, 
                                body = [ast.Return(expr.elt)], 
                                parent = self)
    seq = self.visit(gen.iter)
    ifs = gen.ifs
    assert len(ifs) == 0, "Parakeet: Conditions in array comprehensions not yet supported"
    # short-circuit conversion from Map(Range) to IndexMap for simple cases
    if seq.__class__ is Range and \
      (seq.start is None or seq.start == zero_i64) and  \
      (seq.step is None or seq.step == one_i64):
      return IndexMap(fn = fn, shape = seq.stop)
    else:
      return Map(fn = fn, args=(seq,), axis = zero_i64)
    

    
  def visit_Attribute(self, expr):
    # TODO:
    # Recursive lookup to see if:
    #  (1) base object is local, if so-- create chain of attributes
    #  (2) base object is global but an adverb primitive-- use it locally
    #      without adding it to nonlocals
    #  (3) not local at all-- in which case, add the whole chain of strings
    #      to nonlocals
    #
    #  AN IDEA:
    #     Allow external values to be brought into the syntax tree as 
    #     a designated ExternalValue node
    #     and then here check if the LHS is an ExternalValue and if so, 
    #     pull out the value. If it's a constant, then make it into syntax, 
    #     if it's a function, then parse it, else raise an error. 
    #
    
    from ..mappings import property_mappings, method_mappings
    value = self.visit(expr.value)
    attr = expr.attr
    if isinstance(value, ExternalValue):
      value = value.value 
      assert hasattr(value, attr), "Couldn't find attribute '%s' in %s" % (attr, value)
      value = getattr(value, attr)
      if is_static_value(value):
        return value_to_syntax(value)
      else:
        return ExternalValue(value) 
    elif attr in property_mappings:
      fn = property_mappings[attr]
      if isinstance(fn, macro):
        return fn.transform( [value] )
      else:
        return Call(translate_function_value(fn),
                    ActualArgs(positional = (value,)))  
    elif attr in method_mappings:
      fn_python = method_mappings[attr]
      fn_syntax = translate_function_value(fn_python)
      return Closure(fn_syntax, args=(value,))
    else:
      assert False, "Attribute %s not supported" % attr 


  def visit_Num(self, expr):
    return Const(expr.n)

  def visit_Tuple(self, expr):
    return syntax.Tuple(self.visit_list(expr.elts))

  def visit_IfExp(self, expr):
    cond = self.visit(expr.test)
    if_true = self.visit(expr.body)
    if_false = self.visit(expr.orelse)
    return Select(cond, if_true, if_false)
    
  def visit_lhs(self, lhs):
    
    if isinstance(lhs, ast.Name):
      return  self.fresh_var(lhs.id)
    elif isinstance(lhs, ast.Tuple):
      return syntax.Tuple( map(self.visit_lhs, lhs.elts))
    else:
      # in case of slicing or attributes
      res = self.visit(lhs)
      return res

  def visit_Assign(self, stmt):  
    # important to evaluate RHS before LHS for statements like 'x = x + 1'
    ssa_rhs = self.visit(stmt.value)
    ssa_lhs = self.visit_lhs(stmt.targets[0])
    return Assign(ssa_lhs, ssa_rhs)
  
  def visit_AugAssign(self, stmt):
    ssa_incr = self.visit(stmt.value)
    ssa_old_value = self.visit(stmt.target)
    ssa_new_value = self.visit_lhs(stmt.target)
    prim = prims.find_ast_op(stmt.op) 
    return Assign(ssa_new_value, PrimCall(prim, [ssa_old_value, ssa_incr]))

  def visit_Return(self, stmt):
    return syntax.Return(self.visit(stmt.value))

  def visit_If(self, stmt):
    cond = self.visit(stmt.test)
    true_scope, true_block  = self.visit_block(stmt.body)
    false_scope, false_block = self.visit_block(stmt.orelse)
    merge = self.create_phi_nodes(true_scope, false_scope)
    return syntax.If(cond, true_block, false_block, merge)

  def visit_loop_body(self, body, *exprs):
    merge = {}
    substitutions = {}
    curr_scope = self.current_scope()
    exprs = [self.visit(expr) for expr in exprs]
    scope_after, body = self.visit_block(body)
    for (k, name_after) in scope_after.iteritems():
      if k in self.scopes:
        name_before = self.scopes[k]
        new_name = names.fresh(k + "_loop")
        merge[new_name] = (Var(name_before), Var(name_after))
        substitutions[name_before]  = new_name
        curr_scope[k] = new_name
    
    exprs = [subst_expr(expr, substitutions) for expr in exprs]
    body = subst_stmt_list(body, substitutions)
    return body, merge, exprs 

  def visit_While(self, stmt):
    assert not stmt.orelse
    body, merge, (cond,) = self.visit_loop_body(stmt.body, stmt.test)
    return syntax.While(cond, body, merge)

  def assign(self, lhs, rhs):
    self.current_block().append(Assign(lhs,rhs))
 
  def assign_to_var(self, rhs, name = None):
    if isinstance(rhs, (Var, Const)):
      return rhs 
    if name is None:
      name = "temp"
    var = self.fresh_var(name)
    self.assign(var, rhs)
    return var 
  
    
  def add(self, x, y, temp = True):
    expr = PrimCall(prims.add, [x,y])
    if temp:
      return self.assign_to_var(expr, "add")
    else:
      return expr 
  
  def sub(self, x, y, temp = True):
    expr = PrimCall(prims.subtract, [x,y])
    if temp:
      return self.assign_to_var(expr, "sub")
    else:
      return expr 
  
  def mul(self, x, y, temp = True):
    expr = PrimCall(prims.multiply, [x,y])
    if temp:
      return self.assign_to_var(expr, "mul")
    else:
      return expr 
  
  def div(self, x, y, temp = True):
    expr = PrimCall(prims.divide, [x,y])
    if temp:
      return self.assign_to_var(expr, "div")
    else:
      return expr 
  
  
  def len(self, x):
    if isinstance(x, Enumerate):
      return self.len(x.value)
    elif isinstance(x, Zip):
      elt_lens = [self.len(v) for v in x.values]
      result = elt_lens[0]
      for n in elt_lens[1:]:
        result = PrimCall(prims.minimum, [result, n])
      return result 
    elif isinstance(x, (Array, Tuple)):
      return Const(len(x.elts))
    
    elif isinstance(x, Range):
      # if it's a range from 0..len(x), then just return len(x)
      if isinstance(x.stop, Len):
        if isinstance(x.start, Const) and x.start.value == 0:
          if isinstance(x.step, Const) and x.stop.value in (1,-1, None):
            return x.stop
    seq_var = self.assign_to_var(x, "len_input")
    return self.assign_to_var(Len(seq_var), "len_result")


  def is_none(self, v):
    return v is None or isinstance(v, Const) and v.value is None 
  
  def for_loop_bindings(self, idx, lhs, rhs):
    if isinstance(rhs, Enumerate):
      array = rhs.value 
      elt = Index(array, idx)
      if isinstance(lhs, Tuple):
        var_names = ", ".join(str(elt) for elt in lhs.elts)
        if len(lhs.elts) < 2:
          raise SyntaxError("Too many values to unpack: 'enumerate' expects 2 but given %s" % var_names)
        elif len(lhs.elts) > 2:
          raise SyntaxError("Need more than 2 values to unpack for LHS of %s" % var_names)
        idx_var, seq_var = lhs.elts
        other_bindings = self.for_loop_bindings(idx, seq_var, array)
        return [Assign(idx_var, idx)] +  other_bindings 
      elif isinstance(lhs, Var):
        seq_var = self.fresh_var("seq_elt")
        other_bindings = self.for_loop_bindings(idx, seq_var, array)
        return [Assign(lhs, Tuple(idx, seq_var))] + other_bindings
      else:
        raise SyntaxError("Unexpected binding in for loop: %s = %s" % (lhs,rhs)) 
      
    elif isinstance(rhs, Zip):
      values_str = ", ".join(str(v) for v in rhs.values)
      if len(rhs.values) < 2:
        raise SyntaxError("'zip' must take at least two arguments, given: %s" % values_str)
      if isinstance(lhs, Tuple):
        if len(lhs.elts) < len(rhs.values):
          raise SyntaxError("Too many values to unpack in %s = %s" % (lhs, rhs))
        elif len(lhs.elts) > len(rhs.values):
          raise SyntaxError("Too few values on LHS of bindings in %s = %s" % (lhs,rhs))
        result = []
        for lhs_var, rhs_value in zip(lhs.elts, rhs.values):
          result.extend(self.for_loop_bindings(idx, lhs_var, rhs_value))
        return result 
      elif isinstance(lhs, Var):
        lhs_vars = [self.fresh_var("elt%d" % i) for i in xrange(len(rhs.values))]
        result = []
        for lhs_var, rhs_value in zip(lhs_vars, rhs.values):
          result.extend(self.for_loop_bindings(idx, lhs_var, rhs_value))
        result.append(Assign(lhs, Tuple(elts=lhs_vars)))
        return result  
      else:
        raise SyntaxError("Unexpected binding in for loop: %s = %s" % (lhs,rhs)) 
      
    elif isinstance(rhs, Range):
      if isinstance(lhs, Tuple):
        raise SyntaxError("Too few values in unpack in for loop binding %s = %s" % (lhs,rhs))
      elif isinstance(lhs, Var):
        start = rhs.start
        if self.is_none(start): 
          start = zero_i64
        step = rhs.step
        if self.is_none(step): 
          step = one_i64 
        return [Assign(lhs, self.add(start, self.mul(idx, step, temp = False), temp= False))]
      else:
        raise SyntaxError("Unexpected binding in for loop: %s = %s" % (lhs,rhs)) 
      
    else:
      return [Assign(lhs, Index(rhs,idx))]
      
      
  def visit_For(self, stmt):
    assert not stmt.orelse 
    var = self.visit_lhs(stmt.target)
    seq = self.visit(stmt.iter)
    body, merge, _ = self.visit_loop_body(stmt.body)

    if isinstance(seq, Range):
      assert isinstance(var, Var), "Expect loop variable to be simple but got '%s'" % var
      return ForLoop(var, seq.start, seq.stop, seq.step, body, merge)
    else:
      idx = self.fresh_var("idx")
      n = self.len(seq)
      bindings = self.for_loop_bindings(idx, var, seq)
      return ForLoop(idx, zero_i64, n, one_i64, bindings + body, merge)
    
  def visit_block(self, stmts):
    self.push()
    curr_block = self.current_block()
    for stmt in stmts:
      parakeet_stmt = self.visit(stmt)
      curr_block.append(parakeet_stmt)
    return self.pop()

  def visit_FunctionDef(self, node):
    """
    Translate a nested function
    """
    fundef = translate_function_ast(node.name, node.args, node.body, parent = self)
    local_var = self.fresh_var(node.name)
    return Assign(local_var, fundef)

  def visit_Lambda(self, node):
    return translate_function_ast("lambda", node.args, [ast.Return(node.body)], parent = self)
    
def translate_function_ast(name, args, body, 
                              globals_dict = None,
                              closure_vars = [], 
                              closure_cells = [],
                              parent = None, 
                              filename = None):
  """
  Helper to launch translation of a python function's AST, and then construct
  an untyped parakeet function from the arguments, refs, and translated body.
  """
  
  assert len(closure_vars) == len(closure_cells)
  closure_cell_dict = dict(zip(closure_vars, closure_cells))
  
  if filename is None and parent is not None:
    filename = parent.filename 
    
  translator = AST_Translator(globals_dict, closure_cell_dict, 
                              parent, function_name = name, filename = filename)

  assert not args.kwarg, "Parakeet doesn't support **kwargs, found in %s%s(%s)" % \
    (filename +":" if filename else "", name, args) 
  ssa_args, assignments = translator.translate_args(args)
  
  doc_string = None
  if len(body) > 0 and isinstance(body[0], ast.Expr):
    if isinstance(body[0].value, ast.Str):
      doc_string = body[0].value.s
      body = body[1:]
     
    
  _, body = translator.visit_block(body)
  body = assignments + body

  ssa_fn_name = names.fresh(name)

  # if function was nested in parakeet, it can have references to its
  # surrounding parakeet scope, which can't be captured with a python ref cell
  original_outer_names = translator.original_outer_names
  localized_outer_names = translator.localized_outer_names
  python_refs = translator.python_refs 
  ssa_args.prepend_nonlocal_args(localized_outer_names)
  if globals_dict:
    assert parent is None
    assert len(original_outer_names) == len(python_refs)
    return UntypedFn(ssa_fn_name, ssa_args, body, python_refs.values(), [])
  else:
    assert parent
    fn = UntypedFn(ssa_fn_name, ssa_args, body, [], original_outer_names, doc_string = doc_string)
    if len(original_outer_names) > 0:
      try: 
        outer_ssa_vars = [parent.lookup(x) for x in original_outer_names]
      except NameNotFound:
        print "Failure while trying to look up non-local names used by function %s" % name 
        raise 
      return syntax.Closure(fn, outer_ssa_vars)
    else:
      return fn

def strip_leading_whitespace(source):
  lines = source.splitlines()
  assert len(lines) > 0
  first_line = lines[0]
  n_removed = len(first_line) - len(first_line.lstrip())
  if n_removed > 0:
    return '\n'.join(line[n_removed:] for line in lines)
  else:
    return source

def translate_function_source(source, 
                              globals_dict, 
                              closure_vars = [],
                              closure_cells = [],
                              filename = None):
  assert len(closure_vars) == len(closure_cells)
  syntax_tree = ast.parse(strip_leading_whitespace(source))

  if isinstance(syntax_tree, (ast.Module, ast.Interactive)):
    assert len(syntax_tree.body) == 1
    syntax_tree = syntax_tree.body[0]
  elif isinstance(syntax_tree, ast.Expression):
    syntax_tree = syntax_tree.body
  
  assert isinstance(syntax_tree, ast.FunctionDef), \
      "Unexpected Python syntax node: %s" % ast.dump(syntax_tree)
  return translate_function_ast(syntax_tree.name, 
                                syntax_tree.args, 
                                syntax_tree.body, 
                                globals_dict, 
                                closure_vars,
                                closure_cells, 
                                filename = filename)

# python value of a user-defined function mapped to its
# untyped representation
_known_python_functions = {}

# keep track of which functions are being translated at this moment 
# to check for recursive calls 
_currently_processing = set([])


def _translate_function_value(fn):
  """
  The core of function translation, should only end up here 
  if the python function's intermediate representation isn't cached
  """
  assert fn not in _currently_processing, \
    "Recursion detected through function value %s" % (fn,)

  if isinstance(fn, Prim):
    fundef = build_untyped_prim_fn(fn)
  elif isinstance(fn, (Type, np.dtype, int, bool, long, float)):
    fundef = build_untyped_cast_fn(fn)
  elif isinstance(fn, type) and Expr in fn.mro():
    fundef = build_untyped_expr_fn(fn)  
  elif isinstance(fn, macro):
    fundef = fn.as_fn()
    
  else:
    # if it's not a macro or some sort of internal expression
    # then we're really dealing with a Python function
    # so get to work pulling apart its AST and translating
    # it into Parakeet IR
    assert type(fn) not in \
      (types.BuiltinFunctionType, types.TypeType, np.ufunc, types.MethodType), \
      "Unsupported function: %s" % (fn,) 
    
    _currently_processing.add(fn) 
    try:
      assert hasattr(fn, 'func_globals'), "Expected function to have globals: %s" % fn
      assert hasattr(fn, 'func_closure'), "Expected function to have closure cells: %s" % fn
      assert hasattr(fn, 'func_code'), "Expected function to have code object: %s" % fn

      source = inspect.getsource(fn)
      filename = inspect.getsourcefile(fn)
    except:
      _currently_processing.remove(fn)
      assert False, "Parakeet couldn't access source of function %s" % fn 
    globals_dict = fn.func_globals

    free_vars = fn.func_code.co_freevars
    closure_cells = fn.func_closure
    if closure_cells is None: 
      closure_cells = ()
    try: 
      fundef = translate_function_source(source,
                                        globals_dict,
                                        free_vars,
                                        closure_cells, 
                                        filename = filename)
    except:
      _currently_processing.remove(fn)
      raise 
    if config.print_untyped_function:
      print "[ast_conversion] Translated %s into untyped function:\n%s" % (fn, repr(fundef))
  
    _currently_processing.remove(fn)              
    _known_python_functions[fn] = fundef
  return fundef 

import threading 
_lock = threading.RLock()

def translate_function_value(fn):
  if fn in _known_python_functions:
    return _known_python_functions[fn]
  
  # if it's already a Parakeet function, just return it 
  if isinstance(fn, UntypedFn):
    return fn 
  
  # short-circuit logic for turning dtypes and Python types into 
  # functions for casting from any value to those types 
  elif isinstance(fn, (np.dtype, int, long, float, bool)): 
    fundef = build_untyped_cast_fn(fn)
      
  else:
    
    # none of the obvious shortcuts worked, 
    # we're going to have to translate this function for real 
    from ..mappings import function_mappings 
    if fn in function_mappings:
      fn = function_mappings[fn]
      
 
    # ...unless we forgot to add it to mappings but some equivalent primitive 
    # got registered 
    if fn in prims.prim_lookup_by_value:
      fn = prims.prim_lookup_by_value[fn]
  
    # if the function has been wrapped with a decorator, unwrap it 
    while isinstance(fn, jit):
      fn = fn.f
    
    # check again if, after unwrapping the function a bunch, it isn't in the cache 
    if fn in _known_python_functions:
      return _known_python_functions[fn]
  
    with _lock:
      fundef = _translate_function_value(fn)
           
  _known_python_functions[fn] = fundef 
  return fundef 

########NEW FILE########
__FILENAME__ = closure_specializations
 
from ..config import print_specialized_function_names
from ..ndtypes.closure_type import _closure_type_cache

def clear_specializations():   
  for clos_t in _closure_type_cache.itervalues():
    clos_t.specializations.clear()


def print_specializations():

  if print_specialized_function_names:
    print
    print "FUNCTION SPECIALIZATIONS"
    count = 0
    for ((untyped,closure_types), clos_t) in sorted(_closure_type_cache.items()):
      specializations = clos_t.specializations.items()
      if len(specializations) > 0:
        name = untyped.name if hasattr(untyped, 'name') else str(untyped)
        print "Closure %s %s" % (name, closure_types)
        for (arg_types, typed_fn) in sorted(specializations):
          print "  -- %s ==> %s" % (arg_types, typed_fn.name)
          count += 1
    print
    print "Total: %d function specializations" % count


import atexit
atexit.register(print_specializations)
########NEW FILE########
__FILENAME__ = decorators

from .. import names 
  
from .. syntax import (Expr, Var, Const, Return, UntypedFn, FormalArgs, DelayUntilTyped,  
                       const, is_python_constant)

from run_function import run_untyped_fn, run_typed_fn, specialize 

class jit(object):
  def __init__(self, f):
    self.f = f
    self.fn = f
    self.untyped = None 

  def __call__(self, *args, **kwargs):
    if '_backend' in kwargs:
      backend_name = kwargs['_backend']
      del kwargs['_backend']
    else:
      backend_name = None
    
    if self.untyped is None:
      import ast_conversion 
      self.untyped = ast_conversion.translate_function_value(self.fn)
    
    typed_fn, linear_args = specialize(self.untyped, args, kwargs)
    return run_typed_fn(typed_fn, linear_args, backend_name)


class macro(object):
  def __init__(self, f, static_names = set([]), call_from_python = None):
    self.f = f
    self.fn = f 
    self.argcount = self.f.func_code.co_argcount 
    self.varnames = self.f.func_code.co_varnames 
    self.n_args = self.f.func_code.co_argcount
    self.defaults = self.f.func_defaults
    
    n_defaults = len(self.defaults) if self.defaults else 0 
    self.defaults_dictionary = {}
    n_pos = self.n_args - n_defaults
    for i, default_name in enumerate(self.varnames[n_pos:n_pos+n_defaults]):
      self.defaults_dictionary[default_name] = self.defaults[i]
    
    self.static_names = static_names
    self.wrappers = {}
    self.call_from_python = call_from_python
    if hasattr(self.f, "__name__"):
      self.name = f.__name__
    elif hasattr(self.f, "name"):
      self.name = self.f.name 
    else:
      self.name = "macro"

  
  _macro_wrapper_cache = {}
  def _create_wrapper(self, n_pos, static_pairs, dynamic_keywords):
    static_pairs = tuple(static_pairs)
    
    key = (n_pos, static_pairs, dynamic_keywords)
    if key in self.wrappers:
      return self.wrappers[key]
    
    args = FormalArgs()
    pos_vars = []
    keyword_vars = {}
    
    for i in xrange(n_pos):
      if i <  self.argcount: 
        raw_name = self.varnames[i]
      else:
        raw_name = "input_%d" % i
      local_name = names.fresh(raw_name)
      args.add_positional(local_name)
      pos_vars.append(Var(local_name))

      
    import ast_conversion
    for visible_name in dynamic_keywords:
      local_name = names.fresh(visible_name)
      args.add_positional(local_name, visible_name)
      keyword_vars[visible_name] = Var(local_name)
      if visible_name in self.defaults_dictionary:
        default_value = self.defaults_dictionary[visible_name]
        parakeet_value = ast_conversion.value_to_syntax(default_value)
        args.defaults[local_name] = parakeet_value

    for (static_name, value) in static_pairs:
      if isinstance(value, Expr):
        assert isinstance(value, Const)
        keyword_vars[static_name] = value
      elif value is not None:
        assert is_python_constant(value), \
            "Unexpected type for static/staged value: %s : %s" % \
            (value, type(value))
        keyword_vars[static_name] = const(value)


    result_expr = self.f(*pos_vars, **keyword_vars)
    body = [Return(result_expr)]
    wrapper_name = "%s_wrapper_%d_%d" % (self.name, n_pos,
                                         len(dynamic_keywords))
    wrapper_name = names.fresh(wrapper_name)
    untyped = UntypedFn(name = wrapper_name, args = args, body = body)
    self.wrappers[key] = untyped
    return untyped 
  
  def as_fn(self):
    n_default = len(self.defaults) if self.defaults else 0 
    n_pos = self.n_args - n_default 
    arg_varnames = self.varnames[:self.argcount]
    keyword_names = arg_varnames[n_pos:]
    wrapper = self._create_wrapper(n_pos, [], keyword_names)
    return wrapper 
    
  def __call__(self, *args, **kwargs):

    if self.call_from_python is not None:
      return self.call_from_python(*args, **kwargs)
    
    if '_backend' in kwargs:
      backend_name = kwargs['_backend']
      del kwargs['_backend']
    else:
      backend_name = None

    n_pos = len(args)
    keywords = kwargs.keys()
    static_pairs = tuple((k,kwargs.get(k)) for k in self.static_names)
   
    dynamic_keywords = tuple(k for k in keywords
                               if k not in self.static_names)

     
    untyped = self._create_wrapper(n_pos, static_pairs, dynamic_keywords)

    dynamic_kwargs = dict( (k, kwargs[k]) for k in dynamic_keywords)
    return run_untyped_fn(untyped, args, dynamic_kwargs, backend = backend_name)
    

  def transform(self, args, kwargs = {}):
    for arg in args:
      assert isinstance(arg, Expr), \
          "Macros can only take syntax nodes as arguments, got %s" % (arg,)
    for (name,arg) in kwargs.iteritems():
      assert isinstance(arg, Expr), \
          "Macros can only take syntax nodes as arguments, got %s = %s" % \
          (name, arg)
    result = self.f(*args, **kwargs)
    assert isinstance(result, Expr), \
        "Expected macro %s to return syntax expression, got %s" % \
        (self.f, result)
    return result

  def __str__(self):
    return "macro(%s)" % self.name

class typed_macro(macro):
  def __init__(self, f, *args, **kwargs):
    macro.__init__(self, f, *args, **kwargs)
    def delayed(*macro_args, **macro_kwargs):
      return DelayUntilTyped(values = macro_args, keywords = macro_kwargs, fn = f)
    self.f = delayed 
    

class staged_macro(object):
  def __init__(self, *static_names, **kwargs):
    self.static_names = tuple(static_names)

    self.call_from_python = kwargs.get('call_from_python')
    assert kwargs.keys() in [[], ['call_from_python']], \
        "Unknown keywords: %s" % kwargs.keys()

  def __call__(self, fn):
    return macro(fn, 
                 self.static_names,
                 call_from_python = self.call_from_python)

axis_macro = staged_macro("axis")
########NEW FILE########
__FILENAME__ = diagnose

from dsltools.testing_helpers import eq
from run_function import specialize 
from ..transforms import Phase, Transform, CloneFunction
from ..transforms.pipeline import loopify

def transform_name(t):
  assert not isinstance(t, Phase)
  if hasattr(t, 'name') and t.name is not None:
    return t.name 
  elif hasattr(t, '__name__'):
    return t.__name__
  else:
    assert hasattr(t, '__class__')
    return t.__class__.__name__ 
  
def linearize_phase(typed_fn, phase):
  if isinstance(phase, (list, tuple)):
    return linearize_phases(typed_fn, phase)
  elif not isinstance(phase, Phase):
    name = transform_name(phase)
    return [ (name, phase) ]
  else:
    group_name = str(phase)
    previous = linearize_phase(typed_fn, phase.depends_on)
    if phase.run_if is not None and not phase.run_if(typed_fn):
      return previous  
    interleave = []
    for (name, t) in linearize_phases(typed_fn, phase.cleanup):
      interleave.append( ("  %s" % ( name, ), t) )
    combined = previous 
    for (name, t) in linearize_phases(typed_fn, phase.transforms):
      new_name = "%s:%s" % (group_name, name)
      combined.append((new_name, t))
      for (cleanup_name, t2) in interleave:
        cleanup_name = cleanup_name + " (after %s)" % new_name
        combined.append( (cleanup_name, t) )
    return combined 
    
def linearize_phases(typed_fn, phases):
  combined = []
  for phase in phases:
    combined.extend(linearize_phase(typed_fn, phase))
  return combined 
         
def get_transform_list(typed_fn, last_phase = loopify):
  return linearize_phase(typed_fn, last_phase)  

def find_broken_transform(fn, inputs, expected, 
                             print_transforms = True, 
                             print_functions = True, 
                             last_phase = loopify):
  from .. import interp 
  

  fn, args = specialize(fn, inputs, optimize=False)  

  try:  
    interp_result = interp.eval(fn, args)
  except:
    print "[Diagnose] Runtime before any optimizations while trying to run", fn
    raise  
    
  
   
  if not eq(interp_result, expected):
    print "[Diagnose] Expected:", expected 
    print "[Diagnose] Result:", interp_result
    print "[Diagnose] This function was busted before we optimized anything!" 
    return None 
  transforms = get_transform_list(fn, last_phase = last_phase)
  print "[Diagnose] Full list of transforms:"
  for (name, t) in transforms:
    print "  -> ", name 
  old_fn = fn 

  for (name, t) in transforms:
    print "[Diagnose] Running %s..." % (name, )
    # in case t is just a class, instantiate it 
    if not isinstance(t, Transform):
      t = t()
    assert isinstance(t, Transform)
    cloner = CloneFunction(parent_transform = t)
    new_fn = t.apply(cloner.apply(old_fn))
    if print_functions:
      print new_fn 
      
    result = interp.eval(new_fn, args)
    if not eq(result, expected):
      print "[Diagnose] Expected %s but got %s " % (expected, result)
      print "[Diagnose] After running ", t
      print "[Diagnose] Old function ", old_fn 
      print "[Diagnose] Transformed function ", new_fn 
      print "[Diagnose] The culprit was:", name 
      return t 
    old_fn = new_fn 
  print "[Diagnose] No problems found!"
  
########NEW FILE########
__FILENAME__ = python_ref
import abc

class Ref(object):
  __meta__ = abc.ABCMeta

  @abc.abstractmethod
  def deref(self):
    pass

class GlobalValueRef(Ref):
  def __init__(self, value):
    self.value = value 
    
  def deref(self):
    return self.value 
  
  def __str__(self):
    return "GlobalValueRef(%s)" % (self.value,)
  
  def __repr__(self):
    return str(self)
  
    def __eq__(self, other):
      return isinstance(other, GlobalValueRef) and self.value is other.value 

class GlobalNameRef(Ref):
  def __init__(self, globals_dict, name):
    self.globals_dict = globals_dict
    self.name = name

  def __str__(self):
    return "GlobalNameRef(%s)" % self.name

  def __repr__(self):
    return str(self)

  def deref(self):
    return self.globals_dict[self.name]

  def __eq__(self, other):
    return isinstance(other, GlobalNameRef) and \
           self.globals_dict is other.globals_dict and \
           self.name == other.name

class ClosureCellRef(Ref):
  def __init__(self, cell, name):
    self.cell = cell
    self.name = name

  def deref(self):
    return self.cell.cell_contents

  def __str__(self):
    return "ClosureCellRef(%s)" % self.name

  def __repr__(self):
    return str(self)

  def __eq__(self, other):
    if isinstance(other, ClosureCellRef):
      try:
        return self.cell == other.cell
      except:
        return self.cell is other.cell
    return False

########NEW FILE########
__FILENAME__ = run_function

from .. import config, type_inference 
from ..ndtypes import type_conv, Type, typeof  
from ..syntax import UntypedFn, ActualArgs
from ..transforms import pipeline

from .. import c_backend
from .. import openmp_backend 

import ast_conversion

# get types of all inputs
def _typeof(arg):
  try:
    return typeof(arg)
  except:
    if hasattr(arg, 'type') and isinstance(arg.type, Type): 
      return arg.type
    else:
      raise 
  
def prepare_args(fn, args, kwargs):
  """
  Fetch the function's nonlocals and return an ActualArgs object of both the arg
  values and their types
  """
  #assert not isinstance(fn, TypedFn), "[prepare_args] Only works for untyped functions"
  if not isinstance(fn, UntypedFn):
    fn = ast_conversion.translate_function_value(fn)   
  nonlocals = tuple(fn.python_nonlocals())
  arg_values = ActualArgs(nonlocals + tuple(args), kwargs)
  arg_types = arg_values.transform(_typeof)
  return arg_values, arg_types
  

def specialize(untyped, args, kwargs = {}, optimize = False):
  """
  Translate, specialize and begin to optimize the given function for the types
  of the supplies arguments.

  Return the untyped and typed representations, along with all the
  arguments in a linear order. 
  """

  if not isinstance(untyped, UntypedFn):
    untyped = ast_conversion.translate_function_value(untyped)
       
  arg_values, arg_types = prepare_args(untyped, args, kwargs)
  
  # convert the awkward mix of positional, named, and starargs 
  # into a positional sequence of arguments
  linear_args = untyped.args.linearize_without_defaults(arg_values)
  
  # propagate types through function representation and all
  # other functions it calls
   
  typed_fn = type_inference.specialize(untyped, arg_types)
  if optimize: 
    from .. transforms.pipeline import normalize 
    # apply high level optimizations 
    typed_fn = normalize.apply(typed_fn)
  return typed_fn, linear_args 

   
def run_typed_fn(fn, args, backend = None):
  actual_types = tuple(type_conv.typeof(arg) for arg in  args)
  expected_types = fn.input_types
  assert actual_types == expected_types, \
    "Arg type mismatch, expected %s but got %s" % \
    (expected_types, actual_types)

      
  if backend is None:
    backend = config.backend
  
  if backend == 'c':
    return c_backend.run(fn, args)
   
  elif backend == 'openmp':
    return openmp_backend.run(fn, args)
  
  elif backend == 'cuda':
    # only selectively import cuda_backend since it required PyCUDA
    from .. import cuda_backend 
    return cuda_backend.run(fn, args)
  
  elif backend == 'llvm':
    from ..llvm_backend.llvm_context import global_context
    from ..llvm_backend import generic_value_to_python 
    from ..llvm_backend import ctypes_to_generic_value, compile_fn 
    lowered_fn = pipeline.lowering.apply(fn)
    llvm_fn = compile_fn(lowered_fn).llvm_fn

    ctypes_inputs = [t.from_python(v) 
                   for (v,t) 
                   in zip(args, expected_types)]
    gv_inputs = [ctypes_to_generic_value(cv, t) 
                 for (cv,t) 
                 in zip(ctypes_inputs, expected_types)]
    exec_engine = global_context.exec_engine
    gv_return = exec_engine.run_function(llvm_fn, gv_inputs)
    return generic_value_to_python(gv_return, fn.return_type)

  elif backend == "interp":
    from .. import interp 
    fn = pipeline.loopify(fn)
    return interp.eval_fn(fn, args)
  
  else:
    assert False, "Unknown backend %s" % backend 

def run_untyped_fn(fn, args, kwargs = None, backend = None):
  assert isinstance(fn, UntypedFn)
  if kwargs is None:
    kwargs = {}
  typed_fn, linear_args = specialize(fn, args, kwargs)
  return run_typed_fn(typed_fn, linear_args, backend)

def run_python_ast(fn_name, fn_args, fn_body, globals_dict, 
                     arg_values, kwarg_values = None, backend = None):
  """
  Instead of giving Parakeet a function to parse, you can construct a Python 
  AST yourself and then have that converted into a Typed Parakeet function
  """
  untyped = ast_conversion.translate_function_ast(fn_name, fn_args, fn_body, globals_dict)
  return run_untyped_fn(untyped, arg_values, kwarg_values, backend)


def run_python_fn(fn, args, kwargs = None, backend = None):
  """
  Given a python function, run it in Parakeet on the supplied args
  """
  # translate from the Python AST to Parakeet's untyped format
  untyped = ast_conversion.translate_function_value(fn)
  return run_untyped_fn(untyped, args, kwargs, backend)
  

########NEW FILE########
__FILENAME__ = typed_repr
from run_function import specialize

def typed_repr(python_fn, args, optimize = True):
  typed_fn, _ = specialize(python_fn, args)
  if optimize:
    from ..transforms import pipeline
    return pipeline.high_level_optimizations.apply(typed_fn)
  else:
    return typed_fn 
  
  

########NEW FILE########
__FILENAME__ = type_conv_decls
import numpy as np
from numpy import  asarray 
import types 


from ..syntax import UntypedFn, TypedFn 
from ..ndtypes import (scalar_types, 
                       make_array_type, ArrayT, 
                       make_tuple_type, TupleT, 
                       make_closure_type, ClosureT, 
                       NoneT, NoneType, TypeValueT)
from ..ndtypes.type_conv import typeof, register   

register(type(None), NoneT, lambda _: NoneType)


def typeof_dtype(dt):
  return TypeValueT(scalar_types.from_dtype(dt))

register([np.dtype], TypeValueT, typeof_dtype) 

def typeof_type(t):
  assert hasattr(t, 'dtype'), "Can only convert numpy types"
  dt = t(0).dtype 
  pt = scalar_types.from_dtype(dt)
  return TypeValueT(pt) 

register(types.TypeType, TypeValueT, typeof_type)

def typeof_tuple(python_tuple):
  return make_tuple_type([typeof(elt_t) for elt_t in python_tuple])

register(types.TupleType, TupleT, typeof_tuple)

def typeof_array(x):
  x = asarray(x)
  elt_t = scalar_types.from_dtype(x.dtype)
  return make_array_type(elt_t, x.ndim)

register(np.ndarray, ArrayT, typeof_array)

from .. import prims 
register((list, xrange), ArrayT, typeof_array)



from decorators  import jit, macro 

 
# ndtypes already register NumPy arrays type converters, 
# but parakeet also treats ranges and lists as arrays 
register((list, xrange), ArrayT, typeof_array)

def typeof_prim(p):
  from ..syntax.wrappers import  build_untyped_prim_fn
  untyped_fn = build_untyped_prim_fn(p)
  return make_closure_type(untyped_fn, ())

register(prims.class_list, ClosureT, typeof_prim)

def typeof_fn(f):
  import ast_conversion
  untyped_fn = ast_conversion.translate_function_value(f)
  closure_args = untyped_fn.python_nonlocals()
  closure_arg_types = map(typeof, closure_args)
  return make_closure_type(untyped_fn, closure_arg_types)


register(types.FunctionType, ClosureT, typeof_fn)
register(jit, ClosureT, typeof_fn)
register(macro, ClosureT, typeof_fn)
register(types.BuiltinFunctionType, ClosureT, typeof_fn)
register(np.ufunc, ClosureT, typeof_fn)
register(UntypedFn, ClosureT, typeof_fn)
register(TypedFn, ClosureT, typeof_fn)

########NEW FILE########
__FILENAME__ = interp
import itertools 
import numpy as np
import types


from frontend import ast_conversion
from ndtypes import ScalarT, StructT,  type_conv     
from syntax import (Expr, Var, Tuple, 
                    UntypedFn, TypedFn, 
                    Return, If, While, ForLoop, ParFor, ExprStmt,   
                    ActualArgs, 
                    Assign, Index, )


class ReturnValue(Exception):
  def __init__(self, value):
    self.value = value

class ClosureVal:
  def __init__(self, fn, fixed_args):
    self.fn = fn
    self.fixed_args = tuple(fixed_args)

  def __call__(self, args):
    if isinstance(args, ActualArgs):
      args = args.prepend_positional(self.fixed_args)
    else:
      args = self.fixed_args + tuple(args)
    return eval_fn(self.fn, args)
  
def rankof(x):
  return x.ndim if hasattr(x, 'ndim') else 0 

def eval(fn, actuals):
  
  result = eval_fn(fn, actuals)
  import ctypes 
  if isinstance(result, ctypes.Structure):
    return type_conv.to_python(result, fn.return_type)
  return result 
   
def eval_fn(fn, actuals):
  
  if isinstance(fn, np.dtype):
    return fn.type(*actuals)
  elif isinstance(fn, TypedFn):
    assert len(fn.arg_names) == len(actuals), \
      "Wrong number of args, expected %s but given %s" % \
      (fn.arg_names, actuals)
    env = {}

    for (k,v) in zip(fn.arg_names, actuals):
      env[k] = v
  elif isinstance(fn, UntypedFn):
    # untyped functions have a more complicated args object
    # which deals with named args, variable arity, etc..
    env = fn.args.bind(actuals)
  elif isinstance(fn, ClosureVal):
    return fn(actuals)
  else:
    return fn(*actuals)
  

  def eval_args(args):
    if isinstance(args, (list, tuple)):
      return map(eval_expr, args)
    else:
      return args.transform(eval_expr)

  def eval_if_expr(maybe_expr):
    return eval_expr(maybe_expr) if isinstance(maybe_expr, Expr) else maybe_expr
  
  def eval_expr(expr):

    if hasattr(expr, 'wrapper'):
      expr = expr.wrapper
    assert isinstance(expr, Expr), "Not an expression-- %s : %s" % \
         (expr, type(expr))
         
    def expr_Const():
      return expr.value

    def _strides(numpy_array):
      strides_bytes = numpy_array.strides
      itemsize = numpy_array.dtype.itemsize
      return tuple(stride / itemsize for stride in strides_bytes)
      
    def expr_Strides():
      value = eval_expr(expr.array)
      return _strides(value)
    
    def expr_Attribute():
      value = eval_expr(expr.value)
      
      if expr.name == 'offset':
        if value.base is None:
          return 0
        else:
          offset_bytes =  value.ctypes.data - value.base.ctypes.data
          return offset_bytes / value.dtype.itemsize 
        
      elif expr.name == 'data':
        return np.ravel(value)
      
      elif expr.name == 'strides':
        return _strides(value)
      
      elif expr.name == 'step':
        step = getattr(value, 'step', 1)
        if step is None: step = 1 
        return step 
      
      elif isinstance(value, tuple):
        if expr.name.startswith('elt'):
          field = int(expr.name[3:])
        else:
          field = int(expr.name)
        return value[field]
        
      else:
        assert hasattr(value, expr.name), "Missing attribute %s from value %s" % (expr.name, value)
        return getattr(value, expr.name)

    def expr_Alloc():
      count = eval_expr(expr.count)
      arr = np.empty(shape = (count,), dtype = expr.elt_type.dtype)
      return arr
      
    def expr_AllocArray():

      shape = eval_expr(expr.shape)

      assert isinstance(shape, tuple), "Expected tuple, got %s" % (shape,)
      assert isinstance(expr.elt_type, ScalarT), \
          "Expected scalar element type for AllocArray, got %s" % (expr.elt_type,)
      dtype = expr.elt_type.dtype
      return  np.ndarray(shape = shape, dtype = dtype) 
    
    
    def expr_ArrayView():

      data = eval_expr(expr.data)
      
      shape  = eval_expr(expr.shape)
      strides = eval_expr(expr.strides)
      offset = eval_expr(expr.offset)
      dtype = expr.type.elt_type.dtype
      bytes_per_elt = dtype.itemsize
      byte_offset = offset * bytes_per_elt
      byte_strides = tuple(si * bytes_per_elt for si in  strides)
      if False:
        print expr 
        print "data", data
        print "shape(data)", np.shape(data)
      
        print "shape",  shape 
        print "strides", strides
        print "offset", offset 
        print "itemsize", bytes_per_elt
        print "byte strides", byte_strides 
        print "byte offset", byte_offset 
        
      if isinstance(data, np.ndarray):
        data = data.data 
 
      return np.ndarray(shape = shape, 
                        offset = byte_offset,  
                        buffer = data, 
                        strides = byte_strides, 
                        dtype = np.dtype(dtype))
      
      
    def expr_Array():
      elt_values = map(eval_expr, expr.elts)
      return np.array(elt_values)

    def expr_ConstArray():
      shape = eval_expr(expr.shape)
      value = eval_expr(expr.value)
      return np.ones(shape, dtype = expr.value.type.dtype) * value 
    
    def expr_ConstArrayLike():
      array = eval_expr(expr.array)
      value = eval_expr(expr.value)
      return np.ones_like(array, dtype = expr.value.type.elt_type.dtype) * value 
    
    def expr_TypeValue():
      t = expr.type_value  
      assert isinstance(t, ScalarT), \
        "Parakeet only supports scalar types as values, not %s" % expr 
      return t.dtype 
    
    def expr_Shape():
      return np.shape(eval_expr(expr.array))
    
    def expr_Reshape():
      array = eval_expr(expr.array)
      shape = eval_expr(expr.shape)
      return array.reshape(shape)
    
    def expr_Index():
      array = eval_expr(expr.value)
      index = eval_expr(expr.index)
      return array[index]

    def expr_PrimCall():
      arg_values = eval_args(expr.args)
      result = expr.prim.fn (*arg_values)
      return result 
    
    def expr_Slice():
      return slice(eval_expr(expr.start), eval_expr(expr.stop),
                   eval_expr(expr.step))

    def expr_Var():
      return env[expr.name]

    def expr_Call():
      fn = eval_expr(expr.fn)
      arg_values = eval_args(expr.args)
      return eval_fn(fn, arg_values)

    def expr_Closure():
      if isinstance(expr.fn, (UntypedFn, TypedFn)):
        fundef = expr.fn
      else:
        assert isinstance(expr.fn, str)
        fundef = UntypedFn.registry[expr.fn]
      closure_arg_vals = map(eval_expr, expr.args)
      return ClosureVal(fundef, closure_arg_vals)

    def expr_Fn():
      return ClosureVal(expr, [])

    def expr_TypedFn():
      return ClosureVal(expr, [])

    def expr_Cast():
      x = eval_expr(expr.value)
      t = expr.type
      assert isinstance(t, ScalarT)
      # use numpy's conversion function
      return t.dtype.type(x)
    
    def expr_Select():
      cond = eval_expr(expr.cond)
      trueval = eval_expr(expr.true_value)
      falseval = eval_expr(expr.false_value)
      return trueval if cond else falseval 

    def expr_Struct():
      assert expr.type, "Expected type on %s!" % expr
      assert isinstance(expr.type, StructT), \
          "Expected %s : %s to be a struct" % (expr, expr.type)
      elts = map(eval_expr, expr.args)
      return expr.type.ctypes_repr(*elts)

    def expr_Tuple():
      return tuple(map(eval_expr, expr.elts))
    
    def expr_TupleProj():
      return eval_expr(expr.tuple)[expr.index]

    def expr_ClosureElt():
      assert isinstance(expr.closure, Expr), \
          "Invalid closure expression-- %s : %s" % \
          (expr.closure, type(expr.closure))
      clos = eval_expr(expr.closure)
      return clos.fixed_args[expr.index]

    def expr_Range():
      return np.arange(eval_expr(expr.start), eval_expr(expr.stop), eval_expr(expr.step))
    
    def expr_Len():
      return len(eval_expr(expr.value))
    
    def normalize_axes(axis, args):
      if isinstance(axis, Expr):
        axis = eval_expr(axis)
        
      if isinstance(axis, tuple):
        axes = axis 
      else:
        axes = (axis,) * len(args)
      
      assert len(axes) == len(args)
    
    def expr_Map():
      fn = eval_expr(expr.fn)
      args = [eval_expr(arg) for arg in expr.args]
      axes = normalize_axes(expr.axis, args)
      
      largest_rank = 0
      largest_arg = None
      iter_space = None
      for arg, axis  in zip(args, axes):
        rank = rankof(arg) 
        if rank > largest_rank and (axis is None or axis < rank):
          largest_rank = rank 
          largest_arg = arg
          if axis is None: 
            iter_space = largest_arg.shape 
          else:
            iter_space = [largest_arg.shape[axis]]
          
      if largest_rank == 0:
        return eval_fn(fn, args)

      results = []
      for idx in np.ndindex(iter_space):
        elt_args = []
        for arg, axis in zip(args, axes):
          r = rankof(arg)
          if r == 0:
            elt_args.append(arg)
          if axis is None:
            curr_indices = idx[-(largest_rank - r):]
            elt_args.append(arg[tuple(curr_indices)])
          elif r > axis:
            indices = [slice(None) if j != axis else idx[0] for j in xrange(rankof(arg)) ]
            elt_args.append(arg[tuple(indices)])
          else:
            elt_args.append(arg)
        results.append(eval_fn(fn, elt_args))
      return np.array(results)
    
    def expr_Reduce():
      fn = eval_expr(expr.fn)
      combine = eval_expr(expr.combine)
      init = eval_expr(expr.init) if expr.init else None 
      args = [eval_expr(arg) for arg in expr.args]
      if isinstance(expr.axis, (int, long)):
        axis = expr.axis
      else:
        axis = eval_expr(expr.axis)
        
      if axis is None:
        args = [np.ravel(arg) for arg in args]
        axis = 0 
      
      largest_rank = 0
      largest_arg = None
      for arg in args:
        rank = rankof(arg) 
        if rank > largest_rank:
          largest_rank = rank 
          largest_arg = arg
          
      if largest_rank == 0:
        acc = eval_fn(fn, args)
        if init is None: return acc 
        else: return eval_fn(combine, [init, acc])
      
      niters = largest_arg.shape[axis]

      if init is not None: acc = init 
      else: acc = None 
        
      for i in xrange(niters):
        elt_args = []
        for arg in args:
          r = rankof(arg)
          if r > 0:
            indices = [slice(None) if j != axis else i for j in xrange(rankof(arg)) ]
            elt_args.append(arg[tuple(indices)])
          else:
            elt_args.append(arg)
        elt_result = eval_fn(fn, elt_args)
        if acc is not None:
          acc = eval_fn(combine, [acc, elt_result])
        else:
          acc = elt_result
      return acc
    
    def expr_Scan():
      assert False, "Scan not implemented"
    
    def expr_IndexMap():
      fn = eval_expr(expr.fn)
      shape = eval_expr(expr.shape)
      dtype = expr.type.elt_type.dtype
      result = np.empty(shape, dtype = dtype)
      for idx in np.ndindex(shape):
        result[idx] = eval_fn(fn, (idx,))
      return result
      
    def expr_IndexReduce():
      fn = eval_expr(expr.fn)
      combine = eval_expr(expr.combine)
      shape = eval_expr(expr.shape)
      if not isinstance(shape, (list, tuple) ):
        shape = [shape]
      ranges = [xrange(n) for n in shape]
      
      acc = eval_if_expr(expr.init)
      for idx in itertools.product(*ranges):
        if len(idx) == 1:
          idx = idx[0]
        elt = eval_fn(fn, (idx,))
        if acc is None:
          acc = elt 
        else:
          elt = eval_fn(combine, (acc, elt))
      return elt 
    
    fn_name = "expr_" + expr.__class__.__name__
    dispatch_fn = locals()[fn_name]
    result = dispatch_fn()
    
    # we don't support python function's inside parakeet,
    # they have to be translated into Parakeet functions
    if isinstance(result, types.FunctionType):
      fundef = ast_conversion.translate_function_value(result)
      return ClosureVal(fundef, fundef.python_nonlocals())
    else:
      return result

  def eval_merge_left(phi_nodes):
    for result, (left, _) in phi_nodes.iteritems():
      env[result] = eval_expr(left)

  def eval_merge_right(phi_nodes):
    for result, (_, right) in phi_nodes.iteritems():
      env[result] = eval_expr(right)

  def assign(lhs, rhs, env):
    if isinstance(lhs, Var):
      env[lhs.name] = rhs
    elif isinstance(lhs, Tuple):
      assert isinstance(rhs, tuple)
      for (elt, v) in zip(lhs.elts, rhs):
        assign(elt, v, env)
    elif isinstance(lhs, Index):
      arr = eval_expr(lhs.value)
      idx = eval_expr(lhs.index)

      arr[idx] = rhs

  

  def eval_parfor_seq(fn, bounds):
    if isinstance(bounds, (list,tuple)) and len(bounds) == 1:
      bounds = bounds[0]
        
    if isinstance(bounds, (int, long)):
      for idx in xrange(bounds):
        eval_fn(fn, (idx,))
    else:
      for idx in np.ndindex(bounds):
        eval_fn(fn, (idx,))
  
    
    
  def eval_stmt(stmt):

    if isinstance(stmt, Return):
      v = eval_expr(stmt.value)
      raise ReturnValue(v)
    
    elif isinstance(stmt, Assign):
      value = eval_expr(stmt.rhs)
      assign(stmt.lhs, value, env)

    elif isinstance(stmt, If):
      cond_val = eval_expr(stmt.cond)

      if cond_val:
        eval_block(stmt.true)
        
        eval_merge_left(stmt.merge)
      else:
        eval_block(stmt.false)
        eval_merge_right(stmt.merge)
        

    elif isinstance(stmt, While):
      eval_merge_left(stmt.merge)
      while eval_expr(stmt.cond):
        eval_block(stmt.body)
        eval_merge_right(stmt.merge)
        
    elif isinstance(stmt, ForLoop):
      start = eval_expr(stmt.start)
      stop = eval_expr(stmt.stop)
      step = eval_expr(stmt.step)
       
      eval_merge_left(stmt.merge)

      for i in xrange(start, stop, step):
        env[stmt.var.name] = i
        eval_block(stmt.body)
        eval_merge_right(stmt.merge)
      
        
    elif isinstance(stmt, ExprStmt):
      eval_expr(stmt.value)
      
    elif isinstance(stmt, ParFor):
      fn = eval_expr(stmt.fn)
      bounds = eval_expr(stmt.bounds)
    
      eval_parfor_seq(fn, bounds)    
      
      
    else:
      raise RuntimeError("Statement not implemented: %s" % stmt)

  def eval_block(stmts):

    for stmt in stmts:
      eval_stmt(stmt)

  try:
   
    eval_block(fn.body)

  except ReturnValue as r:
    return r.value 
  except:
    raise

def run_python_fn(python_fn, args, kwds):
  untyped  = ast_conversion.translate_function_value(python_fn)
  # should eventually roll this up into something cleaner, since
  # top-level functions are really acting like closures over their
  # global dependencies
  global_args = [python_fn.func_globals[n] for n in untyped.nonlocals]
  all_positional = global_args + list(args)
  actuals = args.FormalArgs(all_positional, kwds)
  return eval_fn(untyped, actuals)

########NEW FILE########
__FILENAME__ = adverbs


from .. syntax import (none, zero_i64, 
                       Map, Reduce, Scan, 
                       IndexMap, IndexReduce, 
                       ParFor, OuterMap,
                       Filter, FilterReduce) 

from .. frontend import macro, staged_macro, jit,  translate_function_value 
from lib_helpers import _get_shape 
@jit 
def identity(x):
  return x

@macro 
def parfor(fn, bounds):
  fn = translate_function_value(fn)
  bounds = _get_shape(bounds)
  return ParFor(fn = fn, bounds = bounds)

@staged_macro("axis")
def map(f, *args, **kwds):
  if 'axis' in kwds:
    axis = kwds['axis']
    del kwds['axis']
  else:
    axis = zero_i64
  assert len(kwds) == 0, "map got unexpected keywords %s" % (kwds.keys())
  return Map(fn = f, args = args, axis = axis)
each = map

@staged_macro("axis")
def allpairs(f, x, y, axis = 0):
  return OuterMap(fn = f, args = (x,y), axis = axis)

@staged_macro("axis")
def outer_map(f, *args, **kwds):
  if 'axes' in kwds:
    assert 'axis' not in kwds, "Can't have both 'axis' and 'axes' as keywords"
    axis = kwds['axes']
    del kwds['axes'] 
    
  elif 'axis' in kwds:
    axis = kwds['axis']
    del kwds['axis']
  else:
    axis = zero_i64
  assert len(kwds) == 0, "outer_map got unexpected keywords %s" % (kwds.keys())
  return OuterMap(fn = f, args = args, axis = axis)

@staged_macro("axis")
def reduce(f, *args, **kwds):
  if 'axis' in kwds:
    axis = kwds['axis']
    del kwds['axis']
  else: 
    axis = none
  if 'init' in kwds:
    init = kwds['init']
    del kwds['init']
  else:
    init = none 
  assert len(kwds) == 0, "reduce got unexpected keywords %s" % (kwds.keys())
  
  ident = translate_function_value(identity)
  return Reduce(fn = ident, 
                combine = f, 
                args = args,
                init = init,
                axis = axis)

@staged_macro("axis")
def scan(f, *args, **kwds):
  axis = kwds.get('axis', zero_i64)
  init = kwds.get('init', none)
  
  ident = translate_function_value(identity)
  return Scan(fn = ident,  
                     combine = f,
                     emit = ident, 
                     args = args,
                     init = init,
                     axis = axis)
@macro
def imap(fn, shape):
  return IndexMap(shape = shape, fn = fn)


@macro
def ireduce(fn, combine, shape, init = None):
  return IndexReduce(fn = fn, combine=combine, shape = shape, init = init)

@staged_macro("axis")
def filter(pred, *args, **kwds):
  if 'axis' in kwds:
    axis = kwds['axis']
    del kwds['axis']
  else:
    axis = zero_i64
  assert len(kwds) == 0, "filter got unexpected keywords %s" % (kwds.keys())
  return Filter(fn = pred, args = args, axis = axis)


@staged_macro("axis")
def filter_reduce(f, pred, *args, **kwds):
  if 'axis' in kwds:
    axis = kwds['axis']
    del kwds['axis']
  else: 
    axis = none
  if 'init' in kwds:
    init = kwds['init']
    del kwds['init']
  else:
    init = none 
  assert len(kwds) == 0, "reduce got unexpected keywords %s" % (kwds.keys())
  
  ident = translate_function_value(identity)
  return FilterReduce(fn = ident, 
                      pred = pred, 
                      combine = f, 
                      args = args,
                      init = init,
                      axis = axis)

########NEW FILE########
__FILENAME__ = array_constructors
import __builtin__
import numpy as np 

from .. frontend.decorators import jit, macro, typed_macro 
from .. ndtypes import ( make_array_type,  combine_type_list, repeat_tuple,  
                         ArrayT, TupleT,  Int64, ScalarT) 
from .. syntax import (Range, Cast, AllocArray, TupleProj, Array, 
                       ConstArray, ConstArrayLike, Index, Shape)

from ..syntax.helpers import get_types, one_i64, zero_i64, make_tuple

from adverbs import imap 
from numpy_types import float64
from lib_helpers import _get_type, _get_shape, _get_tuple_elts


@macro
def arange(n, *xs, **kwds):
  if 'dtype' in kwds:
    elt_t = _get_type(kwds['dtype'])
    del kwds['dtype']
  else:
    elt_t = Int64
    
  assert len(kwds) == 0, "Unexpected keyword arguments to 'arange': %s" % kwds
  array_t = make_array_type(elt_t, 1) 
  count = __builtin__.len(xs)
  assert 0 <= count <= 2, "Too many args for range: %s" % ((n,) + tuple(xs))
 
  if count == 0:
    start = zero_i64 
    stop = n 
    step = one_i64 
  elif count == 1:
    start = n 
    stop = xs[0]
    step = one_i64 
  else:
    start = n 
    stop = xs[0]
    step = xs[1]
    
  if elt_t != Int64:
    start = Cast(start, type = elt_t)
    stop = Cast(stop, type = elt_t)
    step = Cast(step, type = elt_t)
  return Range(start, stop, step, type = array_t)
 
@typed_macro
def empty(shape, dtype = float64):
  elt_t = _get_type(dtype)
  assert isinstance(elt_t, ScalarT), "Array element type %s must be scalar" % (elt_t,)
  shape = _get_shape(shape)  
  rank = len(shape.type.elt_types)
  arr_t = make_array_type(elt_t, rank)
  return AllocArray(shape = shape, elt_type = elt_t, type = arr_t)

@jit 
def empty_like(x, dtype = None):
  if dtype is None:
    return empty(x.shape, x.dtype)
  else:
    return empty(x.shape, dtype)
  
@typed_macro   
def zeros(shape, dtype = float64):
  shape = _get_shape(shape)
  elt_type = _get_type(dtype)
  zero = Cast(zero_i64, type = elt_type)
  ndims = len(shape.type.elt_types)
  if ndims == 0:
    return zero 
  else:
    t = make_array_type(elt_type, ndims)
    return ConstArray(shape = shape, value = zero, type = t)
  
@jit
def zeros_like(x, dtype = None):
  if dtype is None:
    dtype = x.dtype
  return zeros(x.shape, dtype)

@typed_macro
def ones(shape, dtype = float64):
  shape = _get_shape(shape)
  elt_type = _get_type(dtype)
  one = Cast(one_i64, type = elt_type)
  ndims = len(shape.type.elt_types)
  if ndims == 0:
    return one 
  else:
    t = make_array_type(elt_type, ndims)
    return ConstArray(shape = shape, value = one, type = t)

@jit  
def ones_like(x, dtype = None):

  if dtype is None:
    dtype = x.dtype
  return ones(x.shape, dtype)

@jit
def copy(x):
  return [xi for xi in x]

@jit 
def linspace(start, stop, num = 50, endpoint = True):
  """
  Copied from numpy.linspace but dropped the 'retstep' option 
  which allows you to optionall return a tuple (that messes with type inference)
  """
  num = int(num)
  if num <= 0:
    return np.array([])
  elif endpoint:
    if num == 1:
      return np.array([float(start)])
    step = (stop-start)/float((num-1))
    y = np.arange(0, num) * step + start
    y[-1] = stop
    return y
  else:
    step = (stop-start)/float(num)
    return np.arange(0, num) * step + start 



@typed_macro
def array(value, dtype = None):
  if dtype is not None:
    expected_elt_type = _get_type(dtype)
    print "Got dtype argument to array: %s, ignoring for now" % expected_elt_type

  if isinstance(value.type, ArrayT):
    return value 
  else:
    assert isinstance(value.type, TupleT), "Don't know how to make array from %s : %s" % (value, value.type)
    elt_types = value.type.elt_types
    assert all(isinstance(t, ScalarT) for t in elt_types), \
      "Can only make array from tuple of scalars, not %s : %s" % (value, value.type)
    elt_t = combine_type_list(value.type.elt_types)
    array_elts = _get_tuple_elts(value, cast_type = elt_t)
    array_t = make_array_type(elt_t, 1)
    return Array(elts = array_elts, type = array_t)

@typed_macro
def tile(A, reps):
  reps = _get_shape(reps)
  reps_dims = len(reps.type.elt_types)
  if reps_dims == 0:
    return A 
  A_rank = A.type.rank if isinstance(A.type, ArrayT) else 0 
  if A_rank == 0:
    # array scalars, ugh!
    if isinstance(A.type, ArrayT):
      A = Index(A, make_tuple(()), type = A.elt_type)
    assert isinstance(A.type, ScalarT), "First argument to 'tile' must be array or scalar"
    array_t = make_array_type(A.type, reps_dims)
    return ConstArray(value = A, shape = reps, type = array_t)
  else:
    A_shape = Shape(A, type = repeat_tuple(Int64, A_rank))
    A_shape_elts = _get_tuple_elts(A_shape)
    reps_elts = _get_tuple_elts(reps)
    result_shape_elts = []
    assert False, "np.tile not yet implemented"
    #for i in xrange(max(len(A_shape_elts), len(reps_elts))):
      
    #result = AllocArray()
    
  
########NEW FILE########
__FILENAME__ = array_properties


from .. import ndtypes 
from .. ndtypes import ArrayT, Int64, elt_type, empty_tuple_t, TupleT, TypeValueT 
from .. frontend import macro, jit, typed_macro 
from .. syntax import (Attribute, Tuple, Ravel, Shape, Reshape, TypeValue, Transpose, Const)
from .. syntax.helpers import const_int, zero_i64 


@macro
def transpose(x):
  return Transpose(x)

@macro 
def ravel(x):
  return Ravel(x)
  
@macro 
def reshape(x):
  return Reshape(x)

@typed_macro 
def get_elt_type(x):
  elt_t = ndtypes.elt_type(x.type)
  return TypeValue(elt_t, type = TypeValueT(elt_t))
  
@typed_macro
def itemsize(xt):
  return const_int(elt_type(xt.type).nbytes)
  
@typed_macro 
def rank(xt):
  return const_int(xt.type.rank) 

@typed_macro 
def size(xt, axis = None):
  if axis is None: 
    axis = zero_i64
  assert isinstance(axis, Const), "Axis argument to 'size' must be a constant, given %s" % axis
  assert axis.value == 0, "Calling 'size' along axes other than 0 not yet supported, given %s" % axis
  if isinstance(xt.type, ArrayT):
    return Attribute(xt, 'size', type = Int64)
  elif isinstance(xt.type, TupleT):
    return const_int(len(xt.type.elt_types))
  else:
    return const_int(1)


@macro  
def shape(x):
  return Shape(x)


########NEW FILE########
__FILENAME__ = builtins
from .. import prims 

from .. frontend import jit,  typed_macro 
from .. ndtypes import make_tuple_type, TupleT, ArrayT
from ..syntax import Tuple, Array 

@jit 
def builtin_or(x, y):
  return x or y

@jit 
def builtin_and(x, y):
  return x and y

@typed_macro 
def builtin_tuple(xt):
  if isinstance(xt.type, TupleT):
    return xt 
  else:
    assert isinstance(xt.type, ArrayT), "Can't create type from %s" % (xt.type,)
    assert isinstance(xt, Array), "Can only create tuple from array of const length"
    elt_types = [e.type for e in xt.elts]
    tuple_t = make_tuple_type(elt_types)
    return Tuple(xt.elts, type = tuple_t)
  

########NEW FILE########
__FILENAME__ = lib_helpers
import numpy as np

from ..frontend import macro, jit 
from ..syntax import (UntypedFn, Return, Cast, TypedFn, TypeValue, Array, Tuple, Expr, Closure, 
                      TupleProj) 
from ..syntax.helpers import get_types, make_tuple  
from ..ndtypes import (type_conv, TypeValueT, ArrayT, IntT, 
                       make_tuple_type, TupleT, ScalarT, ClosureT, FnT, Type)


def _get_type(dtype):
  """
  Defensively try to extract a scalar type from any wacky thing
  that might get passed in as a 'dtype' to an array constructor
  """
  if isinstance(dtype, macro):
    dtype = dtype.as_fn()
  
  while isinstance(dtype, jit):
    dtype = dtype.f 
  
  while isinstance(dtype, Expr):
    if isinstance(dtype, UntypedFn):
      if len(dtype.body) == 1:
        stmt = dtype.body[0]
        if stmt.__class__ is Return:
          expr = stmt.value 
          if expr.__class__ is Cast:
            dtype = expr.type 
            break
      assert False, "Don't know how to convert function %s into Parakeet type" % dtype
    elif isinstance(dtype, TypedFn):
      dtype = dtype.return_type
    if isinstance(dtype, TypeValue):
      dtype = dtype.type_value  
    elif isinstance(dtype.type, TypeValueT):
      dtype = dtype.type
    elif isinstance(dtype, Closure):
      dtype = dtype.fn
    elif isinstance(dtype.type, (ClosureT, FnT)):
      dtype = dtype.type.fn
    else:
      assert False, "Don't know how to turn %s : %s into Parakeet type" % (dtype, dtype.type)  


  if isinstance(dtype, Type):
    if isinstance(dtype, TypeValueT):
      return dtype.type
    else:
      return dtype 
    
  elif isinstance(dtype, (np.dtype, type)):

    return type_conv.equiv_type(dtype)
  elif isinstance(dtype, str):
    return type_conv.equiv_type(np.dtype(dtype))
  assert False, "Don't know how to turn %s into Parakeet type" % dtype  

def _get_shape(value):
  """
  User might pass a scalar, a tuple of integer values, or a literal array of integers
  as the 'shape' argument of array constructor. Normalize those all into a tuple
  """
  if isinstance(value.type, ArrayT):
    assert value.__class__ is Array, "Don't know how to convert %s into tuple" % value
    elts = value.elts 
    
    elt_types = get_types(elts)
    assert all(isinstance(t, IntT) for t in elt_types), \
      "Shape elements must be integers, not %s" % elt_types
    return Tuple(elts = elts, type = make_tuple_type(elt_types))
  elif isinstance(value.type, TupleT):
    assert all(isinstance(t, ScalarT) for t in value.type.elt_types), \
      "Shape tuple %s : %s has non-scalar elements" % (value, value.type)
    return value
  elif isinstance(value.type, ScalarT):
    assert isinstance(value.type, IntT), \
      "Can't make shape tuple from non-integer scalar %s : %s" % (value, value.type)
    return make_tuple((value,))
  assert False, "Can't make shape tuple from value %s : %s" % (value, value.type)
  
def _get_tuple_elts(expr, cast_type = None):
  elts = []
  for i, tuple_elt_t in enumerate(expr.type.elt_types):
    if expr.__class__ is Tuple:
      tuple_elt = expr.elts[i]
    else:
      tuple_elt = TupleProj(expr, i, type = tuple_elt_t)
    if cast_type is not None and tuple_elt_t != cast_type:
      tuple_elt = Cast(tuple_elt, type = cast_type)
    elts.append(tuple_elt)
  return tuple(elts)  
    
########NEW FILE########
__FILENAME__ = linalg

import numpy as np 

from .. import prims 
from ..frontend import typed_macro 
from ..ndtypes import ScalarT, ArrayT, make_array_type
from ..syntax import PrimCall, Call,  OuterMap, Map
from ..syntax.helpers import make_closure 

def _get_vdot_fn(a, b):
  from reductions  import vdot  
  from ..frontend import ast_conversion 
  vdot_untyped = ast_conversion.translate_function_value(vdot)
  from ..type_inference import specialize
  vec_type_a = make_array_type(a.type.elt_type, 1) 
  vec_type_b = make_array_type(b.type.elt_type, 1)
  result_scalar_type = a.type.elt_type.combine(b.type.elt_type)
 
  vdot_typed = specialize(vdot_untyped, [vec_type_a, vec_type_b] )

  assert vdot_typed.return_type == result_scalar_type, \
    "Expected return type %s but got %s from vdot" % (result_scalar_type, vdot_typed.return_type)
  return vdot_typed 

@typed_macro
def dot(a,b):
  if isinstance(a.type, ScalarT):
    return PrimCall(prims.multiply, [a, b], type = b.type.combine(a.type))
  elif isinstance(b.type, ScalarT):
    return PrimCall(prims.multiply, [a, b], type = a.type.combine(b.type))
  
  assert isinstance(a.type, ArrayT), "Expected %s to be array but got %s" % (a, a.type)
  assert isinstance(a.type, ArrayT), "Expected %s to be array but got %s" % (b, b.type)
      
      
  if a.type.rank == 1 and b.type.rank == 1:
    vdot = _get_vdot_fn(a,b)
    return Call(fn = vdot, args = [a, b], type = vdot.return_type)

  elif a.type.rank == 1:
    vdot = _get_vdot_fn(a,b)
    
    assert b.type.rank == 2, "Don't know how to multiply %s and %s" % (a.type, b.type)
    vdot_col = make_closure(vdot, (a,))
    result_vec_type = make_array_type(vdot.return_type, 1)
    return Map(fn = vdot_col, args = (b,), axis = 1, type = result_vec_type)
        
  elif b.type.rank == 1:
    assert a.type.rank == 2, "Don't know how to multiply %s and %s" % (a.type, b.type)
    vdot = _get_vdot_fn(b,a)
    vdot_row = make_closure(vdot, (b,))
    result_vec_type = make_array_type(vdot.return_type, 1)
    return Map(fn = vdot_row, args = (a,), axis = 0, type = result_vec_type)
  else:  
    assert a.type.rank == 2 and b.type.rank == 2, \
        "Don't know how to multiply %s and %s" % (a.type, b.type)
    vdot = _get_vdot_fn(a,b)
    result_matrix_type = make_array_type(vdot.return_type, 2)
    return OuterMap(fn = vdot, args = (a, b), axis = (0,1), 
                    type = result_matrix_type)
      
@typed_macro 
def norm(x, ord=None):
  if ord is None:
    ord = 2
  if isinstance(x.type, ScalarT):
    return x
  assert isinstance(x.type, ArrayT), "Argument to 'norm' must be array, got %s : %s" % (x, x.type)
  assert x.type.rank == 1, "Norm currently only supported for vectors, not for %s : %s" (x, x.type)
  if ord == 0:
    return (x != 0).sum()
  elif ord == 1:
    return abs(x).sum()
  elif ord == 2:
    # not using 'conj' so won't yet work for complex numbers 
    return np.sqrt((x**2).sum())
  else:
    return (abs(x)**ord).sum() ** (1.0 / ord)

  

########NEW FILE########
__FILENAME__ = math
from ..frontend import jit 

import numpy as np 



@jit
def conjugate(x):
  """
  For now we don't have complex numbers so this is just the identity function
  """
  return x


@jit
def real(x):
  """
  For now we don't have complex types, so real is just the identity function
  """
  return x  

def _scalar_sign(x):
  if x > 0:
    return 1
  elif x < 0:
    return -1 
  else:
    return 0
  
@jit
def sign(x):
  return map(_scalar_sign, x)

@jit 
def reciprocal(x):
  return 1 / x

@jit 
def rad2deg(rad):
  return rad * 180 / 3.141592653589793

@jit
def deg2rad(deg):
  return deg * 3.141592653589793 / 180 

@jit 
def hypot(x,y):
  return np.sqrt(x**2 + y**2)

@jit 
def square(x):
  return x * x 

def _logaddexp_scalar(x, y):
  """
  Copied from BioPython (http://biopython.org/)
  """
  if x < y:
    bigger = x 
    smaller = y 
  else:
    bigger = x 
    smaller = y 
    
  diff = smaller - bigger    
  if diff < -100: 
    return bigger 

  return bigger + np.log1p(np.exp(diff))
  

@jit 
def logaddexp(x, y):
  return map(_logaddexp_scalar, x, y)

@jit   
def log2_1p(x):
  return (1.0 / np.log(2)) * np.log1p(x)


@jit 
def logaddexp2(x, y):
  diff = x - y 
  return np.where(diff > 0,  x + log2_1p(2 ** -diff) , y + log2_1p(2 ** diff))

 
@jit 
def true_divide(x, y):
  """
  Not exactly true divide, since I guess it's sometimes supposed to stay an int
  """
  return (x + 0.0) / (y + 0.0)

@jit 
def floor_divide(x, y):
  return np.floor(x / y)  

########NEW FILE########
__FILENAME__ = numpy_misc

from .. import prims 
from ..frontend import jit, macro, typed_macro 
from ..ndtypes import ScalarT, ArrayT, make_array_type
from ..syntax import Select, DelayUntilTyped, PrimCall, Call,  OuterMap



 
@macro
def _select(cond, trueval, falseval):
  return Select(cond, trueval, falseval)

#@macro  
def where(cond, trueval, falseval):
  #return Select(cond, trueval, falseval )
  return map(_select, cond, trueval, falseval)


@jit 
def diff(x):
  """
  TODO:
    - axis selection
    - preserve size by filling with zeros
    - allow n'th differences by recursion
  """
  return x[1:] - x[:-1]


@jit 
def fill(x, v):
  for i in range(len(x)):
    x[i] = v 
 

########NEW FILE########
__FILENAME__ = numpy_types

from .. ndtypes import (Int8, Int16, Int32, Int64, 
                        UInt8, UInt16, UInt32, UInt64,
                        Float32, Float64, Bool) 

from .. frontend import macro, jit 
from .. syntax import Cast 

@macro 
def int8(x):
  return Cast(x, type = Int8) 

@macro 
def int16(x):
  return Cast(x, type = Int16) 

@macro 
def int32(x):
  return Cast(x, type = Int32) 

@macro 
def int64(x):
  return Cast(x, type = Int64) 


@macro 
def uint8(x):
  return Cast(x, type = UInt8) 

@macro 
def uint16(x):
  return Cast(x, type = UInt16) 

@macro 
def uint32(x):
  return Cast(x, type = UInt32) 

@macro 
def uint64(x):
  return Cast(x, type = UInt64)

uint = uint64 

@macro 
def float32(x):
  return Cast(x, type = Float32)

@macro 
def float64(x):
  return Cast(x, type = Float64)

@macro 
def bool(x):
  return Cast(x, type = Bool)

########NEW FILE########
__FILENAME__ = patchmap
import numpy as np 
from .. frontend import jit 
from adverbs import imap 

@jit
def pmap1(f, x, w = 3):
  n = x.shape[0]
  def local_apply(i):
    lower = __builtins__.max(i-w/2, 0)
    upper = __builtins__.min(i+w/2+1, n)
    elts = x[lower:upper]
    return f(elts)
  return imap(local_apply, n)

@jit  
def pmap2(f, x, width = (3,3)):
  """
  Patch-map where the function can accept both interior windows
  and smaller border windows
  """
  width_x, width_y = width 
  n_rows, n_cols = x.shape
  hx = width_x / 2
  hy = width_y / 2
  
  def local_apply((i,j)):
    lx = __builtins__.max(i-hx, 0)
    ux = __builtins__.min(i+hx+1, n_rows)
    ly = __builtins__.max(j-hy, 0)
    uy = __builtins__.min(j+hy+1, n_cols)
    return f(x[lx:ux, ly:uy])
    
  return imap(local_apply, x.shape)
    
@jit  
def pmap2_trim(f, x, width = (3,3), step = (1,1)):
  """
  Patch-map over interior windows, ignoring the border
  """
  width_x, width_y = width 
  step_x, step_y = step 
  n_rows, n_cols = x.shape
  hx = width_x / 2
  hy = width_y / 2
  return [[f(x[i-hx:i+hx+1, j-hy:j+hy+1]) 
           for j in np.arange(hx, n_cols-hx, step_x)] 
           for i in np.arange(hy, n_rows-hy, step_y)]




########NEW FILE########
__FILENAME__ = prob
from ..frontend import jit
import numpy as np

def CND(x):
  """
  Simpler approximation of the cumulative normal density. 
  """
  a1 = 0.31938153
  a2 = -0.356563782
  a3 = 1.781477937
  a4 = -1.821255978
  a5 = 1.330274429
  L = abs(x)
  K = 1.0 / (1.0 + 0.2316419 * L)
  w = 1.0 - 1.0/np.sqrt(2*3.141592653589793)* np.exp(-1*L*L/2.) * (a1*K +
      a2*K*K + a3*K*K*K + a4*K*K*K*K + a5*K*K*K*K*K)
  if x<0:
    w = 1.0-w
  return w

def erf(x):
  return 2 * CND(x * 1.4142135623730951) - 1

def erfc(x):
  return 2 * (1 - CND(x * 1.4142135623730951)) 

"""
P = np.asarray([
  2.46196981473530512524E-10,
  5.64189564831068821977E-1,
  7.46321056442269912687E0,
  4.86371970985681366614E1,
  1.96520832956077098242E2,
  5.26445194995477358631E2,
  9.34528527171957607540E2,
  1.02755188689515710272E3,
  5.57535335369399327526E2
])

Q = np.asarray([
  1.32281951154744992508E1,
  8.67072140885989742329E1,
  3.54937778887819891062E2,
  9.75708501743205489753E2,
  1.82390916687909736289E3,
  2.24633760818710981792E3,
  1.65666309194161350182E3,
  5.57535340817727675546E2
])

R = np.asarray([
  5.64189583547755073984E-1,
  1.27536670759978104416E0,
  5.01905042251180477414E0,
  6.16021097993053585195E0,
  7.40974269950448939160E0,
  2.97886665372100240670E0
])

S = np.asarray([
  2.26052863220117276590E0,
  9.39603524938001434673E0,
  1.20489539808096656605E1,
  1.70814450747565897222E1,
  9.60896809063285878198E0,
  3.36907645100081516050E0
])

T = np.asarray([
  9.60497373987051638749E0,
  9.00260197203842689217E1,
  2.23200534594684319226E3,
  7.00332514112805075473E3,
  5.55923013010394962768E4
])

U = np.asarray([
  3.35617141647503099647E1,
  5.21357949780152679795E2,
  4.59432382970980127987E3,
  2.26290000613890934246E4,
  4.92673942608635921086E4
])

MAXLOG = 7.09782712893383996732E2


@jit
def polevl(x, coef):
  ans = coef[0]
  for i in range(len(coef) - 1, 1, -1):
    ans = ans * x + coef[i]
  return ans


@jit
def p1evl(x, coef):
  ans = x + coef[0]
  for i in range(len(coef) - 1, 1, -1):
    ans = ans * x + coef[i]
  return ans

@jit
def ndtr(a):
  x = a * np.sqrt(2)
  z = np.abs(x)

  if z < np.sqrt(2):
    y = 0.5 + 0.5 * erf(x)
  else:
    y = 0.5 * erfc(z)

  if x > 0:
    y = 1.0 - y
  return y



def erfc(a):
  if a < 0.0:
    x = -a
  else:
    x = a

  if x < 1.0:
    return 1.0 - erf(a)

  z = -a * a
  if z < -MAXLOG:
    if a < 0:
      return 2.0
    else:
      return 0.0

  z = np.exp(z)

  if x < 8.0:
    p = polevl(x, P)
    q = p1evl(x, Q)
  else:
    p = polevl(x, R)
    q = p1evl(x, S)

  y = (z * p) / q

  if a < 0:
    y = 2.0 - y

  return y


def erf(x):
  if np.abs(x) > 1.0:
    return 1.0 - erfc(x)
  z = x * x

  y = x * polevl(z, T, 4) / p1evl(z, U, 5)
  return (y)
"""
########NEW FILE########
__FILENAME__ = random
import numpy as np 

from ..frontend.decorators import jit  

@jit 
def shuffle(x):
  n = len(x)
  r = np.random.randint(0, n, n)
  for i in xrange(n):
    j = np.fmod(r[i], i)
    old_xj = x[j]
    x[j] = x[i]
    x[i] = old_xj
    
@jit
def permutation(x):
  y = np.zeros_like(x)
  return shuffle(y)

  
########NEW FILE########
__FILENAME__ = reductions

from ..frontend import jit, axis_macro 
from .. import prims 
from adverbs import scan 

from .. frontend import translate_function_value

from .. syntax import Reduce, Const 
from ..syntax.helpers import none, false, true, one_i32, zero_i32, zero_i24
 
from adverbs import reduce
from builtins import builtin_and, builtin_or
from parakeet.syntax.delay_until_typed import DelayUntilTyped

def _identity(x):
  return x 

def mk_reduce(combiner, x, init, axis):
  return Reduce(fn = translate_function_value(_identity), 
                combine = translate_function_value(combiner), 
                args = (x,),  
                init = init, 
                axis = axis)

@axis_macro
def reduce_min(x, axis = None):
  return mk_reduce(prims.minimum, x, init = None, axis = axis)


@axis_macro
def reduce_max(x, axis = None):
  return mk_reduce(prims.maximum, x, init = None, axis = axis)


@axis_macro 
def reduce_any(x, axis=None):
  return mk_reduce(builtin_or, x, init  = false, axis = axis, )

@axis_macro
def reduce_all(x, axis = None):
  return mk_reduce(builtin_and, x, init = true, axis = axis)

@axis_macro 
def reduce_sum(x, axis = None):
  return mk_reduce(prims.add, x, init = zero_i24, axis = axis)
  
@jit 
def builtin_min(x, y = None):
  if y is None:
    return reduce_min(x)
  else:
    return prims.minimum(x,y)

@jit
def builtin_max(x, y = None):
  if y is None:
    return reduce_max(x)
  else:
    return prims.maximum(x,y)


@jit 
def prod(x, axis=None):
  return reduce(prims.multiply, x, init=true, axis = axis)

@jit 
def mean(x, axis = None):
  return sum(x, axis = axis) / x.shape[0]

@jit 
def cumsum(x, axis = None):
  return scan(prims.add, x, axis = axis)

@jit 
def cumprod(x, axis = None):
  return scan(prims.multiply, x, axis = axis)

@jit 
def vdot(x,y):
  """
  Inner product between two 1Dvectors
  """
  return sum(x*y)

@jit
def argmax(x):
  """
  Currently assumes axis=None
  TODO: 
    - Support axis arguments
    - use IndexReduce instead of explicit loop
  
      def argmax_map(curr_idx):
        return curr_idx, x[curr_idx]
  
      def argmax_combine((i1,v1), (i2,v2)):
        if v1 > v2:
          return (i1,v1)
        else:
          return (i2,v2)
    
      return ireduce(combine=argmin_combine, shape=x.shape, map_fn=argmin_map, init = (0,x[0]))
  """
  bestval = x[0]
  bestidx = 0
  for i in xrange(1, len(x)):
    currval = x[i]
    if currval > bestval:
      bestval = currval
      bestidx = i
  return bestidx 

@jit
def argmin(x):
  """
  Currently assumes axis=None
  TODO: 
    - Support axis arguments
    - use IndexReduce instead of explicit loop
  
      def argmin_map(curr_idx):
        return curr_idx, x[curr_idx]
  
      def argmin_combine((i1,v1), (i2,v2)):
        if v1 < v2:
          return (i1,v1)
        else:
          return (i2,v2)
    
      return ireduce(combine=argmin_combine, shape=x.shape, map_fn=argmin_map, init = (0,x[0]))
  """
  bestval = x[0]
  bestidx = 0
  for i in xrange(1, len(x)):
    currval = x[i]
    if currval < bestval:
      bestval = currval
      bestidx = i
  return bestidx 



########NEW FILE########
__FILENAME__ = compiler

import llvm.core as llc 
from llvm.core import Builder, ATTR_NO_CAPTURE #, Module 
from llvm.core import Type as lltype

from .. import config, prims, syntax 
from .. analysis import may_escape
from .. ndtypes import BoolT, FloatT, SignedT, UnsignedT, ScalarT, NoneT, Float64
from .. ndtypes import Int32, Int64, PtrT, Bool 
from .. syntax import Var, Struct, Index, TypedFn, Attribute 

import llvm_config 
import llvm_context
import llvm_convert
import llvm_types
import llvm_prims
from llvm_helpers import const, int32, zero 
from llvm_types import llvm_value_type, llvm_ref_type
from parakeet.llvm_backend.llvm_convert import to_bit, from_bit


_escape_analysis_cache = {}
class Compiler(object):
  def __init__(self, fundef, llvm_cxt = llvm_context.global_context):
    self.parakeet_fundef = fundef
    if config.opt_stack_allocation:
      self.may_escape = may_escape(fundef)
    else:
      self.may_escape = None
    self.llvm_context = llvm_cxt
    self.vars = {}
    self.initialized = set([])
    # Initializes the variables dictionary and returns a builder object
    llvm_input_types = map(llvm_ref_type, fundef.input_types)
    llvm_output_type = llvm_ref_type(fundef.return_type)
    llvm_fn_t = lltype.function(llvm_output_type, llvm_input_types)

    self.llvm_fn = self.llvm_context.module.add_function(llvm_fn_t, fundef.name)

    for arg in self.llvm_fn.args:
      if not llvm_types.is_scalar(arg.type):
        arg.add_attribute(ATTR_NO_CAPTURE)

    self.llvm_fn.does_not_throw = True

    self.entry_block, self.entry_builder = self.new_block("entry")
    self._init_vars(self.parakeet_fundef, self.entry_builder)

  def new_block(self, name):
    bb = self.llvm_fn.append_basic_block(name)
    builder = Builder.new(bb)
    return bb, builder

  def _init_vars(self, fundef, builder):
    """
    Create a mapping from variable names to stack locations, these will later be
    converted to SSA variables by the mem2reg pass.
    """
    n_expected = len(fundef.arg_names)
    n_compiled = len(self.llvm_fn.args)
    assert n_compiled == n_expected, \
        "Expected %d args (%s) but compiled code had %d args (%s)" % \
        (n_expected, fundef.arg_names, n_compiled, self.llvm_fn.args)

    for (name, t) in fundef.type_env.iteritems():
      if not name.startswith("$"):
        llvm_t = llvm_ref_type(t)
        stack_val = builder.alloca(llvm_t, name)
        self.vars[name] = stack_val

    for llvm_arg, name in zip(self.llvm_fn.args, fundef.arg_names):
      self.initialized.add(name)
      llvm_arg.name = name
      if name in self.vars:
        builder.store(llvm_arg, self.vars[name])

  def attribute_lookup(self, struct, name, builder):
    """
    Helper for getting the address of an attribute lookup, used both when
    setting and getting attributes
    """
    llvm_struct = self.compile_expr(struct, builder)
    struct_t = struct.type
    field_pos = struct_t.field_pos(name)
    field_type = struct_t.field_type(name)
    indices = [int32(0), int32(field_pos)]
    ptr_name = "%s_ptr" % name
    ptr = builder.gep(llvm_struct, indices, ptr_name)
    return ptr, field_type

  def compile_Var(self, expr, builder):
    name = expr.name
    assert name in self.initialized, "%s uninitialized" % name
    ref = self.vars[expr.name]
    val = builder.load(ref, expr.name + "_val")
    return val

  def compile_Const(self, expr, builder):
    t = expr.type

    if isinstance(t, NoneT):
      return const(0, Int64)
    else:
      assert isinstance(expr.type, ScalarT), \
          "Expected scalar constant but got %s" % expr.type
    return const(expr.value, expr.type)

  def compile_Cast(self, expr, builder):
    llvm_value = self.compile_expr(expr.value, builder)
    return llvm_convert.convert(llvm_value, expr.value.type, expr.type, builder)

  def compile_Struct(self, expr, builder, local = False):
    struct_t = expr.type
    llvm_struct_t = llvm_value_type(struct_t)
    name = expr.type.node_type()
    if local:
      struct_ptr = builder.alloca(llvm_struct_t, name + "_local_ptr")
    else:
      struct_ptr = builder.malloc(llvm_struct_t, name + "_ptr")

    for (i, elt) in enumerate(expr.args):
      field_name, field_type = struct_t._fields_[i]
      assert elt.type == field_type, \
          "Mismatch between expected type %s and given %s for field '%s' " % \
          (field_type, elt.type, field_name)
      elt_ptr = builder.gep(struct_ptr, [int32(0), int32(i)], "field%d_ptr" % i)
      llvm_elt = self.compile_expr(elt, builder)
      builder.store(llvm_elt, elt_ptr)

    return struct_ptr

  def compile_Alloc(self, expr, builder):
    elt_t = expr.elt_type
    llvm_elt_t = llvm_types.llvm_value_type(elt_t)
    n_elts = self.compile_expr(expr.count, builder)
    return builder.malloc_array(llvm_elt_t, n_elts, "data_ptr")

  def compile_Index(self, expr, builder):
    llvm_arr = self.compile_expr(expr.value, builder)
    llvm_index = self.compile_expr(expr.index, builder)
    pointer = builder.gep(llvm_arr, [llvm_index], "elt_pointer")
    elt = builder.load(pointer, "elt", align = 16) #, invariant = True)
    return elt
  
  def compile_Select(self, expr, builder):
    cond = self.compile_expr(expr.cond, builder)
    cond = llvm_convert.to_bit(cond, builder)
    trueval = self.compile_expr(expr.true_value, builder)
    falseval = self.compile_expr(expr.false_value, builder)
    result =  builder.select(cond, trueval, falseval, "select_result")
    return result 
  
  def compile_Attribute(self, expr, builder):
    field_ptr, _ = \
        self.attribute_lookup(expr.value, expr.name, builder)
    field_value = builder.load(field_ptr, "%s_value" % expr.name)
    return field_value

  def compile_TypedFn(self, expr, builder):
    (target_fn, _, _) = compile_fn(expr)
    return target_fn

  def compile_Call(self, expr, builder):
    assert expr.fn.__class__ is TypedFn
    typed_fundef = expr.fn
    (target_fn, _, _) = compile_fn(typed_fundef)
    arg_types = syntax.get_types(expr.args)
    llvm_args = [self.compile_expr(arg, builder) for arg in expr.args]
    assert len(arg_types) == len(llvm_args)
    return builder.call(target_fn, llvm_args, 'call_result')

  def cmp(self, prim, t, llvm_x, llvm_y, builder, result_name = None):
    if result_name is None:
      result_name = prim.name + "_result"

    if isinstance(t, FloatT):
      cmp_op = llvm_prims.float_comparisons[prim]
      return builder.fcmp(cmp_op, llvm_x, llvm_y, result_name)
    elif isinstance(t, SignedT):
      cmp_op = llvm_prims.signed_int_comparisons[prim]
      return builder.icmp(cmp_op, llvm_x, llvm_y, result_name)
    else:
      assert isinstance(t, (BoolT, UnsignedT)), "Unexpected type for comparison %s: %s" % (prim, t)
      cmp_op = llvm_prims.unsigned_int_comparisons[prim]
      return builder.icmp(cmp_op, llvm_x, llvm_y, result_name)

  
  def lt(self, t, llvm_x, llvm_y, builder, result_name = None ):
    return self.cmp(prims.less, t, llvm_x, llvm_y, builder, result_name)
  
  def lte(self, t, llvm_x, llvm_y, builder, result_name = None ):
    return self.cmp(prims.less_equal, t, llvm_x, llvm_y, builder, result_name)
  
  def neq(self, t, llvm_x, llvm_y, builder, result_name = None):
    return self.cmp(prims.not_equal, t, llvm_x, llvm_y, builder, result_name)
  
  def sub(self, t, x, y, builder, result_name = "sub"):
    if isinstance(t, FloatT):
      return builder.fsub(x, y, result_name)
    else:
      return builder.sub(x, y, result_name)
    
  
  def add(self, t, x, y, builder, result_name = "add"):
    if isinstance(t, FloatT):
      return builder.fadd(x, y, result_name)
    else:
      return builder.add(x, y, result_name)
  
  
  def mul(self, t, x, y, builder, result_name = "mul"):
    if isinstance(t, FloatT):
      return builder.fmul(x, y, result_name)
    else:
      return builder.mul(x, y, result_name)
  
    
  def neg(self, x, builder):
    if isinstance(x.type, llc.IntegerType):
      return builder.neg(x, "neg")
    else:
      return builder.fsub(zero(x.type), x, "neg") 
    
  def prim(self, prim, t, llvm_args, builder, result_name = None):
    if result_name is None:
      result_name = prim.name + "_result"

    if isinstance(prim, prims.Cmp):
      bit = self.cmp(prim, t, llvm_args[0], llvm_args[1], builder)
      return llvm_convert.to_bool(bit,builder)
    
    elif prim == prims.maximum:
      x, y = llvm_args
      bit = self.cmp(prims.greater_equal, t, x, y, builder)
      return builder.select(bit, x, y)
    
    elif prim == prims.minimum:
      x,y = llvm_args
      bit = self.cmp(prims.less_equal, t, x, y, builder)
      return builder.select(bit, x, y)
    
    elif prim == prims.negative:
      if t == Bool: 
        bit = llvm_convert.to_bit(llvm_args[0], builder)
        negated = builder.not_(bit)
        return llvm_convert.to_bool(negated, builder)
      return self.neg(llvm_args[0], builder)
    
    # python's remainder is weird in that it preserve's the sign of 
    # the second argument, whereas LLVM's srem/frem operators preserve
    # the sign of the first 
    elif prim == prims.mod:
      x,y = llvm_args 
      if isinstance(t, (UnsignedT, BoolT)):
        return builder.urem(llvm_args[0], llvm_args[1], "modulo")
      elif isinstance(t, SignedT): 
        rem = builder.srem(x,y, "modulo")
      else:
        assert isinstance(t, FloatT)
        rem = builder.frem(llvm_args[0], llvm_args[1], "modulo")

      y_is_negative = self.lt(t, y, zero(y.type), builder, "second_arg_negative")
      rem_is_negative = self.lt(t, rem, zero(rem.type), builder, "rem_is_negative")
      y_nonzero = self.neq(t, y, zero(y.type), builder, "second_arg_nonzero")
      rem_nonzero = self.neq(t, rem, zero(x.type), builder, "rem_nonzero")
      neither_zero = builder.and_(y_nonzero, rem_nonzero, "neither_zero")
      diff_signs = builder.xor(y_is_negative, rem_is_negative, "different_signs")
      should_flip = builder.and_(neither_zero, diff_signs, "should_flip") 
      flipped_rem = self.add(t, y, rem, builder, "flipped_rem")
      return builder.select(should_flip, flipped_rem, rem)

    elif prim == prims.power:
      x,y = llvm_args
       
      if isinstance(t, FloatT):
        new_t = t 
      else:
        new_t = Float64
      x = llvm_convert.convert(x, t, new_t, builder)
      y = llvm_convert.convert(y, t, new_t, builder)
      llvm_op = llvm_prims.get_float_op(prim, new_t)
      result = builder.call(llvm_op, [x,y])
      return llvm_convert.convert(result, new_t, t, builder)
        
    elif isinstance(prim, prims.Arith) or isinstance(prim, prims.Bitwise):
      if isinstance(t, FloatT):
        instr = llvm_prims.float_binops[prim]
      elif isinstance(t, SignedT):
        instr = llvm_prims.signed_binops[prim]
      elif isinstance(t, UnsignedT):
        instr = llvm_prims.unsigned_binops[prim]
      else:
        assert isinstance(t, BoolT)
        instr = llvm_prims.bool_binops[prim]
      op = getattr(builder, instr)
      return op(name = result_name, *llvm_args)

    elif isinstance(prim, prims.Logical):
      if prim == prims.logical_and:
        result = builder.and_(name = result_name, 
                            lhs = to_bit(llvm_args[0], builder), 
                            rhs = to_bit(llvm_args[1], builder))
        return from_bit(result, t, builder)
      elif prim == prims.logical_not:
        result = builder.not_(to_bit(llvm_args[0], builder), name = result_name)
        return from_bit(result, t, builder)
      else:
        assert prim == prims.logical_or
        result = builder.or_(lhs = to_bit(llvm_args[0], builder), 
                           rhs = to_bit(llvm_args[1], builder), name = result_name)
        return from_bit(result, t, builder)
      
    elif prim == prims.abs:
      x = llvm_args[0]
      bit = self.cmp(prims.greater_equal, t,  x, zero(x.type), builder, "gt_zero")
      neg_value = self.neg(x, builder)
      return builder.select(bit, x, neg_value)
    elif isinstance(prim, prims.Float): 
      llvm_op = llvm_prims.get_float_op(prim, t)
      return builder.call(llvm_op, llvm_args)
    
    else:
      assert False, "UNSUPPORTED PRIMITIVE: %s" % prim

  def compile_PrimCall(self, expr, builder):
    args = expr.args
    # type specialization should have made types of arguments uniform,
    # so we only need to check the type of the first arg
    t = args[0].type
    llvm_args = [self.compile_expr(arg, builder) for arg in args]
    return self.prim(expr.prim, t, llvm_args, builder)

  def compile_expr(self, expr, builder):
    method_name = "compile_" + expr.node_type()
    return getattr(self, method_name)(expr, builder)

  def compile_Assign(self, stmt, builder):
    rhs_t = stmt.rhs.type
    # special case for locally allocated structs
    if self.may_escape is not None and \
       stmt.lhs.__class__ is Var and \
       stmt.rhs.__class__ is Struct and \
       stmt.lhs.name  not in self.may_escape:
      value = self.compile_Struct(stmt.rhs, builder, local = True)
    else:
      value = self.compile_expr(stmt.rhs, builder)
    if stmt.lhs.__class__ is Var:
      name = stmt.lhs.name
      lhs_t = stmt.lhs.type
      self.initialized.add(name)
      ref = self.vars[name]
    elif stmt.lhs.__class__ is Index:
      ptr_t = stmt.lhs.value.type
      assert isinstance(ptr_t, PtrT), \
          "Expected pointer, got %s" % ptr_t
      lhs_t = ptr_t.elt_type
      base_ptr = self.compile_expr(stmt.lhs.value, builder)
      index = self.compile_expr(stmt.lhs.index, builder)
      index = llvm_convert.from_signed(index, Int32, builder)
      ref = builder.gep(base_ptr, [index], "elt_ptr")
    else:
      assert stmt.lhs.__class__ is Attribute, \
          "Unexpected LHS: %s" % stmt.lhs
      struct = stmt.lhs.value
      ref, lhs_t = self.attribute_lookup(struct, stmt.lhs.name, builder)

    assert lhs_t == rhs_t, \
        "Type mismatch between LHS %s and RHS %s" % (lhs_t, rhs_t)

    builder.store(value, ref)
    return builder, False

  def compile_ExprStmt(self, stmt, builder):
    self.compile_expr(stmt.value, builder)
    return builder, False

  def compile_Return(self, stmt, builder):
    ret_val = self.compile_expr(stmt.value, builder)
    builder.ret(ret_val)
    return builder, True

  def compile_merge_left(self, phi_nodes, builder):
    for name, (left, _) in phi_nodes.iteritems():
      ref = self.vars[name]
      self.initialized.add(name)
      value = self.compile_expr(left, builder)
      builder.store(value, ref)

  def compile_merge_right(self, phi_nodes, builder):
    for name, (_, right) in phi_nodes.iteritems():
      ref = self.vars[name]
      self.initialized.add(name)
      value = self.compile_expr(right, builder)
      builder.store(value, ref)

  def compile_ForLoop(self, stmt, builder):
    # first compile the starting, boundary, and
    # increment values for the loop counter
    start = self.compile_expr(stmt.start, builder)
    stop = self.compile_expr(stmt.stop, builder)
    step = self.compile_expr(stmt.step, builder)

    # get the memory slot associated with the loop counter
    loop_var = self.vars[stmt.var.name]

    builder.store(start, loop_var)
    self.initialized.add(stmt.var.name)

    # ...and we'll need its Parakeet type later on
    # for calls to 'cmp' and 'prim'
    loop_var_t = stmt.var.type

    # any phi-bound variables should be initialized to their
    # starting values
    self.compile_merge_left(stmt.merge, builder)

    loop_bb, body_start_builder = self.new_block("loop_body")
    after_bb, after_builder = self.new_block("after_loop")

    # WARNING: Assuming loop is always increasing,
    # only enter the loop if we're less than the stopping value
    enter_cond = self.cmp(prims.less, loop_var_t,  start, stop,
                          builder, "enter_cond")
    builder.cbranch(enter_cond, loop_bb, after_bb)

    # TODO: what should we do if the body always ends in a return statement?
    body_end_builder, body_always_returns = \
      self.compile_block(stmt.body, body_start_builder)

    counter_at_end = body_end_builder.load(loop_var)
    # increment the loop counter
    incr = self.prim(prims.add, loop_var_t,
                     [counter_at_end, step],
                     body_end_builder,  "incr_loop_var")
    body_end_builder.store(incr, loop_var)
    self.compile_merge_right(stmt.merge, body_end_builder)

    exit_cond = self.cmp(prims.less, loop_var_t, incr, stop,
                         body_end_builder, "exit_cond")
    body_end_builder.cbranch(exit_cond, loop_bb, after_bb)
    # WARNING: what if the loop doesn't run? Should
    # we still be returning 'body_always_returns'?
    return after_builder, body_always_returns

  def compile_While(self, stmt, builder):
    # current flow ----> loop --------> exit--> after
    #    |                       skip------------|
    #    |----------------------/

    self.compile_merge_left(stmt.merge, builder)
    loop_bb, body_start_builder = self.new_block("loop_body")

    after_bb, after_builder = self.new_block("after_loop")
    enter_cond = self.compile_expr(stmt.cond, builder)
    enter_cond = llvm_convert.to_bit(enter_cond, builder)
    builder.cbranch(enter_cond, loop_bb, after_bb)

    body_end_builder, body_always_returns = \
        self.compile_block(stmt.body, body_start_builder)
    if not body_always_returns:
      exit_bb, exit_builder = self.new_block("loop_exit")
      self.compile_merge_right(stmt.merge, body_end_builder)
      repeat_cond = self.compile_expr(stmt.cond, body_end_builder)
      repeat_cond = llvm_convert.to_bit(repeat_cond, body_end_builder)
      body_end_builder.cbranch(repeat_cond, loop_bb, exit_bb)
      exit_builder.branch(after_bb)

    return after_builder, False

  def compile_If(self, stmt, builder):
    cond = self.compile_expr(stmt.cond, builder)
    cond = llvm_convert.to_bit(cond, builder)

    if len(stmt.true) == 0 and len(stmt.false) == 0:
      # if nothing happens in the loop bodies, just
      # emit select instructions
      for (name, (true_expr, false_expr)) in stmt.merge.iteritems():
        ref = self.vars[name]
        self.initialized.add(name)
        true_val = self.compile_expr(true_expr, builder)
        false_val = self.compile_expr(false_expr, builder)
        select_val = builder.select(cond, true_val, false_val)
        builder.store(select_val, ref)
      return builder, False
    else:
      # compile the two possible branches as distinct basic blocks
      # and then wire together the control flow with branches
      true_bb, true_builder = self.new_block("if_true")
      after_true, true_always_returns = \
        self.compile_block(stmt.true, true_builder)

      false_bb, false_builder = self.new_block("if_false")
      after_false, false_always_returns = \
          self.compile_block(stmt.false, false_builder)

      builder.cbranch(cond, true_bb, false_bb)

      # compile phi nodes as assignments and then branch
      # to the continuation block
      self.compile_merge_left(stmt.merge, after_true)
      self.compile_merge_right(stmt.merge, after_false)

      # if both branches return then there is no point
      # making a new block for more code
      # did both branches end in a return?
      both_always_return = true_always_returns and false_always_returns
      if both_always_return:
        return None, True
      after_bb, after_builder = self.new_block("if_after")
      if not true_always_returns:
        after_true.branch(after_bb)
      if not false_always_returns:
        after_false.branch(after_bb)
      return after_builder, False

  def compile_Comment(self, stmt, builder):
    return builder, False

  def compile_stmt(self, stmt, builder):
    """
    Translate an SSA statement into LLVM. Every translation function returns a
    builder pointing to the end of the current basic block and a boolean
    indicating whether every branch of control flow in that statement ends in a
    return. The latter is needed to avoid creating empty basic blocks, which
    were causing some mysterious crashes inside LLVM.
    """
    method_name = "compile_" + stmt.node_type()
    return getattr(self, method_name)(stmt, builder)

  def compile_block(self, stmts, builder):
    for stmt in stmts:
      builder, always_returns = self.compile_stmt(stmt, builder)
      if always_returns:
        return builder, always_returns
    return builder, False

  def compile_body(self, body):
    return self.compile_block(body, builder = self.entry_builder)
  
  
import os
from collections import namedtuple   
CompiledFn = namedtuple('CompiledFn', ('llvm_fn', 'llvm_exec_engine', 'parakeet_fn')) 

compiled_functions = {}
def compile_fn(fundef):
  key = fundef.cache_key 
  if key in compiled_functions:
    return compiled_functions[key]
  
  if config.print_lowered_function:
    print
    print "=== Lowered function ==="
    print
    print repr(fundef)
    print

  compiler = Compiler(fundef)
  compiler.compile_body(fundef.body)
  if llvm_config.print_unoptimized_llvm:
    print "=== LLVM before optimizations =="
    print
    print compiler.llvm_context.module
    print
  compiler.llvm_context.run_passes(compiler.llvm_fn)

  if config.print_generated_code:
    print "=== LLVM after optimizations =="
    print
    print compiler.llvm_context.module
    print

  if llvm_config.print_x86:
    print "=== Generated assembly =="
    print
    start_printing = False
    w,r = os.popen2("llc")
    w.write(str(compiler.llvm_context.module))
    w.close()
    assembly_str = r.read()
    r.close()
    for l in assembly_str.split('\n'):
      if l.strip().startswith('.globl'):
        if start_printing:
          break
        elif fundef.name in l:
          start_printing = True
      if start_printing:
        print l

  result = CompiledFn(compiler.llvm_fn,
                      compiler.llvm_context.exec_engine, 
                      fundef)
  compiled_functions[key] = result
  return result

########NEW FILE########
__FILENAME__ = gv_helpers
import ctypes 

from llvm.ee import GenericValue 

from .. ndtypes import FloatT, SignedT, IntT, NoneT, PtrT    
from .. ndtypes import type_conv 
import llvm_types 

def python_to_generic_value(x, t):
  if isinstance(t, FloatT):
    llvm_t = llvm_types.llvm_value_type(t)
    return GenericValue.real(llvm_t, x)
  elif isinstance(t, SignedT):
    llvm_t = llvm_types.llvm_value_type(t)
    return GenericValue.int_signed(llvm_t, x)
  elif isinstance(t, IntT):
    llvm_t = llvm_types.llvm_value_type(t)
    return GenericValue.int(llvm_t, x)
  elif isinstance(t, PtrT):
    return GenericValue.pointer(x)
  else:
    ctypes_obj = type_conv.from_python(x)
    return GenericValue.pointer(ctypes.addressof(ctypes_obj))

def ctypes_to_generic_value(cval, t):
  if isinstance(t, FloatT):
    llvm_t = llvm_types.llvm_value_type(t)
    return GenericValue.real(llvm_t, cval.value)
  elif isinstance(t, SignedT):
    llvm_t = llvm_types.llvm_value_type(t)
    return GenericValue.int_signed(llvm_t, cval.value)
  elif isinstance(t, IntT):
    llvm_t = llvm_types.llvm_value_type(t)
    return GenericValue.int(llvm_t, cval.value)
  elif isinstance(t, NoneT):
    return GenericValue.int(llvm_types.int64_t, 0)
  elif isinstance(t, PtrT):
    return GenericValue.pointer(ctypes.addressof(cval.contents))
  else:
    return GenericValue.pointer(ctypes.addressof(cval))

def generic_value_to_python(gv, t):
  if isinstance(t, SignedT):
    return t.dtype.type(gv.as_int_signed() )
  elif isinstance(t, IntT):
    return t.dtype.type( gv.as_int() )
  elif isinstance(t, FloatT):
    llvm_t = llvm_types.ctypes_scalar_to_lltype(t.ctypes_repr)
    return t.dtype.type(gv.as_real(llvm_t))
  elif isinstance(t, NoneT):
    return None
  else:
    addr = gv.as_pointer()
    struct = t.ctypes_repr.from_address(addr)
    return t.to_python(struct)
########NEW FILE########
__FILENAME__ = llvm_config


######################################
#        OPTIMIZER OPTIONS           #
######################################

# run LLVM optimization passes
llvm_optimize = True

# number of times to run optimizations
llvm_num_passes = 4

# run verifier over generated LLVM code?
llvm_verify = True


######################################
#         PRINTING OPTIONS           #
######################################

# print generated assembly of compiled functions
print_x86 = False

# show LLVM bytecode before optimization passes
print_unoptimized_llvm = False

# show LLVM bytecode after optimizations
# print_optimized_llvm = False 


########NEW FILE########
__FILENAME__ = llvm_context

import llvm.core as core
import llvm.ee as ee
import llvm.passes as passes

import llvm_config 

class LLVM_Context:
  """Combine a module, exec engine, and pass manager into a single object"""

  _verify_passes = [
    'preverify',
    'domtree',
    'verify'
  ]

  _opt_passes = [
    'mem2reg',
    'targetlibinfo',
    'no-aa',
    'basicaa',
    'memdep',
    'tbaa',
    'instcombine',
    'simplifycfg',
    'basiccg',
    'memdep',
    'scalarrepl-ssa',
    'sroa',
    'domtree',
    'early-cse',
    'simplify-libcalls',
    'lazy-value-info',
    'correlated-propagation',
    'simplifycfg',
    'instcombine',
    'reassociate',
    'domtree',
    'mem2reg',
    'scev-aa',
    'loops',
    'loop-simplify',
    'lcssa',
    'loop-rotate',
    'licm',
    'lcssa',
    'loop-unswitch',
    'instcombine',
    'scalar-evolution',
    'loop-simplify',
    'lcssa',
    'indvars',
    'loop-idiom',
    'loop-deletion',
    #'loop-vectorize',
     'loop-unroll',
    #'bb-vectorize',
    'memdep',
    'gvn',
    'memdep',
    'sccp',
    'dse',
    'adce',
    'correlated-propagation',
    'jump-threading',
    'simplifycfg',
    'instcombine',
    'licm',
  ]

  def __init__(self, module_name, 
               optimize = llvm_config.llvm_optimize,
               verify = llvm_config.llvm_verify):
    self.module = core.Module.new(module_name)
    self.engine_builder = ee.EngineBuilder.new(self.module)
    self.engine_builder.force_jit()
    opt_level = 3 if optimize else 0
    if optimize:
      self.engine_builder.opt(opt_level)
    else:
      self.engine_builder.opt(opt_level)
    self.exec_engine = self.engine_builder.create()
    tm = ee.TargetMachine.new(opt = opt_level, cm=ee.CM_JITDEFAULT)
    self.tm = tm 
    _, fpm = passes.build_pass_managers(tm, 
                                     opt = opt_level,
                                     loop_vectorize = (opt_level > 0), 
                                     mod = self.module, 
                                     vectorize = (opt_level > 0), 
                                     )
    self.pass_manager = fpm 
    # self.fpm = fpm 
    for p in self._verify_passes:
      self.pass_manager.add(p)
    if optimize:
      for p in (self._opt_passes + self._verify_passes):
        self.pass_manager.add(p)

  def run_passes(self, llvm_fn, n_iters = llvm_config.llvm_num_passes):
    for _ in xrange(n_iters):
      self.pass_manager.run(llvm_fn)
    #passmanagers = passes.build_pass_managers(
    #        tm= self.tm , opt=3, inline_threshold=1000,
    #        loop_vectorize=True, vectorize=True, fpm=False)
    #for optname in self._opt_passes:
    #  passmanagers.pm.add(optname)
    #passmanagers.pm.add('loop-vectorize')
    #passmanagers.pm.run(self.module)
      
global_context = LLVM_Context("module")

########NEW FILE########
__FILENAME__ = llvm_convert

import llvm.core as llcore
import llvm_types

from .. ndtypes import FloatT, SignedT, UnsignedT, BoolT, IntT
from llvm_helpers import zero, one
from llvm_types import int1_t, int8_t, llvm_value_type, nbytes

def to_bit(llvm_value, builder):
  llvm_t = llvm_value.type

  if llvm_t == int1_t:
    return llvm_value
  if isinstance(llvm_t, llcore.IntegerType):
    return builder.icmp(llcore.ICMP_NE, llvm_value, zero(llvm_t), "ne_zero")
  else:
    return builder.fcmp(llcore.FCMP_ONE, llvm_value, zero(llvm_t), "ne_zero")

def from_bit(llvm_value, new_ptype, builder):
  llvm_t = llvm_value_type(new_ptype)
  name = "%s.cast.%s" % (llvm_value.name, new_ptype)
  return builder.select(llvm_value, one(llvm_t), zero(llvm_t), name)

def to_bool(llvm_value, builder):
  """
  bools are stored as bytes, if you need to use a boolean value for control flow
  convert it to a bit instead
  """

  bit = to_bit(llvm_value, builder)
  return builder.zext(bit, int8_t, "bool_val")

def from_float(llvm_value, new_ptype, builder):
  """Convert from an LLVM float value to some other LLVM scalar type"""

  dest_llvm_type = llvm_value_type(new_ptype)
  dest_name = "%s.cast_%s" % (llvm_value.name, new_ptype)

  if isinstance(new_ptype, FloatT):
    if llvm_types.nbytes(llvm_value.type) <= new_ptype.nbytes:
      return builder.fpext(llvm_value, dest_llvm_type, dest_name)
    else:
      return builder.fptrunc(llvm_value, dest_llvm_type, dest_name)
  elif isinstance(new_ptype, SignedT):
    return builder.fptosi(llvm_value, dest_llvm_type, dest_name)
  elif isinstance(new_ptype, UnsignedT):
    return builder.fptoui(llvm_value, dest_llvm_type, dest_name)
  else:
    return to_bool(llvm_value, builder)

def from_signed(llvm_value, new_ptype, builder):
  """Convert from an LLVM float value to some other LLVM scalar type"""

  dest_llvm_type = llvm_value_type(new_ptype)
  dest_name = "%s.cast_%s" % (llvm_value.name, new_ptype)

  if isinstance(new_ptype, FloatT):
    return builder.sitofp(llvm_value, dest_llvm_type, dest_name)
  elif isinstance(new_ptype, BoolT):
    return to_bool(llvm_value, builder)
  else:
    assert isinstance(new_ptype, IntT)
    nbits = llvm_value.type.width
    nbytes = nbits / 8

    if nbytes == new_ptype.nbytes:
      return builder.bitcast(llvm_value, dest_llvm_type, dest_name)
    elif nbytes < new_ptype.nbytes:
      return builder.zext(llvm_value, dest_llvm_type, dest_name)
    else:
      return builder.trunc(llvm_value, dest_llvm_type, dest_name)

def from_unsigned(llvm_value, new_ptype, builder):
  """Convert from an LLVM float value to some other LLVM scalar type"""

  dest_llvm_type = llvm_value_type(new_ptype)
  dest_name = "%s.cast_%s" % (llvm_value.name, new_ptype)

  if isinstance(new_ptype, FloatT):
    return builder.uitofp(llvm_value, dest_llvm_type, dest_name)
  elif isinstance(new_ptype, BoolT):
    return to_bool(llvm_value, builder)
  else:
    assert isinstance(new_ptype, IntT)
    nbytes = llvm_value.type.width / 8

    if nbytes == new_ptype.nbytes:
      return builder.bitcast(llvm_value, dest_llvm_type, dest_name)
    elif nbytes < new_ptype.nbytes:
      return builder.zext(llvm_value, dest_llvm_type, dest_name)
    else:
      return builder.trunc(llvm_value, dest_llvm_type, dest_name)

def convert(llvm_value, old_ptype, new_ptype, builder):
  """
  Given an LLVM value and two parakeet types, generate the instruction to
  perform the conversion
  """

  if old_ptype == new_ptype:
    return llvm_value

  if isinstance(old_ptype, FloatT):
    return from_float(llvm_value, new_ptype, builder)
  elif isinstance(old_ptype, SignedT):
    return from_signed(llvm_value, new_ptype, builder)
  else:
    assert isinstance(old_ptype, (BoolT, UnsignedT)), \
      "Unexpected type %s can't be converted to %s" % (old_ptype, new_ptype)
    return from_unsigned(llvm_value, new_ptype, builder)

########NEW FILE########
__FILENAME__ = llvm_helpers
import llvm.core as llcore
 
from .. ndtypes  import ScalarT, FloatT, Int32, Int64
from llvm_types import llvm_value_type
 
def const(python_scalar, parakeet_type):
  assert isinstance(parakeet_type, ScalarT)
  llvm_type = llvm_value_type(parakeet_type)
  if isinstance(parakeet_type, FloatT):
    return llcore.Constant.real(llvm_type, float(python_scalar))
  else:
    return llcore.Constant.int(llvm_type, int(python_scalar))

def int32(x):
  """Make LLVM constants of type int32"""
  return const(x, Int32)

def int64(x):
  return const(x, Int64)

def zero(llvm_t):
  """
  Make a zero constant of either int or real type. 
  Doesn't (yet) work for vector constants! 
  """
  if isinstance(llvm_t, llcore.IntegerType):
    return llcore.Constant.int(llvm_t, 0)
  else:
    return llcore.Constant.real(llvm_t, 0.0)
  
def one(llvm_t):
  """
  Make a constant 1 of either int or real type. 
  Doesn't (yet) work for vector constants!
  """
  if isinstance(llvm_t, llcore.IntegerType):
    return llcore.Constant.int(llvm_t, 1)
  else:
    return llcore.Constant.real(llvm_t, 1.0)
  
  
########NEW FILE########
__FILENAME__ = llvm_prims

import llvm.core as llc   

import llvm_types
from .. import prims
from .. ndtypes import Float32, Float64, Int32, Int64
from llvm_context import global_context

  

signed_int_comparisons = {
  prims.equal : llc.ICMP_EQ, 
  prims.not_equal : llc.ICMP_NE, 
  prims.greater : llc.ICMP_SGT, 
  prims.greater_equal : llc.ICMP_SGE, 
  prims.less : llc.ICMP_SLT, 
  prims.less_equal : llc.ICMP_SLE
}

unsigned_int_comparisons = {
  prims.equal : llc.ICMP_EQ, 
  prims.not_equal : llc.ICMP_NE, 
  prims.greater : llc.ICMP_UGT, 
  prims.greater_equal : llc.ICMP_UGE, 
  prims.less : llc.ICMP_ULT, 
  prims.less_equal : llc.ICMP_ULE
}  

float_comparisons = { 
  prims.equal : llc.FCMP_OEQ, 
  prims.not_equal : llc.FCMP_ONE, 
  prims.greater : llc.FCMP_OGT, 
  prims.greater_equal : llc.FCMP_OGE, 
  prims.less : llc.FCMP_OLT, 
  prims.less_equal : llc.FCMP_OLE                     
}


signed_binops = { 
  prims.add : 'add',  
  prims.subtract : 'sub', 
  prims.multiply : 'mul', 
  prims.divide : 'sdiv',
}

unsigned_binops = { 
  prims.add : 'add',  
  prims.subtract : 'sub', 
  prims.multiply : 'mul', 
  prims.divide : 'udiv', 
  prims.mod : 'urem', 
}

float_binops = { 
  prims.add : 'fadd',  
  prims.subtract : 'fsub', 
  prims.multiply : 'fmul', 
  prims.divide : 'fdiv', 
}

# Note: there is no division instruction between booleans
# so b1 / b2 should be translated to int(b1) / int(b2) 
bool_binops = { 
  prims.add : 'or_',
  prims.multiply : 'and_', 
  prims.subtract : 'xor',
  prims.divide : 'and_',
  
  prims.mod : 'urem'
  #prims.logical_not : 'not_'
}

int32_fn_t = llc.Type.function(llvm_types.int32_t, [llvm_types.int32_t])
int64_fn_t = llc.Type.function(llvm_types.int64_t, [llvm_types.int64_t])
float32_fn_t = llc.Type.function(llvm_types.float32_t, [llvm_types.float32_t])
float64_fn_t = llc.Type.function(llvm_types.float64_t, [llvm_types.float64_t])

binary_float32_fn_t = llc.Type.function(llvm_types.float32_t, 
                                        [llvm_types.float32_t, llvm_types.float32_t])
binary_float64_fn_t = llc.Type.function(llvm_types.float64_t, 
                                        [llvm_types.float64_t, llvm_types.float64_t])

def float32_fn(name):
  return global_context.module.add_function(float32_fn_t, name)

def float64_fn(name):
  return global_context.module.add_function(float64_fn_t, name)



#import llvmmath 
#mathlib = llvmmath.get_default_math_lib()
#linker = llvmmath.linking.get_linker(mathlib)

_llvm_intrinsics = set(['sqrt', 
                        'powi', 
                        'sin', 
                        'cos',
                        'pow',
                        'exp', 
                        'exp2', 
                        'log', 
                        'log10', 
                        'log2', 
                        'fma', 
                        'fabs', 
                        'floor', 
                        'ceil', 
                        'trunc', 
                        'rint', 
                        'nearbyint',                         
                      ])

from ..c_backend.c_prims import _float_fn_names

def get_float_op(prim, t, _float_decls = {}):
  key = (prim, t)
  if key in _float_decls:
    return _float_decls[key]
  
  assert prim in _float_fn_names, \
    "Unsupported float primitive %s" % prim 
  assert t in (Float32, Float64), \
    "Invalid type %s, expected Float32 or Float64" % t
  
  prim_name = _float_fn_names[prim]
  
  if t == Float32:
    fn_t = float32_fn_t if prim.nin == 1 else binary_float32_fn_t
    if prim_name in _llvm_intrinsics:
      fn_name = "llvm.%s.f32" % prim_name
    else:
      fn_name = prim_name + "f" 
  else:
    assert t == Float64
    fn_t = float64_fn_t if prim.nin == 1 else binary_float64_fn_t
    if prim_name in _llvm_intrinsics:
      fn_name = "llvm.%s.f64" % prim_name 
    else:
      fn_name = prim_name 
  llvm_value = global_context.module.get_or_insert_function(fn_t, fn_name)
  llvm_value.add_attribute(llc.ATTR_NO_UNWIND)
  llvm_value.add_attribute(llc.ATTR_READONLY)
  _float_decls[key] = llvm_value
  return llvm_value 


########NEW FILE########
__FILENAME__ = llvm_types
import ctypes
import llvm.core as llcore

from llvm.core import Type as lltype

from .. ndtypes import ScalarT, PtrT, NoneT

void_t = lltype.void()
int1_t = lltype.int(1)
int8_t = lltype.int(8)
int16_t = lltype.int(16)
int32_t = lltype.int(32)
int64_t = lltype.int(64)

float32_t = lltype.float()
float64_t = lltype.double()

ptr_int8_t = lltype.pointer(int8_t)
ptr_int32_t = lltype.pointer(int32_t)
ptr_int64_t = lltype.pointer(int64_t)

def nbytes(t):
  if t.kind == llcore.TYPE_FLOAT:
    return 4
  elif t.kind == llcore.TYPE_DOUBLE:
    return 8
  else:
    return t.width / 8

def is_scalar(llvm_t):
  return isinstance(llvm_t, llcore.IntegerType) or \
         llvm_t in (float32_t, float64_t)

_ctypes_scalars_to_llvm_types = {
  ctypes.c_bool : int8_t,
  ctypes.c_uint8 : int8_t,
  ctypes.c_int8 : int8_t,

  ctypes.c_uint16 : int16_t,
  ctypes.c_int16 : int16_t,

  ctypes.c_uint32 : int32_t,
  ctypes.c_int32 : int32_t,

  ctypes.c_uint64 : int64_t,
  ctypes.c_int64 : int64_t,

  ctypes.c_float : float32_t,
  ctypes.c_double : float64_t,
}

PyCPointerType = type(ctypes.POINTER(ctypes.c_int32))
PyCStructType = type(ctypes.Structure)

_struct_cache = {}
def ctypes_struct_to_lltype(S, name = None):
  if not name:
    name = S.__class__.__name__

  key = tuple(S._fields_)

  if key in _struct_cache:
    return _struct_cache[key]
  else:
    llvm_field_types = [ctypes_to_lltype(field_type)
                        for (_, field_type) in S._fields_]
    llvm_struct = lltype.struct(llvm_field_types, name)
    _struct_cache[key] = llvm_struct
    return llvm_struct

def ctypes_scalar_to_lltype(ct):
  assert ct in _ctypes_scalars_to_llvm_types, \
      "%s isn't a convertible to an LLVM scalar type" % ct
  return _ctypes_scalars_to_llvm_types[ct]

def ctypes_to_lltype(ctypes_repr, name = None):
  if type(ctypes_repr) == PyCStructType:
    return ctypes_struct_to_lltype(ctypes_repr, name)
  elif type(ctypes_repr) == PyCPointerType:
    elt_t = ctypes_repr._type_
    if elt_t == ctypes.c_bool:
      return lltype.pointer(int8_t)
    else:
      return lltype.pointer(ctypes_to_lltype(elt_t))
  else:
    return ctypes_scalar_to_lltype(ctypes_repr)

def llvm_value_type(t):
  return ctypes_to_lltype(t.ctypes_repr, t.node_type())

def llvm_ref_type(t):
  llvm_value_t = llvm_value_type(t)
  if isinstance(t, (PtrT, ScalarT, NoneT)):
    return llvm_value_t
  else:
    return lltype.pointer(llvm_value_t)

# we allocate heap slots for output scalars before entering the
# function
def to_llvm_output_type(t):
  llvm_type = llvm_value_type(t)
  if isinstance(t, ScalarT):
    return lltype.pointer(llvm_type)
  else:
    return llvm_type

########NEW FILE########
__FILENAME__ = mappings
import math  
import numpy as np
import lib, prims   
from syntax import Zip, Len


function_mappings = {
  # PYTHON BUILTINS          
  zip : Zip, 
  map : lib.map, 
  reduce : lib.reduce, 
  tuple : lib.builtin_tuple, 
  range : lib.arange, 
  xrange : lib.arange, 
  float : lib.numpy_types.float64, 
  int : lib.numpy_types.int64, 
  long : lib.numpy_types.int64, 
  bool : lib.numpy_types.bool, 
  len : Len,  
  min : lib.builtin_min,
  max : lib.builtin_max,
  all : lib.reduce_all, 
  any : lib.reduce_any, 
  sum : lib.reduce_sum, 
  abs : prims.abs,  
  pow : prims.power, 
  
  # TYPES 
  np.int8 : lib.numpy_types.int8, 
  np.int16 : lib.numpy_types.int16, 
  np.int32 : lib.numpy_types.int32, 
  np.int64 : lib.numpy_types.int64, 
  np.uint8 : lib.numpy_types.uint8,
  np.uint16 : lib.numpy_types.uint16, 
  np.uint32 : lib.numpy_types.uint32, 
  np.uint64 : lib.numpy_types.uint64,  
  np.bool : lib.numpy_types.bool, 
  np.bool8 : lib.numpy_types.bool, 
  np.bool_ : lib.numpy_types.bool, 
  np.float32 : lib.numpy_types.float32, 
  np.float64 : lib.numpy_types.float64, 
  
  np.rank : lib.rank, 
  np.alen : Len, 
  np.real : lib.real, 
  # np.imag : lib.imag
  np.size : lib.size, 
  
  np.minimum : prims.minimum, 
  np.maximum : prims.maximum, 
  
  # COMPARISONS
  np.greater : prims.greater, 
  np.greater_equal : prims.greater_equal, 
  np.equal : prims.equal, 
  np.not_equal : prims.not_equal, 
  np.less : prims.less, 
  np.less_equal : prims.less_equal, 
  
  # REDUCTIONS 
  np.min : lib.reduce_min, 
  np.max : lib.reduce_max,
  
  np.argmin : lib.argmin, 
  np.argmax : lib.argmax, 
  
  np.all : lib.reduce_all, 
  
  np.any : lib.reduce_any, 
  np.sum : lib.reduce_sum, 
  np.prod : lib.prod, 
  np.mean : lib.mean, 
  
  
  np.abs : prims.abs, 

  # ARRAY CONSTRUCTORS 
  np.array : lib.array, 
  np.tile : lib.tile, 
  # np.eye : , 
  np.arange  : lib.arange, 
  
  np.empty_like : lib.empty_like, 
  np.empty : lib.empty, 
  np.zeros_like : lib.zeros_like, 
  np.zeros : lib.zeros, 
  np.ones : lib.ones, 
  np.ones_like : lib.ones_like, 
  
  # ARITHMETIC 
  np.add : prims.add, 
  np.subtract : prims.subtract, 
  np.multiply : prims.multiply,
   
  np.divide : prims.divide, 
  np.floor_divide : lib.floor_divide, 
  np.true_divide : lib.true_divide, 
   
  np.mod : prims.remainder,
  np.remainder : prims.remainder,
  np.fmod : prims.fmod,
  # np.modf : prims.modf,
  
  np.logical_and : prims.logical_and, 
  np.logical_not : prims.logical_not, 
  np.logical_or : prims.logical_or,  
    
  math.sqrt : prims.sqrt, 
  np.sqrt : prims.sqrt, 
  
  np.sign : lib.sign, 
  np.reciprocal : lib.reciprocal, 
  np.conjugate : lib.conjugate,
  
  # ROUNDING
  
  np.trunc : prims.trunc, 
  math.trunc : prims.trunc,
   
  np.rint : prims.rint,
  
  np.floor : prims.floor, 
  math.floor : prims.floor,
   
  np.ceil : prims.ceil,
  math.ceil : prims.ceil, 
  
  np.round : prims.round,
 
  # LOGS AND EXPONENTIATION 
  np.square : lib.square,
  math.pow : prims.power, 
  np.power : prims.power,
  
  np.exp : prims.exp,
  math.exp : prims.exp, 
   
  np.exp2 : prims.exp2,
   
  np.expm1 : prims.expm1,
  math.expm1 : prims.expm1, 
   
  np.log : prims.log, 
  math.log : prims.log, 
  
  np.log10 : prims.log10,
  math.log10 : prims.log10, 
   
  np.log2 : prims.log2,
  
  np.log1p : prims.log1p,
  math.log1p : prims.log1p,
  
  np.logaddexp : lib.logaddexp, 
  np.logaddexp2 : lib.logaddexp2, 
  
  # TRIG 
  np.cos : prims.cos, 
  math.cos : prims.cos, 
  np.sin : prims.sin, 
  math.sin : prims.sin, 
  np.tan : prims.tan, 
  math.tan : prims.tan, 
  np.arccos : prims.arccos, 
  math.acos: prims.arccos, 
  np.arcsin : prims.arcsin, 
  math.asin : prims.arcsin, 
  np.arctan : prims.arctan, 
  math.atan : prims.arctan, 
  np.arctan2 : prims.arctan2, 
  math.atan2 : prims.arctan2, 
  math.cosh : prims.cosh, 
  np.cosh : prims.cosh, 
  np.sinh : prims.sinh,           
  math.sinh : prims.sinh, 
  np.tanh : prims.tanh, 
  math.tanh : prims.tanh, 
  math.atanh : prims.arctanh,
  np.arctanh : prims.arctanh,  
  math.asinh : prims.arcsinh, 
  np.arcsinh : prims.arcsinh, 
  math.acosh : prims.arccosh, 
  np.arccosh : prims.arccosh,
  np.rad2deg : lib.rad2deg, 
  np.deg2rad : lib.deg2rad,  
  # np.hypot : lib.hypot,
  
  np.where : lib.where,
  np.linspace : lib.linspace, 
  np.vdot : lib.vdot, 
  np.dot : lib.dot,
  np.linalg.norm : lib.linalg.norm,  
  
  math.erf : lib.prob.erf, 
  math.erfc : lib.prob.erfc, 
}

property_mappings = {
  'dtype' : lib.get_elt_type,                
  # 'imag' : lib.imag,      
  'itemsize' : lib.itemsize,
  'real' :  lib.identity, # ain't no complex numbers yet 
  'shape' : lib.shape, 
  'size' : lib.size,   
  # 'strides' : lib.strides, 
  'ndim' : lib.rank, 
  'T' : lib.transpose, 
}


method_mappings = {
  'fill' : lib.fill, 
  'any' : lib.reduce_any, 
  'all' : lib.reduce_all, 
  'argmax' : lib.argmax, 
  # 'argsort' : lib.argsort, 
  'copy' : lib.copy, 
  'cumprod' : lib.cumprod, 
  'cumsum' : lib.cumsum, 
  'dot' : lib.dot, 
  'fill' : lib.fill, 
  'flatten' : lib.ravel, 
  # 'diagonal' : lib.diagonal, 
  
  'mean' : lib.mean, 
  
  'min' : lib.reduce_min,
  'max' : lib.reduce_max,
  
  'ravel' : lib.ravel, 
  'transpose' : lib.transpose,
  'sum' : lib.reduce_sum,
  }

########NEW FILE########
__FILENAME__ = names
class NameNotFound(Exception):
  def __init__(self, name):
    self.name = name
    

  def __str__(self):
    return self.name 
  
versions = {}
original_names = {}
  
def get(name):
  version = versions.get(name)
  if version is None:
    raise NameNotFound(name)
  else:
    return "%s.%d" % (name, version)
    
def fresh(name):
  version = versions.get(name, 0) + 1 
  versions[name] = version
  if version == 1:
    ssa_name = name 
  else:
    ssa_name = "%s.%d" % (name, version)
  original_names[ssa_name] = name
  # assert ssa_name != 'array_elt.3' 
  return ssa_name 

lcase_chars = [chr(i + 97) for i in xrange(26)]
def fresh_list(count):
  prefixes = lcase_chars[:count]
  while len(prefixes) < count:
    count -= 26
    prefixes += lcase_chars[:count]
  return map(fresh, prefixes)


def original(unique_name):
  original_name = original_names.get(unique_name)

  if original_name is None:
    versions[unique_name] = 1
    original_names[unique_name] = unique_name
    return unique_name  
  else:
    return original_name 
  
def refresh(unique_name):
  """Given an existing unique name, create another versioned name with the same root"""
  try:
    return fresh(original(unique_name))
  except NameNotFound:
    # it wasn't really an SSA name but keep going anyway 
    return fresh(unique_name)
  
def add_prefix(prefix, name):
  base = original(name)
  return fresh(prefix + base)
########NEW FILE########
__FILENAME__ = array_type
from core_types import StructT, IncompatibleTypes, NoneT, Type, NoneType   
from ptr_type import ptr_type
from scalar_types import Int64, ScalarT, IntT, BoolT
from tuple_type import TupleT, repeat_tuple
from slice_type import SliceT
  
class ArrayT(StructT):
  
  def __init__(self, elt_type, rank):
    assert isinstance(elt_type, ScalarT), \
      "Can't create array with element type %s, currently only scalar elements supported" % \
      (elt_type,)
    self.elt_type = elt_type
    self.rank = rank 
    
    tuple_t = repeat_tuple(Int64, self.rank)

    self.shape_t = tuple_t
    self.strides_t = tuple_t
    self.ptr_t = ptr_type(self.elt_type)
    self._fields_ = [
      ('data', self.ptr_t),
      ('shape', tuple_t),
      ('strides', tuple_t),
      ('offset', Int64),
      ('size', Int64),
      # ('dtype', TypeValueT(self.elt_type))
    ]
    self._hash = hash( (elt_type, rank) )
  
  def children(self):
    yield self.elt_type
    yield self.shape_t
    yield self.strides_t
    yield self.ptr_t 
     
  def dtype(self):
    return self.elt_type.dtype

  def __str__(self):
    return "array%d(%s)" % (self.rank, self.elt_type)

  def __repr__(self):
    return str(self)
  
  def __eq__(self, other):
    if self is other:
      return True 
    return other.__class__ is ArrayT and \
        self.elt_type == other.elt_type and \
        self.rank == other.rank

  def __hash__(self):
    return self._hash 

  def combine(self, other):
    if self == other:
      return self
    elif isinstance(other, ScalarT):
      elt_t = self.elt_type
      combined_elt_t = elt_t.combine(other)
      return make_array_type(combined_elt_t, self.rank)
    elif other.__class__ is ArrayT:
      assert self.rank == other.rank
      combined_elt_t = self.elt_type.combine(other.elt_type)
      return make_array_type(combined_elt_t, self.rank)
    else:
      raise IncompatibleTypes(self, other)

  def index_type(self, idx):
    """
    Given the type of my indices, what type of array will result?
    """

    if not isinstance(idx, Type):
      idx = idx.type

    if idx.__class__ is TupleT:
      indices = idx.elt_types
    else:
      indices = (idx,)

    n_indices = len(indices)
    n_required = self.rank
    if n_required > n_indices:
      n_missing = n_required - n_indices
      extra_indices = (NoneType,) * n_missing
      indices = indices + extra_indices

    # we lose one result dimension for each int in the index set
    result_rank = n_required
    for t in indices:
      if isinstance(t, (BoolT, IntT)):
        result_rank -= 1
      else:
        assert isinstance(t,  (TupleT, NoneT, SliceT, ArrayT, )),  "Unexpected index type: %s " % t
    if result_rank > 0:
      return make_array_type(self.elt_type, result_rank)
    else:
      return self.elt_type

_array_types = {}
def make_array_type(elt_t, rank):
  key = (elt_t, rank)
  arr_t = _array_types.get(key)
  if arr_t is None: 
    if rank == 0:  
      arr_t = elt_t 
    else: 
      arr_t = ArrayT(elt_t, rank)
    _array_types[key] = arr_t
  return arr_t

def elt_type(t):
  if t.__class__ is ArrayT:
    return t.elt_type
  else:
    return t

def elt_types(ts):
  return [elt_type(t) for t in ts]

def lower_rank(t, r):
  if t.__class__ is ArrayT:
    assert t.rank >= r
    return make_array_type(t.elt_type, t.rank - r)
  else:
    return t

def increase_rank(t, r):
  if t.__class__ is ArrayT:
    return make_array_type(t.elt_type, t.rank + r)
  else:
    return make_array_type(t, r)

def lower_ranks(arg_types, r):
  return [lower_rank(t, r) for t in arg_types]

def get_rank(t):
  if t.__class__ is ArrayT:
    return t.rank
  else:
    return 0

def rank(t):
  return get_rank(t)

########NEW FILE########
__FILENAME__ = closure_type
import ctypes
from core_types import Type, IncompatibleTypes, StructT, ImmutableT
from scalar_types import  Int64
import type_conv 

###########################################
#
#  Closures!
#
###########################################

class ClosureT(StructT, ImmutableT):
  def __init__(self, fn, arg_types):
    
    self.fn = fn 
    
    if arg_types is None:
      arg_types = ()
    elif not hasattr(arg_types, '__iter__'):
      arg_types = tuple([arg_types])
    else:
      arg_types = tuple(arg_types)
      
    self.arg_types = arg_types 
    self._hash = hash( (fn,) + arg_types)
  
    self.specializations = {}

  def children(self):
    return self.arg_types
  
  def __hash__(self):
    return self._hash 

  def __eq__(self, other):
    
    if other.__class__ is not ClosureT:
      return False
    if self.fn.__class__ != other.fn.__class__:
      return False
    if isinstance(self.fn, str):
      same_fn = (self.fn == other.fn)
    else:
      # checking equality of functions isn't really defined, 
      # so just check that it's the same location 
      same_fn = self.fn is other.fn
    return same_fn and self.arg_types == other.arg_types

  def __str__(self):
    fn_name = self.fn if isinstance(self.fn, str) else self.fn.name
    return "ClosT(%s, {%s})" % (fn_name, ", ".join(str(t)
                                                   for t in self.arg_types))




  def combine(self, other):
    if isinstance(other, ClosureSet):
      return other.combine(self)
    elif isinstance(other, ClosureT):
      if self == other:
        return self
      else:
        return ClosureSet(self, other)
    else:
      raise IncompatibleTypes(self, other)

_closure_type_cache = {}
def make_closure_type(fn, closure_arg_types = []):
  closure_arg_types = tuple(closure_arg_types)
  key = (fn, closure_arg_types)
  if key in _closure_type_cache:
    return _closure_type_cache[key]
  else:
    t = ClosureT(fn, closure_arg_types)
    _closure_type_cache[key] = t
    return t





class ClosureSet(Type):
  """
  If multiple closures meet along control flow paths then join them into a
  closure set. This type should not appear by the time we're generating LLVM
  code.
  """

  _members = ['closures']

  def __init__(self, *closures):
    self.closures = set([])
    for clos_t in closures:
      if isinstance(clos_t, ClosureSet):
        self.closures.update(clos_t.closures)
      else:
        assert isinstance(clos_t, ClosureT)
        self.closures.add(clos_t)

  def combine(self, other):
    if isinstance(other, ClosureSet):
      combined_closures = self.closures.union(other.closures)
      if combined_closures != self.closures:
        return ClosureSet(combined_closures)
      else:
        return self
    else:
      raise IncompatibleTypes(self, other)

  def __eq__(self, other):
    return self.closures == other.closures

  def __iter__(self):
    return iter(self.closures)

  def __len__(self):
    return len(self.closures)

########NEW FILE########
__FILENAME__ = core_types
import ctypes

#from dsltools import Node

class TypeFailure(Exception):
  def __init__(self, msg):
    self.msg = msg

class IncompatibleTypes(Exception):
  def __init__(self, t1, t2):
    self.t1 = t1
    self.t2 = t2

  def __repr__(self):
    return "IncompatibleTypes(%s, %s)" % (self.t1, self.t2)

  def __str__(self):
    return repr(self)

class Type(object):
  def combine(self, other):
    raise IncompatibleTypes(self, other)

  def children(self):
    return ()
  
  def node_type(self):
    return self.__class__.__name__
  
  def __hash__(self):
    assert False, "Hash function not implemented for type %s" % (self,)

  def __eq__(self, _):
    assert False, "Equality not implemented for type %s" % (self,)
  
  def __ne__(self, other):
    return not (self == other)
  
  def __str__(self):
    return self.__class__.__name__

class AnyT(Type):
  """top of the type lattice, absorbs all types"""

  def __hash__(self):
    return 101
  
  def combine(self, other):
    return self

  def __eq__(self, other):
    return other.__class__ is AnyT

# since there's only one Any type, just create an instance of the same name
Any = AnyT()

class UnknownT(Type):
  """Bottom of the type lattice, absorbed by all other types"""

  _hash = hash("unknown")
  
  def __hash__(self):
    return self._hash 
  
  
  _members = []
  def  combine(self, other):
    return other

  def __eq__(self, other):
    return other.__class__ is UnknownT

#single instance of the Unknown type with same name
Unknown = UnknownT()

def combine_type_list(types):
  common_type = Unknown

  for t in types:
    common_type = common_type.combine(t)
  return common_type



class ConcreteT(Type):
  """
  Type which actually have some corresponding runtime values, as opposed to
  "Any" and "Unknown"
  """

  def from_python(self, py_val):
    return self.ctypes_repr(py_val)

  def to_python(self, internal):
    return internal
  
class ImmutableT(ConcreteT):
  """
  Tuples, closures, None, and scalars have no mutable fields
  """
  pass 
# None is fully described by its type, so the
# runtime representation can just be the number zero

class NoneT(ImmutableT):
  _members = []
  rank = 0
  ctypes_repr = ctypes.c_int64


  def combine(self, other):
    if isinstance(other, NoneT):
      return self
    else:
      raise IncompatibleTypes(self, other)

  def __str__(self):
    return "NoneT"

  def __hash__(self):
    return 0

  def __eq__(self, other):
    return other.__class__ is NoneT
  
  def __repr__(self):
    return str(self)

NoneType = NoneT()



def is_struct(c_repr):
  return type(c_repr) == type(ctypes.Structure)

class FieldNotFound(Exception):
  def __init__(self, struct_t, field_name):
    self.struct_t = struct_t
    self.field_name = field_name
    
  def __str__(self):
    return "FieldNotFound(%s, %s)" % (self.struct_t, self.field_name)

class StructT(Type):
  """All concrete types excluding scalars and pointers"""

  # expect each child class to fill this list
  _fields_ = []

  _repr_cache = {}

  def field_type(self, name):
    for (field_name, field_type) in self._fields_:
      if field_name == name:
        return field_type
    raise FieldNotFound(self, name)

  def field_pos(self, name):
    for (i, (field_name, _)) in enumerate(self._fields_):
      if field_name == name:
        return i
    raise FieldNotFound(self, name)

###################################################
#                                                 #
#             SCALAR NUMERIC TYPES                #
#                                                 #
###################################################

###################################################
# helper functions to implement properties of Python scalar objects
###################################################
def always_zero(x):
  return type(x)(0)

def identity(x):
  return x

class TypeValueT(ImmutableT):
  """
  Materialization of a type into a value 
  """
  rank = 0
  
  def __init__(self, type):
    self.type = type 
    self._hash = hash(self.type)

  
  def __str__(self):
    return "TypeValue(%s)" % self.type 
  
  def __hash__(self):
    return self._hash 
  
  def __eq__(self, other):
    return other.__class__ is TypeValueT and self.type == other.type
  

########NEW FILE########
__FILENAME__ = dtypes

import numpy as np

# partial mapping, since ctypes doesn't support
# complex numbers
bool8 = np.dtype('bool8')

int8 = np.dtype('int8')
uint8 = np.dtype('uint8')

int16 = np.dtype('int16')
uint16 = np.dtype('uint16')

int32 = np.dtype('int32')
uint32 = np.dtype('uint32')

int64 = np.dtype('int64')
uint64 = np.dtype('uint64')
  
  
float32 = np.dtype('float32')
float64 = np.dtype('float64')
complex64 = np.dtype('complex64')
complex128 = np.dtype('complex128')
  
def is_float(dtype):
  return dtype.type in np.sctypes['float']

def is_signed(dtype):
  return dtype.type in np.sctypes['int']
  
def is_unsigned(dtype):
  return dtype.type in np.sctypes['uint']

def is_complex(dtype):
  return dtype.type in np.sctypes['complex']
   
def is_bool(dtype):
  return dtype == np.bool8
   
def is_int(dtype):
  return is_bool(dtype) or is_signed(dtype) or is_unsigned(dtype)
  

import ctypes 
_to_ctypes = {
  bool8 : ctypes.c_bool, 
  
  int8  : ctypes.c_int8, 
  uint8 : ctypes.c_uint8, 
  
  int16 : ctypes.c_int16, 
  uint16 : ctypes.c_uint16,
  
  int32 : ctypes.c_int32, 
  uint32 : ctypes.c_uint32,
  
  int64 : ctypes.c_int64, 
  uint64 : ctypes.c_uint64, 
  
  float32 : ctypes.c_float, 
  float64 :  ctypes.c_double, 
}

def to_ctypes(dtype):
  """
  Give the ctypes representation for each numpy scalar type. 
  Beware that complex numbers have no assumed representation 
  and thus aren't valid arguments to this function. 
  """
  if dtype in _to_ctypes:
    return _to_ctypes[dtype]
  else:
    raise RuntimeError("No conversion from %s to ctypes" % dtype)
  

########NEW FILE########
__FILENAME__ = fn_type
from core_types import  IncompatibleTypes, ImmutableT

class FnT(ImmutableT):
  """Type of a typed function"""
  def __init__(self, input_types, return_type):
    self.input_types = tuple(input_types)
    self.return_type = return_type 
    self._hash = hash(self.input_types + (return_type,))

  def __str__(self):
    input_str = ", ".join(str(t) for t in self.input_types)
    return "(%s)->%s" % (input_str, self.return_type)

  def __repr__(self):
    return str(self)

  def __eq__(self, other):
    return other.__class__ is FnT and \
           self.return_type == other.return_type and \
           len(self.input_types) == len(other.input_types) and \
           all(t1 == t2 for (t1, t2) in
               zip(self.input_types, other.input_types))

  def combine(self, other):
    if self == other:
      return self
    else:
      raise IncompatibleTypes(self, other)

  def __hash__(self):
    return self._hash

_fn_type_cache = {}
def make_fn_type(input_types, return_type):
  input_types = tuple(input_types)
  key = input_types, return_type
  if key in _fn_type_cache:
    return _fn_type_cache[key]
  else:
    t = FnT(input_types, return_type)
    _fn_type_cache[key] = t
    return t
########NEW FILE########
__FILENAME__ = ptr_type
from core_types import ConcreteT
from scalar_types import IntT 

###########################################
#
#  Pointers!
#
###########################################

class PtrT(ConcreteT):
  """
  I'm not giving pointer a concrete to_python/from_python conversion or any
  usable fields, so it's up to our ctypes_repr and llvm_backend to appropriately
  interpret objects of this type.
  """
  def __init__(self, elt_type):
    self.elt_type = elt_type 
    self._hash =  hash(elt_type)
    
  rank = 1
  def index_type(self, idx):
    assert isinstance(idx, IntT), \
        "Index into pointer must be of type int, got %s" % (idx)
    return self.elt_type

  def children(self):
    return self.elt_type

  def __str__(self):
    return "ptr(%s)" % self.elt_type

  def __eq__(self, other):
    return other.__class__ is PtrT and self.elt_type == other.elt_type

  def __hash__(self):
    return self._hash 

  def __repr__(self):
    return str(self)

  @property
  def ctypes_repr(self):
    return self._ctypes_repr

_ptr_types = {}
def ptr_type(t):
  if t in _ptr_types:
    return _ptr_types[t]
  else:
    ptr_t = PtrT(t)
    _ptr_types[t] = ptr_t
    return ptr_t

########NEW FILE########
__FILENAME__ = scalar_types
import numpy as np

import dtypes 
import type_conv 

from core_types import IncompatibleTypes, ImmutableT

# base class for all concrete scalar types
# don't actually tag any values with this
class ScalarT(ImmutableT):
  rank = 0
  
  def __init__(self, dtype):
    self.dtype = dtype 
    self.name = self.dtype.name
    self.nbytes = self.dtype.itemsize
    self._hash = hash(self.dtype)
  
  def __eq__(self, other):
    return (self is other) or ( self.__class__ is other.__class__ and self.nbytes == other.nbytes)
  
  def __ne__(self, other):
    return not (self == other) 
  
  def __hash__(self):
    return self._hash

  def __repr__(self):
    return self.name

  def __str__(self):
    return str(self.name)

  def index_type(self, _):
    return self

  def convert_python_value(self, x):
    return self.dtype.type(x)
  
  def combine(self, other):
    if hasattr(other, 'dtype'):
      combined_dtype = np.promote_types(self.dtype, other.dtype)
      if combined_dtype == self.dtype: 
        return self
      elif combined_dtype == other.dtype:
        return other
      else:
        return from_dtype(combined_dtype)
    else:
      raise IncompatibleTypes(self, other)

_dtype_to_parakeet_type = {}
def register_scalar_type(ParakeetClass, dtype, equiv_python_types = []):
  parakeet_type = ParakeetClass(dtype)
  _dtype_to_parakeet_type[dtype] = parakeet_type

  python_types = [dtype.type] + equiv_python_types

  for python_type in python_types:
    type_conv.register(python_type, parakeet_type)

  return parakeet_type



class IntT(ScalarT):
  """Base class for bool, signed and unsigned"""
  pass 

  

class BoolT(IntT):
  """
  The type is called BoolT to distinguish it from its only instantiation called
  Bool.
  """
  
  def __init__(self, dtype):
    ScalarT.__init__(self, dtype)
    self.name = 'bool'

  def __eq__(self, other):
    return other.__class__ is BoolT

Bool = register_scalar_type(BoolT, dtypes.bool8, equiv_python_types = [bool])

class UnsignedT(IntT):
  pass 

UInt8 = register_scalar_type(UnsignedT, dtypes.uint8)

UInt16 = register_scalar_type(UnsignedT, dtypes.uint16)
UInt32 = register_scalar_type(UnsignedT, dtypes.uint32)
UInt64 = register_scalar_type(UnsignedT, dtypes.uint64)

class SignedT(IntT):
  pass 

Int8 = register_scalar_type(SignedT, dtypes.int8)
Int16 = register_scalar_type(SignedT, dtypes.int16) 
Int32 = register_scalar_type(SignedT, dtypes.int32)
Int64 = register_scalar_type(SignedT, dtypes.int64,
                             equiv_python_types = [int, long])

class Int24T(SignedT):
  """
  We don't actually support 24-bit integers, but they're useful  
  for implemented corner-case casting logic, such as summing booleans
  """
  def __init__(self):
    self.name = "int24"
    self.nbytes = 3
    self._hash = hash("int24")
    
  def combine(self, other):
    if isinstance(other, IntT):
      if other.nbytes <= self.nbytes:
        return Int32 
    return other 
  
  def __hash__(self):
    return self._hash 
  
  def __eq__(self, other):
    return self.__class__ is other.__class__ 
    
  def __ne__(self, other):
    return self.__class__ is not other.__class__
  
Int24 = Int24T()

class FloatT(ScalarT):
  pass 

Float32 = register_scalar_type(FloatT, dtypes.float32)
Float64 = register_scalar_type(FloatT, dtypes.float64,
                               equiv_python_types = [float])

def is_scalar_subtype(t1, t2):
  return isinstance(t1, ScalarT) and \
         isinstance(t2, ScalarT) and \
         ((t1 is t2) or 
          (t1 == t2) or 
          (t1.nbytes < t2.nbytes) or 
          (isinstance(t1, IntT) and isinstance(t2, FloatT)))

def from_dtype(dtype):
  if not isinstance(dtype, np.dtype):
    assert hasattr(dtype, 'dtype'), "Expected a dtype but %s" % dtype 
    value = dtype(0)
    dtype = value.dtype 
  parakeet_type = _dtype_to_parakeet_type.get(dtype)
  if parakeet_type is None:
    assert False, "Don't know how to make Parakeet scalar type from dtype %s" % dtype
  return parakeet_type

def from_char_code(c):
  numpy_type = np.typeDict[c]
  return from_dtype(np.dtype(numpy_type))

def is_scalar(t):
  return isinstance(t, ScalarT)

def all_scalars(ts):
  return all(is_scalar(t) for t in  ts)

########NEW FILE########
__FILENAME__ = slice_type
import type_conv

from core_types import StructT, ImmutableT, IncompatibleTypes

class SliceT(StructT, ImmutableT):
  def __init__(self, start_type, stop_type, step_type):
    self.start_type = start_type
    self.stop_type = stop_type
    self.step_type = step_type
    self._fields_ = [('start',start_type), ('stop', stop_type), ('step', step_type)]
    self._hash = hash((self.start_type, self.stop_type, self.step_type))
  
  def children(self):
    yield self.start_type
    yield self.stop_type
    yield self.step_type
    
  def __eq__(self, other):
    return self is other or \
      (other.__class__ is SliceT and
       self.start_type == other.start_type and
       self.stop_type == other.stop_type and
       self.step_type == other.step_type)

  def __hash__(self):
    return self._hash

  def combine(self, other):
    if self == other: return self
    else:raise IncompatibleTypes(self, other)

  def __str__(self):
    return "SliceT(%s, %s, %s)" % (self.start_type,
                                   self.stop_type,
                                   self.step_type)

  def __repr__(self):
    return str(self)

_slice_type_cache = {}
def make_slice_type(start_t, stop_t, step_t):
  key = (start_t, stop_t, step_t)
  if key in _slice_type_cache:
    return _slice_type_cache[key]
  else:
    t = SliceT(start_t, stop_t, step_t)
    _slice_type_cache[key] = t
    return t

def typeof_slice(s):
  start_type = type_conv.typeof(s.start)
  stop_type = type_conv.typeof(s.stop)
  step_type = type_conv.typeof(s.step)
  return make_slice_type(start_type, stop_type, step_type)

type_conv.register(slice, SliceT, typeof_slice)

########NEW FILE########
__FILENAME__ = tuple_type
import ctypes

import type_conv

from core_types import IncompatibleTypes, StructT, ImmutableT


class TupleT(StructT, ImmutableT):
  rank = 0
  def __init__(self, elt_types):
    if elt_types is None:
      elt_types = ()
    else:
      elt_types = tuple(elt_types)
    self.elt_types = elt_types
    self._fields_ = [("elt%d" % i, t) for (i,t) in enumerate(self.elt_types)]
    self._hash = hash(elt_types)
    
  # signals to type inference algorithm to pass the value of an index into
  # index_type rather than its type
  static_indexing = True

  def children(self):
    return self.elt_types
  
  def from_python(self, python_tuple, _keep_forever = []):
    # _keep_forever.append(python_tuple)
    converted_elts = []
    for elt in python_tuple:
      parakeet_type = type_conv.typeof(elt)
      c_elt = parakeet_type.from_python(elt)

      if isinstance(parakeet_type, StructT):
        c_elt = ctypes.pointer(c_elt)
      converted_elts.append(c_elt)
    return self.ctypes_repr(*converted_elts)

  def to_python(self, struct_obj):
    elt_values = []
    for (field_name, field_type) in self._fields_:
      c_elt = getattr(struct_obj, field_name)
      if isinstance(field_type, StructT):
        c_elt = c_elt.contents

      py_elt = field_type.to_python(c_elt)

      elt_values.append(py_elt)
    return tuple(elt_values)

  def dtype(self):
    raise RuntimeError("Do tuples have dtypes?")

  def __eq__(self, other):
    if self is other: return True 
    return other.__class__ is TupleT and self.elt_types == other.elt_types

  def __hash__(self):
    return self._hash 

  def __str__(self):
    return "tuple(%s)" % ", ".join([str(t) for t in self.elt_types])

  def __repr__(self):
    return str(self)

  def __len__(self):
    return len(self.elt_types)

  def __iter__(self):
    return iter(self.elt_types)
  
  def __getitem__(self, idx):
    assert isinstance(idx, (int,long)), "Invalid index for TupleT: %s" % idx 
    return self.elt_types[idx]

  def index_type(self, idx):
    #assert isinstance(idx, Expr), \
    #    "Tuple index not an expression: %s" % idx
    #assert isinstance(idx, Const), "Unsupported expression: %s" % idx
    idx = int(idx.value)
    assert 0 <= idx < len(self.elt_types), \
        "Can't get element %d from tuple of length %d" % \
        (idx, len(self.elt_types))
    return self.elt_types[idx]

  def combine(self, other):
    if isinstance(other, TupleT) and \
       len(other.elt_types) == len(self.elt_types):
      combined_elt_types = [t1.combine(t2) for
                            (t1, t2) in zip(self.elt_types, other.elt_types)]
      if combined_elt_types != self.elt_types:
        return TupleT(combined_elt_types)
      else:
        return self
    else:
      raise IncompatibleTypes(self, other)

_tuple_types = {}
def repeat_tuple(t, n):
  """Given the base type t, construct the n-tuple t*t*...*t"""

  elt_types = tuple([t] * n)
  tuple_t = _tuple_types.get(elt_types)
  if tuple_t is not None:
    return tuple_t 
  tuple_t = TupleT(elt_types)
  _tuple_types[elt_types] = tuple_t
  return tuple_t

def make_tuple_type(elt_types):
  """
  Use this memoized construct to avoid constructing too many distinct tuple type
  objects and speeding up equality checks
  """

  key = tuple(elt_types)
  tuple_t = _tuple_types.get(key)
  if tuple_t is not None:
    return tuple_t 
  tuple_t = TupleT(key)
  _tuple_types[key] = tuple_t
  return tuple_t

empty_tuple_t = make_tuple_type(())

########NEW FILE########
__FILENAME__ = type_conv

_type_mapping = {}
_typeof_functions = {}

# python type -> (python value -> internal value) 
_from_python_fns = {}
# class of ndtype -> (internal value -> python value)
_to_python_fns = {}
def register(python_types, 
              parakeet_type, 
              typeof = None, 
              to_python = None, 
              from_python = None):
  """
  Map each python type to either a parakeet type or a function that returns a
  parakeet type
  """

  if typeof is None:
    typeof = lambda _: parakeet_type

  if not isinstance(python_types, (list, tuple)):
    python_types = [python_types]

  for python_type in python_types:
    _typeof_functions[python_type] = typeof
    _type_mapping[python_type] = parakeet_type
    
  if to_python is not None:
    _to_python_fns[parakeet_type] = to_python
           
  if from_python is not None:
    for python_type in python_types:
      _from_python_fns[python_type] = from_python     
     
  
def equiv_type(python_type):
  assert python_type in _type_mapping, \
      "No type mapping found for %s" % python_type
  return _type_mapping[python_type]

class InvalidType(Exception):
  def __init__(self, python_value, python_type):
    self.python_value = python_value
    self.python_type = python_type 
    
  def __str__(self):
    return "Don't know how to convert value %s : %s" % (self.python_value, self.python_type)

def typeof(python_value):
  python_type = type(python_value)
  try:
    parakeet_type_lookup = _typeof_functions[python_type]
  except:
    raise InvalidType(python_value, python_type)
  return parakeet_type_lookup(python_value)
  
def from_python(python_value):
  """
  Look up the ctypes representation of the corresponding parakeet type and call
  the converter with the ctypes class and python value
  """
  python_type = type(python_value)
  if python_type in _from_python_fns:
    return _from_python_fns[python_type](python_value)
  parakeet_type = typeof(python_value)
  return parakeet_type.from_python(python_value)

def to_python(internal_value, parakeet_type):
  if parakeet_type.__class__ in _to_python_fns:
    return _to_python_fns[parakeet_type.__class__](internal_value)
  return parakeet_type.to_python(internal_value)
  

########NEW FILE########
__FILENAME__ = config
collapse_nested_loops = True
schedule = 'static'

########NEW FILE########
__FILENAME__ = multicore_compiler
import multiprocessing 

from .. import prims 
from ..syntax import Expr, Tuple, Assign, Return, Var, PrimCall 
from ..syntax.helpers import get_fn, return_type
from ..ndtypes import ScalarT, TupleT, ArrayT
from ..c_backend import PyModuleCompiler

import config 

class MulticoreCompiler(PyModuleCompiler):
  
  def __init__(self, depth = 0, *args, **kwargs):
    self.depth = depth
    self.seen_parfor = None 
    PyModuleCompiler.__init__(self, *args, **kwargs)
  
  @property 
  def cache_key(self):
    return self.__class__, self.depth > 0
  
  _loop_var_names = ["i","j","k","l","a","b","c","ii","jj","kk","ll","aa","bb","cc"] 
  def loop_vars(self, count, init_value = "0"):
    assert count <= len(self._loop_var_names)
    return [self.fresh_var("int64_t", self._loop_var_names[i], init_value) 
            for i in xrange(count)]
       
  def visit_NumCores(self, expr):
    # by default we're running sequentially 
    return "%d" % multiprocessing.cpu_count()
  
  def tuple_to_var_list(self, expr):
    assert isinstance(expr, Expr)
    if isinstance(expr, Tuple):
      return self.visit_expr_list(expr)
    elif isinstance(expr.type, TupleT):
      c_val = self.visit_expr(expr)
      return ["%s.elt%d" % (c_val, i) for i in xrange(len(expr.type.elt_types))]
    else:
      assert isinstance(expr.type, ScalarT), "Unexpected expr %s : %s" % (expr, expr.type)
      return [self.visit_expr(expr)]
  
  
  def get_fn_name(self, fn_expr, attributes = [], inline = True):
    return PyModuleCompiler.get_fn_name(self, fn_expr, 
                                        compiler_kwargs = {'depth':self.depth}, 
                                        attributes = attributes, 
                                        inline = inline)
  
  def get_fn_info(self, fn_expr, attributes = [], inline = True):
    """
    Given a function expression, return:
      - the name of its C representation
      - C expressions representing its closure arguments
      - Parakeet input types 
    """

    fn_name = self.get_fn_name(fn_expr, attributes = attributes, inline = inline)
    closure_args = self.get_closure_args(fn_expr)
    root_fn = get_fn(fn_expr)
    input_types = root_fn.input_types 
    return fn_name, closure_args, input_types
    
  
  def build_loop_body(self, fn_expr, loop_vars, target_name = None):
    """
    Inside a loop nest, construct an index tuple, call the given function, 
    and optionally assign its result to the target variable. 
    Returns the string representation of the loop body and the set of
    private variables it uses. 
    """
    fn_name, closure_args, input_types = self.get_fn_info(fn_expr)

    private_vars = [loop_var for loop_var in loop_vars] 
    last_input_type = input_types[-1]
    body = ""
    if isinstance(last_input_type, TupleT):
      # make a tupke out of the loop variables
      
      c_tuple_t = self.to_ctype(last_input_type)
      index_tuple = self.fresh_var(c_tuple_t, "index_tuple")
      private_vars.append(index_tuple)
      for i, loop_var in enumerate(loop_vars):
        body += "\n%s.elt%d = %s;\n" % (index_tuple, i, loop_var)
      combined_args = tuple(closure_args) + (index_tuple,)
      call = "%s(%s)" % (fn_name, ", ".join(combined_args))
    else:
      combined_args = tuple(closure_args) + tuple(loop_vars)
      call = "%s(%s)" % (fn_name, ", ".join(combined_args))
    if target_name:
      body += "%s = %s;\n" % (target_name, call)
    else:
      body += "\n%s;\n" % call
    return body, private_vars
  
  
  def enter_parfor(self):
    self.depth += 1
    if not self.seen_parfor:
      self.seen_parfor = True 
      self.add_compile_flag("-fopenmp")
      self.add_link_flag("-fopenmp")
    
  def exit_parfor(self):
    self.depth -= 1

  def omp_pragma(self, n_loops, private_vars, reduce_op = None, reduce_vars = None):
    if config.collapse_nested_loops:
      omp = "#pragma omp parallel for private(%s) schedule(%s)" % \
        (", ".join(private_vars), config.schedule)
      if n_loops > 1:
        omp += " collapse(%d)" % n_loops
    else:
      omp = "#pragma omp parallel for private(%s) schedule(%s)" % \
          (private_vars[0], config.schedule)
    
    if reduce_op:
      omp += " reduction (%s:%s)" % (reduce_op, ", ".join(reduce_vars))
    return omp 
  
  def visit_ParFor(self, stmt):
    bounds = self.tuple_to_var_list(stmt.bounds)
    n_vars = len(bounds)
    loop_vars = self.loop_vars(n_vars)
    
    self.enter_parfor()
    body, private_vars = self.build_loop_body(stmt.fn, loop_vars)
    loops = self.build_loops(loop_vars, bounds, body)
    self.exit_parfor()
    
    if self.depth == 0:  
      release_gil = "\nPy_BEGIN_ALLOW_THREADS\n"
      acquire_gil = "\nPy_END_ALLOW_THREADS\n" 
      omp = self.omp_pragma(len(loop_vars), private_vars)
      return release_gil + omp + loops + acquire_gil    
    else:
      return loops 
     
  
  
  def get_binop_prim(self, fn):
    """
    If function is a simple binary operator, then return its prim, 
    otherwise return None
    """

    if len(fn.input_types) != 2:
      return None
      
    t1, t2 = fn.input_types
    if t1 != t2:
      return None 
    if not isinstance(t1, ScalarT):
      return None 
    if not isinstance(t2, ScalarT):
      return None 
    body = fn.body

    if len(body) == 2:
      s1 = body[0]
      s2 = body[1]
      if s1.__class__ is not Assign:
        return None
      rhs = s1.rhs
      if rhs.__class__ is not PrimCall:
        return None 
      args = rhs.args 
      if len(args) != 2:
        return None
      x,y = args 
      if x.__class__ is not Var or y.__class__ is not Var:
        return None 
      if x.name != fn.arg_names[0] or y.name != fn.arg_names[1]:
        return None 
      if s2.__class__ is not Return:
        return None 
      if s2.value.__class__ is not Var:
        return None 
      if s2.value != s1.lhs:
        return None
      return s1.rhs.prim
    elif len(body) == 1:
      s = body[0]
      if s.__class__ is not Return:
        return None
      v = s.value 
      if v.__class__ is not PrimCall:
        return None 
      args = v.args 
      if len(args) != 2:
        return None
      x,y = args 
      if x.__class__ is not Var or y.__class__ is not Var:
        return None 
      if x.name != fn.arg_names[0] or y.name != fn.arg_names[1]:
        return None 
      return v.prim 
       
    
      
  def visit_IndexReduce(self, expr):
    """
    For now, just use a sequential implementation for reductions
    """
    bounds = self.tuple_to_var_list(expr.shape)
    n_vars = len(bounds)
    acc = self.fresh_var(expr.type, "acc", self.visit_expr(expr.init))
    # try to get a simple primitive to use as the OpenMP combiner
    # if this isn't possible then run the loops sequentially
    #
    # TODO: instead of falling back on sequential execution 
    # use complex combiners by generating one 
    # accumulator value per thread and then combining those
    combine_prim = self.get_binop_prim(expr.combine)
    if combine_prim is prims.add:
      omp_reduce_op = "+"
    elif combine_prim is prims.multiply:
      omp_reduce_op = "*"
    elif combine_prim is prims.logical_and:
      omp_reduce_op = "&&"
    elif combine_prim is prims.logical_or:
      omp_reduce_op = "||"
    else:
      omp_reduce_op = None 
 
    loop_vars = self.loop_vars(n_vars)
    assert expr.init is not None, "Accumulator required but not given"
    elt = self.fresh_var(return_type(expr.fn), "elt")
    if omp_reduce_op: self.enter_parfor()
    body, private_vars = self.build_loop_body(expr.fn, loop_vars, target_name = elt)
    if omp_reduce_op:
      body += "\n%s = %s %s %s;" % (acc, acc, omp_reduce_op, elt)
    else:
      combine_name, combine_closure_args, _ = self.get_fn_info(expr.combine)
      combine_arg_str = ", ".join(tuple(combine_closure_args) + (acc, elt))
      body += "\n%s = %s(%s);\n" % (acc, combine_name, combine_arg_str)
    loops = self.build_loops(loop_vars, bounds, body)
    if omp_reduce_op: 
      self.exit_parfor()

    if omp_reduce_op and self.depth == 0:
      release_gil = "\nPy_BEGIN_ALLOW_THREADS\n"
      acquire_gil = "\nPy_END_ALLOW_THREADS\n" 
      omp = self.omp_pragma(len(loop_vars), private_vars, 
                            reduce_op = omp_reduce_op, 
                            reduce_vars = [acc])
      loops = release_gil + omp + loops + acquire_gil    
    self.append(loops)
    return acc 
    
  def visit_IndexScan(self, expr):
    """
    For now, just use a sequential implementation for scans
    """
    assert isinstance(expr.type, ArrayT), "Expected output of Scan to be an array"
    
    bounds = self.tuple_to_var_list(expr.shape)
    n_vars = len(bounds)
    
    combine_name, combine_closure_args, _ = self.get_fn_info(expr.combine)
    loop_vars = self.loop_vars(n_vars)
    
    
    result = self.alloc_array(expr.type, expr.shape)
    
    assert expr.init is not None, "Accumulator required but not given"
    
    elt_t = return_type(expr.fn) 
    assert isinstance(elt_t, ScalarT), "Scans of non-scalar values (%s) not yet implemented" % elt_t
    elt = self.fresh_var(elt_t, "elt")
    body, _ = self.build_loop_body(expr.fn, loop_vars, target_name = elt)
    acc = self.fresh_var(expr.init.type, "acc", self.visit_expr(expr.init))
    combine_arg_str = ", ".join(tuple(combine_closure_args) + (acc, elt))
    body += "\n%s = %s(%s);\n" % (acc, combine_name, combine_arg_str)
    emit_name, emit_closure_args, _ = self.get_fn_info(expr.emit)
    body += "\n"
    emit_args = tuple(emit_closure_args) + (acc,)
    emit_args_str = ", ".join(emit_args)
    body += self.setidx(result, 
                        loop_vars, 
                        "%s(%s)" % (emit_name, emit_args_str), 
                        full_array = True, 
                        return_stmt = True)
    self.append(self.build_loops(loop_vars, bounds, body))
    return result
    
  def visit_Map(self, expr):
    assert False, "Map should have been lowered into ParFor by now: %s" % expr 
  
  def visit_OuterMap(self, expr):
    assert False, "OuterMap should have been lowered into ParFor by now: %s" % expr 
  
  def visit_Reduce(self, expr):
    assert False, "Reduce should have been lowered into ParFor by now: %s" % expr 
  
  def visit_Scan(self, expr):
    assert False, "Scan should have been lowered into ParFor by now: %s" % expr
    
  def visit_IndexMap(self, expr):
    assert False, "IndexMap should have been lowered into ParFor by now: %s" % expr 
  

########NEW FILE########
__FILENAME__ = run_function
from .. import config 

from ..c_backend.prepare_args import prepare_args  
from ..transforms.pipeline import lower_to_adverbs  
from ..value_specialization import specialize


from multicore_compiler import MulticoreCompiler 

_cache = {}
def run(fn, args):
  args = prepare_args(args, fn.input_types)
  fn = lower_to_adverbs.apply(fn)
  if config.value_specialization:
    fn = specialize(fn, python_values = args)
  key = fn.cache_key 
  if key in _cache:
    return _cache[key](*args)
  else:
    compiled_fn = MulticoreCompiler().compile_entry(fn)
    c_fn = compiled_fn.c_fn 
    _cache[key] = c_fn 
    return c_fn(*args)
  
########NEW FILE########
__FILENAME__ = package_info
__author__  = 'Alex Rubinsteyn'
__email__   = 'alex -dot- rubinsteyn -at- gmail -dot- com'
__desc__    = 'Runtime compiler for numerical Python'
__license__     = 'BSD3'
__version__     = '0.24'
__website__     = 'http://www.parakeetpython.com' 


########NEW FILE########
__FILENAME__ = prims


import numpy as np
import ndtypes 

from ndtypes import Bool, FloatT, BoolT, Float32, Float64, ScalarT 

prim_lookup_by_value = {}

def find_prim_from_python_value(fn):
  return prim_lookup_by_value[fn]

prim_lookup_by_op_name = {}

def op_name(op):
  return op.__class__.__name__

def is_ast_op(op):
  return op_name(op) in prim_lookup_by_op_name

def find_ast_op(op):
  name = op_name(op)
  if name in prim_lookup_by_op_name:
    return prim_lookup_by_op_name[name]
  else:
    raise RuntimeError("Operator not implemented: %s" % name)

def is_prim(numpy_fn):
  return numpy_fn in prim_lookup_by_value


class Prim(object):
  def __init__(self, fn, python_op_name = None, symbol = None,
               name = None, nin = None, nout = None, 
               extra_signatures = [], 
               doc = None):
    if doc is not None:
      self.__doc__ = doc 
      
    self.fn = fn
    prim_lookup_by_value[fn] = self
    self.symbol = symbol
    self.python_op_name = python_op_name

    if python_op_name is not None:
      prim_lookup_by_op_name[python_op_name] = self

    if name:
      self.name = name
    else:
      self.name = fn.__name__

    if nin:
      self.nin = nin
    elif hasattr(fn, 'nin'):
      self.nin = fn.nin
    else:
      assert hasattr(fn, 'func_code')
      self.nin = fn.func_code.co_argcount

    if nout:
      self.nout = nout
    elif hasattr(fn, 'nout'):
      self.nout = fn.nout
    else:
      self.nout = 1

    self._create_type_table()
    for sig in extra_signatures:
      self._add_signature(sig)
    # table mapping mismatching types i.e. (Int32, Float64) to (Float64, Float64)
    self._upcast_types = {}


  def _add_signature(self, signature):
    # numpy type signatures look like 'ff->f' where each character
    # represents a single type

    arg_codes, result_code = signature.split('->')
    try:
      parakeet_types = [ndtypes.from_char_code(c) for c in arg_codes]
      input_types = tuple(parakeet_types)
      result_type = ndtypes.from_char_code(result_code)
      self.type_table[input_types] = result_type
    except:
      # print "Signature %s failed  for %s" % (signature , self.fn)
      pass
  
  
  def _create_type_table(self):
    # for now only support ufuncs which describe their own type behavior
    if hasattr(self.fn, 'types'):
      "Primitive function %s doesn't supply type signatures" % self.name

      self.type_table = {}
      for signature in self.fn.types:
        self._add_signature(signature)

  def __eq__(self, other):
    return self.fn == other.fn

  def __hash__(self):
    return hash(self.name)

  def __call__(self, *args, **kwds):
    return self.fn(*args, **kwds)

  def __repr__(self):
    return "prim(%s)" % self.name

  def _signature_distance(self, types1, types2):
    dist = 0
    for (t1, t2) in zip(types1, types2):
      if t1 != t2:
        assert isinstance(t1, ScalarT), "Expected scalar type but got %s" % t1
        assert isinstance(t2, ScalarT), "Expected scalar type but got %s" % t2
        # penalty just for being a different type   
        dist += 1 
        
        size_difference = t2.nbytes - t1.nbytes
        
        # if we're downcasting, this sucks 
        if size_difference < 0:
          dist += 10000
        # going from int to float of same type is mildly unsafe  
        elif size_difference > 0:
          dist += np.log2(1 + size_difference)
        elif size_difference == 0:
          if isinstance(t2, FloatT) and not isinstance(t1, FloatT):
            dist += 10
        # can't go from float to int 
        if isinstance(t1, FloatT) and not isinstance(t2, FloatT):
          dist += 1000
        # but going to from int to float is only minor penalty...
        elif isinstance(t2, FloatT) and not isinstance(t1, FloatT):
          dist += 1

        # can't go from int to bool 
        if isinstance(t1, BoolT) and not isinstance(t2, BoolT):
          dist += 1
        elif isinstance(t2, BoolT) and not isinstance(t1, BoolT):
          dist += 1000
    return dist 
  
  def expected_input_types(self, arg_types):
    """Given some argument types, return the desired upcast types"""
    # by default we just figure out the common type and expect every arg to be
    # of that type
    n_inputs = len(arg_types)
    assert n_inputs == self.nin, \
        "Incorrect number of argument types for %s, expected %s but given %d" \
        % (self.name, self.nin, n_inputs)
    
    arg_types = tuple(arg_types)
    if arg_types in self.type_table:
      return arg_types
    elif arg_types in self._upcast_types:
      return self._upcast_types[arg_types]
    else:
      assert all(isinstance(t, ScalarT) for t in arg_types), \
        "Prim %s expects scalar inputs but given %s" % (self, arg_types)
      # search over all possible signatures to figure out 
      best_upcast_types = None
      best_distance = np.inf 
      for candidate_types in self.type_table:
        dist = self._signature_distance(arg_types, candidate_types)
        if dist < best_distance:
          best_distance = dist
          best_upcast_types = candidate_types
      self._upcast_types[arg_types] = best_upcast_types
      return best_upcast_types  
    #common_type = combine_type_list(arg_types)
    #return [common_type] * n_inputs

  def result_type(self, arg_types):
    """
    Given some argument types, look up the result type in the type_table we
    generated from numpy's given signatures
    """
    key = tuple(arg_types)
    if key not in self.type_table:
      raise RuntimeError("Primitives %s doesn't support input types %s, candidates: %s" % (self.name, key, self.type_table))
    else:
      return self.type_table[key]

class Float(Prim):
  """Always returns a float"""
  def expected_input_types(self, arg_types):
    assert all(isinstance(t, ScalarT) for t in arg_types), \
        "Prim %s expects scalar inputs but given %s" % (self, arg_types)
    max_nbytes = max(t.nbytes for t in arg_types)
    if max_nbytes <= 4:
      upcast_types = [Float32.combine(t) for t in arg_types]
    else:
      upcast_types = [Float64.combine(t) for t in arg_types]
    return Prim.expected_input_types(self, upcast_types)
  
  def result_type(self, arg_types):
    t = Prim.result_type(self, arg_types)
    return t.combine(Float32)
  
class Arith(Prim):
  """Basic arithmetic operators"""
  pass

class Logical(Prim):
  """Expects boolean inputs, returns a boolean"""
  def expected_input_types(self, arg_types):
    return [Bool] * len(arg_types)

class Bitwise(Prim):
  """Takes any two identical scalar types, returns the same"""
  pass

class Cmp(Prim):
  """Takes two arguments of any type, returns a boolean"""
  pass

class Round(Prim):
  """
  Rounding operations
  """
  pass

class_list = [Cmp, Bitwise, Logical, Arith, Float, Round]

abs = Float(np.abs, doc = "Absolute value")
sqrt = Float(np.sqrt)

exp = Float(np.exp)
exp2 = Float(np.exp2)
expm1 = Float(np.expm1)

log = Float(np.log)
log10 = Float(np.log10)
log2 = Float(np.log2)
log1p = Float(np.log1p)

cos = Float(np.cos)
cosh = Float(np.cosh)
arccos = Float(np.arccos)
arccosh = Float(np.arccosh)

sin = Float(np.sin)
sinh = Float(np.sinh)
arcsin = Float(np.arcsin)
arcsinh = Float(np.arcsinh)

# TODO: figure out how to derive type table for this:
# sinc = Float(np.sinc)

tan = Float(np.tan)
tanh = Float(np.tanh)
arctan = Float(np.arctan)
arctan2 = Float(np.arctan2)
arctanh = Float(np.arctanh)

logical_and = Logical(np.logical_and, "And")
logical_not = Logical(np.logical_not, "Not")
logical_or = Logical(np.logical_or, "Or")
#logical_xor = Logical(np.logical_xor, 'BitXor')

bitwise_not = Bitwise(np.bitwise_not, 'Invert', '!')
bitwise_and = Bitwise(np.bitwise_and, 'BitAnd', '&')
bitwise_or = Bitwise(np.bitwise_or, 'BitOr', '|')
bitwise_xor = Bitwise(np.bitwise_xor, 'BitXor', '^')

# Adding booleans in Python results in an integer, 
# but add *arrays* of booleans gives a boolean result
# ----
# Since Parakeet unifies the scalar and broadcast behavior of 
# primitive functions, I had to pick one of these two behaviors
# and went with Boolean + Boolean = Integer (since np.mean(bool) is useful) 
add = Arith(np.add, 'Add', '+')

subtract = Arith(np.subtract, 'Sub', '-')
multiply = Arith(np.multiply, 'Mult', '*')


divide = Arith(np.divide, 'Div', '/', extra_signatures = ['??->?'])

remainder = Arith(np.remainder, 'Mod', '%', extra_signatures = ['??->?'])
mod = remainder 
fmod = Arith(np.fmod, doc = "Return the element-wise remainder of division. C-style modulo.")


# used to be Arith but easier if result is always floating point 
power = Float(np.power, 'Pow', '**')
# power_int = Arith(np.power, extra_signatures = [''])


negative = Arith(np.negative, 'USub', '-', None, 1, 1)
maximum = Arith(np.maximum, None, None, 'maximum', 2, 1)
minimum = Arith(np.minimum, None, None, 'minimum', 2, 1)


equal = Cmp(np.equal, 'Eq', '==')
not_equal = Cmp(np.not_equal, 'NotEq', '!=')
less = Cmp(np.less, 'Lt', '<')
less_equal = Cmp(np.less_equal, 'LtE', '<=')
greater = Cmp(np.greater, 'Gt', '>')
greater_equal = Cmp(np.greater_equal, 'GtE', '>=')

is_ = Cmp(lambda x,y: x is y, 'Is', 'is')

trunc = Round(np.trunc)
rint = Round(np.rint)
floor = Round(np.floor)
ceil = Round(np.ceil)
round = Round(np.round) 



########NEW FILE########
__FILENAME__ = shape
class ValueMismatch(Exception):
  """
  Raise this exception whenever two incompatible abstract values get combined
  """

  def __init__(self, v1, v2):
    self.v1 = v1
    self.v2 = v2

  def __str__(self):
    return "ValueMismatch(%s, %s)" % (self.v1, self.v2)

  def __repr__(self):
    return str(self)

class AbstractValue(object):
  def combine(self, other):
    if self == other:
      return self
    else:
      return unknown_value
    
  def __repr__(self):
    return str(self)

class Unknown(object):
  """Bottom of the abstract shape lattice"""

  def __eq__(self, other):
    return other.__class__ is Unknown

  def combine(self, other):
    return other

  def __str__(self):
    return "<unknown>"

  def __repr__(self):
    return str(self)

unknown_value = Unknown()

class AnyValue(object):
  """Top of the abstract shape lattice"""

  def __eq__(self, other):
    return other.__class__ is AnyValue

  def combine(self, other):
    return self

  def __str__(self):
    return "<any>"
  
  def __repr__(self):
    return str(self)

any_value = AnyValue()

class Scalar(AbstractValue):
  """Base class for all scalar operations"""

  def combine(self, other):
    if self == other:
      return self
    else:
      return any_scalar
  rank = 0

class AnyScalar(Scalar):
  def __eq__(self, other):
    return other.__class__ is AnyScalar

  def combine(self, other):
    assert isinstance(other, Scalar), \
        "Can't combine scalar with %s" % other
    return self

  def __str__(self):
    return "Scalar"
  
  def __repr__(self):
    return str(self)

any_scalar = AnyScalar()

class Const(Scalar):
  def __init__(self, value):
    self.value = value

  def __eq__(self, other):
    return isinstance(other, Const) and other.value == self.value

  def __str__(self):
    return str(self.value)

  def combine(self, other):
    if self == other:
      return self
    elif isinstance(other, Scalar):
      return any_scalar
    else:
      raise ValueMismatch(self, other)

def const(x):
  return Const(x)

def is_zero(d):
  return d.__class__ is Const and d.value == 0

def is_one(d):
  return d.__class__ is Const and d.value == 1

def is_none(d):
  return d.__class__ is Const and d.value is None

class Binop(Scalar):
  def __init__(self, x, y):
    self.x = x
    self.y = y

  def __eq__(self, other):
    return self.__class__ == other.__class__ and \
           self.x == other.x and \
           self.y == other.y

  def __repr__(self):
    return str(self)

  def __hash__(self):
    return hash( (self.x, self.y) )

class Sub(Binop):
  def __str__(self):
    return "%s - %s" % (self.x, self.y)

class Add(Binop):
  def __str__(self):
    return "%s + %s" % (self.x, self.y)

class Div(Binop):
  def __str__(self):
    return "%s / %s" % (self.x, self.y)

class Mult(Binop):
  def __str__(self):
    return "%s * %s" % (self.x, self.y)

class Mod(Binop):
  def __str__(self):
    return "%s % %s" % (self.x, self.y)

class Var(Scalar):
  def __init__(self, num):
    self.num = num

  def __eq__(self, other):
    return other.__class__ is Var and\
           self.num == other.num

  def __hash__(self):
    return hash(self.num)

  def __str__(self):
    return "x%d" % self.num

  def __repr__(self):
    return str(self)

  def combine(self, other):
    if self == other:
      return self
    else:
      # combining two different variables returns an unknown scalar
      return any_scalar

class Shape(AbstractValue):
  def __init__(self, dims):
    assert len(dims) > 0
    self.dims = [const(d) if isinstance(d, int) else d for d in dims]
    self.rank = len(dims)

  def __eq__(self, other):
    return other.__class__ is  Shape and \
           len(self.dims) == len(other.dims) and \
           all(d1 == d2 for (d1,d2) in zip(self.dims, other.dims) )

  def __str__(self):
    return "Shape(%s)" % (", ".join(str(d) for d in self.dims))

  def __repr__(self):
    return str(self)

  def combine(self, other):
    if isinstance(other, Shape) and other.rank == self.rank:
      dims = combine_pairs(self.dims, other.dims)
      return make_shape(dims)
    raise ValueMismatch(self, other)

def make_shape(dims):
  if len(dims) == 0:
    return any_scalar
  return Shape(tuple(dims))

def dim(shape, d):
  if isinstance(shape, Shape):
    return shape.dims[d]
  else:
    # if the shape isn't available, getting the d'th
    # dimension returns an unknown scalar
    return any_scalar

def dim_list(shapes, d, exclude_scalars=False):
  if exclude_scalars:
    shapes = [s for s in shapes if not is_scalar(s)]
  return [dim(s,d) for s in shapes]

def array_of_unknown_shape(rank):
  return Shape([any_scalar] * rank)

def lower_rank(x, axis):
  assert isinstance(x, Shape), "Can't decrease rank of %s" % x
  # by convention, lowering a scalar returns a scalar
  if  axis >= x.rank or x.rank == 1:
    return any_scalar

  new_dims = []
  for (i,d) in enumerate(x.dims):
    if i != axis:
      new_dims.append(d)
  return make_shape(new_dims)

def lower_ranks(xs, axis):
  return [lower_rank(x, axis) for x in xs]

def increase_rank(x, axis, dim_expr):
  if isinstance(dim_expr, int):
    dim_expr = const(dim_expr)

  if isinstance(x, Shape):
    # we're taking dims d1...dm and constructing a
    # new shape d1...d_new...dm by inserting d_new
    # in the slot of the given axis
    assert axis <= x.rank
    if axis < len(x.dims):
      new_dims = []
      for (i,d) in enumerate(x.dims):
        if i == axis:
          new_dims.append(dim_expr)
        new_dims.append(d)
    else:
      new_dims = [d for d in x.dims]
      new_dims.append(dim_expr)
    return make_shape(new_dims)
  elif is_scalar(x):
    return Shape([dim_expr])
  else:
    raise RuntimeError("Don't know how to raise rank of value %s" % x)



def is_scalar(v):
  return isinstance(v, Scalar)

class ConstSlice(AbstractValue):
  def __init__(self, nelts):
    self.nelts = nelts

  def __str__(self):
    return "ConstSlice(nelts = %d)" % self.nelts

  def __eq__(self, other):
    return other.__class__ is ConstSlice and \
           other.nelts == self.nelts

  def combine(self, other):
    if other.__class__ is ConstSlice and other.nelts == self.nelts:
      return self
    else:
      return any_slice

class Slice(AbstractValue):
  def __init__(self, start, stop, step):
    self.start = start
    self.stop = stop
    self.step = step

  def __eq__(self, other):
    return other.__class__ is Slice and \
           self.start == other.start and \
           self.stop == other.stop and \
           self.step == other.step

  def __str__(self):
    return "Slice(%s, %s, %s)" % (self.start, self.stop, self.step)

  def combine(self, other):
    if isinstance(other, Slice):
      start = self.start.combine(other.start)
      stop = self.stop.combine(other.stop)
      step = self.step.combine(other.step)
      return Slice(start, stop, step)
    else:
      raise ValueMismatch(self, other)

any_slice = Slice(any_scalar, any_scalar, any_scalar)

class Tuple(AbstractValue):
  def __init__(self, elts):
    self.elts = tuple(elts)

  def __eq__(self, other):
    return other.__class__ is Tuple and \
           len(self.elts) == len(other.elts) and \
           all(e1 == e2 for (e1, e2) in zip(self.elts, other.elts))

  def __len__(self):
    return len(self.elts)

  def __iter__(self):
    return iter(self.elts)

  def __str__(self):
    return "Tuple(%s)" % ", ".join(str(e) for e in self.elts)

  def __getitem__(self, idx):
    return self.elts[idx]

  def combine(self, other):
    if other.__class__ is Tuple:
      if len(self.elts) == len(other.elts):
        return Tuple(combine_pairs(self.elts, other.elts))
    raise ValueMismatch(self, other)

class Ptr(AbstractValue):
  def __init__(self, elt_shape):
    self.elt_shape = elt_shape
    
  def __str__(self):
    return "Ptr(%s)" % self.elt_shape
  
  def __eq__(self, other):
    return other.__class__ is Ptr and self.elt_shape == other.elt_shape
  
  def combine(self, other):
    if self == other:
      return self
    raise ValueMismatch(self, other)


class Closure(AbstractValue):
  def __init__(self, fn, args):
    self.fn = fn
    self.args = args

  def __str__(self):
    return "Closure(fn = %s, %s)" % \
           (self.fn, ", ".join(str(a) for a in self.args))

  def __eq__(self, other):
    return other.__class__ is Closure and \
           self.fn == other.fn and \
           len(self.arg_shapes) == len(other.arg_shapes) and \
           all(v1 == v2 for (v1,v2) in zip(self.args, other.args))

  def combine(self, other):
    if other.__class__ is  Closure:
      # TODO: Implement sets of closures like we have in the type system
      if self.fn == other.fn and \
         len(self.args) == len(other.args):
        combined_args = combine_pairs(self.args, other.args)
        return Closure(self.fn, combined_args)
    raise ValueMismatch(self, other)

class Struct(AbstractValue):
  def __init__(self, field_names, field_values):
    self.fields = field_names
    self.values = field_values

  def __str__(self):
    field_strings = ["%s = %s" % (k,v)
                     for (k,v) in zip(self.fields, self.values)]
    return "Struct(%s)" % ", ".join(field_strings)

  def __eq__(self, other):
      return other.__class__ is Struct and \
             len(self.fields) == len(other.fields) and \
             all(n1 == n2 for (n1,n2) in zip(self.fields, other.fields)) and \
             all(v1 == v2 for (v1, v2) in zip(self.values, other.values))

  def combine(self, other):
    if other.__class__ is  Struct and \
       len(self.fields) == len(other.fields) and \
       all(n1 == n2 for (n1,n2) in zip(self.fields, other.fields)):
      combined_args = combine_pairs(self.values, other.values)
      if any(old_val != new_val
             for (old_val, new_val) in zip(self.values, combined_args)):
        return Struct(self.fields, combined_args)
      else:
        return self
    raise ValueMismatch(self, other)

def combine_list(xs, preserve_const = True):
  acc = unknown_value
  for x in xs:
    acc = acc.combine(x)
  if not preserve_const and isinstance(acc, Const):
    acc = any_scalar
  return acc

def combine_pairs(xs, ys):
  return [xi.combine(yi) for (xi, yi) in zip(xs, ys)]

def computable_dim(d):
  c = d.__class__ 
  return c is Var or c is Const or \
    (isinstance(d, Binop) and computable_dim(d.x) and computable_dim(d.y))


def dims(v):
  if isinstance(v, Shape):
    return tuple(v.dims) 
  elif isinstance(v, Tuple):
    return tuple(v.elts) 
  else:
    return ()

def combine_dims(v1, v2):
  return dims(v1) + dims(v2)
########NEW FILE########
__FILENAME__ = shape_codegen
from dsltools import Traversal 

from ..syntax import const  
from ..ndtypes import ArrayT, ClosureT, ScalarT, TupleT 
from shape import Closure, Scalar, Var, AnyScalar, any_scalar, computable_dim

class ArgConverter(Traversal):
  def __init__(self, codegen):
    self.codegen = codegen
    self.var_counter = 0
    self.env = {}

  def fresh_var(self):
    n = self.var_counter
    self.var_counter += 1
    return Var(n)

  def bind(self, scalar_value):
    v = self.fresh_var()
    self.env[v] = scalar_value

  def convert(self, x):
    t = x.type
    if isinstance(t, ScalarT):
      self.bind(x)
    elif isinstance(t, ArrayT):
      shape = self.codegen.shape(x)
      shape_elts = self.codegen.tuple_elts(shape)
      return self.convert_list(shape_elts)
    elif isinstance(t, TupleT):
      elts = self.codegen.tuple_elts(x)
      self.convert_list(elts)
    elif isinstance(t, ClosureT):
      closure_elts = self.codegen.closure_elts(x)
      return Closure(t.fn, self.convert_list(closure_elts))
    else:
      assert False, "[shape_codegen] Not supported %s : %s" % (x,x.type)

  def convert_list(self, xs):
    for x in xs:
      self.convert(x)

class ShapeCodegen(Traversal):
  def __init__(self, codegen, exprs):
    self.codegen = codegen
    conv = ArgConverter(codegen)
    self.exprs = exprs
    conv.convert_list(exprs)
    self.env = conv.env

  def visit_Var(self, v):
    return self.env[v]

  def visit_Const(self, v):
    return const(v.value)

  def visit_Shape(self, v):
    assert len(v.dims) > 0, "Encountered empty shape"
    assert all(computable_dim(d) for d in v.dims), \
        "Symbolic shape '%s' has non-constant dimensions" % (v,)
    return self.codegen.tuple([self.visit(d) for d in v.dims])

  def visit_Dim(self, v):
    return self.codegen.tuple_proj(self.visit(v.array), v.dim)

  def visit_AnyScalar(self, v):    
    assert False, "Can't generate shape expression for unknown scalar"

  def visit_Tuple(self, v):
    # a tuple is effectively a scalar
    return self.codegen.tuple([])
    # return self.codegen.tuple([self.visit(e) for e in v.elts])

  def binop(self, op_name, v):
    if v.x.__class__ is AnyScalar or v.y.__class__ is AnyScalar:
      return any_scalar 
    x = self.visit(v.x)
    y = self.visit(v.y)
    op = getattr(self.codegen, op_name)
   
    return op(x,y)

  def visit_Sub(self, v):
    return self.binop('sub', v)

  def visit_Add(self, v):
    return self.binop('add', v)

  def visit_Mult(self, v):
    return self.binop('mult', v)

  def visit_Div(self, v):
    return self.binop('div', v)

  def visit_Mod(self, v):
    return self.binop('mod', v)

  def visit_Closure(self, v):
    assert False, "Unexpected closure in result shape: %s" % (v,)

def make_shape_expr(codegen, symbolic_shape, input_exprs):
  
  """
  Given a codegen object we're currently using to create a function, and a
  symbolic result shape of a function call (along with the input expressions
  that went into the function) generate a code expression for the shape of the
  result
  """

  if isinstance(symbolic_shape, Scalar):
    return codegen.tuple([])

  shape_codegen = ShapeCodegen(codegen, input_exprs)
  return shape_codegen.visit(symbolic_shape)

########NEW FILE########
__FILENAME__ = shape_eval
from dsltools import Traversal
 
import types 
import numpy as np 

def transform_value(x):
  """
  Replace arrays with their shapes, 
  and recursively replace any instances of arrays
  in data structures like tuples also with their shapes
  """
  if isinstance(x, np.ndarray):
    return x.shape 
  elif isinstance(x, list):
    return np.array(x).shape 
  elif isinstance(x, tuple):
    return tuple(transform_value(elt) for elt in x)

  elif isinstance(x, (int, long, float, complex, types.NoneType)):
    return x
  else:
    return None

class EvalShape(Traversal):

  def __init__(self, input_values):
    
    self.inputs = []
    for x in input_values:
      y = transform_value(x)
      if hasattr(y, '__iter__'):
        self.inputs.extend(y)
      else:
        self.inputs.append(y)
  
  def visit_Var(self, v):
    return self.inputs[v.num]  
  
  def visit_Const(self, v):
    return v.value 

  def visit_AnyScalar(self, v):
    return None 
    
  def visit_Shape(self, v):
    dims = self.visit_tuple(v.dims)
    assert all(isinstance(d, int) for d in dims)
    return dims
    
  def visit_Dim(self, v):
    return self.visit(v.array)[v.dim]
    
  def visit_Tuple(self, v):
    return self.visit_tuple(v.elts)
    
  def visit_Sub(self, v):
    return self.visit(v.x) - self.visit(v.y)
  
  def visit_Add(self, v):
    return self.visit(v.x) + self.visit(v.y)
  
  def visit_Mult(self, v):
    return self.visit(v.x) * self.visit(v.y)  
  
  def visit_Div(self, v):
    return self.visit(v.x) / self.visit(v.y)  
  
  def visit_Mod(self, v):
    return self.visit(v.x) % self.visit(v.y)  
  
  def visit_Closure(self, v):
    return v.fn, self.visit_tuple(v.args)

def eval_shape(symbolic_shape, input_values):
  evaluator = EvalShape(input_values)
  result = evaluator.visit(symbolic_shape)
  if not isinstance(result, tuple):
    return () 
  else:
    assert all(isinstance(elt, int) for elt in result)
    return result
   

def result_shape(typed_fn, input_values):
  import shape_inference
  symbolic_shape = shape_inference.call_shape_expr(typed_fn)
  return eval_shape(symbolic_shape, input_values)

      
  
########NEW FILE########
__FILENAME__ = shape_from_type
from ..ndtypes import (ArrayT, SliceT, ClosureT, NoneT, ScalarT, StructT, 
                       TypeValueT, TupleT, PtrT, FnT)
from shape import Shape, Tuple, Closure, Var, Slice, Struct, Ptr, any_scalar




def shapes_from_types(types):
  return Converter().from_types(types)

class Converter(object):
  """
  Turn a list of input types into a list of abstract values, 
  numbering the input arrays and scalars but preserving the 
  structure of tuples, closures, and slices
  """
  def __init__(self):
    self.counter = 0
  
  def fresh_var(self):
    n = self.counter 
    self.counter += 1
    return Var(n)
    
  def from_type(self, t):
    if isinstance(t, ScalarT):
      return self.fresh_var()
    
    elif t.__class__ is ArrayT:
      dim_vars = [self.fresh_var() for _ in range(t.rank)]
      return Shape(dim_vars)
    
    elif t.__class__ is TupleT:
      elt_values = self.from_types(t.elt_types)
      return Tuple(elt_values)
    
    elif t.__class__ is SliceT:
      start = self.from_type(t.start_type)
      stop = self.from_type(t.stop_type)
      step = self.from_type(t.step_type)
      return Slice(start, stop, step)
    
    elif t.__class__ is ClosureT:
      arg_vals = self.from_types(t.arg_types)
      return Closure(t.fn, arg_vals)
   
    elif t.__class__ is FnT:
      return Closure(t.fn, ())
    
    elif isinstance(t, StructT):
      field_names = [fn for (fn,_) in t._fields_]
      field_types = [ft for (_,ft) in t._fields_]
      field_vals = self.from_types(field_types)
      return Struct(field_names, field_vals)
    
    elif isinstance(t, (TypeValueT, NoneT)):
      return Tuple(())
    
    elif isinstance(t, PtrT): 
      return Ptr(any_scalar) 
    
    else:
      assert False, "Unsupported type: %s" % t
    
  def from_types(self, arg_types):
    values = []
    for t in arg_types:
      v = self.from_type(t)
      values.append(v)
    return values

########NEW FILE########
__FILENAME__ = shape_inference

from .. import syntax, prims 
from ..analysis import OffsetAnalysis, SyntaxVisitor
from ..ndtypes import  ArrayT, ScalarT, SliceT, TupleT, NoneT
from ..syntax import unwrap_constant, Expr  

import shape
import shape_from_type
from shape import (Var, Const, Shape, Tuple, Closure, Slice, Scalar, Unknown, 
                   ConstSlice, Struct, AnyScalar, Add, Mult, Div, Sub, Mod, 
                   any_scalar, unknown_value, const, any_value, combine_list, 
                   increase_rank, make_shape, is_zero, is_one, Ptr,
                   dims, combine_dims     
                   ) 

class ShapeInferenceFailure(Exception):
  def __init__(self, value, fn):
    self.value = value 
    self.fn = fn 
  
  def __str__(self):
    return "Couldn't infer shape of %s in function %s" % (self.value, self.fn.name)

  
counter = 0
class ShapeInference(SyntaxVisitor):
  def size_along_axis(self, value, axis):
    assert isinstance(value, Shape)
    return value.dims[axis]

  
  def is_tuple(self, x):
    return isinstance(x, Tuple)

  def is_none(self, x):
    return isinstance(x, Const) and x.value is None

  def rank(self, value):
    if isinstance(value, Shape):
      return value.rank
    else:
      return 0
  
  def max_rank(self, values):
    return max(self.rank(v) for v in values)
  
  def int(self, x):
    return const(x)

  def bool(self, x):
    return const(x)


  _scalar_shape_classes = (Const, Var, Add, Sub, Mult, Div, Mod,  AnyScalar)
  def add(self, x, y):
    cx = x.__class__ 
    cy = y.__class__ 

    if cx in self._scalar_shape_classes and cy in self._scalar_shape_classes:
      if is_zero(x):
        return y
      elif is_zero(y):
        return x
      elif cx is Const and cy is Const: 
        return const(x.value + y.value)
      elif cx is AnyScalar or cy is AnyScalar:
        return any_scalar
      else:
        return Add(x,y)
    else:
      return any_value 

  def sub(self, x, y):
    cx = x.__class__ 
    cy = y.__class__ 
    if cx in self._scalar_shape_classes and cy in self._scalar_shape_classes:
      if is_zero(y):
        return x
      elif cx is Const and cy is Const: 
        return const(x.value - y.value)
      elif cx is AnyScalar or cy is AnyScalar:
        return any_scalar
      elif x == y: 
        return Const(0)
      else:
        return Sub(x, y)
    else:
      return any_value 

  def mul(self, x, y):
    cx = x.__class__ 
    cy = y.__class__ 

    if cx in self._scalar_shape_classes and cy in self._scalar_shape_classes:
      if is_zero(x) or is_zero(y):
        return const(0)
      elif is_one(x):
        return y
      elif is_one(y):
        return x
      elif cx is AnyScalar or cy is AnyScalar:
        return any_scalar
      else:
        return Mult(x,y)
    else:
      return any_value
    
  def div(self, x, y):
    assert not is_zero(y), "Encountered divide by zero during shape inference"
    cx = x.__class__ 
    cy = y.__class__ 

    if cx in self._scalar_shape_classes and cy in self._scalar_shape_classes:
      if is_one(y):
        return x
      elif cx is AnyScalar or cy is AnyScalar:
        return any_scalar
      elif cx is Const and cy is Const:
        return const(int(x.value / y.value))
      elif x == y:
        return const(1)
      else:
        return Div(x, y)
    else:
      return any_value 
      

  def shape(self, x):
    if isinstance(x, Shape):
      return Tuple(x.dims)
    else:
      return Tuple(())

  def elt_type(self, x):
    return "DON'T CARE ABOUT ELT TYPES"

  def alloc_array(self, _, dims):
    return make_shape(dims)

  def index(self, arr, idx):

    if isinstance(arr, Scalar):
      return arr
    assert arr.__class__ is Shape
    if isinstance(idx, (Scalar, Slice, ConstSlice)):
      indices = [idx]
    elif idx.__class__ is Tuple:
      indices = idx.elts
    else:
      assert False, "Unexpected index: %s" % (idx,)
    result_dims = []
    for (i, curr_idx) in enumerate(indices):
      old_dim = arr.dims[i]
      if curr_idx is None or \
         (isinstance(curr_idx, Const) and curr_idx.value is None):
        result_dims.append(old_dim)
      elif isinstance(curr_idx, Scalar):
        pass
      elif curr_idx.__class__ is ConstSlice:
        result_dims.append(curr_idx.nelts)
      elif curr_idx.__class__ is Shape:
        if len(curr_idx.dims) == 0:
          # same as unknown scalar 
          pass 
        else:
          assert len(curr_idx.dims) == 1, "Indexing by a multi-dimensional array not yet supported"
          result_dims.append(curr_idx.dims[0])
      else:
        assert curr_idx.__class__ is Slice, "Unsupported index %s" % curr_idx

        if curr_idx.start is None:
          lower = const(0)
        elif isinstance(curr_idx.start, Const):
          if curr_idx.start.value is None:
            lower = const(0)
          elif curr_idx.start.value < 0:
            lower = self.sub(old_dim, curr_idx.start)
          else:
            lower = curr_idx.start
        else:
          lower = any_scalar
         
        if curr_idx.stop is None:
          upper = old_dim 
        elif isinstance(curr_idx.stop, Const):
          if curr_idx.stop.value is None:
            upper = old_dim
          elif curr_idx.stop.value < 0:
            upper = self.sub(old_dim, curr_idx.stop)
          else:
            upper = curr_idx.stop
        else:
          upper = any_scalar

        n = self.sub(upper, lower)
        step = curr_idx.step
        if step and \
            isinstance(step, Const) and \
            step.value is not None and \
            step.value != 1:
          n = self.div(n, step)
        result_dims.append(n)
    n_original = len(arr.dims)
    n_idx= len(indices)
    if n_original > n_idx:
      result_dims.extend(arr.dims[n_idx:])

    return make_shape(result_dims)
  
  def slice_along_axis(self, arr, axis):
    if arr.__class__ is Shape:
      dims = arr.dims[:axis] + arr.dims[(axis+1):]
      if len(dims) > 0:
        return Shape(dims)
      else:
        return any_scalar
    else:
      return arr 
  
  def tuple(self, elts):
    return Tuple(tuple(elts))

  def concat_tuples(self, t1, t2):
    return Tuple(t1.elts + t2.elts)

  def setidx(self, arr, idx, v):
    pass

  def loop(self, start_idx, stop_idx, body):
    body(start_idx)

  class Accumulator(object):
    def __init__(self, v):
      self.v = v

    def update(self, new_v):
      self.v = new_v

    def get(self):
      return self.v

  def accumulate_loop(self, start_idx, stop_idx, body, init):
    acc = self.Accumulator(init)
    body(acc, start_idx)
    return acc.get()

  def check_equal_sizes(self, sizes):
    pass

  def slice_value(self, start, stop, step):
    
    # if all elements of the slice are constant 
    # then we can ignore the exact start/stop/step 
    # and track only the number of elements in the 
    # slice
    if start.__class__ is Const and \
       stop.__class__ is Const and \
       stop.value is not None and \
       step.__class__ is Const:
      start_val = start.value
      if start_val is None:
        start_val = 0
      step_val = step.value
      if step_val is None:
        step_val = 1
      nelts = (stop.value - start_val) / step_val
      # TODO: 
      # Properly handle negative slicing 
      if nelts >= 0:
        return ConstSlice(nelts)
    return Slice(start, stop, step)

  def call(self, fn, args):
    if fn.__class__ is Closure:
      args = tuple(fn.args) + tuple(args)
      fn = fn.fn
    return symbolic_call(fn, args)

  def invoke(self, fn, args):
    return self.call(fn, args)
  none = None
  null_slice = slice(None, None, None)

  def identity_function(self, x):
    return x
  
  def visit_fn(self, fn):
    assert isinstance(fn, syntax.TypedFn), "Expected typed function, got %s" % fn 
    self.fn = fn 
    self.value_env = {}
    self.equivalence_classes = {}

    self.known_offsets = OffsetAnalysis().visit_fn(fn)

    arg_types = [fn.type_env[name] for name in fn.arg_names]
    input_values = shape_from_type.Converter().from_types(arg_types)
    for n,v in zip(fn.arg_names, input_values):
      self.value_env[n] = v
    self.visit_block(fn.body)

  def unify_scalar_var(self, x, y):
    """
    Unification is different than combining in that it imposes constraints on
    the program. If, for example, we're unifying some scalar that's reached the
    top of the lattice (and thus know nothing about it statically), with a
    scalar known to be some constant-- then the result is we expect both
    variables to be equal to that constant.
    """

    assert isinstance(x, Var), "Expected scalar variable, but got: " + str(x)
    assert isinstance(y, Scalar), "Expected scalar, but got: " + str(y)
    if y == any_scalar:
      return x
    equivs = self.equivalence_classes.get(x, set([]))
    equivs.add(y)
    for var in equivs:
      self.equivalence_classes[var] = equivs
    if isinstance(y, Const):
      for var in equivs:
        self.value_env[var] = y
      return y
    else:
      return var

  def unify_scalar_pairs(self, xs, ys):
    result_elts = []
    for xi, yi in zip(xs.elts, ys.elts):
      result_elts.append(self.unify_scalars(xi, yi))
    return result_elts

  def unify_scalar_list(self, values):
    assert len(values) > 0
    acc = any_scalar
    for v in values:
      acc = self.unify_scalars(acc, v)
    return acc

  def unify_scalars(self, x, y):
    if isinstance(x, Unknown):
      return y
    elif isinstance(y, Unknown):
      return x
    elif isinstance(x, Var):
      return self.unify_scalar_var(x, y)
    elif isinstance(y, Var):
      return self.unify_scalar_var(y, x)
    else:
      raise RuntimeError("Unsupported by unify: %s, %s" % (x,y))

  def visit_merge_loop_start(self, merge):
    for (k, (l, _)) in merge.iteritems():
      self.value_env[k] = self.visit_expr(l)
  
  def visit_merge_loop_repeat(self, merge):
    self.visit_merge(merge)

  def visit_merge(self, merge):
    for (k, (l,r)) in merge.iteritems():
      new_l = self.visit_expr(l)
      new_r = self.visit_expr(r)
      self.value_env[k] = new_l.combine(new_r)

  def visit_expr(self, expr):
    abstract_shape = SyntaxVisitor.visit_expr(self, expr)
    assert abstract_shape is not None, \
        "Unsupported expression in shape inference: %s" % expr.node_type()
    return abstract_shape

  def visit_Alloc(self, expr):
    return Ptr(any_scalar)

  def visit_Cast(self, expr):
    return any_scalar 
  
  def visit_TypeValue(self, expr):
    return unknown_value
  
  def visit_Struct(self, expr):
    if isinstance(expr.type, ArrayT):
      shape_tuple = self.visit_expr(expr.args[1])
      return make_shape(shape_tuple.elts)
    elif isinstance(expr.type, TupleT):
      return Tuple(self.visit_expr_list(expr.args))
    elif isinstance(expr.type, SliceT):
      start, stop, step = self.visit_expr_list(expr.args)
      return Slice(start, stop, step)
    else:
      return unknown_value

  def visit_Fn(self, fn):
    return Closure(fn, [])


  def visit_UntypedFn(self, fn):
    return Closure(fn, [])
  
  def visit_TypedFn(self, fn):
    return Closure(fn, [])

  def shape_from_tuple(self, expr):  
    shape_tuple = self.visit_expr(expr)
    if shape_tuple.__class__ is Tuple:
      return make_shape(tuple(shape_tuple.elts))
    elif shape_tuple.__class__ is Const:
      return make_shape((shape_tuple.value,))
    else:
      return make_shape( (any_scalar,) * expr.type.rank)

  def tuple_from_shape(self, expr):
    shape = self.visit_expr(expr)
    if shape.__class__ is Shape:
      return Tuple(tuple(shape.dims))
    elif shape.__class__ is Const:
      return Tuple( (shape.value,) )
    else:
      return Tuple( (any_scalar,) * expr.type.rank) 
   
  def visit_ArrayView(self, expr):
    return self.shape_from_tuple(expr.shape)
  
  def visit_Reshape(self, expr):
    return self.shape_from_tuple(expr.shape)
  
  def visit_Shape(self, expr):
    return self.tuple_from_shape(expr.array)
  
  def visit_Transpose(self, expr):
    shape = self.visit_expr(expr.array)
    if shape.__class__ is Shape:
      return Shape(tuple(reversed(shape.dims)))
    else:
      return shape 
  
  def visit_AllocArray(self, expr):
    return self.shape_from_tuple(expr.shape)
    
  def visit_Array(self, expr):
    elts = self.visit_expr_list(expr.elts)
    elt = combine_list(elts)
    n = len(elts)
    res = increase_rank(elt, 0, const(n))
    return res

  
  def visit_ConstArray(self, expr):
    return self.shape_from_tuple(expr.shape)
  
  def visit_ConstArrayLike(self, expr):
    return self.visit_expr(expr.array)
  
  def ravel(self, shape):        
    if isinstance(shape, Shape):
      nelts = const(1)
      for dim in shape.dims:
        nelts = self.mul(nelts, dim)
      return Shape((nelts,))
    else:
      return any_value 
  

  def visit_Ravel(self, expr):
    shape = self.visit_expr(expr.array)
    return self.ravel(shape)
  
  def visit_Range(self, expr):
    start = self.visit_expr(expr.start)
    stop = self.visit_expr(expr.stop)
    step = self.visit_expr(expr.step)
    slice_value = self.slice_value(start, stop, step)
    if slice_value.__class__ is ConstSlice:
      return Shape( (slice_value.nelts,))
    else:
      return Shape( (any_scalar,) )

  
  def visit_Slice(self, expr):
    step = self.visit_expr(expr.step)
    if expr.start.__class__ is syntax.Var and \
       expr.stop.__class__ is syntax.Var and \
       step.__class__ is Const:
      start_name = expr.start.name

      stop_name = expr.stop.name
      offsets = self.known_offsets.get(stop_name, [])
      step = step.value if step.value else 1
      for (other_var, offset) in offsets:

        if other_var == start_name:
          nelts = (offset + step - 1) /  step
          # assert False, (start_name, stop_name, offsets)
          return ConstSlice(nelts)

    start = self.visit_expr(expr.start)
    stop = self.visit_expr(expr.stop)
    return self.slice_value(start, stop, step)

  def visit_Const(self, expr):
    return Const(expr.value)

  def visit_ClosureElt(self, expr):
    clos = self.visit_expr(expr.closure)
    assert clos.__class__ is Closure, \
        "Unexpected closure shape %s for expression %s" % (clos, expr)
    return clos.args[expr.index]

  def visit_TupleProj(self, expr):
    t = self.visit_expr(expr.tuple)
    assert isinstance(t, Tuple), "Expected tuple type but got %s : %s" % (t, type(t))
    return t.elts[expr.index]

  def visit_Attribute(self, expr):
    v = self.visit_expr(expr.value)
    name = expr.name

    if v.__class__ is Shape:
      if name == 'shape':
        return Tuple(v.dims)
      elif name == 'strides':
        return Tuple((any_scalar,) * len(v.dims) )
      elif name in ('offset', 'size', 'nelts'):
        return any_scalar
      elif name == 'data':
        return Ptr(any_scalar)
    elif v.__class__ is Tuple:
      if name.startswith('elt'):
        idx = int(name[3:])
      else:
        idx = int(name)
      return v[idx]
    
    elif v.__class__ is Slice:
      return getattr(v, name)
    
    elif v.__class__ is Closure:
      if name.startswith('elt'):
        idx = int(name[3:])
      elif name.startswith('closure_elt'):
        idx = int(name[len('closure_elt'):])
      else:
        idx = int(name)
      return v.args[idx]
        
      
    elif v.__class__ is Struct:
      return v.values[v.fields.index(name)]

    t = expr.value.type.field_type(name)
    
    if isinstance(t, ScalarT):
      return any_scalar
    else:
      return any_value

  def visit_PrimCall(self, expr):
    
    p = expr.prim
    args = self.visit_expr_list(expr.args)
    if p == prims.add: 
      return self.add(args[0], args[1])
    elif p == prims.subtract:
      return self.sub(args[0], args[1])
    elif p == prims.multiply:
      return self.mul(args[0], args[1])
    elif p == prims.divide:
      return self.div(args[0], args[1])
    else:
      result = shape.combine_list(args, preserve_const = False)
      if result.__class__ is Shape:
        return result
      else:
        # once a scalar passes through some prim, it's not longer the same value!
        return any_scalar 

  def visit_Select(self, expr):
    cond = self.visit_expr(expr.cond)
    falseval = self.visit_expr(expr.true_value)
    trueval = self.visit_expr(expr.false_value)

    return cond.combine(falseval).combine(trueval)
    
  def visit_Var(self, expr):
    name = expr.name
    if name in self.value_env:
      return self.value_env[name]
    elif name in self.equivalence_classes:
      for other_name in self.equivalence_classes[name]:
        if other_name in self.value_env:
          return self.value_env[other_name]
    raise RuntimeError("Unknown variable: %s in function %s" %  (expr, self.fn.name))

  def visit_Tuple(self, expr):
    return Tuple(self.visit_expr_list(expr.elts))

  def visit_Call(self, expr):
    fn = self.visit_expr(expr.fn)
    args = self.visit_expr_list(expr.args)
    return symbolic_call(fn, args)

  def visit_Closure(self, clos):
    assert not isinstance(clos.fn, str), \
        "[ShapeInference] Function names in closures not supported: " + clos.fn
    fn = self.visit_expr(clos.fn)
    closure_arg_shapes = self.visit_expr_list(clos.args)
    if fn.__class__ is Closure:
      closure_arg_shapes = tuple(fn.args) + tuple(closure_arg_shapes)
      fn = fn.fn
    return Closure(fn, closure_arg_shapes)

  def visit_Index(self, expr):
    arr = self.visit_expr(expr.value)
    idx = self.visit_expr(expr.index)
    
    if arr.__class__ is Tuple and idx.__class__ is Const:
      return arr[idx.value]
    elif arr.__class__ is Shape:
      if isinstance(idx, Scalar):
        return shape.lower_rank(arr, 0)
      
      elif idx.__class__ is Shape:
        assert len(idx.dims) <= len(arr.dims), \
            "Can't index into rank %d array with rank %d indices" % \
            (len(arr.dims), len(idx.dims))
        dims = [d for d in arr.dims]
        for (i,d) in enumerate(idx.dims):
          dims[i] = d
        return shape.make_shape(dims)
      else:
        return self.index(arr, idx)
      
    elif arr.__class__ is Ptr:
      assert isinstance(arr.elt_shape, Scalar)
      assert isinstance(idx, Scalar)
      
      return any_scalar
    
    if isinstance(arr, Scalar):
      assert False, "Expected %s to be array, shape inference found scalar" % (arr,)
    elif arr == shape.any_value:
      raise ShapeInferenceFailure(expr, self.fn)
    assert False, \
        "Can't index (%s) with array shape %s and index shape %s" % \
        (expr, arr, idx)


  
  def visit_IndexMap(self, expr):
    bounds = self.visit_expr(expr.shape)
    clos = self.visit_expr(expr.fn)
    if isinstance(clos.fn.input_types[-1], TupleT) or bounds.__class__ is not Tuple:
      indices = [bounds]
    else:
      indices = bounds.elts 
    elt_result = symbolic_call(clos, indices)
    return make_shape(combine_dims(bounds, elt_result))
    
    
  def visit_IndexReduce(self, expr):
    fn = self.visit_expr(expr.fn)
    combine = self.visit_expr(expr.combine)
    bounds = self.visit_expr(expr.shape)
    if isinstance(fn.fn.input_types[-1], TupleT) or bounds.__class__ is not Tuple:
      indices = [bounds]
    else:
      indices = bounds.elts
    elt_shape = symbolic_call(fn, indices)
    init_shape = elt_shape if self.expr_is_none(expr.init) else self.visit_expr(expr.init) 
    return symbolic_call(combine, [init_shape, elt_shape])

  def visit_IndexScan(self, expr):
    fn = self.visit_expr(expr.fn)
    combine = self.visit_expr(expr.combine)
    emit = self.visit_expr(expr.emit)
    bounds = self.visit_expr(expr.shape)
    if isinstance(fn.fn.input_types[-1], TupleT) or bounds.__class__ is not Tuple:
      indices = [bounds]
    else:
      indices = bounds.elts
    elt_shape = symbolic_call(fn, indices)
    init_shape = elt_shape if self.expr_is_none(expr.init) else self.visit_expr(expr.init) 
    acc_shape = symbolic_call(combine, [init_shape, elt_shape])
    output_elt_shape = symbolic_call(emit, [acc_shape])
    return make_shape(combine_dims(bounds, output_elt_shape))


  def normalize_axes(self, axis, args):
    if isinstance(axis, Expr):
      axis = unwrap_constant(axis)
    if isinstance(axis,tuple):
      axes = axis 
    else:
      axes = (axis,) * len(args)
    
    assert len(axes) == len(args), \
      "Mismatch between args %s and axes %s" % (args, axis)
    return axes 
  
  def adverb_elt_shapes(self, arg_shapes, axes):
    """
    Slice into array shapes along the specified axis 
    """
    elt_shapes = []
    for arg_shape, axis in zip(arg_shapes, axes):
      if axis is None:
        elt_shapes.append(any_scalar)
      elif axis < self.rank(arg_shape):
        elt_shapes.append(self.slice_along_axis(arg_shape, axis))
      else:
        elt_shapes.append(arg_shape)
    return elt_shapes 

  def inner_map_result_shape(self, elt_result, arg_shapes, axes):
    max_rank = self.max_rank(arg_shapes)    
    for i, arg_shape in enumerate(arg_shapes):
      r = self.rank(arg_shape)
      if r == max_rank:
        axis = axes[i]
        if axis is None:
          combined_dims = dims(arg_shape) + dims(elt_result)
          if len(combined_dims) > 0:
            return Shape(combined_dims)
          else:
            return any_scalar 
        else:
          return increase_rank(elt_result, 0, arg_shape.dims[axis])
    return elt_result
    
  def outer_map_result_shape(self, elt_result, arg_shapes, axes):
    result_dims = list(dims(elt_result))
    for i, arg_shape in enumerate(arg_shapes):
      r = self.rank(arg_shape)
      if r > 0:
        axis = axes[i]
        if axis is None:
          result_dims.extend(arg_shape.dims)
        else:
          result_dims.append(arg_shape.dims[axis])
    return make_shape(result_dims)
    
  def visit_Map(self, expr):
    arg_shapes = self.visit_expr_list(expr.args)
    fn = self.visit_expr(expr.fn)
    axes = self.normalize_axes(expr.axis, expr.args)
    elt_shapes = self.adverb_elt_shapes(arg_shapes, axes)    
    elt_result = symbolic_call(fn, elt_shapes)
    return self.inner_map_result_shape(elt_result, arg_shapes, axes)
  
  def expr_is_none(self, expr):
    return expr is None or expr.type.__class__ is NoneT
  
  def visit_Reduce(self, expr):
    fn = self.visit_expr(expr.fn)
    combine = self.visit_expr(expr.combine)
    arg_shapes = self.visit_expr_list(expr.args)
    axes = self.normalize_axes(expr.axis, expr.args)
    elt_shapes = self.adverb_elt_shapes(arg_shapes, axes)
    elt_result = symbolic_call(fn, elt_shapes)
    init = elt_result if self.expr_is_none(expr.init) else self.visit_expr(expr.init) 
    return symbolic_call(combine, [init, elt_result])
      
  def visit_Scan(self, expr):
    fn = self.visit_expr(expr.fn)
    combine = self.visit_expr(expr.combine)
    arg_shapes = self.visit_expr_list(expr.args)
    axes = self.normalize_axes(expr.axis, expr.args)
    elt_shapes = self.adverb_elt_shapes(arg_shapes, axes)
    elt_result = symbolic_call(fn, elt_shapes)
    init = elt_result if self.expr_is_none(expr.init) else self.visit_expr(expr.init) 
    acc_shape = symbolic_call(combine, [init, elt_result])
    emit = self.visit_expr(expr.emit)
    emit_shape = symbolic_call(emit, [acc_shape])
    return self.inner_map_result_shape(emit_shape, arg_shapes, axes)
    
  def visit_OuterMap(self, expr):
    fn = self.visit_expr(expr.fn)
    arg_shapes = self.visit_expr_list(expr.args)
    axes = self.normalize_axes(expr.axis, expr.args)
    elt_shapes = self.adverb_elt_shapes(arg_shapes, axes)    
    elt_result = symbolic_call(fn, elt_shapes)
    return self.outer_map_result_shape(elt_result, arg_shapes, axes)

  def visit_Assign(self, stmt):
    rhs = self.visit_expr(stmt.rhs)
    if stmt.lhs.__class__ in (syntax.Var, syntax.Tuple):
      bind_syntax(stmt.lhs, rhs, self.value_env)
    
  def visit_Return(self, stmt):
    new_value = self.visit_expr(stmt.value)
    old_value = self.value_env.get("$return", unknown_value)
    combined = old_value.combine(new_value)
    self.value_env["$return"] = combined

  def visit_ForLoop(self, stmt):
    self.value_env[stmt.var.name] = any_scalar
    SyntaxVisitor.visit_ForLoop(self, stmt)
    # visit body a second time in case first-pass fixed values relative 
    # to initial value of iteration vars 
    self.visit_block(stmt.body)
  
  def visit_While(self, stmt):
    SyntaxVisitor.visit_While(self, stmt)
    # visit body a second time in case first-pass fixed values relative 
    # to initial value of iteration vars 
    self.visit_block(stmt.body)
    
    
_shape_env_cache = {}
def shape_env(typed_fn):
  key = typed_fn.cache_key
  
  if key in _shape_env_cache:
    return _shape_env_cache[key]

  shape_inference = ShapeInference()
  shape_inference.visit_fn(typed_fn)
  env = shape_inference.value_env
  _shape_env_cache[key] = env
  return env

_shape_cache = {}
def call_shape_expr(typed_fn):
  key = typed_fn.cache_key
  if key in _shape_cache:
    return _shape_cache[key]
  env = shape_env(typed_fn)
  abstract_shape = env.get("$return", Const(None))
  _shape_cache[key] = abstract_shape
  return abstract_shape

def bind_syntax(lhs, rhs, env):
  if isinstance(lhs, syntax.Tuple):
    assert isinstance(rhs, Tuple), "Expected tuple on RHS of binding %s = %s" % (lhs,rhs)
    for l,r in zip(lhs.elts, rhs.elts):
      bind_syntax(l, r, env)
  elif isinstance(lhs, syntax.Var):
    env[lhs.name] = rhs

def bind(lhs, rhs, env):
  if isinstance(lhs, Var):
    env[lhs] = rhs
  elif isinstance(lhs, Shape):
    assert isinstance(rhs, Shape), "Expected %s, got %s" % (lhs, rhs)
    bind_pairs(lhs.dims, rhs.dims, env)
  elif isinstance(lhs, Closure):
    assert isinstance(rhs, Closure)
    bind_pairs(lhs.args, rhs.args, env)
  elif isinstance(lhs, Tuple):
    if rhs == any_value: 
      bind_pairs(lhs.elts, [any_value for _ in lhs.elts], env)
    elif lhs == unknown_value:
      bind_pairs(lhs.elts, [unknown_value for _ in lhs.elts], env)
    else:
      assert isinstance(rhs, Tuple), "Expected tuple on RHS of binding %s = %s" % (lhs,rhs)
      bind_pairs(lhs.elts, rhs.elts, env)
  else:
    raise RuntimeError("Unexpected shape LHS: %s" % lhs)

def bind_pairs(xs, ys, env):
  assert len(xs) == len(ys), \
      "Can't bind %s and %s due to unequal lengths" % (xs, ys)
  for (x,y) in zip(xs,ys):
    bind(x,y,env)

def subst(x, env):
  if isinstance(x, Var):
    assert x in env, "Unknown variable %s" % x
    return env[x]
  elif isinstance(x, Scalar):
    return x
  elif isinstance(x, Shape):
    return make_shape(subst_list(x.dims, env))
  elif isinstance(x, Tuple):
    return Tuple(tuple((subst_list(x.elts, env))))
  elif isinstance(x, Closure):
    return Closure(x.fn, subst_list(x.args, env))
  elif isinstance(x, Ptr):
    return Ptr(subst(x.elt_shape, env))
  else:
    raise RuntimeError("Unexpected abstract expression: %s" % x)

def subst_list(xs, env):
  return [subst(x, env) for x in xs]

def symbolic_call(fn, abstract_inputs):
  # result in terms of variables like input0, (shape: input1, input2), etc..
  if fn.__class__ is Closure:
    closure_elts = tuple(fn.args)
    fn = fn.fn
  else:
    closure_elts = ()

  abstract_result_value = call_shape_expr(fn)
  conv = shape_from_type.Converter()
  shape_formals = conv.from_types(fn.input_types)
  
  env = {}
  bind_pairs(shape_formals, closure_elts + tuple(abstract_inputs), env)

  return subst(abstract_result_value, env)

########NEW FILE########
__FILENAME__ = actual_args
class CombinedIters:
  def __init__(self, i1, i2):
    self.i1 = i1
    self.i2 = i2
    self.first_iter = True

  def next(self):
    if self.first_iter:
      try:
        return self.i1.next()
      except:
        self.first_iter = False
        return self.i2.next()
    else:
      return self.i2.next()

def combine_iters(*iters):
  assert len(iters) > 0
  curr_iter = iters[0]
  for i in iters[1:]:
    curr_iter = CombinedIters(curr_iter, i)
  return curr_iter

class NullIter(object):
  def next(self):
    raise StopIteration

def maybe_iter(obj):
  if obj is None:
    return NullIter()
  else:
    return iter(obj)

class ActualArgs(object):
  __slots__ = ['positional', 'keywords', 'starargs', "_hash"]
  
  def __init__(self, positional, keywords = {}, starargs = None):
    positional = tuple(positional)
    self.positional = positional 
    self.keywords = keywords
    self.starargs = starargs
    self._hash = None 
    
  def transform(self, fn, keyword_name_fn = None, keyword_value_fn = None):
    new_pos = [fn(pos_arg) for pos_arg in self.positional]
    new_keywords = {}
    for (k,v) in self.keywords.iteritems():
      new_name = keyword_name_fn(k) if keyword_name_fn else k
      new_value = keyword_value_fn(v) if keyword_value_fn else fn(v)
      new_keywords[new_name] = new_value
    new_starargs = fn(self.starargs) if self.starargs else None
    return ActualArgs(new_pos, new_keywords, new_starargs)

  def __eq__(self, other):
    if self.positional != other.positional or \
       len(self.keywords) != len(other.keywords) or \
       self.starargs != other.starargs:
      return False
    for (k,v) in self.keywords.iteritems():
      if k not in other.keywords:
        return False
      if other.keywords[k] != v:
        return False
    return True

  def __str__(self):
    arg_strings = []
    for p in self.positional:
      arg_strings.append(str(p))
    for (k,v) in self.keywords.items():
      arg_strings.append("%s = %s" % (k,v))
    if self.starargs:
      arg_strings.append("*" + str(self.starargs))
    return ", ".join(arg_strings)

  def __repr__(self):
    return str(self)

  def __hash__(self):
    if self._hash is None:
      self._hash = hash(self.positional) + len(self.keywords) + hash(self.starargs)
    return self._hash 
  
  def __iter__(self):
    return combine_iters(iter(self.positional),
                         self.keywords.itervalues(),
                         maybe_iter(self.starargs))

  def prepend_positional(self, more_args):
    new_pos = tuple(more_args) + self.positional
    return ActualArgs(new_pos, self.keywords, self.starargs)

########NEW FILE########
__FILENAME__ = adverbs
from dsltools import Node 
from expr import Expr
  


class Adverb(Expr, Node):
  _members = ['fn', 'output', 'type', 'source_info']
  
  def node_init(self):
    assert self.fn, "Can't construct adverb %s without a function argument" % self 
  def functions(self):
    yield self.fn 
    

      
class Accumulative(Expr):
  """
  Adverbs such as Reduce and Scan which carry an accumulated value and require a
  'combine' function to merge the accumulators resulting from parallel
  sub-computations.
  """
  _members = ['combine', 'init']
  
class HasEmit(Expr):
  """
  Common base class for Scan, IndexScan, and whatever other sorts of scans can be dreamed up
  """
  _members = ['emit']
  
  

class IndexAdverb(Adverb):
  _members = ['shape', 'start_index']
  
  
class IndexMap(IndexAdverb):
  """
  Map from each distinct index in the shape to a value 
  """
  pass 


class IndexAccumulative(IndexAdverb, Accumulative):
  pass 
  
class IndexReduce(IndexAccumulative):
  """
  Expect the 'fn' field to take indices and produces 
  element values, whereas 'combine' takes pairs of element 
  values and combines them. 
  """
  pass 

    
class IndexScan(IndexAccumulative, HasEmit):
  pass 
  
class DataAdverb(Adverb):
  _members = ['args', 'axis']

  def fn_to_str(self, fn):
    if hasattr(fn, 'name'):
      return fn.name
    else:
      return str(fn)

  def args_to_str(self):
    if isinstance(self.args, (list, tuple)):
      return ", ".join([str(arg) + ":" + str(arg.type) for arg in self.args])
    else:
      return str(self.args)

  def __repr__(self):
    s = "%s(axis = %s, args = (%s), type=%s, fn = %s)" % \
          (self.node_type(), self.axis,
           self.args_to_str(),
           self.type,
           self.fn_to_str(self.fn),)
    return s

  def __str__(self):
    return repr(self)

class Map(DataAdverb):
  pass 

class OuterMap(DataAdverb):
  pass 



class DataAccumulative(DataAdverb, Accumulative):
  pass 


class Reduce(DataAccumulative):
  def __repr__(self):
    return "Reduce(axis = %s, args = (%s), type = %s, init = %s, map_fn = %s, combine = %s)" % \
        (self.axis,
         self.args_to_str(),
         self.type,
         self.init,
         self.fn_to_str(self.fn),
         self.fn_to_str(self.combine))

class Scan(DataAccumulative, HasEmit):
  def __repr__(self):
    s = "%s(axis = %s, args = {%s}, type = %s, " \
        % (self.node_type(), self.axis,
           self.args_to_str(),
           self.type
          )
    s += "init = %s, map_fn = %s, combine = %s, emit = %s)" % \
         (self.init,
          self.fn_to_str(self.fn),
          self.fn_to_str(self.combine),
          self.fn_to_str(self.emit))
    return s

class Filter(DataAdverb):
  """
  Applies 'fn' to each element of the arguments and 
  returns indices where 'pred' is True for the 
  element values
  """
  _members = ['pred']
  

class IndexFilter(IndexAdverb, Filter):
  pass 


class FilterReduce(Reduce, Filter):
  """
  Like a normal reduce but skips some elements if they don't pass
  the predicate 'pred'
  """
  pass  
  
  
class IndexFilterReduce(FilterReduce):
  pass 

class Tiled(object):
  _members = ['axes', 'fixed_tile_size']

  def __repr__(self):
    s = "%s(axes = %s, args = (%s), type=%s, fn = %s)" % \
          (self.node_type(), self.axes,
           self.args_to_str(),
           self.type,
           self.fn_to_str(self.fn),)
    return s

class TiledMap(Tiled, Map):
  pass

class TiledOuterMap(Tiled, OuterMap):
  pass

class TiledReduce(Tiled, Reduce):
  def __repr__(self):
    s = ("%s(axes = %s, args = {%s}, type = %s, init = %s, map_fn = %s, combine = %s)") % \
        (self.node_type(), self.axes,
         self.args_to_str(),
         self.type,
         self.init,
         self.fn_to_str(self.fn),
         self.fn_to_str(self.combine),
        )
    return s

class TiledScan(Tiled, Scan):
  def __repr__(self):
    s = "%s(axes = %s, args = {%s}, type = %s, " \
        % (self.node_type(), self.axes,
           self.args_to_str(),
           self.type
          )
    s += "init = %s, map_fn = %s, combine = %s, emit = %s)" % \
         (self.init,
          self.fn_to_str(self.fn),
          self.fn_to_str(self.combine),
          self.fn_to_str(self.emit))
    return s

  
class Conv(Adverb):
  _members = ['x', 'window_shape']
  
  def __repr__(self):
    return "Conv(fn = %s, x = %s, window_shape=%s)" % \
      (self.fn, self.x, self.window_shape)
      
  def __str__(self):
      return repr(self)

class ConvBorderFn(Conv):
  _members = ['border_fn']
  
class ConvBorderValue(Conv):
  _members = ['border_value']
  
class ConvPadding(Conv):
  _members = ['fill_value']
  
  
########NEW FILE########
__FILENAME__ = adverb_helpers

   
from .. import names 
from .. ndtypes import ArrayT, Type

from adverbs import Map
from expr import Var
from formal_args import FormalArgs
from helpers import  unwrap_constant, zero_i64, get_types
from stmt import Return 
from untyped_fn import UntypedFn

def max_rank(arg_types):
  """
  Given a list of types, find the maximum rank of the list and also check that
  all other types have either the same rank or are scalars
  """

  curr_max = 0
  for t in arg_types:
    if isinstance(t, ArrayT):
      r =  t.rank
      if r > curr_max:
        curr_max = r  
      #assert curr_max == 0 or curr_max == t.rank,  \
      # "Adverb can't accept inputs of rank %d and %d" % (curr_max, t.rank)
      
  return curr_max

def max_rank_arg(args):
  """Given a list of arguments, return one which has the maximum rank"""

  r = max_rank(get_types(args))
  for arg in args:
    if arg.type.rank == r:
      return arg

def num_outer_axes(arg_types, axis):
  """
  Helper for adverb type inference to figure out how many axes it will loop over
  -- either 1 particular one or all of them when axis is None.
  """

  axis = unwrap_constant(axis)
  if isinstance(arg_types, Type):
    max_arg_rank = arg_types.rank
  else:
    max_arg_rank = max_rank(arg_types)
  return 1 if (max_arg_rank > 0 and axis is not None) else max_arg_rank

_nested_map_cache = {}
def nested_maps(inner_fn, depth, arg_names):
  
  if depth <= 0:
    assert isinstance(inner_fn, UntypedFn)
    return inner_fn

  key = inner_fn.name, depth, tuple(arg_names)
  if key in _nested_map_cache:
    return _nested_map_cache[key]
  args_obj = FormalArgs()
  arg_vars = []
  for var_name in arg_names:
    local_name = names.refresh(var_name)
    args_obj.add_positional(local_name)
    arg_vars.append(Var(local_name))

  name = names.fresh(inner_fn.name + "_broadcast%d" % depth)
  nested_fn = nested_maps(inner_fn, depth - 1, arg_names)

  map_expr = Map(fn = nested_fn, 
                 axis = zero_i64, 
                 args = arg_vars)
  fn = UntypedFn(
    name = name,
    args = args_obj,
    body = [Return(map_expr)]
  )

  _nested_map_cache[key] = fn
  return fn




########NEW FILE########
__FILENAME__ = array_expr
from ..ndtypes import Float64, make_array_type
from seq_expr import SeqExpr 

class ArrayExpr(SeqExpr):
  """
  Common base class for first-order array operations 
  that don't change the underlying data 
  """
  def __init__(self, array, type = None, source_info = None):
    self.array = array 
    self.type = type 
    self.source_info = source_info 
    
  def children(self):
    yield self.array 

class Array(ArrayExpr):
  def __init__(self, elts, type = None, source_info = None):
    self.elts = tuple(elts) 
    self.type = type 
    self.source_info = source_info 

  def __str__(self):
    return "[%s]" % ", ".join(str(e) for e in self.elts)
  
  def children(self):
    return self.elts

  def __hash__(self):
    return hash(self.elts)

class Slice(ArrayExpr):
  def __init__(self, start, stop, step, type = None, source_info = None):
    self.start = start 
    self.stop = stop 
    self.step = step 
    self.type = type 
    self.source_info = source_info 


  def __str__(self):
    return "%s:%s:%s"  % \
        (self.start.short_str(),
         self.stop.short_str(),
         self.step.short_str())

  def __repr__(self):
    return str(self)

  def children(self):
    yield self.start
    yield self.stop
    yield self.step

  def __eq__(self, other):
    return other.__class__ is Slice and \
           other.start == self.start and \
           other.stop == self.stop and \
           other.step == self.step
           
  def __hash__(self):
    return hash((self.start, self.stop, self.step))

class ConstArray(ArrayExpr):
  def __init__(self, shape, value, type = None, source_info = None):
    self.shape = shape 
    self.value = value 
    self.type = type 
    self.source_info = source_info 

  def children(self):
    yield self.shape 
    yield self.value 

class DiagonalArray(ArrayExpr):
  """
  Build a zero array with a filled value on its diagonal 
  Use this to implement np.diag, np.eye 
  """
  def __init__(self, shape, value, offset = None, type = None, source_info = None):
    self.shape = shape 
    self.value = value 
    self.offset = offset
    self.type = type 
    self.source_info = source_info
    
  def __str__(self):
    return "DiagonalArray(shape = %s, value = %s, offset = %s)" % \
      (self.shape, self.value, self.offset)
        
  def children(self):
    yield self.shape 
    yield self.value 
    if self.offset is not None:
      yield self.offset 
      


class ExtractDiagonal(ArrayExpr):
  """
  Go from an n-d input array to 1-d vector of diagonal elements
  """ 
  pass 

class ConstArrayLike(ArrayExpr):
  """
  Create an array with the same shape as the first arg, but with all values set
  to the second arg
  """

  def __init__(self, array, value, type = None, source_info = None):
    self.array = array 
    self.value = value 
    self.type = type 
    self.source_info = source_info 
  
  def children(self):
    yield self.array 
    yield self.value   

class Range(ArrayExpr):
  def __init__(self, start, stop, step, type = None, source_info = None):
    self.start = start 
    self.stop = stop 
    self.step = step 
    self.type = type 
    self.source_info = source_info 

  def children(self):
    yield self.start 
    yield self.stop 
    yield self.step 
  
  
  def __str__(self):
    return "Range(start = %s, stop = %s, step = %s)" % (self.start, self.stop, self.step)


class AllocArray(ArrayExpr):
  """Allocate an unfilled array of the given shape and type"""
  def __init__(self, shape, elt_type, type = None, order = "C", source_info = None):
    # TODO: support a 'fill' field 
    self.shape = shape 
    self.elt_type = elt_type 
    self.type = type 
    self.source_info = source_info 
    
    # TODO: let user specify layout orders
    assert order == "C", \
      "Layouts other than row-major not yet supported, invalid option order = '%s'" % order 
    self.order = order

  def children(self):
    yield self.shape
    
  def __str__(self):
    return "AllocArray(shape = %s, elt_type = %s)" % (self.shape, self.elt_type)


class ArrayView(ArrayExpr):
  """Create a new view on already allocated underlying data"""
  def __init__(self, data, shape, strides, offset, size, type = None, source_info = None):
    self.data = data 
    self.shape = shape 
    self.strides = strides 
    self.offset = offset 
    self.size = size 
    self.type = type 
    self.source_info = source_info

  def __str__(self):
    return "ArrayView(data = %s, shape = %s, strides = %s, offset = %s, size = %s)" % \
      (self.data, self.shape, self.strides, self.offset, self.size)
      
  def children(self):
    yield self.data
    yield self.shape
    yield self.strides
    yield self.offset
    yield self.size

class Ravel(ArrayExpr):
  def children(self):
    return (self.array,)

  def __str__(self):
    return "Ravel(%s)" % self.array 

class Reshape(ArrayExpr):
  def __init__(self, array, shape, type = None, source_info = None):
    self.array = array 
    self.shape = shape 
    self.type = type 
    self.source_info = source_info

  
  def children(self):
    yield self.array 
    yield self.shape
    
  def __str__(self):
    return "Reshape(%s, %s)" % (self.array, self.shape)

class Shape(ArrayExpr):
  
  def __str__(self):
    return "Shape(%s)" % self.array 
  
class Strides(ArrayExpr):
  def __str__(self):
    return "Strides(%s)" % self.array 
  
    
class Transpose(ArrayExpr):
  def children(self):
    yield self.array
  
  def __str__(self):
    return "%s.T" % self.array 
  
class Tile(ArrayExpr):
  def __init__(self, array, reps, type = None, source_info = None):
    self.array = array 
    self.reps = reps 
    self.type = type 
    self.source_info = source_info 

  def children(self):
    yield self.array 
    yield self.reps 
    
  def __str__(self):
    return "Tile(%s, %s)" % (self.array, self.reps)
    

class Where(ArrayExpr):
  """
  Return the non-zero indices of the array
  """
  def __init__(self, array):
    self.array = array 
    
  def __str__(self):
    return "Where(%s)" % self.array 
  
  def children(self):
    yield self.array 
  
  
class Compress(ArrayExpr):
  """
  Slice into array 'data' at positions where 'condition' is True
  """ 
  def __init__(self, condition, data):
    self.condition = condition 
    self.data = data 
  
  def __str__(self):
    return "Compress(%s, %s)" % (self.condition, self.data)
  
  def children(self):
    yield self.data 
    yield self.data 
########NEW FILE########
__FILENAME__ = delay_until_typed

from expr import Expr 


class DelayUntilTyped(Expr):
  """
  Once the list of values has been annotated with locally inferred types, 
  pass them to the given function to construct a final expression 
  """
  def __init__(self, values, keywords, fn, source_info = None):
    """
    No need for a 'type' argument since the user-supplied function 
    will generate a typed expression to replace this one
    """
    if isinstance(values, list): values = tuple(values)
    elif not isinstance(values, tuple): values = (self.values,)
    self.values = values 
    
    if keywords is None: keywords = {}
    self.keywords = keywords 
    
    self.fn = fn 
    self.source_info = source_info 
    self.type = None 
    
  def children(self):
    return tuple(self.values) + tuple(self.keywords.values())  
########NEW FILE########
__FILENAME__ = expr
from dsltools import Node 

from .. ndtypes import NoneT

class Expr(object):
  # _members = ['type', 'source_info']
  
  def __str__(self):
    fields = ["%s = %s" % (k,v) for (k,v) in self.__dict__.iteritems()]
    return "%s(%s)" % (self.__class__.__name__, ", ".join(fields))
  
  def __repr__(self):
    return str(self)
  
  @classmethod
  def node_type(cls):
    return cls.__name__
  
  def children(self):
    for v in self.itervalues():
      if v and isinstance(v, Expr):
        yield v
      elif isinstance(v, (list,tuple)):
        for child in v:
          if isinstance(child, Expr):
            yield child

  def short_str(self):
    return str(self)

  def __eq__(self, other):
    return (self.__class__ is other.__class__) and \
      self.type == other.type and \
      all(c1 == c2 for (c1,c2) in zip(self.children(), other.children()))
    
  def __ne__(self, other):
    return not (self == other)
  
  def hash(self):
    elts = tuple(self.children())
    return hash(elts)
   
class Const(Expr):
  def __init__(self, value, type = None, source_info = None):
    self.value = value 
    self.type = type 
    self.source_info = source_info 
  
  def children(self):
    return ()

  def short_str(self):
    return str(self.value)

  def __repr__(self):
    if self.type and not isinstance(self.type, NoneT):
      return "%s : %s" % (self.value, self.type)
    else:
      return str(self.value)

  def __str__(self):
    return repr(self)

  def __hash__(self):
    return hash(self.value)

  def __eq__(self, other):
    return other.__class__ is Const and \
           self.value == other.value and \
           self.type == other.type
    
  def __ne__(self, other):
    return other.__class__ is not Const or \
           self.value != other.value or \
           self.type != other.type

class Var(Expr):
  def __init__(self, name, type = None, source_info = None):
    assert name is not None 
    self.name = name
    self.type = type 
    self.source_info = source_info 

  # it's possible to speed things up a little
  # by by-passing the Node construct 
  #_members = ['name']
  #def node_init(self):
  #  assert self.name is not None
    
  def short_str(self):
    return self.name

  def __repr__(self):
    if hasattr(self, 'type'):
      return "%s : %s" % (self.name, self.type)
    else:
      return "%s" % self.name

  def __str__(self):
    return self.name

  def __hash__(self):
    return hash(self.name)

  def __eq__(self, other):
    return other.__class__  is Var and \
           self.name == other.name and \
           self.type == other.type


  def __ne__(self, other):
    if other.__class__ is not Var:
      return True 
    if self.name != other.name:
      return True
    return self.type != other.type 
  
  def children(self):
    return ()

class Attribute(Expr):
  def __init__(self, value, name, type = None, source_info = None):
    self.value = value 
    self.name = name 
    self.type = type 
    self.source_info = source_info 

  def children(self):
    yield self.value

  def __str__(self):
    return "attr(%s, '%s')" % (self.value, self.name)

  def __hash__(self):
    return hash ((self.value, self.name))

  def __eq__(self, other):
    return other.__class__ is Attribute and \
           self.name == other.name and \
           self.value == other.value


class Closure(Expr):
  """Create a closure which points to a global fn with a list of partial args"""
  def __init__(self, fn, args, type = None, source_info = None):
    self.fn = fn 
    self.args = args 
    self.type = type 
    self.source_info = source_info 

  def __str__(self):
    fn_str = str(self.fn) #self.fn.name if hasattr(self.fn, 'name') else str(self.fn)
    args_str = ",".join(str(arg) + ":" + str(arg.type) for arg in self.args)
    return "Closure(%s, fixed_args = {%s})" % (fn_str, args_str)

  def node_init(self):
    self.args = tuple(self.args)
    #self.return_type = self.fn.return_type
    #self.input_types = self.fn.input_types[len(self.args):]

  def children(self):
    if isinstance(self.fn, Expr):
      yield self.fn
    for arg in self.args:
      yield arg

  def __hash__(self):
    return hash((self.fn, tuple(self.args)))

class Call(Expr):
  def __init__(self, fn, args, type = None, source_info = None):
    self.fn = fn 
    self.args = args 
    self.type = type 
    self.source_info = source_info 
    
  def __str__(self):
    #if isinstance(self.fn, (Fn, TypedFn)):
    #  fn_name = self.fn.name
    #else:
    fn_name = str(self.fn)
    if self.args.__class__.__name__ == 'ActualArgs':
      arg_str = str(self.args)
    else:
      arg_str = ", ".join(str(arg) for arg in self.args)
    return "%s(%s)" % (fn_name, arg_str)

  def __repr__(self):
    return str(self)

  def children(self):
    yield self.fn
    for arg in self.args:
      yield arg

  def __hash__(self):
    return hash((self.fn, tuple(self.args)))


class PrimCall(Expr):
  """
  Call a primitive function, the "prim" field should be a prims.Prim object
  """
  def __init__(self, prim, args, type = None, source_info = None):
    self.prim = prim 
    self.args = args 
    self.type = type 
    self.source_info = source_info 
    
  #_members = ['prim', 'args']
  
  def _arg_str(self, i):
    arg = self.args[i]
    if arg.__class__ is PrimCall:
      return "(%s)" % arg
    else:
      return str(arg)
  
  def __hash__(self):
    self.args = tuple(self.args)
    return hash((self.prim, self.args))
    
  def __eq__(self, other):
    if other.__class__ is not PrimCall:
      return False 
    
    if other.prim != self.prim:
      return False
    
    my_args = self.args
    other_args = other.args
    n = len(my_args)
    if n != len(other_args):
      return False
    
    for (i,arg) in enumerate(my_args):
      other_arg = other_args[i]
      if arg != other_arg:
        return False
    return True
  
  def __ne__(self, other):
    return not self==other
  
  def __repr__(self):
    if self.prim.symbol:
      if len(self.args) == 1:
        return "%s%s" % (self.prim.symbol, self._arg_str(0))
      else:
        assert len(self.args) == 2
        return "%s %s %s" % (self._arg_str(0), self.prim.symbol, self._arg_str(1))
    else:
      arg_strings = [self._arg_str(i) for i in xrange(len(self.args))]
      combined = ", ".join(arg_strings)
      return "prim<%s>(%s)" % (self.prim.name, combined)

  def __str__(self):
    return repr(self)

  def node_init(self):
    self.args = tuple(self.args)

  def children(self):
    return self.args
  


class ClosureElt(Expr):
  def __init__(self, closure, index, type = None, source_info = None):
    self.closure = closure 
    self.index = index 
    self.type = type 
    self.source_info = source_info 
    
  def __str__(self):
    return "ClosureElt(%s, %d)" % (self.closure, self.index)

  def children(self):
    return (self.closure,)

  def __hash__(self):
    return hash((self.closure, self.index))

class Cast(Expr):
  def __init__(self, value, type, source_info = None):
    self.value = value 
    self.type = type 
    self.source_info = source_info 

  def children(self):
    yield self.value 

  def __hash__(self):
    return hash(self.value)
  
  def __str__(self):
    return "Cast(%s : %s)" % (self.value, self.type) 

class Select(Expr):
  def __init__(self, cond, true_value, false_value, type = None, source_info = None):
    self.cond = cond 
    self.true_value = true_value 
    self.false_value = false_value 
    self.type = type 
    self.source_info = source_info 
    
  # _members = ['cond', 'true_value', 'false_value']
  
  def __hash__(self):
    return hash((self.cond, self.true_value, self.false_value))
  
  def __str__(self):
    return "Select(%s, %s, %s)" % (self.cond, self.true_value, self.false_value)
  
  def children(self):
    yield self.cond 
    yield self.true_value 
    yield self.false_value 
  
  

########NEW FILE########
__FILENAME__ = formal_args
from itertools import izip
from .. import names 
from actual_args import ActualArgs



class UnexpectedKeyword(Exception):
  def __init__(self, keyword_name, fn_name = None):
    self.keyword_name = keyword_name
    self.fn_name = fn_name
  
  def __str__(self):
    if self.fn_name:
      return "Encountered unexpected keyword '%s' in fn %s" % (self.keyword_name, self.fn_name)
    return "Encountered unexpected keyword %s" % self.keyword_name
  
  def __repr__(self):
    return str(self)

class TooManyArgsError(Exception):
  def __init__(self, extra_args, fn_name = None):
    self.extra_args = extra_args
    self.fn_name = fn_name 
  
  def __str__(self):
    if self.fn_name:
      return "Too many args (%s) in call to %s" % (self.extra_args, self.fn_name)
    else:
      return "Too many args: %s" % (self.extra_args,)
    
  
class MissingArg(object):
  pass 
# placeholder object 
missing_arg = MissingArg()


class MissingArgsError(Exception):
  def __init__(self, missing_arg_names, fn_name = None, file_name = None, line_no = None):
    self.missing_arg_names = [names.original(name) for name in missing_arg_names]
    
    self.fn_name = fn_name 
    self.file_name = file_name 
    self.line_no = line_no 
    
  def __str__(self):
    if len(self.missing_arg_names) > 1:
      err_str = "Missing args %s" % ", ".join(["'%s'" % name for name in self.missing_arg_names])
    else:
      err_str = "Missing arg '%s'" % self.missing_arg_names[0]
    
    if self.fn_name is not None:
      err_str += " in call to function '%s'" % self.fn_name
      
    if self.file_name is not None:
      err_str += " in file %s" % self.file_name 
    
    if self.line_no is not None:
      err_str += " on line %d" % self.line_no
    return err_str

class FormalArgs(object):
  def __init__(self):
    self.n_args = 0
    self.nonlocals = ()
    self.positional = []

    self.defaults = {}
    self.starargs = None

    # map visible name to local SSA name
    self.local_names = {}
    # map SSA name to visible (keyword) name
    self.visible_names = {}

    # position of each local name in the bound args list
    self.positions = {}

  def _prepend(self, local_name, visible_name = None):
    self.n_args += 1
    if visible_name:
      self.local_names[visible_name] = local_name
      self.visible_names[local_name] = visible_name
    self.arg_slots = [local_name] + self.arg_slots
    for k in self.positions:
      self.positions[k] += 1
    self.positions[local_name] = 0
    self.positional = [local_name] + self.positional

  def add_positional(self, local_name, visible_name = None):
    self.n_args += 1
    if visible_name:
      self.local_names[visible_name] = local_name
      self.visible_names[local_name] = visible_name
    self.positions[local_name] = len(self.positions)
    self.positional.append(local_name)

  def prepend_nonlocal_args(self, localized_names):
    n_old_nonlocals = len(self.nonlocals)
    n_new_nonlocals = len(localized_names)
    total_nonlocals = n_old_nonlocals + n_new_nonlocals
    self.n_args += n_new_nonlocals
    self.nonlocals = self.nonlocals + tuple(localized_names)

    for (k,p) in self.positions.items():
      if p > n_old_nonlocals:
        self.positions[k] = p + total_nonlocals
    for (i, k) in enumerate(localized_names):
      self.positions[k] = n_old_nonlocals + i

  def __str__(self):
    strs = []
    for local_name in self.positional:
      if local_name in self.visible_names:
        s = "%s{%s}" % (local_name, self.visible_names[local_name])
      else:
        s = local_name
      if local_name in self.defaults:
        s += " = %s" % (self.defaults[local_name],)
      strs.append(s)
    if self.starargs:
      strs.append("*" + str(self.starargs))
    if self.nonlocals:
      strs.append("nonlocals = (%s)" % ", ".join(self.nonlocals))
    return ", ".join(strs)

  def __repr__(self):
    return "Args(positional = %s, defaults=%s, starargs = %s, nonlocal = %s)"% \
           (map(repr, self.positional),
            map(repr, self.defaults.items()),
            self.nonlocals,
            self.starargs)

  def bind(self, actuals,
           keyword_fn = None,
           tuple_elts_fn = iter,
           starargs_fn = tuple):
    """
    Like combine_with_actuals but returns a dictionary
    """

    env = {}
    values, extra = self.linearize_values(actuals, keyword_fn, tuple_elts_fn)

    for (k,v) in izip(self.nonlocals + tuple(self.positional), values):
      env[k] = v

    if self.starargs:
      env[self.starargs] = starargs_fn(extra)
    elif len(extra) > 0:
      raise TooManyArgsError(extra)
      
    return env

  def linearize_values(self, actuals, keyword_fn = None, tuple_elts_fn = iter):
    if isinstance(actuals, ActualArgs):
      keyword_values = actuals.keywords
      starargs = actuals.starargs
      positional_values = actuals.positional
    else:
      keyword_values = {}
      starargs = None 
      positional_values = actuals
    
    if len(positional_values) == len(self.positional) and \
       len(keyword_values) == 0 and starargs is None and \
       len(self.defaults) == 0:
      return positional_values, () 
      
    if starargs:
      starargs_elts = tuple(tuple_elts_fn(starargs))
      positional_values = positional_values + starargs_elts

    n = self.n_args
    result = [None] * n
    bound = [False] * n

    def assign(i, v):
      result[i] = v
      assert not bound[i], "%s appears twice in arguments" % self.positional[i]
      bound[i] = True

    if len(positional_values) > n:
      extra = tuple(positional_values[n:])
      positional_values = positional_values[:n]
    else:
      extra = ()

    for (i,p) in enumerate(positional_values):
      assign(i, p)

    for (k,v) in keyword_values.iteritems():
      if k not in self.local_names:
        raise UnexpectedKeyword(k)
      local_name = self.local_names[k]
      assign(self.positions[local_name], v)

    for  (local_name, v) in self.defaults.iteritems():
      i = self.positions[local_name]
      if not bound[i]:
        assign(i, keyword_fn(local_name, v) if keyword_fn else v)
    arg_slots = self.nonlocals + tuple(self.positional)
    missing_args = [arg_slots[i] for i in xrange(n) if not bound[i]]
    if len(missing_args) > 0:
      raise MissingArgsError(missing_args)
    return tuple(result), extra
  

  def linearize_without_defaults(self, actuals, tuple_elts_fn = tuple):
    linear_args, extra = \
        self.linearize_values(actuals, tuple_elts_fn = tuple_elts_fn,
                              keyword_fn = lambda k, v: missing_arg)
    return [x for x in (linear_args + extra) if x is not missing_arg]

  def transform(self, rename_fn = lambda x: x, keyword_value_fn = None):
    args = FormalArgs()

    args.prepend_nonlocal_args(map(rename_fn, self.nonlocals))

    for old_local_name in self.positional:
      new_local_name = rename_fn(old_local_name)
      visible_name = self.visible_names.get(old_local_name)
      args.add_positional(new_local_name, visible_name)
      if old_local_name in self.defaults:
        v = self.defaults[old_local_name]
        if keyword_value_fn:
          v = keyword_value_fn(new_local_name, v)
        args.defaults[new_local_name] = v
    args.starargs = rename_fn(self.starargs) if self.starargs else None
    return args

########NEW FILE########
__FILENAME__ = helpers
import numpy as np 

from .. ndtypes import ( Int8, Int24, Int32, Int64,  Float32, Float64, Bool, FloatT, IntT, BoolT, 
                        NoneType, ScalarT, make_slice_type, make_tuple_type, ClosureT, 
                        FnT, Type, make_closure_type, ArrayT)

from array_expr import Slice
from expr  import Const, Var, Expr, Closure, ClosureElt 
from tuple_expr import Tuple 
from stmt import Return  
from untyped_fn import UntypedFn
from typed_fn import TypedFn 

def const_int(n, t = Int64):
  return Const(n, type = t)

def const_float(f, t = Float64):
  return Const(f, type = t)

def const_bool(b, t = Bool):
  return Const(b, type = t)

def zero(t):
  if isinstance(t, FloatT):
    x = 0.0
  elif isinstance(t, BoolT):
    x = False
  else:
    assert isinstance(t, IntT)
    x = 0
  return Const(x, type = t)

false = zero(Bool)
zero_i8 = zero(Int8)
zero_i24 = zero(Int24)
zero_i32 = zero(Int32)
zero_i64 = zero(Int64)
zero_f32 = zero(Float32)
zero_f64 = zero(Float64)

def one(t):
  if isinstance(t, FloatT):
    x = 1.0
  elif t.__class__ is BoolT:
    x = True
  else:
    assert isinstance(t, IntT)
    x = 1
  return Const(x, type = t)

true = one(Bool)
one_i8 = zero(Int8)
one_i32 = one(Int32)
one_i64 = one(Int64)
one_f32 = one(Float32)
one_f64 = one(Float64)


none_t = NoneType
none = Const(None, type = none_t)

slice_none_t = make_slice_type(none_t, none_t, none_t)
slice_none = Slice(none, none, none, type = slice_none_t)

_py_int_types = (
  int, 
  long, 
  np.int8, 
  np.int16, 
  np.int32, 
  np.int64, 
  np.uint8, 
  np.uint32,
  np.uint64
)
def is_python_int(x):
  return isinstance(x, _py_int_types)

_py_float_types = (float, np.float32, np.float64)

def is_python_float(x):
  return isinstance(x, _py_float_types)

_py_bool_types = (bool, np.bool, np.bool8, np.bool_)
def is_python_bool(x):
  return isinstance(x, _py_bool_types)

def is_python_scalar(x):
  return isinstance(x,  (bool, int, long, float)) or isinstance(x, np.ScalarType)

def is_python_constant(x):
  if isinstance(x, tuple):
    return all(map(is_python_constant, x))
  else:
    return x is None or is_python_scalar(x)

def const_scalar(x):
  if is_python_bool(x):
    return const_bool(x)
  elif is_python_int(x):
    return const_int(x)
  else:
    assert is_python_float(x), "Unexpected value %s" % x 
    return const_float(x)

def make_tuple(elts):
  elt_types = get_types(elts)
  tuple_t = make_tuple_type(elt_types)
  return Tuple(elts, type = tuple_t)

def const_tuple(*python_values):
  return make_tuple(map(const, python_values))

def const(x):
  
  if is_python_scalar(x):
    return const_scalar(x)
  elif isinstance(x, tuple):
    return const_tuple(*map(const, x))
  elif isinstance(x, Expr):
    return x
  else:
    assert x is None, \
        "Can't convert Python value %s into a Parakeet constant" % x
    return none

def unwrap_constant(x):
  if isinstance(x, Expr):
    if x.__class__ is Tuple:
      return tuple(unwrap_constant(elt) for elt in x.elts)
    elif x.__class__ is Slice:
      return slice(
        unwrap_constant(x.start),
        unwrap_constant(x.stop),
        unwrap_constant(x.step))
    elif x.type is NoneType:
      return None
    elif x.__class__ is UntypedFn:
      return x 
    assert x.__class__ is Const, \
        "Expected constant, got %s : %s" % (x, x.type)
    return x.value
  
  else:
    assert is_python_constant(x)
    return x
  
def unwrap_constants(xs):
  return [unwrap_constant(x) for x in xs]

def wrap_if_constant(x):
  """
  If given value isn't already an expression turnhelpers it into one with the const
  helper
  """

  if is_python_constant(x):
    return const(x)
  else:
    assert isinstance(x, Expr), "Expected expression, got " + str(x)
    return x

def wrap_constants(xs):
  return map(wrap_if_constant, xs)

def wrap_var(x):
  if isinstance(x, str):
    return Var(x)
  else:
    assert isinstance(x, Var), "Expected a variable, got %s" % x

def wrap_vars(xs):
  return map(wrap_var, xs)

def get_type(expr):
  return expr.type

def get_types(exprs):
  if hasattr(exprs, 'transform'):
    return exprs.transform(get_type)
  else:
    return [expr.type for expr in exprs]
  
def get_elt_type(expr):
  t = expr.type 
  if isinstance(t, ScalarT):
    return t 
  else:
    assert isinstance(t, ArrayT), "Expected array or scalar, not %s : %s" % (expr, t)
    return t.elt_type 

def get_elt_types(ts):
  return [get_elt_type(t) for t in ts]

def is_zero(expr):
  return expr.__class__ is Const and expr.value == 0

def is_one(expr):
  return expr.__class__ is Const and expr.value == 1

def is_false(expr):
  return expr.__class__ is Const and expr.value == False

def is_true(expr):
  return expr.__class__ is Const and expr.value == True

def is_none(expr):
  return expr.__class__ is Const and expr.value == None

def is_constant(expr):
  return expr.__class__ is Const

def all_constants(exprs):
  return all(map(is_constant, exprs))

def collect_constant(expr):
  return expr.value

def collect_constants(exprs):
  return map(collect_constant, exprs)

def is_scalar(expr):
  return isinstance(expr.type, ScalarT)

def all_scalars(exprs):
  return all(map(is_scalar, exprs))

def is_identity_fn(fn):
  return len(fn.arg_names) == 1 and len(fn.body) == 1 and \
         fn.body[0].__class__ is Return and \
         fn.body[0].value.__class__ is Var and \
         fn.body[0].value.name == fn.arg_names[0]

def get_fn(maybe_closure):
  if isinstance(maybe_closure, TypedFn):
    return maybe_closure 
  elif isinstance(maybe_closure, (FnT, ClosureT, Closure)):
    return maybe_closure.fn 
  elif isinstance(maybe_closure.type, (FnT, ClosureT)):
    return maybe_closure.type.fn 
  else:
    assert False, "Can't get function from %s" % maybe_closure 

def return_type(maybe_closure):
  return get_fn(maybe_closure).return_type

def get_closure_args(maybe_closure):
  if isinstance(maybe_closure, Type):
    assert isinstance(maybe_closure, (FnT, ClosureT))
    maybe_closure = maybe_closure.fn 
  if maybe_closure.__class__ is Closure:
    return tuple(maybe_closure.args)
  elif maybe_closure.type.__class__ is ClosureT:
    return tuple(ClosureElt(maybe_closure, i, type = arg_type)
                 for i, arg_type in enumerate(maybe_closure.type.arg_types))
  else:
    return ()
  
def make_closure(fn, closure_args):
  old_closure_args = get_closure_args(fn)
  fn = get_fn(fn)
  closure_args = tuple(closure_args)
  combined_closure_args = old_closure_args + closure_args 
  t = make_closure_type(fn, combined_closure_args)
  return Closure(fn, combined_closure_args, type = t)
  
def gen_arg_names(n, base_names):
  results = []

  m = len(base_names)
  for i in xrange(n):
    curr = base_names[i % m]
    cycle = i / m
    if cycle > 0:
      curr = "%s_%d" % (curr, cycle+1)
    results.append(curr)
  return results

def gen_data_arg_names(n):
  return gen_arg_names(n, ['x', 'y', 'z', 'a', 'b', 'c', 'd'])

def gen_fn_arg_names(n):
  return gen_arg_names(n, ['f', 'g', 'h', 'p', 'q', 'r', 's'])


########NEW FILE########
__FILENAME__ = list_expr
from seq_expr import SeqExpr 

class List(SeqExpr):
  def __init__(self, elts, type = None, source_info = None):
    self.elts = tuple(elts)
    self.type = type 
    self.source_info = source_info
  
  def children(self):
    return self.elts 
  
########NEW FILE########
__FILENAME__ = low_level
from expr import Expr 
from stmt import Stmt 

class Struct(Expr):
  """
  Eventually all non-scalar data should be transformed to be created with this
  syntax node, signifying explicit struct allocation
  """

  def __init__(self, args, type = None, source_info = None):
    self.args = tuple(args)
    self.type = type 
    self.source_info = source_info 
    
  def __str__(self):
    return "Struct(%s) : %s" % \
           (", ".join(str(arg) for arg in self.args), self.type)

  def children(self):
    return self.args

  def __hash__(self):
    self.args = tuple(self.args)
    return hash(self.args)

class Alloc(Expr):
  """Allocates a block of data, returns a pointer"""
  
  def __init__(self, elt_type, count, type = None, source_info = None):
    self.elt_type = elt_type 
    self.count = count 
    self.type = type 
    self.source_info = source_info
  
  def __str__(self):
    return "alloc<%s>[%s] : %s" % (self.elt_type, self.count, self.type)

  def children(self):
    return (self.count,)

  def __hash__(self):
    return hash((self.elt_type, self.count))

class Free(Expr):
  """Free a manually allocated block of memory"""
  def __init__(self, value, type = None, source_info = None):
    self.value = value 
    self.type = type 
    self.source_info = source_info
    
  def __str__(self):
    return "free(%s)" % self.value 
  
  def children(self):
    yield self.value 
    
  def __hash__(self):
    return hash(self.value)
  
class NumCores(Expr):
  
  """
  Degree of available parallelism, 
  varies depending on backend and how
  ParFor is actually being mapped 
  to executing threads/thread blocks/etc..
  """
  
  def __str__(self):
    return "NUM_CORES"
  
  def __eq__(self, other):
    return other.__class__ is NumCores 
  
  def node_init(self):
    from ..ndtypes import Int64
    self.type = Int64 
  
  def __hash__(self):
    return 0
  
  def children(self):
    return ()

class SourceExpr(Expr):
  """
  Splice this code directly into the low-level representation, 
  should only be used from within a backend that knows what
  the target code should look like 
  """
  def __init__(self, text, type = None, source_info = None):
    self.text = text 
    self.type = type 
    self.source_info = source_info 
  
  def __str__(self):
    return "SourceExpr(%s)" % self.text 

class SourceStmt(Stmt):
  """
  Splice this code directly into the low-level representation, 
  should only be used from within a backend that knows what
  the target code should look like 
  """
  def __init__(self, text, type = None, source_info = None):
    self.text = text 
    self.type = type 
    self.source_info = source_info 
    
  def __str__(self):
    return "SourceStmt(%s)" % self.text 
  
########NEW FILE########
__FILENAME__ = seq_expr
from expr import Expr 

class SeqExpr(Expr):
  def __init__(self, value, type = None, source_info = None):
    self.value = value 
    self.type = type 
    self.source_info = source_info 

  def children(self):
    yield self.value 

class Enumerate(SeqExpr):
  pass 
  
class Zip(SeqExpr):
  def __init__(self, values, type = None, source_info = None):
    self.values = tuple(values) 
    self.type = type 
    self.source_info = source_info
    
  def children(self):
    return self.values

class Len(SeqExpr):
  pass 
  
class Index(SeqExpr):
  """
  TODO: 
    - make all user-defined indexing check_negative=True by default 
    - implement backend logic for lowering check_negative 
  """
  def __init__(self, value, index, check_negative = None, type = None, source_info = None):
    self.value = value 
    self.index = index 
    self.check_negative = check_negative 
    self.type = type 
    self.source_info = source_info
  
  def __eq__(self, other):
    return other.__class__ is Index and \
           other.value == self.value and \
           other.index == self.index
  
  def __hash__(self):
    return hash((self.value, self.index))
  
  def children(self):
    yield self.value
    yield self.index

  def __str__(self):
    return "%s[%s]" % (self.value, self.index)
  

########NEW FILE########
__FILENAME__ = source_info
from collections import namedtuple

SourceInfo = namedtuple('SourceInfo', ('filename', 'line', 'col', 'function'))

########NEW FILE########
__FILENAME__ = stmt
from expr import Expr 
from dsltools import Node

class Stmt(Node):
  _members = ['source_info']

def block_to_str(stmts):
  body_str = '\n'
  body_str += '\n'.join([str(stmt) for stmt in stmts])

  return body_str.replace('\n', '\n    ')

def phi_nodes_to_str(phi_nodes):
  parts = ["%s <- phi(%s, %s)" %
           (var, left, right) for (var, (left, right)) in phi_nodes.items()]
  whole = "\n" + "\n".join(parts)
  # add tabs
  return whole.replace("\n", "\n    ")

class Assign(Stmt):
  _members = ['lhs', 'rhs']

  def __str__(self):
    if hasattr(self.lhs, 'type') and self.lhs.type:
      return "%s : %s = %s" % (self.lhs, self.lhs.type, self.rhs)
    else:
      return "%s = %s" % (self.lhs, self.rhs)

class ExprStmt(Stmt):
  """Run an expression without binding any new variables"""

  _members = ['value']

  def __str__(self):
    assert self.value is not None
    return "ExprStmt(%s)" % self.value

class Comment(Stmt):
  _members = ['text']

  def __str__(self):
    s = "#"
    for (i, c) in enumerate(self.text):
      if i % 78 == 0:
        s += "\n# "
      s += c
    s += "\n#"
    return s


class Return(Stmt):
  _members = ['value']

  def __str__(self):
    return "Return %s" % self.value

class If(Stmt):
  _members = ['cond', 'true', 'false', 'merge']

  def __str__(self):
    s = "if %s:" % self.cond
    if (len(self.true) + len(self.false)) > 0:
      s += "%s\n" % block_to_str(self.true)
    else:
      s += "\n"
    if len(self.false) > 0:
      s += "else:%s\n" % block_to_str(self.false)
    if len(self.merge) > 0:
      s += "(merge-if)%s" % phi_nodes_to_str(self.merge)
    return s

  def __repr__(self):
    return str(self)

class While(Stmt):
  """
  A loop consists of a header, which runs before each iteration, a condition for
  continuing, the body of the loop, and optionally (if we're in SSA) merge nodes
  for incoming and outgoing variables of the form
  [(new_var1, (old_var1,old_var2)]
  """

  _members = ['cond', 'body', 'merge']

  def __repr__(self):
    s = "while %s:\n  "  % self.cond
    if len(self.merge) > 0:
      s += "(header)%s\n  " % phi_nodes_to_str(self.merge)
    if len(self.body) > 0:
      s +=  "(body)%s" % block_to_str(self.body)
    return s

  def __str__(self):
    return repr(self)

class ForLoop(Stmt):
  """
  Having only one loop construct started to become cumbersome, especially now
  that we're playing with loop optimizations.

  So, here we have the stately and ancient for loop.  All hail its glory.
  """

  _members = ['var', 'start', 'stop', 'step', 'body', 'merge']
    
  def __str__(self):
    s = "for %s in range(%s, %s, %s):" % \
      (self.var,
       self.start.short_str(),
       self.stop.short_str(),
       self.step.short_str())

    if self.merge and len(self.merge) > 0:
      s += "\n  (header)%s\n  (body)" % phi_nodes_to_str(self.merge)
    s += block_to_str(self.body)
    return s
  
  
class ParFor(Stmt):
  _members = ['fn', 'bounds', 'read_only', 'write_only']

  def __str__(self):
    return "ParFor(fn = %s, bounds = %s)" % (self.fn, self.bounds)
  
#  
# Consider using these in the future
# instead of having a structured LHS for Assign
#

class SetIndex(Stmt):
  _members = ['array', 'index', 'value']
  
  def __str__(self):
    return "%s[%s] = %s" % (self.array, self.index, self.value)
  

class SetAttr(Stmt):
  _members = ['struct', 'attr', 'value']

  def node_init(self):
    assert isinstance(self.struct, Expr)
    assert isinstance(self.attr, str)
    assert isinstance(self.value, Expr)
    
  def __str__(self):
    return "%s.%s = %s" % (self.struct, self.attr, self.value)
  
class PrintString(Stmt):
  _members = ['text']
  
########NEW FILE########
__FILENAME__ = tuple_expr
from ..ndtypes import make_tuple_type

from expr import Const 
from seq_expr import SeqExpr

class Tuple(SeqExpr):
  def __init__(self, elts, type = None, source_info = None):
    self.elts = tuple(elts)
    self.type = type 
    self.source_info = source_info 
    
    if self.type is None and all(e.type is not None for e in self.elts):
      self.type = make_tuple_type(tuple(e.type for e in self.elts))
  
  def __iter__(self):
    return iter(self.elts)
  
  def __len__(self):
    return len(self.elts)
  
  def __getitem__(self, idx):
    return self.elts[idx]
  
  def __str__(self):
    if len(self.elts) > 0:
      return ", ".join([str(e) for e in self.elts])
    else:
      return "()"
    
  def children(self):
    return self.elts

  def __hash__(self):
    return hash(tuple(self.elts))


class TupleProj(SeqExpr):
  def __init__(self, tuple, index, type = None, source_info = None):
    self.tuple = tuple 
    self.index = index 
    self.type = type 
    self.source_info = source_info 

    if self.type is None:
      if self.tuple.type is not None and self.index.__class__ is Const:
        self.type = self.tuple.type.elt_types[self.index.value]
            
  def __str__(self):
    return "TupleProj(%s, %d)" % (self.tuple, self.index)

  def children(self):
    return (self.tuple,)

  def __hash__(self):
    return hash((self.tuple, self.index))
########NEW FILE########
__FILENAME__ = typed_fn

from .. ndtypes import make_fn_type, Type 


from expr import Expr 
from stmt import block_to_str

################################################################################
#
#  Constructs below here are only used in the typed representation
#
################################################################################
class TransformHistory(object):
  """
  Sequence of transforms which have been applied to a function, 
  used for caching to avoid repeating a transformation
  """
  
  def __init__(self, transforms = None):
    self.transforms = () if transforms is None else transforms 
    self.transform_set = frozenset(self.transforms) 
    self._hash = hash(self.transform_set)
    
  def __contains__(self, T):
    return T in self.transform_set
  
  def add(self, T):
    self.transforms = self.transforms + (T,)
    self.transform_set = self.transform_set.union(frozenset([T]))
    self._hash = hash(self.transform_set)
  

  
  @property
  def cache_key(self):
    return self.transform_set
  
  def __hash__(self):
    return self._hash
  
  def __eq__(self, other):
    return self.transform_set == other.transform_set
  
  def __ne__(self, other):
    return self.transform_set != other.transform_set 
  
  def __str__(self):
    return "TransformHistory(%s)" % str(self.transforms)
  
  def __repr__(self):
    return str(self)
  
  def copy(self):
    return TransformHistory()

class TypedFn(Expr):
  """
  The body of a TypedFn should contain Expr nodes which have been extended with
  a 'type' attribute
  """
  
  def __init__(self, name, arg_names, body, 
                input_types, return_type,
                type_env, 
                created_by = None,
                transform_history = None,  
                source_info = None):
    
    assert isinstance(name, str), "Invalid typed function name: %s" % (name,)
    self.name = name 
    
    assert isinstance(arg_names, (list, tuple)), "Invalid typed function arguments: %s" % (arg_names,)
    self.arg_names = arg_names 

    assert isinstance(input_types, (list, tuple)), "Invalid input types: %s" % (input_types,)
    self.input_types = tuple(input_types)    
    
    assert isinstance(return_type, Type), "Invalid return type: %s" % (return_type,)
    self.return_type = return_type 
    
    assert isinstance(body, list), "Invalid body for typed function: %s" % (body,)
    self.body = body 
    
    assert isinstance(type_env, dict), "Invalid type environment: %s" % (type_env,)
    self.type_env = type_env 

    self.type = make_fn_type(self.input_types, self.return_type)
    
    self.created_by = created_by 
    
    if transform_history is None: 
      transform_history = TransformHistory()
    self.transform_history = transform_history
    
    self.source_info = source_info 


  @property 
  def cache_key(self):
    return self.name, self.created_by, self.transform_history
  
  @property
  def version(self):
    return self.transform_history.cache_key 
  
  def __repr__(self):
    arg_strings = []

    for name in self.arg_names:
      arg_strings.append("%s : %s" % (name, self.type_env.get(name)))

    return "function %s(%s) => %s:%s" % \
           (self.name, ", ".join(arg_strings),
            self.return_type,
            block_to_str(self.body))

  def __str__(self):
    return repr(self)
    # return "TypedFn(%s : %s => %s)" % (self.name, self.input_types, self.return_type)

  def __hash__(self):
    return hash(self.name) + hash(self.created_by)

  def children(self):
    return ()

########NEW FILE########
__FILENAME__ = type_value
from .. ndtypes import TypeValueT 
from expr import Expr 


class TypeValue(Expr):
  """
  Value materialization of a type 
  """
  def __init__(self, type_value, type = None, source_info = None):
    self.type_value = type_value
     
    if type is None: type = TypeValueT(self.type_value)
    assert isinstance(type, TypeValueT)
    assert type.type is not None 
    
    self.type = type 
     
    
########NEW FILE########
__FILENAME__ = untyped_fn

from ..ndtypes import make_closure_type
from expr import Expr
from formal_args import FormalArgs 
from stmt import block_to_str


class UntypedFn(Expr):
  """
  Function definition.
  A top-level function can have references to python values from its enclosing
  scope, which are stored in the 'python_refs' field.

  A nested function, on the other hand, might refer to some variables from its
  enclosing Parakeet scope, whose original names are stored in
  'parakeet_nonlocals'
  """
    
  registry = {}

  def __init__(self, name, args, body, 
               python_refs = None, 
               parakeet_nonlocals = None, 
               doc_string = None, 
               source_info = None):
    assert isinstance(name, str), "Expected string for fn name, got %s" % name
    self.name = name 
    
    assert isinstance(args, FormalArgs), \
        "Expected arguments to fn to be FormalArgs object, got %s" % self.args
    self.args = args 
    
    assert isinstance(body, list), \
        "Expected body of fn to be list of statements, got " + str(body)
    self.body = body 
    
    self.python_refs = python_refs 
    self.parakeet_nonlocals = parakeet_nonlocals 
    
    self.type = make_closure_type(self, ())
    
    self.source_info = source_info 
    self.registry[self.name] = self

    
  
  def __repr__(self):
    return "def %s(%s):%s" % (self.name, self.args, block_to_str(self.body))
  
  def __str__(self):
    return repr(self)
  
  def __hash__(self):
    return hash(self.name)

  def python_nonlocals(self):
    if self.python_refs:
      return [ref.deref() for ref in self.python_refs]
    else:
      return []

  def children(self):
    return ()

########NEW FILE########
__FILENAME__ = wrappers
from .. import names, prims  
from ..ndtypes import ScalarT, Type, type_conv  
from .. syntax import  FormalArgs, Var, UntypedFn, Return, PrimCall, Expr, Cast 


_untyped_fn_cache = {}
def simple_untyped_fn(name, 
                        expr,
                        n_inputs = 1, 
                        fixed_args  = [], 
                        keyword_args = {},                         
                        unpack = False):
  key = name, expr, n_inputs, tuple(fixed_args), tuple(keyword_args.items()), unpack
  if key in _untyped_fn_cache:
    return _untyped_fn_cache[key]
  
  fn_name = names.fresh(name)
  args_obj = FormalArgs()

  arg_vars = []
  for name in names.fresh_list(n_inputs):
    args_obj.add_positional(name)
    arg_vars.append(Var(name))
  
  if unpack:
    combined_args = tuple(fixed_args) + tuple(arg_vars)
  else:
    combined_args = tuple(fixed_args) + (tuple(arg_vars),)
  result = expr(*combined_args, **keyword_args)
  body = [Return(result)]
  fundef = UntypedFn(fn_name, args_obj, body, [])
  _untyped_fn_cache[key] = fundef 
  return fundef 
  
def build_untyped_prim_fn(p):
  """Given a primitive, return an untyped function which calls that prim"""
  assert isinstance(p, prims.Prim), "Expected Prim but got %s" % p 
  return simple_untyped_fn(p.name, PrimCall, p.nin, [p])


def build_untyped_expr_fn(expr, n_args = 1):
  """Given an expression, return a function which applies that expression to arguments"""
  return simple_untyped_fn(expr.__name__ + "_fn", expr, n_args)
    
_untyped_cast_wrappers = {}
def build_untyped_cast_fn(t):
  if not isinstance(t, Type):
    t = type_conv.equiv_type(t)
  assert isinstance(t, ScalarT), "Expected scalar type but got %s" % t 
  return simple_untyped_fn("cast_" + str(t), Cast, 1, keyword_args = {'type': t})
  
########NEW FILE########
__FILENAME__ = system_info
import os
import subprocess
import platform 


mac_os = platform.system() == 'Darwin'
windows = platform.system() == 'Windows'

def check_openmp_available():
  cmd = """echo "int main() {}" | clang -fopenmp -x c++ -"""
  with open(os.devnull, 'w') as devnull:
    p = subprocess.Popen(cmd, shell=True, stderr = devnull, stdout = devnull)
  p.wait()
  code = p.returncode
  return code == 0

openmp_available = check_openmp_available()
########NEW FILE########
__FILENAME__ = testing_helpers
import sys
import time 
import numpy as np

from nose.tools import nottest

from dsltools.testing_helpers import eq, expect_eq, run_local_tests
  
from . import (
  type_conv, type_inference, config, 
  translate_function_value, find_broken_transform,
  run_untyped_fn, openmp_available
)




def _copy(x):
  if isinstance(x, np.ndarray):
    return x.copy()
  else:
    return x

def _copy_list(xs):
  return [_copy(x) for x in xs]


def expect(fn, args, expected, msg = None, valid_types = None):
  """
  Helper function used for testing, assert that Parakeet evaluates given code to
  the correct result
  """
  if hasattr(expected, 'dtype') and expected.dtype == 'float16':
    expected = expected.astype('float32')

  untyped_fn = translate_function_value(fn)
  
  available_backends = ['interp', 'c']
  if openmp_available:
    available_backends.append('openmp')
    
  #import cuda_backend 
  #if cuda_backend.device_info.has_gpu():
  #  available_backends.append('cuda')
     

  for backend in available_backends: #available_backends:
    try: 
      result = run_untyped_fn(untyped_fn, _copy_list(args), backend = backend)
      
    except: 
      if config.testing_find_broken_transform: 
        find_broken_transform(fn, args, expected)
      raise
   
    label = "backend=%s, inputs = %s" % (backend, ", ".join(str(arg) for arg in args))

    if msg is not None:
      label += ": " + str(msg)
    
    try: 
      #if hasattr(result, 'flags'):
      #  print result.flags
      expect_eq(result, expected, label)
    except: 
      if config.testing_find_broken_transform: 
        find_broken_transform(fn, args, expected)
      raise 
    
    if valid_types is not None:
      if not isinstance(valid_types, (tuple, list)):
        valid_types = [valid_types]
      assert type(result) in valid_types, \
        "Expected result to have type in %s but got %s" % (valid_types, type(result))
  
def expect_each(parakeet_fn, python_fn, inputs):
  for x in inputs:
    expect(parakeet_fn, [x], python_fn(x))

def expect_allpairs(parakeet_fn, python_fn, inputs):
  for x in inputs:
    for y in inputs:
      expect(parakeet_fn, [x,y], python_fn(x,y))

def return_type(fn, input_types):
  untyped_fundef = translate_function_value(fn)
  closure_args = untyped_fundef.python_nonlocals()
  closure_arg_types = map(type_conv.typeof, closure_args)
  return type_inference.infer_return_type(untyped_fundef,
                                          closure_arg_types + input_types)

def expect_type(fn, input_types, output_type):
  actual = return_type(fn, input_types)
  assert actual == output_type, "Expected type %s, actual %s" % \
                                (output_type, actual)

def assert_eq_arrays(numpy_result, parakeet_result, test_name = None):
  if test_name is None:
    msg = ""
  else:
    msg = "[%s] " % test_name
  assert type(numpy_result) == type(parakeet_result), \
    "%sExpected type %s but got %s" % (msg, type(numpy_result), type(parakeet_result))
  if hasattr(numpy_result, 'shape'):
    assert hasattr(parakeet_result, 'shape')
    assert numpy_result.shape == parakeet_result.shape, \
      "%sExpected shape %s but got %s" % (msg, numpy_result.shape, parakeet_result.shape)
    assert eq(numpy_result, parakeet_result), \
      "%sExpected value %s but got %s" % (msg, numpy_result, parakeet_result)
    
@nottest
def timed_test(parakeet_fn, parakeet_args, python_fn, 
               python_args = None, min_speedup = None):
  if python_args is None:
    python_args = parakeet_args

  start = time.time()
  _ = parakeet_fn(*parakeet_args)
  parakeet_time_with_comp = time.time() - start 

  start = time.time()
  py_result = python_fn(*python_args)
  py_time = time.time() - start 

  start = time.time()
  parakeet_result = parakeet_fn(*parakeet_args)
  parakeet_time_no_comp = time.time() - start 

  print "Parakeet time (with compilation):", parakeet_time_with_comp
  print "Parakeet time (without compilation):", parakeet_time_no_comp
  print "Python time:", py_time 

  assert eq(parakeet_result, py_result), \
    "Expected %s but got %s" % (py_result, parakeet_result)
  if min_speedup is not None:
    assert py_time / parakeet_time_no_comp > min_speedup, \
        "Parakeet too slow: %.2f slowdown" % (parakeet_time_no_comp/py_time)

########NEW FILE########
__FILENAME__ = clone_function
from .. import names 
from .. syntax import (TypedFn, Var, Const, Attribute, Index, PrimCall, 
                       If, Assign, While, ExprStmt, Return, ForLoop, ParFor,  
                       Slice, Struct, Tuple, TupleProj, Cast, Alloc, Closure, 
                       Map, Reduce, Scan, IndexMap, IndexReduce, IndexScan, 
                       UntypedFn )      
from transform import Transform 
from parakeet.syntax.delay_until_typed import DelayUntilTyped

class CloneFunction(Transform):
  """
  Copy all the objects in the AST of a function
  """
  def __init__(self, parent_transform = None, 
                      rename = False):
    Transform.__init__(self)
    self.rename = rename 
    # self.recursive = recursive
    self.parent_transform = parent_transform 
     
  def transform_Var(self, expr):
    return Var(expr.name, type = expr.type)
  
  def transform_expr(self, expr):
    new_expr = self._transform_expr(expr)
    new_expr.type = expr.type 
    new_expr.source_info = expr.source_info 
    return new_expr 
  
  def _transform_expr(self, expr):
    c = expr.__class__
    if c is Var:
      return Var(expr.name)
      
    elif c is Const:
      return Const(expr.value)
    
    elif c is Tuple:
      new_elts = tuple(self.transform_expr(elt) for elt in expr.elts)
      return Tuple(elts = new_elts)
    
    elif c is Attribute:
      value = self.transform_expr(expr.value)
      return Attribute(value, expr.name)
    
    elif c is Index:
      value = self.transform_expr(expr.value)
      index = self.transform_expr(expr.index)
      return Index(value, index)
    
    elif c is PrimCall:
      args = tuple(self.transform_expr(elt) for elt in expr.args)
      return PrimCall(expr.prim, args)
    
    elif c is TypedFn:
      #if self.recursive:
      #  cloner = CloneFunction(rename = self.rename, recursive = True)
      #  return cloner.apply(expr)
      #else:
      return expr 
    elif c is UntypedFn:
      return expr 
    elif c is Closure: 
      args = self.transform_expr_tuple(expr.args)
      return Closure(fn = expr.fn, args = args)
    
    elif c is Slice: 
      start = self.transform_if_expr(expr.start)
      stop = self.transform_if_expr(expr.stop)
      step = self.transform_if_expr(expr.step)
      return Slice(start, stop, step, type = expr.type)
    
    elif c is Struct: 
      new_args = self.transform_expr_list(expr.args)
      return Struct(args = new_args, type = expr.type)
    
    elif c is TupleProj: 
      new_tuple = self.transform_expr(expr.tuple)
      return TupleProj(new_tuple, expr.index, type = expr.type)
    
    elif c is Cast:
      return Cast(self.transform_expr(expr.value), expr.type)
     
    elif c is Alloc:
      return Alloc(count = self.transform_expr(expr.count), 
                   elt_type = expr.elt_type)
    elif c is Map:
      return Map(fn = self.transform_expr(expr.fn), 
                  args = self.transform_expr_list(expr.args), 
                  axis = self.transform_if_expr(expr.axis), 
                 )
    elif c is Reduce: 
      return Reduce(fn = self.transform_expr(expr.fn), 
                    combine = self.transform_expr(expr.combine), 
                    args = self.transform_expr_list(expr.args), 
                    axis = self.transform_if_expr(expr.axis), 
                    init = self.transform_if_expr(expr.init), 
                  )
     
    else:
      args = {}  
      for k,v in expr.__dict__.iteritems():
        args[k] = self.transform_if_expr(v)
      if c is DelayUntilTyped and 'type' in args:
        del args['type']
      return c(**args)
  
  def transform_Assign(self, stmt):

    new_lhs = self.transform_expr(stmt.lhs)
    new_rhs = self.transform_expr(stmt.rhs)
    return Assign(new_lhs, new_rhs)

  def transform_ExprStmt(self, stmt):
    return ExprStmt(self.transform_expr(stmt.value))

  def transform_Return(self, stmt):
    res = Return(self.transform_expr(stmt.value))
    return res 
  
  def transform_If(self, stmt):
    new_true = self.transform_block(stmt.true)
    new_false = self.transform_block(stmt.false)
    new_merge = self.transform_merge(stmt.merge)
    new_cond = self.transform_expr(stmt.cond)
    return If(new_cond, new_true, new_false, new_merge)
  
  def transform_While(self, stmt):
    new_body = self.transform_block(stmt.body)
    new_merge = self.transform_merge(stmt.merge)
    new_cond = self.transform_expr(stmt.cond)
    return While(new_cond, new_body, new_merge)
  
  def transform_ForLoop(self, stmt):
    new_var = self.transform_expr(stmt.var)
    new_start = self.transform_expr(stmt.start)
    new_stop = self.transform_expr(stmt.stop)
    new_step = self.transform_expr(stmt.step)
    new_body = self.transform_block(stmt.body)
    new_merge = self.transform_merge(stmt.merge)
    return ForLoop(new_var, new_start, new_stop, new_step, new_body, new_merge)  
  
  def transform_ParFor(self, stmt):
    new_bounds = self.transform_expr(stmt.bounds)
    new_fn = self.transform_expr(stmt.fn)
    return ParFor(fn = new_fn, bounds = new_bounds, 
                  read_only = stmt.read_only, write_only = stmt.write_only)
  
  def pre_apply(self, old_fn):
    new_fundef_args = old_fn.__dict__.copy()
    del new_fundef_args['type']
    # new_fundef_args = dict([(m, getattr(old_fn, m)) for m in old_fn._members])
    # create a fresh function with a distinct name and the
    # transformed body and type environment
    if self.rename: 
      new_fundef_args['name'] = names.refresh(old_fn.name)
    else:
      new_fundef_args['name'] = old_fn.name
       
    new_fundef_args['type_env'] = old_fn.type_env.copy()
    new_fundef_args['transform_history'] = old_fn.transform_history.copy()
    new_fundef_args['created_by'] = self.parent_transform
    # don't need to set a new body block since we're assuming 
    # that transform_block will at least allocate a new list 
    new_fundef = TypedFn(**new_fundef_args)
    return new_fundef 


########NEW FILE########
__FILENAME__ = clone_stmt
from .. import names 
from .. analysis.collect_vars import  collect_binding_names
from .. syntax import Var, Assign, ForLoop
from clone_function import CloneFunction
from transform import Transform

class CloneStmt(CloneFunction):
  def __init__(self, outer_type_env):
    Transform.__init__(self)
    self.recursive = False
    self.type_env = outer_type_env
    self.rename_dict = {}

  def rename(self, old_name):
    old_type = self.type_env[old_name]
    new_name = names.refresh(old_name)
    new_var = Var(new_name, old_type)
    self.rename_dict[old_name] = new_var
    self.type_env[new_name] = old_type
    return new_name

  def rename_var(self, old_var):
    new_name = names.refresh(old_var.name)
    new_var = Var(new_name, old_var.type)
    self.rename_dict[old_var.name] = new_var
    self.type_env[new_name] = old_var.type
    return new_var

  def transform_merge(self, merge):
    new_merge = {}
    for (old_name, (l,r)) in merge.iteritems():
      new_name = self.rename(old_name)
      new_left = self.transform_expr(l)
      new_right = self.transform_expr(r)
      new_merge[new_name] = (new_left, new_right)
    return new_merge

  def transform_merge_before_loop(self, merge):
    new_merge = {}
    for (old_name, (l,r)) in merge.iteritems():
      new_name = self.rename(old_name)

      new_left = self.transform_expr(l)
      new_merge[new_name] = (new_left, r)
    return new_merge

  def transform_merge_after_loop(self, merge):
    for (new_name, (new_left, old_right)) in merge.items():
      merge[new_name] = (new_left, self.transform_expr(old_right))
    return merge

  def transform_Assign(self, expr):
    for name in collect_binding_names(expr.lhs):
      self.rename(name)
    new_lhs = self.transform_expr(expr.lhs)
    new_rhs = self.transform_expr(expr.rhs)
    return Assign(new_lhs, new_rhs)

  def transform_Var(self, expr):
    return self.rename_dict.get(expr.name, expr)

  def transform_ForLoop(self, stmt):
    new_var = self.rename_var(stmt.var)

    merge = self.transform_merge_before_loop(stmt.merge)
    new_start = self.transform_expr(stmt.start)
    new_stop = self.transform_expr(stmt.stop)
    new_step = self.transform_expr(stmt.step)
    new_body = self.transform_block(stmt.body)
    merge = self.transform_merge_after_loop(merge)
    return ForLoop(new_var, new_start, new_stop, new_step, new_body, merge)

########NEW FILE########
__FILENAME__ = combine_nested_maps
from collections import namedtuple

from ..analysis import collect_var_names_from_exprs
from ..ndtypes import TupleT 
from ..syntax import (Return, Map, OuterMap, IndexMap, Tuple, TupleProj, Var, Closure, Assign, 
                      Const, TypedFn, UntypedFn, Ravel, Shape, Strides, Transpose, 
                      Reshape, Where, Compress) 
from ..syntax.helpers import none   

from subst import subst_expr_list
from clone_function import CloneFunction
from transform import Transform

class CombineFailed(Exception):
  pass 

class CombineNestedMaps(Transform):
  
  #def translate_expr(self, expr, mapping, forbidden = set([])):
  #  try:
  #    return self._translate_expr(expr, mapping, forbidden)
  #  except CombineFailed:
  #    return None 
  
  def translate_exprs(self, exprs, mapping, forbidden):
      results = tuple(self.translate_expr(elt, mapping, forbidden) 
                      for elt in exprs)
      if any(elt is None for elt in results): 
        raise CombineFailed()
      else:
        return results
  
  def translate_expr(self, expr, mapping, forbidden = set([])):
    c = expr.__class__ 
    if c is Var:
      if expr.name not in forbidden and expr.name in mapping: 
        return mapping[expr.name]
      else:
        raise CombineFailed()
    elif c in (Const, TypedFn, UntypedFn):
      return expr
    elif c is Tuple:
      elts = self.translate_exprs(expr.elts, mapping, forbidden)
      return Tuple(elts = elts, type = expr.type)
    elif c is Closure:
      args = self.translate_exprs(expr.args, mapping, forbidden)
      return Closure(fn = expr.fn, 
                     args = args, 
                     type = expr.type)
    elif c in (Ravel, Shape, Strides, Transpose, Where):
      array = self.translate_expr(expr.array, mapping, forbidden)
      if array is None: 
        raise CombineFailed()
      return c(array = array, type = expr.type)
    elif c is Compress:
      condition = self.translate_expr(expr.condition, mapping, forbidden)
      data = self.translate_expr(expr.data, mapping, forbidden)
      if condition is None or data is None: 
        raise CombineFailed()
      return c(condition = condition, 
               data = data, 
               type = expr.type)
    elif c is TupleProj:
      tup = self.translate_expr(expr.tuple, mapping, forbidden)
      if tup is None: 
        raise CombineFailed()
      return c(tuple = tup, 
               index = expr.index,  
               type = expr.type)
    elif c is Reshape:
      array = self.translate_expr(expr.array, mapping,forbidden)
      shape = self.translate_expr(expr.shape, mapping,forbidden)
      if array is None or shape is None:
        raise CombineFailed()
      return Reshape(array = array, shape = shape,  type = expr.type)

  def build_arg_mapping(self, fn, closure_elts, outer_args = None):  
    if outer_args:
      combined_args = tuple(closure_elts) + tuple(outer_args)
    else:
      combined_args = closure_elts 
    mapping = {}
    for name, expr in zip(fn.arg_names, combined_args):
      mapping[name] = expr  
  
    # don't allow transformations of array elements
    # since that's an interaction between levels of the nested
    # maps 
    forbidden = set(fn.arg_names[len(closure_elts):])
    for stmt in fn.body[:-1]:
      if stmt.__class__ is Assign and stmt.lhs.__class__ is Var:
        try:
          new_rhs = self.translate_expr(stmt.rhs, mapping,forbidden)
        except CombineFailed:
          return None 
        else:
          mapping[stmt.lhs.name] = new_rhs
      else:
        return None 
    return mapping 
  
  def dissect_nested_fn(self, fn, valid_adverb_classes = (Map,OuterMap)):
    if len(fn.body) == 0: 
      return None 
    
    stmt = fn.body[-1]
    
    if stmt.__class__ is not Return: 
      return None 
    
    nested_expr = stmt.value 
    if nested_expr.__class__ not in valid_adverb_classes: 
      return None
    
    return nested_expr
    
    
  
  def combine_maps(self, closure, outer_args, outer_axis, result_type):
    fn = self.get_fn(closure)
    closure_elts = self.closure_elts(closure)
    
    nested_expr = self.dissect_nested_fn(fn)
    if nested_expr is None: return None
     
    arg_mapping = self.build_arg_mapping(fn, closure_elts, outer_args)
    if arg_mapping is None: return None 
    
    nested_outer_args = nested_expr.args
    n_nested_outer_args = len(nested_outer_args)
    nested_map_closure_elts = self.closure_elts(nested_expr.fn) 
    nested_fn = self.get_fn(nested_expr.fn)
    nested_axis = nested_expr.axis
    
    # either both Maps specify an axis of traversal or neither do  
    if self.is_none(outer_axis) != self.is_none(nested_axis): return None 
    
    
    n_array_args = len(outer_args)
    
    inner_array_names = fn.arg_names[-n_array_args:]
    if len(nested_map_closure_elts) < len(inner_array_names):  return None 


    
    # if there's ohe outer arg and it's stuck at the back of the
    # nested args list, try permuting the arguments to make a nested fnh
    # that's compatible with OuterMap   
    if n_array_args == 1 and len(nested_map_closure_elts) > 1 and \
        nested_map_closure_elts[0].__class__ is Var and \
        nested_map_closure_elts[0].name == fn.arg_names[-1]:
      permute_fn = CloneFunction().apply(nested_fn)
      permute_input_types = list(permute_fn.input_types)
      permute_arg_names = list(permute_fn.arg_names)
      permute_closure_elts = list(nested_map_closure_elts)
      first_name = permute_arg_names[0]
      first_type = permute_input_types[0]
      first_closure_elt = permute_closure_elts[0]
      permute_input_types[:-1] = permute_input_types[1:]
      permute_arg_names[:-1] = permute_arg_names[1:]
      permute_closure_elts[:-1] = permute_closure_elts[1:]
      permute_input_types[-1] = first_type
      permute_arg_names[-1] = first_name
      permute_closure_elts[-1] = first_closure_elt
      permute_fn.input_types = tuple(permute_input_types)
      permute_fn.arg_names = tuple(permute_arg_names)
      nested_fn = permute_fn 
      nested_map_closure_elts = tuple(permute_closure_elts)
    
    # if any of the last k closure arguments aren't array elements
    # then abandon ship 
    if any(closure_expr.__class__ is not Var or closure_expr.name not in inner_array_names
           for closure_expr in nested_map_closure_elts[-n_array_args:]):
        return None 
    
    try:
      remapped_inner_outer_args = [self.translate_expr(e,arg_mapping)
                                   for e in nested_outer_args]
      remapped_inner_closure_elts = [self.translate_expr(e, arg_mapping)
                                     for e in nested_map_closure_elts]
    except CombineFailed:
      return None   
    # if the two Maps are both elementwise, then make the OuterMap 
    # also elementwise


    
    new_closure_elts = remapped_inner_closure_elts[:-n_array_args]
    new_outer_args = remapped_inner_closure_elts[-n_array_args:] + remapped_inner_outer_args    

    if self.is_none(outer_axis):
      combined_axis = none
    else:
      # combine the axes since we're going to make a single OuterMap 
      if isinstance(outer_axis.type, TupleT):
        outer_axes = self.tuple_elts(outer_axis)
      else:
        outer_axes = (outer_axis,) * n_array_args
      if isinstance(nested_axis.type, TupleT):
        assert isinstance(nested_axis, Tuple),  \
          "Axis argument must be None, int, or literal tuple, not %s" % nested_axis 
        inner_axes = tuple(nested_axis.elts) 
      else:
        inner_axes = (nested_axis,) * n_nested_outer_args
       
      combined_axes = list(outer_axes + inner_axes)
      for i, array_arg in enumerate(new_outer_args):
        if array_arg.__class__ is Transpose:
          old_axis = combined_axes[i]
          if not self.is_none(old_axis):
            combined_axes[i] = Const(value = array_arg.type.rank - old_axis.value - 1, 
                                     type = old_axis.type)
            new_outer_args[i] = array_arg.array
      combined_axis = self.tuple(combined_axes)
    return  OuterMap(fn = self.closure(nested_fn, new_closure_elts),
                     args = tuple(new_outer_args),   
                     axis = combined_axis, 
                     type = result_type)
    
  
  def combine_index_maps(self, closure, shape, result_type):
    fn = self.get_fn(closure)
    closure_elts = self.closure_elts(closure)
    
    #n_outer_args = len(fn.input_types)
    n_outer_closure_args = len(closure_elts)
    #n_outer_indices = n_outer_args - n_outer_closure_args
    outer_arg_names = fn.arg_names
    outer_index_names = outer_arg_names[n_outer_closure_args:]
    print "OUTER ARG NAMES", outer_arg_names
    print "OUTER INDEX NAMES", outer_index_names
    #outer_index_types = fn.input_types[n_outer_closure_args:]
    
    nested_expr = self.dissect_nested_fn(fn, (IndexMap,))
    if nested_expr is None: return None
    
    inner_shape = nested_expr.shape 
    
    arg_mapping = self.build_arg_mapping(fn, closure_elts)
    if arg_mapping is None:  return None
    

    nested_map_closure_elts = self.closure_elts(nested_expr.fn)
    
      
    # inner closure args must be remapped to the index args of the outer fn 
    
    remapped_closure_args = []
    fixed_closure_args_done = False
    for i, nested_closure_elt in enumerate(nested_map_closure_elts):
      if nested_closure_elt.__class__ is not Var:
        return None
      nested_closure_name = nested_closure_elt.name
      
      if nested_closure_name in outer_index_names:
        fixed_closure_args_done = True
        pos = outer_index_names.index(nested_closure_name)
        # for now, can't change order of arguments
        if i != pos:
          return None
      else:
        # can't do static/data closure args, followed by indices,
        # followed by more data args
        # TODO: just a new function and permute all the arguments 
        if fixed_closure_args_done:
          return None
        elif nested_closure_name not in arg_mapping:
          return None
        remapped_closure_args.append(arg_mapping[nested_closure_name])
          
        
    try:
      remapped_inner_shape = self.translate_expr(inner_shape, arg_mapping)
    except CombineFailed:
      return None
    
    
    nested_fn = self.get_fn(nested_expr.fn)
      
    # if the two Maps are both elementwise, then make the OuterMap 
    # also elementwise

    combined_shape = self.concat_tuples(shape, remapped_inner_shape, "combined_shape")
    return  IndexMap(fn = self.closure(nested_fn, tuple(remapped_closure_args)),
                     shape = combined_shape,    
                     type = result_type)
  
  def transform_Map(self, expr):
    # can't turn Map(-, x, y) into an OuterMap since (x,y) are at the same iteration level 
    if len(expr.args) != 1:
      return expr
    new_expr = self.combine_maps(expr.fn, expr.args, expr.axis, expr.type)
    if new_expr is None: return expr 
    else: return new_expr 
  
  def transform_OuterMap(self, expr):
    new_expr = self.combine_maps(expr.fn, expr.args, expr.axis, expr.type)
    if new_expr is None: return expr 
    else: return new_expr 
  
  def transform_IndexMap(self, expr):
    return self.combine_index_maps(expr.fn, expr.shape, expr.type)
      
########NEW FILE########
__FILENAME__ = copy_elimination
from .. ndtypes import ScalarT, ArrayT

from .. analysis.collect_vars import collect_var_names
from .. analysis import escape_analysis
from .. analysis.find_local_arrays import FindLocalArrays
from .. analysis.usedef import UseDefAnalysis
from .. syntax import AllocArray, ArrayView, Index, Struct, Var

from .. syntax import Adverb
from transform import Transform

class CopyElimination(Transform):
  def apply(self, fn):
    if all(isinstance(t, ScalarT) for t in fn.type_env.itervalues()):
      return fn
    else:
      return Transform.apply(self, fn)

  def pre_apply(self, fn):
    local_array_analysis = FindLocalArrays()
    local_array_analysis.visit_fn(fn)


    self.local_alloc = local_array_analysis.local_allocs
    self.local_arrays = local_array_analysis.local_arrays

    escape_info = escape_analysis(fn)
    self.may_escape = escape_info.may_escape
    self.may_alias = escape_info.may_alias
    self.may_return = escape_info.may_return
    
    self.usedef = UseDefAnalysis()
    self.usedef.visit_fn(fn)

    
    self.pointers_by_size = {}
    self.arrays_by_size = {}
    

  def no_array_aliases(self, array_name):
    alias_set = self.may_alias.get(array_name, [])
    array_aliases = [name for name in alias_set
                     if self.type_env[name].__class__ is ArrayT]
    # you're allowed one alias for yourself, but
    # any extras are other arrays with whom you share data
    # BEWARE: this will get convoluted and probably broken
    # if we ever have mutable compound objects in arrays
    return len(array_aliases) <= 1

  def is_array_alloc(self, expr):
    return expr.__class__ in [ArrayView, Struct, AllocArray] or isinstance(expr, Adverb)
    
  def transform_Assign(self, stmt):
    # pattern match only on statements of the form
    # dest[complex_indexing] = src
    # when:
    #   1) dest hasn't been used before as a value
    #   2) src doesn't escape
    #   3) src was locally allocated
    # ...then transform the code so instead of allocating src

    if stmt.lhs.__class__ is not Index or stmt.lhs.value.__class__ is not Var:
      return stmt 
    
    lhs_name = stmt.lhs.value.name

    # why assign to an array if it never gets used?
    if lhs_name not in self.usedef.first_use and \
       lhs_name not in self.may_escape and \
       self.no_array_aliases(lhs_name):
      return None
    
    # only match statements like array[idx] = some_rhs_var
    if stmt.lhs.type.__class__ is not ArrayT or stmt.rhs.__class__ is not Var:
      return stmt 
    
    curr_path = self.usedef.stmt_paths[id(stmt)]
    rhs_name = stmt.rhs.name

    
    if lhs_name in self.usedef.first_use and self.usedef.first_use[lhs_name] <= curr_path:
      return stmt 
    
    if self.usedef.last_use[rhs_name] != curr_path:
      return stmt
    
    if rhs_name in self.may_return:
      return stmt  
    
    if rhs_name not in self.local_arrays:
      return stmt 
            
    array_stmt = self.local_arrays[rhs_name]
    prev_path = self.usedef.stmt_paths[id(array_stmt)]
    if not self.is_array_alloc(array_stmt.rhs):
      return stmt 
    
    for lhs_depends_on in collect_var_names(stmt.lhs):
      created_on = self.usedef.created_on.get(lhs_depends_on)
      if created_on is None or created_on >= prev_path:
        return stmt 
             
    array_stmt.rhs = stmt.lhs
    return None
    
########NEW FILE########
__FILENAME__ = dead_code_elim


from .. import syntax
from .. analysis.collect_vars import collect_var_names_list
from .. analysis.use_analysis import use_count
from .. syntax import Assign, Const, Index, PrimCall, Tuple, TupleProj, Var, Closure, ClosureElt 
from transform import Transform


class DCE(Transform):
  def __init__(self):
    Transform.__init__(self, reverse = True)

  def pre_apply(self, fn):
    self.use_counts = use_count(fn)
    
  def is_live(self, name):
    count = self.use_counts.get(name)
    return count and count > 0

  def is_live_lhs(self, lhs):
    c = lhs.__class__
    if c is Var:
      return self.is_live(lhs.name)
    elif c is Tuple:
      return any(self.is_live_lhs(elt) for elt in lhs.elts)
    elif isinstance(lhs, (str, tuple)):
      assert False, "Raw data? This ain't the stone age, you know."
    else:
      return True

  def decref(self, expr):
    for var_name in collect_var_names_list(expr):
      self.use_counts[var_name] -= 1


  def transform_merge(self, phi_nodes):
    new_merge = {}
    for (var_name, (l,r)) in phi_nodes.iteritems():
      if self.is_live(var_name):
        new_merge[var_name] = (l,r)
      else:
        self.decref(l)
        self.decref(r)
    return new_merge

  
  def save_lhs_tuple(self, lhs):
    """
    If there's a Tuple assignment on the LHS 
    then all the variables must be kept alive
    together if any of them survive
    """
    for elt in lhs.elts:
      if elt.__class__ is Tuple:
        self.save_lhs_tuple(elt)
      else:
        assert elt.__class__ is Var 
        if not self.is_live(elt.name):
          self.use_counts[elt.name] = 1 
    
  def transform_Assign(self, stmt):
    if self.is_live_lhs(stmt.lhs):
      if stmt.lhs.__class__ is Tuple: 
        self.save_lhs_tuple(stmt.lhs)
      return stmt
    self.decref(stmt.rhs)

    return None

  def is_dead_loop(self, cond, body, merge):
    # pattern match to find loops which only
    # increment a counter without any visible effects
    if syntax.helpers.is_false(cond):
      return True

    rhs_counts = {}
    
    if cond.__class__ is not Var:
      return False
    rhs_counts[cond.name] = 1
 
    def process_rhs(expr):
      klass = expr.__class__
      if klass is Var:
        rhs_counts[expr.name] = rhs_counts.get(expr.name, 0) + 1
      elif klass is Const:
        pass
      elif klass is PrimCall:
        for arg in expr.args:
          process_rhs(arg)
      elif klass is Tuple:
        for elt in expr.elts:
          process_rhs(elt)
      elif klass is TupleProj:
        process_rhs(expr.tuple)
      elif klass is Index:
        process_rhs(expr.value)
        process_rhs(expr.index)
      elif klass is Closure: 
        process_rhs(expr.fn)
        for arg in expr.args:
          process_rhs(arg)
      elif klass is ClosureElt:
        process_rhs(expr.closure)
        process_rhs(expr.index)
      else:
        return False

    for stmt in body:
      # if statements are anything other than x = safe_expr then
      # there might be externally visible effects to this loop
      if stmt.__class__ is not Assign or stmt.lhs.__class__ is not Var:
        return False
      process_rhs(stmt.rhs)

    for output_name in merge.iterkeys():
      if self.use_counts[output_name] != rhs_counts.get(output_name, 0):
        return False
    return True

  def transform_While(self, stmt):
    new_merge = self.transform_merge(stmt.merge)
    new_body = self.transform_block(stmt.body)

    if self.is_dead_loop(stmt.cond, new_body, new_merge):
      return None
    stmt.body = new_body
    stmt.merge = new_merge
    return stmt

  def transform_If(self, stmt):
    cond = stmt.cond

    # Process the phi-merge first 
    # so that variables dead after the If 
    # statement can become dead inside it 
    stmt.merge = self.transform_merge(stmt.merge)

    stmt.true = self.transform_block(stmt.true)

    stmt.false = self.transform_block(stmt.false)

    if len(stmt.merge) == 0 and len(stmt.true) == 0 and \
        len(stmt.false) == 0:
      return None
    elif syntax.helpers.is_true(cond):
      for name, (v, _) in stmt.merge.iteritems():
        self.assign(Var(name, type = v.type), v)
      self.blocks.extend_current(reversed(stmt.true))
      return None
    elif syntax.helpers.is_false(cond):
      for name, (_, v) in stmt.merge.items():
        self.assign(Var(name, type = v.type), v)
      self.blocks.extend_current(reversed(stmt.false))
      return None
    return stmt

  def transform_Return(self, stmt):
    return stmt
  
  def transform_ExprStmt(self, stmt):
    if self.is_pure(stmt.value):
      return None 
    else:
      return stmt 

  def transform_ForLoop(self, stmt):
    stmt.merge = self.transform_merge(stmt.merge)
    stmt.body = self.transform_block(stmt.body)

    if len(stmt.body) > 0 or len(stmt.merge) > 0:
      return stmt

  def post_apply(self, fn):
    type_env = {}
    for (name,t) in fn.type_env.iteritems():
      if self.is_live(name):
        type_env[name] = t

    fn.type_env = type_env
    Transform.post_apply(self, fn)
    return fn

def dead_code_elim(fn):
  return DCE().apply(fn)

########NEW FILE########
__FILENAME__ = flattening
from .. import names 

from ..builder import Builder
from ..ndtypes import (ScalarT, NoneT, NoneType, ArrayT, SliceT, TupleT, make_tuple_type, 
                       Int64, PtrT, ptr_type, ClosureT, FnT, StructT, 
                       TypeValueT)
from ..syntax import (Var, Tuple, 
                      Index, TypedFn, Return, Stmt, Assign, Alloc,  
                      ParFor, PrimCall, If, While, ForLoop, Call, Expr, 
                      IndexReduce, ExprStmt) 
from ..syntax.helpers import none, const_int

from transform import Transform

def concat(seqs):
  result = []
  for seq in seqs:
    for elt in seq:
      result.append(elt)
  return tuple(result)

def concat_args(*seqs):
  return concat(seqs)

def concat_map(f, seq):
  return concat(f(elt) for elt in seq)

def flatten_type(t):
  if isinstance(t, (ScalarT, PtrT)):
    return (t,)
  elif isinstance(t, TupleT):
    return concat(flatten_type(elt_t) for elt_t in t.elt_types)
  elif isinstance(t, (NoneT, FnT, TypeValueT)):
    return ()
  elif isinstance(t, ClosureT):
    return concat(flatten_type(elt_t) for elt_t in t.arg_types)
  elif isinstance(t, ArrayT):
    return concat_args(
      flatten_type(t.ptr_t), 
      flatten_type(t.shape_t),
      flatten_type(t.strides_t),          
      (Int64, Int64), # offset and size
      
    )
  elif isinstance(t, SliceT):
    return flatten_types( (t.start_type, t.stop_type, t.step_type) )
  else:
    assert False, "Unsupported type %s" % (t,)

def flatten_types(ts):
  return concat([flatten_type(t) for t in ts])
  
def field_pos_range(t, field, _cache = {}):
  key = (t, field)
  if key in _cache:
    return _cache[key]
  assert isinstance(t, StructT), "Expected struct got %s.%s" % (t, field)
  offset = 0
  for i, (field_name, field_t) in enumerate(t._fields_):
    n = len(flatten_type(field_t))
    if field_name == field or (isinstance(field, (int, long)) and i == field):
      result = (offset, offset+n)
      _cache[key] = result
      return result
    offset += n 
  assert False, "Field %s not found on type %s" % (field, t)

def get_field_elts(t, values, field):
  start, stop = field_pos_range(t, field)
  assert stop <= len(values), \
    "Insufficient number of flattened fields %s for %s.%s" % (values, t, field)
  return values[start:stop]

def single_type(ts):
  """
  Turn a sequence of types into a single type object
  """
  if len(ts) == 0:
    return NoneType
  elif len(ts) == 1:
    return ts[0]
  else:
    return make_tuple_type(ts)
  
def single_value(values):
  if len(values) == 0:
    return none 
  elif len(values) == 1:
    return values[0]
  else:
    t = make_tuple_type(tuple(v.type for v in values))
    return Tuple(values, type = t)
  
def mk_vars(names, types):
  """
  Combine a list of names and a list of types into a single list of vars
  """
  return [Var(name = name, type = t) for name, t in zip(names, types)]


class BuildFlatFn(Builder):
  def __init__(self, old_fn):
    Builder.__init__(self)
    self.old_fn = old_fn 
    base_name = names.original(old_fn.name)
    flat_fn_name = names.fresh("flat_" + base_name)

    self.type_env = {}
    for (name, t) in old_fn.type_env.iteritems():

      old_var = Var(name = name, type = t)
      for new_var in self.flatten_lhs_var(old_var):
  
        self.type_env[new_var.name] = new_var.type
    old_input_vars = mk_vars(old_fn.arg_names, old_fn.input_types)
        
    # a var of struct type from the old fn 
    # maps to multiple variables in the new
    # function 
    self.var_expansions = {}
    
    new_input_vars = []
    for var in old_input_vars:
      flat_vars = self.flatten_lhs_var(var)
      self.var_expansions[var.name]  = flat_vars 
      new_input_vars.extend(flat_vars)
    
    new_input_names = tuple(var.name for var in new_input_vars)
    new_input_types = tuple(var.type for var in new_input_vars)
    new_return_type =  single_type(flatten_type(old_fn.return_type))
    
    self.flat_fn = \
      TypedFn(name = flat_fn_name, 
        arg_names = new_input_names, 
        body = [], 
        type_env = self.type_env,
        input_types = new_input_types, 
        return_type = new_return_type, 
        created_by = self) 
      
    
  def run(self):
    self.flat_fn.body = self.flatten_block(self.old_fn.body)
    return self.flat_fn
  
  
  #######################
  #
  #     Helpers
  #
  #######################
  
  def flatten_block(self, stmts):
    self.blocks.push()
    for stmt in stmts:
      result = self.flatten_stmt(stmt)
      if result is None:
        continue
      if not isinstance(result, (list,tuple)):
        result = [result]
      for new_stmt in result:
        assert isinstance(new_stmt, Stmt), "Expected statement, got %s" % (new_stmt,)
        self.blocks.append(new_stmt)
    return self.blocks.pop()
  
  
  
  #######################
  #
  #     Statements
  #
  #######################
  
  
  #def bind_name(self):
  #def bind_var(self, lhs, rhs):
  
  def flatten_Assign(self, stmt):
    c = stmt.lhs.__class__
    rhs = self.flatten_expr(stmt.rhs)

    if c is Var:
      
      lhs_vars = self.flatten_lhs_var(stmt.lhs)
      self.var_expansions[stmt.lhs.name] = lhs_vars 
      if isinstance(rhs, Expr):
        if len(lhs_vars) == 1:
          return [Assign(lhs_vars[0], rhs)]
        else:
          # the IR doesn't allow for multiple assignment 
          # so we fake it with tuple literals
          return [Assign(self.tuple(lhs_vars), rhs)]
      assert isinstance(rhs, (list,tuple))
      assert len(lhs_vars) == len(rhs), \
        "Mismatch between LHS %s and RHS %s : %s => %s in stmt %s" % \
        (lhs_vars, stmt.rhs, stmt.rhs.type, rhs, stmt)
      result = []
      for var, value in zip(lhs_vars, rhs):
        result.append(Assign(var, value))
      return result 

    elif c is Index:
      array_t = stmt.lhs.value.type 
      if isinstance(array_t, PtrT):
        return stmt  
      indices = self.flatten_expr(stmt.lhs.index)
      values = self.flatten_expr(stmt.lhs.value)
      data = get_field_elts(array_t, values, 'data')[0]
      shape = get_field_elts(array_t, values, 'shape')
      strides = get_field_elts(array_t, values, 'strides')
      offset = get_field_elts(array_t, values, 'offset')[0]
      n_dims = len(strides)
      n_indices = len(indices)
      assert n_dims == n_indices, \
        "Expected %d indices but only got %d in %s" % (n_dims, n_indices, stmt)
      for idx, stride in zip(indices, strides):
        offset = self.add(offset, self.mul(idx, stride))

      stmt.lhs = self.index(data, offset, temp=False)
      stmt.rhs = rhs[0]
      return [stmt]
    else:
      assert False, "LHS not supported in flattening: %s" % stmt 
  
  def enter_branch(self, phi_nodes):
    for (k, (left, _)) in phi_nodes.iteritems():
      self.var_expansions[k] = self.flatten_lhs_name(k, left.type)
  
  def flatten_merge(self, phi_nodes):
    result = {}
    for (k, (left, right)) in phi_nodes.iteritems():
      t = left.type
      assert right.type == t 
      if isinstance(t, (ScalarT, PtrT)):
        result[k] = (self.flatten_scalar_expr(left), self.flatten_scalar_expr(right))
      elif isinstance(t, (FnT, NoneT)):
        continue 
      else:
        fields = self.var_expansions[k]
        flat_left = self.flatten_expr(left)
        flat_right = self.flatten_expr(right)
        assert len(fields) == len(flat_left)
        assert len(fields) == len(flat_right)
        for i, var in enumerate(fields):
          result[var.name] = (flat_left[i], flat_right[i])
    return result 
   
  def flatten_ForLoop(self, stmt):
    self.enter_branch(stmt.merge)
    var = self.flatten_scalar_lhs_var(stmt.var)
    start = self.flatten_scalar_expr(stmt.start)
    stop = self.flatten_scalar_expr(stmt.stop)
    step = self.flatten_scalar_expr(stmt.step)
    body = self.flatten_block(stmt.body)
    merge = self.flatten_merge(stmt.merge)
    return ForLoop(var, start, stop, step, body, merge)
  
  def flatten_While(self, stmt):
    self.enter_branch(stmt.merge)
    cond = self.flatten_scalar_expr(stmt.cond)
    body = self.flatten_block(stmt.body)
    merge = self.flatten_merge(stmt.merge)
    return While(cond, body, merge)
     
  
  def flatten_If(self, stmt):
    self.enter_branch(stmt.merge)
    cond = self.flatten_scalar_expr(stmt.cond)
    true = self.flatten_block(stmt.true)
    false = self.flatten_block(stmt.false)
    merge = self.flatten_merge(stmt.merge)
    assert merge is not None
    return If(cond, true, false, merge = merge)
  
  def flatten_ExprStmt(self, stmt):
    return ExprStmt(value = self.flatten_expr(stmt.value))
   
  def flatten_ParFor(self, stmt):
    new_fn, closure_elts = self.flatten_fn(stmt.fn)
    closure = self.closure(new_fn, closure_elts)
    bounds = single_value(self.flatten_expr(stmt.bounds))
    return ParFor(fn = closure, bounds = bounds)
    
  def flatten_Return(self, stmt):
    return Return(single_value(self.flatten_expr(stmt.value)))
  
  def flatten_Comment(self, stmt):
    return stmt 
  
  def flatten_stmt(self, stmt):
    method_name = "flatten_%s" % stmt.__class__.__name__
    return getattr(self, method_name)(stmt)
  
  #######################
  #
  #     Expressions 
  #
  #######################
  
  def flatten_expr(self, expr):
    method_name = "flatten_%s" % expr.__class__.__name__
    return getattr(self, method_name)(expr)
  
  def flatten_expr_list(self, exprs):
    return concat_map(self.flatten_expr, exprs)
  
  def flatten_scalar_expr(self, expr):
    """
    Give me back a single expression instead of a list 
    """
    flat_exprs = self.flatten_expr(expr)
    assert len(flat_exprs) == 1
    return flat_exprs[0]
    
  def flatten_scalar_expr_list(self, exprs):
    assert isinstance(exprs, (list, tuple)), "Expected list, got %s" % (exprs,)
    return [self.flatten_scalar_expr(e) for e in  exprs]
  
  def flatten_Const(self, expr):
    if isinstance(expr.type, NoneT):
      return ()
    return (expr,)
  
  def flatten_fn(self, closure):
    fn = self.get_fn(closure)
    import pipeline
    fn = pipeline.indexify.apply(fn)
    flat_fn = build_flat_fn(fn)
    flat_closure_args = self.flatten_expr(closure)
    return flat_fn, flat_closure_args
  
  
  def flatten_Call(self, expr):
    flat_fn, flat_closure_args = self.flatten_fn(expr.fn)
    flat_args = self.flatten_expr_list(expr.args)
    args = tuple(flat_closure_args) + tuple(flat_args)
    return Call(flat_fn, args, type = flat_fn.return_type)
  
  def flatten_Cast(self, expr):
    return [expr]
  
  def flatten_UntypedFn(self, expr):
    return []
  
  def flatten_TypedFn(self, expr):
    return []
    
  def flatten_Var(self, expr):
    if isinstance(expr.type, (ScalarT, PtrT)):
      return (expr,)
    elif isinstance(expr.type, (FnT, NoneT)):
      return ()
    else:
      name = expr.name
      assert name in self.var_expansions, "No fields known for %s : %s" % (expr, expr.type) 
      return self.var_expansions[name]
    
  def flatten_Tuple(self, expr):
    return self.flatten_expr_list(expr.elts)
  
  def flatten_field(self, struct, field):
    elts = self.flatten_expr(struct)
    start, stop = field_pos_range(struct.type, field)
    return elts[start:stop]
   
  def flatten_TupleProj(self, expr):
    result = self.flatten_field(expr.tuple, expr.index)
    return result
    
  def flatten_Closure(self, expr):
    return self.flatten_expr_list(expr.args)
    
  def flatten_ClosureElt(self, expr):
    return self.flatten_field(expr.closure, expr.index)
  
  def flatten_Attribute(self, expr):
    return self.flatten_field(expr.value, expr.name)
  
  def flatten_Select(self, expr):
    cond = self.flatten_scalar_expr(expr.cond)
    true_values = self.flatten_expr(expr.true_value)
    false_values = self.flatten_expr(expr.false_value)
    assert len(true_values) == len(false_values)
    return [self.select(cond,t,f) for t,f in zip(true_values, false_values)]
    
  def flatten_Alloc(self, expr):
    count_exprs = self.flatten_expr(expr.count)
    assert len(count_exprs) == 1
    return Alloc(count = count_exprs[0], elt_type = expr.elt_type, type = expr.type)
    
  def flatten_Array(self, expr):
    assert False, "Array node should be an explicit allocation by now"
    # or, if we flatten structured elts, maybe we should handle it here?
  
    
  def flatten_Index(self, expr):

    t = expr.value.type 
    if isinstance(t, PtrT):
      return [expr]   
    assert isinstance(t, ArrayT), "Expected Index to take array, got %s" % (expr.type,)
    array_fields = self.flatten_expr(expr.value)
    data_fields = get_field_elts(t, array_fields, 'data')
    shape = get_field_elts(t, array_fields, 'shape')
    strides = get_field_elts(t, array_fields, 'strides')
    offset = get_field_elts(t, array_fields, 'offset')[0]
    
    index = expr.index 
    if isinstance(index.type, (NoneT, SliceT, ScalarT)):
      indices = [index]
    elif isinstance(index, Tuple):
      indices = index.elts 
    else:
      assert isinstance(index.type, TupleT), "Expected index to scalar, slice, or tuple"
      indices = self.tuple_elts(index)


      
    #indices = self.flatten_expr(expr.index)
   
    n_indices = len(indices)
    n_strides = len(strides)
    assert n_indices == n_strides, \
      "Not supported:  indices vs. dimensions: %d != %d in %s" % (n_indices, n_strides, expr)


    # fast-path for the common case when we're indexing
    # by all scalars to retrieve a scalar result
    #if syntax.helpers.all_scalars(indices):
    for i, idx in enumerate(indices):
      offset = self.add(offset, self.mul(idx, strides[i]))
    return [self.index(data_fields[0], offset)]
     
  
  def flatten_PrimCall(self, expr):
    args = self.flatten_scalar_expr_list(expr.args)
    return [PrimCall(prim = expr.prim, args = args, type = expr.type)]
  
  def flatten_Slice(self, expr):
    return self.flatten_expr_list([expr.start, expr.stop, expr.step])
  
  def flatten_Len(self, expr):
    assert False, "Not implemented" 
  
  def flatten_ConstArray(self, expr):
    assert False, "Not implemented" 
  
  def flatten_ConstArrayLike(self, expr):
    assert False, "Not implemented" 
  
  def flatten_Range(self, expr):
    assert False, "Not implemented" 
  
  def strides_from_shape_elts(self, shape_elts):
    strides = [const_int(1)]
    for dim in reversed(shape_elts[1:]):
      strides = [self.mul(strides[0], dim)] + strides
    return strides
  
  def flatten_AllocArray(self, expr):
    nelts = const_int(1)
    shape_elts = self.flatten_expr(expr.shape)
    for dim in shape_elts:
      nelts = self.mul(nelts, dim)
    ptr = Alloc(elt_type = expr.elt_type, count = nelts, type = ptr_type(expr.elt_type))
    stride_elts = self.strides_from_shape_elts(shape_elts)
    return (ptr,) + tuple(shape_elts) + tuple(stride_elts) + (self.int(0), nelts)
  
  def flatten_ArrayView(self, expr):
    data = self.flatten_expr(expr.data)
    shape = self.flatten_expr(expr.shape)
    strides = self.flatten_expr(expr.strides)
    offset = self.flatten_expr(expr.offset)
    size = self.flatten_expr(expr.size)
    return data + shape + strides + offset + size
    
  
  def flatten_Ravel(self, expr):
    assert False, "Not implemented" 
  
  def flatten_Reshape(self, expr):
    assert False, "Not implemented" 
  
  def flatten_Shape(self, expr):
    assert False, "Not implemented" 
  
  def flatten_Strides(self, expr):
    assert False, "Not implemented" 
  
  def flatten_Transpose(self, expr):
    assert False, "Not implemented" 
  
  def flatten_Where(self, expr):
    assert False, "Not implemented" 
     
   
    
  #######################
  #
  # Adverbs
  #
  #######################
  def flatten_Map(self, expr):
    assert False, "Unexpected Map encountered during flattening, should be IndexScan"
  
  def flatten_Reduce(self, expr):
    assert False, "Unexpected Reduce encountered during flattening, should be IndexScan"
  
  def flatten_Scan(self, expr):
    assert False, "Unexpected Scan encountered during flattening, should be IndexScan"
  
  def flatten_IndexMap(self, expr):
    #fn, closure_args = self.flatten_fn(expr.fn)
    #shape_elts = self.flatten_expr(expr.shape)
    # import pdb; pdb.set_trace()
    #return IndexMap(fn = self.closure(fn, closure_args), shape = self.tuple(shape_elts), type = None)
    assert False, "Unexpected IndexMap, should have been turned into ParFor before flattening"
    
  
  def flatten_OuterMap(self, expr):
    assert False, "Unexpected OuterMap, should have been turned into ParFor before flattening"
  
  def flatten_IndexReduce(self, expr):
    # assert isinstance(expr.type, ScalarT), "Non-scalar reductions not yet implemented"
    fn, fn_args = self.flatten_fn(expr.fn)
    fn = self.closure(fn, fn_args)
    combine, combine_args = self.flatten_fn(expr.combine)
    combine = self.closure(combine, combine_args)
    shape = self.tuple(self.flatten_expr(expr.shape))
    
    init = self.flatten_expr(expr.init)
    t = flatten_type(expr.type)
    if len(t) == 1:
      init = init[0]
      t = t[0]
    else:
      init = self.tuple(init)
      t = make_tuple_type(t)
    
    result =  IndexReduce(fn = fn, combine = combine, shape = shape, type = t, init = init)
    return [result]
     
      
  def flatten_IndexScan(self, expr):
    assert False, "IndexScan Not implemented" 
  
  def flatten_lhs_name(self, name, t):
    if isinstance(t, (ScalarT, PtrT)):
      return [Var(name = name, type = t)]
    elif isinstance(t, (NoneT, FnT, TypeValueT)):
      return []
    elif isinstance(t, SliceT):
      base = name.replace(".", "_")
      start = Var(name = "%s_start" % base, type = t.start_type)
      stop = Var(name = "%s_stop" % base, type = t.stop_type)
      step = Var(name = "%s_step" % base, type = t.step_type)
      field_vars = [start, stop, step]
    elif isinstance(t, ClosureT):
      base = name.replace(".", "_")
      field_vars = [Var(name = "%s_closure_elt%d" % (base,i) , type = t) 
                    for i,t in enumerate(t.arg_types)]
    elif isinstance(t, TupleT):
      base = name.replace(".", "_")
      field_vars = [Var(name = "%s_elt%d" % (base, i), type = t) 
                    for i,t in enumerate(t.elt_types)]
    elif isinstance(t, ArrayT):
      base = name.replace(".", "_")
      data = Var(name = "%s_data" % base, type = t.ptr_t)
      shape = Var(name = "%s_shape" % base, type = t.shape_t)
      strides = Var(name = "%s_strides" % base, type = t.strides_t)
      offset = Var(name = "%s_offset" % base, type = Int64)
      nelts = Var(name = "%s_nelts" % base, type = Int64)
      field_vars = [data, shape, strides, offset, nelts]
    else:
      assert False, "Unsupport type %s" % (t,)
    return self.flatten_lhs_vars(field_vars)
  
  def flatten_lhs_var(self, old_var):
    t = old_var.type 
    if isinstance(t, (PtrT, ScalarT)):
      return [old_var]
    name = old_var.name 
    return self.flatten_lhs_name(name, t)
  
  def flatten_lhs_vars(self, old_vars):
    return concat_map(self.flatten_lhs_var, old_vars)
  
  def flatten_scalar_lhs_var(self, old_var):
    lhs_vars = self.flatten_lhs_var(old_var)
    assert len(lhs_vars) == 1
    return lhs_vars[0]
  
def build_flat_fn(old_fn, _cache = {}):
  key = old_fn.cache_key
  if key in _cache:
    return _cache[key]
  flat_fn = BuildFlatFn(old_fn).run()
  _cache[key] = flat_fn
  _cache[flat_fn.cache_key] = flat_fn
  return flat_fn
  
class Flatten(Transform):
  
  def unbox_var(self, var):
    t = var.type 

    if isinstance(t, (FnT, NoneT, TypeValueT)):
      return []
    elif isinstance(t, (PtrT, ScalarT)):
      return [var]
    elif isinstance(t, ArrayT):
      # for structured arrays, should this be a tuple? 
      base = var.name.replace(".", "_")
      data = self.attr(var, 'data', name = base + "_data")
      shape = self.attr(var, 'shape', name = base + "_shape")
      strides = self.attr(var, 'strides', name = base + "_strides")
      offset = self.attr(var, 'offset', name = base + "_offset")
      size = self.attr(var, 'size', name = base + "_size")
      return self.unbox_vars([data, shape, strides, offset, size])
    elif isinstance(t, SliceT):
      start = self.attr(var, 'start')
      stop = self.attr(var, 'stop')
      step = self.attr(var, 'step')
      return self.unbox_vars([start, stop, step])
    elif isinstance(t, ClosureT):
      base = var.name.replace(".", "_")
      closure_elts = [self.closure_elt(var, i, name = base + "_closure_elt%d" % i)
                      for i in xrange(len(t.arg_types))]
      return self.unbox_vars(closure_elts)
    elif isinstance(t, TupleT):
      base = var.name.replace(".", "_")
      tuple_elts = [self.assign_name(self.tuple_proj(var, i), name = base + "_elt%d" % i)
                      for i in xrange(len(t.elt_types))]
      return self.unbox_vars(tuple_elts)
    else:
      assert False, "Unsupported type %s" % (t,)
  
  def unbox_vars(self, exprs):
    return concat_map(self.unbox_var, exprs)
  
  def to_seq(self, expr):
    if isinstance(expr.type, TupleT):
      return self.tuple_elts(expr)
    elif isinstance(expr.type, (FnT, NoneT)):
      return []
    else:
      return [expr]
  
  def box(self, t, elts):
    if isinstance(t, NoneT):
      assert len(elts) == 0, "Expected 0 values for None, got %s" % (elts,)
      return none 
    elif isinstance(t, ScalarT):
      assert len(elts) == 1
      return elts[0]
    elif isinstance(t, SliceT):
      assert len(elts) == 3
      start, stop, step = elts
      return self.slice_value(start, stop, step)
    elif isinstance(t, ArrayT):
      data = get_field_elts(t, elts, 'data')[0]
      shape = self.tuple(get_field_elts(t, elts, 'shape'))
      strides = self.tuple(get_field_elts(t, elts, 'strides'))
      offset = get_field_elts(t, elts, 'offset')[0]
      nelts = get_field_elts(t, elts, 'size')[0]
      return self.array_view(data, shape, strides, offset, nelts)
    elif isinstance(t, TupleT):
      boxed_elts = []
      for i, elt_t in enumerate(t.elt_types):
        elt = self.box(elt_t, get_field_elts(t, elts, i))
        boxed_elts.append(elt)
      return self.tuple(boxed_elts)
    elif isinstance(t, ClosureT):
      assert False, "Not implemented: ClosureT" 
    elif isinstance(t, FnT):
      assert False, "Not implemented: FnT" 
  
  def transform_block(self, stmts):
    return stmts
    
  def pre_apply(self, old_fn, _cache = {}):

    key = old_fn.cache_key
    if key  in _cache:
      return _cache[key]
    
    flat_fn = build_flat_fn(old_fn)
    
    flat_fn.created_by = old_fn.created_by
    flat_fn.transform_history = old_fn.transform_history.copy()
    
    input_vars = mk_vars(old_fn.arg_names, old_fn.input_types)
    self.blocks.push()
    unboxed_inputs = self.unbox_vars(input_vars)
    assert len(unboxed_inputs) == len(flat_fn.input_types)
    unboxed_result = self.call(flat_fn, unboxed_inputs, name = "unboxed_result")
    unboxed_elts = self.to_seq(unboxed_result)
    boxed_result = self.box(old_fn.return_type, unboxed_elts)
    self.return_(boxed_result)
    old_fn.body = self.blocks.pop()
    _cache[key] = old_fn
    _cache[flat_fn.cache_key] = flat_fn 
    return old_fn
  
########NEW FILE########
__FILENAME__ = fusion
from ..  import names, syntax
from ..analysis.use_analysis import use_count 
from ..ndtypes import ArrayT 
from ..transforms import inline, Transform 
from .. syntax import Var, Const,  Return, TypedFn, DataAdverb, Adverb
from .. syntax import IndexMap, IndexReduce, Map, Reduce, OuterMap 
from ..syntax.helpers import zero_i64, none 

def fuse(prev_fn, prev_fixed_args, next_fn, next_fixed_args, fusion_args):
  if syntax.helpers.is_identity_fn(next_fn):
    assert len(next_fixed_args) == 0
    return prev_fn, prev_fixed_args

  """
  Expects the prev_fn's returned value to be one or more of the arguments to
  next_fn. Any element in 'const_args' which is None gets replaced by the
  returned Var
  """
  fused_formals = []
  fused_input_types = []
  fused_type_env = prev_fn.type_env.copy()
  next_name = names.original(next_fn.name)
  prev_name = names.original(prev_fn.name)
  if prev_name.startswith("fused_") or next_name.startswith("fused_"):
    fused_name = names.fresh(prev_name + "_" + next_name)
  else:
    fused_name = names.fresh("fused_" + prev_name +  "_" + next_name)

  prev_closure_formals = prev_fn.arg_names[:len(prev_fixed_args)]

  for prev_closure_arg_name in prev_closure_formals:
    t = prev_fn.type_env[prev_closure_arg_name]
    fused_formals.append(prev_closure_arg_name)
    fused_input_types.append(t)

  next_closure_formals = next_fn.arg_names[:len(next_fixed_args)]
  for next_closure_arg_name in next_closure_formals:
    t = next_fn.type_env[next_closure_arg_name]
    new_name = names.refresh(next_closure_arg_name)
    fused_type_env[new_name] = t
    fused_formals.append(new_name)
    fused_input_types.append(t)

  prev_direct_formals = prev_fn.arg_names[len(prev_fixed_args):]
  for arg_name in prev_direct_formals:
    t = prev_fn.type_env[arg_name]
    fused_formals.append(arg_name)
    fused_input_types.append(t)

  prev_return_var, fused_body = \
      inline.replace_return_with_var(prev_fn.body,
                                     fused_type_env,
                                     prev_fn.return_type)
  # for now we're restricting both functions to have a single return at the
  # outermost scope
  inline_args = list(next_closure_formals)
  for arg in fusion_args:
    if arg is None:
      inline_args.append(prev_return_var)
    elif isinstance(arg, int):
      # positional arg which is not being fused out
      inner_name = next_fn.arg_names[arg]
      inner_type = next_fn.type_env[inner_name]
      new_name = names.refresh(inner_name)
      fused_formals.append(new_name)
      fused_type_env[new_name] = inner_type
      fused_input_types.append(inner_type)
      var = Var(new_name, inner_type)
      inline_args.append(var)
    else:
      assert arg.__class__ is Const, \
         "Only scalars can be spliced as literals into a fused fn: %s" % arg
      inline_args.append(arg)
  next_return_var = inline.do_inline(next_fn, inline_args,
                                     fused_type_env, fused_body)
  fused_body.append(Return(next_return_var))

  # we're not renaming variables that originate from the predecessor function
  new_fn = TypedFn(name = fused_name,
                   arg_names = fused_formals,
                   body = fused_body,
                   input_types = tuple(fused_input_types),
                   return_type = next_fn.return_type,
                   type_env = fused_type_env)

  combined_args = prev_fixed_args + next_fixed_args
  return new_fn, combined_args 

class Fusion(Transform):
  def __init__(self, recursive=True):
    Transform.__init__(self)
    # name of variable -> Map or Scan adverb
    self.adverb_bindings = {}
    self.recursive = True

  def pre_apply(self, fn):
    # map each variable to
    self.use_counts = use_count(fn)

  def transform_TypedFn(self, fn):
    return run_fusion(fn)
    #import pipeline
    #return pipeline.high_level_optimizations(fn)

  def has_required_rank(self, var_name, required_rank):
    if var_name not in self.type_env:
      return False 
    
    t = self.type_env[var_name]
     
    if t.__class__ is ArrayT:
      return t.rank == required_rank
    else:
      return required_rank == 0 
  
  def fuse_expr(self, rhs):
    if not isinstance(rhs, DataAdverb): return rhs 
    
    # TODO: figure out how to fuse OuterMap(Map(...), some_other_arg)
    # if rhs.__class__ is OuterMap: return stmt
     
    rhs_fn = self.get_fn(rhs.fn)

    if not inline.can_inline(rhs_fn): return rhs

    args = rhs.args
    if any(arg.__class__ not in (Var, Const) for arg in args): return rhs 
    
    arg_names = [arg.name for arg in args if arg.__class__ is Var]
    unique_vars = set(arg_names)
    
     
    valid_fusion_vars = [name for name in unique_vars
                         if name in self.adverb_bindings]

    # only fuse over variables which of the same rank as whatever is the largest
    # array being processed by this operator 
    max_rank = max(self.rank(arg) for arg in args)

    valid_fusion_vars = [name for name in valid_fusion_vars
                         if self.has_required_rank(name,max_rank)]
    
    for arg_name in valid_fusion_vars:
      n_occurrences = sum((name == arg_name for name in arg_names))
      prev_adverb = self.adverb_bindings[arg_name]
      prev_adverb_fn = self.get_fn(prev_adverb.fn)
      
      if not inline.can_inline(prev_adverb_fn):
        continue
      
      # if we're doing a cartesian product between inputs then 
      # can't introduce multiple new array arguments 
      if rhs.__class__ is OuterMap and len(prev_adverb.args) != 1:
        continue 
       
      # 
      # Map(Map) -> Map
      # Reduce(Map) -> Reduce 
      # Scan(Map) -> Scan
      # ...but for some reason, not:
      # OuterMap(Map) -> OuterMap 
      # 
      if prev_adverb.__class__ is Map and rhs.axis == prev_adverb.axis:
   
        surviving_array_args = []
        fusion_args = []
        for (pos, arg) in enumerate(rhs.args):
          c = arg.__class__
          if c is Var and arg.name == arg_name:
            fusion_args.append(None)
          elif c is Const:
            fusion_args.append(arg)
          else:
            surviving_array_args.append(arg)
            fusion_args.append(pos)
        
        new_fn, clos_args = \
              fuse(self.get_fn(prev_adverb.fn),
                   self.closure_elts(prev_adverb.fn),
                   self.get_fn(rhs.fn),
                   self.closure_elts(rhs.fn),
                   fusion_args)

        assert new_fn.return_type == self.return_type(rhs.fn)
        
        if self.use_counts[arg_name] == n_occurrences:
          del self.adverb_bindings[arg_name]
        if self.fn.created_by is not None:
          new_fn = self.fn.created_by.apply(new_fn)
        new_fn.created_by = self.fn.created_by  
        rhs.fn = self.closure(new_fn, clos_args)
        rhs.args = prev_adverb.args + surviving_array_args
      
      # 
      # Reduce(IndexMap) -> IndexReduce
      #   
      elif prev_adverb.__class__ is IndexMap and \
            rhs.__class__ is Reduce and \
            len(rhs.args) == 1 and \
            (self.is_none(rhs.axis) or rhs.args[0].type.rank == 1):
              
        new_fn, clos_args = \
            fuse(self.get_fn(prev_adverb.fn),
                 self.closure_elts(prev_adverb.fn),
                 self.get_fn(rhs.fn),
                 self.closure_elts(rhs.fn),
                 rhs.args)
        assert new_fn.return_type == self.return_type(rhs.fn)
        if self.use_counts[arg_name] == n_occurrences:
          del self.adverb_bindings[arg_name]
        
        if self.fn.created_by is not None:
          new_fn = self.fn.created_by.apply(new_fn)
        new_fn.created_by = self.fn.created_by 
        rhs = IndexReduce(fn = self.closure(new_fn, clos_args), 
                               shape = prev_adverb.shape, 
                               combine = rhs.combine, 
                               type = rhs.type,
                               init = rhs.init)
      # 
      # Map(IndexMap) -> IndexMap
      #   
      elif prev_adverb.__class__ is IndexMap and \
            rhs.__class__ is Map and \
            len(rhs.args) == 1 and \
            (self.is_none(rhs.axis) or rhs.args[0].type.rank == 1):
        new_fn, clos_args = \
            fuse(self.get_fn(prev_adverb.fn),
                 self.closure_elts(prev_adverb.fn),
                 self.get_fn(rhs.fn),
                 self.closure_elts(rhs.fn),
                 rhs.args)
        assert new_fn.return_type == self.return_type(rhs.fn)
        if self.use_counts[arg_name] == n_occurrences: 
          del self.adverb_bindings[arg_name]
        
        if self.fn.created_by is not None:
          new_fn = self.fn.created_by.apply(new_fn)
        new_fn.created_by = self.fn.created_by 
        rhs = IndexMap(fn = self.closure(new_fn, clos_args), 
                       shape = prev_adverb.shape, 
                       type = rhs.type)
    return rhs 


  def transform_Assign(self, stmt):
    old_rhs = stmt.rhs 
    if self.recursive: 
      old_rhs = self.transform_expr(old_rhs)
    
    if not isinstance(old_rhs, DataAdverb):
      return stmt 

    new_rhs = self.fuse_expr(old_rhs)
    if stmt.lhs.__class__ is Var and isinstance(new_rhs, Adverb):
      self.adverb_bindings[stmt.lhs.name] = new_rhs
    stmt.rhs = new_rhs
    return stmt
  
  def transform_Return(self, stmt):
    old_rhs = stmt.value 
    if not isinstance(old_rhs, DataAdverb):
      return stmt 
    new_rhs = self.fuse_expr(old_rhs)
    stmt.value = new_rhs 
    return stmt 

  
from ..analysis import contains_adverbs, contains_calls
def run_fusion(fn):
  if contains_adverbs(fn) or contains_calls(fn):
    return Fusion().apply(fn)
  else:
    return fn 
      

########NEW FILE########
__FILENAME__ = imap_elim
from transform import Transform

class IndexMapElimination(Transform):
  """
  a = IndexMap(f, bounds)
  ... a[i] ... 
  
  -becomes-
  
  ... f(i) ... 
  """
  def transform_IndexMap(self, expr):
    return expr 
########NEW FILE########
__FILENAME__ = indexify_adverbs
import itertools 


from .. import names 
from ..builder import build_fn 
from ..ndtypes import Int64, repeat_tuple, NoneType, ScalarT, TupleT, ArrayT 
from ..syntax import (ParFor, IndexReduce, IndexScan, Index, Map, OuterMap, Var, Const, Expr)
from ..syntax.helpers import get_types, none, zero_i64 
from ..syntax.adverb_helpers import max_rank_arg, max_rank 
from transform import Transform


class IndexifyAdverbs(Transform):
  """
  Take all the adverbs whose parameterizing functions assume they 
  get fed slices of input data and turn them into version which take explicit
  input indices
  """
  
  def fresh_input_name(self, expr):
    if expr is Var:
      return names.refresh(expr.name)
    else:
      return names.fresh("input")
  
  def fresh_fn_name(self, prefix, fn):
    return names.fresh(prefix + names.original(self.get_fn(fn).name))
  
  def fresh_index_names(self, n):
    name_supply = itertools.cycle(["i","j","k","l","ii","jj","kk","ll"])
    return [names.fresh(name_supply.next()) for _ in xrange(n)]
  
  _indexed_fn_cache = {}
  def indexify_fn(self, fn, 
                   axis,
                   array_args, 
                   cartesian_product = False,
                   output = None, 
                   index_offsets = None):
    """
    Take a function whose last k values are slices through input data 
    and transform it into a function which explicitly extracts its arguments
    """  
    array_args = tuple(array_args)
    
    array_arg_types = tuple(get_types(array_args))

    closure_args = self.closure_elts(fn)
    closure_arg_types = tuple(get_types(closure_args))
    n_closure_args = len(closure_args)
    fn = self.get_fn(fn)
    
    axes = self.normalize_axes(array_args, axis)
    
    
    key = (  fn.cache_key, 
             axes,
             closure_arg_types, 
             array_arg_types, 
             output is None,  
             cartesian_product, 
             index_offsets, 
           )

    def mk_closure():
      new_fn = self._indexed_fn_cache[key] 
      if output is None:
        return self.closure(new_fn, closure_args + array_args)
      else:
        return self.closure(new_fn, (output, ) + closure_args + array_args)
    
    if key in self._indexed_fn_cache:
      result = mk_closure()
      return result 
    
    if cartesian_product:
      # if we're doing a cartesian product then each argument may need 
      # a different number of indices depending on whether it's a scalar
      # and when its axis is None or given as an int 
      n_indices = 0
      for axis, arg_t in zip(axes, array_arg_types):
        if axis is None:
          n_indices += self.rank(arg_t)
        elif isinstance(arg_t, ArrayT):
          n_indices += 1
    else:
      # if we're doing an element-wise map, 
      # then either the axes are all None, in which case 
      # we need indices for the largest arg
      # or, we're just picking off one slice from 
      # every argument 
      if any(axis is None for axis in axes):
        assert all(axis is None for axis in axes), "Incompatible axes %s" % axes 
        n_indices = max_rank(array_arg_types)
      else:
        assert all(isinstance(axis, (int,long)) for axis in axes), "Invalid axes %s" % axes 
        n_indices = 1
        
    #index_input_type = Int64 if n_indices == 1 else repeat_tuple(Int64, n_arrays) 
    index_input_types = (Int64,) * n_indices  
    
    if output is None:
      inner_input_types = closure_arg_types + array_arg_types + index_input_types
      new_return_type = fn.return_type 
    else:
      inner_input_types = (output.type,) + closure_arg_types +  array_arg_types + index_input_types
      new_return_type = NoneType 
    
    input_names = []
    if output is not None:
      if output is Var:
        local_output_name = names.refresh(output.name)
      else:
        local_output_name = names.fresh("local_output")
      input_names.append(local_output_name) 
    
    for old_input_name in fn.arg_names:
      input_names.append(names.refresh(old_input_name)) 
    
    
    input_names.extend(self.fresh_index_names(n_indices))
      
    
    new_fn_name = self.fresh_fn_name("idx_", fn)
    
    new_fn, builder, input_vars = build_fn(inner_input_types, 
                                           new_return_type,
                                           name = new_fn_name,  
                                           input_names = input_names)

    index_input_vars = input_vars[-n_indices:]
    if output is None:
      output_var = None
      closure_arg_vars = input_vars[:n_closure_args]
      array_arg_vars = input_vars[n_closure_args:-n_indices]
    else:
      output_var = input_vars[0]
      closure_arg_vars = input_vars[1:n_closure_args+1]
      array_arg_vars = input_vars[n_closure_args+1:-n_indices]
    
    slice_values = \
      self.get_slices(builder, array_arg_vars, axes, index_input_vars, 
                      cartesian_product, index_offsets)


    elt_result = builder.call(fn, tuple(closure_arg_vars) + tuple(slice_values))
    if output is None: 
      builder.return_(elt_result)
    else:
      if len(index_input_vars) > 1:
        builder.setidx(output_var, builder.tuple(index_input_vars), elt_result)
      else:
        builder.setidx(output_var, index_input_vars[0], elt_result)
      builder.return_(none)
    
    new_fn.created_by = self.fn.created_by 
    new_fn.transform_history = self.fn.transform_history 
    self._indexed_fn_cache[key] = new_fn
    return mk_closure()
          
    
  
  def get_slices(self, builder, array_arg_vars, axes, index_input_vars, 
                 cartesian_product = False, index_offsets = None): 
    slice_values = []
    axes = self.normalize_axes(array_arg_vars, axes)
    # only gets incremented if we're doing a cartesian product
    if cartesian_product:
      idx_counter = 0
      for i, curr_array in enumerate(array_arg_vars):
        axis = axes[i]
        rank = curr_array.type.rank
        if rank <= 1 and axis is None:
          axis = 0
           
        if axis is None:
          start = idx_counter
          stop = idx_counter + rank
          curr_indices = index_input_vars[start:stop]
          idx_counter = stop 
          curr_slice = builder.index(curr_array, curr_indices)
        elif rank > axis:
          idx = index_input_vars[idx_counter]
          idx_counter += 1
          curr_slice = builder.slice_along_axis(curr_array, axis, idx)
        else:
          curr_slice = curr_array 
        slice_values.append(curr_slice)
    else:
      for i, curr_array in enumerate(array_arg_vars):
        axis = axes[i]
        rank = curr_array.type.rank

        if rank <= 1 and axis is None:
          axis = 0
        if axis is None:
          assert len(index_input_vars) <= rank, \
            "Insufficient indices for array arg %s : %s" % (curr_array, curr_array.type)
            
          # to be compatible with NumPy's broadcasting, we pull out the *last* r
          # indices so that Matrix + Vector will replicate the vector as columns, not rows
          curr_indices = index_input_vars[-rank:]
          curr_slice = builder.index(curr_array, curr_indices)
        elif rank > axis:
          curr_idx = index_input_vars[0]
          if index_offsets is not None:
            assert len(index_offsets) > i
            curr_offset = index_offsets[i]
            if not isinstance(curr_offset, Expr): 
              curr_offset = builder.int(curr_offset)
            curr_idx = builder.add(curr_idx, curr_offset)
          curr_slice = builder.slice_along_axis(curr_array, axis, curr_idx) 
        else:
          # if we're trying to map over axis 1 of a 1-d object, then there aren't
          # enough dims to slice anything, so it just gets passed in without modification
          
          curr_slice = curr_array 
        
          
        slice_values.append(curr_slice)
    return slice_values

  

  def create_map_output_array(self, 
                                 fn, array_args, axes, 
                                 cartesian_product = False, 
                                name = "output"):
    """
    Given a function and its argument, use shape inference to figure out the
    result shape of the array and preallocate it.  If the result should be a
    scalar, just return a scalar variable.
    """
    assert self.is_fn(fn), \
      "Expected function, got %s" % (fn,)
    assert isinstance(array_args, (list,tuple)), \
      "Expected list of array args, got %s" % (array_args,)
    axes = self.normalize_axes(array_args, axes)
    
    
    
    n_indices = 0
    for arg, axis in zip(array_args, axes):
      r = self.rank(arg)
      if r == 0:
        continue 
      if axis is None:
        if cartesian_product:
          n_indices += self.rank(arg)
        else:
          n_indices = max(n_indices, self.rank(arg))
      elif r <= axis:
        continue 
      else:
        if cartesian_product:
          n_indices += 1
        else:
          n_indices = max(n_indices, 1)
    
    
    # take the 0'th slice just to have a value in hand 
    inner_args = self.get_slices(builder = self, 
                                 array_arg_vars = array_args, 
                                 axes = axes, 
                                 index_input_vars = [zero_i64] * (n_indices),
                                 cartesian_product = cartesian_product)
     
                  
    
    if cartesian_product:
      extra_dims = []
      for array, axis in zip(array_args, axes):
        rank = self.rank(array)
        if axis is None:
          dim = 1
        elif rank > axis:
          dim = self.shape(array, axis)
        else:
          dim = 1 
        extra_dims.append(dim)
      outer_shape_tuple = self.tuple(extra_dims)
    else:
      outer_shape_tuple = self.iter_bounds(array_args, axes)
      if isinstance(outer_shape_tuple.type, ScalarT):
        outer_shape_tuple = self.tuple([outer_shape_tuple])

    return self.create_output_array(fn, inner_args, outer_shape_tuple, name)

  
  def insert_parfor(self, index_fn, bounds, n_read_only, n_write_only=1):
    read_only = ([False] * n_write_only) + ([True] * n_read_only)
    write_only = ([True] * n_write_only) + ([False] * n_read_only)
    stmt = ParFor(index_fn, bounds, read_only = read_only, write_only = write_only)
    self.blocks += stmt 
    
  def transform_Map(self, expr, output = None):
    # TODO: 
    # - recursively descend down the function bodies to pull together nested ParFors
    

    args = self.transform_expr_list(expr.args)
    axes = self.normalize_axes(args, expr.axis)
    old_fn = expr.fn

    if output is None:
      output = self.create_map_output_array(old_fn, args, axes)

    bounds = self.iter_bounds(args, axes)
    index_fn = self.indexify_fn(expr.fn, axes, args, 
                                cartesian_product=False, 
                                output = output)
    
    
    self.insert_parfor(index_fn, bounds, n_read_only = len(args), n_write_only = 1)
    return output 
  
  def transform_OuterMap(self, expr):
    args = self.transform_expr_list(expr.args)
    axes = self.normalize_axes(args, expr.axis)
    
    fn = expr.fn 
    outer_shape = self.iter_bounds(args, axes, cartesian_product = True)
    # recursively descend down the function bodies to pull together nested ParFors 
    zero = self.int(0)
    
    first_values = [self.slice_along_axis(arg, axis, zero) 
                    for (arg,axis) in zip(args, axes)]
    # self.create_output_array(fn, inner_args, outer_shape, name)
    output =  self.create_output_array(fn, first_values, outer_shape)

    loop_body = self.indexify_fn(fn, axes, args, 
                                 cartesian_product = True, 
                                 output = output)
    self.insert_parfor(loop_body, outer_shape, len(args))
    return output 
  
  def transform_IndexMap(self, expr, output = None):
    shape = expr.shape
    
    fn = expr.fn 
    dims = self.tuple_elts(shape)
    n_dims = len(dims)
    if n_dims == 1: shape = dims[0]
    if output is None: output = self.create_output_array(fn, [shape], shape)
    old_closure_args = self.closure_elts(fn)
    old_closure_arg_types = get_types(old_closure_args)
    fn = self.get_fn(fn)
    
    closure_arg_names = [self.fresh_input_name(clos_arg) for clos_arg in old_closure_args] 
    new_closure_vars = [Var(name, type=t) 
                        for name, t in 
                        zip(closure_arg_names, old_closure_arg_types)]
    
      
    old_input_types = fn.input_types
    last_input_type = old_input_types[-1]
    index_is_tuple = isinstance(last_input_type, TupleT)
    if index_is_tuple:
      index_types = last_input_type.elt_types
    else:
      index_types = old_input_types[-n_dims:]
    
    idx_names = self.fresh_index_names(n_dims)
    assert len(index_types) == n_dims, \
        "Mismatch between bounds of IndexMap %s and %d index formal arguments" % (dims, len(index_types))
    output_name = names.refresh("output")  
    
    new_input_names = [output_name] + closure_arg_names + idx_names            
    new_input_types =  [output.type]  + old_closure_arg_types + list(index_types)
    new_fn_name = names.fresh("idx_" + names.original(fn.name))
    new_fn, builder, input_vars = build_fn(new_input_types, NoneType,
                                           name = new_fn_name,  
                                           input_names = new_input_names)
    new_fn.created_by = self.fn.created_by 
    output_var = input_vars[0]
    
    idx_vars = input_vars[-n_dims:]
    
    if not self.is_none(expr.start_index):
      if isinstance(expr.start_index.type, ScalarT):
        idx_vars = [builder.add(idx, expr.start_index, "idx") for idx in idx_vars]
      else:
        start_indices = builder.tuple_elts(expr.start_index)
        assert len(start_indices) == len(idx_vars), \
          "Mismatch between number of indices %s and start offsets %s" % (idx_vars, start_indices)
        idx_vars = [builder.add(idx, offset, "idx%d" % i) 
                    for i, (idx,offset) 
                    in enumerate(zip(idx_vars, start_indices))]
           
    
    if index_is_tuple:
      elt_result = builder.call(fn, new_closure_vars + [builder.tuple(idx_vars)])
    else:
      elt_result = builder.call(fn, new_closure_vars + idx_vars)
    if len(idx_vars) == 1:
      builder.setidx(output_var, idx_vars[0], elt_result)
    else:
      builder.setidx(output_var, builder.tuple(idx_vars), elt_result)

      
    builder.return_(none)
    new_closure = self.closure(new_fn, (output,) + tuple(old_closure_args)  )
    self.insert_parfor(new_closure, shape, len(old_closure_args))
    return output
  
  def transform_Reduce(self, expr):
    fn = expr.fn 
    combine = expr.combine 
    init = expr.init 
    
    args = []
    axes = []
    raw_axes = self.normalize_axes(expr.args, expr.axis)
    for axis, arg in zip(raw_axes, expr.args):
      if self.is_none(axis):
        args.append(self.ravel(arg))
        axes.append(0)
      else:
        args.append(arg)
        axes.append(axis)
    
    max_arg = max_rank_arg(args)
    nelts = self.shape(max_arg, axis)
    
    if self.is_none(axis):
      nelts = self.prod(nelts)
      
    if init is None or self.is_none(init):
      init_args = [self.index_along_axis(arg, axis, self.int(0)) for arg, axis in zip(args, axes)]
      init = self.call(fn, init_args)
      index_offsets = (1,) * len(axes)
      assert init.type == fn.return_type
      nelts = self.sub(nelts, self.int(1), "nelts") 
    else:
      index_offsets = None
    
    index_fn = self.indexify_fn(fn, 
                                axis, 
                                args, 
                                cartesian_product=False, 
                                index_offsets = index_offsets)

    return IndexReduce(fn = index_fn, 
                       init = init, 
                       combine = combine, 
                       shape = nelts, 
                       type = expr.type)
   
  def transform_Scan(self, expr, output = None):
    
    combine = expr.combine 
    init = expr.init 
    
    
    args = []
    axes = []
    for (arg, axis)  in zip(expr.args, self.normalize_axes(expr.args, expr.axis)):
      if self.is_none(axis):
        args.append(self.ravel(arg))
        axis = 0
      else:
        args.append(arg)
      axes.append(axis)
     
    bounds = self.iter_bounds(args, axes)

    if self.is_none(init):
      assert len(args) == 1
      init_args = [self.index_along_axis(arg, axis, self.int(0))
                   for arg in args]
      init = self.call(expr.fn, init_args)  
      index_offsets = (1,) * len(bounds)
      bounds = tuple(self.sub(bound, self.int(1), "scan_niters") for bound in bounds)
    else:
      index_offsets = None
      
    index_fn = self.indexify_fn(expr.fn, 
                                axis, 
                                args, 
                                cartesian_product=False, 
                                index_offsets = index_offsets)
    

    return IndexScan(fn = index_fn,
                     init = init, 
                     combine = combine,
                     emit = expr.emit, 
                     shape = bounds,
                     type = expr.type)
  
  def transform_Filter(self, expr):
    assert False, "Filter not implemented"
  
  def transform_FilterReduce(self, expr):
    raise NotImplemented
  
  def transform_IndexFilter(self, expr):
    raise NotImplemented 
  
  def transform_IndexFilterReduce(self, expr):
    raise NotImplemented 
  
  
  def transform_Assign(self, stmt):
    """
    If you encounter an adverb being written to an output location, 
    then why not just use that as the output directly? 
    """
    if stmt.lhs.__class__ is Index:
      rhs_class = stmt.rhs.__class__ 
      if rhs_class is Map:
        self.transform_Map(stmt.rhs, output = stmt.lhs)
        return None 
      elif rhs_class is OuterMap:
        self.transform_OuterMap(stmt.rhs, output = stmt.lhs)
    return Transform.transform_Assign(self, stmt)

  

########NEW FILE########
__FILENAME__ = index_elimination


from .. analysis.index_elim_analysis import IndexElimAnalysis
from .. analysis.index_elim_analysis import \
  (unknown, ConstValue, ConstElts, RangeArray, IndexMapResult)
from .. ndtypes import ScalarT, TupleT 
from .. syntax import Const, Var 
from transform import Transform   
 

        
class IndexElim(Transform):
  def pre_apply(self, fn):
    analysis = IndexElimAnalysis() 
    analysis.visit_fn(fn)
    self.array_values = analysis.array_values
     
  def transform_Index(self, expr):
    if expr.value.__class__ is not Var: return expr 
    x = expr.value.name 
    if x not in self.array_values: return expr
    v = self.array_values[x]
    if v is unknown: return expr 
    if self.is_tuple(expr.index):
      indices = self.tuple_elts(expr.index)
    else:
      indices = [expr.index]
    if not all(isinstance(idx.type, ScalarT) for idx in indices): return expr
     
    n_indices = len(indices)
    if v.__class__ is ConstValue:
      if n_indices != v.type.rank: return expr
      return Const(v.value, v.type)
    elif v.__class__ is ConstElts  and all(idx.__class__ is Const for idx in indices):
      if n_indices != v.type.rank: return expr
      idx_values = [idx.value for idx in indices]
      return Const(v.array[tuple(idx_values)], type = v.type.elt_type)
    elif v.__class__ is IndexMapResult: 
      fn = self.get_fn(v.fn)
      closure_args = self.get_closure_args(v.fn)
      input_types = fn.input_types
      last_input_type =  input_types[-1]
      if last_input_type.__class__ is TupleT:
        n_expected_indices = len(last_input_type.elt_types)
        indices_are_tuple = True 
      else:
        indices_are_tuple = False
        n_expected_indices = len(input_types) - len(closure_args)
      if n_indices != n_expected_indices:
        return expr 
      if indices_are_tuple:
        index_exprs = [self.tuple(indices)]
      else:
        index_exprs = list(indices)
      return self.call(v.fn, index_exprs)
        
    assert v.__class__ is RangeArray
    assert len(indices) == 1
    idx = indices[0]
    if idx.__class__ is Const and v.step.__class__ is Const and v.start.__class__ is Const:
      return Const(idx.value * v.step.value + v.start.value, type = v.type) 

    return self.add(v.start, self.mul(idx, v.step))
      
########NEW FILE########
__FILENAME__ = inline

from .. import names
from .. analysis import contains_calls, can_inline  
from .. syntax import (If, Assign, While, Return, ForLoop, Var, TypedFn, Const, ExprStmt)

 
from subst import subst_stmt_list
from transform import Transform


def replace_returns(stmts, output_var):
  """Change any returns at the outer scope into assignments to the output var"""

  new_stmts = []
  for stmt in stmts:
    c = stmt.__class__
    if c is Return:
      if output_var:
        new_stmts.append(Assign(output_var, stmt.value))
      continue
    if c is If:
      stmt.true = replace_returns(stmt.true, output_var)
      stmt.false = replace_returns(stmt.false, output_var)
    elif c in (ForLoop, While):
      stmt.body = replace_returns(stmt.body, output_var)

    new_stmts.append(stmt)
  return new_stmts


def replace_return_with_var(body, type_env, return_type):
  result_name = names.fresh("result")
  type_env[result_name] = return_type
  result_var = Var(result_name, type = return_type)
  new_body = replace_returns(body, result_var)
  return result_var, new_body

def do_inline(src_fundef, args, dest_type_env, dest_block, result_var = None):
  arg_names = src_fundef.arg_names
  n_expected = len(arg_names)
  n_given = len(args)
  arg_str = ",".join(src_fundef.arg_names)
  assert n_expected == n_given, \
      "Function %s expects %d args (%s) but given %d (%s)" % \
      (src_fundef.name, n_expected, arg_str, n_given, args)

  rename_dict = {}

  def rename_var(old_name):
    t = src_fundef.type_env[old_name]
    new_name = names.refresh(old_name)
    new_var = Var(new_name, type = t)
    rename_dict[old_name] = new_var
    dest_type_env[new_name] = t

    return new_var

  for (old_formal, actual) in zip(arg_names, args):
    if actual.__class__ in (Var, Const):
      rename_dict[old_formal] = actual
    else:
      new_var = rename_var(old_formal)
      dest_block.append(Assign(new_var, actual))

  for old_name in src_fundef.type_env.iterkeys():
    if old_name not in arg_names:
      rename_var(old_name)

  new_body = subst_stmt_list(src_fundef.body, rename_dict)
  if result_var is None:
    result_var, new_body = \
        replace_return_with_var(new_body, dest_type_env, src_fundef.return_type)
  else:
    new_body = replace_returns(new_body, result_var)
  dest_block.extend(new_body)
  return result_var

class Inliner(Transform):
  def __init__(self):
    Transform.__init__(self)
    self.count = 0


  def transform_TypedFn(self, expr):
    return Inliner().apply(expr)
    #if self.fn.created_by is not None:
    #  return self.fn.created_by.apply(expr)
    #else:
    #  # at the very least, apply high level optimizations
    #  import pipeline
    #  return pipeline.high_level_optimizations.apply(expr)
     
  
  def transform_ExprStmt(self, stmt):

    return Transform.transform_ExprStmt(self, stmt)
  
  def transform_Call(self, expr):

    fn = self.transform_expr(expr.fn)
    target = self.get_fn(fn)
    if target.__class__ is TypedFn:
      closure_args = self.closure_elts(fn)
      if not can_inline(target):
        # print "[Warning] Can't inline %s" % target
        return expr  
      self.count += 1
      curr_block = self.blocks.current()
      combined_args = tuple(closure_args) + tuple(expr.args)
      result_var = do_inline(target, combined_args,
                             self.type_env, curr_block)    
      return result_var
    else:
      return expr


  
  def apply(self, fn):
    if contains_calls(fn):
      return Transform.apply(self, fn)
    else:
      return fn
    

  
########NEW FILE########
__FILENAME__ = licm

from dsltools import ScopedSet 

from .. analysis.collect_vars import collect_var_names, collect_binding_names
from .. analysis.escape_analysis import may_alias
from .. analysis.syntax_visitor import SyntaxVisitor

from .. ndtypes import ImmutableT, ArrayT, PtrT 
from .. syntax import Var, Assign, Return, While, If, Index, Alloc, AllocArray  
from .. syntax import Array, ArrayView, Slice, Struct 

from transform import Transform

class Find_LICM_Candidates(SyntaxVisitor):
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.mutable_types = None
    self.volatile_vars = ScopedSet()
    self.depends_on = {}
    self.safe_to_move = set([])
    self.curr_block_id = None
    self.block_contains_return = set([])
    self.may_alias = None

  def visit_fn(self, fn):
    self.volatile_vars.push(fn.arg_names)
    self.may_alias = may_alias(fn)
    SyntaxVisitor.visit_fn(self, fn)
    return self.safe_to_move

  def mark_safe_assignments(self, block, volatile_set):
    for stmt in block:
      klass = stmt.__class__
      if klass is Assign and \
         stmt.lhs.__class__ is Var:
        name = stmt.lhs.name
        dependencies = self.depends_on.get(name, set([]))
        volatile = name in volatile_set or \
                   any(d in volatile_set for d in dependencies)
        if not volatile:
          self.safe_to_move.add(name)
      # just in case there are Returns in nested control flow
      # we should probably avoid changing the performance characteristics
      # by pulling out statements which will never run
      elif klass is If:
        if id(stmt.true) in self.block_contains_return or \
           id(stmt.false) in self.block_contains_return:
          break
      elif klass is While:
        if id(stmt.body) in self.block_contains_return:
          break
      elif klass is Return:
        break

  def mark_curr_block_returns(self):
    self.block_contains_return.add(self.curr_block_id)

  def does_block_return(self, block):
    return id(block) in self.block_contains_return

  def visit_Return(self, stmt):
    self.mark_curr_block_returns()

  def visit_block(self, stmts):
    self.curr_block_id = id(stmts)
    SyntaxVisitor.visit_block(self, stmts)

  def visit_merge(self, merge, both_branches = True):
    pass

  def visit_ForLoop(self, stmt):
    self.volatile_vars.push(stmt.merge.keys())
    self.volatile_vars.add(stmt.var.name)
    SyntaxVisitor.visit_ForLoop(self, stmt)
    if self.does_block_return(stmt.body):
      self.block_contains_return()
    volatile_in_scope = self.volatile_vars.pop()
    self.mark_safe_assignments(stmt.body, volatile_in_scope)

  def visit_While(self, stmt):
    self.volatile_vars.push(stmt.merge.keys())
    SyntaxVisitor.visit_While(self, stmt)
    if self.does_block_return(stmt.body):
      self.block_contains_return()
    volatile_in_scope = self.volatile_vars.pop()
    self.mark_safe_assignments(stmt.body, volatile_in_scope)

  def visit_Var(self, expr):
    self.volatile_vars.add(expr.name)

  def visit_If(self, stmt):
    self.volatile_vars.push(stmt.merge.keys())
    self.visit_expr(stmt.cond)
    SyntaxVisitor.visit_If(self, stmt) 
    if self.does_block_return(stmt.true) or self.does_block_return(stmt.false):
      self.mark_curr_block_returns()
    volatile_in_scope = self.volatile_vars.pop()
    self.mark_safe_assignments(stmt.true, volatile_in_scope)
    self.mark_safe_assignments(stmt.false, volatile_in_scope)

  def is_mutable_alloc(self, expr):
    return isinstance(expr.type, (PtrT, ArrayT)) 
    #c = expr.__class__
    #return c in (Alloc, AllocArray, )
    #return c is Alloc or \
    #       c is AllocArray or \
    #       c is Array or \
    #       c is ArrayView or \
    #       c is 
    #       (c is Struct and not isinstance(expr.type, ImmutableT))

  def visit_Assign(self, stmt):
     
    lhs_names = collect_binding_names(stmt.lhs)
    rhs_names = collect_var_names(stmt.rhs)
   
    for x in lhs_names:
      dependencies = self.depends_on.get(x, set([]))
      dependencies.update(rhs_names)
      self.depends_on[x] = dependencies

    if any(x in self.volatile_vars for x in rhs_names):
      self.volatile_vars.update(lhs_names)
    elif self.is_mutable_alloc(stmt.rhs):
      if len(lhs_names) == 1 and \
         len(self.may_alias.get(lhs_names[0], [])) <= 1:
        pass
      else:
        self.volatile_vars.update(lhs_names)
    # mark any array writes as volatile 
    if stmt.lhs.__class__ is Index:
      assert stmt.lhs.value.__class__ is Var, \
        "Expected LHS array to be variable but instead got %s" % stmt  
      self.volatile_vars.add(stmt.lhs.value.name)
      #print 
      #print "STMT", stmt
      #print "lhs names", lhs_names 
      #print "rhs names", rhs_names 
      #print "volatile vars", self.volatile_vars
      #print 
      

class LoopInvariantCodeMotion(Transform):
  def __init__(self):
    Transform.__init__(self)

  def pre_apply(self, fn):
    self.analysis = Find_LICM_Candidates()
    self.safe_to_move = self.analysis.visit_fn(fn)
    self.binding_depth = {}
    self.mark_binding_depths(fn.arg_names, 0)

  def mark_binding_depths(self, names, depth_offset = 0):
    curr_depth = len(self.blocks._blocks) - 1 + depth_offset
    for name in names:
      self.binding_depth[name] = curr_depth

  def transform_ForLoop(self, stmt):
    self.mark_binding_depths(stmt.merge.iterkeys(), 1)
    self.mark_binding_depths([stmt.var.name], 1)
    return Transform.transform_ForLoop(self, stmt)

  def transform_While(self, stmt):
    self.mark_binding_depths(stmt.merge.iterkeys(), 1)
    return Transform.transform_While(self, stmt)

  def transform_If(self, stmt):
    self.mark_binding_depths(stmt.merge.iterkeys(), 1)
    return Transform.transform_If(self, stmt)

  def transform_Assign(self, stmt):
    # TODO: Allow possibility of indexing into variables that are read-only
    if stmt.lhs.__class__ is Var and stmt.rhs.__class__ is not Index:
      name = stmt.lhs.name
      if name in self.safe_to_move:
        deps = self.analysis.depends_on[name]

        if all(d in self.binding_depth for d in deps):
          if len(deps) > 0:
            target_level = max(self.binding_depth[d] for d in deps)
          else:
            target_level = 0
 
          if target_level >= 0 and target_level < self.blocks.depth():
            self.blocks._blocks[target_level].append(stmt)
            self.binding_depth[name] = target_level
            return None
    self.mark_binding_depths(collect_binding_names(stmt.lhs))
    return stmt

########NEW FILE########
__FILENAME__ = loop_transform

from ..analysis.collect_vars import collect_binding_names, collect_var_names
from ..analysis.escape_analysis import may_alias

from ..syntax import Return, While, ForLoop, If, Assign, Var, Index, ExprStmt
from transform import Transform

class LoopTransform(Transform):
  def is_simple_block(self, stmts, allow_branches = True):
    for stmt in stmts:
      if stmt.__class__ is If:
        if not allow_branches or \
           not self.is_simple_block(stmt.true) or \
           not self.is_simple_block(stmt.false):
          return False
      elif stmt.__class__ not in (Assign, ExprStmt):
        return False 
    return True

  def pre_apply(self, fn):
    self.may_alias = may_alias(fn)

  def collect_loop_vars(self, loop_vars, loop_body):
    """Gather the variables whose values change between loop iterations"""
    for stmt in loop_body:
      assert stmt.__class__ in (Assign, If, ExprStmt), \
        "Unexpected statement in simple block: %s" % stmt 
      if stmt.__class__ is Assign:
        lhs_names = collect_binding_names(stmt.lhs)
        rhs_names = collect_var_names(stmt.rhs)
        if any(name in loop_vars for name in rhs_names):
          loop_vars.update(lhs_names)

  def collect_memory_accesses(self, loop_body):
      # Assume code is normalized so a read will
      # directly on rhs of an assignment and
      # a write will be directly on the LHS

      # map from variables to index sets
      reads = {}
      writes = {}
      for stmt in loop_body:
        if stmt.__class__ is Assign:
          if stmt.lhs.__class__ is Index:
            assert stmt.lhs.value.__class__ is Var, "Unexpected LHS %s" % stmt.lhs 
            writes.setdefault(stmt.lhs.value.name, set([])).add(stmt.lhs.index)
          if stmt.rhs.__class__ is Index:
            assert stmt.rhs.value.__class__ is Var, "Unexpected LHS %s" % stmt.lhs 
            reads.setdefault(stmt.rhs.value.name, set([])).add(stmt.rhs.index)
      return reads, writes

  def is_loop_var(self, loop_vars, expr):
    #if expr.__class__ is Tuple:
    #  return all(self.is_loop_var(loop_vars, elt) for elt in expr.elts)
    #else:
    return expr.__class__ is Var and expr.name in loop_vars

  def any_loop_vars(self, loop_vars, expr_set):
    return any(self.is_loop_var(loop_vars, expr) for expr in expr_set)

########NEW FILE########
__FILENAME__ = loop_unrolling
from .. import syntax
from .. syntax import Const, ForLoop, Var
from .. syntax.helpers import const_int
from ..transforms import CloneStmt 

from loop_transform import LoopTransform


def safediv(m,n):
  return (m+n-1)/n

class LoopUnrolling(LoopTransform):
  def __init__(self, unroll_factor = 4,
                      max_static_unrolling = 8,
                      max_block_size = 50):
    LoopTransform.__init__(self)
    self.unroll_factor = unroll_factor
    if max_static_unrolling is not None:
    # should we unroll static loops more than ones with unknown iters?
      self.max_static_unrolling = max_static_unrolling
    else:
      self.max_static_unrolling = unroll_factor

    self.max_block_size = max_block_size

  def pre_apply(self, fn):
    # skip the alias analysis that's default for LoopTransform
    return fn

  def copy_loop_body(self, stmt, outer_loop_var, iter_num, phi_values = None):
    """Assume the current codegen block is the unrolled loop"""

    cloner = CloneStmt(self.type_env)
    # make a fresh copy of the loop
    loop = cloner.transform_ForLoop(stmt)
    i = const_int(iter_num, loop.var.type)
    loop_var_value = self.add(outer_loop_var, self.mul(stmt.step, i))
    self.assign(loop.var, loop_var_value)

    # if this isn't the first iteration of unrolling
    # then propagate old versions of phi-bound values
    # into this block
    if phi_values is not None:
      for (old_name, input_value) in phi_values.iteritems():
        new_var = cloner.rename_dict[old_name]
        self.assign(new_var, input_value)

    self.blocks.top().extend(loop.body)

    output_values = {}
    for old_name in stmt.merge.iterkeys():
        new_var = cloner.rename_dict[old_name]
        output_values[old_name] = loop.merge[new_var.name][1]

    return output_values, cloner.rename_dict

  def transform_ForLoop(self, stmt):
    assert self.unroll_factor > 0
    if self.unroll_factor == 1:
      return stmt

    if stmt.step.__class__ is Const:
      assert stmt.step.value > 0, "Downward loops not yet supported"

    stmt = LoopTransform.transform_ForLoop(self, stmt)

    if not self.is_simple_block(stmt.body) or \
       len(stmt.body) > self.max_block_size:
      return stmt

    start, stop, step = stmt.start, stmt.stop, stmt.step

    # if loop has static bounds, fully unroll unless it's too big
    unroll_factor = self.unroll_factor

    # number of iterations of loop iterations is not generally known
    if start.__class__ is Const and \
       stop.__class__ is Const and \
       step.__class__ is Const:
      niters = safediv(stop.value - start.value, step.value)
      if niters <= self.max_static_unrolling:
        unroll_factor = niters

    # push the unrolled body onto the stack
    self.blocks.push()

    phi_values = None
    loop_var = self.fresh_var(stmt.var.type,  "loop_counter")
    name_mappings = None
    for iter_num in xrange(unroll_factor):
      #self.comment("Unrolling iteration %d" % iter_num)
      phi_values, curr_names = \
          self.copy_loop_body(stmt, loop_var, iter_num, phi_values)
      if name_mappings is None:
        name_mappings = curr_names

    unrolled_body = self.blocks.pop()
    unroll_value = syntax.helpers.const_int(unroll_factor, stmt.var.type)
    unrolled_step = self.mul(unroll_value, stmt.step, "unrolled_step")
    n_total_iters = self.sub(stop, start, name = "niters")
    n_unrolled_iters = self.div(n_total_iters, unrolled_step, name = "unrolled_iters")
    # Python's division doesn't behave like C, so that small_negative/big_positive = -1
    # ..which is crappy when expecting truncation! 
    n_unrolled_iters = self.max(n_unrolled_iters, self.int(0), name = "unrolled_iters")
    trunc = self.mul(n_unrolled_iters, unrolled_step, "trunc")
    unrolled_stop = self.add(stmt.start, trunc, "unrolled_stop")
    final_merge = {}
    for (old_name, (input_value, _)) in stmt.merge.iteritems():
      first_name_in_loop = name_mappings[old_name].name
      output_value = phi_values[old_name]
      final_merge[first_name_in_loop] = (input_value, output_value)

    unrolled_loop = ForLoop(var = loop_var,
                            start = stmt.start,
                            stop = unrolled_stop,
                            step = unrolled_step,
                            body = unrolled_body,
                            merge = final_merge)

    if unrolled_loop.start.__class__ is Const and \
       unrolled_loop.stop.__class__ is Const and \
       unrolled_loop.step.__class__ is Const:
      start_value = unrolled_loop.start.value
      stop_value = unrolled_stop.value
      step_value = unrolled_loop.step.value
      if start_value + step_value == stop_value:
        self.assign(unrolled_loop.var, unrolled_loop.start)
        # assign all loop-carried variables to their initial values
        if len(final_merge) > 0:
          self.comment("Initialize loop-carried values")
        for (acc_name, (input_value, _)) in final_merge.iteritems():
          var = Var(acc_name, type = input_value.type)
          self.assign(var, input_value)
        # inline loop body
        self.blocks.top().extend(unrolled_body)
        # since we're not going to have a cleanup loop,
        # need to assign all the original phi-carried variables
        if len(stmt.merge) > 0:
          self.comment("Finalize loop-carried values")
        for old_acc_name in stmt.merge.iterkeys():
          last_value = phi_values[old_acc_name]
          var = Var(old_acc_name, last_value.type)
          self.assign(var, last_value)
        return None

    self.blocks.append(unrolled_loop)

    if unrolled_loop.stop.__class__ is not Const or \
       stop.__class__ is not Const or \
       unrolled_loop.stop.value != stop.value:
      cleanup_merge = {}
      for (old_name, (_, output_value)) in stmt.merge.iteritems():
        input_var = name_mappings[old_name]
        cleanup_merge[old_name] = (input_var, output_value)
      stmt.merge = cleanup_merge
      stmt.start = unrolled_loop.stop
    return stmt

########NEW FILE########
__FILENAME__ = lower_adverbs
from ..ndtypes import TupleT, Int64 
from ..syntax import Index, Map, unwrap_constant, zero_i64 
from transform import Transform
from parakeet.syntax.stmt import ForLoop

class LowerAdverbs(Transform):
    
  def transform_TypedFn(self, expr):
    from pipeline import loopify  
    return loopify(expr)
  

  
  def transform_IndexMap(self, expr, output = None):  
    # recursively descend down the function bodies to pull together nested ParFors
    fn = self.transform_expr(expr.fn)
    shape = expr.shape 
    
    dims = self.tuple_elts(shape)
    if len(dims) == 1:
      shape = dims[0]
    
    if output is None:
      output = self.create_output_array(fn, [shape], shape)

    def loop_body(idx):
      elt_result =  self.call(fn, (idx,))
      self.setidx(output, idx, elt_result)
    self.nested_loops(dims, loop_body)    
    return output
  
  _loop_counters = ["i", "j", "k", "l", "ii", "jj", "kk", "ll"]
  
  def get_loop_counter(self, depth):
    loop_counter_name = self._loop_counters[depth % len(self._loop_counters)] 
    return  self.fresh_var(Int64, loop_counter_name)
  
   
  def build_nested_reduction(self, indices, starts, bounds, old_acc, body_fn):
      if len(bounds) == 0:
        if len(indices) > 1:
          indices = self.tuple(indices)
        else:
          indices = indices[0]
        new_acc = body_fn(indices, old_acc)
        return new_acc 
      else:
        acc_t = old_acc.type 
        acc_before = self.fresh_var(acc_t, "acc_before")
        loop_counter = self.get_loop_counter(len(indices)) 
        
        future_indices = indices + (loop_counter,)
        future_starts = starts[1:]
        future_bounds = bounds[1:]
        self.blocks.push()
        acc_after = self.build_nested_reduction(future_indices, 
                                                future_starts, 
                                                future_bounds, 
                                                acc_before, 
                                                body_fn)
        body = self.blocks.pop()
        merge = {acc_before.name : (old_acc, acc_after)}
        for_loop = ForLoop(var = loop_counter, 
                           start = starts[0],
                           stop = bounds[0], 
                           step = self.int(1), 
                           body = body, 
                           merge = merge)
        self.blocks.append_to_current(for_loop)
        return acc_before # should this be acc_after?

  
  def transform_IndexReduce(self, expr):
    init = self.transform_if_expr(expr.init)
    fn = self.transform_expr(expr.fn)
    combine = self.transform_expr(expr.combine)
    shape = expr.shape 
    # n_loops = len(dims)
    
    assert init is not None, "Can't have empty 'init' field for IndexReduce"
   
    if isinstance(shape.type, TupleT): 
      bounds = self.tuple_elts(shape)
    else:
      bounds = [shape]
      
    if expr.start_index is not None:
      if isinstance(expr.start_index.type, TupleT):
        starts = self.tuple_elts(expr.start_index)
      else:
        starts = [expr.start_index]
    else:
      starts = [self.int(0)] * len(bounds)
      
    
    def body(indices, old_acc):
      elt = self.call(fn, (indices,))
      return self.call(combine, (old_acc, elt))
       
    return self.build_nested_reduction(
              indices = (), 
              starts = starts, 
              bounds = bounds, 
              old_acc = init, 
              body_fn = body)


  def transform_IndexScan(self, expr, output = None):
    init = self.transform_if_expr(expr.init)
    fn = self.transform_expr(expr.fn)
    combine = self.transform_expr(expr.combine)
    
    shape = expr.shape 
    if output is None:
      output = self.create_output_array(fn, [shape], shape)
      
    
    assert init is not None, "Can't have empty 'init' field for IndexScan"
    assert init.type == self.return_type(fn), \
      "Mismatching types init=%s, fn returns %s" % (init.type, self.return_type(fn))
    if isinstance(shape.type, TupleT): 
      bounds = self.tuple_elts(shape)
    else:
      bounds = [shape]
      
    def body(indices, old_acc):
      elt = self.call(fn, (indices,))
      new_acc = self.call(combine, (old_acc, elt))
      self.setidx(output, indices, new_acc)
      return new_acc
    starts = [self.int(0)] * len(bounds)
    self.build_nested_reduction(indices = (),
                                starts = starts, 
                                bounds = bounds, 
                                old_acc = init, body_fn = body)
    return output 
  
  def transform_IndexFilter(self, expr):
    assert False, "IndexFilter not implemented"

  
  def transform_IndexFilterReduce(self, expr):
    assert False, "IndexFilterReduce not implemented"
    
  

  

########NEW FILE########
__FILENAME__ = lower_array_operators
from ..builder import build_fn
from ..syntax import  Alloc,  Index, ArrayView, Const, Transpose, Ravel 
from ..syntax.helpers import zero_i64, one_i64, const_int, const_tuple, true, false   
from ..ndtypes import (ScalarT, PtrT, TupleT, ArrayT, ptr_type, Int64)

from transform import Transform


class LowerArrayOperators(Transform):
  """
  Lower first-order array operators such as Ravel, transpose
  into views or copies
  """


  def transform_Array(self, expr):
    """Array literal"""
    array_t = expr.type
    assert isinstance(array_t, ArrayT), "Expected array but got %s" % array_t
    elt_t = array_t.elt_type
    assert isinstance(elt_t, ScalarT), "Expected array to have scalar elements but got %s" % elt_t

    elts = self.transform_expr_list(expr.elts)
    n = len(elts)

    ptr_t = ptr_type(elt_t)
    alloc = Alloc(elt_t, const_int(n), type = ptr_t)
    ptr_var = self.assign_name(alloc, "data")
    array = self.array_view(ptr_var, 
                            shape = const_tuple(n), 
                            strides = const_tuple(1), 
                            offset = zero_i64,
                            nelts = const_int(n))
    for (i, elt) in enumerate(elts):
      self.setidx(array, const_int(i), elt)
    return array 
  
      
  def transform_Len(self, expr):
    return self.shape(expr, 0)

  def transform_AllocArray(self, expr):
    dims = self.transform_expr(expr.shape)
    return self.alloc_array(elt_t = expr.type.elt_type,
                            dims = dims, 
                            name = "array",
                            order = "C", 
                            array_view = True, 
                            explicit_struct = False)
    
  def transform_Reshape(self, expr):
    assert False, "Reshape not implemented"
    
  def transform_Shape(self, expr):

    return self.shape(self.transform_expr(expr.array))
    
  def transform_Strides(self, expr):
    return self.strides(self.transform_expr(expr.array))
  
  """
  Given first-order array constructors, turn them into IndexMaps
  """

  def mk_range_fn(self, start, step, output_type, _range_fn_cache = {}):
    """
    Given expressions for start, stop, and step of an iteration range
    and a desired output type. 
    
    Create a function which maps indices i \in 0...(stop-start)/step into
    f(i) = cast(start + i * step, output_type)
    """
    
      
    start_is_const = start.__class__ is Const
    step_is_const = step.__class__ is Const
    
    key = output_type, (start.type, start_is_const), (step.type, step_is_const)
    if key in _range_fn_cache:
      fn = _range_fn_cache[key]
    else:
      input_types = []
      if not start_is_const: input_types.append(start.type)
      if not step_is_const: input_types.append(step.type)
      input_types.append(Int64)
      fn, builder, input_vars = build_fn(input_types, output_type)
    
      assert len(input_vars) == (3 - start_is_const - step_is_const), \
        "Unexpected # of input vars %d" % len(input_vars)
        
      counter = 0
      if start_is_const:
        inner_start = start 
      else:
        inner_start = input_vars[counter]
        counter += 1
    
      if step_is_const:
        inner_step = step 
      else:
        inner_step = input_vars[counter]
        counter += 1
    
      idx = input_vars[counter]
    
      value = builder.add(builder.mul(idx, inner_step), inner_start)
      builder.return_(builder.cast(value, output_type))
      fn.created_by = self.fn.created_by 
      _range_fn_cache[key] = fn
    
    closure_args = []
    if not start_is_const: 
      closure_args.append(start)
    if not step_is_const: 
      closure_args.append(step)
    return self.closure(fn, closure_args) 
      
  def transform_Range(self, expr):
    start = expr.start 
    stop = expr.stop
    step = expr.step 
    caster = self.mk_range_fn(start, step, expr.type.elt_type)
    nelts = self.elts_in_slice(start, stop, step)
    
    return self.imap(caster, nelts)

  # TODO: Add support for np.ndindex through this operator 
  def transform_NDIndex(self, expr):
    shape = self.transform_expr(expr.shape)
    assert False, "NDIndex not yet implemented"
    
  
  def transform_Ravel(self, expr):
    
    array = expr.array 
    while array.__class__ is Ravel:
      array = expr.array 
    array = self.transform_expr(array)
    strides = self.tuple_elts(self.attr(array, 'strides'))
    shape = self.tuple_elts(self.attr(array, 'shape'))
    any_unit = false
    nelts = one_i64 
    for i in xrange(len(shape)):
      shape_elt = shape[i]
      nelts = self.mul(nelts, shape_elt)
      stride_elt = strides[i]
      any_unit = self.or_(any_unit, self.eq(stride_elt, one_i64))      
    array_result = self.fresh_var(expr.type, prefix = "raveled")
    data = self.attr(array, 'data')
    offset = self.attr(array, 'offset')
    def contiguous(x):
      view =  self.array_view(data, 
                              shape = self.tuple([nelts]), 
                              strides = self.tuple([one_i64]), 
                              offset = offset, 
                              nelts = nelts)
      self.assign(x, view)
    def not_contiguous(x):
      new_array = self.alloc_array(x.type.elt_type, 
                                   dims = self.attr(array, 'shape'),
                                   name = "fresh_array", 
                                   explicit_struct = False, 
                                   array_view = True, 
                                   order = "C")
      self.array_copy(array, new_array)
      data = self.attr(new_array, 'data')
      flat_view = self.array_view(data, 
                              shape = self.tuple([nelts]), 
                              strides = self.tuple([one_i64]), 
                              offset = offset, 
                              nelts = nelts)
      self.assign(x, flat_view)
    self.if_(any_unit, contiguous, not_contiguous, [array_result])
    return array_result 

    
  def transform_Transpose(self, expr):
    if expr.array.__class__ is Transpose:
      return self.transform_expr(expr.array.array)
    
    array = self.transform_expr(expr.array)
    if isinstance(array.type, ScalarT):
      return array 
    assert isinstance(array.type, ArrayT), "Can't transpose %s : %s" % (array, array.type)
    ndims = array.type.rank 
    if ndims <= 1:
      return array 
    assert ndims == 2, "Tranposing arrays of rank %d not yet supported" % (ndims,)
    
    data = self.attr(array, 'data')
    shape = self.shape(array)
    strides = self.strides(array)
    offset = self.attr(array, 'offset')
    stride_elts = self.tuple_elts(strides)
    shape_elts = self.tuple_elts(shape)
    new_shape = self.tuple(tuple(reversed(shape_elts)))
    new_strides = self.tuple(tuple(reversed(stride_elts)))
    size = self.attr(array, 'size')
    return self.array_view(data, new_shape, new_strides, offset, size)
    
  def transform_Where(self, expr):
    assert False, "Where not implemented"
  
  
  def mk_const_fn(self, idx_type, value, _const_fn_cache = {}):
    if isinstance(idx_type, TupleT) and len(idx_type.elt_types) == 1:
      idx_type = idx_type.elt_types[0]
    key = idx_type, value
    if value in _const_fn_cache:
      return _const_fn_cache[key]
    fn, builder, _ = build_fn([idx_type], value.type)
    builder.return_(value)
    fn.created_by = self.fn.created_by 
    _const_fn_cache[key] = fn 
    return fn 
    
  
  def transform_ConstArray(self, expr):
    const_fn = self.mk_const_fn(expr.shape.type, expr.value)
    return self.imap(const_fn, expr.shape)
  
  def transform_ConstArrayLike(self, expr):
    array = self.transform_expr(expr.array)
    shape = self.shape(array)
    const_fn = self.mk_const_fn(shape.type, expr.value)
    return self.imap(const_fn, shape)
  
  
  
########NEW FILE########
__FILENAME__ = lower_indexing

from .. import syntax
from .. builder import Builder
from .. ndtypes import SliceT, make_array_type, ArrayT 
from .. ndtypes import NoneT, ScalarT, Int64, PtrT, IntT
from .. syntax import Const, Index, Tuple, Var, ArrayView, Assign, Slice, Struct
from ..syntax.helpers import zero_i64, one_i64

from transform import Transform

class LowerIndexing(Transform):
  def pre_apply(self, fn):
    self.bindings = {}

  def tuple_proj(self, tup, idx, explicit_struct = False):
    if tup.__class__ is Var and tup.name in self.bindings:
      stored = self.bindings[tup.name]
      if stored.__class__ is Tuple:
        return stored.elts[idx]
      else:
        return stored.args[idx]
    else:
      return Builder.tuple_proj(self, tup, idx,
                                  explicit_struct = explicit_struct)

  def attr(self, obj, field):
    if obj.__class__ is Var and obj.name in self.bindings:
      stored = self.bindings[obj.name]
      stored_class = stored.__class__
      if stored_class is Struct:
        pos = stored.type.field_pos(field)
        return stored.args[pos]
      elif stored_class  is Slice or stored_class is ArrayView:
        return getattr(stored, field)

    return Builder.attr(self, obj, field)


  def array_slice(self, arr, indices):
    data_ptr = self.attr(arr, "data")
    shape = self.attr(arr, "shape")
    strides = self.attr(arr, "strides")
    elt_offset = self.attr(arr, "offset")
    size = self.attr(arr, "size")

    new_strides = []
    new_shape = []

    for (i, idx) in enumerate(indices):
      stride_i = self.tuple_proj(strides, i)
      shape_i = self.tuple_proj(shape, i)
      idx_t = idx.type
      if isinstance(idx_t, ScalarT):
        offset_i = self.mul(idx, stride_i, "offset_%d" % i)
        elt_offset = self.add(elt_offset, offset_i)
      elif idx_t.__class__ is NoneT:
        new_strides.append(stride_i)
        new_shape.append(shape_i)
      elif idx_t.__class__ is SliceT:
        if isinstance(idx_t.start_type, NoneT):
          start =  zero_i64
        else:
          start = self.attr(idx, "start")
        
        if isinstance(idx_t.step_type, NoneT):
          step = one_i64
        else:
          step = self.attr(idx, "step")
        
        if isinstance(idx_t.stop_type, NoneT):
          stop = shape_i
        else:
          stop = self.attr(idx, "stop")
        
        offset_i = self.mul(start, stride_i, "offset_%d" % i)
        elt_offset = self.add(elt_offset, offset_i)
        dim_i = self.sub(stop, start, "dim_%d" % i)
        # don't forget to cast shape elements to int64
        new_shape.append(self.cast(dim_i, Int64))
        new_strides.append(self.mul(stride_i, step))
      else:
        raise RuntimeError("Unsupported index type: %s" % idx_t)

    elt_t = arr.type.elt_type
    new_rank = len(new_strides)
    new_array_t = make_array_type(elt_t, new_rank)
    new_strides = self.tuple(new_strides, "strides")
    new_shape = self.tuple(new_shape, "shape")
    return ArrayView(data_ptr, new_shape, new_strides,
                            elt_offset, size,
                            type = new_array_t)

  def transform_Index(self, expr):
    arr = self.transform_expr(expr.value)
    idx = self.transform_expr(expr.index)
    idx = self.assign_name(idx, "idx")

    arr_t = arr.type
    if arr_t.__class__ is PtrT:
      assert isinstance(idx.type, IntT)
      return expr

    assert arr_t.__class__ is  ArrayT, "Unexpected array %s : %s" % (arr, arr.type)

    if self.is_tuple(idx):
      indices = self.tuple_elts(idx)
    else:
      indices = [idx]
    # right pad the index expression with None for each missing index
    n_given = len(indices)
    n_required = arr_t.rank
    if n_given < n_required:
      extra_indices = [syntax.helpers.slice_none] * (n_required - n_given)
      indices.extend(extra_indices)

    # fast-path for the common case when we're indexing
    # by all scalars to retrieve a scalar result
    if syntax.helpers.all_scalars(indices):
      data_ptr = self.attr(arr, "data")
      strides = self.attr(arr, "strides")
      offset_elts = self.attr(arr, "offset")
      for (i, idx_i) in enumerate(indices):
        stride_i = self.tuple_proj(strides, i)

        elts_i = self.mul(stride_i, idx_i, "offset_elts_%d" % i)
        offset_elts = self.add(offset_elts, elts_i, "total_offset")
      return self.index(data_ptr, offset_elts, temp = False)
    else:
      return self.array_slice(arr, indices)

  def transform_Assign(self, stmt):
    lhs = stmt.lhs
    lhs_class = lhs.__class__
    rhs = self.transform_expr(stmt.rhs)
    if lhs_class is Tuple:
      for (i, _) in enumerate(lhs.type.elt_types):
        lhs_i = self.tuple_proj(lhs, i)
        rhs_i = self.tuple_proj(rhs, i)
        # TODO: make this recursive, otherwise nested
        # complex assignments won't get implemented
        assert lhs_i.__class__ not in (ArrayView, Tuple)
        self.assign(lhs_i, rhs_i)
      return None
    
    
    elif lhs_class is Index:
      lhs = self.transform_Index(lhs)
      if lhs.__class__ is ArrayView:
        copy_loop = self.array_copy(src = rhs, dest = lhs, return_stmt = True)
        copy_loop = self.transform_stmt(copy_loop)
        return copy_loop
    elif lhs_class is Var and \
         stmt.rhs.__class__ in (Slice, Struct, ArrayView, Tuple):
      self.bindings[lhs.name] = rhs

    return Assign(lhs, rhs)

########NEW FILE########
__FILENAME__ = lower_slices

from .. builder import build_fn 
from .. ndtypes import (NoneT, ScalarT, Int64, SliceT, TupleT, NoneType,repeat_tuple)
from .. syntax import  Index, Tuple, Var, ArrayView
from ..syntax.helpers import zero_i64, one_i64, all_scalars, slice_none, none


from transform import Transform
from phase import Phase 


class LowerSlices(Transform):  
  

  
  _setidx_cache = {}
  def make_setidx_fn(self, lhs_array_type, 
                             rhs_value_type, 
                             fixed_positions, 
                             slice_positions):
    fixed_positions = tuple(fixed_positions)
    slice_positions = tuple(slice_positions)
    key = lhs_array_type, rhs_value_type, fixed_positions, slice_positions
    if key in self._setidx_cache: 
      return self._setidx_cache[key]

    
    n_fixed_indices = len(fixed_positions)
    n_parfor_indices = len(slice_positions)
    n_indices = n_fixed_indices + n_parfor_indices
    parfor_idx_t = repeat_tuple(Int64, n_parfor_indices) if n_parfor_indices > 1 else Int64 
    fixed_index_types = [Int64] * n_fixed_indices
    slice_start_types = [Int64] * n_parfor_indices
    slice_step_types = [Int64] * n_parfor_indices
    input_types = [lhs_array_type, rhs_value_type] + fixed_index_types + \
      slice_start_types + slice_step_types + [parfor_idx_t]
    name = "setidx_array%d_%s_par%d" % \
      (lhs_array_type.rank, lhs_array_type.elt_type, n_parfor_indices)
      
    #build_fn
    # Inputs:  
    #  - input_types
    #  - return_type=NoneType
    #  - name=None
    #  - input_names=None
    # Outputs:
    #  f, builder, input_vars 
    idx_names = ["idx%d" % (i+1) for i in xrange(n_fixed_indices)]
    start_names = ["start%d" % (i+1) for i in xrange(n_parfor_indices)]
    step_names = ["step%d" % (i+1) for i in xrange(n_parfor_indices)]
    
    input_names = ["output_array", "input_array"] + idx_names + start_names + step_names + ["sliceidx"]
    
    fn, builder, input_vars = build_fn(input_types, NoneType, name, input_names)
    lhs = input_vars[0]
    rhs = input_vars[1]
    fixed_indices = input_vars[2:(2+n_fixed_indices)]
    starts = input_vars[(2+n_fixed_indices):(2+n_fixed_indices+n_parfor_indices)]
    steps = input_vars[(2+n_fixed_indices+n_parfor_indices):(2+n_fixed_indices+2*n_parfor_indices)]
    assert (2+n_fixed_indices+2*n_parfor_indices+1) == len(input_vars), \
      "Wrong number of vars: %s, expected %d but got %d" % \
      (input_vars, 2+n_fixed_indices+2*n_parfor_indices+1, len(input_vars))
    parfor_idx = input_vars[-1]
    if n_parfor_indices > 1:
      parfor_indices = builder.tuple_elts(parfor_idx)
    else:
      parfor_indices = [parfor_idx]
    
    indices = []   
    # interleave the fixed and slice indices in their appropriate order
    # This would be easier if I had OCaml cons lists!
    slice_counter = 0
    fixed_counter = 0
    for i in xrange(n_indices):
      if fixed_counter < len(fixed_positions)  and i == fixed_positions[fixed_counter]:
        indices.append(fixed_indices[fixed_counter])
        fixed_counter += 1 
      else:
        assert slice_counter < len(slice_positions)  and slice_positions[slice_counter] == i, \
          "Bad positions for indices, missing %d" % i  
        start = starts[slice_counter]
        step = steps[slice_counter]
        parfor_idx = parfor_indices[slice_counter]
        indices.append(builder.add(start, builder.mul(step, parfor_idx)))
        slice_counter += 1
    
    value = builder.index(rhs, parfor_indices)
    builder.setidx(lhs, builder.tuple(indices), value)
    builder.return_(none)
    self._setidx_cache[key] = fn
    return fn
    
    
  def dissect_index_expr(self, expr):
    """
    Split up an indexing expression into 
    fixed scalar indices and the start/stop/step of all slices
    """

    if isinstance(expr.index.type, TupleT):
      indices = self.tuple_elts(expr.index)
    else:
      indices = [expr.index]
    
    n_dims = expr.value.type.rank 
    n_indices = len(indices)
    assert n_dims >= n_indices, \
      "Not yet supported: more indices (%d) than dimensions (%d) in %s" % (n_indices, n_dims, expr) 
    if n_indices < n_dims:
      indices = indices + [slice_none] * (n_dims - n_indices)
    
    if all_scalars(indices):
      # if there aren't any slice expressions, don't bother with the rest of this function
      return indices, range(len(indices)), [], []
    
    shape = self.shape(expr.value)
    shape_elts = self.tuple_elts(shape)
    slices = []
    slice_positions = []
    scalar_indices = []
    scalar_index_positions = []
    
    for i, shape_elt in enumerate(shape_elts):
      idx = indices[i]
      t = idx.type
      if isinstance(t, ScalarT):
        scalar_indices.append(idx)
        scalar_index_positions.append(i)
      elif isinstance(t, NoneT):
        slices.append( (zero_i64, shape_elt, one_i64) )
        slice_positions.append(i)
      else:
        assert isinstance(t, SliceT), "Unexpected index type: %s in %s" % (t, expr) 
        start = zero_i64 if t.start_type == NoneType else self.attr(idx, 'start')
        stop = shape_elt if t.stop_type == NoneType else self.attr(idx, 'stop')
        step = one_i64 if t.step_type == NoneType else self.attr(idx, 'step')
        slices.append( (start, stop, step) )
        slice_positions.append(i)   
    return scalar_indices, scalar_index_positions, slices, slice_positions
    
  def transform_Index(self, expr):

    ndims = expr.value.type.rank
    if ndims == 1 and isinstance(expr.index.type, ScalarT):
      return expr
    elif isinstance(expr.index.type, TupleT) and \
        len(expr.index.type.elt_types) == ndims and \
        all(isinstance(t, ScalarT) for t in expr.index.type.elt_types):
      return expr 
    
    scalar_indices, scalar_index_positions, slices, slice_positions = \
      self.dissect_index_expr(expr)
    assert len(scalar_indices) == len(scalar_index_positions)
    assert len(slices) == len(slice_positions)
    if len(slices) == 0:
      return expr
    
    array = self.transform_expr(expr.value)
    data = self.attr(array, 'data')
    shape = self.shape(array)
    shape_elts = self.tuple_elts(shape)
    strides = self.strides(array)
     
    stride_elts = self.tuple_elts(strides)
    offset = self.attr(array, 'offset')
    new_shape = []
    new_strides = []
    n_indices = len(scalar_indices) + len(slices)
    fixed_count = 0
    slice_count = 0

    for i in xrange(n_indices):
      if fixed_count < len(scalar_index_positions) and scalar_index_positions[fixed_count] == i:
        idx = scalar_indices[fixed_count]
        fixed_count += 1
        offset = self.add(offset, self.mul(idx, stride_elts[i]), name = "offset")

      else:
        assert slice_positions[slice_count] == i
        (start, stop, step) = slices[slice_count]
        slice_count += 1
        dim_offset = self.mul(start, stride_elts[i], name = "dim_offset")
        offset = self.add(offset, dim_offset, "offset")
        span = self.sub(stop, start, name = "span")
        shape_elt = self.div_round_up(span, step, name = "new_shape")
        new_shape.append(shape_elt)
        stride_elt = self.mul(stride_elts[i], step, name = "new_stride")
        new_strides.append(stride_elt)
 
    size = self.prod(new_shape)
    new_rank = len(slices)
    assert len(new_shape) == new_rank
    assert len(new_strides) == new_rank

    new_array = self.array_view(data, self.tuple(new_shape), self.tuple(new_strides), offset, size)
    assert new_array.type.rank == new_rank
    #if len(stride_elts) > 1:
    #  print "STRIDES", stride_elts 
    #  print "EXPR", expr 
    #  print "NEW ARRAY", new_array
    #  print 
    return new_array
    
    
      
  def assign_index(self, lhs, rhs):
    if isinstance(lhs.index.type, ScalarT) and isinstance(rhs.type, ScalarT):
      self.assign(lhs,rhs)
      return 
    
    scalar_indices, scalar_index_positions, slices, slice_positions = \
      self.dissect_index_expr(lhs)
    assert len(scalar_indices) == len(scalar_index_positions)
    assert len(slices) == len(slice_positions)
    if len(slices) == 0:
      self.setidx(lhs.value, self.tuple(scalar_indices), rhs)
      return 
    

    # if we've gotten this far then there is a slice somewhere in the indexing 
    # expression, so we're going to turn the assignment into a setidx parfor 
    bounds = self.tuple([self.div(self.sub(stop, start), step)
                         for (start, stop, step) in slices])
    
    setidx_fn = self.make_setidx_fn(lhs.value.type, 
                                    rhs.type, 
                                    scalar_index_positions, 
                                    slice_positions)
    starts = [start for (start, _, _) in slices]
    steps = [step for (_, _, step) in slices]
    closure = self.closure(setidx_fn, 
                           [lhs.value, rhs] + scalar_indices + starts + steps)


    self.parfor(closure, bounds)
  
  def transform_TypedFn(self, expr):
    if self.fn.created_by is not None and isinstance (self.fn.created_by, Phase):
      return self.fn.created_by.apply(expr)
    import pipeline
    return pipeline.indexify.apply(expr)
  
  def transform_Assign(self, stmt):
    lhs_class = stmt.lhs.__class__
    rhs = self.transform_expr(stmt.rhs)
     
    if lhs_class is Tuple:
      for (i, _) in enumerate(stmt.lhs.type.elt_types):
        lhs_i = self.tuple_proj(stmt.lhs, i)
        rhs_i = self.tuple_proj(rhs, i)
        # TODO: make this recursive, otherwise nested
        # complex assignments won't get implemented
        assert lhs_i.__class__ not in (ArrayView, Tuple)
        if lhs_i.__class__ is Index:
          self.assign_index(lhs_i)
        else:
          assert lhs_i.__class__ is Var, "Unexpcted LHS %s : %s" % (lhs_i, lhs_i.type)
          self.assign(lhs_i, rhs_i)
      return None
    elif lhs_class is Index:
      
      self.assign_index(stmt.lhs, rhs)
      return None
    else:
      stmt.rhs = rhs
      return stmt
    



































########NEW FILE########
__FILENAME__ = lower_structs




 

from .. import  syntax

from .. ndtypes import (ScalarT, ArrayT, make_array_type, TupleT, 
                        Int32, Int64,  ptr_type, PtrT, closure_type)  
from .. syntax import Struct, Assign, Const, Index, Attribute, Var, Tuple, Alloc  
from .. syntax.helpers import const_int, const_tuple, zero
from transform import Transform


class LowerStructs(Transform):
  """The only non-scalar objects should all be created as explicit Structs"""

  def transform_TypedFn(self, expr):
    import pipeline
    return pipeline.lowering.apply(expr)

  def transform_Tuple(self, expr):
    struct_args = self.transform_expr_list(expr.elts)
    return Struct(struct_args, type = expr.type)

  def transform_TupleProj(self, expr):
    new_tuple = self.transform_expr(expr.tuple)
    assert isinstance(expr.index, int)
    tuple_t = expr.tuple.type
    field_name, field_type  = tuple_t._fields_[expr.index]
    return Attribute(new_tuple, field_name, type = field_type)

  def transform_Slice(self, expr):
    struct_args = self.transform_expr_list([expr.start, expr.stop, expr.step])
    return Struct(struct_args, type = expr.type)

  def transform_Assign(self, stmt):
    lhs, rhs = stmt.lhs, stmt.rhs
    if isinstance(lhs, Tuple):
      for (i, lhs_elt) in enumerate(lhs.elts):
        self.assign(lhs_elt, self.tuple_proj(rhs, i), recursive = True)
    else:
      assert isinstance(lhs, (Var, Index, Attribute)), \
          "Invalid LHS: %s" % (lhs,)
      return Assign(stmt.lhs, self.transform_expr(rhs))

  def transform_Closure(self, expr):
    _ = self.transform_expr(expr.fn)
    closure_args = self.transform_expr_list(expr.args)
    closure_id = closure_type.id_of_closure_type(expr.type)
    closure_id_node = Const(closure_id, type = Int64)
    return Struct([closure_id_node] + closure_args, type = expr.type)

  def transform_ClosureElt(self, expr):
    new_closure = self.transform_expr(expr.closure)
    assert isinstance(expr.index, int)
    # first field is always the closure ID, so we have to
    # index 1 higher
    field_name, field_type = new_closure.type._fields_[expr.index + 1]
    return Attribute(new_closure, field_name, type = field_type)

  def array_view(self, data, shape, strides, offset, nelts):
    """Helper function used by multiple array-related transformations"""

    data = self.assign_name(self.transform_expr(data), "data_ptr")
    data_t = data.type
    assert isinstance(data_t, PtrT), \
        "Data field of array must be a pointer, got %s" % data_t

    shape = self.assign_name(self.transform_expr(shape), "shape")
    shape_t = shape.type
    assert isinstance(shape_t, TupleT), \
        "Shape of array must be a tuple, got: %s" % shape_t

    strides = self.assign_name(self.transform_expr(strides), "strides")
    strides_t = strides.type
    assert isinstance(strides_t, TupleT), \
        "Strides of array must be a tuple, got: %s" % strides_t

    rank = len(shape_t.elt_types)
    strides_rank = len(strides_t.elt_types)
    assert rank == strides_rank, \
        "Shape and strides must be of same length, but got %d and %d" % \
        (rank, strides_rank)
    array_t = make_array_type(data_t.elt_type, rank)
    return Struct([data, shape, strides, offset, nelts], type = array_t)

  def transform_ArrayView(self, expr):
    array_struct = self.array_view(expr.data, expr.shape, expr.strides,
                                   expr.offset, expr.size)
    assert expr.type == array_struct.type, \
        "Mismatch between original type %s and transformed type %s" % \
        (expr.type, array_struct.type)
    return array_struct

  def transform_Array(self, expr):
    """Array literal"""

    array_t = expr.type
    assert isinstance(array_t, ArrayT)
    elt_t = array_t.elt_type
    assert isinstance(elt_t, ScalarT)

    elts = self.transform_expr_list(expr.elts)
    n = len(elts)

    ptr_t = ptr_type(elt_t)
    alloc = Alloc(elt_t, const_int(n), type = ptr_t)
    ptr_var = self.assign_name(alloc, "data")

    for (i, elt) in enumerate(elts):
      idx = Const(i, type = Int32)
      lhs = Index(ptr_var, idx, type = elt_t)
      self.assign(lhs, elt)

    return self.array_view(ptr_var, const_tuple(n), const_tuple(1),
                           offset = const_int(0), nelts = const_int(n))


  def transform_Range(self, expr):
    diff = self.sub(expr.stop, expr.start, "range_diff")
    nelts = self.safediv(diff, expr.step, name="nelts_raw")
    result = self.alloc_array(Int64, 
                              (nelts,), 
                              name = "range_result", 
                              explicit_struct = True)
    ptr = self.attr(result, "data", "data_ptr")
    def loop_body(i):
      v = self.add(expr.start, self.mul(i, expr.step)) 
      self.setidx(ptr, i, v)
    self.loop(zero(Int64), nelts, loop_body, return_stmt = False, while_loop = False)
    return result 
  
  def transform_ConstArray(self, expr):
    assert False, "ConstArray not implemented"
    
  def transform_Ravel(self, expr):
    return self.ravel(self.transform_expr(expr.array), explicit_struct = True)
########NEW FILE########
__FILENAME__ = negative_index_elim

from ..analysis.value_range_analysis import TupleOfIntervals, SliceOfIntervals
from ..ndtypes import ArrayT, TupleT, SliceT, ScalarT, NoneType 
from ..syntax import Tuple, Slice, Var, Index  
from ..syntax.helpers import zero_i64, one_i64, slice_none, make_slice_type, none

from range_transform import RangeTransform, Interval, NoneValue

class NegativeIndexElim(RangeTransform):
  
  """
  This doesn't entirely obviate the need for runtime checks on indexing 
  but does handle the simple case where a negative index can be inferred from the source 
  """
  
  def has_negative(self, range_value, inclusive = False):
    if isinstance(range_value, Interval):
      return self.always_negative(range_value, inclusive = inclusive)
    elif isinstance(range_value, TupleOfIntervals):
      return any(self.has_negative(elt) for elt in range_value.elts)
    elif isinstance(range_value, SliceOfIntervals):
      return self.has_negative(range_value.start) or \
        self.has_negative(range_value.stop) or \
        self.has_negative(range_value.step) 
    
  
  def transform_Index(self, expr):

    arr = expr.value 
    arr_t = arr.type 
    if arr_t.__class__ is not ArrayT:
      return expr 
    
    index = expr.index 
    
    if isinstance(index.type, (SliceT, ScalarT)):
      assert arr_t.rank > 0, "Unexpected zero-rank array in %s" % expr 
      extra_indices = [slice_none] * (arr_t.rank - 1) 
      index_elts = [index] + extra_indices
    elif isinstance(index.type, TupleT):
      n_given = len(index.type.elt_types)
      n_expected = arr_t.rank  
      assert n_given <= n_expected, "Too many indices in %s" % expr 
      
      # replace None in the indexing with Slice(None,None,None)
      given_elts = [given if given.type != NoneType else slice_none 
                    for given in self.tuple_elts(index)]
      index_elts = given_elts + [slice_none] * (n_expected - n_given)
      
    index_elts = list(index_elts)
    index_ranges = [self.get(idx_elt) for idx_elt in index_elts]
    
    assert len(index_ranges) == len(index_elts)
    shape = self.shape(expr.value, temp = False)
    shape_elts = self.tuple_elts(shape)
  
    assert len(shape_elts) == len(index_elts), \
      "Mismatch between number of indices %d and rank %d in %s"  % \
      (len(index_elts), len(shape_elts), expr)
    for i, (index_range, index_elt) in enumerate(zip(index_ranges, index_elts)):
      if isinstance(index_range, Interval):
        if index_range.upper == index_range.lower:
          index_elt = self.int(index_range.upper, "const_idx")
          
        if self.always_negative(index_range, inclusive=False):
          index_elts[i] = self.add(shape_elts[i], index_elt)
         
      elif isinstance(index_range, SliceOfIntervals) and self.has_negative(index_range):
        
        if isinstance(index_range.start, NoneValue):
          start = none
        elif index_range.start.upper == index_range.start.lower:
          start = self.int(index_range.start.lower)
        else:
          start = self.attr(index_elt, 'start', temp = False)
        
        if isinstance(index_range.stop, NoneValue):
          stop = none 
        elif index_range.stop.upper == index_range.stop.lower:
          stop = self.int(index_range.stop.lower)
        else:
          stop = self.attr(index_elt, 'stop', temp = False)
        
        if isinstance(index_range.step, NoneValue):
          step = none 
        elif index_range.step.lower == index_range.step.upper:
          step = self.int(index_range.step.lower)
        else: 
          step = self.attr(index_elt, 'step', temp = False)
          
        shape_elt = shape_elts[i]
        
        if self.always_negative(index_range.start, inclusive=False):
          start = self.add(shape_elt, start, "start")
        
        if self.always_negative(index_range.stop, inclusive=False):
          stop = self.add(shape_elt, stop, "stop")
          
        if self.always_negative(index_range.step):
          if self.is_none(start):
            start = self.sub(shape_elt, one_i64, "last_pos")
          
          #if self.is_none(stop):
          # if it's a negative step then (stupidly) there's
          # no valid stop value
          if self.is_none(stop):
            # from this point forward, negative stop just means include the element 0
            stop = self.int(-1)
            #assert False, \
            #  "Not yet supported: Python's peculiar behavior with negative slice steps and None stop"
          
            
        else:
          if self.is_none(step):
            step = one_i64 
            
          if self.is_none(start):
            start = zero_i64
            
          if self.is_none(stop):
            stop = shape_elt
        
        
        slice_t = make_slice_type(start.type, stop.type, step.type)
        index_elts[i] = Slice(start=start, stop=stop, step = step, type = slice_t)
      if len(index_elts) > 1:
        index = self.tuple(index_elts)
      else:
        index = index_elts[0]
      expr.index = index  

    return expr 
          
    
    
    
        
########NEW FILE########
__FILENAME__ = offset_propagation
from .. import prims 
from .. analysis.offset_analysis import OffsetAnalysis
from .. syntax import Var, Const 
from .. syntax.helpers import true, false, const
from transform import Transform 


class OffsetPropagation(Transform):
  def pre_apply(self, fn):
    # map variables to pairs which include low/exclude high  
    self.known_ranges = {}
    self.known_offsets = OffsetAnalysis().visit_fn(fn)
    self.seen_binding = set(fn.arg_names)

  def transform_Assign(self, stmt):
    result = Transform.transform_Assign(self, stmt )
    if result.lhs.__class__ is Var:
      self.seen_binding.add(result.lhs.name)
    return result 
  
  def transform_ForLoop(self, stmt):
    self.known_ranges[stmt.var.name] = (stmt.start, stmt.stop)
    stmt.body = self.transform_block(stmt.body)
    return stmt 
  
  def compare_lt(self, x, y, inclusive = True):
    if x.__class__ is Var:
      offsets = [(x.name, 0)]
      if x.name in self.known_offsets:
        offsets = self.known_offsets[x.name].union(set(offsets))
      for (var_name, offset) in offsets:
        if var_name in self.known_ranges:
          (low,high) = self.known_ranges[var_name]
          if y == high:
            if (offset >= 0 and inclusive) or (offset > 0):
              return false
          elif y == low:
            if (offset <= 0 and inclusive) or (offset < 0):
              return true
    return None 
  
  def const_additive_cancellation(self, x_name, y_value, output_type):
    """
    Return a single variable in situations like 
      x = prev_var + const 
      y = x + (- const) 
    """ 
    if x_name in self.known_offsets:
      for (prev_var_name, offset) in self.known_offsets[x_name]:
        if prev_var_name in self.seen_binding:
          if offset == -y_value:
            prev_type = self.fn.type_env[prev_var_name]
            prev_var = Var(name=prev_var_name, type = prev_type)
            return self.cast(prev_var, output_type)
  
  def var_subtractive_cancellation(self, x_name, y_name, output_type):
    """
    Return a single constant in cases like:
      a = b + const 
      c = b - a
    """
    if x_name in self.known_offsets:
      for (prev_var_name, offset) in self.known_offsets[x_name]:
        if prev_var_name == y_name:
          return self.cast(const(offset), output_type)
        
  
  def transform_PrimCall(self, expr):
    p = expr.prim
    
    result = None 
    if isinstance(p, prims.Cmp):
      x,y = expr.args   
      if p == prims.less_equal:
        result = self.compare_lt(x,y, inclusive=True)
      elif p == prims.less:
        result = self.compare_lt(x,y, inclusive=False)
      elif p == prims.greater_equal:
        result = self.compare_lt(y, x, inclusive=True)
      elif p == prims.greater:
        result = self.compare_lt(y, x, inclusive=False)
      elif p == prims.equal:
        result1 = self.compare_lt(x, y, inclusive=True)
        result2 = self.compare_lt(y, x, inclusive=True)
        if result1 is not None and result2 is not None and result1 == result2:
          result = result1 
          
          
    # THIS IS MUCH MORE LIMITED THAN THE SORT OF SYMBOLIC REWRITES
    # WE COULD DO IN GENERAL, i.e. y = x + 2, z = x - 2, (y-z) ==> 4
    elif p == prims.add:
      x, y = expr.args
      result = None  
      if x.__class__ is Var and y.__class__ is Const:
        result = self.const_additive_cancellation(x.name, y.value, expr.type)
      elif x.__class__ is Const and y.__class__ is Var:
        result = self.const_additive_cancellation(y.name, x.value, expr.type)
        
      if result is not None: return result
         
    elif p == prims.subtract:

      x, y = expr.args 
      result = None 
      if x.__class__ is Var and y.__class__ is Const:
        result = self.const_additive_cancellation(x.name, -y.value, expr.type)
      elif x.__class__ is Var and y.__class__ is Var:
        result = self.var_subtractive_cancellation(x.name, y.name, expr.type)
      if result is not None: return result
    if result is not None:
      return result
    else:
      return expr
      

########NEW FILE########
__FILENAME__ = parallelize_loops
from itertools import izip 
from ..syntax import If, PrimCall, Var, Assign, AllocArray
from transform import Transform

class ExtractParallelLoops(Transform):
  
  def __init__(self):
    Transform.__init__(self)
    
  def extract_map(self, alloc_stmt, loop_stmt):
    pass 
  
  def extract_parfor(self, stmt):
    pass 
  
  def extract_reduce(self, stmt):
    last_stmt = stmt.body[-1]
    # Check if the accumulator variables 
    # get conditionally updated based on 
    # a comparison with some acc vars  
    if last_stmt.__class__ is not If:
      return stmt
    
    cond = stmt.cond
    if cond.__class__ is Var:
      pass 

    other_stmts = stmt.body[:-1]
    
  def transform_block(self, stmts):
    self.last_stmt = None 
    return Transform.transform_block(stmts)
  
  def transform_stmt(self, stmt):
    result = Transform.transform_stmt(stmt)
    self.last_stmt = result 
    return result
   
  def transform_ForLoop(self, stmt):
    #
    # Collection of heuristic patterns
    # of loops which can be lifted into 
    # higher order array operators  
    #
    
    if len(stmt.body) == 0: 
      # why is an empty loop even still here? 
      return stmt 
    
    phi_nodes = stmt.merge
    
    # if there aren't any values being 
    # accumulated between loop iterations
    # then this can't be a Reduction 
    if len(phi_nodes) == 0:
      # currently only the simplest case gets lifted into a map
      # if we allocate the result array immediately before using it
      # in a simple loop 
      if self.last_stmt and \
         self.last_stmt.__class__ is Assign and \
         self.last_stmt.rhs.__class__ is AllocArray:
        map_expr = self.extract_map(self.last_stmt, stmt)
        if map_expr is not None:
          new_stmt = Assign(self.last_stmt.lhs, map_expr)
          # can't delete the last stmt since it's already in the AST 
          # so just make it a null assignment () = ()
          self.last_stmt.lhs = self.tuple([])
          self.last_stmt.rhs = self.tuple([])
          return new_stmt 
        
      # if nothing is being accumulated then try to infer a simple map
      parfor = self.extract_parfor(stmt)
      if parfor is None: return stmt 
      else: return parfor 
    else:
      reduce_expr = self.extract_reduce(stmt)
      if reduce_expr is None: 
        return stmt 
      else:
        phi_var_names = phi_nodes.keys()
        phi_var_types = [left_val.type for (left_val, _) in phi_nodes.itervalues()]
        phi_vars = [Var(name = name, type = t) 
                    for name, t in 
                    izip(phi_var_names, phi_var_types)]
        lhs_tuple = self.tuple(phi_vars)
        return self.assign(lhs_tuple, reduce_expr)    
    

########NEW FILE########
__FILENAME__ = parfor_to_nested_loops
from transform import Transform

class ParForToNestedLoops(Transform):
  def transform_ParFor(self, stmt):
    fn = self.transform_expr(stmt.fn)
    self.nested_loops(stmt.bounds, fn)
    
    
########NEW FILE########
__FILENAME__ = permute_reductions
from ..builder import build_fn
from ..ndtypes import ArrayT, lower_rank
from ..prims import Prim  
from ..syntax import Return, Map 
from ..syntax.helpers import is_identity_fn, unwrap_constant
from transform import Transform

def get_nested_map(fn):
  if len(fn.body) != 1:
    return None 
  stmt = fn.body[0]
  if stmt.__class__ is not Return:
    return None 
  if stmt.value.__class__ is not Map:
    return None 
  return stmt.value 
     

class PermuteReductions(Transform):
  """
  When we have a reduction of array values, such as:
     Reduce(combine = Map(f), X, axis = 0) 
  it can be more efficient to interchange the Map and Reduce:
     Map(combine = f, X, axis = 1)
  """
  
  
  def transform_Reduce(self, expr):
    return expr 
  
  def transform_Scan(self, expr):
    if len(expr.args) > 1:
      return expr 
    
    axis = unwrap_constant(expr.axis)
    if axis is None or not isinstance(axis, (int,long)) or axis > 1 or axis < 0:
      return expr
     
    if not isinstance(expr.type, ArrayT) or expr.type.rank != 2:
      return expr
    
    fn = self.get_fn(expr.fn)
    fn_closure_args = self.closure_elts(expr.fn)
    if len(fn_closure_args) > 0:
      return expr 
    
    combine = self.get_fn(expr.combine)
    combine_closure_args = self.closure_elts(expr.closure)
    if len(combine_closure_args) > 0:
      return expr 
    
    if is_identity_fn(fn):
      nested_map = get_nested_map(combine)
    
    if not isinstance(nested_map.fn, Prim):
      return expr  
    
    arg_t = expr.args[0].type
    elt_t = lower_rank(arg_t, 1)
    new_nested_fn = None 
    return Map(fn = new_nested_fn,  
               args = expr.args, 
               axis = 1 - axis, 
               type = expr.type)
      
       
  
    
########NEW FILE########
__FILENAME__ = phase
from itertools import izip 

from .. import config

from .. syntax import TypedFn
from clone_function import CloneFunction
from recursive_apply import RecursiveApply
from transform import Transform 

name_stack = []
def apply_transforms(fn, transforms, 
                       cleanup = [], 
                       phase_name = None, 
                       transform_history = None):
  if len(transforms) == 0:
    return fn 
  if phase_name: name_stack.append("{" + phase_name + " :: " + fn.name +  "}")
  for T in transforms:
    t = T() if type(T) == type else T
    
    if isinstance(t, Transform):
      name_stack.append(str(t))
      if config.print_transform_names:
        print "-- %s" % ("->".join(name_stack),)
      
    elif isinstance(t, Phase) and t.should_skip(fn) and not t.depends_on:
      continue 

    fn = t.apply(fn)

    assert fn is not None, "%s transformed fn into None" % T

    if isinstance(t, Transform): name_stack.pop()

    if len(cleanup) > 0:
      fn = apply_transforms(fn, cleanup, [], phase_name = "cleanup")
    
    if transform_history is not None:
      transform_history.add(T)
  
  if phase_name: name_stack.pop()
  return fn

name_stack = []
class Phase(object):
  def __init__(self,
               transforms,
               depends_on = [],
               copy = False,
               cleanup = [],
               config_param = None,
               run_if = None,
               rename = False,
               post_apply = None,
               memoize = True,
               name = None, 
               recursive = True):
    self.cache = {}
    if not isinstance(transforms, (tuple, list)):
      transforms = [transforms]
    self.transforms = transforms

    if not depends_on:
      depends_on = []
    if not isinstance(depends_on, (tuple, list)):
      depends_on = [depends_on]
    self.depends_on = depends_on

    self.copy = copy

    if not cleanup:
      cleanup = []
    if not isinstance(cleanup, (list,tuple)):
      cleanup = [cleanup]
    self.cleanup = cleanup

    self.config_param = config_param
    self.run_if = run_if
    self.rename = rename
    self.post_apply = post_apply
    self.memoize = memoize
    self.name = name
    self.recursive = recursive 
    self._hash = hash(str(self))

  def __str__(self):
    if self.name:
      return self.name
    else:
      names = []
      for t in self.transforms:
        if type(t) == type:
          names.append(t.__name__)
        else:
          names.append(str(t))
      if len(names) == 1:
        return names[0]
      else:
        return "Phase(%s)" % (",".join(names))

  def __repr__(self):
    return str(self)

  def __eq__(self, other):
    if self is other:
      return True
    
    else:
      return (self.__class__ is other.__class__) and\
             (self.name == other.name) and \
             (self.copy == other.copy) and \
             (self.memoize == other.memoize) and \
             (len(self.transforms) == len(other.transforms)) and \
             (all(t1 == t2 for t1, t2 in izip(self.transforms, other.transforms))) 

  def __hash__(self):
    return self._hash 
  
  def __call__(self, fn, run_dependencies = True):
    return self.apply(fn, run_dependencies)

  def should_skip(self, fn):
    if self.config_param is not None and getattr(config, self.config_param) == False:
      return True
   
    if self.memoize and  self in fn.transform_history:
      return True
    
    if self.run_if:
      return not self.run_if(fn)
    
    return False 
    
  def is_cached(self, fn):
    return fn.cache_key in self.cache 
  
  def needs_cleanup(self, fn):
    return not (self.should_skip(fn) or self.is_cached(fn)) 
    
  def apply(self, fn, run_dependencies = True):

    if self.memoize:
      
      original_key = fn.cache_key
      if fn.created_by is self:
        return fn
      elif self in fn.transform_history:
        return fn   
      elif original_key in self.cache:
        return self.cache[original_key] 
    
    if self.depends_on and run_dependencies:
      fn = apply_transforms(fn, self.depends_on)
    
    if self.copy:
      fn = CloneFunction(parent_transform = self, rename = self.rename).apply(fn)
      if fn.cache_key  in self.cache:
        print "Warning: Typed function %s (key = %s) already registered, encountered while cloning before %s" % \
        (fn.name, fn.cache_key, self)
    
    if self.recursive:
      fn = RecursiveApply(self).apply(fn)
      
    if not self.should_skip(fn):
      fn = apply_transforms(fn, self.transforms, 
                           cleanup = self.cleanup, 
                           phase_name = str(self), 
                           transform_history = fn.transform_history)
    
      if self.post_apply:
        new_fn = self.post_apply(fn)
        if new_fn.__class__ is TypedFn:
          fn = new_fn

    fn.transform_history.add(self)
      
    if self.memoize:
      self.cache[original_key] = fn
      self.cache[fn.cache_key] = fn
    return fn

########NEW FILE########
__FILENAME__ = pipeline
from .. import config 
from ..analysis import (contains_adverbs, contains_calls, contains_loops, 
                        contains_structs, contains_array_operators)

from combine_nested_maps import CombineNestedMaps 
from copy_elimination import CopyElimination
from dead_code_elim import DCE

from flattening import Flatten
from fusion import Fusion
from imap_elim import IndexMapElimination
from index_elimination import IndexElim
from indexify_adverbs import IndexifyAdverbs

from inline import Inliner
from licm import LoopInvariantCodeMotion
from loop_unrolling import LoopUnrolling
from lower_adverbs import LowerAdverbs
from lower_array_operators import LowerArrayOperators
from lower_indexing import LowerIndexing
from lower_slices import LowerSlices
from lower_structs import LowerStructs
from negative_index_elim import NegativeIndexElim
from offset_propagation import OffsetPropagation
from parfor_to_nested_loops import ParForToNestedLoops
from phase import Phase
from range_propagation import RangePropagation
from redundant_load_elim import RedundantLoadElimination
from scalar_replacement import ScalarReplacement
from shape_elim import ShapeElimination
from simplify import Simplify
from simplify_array_operators import SimplifyArrayOperators
from specialize_fn_args import SpecializeFnArgs

####################################
#                                  #
#    HIGH LEVEL OPTIMIZATIONS      # 
#                                  #
####################################

normalize = Phase([Simplify], 
                  memoize = True, 
                  copy = False, 
                  cleanup = [], 
                  name = "Normalize")

fusion_opt = Phase(Fusion, 
                   config_param = 'opt_fusion',
                   memoize = False,
                   copy = False, 
                   run_if = contains_adverbs)

inline_opt = Phase(Inliner, 
                   config_param = 'opt_inline', 
                   cleanup = [], 
                   run_if = contains_calls, 
                   memoize = False, 
                   copy = False)



licm = Phase(LoopInvariantCodeMotion, 
             config_param = 'opt_licm',
             run_if = contains_loops, 
             memoize = False, 
             name = "LICM")


symbolic_range_propagation = Phase([RangePropagation, OffsetPropagation], 
                                    config_param = 'opt_range_propagation',
                                    name = "SymRangeProp",
                                    copy = False,  
                                    memoize = False, 
                                    cleanup = [])
 
shape_elim = Phase(ShapeElimination,
                   config_param = 'opt_shape_elim')





early_optimizations = Phase([
                               inline_opt,
                               symbolic_range_propagation,   
                               licm,
                             ],
                            name = "EarlyOpt",
                            copy = True, 
                            memoize = True, 
                            cleanup = [Simplify, DCE], 
                            depends_on = normalize, 
 
                            )

simplify_array_operators = Phase([SimplifyArrayOperators], 
                                 copy = False, 
                                 memoize = False, 
                                 run_if = contains_array_operators, 
                                 config_param = "opt_simplify_array_operators")

combine_nested_maps = Phase([CombineNestedMaps],
                            copy = False, 
                            memoize = False, 
                            run_if = contains_adverbs, 
                            config_param = "opt_combine_nested_maps")

arg_specialization = Phase([SpecializeFnArgs],
                           copy = False, 
                           memoize = False, 
                           config_param = "opt_specialize_fn_args")

adverb_optimizations = Phase([
                                fusion_opt,
                                simplify_array_operators, 
                                combine_nested_maps,
                                arg_specialization,
                                fusion_opt, 
                                arg_specialization,
                              ], 
                             run_if = contains_adverbs, 
                             depends_on = early_optimizations, 
                             copy = True, 
                             memoize = True, 
                             cleanup = [Simplify, DCE])

high_level_optimizations = Phase([ 
                                    LowerArrayOperators,
                                     
                                    NegativeIndexElim,
                                    shape_elim, 
                                    symbolic_range_propagation, 
                                    
                                 ], 
                                 depends_on = adverb_optimizations,
                                 name = "HighLevelOpts", 
                                 copy = True, 
                                 memoize = True, 
                                 cleanup = [Simplify, DCE])



copy_elim = Phase(CopyElimination, 
                  config_param = 'opt_copy_elimination', 
                  memoize = False)


def print_indexified(fn):
  if config.print_indexified_function:
    print
    print "=== Indexified function ==="
    print
    print repr(fn)
    print


indexify = Phase([
                    IndexifyAdverbs, Simplify, DCE, 
                    
                 ],
                 name = "Indexify",  
                 run_if = contains_adverbs, 
                 depends_on= high_level_optimizations, 
                 copy = True,
                 memoize = True, 
                 post_apply = print_indexified) 

optimize_indexified_code = Phase([copy_elim, Simplify, DCE, 
                        LowerSlices, 
                        inline_opt, Simplify, DCE, 
                        IndexMapElimination], 
                       name = "AfterIndexify", 
                       depends_on = indexify, 
                       copy = True, 
                       memoize = True)


flatten = Phase([Flatten, inline_opt, Simplify, DCE ], name="Flatten", 
                depends_on=optimize_indexified_code,
                run_if = contains_structs,  
                copy=True, 
                memoize = True)

####################
#                  #
#     LOOPIFY      # 
#                  #
####################

def print_loopy(fn):
  if config.print_loopy_function:
    print
    print "=== Loopy function ==="
    print
    print repr(fn)
    print




# loop_fusion = Phase(LoopFusion, config_param = 'opt_loop_fusion')


index_elim = Phase([NegativeIndexElim, IndexElim], config_param = 'opt_index_elimination')



lower_adverbs = Phase([LowerAdverbs, LowerSlices], run_if = contains_adverbs)


# Currently incorrect in the presence of function calls
# TODO: Make this mark an slices that are call arguments as read & written to
load_elim = Phase(RedundantLoadElimination,
                  config_param = 'opt_redundant_load_elimination', 
                  run_if = lambda fn: not contains_calls(fn))



loopify = Phase([lower_adverbs,
                 ParForToNestedLoops,
                 inline_opt,
                 LowerSlices, 
                 licm, 
                 shape_elim, 
                 symbolic_range_propagation,
                 load_elim,  
                 index_elim, 
                ],
                depends_on = optimize_indexified_code,
                cleanup = [Simplify, DCE],
                copy = True,
                memoize = True, 
                name = "Loopify",
                post_apply = print_loopy)


####################
#                  #
#     LOWERING     # 
#                  #
####################



lowering = Phase([
                    LowerIndexing,
                    licm,
                 ],
                 name = "Lowering", 
                 cleanup = [Simplify, DCE])

lower_structs = Phase([LowerStructs], 
                      depends_on = lowering, 

                      name = "LowerStructs", 
                      cleanup = [Simplify, DCE])

############################
#                          #
#  FINAL LOOP OPTIMIZATINS #
#                          #
############################

scalar_repl = Phase(ScalarReplacement, 
                    config_param = 'opt_scalar_replacement', 
                    run_if = contains_loops)

unroll = Phase([LoopUnrolling, licm], 
               config_param = 'opt_loop_unrolling', 
               run_if = contains_loops)

final_optimizations = Phase([
                              licm, 
                              unroll, 
                              load_elim, 
                              scalar_repl, 
                              Simplify
                            ],
                           depends_on = lowering, 
                           cleanup = [Simplify, DCE], 
                           name = "FinalLoopOptimizations"
                           )

lower_to_loops = Phase([loopify, lowering, final_optimizations], 
                       name = "LowerToLoops", 
                       copy = True, 
                       recursive = True, 
                       memoize = True, 
                       depends_on = optimize_indexified_code, 
                       )
lower_to_adverbs = Phase([lowering, final_optimizations], 
                         name = "LowerToLoops", 
                         copy = True, 
                         recursive = True, 
                         memoize = True, 
                         depends_on = optimize_indexified_code,)
########NEW FILE########
__FILENAME__ = range_propagation


from .. import prims 
from ..syntax import Const 
from ..analysis.value_range_analysis import Interval  
from range_transform import RangeTransform 

class RangePropagation(RangeTransform):
  
  def visit_Var(self, expr):
    if expr.name in self.ranges:
      range_val = self.ranges[expr.name]
      if isinstance(range_val, Interval) and range_val.lower == range_val.upper:
        return Const(range_val.lower, type = expr.type)
    return expr 
  
  def visit_PrimCall(self, expr):
    p = expr.prim 
    result = None 
    if isinstance(p, prims.Cmp):
      argx, argy = expr.args
      x, y = self.get(argx), self.get(argy)
      if x is not None and y is not None:        
        if p == prims.equal:
          result = self.cmp_eq(x,y)
        elif p == prims.less:
          result = self.cmp_lt(x,y)
        elif p == prims.less_equal:
          result = self.cmp_lte(x, y)
        elif p == prims.greater:
          result = self.cmp_lt(y, x)
        elif p == prims.greater_equal:
          result = self.cmp_lte(y, x)
     
          
    if result is None:
      return expr 
    else:
      return result 

  
########NEW FILE########
__FILENAME__ = range_transform
from ..analysis.value_range_analysis import (ValueRangeAnalyis, Interval, NoneValue)
from ..syntax.helpers import true, false 
from transform import Transform

class RangeTransform(Transform, ValueRangeAnalyis):
  """
  Base class for transforms which use value ranges gathered from analysis
  """
  
  def pre_apply(self, old_fn):
    ValueRangeAnalyis.__init__(self)
    self.visit_fn(old_fn)
    
  def cmp_eq(self, x, y):
    if not isinstance(x, Interval) or not isinstance(y, Interval):
      return None 
    if x.lower == y.lower and x.upper == y.upper:
      return true 
    if x.upper < y.lower or y.upper < x.lower:
      return false
    return None   
  
  def cmp_lt(self, x, y):
    if not isinstance(x, Interval) or not isinstance(y, Interval):
      return None 
    if x.upper < y.lower:
      return true 
    elif x.lower > y.upper:
      return false
    else:
      return None 
  
  def cmp_lte(self, x, y):
    if not isinstance(x, Interval) or not isinstance(y, Interval):
      return None 
    if x.upper < y.lower:
      return true 
    elif x.lower > y.upper:
      return false
    else:
      return None 
    
########NEW FILE########
__FILENAME__ = recursive_apply

from dsltools import ScopedDict

from .. import config 
from ..analysis.verify import verify
from ..ndtypes import (NoneT, SliceT, ScalarT, ArrayT, ClosureT, TupleT, PtrT, 
                       StructT, make_closure_type) 
from ..syntax import TypedFn
from transform import Transform


class RecursiveApply(Transform):  
  cache = ScopedDict()
  
  def __init__(self, transform_to_apply = None):
    Transform.__init__(self)
    self.transform = transform_to_apply
    self.cache.push()
  
  def __del__(self):
    self.cache.pop()
  
  
  first_order_types = (SliceT, NoneT, ArrayT, PtrT)
  
  def contains_function_type(self, t):
    c = t.__class__ 
    if c is ClosureT: 
      return True 
    elif c is TupleT: 
      return any(self.contains_function_type(elt_t) for elt_t in t.elt_types)
    elif c in self.first_order_types or isinstance(t, ScalarT):
      return False 
    else:
      assert isinstance(t, StructT), "Unexpected type %s" % t
      return any(self.contains_function_type(field_t) for field_t in t.field_types)
    
  def transform_type(self, t):
    c = t.__class__ 
    if c is ClosureT:
      old_fn = t.fn 
      if isinstance(old_fn, TypedFn):
        new_fn = self.transform_TypedFn(old_fn)
        if new_fn is not old_fn:
          return make_closure_type(new_fn, t.arg_types)
    elif c is TupleT and self.contains_function_type(t):
      new_elt_types = []
      for elt_t in t.elt_types:
        new_elt_types.append(self.transform_type(elt_t))
      return TupleT(tuple(new_elt_types))
    # if it's neither a closure nor a structure which could contain closures, 
    # just return it 
    return t 
    
      
  def transform_TypedFn(self, expr):
    key = expr.cache_key
    if key in self.cache:
      return self.cache[key]  
    new_fn = self.transform.apply(expr)
    if config.opt_verify:
      try: 
        verify(new_fn)
      except:
        print "[RecursiveApply] Error after applying %s to function %s" % (self.transform, expr)
        raise 
    self.cache[key] = new_fn 
    return new_fn 
  
  def transform_Closure(self, expr):
    args = self.transform_expr_list(expr.args)
    new_fn = self.transform_expr(expr.fn)
    return self.closure(new_fn, args)
  
  def transform_Var(self, expr):
    expr.type = self.transform_type(expr.type)
    return expr
  
  def transform_Assign(self, stmt):
    """
    If we have an assignment like 
      a : (Fn1T, Fn2T) = (fn1, fn2)
    we might need to change it to 
      a : (Fn1T', Fn2T') = (fn1', fn2') if the RHS functions get updated
    """ 
    stmt.rhs = self.transform_expr(stmt.rhs)
    stmt.lhs.type = self.transform_type(stmt.lhs.type)
    return stmt 
  
  def pre_apply(self, fn):
    if self.transform is None:
      self.transform = fn.created_by 
    
    assert self.transform is not None, "No transform specified for RecursiveApply"
    fn.input_types = tuple(self.transform_type(t) for t in fn.input_types)
    fn.return_type = self.transform_type(fn.return_type)
    for k,t in fn.type_env.items():
      fn.type_env[k] = self.transform_type(t)
    return fn   
########NEW FILE########
__FILENAME__ = redundant_load_elim
from .. syntax import Assign, Index, Var

from loop_transform import LoopTransform

class RedundantLoadElimination(LoopTransform):
  def transform_block(self, stmts):
    stmts = LoopTransform.transform_block(self, stmts) 
    if self.is_simple_block(stmts, allow_branches = False):
      reads, writes = self.collect_memory_accesses(stmts)
      safe_arrays = set([])
      for name in reads:
        # if any alias of this array gets written to, consider it unsafe 
        aliases = self.may_alias.get(name, set([]))
        aliases.add(name)
        unsafe = any(alias in writes for alias in aliases) 
        if not unsafe:
          safe_arrays.add(name)
      available_expressions = {}
      new_stmts = []
      for stmt in stmts:
        if stmt.__class__ is Assign:
          if stmt.rhs.__class__ is Index and \
             stmt.rhs.value.__class__ is Var and \
             stmt.rhs.value.name in safe_arrays:
            key = (stmt.rhs.value.name, stmt.rhs.index)
            if key in available_expressions:
              stmt.rhs = available_expressions[key]
            elif stmt.lhs.__class__ is Var:
              available_expressions[key] = stmt.lhs
            else:
              temp = self.fresh_var(stmt.rhs.type,  "load")
              new_stmts.append(Assign(temp, stmt.rhs))
              stmt.rhs = temp
              available_expressions[key] = temp
          new_stmts.append(stmt)
      return new_stmts
    else:
      return stmts
        
        
########NEW FILE########
__FILENAME__ = scalar_replacement
from .. syntax import Assign, Index, Var

from loop_transform import LoopTransform 


class ScalarReplacement(LoopTransform):
  """
  When a loop reads and writes to a non-varying memory location, 
  we can keep the value of that location in a register and write 
  it to the heap after the loop completes. 
  
  Transform code like this:
      for i in low .. high: 
        z = x[const]
        q = z ** 2
        x[const] = q + i 
  into 
      z_in = x[const]
      for i in low .. high:
        (header)
          z_loop = phi(z_in, z_out)
        (body) 
          q = z_loop ** 2
          z_out = q + i 
      x[const] = z_loop   
  """
  
  
  
  def preload_reads(self, reads):
    scalar_vars = {}
    for (array_name, index_set) in reads.iteritems():
      t = self.type_env[array_name]
      for index_expr in index_set:
        scalar = self.index(Var(array_name, type = t), index_expr, 
                            temp = True, name = "scalar_repl_input")
       
        scalar_vars[(array_name, index_expr)] = scalar
    return scalar_vars

  def replace_indexing(self, loop_body, loop_scalars):
    """
    Given a map from (array_name, index_expr) pairs to 
    scalar variables, replace reads/writes with scalars
    """
    final_scalars = loop_scalars.copy()
    for stmt in loop_body:
      if stmt.__class__ is not Assign: 
        continue 
      if stmt.rhs.__class__ is Index:
        key = stmt.rhs.value.name, stmt.rhs.index
        if key in final_scalars:
          stmt.rhs = final_scalars[key]  
      if stmt.lhs.__class__ is Index:
        key = stmt.lhs.value.name, stmt.lhs.index
        if key in final_scalars:
          new_var = self.fresh_var(stmt.lhs.type, "scalar_repl_out")
          stmt.lhs = new_var
          final_scalars[key] = new_var  
    return final_scalars
  
  
  def transform_ForLoop(self, stmt):
    
    if self.is_simple_block(stmt.body):
      loop_vars = set([stmt.var.name])
      loop_vars.update(stmt.merge.keys())
      # gather all the variables whose values change between loop iters
      self.collect_loop_vars(loop_vars, stmt.body)
      # full set of array reads & writes 
      reads, writes = self.collect_memory_accesses(stmt.body)
      
      # which arrays are written to at loop-dependent locations? 
      unsafe = set([])
      for (name, write_indices) in writes.iteritems():
        if self.any_loop_vars(loop_vars, write_indices):
          unsafe.add(name)
      safe_writes = dict([(k,v) for (k,v) in writes.items() 
                          if k not in unsafe])
      
      safe_reads = dict([(k,v) for (k,v) in reads.items() 
                         if k not in unsafe and not self.any_loop_vars(loop_vars, v)])

      
      safe_locations = {}
      all_keys = set(safe_writes.keys()).union(set(safe_reads.keys()))
      for name in all_keys:
        index_set = safe_reads.get(name, set([])).union(safe_writes.get(name, set([])))
        safe_locations[name] = index_set 
      # move all safe/loop-invariant reads into registers at the top of the loop
      # I'm also including the writes (by passing safe_locations instead of safe_reads)
      # so that they become loop-carried accumulator values, otherwise where else would
      # they get their initial values?
      input_scalars = self.preload_reads(safe_locations)

      
      # need to rename the scalars so they have an SSA variable for the beginning of the loop
      loop_scalars = {}
      for ( (name,index_expr), input_var) in input_scalars.iteritems():
        loop_var = self.fresh_var(input_var.type, "scalar_repl_acc")
        loop_scalars[(name, index_expr)] = loop_var
      
      
      # propagate register names for all writes 
      final_scalars = self.replace_indexing(stmt.body, loop_scalars)
      
      for (key, final_var) in final_scalars.iteritems():
        loop_var = loop_scalars[key]
        input_var = input_scalars[key]
        stmt.merge[loop_var.name] = (input_var, final_var)
      
      self.blocks.append(stmt)
      # write out the results back to memeory 
      for ( (array_name, index_expr), loop_var) in loop_scalars.iteritems():
        array_type = self.type_env[array_name]
        lhs = self.index(Var(array_name, type = array_type), index_expr, temp = False)
        self.assign(lhs, loop_var) 
      return None
    else:     
      stmt.body = self.transform_block(stmt.body)
      return stmt 
########NEW FILE########
__FILENAME__ = shape_elim
from .. import syntax
from ..ndtypes import ArrayT, SliceT, ScalarT, TupleT, Int64, StructT, ClosureT, IntT, NoneT 
from ..syntax import Attribute, TupleProj, Var, ClosureElt
from ..shape_inference import shape_env, shape
from ..transforms import Transform


class Counter(object):
  """
  Auto-incrementing counter
  """
  def __init__(self):
    self.n = 0
    
  def get(self):
    n = self.n 
    self.n += 1
    return n 
  
class ShapeElimination(Transform):
  
   
  def pre_apply(self, fn):
    self.shape_env = shape_env(fn)
    # map from shape vars to expressions 
    self.shape_vars = {}
    self.shape_var_counter = Counter()
    input_vars = [Var(arg_name, type = arg_type)
                  for arg_name, arg_type in zip(fn.arg_names, fn.input_types)]
    self.fill_shape_vars_list(input_vars)
    
    
      
  def fill_shape_vars(self, expr):
    t = expr.type 
    c = t.__class__ 
    if c is TupleT:
      elts = [TupleProj(expr,i,type=elt_t) 
              for i, elt_t in enumerate(t.elt_types)]
      self.fill_shape_vars_list(elts)
    elif c is ClosureT:
      elts = [ClosureElt(expr,i,type=elt_t) 
              for i, elt_t in enumerate(t.arg_types)]
      self.fill_shape_vars_list(elts)
    elif c is ArrayT:
      shape = Attribute(expr, 'shape', t.shape_t)
      dims = [TupleProj(shape,i,type=Int64) for i in xrange(t.rank)]
      self.fill_shape_vars_list(dims)
    elif c is StructT:
      fields = [Attribute(expr, field_name, type = field_type)
                for field_name, field_type in zip(t.field_names, t.field_types)]
      self.fill_shape_vars_list(fields)
    elif c is SliceT:
      fields = []
      start_t = expr.type.start_type 
      if start_t != NoneT: fields.append(Attribute(expr, 'start', type = start_t))
      stop_t = expr.type.stop_type 
      if stop_t != NoneT: fields.append(Attribute(expr, 'stop', type = stop_t))
      step_t = expr.type.step_type 
      if step_t != NoneT: fields.append(Attribute(expr, 'step', type = step_t))
      self.fill_shape_vars_list(fields)
    elif isinstance(t, ScalarT):
      self.shape_vars[self.shape_var_counter.get()] = expr 
      
      
  def fill_shape_vars_list(self, arg_exprs):
    for e in arg_exprs:
      self.fill_shape_vars(e)

  def transform_Var(self, expr):
    #print expr, self.shape_env.get(expr.name) 
    if expr.name in self.shape_env:
      v = self.shape_env[expr.name]
      # don't trust shape propagation to correctly handle floating point errors
      if v.__class__ is shape.Const and isinstance(expr.type, IntT):
        return syntax.Const(value = v.value, type = expr.type)
      elif v.__class__ is shape.Var: 
        new_expr = self.shape_vars[v.num]
        
        if new_expr != expr:
          return self.cast(self.shape_vars[v.num], expr.type)
    return expr
  
  def transform_lhs(self, lhs):
    return lhs
########NEW FILE########
__FILENAME__ = simplify

from dsltools import ScopedDict

from .. import prims, syntax 
from .. analysis.collect_vars import collect_var_names
from .. analysis.mutability_analysis import TypeBasedMutabilityAnalysis
from .. analysis.use_analysis import use_count
from .. ndtypes import (ArrayT,  ClosureT, NoneT, ScalarT, TupleT, ImmutableT, NoneType, 
                        SliceT, FnT, FloatT)

from .. syntax import (AllocArray, Assign, ExprStmt, Expr, 
                       Const, Var, Tuple, TupleProj, Closure, ClosureElt, Cast,
                       Slice, Index, Array, ArrayView, Attribute, Struct, Select, 
                       PrimCall, Call, TypedFn, UntypedFn, 
                       OuterMap, Map, Reduce, Scan, IndexMap, IndexReduce, 
                       IndexScan, FilterReduce)
from .. syntax.helpers import (collect_constants, is_one, is_zero, is_false, is_true, all_constants,
                               get_types, 
                               slice_none_t, const_int, one, none, true, false, slice_none, 
                               zero_i64, one_i64) 
import subst
import transform 
from transform import Transform


# classes of expressions known to have no side effects
# and to be unaffected by changes in mutable state as long
# as all their arguments are SSA variables or constants
#
# examples of unsafe expressions:
#  - Index: the underlying array is mutable, thus the expression depends on
#    any data modifications
#  - Call: Unless the function is known to contain only safe expressions it
#    might depend on mutable state or modify it itself

class Simplify(Transform):
  def __init__(self):
    transform.Transform.__init__(self)
    # associate var names with any immutable values
    # they are bound to
    self.bindings = ScopedDict()

    # which expressions have already been computed
    # and stored in some variable?
    self.available_expressions = ScopedDict()

  def pre_apply(self, fn):
    ma = TypeBasedMutabilityAnalysis()

    # which types have elements that might
    # change between two accesses?
    self.mutable_types = ma.visit_fn(fn)
    self.use_counts = use_count(fn)
    
  def immutable_type(self, t):
    return t not in self.mutable_types


  _immutable_classes = set([Const,  Var, 
                            Closure, ClosureElt, 
                            Tuple, TupleProj, 
                            Cast, PrimCall, 
                            TypedFn, UntypedFn, 
                            ArrayView, 
                            Slice, 
                            Map, Reduce, Scan, OuterMap, 
                            IndexMap, IndexReduce, IndexScan, 
                            ])
  
  def immutable(self, expr):
    """
    TODO: make all this mutability/immutability stuff sane 
    """
    klass = expr.__class__ 
    
    
    result = (klass in self._immutable_classes and 
                (all(self.immutable(c) for c in expr.children()))) or \
             (klass is Attribute and isinstance(expr.type, ImmutableT))
    return result 
    
                      
  def temp(self, expr, name = None, use_count = 1):
    """
    Wrapper around Codegen.assign_name which also updates bindings and
    use_counts
    """
    if self.is_simple(expr):
      return expr 
    else:
      new_var = self.assign_name(expr, name = name)
      self.bindings[new_var.name] = expr
      self.use_counts[new_var.name] = use_count
      return new_var

  def transform_expr(self, expr):
    if self.is_simple(expr):
      if expr.type == NoneType:
        return none 
      else:
        return Transform.transform_expr(self, expr)
    stored = self.available_expressions.get(expr)
    if stored is not None: 
      return stored
    return Transform.transform_expr(self, expr)

    
  def transform_Var(self, expr):
    t = expr.type 
    if t.__class__ is NoneT:
      return none 
    elif t.__class__ is SliceT and \
         t.start_type == NoneType and \
         t.stop_type == NoneType and \
         t.step_type == NoneType:
      return slice_none 
    
    name = expr.name
    prev_expr = expr

    while name in self.bindings:
      prev_expr = expr 
        
      expr = self.bindings[name]
      if expr.__class__ is Var:
        name = expr.name
      else:
        break
    c = expr.__class__ 
    
    if c is Var or c is Const:

      return expr
    else:

      return prev_expr
  
  def transform_Cast(self, expr):
    
    v = self.transform_expr(expr.value)
    if v.type == expr.type:
      return v
    elif v.__class__ is Const and isinstance(expr.type, ScalarT):
      return Const(expr.type.dtype.type(v.value), type = expr.type)
    elif self.is_simple(v):
      expr.value = v
      return expr
    else:
      expr.value = self.assign_name(v)
      return expr

  def transform_Attribute(self, expr):
    v = self.transform_expr(expr.value)
    
    if v.__class__ is Var and v.name in self.bindings:
      stored_v = self.bindings[v.name]
      c = stored_v.__class__
      if c is Var or c is Struct:
        v = stored_v
      elif c is ArrayView:
        if expr.name == 'shape':
          return self.transform_expr(stored_v.shape)
        elif expr.name == 'strides':
          return self.transform_expr(stored_v.strides)
        elif expr.name == 'data':
          return self.transform_expr(stored_v.data)
      elif c is AllocArray:
        if expr.name == 'shape':
          return self.transform_expr(stored_v.shape)
      elif c is Slice:
        if expr.name == "start":
          return self.transform_expr(stored_v.start)
        elif expr.name == "stop":
          return self.transform_expr(stored_v.stop)
        else:
          assert expr.name == "step", "Unexpected attribute for slice: %s" % expr.name  
          return self.transform_expr(stored_v.step)
    if v.__class__ is Struct:
      idx = v.type.field_pos(expr.name)
      return v.args[idx]
    elif v.__class__ is not Var:
      v = self.temp(v, "struct")
    if expr.value == v:
      return expr
    else:
      return Attribute(value = v, name = expr.name, type = expr.type)
  
  def transform_Closure(self, expr):
    expr.args = tuple(self.transform_simple_exprs(expr.args))
    return expr

  def transform_Tuple(self, expr):
    expr.elts = tuple( self.transform_simple_exprs(expr.elts))
    return expr

  def transform_TupleProj(self, expr):
    idx = expr.index
    assert isinstance(idx, int), \
        "TupleProj index must be an integer, got: " + str(idx)
    new_tuple = self.transform_expr(expr.tuple)

    if new_tuple.__class__ is Var and new_tuple.name in self.bindings:
      tuple_expr = self.bindings[new_tuple.name]
      
      if tuple_expr.__class__ is Tuple:
        assert idx < len(tuple_expr.elts), \
          "Too few elements in tuple %s : %s, elts = %s" % (expr, tuple_expr.type, tuple_expr.elts)
        return tuple_expr.elts[idx]
      elif tuple_expr.__class__ is Struct:
        assert idx < len(tuple_expr.args), \
          "Too few args in closure %s : %s, elts = %s" % (expr, tuple_expr.type, tuple_expr.elts) 
        return tuple_expr.args[idx]
    

    #if not self.is_simple(new_tuple):
    #  complex_expr = new_tuple 
    #  new_tuple = self.assign_name(complex_expr, "tuple")
    #  print "MADE ME A NEW TUPLE", complex_expr, new_tuple 
    expr.tuple = new_tuple
    return expr

  def transform_ClosureElt(self, expr):
    idx = expr.index
    assert isinstance(idx, int), \
        "ClosureElt index must be an integer, got: " + str(idx)
    new_closure = self.transform_expr(expr.closure)

    if new_closure.__class__ is Var and new_closure.name in self.bindings:
      closure_expr = self.bindings[new_closure.name]
      if closure_expr.__class__ is Closure:
        return closure_expr.args[idx]

    if not self.is_simple(new_closure):
      new_closure = self.assign_name(new_closure, "closure")
    expr.closure = new_closure
    return expr

  def transform_Call(self, expr):
    fn = self.transform_expr(expr.fn)
    args = self.transform_simple_exprs(expr.args)
    if fn.type.__class__ is ClosureT:
      closure_elts = self.closure_elts(fn)
      combined_args = tuple(closure_elts) + tuple(args)
      if fn.type.fn.__class__ is TypedFn:
        fn = fn.type.fn
      else:
        assert isinstance(fn.type.fn, UntypedFn)
        from .. type_inference import specialize 
        fn = specialize(fn, get_types(combined_args))
      assert fn.return_type == expr.type
      return Call(fn, combined_args, type = fn.return_type)
    else:
      expr.fn = fn
      expr.args = args
      return expr

  def transform_if_simple_expr(self, expr):
    if isinstance(expr, Expr):
      return self.transform_simple_expr(expr)
    else:
      return expr 
    
  def transform_simple_expr(self, expr, name = None):
    if name is None: name = "temp"
    result = self.transform_expr(expr)
    if not self.is_simple(result):
      return self.assign_name(result, name)
    else:
      return result
  
  def transform_simple_exprs(self, args):
    return [self.transform_simple_expr(x) for x in args]

  def transform_Array(self, expr):
    expr.elts = tuple(self.transform_simple_exprs(expr.elts))
    return expr
  
  def transform_Slice(self, expr):
    expr.start = self.transform_simple_expr(expr.start)
    expr.stop = self.transform_simple_expr(expr.stop)
    expr.step = self.transform_simple_expr(expr.step)
    return expr 

  
  def transform_index_expr(self, expr):
    if expr.__class__ is Tuple:
      new_elts = []
      for elt in expr.elts:
        new_elt = self.transform_expr(elt)
        if not self.is_simple(new_elt) and new_elt.type.__class__ is not SliceT:
          new_elt = self.temp(new_elt, "index_tuple_elt")
        new_elts.append(new_elt)
      expr.elts = tuple(new_elts)
      return expr 
    else:
      return self.transform_expr(expr) 
  
  def transform_Index(self, expr):
    expr.value = self.transform_expr(expr.value)

    expr.index = self.transform_index_expr(expr.index)

    if expr.value.__class__ is Array and expr.index.__class__ is Const:
      assert isinstance(expr.index.value, (int, long)) and \
             len(expr.value.elts) > expr.index.value
      return expr.value.elts[expr.index.value]
    
    # take expressions like "a[i][j]" and turn them into "a[i,j]" 
    if expr.value.__class__ is Index: 
      base_array = expr.value.value
      if isinstance(base_array.type, ArrayT):
        base_index = expr.value.index 
        if isinstance(base_index.type, TupleT):
          indices = self.tuple_elts(base_index)
        else:
          assert isinstance(base_index.type, ScalarT), \
            "Unexpected index type %s : %s in %s" % (base_index, base_index.type, expr)
          indices = [base_index]
        if isinstance(expr.index.type, TupleT):
          indices = tuple(indices) + tuple(self.tuple_elts(expr.index))
        else:
          assert isinstance(expr.index.type, ScalarT), \
            "Unexpected index type %s : %s in %s" % (expr.index, expr.index.type, expr)
          indices = tuple(indices) + (expr.index,)
        expr = self.index(base_array, self.tuple(indices))
        return self.transform_expr(expr)
    if expr.value.__class__ is not Var:
      expr.value = self.temp(expr.value, "array")
    return expr

  def transform_Struct(self, expr):
    new_args = self.transform_simple_exprs(expr.args)
    return syntax.Struct(new_args, type = expr.type)

  def transform_Select(self, expr):
    cond = self.transform_expr(expr.cond)
    trueval = self.transform_expr(expr.true_value)
    falseval = self.transform_expr(expr.false_value)
    if is_true(cond):
      return trueval 
    elif is_false(cond):
      return falseval
    elif trueval == falseval:
      return trueval 
    else:
      expr.cond = cond 
      expr.false_value = falseval 
      expr.true_value = trueval 
      return expr    
  
  def transform_PrimCall(self, expr):
    args = self.transform_simple_exprs(expr.args)
    prim = expr.prim
    if all_constants(args):
      return syntax.Const(value = prim.fn(*collect_constants(args)),
                          type = expr.type)
    
    if len(args) == 1:
      x = args[0]
      if prim == prims.logical_not:
        if is_false(x):
          return true 
        elif is_true(x):
          return false 
    if len(args) == 2:
      x,y = args 
      
      if prim == prims.add:
        if is_zero(x):
          return y
        elif is_zero(y):
          return x
        if y.__class__ is Const and y.value < 0:
          expr.prim = prims.subtract
          expr.args = (x, Const(value = -y.value, type = y.type))
          
          return expr 
        elif x.__class__ is Const and x.value < 0:
          expr.prim = prims.subtract
          expr.args = (y, Const(value = -x.value, type = x.type)) 
          return expr 
        
      elif prim == prims.subtract:
        if is_zero(y):
          return x
        elif is_zero(x) and y.__class__ is Var:
           
          stored = self.bindings.get(y.name)
          
          # 0 - (a * b) --> -a * b |or| a * -b 
          if stored and stored.__class__ is PrimCall and stored.prim == prims.multiply:
            
            a,b = stored.args
            if a.__class__ is Const:
              expr.prim = prims.multiply
              neg_a = Const(value = -a.value, type = a.type)
              expr.args = [neg_a, b]
              return expr 
            elif b.__class__ is Const:
              expr.prim = prims.multiply
              neg_b = Const(value = -b.value, type = b.type)
              expr.args = [a, neg_b]
              return expr 
            
      elif prim == prims.multiply:
      
        if is_one(x):
          return y
        elif is_one(y):
          return x
        elif is_zero(x):
          return x
        elif is_zero(y):
          return y
        
      elif prim == prims.divide and is_one(y):
        return x
      
      elif prim == prims.power:
      
        if is_one(y):
          return self.cast(x, expr.type)
        elif is_zero(y):
          return one(expr.type)
        elif y.__class__ is Const:
          if y.value == 2:
            return self.cast(self.mul(x, x, "sqr"), expr.type)
          elif y.value == 1:
            return self.cast(x, expr.type)
          elif y.value == 0:
            return self.cast(one_i64, expr.type)
          elif y.value == 0.5 and isinstance(expr.type, FloatT):
            expr.prim = prims.sqrt
            expr.args = (self.cast(x, expr.type),)
            return expr
      elif prim == prims.logical_and:
        if is_true(x):
          return y
        elif is_true(y):
          return x  
        elif is_false(x) or is_false(y):
          return false 
      elif prim == prims.logical_or:
        if is_true(x) or is_true(y):
          return true
        elif is_false(x):
          return y 
        elif is_false(y):
          return x 
    expr.args = args
    return expr 
  
  def transform_Map(self, expr):

    expr.args = self.transform_simple_exprs(expr.args)
    expr.fn = self.transform_expr(expr.fn)
    expr.axis = self.transform_if_expr(expr.axis)
    
    
    max_rank = max(self.rank(arg) for arg in expr.args)
    # if an axis is the Python value None, turn it into the IR expression for None
    if max_rank == 1 and self.is_none(expr.axis): expr.axis = zero_i64
    elif expr.axis is None: expr.axis = none   
    return expr  
  
  def transform_OuterMap(self, expr):
    expr.args = self.transform_simple_exprs(expr.args)
    expr.fn = self.transform_expr(expr.fn)
    expr.axis = self.transform_if_expr(expr.axis)
    max_rank = max(self.rank(arg) for arg in expr.args)
    # if an axis is the Python value None, turn it into the IR expression for None
    if max_rank == 1 and self.is_none(expr.axis): expr.axis = zero_i64
    elif expr.axis is None: expr.axis = none   
    return expr  
    
  def transform_shape(self, expr):
    if isinstance(expr, Tuple):
      expr.elts = tuple(self.transform_simple_exprs(expr.elts))
      return expr 
    else:
      return self.transform_simple_expr(expr)
  
  def transform_ParFor(self, stmt):
    stmt.bounds = self.transform_shape(stmt.bounds)
    stmt.fn = self.transform_expr(stmt.fn)
    return stmt
  
  def transform_Reduce(self, expr):
    expr.axis = self.transform_if_expr(expr.axis)
    expr.fn = self.transform_expr(expr.fn)
    expr.combine = self.transform_expr(expr.combine)
    expr.init = self.transform_if_simple_expr(expr.init)
    expr.args = self.transform_simple_exprs(expr.args)
    # if an axis is the Python value None, turn it into the IR expression for None
    max_rank = max(self.rank(arg) for arg in expr.args)
    if max_rank == 1 and self.is_none(expr.axis): expr.axis = zero_i64
    elif expr.axis is None: expr.axis = none   
    return expr  
  
  def transform_Scan(self, expr):
    expr.axis = self.transform_if_expr(expr.axis)
    expr.fn = self.transform_expr(expr.fn)
    expr.combine = self.transform_expr(expr.combine)
    expr.emit = self.transform_expr(expr.emit)
    expr.init = self.transform_if_simple_expr(expr.init)
    expr.args = self.transform_simple_exprs(expr.args)
    max_rank = max(self.rank(arg) for arg in expr.args)
    if max_rank == 1 and self.is_none(expr.axis): expr.axis = zero_i64
    elif expr.axis is None: expr.axis = none   
    return expr  
   
  
  def transform_IndexMap(self, expr):
    expr.fn = self.transform_expr(expr.fn)
    expr.shape = self.transform_shape(expr.shape)
    return expr 
  
  def transform_IndexReduce(self, expr):
    expr.fn = self.transform_if_expr(expr.fn)
    expr.combine = self.transform_expr(expr.combine)
    expr.init = self.transform_if_simple_expr(expr.init)
    expr.shape = self.transform_shape(expr.shape)
    return expr 
  
  def transform_IndexScan(self, expr):
    expr.fn = self.transform_if_expr(expr.fn)
    expr.combine = self.transform_expr(expr.combine)
    expr.emit = self.transform_if_expr(expr.emit)
    expr.init = self.transform_if_simple_expr(expr.init)
    expr.shape = self.transform_shape(expr.shape)
    return expr 
  
     
  def transform_ConstArray(self, expr):
    expr.shape = self.transform_shape(expr.shape)
    expr.value = self.transform_simple_expr(expr.value)
    return expr
  
  def transform_ConstArrayLike(self, expr):
    expr.array = self.transform_simple_expr(expr.array)
    expr.value = self.transform_simple_expr(expr.value)
  
  def temp_in_block(self, expr, block, name = None):
    """
    If we need a temporary variable not in the current top scope but in a
    particular block, then use this function. (this function also modifies the
    bindings dictionary)
    """
    if name is None:
      name = "temp"
    var = self.fresh_var(expr.type, name)
    block.append(Assign(var, expr))
    self.bindings[var.name] = expr
    return var

  def set_binding(self, name, value):
    assert value.__class__ is not Var or \
        value.name != name, \
        "Can't set name %s bound to itself" % name
    self.bindings[name] = value

  def bind_var(self, name, rhs):
    if rhs.__class__ is Var:
      old_val = self.bindings.get(rhs.name)
      if old_val and self.is_simple(old_val):
        self.set_binding(name, old_val)
      else:
        self.set_binding(name, rhs)
    else:
      self.set_binding(name, rhs)

  def bind(self, lhs, rhs):
    lhs_class = lhs.__class__
    if lhs_class is Var:
      self.bind_var(lhs.name, rhs)
    elif lhs_class is Tuple and rhs.__class__ is Tuple:
      assert len(lhs.elts) == len(rhs.elts)
      for lhs_elt, rhs_elt in zip(lhs.elts, rhs.elts):
        self.bind(lhs_elt, rhs_elt)

  def transform_lhs_Index(self, lhs):
    lhs.index = self.transform_index_expr(lhs.index)
    if lhs.value.__class__ is Var:
      stored = self.bindings.get(lhs.value.name)
      if stored and stored.__class__ is Var:
        lhs.value = stored
    else:
      lhs.value = self.assign_name(lhs.value, "array")
    return lhs

  def transform_lhs_Attribute(self, lhs):
    # lhs.value = self.transform_expr(lhs.value)
    return lhs

  def transform_ExprStmt(self, stmt):
    """Don't run an expression unless it possibly has a side effect"""

    v = self.transform_expr(stmt.value)
    if self.immutable(v):
      return None
    else:
      stmt.value = v
      return stmt

  def transform_Assign(self, stmt):
    
    lhs = stmt.lhs
    rhs = self.transform_expr(stmt.rhs)
    lhs_class = lhs.__class__
    rhs_class = rhs.__class__

    if lhs_class is Var:
      if lhs.type.__class__ is NoneT and self.use_counts.get(lhs.name,0) == 0:
        return self.transform_stmt(ExprStmt(rhs))
      elif self.immutable(rhs):
        
        self.bind_var(lhs.name, rhs)
        if rhs_class is not Var and rhs_class is not Const:
          self.available_expressions.setdefault(rhs, lhs)
    elif lhs_class is Tuple:
      self.bind(lhs, rhs)

    elif lhs_class is Index:
      if rhs_class is Index and \
         lhs.value == rhs.value and \
         lhs.index == rhs.index:
        # kill effect-free writes like x[i] = x[i]
        return None
      elif rhs_class is Var and \
           lhs.value.__class__ is Var and \
           lhs.value.name == rhs.name and \
           lhs.index.type.__class__ is TupleT and \
           all(elt_t == slice_none_t for elt_t in lhs.index.type.elt_types):
        # also kill x[:] = x
        return None
      else:
        lhs = self.transform_lhs_Index(lhs)
        # when assigning x[j] = [1,2,3]
        # just rewrite it as a sequence of element assignments 
        # to avoid 
        if lhs.type.__class__ is ArrayT and \
           lhs.type.rank == 1 and \
           rhs.__class__ is Array:
          lhs_slice = self.assign_name(lhs, "lhs_slice")
          for (elt_idx, elt) in enumerate(rhs.elts):
            lhs_idx = self.index(lhs_slice, const_int(elt_idx), temp = False)
            self.assign(lhs_idx, elt)
          return None
        elif not self.is_simple(rhs):
          rhs = self.assign_name(rhs)
    else:
      assert lhs_class is Attribute
      assert False, "Considering making attributes immutable"
      lhs = self.transform_lhs_Attribute(lhs)

    if rhs_class is Var and \
       rhs.name in self.bindings and \
       self.use_counts.get(rhs.name, 1) == 1:
      self.use_counts[rhs.name] = 0
      rhs = self.bindings[rhs.name]
    stmt.lhs = lhs
    stmt.rhs = rhs
    return stmt

  def transform_block(self, stmts, keep_bindings = False):
    self.available_expressions.push()
    self.bindings.push()
    
    new_stmts = Transform.transform_block(self, stmts)
    
    self.available_expressions.pop()
    if not keep_bindings:
      self.bindings.pop()
    return new_stmts

  def enter_loop(self, phi_nodes):
    result = {}
    for (k, (left,right)) in phi_nodes.iteritems():
      new_left = self.transform_expr(left)
      if new_left == right:
        self.set_binding(k, new_left)
      else:
        result[k] = (new_left, right)
    return result 
  
  def transform_merge(self, phi_nodes, left_block, right_block):
    result = {}
    for (k, (left, right)) in phi_nodes.iteritems():
      new_left = self.transform_expr(left)
      new_right = self.transform_expr(right)

      if not isinstance(new_left, (Const, Var)):
        new_left = self.temp_in_block(new_left, left_block)
      if not isinstance(new_right, (Const, Var)):
        new_right = self.temp_in_block(new_right, right_block)

      if new_left == new_right:
        # if both control flows yield the same value then
        # we don't actually need the phi-bound variable, we can just
        # replace the left value everywhere
        self.assign(Var(name= k, type = new_left.type), new_left)
        self.set_binding(k, new_left)
      else:
        result[k] = new_left, new_right
    return result

  def transform_If(self, stmt):
    stmt.true = self.transform_block(stmt.true, keep_bindings = True)
    stmt.false = self.transform_block(stmt.false, keep_bindings=True)
    stmt.merge = self.transform_merge(stmt.merge,
                                      left_block = stmt.true,
                                      right_block = stmt.false)
    self.bindings.pop()
    self.bindings.pop()
    stmt.cond = self.transform_simple_expr(stmt.cond, "cond")
    if len(stmt.true) == 0 and len(stmt.false) == 0 and len(stmt.merge) <= 2:
      for (lhs_name, (true_expr, false_expr)) in stmt.merge.items():
        lhs_type = self.lookup_type(lhs_name)
        lhs_var = Var(name = lhs_name, type = lhs_type)
        assert true_expr.type == false_expr.type, \
          "Unexpcted type mismatch: %s != %s" % (true_expr.type, false_expr.type)
        rhs = Select(stmt.cond, true_expr, false_expr, type = true_expr.type)
        self.bind_var(lhs_name, rhs)
        self.assign(lhs_var, rhs)
      return None 
    return stmt

  def transform_loop_condition(self, expr, outer_block, loop_body, merge):
    """Normalize loop conditions so they are just simple variables"""

    if self.is_simple(expr):
      return self.transform_expr(expr)
    else:
      loop_carried_vars = [name for name in collect_var_names(expr)
                           if name in merge]
      if len(loop_carried_vars) == 0:
        return expr

      left_values = [merge[name][0] for name in loop_carried_vars]
      right_values = [merge[name][1] for name in loop_carried_vars]

      left_cond = subst.subst_expr(expr, dict(zip(loop_carried_vars,
                                                  left_values)))
      if not self.is_simple(left_cond):
        left_cond = self.temp_in_block(left_cond, outer_block, name = "cond")

      right_cond = subst.subst_expr(expr, dict(zip(loop_carried_vars,
                                                   right_values)))
      if not self.is_simple(right_cond):
        right_cond = self.temp_in_block(right_cond, loop_body, name = "cond")

      cond_var = self.fresh_var(left_cond.type, "cond")
      merge[cond_var.name] = (left_cond, right_cond)
      return cond_var

    
  def transform_While(self, stmt):
    merge = self.enter_loop(stmt.merge)
    stmt.body = self.transform_block(stmt.body)
    stmt.merge = self.transform_merge(merge,
                                      left_block = self.blocks.current(),
                                      right_block = stmt.body)
    stmt.cond = \
        self.transform_loop_condition(stmt.cond,
                                      outer_block = self.blocks.current(),
                                      loop_body = stmt.body,
                                      merge = stmt.merge)
    return stmt

  def transform_ForLoop(self, stmt):
    

    merge = self.enter_loop(stmt.merge)
    stmt.body = self.transform_block(stmt.body) 
    stmt.merge = self.transform_merge(merge,
                                      left_block = self.blocks.current(),
                                      right_block = stmt.body)
    stmt.start = self.transform_simple_expr(stmt.start, 'start')
    stmt.stop = self.transform_simple_expr(stmt.stop, 'stop')
    if self.is_none(stmt.step):
      stmt.step = one(stmt.start.type)
    else:
      stmt.step = self.transform_simple_expr(stmt.step, 'step')

    # if a loop is only going to run for one iteration, might as well get rid of
    # it
    if stmt.start.__class__ is Const and \
       stmt.stop.__class__ is Const and \
       stmt.step.__class__ is Const:
      if stmt.start.value >= stmt.stop.value:
        for (var_name, (input_value, _)) in stmt.merge.iteritems():
          var = Var(var_name, input_value.type)
          self.blocks.append(Assign(var, input_value))
        return None
      elif stmt.start.value + stmt.step.value >= stmt.stop.value:
        for (var_name, (input_value, _)) in stmt.merge.iteritems():
          var = Var(var_name, input_value.type)
          self.blocks.append(Assign(var, input_value))
        self.assign(stmt.var, stmt.start)
        self.blocks.top().extend(stmt.body)
        return None
    return stmt

  def transform_Return(self, stmt):
    new_value = self.transform_expr(stmt.value)
    if new_value != stmt.value:
      stmt.value = new_value
    return stmt

########NEW FILE########
__FILENAME__ = simplify_array_operators

from ..analysis import collect_var_names
from ..builder import build_fn
from ..ndtypes import ArrayT 
from ..syntax import Var, ArrayExpr, Adverb, Range, Const, IndexMap 
from ..syntax.helpers import is_zero, is_one, is_false, is_true, is_none
from transform import Transform

class SimplifyArrayOperators(Transform):
  """
  Algebraic rewrite rules involving first-order array operations
  such as Range, ConstArray, etc.. 
  
  We do this conservatively by collecting all the array bindings
  and wiping them from the dictionary if there's even any chance they 
  might get modified
  """
  
  def pre_apply(self, fn):
    self.bindings = []
    return fn 
  
  def push(self):
    self.bindings.append({})
    
  def pop(self):
    self.bindings.pop()
    
  def mark_dirty(self, name):
    if isinstance(self.type_env[name], ArrayT):
      for d in self.bindings:
        if name in d:
          del d[name]
  def bind(self, name, v):
    self.bindings[-1][name] = v 
    
  def lookup(self, name):
    return self.bindings[-1].get(name)
  
  def transform_Attribute(self, expr):
    return expr
  
  def transform_Var(self, expr):
    """
    If we're using an array variable *anywhere* assume it might get modified
    """
    self.mark_dirty(expr.name)
    return expr  
  
  def transform_Map(self, expr):
    """
    Map(f, Range(start,stop)) = IndexMap(f', (stop - start))
    Map(f, NDIndex(shape)) = IndexMap(f, shape)
    """
     
    if len(expr.args) != 1:
      return expr 
    arg = expr.args[0]
    if arg.__class__ is not Var:
      return expr 
    name = arg.name
    prev = self.lookup(name)
    if prev is None:
      return expr 
    if prev.__class__ is Range:
      step = prev.step   
      if step.__class__ is not Const:
        return expr 
      if step.value is not None and step.value != 1:
        return expr 
      diff = self.sub(prev.stop, prev.start, "niters")
      return IndexMap(fn = expr.fn, shape = diff, start_index = prev.start, type = expr.type) 
    return expr 
      
  def transform_Assign(self, stmt):
    if isinstance(stmt.lhs, Var):
      if isinstance(stmt.rhs, (ArrayExpr, Adverb)):
        self.bind(stmt.lhs.name, stmt.rhs)
      stmt.rhs = self.transform_expr(stmt.rhs)
    else:
      for name in collect_var_names(stmt.lhs):
        self.mark_dirty(name)
      for name in collect_var_names(stmt.rhs):
        self.mark_dirty(name)
    return stmt 
  
  def transform_Return(self, stmt):
    if isinstance(stmt.value, Adverb):
      stmt.value = self.transform_expr(stmt.value)
    return stmt 
  
  def transform_block(self, stmts):
    self.push()
    stmts = Transform.transform_block(self, stmts)
    self.pop()
    return stmts 
########NEW FILE########
__FILENAME__ = specialize_fn_args
from .. import names
from ..builder import build_fn
from ..syntax import Const, Var
from ..syntax.helpers import is_constant
from inline import Inliner
from transform import Transform

class SpecializeFnArgs(Transform):
  """
  Specialize the arguments to Map, Reduce, Scan, etc..  in two ways (including closure args):
    - constant arguments get moved into the function and are removed
      from the arguments list 
    - if an argument is repeated, only keep the first one. Be careful to not remove two instances
      of an array if it's being traversed along two different axes.
  
  TODO:
    - also get rid of unused arguments 
  """
  
  
  def build_arg_mapping(self, outer_closure_args, outer_array_args, inner_arg_names):
    """
    This function examines a list of actual args to a function (including its closure values), 
    and constructs:
      - a list of which closure and array args to keep
      - a list of which inner names to keep 
      - a dictionary associating discarded inner names with constants
      - a dictionary associating discarded inner names with kept names 
    """
    new_closure_args = []      
    remap = {}
    const = {}
    new_inner_names = []
      
    for pos, arg in enumerate(outer_closure_args):
      inner_name = inner_arg_names[pos]
      if arg.__class__ is Const:
        const[inner_name] = arg 
      elif arg in new_closure_args: 
        prev_pos = new_closure_args.index(arg)
        remap[inner_name] = new_inner_names[prev_pos]
      else:
        new_inner_names.append(inner_name)
        new_closure_args.append(arg)
      
    
    new_array_args = []
    for array_arg_pos, arg in enumerate(outer_array_args):
      pos = array_arg_pos + len(outer_closure_args)
      inner_name = inner_arg_names[pos]
      if arg.__class__ is Const:
        const[inner_name] = arg 
      elif arg in new_array_args:
        prev_pos = len(new_closure_args) + new_array_args.index(arg)
        remap[inner_name] = new_inner_names[prev_pos]
      else:
        new_inner_names.append(inner_name)
        new_array_args.append(arg)
        
    return new_closure_args, new_array_args, remap, const, new_inner_names

  def specialize_closure(self, clos, array_args):    
    closure_args = self.closure_elts(clos)
    fn = self.get_fn(clos)
    array_args = self.transform_expr_list(array_args)
    new_closure_args, new_array_args, remap, const, new_inner_names = \
      self.build_arg_mapping(closure_args, array_args, fn.arg_names)
    if len(new_inner_names) == len(fn.arg_names):
      return clos, array_args 
    
    new_input_types = [fn.type_env[name] for name in new_inner_names]
    
    new_name = "specialized_" + names.original(fn.name)
    new_fn, builder, _  = \
      build_fn(new_input_types,  
               return_type = fn.return_type, 
               name = new_name, 
               input_names = new_inner_names)
      
    new_fn.created_by = fn.created_by
     
    # call the original function 
    call_args = [] 
    for old_name, t  in zip(fn.arg_names, fn.input_types):
      if old_name in const:
        call_args.append(const[old_name])
      elif old_name in remap:
        other_name = remap[old_name]
        call_args.append(Var(name = other_name, type = t))
      else:
        assert old_name in new_inner_names, \
          "Expected %s to be in list of kept function arguments: %s" % (old_name, new_inner_names)
        call_args.append(Var(name = old_name, type = t))
    builder.return_(builder.call(fn, call_args))
    new_fn = Inliner().apply(new_fn)
    new_closure = self.closure(new_fn, new_closure_args)
    return new_closure, new_array_args

  def transform_Map(self, expr):
    if self.is_none(expr.axis) or is_constant(expr.axis):
      new_closure, new_array_args = self.specialize_closure(expr.fn, expr.args)
      expr.fn = new_closure
      expr.args = new_array_args
    return expr 
    
      
########NEW FILE########
__FILENAME__ = subst
from .. syntax import Expr, Var
from clone_function import CloneFunction
from transform import Transform

class RewriteVars(Transform):
  def __init__(self, rename_dict):
    Transform.__init__(self, require_types = False)
    self.rename_dict = rename_dict

  def transform_merge(self, old_merge):
    new_merge = {}
    for (k,(l,r)) in old_merge.iteritems():
      new_name = self.rename_dict.get(k,k)
      if type(new_name) != str:
        assert new_name.__class__ is Var, \
            "Unexpected substitution %s for %s" % (new_name, k)
        new_name = new_name.name
      new_left = self.transform_expr(l)
      new_right = self.transform_expr(r)
      new_merge[new_name] = new_left, new_right
    return new_merge
  
  def transform_Var(self, expr):
    new_value = self.rename_dict.get(expr.name, expr.name)
    if new_value.__class__ is str:
      if new_value != expr.name:
        expr.name = new_value
      return expr
    else:
      assert isinstance(expr, Expr)
      assert new_value.type is not None, \
          "Type of replacement value %s can't be None" % new_value
      return new_value

def subst_expr(expr, rename_dict):
  fresh_expr = CloneFunction().transform_expr(expr)
  return RewriteVars(rename_dict).transform_expr(fresh_expr)

def subst_expr_list(nodes, rename_dict):
  return [subst_expr(node, rename_dict) for node in nodes]

def subst_expr_tuple(elts, rename_dict):
  return tuple(subst_expr_list(elts, rename_dict))

def subst_stmt_list(stmts, rename_dict):
  fresh_stmts = CloneFunction().transform_block(stmts)
  return RewriteVars(rename_dict).transform_block(fresh_stmts)

########NEW FILE########
__FILENAME__ = transform
import time

from .. import config
from .. analysis import verify
from .. builder import Builder  
from .. syntax import (Expr, If, Assign, While, Return, ExprStmt, ForLoop, Comment, ParFor, 
                       Var, Tuple, Index, Attribute, Const, PrimCall, Struct, Alloc, Cast,  
                       TupleProj, Slice, ArrayView, Call, TypedFn,  AllocArray, Len, UntypedFn,  
                       Map, Reduce) 

transform_timings = {}
transform_counts = {}
if config.print_transform_timings:
  import atexit
  def print_timings():
    print "TRANSFORM TIMINGS"
    items = transform_timings.items()
    items.sort(key = lambda (_,t): t)
    items.reverse()
    for k, t in items:
      count = transform_counts[k]
      print "  %30s  Total = %6dms, Count = %4d, Avg = %3fms" % \
            (k.__name__, t*1000, count, (t/count)*1000)
  atexit.register(print_timings)

class Transform(Builder):
  def __init__(self, verify=config.opt_verify,
                     reverse=False,
                     require_types=True):

    Builder.__init__(self)
    self.fn = None
    self.verify = verify
    self.reverse = reverse
    self.require_types = require_types

  def __str__(self):
    return self.__class__.__name__ 
  
  def __repr__(self):
    return str(self)
  
  def __hash__(self):
    return hash(str(self))
  
  def __eq__(self, other):
    return str(self) == str(other)
  
  def lookup_type(self, name):
    assert self.type_env is not None
    return self.type_env[name]

  def transform_if_expr(self, maybe_expr):
    if isinstance(maybe_expr, Expr):
      return self.transform_expr(maybe_expr)
    elif isinstance(maybe_expr, tuple):
      return tuple([self.transform_if_expr(x) for x in maybe_expr])
    elif isinstance(maybe_expr, list):
      return [self.transform_if_expr(x) for x in maybe_expr]
    #elif hasattr(maybe_expr, 'transform'):
    #  return maybe_expr.transform(self.transform_expr)
    else:
      return maybe_expr

  def transform_generic_expr(self, expr):
    for member_name in expr.members():
      old_value = getattr(expr, member_name)
      new_value = self.transform_if_expr(old_value)
      setattr(expr, member_name, new_value)
    return expr

  def find_method(self, expr, prefix = "transform_"):
    assert expr, "Expected expression but got %s" % expr 
    method_name = prefix + expr.__class__.__name__
    if hasattr(self, method_name):
      return getattr(self, method_name)
    else:
      return None

  """
  Common cases for expression transforms: we don't need to create a method for
  every sort of expression but these run faster and allocate less memory than
  transform_generic_expr
  """
  def transform_Var(self, expr):
    return expr

  def transform_Tuple(self, expr):
    expr.elts = tuple(self.transform_expr(elt) for elt in expr.elts)
    return expr

  def transform_Const(self, expr):
    return expr

  def transform_Index(self, expr):
    expr.value = self.transform_expr(expr.value)
    expr.index = self.transform_expr(expr.index)
    return expr

  def transform_Attribute(self, expr):
    expr.value = self.transform_expr(expr.value)
    return expr

  def transform_PrimCall(self, expr):
    expr.args = self.transform_expr_tuple(expr.args)
    return expr

  def transform_Call(self, expr):
    expr.fn = self.transform_expr(expr.fn)
    expr.args = self.transform_expr_tuple(expr.args)
    return expr

  def transform_Alloc(self, expr):
    expr.count = self.transform_expr(expr.count)
    return expr

  def transform_Struct(self, expr):
    expr.args = self.transform_expr_tuple(expr.args)
    return expr

  def transform_Cast(self, expr):
    expr.value = self.transform_expr(expr.value)
    return expr

  def transform_Select(self, expr):
    expr.cond = self.transform_expr(expr.cond)
    expr.true_value = self.transform_expr(expr.true_value)
    expr.false_value = self.transform_expr(expr.false_value)
    return expr 

  def transform_TupleProj(self, expr):
    expr.tuple = self.transform_expr(expr.tuple)
    return expr

  def transform_TypedFn(self, expr):
    """By default, don't do recursive transformation of referenced functions"""
    return expr
  
  def transform_UntypedFn(self, expr):
    """By default, don't do recursive transformation of referenced functions"""
    return expr


  def transform_Slice(self, expr):
    expr.start = self.transform_expr(expr.start) if expr.start else None
    expr.stop = self.transform_expr(expr.stop) if expr.stop else None
    expr.step = self.transform_expr(expr.step) if expr.step else None
    return expr
  
  def transform_Range(self, expr):
    expr.start = self.transform_expr(expr.start) if expr.start else None
    expr.stop = self.transform_expr(expr.stop) if expr.stop else None
    expr.step = self.transform_expr(expr.step) if expr.step else None
    return expr
    

  def transform_Array(self, expr):
    expr.elts = self.transform_expr_tuple(expr.elts) 
    return expr
    
  def transform_ArrayView(self, expr):
    expr.data = self.transform_expr(expr.data)
    expr.shape = self.transform_expr(expr.shape)
    expr.strides = self.transform_expr(expr.strides)
    expr.offset = self.transform_expr(expr.offset)
    expr.size = self.transform_expr(expr.size)
    return expr
  
  def transform_AllocArray(self, expr):
    expr.shape = self.transform_expr(expr.shape)
    return expr 
  
  def transform_ConstArray(self, expr):
    expr.shape = self.transform_expr(expr.shape)
    expr.value = self.transform_expr(expr.value)
    
  def transform_ConstArrayLike(self, expr):
    expr.array = self.transform_expr(expr.array)
    expr.value = self.transform_expr(expr.value)

  def transform_Ravel(self, expr):
    expr.array = self.transform_expr(expr.array)
    return expr 
  
  def transform_Reshape(self, expr):
    expr.array = self.transform_expr(expr.array)
    expr.shape = self.transform_expr(expr.shape)
    return expr
  
  def transform_Transpose(self, expr):
    expr.array = self.transform_expr(expr.array)
    return expr 
  
  def transform_Shape(self, expr):
    expr.array = self.transform_expr(expr.array)
    return expr 
  
  def transform_Len(self, expr):
    expr.value = self.transform_expr(expr.value) 
    return expr

  def transform_IndexMap(self, expr):
    expr.fn = self.transform_expr(expr.fn)
    expr.shape = self.transform_expr(expr.shape)
    return expr 
  
  def transform_IndexReduce(self, expr):
    expr.fn = self.transform_expr(expr.fn)
    expr.combine = self.transform_if_expr(expr.combine)
    expr.shape = self.transform_expr(expr.shape)
    expr.init = self.transform_if_expr(expr.init)
    return expr
  
  def transform_IndexScan(self, expr):
    expr.fn = self.transform_expr(expr.fn)
    expr.combine = self.transform_if_expr(expr.combine)
    expr.shape = self.transform_expr(expr.shape)
    expr.init = self.transform_if_expr(expr.init)
    return expr
  
  def transform_Map(self, expr):
    expr.axis = self.transform_if_expr(expr.axis)
    expr.fn = self.transform_expr(expr.fn)
    expr.args = self.transform_expr_list(expr.args)
    return expr
  
  def transform_Reduce(self, expr):
    expr.axis = self.transform_if_expr(expr.axis)
    expr.init = self.transform_if_expr(expr.init)
    expr.fn = self.transform_expr(expr.fn)
    expr.combine = self.transform_if_expr(expr.combine)
    expr.args = self.transform_expr_list(expr.args)
    return expr
  
  def transform_Scan(self, expr):
    expr.axis = self.transform_if_expr(expr.axis)
    expr.init = self.transform_if_expr(expr.init)
    expr.fn = self.transform_expr(expr.fn)
    expr.combine = self.transform_if_expr(expr.combine)
    expr.args = self.transform_expr_list(expr.args)
    expr.emit = self.transform_expr(expr.emit)
    return expr
  
  def transform_OuterMap(self, expr):
    expr.axis = self.transform_if_expr(expr.axis)
    expr.fn = self.transform_expr(expr.fn)
    expr.args = self.transform_expr_tuple(expr.args)
    return expr
  
  def transform_Closure(self, expr):
    expr.args = self.transform_expr_tuple(expr.args)
    expr.fn = self.transform_expr(expr.fn)
    return expr
  
  
  def transform_ClosureElt(self, expr):
    expr.closure = self.transform_expr(expr.closure)
    return expr

  def transform_TypeValue(self, expr):
    pass 
  
  def transform_DelayUntilTyped(self, expr):
    expr.values = self.transform_expr_tuple(expr.values)
    return expr 
  
  def transform_expr(self, expr):
    """Dispatch on the node type and call the appropriate transform method"""

    expr_class = expr.__class__
    if expr_class is Var:
      result = self.transform_Var(expr)
    elif expr_class is Const:
      result = self.transform_Const(expr)
    elif expr_class is Tuple:
      result = self.transform_Tuple(expr)
    elif expr_class is TupleProj:
      result = self.transform_TupleProj(expr)
    elif expr_class is Index:
      result = self.transform_Index(expr)
    elif expr_class is Slice:
      result = self.transform_Slice(expr)
    elif expr_class is Attribute:
      result = self.transform_Attribute(expr)
    elif expr_class is PrimCall:
      result = self.transform_PrimCall(expr)
    elif expr_class is Struct:
      result = self.transform_Struct(expr)
    elif expr_class is AllocArray:
      result = self.transform_AllocArray(expr)
    elif expr_class is Alloc:
      result = self.transform_Alloc(expr)
    elif expr_class is Cast:
      result = self.transform_Cast(expr)
    elif expr_class is ArrayView:
      result = self.transform_ArrayView(expr)
    elif expr_class is TypedFn:
      result = self.transform_TypedFn(expr)
    elif expr_class is UntypedFn:
      result = self.transform_UntypedFn(expr)
    elif expr_class is Call:
      result = self.transform_Call(expr)
    elif expr_class is Map:
      result = self.transform_Map(expr)
    elif expr_class is Reduce:
      result = self.transform_Reduce(expr)
    elif expr_class is Len:
      result = self.transform_Len(expr)
    else:
      method = self.find_method(expr, "transform_")
      if method:
        result = method(expr)

      else:
        assert False, "Unsupported expr %s" % (expr,)
        result = self.transform_generic_expr(expr)
    if result is None:
      return expr 
    else:
      assert isinstance(result, Expr), \
        "Invalid result type in transformation: %s" % (type(result),)
      result.source_info = expr.source_info 
      return result 
  
  def transform_lhs_Var(self, expr):
    return self.transform_Var(expr)

  def transform_lhs_Tuple(self, expr):
    return self.transform_Tuple(expr)

  def transform_lhs_Index(self, expr):
    return self.transform_Index(expr)

  def transform_lhs_Attribute(self, expr):
    return self.transform_Attribute(expr)

  def transform_lhs(self, lhs):
    """
    Overload this is you want different behavior for transformation of left-hand
    side of assignments
    """

    lhs_class = lhs.__class__
    if lhs_class is Var:
      return self.transform_lhs_Var(lhs)
    elif lhs_class is Tuple:
      return self.transform_lhs_Tuple(lhs)
    elif lhs_class is Index:
      return self.transform_lhs_Index(lhs)
    elif lhs_class is Attribute:
      return self.transform_lhs_Attribute(lhs)

    lhs_method = self.find_method(lhs, prefix = "transform_lhs_")
    if lhs_method:
      return lhs_method(lhs)

    method = self.find_method(lhs, prefix = "transform_")
    assert method, "Unknown expression of type %s" % lhs_class
    return method(lhs)

  def transform_expr_list(self, exprs):
    return [self.transform_expr(e) for e in exprs]

  def transform_expr_tuple(self, exprs):
    return tuple(self.transform_expr_list(exprs))

  

  def transform_merge(self, phi_nodes):
    result = {}
    for (k, (left, right)) in phi_nodes.iteritems():
      new_left = self.transform_expr(left)
      new_right = self.transform_expr(right)
      result[k] = new_left, new_right
    return result

  def transform_merge_before_loop(self, phi_nodes):
    return phi_nodes 
  
  def transform_merge_after_loop(self, phi_nodes):
    return self.transform_merge(phi_nodes)
  
  def transform_Assign(self, stmt):
    stmt.rhs = self.transform_expr(stmt.rhs)
    stmt.lhs = self.transform_lhs(stmt.lhs)
    return stmt

  def transform_ExprStmt(self, stmt):
    stmt.value = self.transform_expr(stmt.value)
    return stmt
  
  def transform_Return(self, stmt):
    stmt.value = self.transform_expr(stmt.value)
    return stmt

  def transform_If(self, stmt):
    stmt.true = self.transform_block(stmt.true)
    stmt.false = self.transform_block(stmt.false)
    stmt.merge = self.transform_merge(stmt.merge)
    stmt.cond = self.transform_expr(stmt.cond)
    return stmt

  def transform_While(self, stmt):
    stmt.merge = self.transform_merge_before_loop(stmt.merge)
    stmt.cond = self.transform_expr(stmt.cond)
    stmt.body = self.transform_block(stmt.body)
    stmt.merge = self.transform_merge_after_loop(stmt.merge)
    return stmt
  
  def transform_ForLoop(self, stmt):
    stmt.var = self.transform_expr(stmt.var)
    stmt.merge = self.transform_merge_before_loop(stmt.merge)
    stmt.start = self.transform_expr(stmt.start)
    stmt.stop = self.transform_expr(stmt.stop)
    stmt.step = self.transform_expr(stmt.step)
    stmt.body = self.transform_block(stmt.body)
    stmt.merge = self.transform_merge_after_loop(stmt.merge)
    return stmt 
  
  def transform_Comment(self, stmt):
    return stmt 
  
  def transform_ParFor(self, stmt):
    stmt.fn = self.transform_expr(stmt.fn)
    stmt.bounds= self.transform_expr(stmt.bounds)
    return stmt 
  
  def transform_stmt(self, stmt):
    
    stmt_class = stmt.__class__
    if stmt_class is Assign:
      return self.transform_Assign(stmt)
    elif stmt_class is ForLoop:
      return self.transform_ForLoop(stmt)
    elif stmt_class is While:
      return self.transform_While(stmt)
    elif stmt_class is If:
      return self.transform_If(stmt)
    elif stmt_class is Return:
      return self.transform_Return(stmt)
    elif stmt_class is ExprStmt:
      return self.transform_ExprStmt(stmt)
    elif stmt_class is ParFor:
      return self.transform_ParFor(stmt)
    elif stmt_class is Comment:
      return self.transform_Comment(stmt)
    else:
      assert False, "Unexpected statement %s" % stmt_class

  def transform_block(self, stmts):
    
    self.blocks.push()
    
    if self.reverse: stmts = reversed(stmts)
    
    for old_stmt in stmts:
      new_stmt = self.transform_stmt(old_stmt)
      if new_stmt is not None:
        self.blocks.append_to_current(new_stmt)
    new_block = self.blocks.pop()
    if self.reverse: new_block.reverse()
    return new_block

  def pre_apply(self, old_fn):
    pass  

  def post_apply(self, new_fn):
    pass 

  def apply(self, fn):
    if config.print_transform_timings:
      start_time = time.time()

    transform_name = self.__class__.__name__
    
      
    if config.print_functions_before_transforms == True or \
        (isinstance(config.print_functions_before_transforms, list) and
         transform_name in config.print_functions_before_transforms):
      print
      print "Running transform %s" % transform_name
      print "--- before ---"
      print repr(fn)
      print
    
    self.fn = fn
    self.type_env = fn.type_env

    # push an extra block onto the stack just in case
    # one of the pre_apply methods want to put statements somewhere
    self.blocks.push()
    pre_fn = self.pre_apply(self.fn)
    pre_block = self.blocks.pop()

    if pre_fn is not None:
      fn = pre_fn

    self.fn = fn
    self.type_env = fn.type_env

    # pop the outermost block, which have been written to by
    new_body = self.transform_block(fn.body)
    
    if len(pre_block) > 0:
      new_body = pre_block  + new_body

    fn.body = new_body
    fn.type_env = self.type_env

    self.blocks.push()
    new_fn = self.post_apply(fn)
    post_block = self.blocks.pop()
    if new_fn is None:
      new_fn = fn

    if len(post_block) > 0:
      new_fn.body = new_fn.body + post_block

    if config.print_functions_after_transforms == True or \
        (isinstance(config.print_functions_after_transforms, list) and
         transform_name in config.print_functions_after_transforms):
      print
      print "Done with  %s" % transform_name
      print "--- after ---"
      print repr(new_fn)
      print

    if self.verify:
      try:
        verify(new_fn)
      except:
        print "ERROR after running %s on %s" % (transform_name , new_fn)
        raise

    if config.print_transform_timings:
      end_time = time.time()
      c = self.__class__
      total_time = transform_timings.get(c, 0)
      transform_timings[c] = total_time + (end_time - start_time)
      transform_counts[c] = transform_counts.get(c, 0) + 1
    return new_fn


########NEW FILE########
__FILENAME__ = vectorize
from ..ndtypes import Int32, Int64, Float32, Float64

from loop_transform import LoopTransform

class Vectorize(LoopTransform):
  
  def pre_apply(self, _):
    # skip the may-alias analysis from LoopTransform
    pass 
  
  vector_elt_types = (Int32, Int64, Float32, Float64) 
  def vectorize_loop(self, stmt):
     
    candidate_names = set([])
    before_values = {}
    after_values = {}
    for (k, (before, after)) in stmt.merge.iteritems():
      t = before.type
      if t in self.vector_elt_types:
        candidate_names.add(k)
        before_values[k] = before
        after_values[k] = after
      
  
  def transform_ForLoop(self, stmt):
    # for now we only vectorize if 
    # something is being accumulated, 
    # otherwise you need more sophisticated
    # cost-base heuristic to avoid lots of 
    # redundant SIMD packing and unpacking
    if self.is_simple_block(stmt.body) and len(stmt.merge) > 1:
      return self.vectorize_loop(stmt)
    else:
      return stmt 
      
########NEW FILE########
__FILENAME__ = helpers

from .. import names 
from ..ndtypes import ClosureT, make_closure_type
from ..syntax import Expr, UntypedFn, Closure, Var, Return, ClosureElt, TypedFn, FormalArgs  
from ..syntax.helpers import get_types 

def unpack_closure(closure):
  """
  Given an object which could be either a function, a function's name, a
  closure, or a closure type:
  Return the underlying untyped function and the closure arguments
  """

  if closure.__class__ is ClosureT:
    fn, closure_args = closure.fn, closure.arg_types
  elif closure.__class__ is Closure:
    fn = closure.fn 
    closure_args = closure.args 
  elif closure.type.__class__ is ClosureT:
    fn, arg_types = closure.type.fn, closure.type.arg_types
    closure_args = \
        [ClosureElt(closure, i, type = arg_t)
         for (i, arg_t) in enumerate(arg_types)]
  else:
    fn = closure
    closure_args = []
    # fn = UntypedFn.registry[fn]
  return fn, closure_args


def make_typed_closure(clos, typed_fn):
  if clos.__class__ is UntypedFn:
    return typed_fn

  assert isinstance(clos, Expr) and clos.type.__class__ is ClosureT
  _, closure_args = unpack_closure(clos)
  if len(closure_args) == 0:
    return typed_fn
  else:
    t = make_closure_type(typed_fn, get_types(closure_args))
    return Closure(typed_fn, closure_args, t)

def mk_untyped_identity():
  var_name = names.fresh('x')
  var_expr = Var(var_name)
  fn_name = names.fresh('identity')
  args_obj = FormalArgs()
  args_obj.add_positional(var_name)
  return UntypedFn(name = fn_name, 
                   args = args_obj, 
                   body = [Return(var_expr)])

untyped_identity_function = mk_untyped_identity()

def _get_fundef(fn):
  c = fn.__class__ 
  if c is UntypedFn or c is TypedFn:
    return fn
  assert c is str, "Unexpected function %s : %s"  % (fn, fn.type)
  return UntypedFn.registry[fn]

def _get_closure_type(fn):
  c = fn.__class__ 
  if c is ClosureT:
    return fn
  elif c is Closure:
    return fn.type
  elif c is Var:
    assert fn.type.__class__ is ClosureT
    return fn.type
  else:
    fundef = _get_fundef(fn)
    return make_closure_type(fundef, [])
########NEW FILE########
__FILENAME__ = linearize_args

from ..ndtypes import type_conv
from ..syntax import TupleProj, get_type, ActualArgs


from helpers import unpack_closure
from ..syntax.formal_args import FormalArgs, UnexpectedKeyword

def linearize_arg_types(fn, args):

  """
  Given a function object which might be one of:
    (1) a closure type
    (2) the name of an untyped function
    (3) an untyped fn object
  and some argument types which might
    (1) a list
    (2) a tuple
    (3) an ActualArgs object
  linearize the argument types with respect to
  the untyped function's argument order and return
  both the untyped function and list of arguments
  """

  untyped_fundef, closure_args = unpack_closure(fn)
  if isinstance(args, (list, tuple)):
    args = ActualArgs(args)
    
  if not isinstance(untyped_fundef.args, FormalArgs):
    assert False, "Expected function %s to have FormalArgs" % untyped_fundef.name 
    #formals = FormalArgs()
    #for arg in untyped_fundef.args:
    #  formals.add_positional(arg)
  else:
    formals = untyped_fundef.args

  if len(closure_args) > 0:
    args = args.prepend_positional(closure_args)

  def keyword_fn(_, v):
    return type_conv.typeof(v)

  try: 
    linear_args, extra = formals.linearize_values(args, keyword_fn = keyword_fn)
  except UnexpectedKeyword as e:
    e.fn_name = untyped_fundef.name  
    raise 
      
  return untyped_fundef, tuple(linear_args + extra)

def tuple_elts(tup):
  return [TupleProj(tup, i, t)
          for (i,t) in enumerate(tup.type.elt_types)]

def flatten_actual_args(args):
  if isinstance(args, (list,tuple)):
    return args
  assert isinstance(args, ActualArgs), \
      "Unexpected args: %s" % (args,)
  assert len(args.keywords) == 0
  result = list(args.positional)
  if args.starargs:
    result.extend(tuple_elts(args.starargs))
  return result

def linearize_actual_args(fn, args):
    untyped_fn, closure_args = unpack_closure(fn)
    if isinstance(args, (list, tuple)):
      args = ActualArgs(args)
    args = args.prepend_positional(closure_args)

    arg_types = args.transform(get_type)

    # Drop arguments that are assigned defaults,
    # since we're assuming those are set in the body
    # of the function
    if isinstance(untyped_fn.args, FormalArgs):
      try:
        combined_args = untyped_fn.args.linearize_without_defaults(args, tuple_elts)
      except UnexpectedKeyword as e:
        e.fn_name = fn.name 
        raise 
          
    else:
      combined_args = list(args)
    
    return untyped_fn, combined_args, arg_types
########NEW FILE########
__FILENAME__ = local_inference
from .. import names
from ..ndtypes import (StructT, Type, Unknown, SliceT,
                       ArrayT, TypeValueT, TupleT,
                       ScalarT, IntT, Int64,  Bool, Float64,    
                       combine_type_list, increase_rank, lower_rank,  
                       make_array_type, make_tuple_type, make_slice_type, make_closure_type, 
                       repeat_tuple, 
                       type_conv, )

from ..syntax import (Array, AllocArray, Attribute, 
                      Cast, Closure, Const, ConstArray, ConstArrayLike, 
                      Expr, Index, 
                      Range, Ravel, Reshape, Shape,
                      Select, Slice, 
                      Transpose, Tuple, TupleProj, TypeValue,  Var,  
                      ForLoop, While, Assign, Return, If)

from ..syntax.helpers import get_types, is_true, is_false, const_int, unwrap_constant, make_tuple
from ..syntax.wrappers import build_untyped_prim_fn

from ..transforms import Transform


class LocalTypeInference(Transform):
  """
  Local type inference information which doesn't 
  require recursive calls back into the inference 
  algorithm. 
  """
  def __init__(self, tenv, var_map):
    Transform.__init__(self)
    self.type_env = tenv 
    self.var_map = var_map
    
  def transform_expr(self, expr):
    from ..frontend import ast_conversion
    if not isinstance(expr, Expr):
      expr = ast_conversion.value_to_syntax(expr)
    result = Transform.transform_expr(self, expr)  
    assert result.type is not None, \
       "Unsupported expression encountered during type inference: %s" % (expr,) 
    return result 


  def transform_Index(self, expr):
    value = self.transform_expr(expr.value)
    index = self.transform_expr(expr.index)
    
    
    if isinstance(value.type, TupleT):
      
      assert isinstance(index.type, (SliceT, IntT)), \
        "Tuple index can't be %s" % (index.type,)
      assert isinstance(index, (Const, Slice)), \
        "Unexpected tuple index %s" % (index,)
      i = unwrap_constant(index)
      assert isinstance(i, (int, slice, tuple)), \
        "Unexpected tuple index %s" % (i,)
      
      elt_types = value.type.elt_types
      if isinstance(i, int):
        assert i < len(elt_types), \
          "Can't get element %d of length %d tuple %s : %s" % \
          (i, len(elt_types), value, value.type)
        elt_t = value.type.elt_types[i]
        return TupleProj(value, i, type = elt_t)
      else:
        assert isinstance(i, slice)
        elt_indices = range(len(elt_types))[i]
        elts = [TupleProj(value, i, type = elt_types[i]) for i in elt_indices]
        return make_tuple(elts)
    else:
      result_type = value.type.index_type(index.type)
      return Index(value, index, type = result_type)

  
  def transform_Array(self, expr):
    
    new_elts = self.transform_args(expr.elts)
    if expr.type is None:
      elt_types = get_types(new_elts)
      if len(elt_types) > 0:
        common_t = combine_type_list(elt_types)
        if common_t is Unknown:
          raise TypeError("Couldn't find commom type for elements of array %s" % expr)
      else:
        # numpy defaults to Float arrays 
        common_t = Float64 
    else:
      common_t = expr.type 
    array_t = increase_rank(common_t, 1)
    return Array(new_elts, type = array_t)

  def transform_AllocArray(self, expr):
    elt_type = expr.elt_type
    assert isinstance(elt_type, ScalarT), \
      "Invalid array element type  %s" % (elt_type)
      
    shape = self.transform_expr(expr.shape)
    if isinstance(shape, ScalarT):
      shape = self.cast(shape, Int64)
      shape = self.tuple((shape,), "array_shape")
    assert isinstance(shape, TupleT), \
      "Invalid shape %s" % (shape,)
    rank = len(shape.elt_types)
    t = make_array_type(elt_type, rank)
    return AllocArray(shape, elt_type, type = t)

  def transform_ConstArray(self, expr):
    shape = self.transform_expr(expr.shape)
    value = self.transform_expr(expr.value)
    assert isinstance(value.type, ScalarT), \
      "ConstArray expects scalar value, got %s: %s" % (value, value.type)
    if isinstance(shape.type, ScalarT):
      shape = self.tuple([shape])
    assert isinstance(shape.type, TupleT), \
      "Expected shape of ConstArray to be tuple, got %s : %s" % (shape, shape.type)
    ndims = len(shape.type.elt_types)
    if ndims == 0:
      return value 
    array_t = make_array_type(value.type, ndims)
    return ConstArray(shape, value, type = array_t) 
  
  def transform_ConstArrayLike(self, expr):
    array = self.transform_expr(expr.array)
    value = self.transform_expr(expr.value)
    assert isinstance(value.type, ScalarT), \
      "ConstArray expects scalar value, got %s: %s" % (value, value.type)
    if isinstance(array.type, ScalarT):
      return value 
    elif isinstance(array.type, TupleT):
      array = self.transform_expr(Array(elts = self.tuple_elts(array)))
      
    assert isinstance(array.type, ArrayT), \
      "ConstArrayLike expected array, got %s : %s" % (array, array.type)
    shape = self.shape(array)
    ndims = len(shape.type.elt_types)
    if ndims == 0:
      return value 
    array_t = make_array_type(value.type, ndims)
    return ConstArrayLike(array = array, value = value, type = array_t)
    

  
  def transform_Range(self, expr):
    start = self.transform_expr(expr.start) if expr.start else None
    stop = self.transform_expr(expr.stop) if expr.stop else None
    step = self.transform_expr(expr.step) if expr.step else None
    
    # by default we're generating ranges of Int64
    # but also allow for floating values 
    elt_t = Int64 
    if not self.is_none(start): elt_t = elt_t.combine(start.type)
    if not self.is_none(stop): elt_t = elt_t.combine(stop.type)
    if not self.is_none(step): elt_t = elt_t.combine(step.type)
    array_t = make_array_type(elt_t, 1)
    return Range(start, stop, step, type = array_t)

  def transform_Slice(self, expr):
    start = self.transform_expr(expr.start)
    stop = self.transform_expr(expr.stop)
    step = self.transform_expr(expr.step)
    slice_t = make_slice_type(start.type, stop.type, step.type)
    return Slice(start, stop, step, type = slice_t)


  def transform_Var(self, expr):
    old_name = expr.name
    if old_name not in self.var_map._vars:
      raise names.NameNotFound(old_name)
    new_name = self.var_map.lookup(old_name)
    
    assert new_name in self.type_env, \
        "Unknown var %s (previously %s)" % (new_name, old_name)
    t = self.type_env[new_name]
    return Var(new_name, type = t)

  def transform_Tuple(self, expr):
    elts = self.transform_expr_list(expr.elts)
    return self.tuple(elts)

  def transform_Const(self, expr):
    if expr.type is None:
      t = type_conv.typeof(expr.value)
    else:
      t = expr.type 
    return Const(expr.value, type = t)
  

  def transform_Shape(self, expr):
    array = self.transform_expr(expr.array)
    if isinstance(array.type, ArrayT):
      t = repeat_tuple(Int64, array.type.rank)
      return Shape(array = array, type = t)
    elif isinstance(array.type, TupleT):
      elt = const_int(len(array.type.elt_types))
      return self.tuple( [elt]  )
    else:
      return self.tuple( [] )
      

  def transform_Reshape(self, expr):
    array = self.transform_expr(expr.array)
    shape = self.transform_expr(expr.shape)
    rank = len(shape.type.elt_types)
    assert array.type.__class__ is ArrayT
    t = make_array_type(array.elt_type, rank)
    return Reshape(array, shape, type = t)
  
  def transform_Ravel(self, expr):
    array = self.transform_expr(expr.array)
    if isinstance(array.type, ScalarT):
      return array 
    assert array.type.__class__ is ArrayT, \
      "Can't ravel/flatten %s of type %s" % (array, array.type) 
    t = make_array_type(array.type.elt_type, 1)
    return Ravel(array, type = t)
  
  def transform_Transpose(self, expr):
    array = self.transform_expr(expr.array)
    if isinstance(array.type, ScalarT):
      return array
    assert array.type.__class__ is ArrayT, \
      "Can't transpose %s of type %s" % (array, array.type) 
    return Transpose(array = array, type = array.type)
  
  def transform_Cast(self, expr):
    v = self.transform_expr(expr.value)
    return Cast(v, type = expr.type)
  
  def transform_Len(self, expr):
    v = self.transform_expr(expr.value)
    t = v.type
    if t.__class__ is ArrayT:
      shape_t = make_tuple_type([Int64] * t.rank)
      shape = Attribute(v, 'shape', type = shape_t)
      return TupleProj(shape, 0, type = Int64)
    else:
      assert t.__class__ is TupleT, "Unexpected argument for 'len' - %s : %s" % (expr.value, t)
      return Const(len(t.elt_types), type = Int64)

  def transform_DelayUntilTyped(self, expr):
    new_values = self.transform_expr_tuple(expr.values)
    if expr.keywords:
      typed_keywords = {}
      for k, v in expr.keywords.iteritems():
        typed_keywords[k] = self.transform_expr(v)
      new_syntax = expr.fn(*new_values, **typed_keywords)
    else:
      new_syntax = expr.fn(*new_values)
    assert new_syntax.type is not None, \
      "Error in %s, new expression %s lacks type" % (expr, new_syntax)
    return new_syntax
  
  def transform_TypeValue(self, expr):
    t = expr.type_value 
    assert isinstance(t, Type), "Invalid type value %s" % (t,)
    return TypeValue(t, type=TypeValueT(t))
    
  def transform_Closure(self, expr):
    new_args = self.transform_expr_list(expr.args)
    t = make_closure_type(expr.fn, get_types(new_args))
    return Closure(expr.fn, new_args, type = t)

  def transform_Arith(self, expr):
    return build_untyped_prim_fn(expr)
    #t = make_closure_type(untyped_fn, ())
    #return Closure(untyped_fn, (), type = t)

  def transform_UntypedFn(self, expr):
    return expr 

  def transform_Attribute(self, expr):
    value = self.transform_expr(expr.value)
    assert isinstance(value.type, StructT)
    result_type = value.type.field_type(expr.name)
    return Attribute(value, expr.name, type = result_type)
  
  def transform_Select(self, expr):
    trueval = self.transform_expr(expr.true_value)
    falseval = self.transform_expr(expr.false_value)
    t = trueval.type.combine(falseval.type)
    trueval = self.cast(trueval, t)
    falseval = self.cast(falseval, t) 
    cond = self.transform_expr(expr.cond)
    cond = self.cast(cond, Bool)
    return Select(cond = cond, true_value = trueval, false_value = falseval, type = t)
  
  def infer_phi(self, result_var, val):
    """
    Don't actually rewrite the phi node, just add any necessary types to the
    type environment
    """

    new_val = self.transform_expr(val)
    new_type = new_val.type
    old_type = self.type_env.get(result_var, Unknown)
    new_result_var = self.var_map.lookup(result_var)
    self.type_env[new_result_var]  = old_type.combine(new_type)

  def infer_phi_nodes(self, nodes, direction):
    for (var, values) in nodes.iteritems():
      self.infer_phi(var, direction(values))

  def infer_left_flow(self, nodes):
    return self.infer_phi_nodes(nodes, lambda (x, _): x)

  def infer_right_flow(self, nodes):
    return self.infer_phi_nodes(nodes, lambda (_, x): x)

  
  def transform_phi_node(self, result_var, (left_val, right_val)):
    """
    Rewrite the phi node by rewriting the values from either branch, renaming
    the result variable, recording its new type, and returning the new name
    paired with the annotated branch values
    """

    new_left = self.transform_expr(left_val)
    new_right = self.transform_expr(right_val)
    old_type = self.type_env.get(result_var, Unknown)
    new_type = old_type.combine(new_left.type).combine(new_right.type)
    new_var = self.var_map.lookup(result_var)
    self.type_env[new_var] = new_type
    return (new_var, (new_left, new_right))

  def transform_phi_nodes(self, nodes):
    new_nodes = {}
    for old_k, (old_left, old_right) in nodes.iteritems():
      new_name, (left, right) = self.transform_phi_node(old_k, (old_left, old_right))
      new_nodes[new_name] = (left, right)
    return new_nodes

  def annotate_lhs(self, lhs, rhs_type):

    lhs_class = lhs.__class__
    #
    # Am I a tuple of Assignments? 
    #
    if lhs_class is Tuple:
      if rhs_type.__class__ is TupleT:
        assert len(lhs.elts) == len(rhs_type.elt_types)
        new_elts = [self.annotate_lhs(elt, elt_type) 
                    for (elt, elt_type) 
                    in zip(lhs.elts, rhs_type.elt_types)]
      else:
        assert rhs_type.__class__ is ArrayT, \
            "Unexpected right hand side type %s for %s" % (rhs_type, lhs)
        elt_type = lower_rank(rhs_type, 1)
        new_elts = [self.annotate_lhs(elt, elt_type) for elt in lhs.elts]
      tuple_t = make_tuple_type(get_types(new_elts))
      return Tuple(new_elts, type = tuple_t)
    
    #
    # Index Assignment
    #
    elif lhs_class is Index:
      new_arr = self.transform_expr(lhs.value)
      new_idx = self.transform_expr(lhs.index)
      assert new_arr.type.__class__ is ArrayT, \
          "Expected array, got %s" % new_arr.type
      elt_t = new_arr.type.index_type(new_idx.type)
      return Index(new_arr, new_idx, type = elt_t)
    
    # 
    # Attribute Assignment for Mutable Objects
    # 
    elif lhs_class is Attribute:
      name = lhs.name
      struct = self.transform_expr(lhs.value)
      struct_t = struct.type
      assert isinstance(struct_t, StructT), \
          "Can't access fields on value %s of type %s" % \
          (struct, struct_t)
      field_t = struct_t.field_type(name)
      return Attribute(struct, name, field_t)
    
    # 
    # Regular Binding of Names to Values
    # 
    else:
      assert lhs_class is Var, "Unexpected LHS: %s" % (lhs,)
      new_name = self.var_map.lookup(lhs.name)
      old_type = self.type_env.get(new_name, Unknown)
      new_type = old_type.combine(rhs_type)
      self.type_env[new_name] = new_type
      return Var(new_name, type = new_type)

  def transform_Assign(self, stmt):
    rhs = self.transform_expr(stmt.rhs)
    lhs = self.annotate_lhs(stmt.lhs, rhs.type)
    return Assign(lhs, rhs)
  
  def transform_If(self, stmt):

    cond = self.transform_expr(stmt.cond) 

    assert isinstance(cond.type, ScalarT), \
        "Condition %s has type %s but must be convertible to bool" % (cond, cond.type)
    # it would be cleaner to not have anything resembling an optimization 
    # inter-mixed with the type inference, but I'm not sure how else to 
    # support 'if x is None:...'
    if is_true(cond):
      self.blocks.top().extend(self.transform_block(stmt.true))
      for (name, (left,_)) in stmt.merge.iteritems():
        typed_left = self.transform_expr(left)
        typed_var = self.annotate_lhs(Var(name), typed_left.type) 
        self.assign(typed_var, typed_left)
      return
    
    if is_false(cond):
      self.blocks.top().extend(self.transform_block(stmt.false))
      for (name, (_,right)) in stmt.merge.iteritems():
        typed_right = self.transform_expr(right)
        typed_var = self.annotate_lhs(Var(name), typed_right.type)
        self.assign(typed_var, typed_right)
      return
    true = self.transform_block(stmt.true)
    false = self.transform_block(stmt.false) 
    merge = self.transform_phi_nodes(stmt.merge)
    return If(cond, true, false, merge)

  def transform_Return(self, stmt):
    ret_val = self.transform_expr(stmt.value)
    curr_return_type = self.type_env["$return"]
    self.type_env["$return"] = curr_return_type.combine(ret_val.type)
    return Return(ret_val)

  def transform_While(self, stmt):
    self.infer_left_flow(stmt.merge)
    cond = self.transform_expr(stmt.cond)
    body = self.transform_block(stmt.body)
    merge = self.transform_phi_nodes(stmt.merge)
    return While(cond, body, merge)

  def transform_ForLoop(self, stmt):
    start = self.transform_expr(stmt.start)
    stop = self.transform_expr(stmt.stop)
    step = self.transform_expr(stmt.step)
    lhs_t = start.type.combine(stop.type).combine(step.type)
    var = self.annotate_lhs(stmt.var, lhs_t)
    self.infer_left_flow(stmt.merge)
    body = self.transform_block(stmt.body)
    merge = self.transform_phi_nodes(stmt.merge)
    return ForLoop(var, start, stop, step, body, merge)


########NEW FILE########
__FILENAME__ = rewrite_typed

from .. import names, syntax 
from ..builder import mk_cast_fn 
from ..ndtypes import (
  IncompatibleTypes, 
  Bool, Type,  ArrayT, Int64, TupleT,
  NoneT, SliceT, ScalarT,  
  make_tuple_type, make_array_type, lower_rank
)

from ..syntax import (
  Assign, Tuple, Var, Return, Index, Map, ConstArrayLike, Const, Cast
)
from ..syntax.helpers import get_types, zero_i64, none, const 
from ..transforms import Transform 


class RewriteTyped(Transform):
  def __init__(self, return_type = None):
    Transform.__init__(self, verify = False)
    self.forced_return_type = return_type 
    
    
  def pre_apply(self, fn):
    
    if self.forced_return_type is None:
      self.fn_return_type = self.fn.type_env["$return"]
    else:
      self.fn_return_type = self.forced_return_type
  
  def post_apply(self, fn):
    fn.return_type = self.fn_return_type  
  
  def coerce_expr(self, expr, t):
    assert t is not None
    expr = self.transform_expr(expr)
    if expr.type == t:
      return expr
    elif expr.__class__ is Tuple:
      if t.__class__ is not TupleT or \
          len(expr.type.elt_types) != t.elt_types:
        raise IncompatibleTypes(expr.type, t)
      else:
        new_elts = []
        for elt, elt_t in zip(expr.elts, t.elt_types):
          new_elts.append(self.coerce_expr(elt, elt_t))
        return Tuple(new_elts, type = t)
    elif isinstance(expr.type, ArrayT) and \
        isinstance(t, ArrayT) and \
        expr.type.rank == t.rank:
      scalar_from_t = expr.type.elt_type
      scalar_to_t = t.elt_type
      caster = mk_cast_fn(scalar_from_t, scalar_to_t)
      return Map(fn = caster, args = (expr,), type = t)
    else:
      assert \
          isinstance(expr.type, ScalarT) and \
          isinstance(t, ScalarT), \
          "Can't cast type %s into %s" % (expr.type, t)
      return Cast(expr, type=t)

  def transform_merge(self, merge):
    new_merge = {}
    for (var, (left, right)) in merge.iteritems():
      t = self.type_env[var]
      new_left = self.coerce_expr(left, t)
      new_right = self.coerce_expr(right, t)
      new_merge[var] = (new_left, new_right)
    return new_merge

  def transform_lhs(self, lhs):
    if isinstance(lhs, Var):
      t = self.lookup_type(lhs.name)
      if t == lhs.type:
        return lhs
      else:
        return Var(lhs.name, type = t)
    elif isinstance(lhs, syntax.Tuple):
      elts = map(self.transform_lhs, lhs.elts)
      elt_types = get_types(elts)
      if elt_types != lhs.type.elt_types:
        return syntax.Tuple(elts, type = make_tuple_type(elt_types))
      else:
        return lhs
    else:
      return lhs

  def transform_Var(self, expr):
    expr.type = self.fn.type_env[expr.name]
    return expr

  def transform_PrimCall(self, expr):
      arg_types = get_types(expr.args)
      upcast_types = expr.prim.expected_input_types(arg_types)
      result_type = expr.prim.result_type(upcast_types)
      upcast_args = [self.coerce_expr(x, t)
                     for (x,t) in zip(expr.args, upcast_types)]
      return syntax.PrimCall(expr.prim, upcast_args, type = result_type)

  def transform_Array(self, expr):

    array_t = expr.type
    elt_t = array_t.elt_type
    assert array_t.rank > 0
    if array_t.rank == 1:
      new_elts = [self.coerce_expr(elt, elt_t) for elt in expr.elts]
      return syntax.Array(new_elts, type = array_t)
    else:
      # need to allocate an output array and copy the elements in
      first_elt = self.assign_name(expr.elts[0], "first_elt")
      elt_dims = [self.shape(first_elt, i) for i in xrange(array_t.rank - 1)]
      n = len(expr.elts)
      outer_dim = const(n)
      all_dims = (outer_dim,) + tuple(elt_dims)
      array = self.alloc_array(elt_t, all_dims, "array_literal")
      for i, elt in enumerate(expr.elts):
        idx_expr = self.index(array, i, temp = False)
        # transform indexing to make missing indices explicit
        self.assign(idx_expr, expr.elts[i])
      return array

  def transform_Reduce(self, expr):
    acc_type = self.return_type(expr.combine)
    init = self.transform_if_expr(expr.init)
    args = self.transform_expr_tuple(expr.args)
    if init and not self.is_none(init) and init.type != acc_type:
      assert len(args) == 1
      init = self.coerce_expr(init, acc_type)
    expr.args = args 
    expr.init = init
    return expr
    
  def transform_Scan(self, expr):
    acc_type = self.return_type(expr.combine)
    init = self.transform_if_expr(expr.init)
    args = self.transform_expr_tuple(expr.args)
    if init and not self.is_none(init) and init.type != acc_type:
      if isinstance(acc_type, ScalarT):
        init = self.coerce_expr(init, acc_type)
      elif isinstance(init.type, ScalarT) and \
           isinstance(expr.axis, Const) and \
           len(args) == 1:
        arr_slice = self.slice_along_axis(args[0], expr.axis, zero_i64)
        init_type = make_array_type(elt_type = expr.init.type, rank = arr_slice.type.rank)
        init = ConstArrayLike(array = arr_slice, 
                                   value = expr.init, 
                                   type = init_type)
      else:
        assert False, \
          "Scan with scalar init of type %s and accumulator of type %s not yet supported" % \
                    (expr.init.type, acc_type)
    expr.args = args
    expr.init = init
    return expr

  def transform_Slice(self, expr):
    """
    # None step defaults to 1
    if isinstance(expr.step.type, core_types.NoneT):
      start_t = expr.start.type
      stop_t = expr.stop.type
      step = syntax_helpers.one_i64
      step_t = step.type
      slice_t = array_type.make_slice_type(start_t, stop_t, step_t)
      expr.step = step
      expr.type = slice_t
    """
    return expr
  

  def get_index_fn(self, array_t, index_types, _index_function_cache = {}):
    index_types = tuple(index_types)
    key = (array_t, index_types) 
    if key in _index_function_cache:
      return _index_function_cache[key]
    array_name = names.fresh("array")
    array_var = Var(array_name, type = array_t)
    idx_vars = []
    idx_names = []
    idx_types = []
    lower_rank_by = 0
    type_env = {array_name:array_t}
    
    for i, idx_t in enumerate(index_types):
      # indexing with None or a Slice doesn't decrease the rank
      # whereas by a scalar does 
      if isinstance(idx_t, ScalarT):
        lower_rank_by += 1 
      idx_name = names.fresh("idx%d" % (i+1))
      idx_names.append(idx_name)
      idx_var = Var(idx_name, type = idx_t)
      if idx_t is not Int64:
        idx_var = Cast(value = idx_var,  type = Int64)
        idx_t = Int64 
      idx_types.append(index_types)
      idx_vars.append(idx_var)
      type_env[idx_name] = idx_t
    
    elt_t = lower_rank(array_t, lower_rank_by)
    if len(idx_vars) > 1:
      idx = Tuple(idx_vars, type = make_tuple_type(index_types))
    else:
      idx = idx_vars[0]
    
    fn = syntax.TypedFn(
        name = names.fresh("fancy_indexing_helper"), 
        arg_names = (array_name,) + tuple(idx_names),
        input_types = (array_t,) + tuple(index_types),
        return_type = elt_t,
        type_env = type_env, 
        body = [Return (Index(array_var, idx, type = elt_t))]) 
    _index_function_cache[key] = fn 
    return fn 
    
  def transform_Index(self, expr):
    # TODO: Make fancy indexing work 
    # with multiple indices, boolean index elements, 
    # and multi-dimensional indexing

    index = expr.index
    if index.type.__class__ is TupleT:
      indices = self.tuple_elts(index) 
    else:
      indices = [index]
    
    if all(isinstance(idx.type, (NoneT, SliceT, ScalarT)) for idx in indices):
      return expr 
    
    map_args = []
    index_elt_types = []
    
    for index in indices:
      if index.type.__class__ is ArrayT:
        assert index.type.rank == 1, \
          "Don't yet support indexing by %s" % index.type 
        index_elt_t = index.type.elt_type
        
        if index_elt_t == Bool:
          assert False, "Indexing by boolean vector not yet implemented"
          #index_array = Where(expr.index)
          #index_elt_t = Int64
        else:
          map_args.append(expr.index)
          index_elt_types.append(index_elt_t)
      else:
        map_args.append(expr.index)
        index_elt_types.append(expr.index.type)
      
    index_fn = self.get_index_fn(expr.value.type, index_elt_types)
    index_closure = self.closure(index_fn, [expr.value])
    return Map(fn = index_closure, 
                   args = map_args, 
                   type = expr.value.type, 
                   axis = none)
    
  def transform_Assign(self, stmt):
    new_lhs = self.transform_lhs(stmt.lhs)
    lhs_t = new_lhs.type
    rhs = self.transform_expr(stmt.rhs)
    assert lhs_t is not None, "Expected a type for %s!" % stmt.lhs
    if new_lhs.__class__ is Tuple: 
      rhs = self.assign_name(rhs)
      for (i, lhs_elt) in enumerate(new_lhs.elts):
        if lhs_elt.__class__ is Var:
          name = names.original(lhs_elt.name) 
        else:
          name = None 
        idx = self.index(rhs, i, name = name )
        elt_stmt = self.transform_Assign(Assign(lhs_elt, idx))
        self.blocks.append_to_current(elt_stmt)
      return None  
    elif new_lhs.__class__ is Index and \
         isinstance(new_lhs.value.type, ArrayT) and \
         isinstance(rhs.type, ScalarT):
      new_rhs = self.coerce_expr(rhs, new_lhs.value.type.elt_type)
      assert new_rhs.type and isinstance(new_rhs.type, Type), \
          "Expected type annotation on %s, but got %s" % (new_rhs, new_rhs.type)
      
      stmt.lhs = new_lhs
      stmt.rhs = new_rhs
      return stmt 

    else:     
      new_rhs = self.coerce_expr(rhs, lhs_t)
      assert new_rhs.type and isinstance(new_rhs.type, Type), \
          "Expected type annotation on %s, but got %s" % (new_rhs, new_rhs.type)
      stmt.lhs = new_lhs
      stmt.rhs = new_rhs
      return stmt
    
  def transform_If(self, stmt):
    stmt.cond = self.coerce_expr(stmt.cond, Bool)
    stmt.true = self.transform_block(stmt.true)
    stmt.false = self.transform_block(stmt.false)
    stmt.merge = self.transform_merge(stmt.merge)
    return stmt

  def transform_Return(self, stmt):
    stmt.value = self.coerce_expr(stmt.value, self.fn_return_type)
    return stmt

  def transform_ForLoop(self, stmt):
    var_t = stmt.var.type
    stmt.start = self.coerce_expr(stmt.start, var_t)
    stmt.stop = self.coerce_expr(stmt.stop, var_t)
    stmt.step = self.coerce_expr(stmt.step, var_t)
    stmt.body = self.transform_block(stmt.body)
    stmt.merge = self.transform_merge(stmt.merge)
    return stmt 
  
  def transform_While(self, stmt):
    stmt.cond = self.coerce_expr(stmt.cond, Bool)
    stmt.body = self.transform_block(stmt.body)
    stmt.merge = self.transform_merge(stmt.merge)
    return stmt

def rewrite_typed(typed_fundef, return_type = None):
  return RewriteTyped(return_type).apply(typed_fundef)

########NEW FILE########
__FILENAME__ = type_inference
from itertools import izip 

from .. import config, names,  prims, syntax

from ..builder import mk_prim_fn 
from ..ndtypes import (Type, 
                       array_type, closure_type, tuple_type, type_conv, 
                       Bool, IntT, Int64,  ScalarT, ArrayT,  
                       NoneType, NoneT, Unknown, UnknownT, 
                       TypeValueT, 
                       TupleT, make_tuple_type, 
                       lower_rank, make_array_type, 
                       ClosureT)

from ..syntax import adverb_helpers
from ..syntax import (UntypedFn, TypedFn, Closure,  Var, Map, Expr, 
                      ActualArgs, FormalArgs, MissingArgsError, TooManyArgsError)

from ..syntax.helpers import (get_type, get_types,  get_elt_types, 
                              one_i64, zero_i64, none, true, false, 
                              gen_data_arg_names, unwrap_constant)
from ..syntax.wrappers import build_untyped_prim_fn, build_untyped_cast_fn


from helpers import untyped_identity_function, make_typed_closure, _get_closure_type, _get_fundef
from linearize_args import linearize_actual_args, flatten_actual_args
from local_inference import LocalTypeInference 
from var_map import VarMap 


class TypeInferenceError(Exception):
  def __init__(self, msg, expr):
    self.msg = msg
    self.expr = expr 

_invoke_type_cache = {}
def invoke_result_type(fn, arg_types):
  if fn.__class__ is TypedFn:
    assert isinstance(arg_types, (list, tuple))
    assert len(arg_types) == len(fn.input_types), \
        "Type mismatch between expected inputs %s and %s" % \
        (fn.input_types, arg_types)
    assert all(t1 == t2 for (t1,t2) in zip(arg_types, fn.input_types))
    return fn.return_type

  if isinstance(arg_types, (list, tuple)):
    arg_types = ActualArgs(arg_types)
  key = (fn, arg_types)
  if key in _invoke_type_cache:
    return _invoke_type_cache[key]

  if fn.__class__ is ClosureT:
    closure_set = closure_type.ClosureSet(fn)
  else:
    assert isinstance(fn, closure_type.ClosureSet), \
        "Invoke expected closure, but got %s" % (fn,)
    closure_set = fn

  result_type = Unknown
  for closure_t in closure_set.closures:
    typed_fundef = specialize(closure_t, arg_types)
    result_type = result_type.combine(typed_fundef.return_type)
  _invoke_type_cache[key] = result_type
  return result_type

class TypeInference(LocalTypeInference):
  
  
  def transform_args(self, args, flat = False):
    if isinstance(args, (list, tuple)):
      return self.transform_expr_list(args)
    else:
      new_args = args.transform(self.transform_expr)
      if flat:
        return flatten_actual_args(new_args)
      else:
        return new_args

  def transform_keywords(self, kwds_dict):
    if kwds_dict is None:
      return {}
    else:
      result = {}
      for (k,v) in kwds_dict.iteritems():
        result[k] = self.transform_expr(v)
      return result

  
  def transform_fn(self, f):
  
    """
    If you're calling a Type, turn it into a wrapper function around the Cast expression. 
    If you're calling a Prim, turn it into a wrapper function around the PrimCall expression.
    """
    if isinstance(f, Type):
      expr = build_untyped_cast_fn(f)
    elif isinstance(f, prims.Prim):
      expr = build_untyped_prim_fn(f)
    else:
      expr = f 
    return self.transform_expr(expr)
  
  def transform_Call(self, expr):
    closure = self.transform_fn(expr.fn)
    args = self.transform_args(expr.args)
    if closure.type.__class__ is TypeValueT:
      assert args.__class__ is  ActualArgs
      assert len(args.positional) == 1
      assert len(args.keywords) == 0
      assert args.starargs is None 
      return self.cast(args.positional[0], closure.type.type)
    
    untyped_fn, args, arg_types = linearize_actual_args(closure, args)
    typed_fn = specialize(untyped_fn, arg_types)
    return syntax.Call(typed_fn, tuple(args), typed_fn.return_type)

  def transform_PrimCall(self, expr):
    args = self.transform_args(expr.args)

    arg_types = get_types(args)
    if all(isinstance(t, ScalarT) for t in arg_types):
      upcast_types = expr.prim.expected_input_types(arg_types)
      result_type = expr.prim.result_type(upcast_types)
      return syntax.PrimCall(expr.prim, args, type = result_type)
    elif expr.prim == prims.is_:
      if arg_types[0] != arg_types[1]:
        return false
      elif arg_types[0] == NoneType:
        return true
      else:
        return syntax.PrimCall(prims.is_, args, type = Bool)
    elif all(t.rank == 0 for t in arg_types):
      # arguments should then be tuples
      assert len(arg_types) == 2, \
        "Expected two arguments but got [%s] in %s" % (", ".join(str(t) for t in arg_types), expr)
      xt, yt = arg_types
      x, y = args
      assert xt.__class__ is TupleT, \
        "Unexpected argument types (%s,%s) for operator %s" % (xt, yt, expr.prim)
      assert yt.__class__ is TupleT, \
        "Unexepcted argument types (%s,%s) for operator %s" % (xt, yt, expr.prim)
      x_elts = self.tuple_elts(x)
      y_elts = self.tuple_elts(y)
      nx = len(x_elts)
      ny = len(y_elts)
      if expr.prim is prims.add:
        return self.tuple( tuple(x_elts) + tuple(y_elts) )
      elif expr.prim is prims.equal:
        assert len(x_elts) == len(y_elts), \
          "Can't compare tuple of unequal lengths %d and %d" % (nx, ny)
        result = true  
        for (xi, yi) in zip(x_elts, y_elts):
          elts_eq = syntax.PrimCall(prims.equal, (xi, yi), type=Bool)
          result = syntax.PrimCall(prims.logical_and, (result, elts_eq), type=Bool) 
        return result  
      elif expr.prim is prims.not_equal:
        assert len(x_elts) == len(y_elts), \
          "Can't compare tuple of unequal lengths %d and %d" % (nx, ny)
        result = false  
        for (xi, yi) in zip(x_elts, y_elts):
          elts_eq = syntax.PrimCall(prims.not_equal, (xi, yi), type=Bool)
          result = syntax.PrimCall(prims.logical_or, (result, elts_eq), type=Bool) 
        return result  
      
      else:
        assert False, "Unsupported tuple operation %s" % expr  
    else:
      assert all(t.__class__ is not NoneT for t in arg_types), \
        "Invalid argument types for prim %s: %s" % (expr.prim, arg_types,)
      elt_types = [t.elt_type if isinstance(t, ArrayT) else t for t in arg_types]
      typed_prim_fn = mk_prim_fn(expr.prim, elt_types)
      max_rank = adverb_helpers.max_rank(arg_types)
      if max_rank == 0:
        return self.call(typed_prim_fn, args)
      elif max_rank == 1:
        axis = zero_i64 
      else:
        axis = none 
      result_t = make_array_type(typed_prim_fn.return_type, max_rank)
      return Map(fn = typed_prim_fn, args = args, type = result_t, axis = axis)
      

  def transform_Zip(self, expr):
    assert isinstance(expr.values, (list,tuple)), "Expected multiple values but got %s" % expr.values
    typed_values = self.transform_expr_tuple(expr.values)
    # if you're just zipping tuples together, 
    # then the result can also be a tuple
    if all(isinstance(v.type, TupleT) for v in typed_values):
      # keep the shortest tuple
      n = min(len(v.type.elt_types) for v in typed_values)
      zip_inputs = []
      for v in typed_values:
        zip_inputs.append(self.tuple_elts(v)[:n])
      return self.tuple([self.tuple(group) for group in zip(*zip_inputs)])
    
    # if any are tuples, that puts a max length on any sequence
    elif any(isinstance(v.type, TupleT) for v in typed_values):
      # keep the shortest tuple
      n = min(len(v.type.elt_types) for v in typed_values if isinstance(v.type, TupleT))
      assert False, "Zipping of mixed tuples and arrays not yet supported"  
    else:
      assert all(isinstance(v.type, ArrayT) for v in typed_values), \
        "Expected all inputs to zip to be arrays but got %s" % \
        ", ".join(str(v.type) for v in typed_values)
      
      elt_t = make_tuple_type([v.type.elt_type for v in typed_values])  
      result_t = make_array_type(elt_t, 1)
      def tupler(*args):
        return args 
      from ..frontend import ast_conversion
      untyped = ast_conversion.translate_function_value(tupler)
      typed = specialize(untyped, [v.type.elt_type for v in typed_values])
      result = Map(fn = typed, args = typed_values, axis = zero_i64, type = result_t)
      assert False, "Materializing the result of zipping arrays not yet supported"
      #return result
 
  def transform_IndexMap(self, expr):
    shape = self.transform_expr(expr.shape)
    if not isinstance(shape.type, TupleT):
      assert isinstance(shape.type, ScalarT), "Invalid shape for IndexMap: %s : %s" % (shape, shape.type)
      shape = self.tuple((shape,))
    closure = self.transform_fn(expr.fn)
    shape_t = shape.type
    if isinstance(shape_t, IntT):
      shape = self.cast(shape, Int64)
      n_indices = 1
    else:
      assert isinstance(shape_t, TupleT), "Expected shape to be tuple, instead got %s" % (shape_t,)
      assert all(isinstance(t, ScalarT) for t in shape_t.elt_types)
      n_indices = len(shape_t.elt_types)
      if not all(t == Int64 for t in shape_t.elt_types):
        elts = tuple(self.cast(elt, Int64) for elt in self.tuple_elts(shape))
        shape = self.tuple(elts)
    result_type, typed_fn = specialize_IndexMap(closure.type, n_indices)
    return syntax.IndexMap(shape = shape, 
                           fn = make_typed_closure(closure, typed_fn), 
                           type = result_type)
  
  def transform_IndexReduce(self, expr):
    shape = self.transform_expr(expr.shape)
    map_fn_closure = self.transform_fn(expr.fn if expr.fn else untyped_identity_function)
    combine_closure = self.transform_fn(expr.combine)
    init = self.transform_if_expr(expr.init)
    shape_t = shape.type
    if isinstance(shape_t, IntT):
      shape = self.cast(shape, Int64)
      n_indices = 1
    else:
      assert isinstance(shape_t, TupleT)
      assert all(isinstance(t, ScalarT) for t in shape_t.elt_types)
      n_indices = len(shape_t.elt_types)
      if not all(t == Int64 for t in shape_t.elt_types):
        elts = tuple(self.cast(elt, Int64) for elt in self.tuple_elts(shape))
        shape = self.tuple(elts)
    result_type, typed_fn, typed_combine = \
      specialize_IndexReduce(map_fn_closure.type, combine_closure, n_indices, init)
    if not self.is_none(init):
      init = self.cast(init, result_type)
    return syntax.IndexReduce(shape = shape, 
                              fn = make_typed_closure(map_fn_closure, typed_fn),
                              combine = make_typed_closure(combine_closure, typed_combine),
                              init = init,  
                              type = result_type)
  
  def transform_Map(self, expr):
    closure = self.transform_fn(expr.fn)
    new_args = self.transform_args(expr.args, flat = True)
    arg_types = get_types(new_args)
    assert len(arg_types) > 0, "Map requires array arguments"
    # if all arguments are scalars just handle map as a regular function call
    if all(isinstance(t, ScalarT) for t in arg_types):
      return self.invoke(closure, new_args)
    # if any arguments are tuples then all of them should be tuples of same len
    elif any(isinstance(t, TupleT) for t in arg_types):
      assert all(isinstance(t, TupleT) for t in arg_types), \
        "Map doesn't support input types %s" % (arg_types,)
      nelts = len(arg_types[0].elt_types)
      assert all(len(t.elt_types) == nelts for t in arg_types[1:]), \
       "Tuple arguments to Map must be of same length"
      zipped_elts = []
      for i in xrange(nelts):
        zipped_elts.append([self.tuple_proj(arg,i) for arg in new_args])
      return self.tuple([self.invoke(closure, elts) for elts in zipped_elts])
    axis = self.transform_if_expr(expr.axis)
    axes = self.normalize_axes(new_args, axis)
    result_type, typed_fn = specialize_Map(closure.type, arg_types, axes)
    return syntax.Map(fn = make_typed_closure(closure, typed_fn),
                       args = new_args,
                       axis = axis,
                       type = result_type)

    
  def transform_Reduce(self, expr):
    assert len(expr.args) > 0, "Can't have Reduce without any arguments %s" % expr 
    new_args = self.transform_args(expr.args, flat = True)
    arg_types = get_types(new_args)
    axis = self.transform_if_expr(expr.axis)

    map_fn = self.transform_fn(expr.fn if expr.fn else untyped_identity_function) 
    combine_fn = self.transform_fn(expr.combine)
   
    # if there aren't any arrays, just treat this as a function call
    if all(isinstance(t, ScalarT) for t in arg_types):
      scalar_result = self.invoke(map_fn, new_args)
      cast_t = self.invoke_type(combine_fn, [scalar_result, scalar_result])
      return self.cast(scalar_result, cast_t)
    
    if self.is_none(expr.init):
      init = none 
    else: 
      init = self.transform_expr(expr.init)
    
    axes = self.normalize_axes(new_args, axis)
    result_type, typed_map_fn, typed_combine_fn = \
      specialize_Reduce(map_fn.type,
                        combine_fn.type,
                        arg_types, 
                        axes, 
                        init.type)

    typed_map_closure = make_typed_closure (map_fn, typed_map_fn)
    typed_combine_closure = make_typed_closure(combine_fn, typed_combine_fn)
    # if we encounter init = 0 for a Reduce which produces an array
    # then need to broadcast to get an initial value of the appropriate rank 
    if init.type.__class__ is not NoneT and \
       init.type != result_type and \
       array_type.rank(init.type) < array_type.rank(result_type):
      assert len(new_args) == 1, "Can't have more than one arg in " % expr  
      arg = new_args[0]
      first_elt = self.slice_along_axis(arg, axis, zero_i64)
      first_combine = specialize(combine_fn, (init.type, first_elt.type))
      
      first_combine_closure = make_typed_closure(combine_fn, first_combine)
      init = self.call(first_combine_closure, (init, first_elt))
      slice_rest = syntax.Slice(start = one_i64, stop = none, step = one_i64, 
                                   type = array_type.SliceT(Int64, NoneType, Int64))
      rest = self.slice_along_axis(arg, axis, slice_rest)
      new_args = (rest,)  

    return syntax.Reduce(fn = typed_map_closure,
                         combine = typed_combine_closure,
                         args = new_args,
                         axis = axis,
                         type = result_type,
                         init = init)

  def transform_Scan(self, expr):
    map_fn = self.transform_fn(expr.fn if expr.fn else untyped_identity_function)
    combine_fn = self.transform_fn(expr.combine)
    emit_fn = self.transform_fn(expr.emit)
    new_args = self.transform_args(expr.args, flat = True)
    arg_types = get_types(new_args)
    
    init = self.transform_expr(expr.init) if expr.init else None
    
    init_type = get_type(init) if init else None
    
    axis = self.transform_if_expr(expr.axis)
    axes = self.normalize_axes(new_args, axis)
    result_type, typed_map_fn, typed_combine_fn, typed_emit_fn = \
        specialize_Scan(map_fn.type, combine_fn.type, emit_fn.type,
                        arg_types, axes, init_type)
    map_fn.fn = typed_map_fn
    combine_fn.fn = typed_combine_fn
    emit_fn.fn = typed_emit_fn
    return syntax.Scan(fn = make_typed_closure(map_fn, typed_map_fn),
                       combine = make_typed_closure(combine_fn,
                                                    typed_combine_fn),
                       emit = make_typed_closure(emit_fn, typed_emit_fn),
                       args = new_args,
                       axis = axis,
                       type = result_type,
                       init = init)

  def transform_OuterMap(self, expr):
    closure = self.transform_fn(expr.fn)
    new_args = self.transform_args (expr.args, flat = True)
    arg_types = get_types(new_args)
    n_args = len(arg_types)
    assert n_args > 0
    axis = self.transform_if_expr(expr.axis)
    axes = self.normalize_axes(new_args, axis)
    result_type, typed_fn = specialize_OuterMap(closure.type, arg_types, axes)
    result = syntax.OuterMap(fn = make_typed_closure(closure, typed_fn),
                             args = new_args,
                             axis = axis,
                             type = result_type)
    return result 


def infer_types(untyped_fn, types):
  """
  Given an untyped function and input types, propagate the types through the
  body, annotating the AST with type annotations.

  NOTE: The AST won't be in a correct state until a rewrite pass back-propagates
  inferred types throughout the program and inserts adverbs for scalar operators
  applied to arrays
  """
  
  var_map = VarMap()
  typed_args = untyped_fn.args.transform(rename_fn = var_map.rename)
  if untyped_fn.args.starargs:
    assert typed_args.starargs, "Missing star-args in call to %s(%s)" % (untyped_fn.name, typed_args,)

  unbound_keywords = []
  def keyword_fn(local_name, value):
    unbound_keywords.append(local_name)

    if isinstance(value, Expr):
      value = unwrap_constant(value) 
    return type_conv.typeof(value)

  try: 
    tenv = typed_args.bind(types,
                           keyword_fn = keyword_fn,
                           starargs_fn = tuple_type.make_tuple_type)
  except (MissingArgsError, TooManyArgsError) as e:
    e.fn_name = untyped_fn.name 
    raise e
  except: 
    print "Error while calling %s with types %s" % (untyped_fn, types)
    raise
  # keep track of the return
  tenv['$return'] = Unknown
  annotator = TypeInference(tenv, var_map)
  body = annotator.transform_block(untyped_fn.body)
  arg_names = [local_name for local_name
               in
               typed_args.nonlocals + tuple(typed_args.positional)
               if local_name not in unbound_keywords]
  if len(unbound_keywords) > 0:
    default_assignments = []
    from ..frontend.ast_conversion import value_to_syntax 
    for local_name in unbound_keywords:
      t = tenv[local_name]
      python_value = typed_args.defaults[local_name]
      var = Var(local_name, type = t)
      parakeet_value = value_to_syntax(python_value)
      parakeet_value.type = t 
      stmt = syntax.Assign(var, parakeet_value)
      default_assignments.append(stmt)
    body = default_assignments + body

  input_types = tuple([tenv[arg_name] for arg_name in arg_names])

  # starargs are all passed individually and then packaged up
  # into a tuple on the first line of the function
  if typed_args.starargs:
    local_starargs_name = typed_args.starargs

    starargs_t = tenv[local_starargs_name]
    assert starargs_t.__class__ is TupleT, \
        "Unexpected starargs type %s" % starargs_t
    extra_arg_vars = []
    for (i, elt_t) in enumerate(starargs_t.elt_types):
      arg_name = "%s_elt%d" % (names.original(local_starargs_name), i)
      tenv[arg_name] = elt_t
      arg_var = Var(name = arg_name, type = elt_t)
      arg_names.append(arg_name)
      extra_arg_vars.append(arg_var)
    input_types = input_types + starargs_t.elt_types
    tuple_lhs = Var(name = local_starargs_name, type = starargs_t)
    tuple_rhs = syntax.Tuple(elts = extra_arg_vars, type = starargs_t)
    stmt = syntax.Assign(tuple_lhs, tuple_rhs)
    body = [stmt] + body

  return_type = tenv["$return"]
  # if nothing ever gets returned, then set the return type to None
  if return_type.__class__ is  UnknownT:
    body.append(syntax.Return(none))
    tenv["$return"] = NoneType
    return_type = NoneType

  return TypedFn(
    name = names.refresh(untyped_fn.name),
    body = body,
    arg_names = arg_names,
    input_types = input_types,
    return_type = return_type,
    type_env = tenv)

def _specialize(fn, arg_types, return_type = None):
  """
  Do the actual work of type specialization, whereas the wrapper 'specialize'
  pulls out untyped functions from closures, wraps argument lists in ActualArgs
  objects and performs memoization
  """
  if fn.__class__ is TypedFn:
    return fn
  typed_fundef = infer_types(fn, arg_types)
  from rewrite_typed import rewrite_typed
  return rewrite_typed(typed_fundef, return_type)

def specialize(fn, arg_types, return_type = None):
  if fn.__class__ is TypedFn:
    assert len(fn.input_types) == len(arg_types)
    assert all(t1 == t2 for t1,t2 in izip(fn.input_types, arg_types))
    if return_type is not None: assert fn.return_type == return_type 
    return fn
  elif isinstance(arg_types, (list, tuple)):
    arg_types = ActualArgs(arg_types)
    
  closure_t = _get_closure_type(fn)
  key = arg_types, return_type
  
  if key in closure_t.specializations:
    return closure_t.specializations[key]

  if config.print_before_specialization:
    if return_type:
      print "==== Specializing", fn, "for input types", arg_types, "and return type", return_type
    else:  
      print "=== Specializing", fn, "for types", arg_types 
  
  full_arg_types = arg_types.prepend_positional(closure_t.arg_types)
  fundef = _get_fundef(closure_t.fn)
  typed =  _specialize(fundef, full_arg_types, return_type)
  closure_t.specializations[key] = typed

  if config.print_specialized_function:
    if return_type:
      print "=== Specialized %s for input types %s and return type %s ==="  % \
          (fundef.name, full_arg_types, return_type)
    else:
      print "=== Specialized %s for input types %s ==="  % \
          (fundef.name, full_arg_types)
    
    print
    print repr(typed)
    print

  return typed

def infer_return_type(untyped, arg_types):
  """
  Given a function definition and some input types, gives back the return type
  and implicitly generates a specialized version of the function.
  """
  return specialize(untyped, arg_types).return_type

def specialize_IndexMap(fn, n_indices):
  idx_type = make_tuple_type( (Int64,) * n_indices) if n_indices > 1 else Int64
  typed_fn = specialize(fn, (idx_type,))
  result_type = array_type.increase_rank(typed_fn.return_type, n_indices)
  return result_type, typed_fn

def specialize_IndexReduce(fn, combine, n_indices, init = None):
  idx_type = make_tuple_type( (Int64,) * n_indices) if n_indices > 1 else Int64
  if init is None or init.type.__class__ is  NoneT:
    typed_fn = specialize(fn, (idx_type,))
  else:
    typed_fn = specialize(fn, (idx_type,), return_type = init.type)
  elt_type = typed_fn.return_type
  typed_combine = specialize(combine, (elt_type, elt_type))
  return elt_type, typed_fn, typed_combine 

def peel_adverb_input_types(array_types, axes):
  assert len(array_types) == len(axes), \
    "Mismatch between types %s and axes %s" % (array_types, axes)
  result = []
  for t, axis in zip(array_types, axes):
    if axis is None:
      result.append(array_type.elt_type(t))
    else:
      result.append(array_type.lower_rank(t,1))
  return tuple(result)

def rank(t):
  if t.__class__ is ArrayT:
    return t.rank 
  else:
    return 0 

def increase_adverb_output_rank(array_types, axes, elt_result_type, cartesian_product = False):
  delta_rank = 0
  assert len(array_types) == len(axes), \
    "Mismatch between types %s and axes %s" % (array_types, axes)
  for (t,axis) in zip(array_types, axes):
    if axis is None: r = rank(t)
    else: r = 1
    if cartesian_product: delta_rank += r 
    else: delta_rank = max(r, delta_rank)
  return array_type.increase_rank(elt_result_type, delta_rank)

def specialize_Map(map_fn, array_types, axes, return_type = None):
  elt_types = peel_adverb_input_types(array_types, axes)
  typed_map_fn = specialize(map_fn, elt_types, return_type = return_type)
  elt_result_t = typed_map_fn.return_type
  result_t = increase_adverb_output_rank(array_types, axes, elt_result_t)
  return result_t, typed_map_fn

def specialize_Reduce(map_fn, combine_fn, array_types, axes, init_type = None):
  _, typed_map_fn = specialize_Map(map_fn, array_types, axes)
  elt_type = typed_map_fn.return_type
  
  if init_type is None or init_type.__class__ is NoneT:
    acc_type = elt_type
  else:
    acc_type = init_type
  typed_combine_fn = specialize(combine_fn, [acc_type, elt_type])
  new_acc_type = typed_combine_fn.return_type
  if new_acc_type != acc_type or typed_map_fn.return_type != new_acc_type:
    _, typed_map_fn = specialize_Map(map_fn, array_types, axes, return_type = new_acc_type)
    typed_combine_fn = specialize(combine_fn, [new_acc_type, new_acc_type])
    new_acc_type = typed_combine_fn.return_type
  return new_acc_type, typed_map_fn, typed_combine_fn

def specialize_Scan(map_fn, combine_fn, emit_fn, array_types, axes,  init_type = None):
  acc_type, typed_map_fn, typed_combine_fn = \
      specialize_Reduce(map_fn, combine_fn, array_types, axes, init_type)
  typed_emit_fn = specialize(emit_fn, [acc_type])
  elt_result_t = typed_emit_fn.return_type
  result_t = increase_adverb_output_rank(array_types, axes, elt_result_t)
  return result_t, typed_map_fn, typed_combine_fn, typed_emit_fn

def specialize_OuterMap(fn, array_types, axes):
  elt_types = peel_adverb_input_types(array_types, axes)
  typed_map_fn = specialize(fn, elt_types)
  elt_result_t = typed_map_fn.return_type
  result_t = increase_adverb_output_rank(array_types, axes, elt_result_t, cartesian_product = True)
  return result_t, typed_map_fn


########NEW FILE########
__FILENAME__ = var_map

from .. import names 

class VarMap:
  def __init__(self):
    self._vars = {}

  def rename(self, old_name):
    new_name = names.refresh(old_name)
    self._vars[old_name] = new_name
    return new_name

  def lookup(self, old_name):
    if old_name in self._vars:
      return self._vars[old_name]
    else:
      return self.rename(old_name)

  def __str__(self):
    return "VarMap(%s)" % self._vars

########NEW FILE########
__FILENAME__ = abstract_value
import numpy as np 

class AbstractValue(object):
  def __repr__(self):
    return str(self)
  
  def __hash__(self):
    assert False, "Hash function not implemented for %s : %s" % (self, self.__class__)
  
  def __eq__(self):
    return False 
  
  def __ne__(self, other):
    return not (self == other)

class Unknown(AbstractValue):
  def __str__(self):
    return "unknown"
  
  def __eq__(self, other):
    return other.__class__ is Unknown 
  
  def __hash__(self):
    return 107

unknown = Unknown()

class Tuple(AbstractValue):
  
  __slots__ = ['elts', '_hash']
  
  def __init__(self, elts):
    self.elts = tuple(elts)
    self._hash = hash(self.elts)
  
  def __str__(self):
    return "Tuple(%s)" % ", ".join(str(elt) for elt in self.elts)
  
  def __hash__(self):
    return self._hash 
  
  def __eq__(self, other):
    return other.__class__ is Tuple and self.elts == other.elts 
      
  
class Array(AbstractValue):
  # mark known strides with integer constants 
  # and all others as unknown
  
  __slots__ = ['strides', 'shape', 'offset' '_hash']
  
  def __init__(self, strides, shape = unknown, offset = unknown):
    self.strides = strides
    self.shape = shape
    self.offset = offset 
    self._hash = hash(self.shape) + hash(self.offset) + hash(self.strides.elts) + 1
  
  def __str__(self):
    return "Array(strides = %s)" % self.strides
  
  def __hash__(self):
    return self._hash 
  
  def __eq__(self, other):
    return other.__class__ is Array and \
      self.strides == other.strides


class Struct(AbstractValue):
  def __init__(self, fields):
    self.fields = fields 
    
  def __str__(self):
    return "Struct(%s)" % self.fields
  
  def __eq__(self, other):
    if other.__class__ != Struct:
      return False
    my_fields = set(self.fields.keys())
    other_fields = set(other.fields.keys())
    if my_fields != other_fields:
      return False 
    for f in my_fields:
      if self.fields[f] != other.fields[f]:
        return False
    return True
  
  def __hash__(self):
    return hash(tuple(self.fields.items()))

   
class Const(AbstractValue):
  __slots__ = ['value']
  def __init__(self, value):
    self.value = value
  
  def __hash__(self):
    return int(self.value) 
  
  def __str__(self):
    return str(self.value)
  
  def __eq__(self, other):
    return other.__class__ is Const and self.value == other.value

zero = Const(0)
one = Const(1)

def specialization_const(x, specialize_all = False):
  if x == 0:
    return zero 
  elif x == 1:
    return one
  elif specialize_all:
    return Const(x)
  else:
    return unknown

def abstract_tuple(elts):
  return Tuple(tuple(elts))

def abstract_array(strides):
  return Array(abstract_tuple(strides))



########NEW FILE########
__FILENAME__ = find_constant_values

import numpy as np 

from .. ndtypes import ArrayT, TupleT   
from .. syntax import TypedFn, Var
from ..analysis import SyntaxVisitor
from abstract_value import one, zero, Const, unknown, Tuple, Array, Struct, abstract_tuple, abstract_array  





def bind_list(arg_names, abstract_values):
  assert len(arg_names) == len(abstract_values)
  env = {}
  for name, abstract_value in zip(arg_names, abstract_values):
    env[name] = abstract_value
  return env 

_cache = {}
def symbolic_call(fn, symbolic_inputs):
  key = fn.cache_key, tuple(symbolic_inputs)
  if key in _cache:
    return _cache[key]
  analysis = FindConstantValues(fn, symbolic_inputs)
  return_value = analysis.visit_fn(fn)
  result = (analysis.env, return_value)
  _cache[key] = result 
  return result 

class FindConstantValues(SyntaxVisitor):
  def __init__(self, fn, abstract_values):
    self.env = bind_list(fn.arg_names, abstract_values)
    self.return_value = None 
    
  def visit_expr(self, expr):
    result = SyntaxVisitor.visit_expr(self, expr)
    if result is None: 
      return unknown
    else: 
      return result 
  
  def visit_fn(self, fn):
    SyntaxVisitor.visit_fn(self, fn)
    
    if self.return_value is None:
      return unknown 
    else:
      return self.return_value
  
  def visit_Call(self, expr):
    if expr.fn.__class__ is TypedFn: 
      args = self.visit_expr_list(expr.args)
      _, return_value = symbolic_call(expr.fn, args)
      return return_value 
    else:
      return unknown 

  def visit_Var(self, expr):
    return self.env.get(expr.name, unknown)
  
  def visit_ArrayView(self, expr):
    strides_tuple = self.visit_expr(expr.strides)
    if strides_tuple.__class__ is Const:
      strides_tuple =  abstract_tuple([strides_tuple])
    shape_tuple = self.visit_expr(expr.shape)
    if shape_tuple.__class__ is Const:
      shape_tuple = abstract_tuple([shape_tuple])
    offset = self.visit_expr(expr.offset)
    if strides_tuple.__class__ is Tuple or shape_tuple.__class__ is Tuple or offset.__class__ is Const:
      return Array(strides_tuple, shape_tuple, offset)
    else:
      return unknown 
  
  def visit_AllocArray(self, expr):
    if expr.order == "C":
      elts = (unknown,) * (expr.type.rank-1) + (one,)
      strides =  abstract_tuple(elts)
    elif expr.order == "F":
      elts = (one, ) + (unknown,) * (expr.type.rank-1) 
      strides = abstract_tuple(elts)
    else:
      strides = unknown 
    shape = self.visit_expr(expr.shape)
    offset = Const(0)
    return Array(strides, shape, offset)
  
  def visit_Const(self, expr):
    if expr.value == 0:
      return zero
    elif expr.value == 1:
      return one 
    elif isinstance(expr.value, int):
      return Const(int(expr.value))
    else:
      return unknown
  
  def visit_Attribute(self, expr):
    value = self.visit_expr(expr.value)
    if value.__class__ is Tuple:
      # assume tuples lowered into structs at this point
      pos = expr.value.type.field_pos(expr.name)
      return value.elts[pos]
    elif value.__class__ is Array and expr.name == 'strides':
      return value.strides
    elif value.__class__ is Array and expr.name == 'shape':
      return value.shape
    elif value.__class__ is Array and expr.name == 'offset':
      return value.offset
    
    elif value.__class__ is Struct and expr.name in value.fields:
      return value.fields[expr.name]
    else:
      return unknown

  def visit_Tuple(self, expr):
    return abstract_tuple(self.visit_expr_list(expr.elts))
  
  def visit_TupleProj(self, expr):
    value = self.visit_expr(expr.tuple)
    if isinstance(value, Tuple):
      return value.elts[expr.index]
    else:
      return unknown 
    
  def visit_PrimCall(self, expr):
    abstract_values = self.visit_expr_list(expr.args)
    if all(v.__class__ is Const for v in abstract_values):
      ints = [v.value for v in abstract_values]
      return Const(expr.prim.fn(*ints))
    else:
      return unknown

  def visit_Struct(self, expr):
    if expr.type.__class__ is TupleT:
      return abstract_tuple(self.visit_expr_list(expr.args))
    elif expr.type.__class__ is ArrayT:
      stride_pos = expr.type.field_pos("strides")
      stride_arg = expr.args[stride_pos]
      stride_val = self.visit_expr(stride_arg)
      return Array(stride_val)
    else:
      return unknown   
    
  def visit_Alloc(self, expr):
    return unknown 
  
  def visit_Index(self, expr):
    return unknown 
  
  def visit_Assign(self, stmt):
    if stmt.lhs.__class__ is Var:
      value = self.visit_expr(stmt.rhs)
      assert value is not None, \
        "%s : %s returned None in ConstantStride analysis" % (stmt.rhs, stmt.rhs.node_type()) 
      self.env[stmt.lhs.name] = value 
      
  def visit_Return(self, stmt):
    value = self.visit_expr(stmt.value)
    if self.return_value is None:
      self.return_value = value 
    elif self.return_value != value:
      self.return_value = unknown 
########NEW FILE########
__FILENAME__ = value_specialization
import numpy as np 
from numpy import ndarray 

from .. import syntax 
from .. syntax.helpers import const 
from ..transforms  import Transform, Simplify, Phase, DCE 


from find_constant_values import symbolic_call

from abstract_value import ( 
   Const, Array, Struct, Tuple, 
   specialization_const, abstract_tuple, abstract_array, 
   unknown, zero, one 
)

class ValueSpecializer(Transform):
  def __init__(self, abstract_inputs):
    Transform.__init__(self)
    self.abstract_inputs = abstract_inputs 
  
  def pre_apply(self, fn):
    env, _ = symbolic_call(fn, self.abstract_inputs)
    self.env = env 
  
  def lookup_expr(self, expr):
    c = expr.__class__ 
    if c is syntax.Var:
      return self.env.get(expr.name, unknown)
    else:
      return unknown 
    
  def lookup_expr_list(self, exprs):
    return [self.lookup_expr(e) for e in exprs]
  
  def specialize_closure(self, clos):
    fn = self.get_fn(clos)
    closure_elts = self.closure_elts(clos)
    abstract_values = self.lookup_expr_list(closure_elts)
    # all the index arguments have to be marked unknown 
    for _ in xrange(len(abstract_values), len(fn.input_types)):
      abstract_values.append(unknown)
    specialized_fn = specialize_abstract_values(fn, tuple(abstract_values))
    return self.closure(specialized_fn, closure_elts)
  
  def transform_IndexReduce(self, stmt):
    stmt.init = self.transform_if_expr(stmt.init)
    stmt.fn = self.specialize_closure(stmt.fn)
    stmt.combine = self.specialize_closure(stmt.combine)
    stmt.shape = self.transform_expr(stmt.shape)
    return stmt 
  
  def transform_IndexScan(self, stmt):
    stmt.init = self.transform_if_expr(stmt.init)
    stmt.fn = self.specialize_closure(stmt.fn)
    stmt.combine = self.specialize_closure(stmt.combine)
    stmt.emit = self.specialize_closure(stmt.emit)
    stmt.shape = self.transform_expr(stmt.shape)
    return stmt 
  
  def transform_ParFor(self, stmt):
    stmt.fn = self.specialize_closure(stmt.fn)
    stmt.bounds = self.transform_expr(stmt.bounds)
    return stmt 
    
  def transform_Var(self, expr):
    if expr.name in self.env:
      abstract_value = self.env[expr.name]
      if abstract_value.__class__ is Const:
        return syntax.Const(value = abstract_value.value, type = expr.type)
    return expr
  
  def transform_lhs(self, lhs):
    return lhs
  
def has_small_const(abstract_value):
  c = abstract_value.__class__
  
  if c is Const:
    return abstract_value.value in (0,1)
  elif c is Array:
    return has_small_const(abstract_value.strides)
  elif c is Tuple:
    return any(has_small_const(elt) 
               for elt in abstract_value.elts)
  elif c is Struct:
    return any(has_small_const(field_val) 
               for field_val 
               in abstract_value.fields.itervalues())
  else:
    return False


def from_python(python_value):
  t = type(python_value)
  if t is ndarray:
    elt_size = python_value.dtype.itemsize 
    strides = []
    for s in python_value.strides:
      strides.append(specialization_const(s/elt_size))
    strides = abstract_tuple(strides)
    shape = abstract_tuple([specialization_const(dim) for dim in python_value.shape])
    return Array(strides, shape)
  elif t is tuple:
    return abstract_tuple(from_python_list(python_value))
  elif python_value == 0:
    return zero 
  elif python_value == 1:
    return one 
  else:
    return unknown 
    
  
def from_python_list(python_values):
  return tuple([from_python(v) for v in python_values]) 

_cache = {}
def specialize_abstract_values(fn, abstract_values):
  key = (fn.cache_key, abstract_values)
  if key in _cache:
    return _cache[key]
  if any(has_small_const(v) for v in abstract_values):
    specializer = ValueSpecializer(abstract_values)
    transforms = Phase([specializer, Simplify, DCE],
                        memoize = False, 
                        copy = True, 
                        name = "StrideSpecialization for %s" % (abstract_values,), 
                        recursive = False)
    new_fn = transforms.apply(fn)
  else:
    new_fn = fn
  _cache[key] = new_fn
  return new_fn


def specialize(fn, python_values):
  return specialize_abstract_values(fn,  from_python_list(python_values))
  
########NEW FILE########
__FILENAME__ = test_allpairs
import numpy as np 
import parakeet 
from parakeet.testing_helpers import expect, run_local_tests, eq, expect_allpairs 

int_mat = np.reshape(np.arange(9), (3,3))
float_mat = np.sqrt(int_mat)
bool_mat = int_mat % 2
matrices = [int_mat, float_mat, bool_mat]
vecs = [m[0] for m in matrices]

def dot(x,y):
  return sum(x*y)

def test_dot():
  expect_allpairs(dot, np.dot, vecs)

def adverb_matmult(X,Y):
  return parakeet.allpairs(dot, X, Y)


def test_adverb_matmult():
    expect_allpairs(adverb_matmult, lambda x, y: np.dot(x, y.T), matrices)

def allpairs_elt_diff(x,y):
    return parakeet.allpairs(lambda xi,yi: xi - yi, x, y)

def test_allpairs_elt_diff():
    def python_impl(x,y):
      nx = len(x)
      ny = len(y)
      result = np.zeros(shape = (nx,ny), dtype=(x[0]-y[0]).dtype)
      for i in xrange(nx):
          for j in xrange(ny):
              result[i,j] = x[i] - y[j]
      return result 
    expect_allpairs(allpairs_elt_diff, python_impl, vecs)


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_imap
import numpy as np
import parakeet
from parakeet import jit 
from parakeet.testing_helpers import expect, run_local_tests, eq


@jit 
def fill_with_const(a, k):
  def return_const(idx):
    return k
  return parakeet.imap(return_const, a.shape)
  
def test_fill_with_const():
  shape=(1,2,1,1)
  a = np.empty(shape=shape, dtype='float32')
  k = 3.137
  expected = np.ones(shape = shape, dtype='float32') * k
  expect(fill_with_const, [a, k], expected)


def test_identity():
  x = np.ones((1,1,2,1))
  def get_idx(idx):
    return x[idx]
  y = parakeet.imap(get_idx, x.shape)
  assert eq(x,y)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_map
import numpy as np

from parakeet import each
from parakeet.testing_helpers import (run_local_tests, expect, expect_each, eq, expect_allpairs)

ints_1d = np.arange(300, dtype='int')
floats_1d = np.arange(300, dtype='float')

ints_2d = np.reshape(ints_1d, (25,12))
floats_2d = np.reshape(ints_2d, (25,12))

bools_1d = ints_1d % 2 == 0
bools_2d = ints_2d % 2 == 0

vecs = [ints_1d, floats_1d, bools_1d]
matrices = [ints_2d, floats_2d, bools_2d]
all_arrays = vecs + matrices

def add1_scalar(x):
  return x+1

def test_add1_external_map():
  parakeet_result = each(add1_scalar, ints_1d)
  python_result = ints_1d + 1
  assert eq(parakeet_result, python_result), \
         "Python %s != Parakeet %s" % (python_result, parakeet_result)

def add1_map(x_vec):
  return each(add1_scalar, x_vec)

def test_add1_internal_map_vecs():
  expect_each(add1_map, add1_scalar, vecs)

def test_add1_internal_map_matrices():
  expect_each(add1_map, add1_scalar, matrices)

def add(x,y):
  return x + y

def test_implicit_add_vec():
  expect_allpairs(add, np.add, vecs)

def test_implicit_add_mat():
  expect_allpairs(add, np.add, matrices)

def each_add(x,y):
  return each(add, x, y)

def test_explicit_add_vec():
  expect_allpairs(each_add, np.add, vecs)

def test_explicit_add_mat():
  expect_allpairs(each_add, np.add, matrices)

def conditional_div(x,y):
  if y == 0:
    return 0
  else:
    return x / y

def python_conditional_div(x,y):
  result = [xi / yi if yi != 0 else 0 for (xi,yi) in zip(x,y)]
  return np.array(result)

def each_conditional_div(x,y):
  return each(conditional_div, x, y)

def test_conditional_div():
    expect_allpairs(each_conditional_div, python_conditional_div,
                    [ints_1d, floats_1d])

def second_elt(x):
  return x[1]

X = np.array([[1,2,3],[4,5,6]])

def second_of_columns(X):
  return each(second_elt, X, axis=1)

def test_second_of_columns():
  expect(second_of_columns, [X], np.array([4,5,6]))

def second_of_rows(X):
  return each(second_elt, X, axis=0)

def test_second_of_rows():
  expect(second_of_rows, [X], np.array([2,5]))

def nested_each(x):
  def dummy(x):
    def dummy2():
      return x
    return dummy2()
  return each(dummy, x)

def test_nested_each():
  expect(nested_each, [X], X)

def ident(x):
  return x

def each_ident(x):
  return each(ident, x)

def each_each_ident(x):
  return each(each_ident, x)

def test_map_map():
  expect(each_each_ident, [X], X)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_outer_prod
import numpy as np
import time

from parakeet import allpairs, multiply 
from parakeet.testing_helpers import expect, expect_allpairs, run_local_tests

bool_vec = np.array([True, False, ])
int_vec = np.array([1,2,])
float_vec = np.array([10.0, 20.0, ])

vectors = [bool_vec, int_vec, float_vec]

def loop_outer_prod(x,y,z):
  nx = x.shape[0]
  ny = y.shape[0]
  for i in xrange(0, nx):
    for j in xrange(0, ny): 
      z[i,j] = x[i] * y[j]
  return z

def test_loop_outer_prod():
  for v1 in vectors:
    for v2 in vectors:
      res = np.outer(v1, v2)
      v3 = np.zeros_like(res)
      expect(loop_outer_prod, [v1, v2, v3], res)

def adverb_outer_prod(x,y):
  return allpairs(multiply, x, y)

def test_adverb_outer_prod():
  expect_allpairs(adverb_outer_prod, np.multiply.outer, vectors)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_par_each
import numpy as np
from parakeet import each, testing_helpers

def add1(xi):
  return xi + 1

def add11d(x):
  return each(add1, x)

int_vec = np.arange(128, dtype=np.float).reshape(16,8)

def test_add1():
  result = each(add11d, int_vec)
  expected = int_vec + 1
  assert testing_helpers.eq(result, expected), \
      "Expected %s, got %s" % (expected, result)

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_reduce
import numpy as np
import parakeet
from parakeet import testing_helpers 

int_vec = 300 + np.arange(300, dtype=int)
float_vec = int_vec.astype(float)
bool_vec = float_vec < np.mean(float_vec)

a = np.arange(500, dtype=float).reshape(50,10)
b = np.arange(100,600).reshape(50,10)

def my_sum(xs):
  return parakeet.reduce(parakeet.add, xs, init=0)

def test_int_sum():
  testing_helpers.expect(my_sum, [int_vec], np.sum(int_vec))

def test_float_sum():
  testing_helpers.expect(my_sum, [float_vec], np.sum(float_vec))

def test_bool_sum():
  testing_helpers.expect(my_sum, [bool_vec], np.sum(bool_vec))

def sqr_dist(y, x):
  return sum((x-y)*(x-y))

def test_sqr_dist():
  z = a[0]
  def run_sqr_dist(c):
    return sqr_dist(z, c)
  par_rslt = parakeet.each(run_sqr_dist, a)
  py_rslt = np.array(map(run_sqr_dist, a))
  assert testing_helpers.eq(par_rslt, py_rslt), \
      "Expected %s but got %s" % (py_rslt, par_rslt)

def reduce_2d(Ys):
  init = parakeet.zeros_like(Ys[0])
  return parakeet.reduce(parakeet.add, Ys, init = init, axis = 0)

def test_2d_reduce():
  par_rslt = reduce_2d(a)
  np_rslt = np.sum(a, 0)
  assert testing_helpers.eq(par_rslt, np_rslt), \
    "Expected %s but got %s" % (np_rslt, par_rslt)

def avg_along_axis_0(Xs):
  assign = np.array([0,0,1,0,1,0,1,0,1,1])
  Ys = Xs[assign == 1]

  def zero(x):
    return 0.0
  zeros = parakeet.each(zero, Xs[0])
  s = reduce(parakeet.add, Ys, init=zeros)
  def d(s):
    return s / Ys.shape[0]
  return parakeet.each(d, s)

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_scan
import numpy as np

from parakeet import scan, add
from parakeet.testing_helpers import run_local_tests, expect, expect_each

int_1d = np.arange(5)
float_1d = np.arange(5, dtype='float')
int_2d = np.array([int_1d, int_1d, int_1d])
float_2d = np.array([float_1d, float_1d, float_1d])

def running_sum(x):
  return scan(add, x, init = 0)

def test_scan_add_1d():
  expect_each(running_sum, np.cumsum, [int_1d, float_1d])

def loop_row_sums(x):
  n_rows = x.shape[0]
  y = np.zeros_like(x)
  y[0, :] = x[0, :]
  for i in xrange(1,n_rows):
    y[i, :] = y[i-1, :] + x[i, :]
  return y
"""
def test_scan_add_2d():
  expect_each(running_sum, loop_row_sums, [int_2d, float_2d])
"""
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_2d_diff

import numpy as np 
from parakeet.testing_helpers import expect_each, run_local_tests

size = (5,5)
float_mat = np.random.uniform(0,1,size=size)
int_mat = np.random.random_integers(0,255,size=size)

matrices = [float_mat, int_mat]

def diff_x(I):
  m = I.shape[0]
  return (I[1:, :] - I[:m-1, :])[:, 1:]

def test_diff_x():
  expect_each(diff_x, diff_x, matrices)

def diff_y(I):
  n = I.shape[1]
  return (I[:, 1:] - I[:, :n-1])[1:, :]

def test_diff_y():
  expect_each(diff_x, diff_x, matrices)

if __name__ == "__main__":
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_arc_distance
"""
Authors: Federico Vaggi
License: MIT
Source: https://bitbucket.org/FedericoV/numpy-tip-complex-modeling/

Source: https://github.com/numfocus/python-benchmarks/blob/master/arc_distance/arc_distance_python.py
"""

from parakeet import jit, testing_helpers
import numpy as np
from math import * 
n = 10 

a = np.random.rand(n, 2)
b = np.random.rand(n, 2)


def arc_distance_for_loops(a, b):
    """
    Calculates the pairwise arc distance between all points in vector a and b.
    """
    a_nrows = a.shape[0]
    b_nrows = b.shape[0]

    distance_matrix = np.zeros([a_nrows, b_nrows])

    for i in range(a_nrows):
        theta_1 = a[i, 0]
        phi_1 = a[i, 1]
        for j in range(b_nrows):
            theta_2 = b[j, 0]
            phi_2 = b[j, 1]
            temp = (pow(sin((theta_2 - theta_1) / 2), 2)
                    +
                    cos(theta_1) * cos(theta_2)
                    * pow(sin((phi_2 - phi_1) / 2), 2))
            distance_matrix[i, j] = 2 * (atan2(sqrt(temp), sqrt(1 - temp)))
    return distance_matrix

def test_arc_distance_for_loops():
  testing_helpers.expect(arc_distance_for_loops, [a,b], arc_distance_for_loops(a,b))

"""
def arc_distance_tile(a, b):
    "Calculates the pairwise arc distance between all points in vector a and b."
    theta_1 = np.tile(a[:, 0], (b.shape[0], 1)).T
    phi_1 = np.tile(a[:, 1], (b.shape[0], 1)).T

    theta_2 = np.tile(b[:, 0], (a.shape[0], 1))
    phi_2 = np.tile(b[:, 1], (a.shape[0], 1))

    temp = (np.sin((theta_2 - theta_1) / 2)**2
            +
            np.cos(theta_1) * np.cos(theta_2)
            * np.sin((phi_2 - phi_1) / 2)**2)
    distance_matrix = 2 * (np.arctan2(np.sqrt(temp), np.sqrt(1 - temp)))

    return distance_matrix

def test_arc_distance_tile():
  testing_helpers.expect(arc_distance_tile, [a,b], arc_distance_tile(a,b))

"""

"""
# data-dependent broadcasting will never work with static
# map insertion so might as well drop this test since 
# it's hopeless
def arc_distance_broadcast(a, b):
    "Calculates the pairwise arc distance between all points in vector a and b."
    theta_1 = a[:, 0][:, None]
    theta_2 = b[:, 0][None, :]
    phi_1 = a[:, 1][:, None]
    phi_2 = b[:, 1][None, :]

    temp = (np.sin((theta_2 - theta_1) / 2)**2
            +
            np.cos(theta_1) * np.cos(theta_2)
            * np.sin((phi_2 - phi_1) / 2)**2)
    distance_matrix = 2 * (np.arctan2(np.sqrt(temp), np.sqrt(1 - temp)))
    return distance_matrix

def test_arc_distance_broadcast():
   testing_helpers.expect(arc_distance_broadcast, [a,b], arc_distance_broadcast(a,b))
"""

if __name__ == "__main__":
  testing_helpers.run_local_tests()
  

########NEW FILE########
__FILENAME__ = test_black_scholes
from parakeet import jit 
from parakeet.testing_helpers import eq, run_local_tests, expect 

from numpy import exp, log, sqrt


def CND(x):
  a1 = 0.31938153
  a2 = -0.356563782
  a3 = 1.781477937
  a4 = -1.821255978
  a5 = 1.330274429
  L = abs(x)
  K = 1.0 / (1.0 + 0.2316419 * L)
  w = 1.0 - 1.0/sqrt(2*3.141592653589793)* exp(-1*L*L/2.) * (a1*K +
      a2*K*K + a3*K*K*K + a4*K*K*K*K + a5*K*K*K*K*K)
  if x<0:
    w = 1.0-w
  return w

def black_scholes(CallFlag,S,X,T,r,v):
  d1 = ((r+v*v/2.)*T+log(S/X))/(v*sqrt(T))
  d2 = d1-v*sqrt(T)
  z = exp(-1.0*r*T) * X
  if CallFlag:
    return S*CND(d1) - z*CND(d2)
  else:
    return z*CND(-1.0*d2) - S*CND(-1.0*d1)

black_scholes_parakeet = jit(black_scholes)

def test_black_scholes():
  x1 = (False, 10.0, 10.0, 2.0, 2.0, 2.0)
  x2 = (True, 10.0, 10.0, 2.0, 2.0, 2.0)
  xs = [x1, x2]
  for x in xs:
    expect(black_scholes, x, black_scholes(*x))

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_conv
import numpy as np 
from parakeet.testing_helpers import expect, run_local_tests

# Simple convolution of 3x3 patches from a given array x
# by a 3x3 array of filter weights
 
def conv_3x3_trim(x, weights):
  return np.array([[(x[i-1:i+2, j-1:j+2]*weights).sum() 
                    for j in xrange(1, x.shape[1] -1)]
                    for i in xrange(1, x.shape[0] -1)])
 
x = np.random.randn(4,4)
w = np.random.randn(3,3)
"""
def test_conv_float32():
    x_f32 = x.astype('float32')
    w_f32 = w.astype('float32')
    expect(conv_3x3_trim, [x_f32, w_f32], conv_3x3_trim(x_f32,w_f32))

def test_conv_int16():
    x_i16 = x.astype('int16')
    w_i16 = w.astype('int16')
    expect(conv_3x3_trim, [x_i16, w_i16], conv_3x3_trim(x_i16,w_i16))

def test_conv_bool():
    xb = x > 0
    wb = w > 0
    expect(conv_3x3_trim, [xb, wb], conv_3x3_trim(xb,wb))
"""
def conv_3x3_trim_loops(image, weights):
  result = np.zeros_like(image[1:-1, 1:-1])
  m,n = image.shape 
  for i in xrange(1,m-1):
    for j in xrange(1,n-1):
      for ii in xrange(3): 
        for jj in xrange(3):
          result[i-1,j-1] += image[i-ii+1, j-jj+1] * weights[ii, jj] 
  return result

def test_conv_loops_float32():
    x_f32 = x.astype('float32')
    w_f32 = w.astype('float32')
   
    expect(conv_3x3_trim_loops, [x_f32, w_f32], conv_3x3_trim_loops(x_f32,w_f32))

def test_conv_loops_int16():
    x_i16 = x.astype('int16')
    w_i16 = w.astype('int16')
    expect(conv_3x3_trim_loops, [x_i16, w_i16], conv_3x3_trim_loops(x_i16,w_i16))

def test_conv_loops_bool():
    xb = x > 0
    wb = w > 0
    expect(conv_3x3_trim_loops, [xb, wb], conv_3x3_trim_loops(xb,wb))

if __name__ == "__main__":
    run_local_tests()

########NEW FILE########
__FILENAME__ = test_diffuse
import numpy as np 
from parakeet import testing_helpers




mu = 0.1
Lx, Ly = 7, 7

def diffuse_loops(iter_num):
    u = np.zeros((Lx, Ly), dtype=np.float64)
    temp_u = np.zeros_like(u)
    temp_u[Lx / 2, Ly / 2] = 1000.0
    for _ in range(iter_num):
        for i in range(1, Lx - 1):
            for j in range(1, Ly - 1):
                u[i, j] = mu * (temp_u[i + 1, j] + temp_u[i - 1, j] +
                                temp_u[i, j + 1] + temp_u[i, j - 1] -
                                4 * temp_u[i, j])
        temp = u
        u = temp_u
        temp_u = temp
    return u

def test_diffuse_loops():
  testing_helpers.expect(diffuse_loops, [3], diffuse_loops(3))


def diffuse_array_expressions(iter_num):
    u = np.zeros((Lx, Ly), dtype=np.float64)
    temp_u = np.zeros_like(u)
    temp_u[Lx / 2, Ly / 2] = 1000.0

    for _ in range(iter_num):
        u[1:-1, 1:-1] = mu * (temp_u[2:, 1:-1] + temp_u[:-2, 1:-1] +
                              temp_u[1:-1, 2:] + temp_u[1:-1, :-2] -
                              4 * temp_u[1:-1, 1:-1])
        temp = u
        u = temp_u
        temp_u = temp
    return u

def test_diffuse_array_expressions():
  testing_helpers.expect(diffuse_array_expressions, [2], diffuse_array_expressions(2))

if __name__ == "__main__":
  testing_helpers.run_local_tests()


########NEW FILE########
__FILENAME__ = test_dist
import numpy as np
import time 

import parakeet 
from parakeet import testing_helpers

def sqr_dist(x,y):
  return sum( (x-y) ** 2)

def allpairs_dist_adverb(X,Y):
  return parakeet.allpairs(sqr_dist, X, Y)

def allpairs_dist_comprehensions_external(X,Y):
  return np.array([[sqr_dist(x,y) for y in Y] for x in X])

def allpairs_dist_comprehensions_internal(X,Y):
  def local_sqr_dist(x,y):
    return np.sum( (x-y)**2 )
  return np.array([[local_sqr_dist(x,y) for y in Y] for x in X])

m = 2
n = 3
d = 5 
X = np.random.randn(m,d)
Y = np.random.randn(n,d)
python_dists = np.array([[sqr_dist(x,y) for y in Y] for x in X])

def test_dists_adverb():
  testing_helpers.expect(allpairs_dist_adverb, [X,Y], python_dists)

def test_dists_comprehensions_external():
  testing_helpers.expect(allpairs_dist_comprehensions_external, [X,Y], python_dists)
  
def test_dists_comprehensions_internal():
  testing_helpers.expect(allpairs_dist_comprehensions_external, [X,Y], python_dists)

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_dot
import numpy as np
from parakeet.testing_helpers import  expect_allpairs, run_local_tests

bool_vec = np.array([True, False, True, ])
int_vec = np.array([1,2,3,])
float_vec = np.array([10.0, 20.0, 30.0 ])

vectors = [bool_vec, int_vec, float_vec]

def loop_dot(x,y):
  n = x.shape[0]
  result = x[0] * y[0]
  i = 1
  while i < n:
    result += x[i] * y[i]
    i = i + 1
  return result

def test_loopdot():
  expect_allpairs(loop_dot, np.dot, vectors)

def dot(x,y):
  return sum(x*y)

def test_adverb_dot():
  expect_allpairs(dot, lambda x,y: np.sum(x*y), vectors)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_find_border
import numpy as np
import parakeet 
from parakeet.testing_helpers import expect, run_local_tests

parakeet.config.print_untyped_function = True
parakeet.config.print_specialized_function = True

def find_border(is_background):
  h, w = is_background.shape[:2]

  top, left, right, bottom = (-1, -1, -1, -1)
  # find top
  for i in range(h):
    if is_background[i, :].sum() != w and top == -1:
      top = i

  for i in range(h - 1, 0, -1):
    if is_background[i, :].sum() != w and bottom == -1:
      bottom = i

  for i in range(w):
    if is_background[:, i].sum() != h and left == -1:
      left = i

  for i in range(w - 1, 0, -1):
    if is_background[:, i].sum() != h and right == -1:
      right = i

  return top, left, right, bottom

def test_find_border():
  is_background = np.empty((20, 20), dtype=np.bool)
  expect(find_border, [is_background], find_border(is_background))


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_floyd_warshall
import numpy as np

import parakeet
from parakeet import jit, testing_helpers 

def init(W):
  C = W.copy()
  #if E is not None:
  #  C[~E] = np.inf
  for i in xrange(C.shape[0]):
    C[i,i] = 0
  return C

def loop_dists(W):
  C = init(W)
  m,n = C.shape
  #assert m == n
  for k in range(n):
    for i in range(n):
      for j in range(n):
        C[i,j] = min(C[i,j], C[i,k] + C[k,j])
  return C

"""
def outer_dists(W,E = None):
  C = init(W,E)
  m,n = C.shape
  assert m == n
  vertices = np.arange(n)
  for k in vertices:
    from_k = C[:, k]
    to_k = C[k, :]
    A = np.add.outer(from_k, to_k)
    mask = (A<=C)
    C = mask * A + (1-mask) * C
    A = np.add.outer(from_k, to_k)
    mask = (A<=C)
    C = mask * A + ~mask * C
  return C
"""

"""
def np_dists(W, E = None):
  C = init(W,E)
  n = C.shape[0]
  for k in range(n):
    C = np.minimum(C, np.tile(C[:,k].reshape(-1,1),(1,n)) + np.tile(C[k,:],(n,1)))
  return C
"""
"""
def parakeet_dists(W, E = None):
  C = init(W,E)
  m,n = C.shape
  # assert m == n
  vertices = np.arange(n)
  def min_row(c_row, a_row):
    return map(min, c_row, a_row)
  
  for k in vertices:
    from_k = C[:, k]
    to_k = C[k, :]
    A = parakeet.allpairs(parakeet.add, from_k, to_k)
    C = parakeet.each(min_row, C, A)
  return C
"""
def test_dists():
  n = 10
  W = (np.random.randn(n,n)*2)**2

  testing_helpers.expect(loop_dists, [W], loop_dists(W))  
  """
  C1 = outer_dists(W)
  C2 = np_dists(W)
  assert np.all(np.ravel(C1 == C2)), \
      "C2 wrong: Expected %s but got %s" % (C1, C2)
      
  C3 = parakeet_dists(W)
  assert np.all(np.ravel(C1 == C3)), \
      "Parakeet wrong: Expected %s but got %s" % (C1, C3)
  """
if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_growcut

# Authors: Nathan Faggian, Stefan van der Walt, Aron Ahmadia, Olivier Grisel
# https://github.com/stefanv/growcut_py

import numpy as np

def growcut_python(image, state, state_next, window_radius):
    changes = 0
    height = image.shape[0]
    width = image.shape[1]
    for j in xrange(width):
        for i in xrange(height):
            winning_colony = state[i, j, 0]
            defense_strength = state[i, j, 1]
            for jj in xrange(max(j-window_radius,0), min(j+window_radius+1, width)):
                for ii in xrange(max(i-window_radius, 0), min(i+window_radius+1, height)):
                    if ii != i or jj != j:
                        d = image[i, j, :] - image[ii, jj, :]
                        s = np.sum(d**2) 
                        gval = 1.0 - np.sqrt(s) / np.sqrt(3)
                        attack_strength = gval * state[ii, jj, 1]
                        if attack_strength > defense_strength:
                            defense_strength = attack_strength
                            winning_colony = state[ii, jj, 0]
                            changes += 1
            state_next[i, j, 0] = winning_colony
            state_next[i, j, 1] = defense_strength
    return changes

from parakeet.testing_helpers import expect, run_local_tests 

def run_growcut(N = 3, window_radius = 2):
  image = np.zeros((N, N, 3), dtype=np.double)
  state = np.zeros((N, N, 2), dtype=np.double)
  state_next = np.empty_like(state)

  # colony 1 is strength 1 at position 0,0
  # colony 0 is strength 0 at all other positions
  state[0, 0, 0] = 1
  state[0, 0, 1] = 1
  growcut_python(image, state, state_next, window_radius)
  return state_next 

def test_growcut():
  res = run_growcut()
  expect(run_growcut, [], res)


if __name__ == "__main__":
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_harris_corner
import numpy as np
import time 

import parakeet 
from parakeet.testing_helpers import expect_each, run_local_tests


size = (5,5)
float_mat = np.random.uniform(0,1,size=size)
int_mat = np.random.random_integers(0,255,size=size)

matrices = [float_mat, int_mat]

def harris(I):
  m,n = I.shape 
  dx = (I[1:, :] - I[:m-1, :])[:, 1:]
  dy = (I[:, 1:] - I[:, :n-1])[1:, :]
  #
  #   At each point we build a matrix
  #   of derivative products
  #   M =
  #   | A = dx^2     C = dx * dy |
  #   | C = dy * dx  B = dy * dy |
  #
  #   and the score at that point is:
  #      det(M) - k*trace(M)^2
  #
  A = dx * dx
  B = dy * dy
  C = dx * dy
  tr = A + B
  det = A * B - C * C
  k = 0.05
  return det - k * tr * tr


def test_harris():
  expect_each(harris, harris, matrices)


  
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_histogram_intersection
from parakeet import jit, testing_helpers
import numpy as np


 
def kernel(X):
  return np.array([[np.sum(np.minimum(x,y)) for y in X] for x in X]) 

def test_histogram_intersection_kernel():
  n,d = 3,4
  X = np.arange(n*d).reshape((n,d))
  K = kernel(X)
  testing_helpers.expect(kernel, [X], K)

if __name__ == "__main__":
  testing_helpers.run_local_tests



########NEW FILE########
__FILENAME__ = test_hyst
#
# Test for Issue #18: https://github.com/iskandr/parakeet/issues/18
#
import numpy as np 
from parakeet import jit
from parakeet.testing_helpers import expect, run_local_tests

def hyst_(mag, edge_map, labels, num_labels, high_thresh):
  for i in range(num_labels):
    if np.max(mag[labels == i]) < high_thresh:
        edge_map[labels == i] = False
  return edge_map

def hyst(n):
  mag = np.arange(n)
  labels = np.ones(n)
  labels[int(n/2)] = 0
  edge_map = np.ones(n, dtype=np.bool)
  num_labels = 2
  high_thresh = int(n/2)
  return hyst_(mag, edge_map, labels, num_labels, high_thresh)

"""
TODO: implement boolean indexing
def test_hyst():
  expect(hyst, [10], hyst(10))
"""
if __name__ == '__main__':
  run_local_tests()
  

########NEW FILE########
__FILENAME__ = test_is_prime

from parakeet.testing_helpers import expect, run_local_tests 


def is_prime(n):
  for i in xrange(2,n-1):
    if n % i == 0:
      return False 
  return True 

def test_is_prime():
  expect(is_prime, [2], True)  
  expect(is_prime, [3], True)
  expect(is_prime, [4], False)
  expect(is_prime, [5], True)
  expect(is_prime, [6], False) 

if __name__ == '__main__':
    run_local_tests()

########NEW FILE########
__FILENAME__ = test_julia
"""
Authors: Kurt W. Smith, Serge Guelton
License: MIT
Source: https://github.com/numfocus/python-benchmarks/blob/master/julia/julia_python.py
"""

import numpy as np 
from parakeet import testing_helpers

def kernel(zr, zi, cr, ci, lim, cutoff):
    ''' Computes the number of iterations `n` such that 
        |z_n| > `lim`, where `z_n = z_{n-1}**2 + c`.
    '''
    count = 0
    while ((zr*zr + zi*zi) < (lim*lim)) and count < cutoff:
        zr, zi = zr * zr - zi * zi + cr, 2 * zr * zi + ci
        count += 1
    return count

def julia_loops(cr, ci, N, bound=1.5, lim=1000., cutoff=1e6):
    ''' Pure Python calculation of the Julia set for a given `c`.  No NumPy
        array operations are used.
    '''
    julia = np.empty((N, N), dtype=np.uint32)
    grid_x = np.linspace(-bound, bound, N)
    for i, x in enumerate(grid_x):
        for j, y in enumerate(grid_x):
            julia[i,j] = kernel(x, y, cr, ci, lim, cutoff=cutoff)
    return julia

def test_julia():
  cr=0.285
  ci=0.01
  N=10
  testing_helpers.expect(julia_loops, [cr, ci, N], julia_loops(cr,ci,N))


if __name__ == "__main__":
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_kmeans
import numpy as np
import time

import parakeet
from parakeet import allpairs, each
from parakeet.testing_helpers import eq, run_local_tests

def python_update_assignments(X, centroids):
  dists = [[np.sum( (x-y) ** 2) for y in centroids] for x in X]
  return np.argmin(dists, 1)

def python_update_centroids(X, assignments, k):
  d = X.shape[1]
  new_centroids = np.zeros((k,d), dtype=X.dtype)
  for i in xrange(k):
    mask = assignments == i
    count = np.sum(mask)
    assigned_data = X[mask]
    if count > 1:
      new_centroids[i, :] = np.mean(assigned_data)
    elif count == 1:
      new_centroids[i,:] = assigned_data[0]

  return new_centroids

def python_kmeans(X, k, maxiters = 100, initial_assignments = None):
  n = X.shape[0]
  if initial_assignments is None:
    assignments = np.random.randint(0, k, size=n)
  else:
    assignments = initial_assignments
  centroids = python_update_centroids(X, assignments, k)
  for _ in xrange(maxiters):
    old_assignments = assignments

    assignments = python_update_assignments(X, centroids)

    if all(old_assignments == assignments):
      break
    centroids = python_update_centroids(X, assignments, k)

  return centroids

def sqr_dist(x,y):
  return sum((x-y) ** 2)

def parakeet_update_assignments(X, centroids):
  dists = allpairs(sqr_dist, X, centroids)

  return np.argmin(dists, 1)

def mean(X):
  return sum(X) / len(X)

def parakeet_update_centroids(X, assignments, k):
  d = X.shape[1]

  new_centroids = np.zeros((k,d), dtype=X.dtype)
  for i in xrange(k):
    mask = (assignments == i)
    count = np.sum(mask)
    assigned_data = X[mask]
    if count == 1:
      new_centroids[i, :] = assigned_data[0]
    elif count > 1:
      new_centroids[i,:] = parakeet.mean(assigned_data)
  return new_centroids

def parakeet_kmeans(X, k, maxiters = 100, initial_assignments = None):
  n = X.shape[0]
  if initial_assignments is None:
    assignments = np.random.randint(0, k, size = n)
  else:
    assignments = initial_assignments

  centroids = python_update_centroids(X, assignments, k)
  for _ in xrange(maxiters):
    old_assignments = assignments
    assignments = parakeet_update_assignments(X, centroids)
    if all(old_assignments == assignments):
      break
    centroids = python_update_centroids(X, assignments, k)

  return centroids

def test_kmeans():
  n = 200
  d = 4
  X = np.random.randn(n*d).reshape(n,d)
  k = 2
  niters = 10
  assignments = np.random.randint(0, k, size = n)
  parakeet_C = parakeet_kmeans(X, k, niters, assignments)
  python_C = python_kmeans(X, k, niters, assignments)
  assert eq(parakeet_C.shape, python_C.shape), \
      "Got back wrong shape, expect %s but got %s" % \
      (python_C.shape, parakeet_C.shape)
  assert eq(parakeet_C, python_C), \
      "Expected %s but got %s" % (python_C, parakeet_C)


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_local_maxima
import numpy as np 
import parakeet 
import parakeet.c_backend

from parakeet.testing_helpers import run_local_tests, expect 
def wrap(pos, offset, bound):
    return ( pos + offset ) % bound

def clamp(pos, offset, bound):
    return min(bound-1,max(0,pos+offset))

def reflect(pos, offset, bound):
    idx = pos+offset
    return min(2*(bound-1)-idx,max(idx,-idx))


def python_local_maxima(data, wsize, mode=wrap):
  result = np.ones(shape=data.shape,dtype='bool')
  for pos in np.ndindex(data.shape):
    myval = data[pos]  
    for offset in np.ndindex(wsize):
      neighbor_idx = tuple(mode(p, o-w/2, w) for (p, o, w) in zip(pos, offset, wsize))
      result[pos] &= (data[neighbor_idx] <= myval)
  return result 

@parakeet.jit 
def local_maxima(data, wsize, mode=wrap):
  def is_max(pos):
    def is_smaller_neighbor(offset):
      neighbor_idx = tuple(mode(p, o-w/2, w) for (p, o, w) in zip(pos, offset, wsize))
      return data[neighbor_idx] <= data[pos]
    return np.all(parakeet.imap(is_smaller_neighbor, wsize))
  return parakeet.imap(is_max, data.shape)
  

shape = (4,3,3,3)
x = np.random.randn(*shape)
wsize = (3,3,3,3)

def test_local_maxima():
  expect(local_maxima, [x, wsize], python_local_maxima(x, wsize))
  


if __name__ == '__main__':
    run_local_tests()
  

########NEW FILE########
__FILENAME__ = test_matmult_allpairs
import numpy as np

from parakeet import jit, allpairs 
from parakeet.testing_helpers import run_local_tests, expect

int_mat = np.reshape(np.arange(6), (2,3))
float_mat = np.sqrt(int_mat)
bool_mat = int_mat % 2

matrices = [int_mat, float_mat, bool_mat]
      
@jit 
def dot(x,y):
  return sum(x*y)

@jit 
def matmult_allpairs(X,Y):
  return allpairs(dot, X, Y, axis = (0,1))

def test_matmult_allpairs():
  for X in matrices:
    for Y in matrices:
      res = np.dot(X, Y.T)
      expect(matmult_allpairs, [X,Y.T], res)

      
if __name__ == "__main__":
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_matmult_comprehensions
import numpy as np

from parakeet import jit 
from parakeet.testing_helpers import run_local_tests, expect

int64_mat = np.reshape(np.arange(6), (2,3)).astype('int64')
int32_mat = int64_mat.astype('int32')

float64_mat = int64_mat.astype('float64')
float32_mat = int64_mat.astype('float32') 
bool_mat = int64_mat % 2

matrices = [int64_mat, int32_mat, float64_mat, float32_mat, bool_mat]

def matmult_comprehensions(X,Y):
  return np.array([[np.dot(x,y) for y in Y.T] for x in X])

def test_matmult_int64_int32():
  expect(matmult_comprehensions, [int64_mat, int32_mat.T], np.dot(int64_mat, int32_mat.T))

def test_matmult_int32_int32():
  expect(matmult_comprehensions, [int32_mat, int32_mat.T], np.dot(int32_mat, int32_mat.T))

def test_matmult_float64_float32():
  expect(matmult_comprehensions, [float64_mat, float32_mat.T], np.dot(float64_mat, float32_mat.T))

def test_matmult_float32_float32():
  expect(matmult_comprehensions, [float32_mat, float32_mat.T], np.dot(float32_mat, float32_mat.T))

def test_matmult_float32_int32():
  expect(matmult_comprehensions, [float32_mat, int32_mat.T], np.dot(float32_mat, int32_mat.T))

def test_matmult_float64_bool():
  expect(matmult_comprehensions, [float64_mat, bool_mat.T], np.dot(float64_mat, bool_mat.T))

def test_matmult_bool_bool():
  expect(matmult_comprehensions, [bool_mat, bool_mat.T], np.dot(bool_mat, bool_mat.T))

def test_matmult_bool_int32():
  expect(matmult_comprehensions, [bool_mat, int32_mat.T], np.dot(bool_mat, int32_mat.T))
      
if __name__ == "__main__":
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_matmult_loops
import numpy as np

from parakeet import jit
from parakeet.testing_helpers import run_local_tests, expect

int_mat = np.reshape(np.arange(6), (2,3))
float_mat = np.sqrt(int_mat)
bool_mat = int_mat % 2

matrices = [int_mat, float_mat, bool_mat]

@jit 
def mm_loops(X,Y,Z):
  m,d = X.shape
  n = Y.shape[1]
    
  for i in range(m):
      for j in range(n):
          total = 0
          for k in range(d):
              total += X[i,k] * Y[k,j]
          Z[i,j] = total 
  return Z

def test_matmult_loops():
  for X in matrices:
    for Y in matrices:
      res = np.dot(X, Y.T)
      Z = np.zeros(res.shape, dtype = res.dtype)
      expect(mm_loops, [X,Y.T,Z], res)
      
if __name__ == "__main__":
  run_local_tests()
########NEW FILE########
__FILENAME__ = test_matmult_tropical
import parakeet
import parakeet.testing_helpers
import numpy as np 


def matmult_high_level(X,Y):
  return np.array([[np.min(x+y) for y in Y.T] for x in X])

def test_matmult_tropical():
  n, d = 4,5
  m = 3
  X = np.random.randn(m,d)
  Y = np.random.randn(d,n)
  parakeet.testing_helpers.expect(matmult_high_level, [X,Y], matmult_high_level(X,Y))

if __name__ == '__main__':
    parakeet.testing_helpers.run_local_tests()



########NEW FILE########
__FILENAME__ = test_maxpool

import numpy as np 
import parakeet 
from parakeet import jit, testing_helpers

@jit
def maxpool(array, pool_size):
  n, w, h = array.shape
  def _inner(args):
    img, x, y = args
    max_val = -1e12
    for i in xrange(0, pool_size):
      for j in xrange(0, pool_size):
        if x + i < w and y + j < h:
          max_val = max(array[img, x + i, y + j])
    return max_val
  return parakeet.imap(_inner, (n, w, h))

def test_maxpool():
  x = np.array([[1,2,3],
                [4,5,6],
                [7,8,9]])
  xs = np.array([x,x])
  expected_single =  np.array([[5, 6, 6],
                               [8, 9, 9],
                               [8, 9, 9]])
  expected = np.array([expected_single, expected_single])
  testing_helpers.expect(maxpool, [xs, 2], expected)

if __name__ == "__main__":
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_nn
import parakeet as par
from parakeet import testing_helpers, jit 
import numpy as np


class Network(object):
  def __init__(self):
    self.layers = []
    
    self.start_epoch()
    
  def start_epoch(self):
    self.sse = 0
    self.n_updates = 0
     
  def mse(self):
    return self.sse / self.n_updates 
   
  def __iadd__(self, layer):
    self.layers.append(layer)
    return self
    
  def predict(self, x):
    for layer in self.layers:
      x = layer.fprop(x)
    return x
  
  def bprop(self, err):
    for layer in reversed(self.layers[1:]):
      err = layer.bprop(err)
  
  def update(self, x, y):
    y_pred = self.predict(x)
    err = y_pred - y
    self.sse += err**2 
    self.n_updates += 1
    self.bprop(err)


@jit
def sigmoid(x):
  return 1.0 / (1.0 + par.exp(-x))

@jit
def d_sigmoid(x):
  s = sigmoid(x)
  return s * (1-s) 

@jit
def dot(x,y):
  return sum(x*y)

@jit
def fprop_linear(x, W, b):
  def dot_add(w_row, b_elt):
    return sum(w_row * x) + b_elt 
  return par.each(dot_add, W, b)
 
@jit 
def fprop_logistic(x, W, b):
  return sigmoid(fprop_linear(x,W,b))

@jit
def bprop_logistic(x, W, b, err):
  return W*0.001

class LogisticLayer(object):
  def __init__(self, n_in, n_out, learning_rate = 0.001):
    self.W = np.random.randn( n_out, n_in)
    self.bias = np.random.randn(n_out)
    self.last_input = None
    self.last_output = None 
    self.learning_rate = learning_rate
    
  def __str__(self):
    return "LogisticLayer(in = %d, out = %d)" % (self.W.shape[0], self.W.shape[1])
  
  def fprop(self, x):
    self.last_input = x
    self.last_output = fprop_logistic(x, self.W, self.bias)

    return self.last_output
  
  def bprop(self, err):
    weighted_err = 0  
    return weighted_err 
    

@jit 
def tanh_fprop(x, w, b):
  return par.tanh(sum(x*w) + b)

def tanh_bprop(x, w, b, err):
  return 0

class TanhLayer(object):
  def __init__(self, n, w = None, bias = None):
    if w is None:
      w = np.random.randn(n)
    if bias is None:
      bias = np.random.randn()
    self.w = w
    self.bias = bias
    self.last_x = None
    
  def fprop(self, data):
    self.last_data = data
    self.last_output = tanh_fprop(data, self.w, self.bias)
    return self.last_output
     
  def bprop(self, err):
    self.last_delta = tanh_bprop(self.last_data, self.w, self.bias, err)
    

n_in = 1000 
n_hidden = 50
n_out = 1 

mlp = Network()
mlp += LogisticLayer(n_in, n_hidden)
mlp += LogisticLayer(n_hidden, n_out)

def test_mlp():
  x = np.random.randn(1000)
  y = mlp.predict(x)
  assert len(y) == n_out
  assert np.all(y >= 0)
  assert np.all(y <= 1)
  


if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_nn_simple
from parakeet import jit, testing_helpers 
import numpy as np 

def init_layer(input_size, n_elts):
  return np.random.randn(n_elts, input_size)

def create_mlp(n_inputs, n_hidden, n_outputs):
  W1 = init_layer(n_inputs, n_hidden)
  W2 = init_layer(n_hidden, n_outputs)
  return (W1, W2)

def dot(x,y):
  return sum(x*y)

def fprop_elt(x,w):
  return np.tanh(dot(x,w))

def fprop_layer(layer, x):
  return [fprop_elt(x,w) for w in layer]

fprop_layer_parakeet = jit(fprop_layer)

def fprop(network, x):
  for layer in network:
    x = fprop_layer_parakeet(layer, x)
  return x

def fprop_python(network, x): 
  for layer in network:
    x = np.array(fprop_layer(layer,x))
  return x

n = 1000
d = 500
Xpos = np.random.randn(n,d)+1.0
Xneg = np.random.randn(n,d)-1.0
X = np.vstack([Xpos, Xneg])
Y = np.vstack([np.ones(n), np.ones(n)*-1])

network = create_mlp(d,50,1)

def test_fprop():
  parakeet_result = fprop(network, X[0])
  print "Parakeet result", parakeet_result
  python_result = fprop_python(network, X[0])
  print "Python result", python_result
  assert testing_helpers.eq(parakeet_result, python_result)
    
if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_norm
"""
Example taken from Numba discussion list 
https://groups.google.com/a/continuum.io/forum/#!topic/numba-users/pCpNGdtKlRc
"""

import math 
import numpy as np 
from parakeet import testing_helpers

def norm(vec):
    """ Calculate the norm of a 3d vector. """
    return math.sqrt(vec[0]**2 + vec[1]**2 + vec[2]**2)

def normalize(vec):
    """ Calculate the normalized vector (norm: one). """
    return vec / norm(vec)


def test_normalize():
  v = np.array((1.0, 2.0, 3.0))
  testing_helpers.expect(normalize, [v], normalize(v))

if __name__ == "__main__":
  testing_helpers.run_local_tests()


########NEW FILE########
__FILENAME__ = test_partialDU
def gradients(d, re, preDz, preWz, SRW, RSW, yxV, xyU, resid):
    """ computes the linear algebra intensive part of the gradients of the grae
    """
    fprime = lambda x: 1 - power(tanh(x), 2)

    partialDU = zeros((d+1, re, 2*d, d))
    for k in range(2*d):
        for i in range(d):
            partialDU[:,:,k,i] = fprime(preDz[k]) * fprime(preWz[i]) * (SRW[i,k] + RSW[i,k]) * yxV[:,:,i]
    
    return partialDU

def test_gradients():
  pass 

########NEW FILE########
__FILENAME__ = test_rad2deg

from parakeet.testing_helpers import run_local_tests, expect, expect_eq

def rad2deg(rad):
  return (rad * 180) / 3.14159265359

import numpy as np 

def test_rad2deg_int32():
  rads = np.array([0,1]).astype('int32')
  degs = rad2deg(rads)
  expected = np.rad2deg(rads)
  expect_eq(degs, expected)

def test_rad2deg_bool():
  rads = np.array([False,True])
  degs = rad2deg(rads)
  expected = np.rad2deg(rads)
  expect_eq(degs, expected)

def test_rad2deg_float32():
  rads = np.array([0.1, 0.2]).astype('float32')
  degs = rad2deg(rads)
  expected = np.rad2deg(rads)
  expect_eq(degs, expected)


def test_rad2deg_float64():
  rads = np.array([0.1, 0.2]).astype('float64')
  degs = rad2deg(rads)
  expected = np.rad2deg(rads)
  expect_eq(degs, expected)

if __name__ == '__main__':
  run_local_tests()





########NEW FILE########
__FILENAME__ = test_rotate
from numpy import sin, cos, dot, array 


def rotate(phi, theta, orig): 
  """
  This function rotates the point at orig about axis u(depends on phi) by the angle theta.  
  orig and output are in Cartesian coordinates
  """
  
  u = (-sin(phi), cos(phi), 0.0)
  rotM = array([
                 [cos(theta)+u[0]**2*(1-cos(theta)), u[0]*u[1]*(1-cos(theta)), u[1]*sin(theta)],
                 [u[0]*u[1]*(1-cos(theta)), cos(theta)+u[1]**2*(1-cos(theta)), -u[0]*sin(theta)],
                 [-u[1]*sin(theta), u[0]*sin(theta), cos(theta)]
                ])
  rotP = dot(rotM,orig)
  return rotP


from parakeet import testing_helpers
import numpy as np 

def test_rotate():
  x = np.arange(3) / 2.0
  for phi in (0, 0.25):
    for theta in (0, 0.25):
      testing_helpers.expect(rotate, [phi, theta, x], rotate(phi, theta, x))

if __name__ == "__main__":
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_rule_30
import numpy as np
import parakeet

from parakeet.testing_helpers import eq, run_local_tests

size = 21
init = np.array(([0] * (size/2)) + [1] + ([0] * (size - size/2 - 1)))

def rule30(extended, i):
  a = extended[i-1]
  b = extended[i]
  c = extended[i+1]

  if ((a == 1 and b == 0 and c == 0) or
      (a == 0 and b == 1 and c == 1) or
      (a == 0 and b == 1 and c == 0) or
      (a == 0 and b == 0 and c == 1)):
    return 1
  else:
    return 0


def test_rule30():
  output = init.copy()
  cur = init
  zero_array = np.array([0])
  indices = np.arange(1,size+1)
  for _ in range(size/2):
    extended = np.concatenate((zero_array, cur, zero_array))

    def run_rule30(i):
      return rule30(extended, i)
    parakeet_iter = parakeet.map(run_rule30, indices)
    cur = np.array(map(run_rule30, indices))
    assert eq(parakeet_iter, cur), \
       "Parakeet result (%s) didn't match Python result(%s)" % \
       (parakeet_iter, cur)
    output = np.vstack((output,cur))


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_simple_regression
from parakeet.testing_helpers import expect, run_local_tests  


def covariance(x,y):
  return ((x-x.mean()) * (y-y.mean())).mean()

def fit_simple_regression(x,y):
  slope = covariance(x,y) / covariance(x,x)
  offset = y.mean() - slope * x.mean() 
  return slope, offset

import numpy as np 

def test_simple_regression():
  N = 10
  x = np.random.randn(N).astype('float64')
  slope = 903.29
  offset = 102.1
  y = slope * x + offset
  expect(fit_simple_regression, [x,y], (slope,offset))


if __name__ == '__main__':
  run_local_tests()




########NEW FILE########
__FILENAME__ = test_sph_kernel
"""
Kernel from SPH renderer, modified from Numba version at:
https://gist.github.com/rokroskar/bdcf6c6b210ff0efc738#file-gistfile1-txt-L55
"""
 
 
import numpy as np
from parakeet import testing_helpers, config 


def kernel_func(d, h) : 
    if d < 1 : 
        f = 1.-(3./2)*d**2 + (3./4.)*d**3
    elif d<2 :
        f = 0.25*(2.-d)**3
    else :
        f = 0
    return f/(np.pi*h**3)
 
 
def compute_kernel( start = 0, stop = 2.01, step = 0.01,  h = 1.0): 
    # set up the kernel values
    kernel_samples = np.arange(start, stop, step)
    return np.array([kernel_func(x, h) for x in kernel_samples])

 
print compute_kernel(0, 0.1, 0.05)

def test_kernel():
  testing_helpers.expect(compute_kernel, [0, 0.5], compute_kernel(0,0.5))

if __name__ == "__main__":
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_sph_render
"""
SPH renderer, modified from Numba version at:
https://gist.github.com/rokroskar/bdcf6c6b210ff0efc738#file-gistfile1-txt-L55
"""
 
 
import numpy as np
from numpy import int32
from parakeet import testing_helpers 



 
def kernel_func(d, h) : 
    if d < 1 : 
        f = 1.-(3./2)*d**2 + (3./4.)*d**3
    elif d<2 :
        f = 0.25*(2.-d)**3
    else :
        f = 0
    return f/(np.pi*h**3)
 
def distance(x,y,z) : 
    return np.sqrt(x**2+y**2+z**2)
 
def physical_to_pixel(xpos,xmin,dx) : 
    return int32((xpos-xmin)/dx)
 
def pixel_to_physical(xpix,x_start,dx) : 
    return dx*xpix+x_start
 
def render_image(xs, ys, zs, hs, qts, mass, rhos, nx, ny, 
                 xmin = 0.0, xmax = 1.0, ymin = 0.0, ymax = 1.0): 
    MAX_D_OVER_H = 2.0
 
    image = np.zeros((nx,ny))
 
    dx = (xmax-xmin)/nx
    dy = (ymax-ymin)/ny
 
    x_start = xmin+dx/2
    y_start = ymin+dy/2
    zplane = 0.0
 
    # set up the kernel values
    kernel_samples = np.arange(0, 2.01, .01)
    kernel_vals = np.array([kernel_func(x,1.0) for x in kernel_samples])
    for i, (x,y,z,h) in enumerate(zip(xs,ys,zs,hs)):
        qt = qts[i] * mass[i] / rhos[i]

        # is the particle in the frame?
        if ((x > xmin-2*h) and (x < xmax+2*h) and 
            (y > ymin-2*h) and (y < ymax+2*h) and 
            (np.abs(z-zplane) < 2*h)) : 
        
                    
            if (MAX_D_OVER_H*h/dx < 1 ) and (MAX_D_OVER_H*h/dy < 1) : 
                # pixel coordinates 
                xpos = physical_to_pixel(x,xmin,dx)
                ypos = physical_to_pixel(y,ymin,dy)
                # physical coordinates of pixel
                xpixel = pixel_to_physical(xpos,x_start,dx)
                ypixel = pixel_to_physical(ypos,y_start,dy)
                zpixel = zplane
 
                dxpix, dypix, dzpix = [x-xpixel,y-ypixel,z-zpixel]
                d = distance(dxpix,dypix,dzpix)
                if (xpos > 0) and (xpos < nx) and (ypos > 0) and (ypos < ny) and (d/h < 2) : 
                    kernel_val = kernel_vals[int(d/(.01*h))]/(h*h*h)
                    image[xpos,ypos] += qt*kernel_val
 
            else :
                # bottom left of pixels the particle will contribute to
                x_pix_start = int32(physical_to_pixel(x-MAX_D_OVER_H*h,xmin,dx))
                x_pix_stop  = int32(physical_to_pixel(x+MAX_D_OVER_H*h,xmin,dx))
                y_pix_start = int32(physical_to_pixel(y-MAX_D_OVER_H*h,ymin,dy))
                y_pix_stop  = int32(physical_to_pixel(y+MAX_D_OVER_H*h,ymin,dy))
            
                if(x_pix_start < 0):  x_pix_start = 0
                if(x_pix_stop  > nx): x_pix_stop  = int32(nx-1)
                if(y_pix_start < 0):  y_pix_start = 0
                if(y_pix_stop  > ny): y_pix_stop  = int32(ny-1)
    
                
                for xpix in range(x_pix_start, x_pix_stop) : 
                    for ypix in range(y_pix_start, y_pix_stop) : 
                        # physical coordinates of pixel
                        xpixel = pixel_to_physical(xpix,x_start,dx)
                        ypixel = pixel_to_physical(ypix,y_start,dy)
                        zpixel = zplane
 
                        dxpix, dypix, dzpix = [x-xpixel,y-ypixel,z-zpixel]
                        d = distance(dxpix,dypix,dzpix)
                        if (d/h < 2) : 
                            kernel_val = kernel_vals[int(d/(.01*h))]/(h*h*h)
                            image[xpix,ypix] += qt*kernel_val
    
 
    return image
 
 
def test_image_render() : 
    N = 4
    x = y = z = hs= qts = mass = rhos = np.random.rand(N)
    nx=ny=2
    args = (x,y,z,hs,qts,mass,rhos,nx,ny)
    testing_helpers.expect(render_image, args, render_image(*args))

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_sum_primes
from parakeet.testing_helpers import expect, run_local_tests 

def sum_primes(n):
  total = 0
  count = 0
  for i in xrange(2, n):
    found_divisor = False
    for j in xrange(2,i-1):
      if i % j == 0:
        found_divisor = True
    if not found_divisor:
      total += i
      count += 1
  return total, count 

def test_sum_primes():
  expect(sum_primes, [20], sum_primes(20))  

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_tanh_rescale
import numpy as np 
from parakeet import jit, testing_helpers


alpha = 0.5
beta = 0.3
x = np.array([1,2,3])
y = np.tanh(x) * alpha + beta

@jit
def numpy_expressions(x, alpha = 0.5, beta = 0.3):
  return np.tanh(x) * alpha + beta 

def test_numpy_expressions():
  res = numpy_expressions(x)
  assert np.allclose(res, y), "Expected %s but got %s" % (y, res)

@jit 
def loopy(x, alpha = 0.5, beta = 0.3):
  y = np.empty_like(x, dtype=float)
  for i in xrange(len(x)):
    y[i] = np.tanh(x[i]) * alpha + beta
  return y

def test_loopy():
  res = loopy(x)
  assert np.allclose(res, y), "Expected %s but got %s" % (y, res)

@jit
def comprehension(x, alpha = 0.5, beta = 0.3):
  return np.array([np.tanh(xi)*alpha + beta for xi in x])

def test_comprehensions():
  res = comprehension(x)
  assert np.allclose(res, y), "Expected %s but got %s" % (y, res)

if __name__ == "__main__":
  testing_helpers.run_local_tests()
  

########NEW FILE########
__FILENAME__ = test_thresholds
import numpy as np 
import time 


import parakeet
from parakeet import testing_helpers 

def count_thresh_orig(values, thresh):
  n = 0
  for elt in values:
    n += elt < thresh
  return n

count_thresh = parakeet.jit(count_thresh_orig)

def np_thresh(values, thresh):
  return np.sum(values < thresh)

def par_thresh(values, thresh):
  return parakeet.sum(values < thresh)

def test_count_thresh():
  v = np.array([1.2, 1.4, 5.0, 2, 3])
  parakeet_result = count_thresh(v, 2.0)
  python_result = count_thresh_orig(v,2.0)
  testing_helpers.expect(count_thresh, [v, 2.0], count_thresh_orig(v, 2.0))


if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_arange

import numpy as np

from parakeet import jit, config 
from parakeet.testing_helpers import expect, run_local_tests 



@jit 
def arange1(a):
  return np.arange(a)

@jit  
def arange2(a,b):
  return np.arange(a,b)

@jit  
def arange3(a,b,c):
  return np.arange(a,b,c)

def test_range1_int():
  a = 5
  expect(arange1, [a], np.arange(a))

def test_range1_float():
  a = 3.5
  expect(arange1, [a], np.arange(a))
  
def test_range2_int():
  a = 2
  b = 6
  expect(arange2, [a,b], np.arange(a,b))

def test_range2_float():
  a = 3.5
  b = 7.1
  expect(arange2, [a,b], np.arange(a,b))
  
def test_range3_int():
  a = 2
  b = 10
  c = 3
  expect(arange3, [a,b,c], np.arange(a,b,c))

def test_range3_float():
  a = 3.5
  b = 9.1
  c = 1.7
  expect(arange3, [a,b,c], np.arange(a,b,c))

def test_range3_int_reverse():
  a = 10
  b = 2
  c = -3
  expect(arange3, [a,b,c], np.arange(a,b,c))

def test_range3_float_reverse():
  a = 9.1
  b = 3.5
  c = -1.7
  expect(arange3, [a,b,c], np.arange(a,b,c))  
    


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_array_literal
import numpy as np

from parakeet.testing_helpers import expect, run_local_tests

def create_const(x):
  return [x,x,x,x]

def test_create_const():
  expect(create_const, [1],  np.array([1,1,1,1]))
  expect(create_const, [1.0], np.array([1.0, 1.0, 1.0, 1.0]))
  expect(create_const, [True], np.array([True, True, True, True]))


def test_nested_2d():
  expect(create_const, [np.array([1,2])] , np.array([[1,2],[1,2],[1,2],[1,2]]))
  expect(create_const, [np.array([1.0,2.0])] , np.array([[1.0,2.0],[1.0,2.0],[1.0,2.0],[1.0,2.0]]))
  expect(create_const, [np.array([True,False])] , 
         np.array([[True,False],[True,False],[True,False],[True,False]]))
  
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_assign_slice
import parakeet 
from  parakeet import testing_helpers
import numpy as np 

@parakeet.jit
def setidx(x, idx, y):
    x[idx] = y
    return x

def test_set_1d_simple_slice():
    x = np.array([1,2,3,4,5,6]) 
    idx = slice(2,4)
    y = [10, 20]
    x2 = x.copy()
    x[idx] = y
    testing_helpers.expect(setidx, [x2, idx, y], x)
    
def test_set_1d_simple_slice_to_const():
    x = np.array([1,2,3,4,5,6]) 
    idx = slice(2,4)
    y = 0
    x2 = x.copy()
    x[idx] = y
    testing_helpers.expect(setidx, [x2, idx, y], x)

def test_set_1d_step_slice_to_const():
    x = np.array([1,2,3,4,5,6]) 
    idx = slice(2,4,2)
    y = 0
    x2 = x.copy()
    x[idx] = y
    testing_helpers.expect(setidx, (x2, idx, y), x)

def test_set_1d_negative_slice():
    x = np.array([1,2,3,4,5,6]) 
    idx = slice(4,2,-1)
    y = [10, 20]
    x2 = x.copy()
    x[idx] = y
    testing_helpers.expect(setidx, (x2, idx, y), x)

if __name__ == '__main__':
    testing_helpers.run_local_tests()


########NEW FILE########
__FILENAME__ = test_broadcasting
import numpy as np
from parakeet import jit  
from parakeet.testing_helpers import expect_eq, run_local_tests

@jit 
def add(x,y):
  return x + y

def test_2d_scalar():
  x = np.zeros((2,3))
  res = add(x, 1)
  expect_eq(res, x + 1)

def test_2d_1d():
  x = np.zeros((2,3))
  y = np.ones((3,))
  res = add(x, y)
  expect_eq(res, x + y)

def test_2d_2d():
  x = np.zeros((2,3))
  y = np.ones((2,3))
  res = add(x, y)
  expect_eq(res, x + y)
  


if __name__ == '__main__':
  run_local_tests()
  

########NEW FILE########
__FILENAME__ = test_empty
import numpy as np
import parakeet 

from parakeet.testing_helpers import expect, run_local_tests 


def test_empty():
  s = (20, 20, 3)
  x = np.empty(s)
  assert x.shape == s
  
def test_empty_int():
  s = (2,2,2,2)
  x = np.empty(s, dtype=np.uint8)
  assert x.shape == s
  assert x.dtype == np.uint8


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_fancy_indexing
import numpy as np
from parakeet.testing_helpers import expect, run_local_tests

n = 8
vec_int = np.arange(n)
vec_bool = vec_int % 2
vec_float = vec_bool * 2.75
vectors = [vec_int, vec_bool, vec_float]

indices = np.arange(n)[vec_bool]

def idx(x,i):
  return x[i]

def test_1d_by_1d():
  for v in vectors:
    expect(idx, [v, indices], idx(v,indices))

mat_int = np.array([vec_int]*n)
mat_float = np.array([vec_float]*n)
mat_bool = np.array([vec_bool]*n)
matrices = [mat_int, mat_float, mat_bool]

def test_2d_by_idx():
  for m in matrices:
    expect(idx, [m, indices], idx(m, indices))

if __name__ == '__main__':
    run_local_tests()

########NEW FILE########
__FILENAME__ = test_indexing
import numpy as np
from parakeet.testing_helpers import expect, run_local_tests

shape_1d = 40
ints_1d = np.arange(shape_1d)
floats_1d = np.arange(shape_1d, dtype='float')
bools_1d = ints_1d % 2

vecs = [ints_1d, floats_1d, bools_1d]

shape_2d = (4,10)
matrices = [np.reshape(vec, shape_2d) for vec in vecs]

shape_3d = (4,5,2)
tensors = [np.reshape(mat, shape_3d) for mat in matrices]

def index_1d(x, i):
  return x[i]

def test_index_1d():
  for vec in vecs:
    expect(index_1d, [vec, 20], vec[20])

def index_2d(x, i, j):
  return x[i, j]

def test_index_2d():
  for mat in matrices:
    expect(index_2d, [mat, 2, 5], mat[2,5])

def index_3d(x, i, j, k):
  return x[i, j, k]

def test_index_3d():
  for x in tensors:
    expect(index_3d, [x, 2, 2, 1], x[2,2,1])




if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_methods
import numpy as np
from parakeet import testing_helpers, jit 

values = [np.array(0), np.array(1.0), np.array([True]), 
          np.array([1,2,3]), np.array([1.0, 2.0, 3.0]), np.array([True, False]),
          np.array([[1,2,3],[10,20,30]]), np.array([[1.0,2.0,3.0],[10.0,20.0, 30.0]]), 
          np.array([[True], [False]])
         ]

def run(fn, method_name):
  fn = jit(fn)
  for v in values:
    method = getattr(v, method_name)
    expected = method()
    result = fn(v)
    try:
      if result == expected:
        return
    except:
      pass 
    if hasattr(expected, 'dtype') and hasattr(result, 'dtype'):
      assert expected.dtype == result.dtype, \
        "Mismatching dtype, expected %s but got %s" % (expected.dtype, result.dtype)
    else:
      assert type(expected) == type(result), \
        "Mismatching type, expected %s but got %s" % (type(expected), type(result))
    assert np.allclose(expected, result), \
      "For test input %s, expected %s but got %s" % (v, expected, result)

def test_min():
  def call_min(x):
    return x.min()
  run(call_min, 'min')


def test_max():
  def call_max(x):
    return x.max()
  run(call_max, 'max')


def test_any():
  def call_any(x):
    return x.any()
  run(call_any, 'any')

def test_all():
  def call_all(x):
    return x.all()
  run(call_all, 'all')

def test_copy():
  def call_copy(x):
    return x.copy()
  run(call_copy, 'copy')
  x = np.array([1,2,3])
  y = jit(call_copy)(x)
  old_val = x[0]
  x[0] = 10
  assert y[0] == old_val
  assert y[1] == x[1]

def test_ravel():
  def call_ravel(x):
    return x.ravel()
  run(call_ravel, 'ravel')

def test_flatten():
  def call_flatten(x):
    return x.flatten()
  run(call_flatten, 'flatten')

if __name__ == '__main__':
  testing_helpers.run_local_tests() 

########NEW FILE########
__FILENAME__ = test_negative_indexing
import numpy as np 
from parakeet.testing_helpers import expect, run_local_tests
from parakeet import config  

config.print_specialized_function = False
config.print_loopy_function = False


vec = np.arange(16)
mat = vec.reshape(4,4)

def negative_stop_1d(x):
    return x[:-2]

def test_negative_stop_1d_vec():
    expect(negative_stop_1d, [vec], negative_stop_1d(vec))

def test_negative_stop_1d_mat():
    expect(negative_stop_1d, [mat], negative_stop_1d(mat))

def negative_stop_2d(x):
    return x[:-2, :-3]

def test_negative_stop_2d():
    expect(negative_stop_2d, [mat], negative_stop_2d(mat))

def negative_start_1d(x):
    return x[-2:]

def test_negative_start_1d_vec():
    expect(negative_start_1d, [vec], vec[-2:])

def test_negative_start_1d_mat():
    expect(negative_start_1d, [mat], mat[-2:])

def negative_start_2d(x):
    return x[-2:, -3:]

def test_negative_start_2d():
    expect(negative_start_2d, [mat], mat[-2:, -3:])

def negative_step_1d(x):
    return x[::-2]

def test_negative_step_1d_vec():
    expect(negative_step_1d, [vec], vec[::-2])

def test_negative_step_1d_mat():
    expect(negative_step_1d, [mat], mat[::-2])

def negative_step_2d(x):
    return x[::-2, ::-3]

def test_negative_step_2d():
    expect(negative_step_2d, [mat], mat[::-2, ::-3])

if __name__ == "__main__":
    run_local_tests()


########NEW FILE########
__FILENAME__ = test_ones
import numpy as np
import parakeet 

from parakeet.testing_helpers import expect, run_local_tests 

def test_ones_1d():
  expect(np.ones, [(10,)], np.ones(10))

def test_ones_2d():
  expect(np.ones, [(10,2)], np.ones((10,2)))

def test_ones_1d_int64():
  expect(np.ones, [(10,), np.int64], np.ones(10, dtype=np.int64))

def test_ones_2d_int64():
  expect(np.ones, [(10,2), np.int64], np.ones((10,2), dtype=np.int64))

def test_ones_1d_float64():
  expect(np.ones, [(10,), np.float64], np.ones(10, dtype=np.float64))

def test_ones_2d_float64():
  expect(np.ones, [(10,2), np.float64], np.ones((10,2), dtype=np.float64))

def test_ones_4d_float64():
  expect(np.ones, [(10,2,2,2), np.float64], np.ones((10,2,2,2), dtype=np.float64))

if __name__ == "__main__":
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_ones_like
import numpy as np
import parakeet 

from parakeet.testing_helpers import expect, run_local_tests 


int_vec = np.array([1,2,3])
float_vec = np.array([1.0,2.0,3.0])
bool_vec = np.array([True,False])

int_mat = np.array([int_vec, int_vec])
float_mat = np.array([float_vec, float_vec])
bool_mat = np.array([bool_vec, bool_vec])


def test_ones_like():
  expect(np.ones_like, [int_vec], np.ones_like(int_vec))

def test_ones_like_intmat():
  expect(np.ones_like, [int_mat], np.ones_like(int_mat))

def test_ones_like_floatvec():
  expect(np.ones_like, [float_vec], np.ones_like(float_vec))

def test_ones_like_floatmat():
  expect(np.ones_like, [float_mat], np.ones_like(float_mat))

def test_ones_like_boolvec():
  expect(np.ones_like, [bool_vec], np.ones_like(bool_vec))

def test_ones_like_boolmat():
  expect(np.ones_like, [bool_mat], np.ones_like(bool_mat))

def test_ones_like_boolvec_to_float():
  expect(np.ones_like, [bool_vec, np.dtype('float')], np.ones_like(bool_vec, dtype = np.dtype('float')))

def test_ones_like_boolmat_to_float():
  expect(np.ones_like, [bool_mat, np.dtype('float')], np.ones_like(bool_mat, dtype = np.dtype('float')))


if __name__ == '__main__':
  run_local_tests()
########NEW FILE########
__FILENAME__ = test_properties
import numpy as np 
import parakeet 
from parakeet import testing_helpers 

values = [np.array(0), np.array(0.0), np.array(True), 
          np.array([0]), 
          np.array([[0],[1]]), 
          np.array([[0.0], [1.0]]), 
          np.array([[True], [False]])
         ]

def run(fn, prop ):
  fn = parakeet.jit(fn)
  for v in values:
    expected = getattr(v, prop)
    result = fn(v)
    assert np.allclose(expected, result), "Expected %s but got %s" % (expected, result)
  

def test_prop_real():
  def get_real(x):
    return x.real 
  run(get_real, 'real')

#def test_prop_imag():
#  def get_imag(x):
#    return x.imag 
#  run(get_imag, 'imag')

def test_prop_ndim():
  def get_ndim(x):
    return x.ndim 
  run(get_ndim, 'ndim')

def test_prop_size():
  def get_size(x):
    return x.size
  run(get_size, 'size')

def test_prop_shape():
  def get_shape(x):
    return x.shape 
  run(get_shape, 'shape')

def test_prop_T():
  def get_T(x):
    return x.T
  run(get_T, 'T')

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_range
import numpy as np
import parakeet
from parakeet import testing_helpers

 

@parakeet.jit
def range1(n):
  return range(n)

def test_range1():
  n = 10
  numpy_result = np.arange(n)
  parakeet_result = range1(n)
  testing_helpers.assert_eq_arrays(numpy_result, parakeet_result, "range1")
  
@parakeet.jit
def range2(start, stop):
  return range(start, stop)

def test_range2():
  shape = (100,111)
  numpy_result = np.arange(*shape)
  parakeet_result = range2(*shape)
  testing_helpers.assert_eq_arrays(numpy_result, parakeet_result, "range2")
  
@parakeet.jit
def range3(start, stop, step):
  return range(start, stop, step)

def test_range3():
  shape = (10,20,2)
  numpy_result = np.arange(*shape)
  parakeet_result = range3(10,20,2)
  testing_helpers.assert_eq_arrays(numpy_result, parakeet_result, "range3")


"""
def test_big_step():
  shape = (10,20,100)
  numpy_result =  np.arange(*shape)
  parakeet_result = range3(*shape)
  
  assert_eq_arrays(numpy_result, parakeet_result,"range_big_step")
"""

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_ravel
from parakeet import testing_helpers
import numpy as np 

def ravel_method(x):
  return x.ravel()

vec  = np.arange(12)

def run(x):
  testing_helpers.expect(ravel_method, [x], x.ravel())
  # testing_helpers.expect(np.ravel, [x], x.ravel())

def test_ravel_vec():
  run(vec)

def test_ravel_mat_row_major():
  matrix_row_major = np.reshape(vec, (3,4), order = "C")
  run(matrix_row_major)

def test_ravel_mat_col_major():
  matrix_col_major = np.reshape(vec, (3,4), order = "F")
  run(matrix_col_major)

def test_ravel_cube():
  cube = np.reshape(vec, (3,2,2))
  run(cube)

if __name__ == '__main__':
    testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_setidx
import numpy as np
from parakeet.testing_helpers import expect, run_local_tests

shape_1d = 40
ints_1d = np.arange(shape_1d)
floats_1d = np.arange(shape_1d, dtype='float')
bools_1d = ints_1d % 2

vecs = [ints_1d, floats_1d, bools_1d]

shape_2d = (4,10)
matrices = [np.reshape(vec, shape_2d) for vec in vecs]

shape_3d = (4,5,2)
tensors = [np.reshape(mat, shape_3d) for mat in matrices]

def set_idx_1d(arr,i,val):
  arr[i] = val
  return arr 

def test_set_idx_1d():
  idx = 10
  for vec in vecs:
    vec1, vec2 = vec.copy(), vec.copy()
    val = -vec[idx]
    vec2[idx] = val
    expect(set_idx_1d, [vec1, idx, val], vec2)

def set_idx_2d(arr,i,j,val):
  arr[i, j] = val
  return arr

def test_set_idx_2d():
  i = 2
  j = 2
  for mat in matrices:
    mat1, mat2 = mat.copy(), mat.copy()
    val = -mat[i,j]
    mat2[i,j] = val
    expect(set_idx_2d, [mat1, i, j, val], mat2)

def set_idx_3d(arr, i, j, k, val):
  arr[i, j, k] = val
  return arr

def test_set_idx_3d():
  i = 2
  j = 3
  k = 1
  for x in tensors:
    x1, x2 = x.copy(), x.copy()
    val = -x[i, j, k]
    x2[i, j, k] = val
    expect(set_idx_3d, [x1, i, j, k, val], x2)


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_slices
import numpy as np
from parakeet import jit 
from parakeet.testing_helpers import expect, expect_each, run_local_tests

shape_1d = 40
ints_1d = np.arange(shape_1d)
floats_1d = np.arange(shape_1d, dtype='float')
bools_1d = ints_1d % 2

vecs = [ints_1d, floats_1d, bools_1d]

shape_2d = (4,10)
matrices = [np.reshape(vec, shape_2d) for vec in vecs]

def implicit_slice_first_axis(x,i):
  return x[i]

def test_implicit_slice_first_axis_matrices():
  for m in matrices:
    expect(implicit_slice_first_axis, [m,2], m[2])

def slice_first_axis(x,i):
  return x[i,:]

def test_slice_first_axis_matrices():
  for m in matrices:
    expect(implicit_slice_first_axis, [m, 2], m[2])

def slice_second_axis(x,i):
  return x[:,i]

def test_slice_second_axis_matrices():
  for m in matrices:
    expect(slice_second_axis, [m,2], m[:,2])

def assign_first_axis(x, i, j):
  x[i] = x[j]
  return x

def test_assign_first_axis():
  for m in matrices:
    m_expect = m.copy()
    m_input = m.copy()
    m_expect[1] = m_expect[2]
    expect(assign_first_axis, [m_input, 1, 2], m_expect)

def assign_second_axis(x, i, j):
  x[:, i] = x[:, j]
  return x

def test_assign_second_axis():
  for m in matrices:
    m_expect = m.copy()
    m_input = m.copy()
    m_expect[:,1] = m_expect[:,2]
    expect(assign_second_axis, [m_input, 1, 2], m_expect)

def assign_slices(x, (i,j,k,l), (a,b,c,d)):
  x[i:j, k:l] = x[a:b, c:d]
  return x

def test_assign_slices():
  for m in matrices:
    m_expect = m.copy()
    m_input = m.copy()
    (i,j,k,l) = (0,2,0,4)
    (a,b,c,d) = (1,3,5,9)
    m_expect[i:j, k:l] = m_expect[a:b, c:d]
    expect(assign_slices, [m_input, (i,j,k,l), (a,b,c,d)], m_expect)
    expect(assign_slices, [m_input, (i,j,k,l), (a,b,c,d)], m_expect)

def assign_four_rows(x, y):
  y[0:2,0:5] = x[0:2,0:5]
  y[0:2,5:10] = x[0:2,5:10]
  y[2:4,0:5] = x[2:4,0:5]
  y[2:4,5:10] = x[2:4,5:10]
  return y

def test_assign_four_rows():
  for m in matrices:
    m_expect = m.copy()
    m_input = m.copy()
    m_zeros = np.zeros_like(m_input)
    expect(assign_four_rows, [m_input, m_zeros], m_expect)

def copy_two_rows(x, y):
  # Fill y with the elements of x to try to get an identical copy
  i = 0
  while i < x.shape[1]:
    y[0][i] = x[0][i]
    y[1][i] = x[1][i]
    i = i + 1
  return y

def loop_slice(x, y, z):
  i = 0
  while i < z.shape[0]:
    i_next = i + 2
    z[i:i_next,:] = copy_two_rows(x[i:i_next,:], y)
    i = i_next
  return z

def test_loop_slices():
  m_input = np.arange(40, dtype=np.int64).reshape(4,10)
  m_zeros = np.zeros((2,10), dtype=np.int64)
  m_z = np.zeros_like(m_input)
  m_expect = m_input.copy()
  expect(loop_slice, [m_input, m_zeros, m_z], m_expect)

def lower_right_corner(X):
  m,n = X.shape
  return X[m/2:m, n/2:n]

def test_lower_right_corner():
  expect_each(jit(lower_right_corner), lower_right_corner, matrices)

def multiple_slices(x):
  y = x[2:4,:]
  return y[:,2]

def test_multiple_slices():
  x = np.random.randn(10,10)
  expect_each(jit(multiple_slices), multiple_slices, matrices)
  

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_slice_arith
import numpy as np 
from parakeet import testing_helpers

def add_2d(x):
  return x[1:-1, 2:] + x[1:-1, :-2] - 4 * x[1:-1, 1:-1]

def test_add_2d():
  x = np.array([[1,2,3,4,5,6],[10,20,30,40,50,60]])
  testing_helpers.expect(add_2d, [x], add_2d(x))

if __name__ == "__main__":
  testing_helpers.run_local_tests()


########NEW FILE########
__FILENAME__ = test_zeros
import numpy as np

from parakeet.testing_helpers import expect, run_local_tests 

def test_zeros_1d():
  expect(np.zeros, [(10,)], np.zeros(10))

def test_zeros_2d():
  expect(np.zeros, [(10,2)], np.zeros((10,2)))  

def test_zeros_1d_int64():
  expect(np.zeros, [(10,), np.int64], np.zeros(10, dtype=np.int64))

def test_zeros_2d_int64():
  expect(np.zeros, [(10,2), np.int64], np.zeros((10,2), dtype=np.int64))

def test_zeros_1d_float64():
  expect(np.zeros, [(10,), np.float64], np.zeros(10, dtype=np.float64))

def test_zeros_2d_float64():
  expect(np.zeros, [(10,2), np.float64], np.zeros((10,2), dtype=np.float64))

def test_zeros_4d_float64():
  expect(np.zeros, [(10,2,2,2), np.float64], np.zeros((10,2,2,2), dtype=np.float64))
  
  
if __name__ == "__main__":
  run_local_tests()
  
  
########NEW FILE########
__FILENAME__ = test_zeros_like
import numpy as np
import parakeet 

from parakeet.testing_helpers import expect, run_local_tests 


int_vec = np.array([1,2,3])
float_vec = np.array([1.0,2.0,3.0])
bool_vec = np.array([True,False])

int_mat = np.array([int_vec, int_vec])
float_mat = np.array([float_vec, float_vec])
bool_mat = np.array([bool_vec, bool_vec])


#
# ZEROS_LIKE
#
def test_zeros_like_intvec():
  expect(np.zeros_like, [int_vec], np.zeros_like(int_vec))

def test_zeros_like_intmat():
  expect(np.zeros_like, [int_mat], np.zeros_like(int_mat))

def test_zeros_like_floatvec():
  expect(np.zeros_like, [float_vec], np.zeros_like(float_vec))

def test_zeros_like_floatmat():
  expect(np.zeros_like, [float_mat], np.zeros_like(float_mat))

def test_zeros_like_boolvec():
  expect(np.zeros_like, [bool_vec], np.zeros_like(bool_vec))

def test_zeros_like_boolmat():
  expect(np.zeros_like, [bool_mat], np.zeros_like(bool_mat))

def test_zeros_like_boolvec_to_float():
  expect(np.zeros_like, [bool_vec, np.dtype('float')], np.zeros_like(bool_vec, dtype = np.dtype('float')))

def test_zeros_like_boolmat_to_float():
  expect(np.zeros_like, [bool_mat, np.dtype('float')], np.zeros_like(bool_mat, dtype = np.dtype('float')))
  
if __name__ == "__main__":
  run_local_tests()
  
  
########NEW FILE########
__FILENAME__ = test_max
from parakeet.testing_helpers import expect, run_local_tests

def test_max_int():
  expect(max, [3,4], 4)

def test_max_float():
  expect(max, [3.0,4.0], 4.0)
  
def test_max_int_float():
  expect(max, [3, 4.0], 4.0)

def test_max_bool():
  expect(max, [False, True], True)
  
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_min
from parakeet.testing_helpers import expect, run_local_tests

def test_min_int():
  expect(min, [-3, 4], -3)

def test_min_float():
  expect(min, [-3.0, 4.0], -3.0)


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_range
import numpy as np 
from parakeet import jit 
from parakeet.testing_helpers import expect, run_local_tests

@jit 
def range1(a):
  return range(a)

def test_range1():
  expect(range1, [10], np.arange(10))

@jit 
def range2(a,b):
  return range(a,b)

def test_range2():
  expect(range2, [10, 20], np.arange(10,20))

@jit
def range3(a,b,c):
  return range(a,b,c)
  
def test_range3():
  expect(range3, [20,45,3], np.arange(20,45,3))

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_sum
import numpy as np 
from parakeet.testing_helpers import expect, run_local_tests


def test_sum_int():
  x = np.array([1,2,3])
  expect(sum, [x], sum(x))

def test_sum_float():
  x = np.array([1.0,2.0,3.0])
  expect(sum, [x], sum(x))

def sum_bool():
  x = np.array([True, False, True])
  expect(sum, [x], sum(x))

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_types
from parakeet.testing_helpers import expect, run_local_tests

def test_float_to_int():
  expect(int, [1.2], 1)

def test_int_to_float():
  expect(float, [1], 1.0)
  
def test_bool_to_float():
  expect(float, [True], 1.0)

def test_float_to_long():
  expect(long, [-1.0], -1)
  
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_call_overhead
from parakeet import jit 
from parakeet.testing_helpers import run_local_tests, expect 
import time 

def f(x,y,z=1,q=3):
  return x + y + z + q


jitf = jit(f)

def test_call_overhead_identity():
  n = 2000
  x = 3 
  start_t = time.time()
  for i in xrange(n):
    f(x,x,x,x)
  python_time = time.time() - start_t 
  print "Python time for %d calls: %f" % (n, python_time)
  # warm up!
  jitf(x,x,x,x)
  start_t = time.time()
  for i in xrange(n):
    jitf(x,x,x,x)
  parakeet_time = time.time() - start_t
  print "Parakeet time for %d calls: %f" % (n, parakeet_time)
  print "Slowdown = %f" % (parakeet_time / python_time )
  assert parakeet_time < 1000 * python_time, "Excessive call overhead: %f Python vs. %f Parakeet" % (python_time, parakeet_time)


if __name__ == "__main__":
  run_local_tests()
  

  

########NEW FILE########
__FILENAME__ = test_codegen_add_scalars
import parakeet
from parakeet.testing_helpers import run_local_tests, expect_eq

def mk_adder(t):
  f, b, (x,y) = parakeet.build_fn([t,t], t)
  b.return_(b.add(x,y))
  return f

def double_i64(x):
  fn = mk_adder(parakeet.Int64)
  return parakeet.run_typed_fn(fn, (x,x))
  
def double_f64(x):
  fn = mk_adder(parakeet.Float64)
  return parakeet.run_typed_fn(fn, (x,x))

def test_add():
  expect_eq(double_i64(1), 2)
  expect_eq(double_i64(-1), -2)
  expect_eq(double_f64(1.0), 2.0)
  expect_eq(double_f64(-1.0), -2.0)


if __name__ == '__main__':
  run_local_tests()  

########NEW FILE########
__FILENAME__ = test_codegen_add_vecs
import numpy as np
from parakeet.testing_helpers import run_local_tests, expect_eq 
import parakeet

def mk_scalar_add(t):
  f, b, (x,y) = parakeet.build_fn([t,t], t)
  b.return_(b.add(x,y))
  return f

def mk_vec_add(array_t):
  elt_t = parakeet.elt_type(array_t)
  add_fn = mk_scalar_add(elt_t)
  fn, builder, inputs = parakeet.build_fn([array_t, array_t, array_t])
  x,y,z = inputs
  n = builder.len(x)
  def loop_body(idx):
    elt_x = builder.index(x, idx)
    elt_y = builder.index(y, idx)
    builder.setidx(z, idx, builder.call(add_fn, (elt_x, elt_y)))
  builder.loop(0, n, loop_body)
  builder.return_(builder.none)
  return fn 


def vec_add(x,y):
  assert x.dtype == y.dtype 
  assert len(x.shape) == len(y.shape) == 1 
  z = np.zeros_like(x)
  array_t = parakeet.typeof(x)
  fn = mk_vec_add(array_t)
  parakeet.run_typed_fn(fn, (x,y,z))
  return z 
  
def test_vec_add(): 
  xs,ys = np.array([1,2,3]), np.array([10,20,30])
  zs = vec_add(xs, ys)
  expected = xs + ys 
  expect_eq(zs, expected)
  

if __name__ == '__main__': 
  run_local_tests()  

########NEW FILE########
__FILENAME__ = test_codegen_identity
from parakeet.testing_helpers import run_local_tests, expect_eq

import parakeet


def identity_i64(x):
  fn = parakeet.mk_identity_fn(parakeet.Int64)
  print repr(fn) 
  return parakeet.run_typed_fn(fn, (x,))
  
def identity_f64(x):
  fn = parakeet.mk_identity_fn(parakeet.Float64)
  return parakeet.run_typed_fn(fn, (x,))

def test_identity():
  expect_eq(identity_i64(1), 1)
  expect_eq(identity_i64(-1), -1)
  expect_eq(identity_f64(1.0), 1.0)
  expect_eq(identity_f64(-1.0), -1.0)


if __name__ == '__main__':
 
  run_local_tests()  

########NEW FILE########
__FILENAME__ = test_codegen_sum_loop
import numpy as np 
from parakeet.testing_helpers import run_local_tests, expect_eq

import parakeet

def mk_sum(elt_t):
  array_t = parakeet.ndtypes.make_array_type(elt_t, 1)
  f, b, (x,) = parakeet.build_fn([array_t], elt_t)
  n = b.len(x)
  total, total_after, merge = b.loop_var('total', b.zero(elt_t))
  def loop_body(idx):
    b.assign(total_after, b.add(total, b.index(x,idx)))
  b.loop(0, n, loop_body, merge = merge)
  b.return_(total)
  return f 

def sum_i64(x):
  fn = mk_sum(parakeet.Int64)
  print fn 
  return parakeet.run_typed_fn(fn, (x,))
  
def sum_f64(x):
  fn = mk_sum(parakeet.Float64)
  return parakeet.run_typed_fn(fn, (x,))

def test_sum():
  expect_eq(sum_i64(np.array([1,2,3])), 6)
  expect_eq(sum_f64(np.array([-1.0, 1.0, 2.0])), 2.0)
  

if __name__ == '__main__':
 
  run_local_tests()  

########NEW FILE########
__FILENAME__ = test_escape_analysis


from parakeet import Int64, make_tuple_type, make_array_type
from parakeet.analysis.escape_analysis import EscapeAnalysis
from parakeet.syntax import Tuple, Var, TypedFn, Assign, Return, Array, TupleProj, zero_i64, one_i64
from parakeet.testing_helpers import run_local_tests   

 

array_t = make_array_type(Int64, 1)
tuple_t = make_tuple_type((Int64, array_t))
nested_tuple_t = make_tuple_type((tuple_t, tuple_t))
array_const = Array([one_i64], type = array_t)

a_int = Var("a_int", type = Int64)
b_array = Var("b_array", type = array_t)
c_tuple = Var("c_tuple", type = tuple_t)
d_tuple = Var("d_tuple", type = tuple_t)
e_nested_tuple = Var("e_nested_tuple", type= nested_tuple_t)
f_nested_tuple = Var("f_nested_tuple", type = nested_tuple_t)
body = [
  Assign(a_int, one_i64),  
  Assign(b_array, array_const), 
  Assign(c_tuple, Tuple(elts = (a_int, b_array), type = tuple_t)),
  Assign(d_tuple, Tuple(elts = (a_int, array_const), type = tuple_t)),
  Assign(e_nested_tuple, Tuple(elts = (c_tuple, c_tuple), type = nested_tuple_t)),
  Assign(f_nested_tuple, Tuple(elts = (d_tuple, d_tuple), type = nested_tuple_t)),  
  Return(b_array) 
]
tenv = { "a_int": Int64, 
         "b_array": array_t, 
         "c_tuple": tuple_t, 
         "d_tuple": nested_tuple_t, 
         "e_nested_tuple": nested_tuple_t,
         "f_nested_tuple" : nested_tuple_t, 
        }

fn = TypedFn(name = "test_escape_analysis", 
             type_env = tenv, 
             input_types = (), 
             arg_names = (), 
             body = body, 
             return_type = nested_tuple_t
            )

def test_escape_analysis():
  escape_analysis = EscapeAnalysis()
  escape_analysis.visit_fn(fn)
  may_escape =  escape_analysis.may_escape
 
  assert "a_int" not in may_escape, "Scalars don't count as escaping"
  assert "b_array" in may_escape
  assert "c_tuple" in may_escape
  assert "d_tuple" not in may_escape
  assert "e_nested_tuple" in may_escape 
  assert "f_nested_tuple" not in may_escape
  
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_licm

from parakeet.testing_helpers import expect, run_local_tests

def volatile_licm_mistake():
  i = 0
  x = [0]
  while i < 10:
    alloc = [1]
    if i == 5:
      x = alloc
    else:
      alloc[0] = 100
    i = i + 1
  return x[0]

def test_volatile_licm_mistake():
  expect(volatile_licm_mistake, [], volatile_licm_mistake()) #np.array([1]))

def licm_nested_loops():
  
  total = 0
  a = [1,2,3,4,5]
  for _ in range(3):
    for _ in range(2):
      for j in range(len(a)):
        a[j] *= 10
    total += a[1]
  return total   

def test_licm_nested_loops():
  expect(licm_nested_loops, [], licm_nested_loops())

def loop_invariant_alloc():
  b = [1,2,3]
  total = 0
  for i in xrange(3):
    a = [0,0,0]
    for j in xrange(3):
      a[j] = b[j]
    total += a[i]
  return total 
  
def test_loop_invariant_alloc():
  expect(loop_invariant_alloc, [], loop_invariant_alloc())
  

if __name__ == "__main__":
  run_local_tests()
########NEW FILE########
__FILENAME__ = test_mutability_analysis
import numpy as np 



from parakeet.analysis import mutability_analysis
from parakeet import make_array_type, Int64, Bool, ptr_type, specialize
from parakeet.transforms.pipeline import lowering 
from parakeet.testing_helpers import run_local_tests

def f(x,y):
    x[0] = True
    return y

def test_mutable_array():
  bool_vec = np.array([True, False])
  int_vec = np.array([1,2])
  typed, _, =  specialize(f, [bool_vec, int_vec])
   
  mutable_types = mutability_analysis.find_mutable_types(typed)
  int_array_t = make_array_type(Int64, 1)
  bool_array_t = make_array_type(Bool, 1)
  assert int_array_t not in mutable_types, \
      "Didn't expect %s in mutable_types %s" % (int_array_t, mutable_types)
  assert bool_array_t in mutable_types, \
      "Expected %s in mutable_types %s" % (bool_array_t, mutable_types)
      
  lowered = lowering.apply(typed)
  mutable_types = mutability_analysis.find_mutable_types(lowered)
  ptr_bool_t = ptr_type(Bool)
  ptr_int_t = ptr_type(Int64) 
  assert ptr_int_t not in mutable_types, \
      "Didn't expect %s in lowered mutable types %s" % \
      (ptr_int_t, mutable_types)
  assert ptr_bool_t in mutable_types, \
      "Expected %s in lowered mutable_types %s" % (ptr_bool_t, mutable_types)

"""
# Removed this test when I made attributes immutable 
def f():
  x = slice(1, 2, 3)
  x.start = 10
  x.stop = 20
  y = slice(1,2,3)
  if 3 < y.step:
    y.step = y.step + 1
  else:
    y.step = 0

def test_mutable_slice():
  _, typed, _, _ =  testing_helpers.specialize_and_compile(f, [])
  mutable_types = mutability_analysis.find_mutable_types(typed)

  assert len(mutable_types) == 1, mutable_types
  lowered = lowering.apply(typed)
  mutable_types = mutability_analysis.find_mutable_types(lowered)
  assert len(mutable_types) == 1, mutable_types
"""
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_optimizations
import numpy as np

import parakeet
from parakeet import config, each, syntax
from parakeet.transforms.pipeline import lowering, high_level_optimizations
from parakeet.syntax import OuterMap, Return
from parakeet.analysis.syntax_visitor import SyntaxVisitor
from parakeet.testing_helpers import expect, run_local_tests

def A(x):
  return x + 1

def B(x):
  return A(x)

def C(x):
  return B(x)

def simple_args(exprs):
  return all(simple_expr(e) for e in exprs)

def simple_primcall(e):
  return isinstance(e, syntax.PrimCall) and simple_args(e.args)

def simple_expr(expr):
  return isinstance(expr, (syntax.Const, syntax.Var)) or \
         simple_primcall(expr)

def simple_stmt(stmt):
  """
  Is this a statement from a simple straightline function?

  (can only contain scalar computations and no control flow)
  """
  if isinstance(stmt, syntax.Return):
    return simple_expr(stmt.value)
  elif isinstance(stmt, syntax.Assign):
    return simple_expr(stmt.rhs)
  else:
    return False

def simple_block(stmts):
  return all(simple_stmt(s) for s in stmts)

def simple_fn(fn):
  return simple_block(fn.body)

def test_inline():
  typed_fn = parakeet.typed_repr(C, [1])
  assert len(typed_fn.body) in [1,2], \
      "Expected body to be 1 or 2 statements, got %s" % typed_fn
  assert simple_fn(typed_fn)
  expect(C, [1], 2)

def lots_of_arith(x):
  y = 4 * 1
  z = y + 1
  a = z / 5
  b = x * a
  return b

def test_simple_constant_folding():
  expect(lots_of_arith, [1], 1)
  typed_fn = parakeet.typed_repr(lots_of_arith, [1], optimize = True)
  assert len(typed_fn.body) == 1, \
      "Insufficiently simplified body: %s" % typed_fn

def const_across_control_flow(b):
  if b:
    x = 1
  else:
    x = 1
  return x

def test_constants_across_control_flow():
  expect(const_across_control_flow, [True], 1)
  typed_fn = parakeet.typed_repr(const_across_control_flow, [True])
  assert len(typed_fn.body) == 1, "Fn body too long: " + str(typed_fn.body)
  stmt = typed_fn.body[0]
  assert isinstance(stmt, syntax.Return)
  assert isinstance(stmt.value, syntax.Const)

def always_true_branch():
  x = 1 + 1
  if x == 2:
    res = 0 + 0
    return res
  else:
    res = 1 * 1 + 0
    return res

def test_always_true():
  expect(always_true_branch, [], 0)
  typed_fn = parakeet.typed_repr(always_true_branch, [])
  assert len(typed_fn.body) == 1, "Fn body too long: " + str(typed_fn.body)
  stmt = typed_fn.body[0]
  assert isinstance(stmt, syntax.Return)
  assert isinstance(stmt.value, syntax.Const)

def always_false_branch():
  x = 1 + 2
  if x == 2:
    res = 1 * 0 + 0
    return res
  else:
    res = 0 + 1 * 1
    return res

def test_always_false():
  expect(always_false_branch, [], 1)
  typed_fn = parakeet.typed_repr(always_false_branch, [])
  assert len(typed_fn.body) == 1, "Fn body too long: " + str(typed_fn.body)
  stmt = typed_fn.body[0]
  assert isinstance(stmt, syntax.Return)
  assert isinstance(stmt.value, syntax.Const)


def g(x):
  def h(xi):
    return xi + 1.0
  return each(h,x)

def nested_add1(X):
  return each(g, X)

class CountLoops(SyntaxVisitor):
  def __init__(self):
    SyntaxVisitor.__init__(self)
    self.count = 0

  def visit_While(self, stmt):
    self.count += 1
    SyntaxVisitor.visit_While(self, stmt)

def count_loops(fn):
  Counter = CountLoops()
  Counter.visit_fn(fn)
  return Counter.count

def test_copy_elimination():
  x = np.array([[1,2,3],[4,5,6]])
  expect(nested_add1, [x], x + 1.0)
  typed_fn = parakeet.typed_repr(nested_add1, [x])
  lowered = lowering.apply(typed_fn)
  n_loops = count_loops(lowered)
  n_expected = 3 if config.opt_loop_unrolling else 2
  assert n_loops <= n_expected, \
      "Too many loops generated! Expected at most 2, got %d" % n_loops

def allpairs_dist(x):
  return np.array([[np.sqrt(np.sum( (x1-x2)**2)) for x2 in x] for x1 in x])
  
def test_combine_nested_maps():
  x = np.array([[1,2], [3,4]])
  typed_fn = parakeet.typed_repr(allpairs_dist, [x])
  typed_fn = high_level_optimizations.apply(typed_fn)
  assert len(typed_fn.body) == 1
  stmt = typed_fn.body[0]
  assert stmt.__class__ is Return, "Expected Return but got %s" % stmt 
  v = stmt.value 
  assert v.__class__ is OuterMap, "Expected OuterMap but got %s" % v
  assert len(v.args) == 2, "Expected OuterMap to have two args, but got %s" % (v.args,)


def allpairs_add_imap(n):
  return np.array([[i+j for j in xrange(n)] for i in xrange(n)])


from parakeet.syntax import IndexMap, PrimCall, get_fn
def test_combine_nested_index_maps():
  n = 3
  typed_fn = parakeet.typed_repr(allpairs_add_imap, [n])
  print "TYPED FN", typed_fn 
  typed_fn = high_level_optimizations.apply(typed_fn)
  print "OPT FN", typed_fn 
  assert len(typed_fn.body) in (1,2)
  stmt = typed_fn.body[-1]
  print "STMT", stmt 
  assert stmt.__class__ is Return, "Expected Return but got %s" % stmt 
  v = stmt.value 
  print "VALUE", v 
  assert v.__class__ is IndexMap, "Expected IndexMap but got %s" % v
  nested_fn = get_fn(v.fn) 
  assert len(nested_fn.body) == 1
  nested_stmt = nested_fn.body[0]
  assert nested_stmt.__class__ is Return 
  nested_value = nested_stmt.value 
  assert nested_value.__class__ is PrimCall, "Expected PrimCall but got %s" % nested_value 
  
 

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_shape_inference
import numpy as np
import parakeet
from parakeet import shape_inference
from parakeet.shape_inference import call_shape_expr
from parakeet.shape_inference.shape import const, Shape, Var 
from parakeet import testing_helpers

def expect_shape(python_fn, args_list, expected):
  print "[expect_shape]"
  print " -- fn: ", python_fn
  print " -- args: ", args_list
  typed_fn = parakeet.typed_repr(python_fn, args_list)
  print " -- types: ", typed_fn.input_types
  result_shape = call_shape_expr(typed_fn)
  assert result_shape == expected, \
      "Expected shape %s, but got: %s" % (expected, result_shape)

def const_scalar():
  return 1

def test_const_scalar():
  expect_shape(const_scalar, [], const(1))

def merge_scalars(b):
  if b:
    return 1
  else:
    return 2

def test_any_scalar():
  expect_shape(merge_scalars, [True], shape_inference.any_scalar)

def array_literal():
  return [1,2,3]

def test_array_literal():
  expect_shape(array_literal, [], Shape([3]))

vec = np.array([1,2,3])
mat = np.array([[1,2,3],[4,5,6],[7,8,9]])

def ident(x):
  return x

def test_ident_1d():
  expect_shape(ident, [vec], Shape([Var(0)]))

def test_ident_2d():
  expect_shape(ident, [mat], Shape([Var(0), Var(1)]))

def increase_rank(x):
  return [x,x]

def test_increase_rank_1d():
  expect_shape(increase_rank, [1], Shape([2]))

def test_increase_rank_2d():
  expect_shape(increase_rank, [vec], Shape([2, Var(0)]))

def test_increase_rank_3d():
  expect_shape(increase_rank, [mat], Shape([2, Var(0), Var(1)]))

def incr(xi):
  return xi + 1


def simple_map(x):
  return parakeet.map(incr, x)

def test_simple_map_1d():
  expect_shape(simple_map, [vec], Shape([Var(0)]))

def test_simple_map_2d():
  expect_shape(simple_map, [mat], Shape([Var(0), Var(1)]))

def map_increase_rank(x):
  return parakeet.map(increase_rank, x)

def test_map_increase_rank_1d():
  expect_shape(map_increase_rank, [vec], Shape([Var(0), 2]))

def test_map_increase_rank_2d():
  expect_shape(simple_map, [mat], Shape([Var(0), Var(1)]))

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_subtypes
from parakeet.ndtypes import Int8, Int16, Int32, Float32, Float64
from parakeet.ndtypes import is_scalar_subtype
from parakeet.testing_helpers import run_local_tests 

def test_int32_subtypes():
  assert is_scalar_subtype(Int8, Int32)
  assert not is_scalar_subtype(Int32, Int8)
  assert is_scalar_subtype(Int32, Float32)
  assert not is_scalar_subtype(Float32, Int32)

def test_float32_subtypes():
  assert is_scalar_subtype(Int8, Float32)
  assert not is_scalar_subtype(Float32, Int8)
  assert is_scalar_subtype(Float32, Float64)
  assert not is_scalar_subtype(Float64, Float32)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_type_inference
from parakeet import UInt8, Int8, Int32, Int64, Float32, Float64, Bool
from parakeet.testing_helpers import expect_type, run_local_tests

def add1(x):
  return x + 1

def call_add1(x):
  return add1(x)

def test_add1():
  expect_type(add1, [Int32], Int64)
  expect_type(add1, [Int8], Int64)
  expect_type(add1, [UInt8], Int64)
  expect_type(add1, [Bool], Int64)
  expect_type(add1, [Int64], Int64)
  expect_type(add1, [Float32], Float64)

def test_call_add1():
  expect_type(call_add1, [Int32], Int64)
  expect_type(call_add1, [Float32], Float64)
  expect_type(call_add1, [Bool], Int64)
  expect_type(add1, [Float32], Float64)

def add(x,y):
  return x + y

def test_add_bools():
  """
  test_add_bools:
  Parakeet booleans don't behave like the default Python type but rather like
  numpy.bool8, where (+) == (or) and (*) == (and)
  """
  expect_type(add, [Bool, Bool], Bool)

def branch_return(b):
  if b:
    return 1
  else:
    return 1.0

def test_branch_return():
  expect_type(branch_return, [Bool], Float64)

def branch_assign(b):
  if b:
    x = 0
  else:
    x = 1.0
  return x

def test_branch_assign():
  expect_type(branch_assign, [Bool], Float64)

def incr_loop(init, count):
  x = init
  while x < count:
    x = x + 1
  return x

def test_incr_loop():
  expect_type(incr_loop, [Int32, Int32], Int64)
  expect_type(incr_loop, [Float64, Int32], Float64)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_value_range_analysis
import numpy as np 
from parakeet import testing_helpers, frontend, analysis, type_inference, transforms

def infer_ranges(python_fn, input_types):
  untyped = frontend.translate_function_value(python_fn)
  typed = type_inference.specialize(untyped, input_types)

  vra = analysis.ValueRangeAnalyis()
  
  vra.visit_fn(typed)
  return vra.ranges 

def expect_const(k, v, c):
  assert v.__class__ is analysis.value_range_analysis.Interval, \
    "Expected %s to be const interval but got %s" % (k,v) 
  assert v.lower == c, "Expected %s to be const value %s not %s" % (k, c,v)
  assert v.upper == c, "Expected %s to be const value %s not %s" % (k, c,v)
  

def expect_range(k, v, lower = None, upper = None):
  assert v.__class__ is analysis.value_range_analysis.Interval, \
    "Expected %s to be interval but got %s" % (k,v)
  if lower is not None:
    assert v.lower == lower, \
      "Expected %s's lower value %s but got %s" % (k, lower,v)
  if upper is not None: 
    assert v.upper == upper, \
      "Expected %s's upper value %s but got %s" % (k, upper,v)

def incr_loop():
  x = 1
  for i in xrange(10):
    x = x + 1
  return x 

def test_incr_loop():
  ranges = infer_ranges(incr_loop, [])
  assert len(ranges) == 4, "Wrong number of variables in %s" % ranges 
  for (k, v) in sorted(ranges.items(), key = lambda (k,v): k):
    if k.startswith("i"):
      expect_range(k, v, 0, 10)
    elif k.startswith("x"):
      assert v.__class__ is analysis.value_range_analysis.Interval, \
        "Expected %s to be interval but got %s" % (k,v)
      assert v.lower in (1,2), "Expected %s to have lower bound of 1 or 2 but got %s" % v 
    else:
      assert False, "Unexpected variables %s = %s" % (k,v)

def const():
  x = 1 
  return x 

def test_const():
  ranges = infer_ranges(const, [])
  assert len(ranges) == 1, "Too many variables: %s" % ranges 
  k,v = ranges.items()[0]
  expect_const(k,v, 1)

def nested_loops():
  x = 0
  y = 0
  z = 0 
  for i in xrange(10):
    x = x + i
    for j in xrange(3,12,1):
      y = y + x
      for k in xrange(3):
        z = z + y
  q = z
  return q

def test_nested_loops():
  ranges = infer_ranges(nested_loops, [])    
  print ranges 
  for k, v in ranges.iteritems():
    if k.startswith("i"):
      expect_range(k,v,0,10)
    elif k.startswith("j"):
      expect_range(k,v,3,12)
    elif k.startswith("x"):
      try:
        expect_range(k,v,0,None)
      except:
        expect_range(k,v,3,np.inf)
    elif k.startswith("q"):
      expect_range(k,v,None,np.inf)
if __name__ == "__main__":
  testing_helpers.run_local_tests()


########NEW FILE########
__FILENAME__ = test_args
from parakeet.testing_helpers import expect, run_local_tests

def varargs_return(*x):
  return x

def test_varargs_return():
  expect(varargs_return, [1,2], (1,2))

def varargs_add(*x):
  return x[0] + x[1]

def test_varargs_add():
  expect(varargs_add, [1,2], 3)

def call_varargs_add(x,y):
  local_tuple = (x,y)
  return varargs_add(*local_tuple)

def test_call_varargs_add():
  expect(call_varargs_add, [1,2], 3)
  expect(call_varargs_add, [True,2.0], 3.0)

def add_defaults(x = 1, y = 2):
    return x + y

def test_add_defaults():
  expect(add_defaults, [], 3)
  expect(add_defaults, [10], 12)
  expect(add_defaults, [10, 20], 30)
  expect(add_defaults, [10, 20.0], 30.0)

def call_add_defaults():
  return add_defaults(10)

def test_call_add_defaults():
  expect(call_add_defaults, [], 12)

def call_add_defaults_with_names():
  return add_defaults(y = 10, x = 20)

def test_call_defaults_with_names():
  expect(call_add_defaults_with_names, [], 30)

def sub(x,y):
  return x - y

def call_pos_with_names():
  return sub(y = 10, x = 20)

def test_call_pos_with_names():
  expect(call_pos_with_names, [], 10)
  
def tuple_default(x = (1,2)):
  return x[0] + x[1]

def test_tuple_default():
  expect(tuple_default, [], 3)
  expect(tuple_default, [(1,3)], 4)

def default_closure(x = 1, y=(1,2)):
  def inner(z, r = 0):
    return y[0] + y[1] + z + r
  return inner(x)

def test_default_closure(): 
  expect(default_closure, [], 4)
  expect(default_closure, [10], 13)
  expect(default_closure, [10, (20,30)], 60)

def default_none(x, y = None):
  if y is None:
    return x * 2
  else:
    return x + y

def test_default_none():
  expect(default_none, [10], 20)
  expect(default_none, [10, None], 20)
  expect(default_none, [10, 1], 11)

def add1(x):
  return x + 1

def add2(x): 
  return x + 2 


def fn_as_default(f = add1):
  return f(1)

def test_fn_as_default():
  expect(fn_as_default, [], 2)
  expect(fn_as_default, [add2], 3)

def lambda_closure(x):
  y = 1.0
  def g(f, x):
    return f(x)
  return g(lambda z: z + y, x)

def test_lambda_closure():
  expect(lambda_closure, [1], 2.0) 

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_arith
import numpy as np 
from parakeet import jit, testing_helpers


values = [1, 
          1.0,
          np.array([True, True, True]), 
          #np.array([1,2,3], dtype='int8'),
          np.array([1,2,3], dtype='int16'),
          #np.array([1,2,3], dtype='int32'), 
          #np.array([1,2,3], dtype='int64'),
          np.array([1,2,3], dtype='float32'), 

          #np.array([1,2,3], dtype='float64')
        ]

def run(parakeet_fn, python_fn):
  testing_helpers.expect_allpairs(jit(parakeet_fn), python_fn, values)
  
def add(x,y):
  return x + y

def test_add():
  run(add, np.add)

def sub(x,y):
  return x - y 

def test_sub():
  run(sub, np.subtract)
  
def mult(x,y):
  return x * y 

def test_mult():
  run(mult, np.multiply)
  
def div(x,y):
  return x / y 

def test_div():
  run(div, np.divide)
 
def mod(x,y):
  return x % y 

def test_mod():
  run(mod, np.mod)
 
if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_cast
from parakeet.testing_helpers import expect, run_local_tests

def implicit_to_float(x):
  return x + 0.5

def test_implicit_to_float():
  expect(implicit_to_float, [1], 1.5)
  expect(implicit_to_float, [True], 1.5)

def implicit_to_bool(b):
  if b:
    return 10
  else:
    return -10

def test_implicit_to_bool():
  expect(implicit_to_bool, [1], 10)
  expect(implicit_to_bool, [2], 10)
  expect(implicit_to_bool, [0], -10)
  expect(implicit_to_bool, [1.0], 10)
  expect(implicit_to_bool, [2.0], 10)
  expect(implicit_to_bool, [0.0], -10)

if __name__ == '__main__':
    run_local_tests()

########NEW FILE########
__FILENAME__ = test_closure_args
from parakeet.testing_helpers import expect, run_local_tests
import numpy as np 

global_int = 5 
global_tuple = (5,3.0)
global_array = np.array([1,2,3])

def use_globals(x):
  return x + global_int + global_array + global_tuple[1]

def test_use_globals():
  expect(use_globals, [2], use_globals(2))

def use_locals(x):
  local_int = global_int
  local_array = global_array 
  local_tuple = global_tuple
  local_int2 = 3
  local_tuple2 = (5.0,9.0)
  local_array2 = np.array([4,5,6])
  def nested(y):
    return x + y + local_int + local_array + local_tuple[1] + local_int2 + local_tuple2[1] + local_array2
  return nested(x+1)

def test_use_locals():
  expect(use_locals, [True], use_locals(True))

if __name__ == "__main__":
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_div_bool
import numpy as np 
from parakeet import jit, testing_helpers 

@jit 
def true_divided(x):
    return True / x

def test_true_divided_bool():
    testing_helpers.expect(true_divided, [True], True)

def test_true_divided_int():
    testing_helpers.expect(true_divided, [1], 1)
    testing_helpers.expect(true_divided, [2], 0)

def test_true_divided_float():
    testing_helpers.expect(true_divided, [1.0], 1.0)
    testing_helpers.expect(true_divided, [2.0], 0.5)

def test_true_divided_uint8():
    testing_helpers.expect(true_divided, [np.uint8(1)], 1)
    testing_helpers.expect(true_divided, [np.uint8(2)], 0)

if __name__ == '__main__':
    testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_lambda
from parakeet.testing_helpers import expect, run_local_tests

def call_const_lambda():
  return (lambda: 1)()

def test_call_const_lambda():
  expect(call_const_lambda, [], 1)

def call_identity_lambda(x):
  return (lambda y: y)(x)

def test_call_identity_lambda():
  expect(call_identity_lambda, [1], 1)

def call_closure_lambda(x):
  return (lambda y: x + y)(x)

def test_call_closure_lambda():
  expect(call_closure_lambda, [1], 2)

if __name__ == "__main__":
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_list_comp
import numpy as np 
from parakeet.testing_helpers import expect, run_local_tests


x = np.array([1,2,3,4,5])

def identity_comprehension(x):
  return [xi for xi in x]

def test_identity_comprehension():
  expect(identity_comprehension, [x], x)


def sqr_elts(x):
  return [xi**2 for xi in x]

def test_sqr_elts():
  expect(sqr_elts, [x], x**2)
  
def outer_prod(x, y):
  return [[xi * yi for xi in x] for yi in y]


def test_outer_prod():
  x = np.array([1.0,2.0,3.0])
  y = np.array([10,20])
  res = np.array(outer_prod(x,y))
  expect(outer_prod, [x,y], res)
  
def triple_nesting(x):
  return [[[zi + yi + xi for zi in x] for yi in x] for xi in x]

def test_triple_nesting():
  x = np.array([1,2,3])
  expect(triple_nesting, [x], np.array(triple_nesting(x)))

def repeat_elts(x):
  return [x[i:i+2] for i in range(len(x)-1)]

def test_repeat_elts():
  x = np.array([0,1,2,3,4,5])
  y = np.array([[0,1], [1,2], [2,3], [3,4], [4,5]])
  expect(repeat_elts, [x], y)


if __name__ == '__main__':
  run_local_tests()
  

########NEW FILE########
__FILENAME__ = test_loops
from parakeet.testing_helpers import expect, run_local_tests

def for_range_loop(start, stop, step):
  x = 0
  for _ in range(start, stop, step):
    x = x + 1
  return x

def test_for_range_loop():
  expect(for_range_loop, [1,10,2], for_range_loop(1,10,2))

def test_for_backwards():
  expect(for_range_loop, [10,1,-1], for_range_loop(10,1,-1))

def count_loop(init, count):
  x = init
  while x < count:
    x = x + 1
  return x

def test_count_loop():
  expect(count_loop, [0, 300], 300)
  expect(count_loop, [0.0, 400], 400.0)
  expect(count_loop, [0.0, 500.0], 500.0)

def nested_double_count(x):
  total = 0
  i = 0
  while i < x:
    j = 0
    total = total + 1
    while j < x:
      total = total + 1
      j = j + 1
    i = i + 1
  return total

def test_nested_double_count():
  expect(nested_double_count, [10], 110)

def nested_mult(x,y):
  total_count = 0
  i = 0
  while i < x:
    j = 0
    while j < y:
      total_count = total_count + 1
      j = j + 1
    i = i + 1
  return total_count

def test_nested_mult():
  expect(nested_mult, [10, 11], 110)

def conditional_loop(x):
  i = 0
  if x:
    while i < 10:
      i = i + 1
  return i

def test_conditional_loop():
  expect(conditional_loop, [True], 10)
  expect(conditional_loop, [False], 0)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_modulo
from parakeet import jit 
from parakeet.testing_helpers import run_local_tests, expect

@jit 
def mod(x,y):
    return x % y

def run_mod(x, y):
  expect(mod, [x,y], x%y)

def test_mod_positive():
  run_mod(0,4)
  run_mod(4,4)
  run_mod(4,5)
  run_mod(5,4)
  run_mod(1,10000001)
  run_mod(10000001,1)

def test_mod_1st_negative():
  run_mod(-4,4)
  run_mod(-4,5)
  run_mod(-5,4)
  run_mod(-1,10000001)
  run_mod(-10000001,1)

def test_mod_2nd_negative():
  run_mod(0,-4)
  run_mod(4,-4)
  run_mod(4,-5)
  run_mod(5,-4)
  run_mod(1,-10000001)
  run_mod(10000001,-1)


def test_mod_both_negative():
  run_mod(-4,-4)
  run_mod(-4,-5)
  run_mod(-5,-4)
  run_mod(-1,-10000001)
  run_mod(-10000001,-1)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_simple
from parakeet.testing_helpers import expect, run_local_tests 
import numpy as np 

def always_1():
  return 1

def test_always_1():
  expect(always_1, [], 1)

def always_neg10():
  return -10

def test_always_neg10():
  expect(always_neg10, [], -10)

def always_true():
  return True

def test_always_true():
  expect(always_true, [], True, valid_types = (np.bool, np.bool8, np.bool_, bool))

def always_false():
  return False 

def test_always_false():
  expect(always_false, [], False, valid_types = (np.bool, np.bool8, np.bool_, bool))
  
def add1(x):
  return x + 1

def test_add1():
  expect(add1, [1],  2)

def call_add1(x):
  return add1(x)

def test_call_add1():
  expect(call_add1, [1],  2)

def call_nested_ident(x):
  def ident(y):
    return y
  return ident(x)

def test_nested_ident():
  expect(call_nested_ident, [1], 1)

global_val = 5
def use_global(x):
  return x + global_val

def test_use_global():
  expect(use_global, [3], 8)

def use_if_exp(x):
  return 1 if x < 10 else 2

def test_if_exp():
  expect(use_if_exp, [9], 1)
  expect(use_if_exp, [10], 2)

def simple_branch(x):
  if x < 10:
    return 1
  else:
    return 2

def test_simple_branch():
  expect(simple_branch, [9], 1)
  expect(simple_branch, [10], 2)

def simple_merge(x):
  if x == 0:
    y = 1
  else:
    y = x
  return y

def test_simple_merge():
  expect(simple_merge, [2], 2)
  expect(simple_merge, [0], 1)

def one_sided_merge(x,b):
  if b:
      x = 1
  return x

def test_one_sided_merge():
  expect(one_sided_merge, [100,True], 1)
  expect(one_sided_merge, [100,False], 100)

def if_true_const():
  if True:
    return 1
  else:
    return 2

def test_if_true_const():
  expect(if_true_const, [], 1)




if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_square
from parakeet.testing_helpers import run_local_tests, expect_eq

def square(x):
  return np.square(x)

def test_square_scalars():
  expect_eq(square(1), 1)
  expect_eq(square(2), 4)
  expect_eq(square(-2), 4)
  expect_eq(square(True), True)
  expect_eq(square(False), False)
  expect_eq(square(-4.0), 16.0)

import numpy as np 
intvec = np.array([-3,-2,-1,0,1,2,3])
floatvec = intvec.astype('float')

def test_square_vectors():
  expect_eq(square(intvec), intvec*intvec)
  expect_eq(square(floatvec), floatvec*floatvec)

intmat = np.array([intvec, intvec])
floatmat = intmat.astype('float')

def test_square_matrices():
  expect_eq(square(intmat), intmat*intmat)
  expect_eq(square(floatmat), floatmat*floatmat)


if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_tuples
import numpy as np
from parakeet.testing_helpers import expect, run_local_tests

def return_pair():
  return (-1.0, 200)

def test_return_pair():
  expect(return_pair, [], (-1.0, 200))

ints = (np.int32(1), np.int32(2))
mixed = (1.0, 200L)
nested = (ints, mixed)
nested2 = (nested, nested)

def all_tuples(f, unpack_args = True):
  """
  Given a function which should act as the identity, test it on multiple tuples
  """

  for t in [ints, mixed, nested2, nested2]:
    if unpack_args:
      expect(f, t, t)
    else:
      expect(f, [t], t)

def create_tuple(x,y):
  return (x,y)

def test_create_tuple():
  all_tuples(create_tuple)

#def tuple_arg((x,y)):
#  return (x,y)

#def test_tuple_arg():
#  all_tuples(tuple_arg, unpack_args = False)

def tuple_lhs(t):
  x,y = t
  return x,y

def test_tuple_lhs():
  all_tuples(tuple_lhs, unpack_args = False)

def tuple_lhs_sum(t):
  x, y, z = t
  return x + y + z

def test_tuple_lhs_sum():
  tuples = [(True, 0, 1.0), (1, True, 0)]
  for t in tuples:
    expect(tuple_lhs_sum, [t], sum(t))

def tuple_indexing(t):
  return (t[0], t[1])

def test_tuple_indexing():
  all_tuples(tuple_indexing, unpack_args = False)

def or_elts((b1,b2)):
  if b1 or b2:
    return 1
  else:
    return 0

def test_or_elts():
  expect(or_elts, [(True, True)], 1)
  expect(or_elts, [(True, False)], 1)
  expect(or_elts, [(False, True)], 1)
  expect(or_elts, [(False, False)], 0)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_type_values
import numpy as np
import parakeet 

from parakeet import jit 
from parakeet.testing_helpers import run_local_tests, expect

@jit 
def float32_cast(x):
  return parakeet.float32(x)

def test_int_to_float32():
  res = float32_cast(0)
  assert type(res) == np.float32 
  assert res == 0.0
  res = float32_cast(1)
  assert type(res) == np.float32 
  assert res == 1.0
  res = float32_cast(-1)
  assert type(res) == np.float32 
  assert res == -1.0

def test_float_to_float32():
  res = float32_cast(0.0)
  assert type(res) == np.float32 
  assert res == 0.0
  res = float32_cast(1.0)
  assert type(res) == np.float32 
  assert res == 1.0
  res = float32_cast(-1.0)
  assert type(res) == np.float32 
  assert res == -1.0
  

def test_bool_to_float32():
  res = float32_cast(False)
  assert type(res) == np.float32 
  assert res == 0.0
  res = float32_cast(True)
  assert type(res) == np.float32 
  assert res == 1.0

@jit 
def uint8_cast(x):
  return parakeet.uint8(x)

def test_int_to_uint8():
  res = uint8_cast(0)
  assert type(res) == np.uint8 
  assert res == 0.0
  res = uint8_cast(1)
  assert type(res) == np.uint8 
  assert res == 1
  res = uint8_cast(-1)
  assert type(res) == np.uint8 
  assert res == 255

def test_float_to_uint8():
  res = uint8_cast(0.0)
  assert type(res) == np.uint8 
  assert res == 0
  res = uint8_cast(1.0)
  assert type(res) == np.uint8 
  assert res == 1
  res = uint8_cast(-1.0)
  assert type(res) == np.uint8 
  assert res == 255
  

def test_bool_to_uint8():
  res = uint8_cast(False)
  assert type(res) == np.uint8 
  assert res == 0
  res = uint8_cast(True)
  assert type(res) == np.uint8 
  assert res == 1
  
@jit
def type_as_arg(t):
  return t(1)
  
def test_type_as_arg():
  int_res = type_as_arg(np.dtype('int16'))
  assert int_res == 1
  assert type(int_res) == np.int16
  
  float_res = type_as_arg(np.dtype('float64'))
  assert float_res == 1.0
  assert type(float_res) == np.float64
  
@jit
def type_as_default_arg(n, t=parakeet.float64):
  return t(n)
  
def test_type_as_default_arg():
  float_res = type_as_default_arg(-10)
  assert float_res == -10.0
  assert type(float_res) == np.float64
  
  uint_res = type_as_default_arg(10, np.dtype('uint64'))
  assert uint_res == 10
  assert type(uint_res) == np.uint64

@jit
def call_type_conv(n, t):
  return type_as_default_arg(n, t)

def test_call_type_conv():
  float_res = call_type_conv(-1000, parakeet.float64)
  assert type(float_res) == np.float64
  assert float_res == -1000.0, "Expected -1000.0, got %s" % float_res

@jit
def type_as_value(n):
  t2 = np.bool8
  return t2(n)


def test_type_as_value():
  expect(type_as_value, [1], True)
  expect(type_as_value, [0], False)
  
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_zip
import numpy as np 
from parakeet.testing_helpers import run_local_tests, expect 

def use_zip(x,y):
  return zip(x,y)

def test_zip_tuples():
  a = (1,2,3)
  b = (np.array([1.0]), np.array([2.0]), np.array([3.0]))
  expect(use_zip, [a,b], tuple(zip(a,b)))

# TODO: 
# Figure out some plan for returning 'dtype' containing
# multiple 
#def test_zip_arrays():
#  a = np.array([1,2,3])
#  b = np.array([True,False,False])
#  expect(use_zip, [a,b], zip(a,b))


if __name__ == '__main__':
  run_local_tests() 


########NEW FILE########
__FILENAME__ = test_morphology
import numpy as np
import time 

import parakeet

from parakeet import jit, pmap2 
from parakeet.testing_helpers import eq, run_local_tests


def winmin(x):
  m,n = x.shape
  v = x[0,0]
  for i in range(m):
    for j in range(n):
      v2 = x[i,j]
      if v2 < v:
        v = v2
  return v

def winmax(x):
  m,n = x.shape
  v = x[0,0]
  for i in range(m):
    for j in range(n):
      v2 = x[i,j]
      if v2 > v:
        v = v2
  return v

def erode(X, window_size = (3,3)):
  return pmap2(winmin, X, window_size)
 
def dilate(X, window_size = (3,3)):
  return pmap2(winmax, X, window_size)



X = np.array([[0, 0,   0,   0,   0],
              [0, 0.1,  0.2, 0.3, 0],
              [0, 0.3,  0.4, 0.3, 0],
              [0, 0.2,  0.1, 0.5, 0],
              [0, 0,    0,   0,   0]])

# what we would expect after a 3x3 erosion 
X_erode = np.array([[0, 0, 0,   0, 0],
                    [0, 0, 0,   0, 0],
                    [0, 0, 0.1, 0, 0],
                    [0, 0, 0,   0, 0],
                    [0, 0, 0,   0, 0]])

X_dilate = np.array([[0.1, 0.2, 0.3, 0.3, 0.3],
                     [0.3, 0.4, 0.4, 0.4, 0.3],
                     [0.3, 0.4, 0.5, 0.5, 0.5],
                     [0.3, 0.4, 0.5, 0.5, 0.5],
                     [0.2, 0.2, 0.5, 0.5, 0.5]])

def test_erode():
  res = erode(X, (3,3))
  assert res.shape == X_erode.shape, "Expected shape %s but got %s" % (X_erode.shape, res.shape)
  assert (res == X_erode).all(), "Expected %s but got %s" % (X_erode, res)

def test_dilate():
  res = dilate(X, (3,3))
  assert res.shape == X_dilate.shape, "Expected shape %s but got %s" % (X_dilate.shape, res.shape)
  assert (res == X_dilate).all(), "Expected %s but got %s" % (X_dilate, res)



def dilate_1d_naive(x_strip, y_strip, window_size):
  """
  Given a 1-dimensional input and 1-dimensional output, 
  fill output with 1d dilation of input 
  """
  nelts = len(x_strip)
  half = window_size / 2 
  for i in xrange(nelts):
    left_idx = max(i-half,0)
    right_idx = min(i+half+1, nelts)
    currmax = x_strip[left_idx]
    for j in xrange(left_idx+1, right_idx):
      elt = x_strip[j]
      if elt > currmax:
        currmax = elt
    y_strip[i] = currmax 

@jit 
def dilate_decompose(x, window_size): 
  m,n = x.shape
  k,l = window_size
  y = np.empty_like(x)
  z = np.empty_like(x)
  for row_idx in xrange(m):
    dilate_1d_naive(x[row_idx,:], y[row_idx,:], k)
  for col_idx in xrange(n):
    dilate_1d_naive(y[:, col_idx], z[:, col_idx], l)
  return z

def test_dilate_decompose():
  res = dilate_decompose(X, (3,3))
  assert res.shape == X_dilate.shape, "Expected shape %s but got %s" % (X_dilate.shape, res.shape)
  assert (res == X_dilate).all(), "Original \n%s Expected dilation \n%s but got \n%s unequal elts \n%s" % \
    (X, X_dilate, res, X_dilate != res)

def morph_open(x, erode_shape, dilate_shape = None):
  if dilate_shape is None: 
    dilate_shape = erode_shape
  return dilate(erode(x, erode_shape), dilate_shape)


def morph_close(x, dilate_shape, erode_shape = None):
  if erode_shape is None: 
    erode_shape = dilate_shape
  return erode(dilate(x, dilate_shape), erode_shape)


plot_rgb = False 

if __name__ == '__main__':
  run_local_tests() 

########NEW FILE########
__FILENAME__ = test_patchmap
import numpy as np

import parakeet 
from parakeet import testing_helpers

def erode(x, shape):
  return parakeet.pmap2(min, x, shape)

x = np.array([[1,2,3],[4,5,6]])

def test_erode_identity_shape():
  testing_helpers.expect(erode, [x, (1,1)], x)

def test_erode():
  shape = (3,3)
  expected = np.array([[1, 1, 2], [1,1,2]])
  testing_helpers.expect(erode, [x, shape], expected)

if __name__ == '__main__':
  testing_helpers.run_local_tests()

########NEW FILE########
__FILENAME__ = test_windowed
import parakeet 
from parakeet import jit, testing_helpers, c_backend, config 
import numpy as np

c_backend.print_function_source = True
c_backend.print_module_source = True 

@jit
def avg1d(x):
  return sum(x) / float(len(x))

def test_avg1d():
  x = np.random.randn(20)
  testing_helpers.eq(x.mean(), avg1d(x))

@jit
def winmap_avg1d(x, w = 3):
  return parakeet.pmap1(avg1d, x, w)

def test_winmap_avg1d():
  x = np.random.randn(20)**2
  y = winmap_avg1d(x)
  assert x.shape == y.shape

 
@jit
def avg2d(x):
  nelts = x.shape[0] * x.shape[1]
  return sum(sum(x)) / float(nelts)

def test_avg2d():
  x = np.random.randn(20,30)
  testing_helpers.eq(x.mean(), avg2d(x))

@jit
def winmap_zeros(x, wx = 3, wy = 3):
  def zero(_):
    return 0
  return parakeet.pmap2(zero, x, (wx, wy))

def test_winmap_zeros():
  x = np.random.randn(100,100)
  y = winmap_zeros(x)
  assert y.sum() == 0

@jit
def winmap_first_elt(x, wx = 3, wy = 3):
  def f(window):
    return window[0,0]
  return parakeet.pmap2(f, x, wx, wy)


def test_window_shapes():
  x = np.array([0,1,2,3,4])
  def winshape(x):
    return x.shape[0] 
  y = parakeet.pmap1(winshape, x, 3)
  
  expected = np.array([2,3,3,3,2])
  assert y.shape == expected.shape, "Expected shape %s but got %s" % (expected.shape, y.shape)
  assert np.all(y == expected), "Expected array %s but got %s" % (expected, y)

@jit
def winavg2d( x, wx = 3, wy = 3):
  return parakeet.pmap2(avg2d, x, (wx, wy))

def test_winavg2d():
  x = np.random.rand(100,100)
  x[50:65, 50:65] = 0
  y = winavg2d(x)
  assert x.shape==y.shape
  assert x.max() >= y.max()
  assert x.min() <= y.min()
  
if __name__ == '__main__':
  testing_helpers.run_local_tests()
  

########NEW FILE########
__FILENAME__ = test_dot
import numpy as np


from parakeet.testing_helpers import run_local_tests, expect

mat_shape = (2,3)
vec_len = np.prod(mat_shape)
int64_vec = np.arange(vec_len)

def get_vector(t):
  if t == 'bool':
    return int64_vec % 2
  else:
    return int64_vec.astype(t)

def run_vv(xtype, ytype):
  x = get_vector(xtype)
  y = get_vector(ytype)
  expect(np.dot, [x,y], np.dot(x,y))


def run_mv(xtype, ytype):
  x = get_vector(xtype)
  y = get_vector(ytype)
  x = x.reshape(mat_shape)
  y = y[:x.shape[1]]
  expect(np.dot, [x,y], np.dot(x,y))


def run_vm(xtype, ytype):
  x = get_vector(xtype)
  y = get_vector(ytype)
  
  y = y.reshape(mat_shape)
  x = x[:y.shape[0]]

  expect(np.dot, [x,y], np.dot(x,y))

def run_mm(xtype, ytype):
  x = get_vector(xtype)
  y = get_vector(ytype)
  x = x.reshape(mat_shape)
  y = y.reshape(mat_shape).T
  expect(np.dot, [x,y], np.dot(x,y))

#
# Vector-Vector
#
def test_dot_vv_i64_i32():
  run_vv('int64', 'int32')

def test_dot_vv_i64_f32():
  run_vv('int64', 'float32')

def test_dot_vv_i64_bool():
  run_vv('int64', 'bool')

def test_dot_vv_f64_f32():
  run_vv('float64', 'float32')

def test_dot_vv_f64_i32():
  run_vv('float64', 'int32')

def test_dot_vv_f64_bool():
  run_vv('float64', 'bool')

def test_dot_vv_i32_i32():
  run_vv('int32', 'int32')

def test_dot_vv_f32_f32():
  run_vv('float32', 'float32')

def test_dot_vv_bool_bool():
  run_vv('bool', 'bool')
#
# Matrix-Matrix
#

def test_dot_mm_i64_i32():
  run_mm('int64', 'int32')

def test_dot_mm_i64_f32():
  run_mm('int64', 'float32')

def test_dot_mm_bool_f32():
  run_mm('bool', 'float32')

#
#  Matrix-Vector 
#


def test_dot_mv_i64_i32():
  run_mv('int64', 'int32')

def test_dot_mv_i64_i32():
  run_mv('int64', 'int32')

#
# Vector-Matrix
#
def test_dot_vm_i64_i32():
  run_vm('int64', 'int32')

def test_dot_vm_bool_float64():
  run_vm('bool', 'float64')

if __name__ == '__main__':
    run_local_tests()

########NEW FILE########
__FILENAME__ = test_linspace
import numpy as np 
from parakeet import testing_helpers 

def test_simple_linspace():
  testing_helpers.expect(np.linspace, [0.2, 0.9], np.linspace(0.2, 0.9))

def test_linspace_with_count():
  testing_helpers.expect(np.linspace, [-0.2, 0.9, 30], np.linspace(-0.2, 0.9, 30))

if __name__ == "__main__":
  testing_helpers.run_local_tests()



########NEW FILE########
__FILENAME__ = test_log1p

import numpy as np 

from parakeet import config 
from parakeet.testing_helpers import run_local_tests, expect 

config.print_transform_times = True 

w,h = 2,2
n = w *h 
vec = np.arange(n)
mat = vec.reshape((w,h))

dtypes = ['int8', 'int16', 'int32', 'int64', 
          'uint8', 'uint16', 'uint32', 'uint64', 
          'float32', 'float64', 'bool']

scalars = [True, 1, 1.0]
vecs = []
mats = []
for dtype_name in dtypes:
  scalars.append(np.dtype(dtype_name).type(3))
  vecs.append(vec.astype(dtype_name))
  mats.append(mat.astype(dtype_name))

def test_log1p_scalar():
  for scalar in scalars:
    print scalar, type(scalar)
    expect(np.log1p, [scalar], np.log1p(scalar))

def test_log1p_vec():
  for vec in vecs:
    expect(np.log1p, [vec], np.log1p(vec))

def test_log1p_mat():
  for mat in mats:
    expect(np.log1p, [mat], np.log1p(mat))

if __name__ == "__main__": 
    run_local_tests()

########NEW FILE########
__FILENAME__ = test_logaddexp2

import numpy as np
from parakeet.testing_helpers import expect, run_local_tests


int_vec = np.array([2,3])
int_mat = np.array([int_vec, int_vec])

bool_vec = np.array([True, True])
bool_mat = np.array([bool_vec, bool_vec])

float32_vec = np.array([1.0, 2.0], dtype='float32')
float32_mat = np.array([float32_vec, float32_vec])

float64_vec = np.array([1.0, 10.0])
float64_mat = np.array([float64_vec, float64_vec])

arrays = [int_vec, bool_vec, float32_vec, float64_mat]

def test_logaddexp2_bool_bool_vec():
  expect(np.logaddexp2, [bool_vec, bool_vec], np.logaddexp2(bool_vec,bool_vec))

def test_logaddexp2_bool_bool_mat():
  expect(np.logaddexp2, [bool_mat, bool_mat], np.logaddexp2(bool_mat,bool_mat))

def test_logaddexp2_int_bool_vec():
  expect(np.logaddexp2, [int_vec, bool_vec], np.logaddexp2(int_vec,bool_vec))

def test_logaddexp2_int_bool_mat():
  expect(np.logaddexp2, [int_mat, bool_mat], np.logaddexp2(int_mat,bool_mat))

def test_logaddexp2_int_int_vec():
  expect(np.logaddexp2, [int_vec, int_vec], np.logaddexp2(int_vec,int_vec))

def test_logaddexp2_int_int_mat():
  expect(np.logaddexp2, [int_mat, int_mat], np.logaddexp2(int_mat,int_mat))

def test_logaddexp2_int_float32_vec():
  expect(np.logaddexp2, [int_vec, float32_vec], np.logaddexp2(int_vec,float32_vec))

def test_logaddexp2_int_float32_mat():
  expect(np.logaddexp2, [int_mat, float32_mat], np.logaddexp2(int_mat,float32_mat))

def test_logaddexp2_int_float64_vec():
  expect(np.logaddexp2, [int_vec, float64_vec], np.logaddexp2(int_vec,float64_vec))

def test_logaddexp2_int_float64_mat():
  expect(np.logaddexp2, [int_mat, float64_mat], np.logaddexp2(int_mat,float64_mat))

def test_logaddexp2_float32_float64_vec():
  expect(np.logaddexp2, [float32_vec, float64_vec], np.logaddexp2(float32_vec,float64_vec))

def test_logaddexp2_float32_float64_mat():
  expect(np.logaddexp2, [float32_mat, float64_mat], np.logaddexp2(float32_mat,float64_mat))

if __name__ == "__main__":
  run_local_tests()


########NEW FILE########
__FILENAME__ = test_prob
from parakeet import  testing_helpers
import math 

def test_erfc():
  testing_helpers.expect(math.erfc, [0], math.erfc(0))
  testing_helpers.expect(math.erfc, [0.5], math.erfc(0.5))
  testing_helpers.expect(math.erfc, [1], math.erfc(1))

def test_erf():
  testing_helpers.expect(math.erf, [0], math.erf(0))
  testing_helpers.expect(math.erf, [0.5], math.erf(0.5))
  testing_helpers.expect(math.erf, [1], math.erf(1))
  
if __name__ == "__main__":
  testing_helpers.run_local_tests()
########NEW FILE########
__FILENAME__ = test_rint
import numpy as np 
import parakeet
from parakeet.testing_helpers import run_local_tests 


@parakeet.jit
def rint(x):
    return np.rint(x)

def test_rint():
    assert np.rint(1.2) == rint(1.2)
    assert np.rint(-1) == rint(-1)
    assert np.rint(-0.1) == rint(-0.1)
    x = np.array([-1, 2, 0])
    assert np.allclose(np.rint(x), rint(x))
    y = np.array([-1.1, -0.6, -0.4, 0.4, 0.6])
    assert np.allclose(np.rint(y), rint(y))

if __name__ == "__main__":
    run_local_tests()

########NEW FILE########
__FILENAME__ = test_sum
import numpy as np
import parakeet
from parakeet.testing_helpers import expect, run_local_tests, expect_each


m = 5
n = 3
float_mat = np.random.random((m,n))
int_mat = float_mat.astype('int16')
bool_mat = int_mat > 0
matrices = [float_mat, int_mat, bool_mat]

def each_sum(X):
  return parakeet.each(np.sum, X)

def test_each_sum():
  expect_each(each_sum, lambda X: np.sum(X, axis=1), matrices)

def sum_cols(X):
  return np.sum(X, axis = 0)

def test_col_sum():
  expect_each(sum_cols, lambda X: np.sum(X, axis=0), matrices)

def sum_rows(X):
  return np.sum(X, axis = 1)

def test_sum_rows():
  expect_each(sum_rows, lambda X: np.sum(X, axis=1), matrices)

def test_sum_elts():
  expect_each(np.sum, np.sum, matrices)

if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_transcendental
import math 
import numpy as np
import parakeet
from parakeet.testing_helpers import run_local_tests, expect_each

xs = [0.1, 0.5, 2.0]

def test_sin():
  expect_each(parakeet.sin, np.sin, xs)
  expect_each(math.sin, math.sin, xs)
  expect_each(np.sin, np.sin, xs)


def test_parakeet_sinh():
  expect_each(parakeet.sinh, np.sinh, xs)
  expect_each(math.sinh, np.sinh, xs)
  expect_each(np.sinh, np.sinh, xs)


def test_cos():
  expect_each(parakeet.cos, np.cos, xs)
  expect_each(math.cos, np.cos, xs)
  expect_each(np.cos, np.cos, xs)

def test_cosh():
  expect_each(parakeet.cosh, np.cosh, xs)
  expect_each(math.cosh, np.cosh, xs)
  expect_each(np.cosh, np.cosh, xs)

def test_tan():
  expect_each(parakeet.tan, np.tan, xs)
  expect_each(math.tan, np.tan, xs)
  expect_each(np.tan, np.tan, xs)

def test_tanh():
  expect_each(parakeet.tanh, np.tanh, xs)
  expect_each(math.tanh, np.tanh, xs)
  expect_each(np.tanh, np.tanh, xs)
  
def test_log():
  expect_each(parakeet.log, np.log, xs)
  expect_each(math.log, np.log, xs)
  expect_each(np.log, np.log, xs)

def test_log10():
  expect_each(parakeet.log10, np.log10, xs)
  expect_each(math.log10, np.log10, xs)
  expect_each(np.log10, np.log10, xs)

def test_exp():
  expect_each(parakeet.exp, np.exp, xs)
  expect_each(math.exp, np.exp, xs)
  expect_each(np.exp, np.exp, xs)
  
if __name__ == '__main__':
  run_local_tests()

########NEW FILE########
__FILENAME__ = test_ufunc_math_binary
import numpy as np 

from parakeet.testing_helpers import run_local_tests, expect 


int_vec = np.array([2,3])
int_mat = np.array([int_vec, int_vec])

bool_vec = np.array([True, True])
bool_mat = np.array([bool_vec, bool_vec])

float32_vec = np.array([1.0, 2.0], dtype='float32')
float32_mat = np.array([float32_vec, float32_vec])

float64_vec = np.array([1.0, 10.0])
float64_mat = np.array([float64_vec, float64_vec])

vecs = [int_vec, bool_vec, float32_vec, float64_vec]
mats = [int_mat, bool_mat, float32_mat, float64_mat]

def binary(fn):
  for x in mats:
    for y in mats:
      expect(fn, [x,y], fn(x,y), x.dtype)
      
def test_add():
  binary(np.add)

def test_subtract():
  binary(np.subtract)

def test_multiply():
  binary(np.multiply)
  
def test_divide():
  binary(np.divide)
  
def test_logaddexp():
  binary(np.logaddexp)
  
def test_logaddexp2():
  binary(np.logaddexp2)

def test_true_divide():
  binary(np.true_divide)
  
def test_floor_divide():
  binary(np.floor_divide)
def test_power():
  binary(np.power)

def test_remainder():
  binary(np.remainder)
  
def test_mod():
  binary(np.mod)
  
def test_fmod():
  binary(np.fmod)
  
      
if __name__ == '__main__':
  run_local_tests() 
  

########NEW FILE########
__FILENAME__ = test_ufunc_math_unary

import numpy as np 

from parakeet.testing_helpers import run_local_tests, expect 

int_vec = np.array([2,3])
int_mat = np.array([int_vec, int_vec])

bool_vec = np.array([True, True])
bool_mat = np.array([bool_vec, bool_vec])

float32_vec = np.array([1.0, 2.0], dtype='float32')
float32_mat = np.array([float32_vec, float32_vec])

float64_vec = np.array([1.0, 10.0])
float64_mat = np.array([float64_vec, float64_vec])

vecs = [int_vec, bool_vec, float32_vec, float64_vec]
mats = [int_mat, bool_mat, float32_mat, float64_mat]


def unary(fn): 
  for x in mats:
    try:
      expected = fn(x)
    except:
      expected = fn(x.astype('int'))
    if expected.dtype == 'float16':
      expected = fn(x.astype('int'))
    expect(fn, [x], expected, fn.__name__ + "-" + str(x.dtype) + str(len(x.shape)))

def test_negative():
  unary(np.negative)

def test_absolute():
  unary(np.absolute) 
  
def test_rint():
  unary(np.rint)

def test_sign():
  unary(np.sign)
  
def test_conj():
  unary(np.conj)
    
def test_exp():
  unary(np.exp)

def test_exp2():
  unary(np.exp2)
  
def test_log():
  unary(np.log)
  
def test_log2():
  unary(np.log2)

def test_log10():
  unary(np.log10)

def test_expm1():
  unary(np.expm1)
  
def test_log1p():
  unary(np.log1p)
  
def test_sqrt():
  unary(np.sqrt)
  
def test_square():
  unary(np.square)
  
def test_reciprocal():
  unary(np.reciprocal)
  
def test_ones_like():
  unary(np.ones_like)

########NEW FILE########
__FILENAME__ = test_ufunc_trig
import numpy as np 

from parakeet.testing_helpers import run_local_tests, expect  


int_array = np.array([0,1])
bool_array = np.array([False,True])
float_array = np.array([0.1, 0.2])
arrays = [int_array, bool_array, float_array]

def unary(fn, data = None): 
  for x in (arrays if data is None else data):
    expect(fn, [x], fn(x))

def binary(fn):
  for x in arrays:
      for y in arrays:
          expect(fn, [x,y], fn(x,y))

def test_sin():
  unary(np.sin)

def test_cos():
  unary(np.cos)
  
def test_tan():
  unary(np.tan)
  
def test_arcsin():
  unary(np.arcsin)
  
def test_arccos():
  unary(np.arccos)
  
def test_arctan():
  unary(np.arctan)
  
def test_arctan2():
  binary(np.arctan2)
  
  
def test_sinh():
  unary(np.sinh)
  
def test_cosh():
  unary(np.cosh)
  
def test_tanh():
  unary(np.tanh)
  
def test_arcsinh():
  unary(np.arcsinh)
  
def test_arccosh():
  unary(np.arccosh, data = np.array([2.0, 3.0]))
  
def test_arctanh():
  unary(np.arctanh, data = np.array([0.2, 0.3]))
  
def test_deg2rad():
  unary(np.deg2rad)
  
def test_rad2deg():
  unary(np.rad2deg)
  
#def test_hypot():
#  binary(np.hypot)

if __name__ == '__main__':
  run_local_tests() 

########NEW FILE########
