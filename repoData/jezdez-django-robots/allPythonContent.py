__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# django-robots documentation build configuration file, created by
# sphinx-quickstart on Wed Nov 21 11:54:26 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('..'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.txt'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'django-robots'
copyright = u'2008-2014, Jannis Leidel'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
try:
    from robots import __version__
    # The short X.Y version.
    version = '.'.join(__version__.split('.')[:2])
    # The full version, including alpha/beta/rc tags.
    release = __version__
except ImportError:
    version = release = 'dev'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'django-robotsdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'django-robots.tex', u'django-robots Documentation',
   u'Jannis Leidel', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'django-robots', u'django-robots Documentation',
     [u'Jannis Leidel'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'django-robots', u'django-robots Documentation',
   u'Jannis Leidel', 'django-robots', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from django.utils.translation import ugettext_lazy as _

from robots.models import Url, Rule
from robots.forms import RuleAdminForm


class RuleAdmin(admin.ModelAdmin):
    form = RuleAdminForm
    fieldsets = (
        (None, {'fields': ('robot', 'sites')}),
        (_('URL patterns'), {
            'fields': ('allowed', 'disallowed'),
        }),
        (_('Advanced options'), {
            'classes': ('collapse',),
            'fields': ('crawl_delay',),
        }),
    )
    list_filter = ('sites',)
    list_display = ('robot', 'allowed_urls', 'disallowed_urls')
    search_fields = ('robot', 'urls')

admin.site.register(Url)
admin.site.register(Rule, RuleAdmin)

########NEW FILE########
__FILENAME__ = forms
from django import forms
from django.utils.translation import ugettext_lazy as _

from robots.models import Rule


class RuleAdminForm(forms.ModelForm):
    class Meta:
        model = Rule

    def clean(self):
        if (not self.cleaned_data.get("disallowed", False) and
                not self.cleaned_data.get("allowed", False)):
            raise forms.ValidationError(
                _('Please specify at least one allowed or dissallowed URL.'))
        return self.cleaned_data

########NEW FILE########
__FILENAME__ = 0001_initial
# -*- coding: utf-8 -*-
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Adding model 'Url'
        db.create_table('robots_url', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('pattern', self.gf('django.db.models.fields.CharField')(max_length=255)),
        ))
        db.send_create_signal('robots', ['Url'])

        # Adding model 'Rule'
        db.create_table('robots_rule', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('robot', self.gf('django.db.models.fields.CharField')(max_length=255)),
            ('crawl_delay', self.gf('django.db.models.fields.DecimalField')(null=True, max_digits=3, decimal_places=1, blank=True)),
        ))
        db.send_create_signal('robots', ['Rule'])

        # Adding M2M table for field allowed on 'Rule'
        db.create_table('robots_rule_allowed', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('rule', models.ForeignKey(orm['robots.rule'], null=False)),
            ('url', models.ForeignKey(orm['robots.url'], null=False))
        ))
        db.create_unique('robots_rule_allowed', ['rule_id', 'url_id'])

        # Adding M2M table for field disallowed on 'Rule'
        db.create_table('robots_rule_disallowed', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('rule', models.ForeignKey(orm['robots.rule'], null=False)),
            ('url', models.ForeignKey(orm['robots.url'], null=False))
        ))
        db.create_unique('robots_rule_disallowed', ['rule_id', 'url_id'])

        # Adding M2M table for field sites on 'Rule'
        db.create_table('robots_rule_sites', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('rule', models.ForeignKey(orm['robots.rule'], null=False)),
            ('site', models.ForeignKey(orm['sites.site'], null=False))
        ))
        db.create_unique('robots_rule_sites', ['rule_id', 'site_id'])

    def backwards(self, orm):
        # Deleting model 'Url'
        db.delete_table('robots_url')

        # Deleting model 'Rule'
        db.delete_table('robots_rule')

        # Removing M2M table for field allowed on 'Rule'
        db.delete_table('robots_rule_allowed')

        # Removing M2M table for field disallowed on 'Rule'
        db.delete_table('robots_rule_disallowed')

        # Removing M2M table for field sites on 'Rule'
        db.delete_table('robots_rule_sites')

    models = {
        'robots.rule': {
            'Meta': {'object_name': 'Rule'},
            'allowed': ('django.db.models.fields.related.ManyToManyField', [], {'symmetrical': 'False', 'related_name': "'allowed'", 'blank': 'True', 'to': "orm['robots.Url']"}),
            'crawl_delay': ('django.db.models.fields.DecimalField', [], {'null': 'True', 'max_digits': '3', 'decimal_places': '1', 'blank': 'True'}),
            'disallowed': ('django.db.models.fields.related.ManyToManyField', [], {'symmetrical': 'False', 'related_name': "'disallowed'", 'blank': 'True', 'to': "orm['robots.Url']"}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'robot': ('django.db.models.fields.CharField', [], {'max_length': '255'}),
            'sites': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['sites.Site']", 'symmetrical': 'False'})
        },
        'robots.url': {
            'Meta': {'object_name': 'Url'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'pattern': ('django.db.models.fields.CharField', [], {'max_length': '255'})
        },
        'sites.site': {
            'Meta': {'ordering': "('domain',)", 'object_name': 'Site', 'db_table': "'django_site'"},
            'domain': ('django.db.models.fields.CharField', [], {'max_length': '100'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '50'})
        }
    }

    complete_apps = ['robots']

########NEW FILE########
__FILENAME__ = models
from django.db import models
from django.contrib.sites.models import Site
from django.utils.translation import ugettext_lazy as _
from django.utils.text import get_text_list


class Url(models.Model):
    """
    Defines a URL pattern for use with a robot exclusion rule. It's
    case-sensitive and exact, e.g., "/admin" and "/admin/" are different URLs.
    """
    pattern = models.CharField(_('pattern'), max_length=255, help_text=_(
                               "Case-sensitive. A missing trailing slash does al"
                               "so match to files which start with the name of "
                               "the pattern, e.g., '/admin' matches /admin.html "
                               "too. Some major search engines allow an asterisk"
                               " (*) as a wildcard and a dollar sign ($) to "
                               "match the end of the URL, e.g., '/*.jpg$'."))

    class Meta:
        verbose_name = _('url')
        verbose_name_plural = _('url')

    def __unicode__(self):
        return u"%s" % self.pattern

    def save(self, *args, **kwargs):
        if not self.pattern.startswith('/'):
            self.pattern = '/' + self.pattern
        super(Url, self).save(*args, **kwargs)


class Rule(models.Model):
    """
    Defines an abstract rule which is used to respond to crawling web robots,
    using the robot exclusion standard, a.k.a. robots.txt. It allows or
    disallows the robot identified by its user agent to access the given URLs.
    The Site contrib app is used to enable multiple robots.txt per instance.
    """
    robot = models.CharField(_('robot'), max_length=255, help_text=_(
                             "This should be a user agent string like "
                             "'Googlebot'. Enter an asterisk (*) for all "
                             "user agents. For a full list look at the "
                             "<a target=_blank href='"
                             "http://www.robotstxt.org/db.html"
                             "'> database of Web Robots</a>."))

    allowed = models.ManyToManyField(Url, blank=True, related_name="allowed",
                                     verbose_name=_('allowed'),
                                     help_text=_("The URLs which are allowed "
                                                 "to be accessed by bots."))

    disallowed = models.ManyToManyField(Url, blank=True, related_name="disallowed",
                                        verbose_name=_('disallowed'),
                                        help_text=_("The URLs which are not "
                                                    "allowed to be accessed "
                                                    "by bots."))
    sites = models.ManyToManyField(Site, verbose_name=_('sites'))

    crawl_delay = models.DecimalField(_('crawl delay'), blank=True, null=True,
                                      max_digits=3, decimal_places=1, help_text=_(
                                      "Between 0.1 and 99.0. This field is "
                                      "supported by some search engines and "
                                      "defines the delay between successive "
                                      "crawler accesses in seconds. If the "
                                      "crawler rate is a problem for your "
                                      "server, you can set the delay up to 5 "
                                      "or 10 or a comfortable value for your "
                                      "server, but it's suggested to start "
                                      "with small values (0.5-1), and "
                                      "increase as needed to an acceptable "
                                      "value for your server. Larger delay "
                                      "values add more delay between "
                                      "successive crawl accesses and "
                                      "decrease the maximum crawl rate to "
                                      "your web server."))

    class Meta:
        verbose_name = _('rule')
        verbose_name_plural = _('rules')

    def __unicode__(self):
        return u"%s" % self.robot

    def allowed_urls(self):
        return get_text_list(list(self.allowed.all()), _('and'))
    allowed_urls.short_description = _('allowed')

    def disallowed_urls(self):
        return get_text_list(list(self.disallowed.all()), _('and'))
    disallowed_urls.short_description = _('disallowed')

########NEW FILE########
__FILENAME__ = settings
from django.conf import settings

#: A list of one or more sitemaps to inform robots about:
SITEMAP_URLS = []
SITEMAP_URLS.extend(getattr(settings, 'ROBOTS_SITEMAP_URLS', []))

USE_SITEMAP = getattr(settings, 'ROBOTS_USE_SITEMAP', True)

CACHE_TIMEOUT = getattr(settings, 'ROBOTS_CACHE_TIMEOUT', None)

########NEW FILE########
__FILENAME__ = urls
try:
    from django.conf.urls import patterns, url
except ImportError:
    from django.conf.urls.defaults import patterns, url

urlpatterns = patterns(
    'robots.views',
    url(r'^$', 'rules_list', name='robots_rule_list'),
)

########NEW FILE########
__FILENAME__ = views
from django.core.urlresolvers import reverse, NoReverseMatch
from django.views.decorators.cache import cache_page
from django.views.generic import ListView

from django.contrib.sites.models import Site

from robots.models import Rule
from robots import settings


class RuleList(ListView):
    """
    Returns a generated robots.txt file with correct mimetype (text/plain),
    status code (200 or 404), sitemap url (automatically).
    """
    model = Rule
    context_object_name = 'rules'
    cache_timeout = settings.CACHE_TIMEOUT

    def get_current_site(self, request):
        return Site.objects.get_current()

    def reverse_sitemap_url(self):
        try:
            return reverse('django.contrib.sitemaps.views.index')
        except NoReverseMatch:
            try:
                return reverse('django.contrib.sitemaps.views.sitemap')
            except NoReverseMatch:
                pass

    def get_sitemap_urls(self):
        sitemap_urls = settings.SITEMAP_URLS

        if not sitemap_urls and settings.USE_SITEMAP:
            scheme = self.request.is_secure() and 'https' or 'http'
            sitemap_url = self.reverse_sitemap_url()

            if sitemap_url is not None:
                if not sitemap_url.startswith(('http', 'https')):
                    sitemap_url = "%s://%s%s" % (scheme, self.current_site.domain, sitemap_url)
                if sitemap_url not in sitemap_urls:
                    sitemap_urls.append(sitemap_url)

        return sitemap_urls

    def get_queryset(self):
        return Rule.objects.filter(sites=self.current_site)

    def get_context_data(self, **kwargs):
        context = super(RuleList, self).get_context_data(**kwargs)
        context['sitemap_urls'] = self.get_sitemap_urls()
        return context

    def render_to_response(self, context, **kwargs):
        return super(RuleList, self).render_to_response(context,
            content_type='text/plain', **kwargs)

    def get_cache_timeout(self):
        return self.cache_timeout

    def dispatch(self, request, *args, **kwargs):
        cache_timeout = self.get_cache_timeout()
        self.current_site = self.get_current_site(request)
        super_dispatch = super(RuleList, self).dispatch
        if not cache_timeout:
            return super_dispatch(request, *args, **kwargs)
        key_prefix = self.current_site.domain
        cache_decorator = cache_page(cache_timeout, key_prefix=key_prefix)
        return cache_decorator(super_dispatch)(request, *args, **kwargs)


rules_list = RuleList.as_view()

########NEW FILE########
