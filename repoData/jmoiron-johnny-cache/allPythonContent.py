__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Johnny Cache documentation build configuration file, created by
# sphinx-quickstart on Thu Feb 18 22:05:30 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
projpath = os.path.abspath('..')
sys.path.append(projpath)

from django.core.management import setup_environ
import settings
setup_environ(settings)

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Johnny Cache'
copyright = u'2010, Jason Moiron, Jeremy Self'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = None
for line in open(os.path.join(projpath, 'setup.py'), 'r'):
    if line.startswith('version'):
        exec line
if version is None:
    version = '0.1'
# The full version, including alpha/beta/rc tags.
release = version

print ("Building release: %s, version: %s" % (release, version))

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'nature'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_theme']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = False

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'JohnnyCachedoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'JohnnyCache.tex', u'Johnny Cache Documentation',
   u'Jason Moiron, Jeremy Self', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

########NEW FILE########
__FILENAME__ = filebased
"""
Infinite file-based caching.  Caches forever when passed timeout of 0.
"""

import sys

from django.core.cache.backends import filebased

# NOTE: We aren't using smart_str here, because the underlying library will
# perform a call to md5 that chokes on that type of input;  we'll just not
# fret the encoding, and things will work alright even with unicode table
# names


class FileBasedCache(filebased.FileBasedCache):
    def add(self, key, value, timeout=None, **kwargs):
        if timeout is 0:
            timeout = sys.maxsize
        return super(FileBasedCache, self).add(
            key, value, timeout=timeout, **kwargs)

    def set(self, key, value, timeout=None, **kwargs):
        if timeout is 0:
            timeout = sys.maxsize
        return super(FileBasedCache, self).set(
            key, value, timeout=timeout, **kwargs)

########NEW FILE########
__FILENAME__ = locmem
"""
Infinite caching locmem class.  Caches forever when passed timeout of 0.

This actually doesn't cache "forever", just for a very long time.  On
32 bit systems, it will cache for 68 years, quite a bit longer than any
computer will last.  On a 64 bit machine, your cache will expire about
285 billion years after the Sun goes red-giant and destroys Earth.
"""

import sys
from django.core.cache.backends import locmem


class LocMemCache(locmem.LocMemCache):

    def add(self, key, value, timeout=None, **kwargs):
        if timeout is 0:
            timeout = sys.maxsize
        return super(LocMemCache, self).add(
            key, value, timeout=timeout, **kwargs)

    def set(self, key, value, timeout=None, **kwargs):
        if timeout is 0:
            timeout = sys.maxsize
        return super(LocMemCache, self).set(
            key, value, timeout=timeout, **kwargs)

########NEW FILE########
__FILENAME__ = memcached
"""
Infinite caching memcached classes.  Caches forever when passed a timeout
of 0.
"""

import logging

from django.core.cache.backends import memcached


class MemcachedCache(memcached.MemcachedCache):
    """
    Infinitely Caching version of django's MemcachedCache backend.
    """
    def _get_memcache_timeout(self, timeout=None):
        if timeout == 0:
            return 0  # 2591999
        return super(MemcachedCache, self)._get_memcache_timeout(timeout)


class PyLibMCCache(memcached.PyLibMCCache):
    """
    PyLibMCCache version that interprets 0 to mean, roughly, 30 days.
    This is because `pylibmc interprets 0 to mean literally zero seconds
    <http://sendapatch.se/projects/pylibmc/misc.html#differences-from-python-memcached>`_
    rather than "infinity" as memcached itself does.  The maximum timeout
    memcached allows before treating the timeout as a timestamp is just
    under 30 days.
    """
    def _get_memcache_timeout(self, timeout=None):
        # pylibmc doesn't like our definition of 0
        if timeout == 0:
            return 2591999
        return super(PyLibMCCache, self)._get_memcache_timeout(timeout)


class FailSilentlyMemcachedCache(MemcachedCache):
    """
    It may happen that we're trying to cache something bigger that the
    max allowed per key on memcached. Instead of failing with a ValueError
    exception, this backend allows to ignore that, even if it means
    not to store the cached value, but at least the application will
    keep working.
    """
    def set(self, *args, **kwargs):
        try:
            super(FailSilentlyMemcachedCache, self).set(*args, **kwargs)
        except ValueError:
            logging.warning("Couldn't set the key for the cache")

########NEW FILE########
__FILENAME__ = redis
"""
Redis cache classes that forcebly limits the timeout of the redis
cache backend to 30 days to make sure the cache doesn't fill up
when johnny always caches queries. Redis doesn't have an automatic
cache invalidation other than timeouts.

This module depends on the ``django-redis-cache`` app from PyPI.
"""

from redis_cache import cache as redis


class RedisCache(redis.RedisCache):
    def set(self, key, value, timeout=None, *args, **kwargs):
        if timeout == 0:
            timeout = 2591999
        return super(RedisCache, self).set(key, value, timeout,
                                           *args, **kwargs)

########NEW FILE########
__FILENAME__ = cache
"""Johnny's main caching functionality."""

from hashlib import md5
from uuid import uuid4

import django
from django.db.models.signals import post_save, post_delete

from . import localstore, signals
from . import settings
from .compat import (
    force_bytes, force_text, string_types, text_type, empty_iter)
from .decorators import wraps, available_attrs
from .transaction import TransactionManager


class NotInCache(object):
    #This is used rather than None to properly cache empty querysets
    pass

no_result_sentinel = "22c52d96-156a-4638-a38d-aae0051ee9df"
local = localstore.LocalStore()


def disallowed_table(*tables):
    """Returns True if a set of tables is in the blacklist or, if a whitelist is set,
    any of the tables is not in the whitelist. False otherwise."""
    # XXX: When using a black or white list, this has to be done EVERY query;
    # It'd be nice to make this as fast as possible.  In general, queries
    # should have relatively few tables involved, and I don't imagine that
    # blacklists would grow very vast.  The fastest i've been able to come
    # up with is to pre-create a blacklist set and use intersect.
    return not bool(settings.WHITELIST.issuperset(tables)) if settings.WHITELIST\
        else bool(settings.BLACKLIST.intersection(tables))


def get_backend(**kwargs):
    """
    Get's a QueryCacheBackend object for the given options and current
    version of django.  If no arguments are given, and a QCB has been
    created previously, ``get_backend`` returns that.  Otherwise,
    ``get_backend`` will return the default backend.
    """
    cls = QueryCacheBackend
    return cls(**kwargs)

def enable():
    """Enable johnny-cache, for use in scripts, management commands, async
    workers, or other code outside the django request flow."""
    get_backend().patch()

def disable():
    """Disable johnny-cache.  This will disable johnny-cache for the whole
    process, and if writes happen during the time where johnny is disabled,
    tables will not be invalidated properly.  Use Carefully."""
    get_backend().unpatch()

patch,unpatch = enable,disable

def resolve_table(x):
    """Return a table name for x, where x is either a model instance or a string."""
    if isinstance(x, string_types):
        return x
    return x._meta.db_table


def invalidate(*tables, **kwargs):
    """Invalidate the current generation for one or more tables.  The arguments
    can be either strings representing database table names or models.  Pass in
    kwarg ``using`` to set the database."""
    backend = get_backend()
    db = kwargs.get('using', 'default')

    if backend._patched:
        for t in map(resolve_table, tables):
            backend.keyhandler.invalidate_table(t, db)


def get_tables_for_query(query):
    """
    Takes a Django 'query' object and returns all tables that will be used in
    that query as a list.  Note that where clauses can have their own
    querysets with their own dependent queries, etc.
    """
    from django.db.models.sql.where import WhereNode, SubqueryConstraint
    from django.db.models.query import QuerySet
    tables = [v[0] for v in getattr(query,'alias_map',{}).values()]

    def get_sub_query_tables(node):
        query = node.query_object
        if not hasattr(query, 'field_names'):
            query = query.values(*node.targets)
        else:
            query = query._clone()
        query = query.query
        return [v[0] for v in getattr(query, 'alias_map',{}).values()]

    def get_tables(node, tables):
        if isinstance(node, SubqueryConstraint):
            return get_sub_query_tables(node)
        for child in node.children:
            if isinstance(child, WhereNode):  # and child.children:
                tables = get_tables(child, tables)
            elif not hasattr(child, '__iter__'):
                continue
            else:
                for item in (c for c in child if isinstance(c, QuerySet)):
                    tables += get_tables_for_query(item.query)
        return tables

    if query.where and query.where.children:
        where_nodes = [c for c in query.where.children if isinstance(c, (WhereNode, SubqueryConstraint))]
        for node in where_nodes:
            tables += get_tables(node, tables)

    return list(set(tables))

def get_tables_for_query_pre_16(query):
    """
    Takes a Django 'query' object and returns all tables that will be used in
    that query as a list.  Note that where clauses can have their own
    querysets with their own dependent queries, etc.
    """
    from django.db.models.sql.where import WhereNode
    from django.db.models.query import QuerySet
    tables = [v[0] for v in getattr(query,'alias_map',{}).values()]

    def get_tables(node, tables):
        for child in node.children:
            if isinstance(child, WhereNode):  # and child.children:
                tables = get_tables(child, tables)
            elif not hasattr(child, '__iter__'):
                continue
            else:
                for item in (c for c in child if isinstance(c, QuerySet)):
                    tables += get_tables_for_query(item.query)
        return tables

    if query.where and query.where.children:
        where_nodes = [c for c in query.where.children if isinstance(c, WhereNode)]
        for node in where_nodes:
            tables += get_tables(node, tables)

    return list(set(tables))


if django.VERSION[:2] < (1, 6):
    get_tables_for_query = get_tables_for_query_pre_16


# The KeyGen is used only to generate keys.  Some of these keys will be used
# directly in the cache, while others are only general purpose functions to
# generate hashes off of one or more values.

class KeyGen(object):
    """This class is responsible for generating keys."""

    def __init__(self, prefix):
        self.prefix = prefix

    def random_generator(self):
        """Creates a random unique id."""
        return self.gen_key(force_bytes(uuid4()))

    def gen_table_key(self, table, db='default'):
        """
        Returns a key that is standard for a given table name and database
        alias. Total length up to 212 (max for memcache is 250).
        """
        table = force_text(table)
        db = force_text(settings.DB_CACHE_KEYS[db])
        if len(table) > 100:
            table = table[0:68] + self.gen_key(table[68:])
        if db and len(db) > 100:
            db = db[0:68] + self.gen_key(db[68:])
        return '%s_%s_table_%s' % (self.prefix, db, table)

    def gen_multi_key(self, values, db='default'):
        """Takes a list of generations (not table keys) and returns a key."""
        db = settings.DB_CACHE_KEYS[db]
        if db and len(db) > 100:
            db = db[0:68] + self.gen_key(db[68:])
        return '%s_%s_multi_%s' % (self.prefix, db, self.gen_key(*values))

    @staticmethod
    def _convert(x):
        if isinstance(x, text_type):
            return x.encode('utf-8')
        return force_bytes(x)

    @staticmethod
    def _recursive_convert(x, key):
        for item in x:
            if isinstance(item, (tuple, list)):
                KeyGen._recursive_convert(item, key)
            else:
                key.update(KeyGen._convert(item))

    def gen_key(self, *values):
        """Generate a key from one or more values."""
        key = md5()
        KeyGen._recursive_convert(values, key)
        return key.hexdigest()


class KeyHandler(object):
    """Handles pulling and invalidating the key from from the cache based
    on the table names.  Higher-level logic dealing with johnny cache specific
    keys go in this class."""
    def __init__(self, cache_backend, keygen=KeyGen, prefix=None):
        self.prefix = prefix
        self.keygen = keygen(prefix)
        self.cache_backend = cache_backend

    def get_generation(self, *tables, **kwargs):
        """Get the generation key for any number of tables."""
        db = kwargs.get('db', 'default')
        if len(tables) > 1:
            return self.get_multi_generation(tables, db)
        return self.get_single_generation(tables[0], db)

    def get_single_generation(self, table, db='default'):
        """Creates a random generation value for a single table name"""
        key = self.keygen.gen_table_key(table, db)
        val = self.cache_backend.get(key, None, db)
        #if local.get('in_test', None): print force_bytes(val).ljust(32), key
        if val is None:
            val = self.keygen.random_generator()
            self.cache_backend.set(key, val, settings.MIDDLEWARE_SECONDS, db)
        return val

    def get_multi_generation(self, tables, db='default'):
        """Takes a list of table names and returns an aggregate
        value for the generation"""
        generations = []
        for table in tables:
            generations.append(self.get_single_generation(table, db))
        key = self.keygen.gen_multi_key(generations, db)
        val = self.cache_backend.get(key, None, db)
        #if local.get('in_test', None): print force_bytes(val).ljust(32), key
        if val is None:
            val = self.keygen.random_generator()
            self.cache_backend.set(key, val, settings.MIDDLEWARE_SECONDS, db)
        return val

    def invalidate_table(self, table, db='default'):
        """Invalidates a table's generation and returns a new one
        (Note that this also invalidates all multi generations
        containing the table)"""
        key = self.keygen.gen_table_key(table, db)
        val = self.keygen.random_generator()
        self.cache_backend.set(key, val, settings.MIDDLEWARE_SECONDS, db)
        return val

    def sql_key(self, generation, sql, params, order, result_type,
                using='default'):
        """
        Return the specific cache key for the sql query described by the
        pieces of the query and the generation key.
        """
        # these keys will always look pretty opaque
        suffix = self.keygen.gen_key(sql, params, order, result_type)
        using = settings.DB_CACHE_KEYS[using]
        return '%s_%s_query_%s.%s' % (self.prefix, using, generation, suffix)


# XXX: Thread safety concerns?  Should we only need to patch once per process?
class QueryCacheBackend(object):
    """This class is the engine behind the query cache. It reads the queries
    going through the django Query and returns from the cache using
    the generation keys, or on a miss from the database and caches the results.
    Each time a model is updated the table keys for that model are re-created,
    invalidating all cached querysets for that model.

    There are different QueryCacheBackend's for different versions of django;
    call ``johnny.cache.get_backend`` to automatically get the proper class.
    """
    __shared_state = {}

    def __init__(self, cache_backend=None, keyhandler=None, keygen=None):
        self.__dict__ = self.__shared_state
        self.prefix = settings.MIDDLEWARE_KEY_PREFIX
        if keyhandler:
            self.kh_class = keyhandler
        if keygen:
            self.kg_class = keygen
        if not cache_backend and not hasattr(self, 'cache_backend'):
            cache_backend = settings._get_backend()

        if not keygen and not hasattr(self, 'kg_class'):
            self.kg_class = KeyGen
        if keyhandler is None and not hasattr(self, 'kh_class'):
            self.kh_class = KeyHandler

        if cache_backend:
            self.cache_backend = TransactionManager(cache_backend,
                                                    self.kg_class)
            self.keyhandler = self.kh_class(self.cache_backend,
                                            self.kg_class, self.prefix)
        self._patched = getattr(self, '_patched', False)

    def _monkey_select(self, original):
        from django.db.models.sql.constants import MULTI
        from django.db.models.sql.datastructures import EmptyResultSet

        @wraps(original, assigned=available_attrs(original))
        def newfun(cls, *args, **kwargs):
            if args:
                result_type = args[0]
            else:
                result_type = kwargs.get('result_type', MULTI)

            if any([isinstance(cls, c) for c in self._write_compilers]):
                return original(cls, *args, **kwargs)
            try:
                sql, params = cls.as_sql()
                if not sql:
                    raise EmptyResultSet
            except EmptyResultSet:
                if result_type == MULTI:
                    return empty_iter()
                else:
                    return

            db = getattr(cls, 'using', 'default')
            key, val = None, NotInCache()
            # check the blacklist for any of the involved tables;  if it's not
            # there, then look for the value in the cache.
            tables = get_tables_for_query(cls.query)
            # if the tables are blacklisted, send a qc_skip signal
            blacklisted = disallowed_table(*tables)

            try:
                ordering_aliases = cls.ordering_aliases
            except AttributeError:
                ordering_aliases = cls.query.ordering_aliases

            if blacklisted:
                signals.qc_skip.send(sender=cls, tables=tables,
                    query=(sql, params, ordering_aliases),
                    key=key)
            if tables and not blacklisted:
                gen_key = self.keyhandler.get_generation(*tables, **{'db': db})
                key = self.keyhandler.sql_key(gen_key, sql, params,
                                              cls.get_ordering(),
                                              result_type, db)
                val = self.cache_backend.get(key, NotInCache(), db)

            if not isinstance(val, NotInCache):
                if val == no_result_sentinel:
                    val = []

                signals.qc_hit.send(sender=cls, tables=tables,
                        query=(sql, params, ordering_aliases),
                        size=len(val), key=key)
                return val

            if not blacklisted:
                signals.qc_miss.send(sender=cls, tables=tables,
                    query=(sql, params, ordering_aliases),
                    key=key)

            val = original(cls, *args, **kwargs)

            if hasattr(val, '__iter__'):
                #Can't permanently cache lazy iterables without creating
                #a cacheable data structure. Note that this makes them
                #no longer lazy...
                #todo - create a smart iterable wrapper
                val = list(val)
            if key is not None:
                if not val:
                    self.cache_backend.set(key, no_result_sentinel, settings.MIDDLEWARE_SECONDS, db)
                else:
                    self.cache_backend.set(key, val, settings.MIDDLEWARE_SECONDS, db)
            return val
        return newfun

    def _monkey_write(self, original):
        @wraps(original, assigned=available_attrs(original))
        def newfun(cls, *args, **kwargs):
            db = getattr(cls, 'using', 'default')
            from django.db.models.sql import compiler
            # we have to do this before we check the tables, since the tables
            # are actually being set in the original function
            ret = original(cls, *args, **kwargs)

            if isinstance(cls, compiler.SQLInsertCompiler):
                #Inserts are a special case where cls.tables
                #are not populated.
                tables = [cls.query.model._meta.db_table]
            else:
                #if cls.query.tables != list(cls.query.table_map):
                #    pass
                #tables = list(cls.query.table_map)
                tables = cls.query.tables
            for table in tables:
                if not disallowed_table(table):
                    self.keyhandler.invalidate_table(table, db)
            return ret
        return newfun

    def patch(self):
        """
        monkey patches django.db.models.sql.compiler.SQL*Compiler series
        """
        from django.db.models.sql import compiler

        self._read_compilers = (
            compiler.SQLCompiler,
            compiler.SQLAggregateCompiler,
            compiler.SQLDateCompiler,
        )
        self._write_compilers = (
            compiler.SQLInsertCompiler,
            compiler.SQLDeleteCompiler,
            compiler.SQLUpdateCompiler,
        )
        if not self._patched:
            self._original = {}
            for reader in self._read_compilers:
                self._original[reader] = reader.execute_sql
                reader.execute_sql = self._monkey_select(reader.execute_sql)
            for updater in self._write_compilers:
                self._original[updater] = updater.execute_sql
                updater.execute_sql = self._monkey_write(updater.execute_sql)
            self._patched = True
            self.cache_backend.patch()
            self._handle_signals()

    def unpatch(self):
        """un-applies this patch."""
        if not self._patched:
            return
        for func in self._read_compilers + self._write_compilers:
            func.execute_sql = self._original[func]
        self.cache_backend.unpatch()
        self._patched = False

    def invalidate(self, instance, **kwargs):
        if self._patched:
            table = resolve_table(instance)
            using = kwargs.get('using', 'default')
            if not disallowed_table(table):
                self.keyhandler.invalidate_table(table, db=using)

            tables = set()
            tables.add(table)

            try:
                 instance._meta._related_objects_cache
            except AttributeError:
                 instance._meta._fill_related_objects_cache()

            for obj in instance._meta._related_objects_cache.keys():
                obj_table = obj.model._meta.db_table
                if obj_table not in tables:
                    tables.add(obj_table)
                    if not disallowed_table(obj_table):
                        self.keyhandler.invalidate_table(obj_table)

    def _handle_signals(self):
        post_save.connect(self.invalidate, sender=None)
        post_delete.connect(self.invalidate, sender=None)

    def flush_query_cache(self):
        from django.db import connection
        tables = connection.introspection.table_names()
        #seen_models = connection.introspection.installed_models(tables)
        for table in tables:
            # we want this to just work, so invalidate even things in blacklist
            self.keyhandler.invalidate_table(table)

########NEW FILE########
__FILENAME__ = compat
# coding: utf-8

"""
Tools to ease compatibility across supported versions of Django & Python.
"""

from __future__ import unicode_literals
from django.db.models.sql import compiler


try:
    from queue import Queue
except ImportError:  # Python < 3.0
    from Queue import Queue

import django
from django.db import transaction

try:
    from django.utils.encoding import force_bytes, force_text
    from django.utils.six import string_types, text_type
except ImportError:  # Django < 1.4.2
    force_bytes = str
    force_text = unicode
    string_types = (str, unicode)
    text_type = unicode


__all__ = (
    'Queue', 'force_bytes', 'force_text', 'string_types', 'text_type',
    'empty_iter', 'is_managed', 'managed',
)


def empty_iter():
    if django.VERSION[:2] >= (1, 5):
        return iter([])
    return compiler.empty_iter()


def is_managed(using=None):
    if django.VERSION[:2] < (1, 6):
        return transaction.is_managed(using=using)
    elif django.VERSION[:2] >= (1, 6):
        # See https://code.djangoproject.com/ticket/21004
        return not transaction.get_autocommit(using=using)
    return False


def managed(flag=True, using=None):
    if django.VERSION[:2] < (1, 6):
        transaction.managed(flag=flag, using=using)
    elif django.VERSION[:2] >= (1, 6):
        transaction.set_autocommit(not flag, using=using)

########NEW FILE########
__FILENAME__ = decorators
from functools import wraps, WRAPPER_ASSIGNMENTS


def available_attrs(fn):
    """
    Return the list of functools-wrappable attributes on a callable.
    This is required as a workaround for http://bugs.python.org/issue3445.
    """
    return tuple(a for a in WRAPPER_ASSIGNMENTS if hasattr(fn, a))

########NEW FILE########
__FILENAME__ = localstore
"""Threadlocal OpenStruct-like cache."""

import re
import fnmatch
import threading
import warnings

from .compat import string_types


class LocalStore(threading.local):
    """
    A thread-local OpenStruct that can be used as a local cache.  An instance
    is located at ``johnny.cache.local``, and is cleared on every request by
    the ``LocalStoreClearMiddleware``.  It can be a thread-safe way to handle
    global contexts.
    """
    def __init__(self, **d):
        threading.local.__init__(self)
        for k, v in d.items():
            threading.local.__setattr__(self, k, v)

    # dictionary API
    def __getitem__(self, key):
        return self.__dict__[key]

    def __setitem__(self, key, value):
        self.__dict__[key] = value

    def __delitem__(self, key):
        if key in self.__dict__:
            del self.__dict__[key]

    def __iter__(self):
        return iter(self.__dict__)

    def __len__(self):
        return len(self.__dict__)

    def keys(self):
        return self.__dict__.keys()

    def values(self):
        return self.__dict__.values()

    def items(self):
        return self.__dict__.items()

    def iterkeys(self):
        warnings.warn(
            'LocalStore.iterkeys() is deprecated, use .keys() instead',
            DeprecationWarning)
        return self.__dict__.keys()

    def itervalues(self):
        warnings.warn(
            'LocalStore.itervalues() is deprecated, use .values() instead',
            DeprecationWarning)
        return self.__dict__.values()

    def iteritems(self):
        warnings.warn(
            'LocalStore.iteritems() is deprecated, use .items() instead',
            DeprecationWarning)
        return self.__dict__.items()

    def get(self, *args):
        return self.__dict__.get(*args)

    def update(self, d):
        self.__dict__.update(d)

    def setdefault(self, name, value):
        return self.__dict__.setdefault(name, value)

    def mget(self, pat=None):
        """
        Get a dictionary mapping of all k:v pairs with key matching
        glob style expression `pat`.
        """
        if pat is None:
            return {}
        expr = re.compile(fnmatch.translate(pat))
        m = {}
        for key in tuple(self.keys()):
            #make sure the key is a str first
            if isinstance(key, string_types):
                if expr.match(key):
                    m[key] = self[key]
        return m

    def clear(self, pat=None):
        """
        Minor diversion with built-in dict here;  clear can take a glob
        style expression and remove keys based on that expression.
        """
        if pat is None:
            return self.__dict__.clear()

        expr = re.compile(fnmatch.translate(pat))
        for key in tuple(self.keys()):
            #make sure the key is a str first
            if isinstance(key, string_types):
                if expr.match(key):
                    del self.__dict__[key]

    def __repr__(self):
        return repr(self.__dict__)

    def __str__(self):
        return str(self.__dict__)

########NEW FILE########
__FILENAME__ = middleware
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Middleware classes for johnny cache."""

from johnny import cache, settings


class QueryCacheMiddleware(object):
    """
    This middleware class monkey-patches django's ORM to maintain
    generational info on each table (model) and to automatically cache all
    querysets created via the ORM.  This should be the first middleware
    in your middleware stack.
    """
    __state = {}  # Alex Martelli's borg pattern

    def __init__(self):
        self.__dict__ = self.__state
        self.disabled = settings.DISABLE_QUERYSET_CACHE
        self.installed = getattr(self, 'installed', False)
        if not self.installed and not self.disabled:
            # when we install, lets refresh the blacklist, just in case johnny
            # was loaded before the setting exists somehow...
            cache.blacklist = settings.BLACKLIST
            self.query_cache_backend = cache.get_backend()
            self.query_cache_backend.patch()
            self.installed = True

    def unpatch(self):
        self.query_cache_backend.unpatch()
        self.query_cache_backend.flush_query_cache()
        self.installed = False


class LocalStoreClearMiddleware(object):
    """
    This middleware clears the localstore cache in `johnny.cache.local`
    at the end of every request.
    """
    def process_exception(self, *args, **kwargs):
        cache.local.clear()

    def process_response(self, req, resp):
        cache.local.clear()
        return resp

########NEW FILE########
__FILENAME__ = models

########NEW FILE########
__FILENAME__ = settings
from warnings import warn

from django.conf import settings
from django.core.cache import get_cache, cache

DISABLE_QUERYSET_CACHE = getattr(settings, 'DISABLE_QUERYSET_CACHE', False)

DEFAULT_BLACKLIST = ['south_migrationhistory']

BLACKLIST = getattr(settings, 'MAN_IN_BLACKLIST',
            getattr(settings, 'JOHNNY_TABLE_BLACKLIST', [])) + DEFAULT_BLACKLIST
BLACKLIST = set(BLACKLIST)

WHITELIST = set(getattr(settings, 'JOHNNY_TABLE_WHITELIST', []))

DB_CACHE_KEYS = dict((name, db.get('JOHNNY_CACHE_KEY', name))
                 for name, db in settings.DATABASES.items())

MIDDLEWARE_KEY_PREFIX = getattr(settings, 'JOHNNY_MIDDLEWARE_KEY_PREFIX', 'jc')

MIDDLEWARE_SECONDS = getattr(settings, 'JOHNNY_MIDDLEWARE_SECONDS', 0)

CACHE_BACKEND = getattr(settings, 'JOHNNY_CACHE_BACKEND',
                getattr(settings, 'CACHE_BACKEND', None))

CACHES = getattr(settings, 'CACHES', {})


def _get_backend():
    """
    Returns the actual django cache object johnny is configured to use.
    This relies on the settings only;  the actual active cache can
    theoretically be changed at runtime.
    """
    enabled = [n for n, c in sorted(CACHES.items())
               if c.get('JOHNNY_CACHE', False)]
    if len(enabled) > 1:
        warn("Multiple caches configured for johnny-cache; using %s." %
             enabled[0])
    if enabled:
        return get_cache(enabled[0])
    if CACHE_BACKEND:
        backend = get_cache(CACHE_BACKEND)
        if backend not in CACHES:
            from django.core import signals
            # Some caches -- python-memcached in particular -- need to do a
            # cleanup at the end of a request cycle. If the cache provides a
            # close() method, wire it up here.
            if hasattr(backend, 'close'):
                signals.request_finished.connect(backend.close)
        return backend
    return cache

########NEW FILE########
__FILENAME__ = signals
"""Defined signals for johnny-cache."""

from django.dispatch import Signal

# sent when the query cache finds a valid result in the cache
qc_hit = Signal(providing_args=['key', 'tables', 'query', 'size'])
# sent when the query cache cannot find a valid result in the cache
qc_miss = Signal(providing_args=['key', 'tables', 'query'])
# sent when johnny skips a statement because of blacklisting
qc_skip = Signal(providing_args=['key', 'tables', 'query'])

########NEW FILE########
__FILENAME__ = base
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Base test class for Johnny Cache Tests."""

from __future__ import print_function
from pprint import pformat

from django.test import TestCase, TransactionTestCase
from django.conf import settings
from django.core.management import call_command
from django.db.models.loading import load_app

from johnny import settings as johnny_settings
from johnny.compat import Queue
from johnny.decorators import wraps, available_attrs
from johnny.signals import qc_hit, qc_miss, qc_skip

# order matters here;  I guess we aren't deferring foreign key checking :\
johnny_fixtures = ['authors.json', 'genres.json', 'publishers.json', 'books.json']

def show_johnny_signals(hit=None, miss=None):
    """A decorator that can be put on a test function that will show the
    johnny hit/miss signals by default, or do what is provided by the hit
    and miss keyword args."""
    def _hit(*args, **kwargs):
        print("hit:\n\t%s\n\t%s\n" % (pformat(args), pformat(kwargs)))
    def _miss(*args, **kwargs):
        print("miss:\n\t%s\n\t%s\n" % (pformat(args), pformat(kwargs)))
    hit = hit or _hit
    miss = miss or _miss
    def deco(func):
        @wraps(func, assigned=available_attrs(func))
        def wrapped(*args, **kwargs):
            qc_hit.connect(hit)
            qc_miss.connect(miss)
            try:
                ret = func(*args, **kwargs)
            finally:
                qc_hit.disconnect(hit)
                qc_miss.disconnect(miss)
            return ret
        return wrapped
    return deco

def _pre_setup(self):
    self.saved_INSTALLED_APPS = settings.INSTALLED_APPS
    self.saved_DEBUG = settings.DEBUG
    test_app = 'johnny.tests.testapp'
    settings.INSTALLED_APPS = tuple(
        list(self.saved_INSTALLED_APPS) + [test_app]
    )
    settings.DEBUG = True
    # load our fake application and syncdb
    load_app(test_app)
    call_command('syncdb', verbosity=0, interactive=False)
    if hasattr(settings, 'DATABASES'):
        for dbname in settings.DATABASES:
            if dbname != 'default':
                call_command('syncdb', verbosity=0, interactive=False, database=dbname)

def _post_teardown(self):
    settings.INSTALLED_APPS = self.saved_INSTALLED_APPS
    settings.DEBUG = self.saved_DEBUG

class JohnnyTestCase(TestCase):
    def _pre_setup(self):
        _pre_setup(self)
        super(JohnnyTestCase, self)._pre_setup()

    def _post_teardown(self):
        _post_teardown(self)
        super(JohnnyTestCase, self)._post_teardown()

class TransactionJohnnyTestCase(TransactionTestCase):
    def _pre_setup(self):
        _pre_setup(self)
        super(TransactionJohnnyTestCase, self)._pre_setup()

    def _post_teardown(self):
        _post_teardown(self)
        super(TransactionJohnnyTestCase, self)._post_teardown()

class TransactionJohnnyWebTestCase(TransactionJohnnyTestCase):
    def _pre_setup(self):
        self.saved_MIDDLEWARE_CLASSES = settings.MIDDLEWARE_CLASSES
        if getattr(self, 'middlewares', None):
            settings.MIDDLEWARE_CLASSES = self.middlewares
        self.saved_DISABLE_SETTING = getattr(johnny_settings, 'DISABLE_QUERYSET_CACHE', False)
        johnny_settings.DISABLE_QUERYSET_CACHE = False
        self.saved_TEMPLATE_LOADERS = settings.TEMPLATE_LOADERS
        if 'django.template.loaders.app_directories.Loader' not in settings.TEMPLATE_LOADERS:
            settings.TEMPLATE_LOADERS += ('django.template.loaders.app_directories.Loader',)
        self.saved_ROOT_URLCONF = settings.ROOT_URLCONF
        settings.ROOT_URLCONF = 'johnny.tests.testapp.urls'
        super(TransactionJohnnyWebTestCase, self)._pre_setup()

    def _post_teardown(self):
        johnny_settings.DISABLE_QUERYSET_CACHE = self.saved_DISABLE_SETTING
        settings.MIDDLEWARE_CLASSES = self.saved_MIDDLEWARE_CLASSES
        settings.ROOT_URLCONF = self.saved_ROOT_URLCONF
        settings.TEMPLATE_LOADERS = self.saved_TEMPLATE_LOADERS
        super(TransactionJohnnyWebTestCase, self)._post_teardown()

class message_queue(object):
    """Return a message queue that gets 'hit' or 'miss' messages.  The signal
    handlers use weakrefs, so if we don't save references to this object they
    will get gc'd pretty fast."""
    def __init__(self):
        self.q = Queue()
        qc_hit.connect(self._hit)
        qc_miss.connect(self._miss)
        qc_skip.connect(self._skip)

    def _hit(self, *a, **k): self.q.put(True)
    def _miss(self, *a, **k): self.q.put(False)
    def _skip(self, *a, **k): self.q.put(False)

    def clear(self):
        while not self.q.empty():
            self.q.get_nowait()
    def get(self): return self.q.get()
    def get_nowait(self): return self.q.get_nowait()
    def qsize(self): return self.q.qsize()
    def empty(self): return self.q.empty()

def supports_transactions(con):
    """A convenience function which will work across multiple django versions
    that checks whether or not a connection supports transactions."""
    features = con.features
    vendor = con.vendor
    if features.supports_transactions:
        if vendor == 'mysql' \
                and getattr(features, '_mysql_storage_engine', '') != 'InnoDB':
            print('MySQL connection reports transactions supported '
                  'but storage engine != InnoDB.')
            return False
        return True
    return False

########NEW FILE########
__FILENAME__ = cache
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Tests for the QueryCache functionality of johnny."""

from __future__ import print_function
from threading import Thread

from django.conf import settings
from django.core.paginator import Paginator
from django.db import connection, connections, transaction, IntegrityError
from django.db.models import Q, Count, Sum
from johnny import middleware, settings as johnny_settings, cache
from johnny.cache import get_tables_for_query, invalidate
from johnny.compat import is_managed, managed, Queue
from johnny.signals import qc_hit, qc_miss, qc_skip
from . import base
from .testapp.models import (
    Genre, Book, Publisher, Person, PersonType, Issue24Model as i24m)


# put tests in here to be included in the testing suite
__all__ = ['MultiDbTest', 'SingleModelTest', 'MultiModelTest', 'TransactionSupportTest', 'BlackListTest', 'TransactionManagerTestCase']


def is_multithreading_safe(db_using=None):
    # SQLite is not thread-safe.
    if db_using is None:
        return all(is_multithreading_safe(db_using=db_name)
                   for db_name in settings.DATABASES)
    db_engine = settings.DATABASES.get(db_using, {}).get('ENGINE', 'sqlite3')
    return not db_engine.endswith('sqlite3')


def _pre_setup(self):
    self.saved_DISABLE_SETTING = getattr(johnny_settings, 'DISABLE_QUERYSET_CACHE', False)
    johnny_settings.DISABLE_QUERYSET_CACHE = False
    self.middleware = middleware.QueryCacheMiddleware()

def _post_teardown(self):
    self.middleware.unpatch()
    johnny_settings.DISABLE_QUERYSET_CACHE = self.saved_DISABLE_SETTING

class QueryCacheBase(base.JohnnyTestCase):
    def _pre_setup(self):
        _pre_setup(self)
        super(QueryCacheBase, self)._pre_setup()

    def _post_teardown(self):
        _post_teardown(self)
        super(QueryCacheBase, self)._post_teardown()

class TransactionQueryCacheBase(base.TransactionJohnnyTestCase):
    def _pre_setup(self):
        _pre_setup(self)
        super(TransactionQueryCacheBase, self)._pre_setup()

    def _post_teardown(self):
        _post_teardown(self)
        super(TransactionQueryCacheBase, self)._post_teardown()
        if transaction.is_dirty():
            transaction.rollback()
        if is_managed():
            managed(False)

class BlackListTest(QueryCacheBase):
    fixtures = base.johnny_fixtures

    def test_basic_blacklist(self):
        q = base.message_queue()
        old = johnny_settings.BLACKLIST
        johnny_settings.BLACKLIST = set(['testapp_genre'])
        Book.objects.get(id=1)
        Book.objects.get(id=1)
        self.assertFalse(q.get_nowait())
        self.assertTrue(q.get_nowait())
        list(Genre.objects.all())
        list(Genre.objects.all())
        self.assertFalse(q.get_nowait())
        self.assertFalse(q.get_nowait())
        johnny_settings.BLACKLIST = old


class MultiDbTest(TransactionQueryCacheBase):
    multi_db = True
    fixtures = ['genres.json', 'genres2.json']

    def _run_threaded(self, query, queue, data):
        """Runs a query (as a string) from testapp in another thread and
        puts (hit?, result) on the provided queue."""
        def _inner(_query):
            msg = []
            def hit(*args, **kwargs):
                msg.append(True)
            def miss(*args, **kwargs):
                msg.append(False)
            def skip(*args, **kwargs):
                msg.append(False)
            qc_hit.connect(hit)
            qc_miss.connect(miss)
            qc_skip.connect(skip)
            obj = eval(_query, data)
            msg.append(obj)
            queue.put(msg)
        t = Thread(target=_inner, args=(query,))
        t.start()
        t.join()

    def test_basic_queries(self):
        """Tests basic queries and that the cache is working for multiple db's"""
        if len(getattr(settings, "DATABASES", [])) <= 1:
            print("\n  Skipping multi database tests")
            return

        self.assertTrue("default" in getattr(settings, "DATABASES"))
        self.assertTrue("second" in getattr(settings, "DATABASES"))

        g1 = Genre.objects.using("default").get(pk=1)
        g1.title = "A default database"
        g1.save(using='default')
        g2 = Genre.objects.using("second").get(pk=1)
        g2.title = "A second database"
        g2.save(using='second')
        #fresh from cache since we saved each
        with self.assertNumQueries(1, using='default'):
            g1 = Genre.objects.using('default').get(pk=1)
        with self.assertNumQueries(1, using='second'):
            g2 = Genre.objects.using('second').get(pk=1)
        self.assertEqual(g1.title, "A default database")
        self.assertEqual(g2.title, "A second database")
        #should be a cache hit
        with self.assertNumQueries(0, using='default'):
            g1 = Genre.objects.using('default').get(pk=1)
        with self.assertNumQueries(0, using='second'):
            g2 = Genre.objects.using('second').get(pk=1)

    def test_cache_key_setting(self):
        """Tests that two databases use a single cached object when given the same DB cache key"""
        if len(getattr(settings, "DATABASES", [])) <= 1:
            print("\n  Skipping multi database tests")
            return

        self.assertTrue("default" in getattr(settings, "DATABASES"))
        self.assertTrue("second" in getattr(settings, "DATABASES"))

        old_cache_keys = johnny_settings.DB_CACHE_KEYS
        johnny_settings.DB_CACHE_KEYS = {'default': 'default', 'second': 'default'}

        g1 = Genre.objects.using("default").get(pk=1)
        g1.title = "A default database"
        g1.save(using='default')
        g2 = Genre.objects.using("second").get(pk=1)
        g2.title = "A second database"
        g2.save(using='second')
        #fresh from cache since we saved each
        with self.assertNumQueries(1, using='default'):
            g1 = Genre.objects.using('default').get(pk=1)
        with self.assertNumQueries(0, using='second'):
            g2 = Genre.objects.using('second').get(pk=1)
        johnny_settings.DB_CACHE_KEYS = old_cache_keys

    def test_transactions(self):
        """Tests transaction rollbacks and local cache for multiple dbs"""

        if len(getattr(settings, "DATABASES", [])) <= 1:
            print("\n  Skipping multi database tests")
            return

        if not is_multithreading_safe():
            print("\n  Skipping test requiring multiple threads.")
            return

        for conname in connections:
            con = connections[conname]
            if not base.supports_transactions(con):
                print("\n  Skipping test requiring transactions.")
                return

        q = Queue()
        other = lambda x: self._run_threaded(x, q, {'Genre': Genre})


        # sanity check 
        self.assertFalse(is_managed())
        self.assertFalse(transaction.is_dirty())
        self.assertTrue("default" in getattr(settings, "DATABASES"))
        self.assertTrue("second" in getattr(settings, "DATABASES"))

        # this should seed this fetch in the global cache
        g1 = Genre.objects.using("default").get(pk=1)
        g2 = Genre.objects.using("second").get(pk=1)
        start_g1 = g1.title

        transaction.enter_transaction_management(using='default')
        managed(using='default')
        transaction.enter_transaction_management(using='second')
        managed(using='second')

        g1.title = "Testing a rollback"
        g2.title = "Testing a commit"
        g1.save()
        g2.save()

        # test outside of transaction, should be cache hit and 
        # not contain the local changes
        other("Genre.objects.using('default').get(pk=1)")
        hit, ostart = q.get()
        self.assertEqual(ostart.title, start_g1)
        self.assertTrue(hit)

        transaction.rollback(using='default')
        transaction.commit(using='second')
        managed(False, using='default')
        managed(False, using='second')

        #other thread should have seen rollback
        other("Genre.objects.using('default').get(pk=1)")
        hit, ostart = q.get()
        self.assertEqual(ostart.title, start_g1)
        self.assertTrue(hit)

        #should be a cache hit due to rollback
        with self.assertNumQueries(0, using='default'):
            g1 = Genre.objects.using("default").get(pk=1)
        #should be a db hit due to commit
        with self.assertNumQueries(1, using='second'):
            g2 = Genre.objects.using("second").get(pk=1)

        #other thread sould now be accessing the cache after the get
        #from the commit.
        other("Genre.objects.using('second').get(pk=1)")
        hit, ostart = q.get()
        self.assertEqual(ostart.title, g2.title)
        self.assertTrue(hit)

        self.assertEqual(g1.title, start_g1)
        self.assertEqual(g2.title, "Testing a commit")
        transaction.leave_transaction_management("default")
        transaction.leave_transaction_management("second")

    def test_savepoints(self):
        """tests savepoints for multiple db's"""
        q = Queue()
        other = lambda x: self._run_threaded(x, q, {'Genre': Genre})

        if len(getattr(settings, "DATABASES", [])) <= 1:
            print("\n  Skipping multi database tests")
            return

        if not is_multithreading_safe():
            print("\n  Skipping test requiring multiple threads.")
            return

        for name, db in settings.DATABASES.items():
            con = connections[name]
            if not con.features.uses_savepoints:
                print("\n  Skipping test requiring savepoints.")
                return

        # sanity check 
        self.assertFalse(is_managed())
        self.assertFalse(transaction.is_dirty())
        self.assertTrue("default" in getattr(settings, "DATABASES"))
        self.assertTrue("second" in getattr(settings, "DATABASES"))

        g1 = Genre.objects.using("default").get(pk=1)
        start_g1 = g1.title
        g2 = Genre.objects.using("second").get(pk=1)
        start_g2 = g2.title

        transaction.enter_transaction_management(using='default')
        managed(using='default')
        transaction.enter_transaction_management(using='second')
        managed(using='second')

        g1.title = "Rollback savepoint"
        g1.save()

        g2.title = "Committed savepoint"
        g2.save(using="second")
        sid2 = transaction.savepoint(using="second")

        sid = transaction.savepoint(using="default")
        g1.title = "Dirty text"
        g1.save()

        #other thread should see the original key and cache object from memcache,
        #not the local cache version
        other("Genre.objects.using('default').get(pk=1)")
        hit, ostart = q.get()
        self.assertTrue(hit)
        self.assertEqual(ostart.title, start_g1)
        #should not be a hit due to rollback
        transaction.savepoint_rollback(sid, using="default")
        g1 = Genre.objects.using("default").get(pk=1)

        # i think it should be "Rollback Savepoint" here
        self.assertEqual(g1.title, start_g1)

        #will be pushed to dirty in commit
        g2 = Genre.objects.using("second").get(pk=1)
        self.assertEqual(g2.title, "Committed savepoint")
        transaction.savepoint_commit(sid2, using="second")

        #other thread should still see original version even 
        #after savepoint commit
        other("Genre.objects.using('second').get(pk=1)")
        hit, ostart = q.get()
        self.assertTrue(hit)
        self.assertEqual(ostart.title, start_g2)

        with self.assertNumQueries(0, using='second'):
            g2 = Genre.objects.using("second").get(pk=1)

        transaction.commit(using="second")
        managed(False, using='second')

        with self.assertNumQueries(0, using='second'):
            g2 = Genre.objects.using("second").get(pk=1)
        self.assertEqual(g2.title, "Committed savepoint")

        #now committed and cached, other thread should reflect new title
        #without a hit to the db
        other("Genre.objects.using('second').get(pk=1)")
        hit, ostart = q.get()
        self.assertEqual(ostart.title, g2.title)
        self.assertTrue(hit)

        transaction.commit(using="default")
        managed(False, 'default')
        transaction.leave_transaction_management("default")
        transaction.leave_transaction_management("second")


class SingleModelTest(QueryCacheBase):
    fixtures = base.johnny_fixtures

    def test_multi_where_cache_coherency(self):
        """A test to detect the issue described in bitbucket #24:
        https://bitbucket.org/jmoiron/johnny-cache/issue/24/
        """
        i24m.objects.get_or_create(one=1, two=1)
        i24m.objects.get_or_create(one=1, two=2)
        i24m.objects.get_or_create(one=2, two=1)
        i24m.objects.get_or_create(one=2, two=2)

        ones = i24m.objects.filter(one=1)
        twos = i24m.objects.filter(two=1)

        res = i24m.objects.filter(one__in=ones).exclude(two=twos).all()
        # XXX: I'm afraid I don't even understand what this is supposed
        # to be doing here, and in any case this test case fails.  I've
        # included something similar to the patch in #24, if someone knows
        # how to write a test case to create that condition please do so here

    def test_exists_hit(self):
        """Tests that an exist failure caches properly"""
        with self.assertNumQueries(1):
            Publisher.objects.filter(title="Doesn't Exist").exists()
            Publisher.objects.filter(title="Doesn't Exist").exists()

    def test_basic_querycaching(self):
        """A basic test that querycaching is functioning properly and is
        being invalidated properly on singular table reads & writes."""

        with self.assertNumQueries(1):
            starting_count = Publisher.objects.count()
            starting_count = Publisher.objects.count()
        self.assertEqual(starting_count, 1)

        # this write should invalidate the key we have
        Publisher(title='Harper Collins', slug='harper-collins').save()
        with self.assertNumQueries(1):
            new_count = Publisher.objects.count()
        self.assertEqual(new_count, 2)
        # this tests the codepath after 'except EmptyResultSet' where
        # result_type == MULTI
        self.assertFalse(list(Publisher.objects.filter(title__in=[])))
        # test for a regression on the WhereNode, bitbucket #20
        g1 = Genre.objects.get(pk=1)
        g1.title = "Survival Horror"
        g1.save()
        g1 = Genre.objects.get(Q(title__iexact="Survival Horror"))

    def test_querycache_return_results(self):
        """Test that the return results from the query cache are what we
        expect;  single items are single items, etc."""
        with self.assertNumQueries(1):
            pub = Publisher.objects.get(id=1)
            pub2 = Publisher.objects.get(id=1)
        self.assertEqual(pub, pub2)
        with self.assertNumQueries(1):
            pubs = list(Publisher.objects.all())
            pubs2 = list(Publisher.objects.all())
        self.assertEqual(pubs, pubs2)

    def test_delete(self):
        """Test that a database delete clears a table cache."""
        g1 = Genre.objects.get(pk=1)
        begin = Genre.objects.all().count()
        g1.delete()
        self.assertRaises(Genre.DoesNotExist, lambda: Genre.objects.get(pk=1))
        with self.assertNumQueries(1):
            self.assertEqual(Genre.objects.all().count(), begin - 1)
        Genre(title='Science Fiction', slug='scifi').save()
        Genre(title='Fantasy', slug='rubbish').save()
        Genre(title='Science Fact', slug='scifact').save()
        count = Genre.objects.count()
        Genre.objects.get(title='Fantasy')
        q = base.message_queue()
        Genre.objects.filter(title__startswith='Science').delete()
        # this should not be cached
        Genre.objects.get(title='Fantasy')
        self.assertFalse(q.get_nowait())

    def test_update(self):
        with self.assertNumQueries(3):
            g1 = Genre.objects.get(pk=1)
            Genre.objects.all().update(title="foo")
            g2 = Genre.objects.get(pk=1)
        self.assertNotEqual(g1.title, g2.title)
        self.assertEqual(g2.title, "foo")

    def test_empty_count(self):
        """Test for an empty count aggregate query with an IN"""
        books = Genre.objects.filter(id__in=[])
        count = books.count()
        self.assertEqual(count, 0)

    def test_aggregate_annotation(self):
        """Test aggregating an annotation """
        author_count = Book.objects.annotate(author_count=Count('authors')).aggregate(Sum('author_count'))
        self.assertEqual(author_count['author_count__sum'], 2)
        # also test using the paginator, although this shouldn't be a big issue..
        books = Book.objects.all().annotate(num_authors=Count('authors'))
        paginator = Paginator(books, 25)
        list_page = paginator.page(1)

    def test_queryset_laziness(self):
        """This test exists to model the laziness of our queries;  the
        QuerySet cache should not alter the laziness of QuerySets."""
        with self.assertNumQueries(1):
            qs = Genre.objects.filter(title__startswith='A')
            qs = qs.filter(pk__lte=1)
            qs = qs.order_by('pk')
            # we should only execute the query at this point
            arch = qs[0]

    def test_order_by(self):
        """A basic test that our query caching is taking order clauses
        into account."""
        with self.assertNumQueries(2):
            first = list(Genre.objects.filter(title__startswith='A').order_by('slug'))
            second = list(Genre.objects.filter(title__startswith='A').order_by('-slug'))
        # test that the orders of the results are reversed
        self.assertEqual((first[0], first[1]), (second[1], second[0]))

    def test_signals(self):
        """Test that the signals we say we're sending are being sent."""
        misses = []
        hits = []
        def qc_hit_listener(sender, **kwargs):
            hits.append(kwargs['key'])
        def qc_miss_listener(*args, **kwargs):
            misses.append(kwargs['key'])
        qc_hit.connect(qc_hit_listener)
        qc_miss.connect(qc_miss_listener)
        qc_skip.connect(qc_miss_listener)
        first = list(Genre.objects.filter(title__startswith='A').order_by('slug'))
        second = list(Genre.objects.filter(title__startswith='A').order_by('slug'))
        self.assertEqual(len(misses), 1)
        self.assertEqual(len(hits), 1)

    def test_in_values_list(self):
        pubs = Publisher.objects.all()
        books = Book.objects.filter(publisher__in=pubs.values_list("id", flat=True))
        tables = list(sorted(get_tables_for_query(books.query)))
        self.assertEqual(["testapp_book", "testapp_publisher"], tables)


class MultiModelTest(QueryCacheBase):
    fixtures = base.johnny_fixtures

    def test_foreign_keys(self):
        """Test that simple joining (and deferred loading) functions as we'd
        expect when involving multiple tables.  In particular, a query that
        joins 2 tables should invalidate when either table is invalidated."""
        with self.assertNumQueries(1):
            books = list(Book.objects.select_related('publisher'))
            books = list(Book.objects.select_related('publisher'))
            str(books[0].genre)
        books = list(Book.objects.select_related('publisher'))
        # invalidate the genre key, which shouldn't impact the query
        Genre(title='Science Fiction', slug='scifi').save()
        with self.assertNumQueries(0):
            books = list(Book.objects.select_related('publisher'))
        # now invalidate publisher, which _should_
        p = Publisher(title='McGraw Hill', slug='mcgraw-hill')
        p.save()
        with self.assertNumQueries(1):
            books = list(Book.objects.select_related('publisher'))
        # the query should be cached again...
        books = list(Book.objects.select_related('publisher'))
        # this time, create a book and the query should again be uncached..
        Book(title='Anna Karenina', slug='anna-karenina', publisher=p).save()
        with self.assertNumQueries(1):
            books = list(Book.objects.select_related('publisher'))

    def test_invalidate(self):
        """Test for the module-level invalidation function."""
        q = base.message_queue()
        b = Book.objects.get(id=1)
        invalidate(Book)
        b = Book.objects.get(id=1)
        first, second = q.get_nowait(), q.get_nowait()
        self.assertFalse(first)
        self.assertFalse(second)
        g = Genre.objects.get(id=1)
        p = Publisher.objects.get(id=1)
        invalidate('testapp_genre', Publisher)
        g = Genre.objects.get(id=1)
        p = Publisher.objects.get(id=1)
        fg,fp,sg,sp = [q.get() for i in range(4)]
        self.assertFalse(fg)
        self.assertFalse(fp)
        self.assertFalse(sg)
        self.assertFalse(sp)

    def test_many_to_many(self):
        b = Book.objects.get(pk=1)
        p1 = Person.objects.get(pk=1)
        p2 = Person.objects.get(pk=2)
        b.authors.add(p1)

        #many to many should be invalidated
        with self.assertNumQueries(1):
            list(b.authors.all())

        b.authors.remove(p1)
        b = Book.objects.get(pk=1)
        list(b.authors.all())
        #can't determine the queries here, 1.1 and 1.2 uses them differently

        #many to many should be invalidated, 
        #person is not invalidated since we just want
        #the many to many table to be
        with self.assertNumQueries(0):
            p1 = Person.objects.get(pk=1)

        p1.books.add(b)

        #many to many should be invalidated,
        #this is the first query
        with self.assertNumQueries(1):
            list(p1.books.all())
            b = Book.objects.get(pk=1)

        #query should be cached
        with self.assertNumQueries(0):
            self.assertEqual(len(list(p1.books.all())), 1)

        #testing clear
        b.authors.clear()
        self.assertEqual(b.authors.all().count(), 0)
        self.assertEqual(p1.books.all().count(), 0)
        b.authors.add(p1)
        self.assertEqual(b.authors.all().count(), 1)

        with self.assertNumQueries(0):
            b.authors.all().count()
        self.assertEqual(p1.books.all().count(), 1)
        p1.books.clear()
        self.assertEqual(b.authors.all().count(), 0)

    def test_subselect_support(self):
        """Test that subselects are handled properly."""
        with self.assertNumQueries(0):
            author_types = PersonType.objects.filter(title='Author')
            author_people = Person.objects.filter(person_types__in=author_types)
            written_books = Book.objects.filter(authors__in=author_people)
        q = base.message_queue()
        count = written_books.count()
        self.assertFalse(q.get())
        # execute the query again, this time it's cached
        self.assertEqual(written_books.count(), count)
        self.assertTrue(q.get())
        # change the person type of 'Author' to something else
        pt = PersonType.objects.get(title='Author')
        pt.title = 'NonAuthor'
        pt.save()
        self.assertEqual(PersonType.objects.filter(title='Author').count(), 0)
        q.clear()
        # now execute the same query;  the result should be diff and it should be
        # a cache miss
        new_count = written_books.count()
        self.assertNotEqual(new_count, count)
        self.assertFalse(q.get())
        PersonType.objects.filter(title='NonAuthor').order_by('-title')[:5]

    def test_foreign_key_delete_cascade(self):
        """From #32, test that if you have 'Foo' and 'Bar', with bar.foo => Foo,
        and you delete foo, bar.foo is also deleted, which means you have to
        invalidate Bar when deletions are made in Foo (but not changes)."""


class TransactionSupportTest(TransactionQueryCacheBase):
    fixtures = base.johnny_fixtures

    def _run_threaded(self, query, queue):
        """Runs a query (as a string) from testapp in another thread and
        puts (hit?, result) on the provided queue."""
        def _inner(_query):
            msg = []
            def hit(*args, **kwargs):
                msg.append(True)
            def miss(*args, **kwargs):
                msg.append(False)
            qc_hit.connect(hit)
            qc_miss.connect(miss)
            qc_skip.connect(miss)
            obj = eval(_query)
            msg.append(obj)
            queue.put(msg)
            if connections is not None:
                #this is to fix a race condition with the
                #thread to ensure that we close it before 
                #the next test runs
                connections['default'].close()
        t = Thread(target=_inner, args=(query,))
        t.start()
        t.join()

    def setUp(self):
        super(TransactionSupportTest, self).setUp()
        if is_managed():
            managed(False)

    def tearDown(self):
        if is_managed():
            if transaction.is_dirty():
                transaction.rollback()
            managed(False)

    def test_transaction_commit(self):
        """Test transaction support in Johnny."""
        if not is_multithreading_safe(db_using='default'):
            print("\n  Skipping test requiring multiple threads.")
            return

        self.assertFalse(is_managed())
        self.assertFalse(transaction.is_dirty())
        cache.local.clear()
        q = Queue()
        other = lambda x: self._run_threaded(x, q)
        # load some data
        start = Genre.objects.get(id=1)
        other('Genre.objects.get(id=1)')
        hit, ostart = q.get()
        # these should be the same and should have hit cache
        self.assertTrue(hit)
        self.assertEqual(ostart, start)
        # enter manual transaction management
        transaction.enter_transaction_management()
        managed()
        start.title = 'Jackie Chan Novels'
        # local invalidation, this key should hit the localstore!
        nowlen = len(cache.local)
        start.save()
        self.assertNotEqual(nowlen, len(cache.local))
        # perform a read OUTSIDE this transaction... it should still see the
        # old gen key, and should still find the "old" data
        other('Genre.objects.get(id=1)')
        hit, ostart = q.get()
        self.assertTrue(hit)
        self.assertNotEqual(ostart.title, start.title)
        transaction.commit()
        # now that we commit, we push the localstore keys out;  this should be
        # a cache miss, because we never read it inside the previous transaction
        other('Genre.objects.get(id=1)')
        hit, ostart = q.get()
        self.assertFalse(hit)
        self.assertEqual(ostart.title, start.title)
        managed(False)
        transaction.leave_transaction_management()

    def test_transaction_rollback(self):
        """Tests johnny's handling of transaction rollbacks.

        Similar to the commit, this sets up a write to a db in a transaction,
        reads from it (to force a cache write of sometime), then rolls back."""
        if not is_multithreading_safe(db_using='default'):
            print("\n  Skipping test requiring multiple threads.")
            return

        self.assertFalse(is_managed())
        self.assertFalse(transaction.is_dirty())
        cache.local.clear()
        q = Queue()
        other = lambda x: self._run_threaded(x, q)

        # load some data
        start = Genre.objects.get(id=1)
        other('Genre.objects.get(id=1)')
        hit, ostart = q.get()
        # these should be the same and should have hit cache
        self.assertTrue(hit)
        self.assertEqual(ostart, start)
        # enter manual transaction management
        transaction.enter_transaction_management()
        managed()
        start.title = 'Jackie Chan Novels'
        # local invalidation, this key should hit the localstore!
        nowlen = len(cache.local)
        start.save()
        self.assertNotEqual(nowlen, len(cache.local))
        # perform a read OUTSIDE this transaction... it should still see the
        # old gen key, and should still find the "old" data
        other('Genre.objects.get(id=1)')
        hit, ostart = q.get()
        self.assertTrue(hit)
        self.assertNotEqual(ostart.title, start.title)
        # perform a READ inside the transaction;  this should hit the localstore
        # but not the outside!
        nowlen = len(cache.local)
        start2 = Genre.objects.get(id=1)
        self.assertEqual(start2.title, start.title)
        self.assertTrue(len(cache.local) > nowlen)
        transaction.rollback()
        # we rollback, and flush all johnny keys related to this transaction
        # subsequent gets should STILL hit the cache in the other thread
        # and indeed, in this thread.

        self.assertFalse(transaction.is_dirty())
        other('Genre.objects.get(id=1)')
        hit, ostart = q.get()
        self.assertTrue(hit)
        start = Genre.objects.get(id=1)
        self.assertEqual(ostart.title, start.title)
        managed(False)
        transaction.leave_transaction_management()

    def test_savepoint_rollback(self):
        """Tests rollbacks of savepoints"""
        if not connection.features.uses_savepoints or connection.vendor == 'sqlite':
            return
        self.assertFalse(is_managed())
        self.assertFalse(transaction.is_dirty())
        cache.local.clear()
        managed()

        g = Genre.objects.get(pk=1)
        start_title = g.title
        g.title = "Adventures in Savepoint World"
        g.save()
        g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, "Adventures in Savepoint World")
        sid = transaction.savepoint()
        g.title = "In the Void"
        g.save()
        g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, "In the Void")
        transaction.savepoint_rollback(sid)
        g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, "Adventures in Savepoint World")
        transaction.rollback()
        g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, start_title)

    def test_savepoint_rollback_sqlite(self):
        """SQLite savepoints in Django 1.6 don't work correctly with autocommit disabled,
        so we have to use transaction.atomic().
        See https://docs.djangoproject.com/en/dev/topics/db/transactions/#savepoints-in-sqlite
        SQLite doesn't seem to support savepoints in Django < 1.6"""
        if not connection.features.uses_savepoints or connection.vendor != 'sqlite':
            return
        self.assertFalse(is_managed())
        self.assertFalse(transaction.is_dirty())
        cache.local.clear()

        try:
            with transaction.atomic():
                g = Genre.objects.get(pk=1)
                start_title = g.title
                g.title = "Adventures in Savepoint World"
                g.save()
                g = Genre.objects.get(pk=1)
                self.assertEqual(g.title, "Adventures in Savepoint World")
                sid = transaction.savepoint()
                g.title = "In the Void"
                g.save()
                g = Genre.objects.get(pk=1)
                self.assertEqual(g.title, "In the Void")
                transaction.savepoint_rollback(sid)
                g = Genre.objects.get(pk=1)
                self.assertEqual(g.title, "Adventures in Savepoint World")
                raise IntegrityError('Exit transaction')
        except IntegrityError:
            pass
        g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, start_title)

    def test_savepoint_commit(self):
        """Tests a transaction commit (release)
        The release actually pushes the savepoint back into the dirty stack,
        but at the point it was saved in the transaction"""
        if not connection.features.uses_savepoints:
            return
        self.assertFalse(is_managed())
        self.assertFalse(transaction.is_dirty())
        cache.local.clear()
        transaction.enter_transaction_management()
        managed()
        g = Genre.objects.get(pk=1)
        start_title = g.title
        g.title = "Adventures in Savepoint World"
        g.save()
        g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, "Adventures in Savepoint World")
        sid = transaction.savepoint()
        g.title = "In the Void"
        g.save()
        #should be a database hit because of save in savepoint
        with self.assertNumQueries(1):
            g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, "In the Void")
        transaction.savepoint_commit(sid)
        #should be a cache hit against the dirty store
        with self.assertNumQueries(0):
            g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, "In the Void")
        transaction.commit()
        #should have been pushed up to cache store
        with self.assertNumQueries(0):
            g = Genre.objects.get(pk=1)
        self.assertEqual(g.title, "In the Void")
        managed(False)
        transaction.leave_transaction_management()


class TransactionManagerTestCase(base.TransactionJohnnyTestCase):
    def tearDown(self):
        if is_managed():
            managed(False)

    def test_savepoint_localstore_flush(self):
        """
        This is a very simple test to see if savepoints will actually
        be committed, i.e. flushed out from localstore into cache.
        """
        transaction.enter_transaction_management()
        managed()

        TABLE_NAME = 'test_table'
        cache_backend = cache.get_backend()
        cache_backend.patch()
        keyhandler = cache_backend.keyhandler
        keygen = keyhandler.keygen
        
        tm = cache_backend.cache_backend
        
        # First, we set one key-val pair generated for our non-existing table.
        table_key = keygen.gen_table_key(TABLE_NAME)
        tm.set(table_key, 'val1')

        # Then we create a savepoint.
        # The key-value pair is moved into 'trans_sids' item of localstore.
        tm._create_savepoint('savepoint1')
        
        # We then commit all the savepoints (i.e. only one in this case)
        # The items stored in 'trans_sids' should be moved back to the
        # top-level dictionary of our localstore
        tm._commit_all_savepoints()
        # And this checks if it actually happened.
        self.assertTrue(table_key in tm.local)

########NEW FILE########
__FILENAME__ = localstore
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Tests for johnny-cache.  Much of johnny cache is designed to cache things
forever, and because it aims to keep coherency despite that it's important
that it be tested thoroughly to make sure no changes introduce the possibility
of stale data in the cache."""

from threading import Thread, current_thread
from time import sleep
from django.test import TestCase
from johnny import localstore, cache, middleware
from johnny.compat import Queue

class LocalStoreTest(TestCase):
    def test_basic_operation(self):
        store = localstore.LocalStore()
        for x in (1,2,3,4,5):
            store['key%s' % x] = "value%s" % x
        self.assertEqual(store.key3, "value3")
        self.assertEqual(store['key4'], "value4")
        self.assertEqual(len(store), 5)
        self.assertEqual(sorted(list(store)),
                          sorted(store.keys()))
        self.assertEqual(store.setdefault('key6', 'value6'), 'value6')
        self.assertEqual(store['key6'], 'value6')
        del store['key2']
        self.assertEqual(len(store), 5)
        store.clear()
        self.assertEqual(len(store), 0)

    def test_custom_clear(self):
        """Test that clear(glob) works as expected."""
        store = localstore.LocalStore()
        for x in (1,2,3,4,5):
            store['key_%s' % x] = 'value_%s' % x
            store['ex_%s' % x] = 'ecks_%s' % x
        self.assertEqual(len(store), 10)
        store.clear('*4')
        self.assertEqual(len(store), 8)
        store.clear('ex_*')
        self.assertEqual(len(store), 4)
        self.assertEqual(len(store.mget('key*')), 4)
        self.assertEqual(len(store.mget('*_2')), 1)

    def test_thread_locality(self):
        store = localstore.LocalStore()
        store['name'] = "Hi"
        q = Queue()
        def do_test():
            sleep(0.1)
            t = current_thread()
            name = t.name
            store[name] = 1
            store['name'] = name
            q.put(dict(store))
        threads = []
        for x in range(5):
            t = Thread(target=do_test, name='thread%x' % x)
            t.start()
            threads.append(t)
        for t in threads:
            t.join()
        # assert none of the thread stuff touched our queue
        self.assertEqual(store['name'], 'Hi')
        self.assertEqual(q.qsize(), 5)
        qcontents = []
        while not q.empty():
            qcontents.append(q.get())
        self.assertEqual(len(qcontents), 5)
        for d in qcontents:
            self.assertEqual(len(d), 2)
            self.assertNotEqual(d['name'], 'Hi')
            self.assertEqual(d[d['name']], 1)

    def test_localstore_clear_middleware(self):
        cache.local.clear()
        cache.local['eggs'] = 'spam'
        cache.local['charlie'] = 'chaplin'
        self.assertEqual(len(cache.local), 2)
        middleware.LocalStoreClearMiddleware().process_response(None, None)
        self.assertEqual(len(cache.local), 0)



########NEW FILE########
__FILENAME__ = models
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Test models for Johnny-Cache"""

from __future__ import unicode_literals
import django
from django.db import models
from django.db.models import permalink
from django.conf import settings
from django.utils.translation import ugettext_lazy as _

def get_urlfield(*args, **kwargs):
    if django.VERSION[:2] >= (1, 4) and 'verify_exists' in kwargs:
        del kwargs['verify_exists']
        return models.URLField(*args, **kwargs)
    return models.URLField(*args, **kwargs)


#from basic.people.models import Person

class Issue24Model(models.Model):
    one = models.PositiveIntegerField()
    two = models.PositiveIntegerField()

class User(models.Model):
    """User model."""
    first_name = models.CharField('first name', blank=True, max_length=128)
    last_name = models.CharField('last name', blank=True, max_length=128)
    username = models.CharField('username', blank=True, max_length=128)

    def __repr__(self):
        return '<User: %s %s>' % (self.first_name, self.last_name)

class PersonType(models.Model):
    """Person type model."""
    title = models.CharField(_('title'), max_length=100)
    slug = models.SlugField(_('slug'), unique=True)

    class Meta:
        verbose_name = _('person type')
        verbose_name_plural = _('person types')
        ordering = ('title',)

    def __unicode__(self):
        return '%s' % self.title

    @permalink
    def get_absolute_url(self):
        return ('person_type_detail', None, {'slug': self.slug})

# some details left out of the Person model, in order to avoid a requirement
# on python-dateutil

class Person(models.Model):
    """Person model."""
    GENDER_CHOICES = (
        (1, 'Male'),
        (2, 'Female'),
    )
    user = models.ForeignKey(User, blank=True, null=True)
    first_name = models.CharField(_('first name'), blank=True, max_length=100)
    middle_name = models.CharField(_('middle name'), blank=True, max_length=100)
    last_name = models.CharField(_('last name'), blank=True, max_length=100)
    slug = models.SlugField(_('slug'), unique=True)
    gender = models.PositiveSmallIntegerField(_('gender'), choices=GENDER_CHOICES, blank=True, null=True)
    mugshot = models.FileField(_('mugshot'), upload_to='mugshots', blank=True)
    mugshot_credit = models.CharField(_('mugshot credit'), blank=True, max_length=200)
    birth_date = models.DateField(_('birth date'), blank=True, null=True)
    person_types = models.ManyToManyField(PersonType, blank=True)
    website = get_urlfield(_('website'), blank=True, verify_exists=True)

    class Meta:
        verbose_name = _('person')
        verbose_name_plural = _('people')
        ordering = ('last_name', 'first_name',)

    def __unicode__(self):
        return '%s' % self.full_name

    @property
    def full_name(self):
        return '%s %s' % (self.first_name, self.last_name)

    @permalink
    def get_absolute_url(self):
        return ('person_detail', None, {'slug': self.slug})


class Genre(models.Model):
    """Genre model"""
    title = models.CharField(max_length=100)
    slug = models.SlugField(unique=True)

    class Meta:
        ordering = ('title',)

    def __unicode__(self):
        return '%s' % self.title

    @permalink
    def get_absolute_url(self):
        return ('book_genre_detail', None, { 'slug': self.slug })


class Publisher(models.Model):
    """Publisher"""
    title = models.CharField(max_length=100)
    prefix = models.CharField(max_length=20, blank=True)
    slug = models.SlugField(unique=True)
    website = get_urlfield(blank=True, verify_exists=False)

    class Meta:
        ordering = ('title',)

    def __unicode__(self):
        return '%s' % self.full_title

    @property
    def full_title(self):
        return '%s %s' % (self.prefix, self.title)

    @permalink
    def get_absolute_url(self):
        return ('book_publisher_detail', None, { 'slug':self.slug })


class Book(models.Model):
    """Listing of books"""
    title = models.CharField(max_length=255)
    prefix = models.CharField(max_length=20, blank=True)
    subtitle = models.CharField(blank=True, max_length=255)
    slug = models.SlugField(unique=True)
    authors = models.ManyToManyField(Person, limit_choices_to={'person_types__slug__exact': 'author'}, related_name='books')
    isbn = models.CharField(max_length=14, blank=True)
    pages = models.PositiveSmallIntegerField(blank=True, null=True, default=0)
    publisher = models.ForeignKey(Publisher, blank=True, null=True)
    published = models.DateField(blank=True, null=True)
    cover = models.FileField(upload_to='books', blank=True)
    description = models.TextField(blank=True)
    genre = models.ManyToManyField(Genre, blank=True)
    created = models.DateTimeField(auto_now_add=True)
    modified = models.DateTimeField(auto_now=True)

    class Meta:
        ordering = ('title',)

    def __unicode__(self):
        return '%s' % self.full_title

    @property
    def full_title(self):
        if self.prefix:
            return '%s %s' % (self.prefix, self.title)
        else:
            return '%s' % self.title

    @permalink
    def get_absolute_url(self):
        return ('book_detail', None, { 'slug': self.slug })

    @property
    def amazon_url(self):
        if self.isbn:
            try:
                return 'http://www.amazon.com/dp/%s/?%s' % (self.isbn, settings.AMAZON_AFFILIATE_EXTENTION)
            except:
                return 'http://www.amazon.com/dp/%s/' % self.isbn
        return ''


class Highlight(models.Model):
    """Highlights from books"""
    user = models.ForeignKey(User)
    book = models.ForeignKey(Book)
    highlight = models.TextField()
    page = models.CharField(blank=True, max_length=20)
    created = models.DateTimeField(auto_now_add=True)
    modified = models.DateTimeField(auto_now=True)

    def __unicode__(self):
        return '%s' % self.highlight

    @permalink
    def get_absolute_url(self):
        return ('book_detail', None, { 'slug': self.book.slug })

class Page(models.Model):
    """Page model"""
    user = models.ForeignKey(User)
    book = models.ForeignKey(Book)
    current_page = models.PositiveSmallIntegerField(default=0)
    created = models.DateTimeField(auto_now_add=True)

    class Meta:
        ordering = ('-created',)

    def __unicode__(self):
        return '%s' % self.current_page

class Milk(models.Model):
    """A meaningless model designed to test unicode ability.  This might screw
    up databases that can't handle unicode table/column names."""
    name = models.CharField(blank=True, max_length=20, db_column='名前')
    chocolate = models.BooleanField(blank=True, db_column='チョコレート')

    class Meta:
        db_table = 'ミルク'




########NEW FILE########
__FILENAME__ = urls
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""URLconf for Johnny's test app."""

try:
    from django.conf.urls import patterns, url
except ImportError:
    from django.conf.urls.defaults import patterns, url

urlpatterns = patterns('johnny.tests.testapp.views',
   url(r'^test/template_queries', 'template_queries'),
)

########NEW FILE########
__FILENAME__ = views
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Views for Johnny's testapp."""

from django.shortcuts import render_to_response
from .models import *

def template_queries(request):
    """Render a simple template that will perform a query."""
    books = Book.objects.all()
    return render_to_response('template_queries.html', locals())

########NEW FILE########
__FILENAME__ = web
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Tests for the QueryCache functionality of johnny."""

from . import base


# put tests in here to be included in the testing suite
__all__ = ['MiddlewaresTestCase']


class MiddlewaresTestCase(base.TransactionJohnnyWebTestCase):
    fixtures = base.johnny_fixtures
    middlewares = (
        'johnny.middleware.LocalStoreClearMiddleware',
        'johnny.middleware.QueryCacheMiddleware',
        'django.middleware.common.CommonMiddleware',
        'django.contrib.sessions.middleware.SessionMiddleware',
        'django.contrib.auth.middleware.AuthenticationMiddleware',
        'django.contrib.messages.middleware.MessageMiddleware',
        'django.middleware.locale.LocaleMiddleware',
        'django.middleware.gzip.GZipMiddleware',
        'django.middleware.http.ConditionalGetMiddleware',
    )

    def test_queries_from_templates(self):
        """Verify that doing the same request w/o a db write twice does not
        populate the cache properly."""
        with self.assertNumQueries(1):
            self.client.get('/test/template_queries')
        with self.assertNumQueries(0):
            self.client.get('/test/template_queries')

########NEW FILE########
__FILENAME__ = transaction
from django.db import transaction, connection, DEFAULT_DB_ALIAS

from johnny import settings as johnny_settings
from johnny.compat import is_managed
from johnny.decorators import wraps, available_attrs


class TransactionManager(object):
    """
    TransactionManager is a wrapper around a cache_backend that is
    transaction aware.

    If we are in a transaction, it will return the locally cached version.

      * On rollback, it will flush all local caches
      * On commit, it will push them up to the real shared cache backend
        (ex. memcached).
    """
    _patched_var = False

    def __init__(self, cache_backend, keygen):
        from johnny import cache, settings

        self.timeout = settings.MIDDLEWARE_SECONDS
        self.prefix = settings.MIDDLEWARE_KEY_PREFIX

        self.cache_backend = cache_backend
        self.local = cache.local
        self.keygen = keygen(self.prefix)
        self._originals = {}
        self._dirty_backup = {}

        self.local['trans_sids'] = {}

    def _get_sid(self, using=None):
        if 'trans_sids' not in self.local:
            self.local['trans_sids'] = {}
        d = self.local['trans_sids']
        if using is None:
            using = DEFAULT_DB_ALIAS
        if using not in d:
            d[using] = []
        return d[using]

    def _clear_sid_stack(self, using=None):
        if using is None:
            using = DEFAULT_DB_ALIAS
        if using in self.local.get('trans_sids', {}):
            del self.local['trans_sids']

    def is_managed(self, using=None):
        return is_managed(using=using)

    def get(self, key, default=None, using=None):
        if self.is_managed(using) and self._patched_var:
            val = self.local.get(key, None)
            if val:
                return val
            if self._uses_savepoints():
                val = self._get_from_savepoints(key, using)
                if val:
                    return val
        return self.cache_backend.get(key, default)

    def _get_from_savepoints(self, key, using=None):
        sids = self._get_sid(using)
        cp = list(sids)
        cp.reverse()
        for sid in cp:
            if key in self.local[sid]:
                return self.local[sid][key]

    def _trunc_using(self, using):
        if using is None:
            using = DEFAULT_DB_ALIAS
        using = johnny_settings.DB_CACHE_KEYS[using]
        if len(using) > 100:
            using = using[0:68] + self.keygen.gen_key(using[68:])
        return using

    def set(self, key, val, timeout=None, using=None):
        """
        Set will be using the generational key, so if another thread
        bumps this key, the localstore version will still be invalid.
        If the key is bumped during a transaction it will be new
        to the global cache on commit, so it will still be a bump.
        """
        if timeout is None:
            timeout = self.timeout
        if self.is_managed(using=using) and self._patched_var:
            self.local[key] = val
        else:
            self.cache_backend.set(key, val, timeout)

    def _clear(self, using=None):
        self.local.clear('%s_%s_*' %
                         (self.prefix, self._trunc_using(using)))

    def _flush(self, commit=True, using=None):
        """
        Flushes the internal cache, either to the memcache or rolls back
        """
        if commit:
            # XXX: multi-set?
            if self._uses_savepoints():
                self._commit_all_savepoints(using)
            c = self.local.mget('%s_%s_*' %
                                (self.prefix, self._trunc_using(using)))
            for key, value in c.items():
                self.cache_backend.set(key, value, self.timeout)
        else:
            if self._uses_savepoints():
                self._rollback_all_savepoints(using)
        self._clear(using)
        self._clear_sid_stack(using)

    def _patched(self, original, commit=True, unless_managed=False):
        @wraps(original, assigned=available_attrs(original))
        def newfun(using=None):
            original(using=using)
            # copying behavior of original func
            # if it is an 'unless_managed' version we should do nothing if transaction is managed
            if not unless_managed or not self.is_managed(using=using):
                self._flush(commit=commit, using=using)

        return newfun

    def _uses_savepoints(self):
        return connection.features.uses_savepoints

    def _sid_key(self, sid, using=None):
        if using is not None:
            prefix = 'trans_savepoint_%s' % using
        else:
            prefix = 'trans_savepoint'

        if sid is not None and sid.startswith(prefix):
            return sid
        return '%s_%s'%(prefix, sid)

    def _create_savepoint(self, sid, using=None):
        key = self._sid_key(sid, using)

        #get all local dirty items
        c = self.local.mget('%s_%s_*' %
                            (self.prefix, self._trunc_using(using)))
        #store them to a dictionary in the localstore
        if key not in self.local:
            self.local[key] = {}
        for k, v in c.items():
            self.local[key][k] = v
        #clear the dirty
        self._clear(using)
        #append the key to the savepoint stack
        sids = self._get_sid(using)
        if key not in sids:
            sids.append(key)

    def _rollback_savepoint(self, sid, using=None):
        sids = self._get_sid(using)
        key = self._sid_key(sid, using)
        stack = []
        try:
            popped = None
            while popped != key:
                popped = sids.pop()
                stack.insert(0, popped)
            #delete items from localstore
            for i in stack:
                del self.local[i]
            #clear dirty
            self._clear(using)
        except IndexError:
            #key not found, don't delete from localstore, restore sid stack
            for i in stack:
                sids.insert(0, i)

    def _commit_savepoint(self, sid, using=None):
        # commit is not a commit but is in reality just a clear back to that
        # savepoint and adds the items back to the dirty transaction.
        key = self._sid_key(sid, using)
        sids = self._get_sid(using)
        stack = []
        try:
            popped = None
            while popped != key:
                popped = sids.pop()
                stack.insert(0, popped)
            self._store_dirty(using)
            for i in stack:
                for k, v in self.local.get(i, {}).items():
                    self.local[k] = v
                del self.local[i]
            self._restore_dirty(using)
        except IndexError:
            for i in stack:
                sids.insert(0, i)

    def _commit_all_savepoints(self, using=None):
        sids = self._get_sid(using)
        if sids:
            self._commit_savepoint(sids[0], using)

    def _rollback_all_savepoints(self, using=None):
        sids = self._get_sid(using)
        if sids:
            self._rollback_savepoint(sids[0], using)

    def _store_dirty(self, using=None):
        c = self.local.mget('%s_%s_*' %
                            (self.prefix, self._trunc_using(using)))
        backup = 'trans_dirty_store_%s' % self._trunc_using(using)
        self.local[backup] = {}
        for k, v in c.items():
            self.local[backup][k] = v
        self._clear(using)

    def _restore_dirty(self, using=None):
        backup = 'trans_dirty_store_%s' % self._trunc_using(using)
        for k, v in self.local.get(backup, {}).items():
            self.local[k] = v
        del self.local[backup]

    def _savepoint(self, original):
        @wraps(original, assigned=available_attrs(original))
        def newfun(using=None):
            if using is not None:
                sid = original(using=using)
            else:
                sid = original()
            if self._uses_savepoints():
                self._create_savepoint(sid, using)
            return sid
        return newfun

    def _savepoint_rollback(self, original):
        def newfun(sid, *args, **kwargs):
            original(sid, *args, **kwargs)
            if self._uses_savepoints():
                if len(args) == 2:
                    using = args[1]
                else:
                    using = kwargs.get('using', None)
                self._rollback_savepoint(sid, using)
        return newfun

    def _savepoint_commit(self, original):
        def newfun(sid, *args, **kwargs):
            original(sid, *args, **kwargs)
            if self._uses_savepoints():
                if len(args) == 1:
                    using = args[0]
                else:
                    using = kwargs.get('using', None)
                self._commit_savepoint(sid, using)
        return newfun

    def _getreal(self, name):
        return getattr(transaction, 'real_%s' % name,
                getattr(transaction, name))

    def patch(self):
        """
        This function monkey patches commit and rollback
        writes to the cache should not happen until commit (unless our state
        isn't managed). It does not yet support savepoints.
        """
        if not self._patched_var:
            self._originals['rollback'] = self._getreal('rollback')
            self._originals['rollback_unless_managed'] = self._getreal('rollback_unless_managed')
            self._originals['commit'] = self._getreal('commit')
            self._originals['commit_unless_managed'] = self._getreal('commit_unless_managed')
            self._originals['savepoint'] = self._getreal('savepoint')
            self._originals['savepoint_rollback'] = self._getreal('savepoint_rollback')
            self._originals['savepoint_commit'] = self._getreal('savepoint_commit')
            transaction.rollback = self._patched(transaction.rollback, False)
            transaction.rollback_unless_managed = self._patched(transaction.rollback_unless_managed,
                                                                       False, unless_managed=True)
            transaction.commit = self._patched(transaction.commit, True)
            transaction.commit_unless_managed = self._patched(transaction.commit_unless_managed,
                                                                     True, unless_managed=True)
            transaction.savepoint = self._savepoint(transaction.savepoint)
            transaction.savepoint_rollback = self._savepoint_rollback(transaction.savepoint_rollback)
            transaction.savepoint_commit = self._savepoint_commit(transaction.savepoint_commit)

            self._patched_var = True

    def unpatch(self):
        for fun in self._originals:
            setattr(transaction, fun, self._originals[fun])
        self._patched_var = False

########NEW FILE########
__FILENAME__ = utils
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Extra johnny utilities."""

from johnny.cache import get_backend, local, patch, unpatch
from johnny.decorators import wraps, available_attrs


__all__ = ["celery_enable_all", "celery_task_wrapper", "johnny_task_wrapper"]


def prerun_handler(*args, **kwargs):
    """Celery pre-run handler.  Enables johnny-cache."""
    patch()

def postrun_handler(*args, **kwargs):
    """Celery postrun handler.  Unpatches and clears the localstore."""
    unpatch()
    local.clear()

def celery_enable_all():
    """Enable johnny-cache in all celery tasks, clearing the local-store
    after each task."""
    from celery.signals import task_prerun, task_postrun, task_failure
    task_prerun.connect(prerun_handler)
    task_postrun.connect(postrun_handler)
    # Also have to cleanup on failure.
    task_failure.connect(postrun_handler)

def celery_task_wrapper(f):
    """
    Provides a task wrapper for celery that sets up cache and ensures
    that the local store is cleared after completion
    """
    from celery.utils import fun_takes_kwargs

    @wraps(f, assigned=available_attrs(f))
    def newf(*args, **kwargs):
        backend = get_backend()
        was_patched = backend._patched
        get_backend().patch()
        # since this function takes all keyword arguments,
        # we will pass only the ones the function below accepts,
        # just as celery does
        supported_keys = fun_takes_kwargs(f, kwargs)
        new_kwargs = dict((key, val) for key, val in kwargs.items()
                                if key in supported_keys)

        try:
            ret = f(*args, **new_kwargs)
        finally:
            local.clear()
        if not was_patched:
            get_backend().unpatch()
        return ret
    return newf

# backwards compatible alias
johnny_task_wrapper = celery_task_wrapper


########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python

import os
import sys
from django.core.management import execute_from_command_line

if __name__ == '__main__':
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'settings')
    execute_from_command_line(sys.argv)

########NEW FILE########
__FILENAME__ = routers
# routers for johnny's tests
class MultiSyncedRouter(object):
    def db_for_read(self, *args, **kwargs): return None
    def db_for_write(self, *args, **kwargs): return None
    def allow_relation(self, *args, **kwargs): return None
    def allow_sync_db(self, db, model):
        return True

########NEW FILE########
__FILENAME__ = settings
# Django settings for proj project.

import os
import warnings
import django

DEBUG = True
TEMPLATE_DEBUG = DEBUG

ADMINS = ()

MANAGERS = ADMINS

db_engine = os.environ.get('DB_ENGINE', 'sqlite3')
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.' + db_engine,
        'NAME': 'johnny_db',
        'TEST_NAME': 'test_johnny_db',
    },
    'second': {
        'ENGINE': 'django.db.backends.' + db_engine,
        'NAME': 'johnny2_db',
        'TEST_NAME': 'test_johnny2_db',
    },
}
if db_engine == 'postgresql_psycopg2':
    DATABASES['default']['OPTIONS'] = {'autocommit': True}
    DATABASES['second']['OPTIONS'] = {'autocommit': True}

# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# If running in a Windows environment this must be set to the same as your
# system time zone.
TIME_ZONE = 'America/Chicago'

# Language code for this installation. All choices can be found here:
# http://www.i18nguy.com/unicode/language-identifiers.html
LANGUAGE_CODE = 'en-us'

SITE_ID = 1

# If you set this to False, Django will make some optimizations so as not
# to load the internationalization machinery.
USE_I18N = True

# Absolute path to the directory that holds media.
# Example: "/home/media/media.lawrence.com/"
MEDIA_ROOT = ''

# URL that handles the media served from MEDIA_ROOT. Make sure to use a
# trailing slash if there is a path component (optional in other cases).
# Examples: "http://media.lawrence.com", "http://example.com/media/"
MEDIA_URL = ''

# URL prefix for admin media -- CSS, JavaScript and images. Make sure to use a
# trailing slash.
# Examples: "http://foo.com/media/", "/media/".
ADMIN_MEDIA_PREFIX = '/media/'

cache_backend = os.environ.get('CACHE_BACKEND', 'memcached')
if cache_backend == 'memcached':
    CACHES = {
        'default': {
            'BACKEND': 'johnny.backends.memcached.MemcachedCache',
            'LOCATION': ['localhost:11211'],
            'JOHNNY_CACHE': True,
        }
    }
elif cache_backend == 'redis':
    CACHES = {
        'default': {
            'BACKEND': 'johnny.backends.redis.RedisCache',
            'LOCATION': 'localhost:6379:0',
            'JOHNNY_CACHE': True,
        }
    }
elif cache_backend == 'locmem':
    CACHES = {
        'default': {
            'BACKEND': 'johnny.backends.locmem.LocMemCache',
        }
    }
    warnings.warn('Some tests may fail with the locmem cache backend!')
elif cache_backend == 'filebased':
    CACHES = {
        'default': {
            'BACKEND': 'johnny.backends.filebased.FileBasedCache',
            'LOCATION': '_cache',
        }
    }
    warnings.warn('Some tests may fail with the file-based cache backend!')
else:
    raise ValueError('The CACHE_BACKEND environment variable is invalid.')


# Make this unique, and don't share it with anybody.
SECRET_KEY = '_vpn1a^j(6&+3qip2me4f#&8#m#*#icc!%=x=)rha4k=!4m8s4'

# List of callables that know how to import templates from various sources.
TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
#    'django.template.loaders.app_directories.Loader',
#    'django.template.loaders.eggs.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
)

ROOT_URLCONF = 'proj.urls'

TEMPLATE_DIRS = (
    # Put strings here, like "/home/html/django_templates" or "C:/www/django/templates".
    # Always use forward slashes, even on Windows.
    # Don't forget to use absolute paths, not relative paths.
)

INSTALLED_APPS = (
    #'django.contrib.auth',
    #'django.contrib.sessions',
    #'django.contrib.sites',
    'johnny',
)

try:
    from local_settings import *
except ImportError:
    pass

# set up a multi-db router if there are multiple databases set
lcls = locals()
if 'DATABASES' in lcls and len(lcls['DATABASES']) > 1:
    DATABASE_ROUTERS = ['routers.MultiSyncedRouter']

TEST_RUNNER = 'django.test.simple.DjangoTestSuiteRunner'

########NEW FILE########
