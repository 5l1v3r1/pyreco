__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os
from datetime import datetime

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
projpath = os.path.abspath('..')
sys.path.append(projpath)

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = 'speedparser'
copyright = u'2010 Jason Moiron'

# The short X.Y version.
version = '0.1'
# The full version, including alpha/beta/rc tags.
version = None
for line in open(os.path.join(projpath, 'setup.py'), 'r'):
    if line.startswith('version'):
        exec line
if version is None:
    version = '0.1'
# The full version, including alpha/beta/rc tags.
release = version

print ("Building release: %s, version: %s" % (release, version))

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
#html_theme = 'default'
html_theme = 'nature'
html_theme_path = ['_theme']
#html_theme_options = {}

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'speedparserdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'speedparser.tex', u'speedparser Documentation',
   u'Jason Moiron', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True


########NEW FILE########
__FILENAME__ = feedparsercompat
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Feedparser data pulled out of feedparser 5.0.1 in case feedparser is not
installed."""

import re
import time

try: import rfc822
except ImportError:
    from email import _parseaddr as rfc822

class FeedParserDict(dict):
    keymap = {'channel': 'feed',
              'items': 'entries',
              'guid': 'id',
              'date': 'updated',
              'date_parsed': 'updated_parsed',
              'description': ['summary', 'subtitle'],
              'url': ['href'],
              'modified': 'updated',
              'modified_parsed': 'updated_parsed',
              'issued': 'published',
              'issued_parsed': 'published_parsed',
              'copyright': 'rights',
              'copyright_detail': 'rights_detail',
              'tagline': 'subtitle',
              'tagline_detail': 'subtitle_detail'}
    def __getitem__(self, key):
        if key == 'category':
            try:
                return dict.__getitem__(self, 'tags')[0]['term']
            except IndexError:
                raise KeyError, "object doesn't have key 'category'"
        elif key == 'enclosures':
            norel = lambda link: FeedParserDict([(name,value) for (name,value) in link.items() if name!='rel'])
            return [norel(link) for link in dict.__getitem__(self, 'links') if link['rel']==u'enclosure']
        elif key == 'license':
            for link in dict.__getitem__(self, 'links'):
                if link['rel']==u'license' and 'href' in link:
                    return link['href']
        else:
            realkey = self.keymap.get(key, key)
            if isinstance(realkey, list):
                for k in realkey:
                    if dict.__contains__(self, k):
                        return dict.__getitem__(self, k)
            elif dict.__contains__(self, realkey):
                return dict.__getitem__(self, realkey)
        return dict.__getitem__(self, key)

    def __contains__(self, key):
        try:
            self.__getitem__(key)
        except KeyError:
            return False
        else:
            return True

    has_key = __contains__

    def get(self, key, default=None):
        try:
            return self.__getitem__(key)
        except KeyError:
            return default

    def __setitem__(self, key, value):
        key = self.keymap.get(key, key)
        if isinstance(key, list):
            key = key[0]
        return dict.__setitem__(self, key, value)

    def setdefault(self, key, value):
        if key not in self:
            self[key] = value
            return value
        return self[key]

    def __getattr__(self, key):
        # __getattribute__() is called first; this will be called
        # only if an attribute was not already found
        try:
            return self.__getitem__(key)
        except KeyError:
            raise AttributeError, "object has no attribute '%s'" % key

    def __hash__(self):
        return id(self)

class _FeedParserMixin:
    namespaces = {
        '': '',
        'http://backend.userland.com/rss': '',
        'http://blogs.law.harvard.edu/tech/rss': '',
        'http://purl.org/rss/1.0/': '',
        'http://my.netscape.com/rdf/simple/0.9/': '',
        'http://example.com/newformat#': '',
        'http://example.com/necho': '',
        'http://purl.org/echo/': '',
        'uri/of/echo/namespace#': '',
        'http://purl.org/pie/': '',
        'http://purl.org/atom/ns#': '',
        'http://www.w3.org/2005/Atom': '',
        'http://purl.org/rss/1.0/modules/rss091#': '',

        'http://webns.net/mvcb/':                                'admin',
        'http://purl.org/rss/1.0/modules/aggregation/':          'ag',
        'http://purl.org/rss/1.0/modules/annotate/':             'annotate',
        'http://media.tangent.org/rss/1.0/':                     'audio',
        'http://backend.userland.com/blogChannelModule':         'blogChannel',
        'http://web.resource.org/cc/':                           'cc',
        'http://backend.userland.com/creativeCommonsRssModule':  'creativeCommons',
        'http://purl.org/rss/1.0/modules/company':               'co',
        'http://purl.org/rss/1.0/modules/content/':              'content',
        'http://my.theinfo.org/changed/1.0/rss/':                'cp',
        'http://purl.org/dc/elements/1.1/':                      'dc',
        'http://purl.org/dc/terms/':                             'dcterms',
        'http://purl.org/rss/1.0/modules/email/':                'email',
        'http://purl.org/rss/1.0/modules/event/':                'ev',
        'http://rssnamespace.org/feedburner/ext/1.0':            'feedburner',
        'http://freshmeat.net/rss/fm/':                          'fm',
        'http://xmlns.com/foaf/0.1/':                            'foaf',
        'http://www.w3.org/2003/01/geo/wgs84_pos#':              'geo',
        'http://postneo.com/icbm/':                              'icbm',
        'http://purl.org/rss/1.0/modules/image/':                'image',
        'http://www.itunes.com/DTDs/PodCast-1.0.dtd':            'itunes',
        'http://example.com/DTDs/PodCast-1.0.dtd':               'itunes',
        'http://purl.org/rss/1.0/modules/link/':                 'l',
        'http://search.yahoo.com/mrss':                          'media',
        # Version 1.1.2 of the Media RSS spec added the trailing slash on the namespace
        'http://search.yahoo.com/mrss/':                         'media',
        'http://madskills.com/public/xml/rss/module/pingback/':  'pingback',
        'http://prismstandard.org/namespaces/1.2/basic/':        'prism',
        'http://www.w3.org/1999/02/22-rdf-syntax-ns#':           'rdf',
        'http://www.w3.org/2000/01/rdf-schema#':                 'rdfs',
        'http://purl.org/rss/1.0/modules/reference/':            'ref',
        'http://purl.org/rss/1.0/modules/richequiv/':            'reqv',
        'http://purl.org/rss/1.0/modules/search/':               'search',
        'http://purl.org/rss/1.0/modules/slash/':                'slash',
        'http://schemas.xmlsoap.org/soap/envelope/':             'soap',
        'http://purl.org/rss/1.0/modules/servicestatus/':        'ss',
        'http://hacks.benhammersley.com/rss/streaming/':         'str',
        'http://purl.org/rss/1.0/modules/subscription/':         'sub',
        'http://purl.org/rss/1.0/modules/syndication/':          'sy',
        'http://schemas.pocketsoap.com/rss/myDescModule/':       'szf',
        'http://purl.org/rss/1.0/modules/taxonomy/':             'taxo',
        'http://purl.org/rss/1.0/modules/threading/':            'thr',
        'http://purl.org/rss/1.0/modules/textinput/':            'ti',
        'http://madskills.com/public/xml/rss/module/trackback/': 'trackback',
        'http://wellformedweb.org/commentAPI/':                  'wfw',
        'http://purl.org/rss/1.0/modules/wiki/':                 'wiki',
        'http://www.w3.org/1999/xhtml':                          'xhtml',
        'http://www.w3.org/1999/xlink':                          'xlink',
        'http://www.w3.org/XML/1998/namespace':                  'xml',
    }


_date_handlers = []
def registerDateHandler(func):
    '''Register a date handler function (takes string, returns 9-tuple date in GMT)'''
    _date_handlers.insert(0, func)

# ISO-8601 date parsing routines written by Fazal Majid.
# The ISO 8601 standard is very convoluted and irregular - a full ISO 8601
# parser is beyond the scope of feedparser and would be a worthwhile addition
# to the Python library.
# A single regular expression cannot parse ISO 8601 date formats into groups
# as the standard is highly irregular (for instance is 030104 2003-01-04 or
# 0301-04-01), so we use templates instead.
# Please note the order in templates is significant because we need a
# greedy match.
_iso8601_tmpl = ['YYYY-?MM-?DD', 'YYYY-0MM?-?DD', 'YYYY-MM', 'YYYY-?OOO',
                'YY-?MM-?DD', 'YY-?OOO', 'YYYY',
                '-YY-?MM', '-OOO', '-YY',
                '--MM-?DD', '--MM',
                '---DD',
                'CC', '']
_iso8601_re = [
    tmpl.replace(
    'YYYY', r'(?P<year>\d{4})').replace(
    'YY', r'(?P<year>\d\d)').replace(
    'MM', r'(?P<month>[01]\d)').replace(
    'DD', r'(?P<day>[0123]\d)').replace(
    'OOO', r'(?P<ordinal>[0123]\d\d)').replace(
    'CC', r'(?P<century>\d\d$)')
    + r'(T?(?P<hour>\d{2}):(?P<minute>\d{2})'
    + r'(:(?P<second>\d{2}))?'
    + r'(\.(?P<fracsecond>\d+))?'
    + r'(?P<tz>[+-](?P<tzhour>\d{2})(:(?P<tzmin>\d{2}))?|Z)?)?'
    for tmpl in _iso8601_tmpl]
try:
    del tmpl
except NameError:
    pass
_iso8601_matches = [re.compile(regex).match for regex in _iso8601_re]
try:
    del regex
except NameError:
    pass
def _parse_date_iso8601(dateString):
    '''Parse a variety of ISO-8601-compatible formats like 20040105'''
    m = None
    for _iso8601_match in _iso8601_matches:
        m = _iso8601_match(dateString)
        if m:
            break
    if not m:
        return
    if m.span() == (0, 0):
        return
    params = m.groupdict()
    ordinal = params.get('ordinal', 0)
    if ordinal:
        ordinal = int(ordinal)
    else:
        ordinal = 0
    year = params.get('year', '--')
    if not year or year == '--':
        year = time.gmtime()[0]
    elif len(year) == 2:
        # ISO 8601 assumes current century, i.e. 93 -> 2093, NOT 1993
        year = 100 * int(time.gmtime()[0] / 100) + int(year)
    else:
        year = int(year)
    month = params.get('month', '-')
    if not month or month == '-':
        # ordinals are NOT normalized by mktime, we simulate them
        # by setting month=1, day=ordinal
        if ordinal:
            month = 1
        else:
            month = time.gmtime()[1]
    month = int(month)
    day = params.get('day', 0)
    if not day:
        # see above
        if ordinal:
            day = ordinal
        elif params.get('century', 0) or \
                 params.get('year', 0) or params.get('month', 0):
            day = 1
        else:
            day = time.gmtime()[2]
    else:
        day = int(day)
    # special case of the century - is the first year of the 21st century
    # 2000 or 2001 ? The debate goes on...
    if 'century' in params:
        year = (int(params['century']) - 1) * 100 + 1
    # in ISO 8601 most fields are optional
    for field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:
        if not params.get(field, None):
            params[field] = 0
    hour = int(params.get('hour', 0))
    minute = int(params.get('minute', 0))
    second = int(float(params.get('second', 0)))
    # weekday is normalized by mktime(), we can ignore it
    weekday = 0
    daylight_savings_flag = -1
    tm = [year, month, day, hour, minute, second, weekday,
          ordinal, daylight_savings_flag]
    # ISO 8601 time zone adjustments
    tz = params.get('tz')
    if tz and tz != 'Z':
        if tz[0] == '-':
            tm[3] += int(params.get('tzhour', 0))
            tm[4] += int(params.get('tzmin', 0))
        elif tz[0] == '+':
            tm[3] -= int(params.get('tzhour', 0))
            tm[4] -= int(params.get('tzmin', 0))
        else:
            return None
    # Python's time.mktime() is a wrapper around the ANSI C mktime(3c)
    # which is guaranteed to normalize d/m/y/h/m/s.
    # Many implementations have bugs, but we'll pretend they don't.
    return time.localtime(time.mktime(tuple(tm)))
registerDateHandler(_parse_date_iso8601)

# 8-bit date handling routines written by ytrewq1.
_korean_year  = u'\ub144' # b3e2 in euc-kr
_korean_month = u'\uc6d4' # bff9 in euc-kr
_korean_day   = u'\uc77c' # c0cf in euc-kr
_korean_am    = u'\uc624\uc804' # bfc0 c0fc in euc-kr
_korean_pm    = u'\uc624\ud6c4' # bfc0 c8c4 in euc-kr

_korean_onblog_date_re = \
    re.compile('(\d{4})%s\s+(\d{2})%s\s+(\d{2})%s\s+(\d{2}):(\d{2}):(\d{2})' % \
               (_korean_year, _korean_month, _korean_day))
_korean_nate_date_re = \
    re.compile(u'(\d{4})-(\d{2})-(\d{2})\s+(%s|%s)\s+(\d{,2}):(\d{,2}):(\d{,2})' % \
               (_korean_am, _korean_pm))
def _parse_date_onblog(dateString):
    '''Parse a string according to the OnBlog 8-bit date format'''
    m = _korean_onblog_date_re.match(dateString)
    if not m:
        return
    w3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \
                {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\
                 'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\
                 'zonediff': '+09:00'}
    return _parse_date_w3dtf(w3dtfdate)
registerDateHandler(_parse_date_onblog)

def _parse_date_nate(dateString):
    '''Parse a string according to the Nate 8-bit date format'''
    m = _korean_nate_date_re.match(dateString)
    if not m:
        return
    hour = int(m.group(5))
    ampm = m.group(4)
    if (ampm == _korean_pm):
        hour += 12
    hour = str(hour)
    if len(hour) == 1:
        hour = '0' + hour
    w3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \
                {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\
                 'hour': hour, 'minute': m.group(6), 'second': m.group(7),\
                 'zonediff': '+09:00'}
    return _parse_date_w3dtf(w3dtfdate)
registerDateHandler(_parse_date_nate)

# Unicode strings for Greek date strings
_greek_months = \
  { \
   u'\u0399\u03b1\u03bd': u'Jan',       # c9e1ed in iso-8859-7
   u'\u03a6\u03b5\u03b2': u'Feb',       # d6e5e2 in iso-8859-7
   u'\u039c\u03ac\u03ce': u'Mar',       # ccdcfe in iso-8859-7
   u'\u039c\u03b1\u03ce': u'Mar',       # cce1fe in iso-8859-7
   u'\u0391\u03c0\u03c1': u'Apr',       # c1f0f1 in iso-8859-7
   u'\u039c\u03ac\u03b9': u'May',       # ccdce9 in iso-8859-7
   u'\u039c\u03b1\u03ca': u'May',       # cce1fa in iso-8859-7
   u'\u039c\u03b1\u03b9': u'May',       # cce1e9 in iso-8859-7
   u'\u0399\u03bf\u03cd\u03bd': u'Jun', # c9effded in iso-8859-7
   u'\u0399\u03bf\u03bd': u'Jun',       # c9efed in iso-8859-7
   u'\u0399\u03bf\u03cd\u03bb': u'Jul', # c9effdeb in iso-8859-7
   u'\u0399\u03bf\u03bb': u'Jul',       # c9f9eb in iso-8859-7
   u'\u0391\u03cd\u03b3': u'Aug',       # c1fde3 in iso-8859-7
   u'\u0391\u03c5\u03b3': u'Aug',       # c1f5e3 in iso-8859-7
   u'\u03a3\u03b5\u03c0': u'Sep',       # d3e5f0 in iso-8859-7
   u'\u039f\u03ba\u03c4': u'Oct',       # cfeaf4 in iso-8859-7
   u'\u039d\u03bf\u03ad': u'Nov',       # cdefdd in iso-8859-7
   u'\u039d\u03bf\u03b5': u'Nov',       # cdefe5 in iso-8859-7
   u'\u0394\u03b5\u03ba': u'Dec',       # c4e5ea in iso-8859-7
  }

_greek_wdays = \
  { \
   u'\u039a\u03c5\u03c1': u'Sun', # caf5f1 in iso-8859-7
   u'\u0394\u03b5\u03c5': u'Mon', # c4e5f5 in iso-8859-7
   u'\u03a4\u03c1\u03b9': u'Tue', # d4f1e9 in iso-8859-7
   u'\u03a4\u03b5\u03c4': u'Wed', # d4e5f4 in iso-8859-7
   u'\u03a0\u03b5\u03bc': u'Thu', # d0e5ec in iso-8859-7
   u'\u03a0\u03b1\u03c1': u'Fri', # d0e1f1 in iso-8859-7
   u'\u03a3\u03b1\u03b2': u'Sat', # d3e1e2 in iso-8859-7
  }

_greek_date_format_re = \
    re.compile(u'([^,]+),\s+(\d{2})\s+([^\s]+)\s+(\d{4})\s+(\d{2}):(\d{2}):(\d{2})\s+([^\s]+)')

def _parse_date_greek(dateString):
    '''Parse a string according to a Greek 8-bit date format.'''
    m = _greek_date_format_re.match(dateString)
    if not m:
        return
    wday = _greek_wdays[m.group(1)]
    month = _greek_months[m.group(3)]
    rfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \
                 {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\
                  'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\
                  'zonediff': m.group(8)}
    return _parse_date_rfc822(rfc822date)
registerDateHandler(_parse_date_greek)

# Unicode strings for Hungarian date strings
_hungarian_months = \
  { \
    u'janu\u00e1r':   u'01',  # e1 in iso-8859-2
    u'febru\u00e1ri': u'02',  # e1 in iso-8859-2
    u'm\u00e1rcius':  u'03',  # e1 in iso-8859-2
    u'\u00e1prilis':  u'04',  # e1 in iso-8859-2
    u'm\u00e1ujus':   u'05',  # e1 in iso-8859-2
    u'j\u00fanius':   u'06',  # fa in iso-8859-2
    u'j\u00falius':   u'07',  # fa in iso-8859-2
    u'augusztus':     u'08',
    u'szeptember':    u'09',
    u'okt\u00f3ber':  u'10',  # f3 in iso-8859-2
    u'november':      u'11',
    u'december':      u'12',
  }

_hungarian_date_format_re = \
  re.compile(u'(\d{4})-([^-]+)-(\d{,2})T(\d{,2}):(\d{2})((\+|-)(\d{,2}:\d{2}))')

def _parse_date_hungarian(dateString):
    '''Parse a string according to a Hungarian 8-bit date format.'''
    m = _hungarian_date_format_re.match(dateString)
    if not m or m.group(2) not in _hungarian_months:
        return None
    month = _hungarian_months[m.group(2)]
    day = m.group(3)
    if len(day) == 1:
        day = '0' + day
    hour = m.group(4)
    if len(hour) == 1:
        hour = '0' + hour
    w3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \
                {'year': m.group(1), 'month': month, 'day': day,\
                 'hour': hour, 'minute': m.group(5),\
                 'zonediff': m.group(6)}
    return _parse_date_w3dtf(w3dtfdate)
registerDateHandler(_parse_date_hungarian)

# W3DTF-style date parsing adapted from PyXML xml.utils.iso8601, written by
# Drake and licensed under the Python license.  Removed all range checking
# for month, day, hour, minute, and second, since mktime will normalize
# these later
# Modified to also support MSSQL-style datetimes as defined at:
# http://msdn.microsoft.com/en-us/library/ms186724.aspx
# (which basically means allowing a space as a date/time/timezone separator)
def _parse_date_w3dtf(dateString):
    def __extract_date(m):
        year = int(m.group('year'))
        if year < 100:
            year = 100 * int(time.gmtime()[0] / 100) + int(year)
        if year < 1000:
            return 0, 0, 0
        julian = m.group('julian')
        if julian:
            julian = int(julian)
            month = julian / 30 + 1
            day = julian % 30 + 1
            jday = None
            while jday != julian:
                t = time.mktime((year, month, day, 0, 0, 0, 0, 0, 0))
                jday = time.gmtime(t)[-2]
                diff = abs(jday - julian)
                if jday > julian:
                    if diff < day:
                        day = day - diff
                    else:
                        month = month - 1
                        day = 31
                elif jday < julian:
                    if day + diff < 28:
                       day = day + diff
                    else:
                        month = month + 1
            return year, month, day
        month = m.group('month')
        day = 1
        if month is None:
            month = 1
        else:
            month = int(month)
            day = m.group('day')
            if day:
                day = int(day)
            else:
                day = 1
        return year, month, day

    def __extract_time(m):
        if not m:
            return 0, 0, 0
        hours = m.group('hours')
        if not hours:
            return 0, 0, 0
        hours = int(hours)
        minutes = int(m.group('minutes'))
        seconds = m.group('seconds')
        if seconds:
            seconds = int(seconds)
        else:
            seconds = 0
        return hours, minutes, seconds

    def __extract_tzd(m):
        '''Return the Time Zone Designator as an offset in seconds from UTC.'''
        if not m:
            return 0
        tzd = m.group('tzd')
        if not tzd:
            return 0
        if tzd == 'Z':
            return 0
        hours = int(m.group('tzdhours'))
        minutes = m.group('tzdminutes')
        if minutes:
            minutes = int(minutes)
        else:
            minutes = 0
        offset = (hours*60 + minutes) * 60
        if tzd[0] == '+':
            return -offset
        return offset

    __date_re = ('(?P<year>\d\d\d\d)'
                 '(?:(?P<dsep>-|)'
                 '(?:(?P<month>\d\d)(?:(?P=dsep)(?P<day>\d\d))?'
                 '|(?P<julian>\d\d\d)))?')
    __tzd_re = ' ?(?P<tzd>[-+](?P<tzdhours>\d\d)(?::?(?P<tzdminutes>\d\d))|Z)?'
    __time_re = ('(?P<hours>\d\d)(?P<tsep>:|)(?P<minutes>\d\d)'
                 '(?:(?P=tsep)(?P<seconds>\d\d)(?:[.,]\d+)?)?'
                 + __tzd_re)
    __datetime_re = '%s(?:[T ]%s)?' % (__date_re, __time_re)
    __datetime_rx = re.compile(__datetime_re)
    m = __datetime_rx.match(dateString)
    if (m is None) or (m.group() != dateString):
        return
    gmt = __extract_date(m) + __extract_time(m) + (0, 0, 0)
    if gmt[0] == 0:
        return
    return time.gmtime(time.mktime(gmt) + __extract_tzd(m) - time.timezone)
registerDateHandler(_parse_date_w3dtf)

def _parse_date_rfc822(dateString):
    '''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''
    data = dateString.split()
    if not data:
        return None
    if data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:
        del data[0]
    if len(data) == 4:
        s = data[3]
        i = s.find('+')
        if i > 0:
            data[3:] = [s[:i], s[i+1:]]
        else:
            data.append('')
        dateString = " ".join(data)
    # Account for the Etc/GMT timezone by stripping 'Etc/'
    elif len(data) == 5 and data[4].lower().startswith('etc/'):
        data[4] = data[4][4:]
        dateString = " ".join(data)
    if len(data) < 5:
        dateString += ' 00:00:00 GMT'
    tm = rfc822.parsedate_tz(dateString)
    if tm:
        # Jython doesn't adjust for 2-digit years like CPython does,
        # so account for it by shifting the year so that it's in the
        # range 1970-2069 (1970 being the year of the Unix epoch).
        if tm[0] < 100:
            tm = (tm[0] + (1900, 2000)[tm[0] < 70],) + tm[1:]
        return time.gmtime(rfc822.mktime_tz(tm))
# rfc822.py defines several time zones, but we define some extra ones.
# 'ET' is equivalent to 'EST', etc.
_additional_timezones = {'AT': -400, 'ET': -500, 'CT': -600, 'MT': -700, 'PT': -800}
rfc822._timezones.update(_additional_timezones)
registerDateHandler(_parse_date_rfc822)

def _parse_date_perforce(aDateString):
    """parse a date in yyyy/mm/dd hh:mm:ss TTT format"""
    # Fri, 2006/09/15 08:19:53 EDT
    _my_date_pattern = re.compile( \
        r'(\w{,3}), (\d{,4})/(\d{,2})/(\d{2}) (\d{,2}):(\d{2}):(\d{2}) (\w{,3})')

    m = _my_date_pattern.search(aDateString)
    if m is None:
        return None
    dow, year, month, day, hour, minute, second, tz = m.groups()
    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
    dateString = "%s, %s %s %s %s:%s:%s %s" % (dow, day, months[int(month) - 1], year, hour, minute, second, tz)
    tm = rfc822.parsedate_tz(dateString)
    if tm:
        return time.gmtime(rfc822.mktime_tz(tm))
registerDateHandler(_parse_date_perforce)

def parse_date(dateString):
    '''Parses a variety of date formats into a 9-tuple in GMT'''
    if not dateString:
        return None
    for handler in _date_handlers:
        try:
            date9tuple = handler(dateString)
        except (KeyError, OverflowError, ValueError):
            continue
        if not date9tuple:
            continue
        if len(date9tuple) != 9:
            continue
        return date9tuple
    return None

_parse_date = parse_date

########NEW FILE########
__FILENAME__ = speedparser
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""An attempt to be able to parse 80% of feeds that feedparser would parse
in about 1% of the time feedparser would parse them in.

LIMITATIONS:

 * result.feed.namespaces will only contain namespaces declaed on the
   root object, not those declared later in the file
 * general lack of support for many types of feeds
 * only things verified in test.py are guaranteed to be there;  many fields
   which were not considered important were skipped

"""

import re
import os
import sys
import time
import urlparse
import chardet
from lxml import etree, html
from lxml.html import clean

try:
    import feedparser
except:
    import feedparsercompat as feedparser

keymap = feedparser.FeedParserDict.keymap
fpnamespaces = feedparser._FeedParserMixin.namespaces

xmlns_map = {
    'http://www.w3.org/2005/atom': 'atom10',
    'http://purl.org/rss/1.0/': 'rss10',
    'http://my.netscape.com/rdf/simple/0.9/': 'rss090',
}

default_cleaner = clean.Cleaner(comments=True, javascript=True,
        scripts=True, safe_attrs_only=True, page_structure=True,
        style=True)

simple_cleaner = clean.Cleaner(safe_attrs_only=True, page_structure=True)

class FakeCleaner(object):
    def clean_html(self, x): return x

class IncompatibleFeedError(Exception):
    pass

fake_cleaner = FakeCleaner()

# --- text utilities ---

def unicoder(txt, hint=None, strip=True):
    if txt is None:
        return None
    if strip:
        txt = txt.strip()
    if hint:
        try: return txt.decode(hint)
        except: return unicoder(txt)
    try:
        return txt.decode('utf-8')
    except:
        try: return txt.decode('latin-1')
        except:
            try: return txt.decode('iso-8859')
            except: return txt

def first_text(xpath_result, default='', encoding='utf-8'):
    if xpath_result:
        return unicoder(xpath_result[0].text, encoding) or default
    return default

def strip_outer_tag(text):
    """Strips the outer tag, if text starts with a tag.  Not entity aware;
    designed to quickly strip outer tags from lxml cleaner output.  Only
    checks for <p> and <div> outer tags."""
    if not text or not isinstance(text, basestring):
        return text
    stripped = text.strip()
    if (stripped.startswith('<p>') or stripped.startswith('<div>')) and \
        (stripped.endswith('</p>') or stripped.endswith('</div>')):
        return stripped[stripped.index('>')+1:stripped.rindex('<')]
    return text

nsre = re.compile(r'xmlns\s*=\s*[\'"](.+?)[\'"]')
def strip_namespace(document):
    if document[:1000].count('xmlns') > 5:
        if 'xmlns' not in document[:1000]:
            return None, document
    elif 'xmlns' not in document[:400]:
        return None, document
    match = nsre.search(document)
    if match:
        return match.groups()[0], nsre.sub('', document)
    return None, document

def munge_author(author):
    """If an author contains an email and a name in it, make sure it is in
    the format: "name (email)"."""
    # this loveliness is from feedparser but was not usable as a function
    if '@' in author:
        emailmatch = re.search(ur'''(([a-zA-Z0-9\_\-\.\+]+)@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.)|(([a-zA-Z0-9\-]+\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\]?))(\?subject=\S+)?''', author)
        if emailmatch:
            email = emailmatch.group(0)
            # probably a better way to do the following, but it passes all the tests
            author = author.replace(email, u'')
            author = author.replace(u'()', u'')
            author = author.replace(u'<>', u'')
            author = author.replace(u'&lt;&gt;', u'')
            author = author.strip()
            if author and (author[0] == u'('):
                author = author[1:]
            if author and (author[-1] == u')'):
                author = author[:-1]
            author = author.strip()
            return '%s (%s)' % (author, email)
    return author

# --- common xml utilities ---

def reverse_namespace_map(nsmap):
    d = fpnamespaces.copy()
    d.update(dict([(v,k) for (k,v) in nsmap.iteritems()]))
    return d

def base_url(root):
    """Determine the base url for a root element."""
    for attr,value in root.attrib.iteritems():
        if attr.endswith('base') and 'http' in value:
            return value
    return None

def full_href(href, base=None):
    return urlparse.urljoin(base, href)

def full_href_attribs(attribs, base=None):
    if base is None: return dict(attribs)
    d = dict(attribs)
    for key,value in d.iteritems():
        if key == 'href':
            d[key] = full_href(value, base)
    return d

def clean_ns(tag):
    """Return a tag and its namespace separately."""
    if '}' in tag:
        split = tag.split('}')
        return split[0].strip('{'), split[-1]
    return '', tag

def xpath(node, query, namespaces={}):
    """A safe xpath that only uses namespaces if available."""
    if namespaces and 'None' not in namespaces:
        return node.xpath(query, namespaces=namespaces)
    return node.xpath(query)

def innertext(node):
    """Return the inner text of a node.  If a node has no sub elements, this
    is just node.text.  Otherwise, it's node.text + sub-element-text +
    node.tail."""
    if not len(node): return node.text
    return (node.text or '') + ''.join([etree.tostring(c) for c in node]) + (node.tail or '')

class SpeedParserEntriesRss20(object):
    entry_xpath = '/rss/item | /rss/channel/item'
    tag_map = {
        'pubDate' : 'date',
        'pubdate' : 'date',
        'date': 'date',
        'updated' : 'date',
        'modified' : 'date',
        'link' : 'links',
        'title': 'title',
        'creator': 'author',
        'author': 'author',
        'name': 'author',
        'guid': 'guid',
        'id': 'guid',
        'comments': 'comments',
        'encoded': 'content',
        'content': 'content',
        'summary': 'summary',
        'description': 'summary',
        'media:content': 'media_content',
        'media:thumbnail': 'media_thumbnail',
        'media_thumbnail': 'media_thumbnail',
        'media:group': 'media_group',
        'itunes:summary': 'content',
        'gr:annotation': 'annotation',
    }

    def __init__(self, root, namespaces={}, version='rss20', encoding='utf-8', feed={},
            cleaner=default_cleaner, unix_timestamp=False):
        self.encoding = encoding
        self.namespaces = namespaces
        self.unix_timestamp = unix_timestamp
        self.nslookup = reverse_namespace_map(namespaces)
        self.cleaner = cleaner
        self.entry_objects = xpath(root, self.entry_xpath, namespaces)
        self.feed = feed
        self.baseurl = base_url(root)
        if not self.baseurl and 'link' in self.feed:
            self.baseurl = self.feed.link
        entries = []
        for obj in self.entry_objects:
            d = self.parse_entry(obj)
            if d: entries.append(d)
        self.entries = entries

    def clean(self, text):
        if text and isinstance(text, basestring):
            return self.cleaner.clean_html(text)
        return text

    def parse_entry(self, entry):
        """An attempt to parse pieces of an entry out w/o xpath, by looping
        over the entry root's children and slotting them into the right places.
        This is going to be way messier than SpeedParserEntries, and maybe
        less cleanly usable, but it should be faster."""

        e = feedparser.FeedParserDict()
        tag_map = self.tag_map
        nslookup = self.nslookup

        for child in entry.getchildren():
            if isinstance(child, etree._Comment):
                continue
            ns, tag = clean_ns(child.tag)
            mapping = tag_map.get(tag, None)
            if mapping:
                getattr(self, 'parse_%s' % mapping)(child, e, nslookup.get(ns, ns))
            if not ns:
                continue
            fulltag = '%s:%s' % (nslookup.get(ns, ''), tag)
            mapping = tag_map.get(fulltag, None)
            if mapping:
                getattr(self, 'parse_%s' % mapping)(child, e, nslookup[ns])

        lacks_summary = 'summary' not in e or e['summary'] is None
        lacks_content = 'content' not in e or not bool(e.get('content', None))

        if not lacks_summary and lacks_content:
            e['content'] = [{'value': e.summary}]

        # feedparser sometimes copies the first content value into the
        # summary field when summary was completely missing;  we want
        # to do that as well, but avoid the case where summary was given as ''
        if lacks_summary and not lacks_content:
            e['summary'] = e['content'][0]['value']

        if e.get('summary', False) is None:
            e['summary'] = u''

        # support feed entries that have a guid but no link
        if 'guid' in e and 'link' not in e:
            e['link'] = full_href(e['guid'], self.baseurl)

        return e

    def parse_date(self, node, entry, ns=''):
        value = unicoder(node.text)
        entry['updated'] = value
        date = feedparser._parse_date(value)
        if self.unix_timestamp and date:
            date = time.mktime(date)
        entry['updated_parsed'] = date

    def parse_title(self, node, entry, ns=''):
        if ns in ('media',) and 'title' in entry:
            return
        title = unicoder(node.text)
        if title is not None:
            title = strip_outer_tag(self.clean(title.strip()))
        entry['title'] = title or ''

    def parse_author(self, node, entry, ns=''):
        if ns and ns in ('itunes', 'dm') and 'author' in entry:
            return
        if node.text and len(list(node)) == 0:
            entry['author'] = munge_author(unicoder(node.text))
            return
        name, email = None, None
        for child in node:
            if child.tag == 'name': name = unicoder(child.text or '')
            if child.tag == 'email': email = unicoder(child.text or '')
        if name and not email:
            entry['author'] = munge_author(name)
        elif not name and not email:
            entry['author'] = ''
        else:
            entry['author'] = '%s (%s)' % (name, email)

    def parse_guid(self, node, entry, ns=''):
        if node.text:
            entry['guid'] = unicoder(node.text)

    def parse_annotation(self, node, entry, ns='gr'):
        if entry.get('author', '') and 'unknown' not in entry['author'].lower():
            return
        for child in node:
            if child.tag.endswith('author'):
                self.parse_author(child, entry, ns='')

    def parse_links(self, node, entry, ns=''):
        if unicoder(node.text):
            entry['link'] = full_href(unicoder(node.text).strip('#'), self.baseurl)
        if 'link' not in entry and node.attrib.get('rel', '') == 'alternate' and 'href' in node.attrib:
            entry['link'] = full_href(unicoder(node.attrib['href']).strip('#'), self.baseurl)
        if 'link' not in entry and 'rel' not in node.attrib and 'href' in node.attrib:
            entry['link'] = full_href(unicoder(node.attrib['href']).strip('#'), self.baseurl)
        entry.setdefault('links', []).append(full_href_attribs(node.attrib, self.baseurl))
        # media can be embedded within links..
        for child in node:
            ns, tag = clean_ns(child.tag)
            if self.nslookup[ns] == 'media' and tag == 'content':
                self.parse_media_content(child, entry)

    def parse_comments(self, node, entry, ns=''):
        if 'comments' in entry and ns: return
        entry['comments'] = strip_outer_tag(self.clean(unicoder(node.text)))

    def parse_content(self, node, entry, ns=''):
        # media:content is processed as media_content below
        if ns and node.tag.endswith('content') and ns not in ('itunes',):
            return
        content = unicoder(innertext(node))
        content = self.clean(content)
        entry.setdefault('content', []).append({'value': content or ''})

    def parse_summary(self, node, entry, ns=''):
        if ns in ('media', ): return
        if ns == 'itunes' and entry.get('summary', None):
            return
        if 'content' in entry:
            entry['summary'] = entry['content'][0]['value']
            return
        summary = unicoder(innertext(node))
        summary = self.clean(summary)
        entry['summary'] = summary or ''

    def parse_media_content(self, node, entry, ns='media'):
        entry.setdefault('media_content', []).append(dict(node.attrib))
        for child in node:
            if child.tag.endswith('thumbnail'):
                entry.setdefault('media_thumbnail', []).append(dict(child.attrib))

    def parse_media_thumbnail(self, node, entry, ns='media'):
        entry.setdefault('media_thumbnail', []).append(dict(node.attrib))

    def parse_media_group(self, node, entry, ns='media'):
        for child in node:
            if child.tag.endswith('content'):
                self.parse_media_content(child, entry)
            elif child.tag.endswith('thumbnail'):
                self.parse_media_thumbnail(child, entry)

    def entry_list(self):
        return self.entries

class SpeedParserEntriesRdf(SpeedParserEntriesRss20):
    entry_xpath = '/rdf:RDF/item | /rdf:RDF/channel/item'

class SpeedParserEntriesAtom(SpeedParserEntriesRss20):
    entry_xpath = '/feed/entry'

    def parse_summary(self, node, entry, ns=''):
        if 'content' in entry:
            entry['summary'] = entry['content'][0]['value']
        else:
            super(SpeedParserEntriesAtom, self).parse_summary(node, entry, ns)

class SpeedParserFeedRss20(object):
    channel_xpath = '/rss/channel'
    tag_map = {
        'title' : 'title',
        'description' : 'subtitle',
        'tagline' : 'subtitle',
        'subtitle' : 'subtitle',
        'link' : 'links',
        'pubDate': 'date',
        'updated' : 'date',
        'modified' : 'date',
        'date': 'date',
        'generator' : 'generator',
        'generatorAgent': 'generator',
        'language' : 'lang',
        'id': 'id',
        'lastBuildDate' : 'date',
    }

    def __init__(self, root, namespaces={}, encoding='utf-8', type='rss20', cleaner=default_cleaner,
            unix_timestamp=False):
        """A port of SpeedParserFeed that uses far fewer xpath lookups, which
        ends up simplifying parsing and makes it easier to catch the various
        names that different tags might come under."""
        self.root = root
        self.unix_timestamp = unix_timestamp
        nslookup = reverse_namespace_map(namespaces)
        self.cleaner = cleaner
        self.baseurl = base_url(root)

        feed = feedparser.FeedParserDict()
        tag_map = self.tag_map

        channel = xpath(root, self.channel_xpath, namespaces)
        if len(channel) == 1:
            channel = channel[0]

        for child in channel:
            if isinstance(child, etree._Comment):
                continue
            ns, tag = clean_ns(child.tag)
            mapping = tag_map.get(tag, None)
            if mapping:
                getattr(self, 'parse_%s' % mapping)(child, feed, nslookup.get(ns, ns))
            if not ns:
                continue
            fulltag = '%s:%s' % (nslookup.get(ns, ''), tag)
            mapping = tag_map.get(fulltag, None)
            if mapping:
                getattr(self, 'parse_%s' % mapping)(child, feed, nslookup[ns])


        # this copies feedparser behavior if, say, xml:lang is defined in the
        # root feed element, even though this element tends to have garbage like
        # "utf-8" in it rather than an actual language
        if 'language' not in feed:
            for attr in root.attrib:
                if attr.endswith('lang'):
                    feed['language'] = root.attrib[attr]

        if 'id' in feed and 'link' not in feed:
            feed['link'] = feed['id']

        self.feed = feed

    def clean(self, text, outer_tag=True):
        if text and isinstance(text, basestring):
            if not outer_tag:
                txt = self.cleaner.clean_html(text)
                frag = lxml.html.fragment_fromstring(txt)
            return self.cleaner.clean_html(text)
        return text

    def parse_title(self, node, feed, ns=''):
        feed['title'] = strip_outer_tag(self.clean(unicoder(node.text))) or ''

    def parse_subtitle(self, node, feed, ns=''):
        feed['subtitle'] = strip_outer_tag(self.clean(unicoder(node.text))) or ''

    def parse_links(self, node, feed, ns=''):
        if node.text:
            feed['link'] = full_href(unicoder(node.text).strip('#'), self.baseurl)
        if 'link' not in feed and node.attrib.get('rel', '') == 'alternate' and 'href' in node.attrib:
            feed['link'] = full_href(unicoder(node.attrib['href']).strip('#'), self.baseurl)
        if 'link' not in feed and 'rel' not in node.attrib and 'href' in node.attrib:
            feed['link'] = full_href(unicoder(node.attrib['href']).strip('#'), self.baseurl)
        feed.setdefault('links', []).append(full_href_attribs(node.attrib, self.baseurl))

    def parse_date(self, node, feed, ns=''):
        value = unicoder(node.text)
        feed['updated'] = value
        date = feedparser._parse_date(value)
        if self.unix_timestamp and date:
            date = time.mktime(date)
        feed['updated_parsed'] = date

    def parse_lang(self, node, feed, ns=''):
        feed['language'] = unicoder(node.text)

    def parse_generator(self, node, feed, ns=''):
        value = unicoder(node.text)
        if value:
            feed['generator'] = value
        else:
            for value in node.attrib.itervalues():
                if 'http://' in value:
                    feed['generator'] = value

    def parse_id(self, node, feed, ns=''):
        feed['id'] = unicoder(node.text)

    def feed_dict(self):
        return self.feed

class SpeedParserFeedAtom(SpeedParserFeedRss20):
    channel_xpath = '/feed'

class SpeedParserFeedRdf(SpeedParserFeedRss20):
    channel_xpath = '/rdf:RDF/channel'

class SpeedParser(object):

    version_map = {
        'rss2': 'rss20',
    }

    def __init__(self, content, cleaner=default_cleaner, unix_timestamp=False, encoding=None):
        self.cleaner = cleaner
        self.xmlns, content = strip_namespace(content)
        self.unix_timestamp = unix_timestamp
        if self.xmlns and '#' in self.xmlns:
            self.xmlns = self.xmlns.strip('#')
        parser = etree.XMLParser(recover=True)
        tree = etree.fromstring(content, parser=parser)
        if isinstance(tree, etree._ElementTree):
            self.tree = tree
            self.root = tree.getroot()
        else:
            self.tree = tree.getroottree()
            self.root = tree
        self.encoding = encoding if encoding else self.parse_encoding()
        self.version = self.parse_version()
        if self.version in self.version_map:
            self.version = self.version_map[self.version]
        if 'unk' in self.version:
            raise IncompatibleFeedError("Could not determine version of this feed.")
        self.namespaces = self.parse_namespaces()
        self.feed = self.parse_feed(self.version, self.encoding)
        self.entries = self.parse_entries(self.version, self.encoding)

    def parse_version(self):
        r = self.root
        root_ns, root_tag = clean_ns(r.tag)
        root_tag = root_tag.lower()
        vers = 'unk'
        if self.xmlns and self.xmlns.lower() in xmlns_map:
            value =  xmlns_map[self.xmlns.lower()]
            if value == 'rss10' and root_tag == 'rss':
                value = 'rss010'
            if not (value.startswith('atom') and root_tag == 'rss'):
                return value
        elif self.xmlns:
            vers = self.xmlns.split('/')[-2].replace('.', '')
        tag = root_tag
        if r.attrib.get('version', None):
            vers = r.attrib['version'].replace('.', '')
        if root_tag in ('rss', 'rdf'):
            tag = 'rss'
        if root_tag in ('feed'):
            tag = 'atom'
        if root_tag == 'rss' and vers == '10' and root_tag == 'rss':
            vers = ''
        return '%s%s' % (tag, vers)

    def parse_namespaces(self):
        nsmap = self.root.nsmap.copy()
        for key in nsmap.keys():
            if key is None:
                nsmap[self.xmlns] = nsmap[key]
                del nsmap[key]
                break
        return nsmap

    def parse_encoding(self):
        return self.tree.docinfo.encoding.lower()

    def parse_feed(self, version, encoding):
        kwargs = dict(
            encoding=encoding,
            unix_timestamp=self.unix_timestamp,
            namespaces=self.namespaces
        )
        if version in ('rss20', 'rss092', 'rss091', 'rss'):
            return SpeedParserFeedRss20(self.root, **kwargs).feed_dict()
        if version in ('rss090', 'rss10'):
            return SpeedParserFeedRdf(self.root, **kwargs).feed_dict()
        if version in ('atom10', 'atom03'):
            return SpeedParserFeedAtom(self.root, **kwargs).feed_dict()
        raise IncompatibleFeedError("Feed not compatible with speedparser.")

    def parse_entries(self, version, encoding):
        kwargs = dict(encoding=encoding, namespaces=self.namespaces,
            cleaner=self.cleaner, feed=self.feed, unix_timestamp=self.unix_timestamp)
        if version in ('rss20', 'rss092', 'rss091', 'rss'):
            return SpeedParserEntriesRss20(self.root, **kwargs).entry_list()
        if version in ('rss090', 'rss10'):
            return SpeedParserEntriesRdf(self.root, **kwargs).entry_list()
        if version in ('atom10', 'atom03'):
            return SpeedParserEntriesAtom(self.root, **kwargs).entry_list()
        raise IncompatibleFeedError("Feed not compatible with speedparser.")

    def update(self, result):
        if self.version:
            result['version'] = self.version
        if self.namespaces:
            result['namespaces'] = self.namespaces
        if self.feed:
            result.feed.update(self.feed)
        if self.entries:
            result['entries'] = self.entries
        if self.encoding:
            result['encoding'] = self.encoding


def parse(document, clean_html=True, unix_timestamp=False, encoding=None):
    """Parse a document and return a feedparser dictionary with attr key access.
    If clean_html is False, the html in the feed will not be cleaned.  If
    clean_html is True, a sane version of lxml.html.clean.Cleaner will be used.
    If it is a Cleaner object, that cleaner will be used.  If unix_timestamp is
    True, the date information will be a numerical unix timestamp rather than a
    struct_time.  If encoding is provided, the encoding of the document will be
    manually set to that."""
    if isinstance(clean_html, bool):
        cleaner = default_cleaner if clean_html else fake_cleaner
    else:
        cleaner = clean_html
    result = feedparser.FeedParserDict()
    result['feed'] = feedparser.FeedParserDict()
    result['entries'] = []
    result['bozo'] = 0
    try:
        parser = SpeedParser(document, cleaner, unix_timestamp, encoding)
        parser.update(result)
    except Exception, e:
        if isinstance(e, UnicodeDecodeError) and encoding is True:
            encoding = chardet.detect(document)['encoding']
            document = document.decode(encoding, 'replace').encode('utf-8')
            return parse(document, clean_html, unix_timestamp, encoding)
        import traceback
        result['bozo'] = 1
        result['bozo_exception'] = e
        result['bozo_tb'] = traceback.format_exc()
    return result

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 2:
        print "Must provide filename of feed."
    filename = sys.argv[1]
    feed = open(filename).read()
    if '-- END TRACEBACK --' in feed:
        feed = feed.split('-- END TRACEBACK --')[1].strip()
    import pprint
    pprint.pprint(parse(feed))


########NEW FILE########
__FILENAME__ = xmlpprint
#!/usr/bin/env python
# -*- coding: utf-8 -*-

""" """

from lxml import etree
import sys

if not len(sys.argv) == 2:
    print "Supply one xml file to xmlpprint.py"
    sys.exit(-1)

tree = etree.parse(sys.argv[1])
print etree.tostring(tree, pretty_print=True)


########NEW FILE########
__FILENAME__ = regressions
#!/usr/bin/env python
# -*- coding: utf-8 -*-

""" """

from unittest import TestCase
from pprint import pprint, pformat
from speedparser import parse


class UnixTimestampError(TestCase):
    def test_unix_timestamp_failure(self):
        """This tests for a bug where a non-existant timestamp is used to
        create a unix timestamp (from None) and throws an exception."""
        feed = '<?xml version="1.0" encoding="UTF-8"?> \n<rss version="2.0"\n\txmlns:content="http://purl.org/rss/1.0/modules/content/"\n\txmlns:wfw="http://wellformedweb.org/CommentAPI/"\n\txmlns:dc="http://purl.org/dc/elements/1.1/"\n\txmlns:atom="http://www.w3.org/2005/Atom"\n\txmlns:sy="http://purl.org/rss/1.0/modules/syndication/"\n\txmlns:slash="http://purl.org/rss/1.0/modules/slash/"\n\t> \n \n<channel>\n<title>betamax - Svpply</title> \n\t<link>http://svpply.com</link> \n\t<description>Svpply is a retail bookmarking and recommendation service.</description> \n\t<lastBuildDate>1323107774</lastBuildDate> \n\t<language>en</language> \n\t<sy:updatePeriod>hourly</sy:updatePeriod> \n\t<sy:updateFrequency>1</sy:updateFrequency> \n\t</channel> \n</rss>'
        result = parse(feed, unix_timestamp=True)
        self.assertTrue('bozo_exception' not in result, str(result))

class NonCleanedTitle(TestCase):
    def test_non_cleaned_title(self):
        """This tests for a bug where titles were not stripped of html despite
        a cleaner being supplied to speedparser."""
        from lxml.html.clean import Cleaner
        feed = '''<?xml version="1.0"?><feed xmlns="http://www.w3.org/2005/Atom"><title>scribble.yuyat.jp</title><link href="http://scribble.yuyat.jp/"/><link type="application/atom+xml" rel="self" href="http://scribble.yuyat.jp/atom.xml"/><updated>2012-01-08T18:34:39-08:00</updated><id>http://scribble.yuyat.jp/</id><author><name>Yuya Takeyama</name></author><entry><id>http://scribble.yuyat.jp/2012/01/07/this-is-just-a-scribble</id><link type="text/html" rel="alternate" href="http://scribble.yuyat.jp/2012/01/07/this-is-just-a-scribble.html"/><title>scribble 始めます &lt;script&gt;alert(1)&lt;/script&gt;</title><updated>2012-01-07T00:00:00-08:00</updated><author><name>Yuya Takeyama</name></author><content type="html">&lt;p&gt;今まで書いて来た &lt;a href='http://blog.yuyat.jp/'&gt;Born Too Late&lt;/a&gt; の住み分けとしては, あっちがいろいろ調べてからまとめる用, こっちはもっと殴り書いていく感じにしたい.&lt;/p&gt;&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='lineno'&gt;1&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;Foo&lt;/span&gt;&lt;span class='lineno'&gt;2&lt;/span&gt;   &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='nf'&gt;bar&lt;/span&gt;&lt;span class='lineno'&gt;3&lt;/span&gt;     &lt;span class='ss'&gt;:baz&lt;/span&gt;&lt;span class='lineno'&gt;4&lt;/span&gt;   &lt;span class='k'&gt;end&lt;/span&gt;&lt;span class='lineno'&gt;5&lt;/span&gt; &lt;span class='k'&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content></entry></feed>'''
        cleaner = Cleaner(comments=True, javascript=True, scripts=True,
            safe_attrs_only=True, page_structure=True, style=True, embedded=False,
            remove_tags=['body'])
        result = parse(feed, unix_timestamp=True, clean_html=cleaner)
        self.assertTrue('bozo_exception' not in result, str(result))
        for e in result.entries:
            self.assertTrue('alert(1)' not in e.title, e.title)
            self.assertTrue(not e.title.startswith('<p>'), e.title)

class NoneTypeNoStrip(TestCase):
    def test_nonetype_no_strip_regression(self):
        """This tests for a bug in 0.1.6 where the strip_outer_tag function
        would be called on None and raise an exception."""
        feed = """<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"><channel><title>Instapaper: Starred</title><link>http://www.instapaper.com/starred</link><description></description><item><title>Toronto News: Flipped Junction homes taken on a wild real estate ride ending in fraud allegations - thestar.com</title><link>http://www.thestar.com/news/article/1111810--flipped-junction-homes-taken-on-a-wild-real-estate-ride-ending-in-fraud-allegations</link><description></description><pubDate>Sat, 07 Jan 2012 18:46:18 EST</pubDate></item></channel></rss>"""
        self.assertTrue(parse(feed).feed.title == "Instapaper: Starred")

class InvalidEntityRecovery(TestCase):
    def test_invalid_entity_recovery(self):
        feed = """<?xml version="1.0"?><rss xmlns:itunes="http://www.itunes.com/DTDs/Podcast-1.0.dtd" version="2.0"><channel><title>Faith Promise Church Podcast</title><description>Weekly message Podcast from Faith Promise Church.  Faith Promise church is an exciting church located in Knoxville, Tennessee. For information about the church, please visit our website at faithpromise.org.  We hope you enjoy and are blessed by our podcast.</description><link>http://faithpromise.org</link><language>en-us</language><item><title>T C & B (Taking Care of Busine</title><link>http://faithpromise.org/media/20111112-13.mp3</link><description>T C & B (Taking Care of Busine - Faith Promise Church Podcasts - Dr. Chris Stephens</description><pubDate>Mon, 14 Nov 2011 11:53:23 -0500</pubDate><enclosure url="http://faithpromise.org/media/20111112-13.mp3" length="36383475" type="audio/mpeg"/></item></channel></rss>"""
        self.assertTrue(parse(feed).bozo == 0)
        self.assertTrue(len(parse(feed).entries) == 1)

class SupportRssVersion2NoZero(TestCase):
    def test_support_rss_version_2_no_zero(self):
        feed = """<?xml version="1.0" encoding="UTF-8"?><rss version="2"><channel><title>Australian Canoeing</title><link>http://canoe.org.au</link><description>Latest News</description><language>en</language><ttl>480</ttl><pubDate>Sat, 21 Jan 2012 14:00:02 UTC</pubDate><item><title>Lifestart Kayak for Kids 2012</title><link>http://canoe.org.au/default.asp?Page=23196</link><description>Kayak for Kids is a unique paddling challenge on beautiful Sydney Harbour for everyone from beginner to serious kayaker.</description><enclosure url="http://canoe.org.au/site/canoeing/image/fullsize/35576.jpg" type="image/jpeg" /><pubDate>Thu, 19 Jan 2012 14:00:00 UTC</pubDate><guid>http://canoe.org.au/default.asp?Page=23196</guid></item></channel></rss>"""
        self.assertTrue(parse(feed).bozo == 0)
        self.assertTrue(len(parse(feed).entries) == 1)

class DetectCharsets(TestCase):
    def test_detect_charsets(self):
        feed = """<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>"ismb2010" - Google Blogs�gning</title><link>http://www.google.com/search?hl=da&amp;lr=&amp;q=%22ismb2010%22&amp;ie=utf-8&amp;tbm=blg</link><description>S�geresultaterne &lt;b&gt;1&lt;/b&gt; - &lt;b&gt;10&lt;/b&gt; ud af ca. &lt;b&gt;59&lt;/b&gt; for &lt;b&gt;&amp;quot;ismb2010&amp;quot;&lt;/b&gt;.</description><opensearch:totalResults>59</opensearch:totalResults><opensearch:startIndex>1</opensearch:startIndex><opensearch:itemsPerPage>10</opensearch:itemsPerPage><item><title>Beyond DNA: &lt;b&gt;ISMB2010&lt;/b&gt; Boston</title><link>http://xiazheng.blogspot.com/2010/07/ismb2010-boston.html</link><description>ISMB of this year was held at Boston on July 10-14. I&amp;#39;m so happy to meet big guys in Bioinformatics whose papers I have ever read, especially Dr. Ratsch from MPI. One information this conference delivered this year is that &lt;b&gt;...&lt;/b&gt;</description><dc:publisher>Beyond DNA</dc:publisher><dc:creator>Zheng Xia</dc:creator><dc:date>Sat, 17 Jul 2010 13:56:00 GMT</dc:date></item></channel></rss>"""
        self.assertTrue(parse(feed, encoding=True).bozo == 0)
        self.assertTrue(len(parse(feed, encoding=True).entries) == 1)

class TextHeartParserError(TestCase):
    def test_text_heart_parser_error(self):
        """This is a placeholder test.  LXML punts because the title trips an
        unrecoverable parser error, and we have no way of cleaning it.  This
        would be a big issue, but FeedParser apparently cannot figure this out
        either as it breaks SAXParser."""
        import feedparser
        feed = """<?xml version="1.0" encoding="UTF-8"?><rss version="2"><channel><title>&lt;3</title><link>http://canoe.org.au</link><description>Latest News</description><language>en</language><ttl>480</ttl><pubDate>Sat, 21 Jan 2012 14:00:02 UTC</pubDate><item><title>&lt;3</title><link>http://canoe.org.au/default.asp?Page=23196</link><description>Kayak for Kids is a unique paddling challenge on beautiful Sydney Harbour for everyone from beginner to serious kayaker.</description><enclosure url="http://canoe.org.au/site/canoeing/image/fullsize/35576.jpg" type="image/jpeg" /><pubDate>Thu, 19 Jan 2012 14:00:00 UTC</pubDate><guid>http://canoe.org.au/default.asp?Page=23196</guid></item></channel></rss>"""
        self.assertTrue(parse(feed).bozo == 1)
        self.assertTrue(feedparser.parse(feed).bozo == 0)


class RdfRss090Support(TestCase):
    def test_rdf_rss_090_support(self):
        feed = """<?xml version="1.0" encoding="utf-8"?><rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns="http://my.netscape.com/rdf/simple/0.9/"><channel><title>heise online News</title><link>http://www.heise.de/newsticker/</link><description>Nachrichten nicht nur aus der Welt der Computer</description></channel><item><title>Am 6. Juni ist World IPv6 Launch Day</title><link>http://www.heise.de/newsticker/meldung/Am-6-Juni-ist-World-IPv6-Launch-Day-1415071.html/from/rss09</link><description>Am 6. Juni 2012 veranstaltet die Internet Society den IPv6 World Launch Day, an dem teilnehmende Internet Service Provider, Netzwerkhersteller und Service-Anbieter dauerhaft IPv6 schalten werden.</description></item></rdf:RDF>"""
        self.assertTrue(parse(feed).bozo == 0)
        self.assertTrue(len(parse(feed).entries) == 1)

class XmlnsSpaceSupport(TestCase):
    def test_xmlns_space_support(self):
        from os import path
        import ipdb; ipdb.set_trace();
        feed = open(path.join(path.dirname(__file__), "test-feeds/co.atom")).read()
        res = parse(feed)
        self.assertTrue(res.bozo == 0)
        self.assertTrue(len(res.entries) == 3)


########NEW FILE########
__FILENAME__ = speedparsertests
#!/usr/bin/env python
# -*- coding: utf-8 -*-

""" """

import os
import time
import difflib
from glob import glob
from unittest import TestCase
from pprint import pprint, pformat

try:
    from speedparser import speedparser
except ImportError:
    import speedparser

import feedparser

try:
    import simplejson as json
except:
    import json

try:
    from jinja2.filters import do_filesizeformat as sizeformat
except:
    sizeformat = lambda x: "%0.2f b" % x

class TimeEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, time.struct_time):
            return time.mktime(o)
        if isinstance(o, Exception):
            return repr(o)
        return json.JSONEncoder.default(self, o)

munge_author = speedparser.munge_author

def load_cache(path):
    """Load a cached feedparser result."""
    jsonpath = path.replace('dat', 'json')
    if not os.path.exists(jsonpath):
        return None
    with open(jsonpath) as f:
        data = json.loads(f.read())
    ret = feedparser.FeedParserDict()
    ret.update(data)
    if 'updated_parsed' in data['feed'] and data['feed']['updated_parsed']:
        try:
            data['feed']['updated_parsed'] = time.gmtime(data['feed']['updated_parsed'])
        except: pass

    ret.feed = feedparser.FeedParserDict(data.get('feed', {}))
    entries = []
    for e in data.get('entries', []):
        if 'updated_parsed' in e and e['updated_parsed']:
            try:
                e['updated_parsed'] = time.gmtime(e['updated_parsed'])
            except: pass
        entries.append(feedparser.FeedParserDict(e))
    ret.entries = entries
    return ret

def update_cache(path, data):
    """Update the feedparser cache."""
    jsonpath = path.replace('dat', 'json')
    if isinstance(data, dict):
        content = json.dumps(data, cls=TimeEncoder)
    elif isinstance(data, basestring):
        content = data
    else:
        return None
    with open(jsonpath, 'w') as f:
        f.write(content)

def feedparse(path):
    with open(path) as f:
        text = f.read()
    try:
        result = feedparser.parse(text)
    except:
        return None
    return (path, json.dumps(result, cls=TimeEncoder).encode('base64'))

def build_feedparser_cache(update=False):
    """Build a feedparser cache."""
    from multiprocessing import Pool, cpu_count

    paths = []
    for path in glob('feeds/*.dat'):
        if not update and os.path.exists(path.replace('dat', 'json')):
            continue
        paths.append(path)

    parser_pool = Pool(cpu_count())
    results = []
    for path in paths:
        results.append(parser_pool.apply_async(feedparse, (path,)))
    for result in results:
        value = result.get()
        if value is None: continue
        path, json = value
        json = json.decode('base64')
        update_cache(path, json)

class TestCaseBase(TestCase):
    def assertPrettyClose(self, s1, s2):
        """Assert that two strings are pretty damn near equal.  This gets around
        differences in little tidy nonsense FP does that SP won't do."""
        threshold = 0.10
        if len(s1) > 1024 and len(s2) > 1024:
            threshold = 0.25
        # sometimes the title is just made up of some unicode escapes, and since
        # fp and sp treat these differently, we don't pay attention to differences
        # so long as the length is short
        if '&#' in s1 and '&#' not in s2 and len(s1) < 50:
            return True
        if len(s1.strip()) == 0 and len(s2.strip()) < 25:
            return True
        matcher = difflib.SequenceMatcher(None, s1, s2)
        ratio = matcher.quick_ratio()
        if ratio < threshold:
            longest_block = matcher.find_longest_match(0, len(s1), 0, len(s2))
            if len(s1) and longest_block.size / float(len(s1)) > threshold:
                return
            if longest_block.size < 50:
                raise AssertionError("%s\n ---- \n%s\n are not similar enough (%0.3f < %0.3f, %d)" %\
                        (s1, s2, ratio, threshold, longest_block.size))

    def assertSameEmail(self, em1, em2):
        """Assert two emails are pretty similar.  FP and SP munge emails into
        one format, but SP is more consistent in providing that format than FP
        is."""
        if em1 == em2: return True
        if em1 == munge_author(em2):
            return True
        if em2 == munge_author(em1):
            return True
        if munge_author(em1) == munge_author(em2):
            return True
        # if we just have somehow recorded more information because we are
        # awesome, do not register that as a bug
        if em1 in em2:
            return True
        if '@' not in em1 and '@' not in em2:
            # i've encountered some issues here where both author fields are
            # absolute garbage and feedparser seems to prefer one to the other
            # based on no particular algorithm
            return True
        raise AssertionError("em1 and em2 not similar enough %s != %s" % (em1, em2))

    def assertSameLinks(self, l1, l2):
        l1 = l1.strip('#').lower().strip()
        l2 = l2.strip('#').lower().strip()
        if l1 == l2: return True
        if l1 in l2: return True
        # google uses weird object enclosure stuff that would be slow to
        # parse correctly;  the default link for the entry is good enough
        # in thee cases
        if 'buzz' in l2: return True
        if 'plus.google.com' in l2: return True
        # feedparser actually has a bug here where it'l strip ;'s from &gt; in
        # url, though a javascript: href is probably utter garbage anyway
        if l2.startswith('javascript:'):
            return self.assertPrettyClose(l1, l2)
        raise AssertionError('link1 and link2 are not similar enough %s != %s' % (l1, l2))

    def assertSameTime(self, t1, t2):
        if not t1 and not t2: return True
        if t1 == t2: return True
        gt1 = time.gmtime(time.mktime(t1))
        gt2 = time.gmtime(time.mktime(t2))
        if t1 == gt2: return True
        if t2 == gt1: return True
        raise AssertionError("time1 and time2 are not similar enough (%r != %r)" % (t1, t2))

def feed_equivalence(testcase, fpresult, spresult):
    self = testcase
    fpf = fpresult.feed
    spf = spresult.feed
    self.assertEqual(fpf.title, spf.title)
    if 'subtitle' in fpf:
        self.assertPrettyClose(fpf.subtitle, spf.subtitle)
    if 'generator' in fpf:
        self.assertEqual(fpf.generator, spf.generator)
    if 'link' in fpf and fpf.link:
        self.assertSameLinks(fpf.link, spf.link)
    if 'language' in fpf:
        self.assertEqual(fpf.language, spf.language)
    if 'updated' in fpf:
        self.assertEqual(fpf.updated, spf.updated)
        self.assertSameTime(fpf.updated_parsed, spf.updated_parsed)

    self.assertEqual(fpresult.version, spresult.version)
    self.assertEqual(fpresult.encoding, spresult.encoding)
    # make sure the namespaces are set up properly;  feedparser adds some
    # root namespaces based on some processing and some dicts that we do not bother with
    if 'namespaces' in spresult:
        for nskey in fpresult.namespaces:
            if nskey and nskey in spresult.namespaces:
                self.assertEqual(fpresult.namespaces[nskey], spresult.namespaces[nskey])

def entry_equivalence(test_case, fpresult, spresult):
    self = test_case
    self.assertEqual(len(fpresult.entries), len(spresult.entries))
    for fpe,spe in zip(fpresult.entries, spresult.entries):
        if 'link' in fpe:
            self.assertSameLinks(fpe.link, spe.link)
        if 'author' in fpe:
            self.assertSameEmail(fpe.author, spe.author)
        if 'comments' in fpe:
            self.assertEqual(fpe.comments, spe.comments)
        if 'updated' in fpe and 'updated_parsed' in fpe and fpe.updated_parsed:
            # we don't care what date fields we used as long as they
            # ended up the same
            #self.assertEqual(fpe.updated, spe.updated)
            self.assertSameTime(fpe.updated_parsed, spe.updated_parsed)
        # lxml's cleaner can leave some stray block level elements around when
        # removing all containing code (like a summary which is just an object
        if 'summary' in fpe:
            if 'summary' not in spe:
                print "%s\n----\n%s\n" % (pformat(fpe), pformat(spe))
            if len(fpe.summary) < 5 and len(spe.summary.replace(' ', '')) < 20:
                pass
            else:
                self.assertPrettyClose(fpe.summary, spe.summary)
        if 'title' in fpe:
            self.assertPrettyClose(fpe.title, spe.title)
        if 'content' in fpe:
            self.assertPrettyClose(fpe.content[0]['value'], spe.content[0]['value'])
        if 'media_content' in fpe:
            self.assertEqual(len(fpe.media_content), len(spe.media_content))
            for fmc,smc in zip(fpe.media_content, spe.media_content):
                for key in fmc:
                    if key == 'isdefault':
                        self.assertEqual(fmc[key], smc['isDefault'])
                    elif key == 'filesize':
                        self.assertEqual(fmc[key], smc['fileSize'])
                    elif ':' in key:
                        # these have generally not been important and are
                        # usually namespaced keys..  if the key *also* exists
                        # in fmc with a ':' in it, or has a ':' in it to start,
                        # lets ignore it
                        continue
                    elif key not in smc:
                        matched = False
                        for k in smc:
                            if k.endswith(key) and ':' in k:
                                matched = True
                                break
                        if matched:
                            continue
                        message = "Non-namespaced key failure in media_content:\n"
                        message += "%s\n-----%s\n" % (pformat(dict(fmc)), pformat(dict(smc)))
                        message += "key %s not found in smc" % key
                        raise AssertionError(message)
                    else:
                        self.assertEqual(fmc[key].lower(), smc[key].lower())
        if 'media_thumbnail' in fpe:
            self.assertEqual(len(fpe.media_thumbnail), len(spe.media_thumbnail))
            for fmt,smt in zip(fpe.media_thumbnail, spe.media_thumbnail):
                for key in fmt:
                    self.assertEqual(fmt[key], smt[key])

class MultiTestEntries(TestCaseBase):
    def setUp(self):
        self.filenames = [
         'feeds/1114.dat',
         'feeds/2047.dat',
         'feeds/2072.dat',
         'feeds/2091.dat',
        ]

    def test_feeds(self):
        for path in self.filenames:
            with open(path) as f:
                doc = f.read()
            fpresult = feedparser.parse(doc)
            spresult = speedparser.parse(doc)
            try:
                feed_equivalence(self, fpresult, spresult)
                entry_equivalence(self, fpresult, spresult)
            except:
                import traceback
                print "Comp Failure: %s" % path
                traceback.print_exc()


class SingleTest(TestCaseBase):
    def setUp(self):
        filename = '0043.dat'
        with open('feeds/%s' % filename) as f:
            self.doc = f.read()

    def tearDown(self):
        self.doc = None

    def test_single_feed(self):
        fpresult = feedparser.parse(self.doc)
        spresult = speedparser.parse(self.doc)

        d = dict(fpresult)
        d['entries'] = d['entries'][:4]
        pprint(d)
        d = dict(spresult)
        d['entries'] = d['entries'][:4]
        pprint(d)
        feed_equivalence(self, fpresult, spresult)
        entry_equivalence(self, fpresult, spresult)

class EntriesCoverageTest(TestCaseBase):
    """A coverage test that does not check to see if the feed level items
    are the same;  it only tests that entries are similar."""
    def setUp(self):
        self.files = [f for f in glob('feeds/*.dat') if not f.startswith('.')]
        self.files.sort()

    def test_entries_coverage(self):
        success = 0
        fperrors = 0
        sperrors = 0
        errcompats = 0
        fperror = False
        total = len(self.files)
        total = 1000
        failedpaths = []
        failedentries = []
        bozoentries = []
        for f in self.files[:total]:
            fperror = False
            with open(f) as fo:
                document = fo.read()
            try:
                fpresult = load_cache(f)
                if fpresult is None:
                    fpresult = feedparser.parse(document)
            except:
                fperrors += 1
                fperror = True
            if fpresult.get('bozo', 0):
                fperrors += 1
                fperror = True
            try:
                spresult = speedparser.parse(document)
            except:
                if fperror:
                    errcompats += 1
                else:
                    sperrors += 1
                    bozoentries.append(f)
                continue
            if 'bozo_exception' in spresult:
                if fperror:
                    errcompats += 1
                else:
                    sperrors += 1
                    bozoentries.append(f)
                continue
            try:
                entry_equivalence(self, fpresult, spresult)
                success += 1
            except:
                import traceback
                print "Failure: %s" % f
                traceback.print_exc()
                failedentries.append(f)
        print "Success: %d out of %d (%0.2f %%, fpe: %d, spe: %d, both: %d)" % (success,
                total, (100 * success)/float(total-fperrors), fperrors, sperrors, errcompats)
        print "Failed entries:\n%s" % pformat(failedentries)
        print "Bozo entries:\n%s" % pformat(bozoentries)


class CoverageTest(TestCaseBase):
    def setUp(self):
        self.files = ['feeds/%s' % f for f in os.listdir('feeds/') if not f.startswith('.')]
        self.files.sort()

    def test_feed_coverage(self):
        success = 0
        fperrors = 0
        sperrors = 0
        total = 300
        failedpaths = []
        failedentries = []
        for f in self.files[:total]:
            fperror = False
            with open(f) as fo:
                document = fo.read()
            try:
                fpresult = feedparser.parse(document)
            except:
                fperrors += 1
                fperror = True
            try:
                spresult = speedparser.parse(document)
            except:
                sperrors += 1
                continue
            try:
                feed_equivalence(self, fpresult, spresult)
                success += 1
            except:
                failedpaths.append(f)
                pass
            try:
                entry_equivalence(self, fpresult, spresult)
            except:
                failedentries.append(f)
        print "Success: %d out of %d (%0.2f %%, fpe: %d, spe: %d)" % (success,
                total, (100 * success)/float(total-fperrors), fperrors, sperrors)
        print "Entry Success: %d out of %d (%0.2f %%)" % (success-len(failedentries),
                success, (100*(success-len(failedentries)))/float(total-fperrors))
        print "Failed Paths:\n%s" % pformat(failedpaths)
        print "Failed entries:\n%s" % pformat(failedentries)

class SpeedTest(TestCaseBase):
    def setUp(self):
        self.files = [f for f in glob('feeds/*.dat') if not f.startswith('.')]
        self.files.sort()

    def test_speed(self):
        total = len(self.files)
        total = 300
        def getspeed(parser, files):
            fullsize = 0
            t0 = time.time()
            for f in files:
                with open(f) as fo:
                    document = fo.read()
                    fullsize += len(document)
                try:
                    parser.parse(document)
                except:
                    pass
            td = time.time() - t0
            return td, fullsize
        #fpspeed = getspeed(feedparser, self.files[:total])
        spspeed, fullsize = getspeed(speedparser, self.files[:total])
        pct = lambda x: total/x
        print "speedparser: %0.2f/sec, %s/sec" % (pct(spspeed), sizeformat(fullsize/spspeed))
        #print "feedparser: %0.2f/sec,  speedparser: %0.2f/sec" % (pct(fpspeed), pct(spspeed))

class SpeedTestNoClean(TestCaseBase):
    def setUp(self):
        self.files = [f for f in glob('feeds/*.dat') if not f.startswith('.')]
        self.files.sort()

    def test_speed(self):
        total = len(self.files)
        def getspeed(parser, files, args=[]):
            fullsize = 0
            t0 = time.time()
            for f in files:
                with open(f) as fo:
                    document = fo.read()
                    fullsize += len(document)
                try:
                    parser.parse(document, *args)
                except:
                    pass
            td = time.time() - t0
            return td, fullsize
        #fpspeed = getspeed(feedparser, self.files[:total])
        spspeed, fullsize = getspeed(speedparser, self.files[:total], args=(False,))
        pct = lambda x: total/x
        print "speedparser (no html cleaning): %0.2f/sec, %s/sec" % (pct(spspeed), sizeformat(fullsize/spspeed))
        #print "feedparser: %0.2f/sec,  speedparser: %0.2f/sec (html cleaning disabled)" % (pct(fpspeed), pct(spspeed))


if __name__ == '__main__':
    build_feedparser_cache()


########NEW FILE########
