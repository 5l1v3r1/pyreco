__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# ramp documentation build configuration file, created by
# sphinx-quickstart on Tue Nov 13 12:58:10 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('../..'))

ON_RTD = os.environ.get('READTHEDOCS', None) == 'True'

# mock out dependencies
class Mock(object):
    def __init__(self, *args, **kwargs):
        pass

    def __call__(self, *args, **kwargs):
        return Mock()

    @classmethod
    def __getattr__(cls, name):
        if name in ('__file__', '__path__'):
            return '/dev/null'
        elif name[0] == name[0].upper():
            mockType = type(name, (), {})
            mockType.__module__ = __name__
            return mockType
        else:
            return Mock()

if ON_RTD:
    MOCK_MODULES = ['numpy', 'scipy', 'scipy.stats', 'pandas', 'sklearn']
    for mod_name in MOCK_MODULES:
        sys.modules[mod_name] = Mock()


# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'ramp'
copyright = u'2012, Ken Van Haren'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.1'
# The full version, including alpha/beta/rc tags.
release = '0.1'

#autodoc settigns
autoclass_content = 'both'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'sphinxdoc'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'rampdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'ramp.tex', u'ramp Documentation',
   u'Ken Van Haren', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'ramp', u'ramp Documentation',
     [u'Ken Van Haren'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'ramp', u'ramp Documentation',
   u'Ken Van Haren', 'ramp', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = classify_insults
import pandas
from ramp import *
from ramp.estimators.sk import BinaryProbabilities
import sklearn
from sklearn import naive_bayes
import gensim
import tempfile

try:
    training_data = pandas.read_csv('train.csv')
except IOError:
    raise IOError("You need to download the 'Detecting Insults' dataset \
                  from Kaggle to run this example. \
                  http://www.kaggle.com/c/detecting-insults-in-social-commentary")


tmpdir = tempfile.mkdtemp()
context = DataContext(
              store=tmpdir,
              data=training_data)


base_config = Configuration(
    target='Insult',
    metrics=[metrics.AUC()],
    )

base_features = [
    Length('Comment'),
    Log(Length('Comment') + 1)
]

factory = ConfigFactory(
    base_config,
    features=[
        # first feature set is basic attributes
        base_features,

        # second feature set adds word features
        base_features + [
            text.NgramCounts(
                text.Tokenizer('Comment'),
                mindocs=5,
                bool_=True)],

        # third feature set creates character 5-grams
        # and then selects the top 1000 most informative
        base_features + [
            trained.FeatureSelector(
                [text.NgramCounts(
                    text.CharGrams('Comment', chars=5),
                    bool_=True,
                    mindocs=30)
                ],
                selector=selectors.BinaryFeatureSelector(),
                n_keep=1000,
                target=F('Insult')),
            ],

        # the fourth feature set creates 100 latent vectors
        # from the character 5-grams
        base_features + [
            text.LSI(
                text.CharGrams('Comment', chars=5),
                mindocs=30,
                num_topics=100),
            ]
    ],

    # we'll try two estimators (and wrap them so
    # we get class probabilities as output):
    model=[
        BinaryProbabilities(
            sklearn.linear_model.LogisticRegression()),
        BinaryProbabilities(
            naive_bayes.GaussianNB())
    ]
)


for config in factory:
    models.cv(config, context, folds=5, repeat=2,
              print_results=True)


def probability_of_insult(config, ctx, txt):
    # create a unique index for this text
    idx = int(md5(txt).hexdigest()[:10], 16)

    # add the new comment to our DataFrame
    d = DataFrame(
            {'Comment':[txt]},
            index=pandas.Index([idx]))
    ctx.data = ctx.data.append(d)

    # Specify which instances to predict with predict_index
    # and make the prediction
    pred, predict_x, predict_y = models.predict(
            config,
            ctx,
            predict_index=pandas.Index([idx]))

    return pred[idx]


########NEW FILE########
__FILENAME__ = iris
import urllib2

import pandas as pd
import sklearn
from sklearn import decomposition

import ramp
from ramp.features import *
from ramp.metrics import PositiveRate, Recall

import logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

# fetch and clean iris data from UCI
data = pd.read_csv(urllib2.urlopen(
    "http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"))
data = data.drop([149]) # bad line
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
data.columns = columns

# all features
features = [FillMissing(f, 0) for f in columns[:-1]]

# features, log transformed features, and interaction terms
expanded_features = (
    features +
    [Log(F(f) + 1) for f in features] +
    [
        F('sepal_width') ** 2,
        combo.Interactions(features),
    ]
)

reporters = [
    ramp.reporters.MetricReporter.factory(Recall(.4)),
    ramp.reporters.DualThresholdMetricReporter.factory(Recall(), PositiveRate())
]


# Define several models and feature sets to explore,
# run 5 fold cross-validation on each and print the results.
# We define 2 models and 4 feature sets, so this will be
# 4 * 2 = 8 models tested.
outcomes = ramp.shortcuts.cv_factory(
    data=data,
    folds=10,

    target=[AsFactor('class')],

    reporter_factories=reporters,

    # Try out two algorithms
    estimator=[
        sklearn.ensemble.RandomForestClassifier(
            n_estimators=20),
        sklearn.linear_model.LogisticRegression(),
        ],

    # and 4 feature sets
    features=[
        expanded_features,

        # Feature selection
        # [trained.FeatureSelector(
        #     expanded_features,
        #     # use random forest's importance to trim
        #     ramp.selectors.BinaryFeatureSelector(),
        #     target=AsFactor('class'), # target to use
        #     data=data,
        #     n_keep=5, # keep top 5 features
        #     )],

        # Reduce feature dimension (pointless on this dataset)
        [combo.DimensionReduction(expanded_features,
                            decomposer=decomposition.PCA(n_components=4))],

        # Normalized features
        [Normalize(f) for f in expanded_features],
    ]
)

print outcomes.values()[0]['reporters'][0]

########NEW FILE########
__FILENAME__ = builders
import logging

from pandas import concat, DataFrame, Series, Index
import numpy as np

from ramp.features.base import BaseFeature, Feature, ConstantFeature
from ramp.utils import _pprint, get_single_column


def build_target_safe(target, data, prep_index=None, train_index=None):
    y, ff = target.build(data, prep_index, train_index)
    return get_single_column(y), ff


def apply_target_safe(target, data, fitted_feature):
    y = target.apply(data, fitted_feature)
    return get_single_column(y)


def build_feature_safe(feature, data, prep_index=None, train_index=None):
    d, ff = feature.build(data, prep_index, train_index)

    # sanity check index is valid
    assert d.index.equals(data.index), "%s: %s\n%s" %(feature, d.index, data.index)

    # columns probably shouldn't be constant...
    if not isinstance(feature, ConstantFeature):
        if any(d.std() < 1e-9):
            logging.warn("Feature '%s' has constant column." % feature.unique_name)
    return d, ff


def build_featureset_safe(features, data, prep_index=None, train_index=None):
    # check for dupes
    colnames = set([f.unique_name for f in features])
    assert len(features) == len(colnames), "Duplicate feature: %s" % colnames
    if not features:
        return
    logging.info("Building %d features... " % len(features))
    feature_datas = []
    fitted_features = []
    for feature in features:
        d, ff = build_feature_safe(feature, data, prep_index, train_index)
        feature_datas.append(d)
        fitted_features.append(ff)
    logging.info("Done building features")
    return concat(feature_datas, axis=1), fitted_features


def apply_feature_safe(feature, data, fitted_feature):
    d = feature.apply(data, fitted_feature)

    # sanity check index is valid
    assert d.index.equals(data.index)

    # columns probably shouldn't be constant...
    if not isinstance(feature, ConstantFeature):
        if any(d.std() < 1e-9):
            logging.warn("Feature '%s' has constant column." % feature.unique_name)
    return d


def apply_featureset_safe(features, data, fitted_features):
    assert len(features) == len(fitted_features)
    feature_datas = []
    logging.info("Applying %d features to %d data points... " % (len(features), len(data)))
    for f, ff in zip(features, fitted_features):
        feature_datas.append(apply_feature_safe(f, data, ff))
    logging.info("Done applying features")
    return concat(feature_datas, axis=1)



# build_featureset = build_featureset_safe
# build_target = build_target_safe
########NEW FILE########
__FILENAME__ = base
import types

__all__ = ['Wrapper', 'Estimator', 'FittedEstimator',
           'Probabilities', 'BinaryProbabilities', 'wrap_sklearn_like_estimator']


class Wrapper(object):
    def __init__(self, obj):
        self._obj = obj
    
    def __getattr__(self, attr):
        
        if hasattr(self._obj, attr):
            attr_value = getattr(self._obj,attr)
            
            if isinstance(attr_value, types.MethodType):
                def callable(*args, **kwargs):
                    return attr_value(*args, **kwargs)
                return callable
            else:
                return attr_value
            
        else:
            raise AttributeError

    def __getstate__(self): return self.__dict__
    def __setstate__(self, d): self.__dict__.update(d)


class Estimator(Wrapper):
    def __init__(self, estimator):
        self.estimator = estimator
        super(Estimator, self).__init__(estimator)

    def fit(self, x, y):
        return self.estimator.fit(x.values, y.values)
    
    def predict_maxprob(self, x):
        """
        Most likely value. Generally equivalent to predict.
        """
        return self.estimator.predict(x.values)
    
    def predict(self, x):
        """
        Model output. Not always the same as scikit_learn predict. E.g., in the
        case of logistic regression, returns the probability of each outome.
        """
        return self.estimator.predict(x.values)


class Probabilities(Estimator):
    """
    Wraps a scikit-learn-like estimator to return probabilities (if
    it supports it)
    """
    def __init__(self, estimator, binary=False):
        """
        binary: If True, predict returns only the probability
            for the positive class. If False, returns probabilities for
            all classes.
        """
        self.estimator = estimator
        self.binary = binary
        super(Probabilities, self).__init__(estimator)

    def __str__(self):
        return u"Probabilites for %s" % self.estimator

    def predict(self, x):
        probs = self.estimator.predict_proba(x)
        if probs.shape[1] == 2 or self.binary:
            return probs[:,1]
        return probs


class BinaryProbabilities(Probabilities):
    def __init__(self, estimator):
        super(BinaryProbabilities, self).__init__(estimator, binary=True)


class FittedEstimator(Wrapper):
    def __init__(self, fitted_estimator, x_train, y_train):
        # compute metadata
        self.fitted_estimator = fitted_estimator
        super(FittedEstimator, self).__init__(fitted_estimator)


def wrap_sklearn_like_estimator(estimator):
    if isinstance(estimator, Estimator):
        return estimator
    elif estimator is None:
        return None
    elif not (hasattr(estimator, "fit") and (hasattr(estimator, "predict")
                                          or hasattr(estimator, "predict_proba"))):
        raise ValueError, "Invalid estimator: %s" % estimator
    elif hasattr(estimator, "predict_proba"):
        return Probabilities(estimator)
    else:
        return Estimator(estimator)


########NEW FILE########
__FILENAME__ = r
import numpy as np
from rpy2.robjects import FloatVector
from rpy2.robjects.packages import importr
from rpy2 import robjects
stats = importr('stats')
base = importr('base')


def matrix_to_r_dataframe(x):
        rx = FloatVector(np.ravel(x))
        rx = robjects.r['matrix'](rx, nrow = len(x), byrow=True)
        return robjects.r["data.frame"](rx)


class REstimator(object):
    def __init__(self, r_estimator, **kwargs):
        self.estimator = r_estimator
        self.kwargs = kwargs

    def fit(self, x, y):
        rx = matrix_to_r_dataframe(x)
        ry = FloatVector(y)
        robjects.globalenv["y"] = ry
        self.estimator_fit = self.estimator("y ~ .",  data=rx,
                **self.kwargs)

    def predict(self, x):
        rx = matrix_to_r_dataframe(x)
        return np.array(stats.predict(self.estimator_fit, rx)[0])


class OrderedLogit(object):
    def fit(self, x, y):
        ordinal = importr('ordinal')
        rx = matrix_to_r_dataframe(x)
        self.levels = range(int(round(min(y))), int(round(max(y)))+1)
        ry = base.factor(FloatVector(y), levels=self.levels, ordered=True)
        robjects.globalenv["y"] = ry
        self.clmfit = ordinal.clm("y ~ .",  data=rx)
        #print base.summary(self.clmfit)

    def predict(self, x):
        rx = matrix_to_r_dataframe(x)
        rfac = stats.predict(self.clmfit, rx, type="class")[0]
        rvec = [self.levels[v - 1] for v in rfac]
        return rvec


class WeightedLM(object):
    def fit(self, x, y, weights):
        rx = matrix_to_r_dataframe(x)
        ry = FloatVector(y)
        rw = FloatVector(weights)
        robjects.globalenv["score"] = ry
        self.lmfit = stats.lm("score ~ .",  data=rx, weights=rw)
        #print base.summary(self.clmfit)

    def predict(self, x):
        rx = matrix_to_r_dataframe(x)
        rvec = stats.predict(self.lmfit, rx)[0]
        return np.array(rvec)


class GBM(REstimator):
    def __init__(self, **kwargs):
        gbm = importr('gbm')
        super(GBM, self).__init__(gbm.gbm, **kwargs)


########NEW FILE########
__FILENAME__ = base
  # -*- coding: utf-8 -*-
'''
Features: Base
-------

The Base Features module provides a set of abstract base classes and simple
feature definitions (such as Length, Log, Power, etc). The ABC's can be built
upon, such as in the text or combo modules. 

These Feature classes allow for chaining and combining feature sets to be 
iterated upon in the Configurator Factory. 

'''
from hashlib import md5
import math
import random
import re

import numpy as np
from pandas import Series, DataFrame, concat

from ramp.store import Storable
from ramp.utils import (_pprint, get_np_hashable, key_from_index,
                        get_single_column, stable_repr, reindex_safe)


available_features = []


class FittedFeature(Storable):

    def __init__(self, feature, train_index, prep_index, prepped_data=None,
                 trained_data=None, inner_fitted_features=None):
        self.feature = feature

        # compute metadata
        self.train_n = len(train_index)
        self.prep_n = len(prep_index)
        self.train_data_key = key_from_index(train_index)
        self.prep_data_key = key_from_index(prep_index)

        self.inner_fitted_features = inner_fitted_features

        self.prepped_data = prepped_data
        self.trained_data = trained_data


class BaseFeature(object):
    """
    BaseFeature wraps a string corresponding to a
    DataFrame column.
    """

    _cacheable = True

    def __init__(self, feature):
        if not isinstance(feature, basestring):
            raise ValueError('Base feature must be a string')
        self.feature = feature

    def __repr__(self):
        return repr(self.feature)

    def __str__(self):
        return str(self.feature)

    @property
    def unique_name(self):
        return str(self)

    def depends_on_y(self):
        return False

    def depends_on_other_x(self):
        return False

    @property
    def is_trained(self):
        return self.depends_on_y()

    @property
    def is_prepped(self):
        return self.depends_on_other_x()

    def __call__(self, *args, **kwargs):
        return self.apply(*args, **kwargs)

    def build(self, data, prep_index=None, train_index=None):
        feature_data = self.apply(data)
        return feature_data, None

    def apply(self, data, fitted_feature=None):
        return DataFrame(data[self.feature],
                         columns=[self.feature])

    def prepare(self, prep_data):
        raise NotImplementedError

    def train(self, train_data):
        raise NotImplementedError

    def __add__(self, other):
        return combo.Add([self, other])

    def __sub__(self, other):
        return combo.Sub([self, other])

    def __div__(self, other):
        return combo.Divide([self, other])

    def __mul__(self, other):
        return combo.Multiply([self, other])

    def __pow__(self, power):
        return Power(self, power)


class ConstantFeature(BaseFeature):

    def __init__(self, feature):
        if not isinstance(feature, int) and not isinstance(feature, float):
            raise ValueError('Constant feature must be a number')
        self.feature = feature

    def apply(self, data, fitted_feature=None):
        return DataFrame(np.repeat(self.feature, len(data)),
                         index=data.index,
                         columns=['%s' % self.feature])


class DummyFeature(BaseFeature):
    """ For testing """

    def __init__(self):
        self.feature = ''

    def apply(self, data, fitted_feature=None):
        return data
AllDataFeature = DummyFeature


class FeatureMetaClass(type):
    def __new__(meta, name, bases, dct):
        available_features.append(name)
        return super(FeatureMetaClass, meta).__new__(meta, name, bases, dct)

    def __call__(cls, *args, **kwargs):
        for arg in list(args) + kwargs.values():
            if hasattr(arg, '__call__') and getattr(arg, '__name__', 0) == (lambda: None).__name__:
                Warning("Feature's should not be passed anonymous (lambda) functions"
                        " as these cannot be serialized reliably")
        return type.__call__(cls, *args, **kwargs)


class ComboFeature(BaseFeature):
    """
    Abstract base for more complex features
    """

    __metaclass__ = FeatureMetaClass

    hash_length = 8
    _cacheable = True
    re_hsh = re.compile(r' \[\w{%d}\]' % hash_length)

    def __init__(self, features):
        """
        Inheriting classes responsible for setting human-readable description of
        feature and parameters on _name attribute.
        """
        self.features = []
        # handle single feature as well
        if not isinstance(features, list) and not isinstance(features, tuple):
            features = [features]
        for feature in features:
            if isinstance(feature, basestring):
                feature = BaseFeature(feature)
            if isinstance(feature, int) or isinstance(feature, float):
                feature = ConstantFeature(feature)
            self.features.append(feature)
        self.set_name()

    def set_name(self):
        cname = self.__class__.__name__
        if cname.endswith('Feature'):
            cname = cname[:-7]
        # _name attribute is for human-readable strings
        self._name = cname

    def __getstate__(self):
        # shallow copy dict and keep references
        dct = self.__dict__.copy()
        return dct

    def __repr__(self):
        return stable_repr(self)

    def _hash(self):
        s = repr(self)
        return md5(s).hexdigest()[:self.hash_length]

    @property
    def unique_name(self):
        """
        Must provide a unique string as a funtion of this feature, its
        parameter settings, and all it's contained features. It should also be
        readable and maintain a reasonable length (by hashing, for instance).
        """
        h = self._hash()
        return '%s [%s]' %(self, h)

    def __str__(self):
        """
        A readable version of this feature (and its contained features).
        Should be as short as possible.
        """
        f = ', '.join([str(f) for f in self.features])
        if self._name:
            return '%s(%s)' %(self._name, f)
        # else this is just a Feature wrapper, no need to add anything
        return f

    def _remove_hashes(self, s):
        return self.re_hsh.sub('', s)

    def column_rename(self, existing_name, hsh=None):
        """
        Like unique_name, but in addition must be unique to each column of this
        feature. accomplishes this by prepending readable string to existing
        column name and replacing unique hash at end of column name.
        """
        try:
            existing_name = str(existing_name)
        except UnicodeEncodeError:
            pass
        if hsh is None:
            hsh = self._hash()
        if self._name:
            return '%s(%s) [%s]' %(self._name, self._remove_hashes(existing_name),
                    hsh)
        return '%s [%s]'%(self._remove_hashes(existing_name),
                    hsh)

    def depends_on_y(self):
        if hasattr(self, '_train'):
            return True
        return any([f.depends_on_y() for f in self.features])

    def depends_on_other_x(self):
        if hasattr(self, '_prepare'):
            return True
        return any([f.depends_on_other_x() for f in self.features])

    def build(self, data, prep_index=None, train_index=None):
        if prep_index is None:
            prep_index = data.index
        if train_index is None:
            train_index = data.index
        datas = []
        fitted_features = []
        for feature in self.features:
            feature_data, ff = feature.build(data, prep_index, train_index)
            datas.append(feature_data)
            fitted_features.append(ff)
        ff = FittedFeature(self,
                           prep_index=prep_index,
                           train_index=train_index,
                           inner_fitted_features=fitted_features)
        ff.prepped_data = self.prepare([reindex_safe(d, prep_index) for d in datas])
        ff.trained_data = self.train([reindex_safe(d, train_index) for d in datas])
        feature_data = self._combine_apply(datas, ff)
        feature_data = self._prepend_feature_name_to_all_columns(feature_data)
        return feature_data, ff

    def _prepend_feature_name_to_all_columns(self, data):
        hsh = self._hash() # cache this so we dont recompute for every column
        data.columns = [self.column_rename(c, hsh) for c in data.columns]
        return data

    def apply(self, data, fitted_feature):
        datas = []
        for feature, inner_fitted_feature in zip(self.features, fitted_feature.inner_fitted_features):
            datas.append(feature.apply(data, inner_fitted_feature))
        feature_data = self._combine_apply(datas, fitted_feature)
        if not isinstance(feature_data, DataFrame):
            raise TypeError("_combine_apply() method must return a DataFrame")
        return self._prepend_feature_name_to_all_columns(feature_data)

    def _apply(self, data, fitted_feature):
        raise NotImplementedError

    def _combine_apply(self, datas, fitted_feature):
        raise NotImplementedError

    def prepare(self, prep_datas):
        if hasattr(self, '_prepare'):
            prepped_data = self._prepare(prep_datas)
            return prepped_data
        else:
            return None

    def train(self, train_datas):
        if hasattr(self, '_train'):
            trained_data = self._train(train_datas)
            return trained_data
        else:
            return None


class Feature(ComboFeature):
    """
    Base class for features operating on a single feature.
    """
    def __init__(self, feature):
        super(Feature, self).__init__([feature])
        self.feature = self.features[0]

    # def apply(self, data, fitted_feature):
    #     # recurse:
    #     data = self.feature.apply(data, fitted_feature.inner_fitted_feature)
    #     # apply this feature's transformation:
    #     feature_data = self._apply(data, fitted_feature)
    #     if not isinstance(feature_data, DataFrame):
    #         raise TypeError("_apply() method must return a DataFrame")
    #     return self._prepend_feature_name_to_all_columns(feature_data)

    def _apply(self, data, fitted_feature):
        return data

    def _combine_apply(self, datas, fitted_feature):
        return self._apply(datas[0], fitted_feature)

    def prepare(self, prep_datas):
        if hasattr(self, '_prepare'):
            prepped_data = self._prepare(prep_datas[0])
            return prepped_data
        else:
            return None

    def train(self, train_datas):
        if hasattr(self, '_train'):
            trained_data = self._train(train_datas[0])
            return trained_data
        else:
            return None
# shortcut
F = Feature


class MissingIndicator(Feature):
    """
    Adds a missing indicator column for this feature.
    Indicator will be 1 if given feature `isnan` (numpy definition), 0 otherwise.
    """
    def _apply(self, data, fitted_feature):
        for col in data.columns:
            missing = data[col].map(lambda x: int(x.isnan()))
            missing.name = 'missing_%s'%col
            data.append(missing)
        return data


class FillMissing(Feature):
    """
    Fills `na` values (pandas definition) with `fill_value`.
    """
    def __init__(self, feature, fill_value):
        self.fill_value = fill_value
        super(FillMissing, self).__init__(feature)

    def _apply(self, data, fitted_feature):
        return data.fillna(self.fill_value)


class MissingIndicatorAndFill(Feature):
    """
    Adds a missing indicator column for this feature.
    Indicator will be 1 if given feature `isnan` (numpy definition), 0 otherwise,
    and then fill NaNs with `fill_value`.
    """
    def __init__(self, feature, fill_value):
        self.fill_value = fill_value
        super(MissingIndicatorAndFill, self).__init__(feature)

    def _apply(self, data, fitted_feature):
        cols = []
        names = []
        for col in data.columns:
            missing = data[col].map(lambda x: int(np.isnan(x)))
            names.append( 'missing_%s'%col)
            cols.append(missing)
        data = concat([data, concat(cols, keys=names, axis=1)], axis=1)
        return data.fillna(self.fill_value)


# class DropConstant(ComboFeature):
#     def _prepare(self, data):
#         dropped_cols = []
#         for col in data.columns:
#             if data[col].std() < 1e-9:
#                 dropped_cols.append(col)
#         print "Dropped %d columns", (len(dropped_cols), dropped_cols)
#         return {"dropped_columns": dropped_cols}

#     def combine(self, datas):
#         data = concat(datas, axis=1)        
#         cols = fitted_feature.prepped_data['dropped_columns']
#         return data.drop(cols, axis=1)


class Length(Feature):
    """
    Applies builtin `len` to feature.
    """
    def _apply(self, data, fitted_feature):
        return data.applymap(lambda x: len(x))


class Normalize(Feature):
    """
    Normalizes feature to mean zero, stdev one.
    """
    def _prepare(self, data):
        cols = {}
        for col in data.columns:
            d = data[col]
            m = d.mean()
            s = d.std()
            cols[col] = (m, s)
        return cols

    def _apply(self, data, fitted_feature):
        eps = 1.0e-10
        col_stats = fitted_feature.prepped_data
        d = DataFrame(index=data.index)
        for col in data.columns:
            m, s = col_stats.get(col, (0, 0))
            if s < eps:
                d[col] = data[col] - m
            else:
                d[col] = (data[col] - m) / s
        return d


class Discretize(Feature):
    """
    Bins values based on given cutoffs.
    """
    def __init__(self, feature, cutoffs, values=None):
        super(Discretize, self).__init__(feature)
        self.cutoffs = cutoffs
        if values is None:
            values = range(len(cutoffs) + 1)
        self.values = values

    def discretize(self, x):
        for i, c in enumerate(self.cutoffs):
            if x < c:
                return self.values[i]
        return self.values[-1]

    def _apply(self, data, fitted_feature):
        return data.applymap(self.discretize)


class Map(Feature):
    """
    Applies given function to feature. Feature *cannot*
    be anonymous (ie lambda). Must be defined in top level
    (and thus picklable).
    """
    def __init__(self, feature, function, name=None):
        super(Map, self).__init__(feature)
        self.function = function
        if name is None:
            name = function.__name__
        self.name = name
        self._name = name

    def _apply(self, data, fitted_feature):
        return data.applymap(self.function)


class AsFactor(Feature):
    """
    Maps nominal values to ints and stores
    mapping. Mapping may be provided at definition.
    """
    def __init__(self, feature, levels=None):
        """ levels is list of tuples """
        super(AsFactor, self).__init__(feature)
        self.levels = levels

    def _prepare(self, data):
        levels = self.levels
        if not levels:
            levels = set(get_single_column(data))
            levels = zip(levels, range(len(levels)))
        return levels

    def _apply(self, data, fitted_feature):
        levels = fitted_feature.prepped_data
        mapping = dict(levels)
        return data.applymap(mapping.get)

    def get_name(self, factor):
        factors = self.get_prep_data()
        inverse = dict([(v,k) for k,v in factors])
        return inverse.get(factor)


class AsFactorIndicators(Feature):
    """
    Maps nominal values to indicator columns. So
    a column with values ['good', 'fair', 'poor'],
    would be mapped to three indicator columns
    if include_all is True otherwise two columns (the
    third implied by zeros on the other two columns)
    """
    def __init__(self, feature, levels=None, include_all=True):
        super(AsFactorIndicators, self).__init__(feature)
        self.levels = levels
        self.include_all = include_all

    def _prepare(self, data):
        levels = self.levels
        if not levels:
            levels = sorted(set(get_single_column(data)))
        return levels

    def _apply(self, data, fitted_feature):
        factors = fitted_feature.prepped_data
        data = get_single_column(data)
        d = DataFrame(index=data.index)
        if self.include_all:
            facts = list(factors)
        else:
            facts = list(factors)[:-1]
        for f in facts:
            d['%s-%s'%(f, data.name)] = data.map(lambda x: int(x == f))
        return d


class IndicatorEquals(Feature):
    """
    Maps feature to one if equals given value, zero otherwise.
    """
    def __init__(self, feature, value):
        super(IndicatorEquals, self).__init__(feature)
        self.value = value
        self._name = self._name + '_%s'%value

    def _apply(self, data, fitted_feature):
        return data.applymap(lambda x: int(x==self.value))


class Log(Map):
    """
    Takes log of given feature. User is responsible for
    ensuring values are in domain.
    """
    def __init__(self, feature):
        super(Log, self).__init__(feature, math.log)


class Power(Feature):
    """
    Takes feature to given power. Equivalent to operator: F('a') ** power.
    """
    def __init__(self, feature, power=2):
        self.power = power
        super(Power, self).__init__(feature)

    def _apply(self, data, fitted_feature):
        return data.applymap(lambda x: x ** self.power)


class GroupMap(Feature):
    """
    Applies a function over specific sub-groups of the data
    Typically this will be with a MultiIndex (hierarchical index).
    If group is encountered that has not been seen, defaults to
    global map.
    TODO: prep this feature
    """

    def __init__(self, feature, function, name=None, **groupargs):
        super(GroupMap, self).__init__(feature)
        self.function = function
        if name is None:
            name = function.__name__
        self.name = name
        self._name = name
        self.groupargs = groupargs

    def _apply(self, data, fitted_feature):
        d = data.groupby(**self.groupargs).applymap(self.function)
        return d


class GroupAggregate(ComboFeature):
    """
    Computes an aggregate value by group.

    Groups can be specified with kw args which will be
    passed to the pandas `groupby` method, or by
    specifying a `groupby_column` which will group by value
    of that column.
    """
    def __init__(self, features, function, name=None, data_column=None,
            trained=False, groupby_column=None, **groupargs):
        super(GroupAggregate, self).__init__(features)
        self.function = function
        self.data_column = data_column
        self.trained = trained
        self.groupby_column = groupby_column
        self._name = name or function.__name__
        self.groupargs = groupargs

    def depends_on_y(self):
        return self.trained or super(GroupAggregate, self).depends_on_y()

    def _prepare(self, data):
        prep = {}
        # global value
        prep['global'] = self.function(data[self.data_column])
        # group specific
        if self.groupby_column:
            g = data.groupby(by=self.groupby_column, **self.groupargs)
        else:
            g = data.groupby(**self.groupargs)
        d = g[self.data_column].apply(self.function)
        prep['groups'] = d
        return prep

    def _create(self, datas):
        data = concat(datas, axis=1)
        prep = fitted_feature.prepped_data
        globl = prep['global']
        groups = prep['groups']
        if self.groupby_column:
            data = DataFrame(data[self.groupby_column].apply(lambda x: groups.get(x, globl)))
        else:
            data = DataFrame([groups.get(x, globl) for x in data.index],
                    columns=[self.data_column],
                    index=data.index)
        return data


def contain(x, mn, mx):
    if mx is not None and x > mx: return mx
    if mn is not None and x < mn: return mn
    return x

class Contain(Feature):
    """
    Trims values to inside min and max.
    """
    def __init__(self, feature, min=None, max=None):
        self.min = min
        self.max = max
        super(Contain, self).__init__(feature)
        self._name = self._name + '(%s,%s)' %(min, max)

    def _apply(self, data, fitted_feature):
        return data.applymap(lambda x: contain(x, self.min, self.max))


class ReplaceOutliers(Feature):
    # TODO: add prep
    def __init__(self, feature, stdevs=7, replace='mean'):
        super(ReplaceOutliers, self).__init__(feature)
        self.stdevs = stdevs
        self.replace = replace
        self._name = self._name + '_%d'%stdevs

    def is_outlier(self, x, mean, std):
        return abs((x-mean)/std) > self.stdevs

    def _apply(self, data, fitted_feature):
        replace = self.replace
        cols = []
        for col in data.columns:
            d = data[col]
            m = d.mean()
            s = d.std()
            if self.replace == 'mean':
                replace = m
            cols.append(d.map(lambda x: x if not self.is_outlier(x, m, s) else
                    replace)
                    )
        return concat(cols, keys=data.columns, axis=1)


class ColumnSubset(Feature):
    def __init__(self, feature, subset, match_substr=False):
        super(ColumnSubset, self).__init__(feature)
        self.subset = subset
        self.match_substr = match_substr

    def _apply(self, data, fitted_feature):
        if self.match_substr:
            cols = [c for c in data.columns if any([s in c for s in self.subset])]
        else:
            cols = self.subset
        return data[cols]


class Lag(Feature):
    """
    Lags given feature by n along index.
    """
    def __init__(self, feature, lag=1, fill=0):
        self.lag = lag
        self.fill = fill
        super(Lag, self).__init__(feature)
        self._name = self._name + '_%d'%self.lag

    def _apply(self, data, fitted_feature):
        cols = []
        for col in data.columns:
            cols.append(Series([self.fill] * self.lag + list(data[col][:-self.lag]),
                            index=data.index))
        return concat(cols, keys=data.columns, axis=1)


def to_feature(feature_like):
    return feature_like if isinstance(feature_like, BaseFeature) else Feature(feature_like)


import combo


########NEW FILE########
__FILENAME__ = combo
from base import ComboFeature, Feature
from pandas import DataFrame, Series, concat


class ComboMap(ComboFeature):
    """ abstract base for binary operations on features """
    def __init__(self, features, name=None, fillna=0):
        super(ComboMap, self).__init__(features)
        self.fillna = fillna

    def _combine(self, datas, function):
        data = datas[0]
        data = data.astype('float64')
        for d in datas[1:]:
            if isinstance(d, DataFrame):
                if len(d.columns) == 1:
                    d = d[d.columns[0]]
                else:
                    raise NotImplementedError
            cols = ['%s, %s' % (c, d.name) for c in data.columns]
            data = function(data, d.astype('float64'), axis=0)
            data = data.fillna(self.fillna)
            data.columns = cols
        return data


class Add(ComboMap):
    def _combine_apply(self, datas, fitted_feature):
        return self._combine(datas, DataFrame.add)
class Divide(ComboMap):
    def _combine_apply(self, datas, fitted_feature):
        return self._combine(datas, DataFrame.div)
class Multiply(ComboMap):
    def _combine_apply(self, datas, fitted_feature):
        return self._combine(datas, DataFrame.mul)
class Sub(ComboMap):
    def _combine_apply(self, datas, fitted_feature):
        return self._combine(datas, DataFrame.sub)


class Interactions(ComboFeature):
    def _combine_apply(self, datas, fitted_feature):
        cols = []
        colnames = []
        data = concat(datas, axis=1)
        n = len(data.columns)
        for i in range(n):
            d1 = data[data.columns[i]]
            for j in range(i+1, n):
                d2 = data[data.columns[j]]
                d = d1.mul( d2)
                colnames.append('%s, %s' % (d1.name, d2.name))
                cols.append(d)
        return concat(cols, keys=colnames, axis=1)


# class OutlierCount(ComboFeature):
#     # TODO: add prep
#     def __init__(self, features, stdevs=5):
#         super(OutlierCount, self).__init__(features)
#         self.stdevs = stdevs
#         self._name = self._name + '_%d'%stdevs

#     def is_outlier(self, x, mean, std):
#         return int(abs((x-mean)/std) > self.stdevs)

#     def _combine_apply(self, datas, fitted_feature):
#         count = DataFrame(np.zeros(len(datas[0])), index=datas[0].index)
#         eps = 1.0e-8
#         col_names = []
#         for data in datas:
#             for col in data.columns:
#                 d = data[col]
#                 m = d.mean()
#                 s = d.std()
#                 if s < eps:
#                     continue
#                 d = d.map(lambda x: self.is_outlier(x, m, s))
#                 col_names.append(col)
#                 count = count.add(d, axis=0)
#         count.columns = [','.join(col_names)]
#         return count


class DimensionReduction(ComboFeature):
    def __init__(self, feature, decomposer=None):
        super(DimensionReduction, self).__init__(feature)
        self.decomposer = decomposer

    def _prepare(self, prep_datas):
        prep_data = concat(prep_datas, axis=1)
        self.decomposer.fit(prep_data.values)
        return self.decomposer

    def _combine_apply(self, datas, fitted_feature):
        data = concat(datas, axis=1)
        decomposer = fitted_feature.prepped_data
        decomp = decomposer.transform(data)
        df = DataFrame(decomp,
                    columns=['Vector%d'%i for i in range(decomp.shape[1])],
                    index=data.index)
        return df



########NEW FILE########
__FILENAME__ = text
import logging
import collections
import hashlib
import re

import numpy as np
from pandas import DataFrame, read_csv, concat, Series
try:
    import gensim
except ImportError:
    logging.exception("""This requires the gensim module:
           http://radimrehurek.com/gensim/index.html""")
from ramp.utils import bag_of_words, cosine, tokenize, tokenize_keep_all, tokenize_with_sentinels
try:
    import nltk
    from nltk.corpus import wordnet
    #wordlist = set(nltk.corpus.words.words('en'))
except ImportError:
    pass

from ramp.features.base import Feature, ComboFeature, get_single_column

debug = False


class SentenceTokenizer(object):
    re_sent = re.compile(r'[.!?]\s+')

    def tokenize(self, s):
        return self.re_sent.split(s)

try:
    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
except:
    sent_tokenizer = SentenceTokenizer()


def make_docs_hash(docs):
    m = hashlib.md5()
    for doc in docs:
        for tok in doc:
            try:
                m.update(tok)
            except UnicodeEncodeError:
                pass
    return m.hexdigest()


class Dictionary(object):
    def __init__(self, mindocs=3, maxterms=100000, maxdocs=.9):
        self.mindocs = mindocs
        self.maxterms = maxterms
        self.maxdocs = maxdocs
        self.dictionary = gensim.corpora.Dictionary

    def name(self, docs, type_='dict'):
        return '%s_%s_%s' % (make_docs_hash(docs), type_,
                '%d,%d,%f'%(self.mindocs, self.maxterms, self.maxdocs))

    def get_dict(self, docs):
        return self._make_dict(docs)

    def _make_dict(self, docs):
        dct = self.dictionary(docs)
        dct.filter_extremes(no_below=self.mindocs, no_above=self.maxdocs,
                keep_n=self.maxterms)
        return dct

    def get_tfidf(self, docs):
        return self._make_tfidf(docs)

    def _make_tfidf(self, docs):
        dct = self.get_dict(docs)
        # corpus = [dct.doc2bow(d) for d in docs]
        tfidf = gensim.models.TfidfModel(dictionary=dct)
        return tfidf


class TopicModelFeature(Feature):

    def __init__(self, feature, topic_modeler=None, num_topics=50, force=False,
            stored_model=None, mindocs=3, maxterms=100000, maxdocs=.9,
            tokenizer=tokenize):
        self.mindocs = mindocs
        self.maxterms = maxterms
        self.maxdocs = maxdocs
        super(TopicModelFeature, self).__init__(feature)
        self.num_topics = num_topics
        self.dictionary = Dictionary(mindocs=mindocs, maxterms=maxterms,
                maxdocs=maxdocs)
        self.topic_modeler = topic_modeler
        self.stored_model = stored_model
        self.force = force
        self._name = '%s_%d' %(self._name, num_topics)

    def make_docs(self, data):
        return data.iloc[:,0].values

    def _prepare(self, prep_data):
        docs = self.make_docs(prep_data)
        dct, tfidf, lsi = self.make_engine(docs)
        return dct, tfidf, lsi

    def _apply(self, data, fitted_feature):
        data = data.iloc[:,0] # to series
        dct, tfidf, lsi = fitted_feature.prepped_data
        vecs = self.make_vectors(data, dct, tfidf, lsi)
        vecs.columns = ['%s_%s'%(c, data.name) for c in vecs.columns]
        return vecs

    def make_engine(self, docs):
        logging.info("Building topic model")
        dct = self.dictionary.get_dict(docs)
        corpus = [dct.doc2bow(d) for d in docs]
        tfidf = self.dictionary.get_tfidf(docs)
        topic_model = self.topic_modeler(corpus=tfidf[corpus], id2word=dct,
                num_topics=self.num_topics)
        logging.debug(topic_model)
        return dct, tfidf, topic_model

    def make_vectors(self, data, dct, tfidf, lsi, n=None):
        vecs = []
        logging.info("Making topic vectors")
        for i, txt in enumerate(data):
            topic_vec = dict(
                    lsi[tfidf[dct.doc2bow(
                        txt)]]
                    )
            if len(topic_vec) != self.num_topics:
                missing = set(range(self.num_topics)) - set(topic_vec.keys())
                for k in missing:
                    topic_vec[k] = 0
            vecs.append(topic_vec)
        tvecs = DataFrame(vecs, index=data.index)
        return tvecs


class LDA(TopicModelFeature):
    def __init__(self, *args, **kwargs):
        kwargs['topic_modeler'] = gensim.models.ldamodel.LdaModel
        super(LDA, self).__init__(*args, **kwargs)


class LSI(TopicModelFeature):
    def __init__(self, *args, **kwargs):
        kwargs['topic_modeler'] = gensim.models.lsimodel.LsiModel
        super(LSI, self).__init__(*args, **kwargs)


class SentenceLSI(TopicModelFeature):
    def __init__(self, *args, **kwargs):
        kwargs['topic_modeler'] = gensim.models.lsimodel.LsiModel
        super(SentenceLSI, self).__init__(*args, **kwargs)

    def make_docs(self, data):
        # TODO: this doesnt exist anymore
        sents = []
        for txt in data:
            sents.extend(sent_tokenizer.tokenize(txt))
        docs = [self.tokenizer(d) for d in sents]
        logging.info("docs:\n" + str(docs[0][:20]))
        self._docs_hash = self.make_docs_hash(docs)
        return docs


class TFIDF(Feature):
    #TODO: prep-ify
    def __init__(self, feature, mindocs=50, maxterms=10000, maxdocs=1.):
        super(TFIDF, self).__init__(feature)
        # self.mindocs = mindocs
        # self.maxterms = maxterms
        # self.maxdocs = maxdocs
        # self.data_dir = get_setting('DATA_DIR')
        # self.force = force
        # self.tokenizer = tokenize
        self.dictionary = Dictionary(mindocs, maxterms, maxdocs)
        # self._hash = self._make_hash(
        #         mindocs,
        #         maxterms,
        #         maxdocs)

    def _apply(self, data, fitted_feature):
        docs = list(data)
        dct = self.dictionary.get_dict(self.context, docs)
        tfidf = self.dictionary.get_tfidf(self.context, docs)
        docs = [dct.doc2bow(d) for d in docs]
        vecs = tfidf[docs]
        df = DataFrame([dict(row) for row in vecs], index=data.index)
        df.columns = ['%s_%s' % (dct[i], data.name) for i in df.columns]
        df = df.fillna(0)
        logging.debug(df)
        return df


class NgramCounts(Feature):
    def __init__(self, feature, mindocs=50, maxterms=100000, maxdocs=1.,
            bool_=False):
        super(NgramCounts, self).__init__(feature)
        self._name = self._name + '_%d,%d,%f'%(mindocs, maxterms, maxdocs)
        self.dictionary = Dictionary(mindocs, maxterms, maxdocs)
        self.bool_ = bool_

    def _prepare(self, prep_data):
        prep_data = get_single_column(prep_data)
        docs = list(prep_data)
        logging.debug(docs[:10])
        dct = self.dictionary.get_dict(docs)
        logging.debug(dct)
        return dct

    def _apply(self, data, fitted_feature):
        dct = fitted_feature.prepped_data
        data = get_single_column(data)
        docs = [dct.doc2bow(d) for d in data]
        ids = sorted(dct.keys())
        df = DataFrame([dict(row) for row in docs],
                columns=ids, index=data.index)
        df.columns = ['%s_%s' % (dct[i], data.name) for i in ids]
        df = df.fillna(0)
        if self.bool_:
            df = df.astype(bool).astype(int)
        return df


class SelectNgramCounts(NgramCounts):
    # TODO: make this a generic pre-selector (so intermediate isn't cached). Is
    # this even possible?
    def __init__(self, feature, selector, target, n_keep=50, train_only=False, *args, **kwargs):
        # this needs work...
        raise NotImplementedError
        super(SelectNgramCounts, self).__init__(feature, *args, **kwargs)
        self.selector = selector
        self.n_keep = n_keep
        self.target = target
        self.train_only = train_only
        # TODO: is cacheability chained through features properly?
        self._cacheable = not train_only 
        self._name = self._name + '_%d_%s'%(n_keep, selector.__class__.__name__)

    def depends_on_y(self):
        return self.train_only

    def _prepare(self, data):
        dct = super(SelectNgramCounts, self)._prepare(data)
        if self.train_only:
            y = get_single_column(self.target.create(self.context)).reindex(self.context.train_index)
            x = data.reindex(self.context.train_index)
        else:
            y = get_single_column(self.target.create(self.context))
            x = data
        cols = self.select(x, y)
        return cols, dct

    def select(self, x, y):
        return self.selector.sets(x, y, self.n_keep)

    def _apply(self, data, fitted_feature):
        # TODO: not sure how to prep this... need to recreate 
        # inherited feature to get at its prepped data
        data = super(SelectNgramCounts, self)._create(data)
        cols = self.get_prep_data(data)
        return data[cols]


try:
    treebank_tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()
except NameError:
    treebank_tokenizer = None

class TreebankTokenize(Feature):

    tokenizer = treebank_tokenizer

    def _apply(self, data, fitted_feature):
        if not self.tokenizer:
            raise NameError("TreebankTokenize requires nltk to be installed")
        return data.applymap(self.tokenizer.tokenize)


def ngrams(toks, n, sep='|'):
    return [sep.join(toks[i:i + n]) for i in range(len(toks) - n + 1)]

class Ngrams(Feature):
    def __init__(self, feature, ngrams=1):
        self.ngrams = ngrams
        super(Ngrams, self).__init__(feature)
        self._name += '_%d'%ngrams

    def _apply(self, data, fitted_feature):
        return data.applymap(lambda x: ngrams(x, self.ngrams))


def chargrams(s, n):
    st = '^'
    end = '$'
    # pad with start and end symbols
    s = st * (n - 1) + s + end * (n - 1)
    return [s[i:i+n] for i in range(len(s) - n + 1)]

class CharGrams(Feature):
    def __init__(self, feature, chars=4):
        self.chars = chars
        super(CharGrams, self).__init__(feature)
        self._name += '_%d' % chars

    def _apply(self, data, fitted_feature):
        return data.applymap(lambda x: set(chargrams(x, self.chars)))


class Tokenizer(Feature):
    def __init__(self, feature, tokenizer=tokenize_keep_all):
        super(Tokenizer, self).__init__(feature)
        self.tokenizer = tokenizer

    def _apply(self, data, fitted_feature):
        return data.applymap(self.tokenizer)


class StringJoin(ComboFeature):
    def __init__(self, features, sep="|"):
        super(StringJoin, self).__init__(features)
        self.sep = sep

    def _combine_apply(self, datas, fitted_feature):
        datas = [get_single_column(d) for d in datas]
        d = []
        for x in zip(*datas):
            d.append(self.sep.join(x))
        return DataFrame(d, columns=['_'.join([c.name for c in datas])])



# import syllables
# class SyllableCount(Feature):
#     def _apply(self, data, fitted_feature):
#         return data.map(lambda txt: sum([syllables.count(w) for w in txt.split()]))

def jaccard(a, b):
    a = set(a)
    b = set(b)
    return len(a & b) / float(len(a | b))

class ClosestDoc(Feature):
    def __init__(self, feature, text, doc_splitter=sent_tokenizer.tokenize,
            tokenizer=tokenize, sim=jaccard):
        super(ClosestDoc, self).__init__(feature)
        self.text = text
        self.tokenizer = tokenizer
        self.doc_splitter=doc_splitter
        self.sim = sim

    def make_docs(self, data):
        docs = []
        for d in data:
            docs.append(self.doc_splitter(d))
        return data.map(
                lambda x: [self.tokenizer(d) for d in self.doc_splitter(x)])

    def score(self, data):
        scores = []
        txt = self.tokenizer(self.text)
        for d in self.make_docs(data):
            score = max([self.sim(txt, doc) for doc in d])
            scores.append(score)
        return scores

    def _apply(self, data, fitted_feature):
        return Series(self.score(data), index=data.index, name=data.name)


chkr = None
try:
    from aspell import Speller
    chkr = Speller()
except ImportError:
    pass

def count_spell_errors(toks, exemptions):
    if not toks:
        return 0
    return sum([not chkr.check(t) for t in toks if t not in exemptions])

class SpellingErrorCount(Feature):
    def __init__(self, feature, exemptions=None):
        if not chkr:
            raise NameError("You need to install the python aspell library")
        super(SpellingErrorCount, self).__init__(feature)
        self.exemptions = exemptions if exemptions else set()

    def _apply(self, data, fitted_feature):
        return data.map(lambda x : count_spell_errors(x, self.exemptions))


spelling_suggestions = {}

def words(text): return re.findall("[a-z']+", text.lower())

def train(features):
    model = collections.defaultdict(lambda: 1)
    for f in features:
        model[f] += 1
    return model


def is_nondict(t):
    return not wordnet.synsets(t) and t not in wordlist

def nondict_w_exemptions(toks, exemptions, count_typos=False):
    cnt = 0
    for t in toks:
        if is_nondict(t) and t not in exemptions:
            if not count_typos:
                sug = suggest_correction(t)
                if sug.replace(' ', '') == t: continue
            cnt += 1
    return cnt

class NonDictCount(Feature):
    def __init__(self, feature, exemptions=None):
        super(NonDictCount, self).__init__(feature)
        self.exemptions = exemptions if exemptions else set()

    def _apply(self, data, fitted_feature):
        return data.map(lambda toks: nondict_w_exemptions(toks, self.exemptions))

class RemoveNonDict(Feature):
    """Expects tokens"""
    def _apply(self, data, fitted_feature):
        return data.map(
                lambda toks: [t for t in toks if not wordnet.synsets(t) and t not in wordlist]
                )

def expanded_tokenize(s):
    toks = []
    for tok in tokenize(s):
        for syn in wordnet.synsets(tok):
            toks.extend(syn.lemma_names)
    return toks

class ExpandedTokens(Feature):

    def _apply(self, data, fitted_feature):
        return data.map(expanded_tokens)

class LongwordCount(Feature):
    def __init__(self, feature, lengths=[6, 7, 8]):
        super(LongwordCount, self).__init__(feature)
        self.lengths = lengths
    def _apply(self, data, fitted_feature):
        cols = []
        for length in self.lengths:
            cols.append(data.map(lambda tokens: sum([len(t) > length for t in
            tokens])))
        return concat(cols, keys=['%s_%s'%(str(w), data.name) for w in self.lengths],
                axis=1)


class SentenceCount(Feature):
    def _apply(self, data, fitted_feature):
        return data.map(
                lambda x: len(sent_tokenizer.tokenize(x)) + 1)

class SentenceSlice(Feature):
    def __init__(self, feature, start=0, end=None):
        super(SentenceSlice, self).__init__(feature)
        self.start = start
        self.end = end

    def _apply(self, data, fitted_feature):
        if self.end is None:
            return data.map(
                    lambda x: ' '.join(sent_tokenizer.tokenize(x)[self.start:])
                    )
        return data.map(
                lambda x: ' '.join(sent_tokenizer.tokenize(x)[self.start:self.end])
                )

class SentenceLength(Feature):
    def _apply(self, data, fitted_feature):
        return data.map(
                lambda x: float(len(x))/(len(sent_tokenizer.tokenize(x)) + 1))

class CapitalizationErrors(Feature):
    def _apply(self, data, fitted_feature):
        return data.map(
                lambda x: len([1 for s in sent_tokenizer.tokenize(x) if
                    s and s[0]!=s[0].upper() or ' i ' in s]))

class LongestSentence(Feature):
    def _apply(self, data, fitted_feature):
        return data.map(
                lambda x: max(map(len, sent_tokenizer.tokenize(x)))
                )

class LongestWord(Feature):
    def _create(self, tokens):
        return tokens.map(
                lambda x: max([len(t) for t in x])
                )

class VocabSize(Feature):
    def _apply(self, data, fitted_feature):
        return data.map(lambda tokens: len(set(tokens)))

class KeywordCount(Feature):
    def __init__(self, feature, words):
        super(KeywordCount, self).__init__(feature)
        self.words = set(words)
    def _apply(self, data, fitted_feature):
        cols = []
        for word in self.words:
            cols.append(data.map(lambda tokens: tokens.count(word)))
        return concat(cols, keys=[ '%s_%s'%(w, data.name) for w in self.words],
                axis=1)

def char_kl(txt):
    if not txt:
        return 0
    # all caps case... not sure what to do
    if txt.upper() == txt:
        return 1
    c = collections.Counter(txt)
    kl = 0
    tot = float(len(txt))
    eps = 1.0e-05
    for ch, p in char_freqs.items():
        if ch not in c:
            q = eps
        else:
            q = c[ch]/tot
        try:
            kl += p * np.log(p/q)
        except e:
            logging.warn(e)
            logging.warn("Arithmetic error with the following values:")
            logging.warn("{p} * np.log({p} / {q})".format(p=p, q=q))
            logging.warn("c={c}; text={txt}".format(c=c, txt=txt))
    return kl


class CharFreqKL(Feature):
    def _apply(self, data, fitted_feature):
        return data.map(char_kl)


class WeightedWordCount(Feature):
    def _create(self, tokens):
        alltoks = reduce(lambda x,y:x+y, tokens)
        n = float(len(alltoks))
        c = Counter(alltoks)
        return tokens.map(
                lambda toks: sum([1-c[t]/n for t in toks])
                )


class NgramCompare(NgramCounts):
    def __init__(self, feature, *args, **kwargs):
        kwargs['mindocs'] = 1
        kwargs['maxdocs'] = 1
        kwargs['maxterms'] = 1000000
        super(NgramCompare, self).__init__(feature, *args, **kwargs)
        self.tokenizer = tokenize_with_sentinels

    def _apply(self, data, fitted_feature):
        raw_docs = [s for s in nltk.corpus.gutenberg.raw().split('[') if len(s) > 10000]
        docs = self.make_docs(raw_docs, 1)
        dct1 = self.dictionary(docs)
        dct1.filter_extremes(no_below=1, no_above=1.,
                keep_n=2000)
        logging.debug(dct1)
        n = 2
        docs = [self.tokenizer(d) for d in raw_docs]
        docs = [[self.sep.join(toks[i:i+n]) for i in range(len(toks)-n+1) if
            all([t in dct1.token2id for t in toks[i:i+n]])] for toks in docs]
        logging.debug("docs: " + str(docs[0][:80]))
        dct = self.dictionary(docs)
        docs = [self.tokenizer(d) for d in data]
        docs = [[self.sep.join(toks[i:i+n]) for i in range(len(toks)-n+1) if
            all([t in dct1.token2id for t in toks[i:i+n]])] for toks in docs]
        logging.debug("docs: " + str(docs[0][:80]))
        unknown = [t for t in docs[0] if t not in dct.token2id][:200]
        logging.debug("Unknown ngrams: " + str(unkown))
        cnts = [sum([t not in dct.token2id for t in toks]) for toks in docs]
        return Series(cnts, index=data.index)

########NEW FILE########
__FILENAME__ = trained
import logging
from pandas import Series, DataFrame, concat

from ramp.builders import build_target_safe
from ramp.features.base import to_feature, ComboFeature, Feature, AllDataFeature
from ramp.modeling import fit_model, generate_test
from ramp.utils import make_folds, get_single_column, reindex_safe


class TrainedFeature(Feature):

    def __init__(self):
        # For trained features, we will need access to all the data
        self.feature = AllDataFeature()
        super(TrainedFeature, self).__init__(self.feature)


class Predictions(TrainedFeature):
    # TODO: update for new context

    def __init__(self, model_def, name=None, external_data=None,
            cv_folds=None):
        """
        If cv-folds is specified, will use k-fold cross-validation to
        provide robust predictions.
        (The predictions returned are those predicted on hold-out sets only.)
        Will not provide overly-optimistic fit like Predictions will, but can
        increase runtime significantly (nested cross-validation).
        Can be int or iteratable of (train, test) indices
        """
        self.cv_folds = cv_folds
        self.model_def = model_def
        self.external_data = external_data
        super(Predictions, self).__init__()
        #TODO
        # if self.external_context is not none:
        #     # dont need to retrain if using external dataset to train
        #     self.trained = false
        if not name:
            name = 'predictions'
        self._name = '%s[%s,%d features]'%(name,
                model_def.estimator.__class__.__name__, len(model_def.features))

    def _train(self, train_data):
        x, y, fitted_model = fit_model(self.model_def, train_data)
        return fitted_model

    def _apply(self, data, fitted_feature):
        fitted_model = fitted_feature.trained_data

        #TODO
        # if self.cv_folds:
        #     if isinstance(self.cv_folds, int):
        #         folds = make_folds(context.train_index, self.cv_folds)
        #     else:
        #         folds = self.cv_folds
        #     preds = []
        #     for train, test in folds:
        #         ctx = context.copy()
        #         ctx.train_index = train
        #         preds.append(self._predict(ctx, test, fit_model=True))
        #     # if there is held-out data, use all of train to predict
        #     # (these predictions use more data, so will be "better",
        #     # not sure if that is problematic...)
        #     remaining = context.data.index - context.train_index
        #     if len(remaining):
        #         preds.append(self._predict(context, remaining))
        #     preds = concat(preds, axis=0)
        # else:
        preds = self._predict(fitted_model, data)
        preds = DataFrame(preds, index=data.index)
        return preds

    def _predict(self, fitted_model, predict_data):
        x_test, y_true = generate_test(self.model_def, predict_data, fitted_model)
        y_preds = self.model_def.estimator.predict(x_test)
        return y_preds

    def make_cross_validated_models(self, data, fitted_feature):
        pass


class Residuals(Predictions):

    def _predict(self, fitted_model, predict_data):
        x_test, y_true = generate_test(self.model_def, predict_data, fitted_model)
        y_preds = self.model_def.estimator.predict(x_test)
        return y_preds - y_true


class FeatureSelector(ComboFeature):

    def __init__(self, features, selector, target, data, n_keep=50,
                 threshold_arg=None):
        """
        """
        super(FeatureSelector, self).__init__(features)
        self.selector = selector
        self.n_keep = n_keep
        self.threshold_arg = threshold_arg
        self.target = target
        self.data = data
        self._name = self._name + '_%d_%s'%(threshold_arg or n_keep, selector.__class__.__name__)

    def _train(self, train_datas):
        train_data = concat(train_datas, axis=1)
        y, ff = build_target_safe(self.target, self.data)
        y = reindex_safe(y, train_data.index)
        arg = self.threshold_arg
        if arg is None:
            arg = self.n_keep
        cols = self.selector.select(train_data, y, arg)
        return cols

    def _combine_apply(self, datas, fitted_feature):
        data = concat(datas, axis=1)
        selected_columns = fitted_feature.trained_data
        return data[selected_columns]


class TargetAggregationByFactor(TrainedFeature):
    """
    """
    def __init__(self, group_by, func=None, target=None, min_sample=10):
        # How terrible of a hack is this?
        super(TargetAggregationByFactor, self).__init__()
        self.group_by = group_by
        self.func = func
        self.target = to_feature(target)
        self.min_sample = min_sample

    def _train(self, train_data):
        y, ff = build_target_safe(self.target, train_data)
        vc = train_data[self.group_by].value_counts()
        keys = [k for k, v in vc.iterkv() if v >= self.min_sample]
        train_data['__grouping'] = train_data[self.group_by].map(lambda x: x if x in keys else '__other')
        train_data['__target'] = y
        vals = train_data.groupby('__grouping').agg({'__target': self.func})['__target'].to_dict()
        logging.debug("Preparing Target Aggregations:")
        logging.debug(str(vals.items()[:10]))
        del train_data['__target']
        del train_data['__grouping']
        return (keys, vals)

    def _apply(self, data, fitted_feature):
        keys, vals = fitted_feature.trained_data
        logging.debug("Loading Target aggs")
        logging.debug(str(vals.items()[:10]))
        logging.debug(str(keys[:10]))
        logging.debug(str(data.columns))
        data = data.applymap(lambda x: vals.get(x if x in keys else '__other'))
        return data

########NEW FILE########
__FILENAME__ = folds
import pandas as pd
import numpy as np
import random
import logging
from builders import build_target_safe

#TODO: how to repeat folds?


class BasicFolds(object):
    def __init__(self, num_folds, data, repeat=1, seed=None):
        self.num_folds = num_folds
        self.data = data
        self.seed = seed
        self.repeat = repeat
    
    def __iter__(self):
        n = len(self.data)
        index = self.data.index
        indices = range(n)
        foldsize = n / self.num_folds
        folds = []
        if self.seed is not None:
            np.random.seed(self.seed)
        for i in range(self.repeat):
            np.random.shuffle(indices)
            for i in range(self.num_folds):
                test = index[indices[i*foldsize:(i + 1)*foldsize]]
                train = index - test
                assert not (train & test)
                fold = (pd.Index(train), pd.Index(test))
                yield fold


class WatertightFolds(BasicFolds):
    """
    Ensure that there is no leakage across a particular factor, given by
    `leakage_column`.
    
    For example, if there are multiple entries for a given user, this could be
    used to ensure that each user is completely contained in a single fold.
    """
    def __init__(self, num_folds, data, leakage_column, **kwargs):
        super(WatertightFolds, self).__init__(num_folds, data, **kwargs)
        self.leakage_column = leakage_column
    
    def __iter__(self):
        n = len(self.data)
        index = self.data.index
        indices = range(n)
        foldsize = n / self.num_folds
        folds = []
        
        if self.seed is not None:
            np.random.seed(self.seed)
        
        for i in range(self.repeat):
            watertight_bins = self.data.groupby(self.leakage_column)[self.data.columns[0]].count()
            watertight_bins = watertight_bins.reindex(np.random.permutation(watertight_bins.index))
            watertight_bins_cum = watertight_bins.cumsum()
            for i in range(self.num_folds):
                test_bins = watertight_bins_cum[(watertight_bins_cum >  i * foldsize) & 
                                                (watertight_bins_cum <= (i+1) * foldsize)]
                test =index[self.data[self.leakage_column].isin(test_bins.index)]
                train = index - test
                
                # Sanity checks
                assert not (train & test)
                if np.abs(len(test) - foldsize) > 0.05*foldsize:
                    # Folds will not be exact in size, but a warning will be
                    # emitted if they are far from the expected value.
                    logging.warn("Fold deviated from expected size. Target: {target} Actual: {actual}".format(target=foldsize, actual=len(test)))
                fold = (pd.Index(train), pd.Index(test))
                yield fold


class BinaryTargetFolds(object):
    def __init__(self, target, data, seed=None):
        self.seed = seed
        self.target = target
        self.data = data
        self.folds = None
        self.y = None

    def compute_folds(self):
        raise NotImplementedError
    
    def build_target(self):
        y, ff = build_target_safe(self.target, self.data)
        self.y = y
        self.negatives = y[~y.astype('bool')].index
        self.positives = y[y.astype('bool')].index

    def randomize(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        neg = pd.Index(np.random.permutation(self.negatives))
        pos = pd.Index(np.random.permutation(self.positives))
        return neg, pos

    def __iter__(self):
        if self.y is None:
            self.build_target()
        if self.folds is None:
            self.compute_folds()
        for fold in self.folds:
            yield fold


class BalancedFolds(BinaryTargetFolds):
    def __init__(self, num_folds, target, data, seed=None):
        self.num_folds = num_folds
        super(BalancedFolds, self).__init__(target, data, seed)

    def compute_folds(self):
        neg, pos = self.randomize()
        nn = len(neg) / self.num_folds
        np = len(pos) / self.num_folds
        folds = []
        for i in range(self.num_folds):
            s = i * nn
            e = (i + 1) * nn
            train_neg, test_neg = neg[:s] + neg[e:], neg[s:e]
            s = i * np
            e = (i + 1) * np
            train_pos, test_pos = pos[:s] + pos[e:], pos[s:e]
            fold = (train_neg + train_pos, test_neg + test_pos)
            folds.append(fold)
        self.folds = folds


class BootstrapFolds(BalancedFolds):
    def __init__(self, num_folds, target, data, seed=None,
                  pos_train=None, pos_test=None, neg_train=None, neg_test=None):
        super(BootstrapFolds, self).__init__(num_folds, target, data, seed)
        if (any([pos_train, pos_test, neg_train, neg_test])
                and not all([pos_train, pos_test, neg_train, neg_test])):
            raise ValueError("Please specify all four sizes, or none at all")
        self.pos_train = pos_train
        self.neg_train = neg_train
        self.pos_test = pos_test
        self.neg_test = neg_test

    def compute_folds(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        folds = []
        for i in range(self.num_folds):
            train_neg = pd.Index(np.random.choice(self.negatives, self.neg_train, replace=True))
            test_neg = pd.Index(np.random.choice(self.negatives - train_neg, self.neg_test, replace=True))
            train_pos = pd.Index(np.random.choice(self.positives, self.pos_train, replace=True))
            test_pos = pd.Index(np.random.choice(self.positives - train_pos, self.pos_test, replace=True))
            fold = (train_neg.append(train_pos), test_neg.append(test_pos))
            folds.append(fold)
        self.folds = folds


make_default_folds = BasicFolds

########NEW FILE########
__FILENAME__ = metrics
﻿  # -*- coding: utf-8 -*-
'''
Metrics
-------

Estimator performance assessment metrics, both custom and imported from the 
sklearn library of metrics. Sklearn metric documentation can be found at
http://scikit-learn.org/stable/modules/model_evaluation.html

Custom metrics/sklearn metrics can be generated/used by subclassing the 
Metric/SKLearnMetric classes

'''

import logging
import math
import numpy as np
from sklearn import metrics
# sklearn api change:
try:
    auc_scorer = metrics.auc_score
except AttributeError:
    auc_scorer = metrics.auc


class Metric(object):
    """
    Implements evaluate method that takes a Result object and outputs a score.
    """
    # lower values are better by default, set reverse to true for
    # "bigger is better" metrics
    reverse = False
    
    @property
    def name(self):
        return self.__class__.__name__.lower()
    
    def score(self, result):
        raise NotImplementedError

class SKLearnMetric(Metric):
    '''SKLearn library metric'''
    
    metric = None
    
    def __init__(self, metric, **kwargs):
        self.metric = metric
        self.kwargs = kwargs

    def score(self, result):
        return self.metric(result.y_test, result.y_preds, **self.kwargs)


# Regression
class RMSE(Metric):
    '''Mean Squared Error: The average of the squares of the errors.'''
    def score(self):
        return sum((result.y_test - result.y_preds)**2)/float(len(result.y_test))


# Classification
class AUC(SKLearnMetric):
    '''
    Area Under the Curve (AUC): area under the reciever operating 
    characteristic (ROS curve)
    '''
    reverse = True
    metric = staticmethod(auc_scorer)


class F1(SKLearnMetric):
    '''F-measure: Weighted average of the precision and recall'''
    reverse = True
    metric = staticmethod(metrics.f1_score)


class HingeLoss(SKLearnMetric):
    '''Hinge Loss (non-regularized): Classifier loss function'''
    metric = staticmethod(metrics.hinge_loss)


class LogLoss(Metric):
    '''
    Logarithmic Loss: Logarithm of the likelihood function for a Bernoulli 
    random distribution. https://www.kaggle.com/wiki/LogarithmicLoss
    '''
    def score(self, result):
        return - sum(result.y_test * np.log(result.y_preds) + (1 - actual) * np.log(1 -
            result.y_preds))/len(result.y_loss)


class MCC(SKLearnMetric):
    """
    Matthew's Correlation Coefficient.
    """
    reverse = True
    metric = staticmethod(metrics.matthews_corrcoef)


class GeneralizedMCC(Metric):
    """ Matthew's Correlation Coefficient generalized to multi-class case """
    def cov(self, c, n, flip=False):
        s = 0
        for k in range(n):
            if flip:
                s1 = sum([c[l,k] for l in range(n)])
                s2 = sum([c[g,f] for g in range(n) for f in range(n) if f != k])
            else:
                s1 = sum([c[k,l] for l in range(n)])
                s2 = sum([c[f,g] for g in range(n) for f in range(n) if f != k])
            s += s1 * s2
        return s
    
    def score(self, result):
        c = metrics.confusion_matrix(result.y_test, result.y_preds)
        n = c.shape[0]
        numer = sum([c[k,k] * c[m,l] - c[l,k] * c[k,m] for k in range(n) for l in range(n) for m in range(n)])
        denom = math.sqrt(self.cov(c, n)) * math.sqrt(self.cov(c, n, flip=True))
        if abs(denom) < .00000001:
            return numer
        return numer/denom



class ArgMetric(Metric):
    """
    Implements an evaluate method that takes a Result object and an argument and
    returns a score.
    """
    def __init__(self, arg=None):
        self.arg = arg
        super(ArgMetric, self).__init__()

    def score(self, result, arg=None):
        raise NotImplementedError


class Recall(ArgMetric):
    """
    Recall: True positives / (True positives + False negatives)
    """
    def score(self, result, threshold=None):
        if threshold is None:
            threshold = self.arg
        return result.y_test[result.y_preds >= threshold].sum() / float(result.y_test.sum())


class WeightedRecall(ArgMetric):
    """
    Recall: Sum of weight column @ true positives  / sum of weight column @ (True positives + False negatives)
    """
    def __init__(self, arg=None, weight_column=None):
        self.weight_column = weight_column
        super(WeightedRecall, self).__init__(arg)

    def score(self, result, threshold=None):
        if threshold is None:
            threshold = self.arg
        positive_indices = result.y_test[result.y_preds >= threshold].index
        return (result.original_data.loc[positive_indices][self.weight_column].sum() /
               float(result.original_data.loc[result.y_test.index][self.weight_column].sum()))


class PositiveRate(ArgMetric):
    """
    Positive rate: (True positives + False positives) / Total count
    """
    name = "Positive Rate"
    
    def score(self, result, threshold=None):
        if threshold is None:
            threshold = self.arg
        return result.y_test[result.y_preds >= threshold].count() / float(result.y_test.count())

########NEW FILE########
__FILENAME__ = modeling
from ramp.builders import build_featureset_safe, build_target_safe, apply_featureset_safe, apply_target_safe
from ramp.estimators.base import FittedEstimator
from ramp.folds import make_default_folds
from ramp.result import Result
from ramp.store import Storable
from ramp.utils import key_from_index


class FittedModel(Storable):

    def __init__(self,
                 model_def,
                 fitted_features,
                 fitted_target,
                 fitted_estimator):
        self.model_def = model_def
        self.fitted_features = fitted_features
        self.fitted_target = fitted_target
        self.fitted_estimator = fitted_estimator


class PackagedModel(Storable):

    def __init__(self, fitted_model, data, data_description, result=None, reports=None):
        super(PackagedModel, self).__init__()
        self.fitted_model = fitted_model
        self.n_rows = len(data)
        self.n_cols = len(data.columns)
        self.data_key = key_from_index(data.index)
        self.data_description = data_description
        self.result = result
        self.reports = reports


def generate_train(model_def, data, prep_index=None, train_index=None):
    # create training set
    x_train, fitted_features = build_featureset_safe(model_def.features, data, prep_index, train_index)
    y_train, fitted_target = build_target_safe(model_def.target, data, prep_index, train_index)
    return x_train, y_train, fitted_features, fitted_target


def generate_test(model_def, predict_data, fitted_model, compute_actuals=True):
    # create test set and predict
    x_test = apply_featureset_safe(model_def.features, predict_data, fitted_model.fitted_features)
    if compute_actuals:
        y_test = apply_target_safe(model_def.target, predict_data, fitted_model.fitted_target)
    else:
        y_test = None
    return x_test, y_test
# ughh
generate_test.__test__ = False

def fit_model(model_def, data, prep_index=None, train_index=None):
    x_train, y_train, fitted_features, fitted_target = generate_train(model_def,
                                                                      data,
                                                                      prep_index,
                                                                      train_index)

    # fit estimator
    model_def.estimator.fit(x_train, y_train)

    # unnecesary?
    fitted_estimator = FittedEstimator(model_def.estimator, x_train, y_train)

    fitted_model = FittedModel(model_def, fitted_features, fitted_target, fitted_estimator)
    return x_train, y_train, fitted_model


def cross_validate(model_def, data, folds, reporters=[], repeat=1):
    """
    """
    results = []

    if isinstance(folds, int):
        folds = make_default_folds(num_folds=folds, data=data)
    
    for i in range(repeat):
        for fold in folds:
            if len(fold) == 2:
                train_index, test_index = fold
                prep_index = None
            elif len(fold) == 3:
                train_index, test_index, prep_index = fold
            else:
                raise ValueError("Fold is not of right dimension (%d, not 2 or 3)"%len(fold))
            x_train, y_train, fitted_model = fit_model(model_def, data, prep_index, train_index)
            x_test, y_test = generate_test(model_def, data, fitted_model)
            y_preds = fitted_model.fitted_estimator.predict(x_test)
            result = Result(x_train, x_test, y_train, y_test, y_preds, model_def, fitted_model, data)
            results.append(result)
            
            for reporter in reporters:
                reporter.update(result)
    return results, reporters


def build_and_package_model(model_def, data, data_description, evaluate=False,
                            reporters=None, prep_index=None, train_index=None):
    x_train, y_train, fitted_model = fit_model(model_def, data, prep_index, train_index)
    y_preds = fitted_model.fitted_estimator.predict(x_train)
    result = None
    if evaluate:
        # only evaluate on train (this seems reasonable)
        result = Result(x_train, x_train, y_train, y_train, y_preds, model_def, fitted_model, data)
        #TODO
        # reports = evaluate(result, reporters)

    # TODO
    reports = []

    packaged_model = PackagedModel(fitted_model,
                                   data,
                                   data_description,
                                   result,
                                   reports)
    return packaged_model

########NEW FILE########
__FILENAME__ = models
import logging
from utils import make_folds, _pprint, get_single_column, pprint_scores
from pandas import Series, concat, DataFrame
import random
import hashlib
import copy
import numpy as np
from sklearn import cross_validation, ensemble, linear_model
from builders import build_featureset, build_target
from prettytable import PrettyTable, ALL

debug = False

def get_x(config, context):
    x = build_featureset(config.features, context)
    if config.column_subset:
        x = x[config.column_subset]
    return x

def get_y(config, context):
    return build_target(config.target, context)

def get_xy(config, context):
    return get_x(config, context), get_y(config, context)


def get_key(config, context):
    return '%r-%s' % (config, context.create_key())


def get_metric_name(metric):
    if hasattr(metric, 'name'):
        name = metric.name
    else:
        name = metric.__name__
    return name


def fit(config, context, model_name=None, load_only=False):
    x, y = None, None
    try:
        # model caching
        config.model = context.store.load(model_name or get_key(config, context))
        logging.info("Loading stored model...")
    except KeyError:
        if load_only:
            raise Exception("Could not load model and load_only=True.")
        x, y = get_xy(config, context)

        train_x = x.reindex(context.train_index)
        train_y = y.reindex(context.train_index)

        config.model.column_names = train_x.columns

        logging.debug(train_x)

        logging.info("Fitting model '{model}' ... ".format(model=config.model))
        
        config.model.fit(train_x.values, train_y.values)
        logging.info("...done.")
        context.store.save(model_name or get_key(config, context), config.model)

    config.update_reporters_with_model(config.model)

    return x, y


def predict(config, context, predict_index, fit_model=True, model_name=None):
    if len(context.train_index & predict_index):
        logging.warning("Train and predict indices overlap...")

    x, y = None, None

    if model_name:
        config.model = context.store.load(model_name)

    if not model_name and fit_model:
        x, y = fit(config, context)

    # TODO: possible to have x loaded without new prediction rows
    if x is None:
        # rebuild just the necessary x:
        ctx = context.copy()
        ctx.data = context.data.ix[predict_index]
        x = get_x(config, ctx)
        try:
            # we may or may not have y's in predict context
            # we get them if we can for metrics and reporting
            y = get_y(config, ctx)
        except KeyError:
            pass

    logging.debug(x.columns)

    predict_x = x.reindex(predict_index)

    logging.info("Making predictions... ")
    # make actual predictions
    ps = config.model.predict(predict_x.values)
    try:
        preds = Series(ps, index=predict_x.index)
    except:
        preds = DataFrame(ps, index=predict_x.index)
    logging.info("...done.")
    # prediction post-processing
    if config.prediction is not None:
        old = context.data
        context.data = context.data.reindex(predict_x.index)
        context.data[config.predictions_name] = preds
        preds = build_target(config.prediction, context)
        preds = preds.reindex(predict_x.index)
        context.data = old
    preds.name = ''
    actuals = y.reindex(predict_index)
    # TODO: handle multi-variate predictions
    predict_x['predictions'] = preds
    predict_x['actuals'] = actuals
    config.update_reporters_with_predictions(context, predict_x, actuals, preds)
    return predict_x


def cv(config, context, folds=5, repeat=2, print_results=False,
       predict_method=None, predict_update_column=None):
    # TODO: too much overloading on folds here
    if isinstance(folds, int):
        folds = make_folds(context.data.index, folds, repeat)
    #else:
        #folds.set_context(config, context)
    scores = {get_metric_name(m): [] for m in config.metrics}
    # we are overwriting indices, so make a copy
    ctx = context.copy()
    i = 0
    folds = list(folds)
    k = len(folds)/repeat
    for train, test in folds:
        logging.info("Cross-Validation fold %d/%d round %d/%d" % (i % k + 1, k, i/k + 1, repeat))
        i += 1
        ctx.train_index = train
        ctx.test_index = test
        fold_scores, result = evaluate(config, ctx, test, predict_method, predict_update_column)
        context.latest_result = result
        for metric_, s in fold_scores.items():
            scores[metric_].append(s)
        for metric_, s in scores.items():
            logging.debug("%s: %s" % (metric_, pprint_scores(s)))
    result = {'config':config, 'scores':scores}

    # report results
    t = PrettyTable(["Reporter", "Report"])
    t.hrules = ALL
    t.align["Reporter"] = "l"
    t.align["Report"] = "l"
    for reporter in config.reporters:
        t.add_row([reporter.__class__.__name__, str(reporter)])
        reporter.reset()
    if print_results:
        print t
    
    return result


def evaluate(config, ctx, predict_index,
             predict_method=None, predict_update_column=None):
    if predict_method is None:
        result = predict(config, ctx, predict_index)
    else:
        # TODO: hacky!
        result = predict_method(config, ctx, predict_index, update_column=predict_update_column)
    preds = result['predictions']
    y = result['actuals']
    
    try:
        if config.actual is not None:
            actuals = build_target(config.actual, ctx).reindex(predict_index)
        else:
            actuals = y.reindex(predict_index)
    #TODO: HACK -- there may not be an actual attribute on the config
    except AttributeError:
        actuals = y.reindex(predict_index)
    
    scores = {}
    for metric in config.metrics:
        name = get_metric_name(metric)
        if hasattr(metric, 'score'):
            scores[name] = metric.score(actuals, preds)
        else:
            scores[name] = metric(actuals, preds)
    return scores, result


def predict_autosequence(config, context, predict_index, fit_model=True, update_column=None):
    if len(context.train_index & predict_index):
        logging.warning("Train and predict indices overlap...")
    
    x, y = None, None
    
    if fit_model:
        x, y = fit(config, context)
    
    logging.debug(x.columns)
    logging.debug(config.model.coef_)
    
    ctx = context.copy()
    ps = []
    for i in predict_index:
        ctx.data = context.data
        x = get_x(config, ctx)
        predict_x = x.reindex([i])
    
        # make actual predictions
        p = config.model.predict(predict_x.values)
        if update_column is not None:
            ctx.data[update_column][i] = p[0]
        ps.append(p[0])
    try:
        preds = Series(ps, index=predict_index)
    except:
        preds = DataFrame(ps, index=predict_index)
    # prediction post-processing
    if config.prediction is not None:
        context.data[config.predictions_name] = preds
        preds = build_target(config.prediction, context)
        preds = preds.reindex(predict_index)
    preds.name = ''
    return preds, x, y


def cv_autosequence(config, context, folds=5, repeat=2, print_results=False, update_column=None):
    return cv(config, context, folds, repeat, print_results, predict_method=predict_autosequence,
            predict_update_column=update_column)


########NEW FILE########
__FILENAME__ = model_definition
﻿  # -*- coding: utf-8 -*-
'''
ModelDefinition
-------

A configuration is a uniquely defined data analysis model, including 
features, estimator, and target metric. ModelDefinitions can be pickled
and retrieved. 

The ConfigFactory is at the core of the power of Ramp. It creates a 
configuration iterator that allows for the exploration of a large number 
of features, models, and metrics

'''

import itertools
from features.base import BaseFeature, Feature
from estimators.base import wrap_sklearn_like_estimator
from utils import _pprint, stable_repr
import logging
import copy

__all__ = ['ModelDefinition', 'model_definition_factory']


class ModelDefinition(object):
    """
    Defines a specific data analysis model,
    including features, estimator and target metric.
    Can be stored (pickled) and retrieved.
    """
    DEFAULT_PREDICTIONS_NAME = '$predictions'
    params = ['target', 'features', 'estimator', 'column_subset'
              'prediction', 'predictions_name', 'actual']

    def __init__(self, target=None, features=None, estimator=None,
                 column_subset=None, prediction=None, 
                 predictions_name=None, actual=None):
        """
        Parameters:
        ___________

        target: `Feature` or string, default None
            `Feature` or basestring specifying the target ("y") variable of 
            the analysis.

        features: `Feature`, default None
            An iterable of `Features <Feature>` to be used by the estimator 
            in the analysis.

        estimator: estimator (compatible with sklearn estimators), default None
            An estimator instance compatible with sklearn estimator
            conventions: Has fit(x, y) and predict(y) methods. If the object is
            not a ramp Estimator, it will be wrapped to add sensible
            prediction methods.
        
        predictions_name: string, default None
            A unique string used as a column identifier for model predictions. 
            Must be unique among all feature names: eg '$logreg_predictions$'
            
        prediction: `Feature`, default None
            A `Feature` transformation of the special `predictions_name` 
            column used to post-process predictions prior to metric scoring.

        actual: `Feature`, default None
            `Feature`. Used if `target` represents a transformation that is 
            NOT the actual target "y" values. Used in conjuction with 
            `prediction` to allow model training, predictions and scoring to 
            operate on different values.
        """
        self.set_attrs(target, features, estimator,
                       column_subset, prediction, predictions_name, 
                       actual)

    def set_attrs(self, target=None, features=None, estimator=None,
                  column_subset=None, prediction=None,
                  predictions_name=None, actual=None):
            
        if prediction is not None:
            if predictions_name is None:
                raise ValueError("If you provide a prediction feature, you "
                "must also specify a _unique_ 'predictions_name'")
                
        if isinstance(target, BaseFeature) or target is None: 
            self.target = target
        else: 
            self.target = Feature(target)                
        
        if isinstance(prediction, BaseFeature) or prediction is None: 
            self.prediction = prediction
        else: 
            self.prediction = Feature(prediction)          
        self.predictions_name = predictions_name
        
        if actual is None:
            actual = self.target
        self.actual = (actual if isinstance(actual, BaseFeature) 
                       else Feature(actual))
        
        if features: 
            self.features = ([f if isinstance(f, BaseFeature) else Feature(f)
                              for f in features])
        else: 
            self.features = None
            
        # Wrap estimator to return probabilities in the case of a classifier
        self.estimator = wrap_sklearn_like_estimator(estimator)

        self.column_subset = column_subset

    def __getstate__(self):
        # shallow copy dict and keep references
        dct = self.__dict__.copy()
        return dct

    def __repr__(self):
        return stable_repr(self)

    def __str__(self):
        if self.features is not None: 
            feature_count = len(self.features)
        else: 
            feature_count = 0
        return 'estimator: %s\nfeatures: %d [%s ...]\ntarget: %s' % (
            self.estimator,
            feature_count, 
            ' '.join([str(f) for f in self.features])[:50],
            self.target
        )
    
    @property
    def summary(self):
        """
        Summary of model definition for labeling. Intended to be somewhat
        readable but unique to a given model definition.
        """
        if self.features is not None: 
            feature_count = len(self.features)
        else: 
            feature_count = 0
        feature_hash = 'feathash:' + str(hash(tuple(self.features)))
        return (str(self.estimator), feature_count, feature_hash, self.target)
    
    def update(self, dct):
        """Update the configuration with new parameters. Must use same 
        kwargs as __init__"""
        d = self.__dict__.copy()
        d.update(dct)
        self.set_attrs(**d)


def model_definition_factory(model_definition, **kwargs):
    """
    Provides an iterator over passed-in
    configuration values, allowing for easy
    exploration of models.
    
    Parameters:
    ___________
    
    base_config: 
        The base `ModelDefinition` to augment
    
    kwargs: 
        Can be any keyword accepted by `ModelDefinition`. 
        Values should be iterables.
    """
    if not kwargs:
        yield config
    else:
        for param in kwargs:
            if not hasattr(model_definition, param):
                raise ValueError("'%s' is not a valid configuration parameter" % param)
        
        for raw_params in itertools.product(*kwargs.values()):
            new_definition = copy.copy(model_definition)
            new_definition.update(dict(zip(kwargs.keys(), raw_params)))
            yield new_definition


########NEW FILE########
__FILENAME__ = reporters
from collections import defaultdict
import logging
import numpy as np
import pandas as pd
from pandas import DataFrame, Series
import pylab as pl
from sklearn import metrics

from ramp.utils import pprint_scores


class Reporter(object):
    """
    A reporter tracks the results of a series of model runs, as well as summary
    metrics which can be reported in text or graphical form.
    """
    defaults = dict(
            )
    
    @classmethod
    def factory(cls, *args, **kwargs):
        """
        Provides a function generating reporter objects with a given set of
        initialization parameters.
        """
        return lambda: cls(*args, **kwargs)
    
    def __init__(self, **kwargs):
        self.config = {}
        self.config.update(self.defaults)
        self.config.update(kwargs)
        self.summary = []
        self.results = []
    
    def __str__(self):
        return str(self.summary)
    
    def recompute(self):
        self.summary = [self.summarize(result) for result in self.results]
    
    def summarize(self, result):
        raise NotImplementedError
    
    def update(self, result):
        """
        Accepts an object of type Result and updates the report's internal representation.
        """
        self.results.append(result)
        ret = self.summarize(result)
        self.summary.append(ret)
        logging.debug("{name}.update returned {ret}".format(name=self.__class__.__name__, ret=ret))
    
    def from_results(self, results):
        for result in results:
            self.update(result)
    
    @staticmethod
    def combine(reporters):
        # TODO: Reporters should know how to combine with other reporters of the same kind to produce aggregate reports.
        raise NotImplementedError


class ModelOutliers(Reporter):
    pass


class ConfusionMatrix(Reporter):
    @staticmethod
    def summarize(result):
        return metrics.confusion_matrix(result.y_test, result.y_preds)


class MislabelInspector(Reporter):
    @staticmethod
    def summarize(result):
        ret = []
        for ind in result.y_test.index:
            a, p = result.y_test.loc[ind], result.y_preds.loc[ind]
            if a != p:
                ret_strings = ["-" * 20]
                ret_strings.append("Actual: %s\tPredicted: %s" % (a, p))
                ret_strings.append(result.x_test.ix[ind])
                ret_strings.append(result.y_test.ix[ind])
                ret.append(ret_strings)
        return '\n'.join(ret)


class RFImportance(Reporter):
    importance_arrays = []
    
    @property
    def summary(self):
        """
        Returns feature importances, sorted with most important features first.
        """
        return pd.concat(self.importance_arrays, axis=1).mean(axis=1).sort(ascending=False)
    
    def update(self, result):
        try:
            imps = result.fitted_model.feature_importances_
        except AttributeError:
            logging.warning("Warning: Model has no feature importances")
            return
        if imps is None:
            logging.warning("Warning: Model has no feature importances")
            return
        self.importance_arrays.append(imps)
        logging.debug(imps) 
        self.results.append(result)
    
    def _repr_html_(self):
        return self.summary._repr_html_()

    
class PRCurve(Reporter):
    def summarize(self, result):
        p, r, t = metrics.precision_recall_curve(result.y_test, result.y_preds)
        ret = DataFrame(
                {'Precision': p, 
                 'Recall': r}, 
                index=t)
        return ret

    
    def plot(self):
        try:
            import pylab as pl
        except ImportError as e:
            logging.error("ERROR: You must install matplotlib in order to see plots")
            return
        for curve in self.summary:
            pl.plot(curve['Precision'], curve['Recall'])
        pl.plot([0, 1], [0, 1], 'k--')
        pl.xlim([0.0, 1.0])
        pl.ylim([0.0, 1.0])
        pl.xlabel('False Positive Rate')
        pl.ylabel('True Positive Rate')
        pl.title('ROC')
        pl.legend(loc="lower right")
        pl.show()


class ROCCurve(Reporter):
    def summarize(self, result):
        fpr, tpr, thresholds = metrics.roc_curve(result.y_test, result.y_preds)
        ret = DataFrame(
                {'FPR':fpr, 
                 'TPR': tpr},
                index=thresholds)
        return ret
    
    def plot(self):
        try:
            import pylab as pl
        except ImportError as e:
            logging.error("ERROR: You must install matplotlib in order to see plots")
            return
        for curve in self.summary:
            pl.plot(curve['FPR'], curve['TPR'])
        pl.plot([0, 1], [0, 1], 'k--')
        pl.xlim([0.0, 1.0])
        pl.ylim([0.0, 1.0])
        pl.xlabel('False Positive Rate')
        pl.ylabel('True Positive Rate')
        pl.title('ROC')
        pl.legend(loc="lower right")
        pl.show()


class OOBEst(Reporter):
    def update(self, model):
        try:
            ret = self.summary.append(model.oob_score_)
            logging.debug("OOB score:" + model.oob_score_)
        except AttributeError:
            logging.exception("Model has no OOB score")
    
    def __str__(self):
        if not self.summary:
            return None
        else:
            return "OOB Est: %s" % (pprint_scores(self.summary))


class MetricReporter(Reporter):
    defaults = dict(
              lower_quantile=.05
            , upper_quantile=.95
            )
    
    def __init__(self, metric, **kwargs):
        """
        Accepts a Metric object and evaluates it at each fold.
        """
        Reporter.__init__(self, **kwargs)
        self.metric = metric
        self.summarize = self.metric.score
    
    def summary_df(self):
        lower_quantile = self.config['lower_quantile']
        upper_quantile = self.config['upper_quantile']
        
        vals = Series(self.summary)
        
        lower_bound = vals.quantile(lower_quantile)
        upper_bound = vals.quantile(upper_quantile)
        median = vals.quantile(0.5)
        mean = vals.mean()
        
        column_names = [ "Mean" , "Median" , "%d_Percentile" % (lower_quantile*100), "%d_Percentile" % (upper_quantile*100)]
        df = pd.DataFrame(dict(zip(column_names, [mean, median, lower_bound, upper_bound])), index=[0])
        
        return df
    
    def __str__(self):
        return str(self.summary_df())
    
    def _repr_html_(self):
        return self.summary_df()._repr_html_()
    
    def plot(self):
        vals = Series(self.summary)
        ax = vals.hist()
        ax.set_title("%s Histogram" % self.metric.name)
        return ax


class DualThresholdMetricReporter(MetricReporter):
    """
    Reports on a pair of metrics which are threshold sensitive.
    
    Thresholds are automatically detected from the evaluation results unless
    explicitly set in the config.
    """
    
    def __init__(self, metric1, metric2, **kwargs):
        """
        Accepts a Metric object and evaluates it at each fold.
        
        Parameters:
        Thresholds: [Float], default: None
            Thresholds for reporting the two metrics. If not set, thresholds
            are model predictions from all runs computed on the fly.
        
        lower_quantile, upper_quantile: Float, default 0.05, 0.095
            Quantiles to be used for reporting metrics.
        """
        Reporter.__init__(self, **kwargs)
        self.metric1 = metric1
        self.metric2 = metric2
        self.results = []
        self.n_cached_curves = 0
    
    @property
    def n_current_results(self):
        return len(self.results)
    
    @property
    def thresholds(self):
        if 'thresholds' in self.config:
            return self.config['thresholds']
        else:
            thresholds = pd.Series()
            for result in self.results:
                thresholds = pd.concat([thresholds, result.y_preds]).unique()
            return list(thresholds)
    
    def reset_thresholds():
        self.config['thresholds'] = None
    
    def summarize(self, result):
        thresholds = sorted(list(set(result.y_preds)))
        ret = DataFrame(
                {self.metric1.name: [self.metric1.score(result, threshold) for threshold in thresholds],
                 self.metric2.name: [self.metric2.score(result, threshold) for threshold in thresholds]},
                index=thresholds)
        logging.debug("preds:")
        logging.debug(result.y_preds)
        logging.debug("thresholds:")
        logging.debug(thresholds)
        return ret
    
    def summary_df(self, thresholds=None, lower_quantile=None, upper_quantile=None):
        """
        Calculates the pair of metrics for each threshold for each result.
        """
        if thresholds is None:
            thresholds = self.thresholds
        if lower_quantile is None:
            lower_quantile = self.config['lower_quantile']
        if upper_quantile is None:
            upper_quantile = self.config['upper_quantile']
        
        if self.n_current_results > self.n_cached_curves:
            # If there are new curves, recompute
            colnames = ['_'.join([metric, stat])
                        for metric in [self.metric1.name, self.metric2.name] 
                        for stat in ['Mean', 'Median',
                                     '%d_Percentile' % (100*lower_quantile),
                                     '%d_Percentile' % (upper_quantile*100)]]
            self.ret = pd.DataFrame(columns=colnames, index=thresholds, dtype='float64')
            
            for threshold in thresholds:
                m1s = Series([self.metric1.score(result, threshold) for result in self.results])
                m2s = Series([self.metric2.score(result, threshold) for result in self.results])
                self.ret.loc[threshold] = (m1s.mean(), m1s.quantile(.5), m1s.quantile(.05), m1s.quantile(.95),
                                           m2s.mean(), m2s.quantile(.5), m2s.quantile(.05), m2s.quantile(.95))
        return self.ret
    
    def plot_error(self, fig_ax=(None, None), color='red', **kwargs):
        curves = self.summary_df()
        
        fig, ax = fig_ax
        if ax is None:
            fig, ax = pl.subplots()
        
        # Plot medians
        ax.plot(curves[curves.columns[1]].values, 
                curves[curves.columns[5]].values, 
                color=color, 
                markeredgecolor=color)
        
        # Plot medians
        ax.fill_between(
            curves[curves.columns[1]].values, 
            curves[curves.columns[6]].values, 
            curves[curves.columns[7]].values, 
            facecolor=color, edgecolor='', interpolate=True, alpha=.33)
        ax.set_xlabel(self.metric1.name.capitalize())
        ax.set_ylabel(self.metric2.name.capitalize())
        
        if fig is None:
            return ax
        else:
            return fig, ax
    
    def plot(self, fig_ax=(None, None), color='red', **kwargs):
        fig, ax = fig_ax
        if ax is None:
            fig, ax = pl.subplots()
        for curve in self.summary:
            pl.plot(curve[self.metric1.name], 
                    curve[self.metric2.name],
                    color=color, 
                    markeredgecolor=color,
                    **kwargs)
        pl.plot([0, 1], [0, 1], 'k--')
        pl.xlim([0.0, 1.0])
        pl.ylim([0.0, 1.0])
        pl.xlabel(self.metric1.name)
        pl.ylabel(self.metric2.name)
        pl.show()


def combine_dual_reports(reports):
    colors = [ '#723C95'
             , '#5697D5'
             , '#9BCECA'
             , '#92BC45'
             , '#C7D632'
             , '#F6E400'
             , '#ECB61B'
             , '#E08C2C'
             , '#D3541F'
             , '#CD1E20'
             , '#C64A98'
             , '#A34F9B']
    
    fig, ax = pl.subplots()
    current_color = 0
    for report in reports:
        current_color = (current_color + 5) % len(colors)
        reports.plot(fig_ax=(fig, ax), color=colors[current_color])
    ax.show()



########NEW FILE########
__FILENAME__ = result
from ramp.store import Storable


class Result(Storable):
    def __init__(self, x_train, x_test, y_train, y_test, y_preds, model_def, fitted_model, original_data):
        """
        Class for storing the result of a single model fit.
        """
        self.x_train = x_train
        self.x_test = x_test
        self.y_train = y_train
        self.y_test = y_test
        self.y_preds = y_preds
        self.model_def = model_def
        self.fitted_model = fitted_model
        self.original_data = original_data


########NEW FILE########
__FILENAME__ = selectors
import logging
from utils import make_folds, _pprint
from pandas import Series, concat
from scipy.stats import norm
import random
import hashlib
import math
import copy
import numpy as np
from sklearn import cross_validation, ensemble, linear_model


class Selector(object):
    def __init__(self, verbose=False):
        self.verbose = verbose

    def __repr__(self):
        return '%s(%s)'%(self.__class__.__name__, _pprint(self.__dict__))


class RandomForestSelector(Selector):

    def __init__(self, n=100, thresh=None, min_=True, classifier=True,
            seed=2345, *args, **kwargs):
        self.n = n
        self.min = min_
        self.thresh = thresh
        self.seed = seed
        self.classifier = classifier
        super(RandomForestSelector, self).__init__(*args, **kwargs)

    def select(self, x, y, n_keep):
        cls = ensemble.RandomForestRegressor
        if self.classifier:
            cls = ensemble.RandomForestClassifier
        rf = cls(n_estimators=self.n,
                random_state=self.seed,
                n_jobs=-1)
        rf.fit(x.values, y.values)
        importances = rf.feature_importances_
        imps = sorted(zip(importances, x.columns),
                reverse=True)
        for i, x in enumerate(imps):
            imp, f = x
            logging.debug('%d\t%0.4f\t%s'%(i,imp, f))
        if self.thresh:
            imps = [t for t in imps if t[0] > self.thresh]
        return [t[1] for t in imps[:n_keep]]
#        sets = [[t[1] for t in imps[:i+1]] for i in range(len(imps))]
#        return sets

    def sets_cv(self, x, y):
        totals = [0]*len(x.columns)
        if self.min:
            totals = [1000] * len(x.columns)
        i = 0
        for train, test in cross_validation.KFold(n=len(y), k=4):
            i += 1
            logging.info("RF selector computing importances for fold {i}".format(i=i))
            cls = ensemble.RandomForestRegressor
            if self.classifier:
                cls = ensemble.RandomForestClassifier
            rf = cls(n_estimators=self.n,
                    random_state=self.seed,
                    n_jobs=-1)
            rf.fit(x.values[train], y.values[train])
            importances = rf.feature_importances_
            if self.min:
                totals = [min(imp, t) for imp, t in zip(importances, totals)]
            else:
                totals = [imp + t for imp, t in zip(importances, totals)]
        imps = sorted(zip(totals, x.columns),
                reverse=True)
        for i, x in enumerate(imps):
            imp, f = x
            logging.debug('%d\t%0.4f\t%s' % (i, imp, f))
        if self.thresh:
            imps = [t for t in imps if t[0] > self.thresh]
        sets = [[t[1] for t in imps[:i+1]] for i in range(len(imps))]
        return sets


class StepwiseForwardSelector(Selector):
    def __init__(self, n=100, min_=True):
        self.n = n
        self.min = min_

    def sets(self, x, y):
        lm = linear_model.LinearRegression(normalize=True)
        remaining = x.columns
        curr = []
        logging.debug("stepwise forward")
        for i in range(self.n):
            if i % 10 == 0:
                logging.debug("{i} features".format(i=i))
            coefs = []
            for col in remaining:
                cols = curr + [col]
                fcf = 1e12
                for train, test in cross_validation.KFold(n=len(y), k=4):
                    # computes fits over folds, uses lowest computed
                    # coefficient as value for comparison
                    lm.fit(x[cols].values[train], y.values[train])
                    cf = lm.coef_[-1]
                    if np.isnan(x[col].std()) or x[col].std() < 1e-7:
                        cf = 0
                    cf = abs(cf)
                    fcf = min(cf, fcf)
                coefs.append(fcf)
            coef, col = max(zip(coefs, remaining))
            logging.debug("adding column: {col}".format(col=col))
            curr.append(col)
            remaining = remaining.drop([col])
            yield list(curr)


class LassoPathSelector(Selector):

    def sets(self, x, y, n_keep):
        alphas, active, coef_path = linear_model.lars_path(x.values, y.values)
        sets = []
        seen = set()
        logging.debug(coef_path)
        for coefs in coef_path.T:
            cols = [x.columns[i] for i in range(len(coefs)) if coefs[i] > 1e-9]
            if len(cols) >= n_keep:
                return cols
        return cols


class BinaryFeatureSelector(Selector):
    """ Only for classification and binary(-able) features """

    def __init__(self, type='bns', *args, **kwargs):
        """ type in ('ig', 'bns', 'acc')
        see: jmlr.csail.mit.edu/papers/volume3/forman03a/forman03a.pdf"""
        self.type = type
        super(BinaryFeatureSelector, self).__init__(*args, **kwargs)

    def select(self, x, y, n_keep):
        cnts = y.value_counts()
        logging.info("Computing binary feature scores for %d features..." % len(x.columns))
        if len(cnts) > 2:
            scores = self.round_robin(x, y, n_keep)
        else:
            scores = self.rank(x, y)
            scores = [s[1] for s in scores]
        logging.debug(scores[:200])
        return scores[:n_keep]

    def round_robin(self, x, y, n_keep):
        """ Ensures all classes get representative features, not just those with strong features """
        vals = y.unique()
        scores = {}
        for cls in vals:
            scores[cls] = self.rank(x, np.equal(cls, y).astype('Int64'))
            scores[cls].reverse()
        keepers = set()
        while len(keepers) < n_keep:
            for cls in vals:
                keepers.add(scores[cls].pop()[1])
        return list(keepers)

    def rank(self, x, y):
        cnts = y.value_counts()
        scores = []

        def e(x, y):
            if abs(y) < .000000001:
                yx = 0
                lyx = 0
            else:
                yx = y / (x + y)
                lyx = math.log(y / (x + y))
            if abs(x) < .000000001:
                xy = 0
                lxy = 0
            else:
                xy = x / (x + y)
                lxy = math.log(x / (x + y))
            return - xy * lxy - yx * lyx

        for c in x.columns:
            true_positives = float(np.count_nonzero(np.logical_and(x[c], y)))
            false_positives = float(np.count_nonzero(np.logical_and(x[c], np.logical_not(y))))
            pos = float(cnts[1])
            neg = float(cnts[0])
            n = pos + neg
            if self.type == 'bns':
                tpr = max(0.0005, true_positives / pos)
                fpr = max(0.0005, false_positives / neg)
                tpr = min(.9995, tpr)
                fpr = min(.9995, fpr)
                score = abs(norm.ppf(tpr) - norm.ppf(fpr))
            elif self.type == 'acc':
                score = abs(tpr - fpr)
            elif self.type == 'ig':
                score = e(pos, neg) - ( (true_positives + false_positives) / n * e(true_positives, false_positives)
                    + (1 - (true_positives + false_positives) / n) * e(pos - true_positives, neg - false_positives))
            scores.append((score, c))
        scores.sort(reverse=True)
        return scores


class InformationGainSelector(Selector):
    """ Only for binary classification """

    def sets(self, x, y, n_keep):
        cnts = y.value_counts()
        assert(len(cnts) == 2)
        logging.info("Computing IG scores...")
        scores = []
        for c in x.columns:
            true_positives = sum(np.logical_and(x[c], y))
            false_positives = sum(np.logical_and(x[c], np.logical_not(y)))
            score = abs(norm.ppf(true_positives / float(cnts[1])) - norm.ppf(false_positives / float(cnts[0])))
            scores.append((score, c))
        scores.sort(reverse=True)
        return [s[1] for s in scores[:n_keep]]

########NEW FILE########
__FILENAME__ = shortcuts
from ramp.model_definition import ModelDefinition, model_definition_factory
from ramp import modeling

def cross_validate(data=None, folds=None, repeat=1, **kwargs):
    """Shortcut to cross-validate a single configuration.

    ModelDefinition variables are passed in as keyword args, along
    with the cross-validation parameters.
    """
    md_kwargs = {}
    for arg in ModelDefinition.params:
        if arg in kwargs:
            md_kwargs[arg] = kwargs.pop(arg)
    model_def = ModelDefinition(**md_kwargs)
    return modeling.cross_validate(model_def, data, folds, repeat=repeat, **kwargs)


def cv_factory(data=None, folds=None, repeat=1, reporter_factories=[], **kwargs):
    """Shortcut to iterate and cross-validate models.
    
    All ModelDefinition kwargs should be iterables that can be
    passed to model_definition_factory.
    """
    cv_runner = kwargs.pop('cv_runner', modeling.cross_validate)
    md_kwargs = {}
    for arg in ModelDefinition.params:
        if arg in kwargs:
            md_kwargs[arg] = kwargs.pop(arg)
    model_def_fact = model_definition_factory(ModelDefinition(), **md_kwargs)
    ret = {}
    for model_def in model_def_fact:
        results, reporters = cv_runner(model_def, data, folds, repeat=repeat, reporters=[factory() for factory in reporter_factories], **kwargs)
        ret[model_def.summary] = {'model_def': model_def,
                                  'results': results,
                                  'reporters': reporters}
    
    return ret

########NEW FILE########
__FILENAME__ = store
﻿  # -*- coding: utf-8 -*-
'''
Store
-------

Data storage classes. The default behavior for this module will always attempt
to read/write from HDF storage first, and will fall back to pickle storage if
required. 

This module uses the MD5 algorithm to create "safe" unique file names based 
on provided key values.  
'''
import cPickle as pickle
import hashlib
import os
import re

import pandas
try:
    import tables
    from tables.exceptions import NoSuchNodeError
except ImportError:
    NoSuchNodeError = None


__all__ = ['Storable']


def dumppickle(obj, fname, protocol=-1):
    """
    Pickle object `obj` to file `fname`.
    """
    with open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows
        pickle.dump(obj, fout, protocol=protocol)


def loadpickle(fname):
    """
    Load pickled object from `fname`
    """
    return pickle.load(open(fname, 'rb'))


class Storable(object):
    
    def to_pickle(self, path):
        dumppickle(self, path)

    @classmethod
    def from_pickle(cls, path):
        obj = loadpickle(path)
        assert type(obj) == cls
        return obj

    def to_string(self):
        pass

    def to_hdf5(self, path):
        pass

    def to_hdf5_or_pickle(self, path):
        pass


# class DummyStore(object):
#     def save(self, k, v):
#         pass
#     def load(self, k):
#         raise KeyError
#     def delete(self, kp):
#         pass


# class Store(object):

#     def __init__(self, path=None, verbose=False):
#         """
#         ABC for Store classes. Inheriting classes should override get
#         and put methods. Currently subclasses for HDF5 and cPickle, but 
#         extendable for other data storage types.
        
#         Parameters: 
#         -----------
#         path: string, default None
#             Path to data folder
#         verbose: bool, default False
#             Set 'True' to print read/write messages
#         """
#         self.path = path
#         self._shelf = None
#         self._uncachables = set()
#         self._cache = {}
#         self.verbose = verbose

#     def register_uncachable(self, un):
#         """Any key containing the substring `un` will NOT be cached """
#         self._uncachables.add(un)

#     def load(self, key):
#         """
#         Loads from cache, otherwise defaults to class 'get' method to load
#         from store. 
#         """
#         try:
#             v = self._cache[key]
#             if self.verbose:
#                 print "Retrieving '%s' from local" % key
#             return v
#         except KeyError:
#             v = self.get(key)
#             if self.verbose:
#                 print "Retrieving '%s' from store" % key
#             return v

#     def save(self, key, value):
#         """
#         Saves to cache, otherwise defaults to class 'put' method to load
#         from store
#         """
#         for un in self._uncachables:
#             if un in key:
#                 # print "not caching", key
#                 return
#         self.put(key, value)
#         self._cache[key] = value

#     def get(self, key):
#         raise NotImplementedError

#     def put(self, key, value):
#         raise NotImplementedError


# class MemoryStore(Store):
#     """
#     Caches values in-memory, no persistence. 
#     """
#     def put(self, key, value): pass
#     def get(self, key): raise KeyError


# re_file = re.compile(r'\W+')
# class PickleStore(Store):
#     """
#     Pickles values to disk and caches in memory.
#     """
#     def safe_name(self, key):
#         """Create hex name from key"""
#         key_name = re_file.sub('_', key)
#         return '_%s__%s' % (hashlib.md5(key).hexdigest()[:10], key_name[:100])

#     def get_fname(self, key):
#         """Get pickled data path"""
#         return os.path.join(self.path, self.safe_name(key))

#     def put(self, key, value):
#         """Write safe-named data to pickle"""
#         dumppickle(value, self.get_fname(key), protocol=0)

#     def get(self, key):
#         """Load pickled data using key value"""
#         try:
#             return loadpickle(self.get_fname(key))
#         except IOError:
#             raise KeyError


# class HDFPickleStore(PickleStore):
#     """
#     Attempts to store objects in HDF5 format (numpy/pandas objects). Pickles them
#     to disk if that's not possible; also caches values in-memory.
#     """
#     def get_store(self):
#         """HDF store on self.path"""
#         return pandas.HDFStore(os.path.join(self.path, 'ramp.h5'))

#     def put(self, key, value):
#         """Write Pandas DataFrame or Series to HDF store. Other data types
#         will default to pickled storage"""
#         if isinstance(value, pandas.DataFrame) or isinstance(value, pandas.Series):
#             self.get_store()[self.safe_name(key)] = value
#         else:
#             super(HDFPickleStore, self).put(key, value)

#     def get(self, key):
#         """Get data from HDF store. If store does not contain key or data, 
#         will try to load pickled data."""
#         try:
#             return self.get_store()[self.safe_name(key)]
#         except (KeyError, NoSuchNodeError):
#             pass
#         return super(HDFPickleStore, self).get(key)


# class ShelfStore(Store):
#     """
#     Deprecated
#     """
#     def get_store(self):
#         if self._shelf is None:
#             self._shelf = shelve.open(self.path)
#         return self._shelf

#     def delete(self, keypart):
#         s = self.get_store()
#         # TODO: iterating keys is stupid slow for a shelf
#         for k in s.keys():
#             if keypart in k:
#                 if self.verbose:
#                     print "Deleting '%s' from store"%k
#                 del s[k]

#     def put(self, key, value):
#         store = self.get_store()
#         store[key] = value
#         self._cache[key] = value

#     def get(self, key):
#         return self.get_store()[key]


# try:
#     tables
#     default_store = HDFPickleStore
# except NameError:
#     print "Defaulting to basic pickle store. It is recommended \
#           you install PyTables for fast HDF5 format."
#     default_store = PickleStore


########NEW FILE########
__FILENAME__ = test_estimators
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from sklearn import linear_model

from ramp.estimators import (Probabilities,
                             BinaryProbabilities,
                             wrap_sklearn_like_estimator)
from ramp import shortcuts
from ramp.tests.test_features import make_data


class DummyProbEstimator(object):
    def __init__(self, n_clses):
        self.n_clses = n_clses
        self._coefs = "coefs"

    def fit(self, x, y):
        pass

    def predict_proba(self, x):
        return np.zeros((len(x), self.n_clses))


class TestEstimators(unittest.TestCase):
    def setUp(self):
        self.data = make_data(10)

    def test_probabilities(self):
        inner_est = DummyProbEstimator(3)
        est = wrap_sklearn_like_estimator(inner_est)

        # test attr wrap
        self.assertEqual(est._coefs, inner_est._coefs)
        self.assertRaises(AttributeError, getattr, est, 'nope_not_attr')

        preds = est.predict(self.data.values)        
        self.assertEqual(preds.shape, (10, 3))

    def test_binary_probabilities(self):
        inner_est = DummyProbEstimator(2)
        est = wrap_sklearn_like_estimator(inner_est)

        # test attr wrap
        self.assertEqual(est._coefs, inner_est._coefs)

        preds = est.predict(self.data.values)        
        self.assertEqual(preds.shape, (10, ))

    def test_sklearn_probabilities(self):
        # test multi-class
        self.data['target'] = [0] * 5 + [1] * 3 + [2] * 2
        inner_est = linear_model.LogisticRegression()
        est = wrap_sklearn_like_estimator(inner_est)
        x = self.data[['a', 'b']]
        est.fit(x, self.data.target)
        preds = est.predict(x)
        self.assertEqual(preds.shape, (10, 3))

        # test binary, single output
        self.data['target'] = [0] * 5 + [1] * 5
        est = BinaryProbabilities(inner_est)
        x = self.data[['a', 'b']]
        est.fit(x, self.data.target)
        preds = est.predict(x)
        self.assertEqual(preds.shape, (10, ))
        

if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = test_features
import os
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index
from pandas.util.testing import assert_almost_equal
from sklearn import decomposition

from ramp.builders import *
from ramp.features import base, text
from ramp.features.base import *
from ramp.features.trained import *
from ramp.model_definition import ModelDefinition


def strip_hash(s):
    return s[:-11]

def make_data(n):
    data = pd.DataFrame(
               np.random.randn(n, 3),
               columns=['a','b','c'],
               index=range(10, n+10))
    data['const'] = np.zeros(n)
    data['ints'] = range(n)
    data['y'] = data['a'] ** 2
    return data


class TestBasicFeature(unittest.TestCase):
    def setUp(self):
        self.data = make_data(10)

    def test_basefeature_reprs(self):
        f = BaseFeature('col1')
        self.assertFalse(f.is_trained)
        self.assertFalse(f.is_prepped)
        self.assertEqual(f.feature, 'col1')
        self.assertEqual(str(f), 'col1')
        self.assertEqual(repr(f), "'col1'")
        self.assertEqual(f.unique_name, 'col1')

    def test_constantfeature_int_reprs(self):
        f = ConstantFeature(1)
        self.assertFalse(f.is_trained)
        self.assertFalse(f.is_prepped)
        self.assertEqual(f.feature, 1)
        self.assertEqual(str(f), '1')
        self.assertEqual(repr(f), '1')
        self.assertEqual(f.unique_name, '1')

    def test_constantfeature_float_reprs(self):
        f = ConstantFeature(math.e)
        self.assertEqual(f.feature, math.e)
        self.assertEqual(str(f), str(math.e))
        self.assertEqual(repr(f), repr(math.e))
        print f.unique_name
        self.assertEqual(f.unique_name, str(math.e))

    def test_combofeature_reprs(self):
        f = ComboFeature(['col1', 'col2'])
        for sf in f.features:
            self.assertIsInstance(sf, BaseFeature)
        self.assertEqual(str(f), 'Combo(col1, col2)')
        self.assertEqual(f.unique_name, 'Combo(col1, col2) [201b1e5d]')
        self.assertEqual(repr(f), "ComboFeature(_name='Combo',"
        "features=['col1', 'col2'])")

    def test_feature_reprs(self):
        f = Feature('col1')
        self.assertFalse(f.is_trained)
        self.assertFalse(f.is_prepped)
        self.assertIsInstance(f.feature, BaseFeature)
        self.assertEqual(str(f), 'col1')
        self.assertEqual(f.unique_name, 'col1 [4e89804a]')
        self.assertEqual(repr(f), "Feature(_name='',"
        "feature='col1',features=['col1'])")

    def test_basic_feature_chaining(self):
        a_mean = self.data.a.mean()
        f = base.Normalize(base.F(10) + base.F('a'))

        # test build
        res, fitted_feature = f.build(self.data)
        self.assertEqual(len(fitted_feature.prepped_data), 1)
        vals = fitted_feature.prepped_data.values()[0]
        self.assertAlmostEqual(vals[0], 10 + a_mean)
        self.assertAlmostEqual(vals[1], self.data.a.std())

        # test fitted feature
        self.assertEqual(len(fitted_feature.inner_fitted_features), 1)
        iff = fitted_feature.inner_fitted_features[0]
        self.assertEqual(len(iff.inner_fitted_features), 2)
        iiff = iff.inner_fitted_features[0]
        self.assertTrue(iiff.inner_fitted_features[0] is None)

        # test apply
        res = f.apply(self.data, fitted_feature)
        self.assertAlmostEqual(res[res.columns[0]].mean(), 0)
        self.assertAlmostEqual(res[res.columns[0]].std(), 1)

        # test callable functionality
        res = f(self.data, fitted_feature)
        self.assertAlmostEqual(res[res.columns[0]].mean(), 0)
        self.assertAlmostEqual(res[res.columns[0]].std(), 1)

        # test no side-effects
        self.assertAlmostEqual(a_mean, self.data.a.mean())

    def test_basic_feature_prep_index(self):
        a_mean = self.data.a.mean()
        f = base.Normalize(base.F(10) + base.F('a'))
        prep_data = self.data.iloc[range(len(self.data) / 2)]
        self.assertFalse(f.is_trained)
        self.assertTrue(f.is_prepped)

        # test build
        res, fitted_feature = f.build(self.data, prep_index=prep_data.index)
        self.assertEqual(len(fitted_feature.prepped_data), 1)
        vals = fitted_feature.prepped_data.values()[0]
        self.assertAlmostEqual(vals[0], 10 + prep_data.a.mean())
        self.assertAlmostEqual(vals[1], prep_data.a.std())

        # test apply
        res = f.apply(self.data, fitted_feature)
        expected = (self.data.a - vals[0] + 10) / vals[1]
        assert_almost_equal(res[res.columns[0]].values, expected.values)

    def test_feature_builders(self):
        # good features
        features = [base.F(10), base.F('a')]
        featureset, fitted_features = build_featureset_safe(features, self.data)
        self.assertEqual(featureset.shape, (len(self.data), 2))
        featureset = apply_featureset_safe(features, self.data, fitted_features)
        self.assertEqual(featureset.shape, (len(self.data), 2))

        # bad feature, drops data
        class BuggyFeature(base.F):
            def _apply(self, data, fitted_feature):
                return data.iloc[:len(data)/2]
        features = [base.F(10), base.F('a'), BuggyFeature('a')]
        with self.assertRaises(AssertionError):
            featureset, ffs = build_featureset_safe(features, self.data)

        # target
        featureset, fitted_feature = build_target_safe(base.F('a'), self.data)
        self.assertEqual(featureset.shape, (len(self.data), ))
        self.assertTrue(isinstance(featureset, Series))
        featureset = apply_target_safe(base.F('a'), self.data, fitted_feature)
        self.assertEqual(featureset.shape, (len(self.data), ))
        self.assertTrue(isinstance(featureset, Series))


class DummyEstimator(object):
    def __init__(self):
        pass

    def fit(self, x, y):
        self.fitx = x
        self.fity = y

    def predict(self, x):
        self.predictx = x
        p = np.zeros(len(x))
        return p


class DummyCVEstimator(object):
    def __init__(self):
        self.fitx = []
        self.fity = []
        self.predictx = []

    def fit(self, x, y):
        self.fitx.append(x)
        self.fity.append(y)

    def predict(self, x):
        self.predictx.append(x)
        p = np.zeros(len(x))
        return p


class TestTrainedFeature(unittest.TestCase):
    def setUp(self):
        self.data = make_data(10)

    def make_model_def_basic(self):
        features = [F(10), F('a')]
        target = F('b')
        estimator = DummyEstimator()

        model_def = ModelDefinition(features=features,
                                    estimator=estimator,
                                    target=target)
        return model_def

    def test_predictions(self):
        model_def = self.make_model_def_basic()
        f = Predictions(model_def)
        self.assertTrue(f.is_trained)
        self.assertFalse(f.is_prepped)

        r, ff = f.build(self.data)
        r = r[r.columns[0]]
        assert_almost_equal(r.values, np.zeros(len(self.data)))
        fitted_model = ff.trained_data
        #TODO uggh fix this
        print fitted_model.fitted_estimator.fitx
        assert_almost_equal(fitted_model.fitted_estimator.fitx.transpose()[1], self.data['a'].values)
        assert_almost_equal(fitted_model.fitted_estimator.predictx.transpose()[1], self.data['a'].values)

    def test_predictions_held_out(self):
        model_def = self.make_model_def_basic()
        f = Predictions(model_def)
        r, ff = f.build(self.data, train_index=self.data.index[:5])
        r = r[r.columns[0]]
        assert_almost_equal(r.values, np.zeros(len(self.data)))
        fitted_model = ff.trained_data
        assert_almost_equal(fitted_model.fitted_estimator.fitx.transpose()[1], self.data['a'].values[:5])
        assert_almost_equal(fitted_model.fitted_estimator.predictx.transpose()[1], self.data['a'].values)

    def test_residuals(self):
        model_def = self.make_model_def_basic()
        f = Residuals(model_def)
        r, ff = f.build(self.data)
        r = r[r.columns[0]]
        assert_almost_equal(r.values, 0 - self.data['b'].values)
        fitted_model = ff.trained_data
        assert_almost_equal(fitted_model.fitted_estimator.fitx.transpose()[1], self.data['a'].values)
        assert_almost_equal(fitted_model.fitted_estimator.predictx.transpose()[1], self.data['a'].values)

    # def test_predictions_cv(self):
    #     idx = 10
    #     est = DummyCVEstimator()

    #     # make 2 folds
    #     folds = [(self.ctx.train_index[:4], self.ctx.train_index[4:8]),
    #             (self.ctx.train_index[4:8], self.ctx.train_index[:4])]

    #     f = Predictions(
    #             Configuration(target='y', features=[F('a')], model=est), cv_folds=folds)
    #     self.ctx.train_index = self.ctx.train_index[:8]
    #     r = f.create(self.ctx)
    #     r = r[r.columns[0]]

    #     # fit three times, one for each fold, one for held out data
    #     self.assertEqual(len(est.fitx), 3)
    #     assert_almost_equal(est.fitx[0].transpose()[0], self.data['a'].values[:4])
    #     assert_almost_equal(est.fitx[1].transpose()[0], self.data['a'].values[4:8])
    #     assert_almost_equal(est.fitx[2].transpose()[0], self.data['a'].values[:8])

    #     assert_almost_equal(est.predictx[0].transpose()[0], self.data['a'].values[4:8])
    #     assert_almost_equal(est.predictx[1].transpose()[0], self.data['a'].values[:4])
    #     assert_almost_equal(est.predictx[2].transpose()[0], self.data['a'].values[8:])

    def test_target_aggregation_by_factor(self):
        self.data['grp'] = [0] * 5 + [1] * (len(self.data) - 5)
        f = TargetAggregationByFactor(group_by='grp', func=np.mean, target='ints', min_sample=1)
        d, ff = f.build(self.data, train_index=self.data.index[:6])
        keys, vals = ff.trained_data
        self.assertEqual(len(keys), 2)
        self.assertAlmostEqual(vals[0], np.mean(range(5)))
        self.assertAlmostEqual(vals[1], np.mean(5))



def make_text_data(n):
    data = pd.DataFrame(
               columns=['a','b','c'],
               index=range(10, n+10))
    data['a'] = [' '.join([chr(random.randint(97,123))*5 for i in range(5)]) for _ in range(n)]
    data['b'] = [' '.join([chr(random.randint(97,123))*5 for i in range(2)]) for _ in range(n)]
    data['c'] = [' '.join([chr(random.randint(97,123))*2 for i in range(2)]) for _ in range(n)]
    return data


class TestTextFeatures(unittest.TestCase):
    def setUp(self):
        self.n = 1000
        self.data = make_text_data(self.n)

    def test_LSI_topics(self):
        f = text.LSI(text.Tokenizer('a'), num_topics=1)
        feature_data, ff = f.build(self.data)
        self.assertEqual(feature_data.shape, (self.n, 1))

    def test_ngramcounts(self):
        f = text.NgramCounts(text.Tokenizer('a'), mindocs=1)
        feature_data, ff = f.build(self.data)
        self.assertEqual(feature_data.shape, (self.n, 26))


# class TestGroupFeatures(unittest.TestCase):
#     def setUp(self):
#         self.data = make_data(10)
#         self.data['groups'] = self.data['ints'].apply(lambda x: x > 5)
#         self.ctx = context.DataContext(store.MemoryStore(verbose=True), self.data)

#     def test_group_agg_col(self):
#         f = GroupAggregate(['a', 'groups'], function=np.mean, data_column='a',
#                 groupby_column='groups')
#         f.context = self.ctx

#         # test prep data
#         prep = f.get_prep_data(self.data)
#         self.assertEqual(len(prep), 2)
#         self.assertAlmostEqual(prep['global'], self.data['a'].mean())
#         g1_mean = self.data['a'][self.data['groups']].mean()
#         g2_mean = self.data['a'][-self.data['groups']].mean()
#         self.assertAlmostEqual(prep['groups'].get(True), g1_mean)
#         self.assertAlmostEqual(prep['groups'].get(False), g2_mean)

#         # test feature creation
#         data = get_single_column(f.create(self.ctx))
#         expected = Series(index=data.index)
#         expected[self.data['groups']] = g1_mean
#         expected[-self.data['groups']] = g2_mean
#         assert_almost_equal(data, expected)



class TestComboFeatures(unittest.TestCase):
    def setUp(self):
        self.data = make_data(10)

    def test_dim_reduction(self):
        decomposer = decomposition.PCA(n_components=2)
        f = combo.DimensionReduction(['a', 'b', 'c'],
                decomposer=decomposer)
        data, ff = build_feature_safe(f, self.data)
        self.assertEqual(data.shape, (len(self.data), 2))
        decomposer = decomposition.PCA(n_components=2)
        expected = decomposer.fit_transform(self.data[['a', 'b', 'c']])
        assert_almost_equal(expected, data.values)


if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = test_folds
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index

from ramp.features.base import F, Map
from ramp.folds import *
from ramp.utils import *


class TestFolds(unittest.TestCase):

    def test_watertight_folds(self):
        n   = 10000
        n_u = 1000
        r   = 4
        n_folds = 10
        df = pd.DataFrame({'a': np.arange(n), 
                           'u': np.random.randint(1, n_u, n)})
        
        wt_folds = WatertightFolds(n_folds, df, 'u', seed=1)
        folds = list(wt_folds)
        self.assertEqual(len(folds), n_folds)
        te = pd.Index([])
        u_sofar = set()
        for train, test in folds:
            self.assertFalse(train & test)
            self.assertFalse(te & test)
            self.assertEqual(len(u_sofar.intersection(df.loc[test])), 0)
            te = te | test
            u_sofar = u_sofar.union(df.loc[test]['u'])
        # ensure all instances were used in test 
        self.assertEqual(len(set(df['u'])), len(u_sofar))
        self.assertEqual(len(te), n)

    def test_balanced_folds(self):
        n = 100000
        r = 4
        n_folds = 4
        df = pd.DataFrame({'a':np.arange(n), 
                           'y':np.hstack([np.ones(n/r), np.zeros(n/r * (r -1))])})
        balanced_folds = BalancedFolds(n_folds, F('y'), df, seed=1)
        folds = list(balanced_folds)
        self.assertEqual(len(folds), n_folds)
        te = pd.Index([])
        for train, test in folds:
            self.assertFalse(train & test)
            self.assertFalse(te & test)
            te = te | test
            train_y = df.y[train]
            test_y = df.y[test]
            # ensure postive ratios are correct
            pos_ratio = sum(train_y) / float(len(train_y))
            self.assertAlmostEqual(pos_ratio, 1. / r)
            pos_ratio = sum(test_y) / float(len(test_y))
            self.assertAlmostEqual(pos_ratio, 1. / r)
        # ensure all instances were used in test
        self.assertEqual(len(te), n)
    
    def test_bootstrapped_folds(self):
        n = 10000
        r = 4
        n_folds = 10
        ptr = 2000
        pte = 500
        ntr = 6000
        nte = 1000
        df = pd.DataFrame({'a':np.arange(n), 'y':np.hstack([np.ones(n/r), np.zeros(n/r * (r -1))])})
        balanced_folds = BootstrapFolds(n_folds, F('y'), df, seed=1,
                                        pos_train=ptr, neg_train=ntr, pos_test=pte, neg_test=nte)
        folds = list(balanced_folds)
        self.assertEqual(len(folds), n_folds)
        te = set()
        for train, test in folds:
            self.assertFalse(set(train) & set(test))
            te = te | set(train)
            train_y = df.y[train]
            test_y = df.y[test]
            # ensure sizes are correct
            self.assertEqual(len(train_y), ptr + ntr)
            self.assertEqual(len(test_y), pte + nte)
            self.assertEqual(sum(train_y), ptr)
            self.assertEqual(sum(test_y), pte)
        # ensure all instances were used in test (well, almost all)
        self.assertEqual(len(te), 9998) # seed is set

    def test_basic_folds(self):
        n = 10000
        n_folds = 10
        df = pd.DataFrame({'a':np.arange(n)})
        bfolds = BasicFolds(n_folds, df)
        folds = list(bfolds)
        self.assertEqual(len(folds), n_folds)
        te = set()
        for train, test in folds:
            self.assertFalse(set(train) & set(test))
            te = te | set(test)
            train_y = df.loc[train]
            test_y = df.loc[test]
            # ensure sizes are correct
            self.assertEqual(len(train_y), n / float(n_folds) * 9)
            self.assertEqual(len(test_y), n / float(n_folds))
        # ensure all instances were used in test
        self.assertEqual(len(te), n)


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_metrics
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index
from pandas.util.testing import assert_almost_equal

from ramp.builders import *
from ramp.features.base import F, Map
from ramp.metrics import *
from ramp.model_definition import ModelDefinition
from ramp import modeling
from ramp.reporters import *
from ramp.result import Result
from ramp.tests.test_features import make_data


class TestMetrics(unittest.TestCase):
    def setUp(self):
        self.data = make_data(100)
        self.result = Result(self.data, self.data,
                             self.data.y, self.data.y,
                             self.data.y, model_def=None,
                             fitted_model=None, original_data=self.data)

    def test_recall(self):
        self.data['y'] = [0]*20 + [1]*80
        self.data['preds'] = [0]*10 + [.5]*60 + [1]*30
        self.result.y_test = self.data.y
        self.result.y_preds = self.data.preds
        m = Recall()
        thresholds = np.arange(0,1,.1)
        expected = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.375, 0.375, 0.375]
        assert_almost_equal(expected, [m.score(self.result, t) for t in thresholds])

    def test_weighted_recall(self):
        self.data['y'] = [0]*20 + [1]*80
        self.data['weights'] = [0]*50 + [10]*50
        self.data['preds'] = [0]*10 + [.5]*60 + [.8]*30
        self.result.y_test = self.data.y
        self.result.y_preds = self.data.preds
        self.result.original_data = self.data
        m = WeightedRecall(weight_column='weights')
        thresholds = np.arange(0,1,.1)
        #          [ 0.   0.1  0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]
        expected = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, .6, .6, .6, 0]
        actuals = [m.score(self.result, t) for t in thresholds]
        assert_almost_equal(expected, actuals)


class TestMetricReporter(unittest.TestCase):
    def setUp(self):
        self.data = make_data(100)
        self.result = Result(self.data, self.data,
                             self.data.y, self.data.y,
                             self.data.y, model_def=None,
                             fitted_model=None, original_data=self.data)

    def test_metric_reporter(self):
        self.data['y'] = [0]*20 + [1]*80
        self.data['preds'] = [0]*10 + [.5]*60 + [.8]*30
        self.result.y_test = self.data.y
        self.result.y_preds = self.data.preds

        r = MetricReporter(Recall(.7))
        r.update(self.result)
        summary = r.summary_df()
        n_thresh = 1
        self.assertEqual(len(summary), n_thresh)

        r.plot()

    def test_dual_threshold_reporter(self):
        self.data['y'] = [0]*20 + [1]*80
        self.data['preds'] = [0]*10 + [.5]*60 + [.8]*30
        self.result.y_test = self.data.y
        self.result.y_preds = self.data.preds

        r = DualThresholdMetricReporter(Recall(), PositiveRate())
        r.update(self.result)
        summary = r.summary_df()
        n_thresh = 3
        self.assertEqual(len(summary), n_thresh)

        r.plot()


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_modeling
import os
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index
from pandas.util.testing import assert_almost_equal

from ramp.estimators.base import Probabilities
from ramp.features.base import F, Map
from ramp.features.trained import Predictions
from ramp.model_definition import ModelDefinition
from ramp import modeling
from ramp.modeling import (fit_model,
                           cross_validate,
                           build_and_package_model,
                           generate_train)
from ramp.tests.test_features import make_data


class DummyEstimator(object):
    def __init__(self):
        pass

    def fit(self, x, y):
        self.fitx = x
        self.fity = y

    def predict(self, x):
        self.predictx = x
        p = np.zeros(len(x))
        return p


class DummyCVEstimator(object):
    def __init__(self):
        self.fitx = []
        self.fity = []
        self.predictx = []

    def fit(self, x, y):
        self.fitx.append(x)
        self.fity.append(y)

    def predict(self, x):
        self.predictx.append(x)
        p = np.zeros(len(x))
        return p


class DummyProbEstimator(object):
    def __init__(self, n_clses):
        self.n_clses = n_clses

    def fit(self, x, y):
        pass

    def predict_proba(self, x):
        return np.zeros((len(x), self.n_clses))


class TestBasicModeling(unittest.TestCase):
    def setUp(self):
        self.data = make_data(10)

    def make_model_def_basic(self):
        features = [F(10), F('a')]
        target = F('b')
        estimator = DummyEstimator()

        model_def = ModelDefinition(features=features,
                                    estimator=estimator,
                                    target=target)
        return model_def

    def test_fit_model(self):
        model_def = self.make_model_def_basic()
        x, y, fitted_model = fit_model(model_def, self.data)
        fe = fitted_model.fitted_estimator
        self.assertEqual(fe.fitx.shape, x.shape)
        self.assertEqual(fe.fity.shape, y.shape)

    def test_predict(self):
        model_def = self.make_model_def_basic()
        x, y, fitted_model = fit_model(model_def, self.data)
        x, y_true = modeling.generate_test(model_def, self.data[:3], fitted_model)
        y_preds = fitted_model.fitted_estimator.predict(x)
        self.assertEqual(len(x), 3)
        self.assertEqual(len(y_true), 3)
        self.assertEqual(len(y_preds), 3)

    def test_cross_validate(self):
        model_def = self.make_model_def_basic()
        results, reporters  = cross_validate(model_def, self.data, folds=3)
        self.assertEqual(len(results), 3)

    def test_build_and_package_model(self):
        model_def = self.make_model_def_basic()
        desc =  "State-of-the-Art Model"
        pkg = build_and_package_model(model_def, self.data, desc,
                                      train_index=self.data.index[:3])
        self.assertEqual(pkg.data_description, desc)
        self.assertTrue(pkg.fitted_model)

        # and evaluate
        pkg = build_and_package_model(model_def, self.data, desc, evaluate=True,
                                      train_index=self.data.index[:3])
        self.assertEqual(pkg.data_description, desc)
        self.assertTrue(pkg.fitted_model)


class TestNestedModeling(unittest.TestCase):
    def setUp(self):
        self.data = make_data(10)

    def test_predictions_nest(self):
        inner_estimator = DummyEstimator()
        inner_model = ModelDefinition(features=[F('a')],
                                      estimator=inner_estimator,
                                      target=F('b'))
        features = [F('c'), Predictions(inner_model)]
        target = F('b')
        estimator = DummyEstimator()

        model_def = ModelDefinition(features=features,
                                    estimator=estimator,
                                    target=target)

        x, y, fitted_model = fit_model(model_def, self.data, train_index=self.data.index[:5])
        self.assertEqual(fitted_model.fitted_features[1].trained_data.fitted_estimator.fitx.shape, (5, 1))
        self.assertEqual(x.shape, (len(self.data), 2))

        x, y_true = modeling.generate_test(model_def, self.data[:3], fitted_model)
        assert_almost_equal(x[x.columns[1]].values, np.zeros(3))



if __name__ == '__main__':
    unittest.main()



########NEW FILE########
__FILENAME__ = test_model_definition
import os
import pickle
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index
from pandas.util.testing import assert_almost_equal
from sklearn import linear_model

from ramp import estimators
from ramp.features.base import F, Map
from ramp.model_definition import ModelDefinition, model_definition_factory


class ModelDefinitionTest(unittest.TestCase):

    def test_model_def_factory(self):
        base = ModelDefinition(
                features=['a'],
                estimator=estimators.Estimator('dummy'),
                target='y'
                )
        factory = model_definition_factory(base,
            features=[
                ['a','b'],
                ['a','b','c'],
                ['a','b','c','y'],
                ],
            estimator=[
                estimators.Estimator('dummy'),
                estimators.Estimator('dummy2'),
                ]
            )
        mds = list(factory)
        self.assertEqual(len(mds), 6)

    def test_model_def_pickle(self):
        c = ModelDefinition(
                features=['a', F('a'), Map('a', len)],
                estimator=linear_model.LogisticRegression()
                )
        s = pickle.dumps(c)
        c2 = pickle.loads(s)
        self.assertEqual(repr(c), repr(c2))

        # lambdas are not picklable, should fail
        c = ModelDefinition(
                features=['a', F('a'), Map('a', lambda x: len(x))],
                estimator=linear_model.LogisticRegression()
                )
        self.assertRaises(pickle.PicklingError, pickle.dumps, c)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_reporter
import sys
sys.path.append('../..')
import os, random, pickle
import unittest

import numpy as np
import pandas
from pandas.util.testing import assert_almost_equal
from sklearn import linear_model

from ramp.result import *
from ramp.features import *
from ramp.features.base import *
from ramp.metrics import *



class ReporterTest(unittest.TestCase):
    pass
########NEW FILE########
__FILENAME__ = test_selectors
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index
from pandas.util.testing import assert_almost_equal

from ramp.builders import *
from ramp.estimators.base import Probabilities
from ramp.features.base import F, Map
from ramp.features.trained import Predictions
from ramp.model_definition import ModelDefinition
from ramp.selectors import BinaryFeatureSelector
from ramp.tests.test_features import make_data


class TestSelectors(unittest.TestCase):
    def setUp(self):
        self.data = make_data(100)

    def test_binary_selectors(self):
        d = self.data
        d['target'] = [0] * 50 + [1] * 50
        d['good_feature'] = [0] * 35 + [1] * 65
        d['best_feature'] = d['target']
        features = map(F, ['a', 'b', 'good_feature', 'best_feature'])
        selector = BinaryFeatureSelector()
        x, ffs = build_featureset_safe(features, self.data)
        y, ff = build_target_safe(F('target'), self.data)
        cols = selector.select(x, y, 2)
        feature_rank = [F('best_feature'), F('good_feature')]
        self.assertEqual(cols, [f.unique_name for f in feature_rank])

    def test_binary_selectors_multiclass(self):
        d = self.data
        d['target'] = [0] * 50 + [1] * 25 + [2] * 25
        d['good_feature'] = [0] * 35 + [1] * 65
        d['best_feature'] = d['target']
        features = map(F, ['a', 'b', 'good_feature', 'best_feature'])
        selector = BinaryFeatureSelector()
        x, ffs = build_featureset_safe(features, self.data)
        y, ff = build_target_safe(F('target'), self.data)
        cols = selector.select(x, y, 2)
        feature_rank = [F('best_feature'), F('good_feature')]
        self.assertEqual(cols, [f.unique_name for f in feature_rank])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_shortcuts
import os
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index
from pandas.util.testing import assert_almost_equal
from sklearn import linear_model

from ramp.estimators.base import Probabilities
from ramp.features.base import F, Map
from ramp.features.trained import Predictions
from ramp.model_definition import ModelDefinition
from ramp.shortcuts import cross_validate, cv_factory
from ramp.tests.test_features import make_data


class TestShortcuts(unittest.TestCase):
    def setUp(self):
        self.data = make_data(10)

    def test_cross_validate(self):
        results, reporters = cross_validate(self.data, folds=3, 
                                          features = [F(10), F('a')],
                                          target = F('b'),
                                          estimator = linear_model.LinearRegression())
        self.assertEqual(len(results), 3)

    def test_cross_validate_factory(self):
        outcomes = cv_factory(self.data, 
                              folds=3, 
                              features=[[F(10), F('a')]],
                              target=[F('b'), F('a')],
                              estimator=[linear_model.LinearRegression()])
        for i in outcomes:
            self.assertEqual(len(outcomes[i]['results']), 3)

if __name__ == '__main__':
    unittest.main()



########NEW FILE########
__FILENAME__ = test_store
import sys
sys.path.append('../..')
import tempfile
import unittest

import pandas as pd
from pandas import DataFrame
from pandas.util.testing import assert_almost_equal

from ramp.features import F, FittedFeature
from ramp.result import Result
from ramp.store import *


class TestStore(unittest.TestCase):
    def setUp(self):
        self.tmp = tempfile.mkdtemp()

    def test_storable(self):
        f = FittedFeature(F('a'), pd.Index([]), pd.Index([]))
        f.to_pickle(self.tmp + 'tst')

        f2 = FittedFeature.from_pickle(self.tmp + 'tst')
        self.assertEqual(f2.prep_n, f.prep_n)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_utils
import sys
sys.path.append('../..')
import unittest

import numpy as np
import pandas as pd
from pandas import DataFrame, Series, Index
from pandas.util.testing import assert_frame_equal, assert_index_equal

from ramp.features.base import F, Map
from ramp.utils import *


class TestUtils(unittest.TestCase):

    def test_shuffle_df(self):
        n = 100
        df = pd.DataFrame({'a':np.random.rand(n), 
                           'b':np.random.rand(n)})
        df_shuffled = shuffle_df(df)
        self.assertEqual(df.shape, df_shuffled.shape)
        self.assertNotEqual(tuple(df['a']), tuple(df_shuffled['a']))
        self.assertNotEqual(tuple(df['b']), tuple(df_shuffled['b']))
        self.assertNotEqual(tuple(df.index), tuple(df_shuffled.index))
        assert_frame_equal(df.sort(), df_shuffled.sort())

    def test_np_hashes(self):
        a = np.random.randn(20)
        h = get_np_hash(a)
        a[0] = 200000
        h2 = get_np_hash(a)
        self.assertNotEqual(h, h2)
        b = a[0]
        a[0] = b
        self.assertEqual(h2, get_np_hash(a))

    def test_stable_repr(self):
        f = F('test')
        f2 = F('test')
        # equalivalent objects should have same repr
        self.assertEqual(stable_repr(f), stable_repr(f2))

        # repr is independent of object ids
        class Test: pass
        f.f = Test()
        r1 = stable_repr(f)
        f.f = Test()
        r2 = stable_repr(f)
        self.assertEqual(r1, r2)


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = utils
import re
import numpy as np
import random
from hashlib import md5


def _pprint(params):
    """prints object state in stable manner"""
    params_list = list()
    line_sep = ','
    for i, (k, v) in enumerate(sorted(params.iteritems())):
        if type(v) is float:
            # use str for representing floating point numbers
            # this way we get consistent representation across
            # architectures and versions.
            this_repr = '%s=%.10f' % (k, v)
        else:
            # use repr of the rest
            this_repr = '%s=%r' % (k, v)
        params_list.append(this_repr)

    lines = ','.join(params_list)
    return lines


def pprint_scores(scores):
    scores = np.array(scores)
    stderr = scores.std() / np.sqrt(len(scores))
    mn = scores.mean()
    s = "%g (+/- %g) [%g, %g]\n" % (
        mn, stderr, mn - 2 * stderr, mn + 2 * stderr)
    return s


def shuffle_df(df):
    """
    return:
        pandas.DataFrame | shuffled dataframe
    params:
        df: pandas.DataFrame
    """
    return df.reindex(np.random.permutation(df.index))

def make_folds(index, nfolds=5, repeat=1, shuffle=True):
    n = len(index)
    indices = range(n)
    foldsize = n / nfolds
    folds = []
    for i in range(repeat):
        if nfolds == 1:
            folds.append((index, index))
            continue
        if shuffle:
            random.shuffle(indices)
        for i in range(nfolds):
            test = index[indices[i*foldsize:(i + 1)*foldsize]]
            train = index - test
            assert not (train & test)
            folds.append((train, test))
    return folds


def make_subset_prediction_folds(index, sub_pred_index, nfolds=5, repeat=1):
    folds = []
    for train, test in make_folds(index, nfolds, repeat):
        folds.append((train, test & sub_pred_index))
    return folds


def get_np_hash(obj):
    return md5(get_np_hashable(obj)).hexdigest()


def get_np_hashable(obj):
    try:
        return np.getbuffer(obj)
    except TypeError:
        return np.getbuffer(obj.flatten())


def key_from_index(idx):
    return get_np_hash(idx.values)
    

def get_single_column(df):
    assert len(df.columns) == 1
    return df[df.columns[0]]


def reindex_safe(df, idx):
    if df.index.equals(idx):
        # Don't make unnecessary copy of data
        df_idx = df
    else:
        df_idx = df.loc[idx]
        # ditto
        assert len(df_idx) == len(idx)
    return df_idx


re_object_repr = re.compile(r'\sat\s\w+>')

def stable_repr(obj):
    state = _pprint(obj.__getstate__())
    # HACK: replace 'repr's that contain object id references
    state = re_object_repr.sub('>', state)
    return '%s(%s)' % (
            obj.__class__.__name__,
            state)

stop_words = set([
    'http',
    'https',
    'com',
    'www',
    'org',
    'web',
    'url',
    'the',
    'a',
    'and',
    'of',
    'it',
'i'      ,
'a',
'about' ,
'an' ,
'are' ,
'as' ,
'at' ,
'be' ,
'by' ,
'com' ,
'for' ,
'from',
'how',
'in' ,
'is' ,
'it' ,
'of' ,
'on' ,
'or' ,
'that',
'the' ,
'this',
'to' ,
'was' ,
'what' ,
'when',
'where',
'who' ,
'will' ,
'with',
'the',
'www'
    ])

contractions = {"aren't":"are not",
"can't":"cannot",
"couldn't":"could not",
"didn't":"did not",
"doesn't":"does not",
"don't":"do not",
"hadn't":"had not",
"hasn't":"has not",
"haven't":"have not",
"he'd":"he had",
"he'll":"he will",
"he's":"he is",
"i'd":"i had",
"i'll":"i will",
"i'm":"i am",
"i've":"i have",
"isn't":"is not",
"it'll":"it will",
"it's":"it is",
"let's":"let us",
"mightn't":"might not",
"mustn't":"must not",
"shan't":"shall not",
"she'd":"she had",
"she'll":"she will",
"she's":"she is",
"shouldn't":"should not",
"that's":"that is",
"there's":"there is",
"they'd":"they had",
"they'll":"they will",
"they're":"they are",
"they've":"they have",
"we'd":"we had",
"we're":"we are",
"we've":"we have",
"weren't":"were not",
"what'll":"what will",
"what're":"what are",
"what's":"what is",
"what've":"what have",
"where's":"where is",
"who'd":"who had",
"who'll":"who will",
"who're":"who are",
"who's":"who is",
"who've":"who have",
"won't":"will not",
"wouldn't":"would not",
"you'd":"you had",
"you'll":"you will",
"you're":"you are",
"you've":"you have",
}

import math
def cosine(vec1, vec2):
    sim = 0
    v2 = dict(vec2)
    for k,v in vec1:
        sim += v * v2.get(k, 0)
    norm1 = math.sqrt(sum([v*v for tid, v in vec1]))
    norm2 = math.sqrt(sum([v*v for tid, v in vec2]))
    denom = (norm1 * norm2)
    if denom < .000000001:
        return 0
    return sim / denom


def clean_url(u):
    return u.split('?')[0]


splits = re.compile(r'[-/,;]')
poss = re.compile(r"'s\b")
bad = re.compile(r'[^0-9a-zA-Z\s]')
compact = re.compile(r'\s+')
sent = re.compile(r'[.!?]')


def normalize(s):
    s = s.lower()
    s = contractions.get(s, s)
    s = poss.sub('', s)
    s = splits.sub(' ', s)
    s = bad.sub('', s)
    return s.strip()


def tokenize(s):
    return [w for w in normalize(s).split() if w not in stop_words and len(w) > 1]


def tokenize_keep_all(s):
    return [w for w in normalize(s).split() if w]


def tokenize_with_sentinels(s):
    s = sent.sub(' SSENTT ', s)
    return [w for w in normalize(s).split() if w]


def bag_of_words(s):
    words = tokenize(s)
    bag = {}
    for word in words:
        if word in bag:
            bag[word] += 1
        else:
            bag[word] = 1
    # n = float(len(words))
    # for k, v in bag.items():
    #     bag[k] = v/n
    return bag


def add_terms(s):
    for t in tokenize(s):
        if len(t) < 3 or t in stop_words:
            continue
        if t in terms:
            terms[t] += 1
        else:
            terms[t] = 1


########NEW FILE########
