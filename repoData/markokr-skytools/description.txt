skytools-3.0 for Debian
-----------------------

The skytools package for 3.0 has been reworked and split into a number of
packages:

skytools3              Skytool's replication and queuing
python-pgq3            Skytool's PGQ python library
python-skytools3       python scripts framework for skytools
skytools-ticker3       PGQ ticker daemon service
skytools-walmgr3       high-availability archive and restore commands
postgresql-8.4-pgq3    PGQ server-side code (C module for PostgreSQL)
postgresql-9.0-pgq3    PGQ server-side code (C module for PostgreSQL)

You can install your script in /etc/skytools/*.ini and the skytools package
will try to start them automatically, using scriptmgr.  Of course you still
need to install pgq for ticker services and londiste for replication.

 -- Dimitri Fontaine <dfontaine@hi-media-techno.com>, Wed,  6 Apr 2011 17:07:35 +0200

skytools-3.0 for Debian
-----------------------

This package is maintained in git and uses a submodule.  To get a fresh
checkout and build the packages, follow those steps:

  ## fetch git tree, from dimitri who maintains the debian package
  ## real upstream is at git://github.com/markokr/skytools-dev.git
  $ git clone http://github.com/dimitri/skytools.git

  ## fetch libusual submodule
  $ git submodule update --init

  ## now build
  $ debuild ...

= SkyTools - tools for PostgreSQL =

This is a package of tools in use in Skype for replication and failover.
It also includes a generic queuing mechanism called PgQ and a utility
library for Python scripts, as well as a script for setting up and
managing WAL based standby servers.

== Overview ==

It contains the following modules:

=== PgQ ===

PgQ is a queuing system written in PL/pgSQL, Python and C code. It is
based on snapshot-based event handling ideas from Slony-I, and is
written for general usage.

PgQ provides an efficient, transactional, queueing system with
multi-node support (including work sharing and splitting, failover and
switchover, for queues and for consumers).

Rules:

- There can be several queues in a database.
- There can be several producers than can insert into any queue.
- There can be several consumers on one queue.
- There can be several subconsumers on a consumer.

PgQ is split into 3 layers: Producers, Ticker and Consumers.

*Producers* and *Consumers* respectively push and read events into a
queue. Producers just need to call PostgreSQL stored procedures (like a
trigger on a table or a PostgreSQL call from the application).
Consumers are frequently written in Python (the preferred language as it
has a powerful Skytools Framework), but are not limited to Python; any
language able to run PostgreSQL stored procedures can be used.

*Ticker* is a daemon which splits the queues into batchs of events and
handle the maintenance of the system. The Ticker is provided with
Skytools.

Documentation:

- PgQ ticker daemon (pgqd) usage: link:doc/pgqd.html[]
- PgQ admin tool (qadm) usage: link:doc/qadmin.html[]
- PgQ SQL API overview: link:doc/pgq-sql.html[]
- PgQ SQL reference: link:pgq/[]

=== Londiste ===

Replication tool written in Python, using PgQ as event transport.

Features:

- Tables can be added one-by-one into set.
- Initial COPY for one table does not block event replay for other tables.
- Can compare tables on both sides.

Documentation:

- Londiste script usage: doc/londiste3.txt
  (also available as `man 1 londiste`)

- Londiste HOWTOs: doc/howto/

=== walmgr ===

This script will setup WAL archiving, does the initial backup, and
runtime WAL archive and restore.

It can also be used for up-to-last-second partial file copying,
so that less than the whole file is lost in case of loss of the master
database server.

== Source tree contents ==

doc/::
    Documentation in AsciiDoc format.  Source for both html and man pages.

python/::
    Python modules and primary executables - walmgr, londiste, qadmin, pgqadm.

python/pgq/::
    Python framework for PgQ.

python/londiste/::
    Londiste replication.

python/skytools/::
    Low-level utilities for writing database scripts in Python.

sql/::
    Database modules.

sql/pgq/::
    Table definitions and functions for PgQ queueing.

sql/pgq_node/::
    Framework for cascaded consuming.

sql/pgq_coop/::
    Functions for cooperative consuming.

sql/londiste/::
    Table definitions and functions for Londiste replication.

sql/ticker/::
    PgQ ticker written in C.

scripts/::
    Python scripts with lesser priority.

lib/::
    libusual C libary, for pgqd.

debian/::
    Debian packaging.  This is for creating private packages,
    official Debian packages use their own packaging code.

misc/::
    Random scripts used for building.

== Upgrade from 2.1 ==

Assuming PgQ + Londiste setup.  This will upgrade PgQ to 3.0 and install
Londiste 3 in parallel with Londiste 2.

1. Install Postgres modules.  They are backwards compatible with 2.1.
2. Stop `pgqadm.py ticker` processes.
3. Apply pgq.upgrade_2.1_to_3.0.sql
3. Apply pgq.upgrade.sql
4. Apply pgq_node.sql
5. Apply londiste.sql - this will throw error on CREATE SCHEMA, but should otherwise apply fine.
6. Start pgqd.

The files mentioned above are installed under $PREFIX/share/skytools3/ directory.


Merge function to be used with londiste 'applyfn' handler.

londiste3 add-table foo --handler=applyfn --handler-arg="func_name=merge_on_time" --handler-arg="func_conf=timefield=modified_date"



logtriga - generic table changes logger
=======================================

logtriga provides generic table changes logging trigger.
It prepares partial SQL statement about a change and
gives it to user query.

Usage
-----

   CREATE TRIGGER foo_log AFTER INSERT OR UPDATE OR DELETE ON foo_tbl
   FOR EACH ROW EXECUTE PROCEDURE logtriga(column_types, query);

Where column_types is a string where each charater defines type of
that column.  Known types:

 * k - one of primary key columns for table.
 * v - data column
 * i - uninteresting column, to be ignored.

Trigger function prepares 2 string arguments for query and executes it.

 * $1 - Operation type: I/U/D.
 * $2 - Partial SQL for event playback.

   * INSERT INTO FOO_TBL (field, list) values (val1, val2)
   * UPDATE FOO_TBL SET field1 = val1, field2 = val2 where key1 = kval1
   * DELETE FROM FOO_TBL WHERE key1 = keyval1

The upper-case part is left out.

Example
-------

Following query emulates Slony-I behaviour:

   insert into SL_SCHEMA.sl_log_1
          (log_origin, log_xid, log_tableid,
           log_actionseq, log_cmdtype, log_cmddata)
   values (CLUSTER_IDENT, SL_SCHEMA.getCurrentXid(), TABLE_OID,
           nextval('SL_SCHEMA.sl_action_seq'), $1, $2)

The upper-case strings should be replaced with actual values
on trigger creation.




Schema overview
===============

pgq.consumer		consumer name <> id mapping
pgq.queue		queue information
pgq.subscription	consumer registrations
pgq.tick		snapshots that group events into batches
pgq.retry_queue		events to be retried
pgq.failed_queue	events that have failed
pgq.event_*		data tables



Track processed batches and events in target DB
================================================

Batch tracking is OK.

Event tracking is OK if consumer does not use retry queue.

Batch tracking
--------------

is_batch_done(consumer, batch)

returns:

  true - batch is done already
  false - batch is not done yet

set_batch_done(consumer, batch)

returns:

  true - tagging successful, batch was not done yet
  false - batch was done already

Event tracking
--------------

is_batch_done(consumer, batch, event)

returns:

  true - event is done
  false - event is not done yet


set_batch_done(consumer, batch, event)

returns:

  true - tagging was successful, event was not done
  false - event is done already


Fastvacuum
----------

pgq.ext.completed_batch
pgq.ext.completed_event
pgq.ext.completed_tick
pgq.ext.partial_batch



txid - 8 byte transaction ID's
==============================

Based on xxid module from Slony-I.  

The goal is to make PostgreSQL internal transaction ID and snapshot
data usable externally.  They cannot be used directly as the
internal 4-byte value wraps around and thus breaks indexing.

This module extends the internal value with wraparound cound (epoch).
It uses relaxed method for wraparound check.  There is a table
txid.epoch (epoch, last_value) which is used to check if the xid
is in current, next or previous epoch.  It requires only occasional
read-write access - ca. after 100k - 500k transactions.

Also it contains type 'txid_snapshot' and following functions:


txid_current() returns int8

  Current transaction ID

txid_current_snapshot() returns txid_snapshot

  Current snapshot

txid_snapshot_xmin( snap ) returns int8

  Smallest TXID in snapshot.  TXID's smaller than this
  are all visible in snapshot.

txid_snapshot_xmax( snap ) returns int8

  Largest TXID in snapshot.  TXID's starting from this one are
  all invisible in snapshot.
	
txid_snapshot_xip( snap ) setof int8

  List of uncommitted TXID's in snapshot, that are invisible
  in snapshot.  Values are between xmin and xmax.

txid_visible_in_snapshot(id, snap) returns bool

  Is TXID visible in snapshot?


Problems
--------

- it breaks when there are more than 2G tx'es between calls.
  Fixed in 8.2
 
- functions that create new txid's should be 'security definers'
  thus better protecting txid_epoch table.

- After loading database from backup you should do:

  UPDATE txid.epoch SET epoch = epoch + 1,
                        last_value = (get_current_txid() & 4294967295);


