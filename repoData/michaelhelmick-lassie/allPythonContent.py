__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Lassie documentation build configuration file, created by
# sphinx-quickstart on Fri Aug  2 00:23:04 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))
sys.path.insert(0, os.path.abspath('..'))

import lassie

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Lassie'
copyright = u'2013, Mike Helmick, Alexander Shibble'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.4.0'
# The full version, including alpha/beta/rc tags.
release = '0.4.0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Lassiedoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Lassie.tex', u'Lassie Documentation',
   u'Mike Helmick, AJ Shibble', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'lassie', u'Lassie Documentation',
     [u'Mike Helmick, AJ Shibble'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Lassie', u'Lassie Documentation',
   u'Mike Helmick, AJ Shibble', 'Lassie', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False

########NEW FILE########
__FILENAME__ = api
# -*- coding: utf-8 -*-

"""
lassie.api
~~~~~~~~~~

This module implements the Lassie API.

"""

from .core import Lassie

def fetch(url, **kwargs):
    """Constructs and sends a :class:`Lassie <Lassie>`
    Retrieves content from the specified url, parses it, and returns
    a beautifully crafted dictionary of important information about that
    web page.

    Priority tree is as follows:
        1. Open Graph
        2. Twitter Card
        3. Other meta content (i.e. description, keywords)

    :param url: URL to send a GET request to
    :param open_graph: (optional) If ``True``, filters web page content for Open Graph meta tags. The content of these properties have top priority on return values.
    :type open_graph: bool
    :param twitter_card: (optional) If ``True``, filters web page content for Twitter Card meta tags
    :type twitter_card: bool
    :param touch_icon: (optional) If ``True``, retrieves Apple touch icons and includes them in the response ``images`` array
    :type touch_icon: bool
    :param favicon: (optional) If ``True``, retrieves any favicon images and includes them in the response ``images`` array
    :type favicon: bool
    :param all_images: (optional) If ``True``, retrieves images inside web pages body and includes them in the response ``images`` array. Default: False
    :type all_images: bool
    :param parser: (optional) String reference for the parser that BeautifulSoup will use
    :type parser: string

    """
    l = Lassie()
    return l.fetch(url, **kwargs)

########NEW FILE########
__FILENAME__ = compat
# -*- coding: utf-8 -*-

"""
lassie.compat
~~~~~~~~~~~~~

This module contains imports and declarations for seamless Python 2 and
Python 3 compatibility.
"""

import sys

_ver = sys.version_info

#: Python 2.x?
is_py2 = (_ver[0] == 2)

#: Python 3.x?
is_py3 = (_ver[0] == 3)

if is_py2:
    from urlparse import urljoin, urlparse

    str = unicode

elif is_py3:
    from urllib.parse import urljoin, urlparse

    str = str

########NEW FILE########
__FILENAME__ = core
# -*- coding: utf-8 -*-

"""
lassie.core
~~~~~~~~~~~

This module contains a Lassie object to maintain settings across lassie.

"""

from bs4 import BeautifulSoup
import requests

from .compat import urljoin
from .exceptions import LassieError
from .filters import FILTER_MAPS
from .utils import clean_text, convert_to_int, normalize_locale


REQUEST_OPTS = {
    'client': ('cert', 'headers', 'hooks', 'max_redirects', 'proxies'),
    'request': ('timeout', 'allow_redirects', 'stream', 'verify'),
}


def merge_settings(fetch_setting, class_setting):
    """Merge settings for ``fetch``, method params have priority."""
    if fetch_setting is None:
        return class_setting

    if class_setting is None:
        return fetch_setting

    return fetch_setting


class Lassie(object):
    __attrs__ = [
        'open_graph', 'twitter_card', 'touch_icon', 'favicon',
        'all_images', 'parser', '_retreive_content', 'client'
    ]

    def __init__(self):
        """Instantiates an instance of Lassie."""
        self.open_graph = True
        self.twitter_card = True
        self.touch_icon = True
        self.favicon = True
        self.all_images = False
        self.parser = 'html5lib'
        self._request_opts = {}
        self.client = requests.Session()

    @property
    def request_opts(self):
        return self._request_opts

    @request_opts.setter
    def request_opts(self, _dict):
        for k, v in _dict.items():
            if (k in REQUEST_OPTS['client'] or k in REQUEST_OPTS['request']):
                self._request_opts[k] = v
            if k in REQUEST_OPTS['client']:
                setattr(self.client, k, v)


    def __repr__(self):
        return '<Lassie [parser: %s]>' % (self.parser)

    def fetch(self, url, open_graph=None, twitter_card=None, touch_icon=None,
              favicon=None, all_images=None, parser=None):
        """Retrieves content from the specified url, parses it, and returns
        a beautifully crafted dictionary of important information about that
        web page.

        Priority tree is as follows:
            1. Open Graph
            2. Twitter Card
            3. Other meta content (i.e. description, keywords)

        :param url: URL to send a GET request to
        :param open_graph: (optional) If ``True``, filters web page content for Open Graph meta tags. The content of these properties have top priority on return values.
        :type open_graph: bool
        :param twitter_card: (optional) If ``True``, filters web page content for Twitter Card meta tags
        :type twitter_card: bool
        :param touch_icon: (optional) If ``True``, retrieves Apple touch icons and includes them in the response ``images`` array
        :type touch_icon: bool
        :param favicon: (optional) If ``True``, retrieves any favicon images and includes them in the response ``images`` array
        :type favicon: bool
        :param all_images: (optional) If ``True``, retrieves images inside web pages body and includes them in the response ``images`` array. Default: False
        :type all_images: bool
        :param parser: (optional) String reference for the parser that BeautifulSoup will use
        :type parser: string

        """

        # Set params, method params have priority over class params
        open_graph = merge_settings(open_graph, self.open_graph)
        twitter_card = merge_settings(twitter_card, self.twitter_card)
        touch_icon = merge_settings(touch_icon, self.touch_icon)
        favicon = merge_settings(favicon, self.favicon)
        all_images = merge_settings(all_images, self.all_images)
        parser = merge_settings(parser, self.parser)

        html = self._retreive_content(url)
        if not html:
            raise LassieError('There was no content to parse.')

        soup = BeautifulSoup(clean_text(html), parser)

        data = {
            'images': [],
            'videos': [],
        }

        if open_graph:
            data.update(self._filter_meta_data('open_graph', soup, data, url))

        if twitter_card:
            data.update(self._filter_meta_data('twitter_card', soup, data))

        data.update(self._filter_meta_data('generic', soup, data))

        if touch_icon:
            data.update(self._filter_link_tag_data('touch_icon', soup, data, url))

        if favicon:
            data.update(self._filter_link_tag_data('favicon', soup, data, url))

        if all_images:
            # Maybe filter out 1x1, no "good" way to do this if image doesn't supply width/height
            data.update(self._find_all_images(soup, data, url))

        # TODO: Find a good place for setting url, title and locale
        lang = soup.html.get('lang') if soup.html.get('lang') else soup.html.get('xml:lang')
        if lang and (not 'locale' in data):
            locale = normalize_locale(lang)
            if locale:
                data['locale'] = locale

        if not 'url' in data:
            data['url'] = url

        if not 'title' in data and hasattr(soup.title, 'string'):
            data['title'] = soup.title.string

        return data

    def _retreive_content(self, url):  # pragma: no cover
        try:
            request_kwargs = {}
            for k, v in self._request_opts.items():
                if k in REQUEST_OPTS['request']:
                    # Set request specific kwarg
                    request_kwargs[k] = v

            response = self.client.get(url, **request_kwargs)
        except requests.exceptions.RequestException as e:
            raise LassieError(e)
        else:
            return response.text

    def _filter_meta_data(self, source, soup, data, url=None):
        """This method filters the web page content for meta tags that match patterns given in the ``FILTER_MAPS``

        :param source: The key of the meta dictionary in ``FILTER_MAPS['meta']``
        :type source: string
        :param soup: BeautifulSoup instance to find meta tags
        :type soup: instance
        :param data: The response dictionary to manipulate
        :type data: (dict)

        """
        meta = FILTER_MAPS['meta'][source]
        meta_map = meta['map']

        html = soup.find_all('meta', {meta['key']: meta['pattern']})

        image = {}
        video = {}

        for line in html:
            prop = line.get(meta['key'])
            value = line.get('content')

            if prop in meta_map and not meta_map[prop] in data:
                # this could be bad in cases where any values that the property
                # is mapped up to (i.e. "src", "type", etc) are found in ``data``
                # TODO: Figure out a smoother way to prevent conflicts ^^^^^^^^
                image_prop = meta['image_key']
                video_prop = meta['video_key']

                if prop.startswith((image_prop, video_prop)) and \
                prop.endswith(('width', 'height')):
                    if prop.endswith(('width', 'height')):
                        value = convert_to_int(value)

                if meta_map[prop] == 'locale':
                    locale = normalize_locale(value)
                    if locale:
                        data['locale'] = locale

                if prop == 'keywords':
                    value = [v.strip() for v in value.split(',')]

                if image_prop and prop.startswith(image_prop) and value:
                    # og:image URLs can be relative
                    if prop == 'og:image' and url:
                        value = urljoin(url, value)
                    image[meta_map[prop]] = value
                elif video_prop and prop.startswith(video_prop) and value:
                    video[meta_map[prop]] = value
                else:
                    data[meta_map[prop]] = value

        if image:
            image['type'] = image_prop
            data['images'].append(image)
        if video:
            data['videos'].append(video)

        return data

    def _filter_link_tag_data(self, source, soup, data, url):
        """This method filters the web page content for link tags that match patterns given in the ``FILTER_MAPS``

        :param source: The key of the meta dictionary in ``FILTER_MAPS['link']``
        :type source: string
        :param soup: BeautifulSoup instance to find meta tags
        :type soup: instance
        :param data: The response dictionary to manipulate
        :type data: (dict)
        :param url: URL used for making an absolute url
        :type url: string

        """
        link = FILTER_MAPS['link'][source]

        html = soup.find_all('link', {link['key']: link['pattern']})

        for line in html:
            data['images'].append({
                'src': urljoin(url, line.get('href')),
                'type': link['type'],
            })

        return data

    def _find_all_images(self, soup, data, url):
        """This method finds all images in the web page content

        :param soup: BeautifulSoup instance to find meta tags
        :type soup: instance
        :param data: The response dictionary to manipulate
        :type data: (dict)

        """
        all_images = soup.findAll('img')
        for image in all_images:
            # Create image list then remove duplicate images?
            img = {
                'src': urljoin(url, image.get('src')),
                'alt': image.get('alt', ''),
                'type': u'body_image',
            }

            # Only include width and height if included as an attribute of the element
            width = convert_to_int(image.get('width'))
            if width:
                img['width'] = width

            height = convert_to_int(image.get('height'))
            if height:
                img['height'] = height

            data['images'].append(img)

        return data

########NEW FILE########
__FILENAME__ = exceptions
# -*- coding: utf-8 -*-

"""
lassie.exceptions
~~~~~~~~~~~~~~~~~

This module contains the set of Lassie exceptions.

"""

class LassieError(Exception):
    """Generic catch-all Exceptions"""
    pass

########NEW FILE########
__FILENAME__ = apple
# -*- coding: utf-8 -*-

"""
lassie.filters.apple
~~~~~~~~~~~~~~~~~~~~

This module contains Apple related content to help Lassie filter for content.

"""

from ..compat import str

import re

APPLE_MAPS = {  # http://i.imgur.com/cla85xT.jpg
    'link': {
        'touch_icon': {
            'pattern': re.compile(r"^(apple-touch-icon|apple-touch-icon-precomposed)", re.I),
            'key': 'icon',
            'type': str('touch_icon'),
        },
    }
}

########NEW FILE########
__FILENAME__ = generic
# -*- coding: utf-8 -*-

"""
lassie.filters.generic
~~~~~~~~~~~~~~~~~~~~~~

This module contains data about generic type content to help Lassie filter for content.

"""

from ..compat import str

import re

GENERIC_MAPS = {
    'meta': {
        'generic': {
            'pattern': re.compile(r"^(description|keywords|title)", re.I),
            'map': {
                'description': 'description',
                'keywords': 'keywords',
                'title': 'title',
            },
            'image_key': '',
            'video_key': '',
            'key': 'name',
        },
    },
    'link': {
        'favicon': {
            'pattern': 'icon',
            'key': 'rel',
            'type': str('favicon'),
        },
    },
}

########NEW FILE########
__FILENAME__ = social
# -*- coding: utf-8 -*-

"""
lassie.filters.social
~~~~~~~~~~~~~~~~~~~~~

This module contains data social related content to help Lassie filter for content.

"""

from ..compat import str

import re

SOCIAL_MAPS = {
    'meta': {
        'open_graph': {  # http://ogp.me/
            'pattern': re.compile(r"^og:", re.I),
            'map': {
                'og:url': 'url',
                'og:title': 'title',
                'og:description': 'description',
                'og:locale': 'locale',

                'og:image': 'src',
                'og:image:width': 'width',
                'og:image:height': 'height',

                'og:video': 'src',
                'og:video:width': 'width',
                'og:video:height': 'height',
                'og:video:type': 'type',
            },
            'image_key': str('og:image'),
            'video_key': str('og:video'),
            'key': 'property',
        },
        'twitter_card': {  # https://dev.twitter.com/docs/cards
            'pattern': re.compile(r"^twitter:", re.I),
            'map': {
                'twitter:url': 'url',
                'twitter:title': 'title',
                'twitter:description': 'description',

                'twitter:image': 'src',
                'twitter:image:width': 'width',
                'twitter:image:height': 'height',

                'twitter:player': 'src',
                'twitter:player:width': 'width',
                'twitter:player:height': 'height',
                'twitter:player:content_type': 'type',
            },
            'image_key': str('twitter:image'),
            'video_key': str('twitter:player'),
            'key': 'name',
        },
    }
}

########NEW FILE########
__FILENAME__ = utils
# -*- coding: utf-8 -*-

"""
lassie.helpers
~~~~~~~~~~~~~~

This module contains the set of helper functions executed by Lassie methods.

"""

from .compat import str

import locale
import re

CLEANER = re.compile(r'[\r\n\t]')
RE_INT = re.compile(r'\d+')

def clean_text(value):
    """Removes all line breaks, new lines and tabs from the specified content

    :param value: Content to be cleansed
    :type value: string

    """
    return CLEANER.sub('', value)

def convert_to_int(value):
    """Attempts to convert a specified value to an integer

    :param value: Content to be converted into an integer
    :type value: string or int

    """
    if not value:
        return None

    # Apart from numbers also accept values that end with px
    value = value.strip(' px')
    try:
        return int(value)
    except (TypeError, ValueError):
        return None

def normalize_locale(value):
    value = value.replace('-', '_')
    the_locale = locale.normalize(value)

    if the_locale != value:
        # Should we return the actual locale, returned from the locale lib instead of splitting?
        try:
            return str(the_locale.split('.')[0])
        except IndexError:  # pragma: no cover
            pass
    return None


########NEW FILE########
__FILENAME__ = base
from lassie.core import Lassie
from lassie.compat import urlparse

from mock import patch
import unittest


def _mock_retreive_content(mock, url):
    filename = urlparse(url).path
    file = open('./templates%s' % filename, 'r')
    html = file.read()
    file.close()

    return html


class LassieBaseTestCase(unittest.TestCase):
    def setUp(self):
        self.patch = patch.object(Lassie, '_retreive_content', _mock_retreive_content)
        self.patch.start()

    def tearDown(self):
        self.patch.stop()

########NEW FILE########
__FILENAME__ = test_core
from .base import LassieBaseTestCase

from lassie import Lassie, LassieError


class LassieCoreTestCase(LassieBaseTestCase):
    def test_core_class_vs_method_settings(self):
        url = 'http://lassie.it/core/class_vs_method_settings.html'

        l = Lassie()
        data = l.fetch(url)

        self.assertEqual(len(data['images']), 1)

        l.open_graph = False
        data = l.fetch(url)

        # open_graph is set to False so there shouldn't be any images in the list this time around
        self.assertEqual(len(data['images']), 0)

    def test_core_class_setting_is_none(self):
        url = 'http://lassie.it/core/class_setting_is_none.html'

        # This is a really odd use-case where they'd set the class attr to None, but it might happen so oh wellz.
        l = Lassie()
        l.open_graph = None
        data = l.fetch(url, open_graph=False)

        self.assertEqual(len(data['images']), 0)

    def test_core_no_content_raises_error(self):
        url = 'http://lassie.it/core/empty.html'

        l = Lassie()
        self.assertRaises(LassieError, l.fetch, url)

    def test_core_retrieve_all_images(self):
        url = 'http://lassie.it/core/retrieve_all_images.html'

        l = Lassie()
        l.all_images = True

        data = l.fetch(url)
        self.assertEqual(len(data['images']), 3)

        last_image = data['images'][2]
        self.assertEqual(last_image['width'], 550)
        self.assertEqual(last_image['height'], 365)

    def test_image_dimensions(self):
        url = 'http://lassie.it/core/image_dimensions.html'

        l = Lassie()
        data = l.fetch(url, all_images=True)

        self.assertEqual(len(data['images']), 4)

        image = data['images'][0]
        self.assertEqual(image['width'], 100)
        self.assertEqual(image['height'], 100)

        image = data['images'][1]
        self.assertEqual(image['width'], 100)
        self.assertEqual(image['height'], 100)

        image = data['images'][2]
        self.assertEqual(image['width'], 100)
        self.assertEqual(image['height'], 100)

        image = data['images'][3]
        self.assertEqual(image['width'], 100)
        self.assertEqual(image['height'], 100)

    def test_bad_image_dimensions(self):
        url = 'http://lassie.it/core/bad_image_dimensions.html'

        l = Lassie()
        data = l.fetch(url, all_images=True)

        # lassie.utils.convert_to_int will except a TypeError or ValueError and pass (not setting a width/height on the image)
        image = data['images'][0]
        self.assertTrue(not 'width' in image)
        self.assertTrue(not 'height' in image)

    def test_request_opts(self):
        l = Lassie()
        l.request_opts = {
            'headers': {
                'User-Agent': 'lassie python',
            },
            'timeout': 3
        }

        self.assertTrue(set(('headers', 'timeout')).issubset(l.request_opts))

        # If they modify one of the keys value, make sure it actually happened
        l.request_opts['headers'].update({'Content-Type': 'application/json'})
        self.assertEqual(len(l.request_opts['headers']), 2)
        self.assertTrue(set(('User-Agent', 'Content-Type')).issubset(l.request_opts['headers']))

    def test_bad_request_opts(self):
        l = Lassie()
        l.request_opts = {
            'bad_key': True,
            'headers': {
                'User-Agent': 'lassie python'
            }
        }

        self.assertTrue('bad_key' not in l.request_opts)
        self.assertTrue('headers' in l.request_opts)

########NEW FILE########
__FILENAME__ = test_generic
from .base import LassieBaseTestCase

import lassie


class LassieTwitterCardTestCase(LassieBaseTestCase):
    def test_generic_all_properties(self):
        url = 'http://lassie.it/generic/all_properties.html'
        data = lassie.fetch(url)

        self.assertEqual(data['locale'], 'en_US')
        self.assertEqual(data['title'], 'Lassie Generic Test | all_properties')
        self.assertEqual(data['description'], 'Just a random description of a web page.')
        self.assertEqual(len(data['keywords']), 5)

    def test_generic_bad_locale(self):
        url = 'http://lassie.it/generic/bad_locale.html'
        data = lassie.fetch(url)

        self.assertTrue(not 'locale' in data)

    def test_generic_favicon(self):
        url = 'http://lassie.it/generic/favicon.html'
        data = lassie.fetch(url)

        self.assertEqual(len(data['images']), 1)
        image = data['images'][0]

        self.assertEqual(image['type'], 'favicon')

    def test_no_title(self):
        url = 'http://lassie.it/generic/no_title.html'
        data = lassie.fetch(url)

        self.assertTrue(not 'title' in data)

########NEW FILE########
__FILENAME__ = test_open_graph
from .base import LassieBaseTestCase

import lassie


class LassieOpenGraphTestCase(LassieBaseTestCase):
    def test_open_graph_all_properties(self):
        url = 'http://lassie.it/open_graph/all_properties.html'
        data = lassie.fetch(url)

        self.assertEqual(data['url'], url)
        self.assertEqual(data['title'], 'Lassie Open Graph All Properies Test')
        self.assertEqual(data['description'], 'Just a test template with OG data!')
        self.assertEqual(data['locale'], 'en_US')

        self.assertEqual(len(data['images']), 1)
        image = data['images'][0]
        self.assertEqual(image['src'], 'http://i.imgur.com/cvoR7zv.jpg')
        self.assertEqual(image['width'], 550)
        self.assertEqual(image['height'], 365)
        self.assertEqual(image['type'], 'og:image')

        self.assertEqual(len(data['videos']), 1)
        video = data['videos'][0]
        self.assertEqual(video['src'], 'http://www.youtube.com/v/dQw4w9WgXcQ?version=3&autohide=1')
        self.assertEqual(video['width'], 640)
        self.assertEqual(video['height'], 480)
        self.assertEqual(video['type'], 'application/x-shockwave-flash')

    def test_open_graph_no_og_title_no_og_url(self):
        url = 'http://lassie.it/open_graph/no_og_title_no_og_url.html'
        data = lassie.fetch(url)

        self.assertEqual(data['url'], url)
        self.assertEqual(data['title'], 'Lassie Open Graph Test | No og:title, No og:url')

    def test_open_graph_og_image_plus_two_body_images(self):
        url = 'http://lassie.it/open_graph/og_image_plus_two_body_images.html'
        data = lassie.fetch(url)

        # Try without passing "all_images", then pass it

        self.assertEqual(len(data['images']), 1)

        data = lassie.fetch(url, all_images=True)

        self.assertEqual(len(data['images']), 3)

        image_0 = data['images'][0]
        image_1 = data['images'][1]
        image_2 = data['images'][2]
        self.assertEqual(image_0['type'], 'og:image')
        self.assertEqual(image_1['type'], 'body_image')
        self.assertEqual(image_2['type'], 'body_image')

    def test_open_graph_og_image_relative_url(self):
        url = 'http://lassie.it/open_graph/og_image_relative_url.html'
        data = lassie.fetch(url)

        self.assertEqual(
            data['images'][0]['src'], 'http://lassie.it/open_graph/name.jpg')
########NEW FILE########
__FILENAME__ = test_twitter_card
from .base import LassieBaseTestCase

import lassie


class LassieTwitterCardTestCase(LassieBaseTestCase):
    def test_twitter_all_properties(self):
        url = 'http://lassie.it/twitter_card/all_properties.html'
        data = lassie.fetch(url)
        self.assertEqual(data['url'], 'http://www.youtube.com/watch?v=fWNaR-rxAic')
        self.assertEqual(data['title'], 'Carly Rae Jepsen - Call Me Maybe')
        self.assertEqual(data['description'], 'Buy Now! http://smarturl.it/CallMeMaybe Music video by Carly Rae Jepsen performing Call Me Maybe. (C) 2011 604 Records Inc. #VEVOCertified on June 8, 2012. h...')

        self.assertEqual(len(data['images']), 1)
        image = data['images'][0]
        self.assertEqual(image['src'], 'http://i1.ytimg.com/vi/fWNaR-rxAic/maxresdefault.jpg')

        self.assertEqual(len(data['videos']), 1)
        video = data['videos'][0]
        self.assertEqual(video['src'], 'https://www.youtube.com/embed/fWNaR-rxAic')
        self.assertEqual(video['width'], 1920)
        self.assertEqual(video['height'], 1080)

    def test_twitter_no_og_title_use_twitter_title(self):
        url = 'http://lassie.it/twitter_card/no_og_title_use_twitter_title.html'
        data = lassie.fetch(url)

        self.assertEqual(data['description'], 'A test case for Lassie!')
        self.assertEqual(data['title'], 'Lassie Twitter Test | no_og_title_use_twitter_title')

########NEW FILE########
