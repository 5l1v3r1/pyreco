__FILENAME__ = celeryconfig

########NEW FILE########
__FILENAME__ = conf
import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('..'))
sys.path.insert(0, os.path.abspath('.'))
os.environ['DJANGO_SETTINGS_MODULE'] = 'test_settings'

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'ElasticUtils'
copyright = u'2011-2014 Mozilla Foundation'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.

try:
    from elasticutils._version import __version__
    # The short X.Y version.
    version = '.'.join(__version__.split('.')[:2])
    # The full version, including alpha/beta/rc tags.
    release = __version__
except ImportError:
    version = release = 'dev'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'ElasticUtilsdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'ElasticUtils.tex', u'ElasticUtils Documentation',
   u'Mozilla Foundation', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'elasticutils', u'ElasticUtils Documentation',
     [u'Dave Dash'], 1)
]

########NEW FILE########
__FILENAME__ = sample_facets
"""
This is a sample program that uses Elasticsearch (from elasticsearch-py)
object to create an index, create a mapping, and index some data. Then
it uses ElasticUtils S to show some behavior with facets.
"""

from elasticutils import get_es, S

from elasticsearch.helpers import bulk_index

URL = 'localhost'
INDEX = 'fooindex'
DOCTYPE = 'testdoc'
 

# This creates an elasticsearch.Elasticsearch object which we can use
# to do all our indexing.
es = get_es(urls=[URL])
 
# First, delete the index, ignore possible 404 - it means the index doesn't
# exist, so there's nothing to delete.
es.indices.delete(index=INDEX, ignore=404)
 
# Define the mapping for the doctype 'testdoc'. It's got an id field,
# a title which is analyzed, and two fields that are lists of tags, so
# we don't want to analyze them.
#
# Note: The alternative for the tags is to analyze them and use the
# 'keyword' analyzer. Both not analyzing and using the keyword
# analyzer treats the values as a single term rather than tokenizing
# them and treating as multiple terms.
mapping = {
    DOCTYPE: {
        'properties': {
            'id': {'type': 'integer'},
            'title': {'type': 'string'},
            'topics': {'type': 'string'},
            'product': {'type': 'string', 'index': 'not_analyzed'},
            }
        }
    }
 
# create the index with defined mappings
es.indices.create(index=INDEX, body={'mappings': mapping})


# This indexes a series of documents each is a Python dict.
documents = [
    {'_id': 1,
     'title': 'Deleting cookies',
     'topics': ['cookies', 'privacy'],
     'product': ['Firefox', 'Firefox for mobile']},
    {'_id': 2,
     'title': 'What is a cookie?',
     'topics': ['cookies', 'privacy', 'basic'],
     'product': ['Firefox', 'Firefox for mobile']},
    {'_id': 3,
     'title': 'Websites say cookies are blocked - Unblock them',
     'topics': ['cookies', 'privacy', 'websites'],
     'product': ['Firefox', 'Firefox for mobile', 'Boot2Gecko']},
    {'_id': 4,
     'title': 'Awesome Bar',
     'topics': ['tips', 'search', 'basic', 'user interface'],
     'product': ['Firefox']},
    {'_id': 5,
     'title': 'Flash',
     'topics': ['flash'],
     'product': ['Firefox']}
    ]

bulk_index(es, documents, index=INDEX, doc_type=DOCTYPE)

# Elasticsearch will refresh the indexes and make those documents
# available for querying in a second or so (it's configurable in
# Elasticsearch), but we want them available right now, so we refresh
# the index.
es.indices.refresh(index=INDEX)

# Let's build a basic S that looks at the right Elasticsearch cluster,
# index and doctype.
basic_s = S().es(urls=[URL]).indexes(INDEX).doctypes(DOCTYPE).values_dict()
 
# Now let's see facet counts for all the products.
s = basic_s.facet('product')

print s.facet_counts()
# Pretty-printed output:
# {u'product': {
#     u'_type': u'terms',
#     u'total': 9,
#     u'terms': [
#         {u'count': 5, u'term': u'Firefox'},
#         {u'count': 3, u'term': u'Firefox for mobile'},
#         {u'count': 1, u'term': u'Boot2Gecko'}
#     ],
#     u'other': 0,
#     u'missing': 0
#     }}

# Let's do a query for 'cookie' and do a facet count.
print s.query(title__text='cookie').facet_counts()
# Pretty-printed output:
# {u'product': {
#     u'_type': u'terms',
#     u'total': 2,
#     u'terms': [
#         {u'count': 1, u'term': u'Firefox for mobile'},
#         {u'count': 1, u'term': u'Firefox'}
#     ],
#     u'other': 0,
#     u'missing': 0
#     }}

# Note that the facet_counts are affected by the query.

# Let's do a filter for 'flash' in the topic.
print s.filter(topics='flash').facet_counts()
# Pretty-printed output:
# {u'product': {
#     u'_type': u'terms',
#     u'total': 9,
#     u'terms': [
#         {u'count': 5, u'term': u'Firefox'},
#         {u'count': 3, u'term': u'Firefox for mobile'},
#         {u'count': 1, u'term': u'Boot2Gecko'}
#     ],
#     u'other': 0,
#     u'missing': 0
#     }}

# Note that the facet_counts are NOT affected by filters.

# Let's do a filter for 'flash' in the topic, and specify
# filtered=True.
print s.facet('product', filtered=True).filter(topics='flash').facet_counts()
# Pretty-printed output:
# {u'product': {
#     u'_type': u'terms',
#     u'total': 1,
#     u'terms': [
#         {u'count': 1, u'term': u'Firefox'}
#     ],
#     u'other': 0,
#     u'missing': 0
#     }}

# Using filtered=True causes the facet_counts to be affected by the
# filters.

# We've done a bunch of faceting on a field that is not
# analyzed. Let's look at what happens when we try to use facets on a
# field that is analyzed.
print basic_s.facet('topics').facet_counts()
# Pretty-printed output:
# {u'topics': {
#     u'_type': u'terms',
#     u'total': 14,
#     u'terms': [
#         {u'count': 3, u'term': u'privacy'},
#         {u'count': 3, u'term': u'cookies'},
#         {u'count': 2, u'term': u'basic'},
#         {u'count': 1, u'term': u'websites'},
#         {u'count': 1, u'term': u'user'},
#         {u'count': 1, u'term': u'tips'},
#         {u'count': 1, u'term': u'search'},
#         {u'count': 1, u'term': u'interface'},
#         {u'count': 1, u'term': u'flash'}
#     ],
#     u'other': 0,
#     u'missing': 0
#     }}

# Note how the facet counts shows 'user' and 'interface' as two
# separate terms even though they're a single topic for document with
# id=4. When that document is indexed, the topic field is analyzed and
# the default analyzer tokenizes it splitting it into two terms.
#
# Moral of the story is that you want fields you facet on to be
# analyzed as keyword fields or not analyzed at all.

########NEW FILE########
__FILENAME__ = sample_quickstart
"""
This is a sample program that uses Elasticsearch (from elasticsearch-py)
object to create an index, create a mapping, and index some data. Then
it uses ElasticUtils S to show some behavior.
"""

from elasticutils import get_es, S

from elasticsearch.helpers import bulk_index

URL = 'localhost'
INDEX = 'fooindex'
DOCTYPE = 'testdoc'
 

# This creates an elasticsearch.Elasticsearch object which we can use
# to do all our indexing.
es = get_es(urls=[URL])
 
# First, delete the index if it exists.
es.indices.delete(index=INDEX, ignore=404)
 
# Define the mapping for the doctype 'testdoc'. It's got an id field,
# a title which is analyzed, and two fields that are lists of tags, so
# we don't want to analyze them.
mapping = {
    DOCTYPE: {
        'properties': {
            'id': {'type': 'integer'},
            'title': {'type': 'string', 'analyzer': 'snowball'},
            'topics': {'type': 'string'},
            'product': {'type': 'string', 'index': 'not_analyzed'},
            }
        }
    }
 
# Create the index 'testdoc' mapping.
es.indices.create(INDEX, body={'mappings': mapping})


# Let's index some documents and make them available for searching.
documents = [
    {'_id': 1,
     'title': 'Deleting cookies',
     'topics': ['cookies', 'privacy'],
     'product': ['Firefox', 'Firefox for mobile']},
    {'_id': 2,
     'title': 'What is a cookie?',
     'topics': ['cookies', 'privacy'],
     'product': ['Firefox', 'Firefox for mobile']},
    {'_id': 3,
     'title': 'Websites say cookies are blocked - Unblock them',
     'topics': ['cookies', 'privacy', 'websites'],
     'product': ['Firefox', 'Firefox for mobile', 'Boot2Gecko']},
    {'_id': 4,
     'title': 'Awesome Bar',
     'topics': ['tips', 'search', 'user interface'],
     'product': ['Firefox']},
    {'_id': 5,
     'title': 'Flash',
     'topics': ['flash'],
     'product': ['Firefox']}
    ]

bulk_index(es, documents, index=INDEX, doc_type=DOCTYPE)
es.indices.refresh(index=INDEX)


# Now let's do some basic queries.

# Let's build a basic S that looks at our Elasticsearch cluster and
# the index and doctype we just indexed our documents in.
basic_s = S().es(urls=[URL]).indexes(INDEX).doctypes(DOCTYPE)

# How many documents are in our index?
print basic_s.count()
# Prints:
# 5

# Print articles with 'cookie' in the title.
print [item['title']
       for item in basic_s.query(title__text='cookie')]
# Prints:
# [u'Deleting cookies', u'What is a cookie?',
#  u'Websites say cookies are blocked - Unblock them']

# Print articles with 'cookie' in the title that are related to
# websites.
print [item['title']
       for item in basic_s.query(title__text='cookie')
                          .filter(topics='websites')]
# Prints:
# [u'Websites say cookies are blocked - Unblock them']

# Print articles in the 'search' topic.
print [item['title']
       for item in basic_s.filter(topics='search')]
# Prints:
# [u'Awesome Bar']

# Do a query and use the highlighter to denote the matching text.
print [(item['title'], item.es_meta.highlight['title'])
       for item in basic_s.query(title__text='cookie').highlight('title')]
# Prints:
# [
#    (u'Deleting cookies', [u'Deleting <em>cookies</em>']),
#    (u'What is a cookie?', [u'What is a <em>cookie</em>?']),
#    (u'Websites say cookies are blocked - Unblock them',
#       [u'Websites say <em>cookies</em> are blocked - Unblock them']
#    )
# ]


# That's the gist of it!

########NEW FILE########
__FILENAME__ = estestcase
"""
With `test_utils` you can use this testcase.
"""
from __future__ import print_function
from django.test import TestCase

from django.conf import settings
from elasticsearch.exceptions import ConnectionError
from elasticsearch.helpers import bulk_index
import six

# Try really really hard to find a valid skip thing.
try:
    from nose import SkipTest
    def skip_this_test():
        raise SkipTest
except ImportError:
    try:
        import pytest
        def skip_this_test():
            pytest.skip('skipping: es not set up')
    except ImportError:
        try:
            from unittest import skip
            def skip_this_test():
                skip('skipping: es not set up')
        except ImportError:
            def skip_this_test():
                print('SKIPPING: es not set up')
                return


from elasticutils import get_es


def testify(indexes):
    """Returns indexes with '_eutest' suffix.

    :arg indexes: dict of mapping type name -> index name(s)

    :returns: dict with ``_eutest`` appended to all index names

    """
    ret = {}
    for k, v in indexes.items():
        if isinstance(v, six.string_types):
            ret[k] = v + '_eutest'
        elif isinstance(v, (list, tuple)):
            ret[k] = [v_item + '_eutest' for v_item in v]
    return ret


class ESTestCase(TestCase):
    """Test case scaffolding for ElasticUtils-using tests.

    If ``ES_URLS`` is empty or missing or you can't connect to
    Elasticsearch specified in ``ES_URLS``, then this will skip each
    individual test. This works with py.test, nose, and unittest in
    Python 2.7. If you don't have one of those, then this will print
    to stdout and just skip the test silently.

    """
    skip_tests = False

    @classmethod
    def setUpClass(cls):
        """Sets up the environment for ES tests

        * pings the ES server---if this fails, it marks all the tests
          for skipping
        * fixes settings
        * deletes the test index if there is one

        """
        super(ESTestCase, cls).setUpClass()
        if not getattr(settings, 'ES_URLS', None):
            cls.skip_tests = True
            return

        try:
            cls.get_es().cluster.health()
        except ConnectionError:
            cls.skip_tests = True
            return

        # Save settings and override them
        cls._old_es_disabled = settings.ES_DISABLED
        settings.ES_DISABLED = False

        cls._old_es_indexes = settings.ES_INDEXES
        settings.ES_INDEXES = testify(settings.ES_INDEXES)

        # This is here in case the previous test run failed and didn't
        # clean up after itself.
        for index in settings.ES_INDEXES.values():
            cls.get_es().indices.delete(index=index, ignore=404)

    def setUp(self):
        """Skips the test if this class is skipping tests."""
        if self.skip_tests:
            return skip_this_test()
        super(ESTestCase, self).setUp()

    @classmethod
    def tearDownClass(cls):
        """Tears down environment

        * unfixes settings
        * deletes the test index

        """
        if not cls.skip_tests:
            # If we didn't skip these tests, we need to do some
            # cleanup.
            for index in settings.ES_INDEXES.values():
                cls.cleanup_index(index)

            # Restore settings
            settings.ES_DISABLED = cls._old_es_disabled
            settings.ES_INDEXES = cls._old_es_indexes

        super(ESTestCase, cls).tearDownClass()

    @classmethod
    def get_es(cls):
        """Returns an ES

        Override this if you need different settings for your
        ES.

        """
        return get_es()

    @classmethod
    def create_index(cls, index, settings=None):
        """Creates index with given settings

        :arg index: the name of the index to create
        :arg settings: dict of settings to set in `create_index` call

        """
        settings = settings or {}

        cls.get_es().indices.create(index=index, **settings)

    @classmethod
    def index_data(cls, documents, index, doctype, id_field='id'):
        """Bulk indexes given data.

        This does a refresh after the data is indexed.

        :arg documents: list of python dicts each a document to index
        :arg index: name of the index
        :arg doctype: mapping type name
        :arg id_field: the field the document id is stored in in the
            document

        """
        documents = (dict(d, _id=d[id_field]) for d in documents)
        bulk_index(cls.get_es(), documents, index=index, doc_type=doctype)
        cls.refresh(index)

    @classmethod
    def cleanup_index(cls, index):
        cls.get_es().indices.delete(index=index, ignore=404)

    @classmethod
    def refresh(cls, index):
        """Refresh index after indexing.

        :arg index: the name of the index to refresh. use ``_all``
            to refresh all of them

        """
        cls.get_es().indices.refresh(index=index)
        cls.get_es().cluster.health(wait_for_status='yellow')

########NEW FILE########
__FILENAME__ = tasks
import logging

from django.conf import settings
from celery.task import task

from elasticutils.utils import chunked


log = logging.getLogger('elasticutils')


@task
def index_objects(mapping_type, ids, chunk_size=100, es=None, index=None):
    """Index documents of a specified mapping type.

    This allows for asynchronous indexing.

    If a mapping_type extends Indexable, you can add a ``post_save``
    hook for the model that it's based on like this::

        @receiver(dbsignals.post_save, sender=MyModel)
        def update_in_index(sender, instance, **kw):
            from elasticutils.contrib.django import tasks
            tasks.index_objects.delay(MyMappingType, [instance.id])


    :arg mapping_type: the mapping type for these ids
    :arg ids: the list of ids of things to index
    :arg chunk_size: the size of the chunk for bulk indexing

        .. Note::

           The default chunk_size is 100. The number of documents you
           can bulk index at once depends on the size of the
           documents.

    :arg es: The `Elasticsearch` to use. If you don't specify an
        `Elasticsearch`, it'll use `mapping_type.get_es()`.
    :arg index: The name of the index to use. If you don't specify one
        it'll use `mapping_type.get_index()`.

    """
    if settings.ES_DISABLED:
        return

    log.debug('Indexing objects {0}-{1}. [{2}]'.format(
            ids[0], ids[-1], len(ids)))

    # Get the model this mapping type is based on.
    model = mapping_type.get_model()

    # Retrieve all the objects that we're going to index and do it in
    # bulk.
    for id_list in chunked(ids, chunk_size):
        documents = []

        for obj in model.objects.filter(id__in=id_list):
            try:
                documents.append(mapping_type.extract_document(obj.id, obj))
            except StandardError as exc:
                log.exception('Unable to extract document {0}: {1}'.format(
                        obj, repr(exc)))

        if documents:
            mapping_type.bulk_index(documents, id_field='id', es=es, index=index)


@task
def unindex_objects(mapping_type, ids, es=None, index=None):
    """Remove documents of a specified mapping_type from the index.

    This allows for asynchronous deleting.

    If a mapping_type extends Indexable, you can add a ``pre_delete``
    hook for the model that it's based on like this::

        @receiver(dbsignals.pre_delete, sender=MyModel)
        def remove_from_index(sender, instance, **kw):
            from elasticutils.contrib.django import tasks
            tasks.unindex_objects.delay(MyMappingType, [instance.id])

    :arg mapping_type: the mapping type for these ids
    :arg ids: the list of ids of things to remove
    :arg es: The `Elasticsearch` to use. If you don't specify an
        `Elasticsearch`, it'll use `mapping_type.get_es()`.
    :arg index: The name of the index to use. If you don't specify one
        it'll use `mapping_type.get_index()`.
    """
    if settings.ES_DISABLED:
        return

    for id_ in ids:
        mapping_type.unindex(id_, es=es, index=index)

########NEW FILE########
__FILENAME__ = test_middleware
from nose.tools import eq_

from django.test import RequestFactory
from django.test.utils import override_settings

from elasticutils.contrib.django import (
    ES_EXCEPTIONS, ESExceptionMiddleware, es_required_or_50x)
from elasticutils.contrib.django.estestcase import ESTestCase


class MiddlewareTest(ESTestCase):
    def setUp(self):
        super(MiddlewareTest, self).setUp()

        def view(request, exc):
            raise exc

        self.func = view
        self.fake_request = RequestFactory().get('/')

    def test_exceptions(self):
        for exc in ES_EXCEPTIONS:
            response = ESExceptionMiddleware().process_exception(
                self.fake_request, exc(Exception))
            eq_(response.status_code, 503)
            self.assertTemplateUsed(response, 'elasticutils/503.html')

    @override_settings(ES_DISABLED=True)
    def test_es_disabled(self):
        response = ESExceptionMiddleware().process_request(self.fake_request)
        eq_(response.status_code, 501)
        self.assertTemplateUsed(response, 'elasticutils/501.html')


class DecoratorTest(ESTestCase):
    def setUp(self):
        super(DecoratorTest, self).setUp()

        @es_required_or_50x()
        def view(request, exc):
            raise exc

        self.func = view
        self.fake_request = RequestFactory().get('/')

    def test_exceptions(self):
        for exc in ES_EXCEPTIONS:
            response = self.func(self.fake_request, exc(Exception))
            eq_(response.status_code, 503)

    @override_settings(ES_DISABLED=True)
    def test_es_disabled(self):
        response = self.func(self.fake_request)
        eq_(response.status_code, 501)

########NEW FILE########
__FILENAME__ = test_models
from nose.tools import eq_

from elasticutils.contrib.django import S, get_es
from elasticutils.contrib.django.tests import (
    FakeDjangoMappingType, FakeModel, reset_model_cache)
from elasticutils.contrib.django.estestcase import ESTestCase


class IndexableTest(ESTestCase):
    @classmethod
    def get_es(cls):
        return get_es()

    def setUp(self):
        super(IndexableTest, self).setUp()
        IndexableTest.create_index(FakeDjangoMappingType.get_index())

    def tearDown(self):
        super(IndexableTest, self).tearDown()
        IndexableTest.cleanup_index(FakeDjangoMappingType.get_index())
        reset_model_cache()

    def persist_data(self, data):
        for doc in data:
            FakeModel(**doc)

            # Index the document with .index()
            FakeDjangoMappingType.index(doc, id_=doc['id'])

        self.refresh(FakeDjangoMappingType.get_index())

    def test_refresh(self):
        FakeDjangoMappingType.refresh_index()

    def test_index(self):
        self.persist_data([
                {'id': 1, 'name': 'odin skullcrusher'},
                {'id': 2, 'name': 'olaf bloodbiter'},
        ])

        # Query it to make sure it's there.
        eq_(len(S(FakeDjangoMappingType).query(name__prefix='odin')), 1)

    def test_get_object(self):
        self.persist_data([
                {'id': 1, 'name': 'odin skullcrusher'},
                {'id': 2, 'name': 'olaf bloodbiter'},
        ])

        s = S(FakeDjangoMappingType).query(name__prefix='odin')
        obj = s[0]
        eq_(obj.object.id, 1)

    def test_get_indexable(self):
        self.persist_data([
                {'id': 1, 'name': 'odin skullcrusher'},
                {'id': 2, 'name': 'olaf bloodbiter'},
        ])

        eq_(list(FakeDjangoMappingType.get_indexable()), [1, 2])

    def test_bulk_index(self):
        documents = [
            {'id': 1, 'name': 'odin skullcrusher'},
            {'id': 2, 'name': 'heimdall kneebiter'},
            {'id': 3, 'name': 'erik rose'}
            ]

        # Generate the FakeModel in our "database"
        for doc in documents:
            FakeModel(**doc)

        # Index the document with .index()
        FakeDjangoMappingType.bulk_index(documents, id_field='id')

        self.refresh(FakeDjangoMappingType.get_index())

        # Query it to make sure they're there.
        eq_(len(S(FakeDjangoMappingType).query(name__prefix='odin')), 1)
        eq_(len(S(FakeDjangoMappingType).query(name__prefix='erik')), 1)

########NEW FILE########
__FILENAME__ = test_query
from nose.tools import eq_

from elasticutils.contrib.django import S, F, InvalidFieldActionError
from elasticutils.contrib.django.tests import FakeDjangoMappingType, FakeModel
from elasticutils.contrib.django.estestcase import ESTestCase
from elasticutils.tests import facet_counts_dict


class QueryTest(ESTestCase):
    @classmethod
    def setUpClass(cls):
        super(QueryTest, cls).setUpClass()

        if cls.skip_tests:
            return

        index = FakeDjangoMappingType.get_index()
        doctype = FakeDjangoMappingType.get_mapping_type_name()

        cls.create_index(index)

        data = [
            {'id': 1, 'foo': 'bar', 'tag': 'awesome', 'width': '2'},
            {'id': 2, 'foo': 'bart', 'tag': 'boring', 'width': '7'},
            {'id': 3, 'foo': 'car', 'tag': 'awesome', 'width': '5'},
            {'id': 4, 'foo': 'duck', 'tag': 'boat', 'width': '11'},
            {'id': 5, 'foo': 'train car', 'tag': 'awesome', 'width': '7'}
            ]
        cls.index_data(data, index=index, doctype=doctype)

        # Generate all the FakeModels in our "database"
        for args in data:
            FakeModel(**args)

        cls.refresh(index)

    def test_q(self):
        eq_(len(S(FakeDjangoMappingType).query(foo='bar')), 1)
        eq_(len(S(FakeDjangoMappingType).query(foo='car')), 2)

    def test_q_all(self):
        eq_(len(S(FakeDjangoMappingType)), 5)

    def test_filter_empty_f(self):
        eq_(len(S(FakeDjangoMappingType).filter(F() | F(tag='awesome'))), 3)
        eq_(len(S(FakeDjangoMappingType).filter(F() & F(tag='awesome'))), 3)
        eq_(len(S(FakeDjangoMappingType).filter(F() | F() | F(tag='awesome'))), 3)
        eq_(len(S(FakeDjangoMappingType).filter(F() & F() & F(tag='awesome'))), 3)
        eq_(len(S(FakeDjangoMappingType).filter(F())), 5)

    def test_filter(self):
        eq_(len(S(FakeDjangoMappingType).filter(tag='awesome')), 3)
        eq_(len(S(FakeDjangoMappingType).filter(F(tag='awesome'))), 3)

    def test_filter_and(self):
        eq_(len(S(FakeDjangoMappingType).filter(tag='awesome', foo='bar')), 1)
        eq_(len(S(FakeDjangoMappingType).filter(tag='awesome').filter(foo='bar')), 1)
        eq_(len(S(FakeDjangoMappingType).filter(F(tag='awesome') & F(foo='bar'))), 1)

    def test_filter_or(self):
        eq_(len(S(FakeDjangoMappingType).filter(F(tag='awesome') | F(tag='boat'))), 4)

    def test_filter_or_3(self):
        eq_(len(S(FakeDjangoMappingType).filter(F(tag='awesome') | F(tag='boat') |
                                     F(tag='boring'))), 5)
        eq_(len(S(FakeDjangoMappingType).filter(or_={'foo': 'bar',
                                          'or_': {'tag': 'boat',
                                                  'width': '5'}
                                          })), 3)

    def test_filter_complicated(self):
        eq_(len(S(FakeDjangoMappingType).filter(F(tag='awesome', foo='bar') |
                                     F(tag='boring'))), 2)

    def test_filter_not(self):
        eq_(len(S(FakeDjangoMappingType).filter(~F(tag='awesome'))), 2)
        eq_(len(S(FakeDjangoMappingType).filter(~(F(tag='boring') | F(tag='boat')))), 3)
        eq_(len(S(FakeDjangoMappingType).filter(~F(tag='boat')).filter(~F(foo='bar'))), 3)
        eq_(len(S(FakeDjangoMappingType).filter(~F(tag='boat', foo='barf'))), 5)

    def test_filter_bad_field_action(self):
        with self.assertRaises(InvalidFieldActionError):
            len(S(FakeDjangoMappingType).filter(F(tag__faux='awesome')))

    def test_facet(self):
        qs = S(FakeDjangoMappingType).facet('tag')
        eq_(facet_counts_dict(qs, 'tag'), dict(awesome=3, boring=1, boat=1))

    def test_filtered_facet(self):
        qs = S(FakeDjangoMappingType).query(foo='car').filter(width=5)

        # filter doesn't apply to facets
        eq_(facet_counts_dict(qs.facet('tag'), 'tag'),
            {'awesome': 2})

        # filter does apply to facets
        eq_(facet_counts_dict(qs.facet('tag', filtered=True), 'tag'),
            {'awesome': 1})

    def test_global_facet(self):
        qs = S(FakeDjangoMappingType).query(foo='car').filter(width=5)

        # facet restricted to query
        eq_(facet_counts_dict(qs.facet('tag'), 'tag'),
            {'awesome': 2})

        # facet applies to all of corpus
        eq_(facet_counts_dict(qs.facet('tag', global_=True), 'tag'),
            dict(awesome=3, boring=1, boat=1))

    def test_facet_raw(self):
        qs = S(FakeDjangoMappingType).facet_raw(tags={'terms': {'field': 'tag'}})
        eq_(facet_counts_dict(qs, 'tags'),
            dict(awesome=3, boring=1, boat=1))

        qs = (S(FakeDjangoMappingType)
              .query(foo='car')
              .facet_raw(tags={'terms': {'field': 'tag'}}))
        eq_(facet_counts_dict(qs, 'tags'),
            {'awesome': 2})

    def test_facet_raw_overrides_facet(self):
        """facet_raw overrides facet with the same facet name."""
        qs = (S(FakeDjangoMappingType)
              .query(foo='car')
              .facet('tag')
              .facet_raw(tag={'terms': {'field': 'tag'}, 'global': True}))
        eq_(facet_counts_dict(qs, 'tag'),
            dict(awesome=3, boring=1, boat=1))

    def test_order_by(self):
        res = S(FakeDjangoMappingType).filter(tag='awesome').order_by('-width')
        eq_([d.id for d in res], [5, 3, 1])

########NEW FILE########
__FILENAME__ = test_s
from unittest import TestCase

from django.conf import settings
from nose.tools import eq_

from elasticutils.contrib.django import S
from elasticutils.contrib.django.tests import FakeDjangoMappingType


class TestS(TestCase):
    def test_require_mapping_type(self):
        """The Django S requires a mapping type."""
        self.assertRaises(TypeError, S)

    def test_get_indexes(self):
        """Test get_indexes always returns a list of strings."""
        # Pulls it from ES_INDEXES (list of strings).
        s = S(FakeDjangoMappingType)
        eq_(s.get_indexes(), ['elasticutilstest'])

        # Pulls it from ES_INDEXES (string).
        old_indexes = settings.ES_INDEXES
        try:
            settings.ES_INDEXES = {'default': 'elasticutilstest'}

            s = S(FakeDjangoMappingType)
            eq_(s.get_indexes(), ['elasticutilstest'])
        finally:
            settings.ES_INDEXES = old_indexes

        # Pulls from indexes.
        s = S(FakeDjangoMappingType).indexes('footest')
        eq_(s.get_indexes(), ['footest'])

        s = S(FakeDjangoMappingType).indexes('footest', 'footest2')
        eq_(s.get_indexes(), ['footest', 'footest2'])

        s = S(FakeDjangoMappingType).indexes('footest').indexes('footest2')
        eq_(s.get_indexes(), ['footest2'])

    def test_get_doctypes(self):
        """Test get_doctypes always returns a list of strings."""
        # Pulls from ._meta.db_table.
        s = S(FakeDjangoMappingType)
        eq_(s.get_doctypes(), ['fake'])

        # Pulls from doctypes.
        s = S(FakeDjangoMappingType).doctypes('footype')
        eq_(s.get_doctypes(), ['footype'])

        s = S(FakeDjangoMappingType).doctypes('footype', 'footype2')
        eq_(s.get_doctypes(), ['footype', 'footype2'])

        s = S(FakeDjangoMappingType).doctypes('footype').doctypes('footype2')
        eq_(s.get_doctypes(), ['footype2'])

########NEW FILE########
__FILENAME__ = test_tasks
from nose.tools import eq_

from elasticutils.contrib.django import get_es
from elasticutils.contrib.django.tasks import index_objects, unindex_objects
from elasticutils.contrib.django.tests import (
    FakeDjangoMappingType, FakeModel, reset_model_cache)
from elasticutils.contrib.django.estestcase import ESTestCase


class TestTasks(ESTestCase):
    @classmethod
    def get_es(cls):
        return get_es()

    def setUp(self):
        super(TestTasks, self).setUp()
        TestTasks.create_index(FakeDjangoMappingType.get_index())
        reset_model_cache()

    def tearDown(self):
        super(TestTasks, self).tearDown()
        TestTasks.cleanup_index(FakeDjangoMappingType.get_index())

    def test_tasks(self):
        documents = [
            {'id': 1, 'name': 'odin skullcrusher'},
            {'id': 2, 'name': 'heimdall kneebiter'},
            {'id': 3, 'name': 'erik rose'}
            ]

        for doc in documents:
            FakeModel(**doc)

        # Test index_objects task
        index_objects(FakeDjangoMappingType, [1, 2, 3])
        FakeDjangoMappingType.refresh_index()
        eq_(FakeDjangoMappingType.search().count(), 3)

        # Test unindex_objects task
        unindex_objects(FakeDjangoMappingType, [1, 2, 3])
        FakeDjangoMappingType.refresh_index()
        eq_(FakeDjangoMappingType.search().count(), 0)

    def test_tasks_kwargs(self):
        """Test chunk size, es, and index parameters affects bulk_index"""
        documents = [
            {'id': 1, 'name': 'odin skullcrusher'},
            {'id': 2, 'name': 'heimdall kneebiter'},
            {'id': 3, 'name': 'erik rose'}
        ]

        for doc in documents:
            FakeModel(**doc)

        class MockMappingType(FakeDjangoMappingType):
            bulk_index_count = 0
            index_kwarg = None
            es_kwarg = None

            @classmethod
            def bulk_index(cls, *args, **kwargs):
                cls.bulk_index_count += 1
                cls.index_kwarg = kwargs.get('index')
                cls.es_kwarg = kwargs.get('es')

        index_objects(MockMappingType, [1, 2, 3])
        eq_(MockMappingType.bulk_index_count, 1)

        MockMappingType.bulk_index_count = 0

        index_objects(MockMappingType, [1, 2, 3], chunk_size=2)
        eq_(MockMappingType.bulk_index_count, 2)

        MockMappingType.bulk_index_count = 0

        index_objects(MockMappingType, [1, 2, 3], chunk_size=1)
        eq_(MockMappingType.bulk_index_count, 3)

        # test index and es kwargs 
        MockMappingType.index_kwarg = None
        MockMappingType.es_kwarg = None
        index_objects(MockMappingType, [1, 2, 3])
        eq_(MockMappingType.index_kwarg, None)
        eq_(MockMappingType.es_kwarg, None)

        index_objects(MockMappingType, [1, 2, 3], es='crazy_es', index='crazy_index')
        eq_(MockMappingType.index_kwarg, 'crazy_index')
        eq_(MockMappingType.es_kwarg, 'crazy_es')

########NEW FILE########
__FILENAME__ = test_utils
# TODO: test es_required


########NEW FILE########
__FILENAME__ = estestcase
from unittest import TestCase

from elasticsearch.helpers import bulk_index
try:
    from nose import SkipTest
except ImportError:
    try:
        from unittest.case import SkipTest
    except ImportError:
        class SkipTest(Exception):
            pass

from elasticutils import get_es, S


class ESTestCase(TestCase):
    """Superclass for Elasticsearch-using test cases.

    :property es_settings: settings to use to build a elasticsearch
        Elasticsearch object
    :property index_name: Name of the index to use for theses tests
    :property mapping_type_name: The mapping type name for the mapping
        you want created
    :property mapping: The mapping to use when creating the index
    :property data: Any documents to index during ``setup_class()``

    For examples of usage, see ``tests/test_*.py`` files.

    You probably want to subclass this and at least set relevant class
    properties. Then use that subclass as the superclass for your
    tests.

    """
    index_name = 'elasticutilstest'
    mapping_type_name = 'elasticutilsmappingtype'
    es_settings = {
        'urls': ['http://localhost:9200']
    }
    mapping = {}
    data = []

    @classmethod
    def setup_class(cls):
        """Sets up the index specified by ``cls.index_name``

        This will create the index named ``cls.index_name`` with the
        mapping specified in ``cls.mapping`` and indexes any data
        specified in ``cls.data``.

        If you need something different, then override this.

        """
        # Note: TestCase has no setup_class, so we don't call super()
        # here.
        cls.cleanup_index()
        cls.create_index(settings={'mappings': cls.mapping})
        if cls.data:
            cls.index_data(cls.data)
            cls.refresh()

    @classmethod
    def teardown_class(cls):
        """Removes the index specified by ``cls.index_name``

        This should clean up anything created in ``cls.setup_class()``
        and anything created by the tests.

        """
        cls.cleanup_index()

    def shortDescription(self):
        # Prevent the docstring being used as the test name because
        # that's irritating as all hell when trying to fix tests.
        pass

    @classmethod
    def get_es(cls):
        """Returns the Elasticsearch object specified by ``cls.es_settings``"""
        return get_es(**cls.es_settings)

    @classmethod
    def get_s(cls, mapping_type=None):
        """Returns an S for the settings on this class

        Uses ``cls.es_settings`` to configure the Elasticsearch
        object. Uses ``cls.index_name`` for the index and
        ``cls.mapping_type_name`` for the MappingType to search.

        :arg mapping_type: The MappingType class to use to create the S

        """
        if mapping_type is not None:
            s = S(mapping_type)
        else:
            s = S()
        return (s.es(**cls.es_settings)
                 .indexes(cls.index_name)
                 .doctypes(cls.mapping_type_name))

    @classmethod
    def create_index(cls, settings=None):
        """Creates an index with specified settings

        Uses ``cls.index_name`` as the index to create.

        :arg settings: Any additional settings to use to create the
            index.

        """
        body = {}
        if settings:
            body['settings'] = settings
        cls.get_es().indices.create(index=cls.index_name, body=body)

    @classmethod
    def index_data(cls, documents, id_field='id'):
        """Indexes specified data

        Uses ``cls.index_name`` as the index to index into.  Uses
        ``cls.mapping_type_name`` as the doctype to index these
        documents as.

        :arg documents: List of documents as Python dicts
        :arg id_field: The field of the document that represents the id

        """
        documents = (dict(d, _id=d[id_field]) for d in documents)
        bulk_index(cls.get_es(), documents, index=cls.index_name,
                   doc_type=cls.mapping_type_name)
        cls.refresh()

    @classmethod
    def cleanup_index(cls):
        """Cleans up the index

        This deletes the index named by ``cls.index_name``.

        """
        cls.get_es().indices.delete(index=cls.index_name, ignore=404)

    @classmethod
    def refresh(cls):
        """Refresh index after indexing

        This refreshes the index specified by ``cls.index_name``.

        """
        cls.get_es().indices.refresh(index=cls.index_name)
        cls.get_es().cluster.health(wait_for_status='yellow')

########NEW FILE########
__FILENAME__ = test_es
from unittest import TestCase

from nose.tools import eq_

from elasticutils import get_es, _cached_elasticsearch


class ESTest(TestCase):
    def setUp(self):
        super(ESTest, self).setUp()

        _cached_elasticsearch.clear()

    def test_get_es_caching(self):
        """Test get_es caching."""
        es = get_es()

        # Cached one item.
        eq_(len(_cached_elasticsearch), 1)

        # Use a different url, make sure that gets cached, too, and
        # it's different than the first one.
        es2 = get_es(urls=['http://example.com:9200'])
        eq_(len(_cached_elasticsearch), 2)
        assert id(es) != id(es2)

        # Use the same url, but pass it as a string, make sure that
        # pulls the previous one.
        es3 = get_es(urls='http://example.com:9200')
        eq_(len(_cached_elasticsearch), 2)
        assert id(es2) == id(es3)

        # Use a different timeout.
        es4 = get_es(timeout=10)
        eq_(len(_cached_elasticsearch), 3)
        assert id(es) != id(es4)

    def test_get_es_force_new(self):
        """Test that force_new works correctly."""
        es = get_es()

        es2 = get_es(force_new=True)

        # force_new prevents the new ElasticSearch instance from getting
        # cached, so we should only have one item in the cache.
        eq_(len(_cached_elasticsearch), 1)

        # However, the two ElasticSearch instances should be different.
        assert id(es) != id(es2)

    def test_get_es_settings_cache(self):
        """Tests **settings and cache."""
        es = get_es(max_retries=5, revival_delay=10)
        eq_(len(_cached_elasticsearch), 1)

        # Switching the order doesn't affect caching.
        es2 = get_es(revival_delay=10, max_retries=5)
        eq_(len(_cached_elasticsearch), 1)
        assert id(es) == id(es2)

        # Different values brings up a new item.
        es3 = get_es(max_retries=4, revival_delay=10)
        eq_(len(_cached_elasticsearch), 2)
        assert id(es) != id(es3)

########NEW FILE########
__FILENAME__ = test_mlt
from nose.tools import eq_

from elasticutils import MLT
from elasticutils.tests import ESTestCase


class MoreLikeThisTest(ESTestCase):
    data = [
        {'id': 1, 'foo': 'bar', 'tag': 'awesome'},
        {'id': 2, 'foo': 'bar', 'tag': 'boring'},
        {'id': 3, 'foo': 'bar', 'tag': 'awesome'},
        {'id': 4, 'foo': 'bar', 'tag': 'boring'},
        {'id': 5, 'foo': 'bar', 'tag': 'elite'},
        {'id': 6, 'foo': 'notbar', 'tag': 'gross'},
        {'id': 7, 'foo': 'notbar', 'tag': 'awesome'},
    ]

    def test_bad_mlt(self):
        """Tests S or index and doc_type is specified."""
        self.assertRaises(ValueError, lambda: MLT(1))
        self.assertRaises(ValueError, lambda: MLT(1, index='foo'))
        self.assertRaises(ValueError, lambda: MLT(1, doctype='foo'))

    def test_mlt_on_foo(self):
        """Verify MLT with the foo field."""
        # We need to pass min_term_freq and min_doc_freq, because the terms
        # we are using are only once in each document.
        mlt = MLT(1, self.get_s(), ['foo'], min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 4)

    def test_mlt_on_foo_no_s(self):
        """Verify MLT with the foo field."""
        index = self.get_s().get_indexes()[0]
        doc_type = self.get_s().get_doctypes()[0]
        es = self.get_s().get_es()

        mlt = MLT(1, mlt_fields=['foo'], index=index, doctype=doc_type,
                  es=es, min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 4)

    def test_mlt_on_tag(self):
        """Verify MLT with the tag field."""
        # We need to pass min_term_freq and min_doc_freq, because the terms
        # we are using are only once in each document.
        mlt = MLT(1, self.get_s(), ['tag'], min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 2)

    def test_mlt_on_two_fields(self):
        """Verify MLT on tag and foo fields."""
        mlt = MLT(1, self.get_s(), ['tag', 'foo'],
                  min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 5)

    def test_mlt_deprecated_fields(self):
        with self.assertRaises(DeprecationWarning):
            MLT(1, self.get_s(), fields=['tag', 'foo'])

    def test_mlt_iter(self):
        mlt = MLT(1, self.get_s(), ['tag', 'foo'],
                  min_term_freq=1, min_doc_freq=1)
        eq_(len(list(mlt)), 5)

    def test_mlt_on_foo_with_filter(self):
        """Verify MLT with the foo field while filtering on tag."""
        # We need to pass min_term_freq and min_doc_freq, because the terms
        # we are using are only once in each document.
        mlt = MLT(1, self.get_s().filter(tag='boring'), ['foo'],
                  min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 2)

        mlt = MLT(1, self.get_s().filter(tag='elite'), ['foo'],
                  min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 1)

        mlt = MLT(1, self.get_s().filter(tag='awesome'), ['foo'],
                  min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 1)

        mlt = MLT(1, self.get_s().filter(tag='gross'), ['foo'],
                  min_term_freq=1, min_doc_freq=1)
        eq_(len(mlt), 0)

########NEW FILE########
__FILENAME__ = test_query
from datetime import datetime, timedelta
from unittest import TestCase

from nose.tools import eq_

from elasticutils import (
    S, F, Q, BadSearch, InvalidFieldActionError, InvalidFacetType,
    InvalidFlagsError, SearchResults, DefaultMappingType, MappingType,
    DEFAULT_INDEXES, DEFAULT_DOCTYPES)
from elasticutils.tests import ESTestCase, facet_counts_dict, require_version
import six


def eqish_(item1, item2):
    """Compare two trees ignoring order of things in lists

    Note: This is really goofy, but works for our specific purposes. If you
    have other needs, you'll likely need to find a new solution here.

    """
    def _eqish(part1, part2):
        if type(part1) != type(part2) or bool(part1) != bool(part2):
            return False

        if isinstance(part1, (tuple, list)):
            # This is kind of awful, but what we need to do is make
            # sure everything in the list part1 is in the list part2
            # in an eqish way.
            part2_left = list(part2)

            for mem1 in part1:
                for i, mem2 in enumerate(part2_left):
                    if _eqish(mem1, mem2):
                        del part2_left[i]
                        break
                else:
                    return False
            return True

        elif isinstance(part1, dict):
            if sorted(part1.keys()) != sorted(part2.keys()):
                return False
            for mem in part1.keys():
                if not _eqish(part1[mem], part2[mem]):
                    return False
            return True

        else:
            return part1 == part2

    if not _eqish(item1, item2):
        raise AssertionError('{0} != {1}'.format(item1, item2))


class TestEqish(TestCase):
    def test_good(self):
        eqish_('a', 'a')
        eqish_(True, True)
        eqish_(1, 1)
        eqish_([1, 2, 3], [1, 2, 3])
        eqish_([1, 2, 3], [3, 2, 1])
        eqish_({'a': [1, 2, 3]},
               {'a': [3, 2, 1]})
        eqish_({'a': {'b': [1, 2, 3]}},
               {'a': {'b': [3, 2, 1]}})
        eqish_(
            {
                'filter': {
                    'or': [
                        {'term': {'foo': 'bar'}},
                        {'or': [
                            {'term': {'tag': 'boat'}},
                            {'term': {'width': '5'}}
                        ]}
                    ]}
            },
            {
                'filter': {
                    'or': [
                        {'or': [
                            {'term': {'width': '5'}},
                            {'term': {'tag': 'boat'}}
                        ]},
                        {'term': {'foo': 'bar'}}
                    ]}
            }
        )


    def test_bad(self):
        self.assertRaises(AssertionError,
                          lambda: eqish_({'a': [1, 2, 3]}, {'b': [1, 2, 3]}))
        self.assertRaises(AssertionError,
                          lambda: eqish_({'a': [1, 2, 3]}, {'a': [2, 3, 4]}))


class FakeMappingType(MappingType):
    @classmethod
    def get_index(cls):
        return 'index123'

    @classmethod
    def get_mapping_type_name(cls):
        return 'doctype123'


class STest(TestCase):
    def test_untyped_s_get_indexes(self):
        eq_(S().get_indexes(), DEFAULT_INDEXES)
        eq_(S().indexes('abc').get_indexes(), ['abc'])

    def test_typed_s_get_indexes(self):
        eq_(S(FakeMappingType).get_indexes(), ['index123'])

    def test_untyped_s_get_doctypes(self):
        eq_(S().get_doctypes(), DEFAULT_DOCTYPES)
        eq_(S().doctypes('abc').get_doctypes(), ['abc'])

    def test_typed_s_get_doctypes(self):
        eq_(S(FakeMappingType).get_doctypes(), ['doctype123'])


class QTest(TestCase):
    def test_q_should(self):
        q = Q(foo__match='abc', bar__match='def', should=True)
        eq_(sorted(q.should_q), [('bar__match', 'def'), ('foo__match', 'abc')])
        eq_(sorted(q.must_q), [])
        eq_(sorted(q.must_not_q), [])

    def test_q_must(self):
        q = Q(foo__match='abc', bar__match='def', must=True)
        eq_(sorted(q.should_q), [])
        eq_(sorted(q.must_q), [('bar__match', 'def'), ('foo__match', 'abc')])
        eq_(sorted(q.must_not_q), [])

    def test_q_must_not(self):
        q = Q(foo__match='abc', bar__match='def', must_not=True)
        eq_(sorted(q.should_q), [])
        eq_(sorted(q.must_q), [])
        eq_(sorted(q.must_not_q), [('bar__match', 'def'), ('foo__match', 'abc')])

    def test_q_must_should(self):
        with self.assertRaises(InvalidFlagsError):
            Q(foo__match='abc', must=True, should=True)

    def test_q_basic_add(self):
        """Adding one Q to another Q combines them."""
        q = Q(foo__match='abc') + Q(bar__match='def')

        eq_(sorted(q.should_q), [])
        eq_(sorted(q.must_q), [('bar__match', 'def'), ('foo__match', 'abc')])
        eq_(sorted(q.must_not_q), [])

    def test_q_order(self):
        q1 = Q(foo__match='abc') + Q(bar__match='def')

        q2 = Q(bar__match='def') + Q(foo__match='abc')
        eq_(q1, q2)

        q2 = Q(bar__match='def')
        q2 += Q(foo__match='abc')
        eq_(q1, q2)

        q2 = Q(foo__match='abc')
        q2 += Q(bar__match='def')
        eq_(q1, q2)

    def test_q_mixed(self):
        q1 = Q(foo__match='should', bar__match='should', should=True)
        q2 = Q(baz='must')
        q3 = Q(bat='must_not', must_not=True)
        q4 = Q(ban='must', must=True)
        q5 = Q(bam='must', must=True)

        q_all = q1 + q2 + q3 + q4 + q5

        eq_(sorted(q_all.should_q),
            [('bar__match', 'should'), ('foo__match', 'should')])

        eq_(sorted(q_all.must_q),
            [('bam', 'must'), ('ban', 'must'), ('baz', 'must')])

        eq_(sorted(q_all.must_not_q),
            [('bat', 'must_not')])


class QueryTest(ESTestCase):
    data = [
        {
            'id': 1,
            'foo': 'bar',
            'tag': 'awesome',
            'width': '2',
            'height': 7
        },
        {
            'id': 2,
            'foo': 'bart',
            'tag': 'boring',
            'width': '7',
            'height': 11
        },
        {
            'id': 3,
            'foo': 'car',
            'tag': 'awesome',
            'width': '5',
            'height': 5
        },
        {
            'id': 4,
            'foo': 'duck',
            'tag': 'boat',
            'width': '11',
            'height': 7
        },
        {
            'id': 5,
            'foo': 'train car',
            'tag': 'awesome',
            'width': '7',
            'height': 2
        }
    ]

    def test_q_all(self):
        eq_(len(self.get_s()), 5)

    def test_q(self):
        eq_(len(self.get_s().query(foo='bar')), 1)
        eq_(len(self.get_s().query(foo='car')), 2)

        eq_(len(self.get_s().query(Q(foo='bar'))), 1)
        eq_(len(self.get_s().query(Q(foo='car'))), 2)

    def test_q_term(self):
        eq_(len(self.get_s().query(foo='car')), 2)
        eq_(len(self.get_s().query(foo__term='car')), 2)

        eq_(len(self.get_s().query(Q(foo='car'))), 2)
        eq_(len(self.get_s().query(Q(foo__term='car'))), 2)

    def test_q_terms(self):
        eq_(len(self.get_s().query(foo__terms=['car', 'duck'])), 3)

        eq_(len(self.get_s().query(Q(foo__terms=['car', 'duck']))), 3)

    def test_q_in(self):
        eq_(len(self.get_s().query(foo__in=['car', 'bar'])), 3)

        eq_(len(self.get_s().query(Q(foo__in=['car', 'bar']))), 3)

    def test_q_range(self):
        eq_(len(self.get_s().query(height__gt=10)), 1)
        eq_(len(self.get_s().query(height__gte=7)), 3)
        eq_(len(self.get_s().query(height__lt=10)), 4)
        eq_(len(self.get_s().query(height__lte=7)), 4)

        eq_(len(self.get_s().query(Q(height__gt=10))), 1)
        eq_(len(self.get_s().query(Q(height__gte=7))), 3)
        eq_(len(self.get_s().query(Q(height__lt=10))), 4)
        eq_(len(self.get_s().query(Q(height__lte=7))), 4)

    def test_q_range_action(self):
        eq_(len(self.get_s().query(height__range=(10, 20))), 1)
        eq_(len(self.get_s().query(height__range=(0, 7))), 4)
        eq_(len(self.get_s().query(height__range=(5, 7))), 3)

        eq_(len(self.get_s().query(Q(height__range=(10, 20)))), 1)
        eq_(len(self.get_s().query(Q(height__range=(0, 7)))), 4)
        eq_(len(self.get_s().query(Q(height__range=(5, 7)))), 3)

        # Try a boosted query to verify it still works.
        eq_(len(self.get_s().query(height__range=(5, 7))
                            .boost(height__range=100)), 3)

    def test_q_match(self):
        eq_(len(self.get_s().query(foo__match='car')), 2)

        eq_(len(self.get_s().query(Q(foo__match='car'))), 2)

    def test_q_prefix(self):
        eq_(len(self.get_s().query(foo__prefix='ca')), 2)

        eq_(len(self.get_s().query(Q(foo__prefix='ca'))), 2)

    def test_q_match_phrase(self):
        # Doing a match query for the two words in either order kicks up
        # two results.
        eq_(len(self.get_s().query(foo__match='train car')), 2)
        eq_(len(self.get_s().query(foo__match='car train')), 2)

        eq_(len(self.get_s().query(Q(foo__match='train car'))), 2)
        eq_(len(self.get_s().query(Q(foo__match='car train'))), 2)

        # Doing a match_phrase query for the two words in the right
        # order kicks up one result.
        eq_(len(self.get_s().query(foo__match_phrase='train car')), 1)

        eq_(len(self.get_s().query(Q(foo__match_phrase='train car'))), 1)

        # Doing a match_phrase query for the two words in the wrong
        # order kicks up no results.
        eq_(len(self.get_s().query(foo__match_phrase='car train')), 0)

        eq_(len(self.get_s().query(Q(foo__match_phrase='car train'))), 0)

    def test_q_fuzzy(self):
        # Mispelled word gets no results with match query.
        eq_(len(self.get_s().query(foo__match='tran')), 0)

        eq_(len(self.get_s().query(Q(foo__match='tran'))), 0)

        # Mispelled word gets one result with fuzzy query.
        eq_(len(self.get_s().query(foo__fuzzy='tran')), 1)

        eq_(len(self.get_s().query(Q(foo__fuzzy='tran'))), 1)

    def test_q_wildcard(self):
        eq_(len(self.get_s().query(foo__wildcard='tra*n')), 1)
        eq_(len(self.get_s().query(foo__wildcard='tra?n')), 1)

        eq_(len(self.get_s().query(Q(foo__wildcard='tra*n'))), 1)
        eq_(len(self.get_s().query(Q(foo__wildcard='tra?n'))), 1)

    def test_q_demote(self):
        s = self.get_s().query(foo__match='car')
        scores = [(sr['id'], sr.es_meta.score) for sr in s.values_dict('id')]

        s = s.demote(0.5, width__term='5')
        demoted_scores = [(sr['id'], sr.es_meta.score) for sr in s.values_dict('id')]

        # These are both sorted by scores. We're demoting one result
        # so the top result in each list is different.
        assert scores[0] != demoted_scores

        # Now we do the whole thing again with Qs.
        s = self.get_s().query(Q(foo__match='car'))
        scores = [(sr['id'], sr.es_meta.score) for sr in s.values_dict('id')]

        s = s.demote(0.5, Q(width__term='5'))
        demoted_scores = [(sr['id'], sr.es_meta.score) for sr in s.values_dict('id')]

        # These are both sorted by scores. We're demoting one result
        # so the top result in each list is different.
        assert scores[0] != demoted_scores

    def test_q_query_string(self):
        eq_(len(self.get_s().query(foo__query_string='car AND train')), 1)
        eq_(len(self.get_s().query(foo__query_string='car OR duck')), 3)

        eq_(len(self.get_s().query(Q(foo__query_string='car AND train'))), 1)
        eq_(len(self.get_s().query(Q(foo__query_string='car OR duck'))), 3)

        # You can query against different fields with the query_string.
        eq_(len(self.get_s().query(foo__query_string='tag:boat OR car')), 3)

        eq_(len(self.get_s().query(Q(foo__query_string='tag:boat OR car'))), 3)

    def test_q_bad_field_action(self):
        with self.assertRaises(InvalidFieldActionError):
            len(self.get_s().query(foo__foo='awesome'))

        with self.assertRaises(InvalidFieldActionError):
            len(self.get_s().query(Q(foo__foo='awesome')))

    def test_deprecated_q_or_(self):
        s = self.get_s().query(or_={'foo': 'car', 'tag': 'boat'})
        eqish_(s.build_search(),
            {
                'query': {
                    'bool': {
                        'should': [
                            {'term': {'foo': 'car'}},
                            {'term': {'tag': 'boat'}}
                        ]
                    }
                }
            }
        )

    def test_bad_search(self):
        with self.assertRaises(BadSearch):
            len(S().doctypes('abc'))

    def test_query_raw(self):
        s = self.get_s().query_raw({'match': {'title': 'example'}})
        eq_(s.build_search(),
            {'query': {'match': {'title': 'example'}}})

    def test_query_raw_overrides_everything(self):
        s = self.get_s().query_raw({'match': {'title': 'example'}})
        s = s.query(foo__match='foo')
        s = s.demote(0.5, title__match='bar')
        s = s.boost(title=5.0)

        eq_(s.build_search(),
            {'query': {'match': {'title': 'example'}}})

    def test_boost(self):
        """Boosted queries shouldn't raise a SearchPhaseExecutionException."""
        q1 = (self.get_s()
                  .boost(foo=4.0)
                  .query(foo='car', foo__match='car', foo__match_phrase='car'))

        # Make sure the query executes without throwing an exception.
        list(q1)

        # Verify it's producing the correct query.
        eqish_(q1.build_search(),
            {
                'query': {
                    'bool': {
                        'must': [
                            {'match_phrase': {'foo': {'query': 'car', 'boost': 4.0}}},
                            {'term': {'foo': {'value': 'car', 'boost': 4.0}}},
                            {'match': {'foo': {'query': 'car', 'boost': 4.0}}}
                        ]
                    }
                }
            })

        # Do the same thing with Qs.
        q1 = (self.get_s()
                  .boost(foo=4.0)
                  .query(Q(foo='car', foo__match='car', foo__match_phrase='car')))

        # Make sure the query executes without throwing an exception.
        list(q1)

        # Verify it's producing the correct query.
        eqish_(q1.build_search(),
            {
                'query': {
                    'bool': {
                        'must': [
                            {'match_phrase': {'foo': {'query': 'car', 'boost': 4.0}}},
                            {'term': {'foo': {'value': 'car', 'boost': 4.0}}},
                            {'match': {'foo': {'query': 'car', 'boost': 4.0}}}
                        ]
                    }
                }
            })

    def test_boost_overrides(self):
        def _get_queries(search):
            # The stuff we want is buried in the search and it's in
            # the 'must' list where each item in the list is a dict
            # with a single key. So we extract that and put it in a
            # dict so we don't have to deal with the order of things
            # in the 'must' list.
            if six.PY2:
                out = dict([clause.items()[0]
                         for clause in search['query']['bool']['must']])
            else:
                out = dict([list(clause.items())[0]
                         for clause in search['query']['bool']['must']])
            return out

        q1 = self.get_s().boost(foo=4.0).query(foo='car', foo__prefix='car')
        eq_(_get_queries(q1.build_search())['term']['foo']['boost'], 4.0)
        eq_(_get_queries(q1.build_search())['prefix']['foo']['boost'], 4.0)

        q1 = q1.boost(foo=2.0)
        eq_(_get_queries(q1.build_search())['term']['foo']['boost'], 2.0)
        eq_(_get_queries(q1.build_search())['prefix']['foo']['boost'], 2.0)

        q1 = q1.boost(foo__prefix=4.0)
        eq_(_get_queries(q1.build_search())['term']['foo']['boost'], 2.0)
        eq_(_get_queries(q1.build_search())['prefix']['foo']['boost'], 4.0)

        # Note: We don't actually want to test whether the score for
        # an item goes up by adding a boost to the search because
        # boosting doesn't actually work like that. There's a
        # queryNorm factor which is 1/sqrt(boosts) which normalizes
        # the results from a query allowing you to compare
        # queries. Thus, doing a query, adding a boost and doing it
        # again doesn't increase the score for the item.
        #
        # Figured I'd mention that in case someone was looking at the
        # tests and was like, "Hey--this is missing!"

    def test_boolean_query_compled(self):
        """Verify that should/must/must_not collapses right"""
        s = self.get_s()

        eq_((s.query(Q(foo='should', should=True),
                     bar='must')
             .build_search()),
            {
                'query': {
                    'bool': {
                        'should': [
                            {'term': {'foo': 'should'}}
                        ],
                        'must': [
                            {'term': {'bar': 'must'}}
                        ]
                    }
                }
            })

        eq_((s.query(Q(foo='should', should=True),
                     bar='must_not', must_not=True)
             .build_search()),
            {
                'query': {
                    'bool': {
                        'should': [
                            {'term': {'foo': 'should'}}
                        ],
                        'must_not': [
                            {'term': {'bar': 'must_not'}}
                        ]
                    }
                }
            })

        eq_((s.query(Q(foo='should', should=True),
                     bar='must_not', must_not=True)
             .query(Q(baz='must'))
             .build_search()),
            {
                'query': {
                    'bool': {
                        'should': [
                            {'term': {'foo': 'should'}},
                        ],
                        'must_not': [
                            {'term': {'bar': 'must_not'}}
                        ],
                        'must': [
                            {'term': {'baz': 'must'}}
                        ]
                    }
                }
            })

        # This is a pathological case. The should=True applies to the
        # foo term query and the must=True doesn't apply to
        # anything--it shouldn't override the should=True in the Q.
        eq_((s.query(Q(foo='should', should=True), must=True)
             .build_search()),
            {
                'query': {
                    'bool': {
                        'should': [
                            {'term': {'foo': 'should'}}
                        ]
                    }
                }
            })

    def test_funkyquery(self):
        """Test implementing query processors"""
        class FunkyS(S):
            def process_query_funkyquery(self, key, val, field_action):
                return {'funkyquery': {'field': key, 'value': val}}

        s = FunkyS().query(foo__funkyquery='bar')
        eq_(s.build_search(),
            {
                'query': {
                    'funkyquery': {'field': 'foo', 'value': 'bar'}
                }
            })

    def test_execute(self):
        s = self.get_s()
        results = s.execute()
        assert isinstance(results, SearchResults)

        cached = s.execute()
        assert cached is results

        # Test caching of empty results
        try:
            self.teardown_class()
            self.create_index(settings={'mappings': self.mapping})
            self.refresh()

            s = self.get_s()
            results = s.execute()
            assert isinstance(results, SearchResults)

            cached = s.execute()
            assert cached is results
        finally:
            self.setup_class()

    def test_count(self):
        s = self.get_s()
        assert isinstance(s.count(), int)

        # Make sure it works with the cached count
        s.execute()
        assert isinstance(s.count(), int)

    def test_count_empty_results(self):
        s = self.get_s()
        s.execute()

        # Simulate a situation where the result cache had 0 objects
        s._results_cache.objects = []
        s._results_cache.count = 123

        # Ensure that we are still retrieving the cached result count
        eq_(s.count(), 123)

    def test_len(self):
        assert isinstance(len(self.get_s()), int)

    def test_all(self):
        assert isinstance(self.get_s().all(), S)

    def test_everything(self):
        ret = self.get_s().everything()
        assert isinstance(ret, SearchResults)
        eq_(len(ret), len(self.data))

    def test_order_by(self):
        res = self.get_s().filter(tag='awesome').order_by('-width')
        eq_([d['id'] for d in res], [5, 3, 1])

    def test_order_by_dict(self):
        res = self.get_s().filter(tag='awesome').order_by({'width': 'desc'})
        eq_([d['id'] for d in res], [5, 3, 1])

    def test_slice(self):
        s = self.get_s().filter(tag='awesome')
        eq_(s.build_search(),
            {'filter': {'term': {'tag': 'awesome'}}})
        assert isinstance(s[0], DefaultMappingType)

        eq_(s[0:1].build_search(),
            {'filter': {'term': {'tag': 'awesome'}}, 'size': 1})

        eq_(s[1:2].build_search(),
            {'filter': {'term': {'tag': 'awesome'}}, 'from': 1, 'size': 1})

    def test_explain(self):
        qs = self.get_s().query(foo='car')

        assert 'explain' not in qs.build_search()

        qs = qs.explain(True)

        # You put the explain in...
        assert qs.build_search()['explain'] == True

        qs = qs.explain(False)

        # You take the explain out...
        assert 'explain' not in qs.build_search()

        # Shake it all about...
        qs = qs.explain(True)

        res = list(qs)
        assert res[0].es_meta.explanation


class FilterTest(ESTestCase):
    mapping = {
        ESTestCase.mapping_type_name: {
            'properties': {
                'id': {'type': 'integer'},
                'foo': {'type': 'string'},
                'tag': {'type': 'string'},
                'width': {'type': 'string', 'null_value': True}
                }
            }
        }

    data = [
        {'id': 1, 'foo': 'bar', 'tag': 'awesome', 'width': '2'},
        {'id': 2, 'foo': 'bart', 'tag': 'boring', 'width': '7'},
        {'id': 3, 'foo': 'car', 'tag': 'awesome', 'width': '5'},
        {'id': 4, 'foo': 'duck', 'tag': 'boat', 'width': '11'},
        {'id': 5, 'foo': 'car', 'tag': 'awesome', 'width': '7'},
        {'id': 6, 'foo': 'caboose', 'tag': 'end', 'width': None}
        ]

    def test_filter_empty_f(self):
        s = self.get_s().filter(F())
        eq_(s.build_search(), {})
        eq_(s.count(), 6)

    def test_filter_empty_f_or_f(self):
        s = self.get_s().filter(F() | F(tag='awesome'))
        eq_(s.build_search(), {'filter': {'term': {'tag': 'awesome'}}})
        eq_(s.count(), 3)

    def test_filter_empty_f_and_f(self):
        s = self.get_s().filter(F() & F(tag='awesome'))
        eq_(s.build_search(), {'filter': {'term': {'tag': 'awesome'}}})
        eq_(s.count(), 3)

    def test_filter_f_and_empty_f(self):
        s = self.get_s().filter(F(tag='awesome') & F())
        eq_(s.build_search(), {'filter': {'term': {'tag': 'awesome'}}})
        eq_(s.count(), 3)

    def test_filter_f_and_ff(self):
        s = self.get_s().filter(F(tag='awesome') & F(foo='car', width='7'))
        eqish_(s.build_search(),
            {
                'filter': {
                    'and': [
                        {'term': {'width': '7'}},
                        {'term': {'foo': 'car'}},
                        {'term': {'tag': 'awesome'}}
                    ]
                }
            }
        )
        eq_(s.count(), 1)

    def test_filter_empty_f_or_empty_f_or_f(self):
        s = self.get_s().filter(F() | F() | F(tag='awesome'))
        eq_(s.build_search(), {'filter': {'term': {'tag': 'awesome'}}})
        eq_(s.count(), 3)

    def test_filter_empty_f_and_empty_f_and_f(self):
        s = self.get_s().filter(F() & F() & F(tag='awesome'))
        eq_(s.build_search(), {'filter': {'term': {'tag': 'awesome'}}})
        eq_(s.count(), 3)

    def test_filter_not_not_f(self):
        f = F(tag='awesome')
        f = ~f
        f = ~f
        s = self.get_s().filter(f)
        eq_(s.build_search(), {'filter': {'term': {'tag': 'awesome'}}})
        eq_(s.count(), 3)

    def test_filter_empty_f_not(self):
        s = self.get_s().filter(~F())
        eq_(s.build_search(), {})
        eq_(s.count(), 6)

    def test_filter(self):
        eq_(len(self.get_s().filter(tag='awesome')), 3)
        eq_(len(self.get_s().filter(F(tag='awesome'))), 3)

    def test_filter_and(self):
        eq_(len(self.get_s().filter(tag='awesome', foo='bar')), 1)
        eq_(len(self.get_s().filter(tag='awesome').filter(foo='bar')), 1)
        eq_(len(self.get_s().filter(F(tag='awesome') & F(foo='bar'))), 1)

    def test_filter_or(self):
        s = self.get_s().filter(F(tag='awesome') | F(tag='boat'))
        eq_(s.count(), 4)

    def test_filter_or_3(self):
        s = self.get_s().filter(F(tag='awesome') | F(tag='boat') |
                                F(tag='boring'))
        eqish_(s.build_search(), {
                'filter': {
                    'or': [
                        {'term': {'tag': 'awesome'}},
                        {'term': {'tag': 'boat'}},
                        {'term': {'tag': 'boring'}}
                    ]
                }
        })
        eq_(s.count(), 5)

        # This is kind of a crazy case.
        s = self.get_s().filter(or_={'foo': 'bar',
                                     'or_': {'tag': 'boat', 'width': '5'}})
        eqish_(s.build_search(), {
                'filter': {
                    'or': [
                        {'or': [
                                {'term': {'width': '5'}},
                                {'term': {'tag': 'boat'}}
                        ]},
                        {'term': {'foo': 'bar'}}
                    ]
                }
        })
        eq_(s.count(), 3)

    def test_filter_complicated(self):
        eq_(len(self.get_s().filter(F(tag='awesome', foo='bar') |
                                     F(tag='boring'))), 2)

    def test_filter_not(self):
        s = self.get_s().filter(~F(tag='awesome'))
        eq_(s.build_search(), {
                'filter': {
                    'not': {
                        'filter': {'term': {'tag': 'awesome'}}
                    }
                }
        })
        eq_(s.count(), 3)

        s = self.get_s().filter(~(F(tag='boring') | F(tag='boat')))
        eqish_(s.build_search(), {
                'filter': {
                    'not': {
                        'filter': {
                            'or': [
                                {'term': {'tag': 'boring'}},
                                {'term': {'tag': 'boat'}}
                            ]
                        }
                    }
                }
        })
        eq_(s.count(), 4)

        s = self.get_s().filter(~F(tag='boat')).filter(~F(foo='bar'))
        eqish_(s.build_search(), {
                'filter': {
                    'and': [
                        {'not': {'filter': {'term': {'tag': 'boat'}}}},
                        {'not': {'filter': {'term': {'foo': 'bar'}}}}
                    ]
                }
        })
        eq_(s.count(), 4)

        s = self.get_s().filter(~F(tag='boat', foo='barf'))
        eqish_(s.build_search(), {
                'filter': {
                    'not': {
                        'filter': {
                            'and': [
                                {'term': {'foo': 'barf'}},
                                {'term': {'tag': 'boat'}}
                            ]
                        }
                    }
                }
        })
        eq_(s.count(), 6)

    def test_filter_in(self):
        eq_(len(self.get_s().filter(foo__in=['car', 'bar'])), 3)

    def test_filter_prefix(self):
        eq_(len(self.get_s().filter(foo__prefix='c')), 3)

    def test_filter_bad_field_action(self):
        with self.assertRaises(InvalidFieldActionError):
            len(self.get_s().filter(F(tag__faux='awesome')))

    def test_filter_with_none_value(self):
        eq_(len(self.get_s().filter(width=None)), 1)

    def test_f_mutation_with_and(self):
        """Make sure AND doesn't mutate operands."""
        f1 = F(fielda='tag', fieldb='boat')
        f2 = F(fieldc='car')

        f1 & f2
        # Should only contain f1 filters.
        eq_(sorted(f1.filters[0]['and']),
            sorted([('fielda', 'tag'), ('fieldb', 'boat')]))

        # Should only contain f2 filters.
        eq_(f2.filters, [('fieldc', 'car')])

    def test_f_mutation_with_or(self):
        """Make sure OR doesn't mutate operands."""
        f1 = F(fielda='tag', fieldb='boat')
        f2 = F(fieldc='car')

        f1 | f2
        # Should only contain f1 filters.
        eq_(sorted(f1.filters[0]['and']),
            sorted([('fielda', 'tag'), ('fieldb', 'boat')]))

        # Should only contain f2 filters.
        eq_(f2.filters, [('fieldc', 'car')])

    def test_f_mutation_with_not(self):
        """Make sure NOT doesn't mutate operands."""
        f1 = F(fielda='tag')
        f2 = ~f1

        # Change f2 to see if it changes f1.
        f2.filters[0]['not']['filter'] = [('fielda', 'boat')]

        # Should only contain f1 filters.
        eq_(f1.filters, [('fielda', 'tag')])

        # Should only contain f2 tweaked filter.
        eq_(f2.filters, [{'not': {'filter': [('fielda', 'boat')]}}])

    def test_funkyfilter(self):
        """Test implementing filter processors"""
        class FunkyS(S):
            def process_filter_funkyfilter(self, key, val, field_action):
                return {'funkyfilter': {'field': key, 'value': val}}

        s = FunkyS().filter(foo__funkyfilter='bar')
        eq_(s.build_search(), {
                'filter': {
                    'funkyfilter': {'field': 'foo', 'value': 'bar'}
                }
        })

    def test_filter_range(self):
        eq_(len(self.get_s().filter(id__gt=3)), 3)
        eq_(len(self.get_s().filter(id__gte=3)), 4)
        eq_(len(self.get_s().filter(id__lt=3)), 2)
        eq_(len(self.get_s().filter(id__lte=3)), 3)


    def test_filter_range_action(self):
        eq_(len(self.get_s().filter(id__range=(3, 10))), 4)
        eq_(len(self.get_s().filter(id__range=(0, 3))), 3)

    def test_filter_raw(self):
        s = self.get_s().filter_raw({'term': {'tag': 'awesome'}})
        eq_(s.build_search(),
            {'filter': {'term': {'tag': 'awesome'}}})

    def test_filter_raw_overrides_everything(self):
        s = self.get_s().filter_raw({'term': {'tag': 'awesome'}})
        s = s.filter(tag='boring')
        s = s.filter(F(tag='end'))
        eq_(s.build_search(),
            {'filter': {'term': {'tag': 'awesome'}}})


class FacetTest(ESTestCase):
    def setUp(self):
        super(FacetTest, self).setUp()
        self.cleanup_index()
        self.create_index()

    def tearDown(self):
        super(FacetTest, self).tearDown()
        self.cleanup_index()

    def test_facet(self):
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome'},
                {'id': 2, 'foo': 'bart', 'tag': 'boring'},
                {'id': 3, 'foo': 'car', 'tag': 'awesome'},
                {'id': 4, 'foo': 'duck', 'tag': 'boat'},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome'},
            ])
        FacetTest.refresh()

        qs = self.get_s().facet('tag')
        eq_(facet_counts_dict(qs, 'tag'), dict(awesome=3, boring=1, boat=1))

    def test_facet_with_size(self):
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome'},
                {'id': 2, 'foo': 'bart', 'tag': 'boring'},
                {'id': 3, 'foo': 'car', 'tag': 'awesome'},
                {'id': 4, 'foo': 'duck', 'tag': 'boat'},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome'},
                {'id': 6, 'foo': 'canoe', 'tag': 'boat'},
            ])
        FacetTest.refresh()

        qs = self.get_s()
        eq_(facet_counts_dict(qs.facet('tag'), 'tag'),
            {u'boring': 1, u'awesome': 3, u'boat': 2})
        eq_(facet_counts_dict(qs.facet('tag', size=2), 'tag'),
            {u'awesome': 3, u'boat': 2})

    def test_filtered_facet(self):
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome', 'width': 1},
                {'id': 2, 'foo': 'bart', 'tag': 'boring', 'width': 2},
                {'id': 3, 'foo': 'car', 'tag': 'awesome', 'width': 1},
                {'id': 4, 'foo': 'duck', 'tag': 'boat', 'width': 5},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome', 'width': 5},
            ])
        FacetTest.refresh()

        qs = self.get_s().query(foo='car').filter(width=5)

        # filter doesn't apply to facets
        eq_(facet_counts_dict(qs.facet('tag'), 'tag'),
            {'awesome': 2})

        # filter does apply to facets
        eq_(facet_counts_dict(qs.facet('tag', filtered=True), 'tag'),
            {'awesome': 1})

    def test_filtered_facet_with_size(self):
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome', 'width': 1},
                {'id': 2, 'foo': 'bart', 'tag': 'boring', 'width': 2},
                {'id': 3, 'foo': 'car', 'tag': 'awesome', 'width': 1},
                {'id': 4, 'foo': 'duck', 'tag': 'boat', 'width': 5},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome', 'width': 5},
                {'id': 6, 'foo': 'canoe', 'tag': 'boat', 'width': 5},
                {'id': 7, 'foo': 'plane', 'tag': 'awesome', 'width': 5},
                {'id': 8, 'foo': 'cargo plane', 'tag': 'boring', 'width': 5},
            ])
        FacetTest.refresh()

        qs = self.get_s().filter(width=5)

        # regular facet
        eq_(facet_counts_dict(qs.facet('tag'), 'tag'),
            {'boring': 2, 'awesome': 4, 'boat': 2})
        # apply the filter
        eq_(facet_counts_dict(qs.facet('tag', filtered=True), 'tag'),
            {'boring': 1, 'awesome': 2, 'boat': 2})
        # apply the filter and restrict the size
        eq_(facet_counts_dict(qs.facet('tag', size=2, filtered=True), 'tag'),
            {'awesome': 2, 'boat': 2})

    def test_filtered_facet_no_filters(self):
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome', 'width': 1},
                {'id': 2, 'foo': 'bart', 'tag': 'boring', 'width': 2},
                {'id': 3, 'foo': 'car', 'tag': 'awesome', 'width': 1},
                {'id': 4, 'foo': 'duck', 'tag': 'boat', 'width': 5},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome', 'width': 5},
            ])
        FacetTest.refresh()

        qs = self.get_s().query(foo='car')

        # filtered=True doesn't cause a KeyError when there are no
        # filters
        eq_(facet_counts_dict(qs.facet('tag', filtered=True), 'tag'),
            {'awesome': 2})

    def test_global_facet(self):
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome'},
                {'id': 2, 'foo': 'bart', 'tag': 'boring'},
                {'id': 3, 'foo': 'car', 'tag': 'awesome'},
                {'id': 4, 'foo': 'duck', 'tag': 'boat'},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome'}
            ])
        FacetTest.refresh()

        qs = self.get_s().query(foo='car').filter(width=5)

        # facet restricted to query
        eq_(facet_counts_dict(qs.facet('tag'), 'tag'),
            {'awesome': 2})

        # facet applies to all of corpus
        eq_(facet_counts_dict(qs.facet('tag', global_=True), 'tag'),
            dict(awesome=3, boring=1, boat=1))

    def test_facet_raw(self):
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome'},
                {'id': 2, 'foo': 'bart', 'tag': 'boring'},
                {'id': 3, 'foo': 'car', 'tag': 'awesome'},
                {'id': 4, 'foo': 'duck', 'tag': 'boat'},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome'}
            ])
        FacetTest.refresh()

        qs = self.get_s().facet_raw(tags={'terms': {'field': 'tag'}})
        eq_(facet_counts_dict(qs, 'tags'),
            dict(awesome=3, boring=1, boat=1))

        qs = (self.get_s()
              .query(foo='car')
              .facet_raw(tags={'terms': {'field': 'tag'}}))
        eq_(facet_counts_dict(qs, 'tags'),
            {'awesome': 2})

    def test_facet_raw_overrides_facet(self):
        """facet_raw overrides facet with the same facet name."""
        FacetTest.index_data([
                {'id': 1, 'foo': 'bar', 'tag': 'awesome'},
                {'id': 2, 'foo': 'bart', 'tag': 'boring'},
                {'id': 3, 'foo': 'car', 'tag': 'awesome'},
                {'id': 4, 'foo': 'duck', 'tag': 'boat'},
                {'id': 5, 'foo': 'train car', 'tag': 'awesome'}
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .query(foo='car')
              .facet('tag')
              .facet_raw(tag={'terms': {'field': 'tag'}, 'global': True}))
        eq_(facet_counts_dict(qs, 'tag'),
            dict(awesome=3, boring=1, boat=1))

    def test_facet_terms(self):
        """Test terms facet"""
        FacetTest.index_data([
                {'id': 1, 'color': 'red'},
                {'id': 2, 'color': 'red'},
                {'id': 3, 'color': 'red'},
                {'id': 4, 'color': 'yellow'},
                {'id': 5, 'color': 'yellow'},
                {'id': 6, 'color': 'green'},
                {'id': 7, 'color': 'blue'},
                {'id': 8, 'color': 'white'},
                {'id': 9, 'color': 'brown'},
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .facet_raw(created1={
                    'terms': {
                        'field': 'color',
                        'size': 2
                    }
              })
        )

        data = qs.facet_counts()
        eq_(data["created1"].data,
            [
                {u'count': 3, u'term': u'red'},
                {u'count': 2, u'term': u'yellow'}
            ]
        )


    def test_facet_terms_other(self):
        """Test terms facet"""
        FacetTest.index_data([
                {'id': 1, 'color': 'red'},
                {'id': 2, 'color': 'red'},
                {'id': 3, 'color': 'red'},
                {'id': 4, 'color': 'yellow'},
                {'id': 5, 'color': 'yellow'},
                {'id': 6, 'color': 'green'},
                {'id': 7, 'color': 'blue'},
                {'id': 8, 'color': 'white'},
                {'id': 9, 'color': 'brown'},
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .facet_raw(created1={
                    'terms': {
                        'field': 'color',
                        'size': 2
                    }
              })
        )

        data = qs.facet_counts()
        eq_(data["created1"].other, 4)

    def test_facet_terms_missing(self):
        """Test terms facet"""
        FacetTest.index_data([
                {'id': 1, 'color': 'red'},
                {'id': 2, 'color': 'red'},
                {'id': 3, 'color': 'red'},
                {'id': 4, 'color': 'yellow'},
                {'id': 5, 'colors': 'yellow'},
                {'id': 6, 'colors': 'green'},
                {'id': 7, 'colors': 'blue'},
                {'id': 8, 'colors': 'white'},
                {'id': 9, 'colors': 'brown'},
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .facet_raw(created1={
                    'terms': {
                        'field': 'color'
                    }
              })
        )

        data = qs.facet_counts()
        eq_(data["created1"].missing, 5)

    def test_facet_range(self):
        """Test range facet"""
        FacetTest.index_data([
                {'id': 1, 'value': 1},
                {'id': 2, 'value': 1},
                {'id': 3, 'value': 1},
                {'id': 4, 'value': 2},
                {'id': 5, 'value': 2},
                {'id': 6, 'value': 3},
                {'id': 7, 'value': 3},
                {'id': 8, 'value': 3},
                {'id': 9, 'value': 4},
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .facet_raw(created1={
                  'range': {
                      'field': 'value',
                      'ranges': [
                          {'from': 0, 'to': 5},
                          {'from': 5, 'to': 20}
                      ]
                  }
              })
        )

        data = qs.facet_counts()
        eq_(data["created1"].data,
            [
                {u'count': 9, u'from': 0.0, u'min': 1.0, u'max': 4.0,
                 u'to': 5.0, u'total_count': 9, u'total': 20.0,
                 u'mean': 2.2222222222222223},
                {u'count': 0, u'from': 5.0, u'total_count': 0,
                 u'to': 20.0, u'total': 0.0, u'mean': 0.0}
            ]
        )

    def test_facet_histogram(self):
        """Test histogram facet"""
        FacetTest.index_data([
                {'id': 1, 'value': 1},
                {'id': 2, 'value': 1},
                {'id': 3, 'value': 1},
                {'id': 4, 'value': 2},
                {'id': 5, 'value': 2},
                {'id': 6, 'value': 3},
                {'id': 7, 'value': 3},
                {'id': 8, 'value': 3},
                {'id': 9, 'value': 4},
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .facet_raw(created1={
                    'histogram': {
                        'interval': 2, 'field': 'value'
                        }
                    }))

        data = qs.facet_counts()
        eq_(data["created1"].data, [
                    {u'key': 0, u'count': 3},
                    {u'key': 2, u'count': 5},
                    {u'key': 4, u'count': 1},
                ])

    def test_facet_date_histogram(self):
        """facet_raw with date_histogram works."""
        today = datetime.now()
        tomorrow = today + timedelta(days=1)

        FacetTest.index_data([
                {'id': 1, 'created': today},
                {'id': 2, 'created': today},
                {'id': 3, 'created': tomorrow},
                {'id': 4, 'created': tomorrow},
                {'id': 5, 'created': tomorrow},
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .facet_raw(created1={
                    'date_histogram': {
                        'interval': 'day', 'field': 'created'
                        }
                    }))

        # TODO: This is a mediocre test because it doesn't test the
        # dates and it probably should.
        facet_counts = [item['count']
                        for item in qs.facet_counts()['created1']]
        eq_(sorted(facet_counts), [2, 3])

    def test_facet_statistical(self):
        """Test statistical facet"""
        FacetTest.index_data([
                {'id': 1, 'value': 1},
                {'id': 2, 'value': 1},
                {'id': 3, 'value': 1},
                {'id': 4, 'value': 2},
                {'id': 5, 'value': 2},
                {'id': 6, 'value': 3},
                {'id': 7, 'value': 3},
                {'id': 8, 'value': 3},
                {'id': 9, 'value': 4},
            ])
        FacetTest.refresh()

        qs = (self.get_s()
              .facet_raw(created1={
                    'statistical': {
                        'field': 'value'
                    }
              })
            )
        data = qs.facet_counts()
        stat = data['created1']
        eq_(stat.count, 9)
        eq_(stat._type, u'statistical')
        eq_(stat.min, 1.0)
        eq_(stat.sum_of_squares, 54.0)
        eq_(stat.max, 4.0)
        eq_(stat.std_deviation, 1.0304020550550783)
        eq_(stat.variance, 1.0617283950617287)
        eq_(stat.total, 20.0)
        eq_(stat.mean, 2.2222222222222223)

    def test_filter_facet(self):
        """Test filter facet"""
        FacetTest.index_data([
            {'id': 1, 'color': 'red'},
            {'id': 2, 'color': 'red'},
            {'id': 3, 'color': 'red'},
            {'id': 4, 'color': 'yellow'},
            {'id': 5, 'color': 'yellow'},
            {'id': 6, 'color': 'green'},
            {'id': 7, 'color': 'blue'},
            {'id': 8, 'color': 'white'},
            {'id': 9, 'color': 'brown'},
        ])
        FacetTest.refresh()

        red_or_yellow_filter = {
            'filter': {
                'or': [
                    {'term': {'color': 'red'}},
                    {'term': {'color': 'yellow'}},
                ]
            }
        }
        qs = (self.get_s().facet_raw(red_or_yellow=red_or_yellow_filter))

        data = qs.facet_counts()
        eq_(data['red_or_yellow']._type, 'filter')
        eq_(data['red_or_yellow'].count, 5)

    def test_query_facet(self):
        """Test query facet"""
        FacetTest.index_data([
            {'id': 1, 'color': 'red'},
            {'id': 2, 'color': 'red'},
            {'id': 3, 'color': 'red'},
            {'id': 4, 'color': 'yellow'},
            {'id': 5, 'color': 'yellow'},
            {'id': 6, 'color': 'green'},
            {'id': 7, 'color': 'blue'},
            {'id': 8, 'color': 'white'},
            {'id': 9, 'color': 'brown'},
        ])
        FacetTest.refresh()

        red_query = {
            'query': {
                'term': {'color': 'red'},
            }
        }
        qs = (self.get_s().facet_raw(red_query=red_query))

        data = qs.facet_counts()

        eq_(data['red_query']._type, 'query')
        eq_(data['red_query'].count, 3)

    def test_invalid_field_type(self):
        """Invalid _type should raise InvalidFacetType."""
        FacetTest.index_data([
                {'id': 1, 'age': 30},
                {'id': 2, 'age': 40}
            ])
        FacetTest.refresh()

        # Note: This uses a terms_stats facet. If we implement handling
        # for that, then we need to pick another facet type to fail on
        # or do the right thing and mock the test.
        # Note: Previously this used histogram and statistical facets,
        # but those were implemented.
        with self.assertRaises(InvalidFacetType):
            (self.get_s()
                 .facet_raw(created1={'terms_stats': {'key_field': 'age',
                                                      'value_field': 'age'}})
                 .facet_counts())


class HighlightTest(ESTestCase):
    data = [
        {'id': 1, 'foo': 'bar', 'tag': 'awesome', 'width': '2'},
        {'id': 2, 'foo': 'bart', 'tag': 'boring', 'width': '7'},
        {'id': 3, 'foo': 'car', 'tag': 'awesome', 'width': '5'},
        {'id': 4, 'foo': 'duck', 'tag': 'boat', 'width': '11'},
        {'id': 5, 'foo': 'train car', 'tag': 'awesome', 'width': '7'}
    ]

    def test_highlight_with_dict_results(self):
        """Make sure highlighting with dict-style results works.

        Highlighting should work on all fields specified in the ``highlight()``
        call, not just the ones mentioned in the query or in ``values_dict()``.

        """
        s = (self.get_s().query(foo__match='car')
                         .filter(id=5)
                         .highlight('tag', 'foo'))
        result = list(s)[0]
        # The highlit text from the foo field should be in index 1 of the
        # excerpts.
        eq_(result.es_meta.highlight['foo'], [u'train <em>car</em>'])

        s = (self.get_s().query(foo__match='car')
                         .filter(id=5)
                         .highlight('tag', 'foo')
                         .values_dict('tag', 'foo'))
        result = list(s)[0]
        # The highlit text from the foo field should be in index 1 of the
        # excerpts.
        eq_(result.es_meta.highlight['foo'], [u'train <em>car</em>'])

    def test_highlight_on_list_results(self):
        """Make sure highlighting with list-style results works.

        Highlighting should work on all fields specified in the ``highlight()``
        call, not just the ones mentioned in the query or in ``values_list()``.

        """
        s = (self.get_s().query(foo__match='car')
                         .filter(id=5)
                         .highlight('tag', 'foo')
                         .values_list('tag', 'foo'))
        result = list(s)[0]
        # The highlit text from the foo field should be in index 1 of the
        # excerpts.
        eq_(result.es_meta.highlight['foo'], [u'train <em>car</em>'])

    def test_highlight_options(self):
        """Make sure highlighting with options works."""
        s = (self.get_s().query(foo__match='car')
                         .filter(id=5)
                         .highlight('tag', 'foo',
                                    pre_tags=['<b>'],
                                    post_tags=['</b>']))
        result = list(s)[0]
        # The highlit text from the foo field should be in index 1 of the
        # excerpts.
        eq_(result.es_meta.highlight['foo'], [u'train <b>car</b>'])

    def test_highlight_cumulative(self):
        """Make sure highlighting fields are cumulative and none clears them."""
        # No highlighted fields means no highlights.
        s = (self.get_s().query(foo__match='car')
                         .filter(id=5)
                         .highlight())
        eq_(list(s)[0].es_meta.highlight, {})

        # Add a field and that gets highlighted.
        s = s.highlight('foo')
        eq_(list(s)[0].es_meta.highlight['foo'], [u'train <em>car</em>'])

        # Set it back to no fields and no highlight.
        s = s.highlight(None)
        eq_(list(s)[0].es_meta.highlight, {})


class SearchTypeTest(ESTestCase):
    # Set the mapping so shard allocation is controlled manually
    mapping = {
        ESTestCase.mapping_type_name: {
            'properties': {
                'id': {'type': 'integer'},
                'shard': {'type': 'integer'},
                'text': {'type': 'string'},
            },
            'order': {
                '_routing': {
                    'required': True,
                    'path': 'shard',
                },
            },
        }
    }

    @classmethod
    def setup_class(cls):
        super(SearchTypeTest, cls).setup_class()
        cls.cleanup_index()

        # Explicitly create an index with 2 shards. The default
        # ES configuration is 5 shards, and should work as well,
        # but that could have been overridden (even though it is
        # a bad practice to create an index with one shard only).
        cls.create_index(settings={
            'number_of_shards': 2,
        })
        # These records will be allocated into different shards
        cls.index_data([
            {'id': 1, 'shard': 1, 'text': 'asdf'},
            {'id': 2, 'shard': 2, 'text': 'asdf'},
        ])
        cls.refresh()

    def test_query_and_fetch(self):
        s = self.get_s().search_type('query_and_fetch')

        # query_and_fetch combines results from every shard, therefore
        # limiting the query to 1 result will still produce two
        eq_(len(s[:1]), 2)


class SuggestionTest(ESTestCase):
    data = [
        {'id': 1, 'name': 'bar'},
        {'id': 2, 'name': 'mark', 'location': 'mart'},
        {'id': 3, 'name': 'car'},
        {'id': 4, 'name': 'duck'},
        {'id': 5, 'name': 'train car'}
    ]

    @require_version('0.90')
    def test_suggestions(self):
        """Make sure correct suggestions are being returned.

        Test adding multiple ``suggest()`` clauses to the query, including
        different fields.

        """
        s = (self.get_s().query(name__match='mary')
                         .suggest('mysuggest', 'mary'))
        suggestions = s.suggestions()
        options = [o['text'] for o in suggestions['mysuggest'][0]['options']]
        eq_(options, ['mark', 'mart'])

        s = (self.get_s().query(name__match='mary')
                         .suggest('mysuggest', 'mary', field='name'))
        suggestions = s.suggestions()
        options = [o['text'] for o in suggestions['mysuggest'][0]['options']]
        eq_(options, ['mark'])


def test_to_python():
    def check_to_python(obj, expected):
        eq_(S().to_python(obj), expected)

    tests = [
        (
            {'date': '2013-05-15T15:00:00'},
            {'date': datetime(2013, 5, 15, 15, 0, 0)}
        ),
        (
            {'foo': {'date': '2013-05-15T15:00:00'}},
            {'foo': {'date': datetime(2013, 5, 15, 15, 0, 0)}}
        ),
        (
            {'foo': ['2013-05-15T15:00:00', '2013-03-03T03:00:00']},
            {'foo': [datetime(2013, 5, 15, 15, 0, 0),
                     datetime(2013, 3, 3, 3, 0, 0)]}
        ),
        (
            {'date': '2013-05-15ou8120:00'},
            {'date': '2013-05-15ou8120:00'},
        ),
        (
            {'date': u'\x00013-05-15T15:00:00'},
            {'date': u'\x00013-05-15T15:00:00'},
        ),
        (
            {'date': '2013-05-15T15:00:00.123456'},
            {'date': datetime(2013, 5, 15, 15, 0, 0, 123456)}
        ),
    ]

    for obj, expected in tests:
        yield check_to_python, obj, expected

########NEW FILE########
__FILENAME__ = test_results
from datetime import date, datetime

from nose.tools import eq_

from elasticutils import S, DefaultMappingType, NoModelError, MappingType
from elasticutils.tests import ESTestCase


model_cache = []


def reset_model_cache():
    del model_cache[0:]


class Meta(object):
    def __init__(self, db_table):
        self.db_table = db_table


class Manager(object):
    def filter(self, id__in=None):
        return [m for m in model_cache if m.id in id__in]


class FakeModel(object):
    _meta = Meta('fake')
    objects = Manager()

    def __init__(self, **kw):
        for key in kw:
            setattr(self, key, kw[key])
        model_cache.append(self)

    @classmethod
    def get(cls, id):
        id = int(id)
        return [m for m in model_cache if m.id == id][0]


class FakeMappingType(MappingType):
    @classmethod
    def get_index(cls):
        return 'elasticutilstestfmt'

    @classmethod
    def get_mapping_type_name(cls):
        return 'elasticutilsdoctypefmt'

    def get_model(self):
        return FakeModel


class TestResultsWithData(ESTestCase):
    data = [
        {'id': 1, 'foo': 'bar', 'tag': 'awesome', 'width': '2'},
        {'id': 2, 'foo': 'bart', 'tag': 'boring', 'width': '7'},
        {'id': 3, 'foo': 'car', 'tag': 'awesome', 'width': '5'},
        {'id': 4, 'foo': 'duck', 'tag': 'boat', 'width': '11'},
        {'id': 5, 'foo': 'train car', 'tag': 'awesome', 'width': '7'}
    ]

    @classmethod
    def teardown_class(cls):
        super(TestResultsWithData, cls).teardown_class()
        reset_model_cache()

    def test_default_results_are_default_mapping_type(self):
        """With untyped S, return dicts."""
        # Note: get_s with no args should return an untyped S
        searcher = list(self.get_s().query(foo='bar'))
        assert isinstance(searcher[0], DefaultMappingType)

    def test_typed_s_returns_type(self):
        """With typed S, return objects of type."""
        searcher = list(self.get_s(FakeMappingType).query(foo='bar'))
        assert isinstance(searcher[0], FakeMappingType)

    def test_values_dict_results(self):
        """With values_dict, return list of dicts."""
        searcher = list(self.get_s().query(foo='bar').values_dict())
        assert isinstance(searcher[0], dict)

    def test_values_list_no_fields(self):
        """Specifying no fields with values_list defaults to ['id']."""
        searcher = list(self.get_s().query(foo='bar').values_list())
        assert isinstance(searcher[0], tuple)
        # We sort the result and expected result here so that the
        # order is stable and comparable.
        eq_(
            sorted(searcher[0], key=str),
            sorted((u'2', u'bar', u'awesome', 1), key=str))

    def test_values_list_results(self):
        """With values_list fields, returns list of tuples."""
        searcher = list(self.get_s().query(foo='bar')
                                    .values_list('foo', 'width'))
        assert isinstance(searcher[0], tuple)

    def test_default_results_form_has_metadata(self):
        """Test default results form has metadata."""
        searcher = list(self.get_s().query(foo='bar'))
        assert hasattr(searcher[0], '_id')
        assert hasattr(searcher[0].es_meta, 'id')
        assert hasattr(searcher[0].es_meta, 'score')
        assert hasattr(searcher[0].es_meta, 'source')
        assert hasattr(searcher[0].es_meta, 'type')
        assert hasattr(searcher[0].es_meta, 'explanation')
        assert hasattr(searcher[0].es_meta, 'highlight')

    def test_values_list_form_has_metadata(self):
        """Test default results form has metadata."""
        searcher = list(self.get_s().query(foo='bar').values_list('id'))
        assert hasattr(searcher[0], '_id')
        assert hasattr(searcher[0].es_meta, 'id')
        assert hasattr(searcher[0].es_meta, 'score')
        assert hasattr(searcher[0].es_meta, 'source')
        assert hasattr(searcher[0].es_meta, 'type')
        assert hasattr(searcher[0].es_meta, 'explanation')
        assert hasattr(searcher[0].es_meta, 'highlight')

    def test_values_dict_form_has_metadata(self):
        """Test default results form has metadata."""
        searcher = list(self.get_s().query(foo='bar').values_dict())
        assert hasattr(searcher[0], '_id')
        assert hasattr(searcher[0].es_meta, 'id')
        assert hasattr(searcher[0].es_meta, 'score')
        assert hasattr(searcher[0].es_meta, 'source')
        assert hasattr(searcher[0].es_meta, 'type')
        assert hasattr(searcher[0].es_meta, 'explanation')
        assert hasattr(searcher[0].es_meta, 'highlight')

    def test_values_dict_no_args(self):
        """Calling values_dict() with no args fetches all fields."""
        eq_(S().query(fld1=2)
               .values_dict()
               .build_search(),
            {"query": {"term": {"fld1": 2}}})

    def test_values_list_no_args(self):
        """Calling values() with no args fetches only id."""
        eq_(S().query(fld1=2)
               .values_list()
               .build_search(),
            {'query': {"term": {"fld1": 2}}})


class TestFakeMappingType(ESTestCase):
    index_name = FakeMappingType.get_index()
    mapping_type_name = FakeMappingType.get_mapping_type_name()
    data = [
        {'id': 1, 'name': 'odin skullcrusher'},
        {'id': 2, 'name': 'olaf bloodbiter'}
    ]

    @classmethod
    def setup_class(cls):
        super(TestFakeMappingType, cls).setup_class()
        for doc in cls.data:
            FakeModel(**doc)

    @classmethod
    def teardown_class(cls):
        super(TestFakeMappingType, cls).setup_class()
        reset_model_cache()

    def test_object(self):
        s = S(FakeMappingType).query(name__prefix='odin')
        eq_(len(s), 1)
        eq_(s[0].object.id, 1)


class TestResultsWithDates(ESTestCase):
    def test_dates(self):
        """Datetime strings in ES results get converted to Python datetimes"""
        self.cleanup_index()
        self.create_index(
            settings={
                'mappings': {
                    self.mapping_type_name: {
                        'id': {'type': 'integer'},
                        'bday': {'type': 'date', 'format': 'YYYY-mm-dd'},
                        'btime': {'type': 'date'}
                    }
                }
            }
        )
        data = [
            {'id': 1, 'bday': date(2012, 12, 1),
             'btime': datetime(2012, 12, 1, 12, 00)},
        ]

        self.index_data(data)
        self.refresh()

        results = list(self.get_s().values_dict())
        eq_(results,
            [{u'bday': datetime(2012, 12, 1, 0, 0),
              u'btime': datetime(2012, 12, 1, 12, 0),
              u'id': 1}]
        )

    def test_dates_lookalikes(self):
        """Datetime strings in ES results get converted to Python datetimes"""
        self.cleanup_index()
        self.create_index(
            settings={
                'mappings': {
                    self.mapping_type_name: {
                        'id': {'type': 'integer'},
                        'bday': {'type': 'string', 'analyzer': 'keyword'}
                    }
                }
            }
        )
        data = [
            {'id': 1, 'bday': 'xxxx-xx-xxTxx:xx:xx'}
        ]

        self.index_data(data)
        self.refresh()

        results = list(self.get_s().values_dict())
        eq_(results,
            [{u'id': 1, u'bday': u'xxxx-xx-xxTxx:xx:xx'}]
        )


class TestMappingType(ESTestCase):
    def setUp(self):
        super(TestMappingType, self).setUp()
        self.cleanup_index()
        self.create_index()

    def tearDown(self):
        self.cleanup_index()
        super(TestMappingType, self).tearDown()

    def test_default_mapping_type(self):
        data = [
            {'id': 1, 'name': 'Alice'}
        ]

        self.index_data(data)
        s = self.get_s(DefaultMappingType)
        result = list(s)[0]

        assert isinstance(result, DefaultMappingType)
        eq_(result.id, 1)
        self.assertRaises(NoModelError, lambda: result.object)

    def test_mapping_type_attribute_override(self):
        data = [
            {'id': 1, '_object': 'foo'}
        ]

        self.index_data(data)
        s = self.get_s(DefaultMappingType)
        result = list(s)[0]

        # Instance attribute (which starts out as None) takes precedence.
        eq_(result._object, None)
        # _object ES result field is available through __getitem__
        eq_(result['_object'], 'foo')  # key/val from ES

        # Get the ES result field id
        eq_(result.id, 1)
        # Set id to something else
        result.id = 'foo'
        # Now it returns the instance attribute
        eq_(result.id, 'foo')
        # id still available through __getitem__
        eq_(result['id'], 1)

        # If it doesn't exist, throw AttributeError
        self.assertRaises(AttributeError, lambda: result.doesnt_exist)
        # If it doesn't exist, throw KeyError
        self.assertRaises(KeyError, lambda: result['doesnt_exist'])

########NEW FILE########
__FILENAME__ = test_types
from nose.tools import eq_

from elasticutils import get_es
from elasticutils import S, MappingType, Indexable
from elasticutils.tests import ESTestCase


class FakeModel(object):
    _cache = {}

    @classmethod
    def reset(cls):
        cls._cache = {}

    def __init__(self, **fields):
        self.__dict__.update(fields)
        self._cache[fields['id']] = self

    def get(self, id):
        return self._cache[id]

    @classmethod
    def get_objects(self):
        return self._cache.values()


class FakeMappingType(MappingType, Indexable):
    @classmethod
    def get_index(cls):
        return ESTestCase.index_name

    @classmethod
    def get_mapping_type_name(cls):
        return ESTestCase.mapping_type_name

    @classmethod
    def get_model(cls):
        return FakeModel

    @classmethod
    def get_es(cls):
        return get_es(**ESTestCase.es_settings)

    @classmethod
    def get_mapping(cls):
        return {
            'properties': {
                'id': {'type': 'integer'},
                'title': {'type': 'string'},
                'tags': {'type': 'string'}
            }
        }

    @classmethod
    def extract_document(cls, obj_id, obj=None):
        if obj == None:
            obj = cls.get_model().get(id=obj_id)

        doc = {}
        doc['id'] = obj.id
        doc['title'] = obj.title
        doc['tags'] = obj.tags
        return doc

    @classmethod
    def get_indexable(cls):
        return cls.get_model().get_objects()


class ModelTest(ESTestCase):
    mapping_type_name = FakeMappingType.get_mapping_type_name()
    mapping = {
        FakeMappingType.get_mapping_type_name(): FakeMappingType.get_mapping()
    }

    def test_refresh(self):
        """Calling refresh_index shouldn't throw an exception"""
        FakeMappingType.refresh_index()

    def setUp(self):
        super(ESTestCase, self).setUp()
        ESTestCase.setup_class()

    def tearDown(self):
        super(ESTestCase, self).tearDown()
        ESTestCase.teardown_class()
        FakeModel.reset()

    def test_index(self):
        obj1 = FakeModel(id=1, title='First post!', tags=['blog', 'post'])
        FakeMappingType.index(
            FakeMappingType.extract_document(obj_id=obj1.id, obj=obj1),
            id_=obj1.id)

        FakeMappingType.refresh_index()

        s = S(FakeMappingType)
        eq_(s.count(), 1)
        eq_(list(s.execute())[0].title, obj1.title)

    def test_bulk_index(self):
        FakeModel(id=1, title='First post!', tags=['blog', 'post'])
        FakeModel(id=2, title='Second post!', tags=['blog', 'post'])

        documents = []
        for obj in FakeModel.get_objects():
            documents.append(
                FakeMappingType.extract_document(obj_id=obj.id, obj=obj))

        FakeMappingType.bulk_index(documents, id_field='id')
        FakeMappingType.refresh_index()
            
        s = S(FakeMappingType)
        eq_(s.count(), 2)
        eq_(sorted([res.title for res in s.execute()]),
            ['First post!', 'Second post!'])

    def test_unindex(self):
        obj1 = FakeModel(id=1, title='First post!', tags=['blog', 'post'])
        FakeMappingType.index(
            FakeMappingType.extract_document(obj_id=obj1.id, obj=obj1),
            id_=obj1.id)

        FakeMappingType.refresh_index()

        s = S(FakeMappingType)
        eq_(s.count(), 1)
        eq_(list(s.execute())[0].title, obj1.title)

        FakeMappingType.unindex(id_=obj1.id)
        FakeMappingType.refresh_index()

        s = S(FakeMappingType)
        eq_(s.count(), 0)

########NEW FILE########
__FILENAME__ = test_utility_functions
from unittest import TestCase

from nose.tools import eq_

from elasticutils import S
from elasticutils.utils import chunked, to_json


class Testto_json(TestCase):
    def test_to_json(self):
        eq_(to_json({'query': {'match': {'message': 'test message'}}}),
            '{"query": {"match": {"message": "test message"}}}')

        eq_(to_json(S().query(message__match='test message').build_search()),
            '{"query": {"match": {"message": "test message"}}}')


class Testchunked(TestCase):
    def test_chunked(self):
        # chunking nothing yields nothing.
        eq_(list(chunked([], 1)), [])

        # chunking list where len(list) < n
        eq_(list(chunked([1], 10)), [(1,)])

        # chunking a list where len(list) == n
        eq_(list(chunked([1, 2], 2)), [(1, 2)])

        # chunking list where len(list) > n
        eq_(list(chunked([1, 2, 3, 4, 5], 2)),
            [(1, 2), (3, 4), (5,)])

########NEW FILE########
__FILENAME__ = utils
from itertools import islice


from elasticsearch.serializer import JSONSerializer


def to_json(data):
    """Convert Python structure to JSON used by Elasticsearch

    This is a helper method that uses the elasticsearch-py
    JSONSerializer to serialize the structure. This is the serializer
    that elasticsearch-py uses to serialize data for Elasticsearch and
    handles dates.

    :arg data: Python structure (e.g. dict, list, ...)

    :returns: string

    Examples:

    >>> to_json({'query': {'match': {'message': 'test message'}}})
    '{"query": {"match": {"message": "test message"}}}'

    >>> from elasticutils import S
    >>> some_s = S().query(message__match='test message')
    >>> to_json(some_s.build_search())
    '{"query": {"match": {"message": "test message"}}}'

    """
    return JSONSerializer().dumps(data)


def chunked(iterable, n):

    """Returns chunks of n length of iterable

    If len(iterable) % n != 0, then the last chunk will have length
    less than n.

    Example:

    >>> chunked([1, 2, 3, 4, 5], 2)
    [(1, 2), (3, 4), (5,)]

    """
    iterable = iter(iterable)
    while 1:
        t = tuple(islice(iterable, n))
        if t:
            yield t
        else:
            return


def format_explanation(explanation, indent='  ', indent_level=0):
    """Return explanation in an easier to read format

    Easier to read for me, at least.

    """
    if not explanation:
        return ''

    # Note: This is probably a crap implementation, but it's an
    # interesting starting point for a better formatter.
    line = ('%s%s %2.4f' % ((indent * indent_level),
                            explanation['description'],
                            explanation['value']))

    if 'details' in explanation:
        details = '\n'.join(
            [format_explanation(subtree, indent, indent_level + 1)
             for subtree in explanation['details']])
        return line + '\n' + details

    return line

########NEW FILE########
__FILENAME__ = _version
# follow pep-386
# Examples:
# * 0.3     - released version
# * 0.3a1   - alpha version
# * 0.3.dev - version in development
__version__ = '0.10.dev'
__releasedate__ = ''

########NEW FILE########
__FILENAME__ = run_tests
#!/usr/bin/env python

import os
import sys

import nose


# Set up the environment for our test project.
ROOT = os.path.abspath(os.path.dirname(__file__))

# import to check for the existence of Django
import django
os.environ.update({'DJANGO_SETTINGS_MODULE': 'test_settings'})
sys.path.insert(0, ROOT)

# This can't be imported until after we've fiddled with the
# environment.
from django.test.utils import setup_test_environment
setup_test_environment()

# Run nose.
#
# nose.run() returns True if tests passed and False otherwise which is
# the inverse of what we want the process to return, so we invert it.
sys.exit(not nose.run())

########NEW FILE########
__FILENAME__ = test_settings
import os


ROOT = os.path.abspath(os.path.dirname(__file__))


ES_URLS = ['http://localhost:9200']
ES_INDEXES = {'default': ['elasticutilstest']}
ES_TIMEOUT = 10
ES_DISABLED = False

CELERY_ALWAYS_EAGER = True

SECRET_KEY = 'super_secret'
TEMPLATE_DIRS = ('%s/elasticutils/templates' % ROOT,)

DATABASES = {
    'default': {
        'NAME': ':memory:',
        'ENGINE': 'django.db.backends.sqlite3'
    }
}

INSTALLED_APPS = [
    'elasticutils.contrib.django'
]

########NEW FILE########
