This directory holds scripts and config files for using "The Grinder".

Quick start:

* Download The Grinder via http://grinder.sourceforge.net and follow it's 
  instructions - in summary:
  - Download the .zip distro and unpack the .jar files somewhere.
  - set CLASSPATH={PATH_YOU_INSTALLED_TO}\lib\grinder.jar

* Run a stand-alone test:
  % java net.grinder.Grinder
  To see the statistic for this type of run, you must view the tail of the
  out_*.log file generated by the run.

* Run a large test
  % java net.grinder.Console
  and on each machine you want to use for testing:
  % java net.grinder.Grinder
  Start the tests from the console.  The cumulative stats will be reported
  in this console.

* Configure the tests: Open the grinder.properties file and look for the
  'linkdrop.*' options.


The error captured here appears to be a transient error - it doesn't
seem to be the "normal" error when the token is invalid.

Note however that it was generated immediately after revoking the token
in the yahoo web interface, so it may turn out to be related to caching
etc and common immediately after being revoked.

Note that this response was received directly after revoking the tokens for
F1 via the Yahoo web interface.  Not clear yet if this is reproducible or if
the 500 is just a coincidence...

Note that this response was received directly after revoking the tokens for
F1 via the Yahoo web interface, but it isn't clear it is related directly to 
that.  The error persisted for at least 30 minutes.

This directory contains a corpus used for testing.  Each test is in its 
own directory.  The intent is that they allow full end-to-end testing of
F1 - from the initial request into F1, the request and response F1 makes
to the actual services, and the final response from F1.

In general, these test cases were first generated by the "protocol capture" 
code (see linkoauth/protocap.py) - each capture was then edited and moved 
into this corpus with an appropriate name.

Each directory uses a naming convention of "protocol-host-req_type-comments"
where

* protocol is either 'http' or 'smtp'
* host is the actual hostname the connection is made to.
* req_type is one of 'auth', 'contacts' or 'send'.
* Comments are free form.

The test runner parses the names so it can work out exactly how to test, so
you must use the names above.  Hopefully that will wind up going away and
being smarter without making assumptions about the dir name.

Test Types
----------

There are 2 types of tests this is designed to support.

1) 'Unexpected' service error response handling

In this kind of test we are checking how F1 behaves when it makes a valid 
request but the service returns an unexpected error code due to a transient
error on that service.  In this type of test, the actual incoming F1 request
and the content of the request made to the service isn't that important - the
service just had a transient error unrelated to the input data.

To make these tests more convenient, some tests don't have any input data
defined - the test runner just synthesizez a simple request, ignores the
content of the request F1 made to the service and just returns the error
response.  The final F1 response given that error response is checked and 
that's about it.

2) F1 functionality tests

In this kind of test, the things we are testing depend directly on the 
incoming F1 request - the content of the request dictates the request we 
make to the service (eg, a 'direct' message versus a 'public' message.)

These tests generally have the full set of input requests specified.  In this
case the test runner uses the specific incoming request, and then checks the
request made to the service itself is as expected.  It then replays the 
appropriate response to F1 and checks the outgoing F1 response is as expected.

Corpus Contents
---------------
Each directory may have the following files:

* meta.json - currently unused.

* f1-request.json - a file which holds the json body of an incoming request
  to F1.  If this file doesn't exist, a "simple" request is synthesized which
  is useful for the "unexpected service errors" tests described above.

* request-n - where n is an integer.  These correspond to the requests we
  expect F1 to make on the external service.  For example, if the
  f1-request.json file specifies a direct message on twitter, request-0 will
  hold the request F1 should make to the twitter directmessage API.  This
  is checked by the test runner.  If no request-n file exists, the test runner
  doesn't check the request at all - it just returns the appropriate response.

* response-n - where n is an integer.  This corresponds to the request-n file 
  above.  This is the response from the external service which the test runner
  returns to the F1 code.  For the example above, this would be the response 
  F1 gets from twitter after a successful direct message.

* expected-f1-response.json - a json file which can describe a full F1 
  response including header values and response code.

* expected-f1-data.json - only used if 'expected-f1-response.json' does not
  exist.  Holds only the response body portion of F1.

The tests always check the final F1 response from the playback is as specified
in the 'expected-f1-*' files.
WARNING: THIS REPOSITORY IS DEPRECATED!

The "F1" project, a way to share links implemented as a Firefox extension, has
been converted to (and thus superseded by) Firefox's "Share" feature.  The
web app portion of this project is now available in the following repositories:

- https://github.com/mozilla/server-share
- https://github.com/mozilla/server-share-core
- https://github.com/mozilla/client-share-web

The browser extension portion of this project has been rolled into Firefox
itself.

ACTIVE DEVELOPMENT IS NO LONGER HAPPENING IN THIS REPOSITORY.

Thanks!





# f1

A link sharing service that consists of a Firefox extension and a web service.

The firefox extension creates an area to show the share UI served from the web service.

The web service handles the OAuth work and sending of messages to different share servers.

Some directory explanations:

* **extensions**: holds the Firefox extension source.
* **web**: holds the UI for the web service.
* **grinder**: a load testing tool.
* **tools**: deployment tools.
* The rest of the files support the web service.

## Installation and Setup

### Get the f1 repository:

    git clone https://github.com/mozilla/f1.git
    cd f1

### Setup dependencies:

    make build

If you are on OS X and you get errors or it does not work, see the OS X troubleshooting
section below.

### Start the virtualenv

    source bin/activate

### Running f1

Run the web server. 'reload' is useful for development, the webserver restarts on file changes, otherwise you can leave it off

    paster serve --reload development.ini

Then visit: [http://127.0.0.1:5000/](http://127.0.0.1:5000/) for an index of api examples

## Troubleshooting OS X installs

If the **make build** command produced errors or results in not being able to start
up the server, use the following steps. It is suggested you re-clone F1 before
doing the following steps, so that it starts out with a clean environment.

1. Make sure XCode 3 is installed.

2. Build your own version of Python:

    sudo svn co http://svn.plone.org/svn/collective/buildout/python/
    sudo chown -R $USER ./python
    cd python
    vi buildout.cfg: then remove any references to python 2.4 and 2.5
    python bootstrap.py
    ./bin/buildout
    cd /usr/local/bin
    sudo ln -s /opt/python/bin/virtualenv-2.6 virtualenv

3. Now edit your .profile to make sure that if you have MacPorts installed, its PATH and MANPATH variables
are last in the list for those environment variables.

I also removed export PYTHONPATH=/Users/aaa/hg/raindrop/server/python:$PYTHONPATH
and removed /Library/Frameworks/Python.framework/Versions/Current/bin from the $PATH variable.

4. Build C libraries via Homebrew:

Homebrew installs into /usr/local by default, and it is best if you chown the files in there to you:

    sudo chown -R $USER /usr/local

If installed things before in these directories, remove these directories: /usr/local/include and /usr/local/lib

    ruby -e "$(curl -fsSLk https://gist.github.com/raw/323731/install_homebrew.rb)"
    brew install memcached libmemcached

Then try the **make build** command above and continue from there.

## Setting up a valid Google domain for OpenID+OAuth

You have to have access to a valid domain that google can get to and where you can install an html file.

Visit: [https://www.google.com/accounts/ManageDomains](https://www.google.com/accounts/ManageDomains)

Add your domain, follow the rest of their instructions.

To test: Once that is done, you can bypass normal access to your domain by adding to your /etc/hosts file:

127.0.0.1 your.host.com

Update development.ini and add your key/secret for the google configuration, restart paster.

Then in the web browser, hit f1 with http://your.host.com.

s is without staging right now...

If you can "ssh linkdrop@rd-admin-01.mozillamessaging.com" then these instructions will work for you.  If not, contact gozer and don't bother with this until you can ssh into the box.


-Initial setup on local machine:

> git clone git@github.com:mozilla/f1.git
> cd f1
> git remote add production linkdrop@rd-admin-01.mozillamessaging.com:f1-prod
> git branch prod

-Switch to a branch

> git checkout prod

-Merge from master branch on production to the current branch:

> git pull production master

-Push from our prod branch to the master branch on production:

> git push production prod:master

-Merge from master branch on github (f1) into our current branch:

> git pull origin master


What I've been doing when I want to push to production:

git checkout prod
git pull production master
git pull origin master
git push production prod:master
git checkout master

That can all be simplified later via git config file (and there may be a better way to do it anyway).  When you push to production, you will see a lot of output.  I'd pipe that to a file and then examine the file for any errors.  That output is basically a sync to the live servers as a git hook in the push.  As I understand from gozer, there is also a cron job that checks every few minutes in case the update from push fails.

I'm not certain about git branches yet, so be sure you do NOT push to origin from the prod branch.

Shane


/*
 * Copyright 2009 Google Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

//
// Contents
//

The Closure Compiler performs checking, instrumentation, and
optimizations on JavaScript code. The purpose of this README is to
explain how to build and run the Closure Compiler.

The Closure Compiler requires Java 6 or higher.
http://www.java.com/


//
// Building The Closure Compiler
//

There are three ways to get a Closure Compiler executable.

1) Use one we built for you.

Pre-built Closure binaries can be found at
http://code.google.com/p/closure-compiler/downloads/list


2) Check out the source and build it with Apache Ant.

First, check out the full source tree of the Closure Compiler. There
are instructions on how to do this at the project site.
http://code.google.com/p/closure-compiler/source/checkout

Apache Ant is a cross-platform build tool.
http://ant.apache.org/

At the root of the source tree, there is an Ant file named
build.xml. To use it, navigate to the same directory and type the
command

ant jar

This will produce a jar file called "build/compiler.jar".


3) Check out the source and build it with Eclipse.

Eclipse is a cross-platform IDE.
http://www.eclipse.org/

Under Eclipse's File menu, click "New > Project ..." and create a
"Java Project."  You will see an options screen. Give the project a
name, select "Create project from existing source," and choose the
root of the checked-out source tree as the existing directory. Verify
that you are using JRE version 6 or higher.

Eclipse can use the build.xml file to discover rules. When you
navigate to the build.xml file, you will see all the build rules in
the "Outline" pane. Run the "jar" rule to build the compiler in
build/compiler.jar.


//
// Running The Closure Compiler
//

Once you have the jar binary, running the Closure Compiler is straightforward.

On the command line, type

java -jar compiler.jar

This starts the compiler in interactive mode. Type

var x = 17 + 25;

then hit "Enter", then hit "Ctrl-Z" (on Windows) or "Ctrl-D" (on Mac or Linux)
and "Enter" again. The Compiler will respond:

var x=42;

The Closure Compiler has many options for reading input from a file,
writing output to a file, checking your code, and running
optimizations. To learn more, type

java -jar compiler.jar --help

You can read more detailed documentation about the many flags at
http://code.google.com/closure/compiler/docs/gettingstarted_app.html


//
// Compiling Multiple Scripts
//

If you have multiple scripts, you should compile them all together with
one compile command.

java -jar compiler.jar --js=in1.js --js=in2.js ... --js_output_file=out.js

The Closure Compiler will concatenate the files in the order they're
passed at the command line.

If you need to compile many, many scripts together, you may start to
run into problems with managing dependencies between scripts. You
should check out the Closure Library. It contains functions for
enforcing dependencies between scripts, and a tool called calcdeps.py
that knows how to give scripts to the Closure Compiler in the right
order.

http://code.google.com/p/closure-library/

//
// Licensing
//

Unless otherwise stated, all source files are licensed under
the Apache License, Version 2.0.


-----
Code under:
src/com/google/javascript/rhino
test/com/google/javascript/rhino

URL: http://www.mozilla.org/rhino
Version:  1.5R3, with heavy modifications
License:  Netscape Public License and MPL / GPL dual license

Description: A partial copy of Mozilla Rhino. Mozilla Rhino is an
implementation of JavaScript for the JVM.  The JavaScript parser and
the parse tree data structures were extracted and modified
significantly for use by Google's JavaScript compiler.

Local Modifications: The packages have been renamespaced. All code not
relavant to parsing has been removed. A JSDoc parser and static typing
system have been added.


-----
Code in:
lib/libtrunk_rhino_parser_jarjared.jar

Rhino
URL: http://www.mozilla.org/rhino
Version:  Trunk
License:  Netscape Public License and MPL / GPL dual license

Description: Mozilla Rhino is an implementation of JavaScript for the JVM.

Local Modifications: None. We've used JarJar to renamespace the code
post-compilation. See:
http://code.google.com/p/jarjar/


-----
Code in:
lib/args4j_deploy.jar

Args4j
URL: https://args4j.dev.java.net/
Version: 2.0.9
License: MIT

Description:
args4j is a small Java class library that makes it easy to parse command line
options/arguments in your CUI application.

Local Modifications: None.


-----
Code in:
lib/google_common_deploy.jar

Guava Libraries
URL: http://code.google.com/p/guava-libraries/
Version:  Trunk
License: Apache License 2.0

Description: Google's core Java libraries.

Local Modifications: None.


-----
Code in:
lib/hamcrest-core-1.1.jar

Hamcrest
URL: http://code.google.com/p/hamcrest
License: BSD
License File: LICENSE

Description:
Provides a library of matcher objects (also known as constraints or
predicates) allowing 'match' rules to be defined declaratively, to be used in
other frameworks. Typical scenarios include testing frameworks, mocking
libraries and UI validation rules.

Local modifications:
The original jars contained both source code and compiled classes.

hamcrest-core-1.1.jar just contains the compiled classes.


----
Code in:
lib/junit.jar

JUnit
URL:  http://sourceforge.net/projects/junit/
Version:  4.5
License:  Common Public License 1.0

Description: A framework for writing and running automated tests in Java.

Local Modifications: None.


---
Code in:
lib/protobuf_deploy.jar

Protocol Buffers
URL: http://code.google.com/p/protobuf/
Version: 2.2.0a
License: New BSD License

Description: Supporting libraries for protocol buffers,
an encoding of structured data.

Local Modifications: None


---
Code in:
lib/ant_deploy.jar

URL: http://ant.apache.org/bindownload.cgi
Version: 1.6.5
License: Apache License 2.0
Description:
  Ant is a Java based build tool. In theory it is kind of like "make"
  without make's wrinkles and with the full portability of pure java code.

Local Modifications:
  Modified apache-ant-1.6.5/bin/ant to look in the ant.runfiles directory

