__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Powerhose documentation build configuration file, created by
# sphinx-quickstart on Fri Feb 24 15:30:44 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os


class Mock(object):
    def __init__(self, *args, **kwargs):
        pass

    def __call__(self, *args, **kwargs):
        return Mock()

    @classmethod
    def __getattr__(self, name):
        if name in ('__file__', '__path__'):
            return '/dev/null'
        elif name[0] == name[0].upper():
            return type(name, (), {})
        else:
            return Mock()

MOCK_MODULES = ['zmq', 'zmq.eventloop']
for mod_name in MOCK_MODULES:
    sys.modules[mod_name] = Mock()

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.

CURDIR = os.path.abspath(os.path.dirname(__file__))
sys.path.append(os.path.join(CURDIR, '..', '..'))
sys.path.append(os.path.join(CURDIR, '..'))

extensions = ['sphinx.ext.autodoc'   ]#, 'sphinxcontrib.blockdiag']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Powerhose'
copyright = u'2012, Mozilla Foundation - 2012'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.4'
# The full version, including alpha/beta/rc tags.
release = '0.4'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
sys.path.append(os.path.abspath('_themes'))
html_theme_path = ['_themes']
html_short_title = "Powerhose"
on_rtd = os.environ.get('READTHEDOCS', None) == 'True'
#if on_rtd:
html_theme = 'default'
#else:
#    html_theme = 'bootstrap'
#html_logo = "images/powerhose32.png"

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

CURDIR = os.path.dirname(__file__)
sidebars = []
for f in os.listdir(CURDIR):
    name, ext = os.path.splitext(f)
    if ext != '.rst':
        continue
    sidebars.append((name, 'indexsidebar.html'))

html_sidebars = dict(sidebars)

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Powerhosedoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Powerhose.tex', u'Powerhose Documentation',
   u'Mozilla Foundation', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'powerhose', u'Powerhose Documentation',
     [u'Mozilla Foundation',], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Powerhose', u'Powerhose Documentation',
   u'Mozilla Foundation', 'Powerhose', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = bench
import hmac
import time
import binascii
import os
import sys
import subprocess
import threading

from powerhose import get_cluster
from powerhose.client import Client


_KEY = binascii.b2a_hex(os.urandom(4096))[:4096]


def _sign(data):
    seed = hmac.new(_KEY, data).hexdigest()
    for i in range(400):
        seed = hmac.new(_KEY, seed).hexdigest()
    return 'OK'


def sign(job):
    return _sign(job.data)


_SIZE = 400
_THREADS = 4
_ONE = _SIZE / _THREADS
_PROC = 10


def timed(msg):
    def _timed(func):
        def __timed(*args, **kw):
            sys.stdout.write(msg + '...')
            sys.stdout.flush()
            start = time.time()
            try:
                return func(*args, **kw)
            finally:
                sys.stdout.write('%.4f s\n' % (time.time() - start))
                sys.stdout.flush()
        return __timed
    return _timed


@timed('%d calls, simple.' % _SIZE)
def simple():
    for i in range(_SIZE):
        _sign(str(i))


@timed('%d calls, %d threads' % (_SIZE, _THREADS))
def simple_3():
    def _t():
        for i in range(_ONE):
            _sign(str(i))

    th = [threading.Thread(target=_t) for i in range(_THREADS)]
    for t in th:
        t.start()

    for t in th:
        t.join()


@timed('%d calls via phose, %d threads %d phose workers' % (_SIZE, _THREADS,
    _PROC))
def _phose3():

    def _t():
        client = Client()
        for i in range(_ONE):
            try:
                client.execute(str(i))
            except:
                print 'error'

    th = [threading.Thread(target=_t) for i in range(_THREADS)]
    for t in th:
        t.start()

    for t in th:
        t.join()


@timed('%d calls via phose. %d workers' % (_SIZE, _PROC))
def _phose(client):
    for i in range(_SIZE):
        try:
            client.execute(str(i))
        except:
            print 'error'


def phose():
    client = Client()
    p = run_cluster()
    time.sleep(5.)
    try:
        _phose(client)
    finally:
        p.terminate()


def phose_3():
    p = run_cluster()
    time.sleep(5.)
    try:
        _phose3()
    finally:
        p.terminate()


def _run_cluster():
    cluster = get_cluster('bench.sign', debug=False,
                          numprocesses=_PROC,
                          logfile='/tmp/phose')
    try:
        cluster.start()
    finally:
        cluster.stop()


def run_cluster():
    cmd = sys.executable + ' -c "import bench; bench._run_cluster()"'
    p = subprocess.Popen(cmd, shell=True)
    return p


@timed('Single job')
def single():
    _sign('1')


if __name__ == '__main__':
    single()
    #simple()
    simple_3()
    #phose()
    phose_3()

########NEW FILE########
__FILENAME__ = crypto
from powerhose.util import set_logger
from powerhose import get_cluster


set_logger(True)
cluster = get_cluster('crypto_worker.sign')
try:
    cluster.start()
finally:
    cluster.stop()

########NEW FILE########
__FILENAME__ = crypto_client
import os
import sys
import binascii
import random
import time

from powerhose.client import Client
from powerhose.job import Job
from powerhose.util import set_logger


set_logger(True)
algs = ('ECDSA256', 'Ed25519', 'RSA2048', 'RSA3248')
client = Client()


def run():
    data = binascii.b2a_hex(os.urandom(256))[:256]
    job = Job(data + '--' + random.choice(algs))
    return client.execute(job)


print 'Running 100 crypto'
start = time.time()


for i in range(100):
    res = run()
    sys.stdout.write('.')
    sys.stdout.flush()

print 'Done in %.2f' % (time.time() - start)

########NEW FILE########
__FILENAME__ = crypto_server
""" Implements a web server that signs data passed into a POST
"""
from bottle import route, request, default_app
from pycryptopp.publickey import rsa
import time
import threading
import thread

from gevent import monkey
from gevent_zeromq import monkey_patch


from powerhose.client import Pool


monkey.patch_all()
monkey_patch()


class RSA3248(object):

    SIZEINBITS = 3248

    def __init__(self):
        self.signer = rsa.generate(sizeinbits=self.SIZEINBITS)

    def sign(self, msg):
        return self.signer.sign(msg)

crypto = RSA3248()

clients = {}
_cl = Pool()

def sign(job):
    crypto.sign(job.data)
    return 'OK'


@route('/', method='POST')
def index():
    data = request.body.read()
    crypto.sign(data)
    return 'OK'


@route('/phose', method='POST')
def phose():
    data = request.body.read()
    return _cl.execute(data)



application = default_app()


if __name__ == '__main__':
    def _t():
        client = Client()
        for i in range(200):
            client.execute(str(i))

    threads = [threading.Thread(target=_t) for i in range(5)]

    start = time.time()
    for t in threads:
        t.start()
    for t in threads:
        t.join()
    print time.time() - start

########NEW FILE########
__FILENAME__ = crypto_worker
# taken from pycryptopp
from pycryptopp.publickey import ecdsa, ed25519, rsa
import random
from powerhose.job import Job
import binascii
import os
import sys


def insecurerandstr(n):
    return ''.join(map(chr, map(random.randrange, [0] * n, [256] * n)))


algs = {}


class ECDSA256(object):
    def __init__(self):
        self.seed = insecurerandstr(12)
        self.signer = ecdsa.SigningKey(self.seed)

    def sign(self, msg):
        return self.signer.sign(msg)


algs['ECDSA256'] = ECDSA256()


class Ed25519(object):
    def __init__(self):
        self.seed = insecurerandstr(32)
        self.signer = ed25519.SigningKey(self.seed)

    def sign(self, msg):
        return self.signer.sign(msg)


algs['Ed25519'] = Ed25519()


class RSA2048(object):
    SIZEINBITS = 2048

    def __init__(self):
        self.signer = rsa.generate(sizeinbits=self.SIZEINBITS)

    def sign(self, msg):
        return self.signer.sign(msg)


algs['RSA2048'] = RSA2048()


class RSA3248(object):
    SIZEINBITS = 3248

    def __init__(self):
        self.signer = rsa.generate(sizeinbits=self.SIZEINBITS)

    def sign(self, msg):
        return self.signer.sign(msg)


algs['RSA3248'] = RSA3248()


def sign(job):
    msg, alg = job.data.split('--')
    ob = algs[alg]
    return ob.sign(msg)


def run():
    data = binascii.b2a_hex(os.urandom(256))[:256]
    job = Job(data + '--' + random.choice(algs.keys()))
    return sign(job)


if __name__ == '__main__':
    while True:
        res = run()
        sys.stdout.write('.')
        sys.stdout.flush()

########NEW FILE########
__FILENAME__ = echo
from powerhose import get_cluster
from powerhose.client import Client


cluster = get_cluster('echo_worker.echo', background=True)
cluster.start()

client = Client()

for i in range(10):
    print client.execute(str(i))

cluster.stop()

########NEW FILE########
__FILENAME__ = echo_client
import random
import sys
import time
import threading

from powerhose.client import Pool
from powerhose.job import Job
from powerhose.util import set_logger
from powerhose.exc import NoWorkerError



class Worker(threading.Thread):

    def __init__(self, pool):
        threading.Thread.__init__(self)
        self.pool = pool
        self.running = False

    def run(self):
        i = 0
        self.running = True

        while self.running:
            data = str(random.randint(1, 1000))
            job = Job(data)
            sys.stdout.write(str(i) +  '-> ')
            sys.stdout.flush()
            try:
                res = self.pool.execute(job)
            except NoWorkerError:
                sys.stdout.write('NO WORKER\n')
            else:
                assert res == data
                sys.stdout.write('OK\n')
            sys.stdout.flush()
            i += 1

    def stop(self):
        self.running = False
        self.join()


if __name__ == '__main__':
    # a pool of 10 workers
    client = Pool()

    # 10 threads hammering the broker
    workers = [Worker(client) for i in range(20)]
    for worker in workers:
        worker.start()

    # just sit there for 100 seconds
    try:
        time.sleep(3600)
    finally:
        for worker in workers:
            worker.stop()



########NEW FILE########
__FILENAME__ = echo_worker

def echo(job):
    """Just echo back the job's data"""
    return job.data

########NEW FILE########
__FILENAME__ = web_client
from wsgiref.simple_server import make_server
import json
import time

from powerhose.client import Client
from powerhose.job import Job
from powerhose.util import set_logger

set_logger(True)

client = Client()


def application(environ, start_response):
    status = '200 OK'
    headers = [('Content-type', 'text/html')]
    start_response(status, headers)
    data = {}
    for key, value in environ.items():
        if key.startswith('wsgi.') or key.startswith('gunicorn.'):
            continue
        data[key] = value

    job = Job(json.dumps(data))

    start = time.time()
    try:
        return client.execute(job)
    finally:
        print('Time : %.4f\n' % (time.time() - start))


if __name__ == '__main__':
    httpd = make_server('', 8000, application)
    print "Listening on port 8000...."
    httpd.serve_forever()

########NEW FILE########
__FILENAME__ = web_worker
import json
import pprint
import StringIO


page = """\
<html>
  <body>
    <h1>Hello</h1>
    <pre>%s
    </pre>
  </body>
</html>
"""


def hello(job):
    environ = json.loads(job.data)
    printed = StringIO.StringIO()
    pprint.pprint(environ, stream=printed)
    printed.seek(0)
    return page % printed.read()

########NEW FILE########
__FILENAME__ = broker
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
""" Jobs runner.
"""
import random
import errno
import sys
import traceback
import argparse
import os
import time
import psutil

from zmq.eventloop import ioloop, zmqstream
import zmq

from powerhose.util import (set_logger, register_ipc_file, DEFAULT_FRONTEND,
                            DEFAULT_BACKEND, DEFAULT_HEARTBEAT, DEFAULT_REG,
                            logger, verify_broker, kill_ghost_brokers)
from powerhose.heartbeat import Heartbeat
from powerhose.exc import DuplicateBrokerError
from powerhose.client import DEFAULT_TIMEOUT_MOVF


DEFAULT_IOTHREADS = 1


class Broker(object):
    """Class that route jobs to workers.

    Options:

    - **frontend**: the ZMQ socket to receive jobs.
    - **backend**: the ZMQ socket to communicate with workers.
    - **heartbeat**: the ZMQ socket to receive heartbeat requests.
    - **register** : the ZMQ socket to register workers
    """
    def __init__(self, frontend=DEFAULT_FRONTEND, backend=DEFAULT_BACKEND,
                 heartbeat=DEFAULT_HEARTBEAT, register=DEFAULT_REG,
                 io_threads=DEFAULT_IOTHREADS,
                 worker_timeout=DEFAULT_TIMEOUT_MOVF):
        # before doing anything, we verify if a broker is already up and
        # running
        logger.debug('Verifying if there is a running broker')
        pid = verify_broker(frontend)
        if pid is not None:    # oops. can't do this !
            logger.debug('Ooops, we have a running broker on that socket')
            raise DuplicateBrokerError(pid)

        logger.debug('Initializing the broker.')

        for endpoint in (frontend, backend, heartbeat):
            if endpoint.startswith('ipc'):
                register_ipc_file(endpoint)

        self.context = zmq.Context(io_threads=io_threads)

        # setting up the three sockets
        self._frontend = self.context.socket(zmq.ROUTER)
        self._frontend.identity = 'broker-' + frontend
        self._frontend.bind(frontend)
        self._backend = self.context.socket(zmq.ROUTER)
        self._backend.bind(backend)
        self._registration = self.context.socket(zmq.PULL)
        self._registration.bind(register)

        # setting up the streams
        self.loop = ioloop.IOLoop()
        self._frontstream = zmqstream.ZMQStream(self._frontend, self.loop)
        self._frontstream.on_recv(self._handle_recv_front)
        self._backstream = zmqstream.ZMQStream(self._backend, self.loop)
        self._backstream.on_recv(self._handle_recv_back)
        self._regstream = zmqstream.ZMQStream(self._registration, self.loop)
        self._regstream.on_recv(self._handle_reg)

        # heartbeat
        self.pong = Heartbeat(heartbeat, io_loop=self.loop, ctx=self.context)

        # status
        self.started = False
        self.poll_timeout = None

        # workers registration and timers
        self._workers = []
        self._worker_times = {}
        self.worker_timeout = worker_timeout

    def _remove_worker(self, worker_id):
        logger.debug('%r removed' % worker_id)
        self._workers.remove(worker_id)
        if worker_id in self._worker_times:
            del self._worker_times[worker_id]

    def _handle_reg(self, msg):
        if msg[0] == 'REGISTER':
            if msg[1] not in self._workers:
                logger.debug('%r registered' % msg[1])
                self._workers.append(msg[1])
        elif msg[0] == 'UNREGISTER':
            if msg[1] in self._workers:
                self._remove_worker(msg[1])

    def _check_worker(self, worker_id):
        # box-specific, will do better later XXX
        exists = psutil.pid_exists(int(worker_id))
        if not exists:
            logger.debug('The worker %r is gone' % worker_id)
            return False

        if worker_id in self._worker_times:

            start, stop = self._worker_times[worker_id]
            if stop is not None:
                duration = start - stop
                if duration > self.worker_timeout:
                    logger.debug('The worker %r is slow (%.2f)' % (worker_id,
                            duration))
                    return False
        return True

    def _handle_recv_front(self, msg, tentative=0):
        # front => back
        # if the last part of the message is 'PING', we just PONG back
        # this is used as a health check
        if msg[-1] == 'PING':
            self._frontstream.send_multipart(msg[:-1] + [str(os.getpid())])
            return

        #logger.debug('front -> back [choosing a worker]')
        if tentative == 3:
            logger.debug('No workers')
            self._frontstream.send_multipart(msg[:-1] +
                    ['%d:ERROR:No worker' % os.getpid()])
            return

        # we want to decide who's going to do the work
        found_worker = False

        while not found_worker and len(self._workers) > 0:
            worker_id = random.choice(self._workers)
            if not self._check_worker(worker_id):
                self._remove_worker(worker_id)
            else:
                found_worker = True

        if not found_worker:
            logger.debug('No worker, will try later')
            later = time.time() + 0.5 + (tentative * 0.2)
            self.loop.add_timeout(later, lambda: self._handle_recv_front(msg,
                                    tentative + 1))
            return

        # start the timer
        self._worker_times[worker_id] = time.time(), None

        # now we can send to the right guy
        msg.insert(0, worker_id)
        #logger.debug('front -> back [%s]' % worker_id)

        try:
            self._backstream.send_multipart(msg)
        except Exception, e:
            # we don't want to die on error. we just log it
            exc_type, exc_value, exc_traceback = sys.exc_info()
            exc = traceback.format_tb(exc_traceback)
            exc.insert(0, str(e))
            logger.error('\n'.join(exc))

    def _handle_recv_back(self, msg):
        # back => front
        #logger.debug('front <- back [%s]' % msg[0])

        # let's remove the worker id and track the time it took
        worker_id = msg[0]
        msg = msg[1:]
        now = time.time()

        if worker_id in self._worker_times:
            start, stop = self._worker_times[worker_id]
            self._worker_times[worker_id] = start, now
        else:
            self._worker_times[worker_id] = now, now

        try:
            self._frontstream.send_multipart(msg)
        except Exception, e:
            # we don't want to die on error. we just log it
            exc_type, exc_value, exc_traceback = sys.exc_info()
            exc = traceback.format_tb(exc_traceback)
            exc.insert(0, str(e))
            logger.error('\n'.join(exc))

    def start(self):
        """Starts the broker.
        """
        logger.debug('Starting the loop')
        if self.started:
            return

        # running the heartbeat
        self.pong.start()

        self.started = True
        while self.started:
            try:
                self.loop.start()
            except zmq.ZMQError as e:
                logger.debug(str(e))

                if e.errno == errno.EINTR:
                    continue
                elif e.errno == zmq.ETERM:
                    break
                else:
                    logger.debug("got an unexpected error %s (%s)", str(e),
                                 e.errno)
                    raise
            else:
                break

    def stop(self):
        """Stops the broker.
        """
        if not self.started:
            return

        self._backstream.flush()
        logger.debug('Stopping the heartbeat')
        self.pong.stop()
        logger.debug('Stopping the loop')
        self.loop.stop()
        self.started = False
        self.context.destroy(0)


def main(args=sys.argv):
    parser = argparse.ArgumentParser(description='Powerhose broker.')

    parser.add_argument('--frontend', dest='frontend',
                        default=DEFAULT_FRONTEND,
                        help="ZMQ socket to receive jobs.")

    parser.add_argument('--backend', dest='backend',
                        default=DEFAULT_BACKEND,
                        help="ZMQ socket for workers.")

    parser.add_argument('--heartbeat', dest='heartbeat',
                        default=DEFAULT_HEARTBEAT,
                        help="ZMQ socket for the heartbeat.")

    parser.add_argument('--register', dest='register',
                        default=DEFAULT_REG,
                        help="ZMQ socket for the registration.")

    parser.add_argument('--io-threads', type=int,
                        default=DEFAULT_IOTHREADS,
                        help="Number of I/O threads")

    parser.add_argument('--debug', action='store_true', default=False,
                        help="Debug mode")

    parser.add_argument('--check', action='store_true', default=False,
                        help=("Use this option to check if there's a running "
                              " broker. Returns the PID if a broker is up."))

    parser.add_argument('--purge-ghosts', action='store_true', default=False,
                        help="Use this option to purge ghost brokers.")

    parser.add_argument('--logfile', dest='logfile', default='stdout',
                        help="File to log in to .")

    args = parser.parse_args()
    set_logger(args.debug, logfile=args.logfile)

    if args.purge_ghosts:
        broker_pids, ghosts = kill_ghost_brokers(args.frontend)
        if broker_pids is None:
            logger.info('No running broker.')
        else:
            logger.info('The active broker runs at PID: %s' % broker_pids)

        if len(ghosts) == 0:
            logger.info('No ghosts where killed.')
        else:
            logger.info('Ghost(s) killed: %s' \
                    % ', '.join([str(g) for g in ghosts]))
        return 0

    if args.check:
        pid = verify_broker(args.frontend)
        if pid is None:
            logger.info('There seem to be no broker on this endpoint')
        else:
            logger.info('A broker is running. PID: %s' % pid)
        return 0

    logger.info('Starting the broker')
    try:
        broker = Broker(frontend=args.frontend, backend=args.backend,
                        heartbeat=args.heartbeat, register=args.register,
                        io_threads=args.io_threads)
    except DuplicateBrokerError, e:
        logger.info('There is already a broker running on PID %s' % e)
        logger.info('Exiting')
        return 1

    logger.info('Listening to incoming jobs at %r' % args.frontend)
    logger.info('Workers may register at %r' % args.backend)
    logger.info('The heartbeat socket is at %r' % args.heartbeat)
    try:
        broker.start()
    except KeyboardInterrupt:
        pass
    finally:
        broker.stop()

    return 0


if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = client
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import threading
from Queue import Queue
from collections import defaultdict
import errno
import contextlib

import zmq

from powerhose.exc import TimeoutError, ExecutionError, NoWorkerError
from powerhose.job import Job
from powerhose.util import (send, recv, DEFAULT_FRONTEND, logger,
                            extract_result, timed)


DEFAULT_TIMEOUT = 5.
DEFAULT_TIMEOUT_MOVF = 7.5
DEFAULT_TIMEOUT_OVF = 1


class Client(object):
    """Class to call a Powerhose cluster.

    Options:

    - **frontend**: ZMQ socket to call.
    - **timeout**: maximum allowed time for a job to run.
      Defaults to 1s.
    - **timeout_max_overflow**: maximum timeout overflow allowed.
      Defaults to 1.5s
    - **timeout_overflows**: number of times in a row the timeout value
      can be overflowed per worker. The client keeps a counter of
      executions that were longer than the regular timeout but shorter
      than **timeout_max_overflow**. When the number goes over
      **timeout_overflows**, the usual TimeoutError is raised.
      When a worker returns on time, the counter is reset.
    """
    def __init__(self, frontend=DEFAULT_FRONTEND, timeout=DEFAULT_TIMEOUT,
                 timeout_max_overflow=DEFAULT_TIMEOUT_MOVF,
                 timeout_overflows=DEFAULT_TIMEOUT_OVF,
                 debug=False, ctx=None):
        self.kill_ctx = ctx is None
        self.ctx = ctx or zmq.Context()
        self.frontend = frontend
        self.master = self.ctx.socket(zmq.REQ)
        self.master.connect(frontend)
        logger.debug('Client connected to %s' % frontend)
        self.poller = zmq.Poller()
        self.poller.register(self.master, zmq.POLLIN)
        self.timeout = timeout * 1000
        self.lock = threading.Lock()
        self.timeout_max_overflow = timeout_max_overflow * 1000
        self.timeout_overflows = timeout_overflows
        self.timeout_counters = defaultdict(int)
        self.debug = debug

    def execute(self, job, timeout=None):
        """Runs the job

        Options:

        - **job**: Job to be performed. Can be a :class:`Job`
          instance or a string. If it's a string a :class:`Job` instance
          will be automatically created out of it.
        - **timeout**: maximum allowed time for a job to run.
          If not provided, uses the one defined in the constructor.

        If the job fails after the timeout, raises a :class:`TimeoutError`.

        This method is thread-safe and uses a lock. If you need to execute a
        lot of jobs simultaneously on a broker, use the :class:`Pool` class.

        """
        if timeout is None:
            timeout = self.timeout_max_overflow

        try:
            duration, res = timed(self.debug)(self._execute)(job, timeout)
            worker_pid, res, data = res

            # if we overflowed we want to increment the counter
            # if not we reset it
            if duration * 1000 > self.timeout:
                self.timeout_counters[worker_pid] += 1

                # XXX well, we have the result but we want to timeout
                # nevertheless because that's been too much overflow
                if self.timeout_counters[worker_pid] > self.timeout_overflows:
                    raise TimeoutError(timeout / 1000)
            else:
                self.timeout_counters[worker_pid] = 0

            if not res:
                if data == 'No worker':
                    raise NoWorkerError()
                raise ExecutionError(data)
        except Exception:
            # logged, connector replaced.
            logger.exception('Failed to execute the job.')
            raise

        return data

    def ping(self, timeout=1.):
        """Can be used to simply ping the broker to make sure
        it's responsive.


        Returns the broker PID"""
        with self.lock:
            send(self.master, 'PING')
            while True:
                try:
                    socks = dict(self.poller.poll(timeout * 1000))
                    break
                except zmq.ZMQError as e:
                    if e.errno != errno.EINTR:
                        return None

        if socks.get(self.master) == zmq.POLLIN:
            res = recv(self.master)
            try:
                res = int(res)
            except TypeError:
                pass
            return res

        return None

    def close(self):
        #self.master.close()
        self.master.setsockopt(zmq.LINGER, 0)
        self.master.close()

        if self.kill_ctx:
            self.ctx.destroy(0)

    def _execute(self, job, timeout=None):
        if isinstance(job, str):
            job = Job(job)

        if timeout is None:
            timeout = self.timeout_max_overflow

        with self.lock:
            send(self.master, job.serialize())

            while True:
                try:
                    socks = dict(self.poller.poll(timeout))
                    break
                except zmq.ZMQError as e:
                    if e.errno != errno.EINTR:
                        raise

        if socks.get(self.master) == zmq.POLLIN:
            return extract_result(recv(self.master))

        raise TimeoutError(timeout / 1000)


class Pool(object):
    """The pool class manage several :class:`CLient` instances
    and publish the same interface,

    Options:

    - **size**: size of the pool. Defaults to 10.
    - **frontend**: ZMQ socket to call.
    - **timeout**: maximum allowed time for a job to run.
      Defaults to 5s.
    - **timeout_max_overflow**: maximum timeout overflow allowed
    - **timeout_overflows**: number of times in a row the timeout value
      can be overflowed per worker. The client keeps a counter of
      executions that were longer than the regular timeout but shorter
      than **timeout_max_overflow**. When the number goes over
      **timeout_overflows**, the usual TimeoutError is raised.
      When a worker returns on time, the counter is reset.
    """
    def __init__(self, size=10, frontend=DEFAULT_FRONTEND,
                 timeout=DEFAULT_TIMEOUT,
                 timeout_max_overflow=DEFAULT_TIMEOUT_MOVF,
                 timeout_overflows=DEFAULT_TIMEOUT_OVF,
                 debug=False, ctx=None):
        self._connectors = Queue()
        self.frontend = frontend
        self.timeout = timeout
        self.timeout_overflows = timeout_overflows
        self.timeout_max_overflow = timeout_max_overflow
        self.debug = debug
        self.ctx = ctx or zmq.Context()

        for i in range(size):
            self._connectors.put(self._create_client())

    def _create_client(self):
        return Client(self.frontend, self.timeout,
                      self.timeout_max_overflow, self.timeout_overflows,
                      debug=self.debug, ctx=self.ctx)

    @contextlib.contextmanager
    def _connector(self, timeout):
        connector = self._connectors.get(timeout=timeout)
        try:
            yield connector
        except Exception:
            # connector replaced
            try:
                connector.close()
            finally:
                self._connectors.put(self._create_client())
            raise
        else:
            self._connectors.put(connector)

    def execute(self, job, timeout=None):
        with self._connector(timeout) as connector:
            return connector.execute(job, timeout)

    def close(self):
        self.ctx.destroy(0)

    def ping(self, timeout=.1):
        with self._connector(self.timeout) as connector:
            return connector.ping(timeout)

########NEW FILE########
__FILENAME__ = exc
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.


class TimeoutError(Exception):
    pass


class ExecutionError(Exception):
    pass


class DuplicateBrokerError(Exception):
    pass


class NoWorkerError(Exception):
    pass

########NEW FILE########
__FILENAME__ = heartbeat
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import threading
import errno
import time

import zmq
from zmq.eventloop import ioloop, zmqstream

from powerhose.util import logger, DEFAULT_HEARTBEAT


class Stethoscope(threading.Thread):
    """Class that implements a ZMQ heartbeat client.

    Options:

    - **endpoint** : The ZMQ socket to call.
    - **warmup_delay** : The delay before starting to Ping. Defaults to 5s.
    - **delay**: The delay between two pings. Defaults to 3s.
    - **retries**: The number of attempts to ping. Defaults to 3.
    - **onbeatlost**: a callable that will be called when a ping failed.
      If the callable returns **True**, the ping quits. Defaults to None.
    - **onbeat**: a callable that will be called when a ping succeeds.
      Defaults to None.
    """
    def __init__(self, endpoint=DEFAULT_HEARTBEAT, warmup_delay=.5, delay=10.,
                 retries=3,
                 onbeatlost=None, onbeat=None, io_loop=None, ctx=None):
        threading.Thread.__init__(self)
        self.loop = io_loop or ioloop.IOLoop.instance()
        self._stop_loop = io_loop is None
        self.daemon = True
        self.context = ctx or zmq.Context()
        self.endpoint = endpoint
        self.running = False
        self.delay = delay
        self.retries = retries
        self.onbeatlost = onbeatlost
        self.onbeat = onbeat
        self.warmup_delay = warmup_delay
        self._endpoint = None
        self._stream = None
        self._timer = None
        self.tries = 0

    def _initialize(self):
        logger.debug('Subscribing to ' + self.endpoint)
        self._endpoint = self.context.socket(zmq.SUB)
        self._endpoint.setsockopt(zmq.SUBSCRIBE, '')
        self._endpoint.linger = 0
        #self._endpoint.identity = str(os.getpid())
        self._endpoint.connect(self.endpoint)
        self._stream = zmqstream.ZMQStream(self._endpoint, self.loop)
        self._stream.on_recv(self._handle_recv)
        self._timer = ioloop.PeriodicCallback(self._delayed,
                self.delay * 1000, io_loop=self.loop)

    def _delayed(self):
        self.tries += 1
        if self.tries >= self.retries:
            logger.debug('Nothing came back')
            if self.onbeatlost is None or self.onbeatlost():
                self.stop()   # bye !

    def _handle_recv(self, msg):
        self.tries = 0
        if self.onbeat is not None:
            self.onbeat()
        logger.debug(msg[0])

    def run(self):
        """Starts the loop"""
        logger.debug('Starting the loop')
        if self.running:
            return

        self._initialize()
        time.sleep(self.warmup_delay)
        self._timer.start()
        self.running = True
        while self.running:
            try:
                self.loop.start()
            except zmq.ZMQError as e:
                logger.debug(str(e))

                if e.errno == errno.EINTR:
                    continue
                elif e.errno == zmq.ETERM:
                    break
                else:
                    logger.debug("got an unexpected error %s (%s)", str(e),
                                 e.errno)
                    raise
            else:
                break

    def stop(self):
        """Stops the Pinger"""
        logger.debug('Stopping the Pinger')
        self.running = False
        try:
            self._stream.flush()
        except zmq.ZMQError:
            pass
        if self._stop_loop:
            self.loop.stop()
        if self.isAlive():
            try:
                self.join()
            except RuntimeError:
                pass


class Heartbeat(object):
    """Class that implements a ZMQ heartbeat server.

    This class sends in a ZMQ socket regular beats.

    Options:

    - **endpoint** : The ZMQ socket to call.
    - **interval** : Interval between two beat.
    """
    def __init__(self, endpoint=DEFAULT_HEARTBEAT, interval=10.,
                 io_loop=None, ctx=None):
        self.loop = io_loop or ioloop.IOLoop.instance()
        self.daemon = True
        self.kill_context = ctx is None
        self.context = ctx or zmq.Context()
        self.endpoint = endpoint
        self.running = False
        self.interval = interval
        logger.debug('Publishing to ' + self.endpoint)
        self._endpoint = self.context.socket(zmq.PUB)
        self._endpoint.linger = 0
        #self._endpoint.identity = b'HB'
        self._endpoint.hwm = 0
        self._endpoint.bind(self.endpoint)
        self._cb = ioloop.PeriodicCallback(self._ping, interval * 1000,
                                           io_loop=self.loop)

    def start(self):
        """Starts the Pong service"""
        self.running = True
        self._cb.start()

    def _ping(self):
        logger.debug('*beat*')
        self._endpoint.send('BEAT')

    def stop(self):
        """Stops the Pong service"""
        self.running = False
        self._cb.stop()
        if self.kill_context:
            self.context.destroy(0)

########NEW FILE########
__FILENAME__ = job
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
""" Job class.
"""


class Job(object):
    """A Job is just a container that's passed into the wire.

    A job is composed of headers and raw data, and offers serialization.

    Options:

    - **data**: the raw string data (default: '')
    - **headers**: a mapping of headers (default: None)
    """
    def __init__(self, data='', headers=None):
        self.data = data
        self.headers = {}

        if headers is not None:
            for name, value in headers.items():
                self.add_header(name, value)

    def add_header(self, name, value):
        """Adds a header.

        Options:

        - **name**: header name
        - **value**: value

        Both values should be strings. If the header already exists
        it's overwritten.
        """
        name = name.replace(':', '\:')
        value = value.replace(':', '\:')
        self.headers[name] = value

    def serialize(self):
        """Serializes the job.

        The output can be sent over a wire. A serialized job
        can be read with a cursor with no specific preprocessing.
        """
        if len(self.headers) == 0:
            headers = ['NONE']
        else:
            headers = ['%s:%s' % (name, value) for name, value in
                       self.headers.items()]

        headers = '::'.join(headers)
        return headers + ':::' + self.data

    @classmethod
    def load_from_string(cls, data):
        """Loads a job from a serialized string and return a Job instance.

        Options:

        - **data** : serialized string.
        """
        if ':::' not in data:
            raise ValueError(data)
        headers, data = data.split(':::', 1)
        res = {}
        for header in headers.split('::'):
            if header == 'NONE':
                break
            header_data = header.strip().split(':')
            if len(header_data) != 2:
                raise ValueError(header_data)
            res[header_data[0]] = header_data[1]

        return cls(data, res)

########NEW FILE########
__FILENAME__ = jobs
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import time
import sys
from powerhose.util import logger, set_logger

set_logger(True, logfile='stdout')


def _p(msg):
    sys.stdout.write(msg + '\n')
    logger.debug(msg)
    sys.stdout.flush()


def fail(job):
    _p('Starting powerhose.tests.jobs.fail')
    try:
        raise ValueError(job.data)
    finally:
        _p('Ending powerhose.tests.jobs.fail')


def timeout(job):
    _p('Starting powerhose.tests.jobs.timeout')
    time.sleep(2.)
    try:
        return job.data
    finally:
        _p('Ending powerhose.tests.jobs.timeout')


def timeout_overflow(job):
    _p('Starting powerhose.tests.jobs.timeout_overflow')
    time.sleep(float(job.data))
    try:
        return 'xx'
    finally:
        _p('Ending powerhose.tests.jobs.timeout_overflow')


def success(job):
    _p('Starting powerhose.tests.jobs.success')
    try:
        return job.data
    finally:
        _p('Ending powerhose.tests.jobs.success')

########NEW FILE########
__FILENAME__ = test_cluster
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import unittest
import logging
import os
import tempfile
import time

import psutil

from powerhose import get_cluster
from powerhose.exc import ExecutionError, TimeoutError
from powerhose import client


logger = logging.getLogger('powerhose')


class TestCluster(unittest.TestCase):

    def setUp(self):
        self.clusters = []
        self.files = []
        self.old_timeout = client.DEFAULT_TIMEOUT
        self.old_movf = client.DEFAULT_TIMEOUT_MOVF
        self.old_ovf = client.DEFAULT_TIMEOUT_OVF
        client.DEFAULT_TIMEOUT = .5
        client.DEFAULT_TIMEOUT_MOVF = 1.
        client.DEFAULT_TIMEOUT_OVF = 1
        self.overflow = str(client.DEFAULT_TIMEOUT + .5)
        self.moverflow = str(client.DEFAULT_TIMEOUT_MOVF + .5)

    def tearDown(self):
        logger.debug('stopping cluster')
        for cl in self.clusters:
            cl.stop()
        for fl in self.files:
            os.remove(fl)
        logger.debug('cluster stopped')
        client.DEFAULT_TIMEOUT = self.old_timeout
        client.DEFAULT_TIMEOUT_MOVF = self.old_movf
        client.DEFAULT_TIMEOUT_OVF = self.old_ovf

    def _get_file(self):
        fd, path = tempfile.mkstemp()
        os.close(fd)
        self.files.append(path)
        return path

    def _get_cluster(self, callable, **kw):
        logger.debug('getting cluster')
        front = 'ipc:///tmp/f-%s' % callable
        back = 'ipc:///tmp/b-%s' % callable
        hb = 'ipc:///tmp/h-%s' % callable
        reg = 'ipc:///tmp/r-%s' % callable

        cl = get_cluster(callable, frontend=front, backend=back, heartbeat=hb,
                         register=reg,
                         numprocesses=1, background=True, debug=True,
                         timeout=client.DEFAULT_TIMEOUT_MOVF, **kw)
        cl.start()
        time.sleep(1.)  # stabilization
        self.clusters.append(cl)
        logger.debug('cluster ready')
        return client.Pool(size=3, frontend=front, debug=True,
                timeout=client.DEFAULT_TIMEOUT,
                timeout_max_overflow=client.DEFAULT_TIMEOUT_MOVF,
                timeout_overflows=client.DEFAULT_TIMEOUT_OVF)

    def test_error(self):
        client = self._get_cluster('powerhose.tests.jobs.fail')
        self.assertRaises(ExecutionError, client.execute, 'xx')

    def test_timeout(self):
        client = self._get_cluster('powerhose.tests.jobs.timeout_overflow')
        self.assertRaises(TimeoutError, client.execute, self.moverflow)

    def test_success(self):
        client = self._get_cluster('powerhose.tests.jobs.success')
        self.assertEqual(client.execute('xx'), 'xx')

    def test_timeout_ovf(self):
        # this should work 1 time then fail
        file = self._get_file()
        client = self._get_cluster('powerhose.tests.jobs.timeout_overflow',
                                   logfile=file)

        # trying a PING
        time.sleep(.5)
        self.assertTrue(client.ping() is not None)

        try:
            self.assertEqual(client.execute(self.overflow), 'xx')
        except Exception:
            with open(file) as f:
                raise Exception(f.read())

        try:
            self.assertRaises(TimeoutError, client.execute, self.overflow)
        except Exception:
            with open(file) as f:
                raise Exception(f.read())

        # calling it back with the right execution time resets the counter
        self.assertEqual(client.execute('.1'), 'xx')
        self.assertEqual(client.execute(self.overflow), 'xx')
        self.assertRaises(TimeoutError, client.execute, self.overflow)

    def test_timeout_dump(self):
        file = self._get_file()
        client = self._get_cluster('powerhose.tests.jobs.timeout_overflow',
                                    logfile=file)

        self.assertRaises(TimeoutError, client.execute, self.moverflow)
        time.sleep(1.)      # give the worker a chance to dump the stack

        with open(file) as f:
            res = [line.strip() for line in f.readlines() if line.strip()]

        # the worker should be blocked on the sleep
        self.assertTrue('time.sleep(float(job.data))' in res)

    def test_worker_max_age(self):
        # a worker with a max age of 1.5
        client = self._get_cluster('powerhose.tests.jobs.success',
                                   max_age=1.5, max_age_delta=0)
        time.sleep(.5)

        self.assertEqual(client.execute('xx'), 'xx')

        cl = self.clusters[-1]

        # get the pid of the current worker
        pid = cl.watchers[1].processes.keys()[0]

        # wait 3 seconds
        time.sleep(2.)

        # should be different
        self.assertNotEqual(pid, cl.watchers[1].processes.keys()[0])

    def test_worker_max_age2(self):

        # we want to run a job, and have the max age reached while the job
        # is being executed, to verify that the job returns before the
        # worker is killed.
        client.DEFAULT_TIMEOUT = 5.
        client.DEFAULT_TIMEOUT_MOVF = 7.
        file = self._get_file()
        _client = self._get_cluster('powerhose.tests.jobs.timeout_overflow',
                                    max_age=5., max_age_delta=0,
                                    logfile=file)

        time.sleep(.2)
        cl = self.clusters[-1]

        # get the pid of the current worker
        pid = cl.watchers[1].processes.keys()[0]

        # work for 3 seconds
        try:
            self.assertEqual(_client.execute('6.0'), 'xx')
        except Exception:
            with open(file) as f:
                print(f.read())
            raise

        # wait until the process is dead
        now = time.time()
        while psutil.pid_exists(int(pid)) and time.time() - now < 10.:
            time.sleep(.1)

        # now give a chance to the new one to stabilize
        time.sleep(1.)

        # should be different
        try:
            self.assertNotEqual(pid, cl.watchers[1].processes.keys()[0])
        except Exception:
            with open(file) as f:
                print(f.read())
            raise

########NEW FILE########
__FILENAME__ = test_heartbeat
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import unittest
import time
from powerhose.heartbeat import Stethoscope, Heartbeat


class TestHeartbeat(unittest.TestCase):

    def test_working(self):
        beats = []
        lost = []

        def onbeat():
            beats.append('.')

        def onbeatlost():
            lost.append('.')

        hb = Heartbeat('ipc:///tmp/stetho.ipc', interval=0.1)
        hb.start()
        time.sleep(.2)

        stetho = Stethoscope('ipc:///tmp/stetho.ipc', onbeat=onbeat,
                             onbeatlost=onbeatlost, delay=1., retries=5.)
        stetho.start()

        time.sleep(5.)

        stetho.stop()
        hb.stop()
        self.assertEqual(len(lost),  0, len(lost))
        self.assertTrue(len(beats) > 10, len(beats))

    def test_lost(self):
        beats = []
        lost = []

        def _onbeat():
            beats.append('.')

        def _onbeatlost():
            lost.append('.')

        hb = Heartbeat('ipc:///tmp/stetho.ipc', interval=0.1)
        hb.start()
        time.sleep(.2)

        stetho = Stethoscope('ipc:///tmp/stetho.ipc', onbeat=_onbeat,
                    onbeatlost=_onbeatlost, delay=0.1)
        stetho.start()

        time.sleep(2.)
        hb.stop()         # the hber stops

        # the stethoer continues for a while
        time.sleep(2.)

        stetho.stop()

        self.assertTrue(len(beats) > 0)
        self.assertTrue(len(lost) > 3)

########NEW FILE########
__FILENAME__ = test_job
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import unittest
from powerhose.job import Job


class TestJob(unittest.TestCase):

    def test_job(self):
        job = Job('somedata', {'one': '1'})
        data = job.serialize()
        job2 = Job.load_from_string(data)
        self.assertTrue(job.data, job2.data)
        self.assertTrue(job.headers.items(), job2.headers.items())

########NEW FILE########
__FILENAME__ = util
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import os
import atexit
import time
import zmq
import logging
import logging.handlers
import sys
import gc
import traceback
import threading

from powerhose.exc import TimeoutError


DEFAULT_FRONTEND = "ipc:///tmp/powerhose-front.ipc"
DEFAULT_BACKEND = "ipc:///tmp/powerhose-back.ipc"
DEFAULT_HEARTBEAT = "ipc:///tmp/powerhose-beat.ipc"
DEFAULT_REG = "ipc:///tmp/powerhose-reg.ipc"

logger = logging.getLogger('powerhose')
_IPC_FILES = []

PARAMS = {}


@atexit.register
def _cleanup_ipc_files():
    for file in _IPC_FILES:
        file = file.split('ipc://')[-1]
        if os.path.exists(file):
            os.remove(file)


def register_ipc_file(file):
    _IPC_FILES.append(file)


def send(socket, msg, more=False, max_retries=3, retry_sleep=0.1):
    retries = 0
    while retries < max_retries:
        try:
            logger.debug('send')

            if more:
                socket.send(msg, zmq.SNDMORE | zmq.NOBLOCK)
            else:
                socket.send(msg, zmq.NOBLOCK)
            return
        except zmq.ZMQError, e:
            logger.debug('Failed on send()')
            logger.debug(str(e))
            if e.errno in (zmq.EFSM, zmq.EAGAIN):
                retries += 1
                time.sleep(retry_sleep)
            else:
                raise

    logger.debug('Sending failed')
    logger.debug(msg)
    raise TimeoutError()


def recv(socket, max_retries=3, retry_sleep=0.1):
    retries = 0
    while retries < max_retries:
        try:
            logger.debug('receive')
            return socket.recv(zmq.NOBLOCK)
        except zmq.ZMQError, e:
            logger.debug('Failed on recv()')
            logger.debug(str(e))
            if e.errno in (zmq.EFSM, zmq.EAGAIN):
                retries += 1
                time.sleep(retry_sleep)
            else:
                raise

    logger.debug('Receiving failed')
    raise TimeoutError()


def set_logger(debug=False, name='powerhose', logfile='stdout'):
    # setting up the logger
    logger_ = logging.getLogger(name)
    logger_.setLevel(logging.DEBUG)

    if logfile == 'stdout':
        ch = logging.StreamHandler()
    else:
        ch = logging.handlers.RotatingFileHandler(logfile, mode='a+')

    if debug:
        ch.setLevel(logging.DEBUG)
    else:
        ch.setLevel(logging.INFO)

    formatter = logging.Formatter('[%(asctime)s][%(name)s] %(message)s')
    ch.setFormatter(formatter)
    logger_.addHandler(ch)


# taken from distutils2
def resolve_name(name):
    """Resolve a name like ``module.object`` to an object and return it.

    This functions supports packages and attributes without depth limitation:
    ``package.package.module.class.class.function.attr`` is valid input.
    However, looking up builtins is not directly supported: use
    ``__builtin__.name``.

    Raises ImportError if importing the module fails or if one requested
    attribute is not found.
    """
    if '.' not in name:
        # shortcut
        __import__(name)
        return sys.modules[name]

    # FIXME clean up this code!
    parts = name.split('.')
    cursor = len(parts)
    module_name = parts[:cursor]
    ret = ''

    while cursor > 0:
        try:
            ret = __import__('.'.join(module_name))
            break
        except ImportError:
            cursor -= 1
            module_name = parts[:cursor]

    if ret == '':
        raise ImportError(parts[0])

    for part in parts[1:]:
        try:
            ret = getattr(ret, part)
        except AttributeError, exc:
            raise ImportError(exc)

    return ret


if sys.platform == "win32":
    timer = time.clock
else:
    timer = time.time


def timed(debug=False):
    def _timed(func):
        def __timed(*args, **kw):
            start = timer()
            try:
                res = func(*args, **kw)
            finally:
                duration = timer() - start
                if debug:
                    logger.debug('%.4f' % duration)
            return duration, res
        return __timed
    return _timed


def decode_params(params):
    """Decode a string into a dict. This is mainly useful when passing a dict
    trough the command line.

    The params passed in "params" should be in the form of key:value, separated
    by a pipe, the output is a python dict.
    """
    output_dict = {}
    for items in params.split('|'):
        key, value = items.split(':')
        output_dict[key] = value
    return output_dict


def encode_params(intput_dict):
    """Convert the dict given in input into a string of key:value separated
    with pipes, like spam:yeah|eggs:blah
    """
    return '|'.join([':'.join(i) for i in intput_dict.items()])


def get_params():
    return PARAMS


def extract_result(data):
    data = data.split(':', 2)
    if len(data) != 3:
        raise ValueError("Wrong data: %s" % data)
    pid, result, data = data
    return long(pid), result == 'OK', data


def dump_stacks():
    dump = []

    # threads
    threads = dict([(th.ident, th.name)
                        for th in threading.enumerate()])

    for thread, frame in sys._current_frames().items():
        dump.append('Thread 0x%x (%s)\n' % (thread, threads[thread]))
        dump.append(''.join(traceback.format_stack(frame)))
        dump.append('\n')

    # greenlets
    try:
        from greenlet import greenlet
    except ImportError:
        return dump

    # if greenlet is present, let's dump each greenlet stack
    for ob in gc.get_objects():
        if not isinstance(ob, greenlet):
            continue
        if not ob:
            continue   # not running anymore or not started
        dump.append('Greenlet\n')
        dump.append(''.join(traceback.format_stack(ob.gr_frame)))
        dump.append('\n')

    return dump


def verify_broker(broker_endpoint=DEFAULT_FRONTEND, timeout=1.):
    """ Return True if there's a working broker bound at broker_endpoint
    """
    from powerhose.client import Client
    client = Client(broker_endpoint)
    try:
        return client.ping(timeout)
    finally:
        client.close()


def kill_ghost_brokers(broker_endpoint=DEFAULT_FRONTEND, timeout=1.):
    """Kills ghost brokers.

    Return a pid, pids tuple. The first pid is the running broker
    and the second is a list of pids that where killed.
    """
    # checking if there's a working broker
    working_broker = verify_broker(broker_endpoint, timeout)
    if working_broker is not None:
        working_broker = int(working_broker)

    # listing running brokers and killing the ghost ones
    killed = []
    import psutil
    for pid in psutil.get_pid_list():
        if pid in (os.getpid(), os.getppid()):
            continue

        p = psutil.Process(pid)
        try:
            cmd = ' '.join(p.cmdline)
        except psutil.error.AccessDenied:
            continue

        cmd = cmd.replace('-', '.')

        if 'powerhose.broker' not in cmd or pid == working_broker:
            continue

        killed.append(pid)
        p.terminate()

    return working_broker, killed

########NEW FILE########
__FILENAME__ = worker
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this file,
# You can obtain one at http://mozilla.org/MPL/2.0/.
import os
import errno
import time
import sys
import traceback
import argparse
import logging
import threading
import Queue
import contextlib
import random

import zmq

from powerhose import util
from powerhose.util import (logger, set_logger, DEFAULT_BACKEND,
                            DEFAULT_HEARTBEAT, DEFAULT_REG)
from powerhose.job import Job
from powerhose.util import resolve_name, decode_params, timed, dump_stacks
from powerhose.heartbeat import Stethoscope
from powerhose.client import DEFAULT_TIMEOUT_MOVF

from zmq.eventloop import ioloop, zmqstream


DEFAULT_MAX_AGE = -1
DEFAULT_MAX_AGE_DELTA = 0


class ExecutionTimer(threading.Thread):

    def __init__(self, timeout=DEFAULT_TIMEOUT_MOVF, interval=.1):
        logger.debug('Initializing the execution timer. timeout is %.2f' \
                % timeout)
        threading.Thread.__init__(self)
        self.armed = self.running = False
        self.timeout = timeout
        self.daemon = True

        # creating a queue for I/O with the worker
        self.queue = Queue.Queue()
        self.interval = interval
        self.timed_out = self.working = False
        self.last_dump = None

    @contextlib.contextmanager
    def run_job(self):
        self.job_starts()
        try:
            yield
        finally:
            self.job_ends()

    def job_starts(self):
        if self.working:
            raise ValueError("The worker is already busy -- call job_ends")
        self.working = True
        self.timed_out = False
        self.queue.put('STARTING')

    def job_ends(self):
        if not self.working:
            raise ValueError("The worker is not busy -- call job_starts")
        self.queue.put('DONE')
        self.working = self.armed = False

    def run(self):
        self.running = True

        while self.running:
            # arming, so waiting for ever
            self.queue.get()
            self.armed = True

            # now waiting for the second call, which means
            # the worker has done the work.
            #
            # This time we time out
            try:
                self.queue.get(timeout=self.timeout)
            except Queue.Empty:
                # too late, we want to log the stack
                self.last_dump = dump_stacks()
                self.timed_out = True
            finally:
                self.armed = False

    def stop(self):
        self.running = False
        if not self.armed:
            self.queue.put('STARTING')
        self.queue.put('DONE')
        if self.isAlive():
            self.join()


class Worker(object):
    """Class that links a callable to a broker.

    Options:

    - **target**: The Python callable that will be called when the broker
      send a job.
    - **backend**: The ZMQ socket to connect to the broker.
    - **heartbeat**: The ZMQ socket to perform PINGs on the broker to make
      sure it's still alive.
    - **register** : the ZMQ socket to register workers
    - **ping_delay**: the delay in seconds betweem two pings.
    - **ping_retries**: the number of attempts to ping the broker before
      quitting.
    - **params** a dict containing the params to set for this worker.
    - **timeout** the maximum time allowed before the thread stacks is dump
      and the job result not sent back.
    - **max_age**: maximum age for a worker in seconds. After that delay,
      the worker will simply quit. When set to -1, never quits.
      Defaults to -1.
    - **max_age_delta**: maximum value in seconds added to max age.
      The Worker will quit after *max_age + random(0, max_age_delta)*
      This is done to avoid having all workers quit at the same instant.
      Defaults to 0. The value must be an integer.
    """
    def __init__(self, target, backend=DEFAULT_BACKEND,
                 heartbeat=DEFAULT_HEARTBEAT, register=DEFAULT_REG,
                 ping_delay=10., ping_retries=3,
                 params=None, timeout=DEFAULT_TIMEOUT_MOVF,
                 max_age=DEFAULT_MAX_AGE, max_age_delta=DEFAULT_MAX_AGE_DELTA):
        logger.debug('Initializing the worker.')
        self.ctx = zmq.Context()
        self.backend = backend
        self._reg = self.ctx.socket(zmq.PUSH)
        self._reg.connect(register)
        self._backend = self.ctx.socket(zmq.REP)
        self._backend.identity = str(os.getpid())
        self._backend.connect(self.backend)
        self.target = target
        self.running = False
        self.loop = ioloop.IOLoop()
        self._backstream = zmqstream.ZMQStream(self._backend, self.loop)
        self._backstream.on_recv(self._handle_recv_back)
        self.ping = Stethoscope(heartbeat, onbeatlost=self.lost,
                                delay=ping_delay, retries=ping_retries,
                                ctx=self.ctx)
        self.debug = logger.isEnabledFor(logging.DEBUG)
        self.params = params
        self.pid = os.getpid()
        self.timeout = timeout
        self.timer = ExecutionTimer(timeout=timeout)
        self.max_age = max_age
        self.max_age_delta = max_age_delta
        self.delayed_exit = None
        self.lock = threading.RLock()

    def _handle_recv_back(self, msg):
        # do the job and send the result
        if self.debug:
            logger.debug('Job received')
            target = timed()(self.target)
        else:
            target = self.target

        duration = -1

        # results are sent with a PID:OK: or a PID:ERROR prefix
        try:
            with self.timer.run_job():
                res = target(Job.load_from_string(msg[0]))

            # did we timout ?
            if self.timer.timed_out:
                # let's dump the last
                for line in self.timer.last_dump:
                    logger.error(line)

            if self.debug:
                duration, res = res
            res = '%d:OK:%s' % (self.pid, res)
        except Exception, e:
            exc_type, exc_value, exc_traceback = sys.exc_info()
            exc = traceback.format_tb(exc_traceback)
            exc.insert(0, str(e))
            res = '%d:ERROR:%s' % (self.pid, '\n'.join(exc))
            logger.error(res)

        if self.timer.timed_out:
            # let's not send back anything, we know the client
            # is gone anyway
            return

        if self.debug:
            logger.debug('Duration - %.6f' % duration)

        try:
            self._backstream.send(res)
        except Exception:
            logging.error("Could not send back the result", exc_info=True)

    def lost(self):
        logger.info('Master lost ! Quitting..')
        self.running = False
        self.loop.stop()
        return True

    def stop(self):
        """Stops the worker.
        """
        if not self.running:
            return

        # telling the broker we are stopping
        try:
            self._reg.send_multipart(['UNREGISTER', str(os.getpid())])
        except zmq.ZMQError:
            logger.debug('Could not unregister')

        # give it a chance to finish a job
        logger.debug('Starting the graceful period')
        self.graceful_delay = ioloop.DelayedCallback(self._stop,
                                                     self.timeout * 1000,
                                                     io_loop=self.loop)
        self.graceful_delay.start()

    def _stop(self):
        logger.debug('Stopping the worker')
        self.running = False
        try:
            self._backstream.flush()
        except zmq.core.error.ZMQError:
            pass
        self.loop.stop()
        self.ping.stop()
        self.timer.stop()
        time.sleep(.1)
        self.ctx.destroy(0)
        logger.debug('Worker is stopped')

    def start(self):
        """Starts the worker
        """
        util.PARAMS = self.params
        logger.debug('Starting the worker loop')

        # running the pinger
        self.ping.start()
        self.timer.start()
        self.running = True

        # telling the broker we are ready
        self._reg.send_multipart(['REGISTER', str(os.getpid())])

        # arming the exit callback
        if self.max_age != -1:
            if self.max_age_delta > 0:
                delta = random.randint(0, self.max_age_delta)
            else:
                delta = 0

            cb_time = self.max_age + delta
            self.delayed_exit = ioloop.DelayedCallback(self.stop,
                                                       cb_time * 1000,
                                                       io_loop=self.loop)
            self.delayed_exit.start()

        while self.running:
            try:
                self.loop.start()
            except zmq.ZMQError as e:
                logger.debug(str(e))

                if e.errno == errno.EINTR:
                    continue
                elif e.errno == zmq.ETERM:
                    break
                else:
                    logger.debug("got an unexpected error %s (%s)", str(e),
                                 e.errno)
                    raise
            else:
                break

        logger.debug('Worker loop over')


def main(args=sys.argv):

    parser = argparse.ArgumentParser(description='Run some watchers.')

    parser.add_argument('--backend', dest='backend',
                        default=DEFAULT_BACKEND,
                        help="ZMQ socket to the broker.")

    parser.add_argument('--register', dest='register',
                        default=DEFAULT_REG,
                        help="ZMQ socket for the registration.")

    parser.add_argument('target', help="Fully qualified name of the callable.")

    parser.add_argument('--debug', action='store_true', default=False,
                        help="Debug mode")

    parser.add_argument('--logfile', dest='logfile', default='stdout',
                        help="File to log in to.")

    parser.add_argument('--heartbeat', dest='heartbeat',
                        default=DEFAULT_HEARTBEAT,
                        help="ZMQ socket for the heartbeat.")

    parser.add_argument('--params', dest='params', default=None,
                        help='The parameters to be used in the worker.')

    parser.add_argument('--timeout', dest='timeout', type=float,
                        default=DEFAULT_TIMEOUT_MOVF,
                        help=('The maximum time allowed before the thread '
                              'stacks is dump and the job result not sent '
                              'back.'))

    parser.add_argument('--max-age', dest='max_age', type=float,
                        default=DEFAULT_MAX_AGE,
                        help=('The maximum age for a worker in seconds. '
                              'After that delay, the worker will simply quit. '
                              'When set to -1, never quits.'))

    parser.add_argument('--max-age-delta', dest='max_age_delta', type=int,
                        default=DEFAULT_MAX_AGE_DELTA,
                        help='The maximum value in seconds added to max_age')

    args = parser.parse_args()
    set_logger(args.debug, logfile=args.logfile)
    sys.path.insert(0, os.getcwd())  # XXX
    target = resolve_name(args.target)
    if args.params is None:
        params = {}
    else:
        params = decode_params(args.params)

    logger.info('Worker registers at %s' % args.backend)
    logger.info('The heartbeat socket is at %r' % args.heartbeat)
    worker = Worker(target, backend=args.backend, heartbeat=args.heartbeat,
                    register=args.register,
                    params=params, timeout=args.timeout, max_age=args.max_age,
                    max_age_delta=args.max_age_delta)

    try:
        worker.start()
    except KeyboardInterrupt:
        return 1
    finally:
        worker.stop()

    return 0


if __name__ == '__main__':
    main()

########NEW FILE########
