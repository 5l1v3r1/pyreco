__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# nitime documentation build configuration file, created by
# sphinx-quickstart on Mon Jul 20 12:30:18 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys
import os
import warnings

# Declare here the things that our documentation build will depend on here, so
# that if they are not present the build fails immediately rather than
# producing possibly obscure errors later on.

# Documentation dependency format: each dep is a pair of two entries, the first
# is a string that should be a valid (possibly dotted) package name, and the
# second a list (possibly empty) of names to import from that package.
doc_deps = [['networkx', []],
            ['mpl_toolkits.axes_grid',  ['make_axes_locatable']],
            ]

# Analyze the dependencies, and fail if  any is unmet, with a hopefully
# reasonable error
failed_deps = []
for package, parts in doc_deps:
    try:
        __import__(package, fromlist=parts)
    except ImportError:
        failed_deps.append([package, parts])

if failed_deps:
    print
    print "*** ERROR IN DOCUMENTATION BUILD ***"
    print "The documentation build is missing these dependencies:"
    for pak, parts in failed_deps:
        if parts:
            print "Package: %s, parts: %s" % (pak, parts)
        else:
            print "Package: %s" % pak

    raise RuntimeError('Unmet dependencies for documentation build')


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath('sphinxext'))

#-----------------------------------------------------------------------------
# Error control in examples and plots
#-----------------------------------------------------------------------------
# We want by default our documentation to NOT build if any plot warnings are
# generated, so we turn PlotWarning into an error.  For now this requires using
# a patched version of the plot_directive, but we'll upstream this to matplotlib.
import plot_directive
# If you *really* want to disable these error checks to be able to finish a doc
# build, comment out the next line.  But please do NOT leave it uncommented in
# a committed file, so that the official build is always in the paranoid mode
# (where the warnings become errors).
warnings.simplefilter('error', plot_directive.PlotWarning)

# -- General configuration -----------------------------------------------------
# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc',
              'sphinx.ext.doctest',
              #'sphinx.ext.intersphinx',
              'sphinx.ext.todo',
              'sphinx.ext.pngmath',
              'numpydoc',
              'sphinx.ext.inheritance_diagram',
              'ipython_console_highlighting',
              'only_directives',
              'math_dollar',  # Support for $x$ math
              # For now, we use our own patched plot directive, we'll revert
              # back to the official one once our changes are upstream.
              #'matplotlib.sphinxext.plot_directive',
              'plot_directive',
              'github'
              ]

# ghissue config
github_project_url = "https://github.com/nipy/nitime"

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'nitime'
copyright = u'2009, Neuroimaging in Python team'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# We read the version info from the source file.
ver = {}
execfile('../nitime/version.py', ver)
# The short X.Y version.
version = '%s.%s' % (ver['_version_major'], ver['_version_minor'])
# The full version, including alpha/beta/rc tags.
release = ver['__version__']

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
today_fmt = '%B %d, %Y, %H:%M PDT'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# Flag to show todo items in rendered output
todo_include_todos = True

# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'sphinxdoc'

# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
html_style = 'nitime.css'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# Content template for the index page.
html_index = 'index.html'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {'index': 'indexsidebar.html'}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {'index': 'index.html'}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = False

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'nitimedoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('documentation', 'nitime.tex', u'nitime Documentation',
   u'Neuroimaging in Python team', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

########NEW FILE########
__FILENAME__ = ar_est_1var
"""

.. _ar:

===============================================
Fitting an AR model: algorithm module interface
===============================================

Auto-regressive (AR) processes are processes that follow the following
equation:

.. math::

   x_t = \sum_{i=1}^{n}a_i * x_{t-i} + \epsilon_t

In this example, we will demonstrate the estimation of the AR model
coefficients and the estimation of the AR process spectrum, based on the
estimation of the coefficients.

We start with imports from numpy, matplotlib and import :mod:`nitime.utils` as
well as :mod:`nitime.algorithms:`

"""

import numpy as np
from matplotlib import pyplot as plt

from nitime import utils
from nitime import algorithms as alg
from nitime.timeseries import TimeSeries
from nitime.viz import plot_tseries

"""

We define some variables, which will be used in generating the AR process:

"""

npts = 2048
sigma = 0.1
drop_transients = 128
Fs = 1000

"""

In this case, we generate an order 2 AR process, with the following coefficients:


"""

coefs = np.array([0.9, -0.5])

"""

This generates the AR(2) time series:

"""

X, noise, _ = utils.ar_generator(npts, sigma, coefs, drop_transients)

ts_x = TimeSeries(X, sampling_rate=Fs, time_unit='s')
ts_noise = TimeSeries(noise, sampling_rate=1000, time_unit='s')

"""

We use the plot_tseries function in order to visualize the process:

"""

fig01 = plot_tseries(ts_x, label='AR signal')
fig01 = plot_tseries(ts_noise, fig=fig01, label='Noise')
fig01.axes[0].legend()

"""

.. image:: fig/ar_est_1var_01.png


Now we estimate back the model parameters, using two different estimation
algorithms.


"""

coefs_est, sigma_est = alg.AR_est_YW(X, 2)
# no rigorous purpose behind 100 transients
X_hat, _, _ = utils.ar_generator(
    N=npts, sigma=sigma_est, coefs=coefs_est, drop_transients=100, v=noise
    )
fig02 = plt.figure()
ax = fig02.add_subplot(111)
ax.plot(np.arange(100, len(X_hat) + 100), X_hat, label='estimated process')
ax.plot(X, 'g--', label='original process')
ax.legend()
err = X_hat - X[100:]
mse = np.dot(err, err) / len(X_hat)
ax.set_title('Mean Square Error: %1.3e' % mse)


plt.show()

########NEW FILE########
__FILENAME__ = ar_est_2vars

"""

.. _mar:

=====================================
Mulitvariate auto-regressive modeling
=====================================

Multivariate auto-regressive modeling uses a simple

This example is based on Ding, Chen and Bressler 2006 [Ding2006]_.


We start by importing the required libraries:


"""

import numpy as np
import matplotlib.pyplot as plt


"""

From nitime, we import the algorithms and the utils:

"""

import nitime.algorithms as alg
import nitime.utils as utils


"""

Setting the random seed assures that we always get the same 'random' answer:

"""

np.random.seed(1981)

"""

We will generate an AR(2) model, with the following coefficients (taken from
[Ding2006]_, eq. 55):

.. math::

   x_t = 0.9x_{t-1} - 0.5 x_{t-2} + \epsilon_t

.. math::

   y_t = 0.8y_{t-1} - 0.5 y_{t-2} + 0.16 x_{t-1} - 0.2 x_{t-2} + \eta_t

Or more succinctly, if we define:

.. math::

    Z_{t}=\left(\begin{array}{c}
    x_{t}\\
    y_{t}\end{array}\right),\,E_t=\left(\begin{array}{c}
    \epsilon_{t}\\
    \eta_{t}\end{array}\right)

then:

.. math::

  Z_t = A_1 Z_{t-1} + A_2 Z_{t-2} + E_t

where:

.. math::

   E_t \sim {\cal N} (\mu,\Sigma) \mathrm{, where} \,\, \Sigma=\left(\begin{array}{cc}var_{\epsilon} & cov_{xy}\\ cov_{xy} & var_{\eta}\end{array}\right)


We now build the two :math:`A_i` matrices with the values indicated above:

"""

a1 = np.array([[0.9, 0],
               [0.16, 0.8]])

a2 = np.array([[-0.5, 0],
               [-0.2, -0.5]])


"""

For implementation reasons, we rewrite the equation (:ref:`eqn_ar`) as follows:

.. math::

    Z_t + \sum_{i=1}^2 a_i Z_{t-i} = E_t

where: $a_i = - A_i$:

"""

am = np.array([-a1, -a2])


"""


The variances and covariance of the processes are known (provided as part of
the example in [Ding2006]_, after eq. 55):


"""

x_var = 1
y_var = 0.7
xy_cov = 0.4
cov = np.array([[x_var, xy_cov],
                [xy_cov, y_var]])


"""

We can calculate the spectral matrix analytically, based on the known
coefficients, for 1024 frequency bins:

"""

n_freqs = 1024

w, Hw = alg.transfer_function_xy(am, n_freqs=n_freqs)
Sw_true = alg.spectral_matrix_xy(Hw, cov)

"""

Next, we will generate 500 example sets of 100 points of these processes, to analyze:


"""

#Number of realizations of the process
N = 500
#Length of each realization:
L = 1024

order = am.shape[0]
n_lags = order + 1

n_process = am.shape[-1]

z = np.empty((N, n_process, L))
nz = np.empty((N, n_process, L))

for i in xrange(N):
    z[i], nz[i] = utils.generate_mar(am, cov, L)

"""

We can estimate the 2nd order AR coefficients, by averaging together N
estimates of auto-covariance at lags k=0,1,2

Each $R^{xx}(k)$ has the shape (2,2), where:

.. math::

   R^{xx}_{00}(k) = E( Z_0(t)Z_0^*(t-k) )

.. math::

   R^{xx}_{01}(k) = E( Z_0(t)Z_1^*(t-k) )

.. math::

   R^{xx}_{10}(k) = E( Z_1(t)Z_0^*(t-k) )

.. math::

   R^{xx}_{11}(k) = E( Z_1(t)Z_1^*(t-k) )


Where $E$ is the expected value and $^*$ marks the conjugate transpose. Thus, only $R^{xx}(0)$ is symmetric.

This is calculated by using the function :func:`utils.autocov_vector`. Notice
that the estimation is done for an assumed known process order. In practice, if
the order of the process is unknown, we will have to use some criterion in
order to choose an appropriate order, given the data.

"""

Rxx = np.empty((N, n_process, n_process, n_lags))

for i in xrange(N):
    Rxx[i] = utils.autocov_vector(z[i], nlags=n_lags)

Rxx = Rxx.mean(axis=0)

R0 = Rxx[..., 0]
Rm = Rxx[..., 1:]

Rxx = Rxx.transpose(2, 0, 1)


"""

We use the Levinson-Whittle(-Wiggins) and Robinson algorithm, as described in [Morf1978]_
, in order to estimate the MAR coefficients and the covariance matrix:

"""

a, ecov = alg.lwr_recursion(Rxx)

"""

Next, we use the calculated coefficients and covariance matrix, in order to
calculate Granger 'causality':

"""

w, f_x2y, f_y2x, f_xy, Sw = alg.granger_causality_xy(a,
                                                     ecov,
                                                     n_freqs=n_freqs)


"""

This results in several different outputs, which we will proceed to plot.

First, we will plot the estimated spectrum. This will be compared to two other
estimates of the spectrum. The first is the 'true' spectrum, calculated from
the known coefficients that generated the data:

"""

fig01 = plt.figure()
ax01 = fig01.add_subplot(1, 1, 1)

# This is the estimate:
Sxx_est = np.abs(Sw[0, 0])
Syy_est = np.abs(Sw[1, 1])

# This is the 'true' value, corrected for one-sided spectral density functions
Sxx_true = Sw_true[0, 0].real
Syy_true = Sw_true[1, 1].real

"""

The other is an estimate based on a multi-taper spectral estimate from the
empirical signals:

"""

c_x = np.empty((L, w.shape[0]))
c_y = np.empty((L, w.shape[0]))

for i in xrange(N):
    frex, c_x[i], nu = alg.multi_taper_psd(z[i][0])
    frex, c_y[i], nu = alg.multi_taper_psd(z[i][1])

"""

We plot these on the same axes, for a direct comparison:

"""

ax01.plot(w, Sxx_true, 'b', label='true Sxx(w)')
ax01.plot(w, Sxx_est, 'b--', label='estimated Sxx(w)')
ax01.plot(w, Syy_true, 'g', label='true Syy(w)')
ax01.plot(w, Syy_est, 'g--', label='estimated Syy(w)')
ax01.plot(w, np.mean(c_x, 0), 'r', label='Sxx(w) - MT PSD')
ax01.plot(w, np.mean(c_y, 0), 'r--', label='Syy(w) - MT PSD')

ax01.legend()

"""

.. image:: fig/ar_est_2vars_01.png

Next, we plot the granger causalities. There are three GCs. One for each
direction of causality between the two processes (X => Y and Y => X). In
addition, there is the instanteaneous causality between the processes:

"""

fig02 = plt.figure()
ax02 = fig02.add_subplot(1, 1, 1)

# x causes y plot
ax02.plot(w, f_x2y, label='X => Y')
# y causes x plot
ax02.plot(w, f_y2x, label='Y => X')
# instantaneous causality
ax02.plot(w, f_xy, label='X:Y')

ax02.legend()

"""

.. image:: fig/ar_est_2vars_02.png


Note that these results make intuitive sense, when you look at the equations
governing the mutual influences. X is entirely influenced by X (no effects of Y
on X in :ref:`eq1`) and there is some influence of X on Y (:ref:`eq2`),
resulting in this pattern.

Finally, we calculate the total causality, which is the sum of all the above
causalities. We compare this to the interdependence between the processes. This is the
measure of total dependence and is closely akin to the coherence between the
processes. We also compare to the empirically calculated coherence:

"""

fig03 = plt.figure()
ax03 = fig03.add_subplot(1, 1, 1)

# total causality
ax03.plot(w, f_xy + f_x2y + f_y2x, label='Total causality')

#Interdepence:
f_id = alg.interdependence_xy(Sw)
ax03.plot(w, f_id, label='Interdependence')

coh = np.empty((N, 33))

for i in xrange(N):
    frex, this_coh = alg.coherence(z[i])
    coh[i] = this_coh[0, 1]

ax03.plot(frex, np.mean(coh, axis=0), label='Coherence')

ax03.legend()

"""

.. image:: fig/ar_est_2vars_03.png


Finally, we call plt.show(), in order to show the figures:

"""

plt.show()

"""


.. [Ding2006] M. Ding, Y. Chen and S.L. Bressler (2006) Granger causality:
   basic theory and application to neuroscience. In Handbook of Time Series
   Analysis, ed. B. Schelter, M. Winterhalder, and J. Timmer, Wiley-VCH
   Verlage, 2006: 451-474

.. [Morf1978] M. Morf, A. Vieira and T. Kailath (1978) Covariance
   Characterization by Partial Autocorrelation Matrices. The Annals of Statistics,
   6: 643-648


"""

########NEW FILE########
__FILENAME__ = ar_est_3vars
"""

.. _mar3:

=====================================================
 Mulitvariate auto-regressive modeling - 3 variables
=====================================================

This example is an extension of the example presented here: :ref:`mar`. Here,
instead of 2 variables and the mutual influences between them, we map out the
mutual interactions between three variables. This example follows closely an
example provided in the paper by Ding, Chen and Bressler (2006) [Ding2006]_.


Start with the necessary imports:

"""

import numpy as np
import matplotlib.pyplot as pp

import nitime.algorithms as alg
import nitime.utils as utils


"""

Set the random seed:

"""

np.random.seed(1981)

"""

simulate two multivariate autoregressive systems.

The first is defined by the following equations:

.. math::

    X(t) = 0.8X_{t-1} - 0.5X_{t-2} + 0.4Z_{t-1} + \epsilon_x

.. math::

    Y(t) = 0.9Y_{t-1} - 0.8Y_{t-2} + \epsilon_y

.. math::

    Z(t) = 0.5Z_{t-1} - 0.2Z_{t-2} + 0.5Y_{t-1} + \epsilon_z

"""


cov = np.diag([0.3, 1.0, 0.2])


a1 = -np.array([[0.8, 0.0, 0.4],
                 [0.0, 0.9, 0.0],
                 [0.0, 0.5, 0.5]])


"""

The second is defined by the following equations:

.. math::

    X(t) = 0.8X_{t-1} - 0.5X_{t-2} + 0.4Z_{t-1} + 0.2Y_{t-2} + \epsilon_x

.. math::

    Y(t) = 0.9Y_{t-1} - 0.8Y_{t-2} + \epsilon_y

.. math::

    Z(t) = 0.5Z_{t-1} -0.2Z_{t-2} + 0.5Y_{t-1} + \epsilon_z

"""


a2 = -np.array([[-0.5, 0.0, 0.0],
                 [0.0, -0.8, 0.0],
                 [0.0, 0.0, -0.2]])

a = np.array([a1.copy(), a2.copy()])

"""

Add some feedback from Y to X at 2 lags:

"""


a2[0, 1] = -0.2

b = np.array([a1.copy(), a2.copy()])


def extract_ij(i, j, m):
    m_ij_rows = m[[i, j]]
    return m_ij_rows[:, [i, j]]

"""

We calculate the transfer function based on the coefficients:

"""

w, Haw = alg.transfer_function_xy(a)
w, Hbw = alg.transfer_function_xy(b)


"""

Generate 500 sets of 100 points

"""


N = 500
L = 100


"""

Generate the instances of the time-series based on the coefficients:

"""

za = np.empty((N, 3, L))
zb = np.empty((N, 3, L))
ea = np.empty((N, 3, L))
eb = np.empty((N, 3, L))
for i in xrange(N):
    za[i], ea[i] = utils.generate_mar(a, cov, L)
    zb[i], eb[i] = utils.generate_mar(b, cov, L)

"""

Try to estimate the 2nd order (m)AR coefficients-- Average together N estimates
of auto-covariance at lags k=0,1,2

"""

Raxx = np.empty((N, 3, 3, 3))
Rbxx = np.empty((N, 3, 3, 3))

for i in xrange(N):
    Raxx[i] = utils.autocov_vector(za[i], nlags=3)
    Rbxx[i] = utils.autocov_vector(zb[i], nlags=3)


"""

Average trials together to find autocovariance estimate, and extract pairwise
components from the ac sequence:

"""

Raxx = Raxx.mean(axis=0)
xyRa = extract_ij(0, 1, Raxx)
xzRa = extract_ij(0, 2, Raxx)
yzRa = extract_ij(1, 2, Raxx)

Rbxx = Rbxx.mean(axis=0)
xyRb = extract_ij(0, 1, Rbxx)
xzRb = extract_ij(0, 2, Rbxx)
yzRb = extract_ij(1, 2, Rbxx)

"""

Now estimate mAR coefficients and covariance from the full and pairwise relationships:

"""

Raxx = Raxx.transpose(2, 0, 1)
a_est, cov_est1 = alg.lwr_recursion(Raxx)
a_xy_est, cov_xy_est1 = alg.lwr_recursion(xyRa.transpose(2, 0, 1))
a_xz_est, cov_xz_est1 = alg.lwr_recursion(xzRa.transpose(2, 0, 1))
a_yz_est, cov_yz_est1 = alg.lwr_recursion(yzRa.transpose(2, 0, 1))

Rbxx = Rbxx.transpose(2, 0, 1)
b_est, cov_est2 = alg.lwr_recursion(Rbxx)
b_xy_est, cov_xy_est2 = alg.lwr_recursion(xyRb.transpose(2, 0, 1))
b_xz_est, cov_xz_est2 = alg.lwr_recursion(xzRb.transpose(2, 0, 1))
b_yz_est, cov_yz_est2 = alg.lwr_recursion(yzRb.transpose(2, 0, 1))


"""

We proceed to visualize these relationships:

"""

fig01 = pp.figure()

w, x2y_a, y2x_a, _, _ = alg.granger_causality_xy(a_xy_est, cov_xy_est1)
w, x2y_b, y2x_b, _, _ = alg.granger_causality_xy(b_xy_est, cov_xy_est2)
ax01 = fig01.add_subplot(321)
ax01.plot(w, x2y_a, 'b--')
ax01.plot(w, x2y_b, 'b')
ax01.set_title('x to y')
ax01.set_ylim((0, 6))
ax02 = fig01.add_subplot(322)
ax02.plot(w, y2x_a, 'b--')
ax02.plot(w, y2x_b, 'b')
ax02.set_title('y to x')
ax02.set_ylim((0, 6))

w, y2z_a, z2y_a, _, _ = alg.granger_causality_xy(a_yz_est, cov_yz_est1)
w, y2z_b, z2y_b, _, _ = alg.granger_causality_xy(b_yz_est, cov_yz_est2)
ax03 = fig01.add_subplot(323)
ax03.plot(w, y2z_a, 'b--')
ax03.plot(w, y2z_b, 'b')
ax03.set_title('y to z')
ax03.set_ylim((0, 6))
ax03 = fig01.add_subplot(324)
ax03.plot(w, z2y_a, 'b--')
ax03.plot(w, z2y_b, 'b')
ax03.set_title('z to y')
ax03.set_ylim((0, 6))

w, x2z_a, z2x_a, _, _ = alg.granger_causality_xy(a_xz_est, cov_xz_est1)
w, x2z_b, z2x_b, _, _ = alg.granger_causality_xy(b_xz_est, cov_xz_est2)
ax04 = fig01.add_subplot(325)
ax04.plot(w, x2z_a, 'b--')
ax04.plot(w, x2z_b, 'b')
ax04.set_title('x to z')
ax04.set_ylim((0, 6))
ax05 = fig01.add_subplot(326)
ax05.plot(w, z2x_a, 'b--')
ax05.plot(w, z2x_b, 'b')
ax05.set_title('z to x')
ax05.set_ylim((0, 6))

pp.show()


"""

Compare to figure 3 in [Ding2006]_:

.. image:: fig/ar_est_3vars_01.png



.. [Ding2006] M. Ding, Y. Chen and S.L. Bressler (2006) Granger causality:
   basic theory and application to neuroscience. In Handbook of Time Series
   Analysis, ed. B. Schelter, M. Winterhalder, and J. Timmer, Wiley-VCH
   Verlage, 2006: 451-474


"""

########NEW FILE########
__FILENAME__ = ar_model_fit
""" .. _model_fit:

========================================
Fitting an MAR model: analyzer interface
========================================

In this example, we will use the Analyzer interface to fit a multi-variate
auto-regressive model with two time-series influencing each other.

We start by importing 3rd party modules:

"""

import numpy as np
import matplotlib.pyplot as plt

"""

And then by importing Granger analysis sub-module, which we will use for fitting the MAR
model:

"""

import nitime.analysis.granger as gc

"""

The utils sub-module includes a function to generate auto-regressive processes
based on provided coefficients:

"""

import nitime.utils as utils


"""

Generate some MAR processes (according to Ding and Bressler [Ding2006]_),

"""

a1 = np.array([[0.9, 0],
               [0.16, 0.8]])

a2 = np.array([[-0.5, 0],
               [-0.2, -0.5]])

am = np.array([-a1, -a2])

x_var = 1
y_var = 0.7
xy_cov = 0.4
cov = np.array([[x_var, xy_cov],
                [xy_cov, y_var]])


"""

Number of realizations of the process

"""

N = 500

"""

Length of each realization:

"""

L = 1024

order = am.shape[0]
n_lags = order + 1

n_process = am.shape[-1]

z = np.empty((N, n_process, L))
nz = np.empty((N, n_process, L))

np.random.seed(1981)
for i in xrange(N):
    z[i], nz[i] = utils.generate_mar(am, cov, L)


"""

We start by estimating the order of the model from the data:

"""

est_order = []
for i in xrange(N):
    this_order, this_Rxx, this_coef, this_ecov = gc.fit_model(z[i][0], z[i][1])
    est_order.append(this_order)

order = int(np.round(np.mean(est_order)))

"""

Once we have estimated the order, we  go ahead and fit each realization of the
MAR model, constraining the model order accordingly (by setting the order
key-word argument) to be always equal to the model order estimated above.

"""

Rxx = np.empty((N, n_process, n_process, n_lags))
coef = np.empty((N, n_process, n_process, order))
ecov = np.empty((N, n_process, n_process))

for i in xrange(N):
    this_order, this_Rxx, this_coef, this_ecov = gc.fit_model(z[i][0], z[i][1], order=order)
    Rxx[i] = this_Rxx
    coef[i] = this_coef
    ecov[i] = this_ecov

"""

We generate a time-series from the recovered coefficients, using the same
randomization seed as the first mar. These should look pretty similar to each other:

"""

np.random.seed(1981)
est_ts, _ = utils.generate_mar(np.mean(coef, axis=0), np.mean(ecov, axis=0), L)

fig01 = plt.figure()
ax = fig01.add_subplot(1, 1, 1)

ax.plot(est_ts[0][0:100])
ax.plot(z[0][0][0:100], 'g--')

"""

.. image:: fig/ar_model_fit_01.png


"""

plt.show()

"""

.. [Ding2006] M. Ding, Y. Chen and S.L. Bressler (2006) Granger causality:
   basic theory and application to neuroscience. In Handbook of Time Series
   Analysis, ed. B. Schelter, M. Winterhalder, and J. Timmer, Wiley-VCH
   Verlage, 2006: 451-474

"""

########NEW FILE########
__FILENAME__ = event_related_fmri
"""

.. _et-fmri:

==================
Event-related fMRI
==================

Extracting the average time-series from one signal, time-locked to the
occurence of some type of event in another signal is a very typical operation
in the analysis of time-series from neuroscience experiments. Therefore, we
have an additional example of this kind of analysis in :ref:`grasshopper`

The following example is taken from an fMRI experiment in which a subject was
viewing a motion stimulus, while fMRI BOLD was recorded. The time-series in
this data set were extracted from motion-sensitive voxels near area MT (a
region containing motion-sensitive cells) in this subject's brain. 6 different
kinds of trials could occur in this experiment (designating different
directions and locations of motion). The following example shows the extraction
of the time-dependent responses of the voxels in this region to the different
stimuli.

We start by importing modules/functions used and define some variables we will
use in the analysis:

"""

import os

from matplotlib.mlab import csv2rec
import matplotlib.pyplot as plt

import nitime
import nitime.timeseries as ts
import nitime.analysis as nta
import nitime.viz as viz

TR = 2.
len_et = 15  # This is given in number of samples, not time!

"""

Next, we load the data into a recarray from the csv file, using csv2rec

"""

data_path = os.path.join(nitime.__path__[0], 'data')

data = csv2rec(os.path.join(data_path, 'event_related_fmri.csv'))


"""

We initialize TimeSeries objects with the data and the TR:

One TimeSeries is initialized for the BOLD data:
"""

t1 = ts.TimeSeries(data.bold, sampling_interval=TR)

"""

And another one for the events (the different stimuli):

"""

t2 = ts.TimeSeries(data.events, sampling_interval=TR)

"""

Note that this example uses the EventRelated analyzer (also used in the
:ref:`grasshopper` example), but here, instead of providing an :class:`Events`
object as input, another :class:`TimeSeries` object is provided, containing an
equivalent time-series with the same dimensions as the time-series on which the
analysis is done, with '0' wherever no event of interest occured and an integer
wherever an even of interest occured (sequential different integers for the
different kinds of events).

"""

E = nta.EventRelatedAnalyzer(t1, t2, len_et)

"""

Two different methods of the EventRelatedAnalyzer are used: :attr:`E.eta`
refers to the event-triggered average of the activity and :attr:`E.ets` refers
to the event-triggered standard error of the mean (where the degrees of freedom
are set by the number of trials). Note that you can also extract the
event-triggered data itself as a list, by referring instead to
:attr:`E.et_data`.

We pass the eta and ets calculations straight into the visualization function,
which plots the result:

"""

fig01 = viz.plot_tseries(E.eta, ylabel='BOLD (% signal change)', yerror=E.ets)

"""

.. image:: fig/event_related_fmri_01.png


In the following example an alternative approach is taken to calculating
the event-related activity, based on the finite impulse-response
model (see [Burock2000]_ for details)


"""

fig02 = viz.plot_tseries(E.FIR, ylabel='BOLD (% signal change)')


"""

.. image:: fig/event_related_fmri_02.png

Yet another method is based on a cross-correlation performed in the frequency
domain (thanks to Lavi Secundo for providing a previous implementation of this
idea). This method can speed up calculation substantially for long time-series,
because the calculation is done using a vector multiplication in the frequency
domain representation of the time-series, instead of a more computationally
expensive convolution-like operation

"""

fig03 = viz.plot_tseries(E.xcorr_eta, ylabel='BOLD (% signal change)')


"""

.. image:: fig/event_related_fmri_03.png


We call plt.show() in order to display all the figures:
"""

plt.show()

"""

.. [Burock2000] M.A. Burock and A.M.Dale (2000). Estimation and Detection of
        Event-Related fMRI Signals with Temporally Correlated Noise: A
        Statistically Efficient and Unbiased Approach. Human Brain Mapping,
        11:249-260

"""

########NEW FILE########
__FILENAME__ = filtering_fmri
"""

.. _filter-fmri:

===================================
Filtering and normalizing fMRI data
===================================

Filtering fMRI data is very important. The time-series usually collected in
fMRI contain a broad-band signal. However, physilogically relevant signals are
thought to be present in only particular parts of the spectrum. For this
reason, filtering operations, such as detrending, are a common pre-processing
step in analysis of fMRI data analysis. In addition, for many fMRI analyses, it
is important to normalize the data in each voxel. This is because data may
differ between different voxels for 'uninteresting' reasons, such as local
blood-flow differences and signal amplitude differences due to the distance
from the receive coil. In the following, we will demonstrate usage of nitimes
analyzer objects for spectral estimation, filtering and normalization on fMRI
data.


We start by importing the needed modules. First modules from the standard lib
and from 3rd parties:

"""

import os

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.mlab import csv2rec


"""

Next, the particular nitime classes we will be using in this example:

"""

import nitime

# Import the time-series objects:
from nitime.timeseries import TimeSeries

# Import the analysis objects:
from nitime.analysis import SpectralAnalyzer, FilterAnalyzer, NormalizationAnalyzer

"""

For starters, let's analyze data that has been preprocessed and is extracted
into indivudal ROIs. This is the same data used in :ref:`multi-taper-coh` and
in :ref:`resting-state` (see these examples for details).

We start by setting the TR and reading the data from the CSV table into which
the data was saved:

"""

TR = 1.89

data_path = os.path.join(nitime.__path__[0], 'data')

data_rec = csv2rec(os.path.join(data_path, 'fmri_timeseries.csv'))

# Extract ROI information from the csv file headers:
roi_names = np.array(data_rec.dtype.names)

# This is the number of samples in each ROI:
n_samples = data_rec.shape[0]

# Make an empty container for the data
data = np.zeros((len(roi_names), n_samples))

# Insert the data from each ROI into a row in the data:
for n_idx, roi in enumerate(roi_names):
    data[n_idx] = data_rec[roi]

# Initialize TimeSeries object:
T = TimeSeries(data, sampling_interval=TR)
T.metadata['roi'] = roi_names


"""

We will start, by examining the spectrum of the original data, before
filtering. We do this by initializing a SpectralAnalyzer for the original data:

"""

S_original = SpectralAnalyzer(T)

# Initialize a figure to put the results into:
fig01 = plt.figure()
ax01 = fig01.add_subplot(1, 1, 1)


"""

The spectral analyzer has several different methods of spectral analysis,
however the all have a common API. This means that for all of them the output
is a tuple. The first item in the tuple are the central frequencies of the
frequency bins in the spectrum and the second item in the tuple are the
magnitude of the spectrum in that frequency bin. For the purpose of this
example, we will only plot the data from the 10th ROI (by indexing into the
spectra). We compare all the methods of spectral estimation by plotting them
together:

"""

ax01.plot(S_original.psd[0],
          S_original.psd[1][9],
          label='Welch PSD')

ax01.plot(S_original.spectrum_fourier[0],
          np.abs(S_original.spectrum_fourier[1][9]),
          label='FFT')

ax01.plot(S_original.periodogram[0],
          S_original.periodogram[1][9],
          label='Periodogram')

ax01.plot(S_original.spectrum_multi_taper[0],
          S_original.spectrum_multi_taper[1][9],
          label='Multi-taper')

ax01.set_xlabel('Frequency (Hz)')
ax01.set_ylabel('Power')

ax01.legend()


"""

.. image:: fig/filtering_fmri_01.png


Notice that, for this data, simply extracting a FFT is hardly informative (the
reasons for that are explained in :ref:`multi-taper-psd`). On the other hand,
the other methods provide different granularity of information, traded-off with
the robustness of the estimation. The cadillac of spectral estimates is the
multi-taper estimation, which provides both robustness and granularity, but
notice that this estimate requires more computation than other estimates
(certainly more estimates than the FFT).

We note that a lot of the power in the fMRI data seems to be concentrated in
frequencies below 0.02 Hz. These extremely low fluctuations in signal are often
considered to be 'noise', rather than reflecting neural processing. In
addition, there is a broad distribution of power up to the Nyquist
frequency. However, some estimates of the hemodynamic response suggest that
information above 0.15 could not reflect the slow filtering of neural response
to the BOLD response measured in fMRI. Thus, it would be advantageous to remove
fluctuations below 0.02 and above 0.15 Hz from the data. Next, we proceed to
filter the data into this range, using different methods.

We start by initializing a FilterAnalyzer. This is initialized with the
time-series containing the data and with the upper and lower bounds of the
range into which we wish to filter (in Hz):

"""

F = FilterAnalyzer(T, ub=0.15, lb=0.02)

# Initialize a figure to display the results:
fig02 = plt.figure()
ax02 = fig02.add_subplot(1, 1, 1)

# Plot the original, unfiltered data:
ax02.plot(F.data[0], label='unfiltered')

"""

As with the SpectralAnalyzer, there is a common API for the different methods
used for filtering. We use the following methods:

- Boxcar filter: The time-series is convolved with a box-car function of the
  right length to smooth the data to such an extent that the frequencies higher
  than represented by the length of this box-car function are no longer present
  in the smoothed version of the time-series. This functions as a low-pass filter. The
  data can then be high-pass filtered by subtracting this version of the data
  from the original. For a band-pass filter, both of these operations are done.

"""

ax02.plot(F.filtered_boxcar.data[0], label='Boxcar filter')

"""

- FIR filter: A digital filter with a finite impulse response. These filters
  have an order of 64 per default, but that can be adjusted by setting the key
  word argument 'filt_order', passed to initialize the FilterAnalyzer. For
  FIR filtering, :mod:`nitime` uses a Hamming window filter, but this can also
  be changed by setting the key word argument 'fir_win'.
  As with the boxcar filter, if band-pass filtering is required, a low-pass
  filter is applied and then a high-pass filter is applied to the resulting
  time-series.

"""

ax02.plot(F.fir.data[0], label='FIR')

"""

- IIR filter: A digital filter with an infinite impulse response function. Per
  default an elliptic filter is used here, but this can be changed, by setting
  the 'iir_type' key word argument used when initializing the FilterAnalyzer.

For both FIR filters and IIR filters, :func:`scipy.signal.filtfilt` is used in
order to achieve zero phase delay filtering.

"""

ax02.plot(F.iir.data[0], label='IIR')

"""

- Fourier filter: this is a quick and dirty filter. The data is FFT-ed into the
  frequency domain. The power in the unwanted frequency bins is removed (by
  replacing the power in these bins with zero) and the data is IFFT-ed back
  into the time-domain.

"""

ax02.plot(F.filtered_fourier.data[0], label='Fourier')
ax02.legend()
ax02.set_xlabel('Time (TR)')
ax02.set_ylabel('Signal amplitude (a.u.)')

"""

.. image:: fig/filtering_fmri_02.png


Examining the resulting time-series closely reveals that large fluctuations in
very slow frequencies have been removed, but also small fluctuations in high
frequencies have been attenuated through filtering.

Comparing the resulting spectra of these different filters shows the various
trade-offs of each filtering method, including the fidelity with which the
original spectrum is replicated within the pass-band and the amount of
attenuation within the stop-bands.

We can do that by initializng a SpectralAnalyzer for each one of the filtered
time-series resulting from the above operation and plotting their spectra. For
ease of compariso, we only plot the spectra using the multi-taper spectral
estimation. At the level of granularity provided by this method, the diferences
between the methods are emphasized:

"""

S_fourier = SpectralAnalyzer(F.filtered_fourier)
S_boxcar = SpectralAnalyzer(F.filtered_boxcar)
S_fir = SpectralAnalyzer(F.fir)
S_iir = SpectralAnalyzer(F.iir)

fig03 = plt.figure()
ax03 = fig03.add_subplot(1, 1, 1)

ax03.plot(S_original.spectrum_multi_taper[0],
          S_original.spectrum_multi_taper[1][9],
          label='Original')

ax03.plot(S_fourier.spectrum_multi_taper[0],
          S_fourier.spectrum_multi_taper[1][9],
          label='Fourier')

ax03.plot(S_boxcar.spectrum_multi_taper[0],
          S_boxcar.spectrum_multi_taper[1][9],
          label='Boxcar')

ax03.plot(S_fir.spectrum_multi_taper[0],
          S_fir.spectrum_multi_taper[1][9],
          label='FIR')

ax03.plot(S_iir.spectrum_multi_taper[0],
          S_iir.spectrum_multi_taper[1][9],
          label='IIR')

ax03.legend()


"""

.. image:: fig/filtering_fmri_03.png


Next, we turn to normalize the filtered data. This can be done in one of two
methods:

- Percent change: the data in each voxel is normalized as percent signal
  change, relative to the mean BOLD signal in the voxel

- Z score: The data in each voxel is normalized to have 0 mean and a standard
  deviation of 1.

We will use the filtered data, in order to demonstrate how the output of one
analyzer can be used as the input to the other:

"""

fig04 = plt.figure()
ax04 = fig04.add_subplot(1, 1, 1)

ax04.plot(NormalizationAnalyzer(F.fir).percent_change.data[0], label='% change')
ax04.plot(NormalizationAnalyzer(F.fir).z_score.data[0], label='Z score')
ax04.legend()
ax04.set_xlabel('Time (TR)')
ax04.set_ylabel('Amplitude (% change or Z-score)')

"""

.. image:: fig/filtering_fmri_04.png


Notice that the same methods of filtering and normalization can be applied to
fMRI data, upon reading it from a nifti file, using :mod:`nitime.fmri.io`.

We demonstrate that in what follows.[Notice that nibabel
(http://nipy.org/nibabel) is required in order to run the following
examples. An error will be thrown if nibabel is not installed]

"""

try:
    from nibabel import load
except ImportError:
    raise ImportError('You need nibabel (http:/nipy.org/nibabel/) in order to run this example')

import nitime.fmri.io as io

"""

We define the TR of the analysis and the frequency band of interest:

"""

TR = 1.35
f_lb = 0.02
f_ub = 0.15


"""

An fMRI data file with some fMRI data is shipped as part of the distribution,
the following line will find the path to this data on the specific computer:

"""

data_file_path = test_dir_path = os.path.join(nitime.__path__[0],
                                              'data')

fmri_file = os.path.join(data_file_path, 'fmri1.nii.gz')


"""

Read in the dimensions of the data, using nibabel:

"""

fmri_data = load(fmri_file)
volume_shape = fmri_data.shape[:-1]
coords = list(np.ndindex(volume_shape))
coords = np.array(coords).T


"""

We use :mod:`nitime.fmri.io` in order to generate a TimeSeries object from spatial
coordinates in the data file. Notice that normalization method is provided as a
string input to the keyword argument 'normalize' and the filter and its
properties are provided as a dict to the keyword argument 'filter':

"""

T_unfiltered = io.time_series_from_file(fmri_file,
                                        coords,
                                        TR=TR,
                                        normalize='percent')

T_fir = io.time_series_from_file(fmri_file,
                              coords,
                              TR=TR,
                              normalize='percent',
                              filter=dict(lb=f_lb,
                                          ub=f_ub,
                                          method='fir',
                                          filt_order=10))

T_iir = io.time_series_from_file(fmri_file,
                              coords,
                              TR=TR,
                              normalize='percent',
                              filter=dict(lb=f_lb,
                                          ub=f_ub,
                                          method='iir',
                                          filt_order=10))

T_boxcar = io.time_series_from_file(fmri_file,
                              coords,
                              TR=TR,
                              normalize='percent',
                              filter=dict(lb=f_lb,
                                          ub=f_ub,
                                          method='boxcar',
                                          filt_order=10))

fig05 = plt.figure()
ax05 = fig05.add_subplot(1, 1, 1)
S_unfiltered = SpectralAnalyzer(T_unfiltered).spectrum_multi_taper
S_fir = SpectralAnalyzer(T_fir).spectrum_multi_taper
S_iir = SpectralAnalyzer(T_iir).spectrum_multi_taper
S_boxcar = SpectralAnalyzer(T_boxcar).spectrum_multi_taper

random_voxel = np.random.randint(0, np.prod(volume_shape))

ax05.plot(S_unfiltered[0], S_unfiltered[1][random_voxel], label='Unfiltered')
ax05.plot(S_fir[0], S_fir[1][random_voxel], label='FIR filtered')
ax05.plot(S_iir[0], S_iir[1][random_voxel], label='IIR filtered')
ax05.plot(S_boxcar[0], S_boxcar[1][random_voxel], label='Boxcar filtered')
ax05.legend()

"""

.. image:: fig/filtering_fmri_05.png


Notice that though the boxcar filter doesn't usually do an amazing job with
long time-series and IIR/FIR filters seem to be superior in those cases, in
this example, where the time-series is much shorter, it sometimes does a
relatively decent job.

We call plt.show() in order to display the figure:

"""

plt.show()

########NEW FILE########
__FILENAME__ = granger_fmri
"""

.. gc-fmri

================================
Granger 'causality' of fMRI data
================================

Granger 'causality' analysis provides an asymmetric measure of the coupling
between two time-series. When discussing this analysis method, we will put the
word 'causality' in single quotes, as we believe that use of this word outside
of quotes should be reserved for particular circumstances, often not fulfilled
in the analysis of simultaneously recorder neuroscientific time-series (see
[Pearl2009]_ for an extensive discussion of this distinction).

The central idea behind this analysis is that time-series can be described in
terms of a time-delayed auto-regressive model of the form:

.. math::

   x_t = \sum_{i=1}^{n}a_i x_{t-i} + \epsilon_t

Here, a the past behaviour of a single time-series is used in order to predict
the current value of the time-series. In Granger 'causality' analysis, we test
whether the addition of a prediction of the time-series from another
time-series through a multi-variate auto-regressive model may improve our
prediction of the present behavior of the time-series (reducing the value of
the error term $\epsilon_t$):

.. math::

   x_t = \sum_{i=1}^{n}a_i x_{t-i} + b_i y_{t-i} + \epsilon_t


In our implementation of the algorithms used for this analysis, we follow
closely the description put forth by Ding et al. ([Ding2006]_). Also, see
:ref:`mar` and :ref:`ar` for examples even more closely modeled on the
examples mentioned in their paper.

Here, we will demonstrate the use of Granger 'causality' analysis with fMRI
data. The data is provided as part of the distribution and is taken from a
'resting state' scan. The data was motion corrected and averaged from several
ROIs.

We start by importing the needed modules:

"""

import os

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.mlab import csv2rec

import nitime
import nitime.analysis as nta
import nitime.timeseries as ts
import nitime.utils as tsu
from nitime.viz import drawmatrix_channels

"""

We then define a few parameters of the data: the TR and the bounds on the
frequency band of interest.

"""

TR = 1.89
f_ub = 0.15
f_lb = 0.02

"""

We read in the resting state fMRI data into a recarray from a csv file:

"""

data_path = os.path.join(nitime.__path__[0], 'data')

data_rec = csv2rec(os.path.join(data_path, 'fmri_timeseries.csv'))

roi_names = np.array(data_rec.dtype.names)
nseq = len(roi_names)
n_samples = data_rec.shape[0]
data = np.zeros((nseq, n_samples))

for n_idx, roi in enumerate(roi_names):
    data[n_idx] = data_rec[roi]

"""

We normalize the data in each of the ROIs to be in units of % change and
initialize the TimeSeries object:

"""

pdata = tsu.percent_change(data)
time_series = ts.TimeSeries(pdata, sampling_interval=TR)

"""

We initialize the GrangerAnalyzer object, while specifying the order of the
autoregressive model to be 1 (predict the current behavior of the time-series
based on one time-point back).

"""

G = nta.GrangerAnalyzer(time_series, order=1)

"""

For comparison, we also initialize a CoherenceAnalyzer and a
CorrelationAnalyzer, with the same TimeSeries object

"""

C1 = nta.CoherenceAnalyzer(time_series)
C2 = nta.CorrelationAnalyzer(time_series)

"""

We are only interested in the physiologically relevant frequency band
(approximately 0.02 to 0.15 Hz).

The spectral resolution is different in these two different analyzers. In the
CoherenceAnalyzer, the spectral resolution depends on the size of the window
used for calculating the spectral density and cross-spectrum, whereas in the
GrangerAnalyzer it is derived, as determined by the user, from the MAR model
used.

For this reason, the indices used to access the relevant part of the spectrum
will be different in the different analyzers.

"""

freq_idx_G = np.where((G.frequencies > f_lb) * (G.frequencies < f_ub))[0]
freq_idx_C = np.where((C1.frequencies > f_lb) * (C1.frequencies < f_ub))[0]


"""

We plot the 'causality' from x to y ($F_{x \rightarrow y}$) and from y to x
($F_{y \rightarrow x}$ for the first two ROIs and compare to the coherence
between these two time-series:

"""

coh = np.mean(C1.coherence[:, :, freq_idx_C], -1)  # Averaging on the last dimension
g1 = np.mean(G.causality_xy[:, :, freq_idx_G], -1)

fig01 = drawmatrix_channels(coh, roi_names, size=[10., 10.], color_anchor=0)

"""

.. image:: fig/granger_fmri_01.png

"""

fig02 = drawmatrix_channels(C2.corrcoef, roi_names, size=[10., 10.], color_anchor=0)

"""

.. image:: fig/granger_fmri_02.png

"""

fig03 = drawmatrix_channels(g1, roi_names, size=[10., 10.], color_anchor=0)

"""

.. image:: fig/granger_fmri_03.png

Differences in the HRF between different ROIs are a potential source of
misattribution of the direction and magnitude of dependence between time-series
in fMRI data (for a particularly extreme example of that see
[David2008]_). Therefore, as suggested by Roebroeck et al. [Roebroeck2005]_ and
[Kayser2009]_ we turn to examine the difference between $F_{x\rightarrow y}$ and
$F_{y\rightarrow x}$

"""

g2 = np.mean(G.causality_xy[:, :, freq_idx_G] - G.causality_yx[:, :, freq_idx_G], -1)
fig04 = drawmatrix_channels(g2, roi_names, size=[10., 10.], color_anchor=0)

"""

.. image:: fig/granger_fmri_04.png

If these values are found to be significantly different than 0, this
constitutes evidence for a correlation with a time-lag between the
regions. This is a necessary (though not necessarily sufficient...) condition
for establishing functional connectivity between the regions.

Finally, we call plt.show(), to show the plots created:

"""

plt.show()

"""

References
----------

.. [Pearl2009] J. Pearl (2009). Causal inference in statistics: An
   overview. Statistics surveys 3: 96-146.

.. [Ding2008] M. Ding, Y. Chen, S.L. Bressler (2006) Granger causality:
   basic theory and application to neuroscience. In Handbook of Time Series
   Analysis, ed. B. Schelter, M. Winterhalder, and J. Timmer, Wiley-VCH
   Verlage, 2006: 451-474

.. [Roebroeck2005] A. Roebroeck, E., Formisano R. Goebel (2005). Mapping
   directed influence over the brain using Granger causality and
   fMRI. NeuroImage 25: 230-242.

.. [Kayser2009] A. Kayser, F. Sun, M. D'Esposito (2009). A comparison of
   Granger causality and coherency in fMRI-based analysis of the motor
   system. NeuroImage 30: 3475-94

.. [David2008] O. David, I. Guillemain, S. Saillet, S. Reyt, C. Deransart,
   C. Segebarth, A. Depaulis (2008). Identifying neural drivers with functional
   MRI: An electrophysiological validation. PLoS Biol 6:e315


"""

########NEW FILE########
__FILENAME__ = grasshopper
"""

.. _grasshopper:


=====================================
 Auditory processing in grasshoppers
=====================================

Extracting the average time-series from one signal, time-locked to the
occurence of some type of event in another signal is a very typical operation in
the analysis of time-series from neuroscience experiments. Therefore, we have
an additional example of this kind of analysis in :ref:`et-fmri`


In the following code-snippet, we demonstrate the calculation of the
spike-triggered average (STA). This is the average of the stimulus wave-form
preceding the emission of a spike in the neuron and can be thought of as the
stimulus 'preferred' by this neuron.

We start by importing the required modules:
"""

import os

import numpy as np

import nitime
import nitime.timeseries as ts
import nitime.analysis as tsa
import nitime.viz as viz
from matplotlib import pyplot as plt

"""

Two data files are used in this example. The first contains the times of action
potentials ('spikes'), recorded intra-cellularly from primary auditory
receptors in the grasshopper *Locusta Migratoria*.

We read in these times and initialize an Events object from them. The
spike-times are given in micro-seconds:

"""

data_path = os.path.join(nitime.__path__[0], 'data')

spike_times = np.loadtxt(os.path.join(data_path, 'grasshopper_spike_times1.txt'))

spike_ev = ts.Events(spike_times, time_unit='us')


"""

The first data file contains the stimulus that was played during the
recording. Briefly, the stimulus played was a pure-tone in the cell's preferred
frequency amplitude modulated by Gaussian white-noise, up to a cut-off
frequency (200 Hz in this case, for details on the experimental procedures and
the stimulus see [Rokem2006]_).

"""

stim = np.loadtxt(os.path.join(data_path, 'grasshopper_stimulus1.txt'))


"""

The stimulus needs to be transformed from Volts to dB:

"""


def volt2dB(stim, maxdB=100):
    stim = (20 * 1 / np.log(10)) * (np.log(stim[:, 1] / 2.0e-5))
    return maxdB - stim.max() + stim

stim = volt2dB(stim, maxdB=76.4286)  # maxdB taken from the spike file header


"""

We create a time-series object for the stimulus, which was sampled at 20 kHz:

"""

stim_time_series = ts.TimeSeries(t0=0,
                                 data=stim,
                                 sampling_interval=50,
                                 time_unit='us')

"""

Note that the time-representation will not change if we now convert the
time-unit into ms. The only thing this accomplishes is to use this time-unit in
subsequent visualization of the resulting time-series

"""

stim_time_series.time_unit = 'ms'

"""

Next, we initialize an EventRelatedAnalyzer:

"""

event_related = tsa.EventRelatedAnalyzer(stim_time_series,
                                         spike_ev,
                                         len_et=200,
                                         offset=-200)

"""

The actual STA gets calculated in this line (the call to 'event_related.eta')
and the result gets input directly into the plotting function:

"""

fig01 = viz.plot_tseries(event_related.eta, ylabel='Amplitude (dB SPL)')

"""

We prettify the plot a bit by adding a dashed line at the mean of the stimulus

"""

ax = fig01.get_axes()[0]
xlim = ax.get_xlim()
ylim = ax.get_ylim()
mean_stim = np.mean(stim_time_series.data)
ax.plot([xlim[0], xlim[1]], [mean_stim, mean_stim], 'k--')


"""

.. image:: fig/grasshopper_01.png

In the following example, a second channel has been added to both the stimulus
and the spike-train time-series. This is the response of the same cell, to a
different stimulus, in which the frequency modulation has a higher frequency
cut-off (800 Hz).

"""


stim2 = np.loadtxt(os.path.join(data_path, 'grasshopper_stimulus2.txt'))
stim2 = volt2dB(stim2, maxdB=76.4286)
spike_times2 = np.loadtxt(os.path.join(data_path, 'grasshopper_spike_times2.txt'))


"""


We loop over the two spike-time events and stimulus time-series:


"""


et = []
means = []
for stim, spike in zip([stim, stim2], [spike_times, spike_times2]):
    stim_time_series = ts.TimeSeries(t0=0, data=stim, sampling_interval=50,
                                     time_unit='us')

    stim_time_series.time_unit = 'ms'

    spike_ev = ts.Events(spike, time_unit='us')
    #Initialize the event-related analyzer
    event_related = tsa.EventRelatedAnalyzer(stim_time_series,
                                             spike_ev,
                                             len_et=200,
                                             offset=-200)

    """

    This is the line which actually executes the analysis

    """

    et.append(event_related.eta)
    means.append(np.mean(stim_time_series.data))

"""

Stack the data from both time-series, initialize a new time-series and plot it:

"""

fig02 = viz.plot_tseries(
    ts.TimeSeries(data=np.vstack([et[0].data, et[1].data]),
                  sampling_rate=et[0].sampling_rate, time_unit='ms'))

ax = fig02.get_axes()[0]
xlim = ax.get_xlim()
ax.plot([xlim[0], xlim[1]], [means[0], means[0]], 'b--')
ax.plot([xlim[0], xlim[1]], [means[1], means[1]], 'g--')


"""

.. image:: fig/grasshopper_02.png


plt.show() is called in order to display the figures

"""

plt.show()

"""

The data used in this example is also available on the `CRCNS data sharing
web-site <http://crcns.org/>`_.


.. [Rokem2006] Ariel Rokem, Sebastian Watzl, Tim Gollisch, Martin Stemmler,
               Andreas V M Herz and Ines Samengo (2006). Spike-timing precision
               underlies the coding efficiency of auditory receptor neurons. J
               Neurophysiol, 95:2541--52

"""

########NEW FILE########
__FILENAME__ = mtm_baseband_power
"""

.. _multi-taper-baseband-power:

===========================================
Multitaper method for baseband demodulation
===========================================

Another application of the Slepian functions is to estimate the
complex demodulate of a narrowband signal. This signal is normally of
interest in neuroimaging when finding the lowpass power envelope and the
instantaneous phase. The traditional technique uses the Hilbert
transform to find the analytic signal. However, this approach suffers
problems of bias and reliability, much like the periodogram suffers in
PSD estimation. Once again, a multi-taper approach can provide an
estimate with lower variance.

The following demonstrates the use of spectra of multiple windows to
compute a power envelope of a signal in a desired band.

"""

import numpy as np
import scipy.signal as signal
import nitime.algorithms as nt_alg
import nitime.utils as nt_ut
import matplotlib.pyplot as pp

"""
We'll set up a test signal with a red spectrum (integrated Gaussian
noise).
"""

N = 10000
nfft = np.power( 2, int(np.ceil(np.log2(N))) )
NW = 40
W = float(NW)/N

"""
Create a nearly lowpass band-limited signal.
"""

s = np.cumsum( np.random.randn(N) )

"""
Strictly enforce the band-limited property in this signal.
"""

(b, a) = signal.butter(3, W, btype='lowpass')
slp = signal.lfilter(b, a, s)

"""
Modulate both signals away from baseband.
"""

s_mod = s * np.cos(2*np.pi*np.arange(N) * float(200) / N)
slp_mod = slp * np.cos(2*np.pi*np.arange(N) * float(200) / N)
fm = int( np.round(float(200) * nfft / N) )

"""
Create Slepians with the desired bandpass resolution (2W).
"""

(dpss, eigs) = nt_alg.dpss_windows(N, NW, 2*NW)
keep = eigs > 0.9
dpss = dpss[keep]; eigs = eigs[keep]

"""

Test 1
------

We'll compare multitaper baseband power estimation with regular
Hilbert transform method under actual narrowband conditions.
"""

# MT method
xk = nt_alg.tapered_spectra(slp_mod, dpss, NFFT=nfft)
mtm_bband = np.sum( 2 * (xk[:,fm] * np.sqrt(eigs))[:,None] * dpss, axis=0 )

# Hilbert transform method
hb_bband = signal.hilbert(slp_mod, N=nfft)[:N]

pp.figure()
pp.subplot(211)
pp.plot(slp_mod, 'g')
pp.plot(np.abs(mtm_bband), color='b', linewidth=3)
pp.title('Multitaper Baseband Power')

pp.subplot(212)
pp.plot(slp_mod, 'g')
pp.plot(np.abs(hb_bband), color='b', linewidth=3)
pp.title('Hilbert Baseband Power')
pp.gcf().tight_layout()

"""

.. image:: fig/mtm_baseband_power_01.png

We see in the narrowband signal case that there's not much difference
between taking the Hilbert transform and calculating the multitaper
complex demodulate.

"""

"""

Test 2
------

Now we'll compare multitaper baseband power estimation with regular
Hilbert transform method under more realistic non-narrowband
conditions.

"""

# MT method
xk = nt_alg.tapered_spectra(s_mod, dpss, NFFT=nfft)
w, n = nt_ut.adaptive_weights(xk, eigs, sides='onesided')
mtm_bband = np.sum( 2 * (xk[:,fm] * np.sqrt(eigs))[:,None] * dpss, axis=0 )

# Hilbert transform method
hb_bband = signal.hilbert(s_mod, N=nfft)[:N]

pp.figure()
pp.subplot(211)
pp.plot(s_mod, 'g')
pp.plot(np.abs(mtm_bband), color='b', linewidth=3)
pp.title('Multitaper Baseband Power')

pp.subplot(212)
pp.plot(s_mod, 'g')
pp.plot(np.abs(hb_bband), color='b', linewidth=3)
pp.title('Hilbert Baseband Power')
pp.gcf().tight_layout()

"""

.. image:: fig/mtm_baseband_power_02.png

Here we see that since the underlying signal is not truly narrowband,
the broadband bias is corrupting the Hilbert transform estimation of
the complex demodulate. However the multi-taper estimate clearly
remains lowpass.

"""

"""
Another property of computing the complex demodulate from the spectra
of multiple windows is that all bandpasses can be computed. In the
above examples, we were only taking a slice from the modulation
frequency that we set up. In practice, we might be interested in
bandpasses at various frequencies. Note here, though, that our
bandwidth is set by the Slepian sequences we used for analysis. The
following plot shows a family of complex demodulates at frequencies
near the modulation frequency.
"""

### Show a family of baseband demodulations from the multitaper method
#eigen_coefs = 2 * (xk[:,(fm-100):(fm+100):10] * np.sqrt(eigs)[:,None])
eigen_coefs = 2 * (xk[:,(fm-100):(fm+100):10] * \
                   np.sqrt(w[:,(fm-100):(fm+100):10]))
mtm_fbband = np.sum( eigen_coefs[:,:,None] * dpss[:,None,:], axis=0 )

pp.figure()
pp.plot(s_mod, 'g')
pp.plot(np.abs(mtm_fbband).T, linestyle='--', linewidth=2)
pp.plot(np.abs(mtm_bband), color='b', linewidth=3)
pp.title('Multitaper Baseband Power: Demodulation Freqs in (fm-100, fm+100)')
pp.gcf().tight_layout()

"""

.. image:: fig/mtm_baseband_power_03.png

"""

pp.show()

########NEW FILE########
__FILENAME__ = mtm_harmonic_test
"""

.. _multi-taper-harmonic-test:


=========================================
Multitaper F-test for harmonic components
=========================================

The Slepian sequences of the multitaper spectral estimation method can
also be used to perform a hypothesis test regarding the presence of a
pure sinusoid at any analyzed frequency. The F-test is used to assess
whether the power at a given frequency can be attributed to a single
line component. In this case, the power would be given by the
summed spectral convolutions of the Slepian frequency functions with
the line power spectrum, which is a dirac delta. The complex Fourier
coefficient of the putative sinusoid is estimated through a linear
regression of the Slepian DC components, and the strength of the
regression coefficient is tested against the residual spectral power
for the F-test.

The following demonstrates the use of the harmonic test.

"""

import numpy as np
import nitime.algorithms as nt_alg
import nitime.utils as nt_ut
import matplotlib.pyplot as pp

"""
We will set up a test signal with 3 harmonic components within
Gaussian noise. The line components must be sufficiently resolved
given the multi-taper bandwidth of 2NW.
"""

N = 10000
fft_pow = int( np.ceil(np.log2(N) + 2) )
NW = 4
lines = np.sort(np.random.randint(100, 2**(fft_pow-6), size=(3,)))
while np.any( np.diff(lines) < 2*NW ):
    lines = np.sort(np.random.randint(2**(fft_pow-6), size=(3,)))
lines = lines.astype('d')

"""
The harmonic test should find *exact* frequencies if they were to fall
on the FFT grid. (Try commenting the following to see.) In the
scenario of real sampled data, increasing the number of FFT points can
help to locate the line components.
"""

lines += np.random.randn(3) # displace from grid locations

"""
Now proceed to specify the frequencies, phases, and amplitudes.
"""

lines /= 2.0**(fft_pow-2) # ensure they are well separated

phs = np.random.rand(3) * 2 * np.pi
amps = np.sqrt(2)/2 + np.abs( np.random.randn(3) )

"""
Set the RMS noise power here. Strategies to detect harmonics in low
SNR include improving the reliability of the spectral estimate
(increasing NW) and/or increasing the number of FFT points. Note that
the former option will limit the ability to resolve lines at nearby
frequencies.
"""

nz_sig = 1

tx = np.arange(N)
harmonics = amps[:,None]*np.cos( 2*np.pi*tx*lines[:,None] + phs[:,None] )
harmonic = np.sum(harmonics, axis=0)
nz = np.random.randn(N) * nz_sig
sig = harmonic + nz

"""
Take a look at our mock signal.
"""

pp.figure()
pp.subplot(211)
pp.plot(harmonics.T)
pp.xlim(*(np.array([0.2, 0.3])*N).astype('i'))
pp.title('Sinusoid components')
pp.subplot(212)
pp.plot(harmonic, color='k', linewidth=3)
pp.plot(sig, color=(.6, .6, .6), linewidth=2, linestyle='--')
#pp.xlim(2000, 3000)
pp.xlim(*(np.array([0.2, 0.3])*N).astype('i'))
pp.title('Signal in noise')
pp.gcf().tight_layout()

"""

.. image:: fig/mtm_harmonic_test_01.png

"""

"""
Here we'll use the :func:`utils.detect_lines` function with the given
Slepian properties (NW), and we'll ensure that we limit spectral bias
by choosing Slepians with concentration factors greater than 0.9. The
arrays returned include the detected line frequencies (f) and their
complex coefficients (b). The frequencies are normalized from :math:`(0,\frac{1}{2})`
"""

f, b = nt_ut.detect_lines(sig, (NW, 2*NW), low_bias=True, NFFT=2**fft_pow)
h_est = 2*(b[:,None]*np.exp(2j*np.pi*tx*f[:,None])).real

pp.figure()
pp.subplot(211)
pp.plot(harmonics.T, 'c', linewidth=3)
pp.plot(h_est.T, 'r--', linewidth=2)
pp.title('%d lines detected'%h_est.shape[0])
pp.xlim(*(np.array([0.2, 0.3])*N).astype('i'))
pp.subplot(212)
err = harmonic - np.sum(h_est, axis=0)
pp.plot( err**2 )
pp.title('Error signal')
pp.show()

"""

.. image:: fig/mtm_harmonic_test_02.png

We can see the quality (or not) of our estimated lines. A breakdown of
the errors in the various estimated quantities follows in the demo.

"""

phs_est = np.angle(b)
phs_est[phs_est < 0] += 2*np.pi

phs_err = np.linalg.norm(phs_est - phs)**2
amp_err = np.linalg.norm(amps - 2*np.abs(b))**2 / np.linalg.norm(amps)**2
freq_err = np.linalg.norm(lines - f)**2

print 'freqs:', lines, '\testimated:', f, '\terr: %1.3e'%freq_err
print 'amp:', amps, '\testimated:', 2*np.abs(b), '\terr: %1.3e'%amp_err
print 'phase:', phs, '\testimated:', phs_est, '\terr: %1.3e'%phs_err
print 'MS error over noise: %1.3e'%(np.mean(err**2)/nz_sig**2,)

########NEW FILE########
__FILENAME__ = multi_taper_coh
"""

.. _multi-taper-coh:


================================
Multi-taper coherence estimation
================================


Coherence estimation can be done using windowed-spectra. This is the method
used in the example :ref:`resting-state`. In addition, multi-taper spectral
estimation can be used in order to calculate coherence and also confidence
intervals for the coherence values that result (see :ref:`multi-taper-psd`)


The data analyzed here is an fMRI data-set contributed by Beth Mormino. The
data is taken from a single subject in a"resting-state" scan, in which subjects
are fixating on a cross and maintaining alert wakefulness, but not performing
any other behavioral task.

We start by importing modules/functions we will use in this example and define
variables which will be used as the sampling interval of the TimeSeries
objects and as upper and lower bounds on the frequency range analyzed:

"""

import os

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.mlab import csv2rec
import scipy.stats.distributions as dist
from scipy import fftpack

import nitime
from nitime.timeseries import TimeSeries
from nitime import utils
import nitime.algorithms as alg
import nitime.viz
from nitime.viz import drawmatrix_channels
from nitime.analysis import CoherenceAnalyzer, MTCoherenceAnalyzer

TR = 1.89
f_ub = 0.15
f_lb = 0.02

"""

We read in the data into a recarray from a csv file:

"""

data_path = os.path.join(nitime.__path__[0], 'data')

data_rec = csv2rec(os.path.join(data_path, 'fmri_timeseries.csv'))


"""

The first line in the file contains the names of the different brain regions
(or ROI = regions of interest) from which the time-series were derived. We
extract the data into a regular array, while keeping the names to be used later:

"""

roi_names = np.array(data_rec.dtype.names)
nseq = len(roi_names)
n_samples = data_rec.shape[0]
data = np.zeros((nseq, n_samples))

for n_idx, roi in enumerate(roi_names):
    data[n_idx] = data_rec[roi]


"""

We normalize the data in each of the ROIs to be in units of % change:

"""

pdata = utils.percent_change(data)

"""

We start by performing the detailed analysis, but note that a significant
short-cut is presented below, so if you just want to know how to do this
(without needing to understand the details), skip on down.

We start by defining how many tapers will be used and calculate the values of
the tapers and the associated eigenvalues of each taper:

"""

NW = 4
K = 2 * NW - 1

tapers, eigs = alg.dpss_windows(n_samples, NW, K)

"""

We multiply the data by the tapers and derive the fourier transform and the
magnitude of the squared spectra (the power) for each tapered time-series:

"""


tdata = tapers[None, :, :] * pdata[:, None, :]
tspectra = fftpack.fft(tdata)
## mag_sqr_spectra = np.abs(tspectra)
## np.power(mag_sqr_spectra, 2, mag_sqr_spectra)


"""

Coherence for real sequences is symmetric, so we calculate this for only half
the spectrum (the other half is equal):

"""

L = n_samples / 2 + 1
sides = 'onesided'

"""

We estimate adaptive weighting of the tapers, based on the data (see
:ref:`multi-taper-psd` for an explanation and references):

"""

w = np.empty((nseq, K, L))
for i in xrange(nseq):
    w[i], _ = utils.adaptive_weights(tspectra[i], eigs, sides=sides)


"""

We proceed to calculate the coherence. We initialize empty data containers:

"""

csd_mat = np.zeros((nseq, nseq, L), 'D')
psd_mat = np.zeros((2, nseq, nseq, L), 'd')
coh_mat = np.zeros((nseq, nseq, L), 'd')
coh_var = np.zeros_like(coh_mat)


"""

Looping over the ROIs:

"""

for i in xrange(nseq):
    for j in xrange(i):

        """

        We calculate the multi-tapered cross spectrum between each two
        time-series:

        """

        sxy = alg.mtm_cross_spectrum(
           tspectra[i], tspectra[j], (w[i], w[j]), sides='onesided'
         )

        """

        And the individual PSD for each:

        """

        sxx = alg.mtm_cross_spectrum(
           tspectra[i], tspectra[i], w[i], sides='onesided'
           )
        syy = alg.mtm_cross_spectrum(
           tspectra[j], tspectra[j], w[j], sides='onesided'
           )

        psd_mat[0, i, j] = sxx
        psd_mat[1, i, j] = syy

        """

        Coherence is : $Coh_{xy}(\lambda) = \frac{|{f_{xy}(\lambda)}|^2}{f_{xx}(\lambda) \cdot f_{yy}(\lambda)}$

        """

        coh_mat[i, j] = np.abs(sxy) ** 2
        coh_mat[i, j] /= (sxx * syy)
        csd_mat[i, j] = sxy

        """

        The variance from the different samples is calculated using a jack-knife
        approach:

        """

        if i != j:
            coh_var[i, j] = utils.jackknifed_coh_variance(
               tspectra[i], tspectra[j], eigs, adaptive=True,
               )


"""

This measure is normalized, based on the number of tapers:

"""

coh_mat_xform = utils.normalize_coherence(coh_mat, 2 * K - 2)


"""

We calculate 95% confidence intervals based on the jack-knife variance
calculation:

"""

t025_limit = coh_mat_xform + dist.t.ppf(.025, K - 1) * np.sqrt(coh_var)
t975_limit = coh_mat_xform + dist.t.ppf(.975, K - 1) * np.sqrt(coh_var)


utils.normal_coherence_to_unit(t025_limit, 2 * K - 2, t025_limit)
utils.normal_coherence_to_unit(t975_limit, 2 * K - 2, t975_limit)

if L < n_samples:
    freqs = np.linspace(0, 1 / (2 * TR), L)
else:
    freqs = np.linspace(0, 1 / TR, L, endpoint=False)


"""

We look only at frequencies between 0.02 and 0.15 (the physiologically
relevant band, see http://imaging.mrc-cbu.cam.ac.uk/imaging/DesignEfficiency:

"""

freq_idx = np.where((freqs > f_lb) * (freqs < f_ub))[0]

"""

We extract the coherence and average over all these frequency bands:

"""

coh = np.mean(coh_mat[:, :, freq_idx], -1)  # Averaging on the last dimension


"""

The next line calls the visualization routine which displays the data

"""


fig01 = drawmatrix_channels(coh,
                            roi_names,
                            size=[10., 10.],
                            color_anchor=0,
                            title='MTM Coherence')


"""

.. image:: fig/multi_taper_coh_01.png

Next we perform the same analysis, using the nitime object oriented interface.

We start by initializing a TimeSeries object with this data and with the
sampling_interval provided above. We set the metadata 'roi' field with the ROI
names.


"""

T = TimeSeries(pdata, sampling_interval=TR)
T.metadata['roi'] = roi_names


"""

We initialize an MTCoherenceAnalyzer object with the TimeSeries object

"""

C2 = MTCoherenceAnalyzer(T)

"""

The relevant indices in the Analyzer object are derived:

"""

freq_idx = np.where((C2.frequencies > 0.02) * (C2.frequencies < 0.15))[0]


"""
The call to C2.coherence triggers the computation and this is averaged over the
frequency range of interest in the same line and then displayed:

"""

coh = np.mean(C2.coherence[:, :, freq_idx], -1)  # Averaging on the last dimension
fig02 = drawmatrix_channels(coh,
                            roi_names,
                            size=[10., 10.],
                            color_anchor=0,
                            title='MTCoherenceAnalyzer')


"""

.. image:: fig/multi_taper_coh_02.png


For comparison, we also perform the analysis using the standard
CoherenceAnalyzer object, which does the analysis using Welch's windowed
periodogram, instead of the multi-taper spectral estimation method (see
:ref:`resting_state` for a more thorough analysis of this data using this
method):

"""

C3 = CoherenceAnalyzer(T)

freq_idx = np.where((C3.frequencies > f_lb) * (C3.frequencies < f_ub))[0]

#Extract the coherence and average across these frequency bands:
coh = np.mean(C3.coherence[:, :, freq_idx], -1)  # Averaging on the last dimension
fig03 = drawmatrix_channels(coh,
                            roi_names,
                            size=[10., 10.],
                            color_anchor=0,
                            title='CoherenceAnalyzer')


"""

.. image:: fig/multi_taper_coh_03.png


plt.show() is called in order to display the figures:


"""

plt.show()

########NEW FILE########
__FILENAME__ = multi_taper_spectral_estimation
"""

.. _multi-taper-psd:

===============================
Multi-taper spectral estimation
===============================

The distribution of power in a signal, as a function of frequency, known as the
power spectrum (or PSD, for power spectral density) can be estimated using
variants of the discrete Fourier transform (DFT). The naive estimate of the
power spectrum, based on the values of the DFT estimated directly from the
signal, using the fast Fourier transform algorithm (FFT) is referred to as a
periodogram (see :func:`algorithms.periodogram`). This estimate suffers from
several problems [NR2007]_:

- Inefficiency: In most estimation problems, additional samples, or a denser
  sampling grid would usually lead to a better estimate (smaller variance of
  the estimate, given a constant level of noise). However, this is not the case
  for the periodogram. Even as we add more samples to our signal, or increase
  our sampling rate, our estimate at frequency $f_k$ does not improve. This is
  because of the effects these kinds of changes have on spectral
  estimates. Adding additional samples will improve the frequency domain
  resolution of our estimate and sampling at a finer rate will change the
  Nyquist frequency, the highest frequency for which the spectrum can be
  estimated. Thus, these changes do not improve the estimate at frequency
  $f_k$.

The inefficiency problem can be solved by treating different parts of the
signal as different samples from the same distribution, while assuming
stationarity of the signal. In this method, a sliding window is applied to
different parts of the signal and the windowed spectrum is averaged from these
different samples. This is sometimes referred to as Welch's periodogram
[Welch1967]_ and it is the default method used in
:func:`algorithms.get_spectra` (with the hanning window as the window function
used and no overlap between the windows).  However, it may lead to the
following problem:

- Spectral leakage and bias: Spectral leakage refers to the fact that the
  estimate of the spectrum at any given frequency bin is contaminated with the
  power from other frequency bands. This is a consequence of the fact that we
  always look at a time-limited signal. In the naive peridogram estimate all
  the samples within the time-limited signal are taken as they are (implicitly
  multiplied by 1) and all the samples outside of this time-limited signal are
  not taken at all (implicitly multiplied by 0). This is akin to what would
  happen if the signal were multiplied sample-by-sample with a 'boxcar' window,
  so called because the shape of this window is square, going from 0 to 1 over
  one sampling window. Multiplying the signal with a boxcar window in the
  time-domain is equivalent (due to the convolution theorem) to convolving it
  in the frequency domain with the spectrum of the boxcar window. The spectral
  leakage induced by this operation is demonstrated in the following example.


We start by importing the modules/functions we will need in this example


"""

import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sig
import scipy.stats.distributions as dist

import nitime.algorithms as tsa
import nitime.utils as utils
from nitime.viz import winspect
from nitime.viz import plot_spectral_estimate

"""
For demonstration, we will use a window of 128 points:
"""

npts = 128

fig01 = plt.figure()

# Boxcar with zeroed out fraction
b = sig.boxcar(npts)
zfrac = 0.15
zi = int(npts * zfrac)
b[:zi] = b[-zi:] = 0
name = 'Boxcar - zero fraction=%.2f' % zfrac
winspect(b, fig01, name)

"""

.. image:: fig/multi_taper_spectral_estimation_01.png

The figure on the left shows a boxcar window and the figure on the right
shows the spectrum of the boxcar function (in dB units, relative to the
frequency band of interest).

These two problems can together be mitigated through the use of other
windows. Other windows have been designed in order to optimize the amount of
spectral leakage and limit it to certain parts of the spectrum. The following
example demonstrates the spectral leakage for several different windows
(including the boxcar):
"""

fig02 = plt.figure()

# Boxcar with zeroed out fraction
b = sig.boxcar(npts)
zfrac = 0.15
zi = int(npts * zfrac)
b[:zi] = b[-zi:] = 0
name = 'Boxcar - zero fraction=%.2f' % zfrac
winspect(b, fig02, name)

winspect(sig.hanning(npts), fig02, 'Hanning')
winspect(sig.bartlett(npts), fig02, 'Bartlett')
winspect(sig.barthann(npts), fig02, 'Modified Bartlett-Hann')

"""

.. image:: fig/multi_taper_spectral_estimation_02.png

As before, the left figure displays the windowing function in the temporal
domain and the figure on the left displays the attentuation of spectral leakage
in the other frequency bands in the spectrum. Notice that though different
windowing functions have different spectral attenuation profiles, trading off
attenuation of leakage from frequency bands near the frequency of interest
(narrow-band leakage) with leakage from faraway frequency bands (broad-band
leakage) they are all superior in both of these respects to the boxcar window
used in the naive periodogram.

Another approach which deals with both the inefficiency problem and with the
spectral leakage problem is the use of taper functions. In this approach, the
entire signal is multiplied by a time-varying function. Several of these
functions may be used in order to emphasize and de-emphasize different parts of
the signal and these can be constructed to be orthogonal to each other,
constructing maximally independent samples at the length of the signal. As we
will see below, this allows for statistical estimation of the distribution of
the spectrum.

Discrete prolate spheroidal sequences (DPSS, also known as Slepian sequences)
[Slepian1978]_ are a class of taper functions which are constructed as a
solution to the problem of concentrating the spectrum to within a pre-specified
bandwidth. These tapers can be constructed using
:func:`algorithms.dpss_windows`, but for the purpose of spectral estimation, it
is sufficient to specify the bandwidth (which defines the boundary between
narrow-band and broad-band leakage) as an input to
:func:`algorithms.mutli_taper_psd` and this function will then construct the
appropriate windows, calculate the tapered spectra and average them.

We will demonstrate the use of DPSS in spectral estimation on a time-series
with known spectral properties generated from an auto-regressive process.

We start by defining a function which will be used throughout this example:

"""


def dB(x, out=None):
    if out is None:
        return 10 * np.log10(x)
    else:
        np.log10(x, out)
        np.multiply(out, 10, out)


"""

And the conversion factor from ln to dB:

"""

ln2db = dB(np.e)


"""

Next, we generate a sequence with known spectral properties:

"""

N = 512
ar_seq, nz, alpha = utils.ar_generator(N=N, drop_transients=10)
ar_seq -= ar_seq.mean()

"""

This is the true PSD for this sequence:

"""

fgrid, hz = tsa.freq_response(1.0, a=np.r_[1, -alpha], n_freqs=N)
psd = (hz * hz.conj()).real

"""

This is a one-sided spectrum, so we double the power:

"""

psd *= 2
dB(psd, psd)


"""

We begin by using the naive periodogram function (:func:`tsa.periodogram` in
order to calculate the PSD and compare that to the true PSD calculated above.


"""

freqs, d_psd = tsa.periodogram(ar_seq)
dB(d_psd, d_psd)

fig03 = plot_spectral_estimate(freqs, psd, (d_psd,), elabels=("Periodogram",))

"""

.. image:: fig/multi_taper_spectral_estimation_03.png


Next, we use Welch's periodogram, by applying :func:`tsa.get_spectra`. Note
that we explicitely provide the function with a 'method' dict, which specifies
the method used in order to calculate the PSD, but the default method is 'welch'.


"""

welch_freqs, welch_psd = tsa.get_spectra(ar_seq,
                                         method=dict(this_method='welch', NFFT=N))
welch_freqs *= (np.pi / welch_freqs.max())
welch_psd = welch_psd.squeeze()
dB(welch_psd, welch_psd)

fig04 = plot_spectral_estimate(freqs, psd, (welch_psd,), elabels=("Welch",))


"""

.. image:: fig/multi_taper_spectral_estimation_04.png


Next, we use the multi-taper estimation method. We estimate the spectrum:

"""

f, psd_mt, nu = tsa.multi_taper_psd(
    ar_seq, adaptive=False, jackknife=False
    )
dB(psd_mt, psd_mt)


"""

And get the number of tapers from here:

"""

Kmax = nu[0] / 2


"""

We calculate a Chi-squared model 95% confidence interval 2*Kmax degrees of
freedom (see [Percival1993]_ eq 258)


"""

p975 = dist.chi2.ppf(.975, 2 * Kmax)
p025 = dist.chi2.ppf(.025, 2 * Kmax)

l1 = ln2db * np.log(2 * Kmax / p975)
l2 = ln2db * np.log(2 * Kmax / p025)

hyp_limits = (psd_mt + l1, psd_mt + l2)

fig05 = plot_spectral_estimate(freqs, psd, (psd_mt,), hyp_limits,
              elabels=(r"MT with $\chi^{2}$ 95% interval",))

"""

.. image:: fig/multi_taper_spectral_estimation_05.png


An iterative method ([Thomson2007]_) can be used in order to adaptively set the
weighting of the different tapers, according to the actual spectral
concentration in the given signal (and not only the theoretical spectral
concentration calculated per default).

"""

f, adaptive_psd_mt, nu = tsa.multi_taper_psd(
    ar_seq,  adaptive=True, jackknife=False
    )
dB(adaptive_psd_mt, adaptive_psd_mt)

p975 = dist.chi2.ppf(.975, nu)
p025 = dist.chi2.ppf(.025, nu)

l1 = ln2db * np.log(nu / p975)
l2 = ln2db * np.log(nu / p025)

hyp_limits = (adaptive_psd_mt + l1, adaptive_psd_mt + l2)

fig06 = plot_spectral_estimate(freqs, psd, (adaptive_psd_mt,), hyp_limits,
                       elabels=('MT with adaptive weighting and 95% interval',))


"""

.. image:: fig/multi_taper_spectral_estimation_06.png

As metioned above, in addition to estimating the spectrum itself, an estimate
of the confidence interval of the spectrum can be generated using a
jack-knifing procedure [Thomson2007]_.

Let us define the following:

| **simple sample estimate**
| :math:`\hat{\theta} = \dfrac{1}{n}\sum_i Y_i`

This is the parameter estimate averaged from all the samples in the
distribution (all the tapered spectra).

| **leave-one-out measurement**
| :math:`\hat{\theta}_{-i} = \dfrac{1}{n-1}\sum_{k \neq i}Y_k`

This defines a group of estimates, where each sample is based on leaving one
measurement (one tapered spectrum) out.

| **pseudovalues**
| :math:`\hat{\theta}_i = n\hat{\theta} - (n-1)\hat{\theta}_{-i}`

The jackknifed esimator is computed as:

:math:`\tilde{\theta} = \dfrac{1}{n}\sum_i \hat{\theta}_i = n\hat{\theta} - \dfrac{n-1}{n}\sum_i \hat{\theta}_{-i}`

This estimator is known [Thomson2007]_ to be distributed about the true parameter \theta approximately as a Student's t distribution, with standard error defined as:

:math:`s^{2} = \dfrac{n-1}{n}\sum_i \left(\hat{\theta}_i - \tilde{\theta}\right)^{2}`

And degrees of freedom which depend on the number of tapers used (Kmax-1):

"""

_, _, jk_var = tsa.multi_taper_psd(ar_seq, adaptive=False, jackknife=True)

jk_p = (dist.t.ppf(.975, Kmax - 1) * np.sqrt(jk_var)) * ln2db

jk_limits = (psd_mt - jk_p, psd_mt + jk_p)


fig07 = plot_spectral_estimate(freqs, psd, (psd_mt,),
                               jk_limits,
                               elabels=('MT with JK 95% interval',))


"""

.. image:: fig/multi_taper_spectral_estimation_07.png

In addition, if the 'adaptive' flag is set to True, an iterative adaptive
method is used in order to correct bias in the spectrum.

Finally, we combine the adaptive estimation of the weights with the
jack-knifing procedure.

"""


_, _, adaptive_jk_var = tsa.multi_taper_psd(
    ar_seq, adaptive=True, jackknife=True
    )

# find 95% confidence limits from inverse of t-dist CDF
jk_p = (dist.t.ppf(.975, Kmax - 1) * np.sqrt(adaptive_jk_var)) * ln2db

adaptive_jk_limits = (adaptive_psd_mt - jk_p, adaptive_psd_mt + jk_p)

fig08 = plot_spectral_estimate(freqs, psd, (adaptive_psd_mt,),
              adaptive_jk_limits,
              elabels=('adaptive-MT with JK 95% interval',))


"""

.. image:: fig/multi_taper_spectral_estimation_08.png

We call plt.show() in order to show all the figures:

"""

plt.show()

"""

References

.. [NR2007] W.H. Press, S.A. Teukolsky, W.T Vetterling and B.P. Flannery (2007)
            Numerical Recipes: The Art of Scientific Computing. Cambridge:
            Cambridge University Press. 3rd Ed.

.. [Thomson2007] D.J. Thomson, Jackknifing Multitaper Spectrum Estimates, IEEE
                 Signal Processing Magazine, 2007, pp. 20-30.

.. [Welch1967] P.D. Welch (1967), The use of the fast fourier transform for the
               estimation of power spectra: a method based on time averaging
               over short modified periodograms. IEEE Transcations on Audio and
               Electroacoustics.

.. [Slepian1978] Slepian, D. Prolate spheroidal wave functions, Fourier
                 analysis, and uncertainty V: The discrete case. Bell System
                 Technical Journal, Volume 57 (1978), 1371430

.. [Percival1993] Percival D.B. and Walden A.T. (1993) Spectral Analysis for
                  Physical Applications: Multitaper and Conventional Univariate
                  Techniques. Cambridge University Press

"""

########NEW FILE########
__FILENAME__ = resting_state_fmri
"""

.. _resting-state:

===============================
Coherency analysis of fMRI data
===============================

The fMRI data-set analyzed in the following examples was contributed by Beth
Mormino. The data is taken from a single subject in a "resting-state" scan, in
which subjects are fixating on a cross and maintaining alert wakefulness, but
not performing any other behavioral task.

The data was pre-processed and time-series of BOLD responses were extracted
from different regions of interest (ROIs) in the brain. The data is organized
in csv file, where each column corresponds to an ROI and each row corresponds
to a sampling point.

In the following, we will demonstrate some simple time-series analysis and
visualization techniques which can be applied to this kind of data.


We start by importing the necessary modules/functions, defining the
sampling_interval of the data (TR, or repetition time) and the frequency band
of interest:

"""

import os

#Import from other libraries:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.mlab import csv2rec

import nitime
#Import the time-series objects:
from nitime.timeseries import TimeSeries
#Import the analysis objects:
from nitime.analysis import CorrelationAnalyzer, CoherenceAnalyzer
#Import utility functions:
from nitime.utils import percent_change
from nitime.viz import drawmatrix_channels, drawgraph_channels, plot_xcorr

#This information (the sampling interval) has to be known in advance:
TR = 1.89
f_lb = 0.02
f_ub = 0.15

"""

We use csv2rec to read the data in from file to a recarray:

"""

data_path = os.path.join(nitime.__path__[0], 'data')

data_rec = csv2rec(os.path.join(data_path, 'fmri_timeseries.csv'))

"""
This data structure contains in its dtype a field 'names', which contains the
first row in each column. In this case, that is the labels of the ROIs from
which the data in each column was extracted. The data from the recarray is
extracted into a 'standard' array and, for each ROI, it is normalized to
percent signal change, using the utils.percent_change function.

"""

#Extract information:
roi_names = np.array(data_rec.dtype.names)
n_samples = data_rec.shape[0]


#Make an empty container for the data
data = np.zeros((len(roi_names), n_samples))

for n_idx, roi in enumerate(roi_names):
    data[n_idx] = data_rec[roi]

#Normalize the data:
data = percent_change(data)


"""

We initialize a TimeSeries object from the normalized data:

"""

T = TimeSeries(data, sampling_interval=TR)
T.metadata['roi'] = roi_names


"""

First, we examine the correlations between the time-series extracted from
different parts of the brain. The following script extracts the data (using the
draw_matrix function, displaying the correlation matrix with the ROIs labeled.

"""

#Initialize the correlation analyzer
C = CorrelationAnalyzer(T)

#Display the correlation matrix
fig01 = drawmatrix_channels(C.corrcoef, roi_names, size=[10., 10.], color_anchor=0)


"""

.. image:: fig/resting_state_fmri_01.png

Notice that setting the color_anchor input to this function to 0 makes sure
that the center of the color map (here a blue => white => red) is at 0. In this
case, positive values will be displayed as red and negative values in blue.

We notice that the left caudate nucleus (labeled 'lcau') has an interesting
pattern of correlations. It has a high correlation with both the left putamen
('lput', which is located nearby) and also with the right caudate nucleus
('lcau'), which is the homologous region in the other hemisphere. Are these two
correlation values related to each other? The right caudate and left putamen
seem to have a moderately low correlation value. One way to examine this
question is by looking at the temporal structure of the cross-correlation
functions. In order to do that, from the CorrelationAnalyzer object, we extract
the normalized cross-correlation function. This results in another TimeSeries`
object, which contains the full time-series of the cross-correlation between
any combination of time-series from the different channels in the time-series
object. We can pass the resulting object, together with a list of indices to
the viz.plot_xcorr function, which visualizes the chosen combinations of
series:

"""

xc = C.xcorr_norm

idx_lcau = np.where(roi_names == 'lcau')[0]
idx_rcau = np.where(roi_names == 'rcau')[0]
idx_lput = np.where(roi_names == 'lput')[0]
idx_rput = np.where(roi_names == 'rput')[0]

fig02 = plot_xcorr(xc,
                   ((idx_lcau, idx_rcau),
                    (idx_lcau, idx_lput)),
                   line_labels=['rcau', 'lput'])


"""

.. image:: fig/resting_state_fmri_02.png


Note that the correlation is normalized, so that the the value of the
cross-correlation functions at the zero-lag point (time = 0 sec) is equal to
the Pearson correlation between the two time-series.  We observe that there are
correlations larger than the zero-lag correlation occurring at other
time-points preceding and following the zero-lag. This could arise because of a
more complex interplay of activity between two areas, which is not captured by
the correlation and can also arise because of differences in the
characteristics of the HRF in the two ROIs. One method of analysis which can
mitigate these issues is analysis of coherency between time-series
[Sun2005]_. This analysis computes an equivalent of the correlation in the
frequency domain:

.. math::

        R_{xy} (\lambda) = \frac{f_{xy}(\lambda)}
        {\sqrt{f_{xx} (\lambda) \cdot f_{yy}(\lambda)}}

Because this is a complex number, this computation results in two
quantities. First, the magnitude of this number, also referred to as
"coherence":

.. math::

   Coh_{xy}(\lambda) = |{R_{xy}(\lambda)}|^2 =
        \frac{|{f_{xy}(\lambda)}|^2}{f_{xx}(\lambda) \cdot f_{yy}(\lambda)}

This is a measure of the pairwise coupling between the two time-series. It can
vary between 0 and 1, with 0 being complete independence and 1 being complete
coupling. A time-series would have a coherence of 1 with itself, but not only:
since this measure is independent of the relative phase of the two time-series,
the coherence between a time-series and any phase-shifted version of itself
will also be equal to 1.

However, the relative phase is another quantity which can be derived from this
computation:

.. math::

   \phi(\lambda) = arg [R_{xy} (\lambda)] = arg [f_{xy} (\lambda)]


This value can be used in order to infer which area is leading and which area
is lagging (according to the sign of the relative phase) and, can be used to
compute the temporal delay between activity in one ROI and the other.

First, let's look at the pair-wise coherence between all our ROIs. This can be
done by creating a CoherenceAnalyzer object.

"""

C = CoherenceAnalyzer(T)

"""

Once this object is initialized with the TimeSeries object, the mid-frequency
of the frequency bands represented in the spectral decomposition of the
time-series can be accessed in the 'frequencies' attribute of the object. The
spectral resolution of this representation is the same one used in the
computation of the coherence.

Since the fMRI BOLD data contains data in frequencies which are not
physiologically relevant (presumably due to machine noise and fluctuations in
physiological measures unrelated to neural activity), we focus our analysis on
a band of frequencies between 0.02 and 0.15 Hz. This is easily achieved by
determining the values of the indices in :attr:`C.frequencies` and using those
indices in accessing the data in :attr:`C.coherence`. The coherence is then
averaged across all these frequency bands.

"""

freq_idx = np.where((C.frequencies > f_lb) * (C.frequencies < f_ub))[0]

"""
The C.coherence attribute is an ndarray of dimensions $n_{ROI}$ by $n_{ROI}$ by
$n_{frequencies}$.

We extract the coherence in that frequency band, average across the frequency
bands of interest and pass that to the visualization function:

"""


coh = np.mean(C.coherence[:, :, freq_idx], -1)  # Averaging on the last dimension
fig03 = drawmatrix_channels(coh, roi_names, size=[10., 10.], color_anchor=0)

"""

.. image:: fig/resting_state_fmri_03.png

We can also focus in on the ROIs we were interested in. This requires a little
bit more manipulation of the indices into the coherence matrix:

"""

idx = np.hstack([idx_lcau, idx_rcau, idx_lput, idx_rput])
idx1 = np.vstack([[idx[i]] * 4 for i in range(4)]).ravel()
idx2 = np.hstack(4 * [idx])

coh = C.coherence[idx1, idx2].reshape(4, 4, C.frequencies.shape[0])

"""

Extract the coherence and average across the same frequency bands as before:

"""


coh = np.mean(coh[:, :, freq_idx], -1)  # Averaging on the last dimension

"""

Finally, in this case, we visualize the adjacency matrix, by creating a network
graph of these ROIs (this is done by using the function drawgraph_channels
which relies on `networkx <http://networkx.lanl.gov>`_):

"""

fig04 = drawgraph_channels(coh, roi_names[idx])

"""

.. image:: fig/resting_state_fmri_04.png

This shows us that there is a stronger connectivity between the left putamen and
the left caudate than between the homologous regions in the other
hemisphere. In particular, in contrast to the relatively high correlation
between the right caudate and the left caudate, there is a rather low coherence
between the time-series in these two regions, in this frequency range.

Note that the connectivity described by coherency (and other measures of
functional connectivity) could arise because of neural connectivity between the
two regions, but also due to a common blood supply, or common fluctuations in
other physiological measures which affect the BOLD signal measured in both
regions. In order to be able to differentiate these two options, we would have
to conduct a comparison between two different behavioral states that affect the
neural activity in the two regions, without affecting these common
physiological factors, such as common blood supply (for an in-depth discussion
of these issues, see [Silver2010]_). In this case, we will simply assume that
the connectivity matrix presented represents the actual neural connectivity
between these two brain regions.

We notice that there is indeed a stronger coherence between left putamen and the
left caudate than between the left caudate and the right caudate. Next, we
might ask whether the moderate coherence between the left putamen and the right
caudate can be accounted for by the coherence these two time-series share with
the time-series derived from the left caudate. This kind of question can be
answered using an analysis of partial coherency. For the time series $x$ and
$y$, the partial coherence, given a third time-series $r$, is defined as:

.. math::

        Coh_{xy|r} = \frac{|{R_{xy}(\lambda) - R_{xr}(\lambda)
        R_{ry}(\lambda)}|^2}{(1-|{R_{xr}}|^2)(1-|{R_{ry}}|^2)}


In this case, we extract the partial coherence between the three regions,
excluding common effects of the left caudate. In order to do that, we generate
the partial-coherence attribute of the :class:`CoherenceAnalyzer` object, while
indexing on the additional dimension which this object had (the coherence
between time-series $x$ and time-series $y$, *given* time series $r$):

"""


idx3 = np.hstack(16 * [idx_lcau])
coh = C.coherence_partial[idx1, idx2, idx3].reshape(4, 4, C.frequencies.shape[0])
coh = np.mean(coh[:, :, freq_idx], -1)

"""


Again, we visualize the result, using both the :func:`viz.drawgraph_channels`
and the :func:`drawmatrix_channels` functions:


"""

fig05 = drawgraph_channels(coh, roi_names[idx])
fig06 = drawmatrix_channels(coh, roi_names[idx], color_anchor=0)

"""

.. image:: fig/resting_state_fmri_05.png


.. image:: fig/resting_state_fmri_06.png


As can be seen, the resulting partial coherence between left putamen and right
caudate, given the activity in the left caudate is smaller than the coherence
between these two areas, suggesting that part of this coherence can be
explained by their common connection to the left caudate.

XXX Add description of calculation of temporal delay here.


We call plt.show() in order to display the figures:

"""

plt.show()


"""

.. [Sun2005] F.T. Sun and L.M. Miller and M. D'Esposito(2005). Measuring
           temporal dynamics of functional networks using phase spectrum of
           fMRI data. Neuroimage, 28: 227-37.

.. [Silver2010] M.A Silver, AN Landau, TZ Lauritzen, W Prinzmetal, LC
   Robertson(2010) Isolating human brain functional connectivity associated
   with a specific cognitive process, in Human Vision and Electronic Imaging
   XV, edited by B.E. Rogowitz and T.N. Pappas, Proceedings of SPIE, Volume
   7527, pp. 75270B-1 to 75270B-9
"""

########NEW FILE########
__FILENAME__ = seed_analysis
"""

=========================================
Seed correlation/coherence with fMRI data
=========================================


Seed-based analysis is the analysis of a bivariate measure (such as correlation
or coherence) between one time-series (termed the 'seed') and many other
time-series (termed the 'targets'). This is a rather typical strategy in the
analysis of fMRI data where one might look for all the areas of the brain that
exhibit high level of connectivity to a particular region of interest.


We start by importing the needed modules. First modules from the standard lib
and from 3rd parties:

"""

import os

import numpy as np
import matplotlib.pyplot as plt


"""

Notice that nibabel (http://nipy.org.nibabel) is required in order to run this
example, so we test whether the user has that installed and throw an
informative error if not:

"""

try:
    from nibabel import load
except ImportError:
    raise ImportError('You need nibabel (http:/nipy.org/nibabel/) in order to run this example')

"""

The following are nitime modules:

"""

import nitime
import nitime.analysis as nta
import nitime.fmri.io as io

"""

We define the TR of the analysis and the frequency band of interest:

"""

TR = 1.35
f_lb = 0.02
f_ub = 0.15


"""

An fMRI data file with some actual fMRI data is shipped as part of the
distribution, the following line will find the path to this data on the
specific setup:

"""

data_path = test_dir_path = os.path.join(nitime.__path__[0], 'data')

fmri_file = os.path.join(data_path, 'fmri1.nii.gz')


"""

Read in the data, using nibabel:

"""

fmri_data = load(fmri_file)


"""
Notice that 'fmri_data' is not an array, but rather a NiftiImage
object. Nibabel cleverly delays the actual allocation of memory and reading
from file as long as possible. In this case, we only want information that is
available through the header of the nifti file, namely the dimensions of the
data.

We extract only the spatial dimensions of the data, excluding the last
dimension which is the time-dimension and generate a coords list:

"""

volume_shape = fmri_data.shape[:-1]

coords = list(np.ndindex(volume_shape))


"""

We choose some number of random voxels to serve as seed voxels:

"""

n_seeds = 3

# Choose n_seeds random voxels to be the seed voxels
seeds = np.random.randint(0, len(coords), n_seeds)
coords_seeds = np.array(coords)[seeds].T


"""

The entire volume is chosen to be the target:

"""

coords_target = np.array(coords).T


"""

We use nitime.fmri.io in order to generate TimeSeries objects from spatial
coordinates in the data file:

"""

# Make the seed time series:
time_series_seed = io.time_series_from_file(fmri_file,
                                coords_seeds,
                                TR=TR,
                                normalize='percent',
                                filter=dict(lb=f_lb,
                                            ub=f_ub,
                                            method='boxcar'))

# Make the target time series:
time_series_target = io.time_series_from_file(fmri_file,
                                          coords_target,
                                          TR=TR,
                                          normalize='percent',
                                          filter=dict(lb=f_lb,
                                                      ub=f_ub,
                                                    method='boxcar'))


"""

The SeedCoherencAnalyzer receives as input both of these TimeSeries and
calculates the coherence of each of the channels in the seed TimeSeries to
*all* the channels in the target TimeSeries. Here we initialize it with these
and with a method dict, which specifies the parameters of the spectral analysis
used for the coherence estimation:

"""

A = nta.SeedCoherenceAnalyzer(time_series_seed, time_series_target,
                            method=dict(NFFT=20))


"""

Similarly, the SeedCorrelationAnalyzer receives as input seed and target
time-series:

"""

B = nta.SeedCorrelationAnalyzer(time_series_seed, time_series_target)

"""

For the coherence, we are only interested in the physiologically relevant
frequency band:

"""

freq_idx = np.where((A.frequencies > f_lb) * (A.frequencies < f_ub))[0]


"""

The results in both analyzer objects are arrays of dimensions: (number of seeds
x number of targets). For the coherence, there is an additional last dimension
of: number of frequency bands, which we will average over.  For the
visualization, we extract the coherence and correlation values for each one of
the seeds separately:

"""

cor = []
coh = []

for this_seed in range(n_seeds):
    # Extract the coherence and average across these frequency bands:
    coh.append(np.mean(A.coherence[this_seed][:, freq_idx], -1))  # Averaging on the
                                                                 # last dimension

    cor.append(B.corrcoef[this_seed])  # No need to do any additional
                                       # computation


"""

We then put the coherence/correlation values back into arrays that have the
original shape of the volume from which the data was extracted:

"""

#For numpy fancy indexing into volume arrays:
coords_indices = list(coords_target)

vol_coh = []
vol_cor = []
for this_vol in range(n_seeds):
    vol_coh.append(np.empty(volume_shape))
    vol_coh[-1][coords_indices] = coh[this_vol]
    vol_cor.append(np.empty(volume_shape))
    vol_cor[-1][coords_indices] = cor[this_vol]


"""

We visualize this by choosing a random slice from the data:

"""

#Choose a random slice to display:
random_slice = np.random.randint(0, volume_shape[-1], 1)


"""

We display the coherence and correlation values for each seed voxel in this slice:

"""

fig01 = plt.figure()
fig02 = plt.figure()
ax_coh = []
ax_cor = []
for this_vox in range(n_seeds):
    ax_coh.append(fig01.add_subplot(1, n_seeds, this_vox + 1))
    ax_coh[-1].matshow(vol_coh[this_vox][:, :, random_slice].squeeze())
    ax_coh[-1].set_title('Seed coords: %s' % coords_seeds[:, this_vox])

    ax_cor.append(fig02.add_subplot(1, n_seeds, this_vox + 1))
    ax_cor[-1].matshow(vol_cor[this_vox][:, :, random_slice].squeeze())
    ax_cor[-1].set_title('Seed coords: %s' % coords_seeds[:, this_vox])

for x in zip(['Coherence', 'Correlation'], [fig01, fig02]):
    suptit = '%s between all the voxels in slice: ' % x[0]
    suptit += '%i and seed voxels' % random_slice
    x[1].suptitle(suptit)


"""

We can now compare the results in the coherence:


.. image:: fig/seed_analysis_01.png


And the correlation:

.. image:: fig/seed_analysis_02.png


We call plt.show() in order to display the figure:

"""

plt.show()

########NEW FILE########
__FILENAME__ = snr_example
"""
==============================================
Caclulation of Signal to noise and information
==============================================

This method is based on ideas described in [Borst1999]_ (Figure 2) and
[Hsu2004]_. The calculation can be used, for example, in order to estimate the
channel capacity of a neuron responding to a repeated stimulus.

The estimate of the information is based on the formula


.. math::

      I(S,R) = \int_{0}^{Nyquist}log_2(1+SNR(\omega))d\omega


Where $SNR(\omega)$ is the ratio of the signal power and the noise power at the
frequency band centered on $\omega$.This equation holde true for a Gaussian
channel and is an upper bound for all other cases.

The signal power is estimated as the power of the mean response to repeated
presentations of the same signal and the noise power is calculated as the
average of the power in the deviation from this average in each trial

We import the neccesary modules:

"""

import numpy as np
import matplotlib.pyplot as plt

import nitime.utils as utils
import nitime.timeseries as ts
import nitime.viz as viz

"""

For this example, we generate an auto-regressive sequence to be the signal:

"""

ar_seq, nz, alpha = utils.ar_generator(N=128, drop_transients=10)
ar_seq -= ar_seq.mean()

"""

The signal will be repeated several times, adding noise to the signal in each
repetition:

"""

n_trials = 12

fig_snr = []
sample = []
fig_tseries = []

"""

We add different levels of noise to the ar_seq variable, in order to
demonstrate the effects of adding noise on signal to noise ratio, as well as
the calculated information

"""

for idx, noise in enumerate([1, 10, 50, 100]):

    """

    Make n_trials repetitions of the signal:

    """

    sample.append(np.ones((n_trials, ar_seq.shape[-1])) + ar_seq)
    n_points = sample[-1].shape[-1]

    """

    Add noise:

    """

    for trial in  xrange(n_trials):
        sample[-1][trial] += np.random.randn(sample[-1][trial].shape[0]) * noise

    """

    This is the estimate of the signal:

    """

    sample_mean = np.mean(sample[-1], 0)

    """

    We plot a comparison of the actual signal (blue) and this estimate(blue). The
    thinner lines n other colors, represent the individual trials:

    """

    fig_tseries.append(plt.figure())
    ax = fig_tseries[-1].add_subplot(1, 1, 1)
    ax.plot(sample[-1].T)
    ax.plot(ar_seq, 'b', linewidth=4)
    ax.plot(sample_mean, 'r', linewidth=4)
    ax.set_xlabel('Time')
    ax.set_ylabel('Amplitude')

    """

    We present this at different levels of noise. With low noise, the estimate
    of the signal and also the response of the system at different repetitions
    is very similar to the original signal.

    """

    tseries = ts.TimeSeries(sample[-1], sampling_rate=1.)
    fig_snr.append(viz.plot_snr(tseries))

"""

.. image:: fig/snr_example_01.png

A special visualization function :func:`viz.plot_snr` is used in order to
display the signal power (blue) and the noise power (green), both in the
left sub-plot. In addition, the SNR (blue) and the cumulative information
(as a function of frequency bands, starting from low frequencies, in red)
are dislplayed in the right subplot.


.. image:: fig/snr_example_02.png

With more added noise, the estimate of the signal deviates further from the
signal.

.. image:: fig/snr_example_03.png

The signal power remains rather similar, but the noise power increases
(across all bands). As a consequence, the signal to noise ratio decreases and the
accumulated information decreases

.. image:: fig/snr_example_04.png

This becomes even more apparent with more noise:

.. image:: fig/snr_example_05.png


.. image:: fig/snr_example_06.png


Until, with the largest amplitued of noise, the signal power is almost completely
overwhelmed with noise:

.. image:: fig/snr_example_07.png

.. image:: fig/snr_example_08.png

Finally, we use :func:`plot_snr_diff` in order to compare information
transmission (on the left) and the signal to noise ratio (on the right) between
the two last noise levels:
"""

ts1 = ts.TimeSeries(sample[-1], sampling_rate=1.)
ts2 = ts.TimeSeries(sample[-2], sampling_rate=1.)
fig_compare = viz.plot_snr_diff(ts1, ts2)
plt.show()

"""

.. image:: fig/snr_example_09.png


References

    .. [Hsu2004] Hsu A, Borst A and Theunissen, FE (2004) Quantifying
    variability in neural responses ans its application for the validation of
    model predictions. Network: Comput Neural Syst 15:91-109

    .. [Borst1999] Borst A and Theunissen FE (1999) Information theory and
    neural coding. Nat Neurosci 2:947-957


"""

########NEW FILE########
__FILENAME__ = docscrape
"""Extract reference documentation from the NumPy source tree.

"""

import inspect
import textwrap
import re
import pydoc
from StringIO import StringIO
from warnings import warn

class Reader(object):
    """A line-based string reader.

    """
    def __init__(self, data):
        """
        Parameters
        ----------
        data : str
           String with lines separated by '\n'.

        """
        if isinstance(data,list):
            self._str = data
        else:
            self._str = data.split('\n') # store string as list of lines

        self.reset()

    def __getitem__(self, n):
        return self._str[n]

    def reset(self):
        self._l = 0 # current line nr

    def read(self):
        if not self.eof():
            out = self[self._l]
            self._l += 1
            return out
        else:
            return ''

    def seek_next_non_empty_line(self):
        for l in self[self._l:]:
            if l.strip():
                break
            else:
                self._l += 1

    def eof(self):
        return self._l >= len(self._str)

    def read_to_condition(self, condition_func):
        start = self._l
        for line in self[start:]:
            if condition_func(line):
                return self[start:self._l]
            self._l += 1
            if self.eof():
                return self[start:self._l+1]
        return []

    def read_to_next_empty_line(self):
        self.seek_next_non_empty_line()
        def is_empty(line):
            return not line.strip()
        return self.read_to_condition(is_empty)

    def read_to_next_unindented_line(self):
        def is_unindented(line):
            return (line.strip() and (len(line.lstrip()) == len(line)))
        return self.read_to_condition(is_unindented)

    def peek(self,n=0):
        if self._l + n < len(self._str):
            return self[self._l + n]
        else:
            return ''

    def is_empty(self):
        return not ''.join(self._str).strip()


class NumpyDocString(object):
    def __init__(self,docstring):
        docstring = textwrap.dedent(docstring).split('\n')

        self._doc = Reader(docstring)
        self._parsed_data = {
            'Signature': '',
            'Summary': [''],
            'Extended Summary': [],
            'Parameters': [],
            'Returns': [],
            'Raises': [],
            'Warns': [],
            'Other Parameters': [],
            'Attributes': [],
            'Methods': [],
            'See Also': [],
            'Notes': [],
            'Warnings': [],
            'References': '',
            'Examples': '',
            'index': {}
            }

        self._parse()

    def __getitem__(self,key):
        return self._parsed_data[key]

    def __setitem__(self,key,val):
        if not self._parsed_data.has_key(key):
            warn("Unknown section %s" % key)
        else:
            self._parsed_data[key] = val

    def _is_at_section(self):
        self._doc.seek_next_non_empty_line()

        if self._doc.eof():
            return False

        l1 = self._doc.peek().strip()  # e.g. Parameters

        if l1.startswith('.. index::'):
            return True

        l2 = self._doc.peek(1).strip() #    ---------- or ==========
        return l2.startswith('-'*len(l1)) or l2.startswith('='*len(l1))

    def _strip(self,doc):
        i = 0
        j = 0
        for i,line in enumerate(doc):
            if line.strip(): break

        for j,line in enumerate(doc[::-1]):
            if line.strip(): break

        return doc[i:len(doc)-j]

    def _read_to_next_section(self):
        section = self._doc.read_to_next_empty_line()

        while not self._is_at_section() and not self._doc.eof():
            if not self._doc.peek(-1).strip(): # previous line was empty
                section += ['']

            section += self._doc.read_to_next_empty_line()

        return section

    def _read_sections(self):
        while not self._doc.eof():
            data = self._read_to_next_section()
            name = data[0].strip()

            if name.startswith('..'): # index section
                yield name, data[1:]
            elif len(data) < 2:
                yield StopIteration
            else:
                yield name, self._strip(data[2:])

    def _parse_param_list(self,content):
        r = Reader(content)
        params = []
        while not r.eof():
            header = r.read().strip()
            if ' : ' in header:
                arg_name, arg_type = header.split(' : ')[:2]
            else:
                arg_name, arg_type = header, ''

            desc = r.read_to_next_unindented_line()
            desc = dedent_lines(desc)

            params.append((arg_name,arg_type,desc))

        return params

    
    _name_rgx = re.compile(r"^\s*(:(?P<role>\w+):`(?P<name>[a-zA-Z0-9_.-]+)`|"
                           r" (?P<name2>[a-zA-Z0-9_.-]+))\s*", re.X)
    def _parse_see_also(self, content):
        """
        func_name : Descriptive text
            continued text
        another_func_name : Descriptive text
        func_name1, func_name2, :meth:`func_name`, func_name3

        """
        items = []

        def parse_item_name(text):
            """Match ':role:`name`' or 'name'"""
            m = self._name_rgx.match(text)
            if m:
                g = m.groups()
                if g[1] is None:
                    return g[3], None
                else:
                    return g[2], g[1]
            raise ValueError("%s is not a item name" % text)

        def push_item(name, rest):
            if not name:
                return
            name, role = parse_item_name(name)
            items.append((name, list(rest), role))
            del rest[:]

        current_func = None
        rest = []
        
        for line in content:
            if not line.strip(): continue

            m = self._name_rgx.match(line)
            if m and line[m.end():].strip().startswith(':'):
                push_item(current_func, rest)
                current_func, line = line[:m.end()], line[m.end():]
                rest = [line.split(':', 1)[1].strip()]
                if not rest[0]:
                    rest = []
            elif not line.startswith(' '):
                push_item(current_func, rest)
                current_func = None
                if ',' in line:
                    for func in line.split(','):
                        push_item(func, [])
                elif line.strip():
                    current_func = line
            elif current_func is not None:
                rest.append(line.strip())
        push_item(current_func, rest)
        return items

    def _parse_index(self, section, content):
        """
        .. index: default
           :refguide: something, else, and more

        """
        def strip_each_in(lst):
            return [s.strip() for s in lst]

        out = {}
        section = section.split('::')
        if len(section) > 1:
            out['default'] = strip_each_in(section[1].split(','))[0]
        for line in content:
            line = line.split(':')
            if len(line) > 2:
                out[line[1]] = strip_each_in(line[2].split(','))
        return out
    
    def _parse_summary(self):
        """Grab signature (if given) and summary"""
        if self._is_at_section():
            return

        summary = self._doc.read_to_next_empty_line()
        summary_str = " ".join([s.strip() for s in summary]).strip()
        if re.compile('^([\w., ]+=)?\s*[\w\.]+\(.*\)$').match(summary_str):
            self['Signature'] = summary_str
            if not self._is_at_section():
                self['Summary'] = self._doc.read_to_next_empty_line()
        else:
            self['Summary'] = summary

        if not self._is_at_section():
            self['Extended Summary'] = self._read_to_next_section()
    
    def _parse(self):
        self._doc.reset()
        self._parse_summary()

        for (section,content) in self._read_sections():
            if not section.startswith('..'):
                section = ' '.join([s.capitalize() for s in section.split(' ')])
            if section in ('Parameters', 'Attributes', 'Methods',
                           'Returns', 'Raises', 'Warns'):
                self[section] = self._parse_param_list(content)
            elif section.startswith('.. index::'):
                self['index'] = self._parse_index(section, content)
            elif section == 'See Also':
                self['See Also'] = self._parse_see_also(content)
            else:
                self[section] = content

    # string conversion routines

    def _str_header(self, name, symbol='-'):
        return [name, len(name)*symbol]

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        if self['Signature']:
            return [self['Signature'].replace('*','\*')] + ['']
        else:
            return ['']

    def _str_summary(self):
        if self['Summary']:
            return self['Summary'] + ['']
        else:
            return []

    def _str_extended_summary(self):
        if self['Extended Summary']:
            return self['Extended Summary'] + ['']
        else:
            return []

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            for param,param_type,desc in self[name]:
                out += ['%s : %s' % (param, param_type)]
                out += self._str_indent(desc)
            out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += self[name]
            out += ['']
        return out

    def _str_see_also(self, func_role):
        if not self['See Also']: return []
        out = []
        out += self._str_header("See Also")
        last_had_desc = True
        for func, desc, role in self['See Also']:
            if role:
                link = ':%s:`%s`' % (role, func)
            elif func_role:
                link = ':%s:`%s`' % (func_role, func)
            else:
                link = "`%s`_" % func
            if desc or last_had_desc:
                out += ['']
                out += [link]
            else:
                out[-1] += ", %s" % link
            if desc:
                out += self._str_indent([' '.join(desc)])
                last_had_desc = True
            else:
                last_had_desc = False
        out += ['']
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        out += ['.. index:: %s' % idx.get('default','')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            out += ['   :%s: %s' % (section, ', '.join(references))]
        return out

    def __str__(self, func_role=''):
        out = []
        out += self._str_signature()
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters','Returns','Raises'):
            out += self._str_param_list(param_list)
        out += self._str_section('Warnings')
        out += self._str_see_also(func_role)
        for s in ('Notes','References','Examples'):
            out += self._str_section(s)
        out += self._str_index()
        return '\n'.join(out)


def indent(str,indent=4):
    indent_str = ' '*indent
    if str is None:
        return indent_str
    lines = str.split('\n')
    return '\n'.join(indent_str + l for l in lines)

def dedent_lines(lines):
    """Deindent a list of lines maximally"""
    return textwrap.dedent("\n".join(lines)).split("\n")

def header(text, style='-'):
    return text + '\n' + style*len(text) + '\n'


class FunctionDoc(NumpyDocString):
    def __init__(self, func, role='func', doc=None):
        self._f = func
        self._role = role # e.g. "func" or "meth"
        if doc is None:
            doc = inspect.getdoc(func) or ''
        try:
            NumpyDocString.__init__(self, doc)
        except ValueError, e:
            print '*'*78
            print "ERROR: '%s' while parsing `%s`" % (e, self._f)
            print '*'*78
            #print "Docstring follows:"
            #print doclines
            #print '='*78

        if not self['Signature']:
            func, func_name = self.get_func()
            try:
                # try to read signature
                argspec = inspect.getargspec(func)
                argspec = inspect.formatargspec(*argspec)
                argspec = argspec.replace('*','\*')
                signature = '%s%s' % (func_name, argspec)
            except TypeError, e:
                signature = '%s()' % func_name
            self['Signature'] = signature

    def get_func(self):
        func_name = getattr(self._f, '__name__', self.__class__.__name__)
        if inspect.isclass(self._f):
            func = getattr(self._f, '__call__', self._f.__init__)
        else:
            func = self._f
        return func, func_name
            
    def __str__(self):
        out = ''

        func, func_name = self.get_func()
        signature = self['Signature'].replace('*', '\*')

        roles = {'func': 'function',
                 'meth': 'method'}

        if self._role:
            if not roles.has_key(self._role):
                print "Warning: invalid role %s" % self._role
            out += '.. %s:: %s\n    \n\n' % (roles.get(self._role,''),
                                             func_name)

        out += super(FunctionDoc, self).__str__(func_role=self._role)
        return out


class ClassDoc(NumpyDocString):
    def __init__(self,cls,modulename='',func_doc=FunctionDoc,doc=None):
        if not inspect.isclass(cls):
            raise ValueError("Initialise using a class. Got %r" % cls)
        self._cls = cls

        if modulename and not modulename.endswith('.'):
            modulename += '.'
        self._mod = modulename
        self._name = cls.__name__
        self._func_doc = func_doc

        if doc is None:
            doc = pydoc.getdoc(cls)

        NumpyDocString.__init__(self, doc)

    @property
    def methods(self):
        return [name for name,func in inspect.getmembers(self._cls)
                if not name.startswith('_') and callable(func)]

    def __str__(self):
        out = ''
        out += super(ClassDoc, self).__str__()
        out += "\n\n"

        #for m in self.methods:
        #    print "Parsing `%s`" % m
        #    out += str(self._func_doc(getattr(self._cls,m), 'meth')) + '\n\n'
        #    out += '.. index::\n   single: %s; %s\n\n' % (self._name, m)

        return out



########NEW FILE########
__FILENAME__ = docscrape_sphinx
import re, inspect, textwrap, pydoc
from docscrape import NumpyDocString, FunctionDoc, ClassDoc

class SphinxDocString(NumpyDocString):
    # string conversion routines
    def _str_header(self, name, symbol='`'):
        return ['.. rubric:: ' + name, '']

    def _str_field_list(self, name):
        return [':' + name + ':']

    def _str_indent(self, doc, indent=4):
        out = []
        for line in doc:
            out += [' '*indent + line]
        return out

    def _str_signature(self):
        return ['']
        if self['Signature']:
            return ['``%s``' % self['Signature']] + ['']
        else:
            return ['']

    def _str_summary(self):
        return self['Summary'] + ['']

    def _str_extended_summary(self):
        return self['Extended Summary'] + ['']

    def _str_param_list(self, name):
        out = []
        if self[name]:
            out += self._str_field_list(name)
            out += ['']
            for param,param_type,desc in self[name]:
                out += self._str_indent(['**%s** : %s' % (param.strip(),
                                                          param_type)])
                out += ['']
                out += self._str_indent(desc,8)
                out += ['']
        return out

    def _str_section(self, name):
        out = []
        if self[name]:
            out += self._str_header(name)
            out += ['']
            content = textwrap.dedent("\n".join(self[name])).split("\n")
            out += content
            out += ['']
        return out

    def _str_see_also(self, func_role):
        out = []
        if self['See Also']:
            see_also = super(SphinxDocString, self)._str_see_also(func_role)
            out = ['.. seealso::', '']
            out += self._str_indent(see_also[2:])
        return out

    def _str_warnings(self):
        out = []
        if self['Warnings']:
            out = ['.. warning::', '']
            out += self._str_indent(self['Warnings'])
        return out

    def _str_index(self):
        idx = self['index']
        out = []
        if len(idx) == 0:
            return out

        out += ['.. index:: %s' % idx.get('default','')]
        for section, references in idx.iteritems():
            if section == 'default':
                continue
            elif section == 'refguide':
                out += ['   single: %s' % (', '.join(references))]
            else:
                out += ['   %s: %s' % (section, ','.join(references))]
        return out

    def _str_references(self):
        out = []
        if self['References']:
            out += self._str_header('References')
            if isinstance(self['References'], str):
                self['References'] = [self['References']]
            out.extend(self['References'])
            out += ['']
        return out

    def __str__(self, indent=0, func_role="obj"):
        out = []
        out += self._str_signature()
        out += self._str_index() + ['']
        out += self._str_summary()
        out += self._str_extended_summary()
        for param_list in ('Parameters', 'Attributes', 'Methods',
                           'Returns','Raises'):
            out += self._str_param_list(param_list)
        out += self._str_warnings()
        out += self._str_see_also(func_role)
        out += self._str_section('Notes')
        out += self._str_references()
        out += self._str_section('Examples')
        out = self._str_indent(out,indent)
        return '\n'.join(out)

class SphinxFunctionDoc(SphinxDocString, FunctionDoc):
    pass

class SphinxClassDoc(SphinxDocString, ClassDoc):
    pass

def get_doc_object(obj, what=None, doc=None):
    if what is None:
        if inspect.isclass(obj):
            what = 'class'
        elif inspect.ismodule(obj):
            what = 'module'
        elif callable(obj):
            what = 'function'
        else:
            what = 'object'
    if what == 'class':
        return SphinxClassDoc(obj, '', func_doc=SphinxFunctionDoc, doc=doc)
    elif what in ('function', 'method'):
        return SphinxFunctionDoc(obj, '', doc=doc)
    else:
        if doc is None:
            doc = pydoc.getdoc(obj)
        return SphinxDocString(doc)


########NEW FILE########
__FILENAME__ = github
"""Define text roles for GitHub

* ghissue - Issue
* ghpull - Pull Request
* ghuser - User

Adapted from bitbucket example here:
https://bitbucket.org/birkenfeld/sphinx-contrib/src/tip/bitbucket/sphinxcontrib/bitbucket.py

Authors
-------

* Doug Hellmann
* Min RK
"""
#
# Original Copyright (c) 2010 Doug Hellmann.  All rights reserved.
#

from docutils import nodes, utils
from docutils.parsers.rst.roles import set_classes

def make_link_node(rawtext, app, type, slug, options):
    """Create a link to a github resource.

    :param rawtext: Text being replaced with link node.
    :param app: Sphinx application context
    :param type: Link type (issues, changeset, etc.)
    :param slug: ID of the thing to link to
    :param options: Options dictionary passed to role func.
    """

    try:
        base = app.config.github_project_url
        if not base:
            raise AttributeError
        if not base.endswith('/'):
            base += '/'
    except AttributeError, err:
        raise ValueError('github_project_url configuration value is not set (%s)' % str(err))

    ref = base + type + '/' + slug + '/'
    set_classes(options)
    prefix = "#"
    if type == 'pull':
        prefix = "PR " + prefix
    node = nodes.reference(rawtext, prefix + utils.unescape(slug), refuri=ref,
                           **options)
    return node

def ghissue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
    """Link to a GitHub issue.

    Returns 2 part tuple containing list of nodes to insert into the
    document and a list of system messages.  Both are allowed to be
    empty.

    :param name: The role name used in the document.
    :param rawtext: The entire markup snippet, with role.
    :param text: The text marked with the role.
    :param lineno: The line number where rawtext appears in the input.
    :param inliner: The inliner instance that called us.
    :param options: Directive options for customization.
    :param content: The directive content for customization.
    """

    try:
        issue_num = int(text)
        if issue_num <= 0:
            raise ValueError
    except ValueError:
        msg = inliner.reporter.error(
            'GitHub issue number must be a number greater than or equal to 1; '
            '"%s" is invalid.' % text, line=lineno)
        prb = inliner.problematic(rawtext, rawtext, msg)
        return [prb], [msg]
    app = inliner.document.settings.env.app
    #app.info('issue %r' % text)
    if 'pull' in name.lower():
        category = 'pull'
    elif 'issue' in name.lower():
        category = 'issues'
    else:
        msg = inliner.reporter.error(
            'GitHub roles include "ghpull" and "ghissue", '
            '"%s" is invalid.' % name, line=lineno)
        prb = inliner.problematic(rawtext, rawtext, msg)
        return [prb], [msg]
    node = make_link_node(rawtext, app, category, str(issue_num), options)
    return [node], []

def ghuser_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
    """Link to a GitHub user.

    Returns 2 part tuple containing list of nodes to insert into the
    document and a list of system messages.  Both are allowed to be
    empty.

    :param name: The role name used in the document.
    :param rawtext: The entire markup snippet, with role.
    :param text: The text marked with the role.
    :param lineno: The line number where rawtext appears in the input.
    :param inliner: The inliner instance that called us.
    :param options: Directive options for customization.
    :param content: The directive content for customization.
    """
    app = inliner.document.settings.env.app
    #app.info('user link %r' % text)
    ref = 'https://www.github.com/' + text
    node = nodes.reference(rawtext, text, refuri=ref, **options)
    return [node], []

def ghcommit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
    """Link to a GitHub commit.

    Returns 2 part tuple containing list of nodes to insert into the
    document and a list of system messages.  Both are allowed to be
    empty.

    :param name: The role name used in the document.
    :param rawtext: The entire markup snippet, with role.
    :param text: The text marked with the role.
    :param lineno: The line number where rawtext appears in the input.
    :param inliner: The inliner instance that called us.
    :param options: Directive options for customization.
    :param content: The directive content for customization.
    """
    app = inliner.document.settings.env.app
    #app.info('user link %r' % text)
    try:
        base = app.config.github_project_url
        if not base:
            raise AttributeError
        if not base.endswith('/'):
            base += '/'
    except AttributeError, err:
        raise ValueError('github_project_url configuration value is not set (%s)' % str(err))

    ref = base + text
    node = nodes.reference(rawtext, text[:6], refuri=ref, **options)
    return [node], []


def setup(app):
    """Install the plugin.
    
    :param app: Sphinx application context.
    """
    app.info('Initializing GitHub plugin')
    app.add_role('ghissue', ghissue_role)
    app.add_role('ghpull', ghissue_role)
    app.add_role('ghuser', ghuser_role)
    app.add_role('ghcommit', ghcommit_role)
    app.add_config_value('github_project_url', None, 'env')
    return

########NEW FILE########
__FILENAME__ = ipython_console_highlighting
"""reST directive for syntax-highlighting ipython interactive sessions.
"""

#-----------------------------------------------------------------------------
# Needed modules

# Standard library
import re

# Third party
from pygments.lexer import Lexer, do_insertions
from pygments.lexers.agile import (PythonConsoleLexer, PythonLexer, 
                                   PythonTracebackLexer)
from pygments.token import Comment, Generic

from sphinx import highlighting


#-----------------------------------------------------------------------------
# Global constants
line_re = re.compile('.*?\n')

#-----------------------------------------------------------------------------
# Code begins - classes and functions

class IPythonConsoleLexer(Lexer):
    """
    For IPython console output or doctests, such as:

    .. sourcecode:: ipython

      In [1]: a = 'foo'

      In [2]: a
      Out[2]: 'foo'

      In [3]: print a
      foo

      In [4]: 1 / 0

    Notes:

      - Tracebacks are not currently supported.

      - It assumes the default IPython prompts, not customized ones.
    """
    
    name = 'IPython console session'
    aliases = ['ipython']
    mimetypes = ['text/x-ipython-console']
    input_prompt = re.compile("(In \[[0-9]+\]: )|(   \.\.\.+:)")
    output_prompt = re.compile("(Out\[[0-9]+\]: )|(   \.\.\.+:)")
    continue_prompt = re.compile("   \.\.\.+:")
    tb_start = re.compile("\-+")

    def get_tokens_unprocessed(self, text):
        pylexer = PythonLexer(**self.options)
        tblexer = PythonTracebackLexer(**self.options)

        curcode = ''
        insertions = []
        for match in line_re.finditer(text):
            line = match.group()
            input_prompt = self.input_prompt.match(line)
            continue_prompt = self.continue_prompt.match(line.rstrip())
            output_prompt = self.output_prompt.match(line)
            if line.startswith("#"):
                insertions.append((len(curcode),
                                   [(0, Comment, line)]))
            elif input_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Prompt, input_prompt.group())]))
                curcode += line[input_prompt.end():]
            elif continue_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Prompt, continue_prompt.group())]))
                curcode += line[continue_prompt.end():]
            elif output_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Output, output_prompt.group())]))
                curcode += line[output_prompt.end():]
            else:
                if curcode:
                    for item in do_insertions(insertions,
                                              pylexer.get_tokens_unprocessed(curcode)):
                        yield item
                        curcode = ''
                        insertions = []
                yield match.start(), Generic.Output, line
        if curcode:
            for item in do_insertions(insertions,
                                      pylexer.get_tokens_unprocessed(curcode)):
                yield item

#-----------------------------------------------------------------------------
# Register the extension as a valid pygments lexer
highlighting.lexers['ipython'] = IPythonConsoleLexer()

########NEW FILE########
__FILENAME__ = math_dollar
import re

def process_dollars(app, docname, source):
    r"""
    Replace dollar signs with backticks.

    More precisely, do a regular expression search.  Replace a plain
    dollar sign ($) by a backtick (`).  Replace an escaped dollar sign
    (\$) by a dollar sign ($).  Don't change a dollar sign preceded or
    followed by a backtick (`$ or $`), because of strings like
    "``$HOME``".  Don't make any changes on lines starting with
    spaces, because those are indented and hence part of a block of
    code or examples.

    This also doesn't replaces dollar signs enclosed in curly braces,
    to avoid nested math environments, such as ::

      $f(n) = 0 \text{ if $n$ is prime}$

    Thus the above line would get changed to

      `f(n) = 0 \text{ if $n$ is prime}`
    """
    s = "\n".join(source)
    if s.find("$") == -1:
        return
    # This searches for "$blah$" inside a pair of curly braces --
    # don't change these, since they're probably coming from a nested
    # math environment.  So for each match, we replace it with a temporary
    # string, and later on we substitute the original back.
    global _data
    _data = {}
    def repl(matchobj):
        global _data
        s = matchobj.group(0)
        t = "___XXX_REPL_%d___" % len(_data)
        _data[t] = s
        return t
    s = re.sub(r"({[^{}$]*\$[^{}$]*\$[^{}]*})", repl, s)
    
    # matches $...$
    dollars = re.compile(r"(?<!\$)(?<!\\)\$([^\$]+?)\$")
    # regular expression for \$
    slashdollar = re.compile(r"\\\$")

    s = dollars.sub(r":math:`\1`", s)

    s = slashdollar.sub(r"$", s)
    # change the original {...} things in:
    for r in _data:
        s = s.replace(r, _data[r])
    # now save results in "source"
    source[:] = [s]

def setup(app):
    app.connect("source-read", process_dollars)

########NEW FILE########
__FILENAME__ = numpydoc
"""
========
numpydoc
========

Sphinx extension that handles docstrings in the Numpy standard format. [1]

It will:

- Convert Parameters etc. sections to field lists.
- Convert See Also section to a See also entry.
- Renumber references.
- Extract the signature from the docstring, if it can't be determined otherwise.

.. [1] http://projects.scipy.org/scipy/numpy/wiki/CodingStyleGuidelines#docstring-standard

"""

import os, re, pydoc
from docscrape_sphinx import get_doc_object, SphinxDocString
import inspect

def mangle_docstrings(app, what, name, obj, options, lines,
                      reference_offset=[0]):
    if what == 'module':
        # Strip top title
        title_re = re.compile(r'^\s*[#*=]{4,}\n[a-z0-9 -]+\n[#*=]{4,}\s*',
                              re.I|re.S)
        lines[:] = title_re.sub('', "\n".join(lines)).split("\n")
    else:
        doc = get_doc_object(obj, what, "\n".join(lines))
        lines[:] = str(doc).split("\n")

    if app.config.numpydoc_edit_link and hasattr(obj, '__name__') and \
           obj.__name__:
        if hasattr(obj, '__module__'):
            v = dict(full_name="%s.%s" % (obj.__module__, obj.__name__))
        else:
            v = dict(full_name=obj.__name__)
        lines += ['', '.. htmlonly::', '']
        lines += ['    %s' % x for x in
                  (app.config.numpydoc_edit_link % v).split("\n")]

    # replace reference numbers so that there are no duplicates
    references = []
    for l in lines:
        l = l.strip()
        if l.startswith('.. ['):
            try:
                references.append(int(l[len('.. ['):l.index(']')]))
            except ValueError:
                print "WARNING: invalid reference in %s docstring" % name

    # Start renaming from the biggest number, otherwise we may
    # overwrite references.
    references.sort()
    if references:
        for i, line in enumerate(lines):
            for r in references:
                new_r = reference_offset[0] + r
                lines[i] = lines[i].replace('[%d]_' % r,
                                            '[%d]_' % new_r)
                lines[i] = lines[i].replace('.. [%d]' % r,
                                            '.. [%d]' % new_r)

    reference_offset[0] += len(references)

def mangle_signature(app, what, name, obj, options, sig, retann):
    # Do not try to inspect classes that don't define `__init__`
    if (inspect.isclass(obj) and
        'initializes x; see ' in pydoc.getdoc(obj.__init__)):
        return '', ''

    if not (callable(obj) or hasattr(obj, '__argspec_is_invalid_')): return
    if not hasattr(obj, '__doc__'): return

    doc = SphinxDocString(pydoc.getdoc(obj))
    if doc['Signature']:
        sig = re.sub("^[^(]*", "", doc['Signature'])
        return sig, ''

def initialize(app):
    try:
        app.connect('autodoc-process-signature', mangle_signature)
    except:
        monkeypatch_sphinx_ext_autodoc()

def setup(app, get_doc_object_=get_doc_object):
    global get_doc_object
    get_doc_object = get_doc_object_
    
    app.connect('autodoc-process-docstring', mangle_docstrings)
    app.connect('builder-inited', initialize)
    app.add_config_value('numpydoc_edit_link', None, True)

#------------------------------------------------------------------------------
# Monkeypatch sphinx.ext.autodoc to accept argspecless autodocs (Sphinx < 0.5)
#------------------------------------------------------------------------------

def monkeypatch_sphinx_ext_autodoc():
    global _original_format_signature
    import sphinx.ext.autodoc

    if sphinx.ext.autodoc.format_signature is our_format_signature:
        return

    print "[numpydoc] Monkeypatching sphinx.ext.autodoc ..."
    _original_format_signature = sphinx.ext.autodoc.format_signature
    sphinx.ext.autodoc.format_signature = our_format_signature

def our_format_signature(what, obj):
    r = mangle_signature(None, what, None, obj, None, None, None)
    if r is not None:
        return r[0]
    else:
        return _original_format_signature(what, obj)

########NEW FILE########
__FILENAME__ = only_directives
#
# A pair of directives for inserting content that will only appear in
# either html or latex.
#

from docutils.nodes import Body, Element
from docutils.parsers.rst import directives

class only_base(Body, Element):
    def dont_traverse(self, *args, **kwargs):
        return []

class html_only(only_base):
    pass

class latex_only(only_base):
    pass

def run(content, node_class, state, content_offset):
    text = '\n'.join(content)
    node = node_class(text)
    state.nested_parse(content, content_offset, node)
    return [node]

def html_only_directive(name, arguments, options, content, lineno,
                        content_offset, block_text, state, state_machine):
    return run(content, html_only, state, content_offset)

def latex_only_directive(name, arguments, options, content, lineno,
                         content_offset, block_text, state, state_machine):
    return run(content, latex_only, state, content_offset)

def builder_inited(app):
    if app.builder.name == 'html':
        latex_only.traverse = only_base.dont_traverse
    else:
        html_only.traverse = only_base.dont_traverse

def setup(app):
    app.add_directive('htmlonly', html_only_directive, True, (0, 0, 0))
    app.add_directive('latexonly', latex_only_directive, True, (0, 0, 0))
    app.add_node(html_only)
    app.add_node(latex_only)

    # This will *really* never see the light of day As it turns out,
    # this results in "broken" image nodes since they never get
    # processed, so best not to do this.
    # app.connect('builder-inited', builder_inited)

    # Add visit/depart methods to HTML-Translator:
    def visit_perform(self, node):
        pass
    def depart_perform(self, node):
        pass
    def visit_ignore(self, node):
        node.children = []
    def depart_ignore(self, node):
        node.children = []

    app.add_node(html_only, html=(visit_perform, depart_perform))
    app.add_node(html_only, latex=(visit_ignore, depart_ignore))
    app.add_node(latex_only, latex=(visit_perform, depart_perform))
    app.add_node(latex_only, html=(visit_ignore, depart_ignore))

########NEW FILE########
__FILENAME__ = plot_directive
"""A special directive for including a matplotlib plot.

The source code for the plot may be included in one of two ways:

  1. A path to a source file as the argument to the directive::

       .. plot:: path/to/plot.py

     When a path to a source file is given, the content of the
     directive may optionally contain a caption for the plot::

       .. plot:: path/to/plot.py

          This is the caption for the plot

     Additionally, one my specify the name of a function to call (with
     no arguments) immediately after importing the module::

       .. plot:: path/to/plot.py plot_function1

  2. Included as inline content to the directive::

     .. plot::

        import matplotlib.pyplot as plt
        import matplotlib.image as mpimg
        import numpy as np
        img = mpimg.imread('_static/stinkbug.png')
        imgplot = plt.imshow(img)

In HTML output, `plot` will include a .png file with a link to a high-res
.png and .pdf.  In LaTeX output, it will include a .pdf.

To customize the size of the plot, this directive supports all of the
options of the `image` directive, except for `target` (since plot will
add its own target).  These include `alt`, `height`, `width`, `scale`,
`align` and `class`.

Additionally, if the `:include-source:` option is provided, the
literal source will be displayed inline in the text, (as well as a
link to the source in HTML).  If this source file is in a non-UTF8 or
non-ASCII encoding, the encoding must be specified using the
`:encoding:` option.

The set of file formats to generate can be specified with the
`plot_formats` configuration variable.
"""

import sys, os, shutil, imp, warnings, cStringIO, re
try:
    from hashlib import md5
except ImportError:
    from md5 import md5

from docutils.parsers.rst import directives
try:
    # docutils 0.4
    from docutils.parsers.rst.directives.images import align
except ImportError:
    # docutils 0.5
    from docutils.parsers.rst.directives.images import Image
    align = Image.align
import sphinx

sphinx_version = sphinx.__version__.split(".")
# The split is necessary for sphinx beta versions where the string is
# '6b1'
sphinx_version = tuple([int(re.split('[a-z]', x)[0])
                        for x in sphinx_version[:2]])

import matplotlib
import matplotlib.cbook as cbook
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.image as image
from matplotlib import _pylab_helpers
from matplotlib.sphinxext import only_directives


class PlotWarning(Warning):
    """Warning category for all warnings generated by this directive.

    By printing our warnings with this category, it becomes possible to turn
    them into errors by using in your conf.py::

      warnings.simplefilter('error', plot_directive.PlotWarning)

    This way, you can ensure that your docs only build if all your examples
    actually run successfully.
    """
    pass


# os.path.relpath is new in Python 2.6
if hasattr(os.path, 'relpath'):
    relpath = os.path.relpath
else:
    # This code is snagged from Python 2.6

    def relpath(target, base=os.curdir):
        """
        Return a relative path to the target from either the current dir or an optional base dir.
        Base can be a directory specified either as absolute or relative to current dir.
        """

        if not os.path.exists(target):
            raise OSError, 'Target does not exist: '+target

        if not os.path.isdir(base):
            raise OSError, 'Base is not a directory or does not exist: '+base

        base_list = (os.path.abspath(base)).split(os.sep)
        target_list = (os.path.abspath(target)).split(os.sep)

        # On the windows platform the target may be on a completely
        # different drive from the base.
        if os.name in ['nt','dos','os2'] and base_list[0] <> target_list[0]:
            raise OSError, 'Target is on a different drive to base. Target: '+target_list[0].upper()+', base: '+base_list[0].upper()

        # Starting from the filepath root, work out how much of the
        # filepath is shared by base and target.
        for i in range(min(len(base_list), len(target_list))):
            if base_list[i] <> target_list[i]: break
        else:
            # If we broke out of the loop, i is pointing to the first
            # differing path elements.  If we didn't break out of the
            # loop, i is pointing to identical path elements.
            # Increment i so that in all cases it points to the first
            # differing path elements.
            i+=1

        rel_list = [os.pardir] * (len(base_list)-i) + target_list[i:]
        if rel_list:
            return os.path.join(*rel_list)
        else:
            return ""

template = """
.. htmlonly::

   %(links)s

   .. figure:: %(prefix)s%(tmpdir)s/%(outname)s.png
%(options)s

%(caption)s

.. latexonly::
   .. figure:: %(prefix)s%(tmpdir)s/%(outname)s.pdf
%(options)s

%(caption)s

"""

exception_template = """
.. htmlonly::

   [`source code <%(linkdir)s/%(basename)s.py>`__]

Exception occurred rendering plot.

"""

template_content_indent = '      '

def out_of_date(original, derived):
    """
    Returns True if derivative is out-of-date wrt original,
    both of which are full file paths.
    """
    return (not os.path.exists(derived) or
            (os.path.exists(original) and
             os.stat(derived).st_mtime < os.stat(original).st_mtime))

def run_code(plot_path, function_name, plot_code):
    """
    Import a Python module from a path, and run the function given by
    name, if function_name is not None.
    """
    # Change the working directory to the directory of the example, so
    # it can get at its data files, if any.  Add its path to sys.path
    # so it can import any helper modules sitting beside it.
    if plot_code is not None:
        exec(plot_code)
    else:
        pwd = os.getcwd()
        path, fname = os.path.split(plot_path)
        sys.path.insert(0, os.path.abspath(path))
        stdout = sys.stdout
        sys.stdout = cStringIO.StringIO()
        os.chdir(path)
        fd = None
        try:
            fd = open(fname)
            module = imp.load_module(
                "__plot__", fd, fname, ('py', 'r', imp.PY_SOURCE))
        finally:
            del sys.path[0]
            os.chdir(pwd)
            sys.stdout = stdout
            if fd is not None:
                fd.close()

        if function_name is not None:
            getattr(module, function_name)()

def run_savefig(plot_path, basename, tmpdir, destdir, formats):
    """
    Once a plot script has been imported, this function runs savefig
    on all of the figures in all of the desired formats.
    """
    fig_managers = _pylab_helpers.Gcf.get_all_fig_managers()
    for i, figman in enumerate(fig_managers):
        for j, (format, dpi) in enumerate(formats):
            if len(fig_managers) == 1:
                outname = basename
            else:
                outname = "%s_%02d" % (basename, i)
            outname = outname + "." + format
            outpath = os.path.join(tmpdir, outname)
            try:
                figman.canvas.figure.savefig(outpath, dpi=dpi)
            except:
                s = cbook.exception_to_str("Exception saving plot %s" % plot_path)
                warnings.warn(s, PlotWarning)
                return 0
            if j > 0:
                shutil.copyfile(outpath, os.path.join(destdir, outname))

    return len(fig_managers)

def clear_state():
    plt.close('all')
    matplotlib.rcdefaults()
    # Set a default figure size that doesn't overflow typical browser
    # windows.  The script is free to override it if necessary.
    matplotlib.rcParams['figure.figsize'] = (5.5, 4.5)

def render_figures(plot_path, function_name, plot_code, tmpdir, destdir,
                   formats):
    """
    Run a pyplot script and save the low and high res PNGs and a PDF
    in outdir.
    """
    plot_path = str(plot_path)  # todo, why is unicode breaking this
    basedir, fname = os.path.split(plot_path)
    basename, ext = os.path.splitext(fname)

    all_exists = True

    # Look for single-figure output files first
    for format, dpi in formats:
        outname = os.path.join(tmpdir, '%s.%s' % (basename, format))
        if out_of_date(plot_path, outname):
            all_exists = False
            break

    if all_exists:
        return 1

    # Then look for multi-figure output files, assuming
    # if we have some we have all...
    i = 0
    while True:
        all_exists = True
        for format, dpi in formats:
            outname = os.path.join(
                tmpdir, '%s_%02d.%s' % (basename, i, format))
            if out_of_date(plot_path, outname):
                all_exists = False
                break
        if all_exists:
            i += 1
        else:
            break

    if i != 0:
        return i

    # We didn't find the files, so build them

    clear_state()
    try:
        run_code(plot_path, function_name, plot_code)
    except:
        s = cbook.exception_to_str("Exception running plot %s" % plot_path)
        warnings.warn(s, PlotWarning)
        return 0

    num_figs = run_savefig(plot_path, basename, tmpdir, destdir, formats)

    if '__plot__' in sys.modules:
        del sys.modules['__plot__']

    return num_figs

def _plot_directive(plot_path, basedir, function_name, plot_code, caption,
                    options, state_machine):
    formats = setup.config.plot_formats
    if type(formats) == str:
        formats = eval(formats)

    fname = os.path.basename(plot_path)
    basename, ext = os.path.splitext(fname)

    # Get the directory of the rst file, and determine the relative
    # path from the resulting html file to the plot_directive links
    # (linkdir).  This relative path is used for html links *only*,
    # and not the embedded image.  That is given an absolute path to
    # the temporary directory, and then sphinx moves the file to
    # build/html/_images for us later.
    rstdir, rstfile = os.path.split(state_machine.document.attributes['source'])
    outdir = os.path.join('plot_directive', basedir)
    reldir = relpath(setup.confdir, rstdir)
    linkdir = os.path.join(reldir, outdir)

    # tmpdir is where we build all the output files.  This way the
    # plots won't have to be redone when generating latex after html.

    # Prior to Sphinx 0.6, absolute image paths were treated as
    # relative to the root of the filesystem.  0.6 and after, they are
    # treated as relative to the root of the documentation tree.  We
    # need to support both methods here.
    tmpdir = os.path.join('build', outdir)
    tmpdir = os.path.abspath(tmpdir)
    if sphinx_version < (0, 6):
        prefix = ''
    else:
        prefix = '/'
    if not os.path.exists(tmpdir):
        cbook.mkdirs(tmpdir)

    # destdir is the directory within the output to store files
    # that we'll be linking to -- not the embedded images.
    destdir = os.path.abspath(os.path.join(setup.app.builder.outdir, outdir))
    if not os.path.exists(destdir):
        cbook.mkdirs(destdir)

    # Properly indent the caption
    caption = '\n'.join(template_content_indent + line.strip()
                        for line in caption.split('\n'))

    # Generate the figures, and return the number of them
    num_figs = render_figures(plot_path, function_name, plot_code, tmpdir,
                              destdir, formats)

    # Now start generating the lines of output
    lines = []

    if plot_code is None:
        shutil.copyfile(plot_path, os.path.join(destdir, fname))

    if options.has_key('include-source'):
        if plot_code is None:
            lines.extend(
                ['.. include:: %s' % os.path.join(setup.app.builder.srcdir, plot_path),
                 '    :literal:'])
            if options.has_key('encoding'):
                lines.append('    :encoding: %s' % options['encoding'])
                del options['encoding']
        else:
            lines.extend(['::', ''])
            lines.extend(['    %s' % row.rstrip()
                          for row in plot_code.split('\n')])
        lines.append('')
        del options['include-source']
    else:
        lines = []

    if num_figs > 0:
        options = ['%s:%s: %s' % (template_content_indent, key, val)
                   for key, val in options.items()]
        options = "\n".join(options)

        for i in range(num_figs):
            if num_figs == 1:
                outname = basename
            else:
                outname = "%s_%02d" % (basename, i)

            # Copy the linked-to files to the destination within the build tree,
            # and add a link for them
            links = []
            if plot_code is None:
                links.append('`source code <%(linkdir)s/%(basename)s.py>`__')
            for format, dpi in formats[1:]:
                links.append('`%s <%s/%s.%s>`__' % (format, linkdir, outname, format))
            if len(links):
                links = '[%s]' % (', '.join(links) % locals())
            else:
                links = ''

            lines.extend((template % locals()).split('\n'))
    else:
        lines.extend((exception_template % locals()).split('\n'))

    if len(lines):
        state_machine.insert_input(
            lines, state_machine.input_lines.source(0))

    return []

def plot_directive(name, arguments, options, content, lineno,
                   content_offset, block_text, state, state_machine):
    """
    Handle the arguments to the plot directive.  The real work happens
    in _plot_directive.
    """
    # The user may provide a filename *or* Python code content, but not both
    if len(arguments):
        plot_path = directives.uri(arguments[0])
        basedir = relpath(os.path.dirname(plot_path), setup.app.builder.srcdir)

        # If there is content, it will be passed as a caption.

        # Indent to match expansion below.  XXX - The number of spaces matches
        # that of the 'options' expansion further down.  This should be moved
        # to common code to prevent them from diverging accidentally.
        caption = '\n'.join(content)

        # If the optional function name is provided, use it
        if len(arguments) == 2:
            function_name = arguments[1]
        else:
            function_name = None

        return _plot_directive(plot_path, basedir, function_name, None, caption,
                               options, state_machine)
    else:
        plot_code = '\n'.join(content)

        # Since we don't have a filename, use a hash based on the content
        plot_path = md5(plot_code).hexdigest()[-10:]

        return _plot_directive(plot_path, 'inline', None, plot_code, '', options,
                               state_machine)

def mark_plot_labels(app, document):
    """
    To make plots referenceable, we need to move the reference from
    the "htmlonly" (or "latexonly") node to the actual figure node
    itself.
    """
    for name, explicit in document.nametypes.iteritems():
        if not explicit:
            continue
        labelid = document.nameids[name]
        if labelid is None:
            continue
        node = document.ids[labelid]
        if node.tagname in ('html_only', 'latex_only'):
            for n in node:
                if n.tagname == 'figure':
                    sectname = name
                    for c in n:
                        if c.tagname == 'caption':
                            sectname = c.astext()
                            break

                    node['ids'].remove(labelid)
                    node['names'].remove(name)
                    n['ids'].append(labelid)
                    n['names'].append(name)
                    document.settings.env.labels[name] = \
                        document.settings.env.docname, labelid, sectname
                    break

def setup(app):
    setup.app = app
    setup.config = app.config
    setup.confdir = app.confdir

    options = {'alt': directives.unchanged,
               'height': directives.length_or_unitless,
               'width': directives.length_or_percentage_or_unitless,
               'scale': directives.nonnegative_int,
               'align': align,
               'class': directives.class_option,
               'include-source': directives.flag,
               'encoding': directives.encoding }

    app.add_directive('plot', plot_directive, True, (0, 2, 0), **options)
    app.add_config_value(
        'plot_formats',
        [('png', 80), ('hires.png', 200), ('pdf', 50)],
        True)

    app.connect('doctree-read', mark_plot_labels)

########NEW FILE########
__FILENAME__ = autoregressive
r"""

Autoregressive (AR) processes are processes of the form:

.. math::

  x(n) = a(1)x(n-1) + a(2)x(n-2) + ... + a(P)x(n-P) + e(n)

where e(n) is a white noise process. The usage of 'e' suggests interpreting
the linear combination of P past values of x(n) as the minimum mean square
error linear predictor of x(n) Thus

.. math::

  e(n) = x(n) - a(1)x(n-1) - a(2)x(n-2) - ... - a(P)x(n-P)

Due to whiteness, e(n) is also pointwise uncorrelated--ie,

.. math::
   :nowrap:

   \begin{align*}
      \text{(i)}   && E\{e(n)e^{*}(n-m)\}& = \delta(n-m) &\\
      \text{(ii)}  && E\{e(n)x^{*}(m)\}  & = 0           & m\neq n\\
      \text{(iii)} && E\{|e|^{2}\} = E\{e(n)e^{*}(n)\} &= E\{e(n)x^{*}(n)\} &
   \end{align*}

These principles form the basis of the methods in this module for
estimating the AR coefficients and the error/innovations power.
"""


import numpy as np
from nitime.lazy import scipy_linalg as linalg

import nitime.utils as utils
from .spectral import freq_response


def AR_est_YW(x, order, rxx=None):
    r"""Determine the autoregressive (AR) model of a random process x using
    the Yule Walker equations. The AR model takes this convention:

    .. math::

      x(n) = a(1)x(n-1) + a(2)x(n-2) + \dots + a(p)x(n-p) + e(n)

    where e(n) is a zero-mean white noise process with variance sig_sq,
    and p is the order of the AR model. This method returns the a_i and
    sigma

    The orthogonality property of minimum mean square error estimates
    states that

    .. math::

      E\{e(n)x^{*}(n-k)\} = 0 \quad 1\leq k\leq p

    Inserting the definition of the error signal into the equations above
    yields the Yule Walker system of equations:

    .. math::

      R_{xx}(k) = \sum_{i=1}^{p}a(i)R_{xx}(k-i) \quad1\leq k\leq p

    Similarly, the variance of the error process is

    .. math::

      E\{e(n)e^{*}(n)\}   = E\{e(n)x^{*}(n)\} = R_{xx}(0)-\sum_{i=1}^{p}a(i)R^{*}(i)


    Parameters
    ----------
    x : ndarray
        The sampled autoregressive random process

    order : int
        The order p of the AR system

    rxx : ndarray (optional)
        An optional, possibly unbiased estimate of the autocorrelation of x

    Returns
    -------
    ak, sig_sq : The estimated AR coefficients and innovations variance

    """
    if rxx is not None and type(rxx) == np.ndarray:
        r_m = rxx[:order + 1]
    else:
        r_m = utils.autocorr(x)[:order + 1]

    Tm = linalg.toeplitz(r_m[:order])
    y = r_m[1:]
    ak = linalg.solve(Tm, y)
    sigma_v = r_m[0].real - np.dot(r_m[1:].conj(), ak).real
    return ak, sigma_v


def AR_est_LD(x, order, rxx=None):
    r"""Levinson-Durbin algorithm for solving the Hermitian Toeplitz
    system of Yule-Walker equations in the AR estimation problem

    .. math::

       T^{(p)}a^{(p)} = \gamma^{(p+1)}

    where

    .. math::
       :nowrap:

       \begin{align*}
       T^{(p)} &= \begin{pmatrix}
          R_{0} & R_{1}^{*} & \cdots & R_{p-1}^{*}\\
          R_{1} & R_{0} & \cdots & R_{p-2}^{*}\\
          \vdots & \vdots & \ddots & \vdots\\
          R_{p-1}^{*} & R_{p-2}^{*} & \cdots & R_{0}
       \end{pmatrix}\\
       a^{(p)} &=\begin{pmatrix} a_1 & a_2 & \cdots a_p \end{pmatrix}^{T}\\
       \gamma^{(p+1)}&=\begin{pmatrix}R_1 & R_2 & \cdots & R_p \end{pmatrix}^{T}
       \end{align*}

    and :math:`R_k` is the autocorrelation of the kth lag

    Parameters
    ----------

    x : ndarray
      the zero-mean stochastic process
    order : int
      the AR model order--IE the rank of the system.
    rxx : ndarray, optional
      (at least) order+1 samples of the autocorrelation sequence

    Returns
    -------

    ak, sig_sq
      The AR coefficients for 1 <= k <= p, and the variance of the
      driving white noise process

    """

    if rxx is not None and type(rxx) == np.ndarray:
        rxx_m = rxx[:order + 1]
    else:
        rxx_m = utils.autocorr(x)[:order + 1]
    w = np.zeros((order + 1, ), rxx_m.dtype)
    # intialize the recursion with the R[0]w[1]=r[1] solution (p=1)
    b = rxx_m[0].real
    w_k = rxx_m[1] / b
    w[1] = w_k
    p = 2
    while p <= order:
        b *= 1 - (w_k * w_k.conj()).real
        w_k = (rxx_m[p] - (w[1:p] * rxx_m[1:p][::-1]).sum()) / b
        # update w_k from k=1,2,...,p-1
        # with a correction from w*_i i=p-1,p-2,...,1
        w[1:p] = w[1:p] - w_k * w[1:p][::-1].conj()
        w[p] = w_k
        p += 1
    b *= 1 - (w_k * w_k.conj()).real
    return w[1:], b


def lwr_recursion(r):
    r"""Perform a Levinson-Wiggins[Whittle]-Robinson recursion to
    find the coefficients a(i) that satisfy the matrix version
    of the Yule-Walker system of P + 1 equations:

    sum_{i=0}^{P} a(i)r(k-i) = 0, for k = {1,2,...,P}

    with the additional equation

    sum_{i=0}^{P} a(i)r(-k) = V

    where V is the covariance matrix of the innovations process,
    and a(0) is fixed at the identity matrix

    Also note that r is defined as:

    r(k) = E{ X(t)X*(t-k) } ( * = conjugate transpose )
    r(-k) = r*(k)


    This routine adapts the algorithm found in eqs (1)-(11)
    in Morf, Vieira, Kailath 1978

    Parameters
    ----------

    r : ndarray, shape (P + 1, nc, nc)

    Returns
    -------

    a : ndarray (P,nc,nc)
      coefficient sequence of order P
    sigma : ndarray (nc,nc)
      covariance estimate

    """

    # r is (P+1, nc, nc)
    nc = r.shape[1]
    P = r.shape[0] - 1

    a = np.zeros((P, nc, nc))  # ar coefs
    b = np.zeros_like(a)  # lp coefs
    sigb = np.zeros_like(r[0])  # forward prediction error covariance
    sigf = np.zeros_like(r[0])  # backward prediction error covariance
    delta = np.zeros_like(r[0])

    # initialize
    idnt = np.eye(nc)
    sigf[:] = r[0]
    sigb[:] = r[0]

    # iteratively find sequences A_{p+1}(i) and B_{p+1}(i)
    for p in range(P):

        # calculate delta_{p+1}
        # delta_{p+1} = r(p+1) + sum_{i=1}^{p} a(i)r(p+1-i)
        delta[:] = r[p + 1]
        for i in range(1, p + 1):
            delta += np.dot(a[i - 1], r[p + 1 - i])

        # intermediate values XXX: should turn these into solution-problems
        ka = np.dot(delta, linalg.inv(sigb))
        kb = np.dot(delta.conj().T, linalg.inv(sigf))

        # store a_{p} before updating sequence to a_{p+1}
        ao = a.copy()
        # a_{p+1}(i) = a_{p}(i) - ka*b_{p}(p+1-i) for i in {1,2,...,p}
        # b_{p+1}(i) = b_{p}(i) - kb*a_{p}(p+1-i) for i in {1,2,...,p}
        for i in range(1, p + 1):
            a[i - 1] -= np.dot(ka, b[p - i])
        for i in range(1, p + 1):
            b[i - 1] -= np.dot(kb, ao[p - i])

        a[p] = -ka
        b[p] = -kb

        sigf = np.dot(idnt - np.dot(ka, kb), sigf)
        sigb = np.dot(idnt - np.dot(kb, ka), sigb)

    return a, sigf


def MAR_est_LWR(x, order, rxx=None):
    r"""
    MAR estimation, using the LWR algorithm, as in Morf et al.


    Parameters
    ----------
    x : ndarray
        The sampled autoregressive random process

    order : int
        The order P of the AR system

    rxx : ndarray (optional)
        An optional, possibly unbiased estimate of the autocovariance of x

    Returns
    -------
    a, ecov : The system coefficients and the estimated covariance
    """
    Rxx = utils.autocov_vector(x, nlags=order)
    a, ecov = lwr_recursion(Rxx.transpose(2, 0, 1))
    return a, ecov


def AR_psd(ak, sigma_v, n_freqs=1024, sides='onesided'):
    r"""
    Compute the PSD of an AR process, based on the process coefficients and
    covariance

    n_freqs : int
        The number of spacings on the frequency grid from [-PI,PI).
        If sides=='onesided', n_freqs/2+1 frequencies are computed from [0,PI]

    sides : str (optional)
        Indicates whether to return a one-sided or two-sided PSD

    Returns
    -------
    (w, ar_psd)
    w : Array of normalized frequences from [-.5, .5) or [0,.5]
    ar_psd : A PSD estimate computed by sigma_v / |1-a(f)|**2 , where
             a(f) = DTFT(ak)


    """
    # compute the psd as |H(f)|**2, where H(f) is the transfer function
    # for this model s[n] = a1*s[n-1] + a2*s[n-2] + ... aP*s[n-P] + v[n]
    # Taken as a IIR system with unit-variance white noise input e[n]
    # and output s[n],
    # b0*e[n] = w0*s[n] + w1*s[n-1] + w2*s[n-2] + ... + wP*s[n-P],
    # where b0 = sqrt(VAR{v[n]}), w0 = 1, and wk = -ak for k>0
    # the transfer function here is H(f) = DTFT(w)
    # leading to Sxx(f)/Exx(f) = |H(f)|**2 = VAR{v[n]} / |W(f)|**2
    w, hw = freq_response(sigma_v ** 0.5, a=np.r_[1, -ak],
                          n_freqs=n_freqs, sides=sides)
    ar_psd = (hw * hw.conj()).real
    return (w, 2 * ar_psd) if sides == 'onesided' else (w, ar_psd)


#-----------------------------------------------------------------------------
# Granger causality analysis
#-----------------------------------------------------------------------------
def transfer_function_xy(a, n_freqs=1024):
    r"""Helper routine to compute the transfer function H(w) based
    on sequence of coefficient matrices A(i). The z transforms
    follow from this definition:

    X[t] + sum_{k=1}^P a[k]X[t-k] = Err[t]

    Parameters
    ----------

    a : ndarray, shape (P, 2, 2)
      sequence of coef matrices describing an mAR process
    n_freqs : int, optional
      number of frequencies to compute in range [0,PI]

    Returns
    -------

    Hw : ndarray
      The transfer function from innovations process vector to
      mAR process X

    """
    # these concatenations follow from the observation that A(0) is
    # implicitly the identity matrix
    ai = np.r_[1, a[:, 0, 0]]
    bi = np.r_[0, a[:, 0, 1]]
    ci = np.r_[0, a[:, 1, 0]]
    di = np.r_[1, a[:, 1, 1]]

    # compute A(w) such that A(w)X(w) = Err(w)
    w, aw = freq_response(ai, n_freqs=n_freqs)
    _, bw = freq_response(bi, n_freqs=n_freqs)
    _, cw = freq_response(ci, n_freqs=n_freqs)
    _, dw = freq_response(di, n_freqs=n_freqs)

    #A = np.array([ [1-aw, -bw], [-cw, 1-dw] ])
    A = np.array([[aw, bw], [cw, dw]])
    # compute the transfer function from Err to X. Since Err(w) is 1(w),
    # the transfer function H(w) = A^(-1)(w)
    # (use 2x2 matrix shortcut)
    detA = (A[0, 0] * A[1, 1] - A[0, 1] * A[1, 0])
    Hw = np.array([[dw, -bw], [-cw, aw]])
    Hw /= detA
    return w, Hw


def spectral_matrix_xy(Hw, cov):
    r"""Compute the spectral matrix S(w), from the convention:

    X[t] + sum_{k=1}^P a[k]X[t-k] = Err[t]

    The formulation follows from Ding, Chen, Bressler 2008,
    pg 6 eqs (11) to (15)

    The transfer function H(w) should be computed first from
    transfer_function_xy()

    Parameters
    ----------

    Hw : ndarray (2, 2, n_freqs)
      Pre-computed transfer function from transfer_function_xy()

    cov : ndarray (2, 2)
      The covariance between innovations processes in Err[t]

    Returns
    -------

    Sw : ndarrays
      matrix of spectral density functions
    """

    nw = Hw.shape[-1]
    # now compute specral density function estimate
    # S(w) = H(w)SigH*(w)
    Sw = np.empty((2, 2, nw), 'D')

    # do a shortcut for 2x2:
    # compute T(w) = SigH*(w)
    # t00 = Sig[0,0] * H*_00(w) + Sig[0,1] * H*_10(w)
    t00 = cov[0, 0] * Hw[0, 0].conj() + cov[0, 1] * Hw[0, 1].conj()
    # t01 = Sig[0,0] * H*_01(w) + Sig[0,1] * H*_11(w)
    t01 = cov[0, 0] * Hw[1, 0].conj() + cov[0, 1] * Hw[1, 1].conj()
    # t10 = Sig[1,0] * H*_00(w) + Sig[1,1] * H*_10(w)
    t10 = cov[1, 0] * Hw[0, 0].conj() + cov[1, 1] * Hw[0, 1].conj()
    # t11 = Sig[1,0] * H*_01(w) + Sig[1,1] * H*_11(w)
    t11 = cov[1, 0] * Hw[1, 0].conj() + cov[1, 1] * Hw[1, 1].conj()

    # now S(w) = H(w)T(w)
    Sw[0, 0] = Hw[0, 0] * t00 + Hw[0, 1] * t10
    Sw[0, 1] = Hw[0, 0] * t01 + Hw[0, 1] * t11
    Sw[1, 0] = Hw[1, 0] * t00 + Hw[1, 1] * t10
    Sw[1, 1] = Hw[1, 0] * t01 + Hw[1, 1] * t11

    return Sw


def coherence_from_spectral(Sw):
    r"""Compute the spectral coherence between processes X and Y,
    given their spectral matrix S(w)

    Parameters
    ----------

    Sw : ndarray
      spectral matrix
    """

    Sxx = Sw[0, 0].real
    Syy = Sw[1, 1].real

    Sxy_mod_sq = (Sw[0, 1] * Sw[1, 0]).real
    Sxy_mod_sq /= Sxx
    Sxy_mod_sq /= Syy
    return Sxy_mod_sq


def interdependence_xy(Sw):
    r"""Compute the 'total interdependence' between processes X and Y,
    given their spectral matrix S(w)

    Parameters
    ----------

    Sw : ndarray
      spectral matrix

    Returns
    -------

    fxy(w)
      interdependence function of frequency
    """

    Cw = coherence_from_spectral(Sw)
    return -np.log(1 - Cw)


def granger_causality_xy(a, cov, n_freqs=1024):
    r"""Compute the Granger causality between processes X and Y, which
    are linked in a multivariate autoregressive (mAR) model parameterized
    by coefficient matrices a(i) and the innovations covariance matrix

    X[t] + sum_{k=1}^P a[k]X[t-k] = Err[t]

    Parameters
    ----------

    a : ndarray, (P,2,2)
      coefficient matrices characterizing the autoregressive mixing
    cov : ndarray, (2,2)
      covariance matrix characterizing the innovations vector
    n_freqs: int
      number of frequencies to compute in the fourier transform

    Returns
    -------

    w, f_x_on_y, f_y_on_x, f_xy, Sw
      1) vector of frequencies
      2) function of the Granger causality of X on Y
      3) function of the Granger causality of Y on X
      4) function of the 'instantaneous causality' between X and Y
      5) spectral density matrix
    """

    w, Hw = transfer_function_xy(a, n_freqs=n_freqs)

    sigma = cov[0, 0]
    upsilon = cov[0, 1]
    gamma = cov[1, 1]

    # this transformation of the transfer functions computes the
    # Granger causality of Y on X
    gamma2 = gamma - upsilon ** 2 / sigma

    Hxy = Hw[0, 1]
    Hxx_hat = Hw[0, 0] + (upsilon / sigma) * Hxy

    xx_auto_component = (sigma * Hxx_hat * Hxx_hat.conj()).real
    cross_component = gamma2 * Hxy * Hxy.conj()
    Sxx = xx_auto_component + cross_component
    f_y_on_x = np.log(Sxx.real / xx_auto_component)

    # this transformation computes the Granger causality of X on Y
    sigma2 = sigma - upsilon ** 2 / gamma

    Hyx = Hw[1, 0]
    Hyy_hat = Hw[1, 1] + (upsilon / gamma) * Hyx
    yy_auto_component = (gamma * Hyy_hat * Hyy_hat.conj()).real
    cross_component = sigma2 * Hyx * Hyx.conj()
    Syy = yy_auto_component + cross_component
    f_x_on_y = np.log(Syy.real / yy_auto_component)

    # now compute cross densities, using the latest transformation
    Hxx = Hw[0, 0]
    Hyx = Hw[1, 0]
    Hxy_hat = Hw[0, 1] + (upsilon / gamma) * Hxx
    Sxy = sigma2 * Hxx * Hyx.conj() + gamma * Hxy_hat * Hyy_hat.conj()
    Syx = sigma2 * Hyx * Hxx.conj() + gamma * Hyy_hat * Hxy_hat.conj()

    # can safely throw away imaginary part
    # since Sxx and Syy are real, and Sxy == Syx*
    detS = (Sxx * Syy - Sxy * Syx).real
    f_xy = xx_auto_component * yy_auto_component
    f_xy /= detS
    f_xy = np.log(f_xy)

    return w, f_x_on_y, f_y_on_x, f_xy, np.array([[Sxx, Sxy], [Syx, Syy]])

########NEW FILE########
__FILENAME__ = cohere
"""

Coherency is an analogue of correlation, calculated in the frequency
domain. This is a useful quantity for describing a system of oscillators
coupled with delay. This is because the coherency captures not only the
magnitude of the time-shift-independent correlation between the time-series
(termed 'coherence'), but can also be used in order to estimate the size of the
time-delay (the phase-delay between the time-series in a particular frequency
band).

This library contains the following functions:

XXX

"""

import numpy as np
from nitime.lazy import scipy_fftpack as fftpack
from nitime.lazy import matplotlib_mlab as mlab

from .spectral import get_spectra, get_spectra_bi
import nitime.utils as utils

# To suppport older versions of numpy that don't have tril_indices:
from nitime.index_utils import tril_indices


def coherency(time_series, csd_method=None):
    r"""
    Compute the coherency between the spectra of n-tuple of time series.
    Input to this function is in the time domain

    Parameters
    ----------

    time_series : n*t float array
       an array of n different time series of length t each

    csd_method : dict, optional.
       See :func:`get_spectra` documentation for details

    Returns
    -------

    f : float array
        The central frequencies for the frequency bands for which the spectra
        are estimated

    c : float array
        This is a symmetric matrix with the coherencys of the signals. The
        coherency of signal i and signal j is in f[i][j]. Note that f[i][j] =
        f[j][i].conj()

    Notes
    -----

    This is an implementation of equation (1) of Sun (2005):

    .. math::

        R_{xy} (\lambda) = \frac{f_{xy}(\lambda)}
        {\sqrt{f_{xx} (\lambda) \cdot f_{yy}(\lambda)}}

    F.T. Sun and L.M. Miller and M. D'Esposito (2005). Measuring temporal
    dynamics of functional networks using phase spectrum of fMRI
    data. Neuroimage, 28: 227-37.

    """
    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    #A container for the coherencys, with the size and shape of the expected
    #output:
    c = np.zeros((time_series.shape[0],
                  time_series.shape[0],
                  f.shape[0]), dtype=complex)  # Make sure it's complex

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            c[i][j] = coherency_spec(fxy[i][j], fxy[i][i], fxy[j][j])

    idx = tril_indices(time_series.shape[0], -1)
    c[idx[0], idx[1], ...] = c[idx[1], idx[0], ...].conj()  # Make it symmetric

    return f, c


def coherency_spec(fxy, fxx, fyy):
    r"""
    Compute the coherency between the spectra of two time series.

    Input to this function is in the frequency domain.

    Parameters
    ----------

    fxy : float array
         The cross-spectrum of the time series

    fyy,fxx : float array
         The spectra of the signals

    Returns
    -------

    complex array
        the frequency-band-dependent coherency

    See also
    --------
    :func:`coherency`
    """

    return fxy / np.sqrt(fxx * fyy)


def coherence(time_series, csd_method=None):
    r"""Compute the coherence between the spectra of an n-tuple of time_series.

    Parameters of this function are in the time domain.

    Parameters
    ----------
    time_series : float array
       an array of different time series with time as the last dimension

    csd_method : dict, optional
       See :func:`algorithms.spectral.get_spectra` documentation for details

    Returns
    -------
    f : float array
        The central frequencies for the frequency bands for which the spectra
        are estimated

    c : float array
        This is a symmetric matrix with the coherencys of the signals. The
        coherency of signal i and signal j is in f[i][j].

    Notes
    -----

    This is an implementation of equation (2) of Sun (2005):

    .. math::

        Coh_{xy}(\lambda) = |{R_{xy}(\lambda)}|^2 =
        \frac{|{f_{xy}(\lambda)}|^2}{f_{xx}(\lambda) \cdot f_{yy}(\lambda)}

    F.T. Sun and L.M. Miller and M. D'Esposito (2005). Measuring temporal
    dynamics of functional networks using phase spectrum of fMRI data.
    Neuroimage, 28: 227-37.

    """
    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    # A container for the coherences, with the size and shape of the expected
    # output:
    c = np.zeros((time_series.shape[0],
                  time_series.shape[0],
                  f.shape[0]))

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            c[i][j] = coherence_spec(fxy[i][j], fxy[i][i], fxy[j][j])

    idx = tril_indices(time_series.shape[0], -1)
    c[idx[0], idx[1], ...] = c[idx[1], idx[0], ...].conj()  # Make it symmetric

    return f, c


def coherence_spec(fxy, fxx, fyy):
    r"""
    Compute the coherence between the spectra of two time series.

    Parameters of this function are in the frequency domain.

    Parameters
    ----------

    fxy : array
         The cross-spectrum of the time series

    fyy, fxx : array
         The spectra of the signals

    Returns
    -------

    float : a frequency-band-dependent measure of the linear association
        between the two time series

    See also
    --------
    :func:`coherence`
    """
    if not np.isrealobj(fxx):
        fxx = np.real(fxx)
    if not np.isrealobj(fyy):
        fyy = np.real(fyy)
    c = np.abs(fxy) ** 2 / (fxx * fyy)
    return c


def coherency_regularized(time_series, epsilon, alpha, csd_method=None):
    r"""
    Compute a regularized measure of the coherence.

    Regularization may be needed in order to overcome numerical imprecisions

    Parameters
    ----------

    time_series: float array
        The time series data for which the regularized coherence is
        calculated. Time as the last dimension.

    epsilon: float
        Small regularization parameter. Should be much smaller than any
        meaningful value of coherence you might encounter

    alpha: float
        Large regularization parameter. Should be much larger than any
        meaningful value of coherence you might encounter (preferably much
        larger than 1).

    csd_method: dict, optional.
        See :func:`get_spectra` documentation for details

    Returns
    -------
    f: float array
        The central frequencies for the frequency bands for which the spectra
        are estimated

    c: float array
        This is a symmetric matrix with the coherencys of the signals. The
        coherency of signal i and signal j is in f[i][j]. Note that f[i][j] =
        f[j][i].conj()


    Notes
    -----
    The regularization scheme is as follows:

    .. math::

        Coh_{xy}^R = \frac{(\alpha f_{xx} + \epsilon) ^2}
                          {\alpha^{2}(f_{xx}+\epsilon)(f_{yy}+\epsilon)}


    """
    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    # A container for the coherences, with the size and shape of the expected
    # output:
    c = np.zeros((time_series.shape[0],
                  time_series.shape[0],
                  f.shape[0]), dtype=complex)  # Make sure it's complex

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            c[i][j] = _coherency_reqularized(fxy[i][j], fxy[i][i],
                                             fxy[j][j], epsilon, alpha)

    idx = tril_indices(time_series.shape[0], -1)
    c[idx[0], idx[1], ...] = c[idx[1], idx[0], ...].conj()  # Make it symmetric

    return f, c


def _coherency_reqularized(fxy, fxx, fyy, epsilon, alpha):

    r"""
    A regularized version of the calculation of coherency, which is more
    robust to numerical noise than the standard calculation

    Input to this function is in the frequency domain.

    Parameters
    ----------

    fxy, fxx, fyy: float arrays
        The cross- and power-spectral densities of the two signals x and y

    epsilon: float
        First regularization parameter. Should be much smaller than any
        meaningful value of coherence you might encounter

    alpha: float
        Second regularization parameter. Should be much larger than any
        meaningful value of coherence you might encounter (preferably much
        larger than 1).

    Returns
    -------
    float array
        The coherence values

    """

    return (((alpha * fxy + epsilon)) /
         np.sqrt(((alpha ** 2) * (fxx + epsilon) * (fyy + epsilon))))


def coherence_regularized(time_series, epsilon, alpha, csd_method=None):
    r"""
    Same as coherence, except regularized in order to overcome numerical
    imprecisions

    Parameters
    ----------

    time_series: n-d float array
       The time series data for which the regularized coherence is calculated

    epsilon: float
       Small regularization parameter. Should be much smaller than any
       meaningful value of coherence you might encounter

    alpha: float
       large regularization parameter. Should be much larger than any
       meaningful value of coherence you might encounter (preferably much
       larger than 1).

    csd_method: dict, optional.
       See :func:`get_spectra` documentation for details

    Returns
    -------
    f: float array
       The central frequencies for the frequency bands for which the spectra
       are estimated

    c: n-d array
       This is a symmetric matrix with the coherencys of the signals. The
       coherency of signal i and signal j is in f[i][j].

    Returns
    -------
    frequencies, coherence

    Notes
    -----
    The regularization scheme is as follows:

    .. math::

        C_{x,y} = \frac{(\alpha f_{xx} + \epsilon)^2}
        {\alpha^{2}((f_{xx}+\epsilon)(f_{yy}+\epsilon))}

    """
    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    #A container for the coherences, with the size and shape of the expected
    #output:
    c = np.zeros((time_series.shape[0],
                  time_series.shape[0],
                  f.shape[0]), complex)

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            c[i][j] = _coherence_reqularized(fxy[i][j], fxy[i][i],
                                             fxy[j][j], epsilon, alpha)

    idx = tril_indices(time_series.shape[0], -1)
    c[idx[0], idx[1], ...] = c[idx[1], idx[0], ...].conj()  # Make it symmetric

    return f, c


def _coherence_reqularized(fxy, fxx, fyy, epsilon, alpha):

    r"""A regularized version of the calculation of coherence, which is more
    robust to numerical noise than the standard calculation.

    Input to this function is in the frequency domain

    Parameters
    ----------

    fxy, fxx, fyy: float arrays
        The cross- and power-spectral densities of the two signals x and y

    epsilon: float
        First regularization parameter. Should be much smaller than any
        meaningful value of coherence you might encounter

    alpha: float
        Second regularization parameter. Should be much larger than any
        meaningful value of coherence you might encounter (preferably much
        larger than 1)

    Returns
    -------
    float array
       The coherence values

    """
    return (((alpha * np.abs(fxy) + epsilon) ** 2) /
         ((alpha ** 2) * (fxx + epsilon) * (fyy + epsilon)))


def coherency_bavg(time_series, lb=0, ub=None, csd_method=None):
    r"""
    Compute the band-averaged coherency between the spectra of two time series.

    Input to this function is in the time domain.

    Parameters
    ----------
    time_series: n*t float array
       an array of n different time series of length t each

    lb, ub: float, optional
       the upper and lower bound on the frequency band to be used in averaging
       defaults to 1,max(f)

    csd_method: dict, optional.
       See :func:`get_spectra` documentation for details

    Returns
    -------
    c: float array
        This is an upper-diagonal array, where c[i][j] is the band-averaged
        coherency between time_series[i] and time_series[j]

    Notes
    -----

    This is an implementation of equation (A4) of Sun(2005):

    .. math::

        \bar{Coh_{xy}} (\bar{\lambda}) =
        \frac{\left|{\sum_\lambda{\hat{f_{xy}}}}\right|^2}
        {\sum_\lambda{\hat{f_{xx}}}\cdot sum_\lambda{\hat{f_{yy}}}}

    F.T. Sun and L.M. Miller and M. D'Esposito (2005). Measuring
    temporal dynamics of functional networks using phase spectrum of fMRI
    data. Neuroimage, 28: 227-37.
    """
    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    lb_idx, ub_idx = utils.get_bounds(f, lb, ub)

    if lb == 0:
        lb_idx = 1  # The lowest frequency band should be f0

    c = np.zeros((time_series.shape[0],
                  time_series.shape[0]), dtype=complex)

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            c[i][j] = _coherency_bavg(fxy[i][j][lb_idx:ub_idx],
                                      fxy[i][i][lb_idx:ub_idx],
                                      fxy[j][j][lb_idx:ub_idx])

    idx = tril_indices(time_series.shape[0], -1)
    c[idx[0], idx[1], ...] = c[idx[1], idx[0], ...].conj()  # Make it symmetric

    return c


def _coherency_bavg(fxy, fxx, fyy):
    r"""
    Compute the band-averaged coherency between the spectra of two time series.

    Input to this function is in the frequency domain.

    Parameters
    ----------

    fxy : float array
         The cross-spectrum of the time series

    fyy,fxx : float array
         The spectra of the signals

    Returns
    -------

    float
        the band-averaged coherency

    Notes
    -----

    This is an implementation of equation (A4) of [Sun2005]_:

    .. math::

        \bar{Coh_{xy}} (\bar{\lambda}) =
        \frac{\left|{\sum_\lambda{\hat{f_{xy}}}}\right|^2}
        {\sum_\lambda{\hat{f_{xx}}}\cdot sum_\lambda{\hat{f_{yy}}}}

    .. [Sun2005] F.T. Sun and L.M. Miller and M. D'Esposito(2005). Measuring
        temporal dynamics of functional networks using phase spectrum of fMRI
        data. Neuroimage, 28: 227-37.
    """

    # Average the phases and the magnitudes separately and then recombine:

    p = np.angle(fxy)
    p_bavg = np.mean(p)

    m = np.abs(coherency_spec(fxy, fxx, fyy))
    m_bavg = np.mean(m)

    # Recombine according to z = r(cos(phi)+sin(phi)i):
    return  m_bavg * (np.cos(p_bavg) + np.sin(p_bavg) * 1j)


def coherence_bavg(time_series, lb=0, ub=None, csd_method=None):
    r"""
    Compute the band-averaged coherence between the spectra of two time series.

    Input to this function is in the time domain.

    Parameters
    ----------
    time_series : float array
       An array of time series, time as the last dimension.

    lb, ub: float, optional
       The upper and lower bound on the frequency band to be used in averaging
       defaults to 1,max(f)

    csd_method: dict, optional.
       See :func:`get_spectra` documentation for details

    Returns
    -------
    c : float
       This is an upper-diagonal array, where c[i][j] is the band-averaged
       coherency between time_series[i] and time_series[j]
    """

    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    lb_idx, ub_idx = utils.get_bounds(f, lb, ub)

    if lb == 0:
        lb_idx = 1  # The lowest frequency band should be f0

    c = np.zeros((time_series.shape[0],
                  time_series.shape[0]))

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            c[i][j] = _coherence_bavg(fxy[i][j][lb_idx:ub_idx],
                                      fxy[i][i][lb_idx:ub_idx],
                                      fxy[j][j][lb_idx:ub_idx])

    idx = tril_indices(time_series.shape[0], -1)
    c[idx[0], idx[1], ...] = c[idx[1], idx[0], ...].conj()  # Make it symmetric

    return c


def _coherence_bavg(fxy, fxx, fyy):
    r"""
    Compute the band-averaged coherency between the spectra of two time series.
    input to this function is in the frequency domain

    Parameters
    ----------

    fxy : float array
         The cross-spectrum of the time series

    fyy,fxx : float array
         The spectra of the signals

    Returns
    -------

    float :
        the band-averaged coherence
    """
    if not np.isrealobj(fxx):
        fxx = np.real(fxx)
    if not np.isrealobj(fyy):
        fyy = np.real(fyy)

    return (np.abs(fxy.sum()) ** 2) / (fxx.sum() * fyy.sum())


def coherence_partial(time_series, r, csd_method=None):
    r"""
    Compute the band-specific partial coherence between the spectra of
    two time series.

    The partial coherence is the part of the coherence between x and
    y, which cannot be attributed to a common cause, r.

    Input to this function is in the time domain.

    Parameters
    ----------

    time_series: float array
       An array of time-series, with time as the last dimension.

    r: float array
        This array represents the temporal sequence of the common cause to be
        partialed out, sampled at the same rate as time_series

    csd_method: dict, optional
       See :func:`get_spectra` documentation for details


    Returns
    -------
    f: array,
        The mid-frequencies of the frequency bands in the spectral
        decomposition

    c: float array
       The frequency dependent partial coherence between time_series i and
       time_series j in c[i][j] and in c[j][i], with r partialed out


    Notes
    -----

    This is an implementation of equation (2) of Sun (2004):

    .. math::

        Coh_{xy|r} = \frac{|{R_{xy}(\lambda) - R_{xr}(\lambda)
        R_{ry}(\lambda)}|^2}{(1-|{R_{xr}}|^2)(1-|{R_{ry}}|^2)}

    F.T. Sun and L.M. Miller and M. D'Esposito (2004). Measuring interregional
    functional connectivity using coherence and partial coherence analyses of
    fMRI data Neuroimage, 21: 647-58.
    """

    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    # Initialize c according to the size of f:
    c = np.zeros((time_series.shape[0],
                  time_series.shape[0],
                  f.shape[0]), dtype=complex)

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            f, fxx, frr, frx = get_spectra_bi(time_series[i], r, csd_method)
            f, fyy, frr, fry = get_spectra_bi(time_series[j], r, csd_method)
            c[i, j] = coherence_partial_spec(fxy[i][j], fxy[i][i],
                                                  fxy[j][j], frx, fry, frr)

    idx = tril_indices(time_series.shape[0], -1)
    c[idx[0], idx[1], ...] = c[idx[1], idx[0], ...].conj()  # Make it symmetric

    return f, c


def coherence_partial_spec(fxy, fxx, fyy, fxr, fry, frr):
    r"""
    Compute the band-specific partial coherence between the spectra of
    two time series. See :func:`partial_coherence`.

    Input to this function is in the frequency domain.

    Parameters
    ----------
    fxy : float array
         The cross-spectrum of the time series

    fyy, fxx : float array
         The spectra of the signals

    fxr, fry : float array
         The cross-spectra of the signals with the event

    Returns
    -------
    float
        the band-averaged coherency
    """
    coh = coherency_spec
    Rxr = coh(fxr, fxx, frr)
    Rry = coh(fry, fyy, frr)
    Rxy = coh(fxy, fxx, fyy)

    return (((np.abs(Rxy - Rxr * Rry)) ** 2) /
           ((1 - ((np.abs(Rxr)) ** 2)) * (1 - ((np.abs(Rry)) ** 2))))


def coherency_phase_spectrum(time_series, csd_method=None):
    r"""
    Compute the phase spectrum of the cross-spectrum between two time series.

    The parameters of this function are in the time domain.

    Parameters
    ----------

    time_series: n*t float array
    The time series, with t, time, as the last dimension

    Returns
    -------

    f: mid frequencies of the bands

    p: an array with the pairwise phase spectrum between the time
    series, where p[i][j] is the phase spectrum between time series[i] and
    time_series[j]

    Notes
    -----

    This is an implementation of equation (3) of Sun et al. (2005) [Sun2005]_:

    .. math::

        \phi(\lambda) = arg [R_{xy} (\lambda)] = arg [f_{xy} (\lambda)]

    F.T. Sun and L.M. Miller and M. D'Esposito (2005). Measuring temporal
    dynamics of functional networks using phase spectrum of fMRI data.
    Neuroimage, 28: 227-37.
    """
    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    p = np.zeros((time_series.shape[0],
                  time_series.shape[0],
                  f.shape[0]))

    for i in range(time_series.shape[0]):
        for j in range(i + 1, time_series.shape[0]):
            p[i][j] = np.angle(fxy[i][j])
            p[j][i] = np.angle(fxy[i][j].conjugate())

    return f, p


def coherency_phase_delay(time_series, lb=0, ub=None, csd_method=None):
    """
    The temporal delay calculated from the coherency phase spectrum.

    Parameters
    ----------

    time_series: float array
       The time-series data for which the delay is calculated.

    lb, ub: float
       Frequency boundaries (in Hz), for the domain over which the delays are
       calculated. Defaults to 0-max(f)

    csd_method : dict, optional.
       See :func:`get_spectra`

    Returns
    -------
    f : float array
       The mid-frequencies for the frequency bands over which the calculation
       is done.
    p : float array
       Pairwise temporal delays between time-series (in seconds).

    """
    if csd_method is None:
        csd_method = {'this_method': 'welch'}  # The default

    f, fxy = get_spectra(time_series, csd_method)

    lb_idx, ub_idx = utils.get_bounds(f, lb, ub)

    if lb_idx == 0:
        lb_idx = 1

    p = np.zeros((time_series.shape[0], time_series.shape[0],
                  f[lb_idx:ub_idx].shape[-1]))

    for i in range(time_series.shape[0]):
        for j in range(i, time_series.shape[0]):
            p[i][j] = _coherency_phase_delay(f[lb_idx:ub_idx],
                                             fxy[i][j][lb_idx:ub_idx])
            p[j][i] = _coherency_phase_delay(f[lb_idx:ub_idx],
                                    fxy[i][j][lb_idx:ub_idx].conjugate())

    return f[lb_idx:ub_idx], p


def _coherency_phase_delay(f, fxy):
    r"""
    Compute the phase delay between the spectra of two signals. The input to
    this function is in the frequency domain.

    Parameters
    ----------

    f: float array
         The frequencies

    fxy : float array
         The cross-spectrum of the time series

    Returns
    -------

    float array
        the phase delay (in sec) for each frequency band.

    """

    return np.angle(fxy) / (2 * np.pi * f)


def correlation_spectrum(x1, x2, Fs=2 * np.pi, norm=False):
    """
    Calculate the spectral decomposition of the correlation.

    Parameters
    ----------
    x1,x2: ndarray
       Two arrays to be correlated. Same dimensions

    Fs: float, optional
       Sampling rate in Hz. If provided, an array of
       frequencies will be returned.Defaults to 2

    norm: bool, optional
       When this is true, the spectrum is normalized to sum to 1

    Returns
    -------
    f: ndarray
       ndarray with the frequencies

    ccn: ndarray
       The spectral decomposition of the correlation

    Notes
    -----

    This method is described in full in: D Cordes, V M Haughton, K Arfanakis, G
    J Wendt, P A Turski, C H Moritz, M A Quigley, M E Meyerand (2000). Mapping
    functionally related regions of brain with functional connectivity MR
    imaging. AJNR American journal of neuroradiology 21:1636-44

    """

    x1 = x1 - np.mean(x1)
    x2 = x2 - np.mean(x2)
    x1_f = fftpack.fft(x1)
    x2_f = fftpack.fft(x2)
    D = np.sqrt(np.sum(x1 ** 2) * np.sum(x2 ** 2))
    n = x1.shape[0]

    ccn = ((np.real(x1_f) * np.real(x2_f) +
           np.imag(x1_f) * np.imag(x2_f)) /
           (D * n))

    if norm:
        ccn = ccn / np.sum(ccn) * 2  # Only half of the sum is sent back
                                     # because of the freq domain symmetry.
                                     # XXX Does normalization make this
                                     # strictly positive?

    f = utils.get_freqs(Fs, n)
    return f, ccn[0:(n / 2 + 1)]


#------------------------------------------------------------------------
#Coherency calculated using cached spectra
#------------------------------------------------------------------------
"""The idea behind this set of functions is to keep a cache of the windowed fft
calculations of each time-series in a massive collection of time-series, so
that this calculation doesn't have to be repeated each time a cross-spectrum is
calculated. The first function creates the cache and then, another function
takes the cached spectra and calculates PSDs and CSDs, which are then passed to
coherency_spec and organized in a data structure similar to the one
created by coherence"""


def cache_fft(time_series, ij, lb=0, ub=None,
                  method=None, prefer_speed_over_memory=False,
                  scale_by_freq=True):
    """compute and cache the windowed FFTs of the time_series, in such a way
    that computing the psd and csd of any combination of them can be done
    quickly.

    Parameters
    ----------

    time_series : float array
       An ndarray with time-series, where time is the last dimension

    ij: list of tuples
      Each tuple in this variable should contain a pair of
      indices of the form (i,j). The resulting cache will contain the fft of
      time-series in the rows indexed by the unique elements of the union of i
      and j

    lb,ub: float
       Define a frequency band of interest, for which the fft will be cached

    method: dict, optional
        See :func:`get_spectra` for details on how this is used. For this set
        of functions, 'this_method' has to be 'welch'


    Returns
    -------
    freqs, cache

        where: cache =
             {'FFT_slices':FFT_slices,'FFT_conj_slices':FFT_conj_slices,
             'norm_val':norm_val}

    Notes
    -----

    - For these functions, only the Welch windowed periodogram ('welch') is
      available.

    - Detrending the input is not an option here, in order to save
      time on an empty function call.

    """
    if method is None:
        method = {'this_method': 'welch'}  # The default

    this_method = method.get('this_method', 'welch')

    if this_method == 'welch':
        NFFT = method.get('NFFT', 64)
        Fs = method.get('Fs', 2 * np.pi)
        window = method.get('window', mlab.window_hanning)
        n_overlap = method.get('n_overlap', int(np.ceil(NFFT / 2.0)))
    else:
        e_s = "For cache_fft, spectral estimation method must be welch"
        raise ValueError(e_s)
    time_series = utils.zero_pad(time_series, NFFT)

    #The shape of the zero-padded version:
    n_channels, n_time_points = time_series.shape

    # get all the unique channels in time_series that we are interested in by
    # checking the ij tuples
    all_channels = set()
    for i, j in ij:
        all_channels.add(i)
        all_channels.add(j)

    # for real time_series, ignore the negative frequencies
    if np.iscomplexobj(time_series):
        n_freqs = NFFT
    else:
        n_freqs = NFFT // 2 + 1

    #Which frequencies
    freqs = utils.get_freqs(Fs, NFFT)

    #If there are bounds, limit the calculation to within that band,
    #potentially include the DC component:
    lb_idx, ub_idx = utils.get_bounds(freqs, lb, ub)

    n_freqs = ub_idx - lb_idx
    #Make the window:
    if mlab.cbook.iterable(window):
        assert(len(window) == NFFT)
        window_vals = window
    else:
        window_vals = window(np.ones(NFFT, time_series.dtype))

    #Each fft needs to be normalized by the square of the norm of the window
    #and, for consistency with newer versions of mlab.csd (which, in turn, are
    #consistent with Matlab), normalize also by the sampling rate:

    if scale_by_freq:
        #This is the normalization factor for one-sided estimation, taking into
        #account the sampling rate. This makes the PSD a density function, with
        #units of dB/Hz, so that integrating over frequencies gives you the RMS
        #(XXX this should be in the tests!).
        norm_val = (np.abs(window_vals) ** 2).sum() * (Fs / 2)

    else:
        norm_val = (np.abs(window_vals) ** 2).sum() / 2

    # cache the FFT of every windowed, detrended NFFT length segement
    # of every channel.  If prefer_speed_over_memory, cache the conjugate
    # as well

    i_times = list(range(0, n_time_points - NFFT + 1, NFFT - n_overlap))
    n_slices = len(i_times)
    FFT_slices = {}
    FFT_conj_slices = {}

    for i_channel in all_channels:
        #dbg:
        #print i_channel
        Slices = np.zeros((n_slices, n_freqs), dtype=np.complex)
        for iSlice in range(n_slices):
            thisSlice = time_series[i_channel,
                                    i_times[iSlice]:i_times[iSlice] + NFFT]

            #Windowing:
            thisSlice = window_vals * thisSlice  # No detrending
            #Derive the fft for that slice:
            Slices[iSlice, :] = (fftpack.fft(thisSlice)[lb_idx:ub_idx])

        FFT_slices[i_channel] = Slices

        if prefer_speed_over_memory:
            FFT_conj_slices[i_channel] = np.conjugate(Slices)

    cache = {'FFT_slices': FFT_slices, 'FFT_conj_slices': FFT_conj_slices,
             'norm_val': norm_val, 'Fs': Fs, 'scale_by_freq': scale_by_freq}

    return freqs, cache


def cache_to_psd(cache, ij):
    """
    From a set of cached windowed fft, calculate the psd

    Parameters
    ----------
    cache : dict
        Return value from :func:`cache_fft`

    ij : list
        A list of tuples of the form (i,j).

    Returns
    -------
    Pxx : dict
        The phases for the intersection of (time_series[i],time_series[j]). The
        keys are the intersection of i,j values in the parameter ij

    """
    # This is the way it is saved by cache_spectra:
    FFT_slices = cache['FFT_slices']
    FFT_conj_slices = cache['FFT_conj_slices']
    norm_val = cache['norm_val']
    # Fs = cache['Fs']

    # This is where the output goes to:
    Pxx = {}
    all_channels = set()
    for i, j in ij:
        all_channels.add(i)
        all_channels.add(j)

    for i in all_channels:
        #dbg:
        #print i
        #If we made the conjugate slices:
        if FFT_conj_slices:
            Pxx[i] = FFT_slices[i] * FFT_conj_slices[i]
        else:
            Pxx[i] = FFT_slices[i] * np.conjugate(FFT_slices[i])

        #If there is more than one window
        if FFT_slices[i].shape[0] > 1:
            Pxx[i] = np.mean(Pxx[i], 0)

        Pxx[i] /= norm_val
        # Correct for the NFFT/2 and DC components:
        Pxx[i][[0, -1]] /= 2

    return Pxx


def cache_to_phase(cache, ij):
    """ From a set of cached set of windowed fft's, calculate the
    frequency-band dependent phase for each of the channels in ij.
    Note that this returns the absolute phases of the time-series, not the
    relative phases between them. In order to get relative phases, use
    cache_to_relative_phase

    Parameters
    ----------
    cache : dict
         The return value of  :func:`cache_fft`

    ij: list
       A list of tuples of the form (i,j) for all the indices for which to
       calculate the phases

    Returns
    -------

    Phase : dict
         The individual phases, keys are all the i and j in ij, such that
         Phase[i] gives you the phase for the time-series i in the input to
         :func:`cache_fft`

    """
    FFT_slices = cache['FFT_slices']

    Phase = {}

    all_channels = set()
    for i, j in ij:
        all_channels.add(i)
        all_channels.add(j)

    for i in all_channels:
        Phase[i] = np.angle(FFT_slices[i])
        #If there is more than one window, average over all the windows:
        if FFT_slices[i].shape[0] > 1:
            Phase[i] = np.mean(Phase[i], 0)

    return Phase


def cache_to_relative_phase(cache, ij):
    """ From a set of cached set of windowed fft's, calculate the
    frequency-band dependent relative phase for the combinations ij.

    Parameters
    ----------
    cache: dict
        The return value from :func:`cache_fft`

    ij: list
       A list of tuples of the form (i,j), all the pairs of indices for which
       to calculate the relative phases

    Returns
    -------

    Phi_xy : dict
        The relative phases between the time-series i and j. Such that
        Phi_xy[i,j] is the phase from time_series[i] to time_series[j].

    Note
    ----

    This function will give you a different result than using
    :func:`coherency_phase_spectrum`. This is because
    :func:`coherency_phase_spectrum` calculates the angle based on the average
    psd, whereas this function calculates the average of the angles calculated
    on individual windows.

    """
    # This is the way it is saved by cache_spectra:
    FFT_slices = cache['FFT_slices']
    FFT_conj_slices = cache['FFT_conj_slices']
    # norm_val = cache['norm_val']

    freqs = cache['FFT_slices'][ij[0][0]].shape[-1]

    ij_array = np.array(ij)

    channels_i = max(1, max(ij_array[:, 0]) + 1)
    channels_j = max(1, max(ij_array[:, 1]) + 1)
    #Pre-allocate for speed:
    Phi_xy = np.zeros((channels_i, channels_j, freqs), dtype=np.complex)

    #These checks take time, so do them up front, not in every iteration:
    if list(FFT_slices.items())[0][1].shape[0] > 1:
        if FFT_conj_slices:
            for i, j in ij:
                phi = np.angle(FFT_slices[i] * FFT_conj_slices[j])
                Phi_xy[i, j] = np.mean(phi, 0)

        else:
            for i, j in ij:
                phi = np.angle(FFT_slices[i] * np.conjugate(FFT_slices[j]))
                Phi_xy[i, j] = np.mean(phi, 0)

    else:
        if FFT_conj_slices:
            for i, j in ij:
                Phi_xy[i, j] = np.angle(FFT_slices[i] * FFT_conj_slices[j])

        else:
            for i, j in ij:
                Phi_xy[i, j] = np.angle(FFT_slices[i] *
                                        np.conjugate(FFT_slices[j]))

    return Phi_xy


def cache_to_coherency(cache, ij):
    """From a set of cached spectra, calculate the coherency
    relationships

    Parameters
    ----------
    cache: dict
        the return value from :func:`cache_fft`

    ij: list
      a list of (i,j) tuples, the pairs of indices for which the
      cross-coherency is to be calculated

    Returns
    -------
    Cxy: dict
       coherence values between the time-series ij. Indexing into this dict
       takes the form Cxy[i,j] in order to extract the coherency between
       time-series i and time-series j in the original input to
       :func:`cache_fft`
    """

    #This is the way it is saved by cache_spectra:
    FFT_slices = cache['FFT_slices']
    FFT_conj_slices = cache['FFT_conj_slices']
    norm_val = cache['norm_val']

    freqs = cache['FFT_slices'][ij[0][0]].shape[-1]

    ij_array = np.array(ij)

    channels_i = max(1, max(ij_array[:, 0]) + 1)
    channels_j = max(1, max(ij_array[:, 1]) + 1)
    Cxy = np.zeros((channels_i, channels_j, freqs), dtype=np.complex)

    #These checks take time, so do them up front, not in every iteration:
    if list(FFT_slices.items())[0][1].shape[0] > 1:
        if FFT_conj_slices:
            for i, j in ij:
                #dbg:
                #print i,j
                Pxy = FFT_slices[i] * FFT_conj_slices[j]
                Pxx = FFT_slices[i] * FFT_conj_slices[i]
                Pyy = FFT_slices[j] * FFT_conj_slices[j]
                Pxx = np.mean(Pxx, 0)
                Pyy = np.mean(Pyy, 0)
                Pxy = np.mean(Pxy, 0)
                Pxy /= norm_val
                Pxx /= norm_val
                Pyy /= norm_val
                Cxy[i, j] = Pxy / np.sqrt(Pxx * Pyy)

        else:
            for i, j in ij:
                Pxy = FFT_slices[i] * np.conjugate(FFT_slices[j])
                Pxx = FFT_slices[i] * np.conjugate(FFT_slices[i])
                Pyy = FFT_slices[j] * np.conjugate(FFT_slices[j])
                Pxx = np.mean(Pxx, 0)
                Pyy = np.mean(Pyy, 0)
                Pxy = np.mean(Pxy, 0)
                Pxy /= norm_val
                Pxx /= norm_val
                Pyy /= norm_val
                Cxy[i, j] = Pxy / np.sqrt(Pxx * Pyy)
    else:
        if FFT_conj_slices:
            for i, j in ij:
                Pxy = FFT_slices[i] * FFT_conj_slices[j]
                Pxx = FFT_slices[i] * FFT_conj_slices[i]
                Pyy = FFT_slices[j] * FFT_conj_slices[j]
                Pxy /= norm_val
                Pxx /= norm_val
                Pyy /= norm_val
                Cxy[i, j] = Pxy / np.sqrt(Pxx * Pyy)

        else:
            for i, j in ij:
                Pxy = FFT_slices[i] * np.conjugate(FFT_slices[j])
                Pxx = FFT_slices[i] * np.conjugate(FFT_slices[i])
                Pyy = FFT_slices[j] * np.conjugate(FFT_slices[j])
                Pxy /= norm_val
                Pxx /= norm_val
                Pyy /= norm_val
                Cxy[i, j] = Pxy / np.sqrt(Pxx * Pyy)

    return Cxy

########NEW FILE########
__FILENAME__ = correlation
import numpy as np

__all__ = ["seed_corrcoef"]


def seed_corrcoef(seed, target):
    """Compute seed-based correlation coefficient"""

    x = target - np.mean(target, -1)[..., np.newaxis]
    y = seed - np.mean(seed)
    xx = np.sum(x ** 2, -1)
    yy = np.sum(y ** 2, -1)
    xy = np.dot(x, y)
    r = xy / np.sqrt(xx * yy)

    return r

########NEW FILE########
__FILENAME__ = event_related
"""

Event-related analysis

"""

import numpy as np
from nitime.lazy import scipy_linalg as linalg
from nitime.lazy import scipy_fftpack as fftpack


def fir(timeseries, design):
    """
    Calculate the FIR (finite impulse response) HRF, according to [Burock2000]_

    Parameters
    ----------

    timeseries : float array
            timeseries data

    design : int array
          This is a design matrix.  It has to have shape = (number
          of TRS, number of conditions * length of HRF)

          The form of the matrix is:

              A B C ...

          where A is a (number of TRs) x (length of HRF) matrix with a unity
          matrix placed with its top left corner placed in each TR in which
          event of type A occured in the design. B is the equivalent for
          events of type B, etc.

    Returns
    -------

    HRF: float array
        HRF is a numpy array of 1X(length of HRF * number of conditions)
        with the HRFs for the different conditions concatenated. This is an
        estimate of the linear filters between the time-series and the events
        described in design.

    Notes
    -----

    Implements equation 4 in Burock(2000):

    .. math::

        \hat{h} = (X^T X)^{-1} X^T y

    M.A. Burock and A.M.Dale (2000). Estimation and Detection of Event-Related
    fMRI Signals with Temporally Correlated Noise: A Statistically Efficient
    and Unbiased Approach. Human Brain Mapping, 11:249-260

    """
    X = np.matrix(design)
    y = np.matrix(timeseries)
    h = np.array(linalg.pinv(X.T * X) * X.T * y.T)
    return h


def freq_domain_xcorr(tseries, events, t_before, t_after, Fs=1):
    """
    Calculates the  event related timeseries, using a cross-correlation in the
    frequency domain.

    Parameters
    ----------
    tseries: float array
       Time series data with time as the last dimension

    events: float array
       An array with time-resolved events, at the same sampling rate as tseries

    t_before: float
       Time before the event to include

    t_after: float
       Time after the event to include

    Fs: float
       Sampling rate of the time-series (in Hz)

    Returns
    -------
    xcorr: float array
        The correlation function between the tseries and the events. Can be
        interperted as a linear filter from events to responses (the
        time-series) of an LTI.

    """
    fft = fftpack.fft
    ifft = fftpack.ifft
    fftshift = fftpack.fftshift

    xcorr = np.real(fftshift(ifft(fft(tseries) *
                                  fft(np.fliplr([events])))))

    return xcorr[0][np.ceil(len(xcorr[0]) / 2) - t_before * Fs:
                np.ceil(len(xcorr[0]) / 2) + t_after / 2 * Fs] / np.sum(events)


def freq_domain_xcorr_zscored(tseries, events, t_before, t_after, Fs=1):
    """
    Calculates the z-scored event related timeseries, using a cross-correlation
    in the frequency domain.

    Parameters
    ----------
    tseries: float array
       Time series data with time as the last dimension

    events: float array
       An array with time-resolved events, at the same sampling rate as tseries

    t_before: float
       Time before the event to include

    t_after: float
       Time after the event to include

    Fs: float
       Sampling rate of the time-series (in Hz)

    Returns
    -------
    xcorr: float array
        The correlation function between the tseries and the events. Can be
        interperted as a linear filter from events to responses (the
        time-series) of an LTI. Because it is normalized to its own mean and
        variance, it can be interperted as measuring statistical significance
        relative to all time-shifted versions of the events.

    """

    fft = fftpack.fft
    ifft = fftpack.ifft
    fftshift = fftpack.fftshift

    xcorr = np.real(fftshift(ifft(fft(tseries) * fft(np.fliplr([events])))))

    meanSurr = np.mean(xcorr)
    stdSurr = np.std(xcorr)

    return (((xcorr[0][np.ceil(len(xcorr[0]) / 2) - t_before * Fs:
                    np.ceil(len(xcorr[0]) / 2) + t_after * Fs])
             - meanSurr)
             / stdSurr)

########NEW FILE########
__FILENAME__ = filter
import numpy as np


def boxcar_filter(time_series, lb=0, ub=0.5, n_iterations=2):
    """
    Filters data into a frequency range.

    For each of the two bounds, a low-passed version is created by convolving
    with a box-car and then the low-passed version for the upper bound is added
    to the low-passed version for the lower bound subtracted from the signal,
    resulting in a band-passed version

    Parameters
    ----------

    time_series: float array
       the signal
    ub : float, optional
      The cut-off frequency for the low-pass filtering as a proportion of the
      sampling rate. Default to 0.5 (Nyquist)
    lb : float, optional
      The cut-off frequency for the high-pass filtering as a proportion of the
      sampling rate. Default to 0
    n_iterations: int, optional
      how many rounds of smoothing to do. Default to 2.

    Returns
    -------
    float array:
      The signal, filtered
    """

    n = time_series.shape[-1]

    len_boxcar_ub = np.ceil(1 / (2.0 * ub))
    boxcar_ub = np.empty(len_boxcar_ub)
    boxcar_ub.fill(1.0 / len_boxcar_ub)
    boxcar_ones_ub = np.ones_like(boxcar_ub)

    if lb == 0:
        lb = None
    else:
        len_boxcar_lb = np.ceil(1 / (2.0 * lb))
        boxcar_lb = np.empty(len_boxcar_lb)
        boxcar_lb.fill(1.0 / len_boxcar_lb)
        boxcar_ones_lb = np.ones_like(boxcar_lb)

    #If the time_series is a 1-d, we add a dimension, so that we can iterate
    #over 2-d inputs:
    if len(time_series.shape) == 1:
        time_series = np.array([time_series])
    for i in range(time_series.shape[0]):
        if ub:
            #Start by applying a low-pass to the signal.  Pad the signal on
            #each side with the initial and terminal signal value:
            pad_s = np.hstack((boxcar_ones_ub *
                               time_series[i, 0], time_series[i]))
            pad_s = np.hstack((pad_s, boxcar_ones_ub * time_series[i, -1]))

            #Filter operation is a convolution with the box-car(iterate,
            #n_iterations times over this operation):
            for iteration in range(n_iterations):
                conv_s = np.convolve(pad_s, boxcar_ub)

            #Extract the low pass signal by excising the central
            #len(time_series) points:
            time_series[i] = conv_s[conv_s.shape[-1] / 2 - np.floor(n / 2.):
                                    conv_s.shape[-1] / 2 + np.ceil(n / 2.)]

        #Now, if there is a high-pass, do the same, but in the end subtract out
        #the low-passed signal:
        if lb:
            pad_s = np.hstack((boxcar_ones_lb *
                               time_series[i, 0], time_series[i]))
            pad_s = np.hstack((pad_s, boxcar_ones_lb * time_series[i, -1]))

            #Filter operation is a convolution with the box-car(iterate,
            #n_iterations times over this operation):
            for iteration in range(n_iterations):
                conv_s = np.convolve(pad_s, boxcar_lb)

            #Extract the low pass signal by excising the central
            #len(time_series) points:
            s_lp = conv_s[conv_s.shape[-1] / 2 - np.floor(n / 2.):
                          conv_s.shape[-1] / 2 + np.ceil(n / 2.)]

            #Extract the high pass signal simply by subtracting the high pass
            #signal from the original signal:
            time_series[i] = time_series[i] - s_lp + np.mean(s_lp)  # add mean
            #to make sure that there are no negative values. This also seems to
            #make sure that the mean of the signal (in % signal change) is
            #close to 0

    return time_series.squeeze()

########NEW FILE########
__FILENAME__ = spectral
"""

Spectral transforms are used in order to estimate the frequency-domain
representation of time-series. Several methods can be used and this module
contains implementations of several algorithms for the calculation of spectral
transforms.

The functions in this module are:

XXX

"""

import numpy as np
from nitime.lazy import matplotlib_mlab as mlab
from nitime.lazy import scipy_linalg as linalg
from nitime.lazy import scipy_signal as sig
from nitime.lazy import scipy_interpolate as interpolate
from nitime.lazy import scipy_fftpack as fftpack

import nitime.utils as utils

# To support older versions of numpy that don't have tril_indices:
from nitime.index_utils import tril_indices, triu_indices


# Set global variables for the default NFFT to be used in spectral analysis and
# the overlap:
default_nfft = 64
default_n_overlap = int(np.ceil(default_nfft / 2.0))

def get_spectra(time_series, method=None):
    r"""
    Compute the spectra of an n-tuple of time series and all of
    the pairwise cross-spectra.

    Parameters
    ----------
    time_series : float array
        The time-series, where time is the last dimension

    method : dict, optional

        contains: this_method:'welch'
           indicates that :func:`mlab.psd` will be used in
           order to calculate the psd/csd, in which case, additional optional
           inputs (and default values) are:

               NFFT=64

               Fs=2pi

               detrend=mlab.detrend_none

               window=mlab.window_hanning

               n_overlap=0

        this_method:'periodogram_csd'
           indicates that :func:`periodogram` will
           be used in order to calculate the psd/csd, in which case, additional
           optional inputs (and default values) are:

               Skx=None

               Sky=None

               N=None

               sides='onesided'

               normalize=True

               Fs=2pi

        this_method:'multi_taper_csd'
           indicates that :func:`multi_taper_psd` used in order to calculate
           psd/csd, in which case additional optional inputs (and default
           values) are:

               BW=0.01

               Fs=2pi

               sides = 'onesided'

    Returns
    -------

    f : float array
        The central frequencies for the frequency bands for which the spectra
        are estimated

    fxy : float array
        A semi-filled matrix with the cross-spectra of the signals. The csd of
        signal i and signal j is in f[j][i], but not in f[i][j] (which will be
        filled with zeros). For i=j fxy[i][j] is the psd of signal i.

    """
    if method is None:
        method = {'this_method': 'welch'}  # The default
    # If no choice of method was explicitly set, but other parameters were
    # passed, assume that the method is mlab:
    this_method = method.get('this_method', 'welch')

    if this_method == 'welch':
        NFFT = method.get('NFFT', default_nfft)
        Fs = method.get('Fs', 2 * np.pi)
        detrend = method.get('detrend', mlab.detrend_none)
        window = method.get('window', mlab.window_hanning)
        n_overlap = method.get('n_overlap', int(np.ceil(NFFT / 2.0)))

        # The length of the spectrum depends on how many sides are taken, which
        # depends on whether or not this is a complex object:
        if np.iscomplexobj(time_series):
            fxy_len = NFFT
        else:
            fxy_len = NFFT / 2.0 + 1

        # If there is only 1 channel in the time-series:
        if len(time_series.shape) == 1 or time_series.shape[0] == 1:
            temp, f = mlab.csd(time_series, time_series,
                               NFFT, Fs, detrend, window, n_overlap,
                               scale_by_freq=True)

            fxy = temp.squeeze()  # the output of mlab.csd has a weird
                                  # shape
        else:
            fxy = np.zeros((time_series.shape[0],
                            time_series.shape[0],
                            fxy_len), dtype=complex)  # Make sure it's complex

            for i in range(time_series.shape[0]):
                for j in range(i, time_series.shape[0]):
                    #Notice funny indexing, in order to conform to the
                    #conventions of the other methods:
                    temp, f = mlab.csd(time_series[j], time_series[i],
                                       NFFT, Fs, detrend, window, n_overlap,
                                       scale_by_freq=True)

                    fxy[i][j] = temp.squeeze()  # the output of mlab.csd has a
                                                # weird shape
    elif this_method in ('multi_taper_csd', 'periodogram_csd'):
        # these methods should work with similar signatures
        mdict = method.copy()
        func = eval(mdict.pop('this_method'))
        freqs, fxy = func(time_series, **mdict)
        f = utils.circle_to_hz(freqs, mdict.get('Fs', 2 * np.pi))

    else:
        raise ValueError("Unknown method provided")

    return f, fxy.squeeze()


def get_spectra_bi(x, y, method=None):
    r"""
    Computes the spectra of two timeseries and the cross-spectrum between them

    Parameters
    ----------

    x,y : float arrays
        Time-series data

    method : dict, optional
       See :func:`get_spectra` documentation for details

    Returns
    -------
    f : float array
        The central frequencies for the frequency
        bands for which the spectra are estimated
    fxx : float array
         The psd of the first signal
    fyy : float array
        The psd of the second signal
    fxy : float array
        The cross-spectral density of the two signals

    """
    f, fij = get_spectra(np.vstack((x, y)), method=method)
    fxx = fij[0, 0].real
    fyy = fij[1, 1].real
    fxy = fij[0, 1]
    return f, fxx, fyy, fxy


# The following spectrum estimates are normalized to the convention
# adopted by MATLAB (or at least spectrum.psd)
# By definition, Sxx(f) = DTFT{Rxx(n)}, where Rxx(n) is the autocovariance
# function of x(n). Therefore the integral from
# [-Fs/2, Fs/2] of Sxx(f)*df is Rxx(0).
# And from the definition of Rxx(n),
# Rxx(0) = Expected-Value{x(n)x*(n)} = Expected-Value{ |x|^2 },
# which is estimated as (x*x.conj()).mean()
# In other words, sum(Sxx) * Fs / NFFT ~ var(x)

def periodogram(s, Fs=2 * np.pi, Sk=None, N=None,
                sides='default', normalize=True):
    """Takes an N-point periodogram estimate of the PSD function. The
    number of points N, or a precomputed FFT Sk may be provided. By default,
    the PSD function returned is normalized so that the integral of the PSD
    is equal to the mean squared amplitude (mean energy) of s (see Notes).

    Parameters
    ----------
    s : ndarray
        Signal(s) for which to estimate the PSD, time dimension in the last
        axis

    Fs : float (optional)
       The sampling rate. Defaults to 2*pi

    Sk : ndarray (optional)
        Precomputed FFT of s

    N : int (optional)
        Indicates an N-point FFT where N != s.shape[-1]

    sides : str (optional) [ 'default' | 'onesided' | 'twosided' ]
         This determines which sides of the spectrum to return.
         For complex-valued inputs, the default is two-sided, for real-valued
         inputs, default is one-sided Indicates whether to return a one-sided
         or two-sided

    PSD normalize : boolean (optional, default=True) Normalizes the PSD

    Returns
    -------
    (f, psd) : tuple
       f: The central frequencies for the frequency bands
       PSD estimate for each row of s

    """
    if Sk is not None:
        N = Sk.shape[-1]
    else:
        N = s.shape[-1] if not N else N
        Sk = fftpack.fft(s, n=N)
    pshape = list(Sk.shape)

    # if the time series is a complex vector, a one sided PSD is invalid:
    if (sides == 'default' and np.iscomplexobj(s)) or sides == 'twosided':
        sides = 'twosided'
    elif sides in ('default', 'onesided'):
        sides = 'onesided'

    if sides == 'onesided':
        # putative Nyquist freq
        Fn = N / 2 + 1
        # last duplicate freq
        Fl = (N + 1) / 2
        pshape[-1] = Fn
        P = np.zeros(pshape, 'd')
        freqs = np.linspace(0, Fs / 2, Fn)
        P[..., 0] = (Sk[..., 0] * Sk[..., 0].conj()).real
        P[..., 1:Fl] = 2 * (Sk[..., 1:Fl] * Sk[..., 1:Fl].conj()).real
        if Fn > Fl:
            P[..., Fn - 1] = (Sk[..., Fn - 1] * Sk[..., Fn - 1].conj()).real
    else:
        P = (Sk * Sk.conj()).real
        freqs = np.linspace(0, Fs, N, endpoint=False)
    if normalize:
        P /= (Fs * s.shape[-1])
    return freqs, P


def periodogram_csd(s, Fs=2 * np.pi, Sk=None, NFFT=None, sides='default',
                    normalize=True):
    """Takes an N-point periodogram estimate of all the cross spectral
    density functions between rows of s.

    The number of points N, or a precomputed FFT Sk may be provided. By
    default, the CSD function returned is normalized so that the integral of
    the PSD is equal to the mean squared amplitude (mean energy) of s (see
    Notes).

    Parameters
    ---------

    s : ndarray
        Signals for which to estimate the CSD, time dimension in the last axis

    Fs : float (optional)
       The sampling rate. Defaults to 2*pi

    Sk : ndarray (optional)
        Precomputed FFT of rows of s

    NFFT : int (optional)
        Indicates an N-point FFT where N != s.shape[-1]

    sides : str (optional)   [ 'default' | 'onesided' | 'twosided' ]
        This determines which sides of the spectrum to return.
        For complex-valued inputs, the default is two-sided, for real-valued
        inputs, default is one-sided Indicates whether to return a one-sided
        or two-sided

    normalize : boolean (optional)
        Normalizes the PSD

    Returns
    -------

    freqs, csd_est : ndarrays
        The estimated CSD and the frequency points vector.
        The CSD{i,j}(f) are returned in a square "matrix" of vectors
        holding Sij(f). For an input array that is reshaped to (M,N),
        the output is (M,M,N)

    """
    s_shape = s.shape
    s.shape = (np.prod(s_shape[:-1]), s_shape[-1])
    # defining an Sk_loc is a little opaque, but it avoids having to
    # reset the shape of any user-given Sk later on
    if Sk is not None:
        Sk_shape = Sk.shape
        N = Sk.shape[-1]
        Sk_loc = Sk.reshape(np.prod(Sk_shape[:-1]), N)
    else:
        if NFFT is not None:
            N = NFFT
        else:
            N = s.shape[-1]
        Sk_loc = fftpack.fft(s, n=N)
    # reset s.shape
    s.shape = s_shape

    M = Sk_loc.shape[0]

    # if the time series is a complex vector, a one sided PSD is invalid:
    if (sides == 'default' and np.iscomplexobj(s)) or sides == 'twosided':
        sides = 'twosided'
    elif sides in ('default', 'onesided'):
        sides = 'onesided'

    if sides == 'onesided':
        # putative Nyquist freq
        Fn = N / 2 + 1
        # last duplicate freq
        Fl = (N + 1) / 2
        csd_pairs = np.zeros((M, M, Fn), 'D')
        freqs = np.linspace(0, Fs / 2, Fn)
        for i in range(M):
            for j in range(i + 1):
                csd_pairs[i, j, 0] = Sk_loc[i, 0] * Sk_loc[j, 0].conj()
                csd_pairs[i, j, 1:Fl] = 2 * (Sk_loc[i, 1:Fl] *
                                             Sk_loc[j, 1:Fl].conj())
                if Fn > Fl:
                    csd_pairs[i, j, Fn - 1] = (Sk_loc[i, Fn - 1] *
                                               Sk_loc[j, Fn - 1].conj())

    else:
        csd_pairs = np.zeros((M, M, N), 'D')
        freqs = np.linspace(0, Fs / 2, N, endpoint=False)
        for i in range(M):
            for j in range(i + 1):
                csd_pairs[i, j] = Sk_loc[i] * Sk_loc[j].conj()
    if normalize:
        csd_pairs /= (Fs*N)

    csd_mat = csd_pairs.transpose(1,0,2).conj()
    csd_mat += csd_pairs
    diag_idc = (np.arange(M), np.arange(M))
    csd_mat[diag_idc] /= 2

    return freqs, csd_mat


def dpss_windows(N, NW, Kmax, interp_from=None, interp_kind='linear'):
    """
    Returns the Discrete Prolate Spheroidal Sequences of orders [0,Kmax-1]
    for a given frequency-spacing multiple NW and sequence length N.

    Parameters
    ----------
    N : int
        sequence length
    NW : float, unitless
        standardized half bandwidth corresponding to 2NW = BW/f0 = BW*N*dt
        but with dt taken as 1
    Kmax : int
        number of DPSS windows to return is Kmax (orders 0 through Kmax-1)
    interp_from : int (optional)
        The dpss can be calculated using interpolation from a set of dpss
        with the same NW and Kmax, but shorter N. This is the length of this
        shorter set of dpss windows.
    interp_kind : str (optional)
        This input variable is passed to scipy.interpolate.interp1d and
        specifies the kind of interpolation as a string ('linear', 'nearest',
        'zero', 'slinear', 'quadratic, 'cubic') or as an integer specifying the
        order of the spline interpolator to use.


    Returns
    -------
    v, e : tuple,
        v is an array of DPSS windows shaped (Kmax, N)
        e are the eigenvalues

    Notes
    -----
    Tridiagonal form of DPSS calculation from:

    Slepian, D. Prolate spheroidal wave functions, Fourier analysis, and
    uncertainty V: The discrete case. Bell System Technical Journal,
    Volume 57 (1978), 1371430
    """
    Kmax = int(Kmax)
    W = float(NW) / N
    nidx = np.arange(N, dtype='d')

    # In this case, we create the dpss windows of the smaller size
    # (interp_from) and then interpolate to the larger size (N)
    if interp_from is not None:
        if interp_from > N:
            e_s = 'In dpss_windows, interp_from is: %s ' % interp_from
            e_s += 'and N is: %s. ' % N
            e_s += 'Please enter interp_from smaller than N.'
            raise ValueError(e_s)
        dpss = []
        d, e = dpss_windows(interp_from, NW, Kmax)
        for this_d in d:
            x = np.arange(this_d.shape[-1])
            I = interpolate.interp1d(x, this_d, kind=interp_kind)
            d_temp = I(np.arange(0, this_d.shape[-1] - 1,
                                 float(this_d.shape[-1] - 1) / N))

            # Rescale:
            d_temp = d_temp / np.sqrt(np.sum(d_temp ** 2))

            dpss.append(d_temp)

        dpss = np.array(dpss)

    else:
        # here we want to set up an optimization problem to find a sequence
        # whose energy is maximally concentrated within band [-W,W].
        # Thus, the measure lambda(T,W) is the ratio between the energy within
        # that band, and the total energy. This leads to the eigen-system
        # (A - (l1)I)v = 0, where the eigenvector corresponding to the largest
        # eigenvalue is the sequence with maximally concentrated energy. The
        # collection of eigenvectors of this system are called Slepian
        # sequences, or discrete prolate spheroidal sequences (DPSS). Only the
        # first K, K = 2NW/dt orders of DPSS will exhibit good spectral
        # concentration
        # [see http://en.wikipedia.org/wiki/Spectral_concentration_problem]

        # Here I set up an alternative symmetric tri-diagonal eigenvalue
        # problem such that
        # (B - (l2)I)v = 0, and v are our DPSS (but eigenvalues l2 != l1)
        # the main diagonal = ([N-1-2*t]/2)**2 cos(2PIW), t=[0,1,2,...,N-1]
        # and the first off-diagonal = t(N-t)/2, t=[1,2,...,N-1]
        # [see Percival and Walden, 1993]
        diagonal = ((N - 1 - 2 * nidx) / 2.) ** 2 * np.cos(2 * np.pi * W)
        off_diag = np.zeros_like(nidx)
        off_diag[:-1] = nidx[1:] * (N - nidx[1:]) / 2.
        # put the diagonals in LAPACK "packed" storage
        ab = np.zeros((2, N), 'd')
        ab[1] = diagonal
        ab[0, 1:] = off_diag[:-1]
        # only calculate the highest Kmax eigenvalues
        w = linalg.eigvals_banded(ab, select='i',
                                  select_range=(N - Kmax, N - 1))
        w = w[::-1]

        # find the corresponding eigenvectors via inverse iteration
        t = np.linspace(0, np.pi, N)
        dpss = np.zeros((Kmax, N), 'd')
        for k in range(Kmax):
            dpss[k] = utils.tridi_inverse_iteration(
                diagonal, off_diag, w[k], x0=np.sin((k + 1) * t)
                )

    # By convention (Percival and Walden, 1993 pg 379)
    # * symmetric tapers (k=0,2,4,...) should have a positive average.
    # * antisymmetric tapers should begin with a positive lobe
    fix_symmetric = (dpss[0::2].sum(axis=1) < 0)
    for i, f in enumerate(fix_symmetric):
        if f:
            dpss[2 * i] *= -1
    # rather than test the sign of one point, test the sign of the
    # linear slope up to the first (largest) peak
    pk = np.argmax( np.abs(dpss[1::2, :N/2]), axis=1 )
    for i, p in enumerate(pk):
        if np.sum(dpss[2 * i + 1, :p]) < 0:
            dpss[2 * i + 1] *= -1

    # Now find the eigenvalues of the original spectral concentration problem
    # Use the autocorr sequence technique from Percival and Walden, 1993 pg 390
    dpss_rxx = utils.autocorr(dpss) * N
    r = 4 * W * np.sinc(2 * W * nidx)
    r[0] = 2 * W
    eigvals = np.dot(dpss_rxx, r)

    return dpss, eigvals

def tapered_spectra(s, tapers, NFFT=None, low_bias=True):
    """
    Compute the tapered spectra of the rows of s.

    Parameters
    ----------

    s : ndarray, (n_arr, n_pts)
        An array whose rows are timeseries.

    tapers : ndarray or container
        Either the precomputed DPSS tapers, or the pair of parameters
        (NW, K) needed to compute K tapers of length n_pts.

    NFFT : int
        Number of FFT bins to compute

    low_bias : Boolean
        If compute DPSS, automatically select tapers corresponding to
        > 90% energy concentration.

    Returns
    -------

    t_spectra : ndarray, shaped (n_arr, K, NFFT)
      The FFT of the tapered sequences in s. First dimension is squeezed
      out if n_arr is 1.
    eigvals : ndarray
      The eigenvalues are also returned if DPSS are calculated here.

    """
    N = s.shape[-1]
    # XXX: don't allow NFFT < N -- not every implementation is so restrictive!
    if NFFT is None or NFFT < N:
        NFFT = N
    rest_of_dims = s.shape[:-1]
    M = int(np.product(rest_of_dims))

    s = s.reshape(int(np.product(rest_of_dims)), N)
    # de-mean this sucker
    s = utils.remove_bias(s, axis=-1)

    if not isinstance(tapers, np.ndarray):
        # then tapers is (NW, K)
        args = (N,) + tuple(tapers)
        dpss, eigvals = dpss_windows(*args)
        if low_bias:
            keepers = (eigvals > 0.9)
            dpss = dpss[keepers]
            eigvals = eigvals[keepers]
        tapers = dpss
    else:
        eigvals = None
    K = tapers.shape[0]
    sig_sl = [slice(None)] * len(s.shape)
    sig_sl.insert(len(s.shape) - 1, np.newaxis)

    # tapered.shape is (M, Kmax, N)
    tapered = s[sig_sl] * tapers

    # compute the y_{i,k}(f) -- full FFT takes ~1.5x longer, but unpacking
    # results of real-valued FFT eats up memory
    t_spectra = fftpack.fft(tapered, n=NFFT, axis=-1)
    t_spectra.shape = rest_of_dims + (K, NFFT)
    if eigvals is None:
        return t_spectra
    return t_spectra, eigvals

def mtm_cross_spectrum(tx, ty, weights, sides='twosided'):
    r"""

    The cross-spectrum between two tapered time-series, derived from a
    multi-taper spectral estimation.

    Parameters
    ----------

    tx, ty : ndarray (K, ..., N)
       The complex DFTs of the tapered sequence

    weights : ndarray, or 2-tuple or list
       Weights can be specified as a length-2 list of weights for spectra tx
       and ty respectively. Alternatively, if tx is ty and this function is
       computing the spectral density function of a single sequence, the
       weights can be given as an ndarray of weights for the spectrum.
       Weights may be

       * scalars, if the shape of the array is (K, ..., 1)
       * vectors, with the shape of the array being the same as tx or ty

    sides : str in {'onesided', 'twosided'}
       For the symmetric spectra of a real sequence, optionally combine half
       of the frequencies and scale the duplicate frequencies in the range
       (0, F_nyquist).

    Notes
    -----

    spectral densities are always computed as

    :math:`S_{xy}^{mt}(f) = \frac{\sum_k
    [d_k^x(f)s_k^x(f)][d_k^y(f)(s_k^y(f))^{*}]}{[\sum_k
    d_k^x(f)^2]^{\frac{1}{2}}[\sum_k d_k^y(f)^2]^{\frac{1}{2}}}`

    """
    N = tx.shape[-1]
    if ty.shape != tx.shape:
        raise ValueError('shape mismatch between tx, ty')

    # pshape = list(tx.shape)

    if isinstance(weights, (list, tuple)):
        autospectrum = False
        weights_x = weights[0]
        weights_y = weights[1]
        denom = (np.abs(weights_x) ** 2).sum(axis=0) ** 0.5
        denom *= (np.abs(weights_y) ** 2).sum(axis=0) ** 0.5
    else:
        autospectrum = True
        weights_x = weights
        weights_y = weights
        denom = (np.abs(weights) ** 2).sum(axis=0)

    if sides == 'onesided':
        # where the nyq freq should be
        Fn = N / 2 + 1
        truncated_slice = [slice(None)] * len(tx.shape)
        truncated_slice[-1] = slice(0, Fn)
        tsl = tuple(truncated_slice)
        tx = tx[tsl]
        ty = ty[tsl]
        # if weights.shape[-1] > 1 then make sure weights are truncated too
        if weights_x.shape[-1] > 1:
            weights_x = weights_x[tsl]
            weights_y = weights_y[tsl]
            denom = denom[tsl[1:]]

    sf = weights_x * tx
    sf *= (weights_y * ty).conj()
    sf = sf.sum(axis=0)
    sf /= denom

    if sides == 'onesided':
        # dbl power at duplicated freqs
        Fl = (N + 1) / 2
        sub_slice = [slice(None)] * len(sf.shape)
        sub_slice[-1] = slice(1, Fl)
        sf[tuple(sub_slice)] *= 2

    if autospectrum:
        return sf.real
    return sf


def multi_taper_psd(
        s, Fs=2 * np.pi, NW=None, BW=None, adaptive=False,
        jackknife=True, low_bias=True, sides='default', NFFT=None
        ):
    """Returns an estimate of the PSD function of s using the multitaper
    method. If the NW product, or the BW and Fs in Hz are not specified
    by the user, a bandwidth of 4 times the fundamental frequency,
    corresponding to NW = 4 will be used.

    Parameters
    ----------
    s : ndarray
       An array of sampled random processes, where the time axis is assumed to
       be on the last axis

    Fs : float
        Sampling rate of the signal

    NW : float
        The normalized half-bandwidth of the data tapers, indicating a
        multiple of the fundamental frequency of the DFT (Fs/N).
        Common choices are n/2, for n >= 4. This parameter is unitless
        and more MATLAB compatible. As an alternative, set the BW
        parameter in Hz. See Notes on bandwidth.

    BW : float
        The sampling-relative bandwidth of the data tapers, in Hz.

    adaptive : {True/False}
       Use an adaptive weighting routine to combine the PSD estimates of
       different tapers.

    jackknife : {True/False}
       Use the jackknife method to make an estimate of the PSD variance
       at each point.

    low_bias : {True/False}
       Rather than use 2NW tapers, only use the tapers that have better than
       90% spectral concentration within the bandwidth (still using
       a maximum of 2NW tapers)

    sides : str (optional)   [ 'default' | 'onesided' | 'twosided' ]
         This determines which sides of the spectrum to return.
         For complex-valued inputs, the default is two-sided, for real-valued
         inputs, default is one-sided Indicates whether to return a one-sided
         or two-sided

    Returns
    -------
    (freqs, psd_est, var_or_nu) : ndarrays
        The first two arrays are the frequency points vector and the
        estimated PSD. The last returned array differs depending on whether
        the jackknife was used. It is either

        * The jackknife estimated variance of the log-psd, OR
        * The degrees of freedom in a chi2 model of how the estimated
          PSD is distributed about the true log-PSD (this is either
          2*floor(2*NW), or calculated from adaptive weights)

    Notes
    -----

    The bandwidth of the windowing function will determine the number
    tapers to use. This parameters represents trade-off between frequency
    resolution (lower main lobe BW for the taper) and variance reduction
    (higher BW and number of averaged estimates). Typically, the number of
    tapers is calculated as 2x the bandwidth-to-fundamental-frequency
    ratio, as these eigenfunctions have the best energy concentration.

    """
    # have last axis be time series for now
    N = s.shape[-1]
    M = int(np.product(s.shape[:-1]))

    if BW is not None:
        # BW wins in a contest (since it was the original implementation)
        norm_BW = np.round(BW * N / Fs)
        NW = norm_BW / 2.0
    elif NW is None:
        # default NW
        NW = 4
    # (else BW is None and NW is not None) ... all set
    Kmax = int(2 * NW)

    # if the time series is a complex vector, a one sided PSD is invalid:
    if (sides == 'default' and np.iscomplexobj(s)) or sides == 'twosided':
        sides = 'twosided'
    elif sides in ('default', 'onesided'):
        sides = 'onesided'

    # Find the direct spectral estimators S_k(f) for k tapered signals..
    # don't normalize the periodograms by 1/N as normal.. since the taper
    # windows are orthonormal, they effectively scale the signal by 1/N
    spectra, eigvals = tapered_spectra(
        s, (NW, Kmax), NFFT=NFFT, low_bias=low_bias
        )
    NFFT = spectra.shape[-1]
    K = len(eigvals)
    # collapse spectra's shape back down to 3 dimensions
    spectra.shape = (M, K, NFFT)

    last_freq = NFFT / 2 + 1 if sides == 'onesided' else NFFT

    # degrees of freedom at each timeseries, at each freq
    nu = np.empty((M, last_freq))
    if adaptive:
        weights = np.empty((M, K, last_freq))
        for i in range(M):
            weights[i], nu[i] = utils.adaptive_weights(
                spectra[i], eigvals, sides=sides
                )
    else:
        # let the weights simply be the square-root of the eigenvalues.
        # repeat these values across all n_chan channels of data
        weights = np.tile(np.sqrt(eigvals), M).reshape(M, K, 1)
        nu.fill(2 * K)

    if jackknife:
        jk_var = np.empty_like(nu)
        for i in range(M):
            jk_var[i] = utils.jackknifed_sdf_variance(
                spectra[i], eigvals, sides=sides, adaptive=adaptive
                )

    # Compute the unbiased spectral estimator for S(f) as the sum of
    # the S_k(f) weighted by the function w_k(f)**2, all divided by the
    # sum of the w_k(f)**2 over k

    # 1st, roll the tapers axis forward
    spectra = np.rollaxis(spectra, 1, start=0)
    weights = np.rollaxis(weights, 1, start=0)
    sdf_est = mtm_cross_spectrum(
        spectra, spectra, weights, sides=sides
        )
    sdf_est /= Fs
    
    if sides == 'onesided':
        freqs = np.linspace(0, Fs / 2, NFFT / 2 + 1)
    else:
        freqs = np.linspace(0, Fs, NFFT, endpoint=False)

    out_shape = s.shape[:-1] + (len(freqs),)
    sdf_est.shape = out_shape
    if jackknife:
        jk_var.shape = out_shape
        return freqs, sdf_est, jk_var
    else:
        nu.shape = out_shape
        return freqs, sdf_est, nu


def multi_taper_csd(s, Fs=2 * np.pi, NW=None, BW=None, low_bias=True,
                    adaptive=False, sides='default', NFFT=None):
    """Returns an estimate of the Cross Spectral Density (CSD) function
    between all (N choose 2) pairs of timeseries in s, using the multitaper
    method. If the NW product, or the BW and Fs in Hz are not specified by
    the user, a bandwidth of 4 times the fundamental frequency, corresponding
    to NW = 4 will be used.

    Parameters
    ----------
    s : ndarray
        An array of sampled random processes, where the time axis is
        assumed to be on the last axis. If ndim > 2, the number of time
        series to compare will still be taken as prod(s.shape[:-1])

    Fs : float, Sampling rate of the signal

    NW : float
        The normalized half-bandwidth of the data tapers, indicating a
        multiple of the fundamental frequency of the DFT (Fs/N).
        Common choices are n/2, for n >= 4. This parameter is unitless
        and more MATLAB compatible. As an alternative, set the BW
        parameter in Hz. See Notes on bandwidth.

    BW : float
        The sampling-relative bandwidth of the data tapers, in Hz.

    adaptive : {True, False}
       Use adaptive weighting to combine spectra

    low_bias : {True, False}
       Rather than use 2NW tapers, only use the tapers that have better than
       90% spectral concentration within the bandwidth (still using
       a maximum of 2NW tapers)

    sides : str (optional)   [ 'default' | 'onesided' | 'twosided' ]
         This determines which sides of the spectrum to return.  For
         complex-valued inputs, the default is two-sided, for real-valued
         inputs, default is one-sided Indicates whether to return a one-sided
         or two-sided

    Returns
    -------
    (freqs, csd_est) : ndarrays
        The estimatated CSD and the frequency points vector.
        The CSD{i,j}(f) are returned in a square "matrix" of vectors
        holding Sij(f). For an input array of (M,N), the output is (M,M,N)

    Notes
    -----

    The bandwidth of the windowing function will determine the number
    tapers to use. This parameters represents trade-off between frequency
    resolution (lower main lobe BW for the taper) and variance reduction
    (higher BW and number of averaged estimates). Typically, the number of
    tapers is calculated as 2x the bandwidth-to-fundamental-frequency
    ratio, as these eigenfunctions have the best energy concentration.

    """
    # have last axis be time series for now
    N = s.shape[-1]
    M = int(np.product(s.shape[:-1]))

    if BW is not None:
        # BW wins in a contest (since it was the original implementation)
        norm_BW = np.round(BW * N / Fs)
        NW = norm_BW / 2.0
    elif NW is None:
        # default NW
        NW = 4
    # (else BW is None and NW is not None) ... all set
    Kmax = int(2 * NW)

    # if the time series is a complex vector, a one sided PSD is invalid:
    if (sides == 'default' and np.iscomplexobj(s)) or sides == 'twosided':
        sides = 'twosided'
    elif sides in ('default', 'onesided'):
        sides = 'onesided'

    # Find the direct spectral estimators S_k(f) for k tapered signals..
    # don't normalize the periodograms by 1/N as normal.. since the taper
    # windows are orthonormal, they effectively scale the signal by 1/N
    spectra, eigvals = tapered_spectra(
        s, (NW, Kmax), NFFT=NFFT, low_bias=low_bias
        )
    NFFT = spectra.shape[-1]
    K = len(eigvals)
    # collapse spectra's shape back down to 3 dimensions
    spectra.shape = (M, K, NFFT)

    # compute the cross-spectral density functions
    last_freq = NFFT / 2 + 1 if sides == 'onesided' else NFFT

    if adaptive:
        w = np.empty((M, K, last_freq))
        nu = np.empty((M, last_freq))
        for i in range(M):
            w[i], nu[i] = utils.adaptive_weights(
                spectra[i], eigvals, sides=sides
                )
    else:
        weights = np.sqrt(eigvals).reshape(K, 1)

    csd_pairs = np.zeros((M, M, last_freq), 'D')
    for i in range(M):
        if adaptive:
            wi = w[i]
        else:
            wi = weights
        for j in range(i + 1):
            if adaptive:
                wj = w[j]
            else:
                wj = weights
            ti = spectra[i]
            tj = spectra[j]
            csd_pairs[i, j] = mtm_cross_spectrum(ti, tj, (wi, wj), sides=sides)

    csdfs = csd_pairs.transpose(1,0,2).conj()
    csdfs += csd_pairs
    diag_idc = (np.arange(M), np.arange(M))
    csdfs[diag_idc] /= 2
    csdfs /= Fs
    
    if sides == 'onesided':
        freqs = np.linspace(0, Fs / 2, NFFT / 2 + 1)
    else:
        freqs = np.linspace(0, Fs, NFFT, endpoint=False)

    return freqs, csdfs


def freq_response(b, a=1., n_freqs=1024, sides='onesided'):
    """
    Returns the frequency response of the IIR or FIR filter described
    by beta and alpha coefficients.

    Parameters
    ----------

    b : beta sequence (moving average component)
    a : alpha sequence (autoregressive component)
    n_freqs : size of frequency grid
    sides : {'onesided', 'twosided'}
       compute frequencies between [-PI,PI), or from [0, PI]

    Returns
    -------

    fgrid, H(e^jw)

    Notes
    -----
    For a description of the linear constant-coefficient difference equation,
    see
    http://en.wikipedia.org/wiki/Z-transform
    """
    # transitioning to scipy freqz
    real_n = n_freqs // 2 + 1 if sides == 'onesided' else n_freqs
    return sig.freqz(b, a=a, worN=real_n, whole=sides != 'onesided')

########NEW FILE########
__FILENAME__ = test_autoregressive
import numpy as np
import numpy.testing as npt
import numpy.testing.decorators as dec

import nitime.algorithms as tsa
import nitime.utils as utils

# Set the random seed:
np.random.seed(1)


def _random_poles(half_poles=3):
    poles_rp = np.random.rand(half_poles * 20)
    poles_ip = np.random.rand(half_poles * 20)

    # get real/imag parts of some poles such that magnitudes bounded
    # away from 1
    stable_pole_idx = np.where(poles_rp ** 2 + poles_ip ** 2 < .75 ** 2)[0]
    # keep 3 of these, and supplement with complex conjugate
    stable_poles = poles_rp[stable_pole_idx[:half_poles]] + \
                   1j * poles_ip[stable_pole_idx[:half_poles]]
    stable_poles = np.r_[stable_poles, stable_poles.conj()]
    # we have the roots, now find the polynomial
    ak = np.poly(stable_poles)
    return ak


def test_AR_est_consistency():
    order = 10  # some even number
    ak = _random_poles(order / 2)
    x, v, _ = utils.ar_generator(N=512, coefs=-ak[1:], drop_transients=100)
    ak_yw, ssq_yw = tsa.AR_est_YW(x, order)
    ak_ld, ssq_ld = tsa.AR_est_LD(x, order)
    npt.assert_almost_equal(ak_yw, ak_ld)
    npt.assert_almost_equal(ssq_yw, ssq_ld)


def test_AR_YW():
    arsig, _, _ = utils.ar_generator(N=512)
    avg_pwr = (arsig * arsig.conjugate()).mean()
    order = 8
    ak, sigma_v = tsa.AR_est_YW(arsig, order)
    w, psd = tsa.AR_psd(ak, sigma_v)
    # the psd is a one-sided power spectral density, which has been
    # multiplied by 2 to preserve the property that
    # 1/2pi int_{-pi}^{pi} Sxx(w) dw = Rxx(0)

    # evaluate this integral numerically from 0 to pi
    dw = np.pi / len(psd)
    avg_pwr_est = np.trapz(psd, dx=dw) / (2 * np.pi)
    # consistency on the order of 10**0 is pretty good for this test
    npt.assert_almost_equal(avg_pwr, avg_pwr_est, decimal=0)

    # Test for providing the autocovariance as an input:
    ak, sigma_v = tsa.AR_est_YW(arsig, order, utils.autocov(arsig))
    w, psd = tsa.AR_psd(ak, sigma_v)
    avg_pwr_est = np.trapz(psd, dx=dw) / (2 * np.pi)
    npt.assert_almost_equal(avg_pwr, avg_pwr_est, decimal=0)


def test_AR_LD():
    """

    Test the Levinson Durbin estimate of the AR coefficients against the
    expercted PSD

    """
    arsig, _, _ = utils.ar_generator(N=512)
    avg_pwr = (arsig * arsig.conjugate()).real.mean()
    order = 8
    ak, sigma_v = tsa.AR_est_LD(arsig, order)
    w, psd = tsa.AR_psd(ak, sigma_v)

    # the psd is a one-sided power spectral density, which has been
    # multiplied by 2 to preserve the property that
    # 1/2pi int_{-pi}^{pi} Sxx(w) dw = Rxx(0)

    # evaluate this integral numerically from 0 to pi
    dw = np.pi / len(psd)
    avg_pwr_est = np.trapz(psd, dx=dw) / (2 * np.pi)
    npt.assert_almost_equal(avg_pwr, avg_pwr_est, decimal=0)

    # Test for providing the autocovariance as an input:
    ak, sigma_v = tsa.AR_est_LD(arsig, order, utils.autocov(arsig))
    w, psd = tsa.AR_psd(ak, sigma_v)
    avg_pwr_est = np.trapz(psd, dx=dw) / (2 * np.pi)
    npt.assert_almost_equal(avg_pwr, avg_pwr_est, decimal=0)


@dec.slow
def test_MAR_est_LWR():
    """

    Test the LWR MAR estimator against the power of the signal

    This also tests the functions: transfer_function_xy, spectral_matrix_xy,
    coherence_from_spectral and granger_causality_xy

    """

    # This is the same processes as those in doc/examples/ar_est_2vars.py:
    a1 = np.array([[0.9, 0],
                   [0.16, 0.8]])

    a2 = np.array([[-0.5, 0],
                   [-0.2, -0.5]])

    am = np.array([-a1, -a2])

    x_var = 1
    y_var = 0.7
    xy_cov = 0.4
    cov = np.array([[x_var, xy_cov],
                    [xy_cov, y_var]])

    n_freqs = 1024
    w, Hw = tsa.transfer_function_xy(am, n_freqs=n_freqs)
    Sw = tsa.spectral_matrix_xy(Hw, cov)

    # This many realizations of the process:
    N = 500
    # Each one this long
    L = 1024

    order = am.shape[0]
    n_lags = order + 1

    n_process = am.shape[-1]

    z = np.empty((N, n_process, L))
    nz = np.empty((N, n_process, L))

    for i in range(N):
        z[i], nz[i] = utils.generate_mar(am, cov, L)

    a_est = []
    cov_est = []

    # This loop runs MAR_est_LWR:
    for i in range(N):
        Rxx = (tsa.MAR_est_LWR(z[i], order=n_lags))
        a_est.append(Rxx[0])
        cov_est.append(Rxx[1])

    a_est = np.mean(a_est, 0)
    cov_est = np.mean(cov_est, 0)

    # This tests transfer_function_xy and spectral_matrix_xy:
    w, Hw_est = tsa.transfer_function_xy(a_est, n_freqs=n_freqs)
    Sw_est = tsa.spectral_matrix_xy(Hw_est, cov_est)

    # coherence_from_spectral:
    c = tsa.coherence_from_spectral(Sw)
    c_est = tsa.coherence_from_spectral(Sw_est)

    # granger_causality_xy:

    w, f_x2y, f_y2x, f_xy, Sw = tsa.granger_causality_xy(am,
                                                         cov,
                                                         n_freqs=n_freqs)

    w, f_x2y_est, f_y2x_est, f_xy_est, Sw_est = tsa.granger_causality_xy(a_est,
                                                             cov_est,
                                                             n_freqs=n_freqs)

    # interdependence_xy
    i_xy = tsa.interdependence_xy(Sw)
    i_xy_est = tsa.interdependence_xy(Sw_est)

    # This is all very approximate:
    npt.assert_almost_equal(Hw, Hw_est, decimal=1)
    npt.assert_almost_equal(Sw, Sw_est, decimal=1)
    npt.assert_almost_equal(c, c_est, 1)
    npt.assert_almost_equal(f_xy, f_xy_est, 1)
    npt.assert_almost_equal(f_x2y, f_x2y_est, 1)
    npt.assert_almost_equal(f_y2x, f_y2x_est, 1)
    npt.assert_almost_equal(i_xy, i_xy_est, 1)


def test_lwr():
    "test solution of lwr recursion"
    for trial in range(3):
        nc = np.random.randint(2, high=10)
        P = np.random.randint(2, high=6)
        # nc is channels, P is lags (order)
        r = np.random.randn(P + 1, nc, nc)
        r[0] = np.dot(r[0], r[0].T)  # force r0 to be symmetric

        a, Va = tsa.lwr_recursion(r)
        # Verify the "orthogonality" principle of the mAR system
        # Set up a system in blocks to compute, for each k
        #   sum_{i=1}^{P} A(i)R(k-i) = -R(k) k > 0
        # = sum_{i=1}^{P} R(k-i)^T A(i)^T = -R(k)^T
        # = sum_{i=1}^{P} R(i-k)A(i)^T = -R(k)^T
        rmat = np.zeros((nc * P, nc * P))
        for k in range(1, P + 1):
            for i in range(1, P + 1):
                im = i - k
                if im < 0:
                    r1 = r[-im].T
                else:
                    r1 = r[im]
                rmat[(k - 1) * nc:k * nc, (i - 1) * nc:i * nc] = r1

        rvec = np.zeros((nc * P, nc))
        avec = np.zeros((nc * P, nc))
        for m in range(P):
            rvec[m * nc:(m + 1) * nc] = -r[m + 1].T
            avec[m * nc:(m + 1) * nc] = a[m].T

        l2_d = np.dot(rmat, avec) - rvec
        l2_d = (l2_d ** 2).sum() ** 0.5
        l2_r = (rvec ** 2).sum() ** 0.5

        # compute |Ax-b| / |b| metric
        npt.assert_almost_equal(l2_d / l2_r, 0, decimal=5)

########NEW FILE########
__FILENAME__ = test_coherence
"""

Tests of functions under algorithms.coherence


"""

import os
import warnings

import numpy as np
import numpy.testing as npt
from scipy.signal import signaltools
import matplotlib
import matplotlib.mlab as mlab
from scipy import fftpack

import nitime
import nitime.algorithms as tsa
import nitime.utils as utils

# Matplotlib older than 0.99 will have some issues with the normalization of t
if float(matplotlib.__version__[:3]) < 0.99:
    w_s = "You have a relatively old version of Matplotlib. " 
    w_s += " Estimation of the PSD DC component might not be as expected."
    w_s += " Consider updating Matplotlib: http://matplotlib.sourceforge.net/"
    warnings.warn(w_s, Warning)
    old_mpl = True
else:
    old_mpl = False


#Define globally
test_dir_path = os.path.join(nitime.__path__[0], 'tests')

# Define these once globally:
t = np.linspace(0, 16 * np.pi, 1024)
x = np.sin(t) + np.sin(2 * t) + np.sin(3 * t) + np.random.rand(t.shape[-1])
y = x + np.random.rand(t.shape[-1])

tseries = np.vstack([x, y])

methods = (None,
           {"this_method": 'welch', "NFFT": 256, "Fs": 2 * np.pi},
           {"this_method": 'multi_taper_csd', "Fs": 2 * np.pi},
           {"this_method": 'periodogram_csd', "Fs": 2 * np.pi, "NFFT": 256},
           {"this_method": 'welch', "NFFT": 256, "Fs": 2 * np.pi,
            "window": mlab.window_hanning(np.ones(256))})


def test_coherency():
    """
    Tests that the coherency algorithm runs smoothly, using the different
    csd routines, that the resulting matrix is symmetric and for the welch
    method, that the frequency bands in the output make sense
    """

    for method in methods:
        f, c = tsa.coherency(tseries, csd_method=method)

        npt.assert_array_almost_equal(c[0, 1], c[1, 0].conjugate())
        npt.assert_array_almost_equal(c[0, 0], np.ones(f.shape))

        if method is not None and method['this_method'] != "multi_taper_csd":
            f_theoretical = utils.get_freqs(method['Fs'], method['NFFT'])
            npt.assert_array_almost_equal(f, f_theoretical)
            npt.assert_array_almost_equal(f, f_theoretical)


def test_coherence():
    """
    Tests that the coherency algorithm runs smoothly, using the different csd
    routines and that the result is symmetrical:
    """

    for method in methods:
        f, c = tsa.coherence(tseries, csd_method=method)
        npt.assert_array_almost_equal(c[0, 1], c[1, 0])
        npt.assert_array_almost_equal(c[0, 0], np.ones(f.shape))


def test_coherency_regularized():
    """
    Tests that the regularized coherency algorithm runs smoothly, using the
    different csd routines and that the result is symmetrical:
    """

    for method in methods:
        f, c = tsa.coherency_regularized(tseries, 0.05, 1000,
                                         csd_method=method)
        npt.assert_array_almost_equal(c[0, 1], c[1, 0].conjugate())


def test_coherence_regularized():
    """

    Tests that the regularized coherence algorithm runs smoothly, using the
    different csd routines and that the result is symmetrical:

    """
    for method in methods:
        f, c = tsa.coherence_regularized(tseries, 0.05, 1000,
                                         csd_method=method)
        npt.assert_array_almost_equal(c[0, 1], c[1, 0])


# Define as global for the following functions:

def test_coherency_bavg():
    ub = [np.pi / 2, None]
    lb = [0, 0.2]
    for method in methods:
        for this_lb in lb:
            for this_ub in ub:
                c = tsa.coherency_bavg(tseries, lb=this_lb, ub=this_ub,
                                       csd_method=method)

                #Test that this gets rid of the frequency axis:
                npt.assert_equal(len(c.shape), 2)
                # And that the result is equal
                npt.assert_almost_equal(c[0, 1], c[1, 0].conjugate())


def test_coherence_bavg():
    ub = [np.pi / 2, None]
    lb = [0, 0.2]
    for method in methods:
        for this_lb in lb:
            for this_ub in ub:
                c = tsa.coherence_bavg(tseries, lb=this_lb, ub=this_ub,
                                       csd_method=method)

                #Test that this gets rid of the frequency axis:
                npt.assert_equal(len(c.shape), 2)
                # And that the result is equal
                npt.assert_almost_equal(c[0, 1], c[1, 0].conjugate())


# XXX FIXME: This doesn't work for the periodogram method:
def test_coherence_partial():
    """ Test partial coherence"""

    x = np.sin(t) + np.sin(2 * t) + np.sin(3 * t) + np.random.rand(t.shape[-1])
    y = x + np.random.rand(t.shape[-1])
    z = y + np.random.rand(t.shape[-1])

    for method in methods:
        if (method is None) or method['this_method'] == 'welch':
            f, c = tsa.coherence_partial(np.vstack([x, y]), z,
                                         csd_method=method)
            npt.assert_array_almost_equal(c[0, 1], c[1, 0].conjugate())


def test_coherence_phase_delay():
    """

    Test the phase spectrum calculation

    """

    # Set up two time-series with a known phase delay:
    nz = np.random.rand(t.shape[-1])
    x = np.sin(t) + nz
    y = np.sin(t + np.pi) + nz

    tseries = np.vstack([x, y])
    for method in methods:
        f1, pdelay = tsa.coherency_phase_spectrum(tseries, csd_method=method)
        f2, tdelay = tsa.coherency_phase_delay(tseries, csd_method=method)
        npt.assert_almost_equal(pdelay[0, 1], -pdelay[1, 0])
        npt.assert_almost_equal(tdelay[0, 1], -tdelay[1, 0])
        # This is the relationship between these two quantities:
        npt.assert_almost_equal(tdelay[0, 1],
                                pdelay[0, 1][1:] / (2 * np.pi * f2))


def test_coherency_cached():
    """Tests that the cached coherency gives the same result as the standard
    coherency"""

    f1, c1 = tsa.coherency(tseries)

    ij = [(0, 1), (1, 0)]
    f2, cache = tsa.cache_fft(tseries, ij)

    c2 = tsa.cache_to_coherency(cache, ij)

    npt.assert_array_almost_equal(c1[1, 0], c2[1, 0])
    npt.assert_array_almost_equal(c1[0, 1], c2[0, 1])


def test_correlation_spectrum():
    """

    Test the correlation spectrum method

    """
    # Smoke-test for now - unclear what to test here...
    f, c = tsa.correlation_spectrum(x, y, norm=True)


# XXX FIXME: http://github.com/nipy/nitime/issues/issue/1
@npt.dec.skipif(True)
def test_coherence_linear_dependence():
    """
    Tests that the coherence between two linearly dependent time-series
    behaves as expected.

    From William Wei's book, according to eq. 14.5.34, if two time-series are
    linearly related through:

    y(t)  = alpha*x(t+time_shift)

    then the coherence between them should be equal to:

    .. :math:

    C(\nu) = \frac{1}{1+\frac{fft_{noise}(\nu)}{fft_{x}(\nu) \cdot \alpha^2}}

    """
    t = np.linspace(0, 16 * np.pi, 2 ** 14)
    x = np.sin(t) + np.sin(2 * t) + np.sin(3 * t) + \
                                            0.1 * np.random.rand(t.shape[-1])
    N = x.shape[-1]

    alpha = 10
    m = 3
    noise = 0.1 * np.random.randn(t.shape[-1])
    y = alpha * np.roll(x, m) + noise

    f_noise = fftpack.fft(noise)[0:N / 2]
    f_x = fftpack.fft(x)[0:N / 2]

    c_t = (1 / (1 + (f_noise / (f_x * (alpha ** 2)))))

    method = {"this_method": 'welch',
              "NFFT": 2048,
              "Fs": 2 * np.pi}

    f, c = tsa.coherence(np.vstack([x, y]), csd_method=method)
    c_t = np.abs(signaltools.resample(c_t, c.shape[-1]))

    npt.assert_array_almost_equal(c[0, 1], c_t, 2)


def test_coherence_matlab():

    """ Test against coherence values calculated with matlab's mscohere"""

    ts = np.loadtxt(os.path.join(test_dir_path, 'tseries12.txt'))

    ts0 = ts[1]
    ts1 = ts[0]

    method = {}
    method['this_method'] = 'welch'
    method['NFFT'] = 64
    method['Fs'] = 1.0
    method['noverlap'] = method['NFFT'] / 2

    ttt = np.vstack([ts0, ts1])
    f, cxy_mlab = tsa.coherence(ttt, csd_method=method)
    cxy_matlab = np.loadtxt(os.path.join(test_dir_path, 'cxy_matlab.txt'))

    npt.assert_almost_equal(cxy_mlab[0][1], cxy_matlab, decimal=5)

@npt.dec.skipif(old_mpl)
def test_cached_coherence():
    """Testing the cached coherence functions """
    NFFT = 64  # This is the default behavior
    n_freqs = NFFT // 2 + 1
    ij = [(0, 1), (1, 0)]
    ts = np.loadtxt(os.path.join(test_dir_path, 'tseries12.txt'))
    freqs, cache = tsa.cache_fft(ts, ij)

    # Are the frequencies the right ones?
    npt.assert_equal(freqs, utils.get_freqs(2 * np.pi, NFFT))

    # Check that the fft of the first window is what we expect:
    hann = mlab.window_hanning(np.ones(NFFT))
    w_ts = ts[0][:NFFT] * hann
    w_ft = fftpack.fft(w_ts)[0:n_freqs]

    # This is the result of the function:
    first_window_fft = cache['FFT_slices'][0][0]

    npt.assert_equal(w_ft, first_window_fft)

    coh_cached = tsa.cache_to_coherency(cache, ij)[0, 1]
    f, c = tsa.coherency(ts)
    coh_direct = c[0, 1]

    npt.assert_almost_equal(coh_direct, coh_cached)

    # Only welch PSD works and an error is thrown otherwise. This tests that
    # the error is thrown:
    npt.assert_raises(ValueError, tsa.cache_fft, ts, ij, method=methods[2])

    # Take the method in which the window is defined on input:
    freqs, cache1 = tsa.cache_fft(ts, ij, method=methods[4])
    # And compare it to the method in which it isn't:
    freqs, cache2 = tsa.cache_fft(ts, ij, method=methods[1])
    npt.assert_equal(cache1, cache2)

    # Do the same, while setting scale_by_freq to False:
    freqs, cache1 = tsa.cache_fft(ts, ij, method=methods[4],
                                  scale_by_freq=False)
    freqs, cache2 = tsa.cache_fft(ts, ij, method=methods[1],
                                  scale_by_freq=False)
    npt.assert_equal(cache1, cache2)

    # Test cache_to_psd:
    psd1 = tsa.cache_to_psd(cache, ij)[0]
    # Against the standard get_spectra:
    f, c = tsa.get_spectra(ts)
    psd2 = c[0][0]

    npt.assert_almost_equal(psd1, psd2)

    # Test that prefer_speed_over_memory doesn't change anything:
    freqs, cache1 = tsa.cache_fft(ts, ij)
    freqs, cache2 = tsa.cache_fft(ts, ij, prefer_speed_over_memory=True)
    psd1 = tsa.cache_to_psd(cache1, ij)[0]
    psd2 = tsa.cache_to_psd(cache2, ij)[0]
    npt.assert_almost_equal(psd1, psd2)


# XXX This is not testing anything substantial for now - I am not sure what to
# test here...
def test_cache_to_phase():
    """
    Test phase calculations from cached windowed FFT

    """
    ij = [(0, 1), (1, 0)]
    x = np.sin(t) + np.sin(2 * t) + np.sin(3 * t) + np.random.rand(t.shape[-1])
    y = np.sin(t) + np.sin(2 * t) + np.sin(3 * t) + np.random.rand(t.shape[-1])
    ts = np.vstack([x, y])
    freqs, cache = tsa.cache_fft(ts, ij)
    ph = tsa.cache_to_phase(cache, ij)


def test_cache_to_coherency():
    """

    Test cache_to_coherency against the standard coherency calculation

    """
    ij = [(0, 1), (1, 0)]
    ts = np.loadtxt(os.path.join(test_dir_path, 'tseries12.txt'))
    freqs, cache = tsa.cache_fft(ts, ij)
    Cxy = tsa.cache_to_coherency(cache, ij)
    f, c = tsa.coherency(ts)
    npt.assert_almost_equal(Cxy[0][1], c[0, 1])

    # Check that it doesn't matter if you prefer_speed_over_memory:
    freqs, cache2 = tsa.cache_fft(ts, ij, prefer_speed_over_memory=True)
    Cxy2 = tsa.cache_to_coherency(cache2, ij)

    npt.assert_equal(Cxy2, Cxy)

    # XXX Calculating the angle of the averaged psd and calculating the average
    # of the angles calculated over different windows does not yield exactly
    # the same number, because the angle is not a linear functions (arctan),
    # so it is unclear how to test this, but we make sure that it runs,
    # whether or not you prefer_speed_over_memory:
    freqs, cache = tsa.cache_fft(ts, ij)
    tsa.cache_to_relative_phase(cache, ij)

    freqs, cache = tsa.cache_fft(ts, ij, prefer_speed_over_memory=True)
    tsa.cache_to_relative_phase(cache, ij)

    # Check that things run alright, even if there is just one window for the
    # entire ts:
    freqs, cache = tsa.cache_fft(ts, ij, method=dict(this_method='welch',
                                                   NFFT=ts.shape[-1],
                                                   n_overlap=0))

    cxy_one_window = tsa.cache_to_coherency(cache, ij)
    ph_one_window = tsa.cache_to_relative_phase(cache, ij)

    # And whether or not you prefer_speed_over_memory
    freqs, cache = tsa.cache_fft(ts, ij, method=dict(this_method='welch',
                                                   NFFT=ts.shape[-1],
                                                   n_overlap=0),
                                prefer_speed_over_memory=True)

    cxy_one_window = tsa.cache_to_coherency(cache, ij)
    ph_one_window = tsa.cache_to_relative_phase(cache, ij)

########NEW FILE########
__FILENAME__ = test_correlation
import numpy as np
import numpy.testing as npt

import nitime.algorithms as tsa

def test_seed_correlation():

    seed = np.random.rand(10)
    targ = np.random.rand(10, 10)

    our_coef_array = tsa.seed_corrcoef(seed, targ)
    np_coef_array = np.array([np.corrcoef(seed, a)[0, 1] for a in  targ])

    npt.assert_array_almost_equal(our_coef_array, np_coef_array)

########NEW FILE########
__FILENAME__ = test_event_related
import numpy as np
import numpy.testing as npt
import nitime
import nitime.algorithms as tsa


def test_xcorr_zscored():
    """

    Test this function, which is not otherwise tested in the testing of the
    EventRelatedAnalyzer

    """

    cycles = 10
    l = 1024
    unit = 2 * np.pi / l
    t = np.arange(0, 2 * np.pi + unit, unit)
    signal = np.sin(cycles * t)
    events = np.zeros(t.shape)
    #Zero crossings:
    idx = np.where(np.abs(signal) < 0.03)[0]
    #An event occurs at the beginning of every cycle:
    events[idx[:-2:2]] = 1

    a = tsa.freq_domain_xcorr_zscored(signal, events, 1000, 1000)
    npt.assert_almost_equal(np.mean(a), 0, 1)
    npt.assert_almost_equal(np.std(a), 1, 1)

########NEW FILE########
__FILENAME__ = test_spectral
"""
Tests for the algorithms.spectral submodule

"""

import numpy as np
import scipy
from scipy import fftpack
import numpy.testing as npt
import numpy.testing.decorators as dec
import nose.tools as nt

import nitime.algorithms as tsa
import nitime.utils as utils


def test_get_spectra():
    """

    Testing spectral estimation

    """

    methods = (None,
           {"this_method": 'welch', "NFFT": 256, "Fs": 2 * np.pi},
           {"this_method": 'welch', "NFFT": 1024, "Fs": 2 * np.pi})

    for method in methods:
        avg_pwr1 = []
        avg_pwr2 = []
        est_pwr1 = []
        est_pwr2 = []
        arsig1, _, _ = utils.ar_generator(N=2 ** 16)  # needs to be that long
                                                  # for the answers to converge
        arsig2, _, _ = utils.ar_generator(N=2 ** 16)

        avg_pwr1.append((arsig1 ** 2).mean())
        avg_pwr2.append((arsig2 ** 2).mean())

        tseries = np.vstack([arsig1, arsig2])

        f, c = tsa.get_spectra(tseries, method=method)

        # \sum_{\omega} psd d\omega:
        est_pwr1.append(np.sum(c[0, 0]) * (f[1] - f[0]))
        est_pwr2.append(np.sum(c[1, 1]) * (f[1] - f[0]))

        # Get it right within the order of magnitude:
        npt.assert_array_almost_equal(est_pwr1, avg_pwr1, decimal=-1)
        npt.assert_array_almost_equal(est_pwr2, avg_pwr2, decimal=-1)


def test_get_spectra_complex():
    """

    Testing spectral estimation

    """

    methods = (None,
           {"this_method": 'welch', "NFFT": 256, "Fs": 2 * np.pi},
           {"this_method": 'welch', "NFFT": 1024, "Fs": 2 * np.pi})

    for method in methods:
        avg_pwr1 = []
        avg_pwr2 = []
        est_pwr1 = []
        est_pwr2 = []

        # Make complex signals:
        r, _, _ = utils.ar_generator(N=2 ** 16)  # It needs to be that long for
                                                 # the answers to converge
        c, _, _ = utils.ar_generator(N=2 ** 16)
        arsig1 = r + c * scipy.sqrt(-1)

        r, _, _ = utils.ar_generator(N=2 ** 16)
        c, _, _ = utils.ar_generator(N=2 ** 16)

        arsig2 = r + c * scipy.sqrt(-1)
        avg_pwr1.append((arsig1 * arsig1.conjugate()).mean())
        avg_pwr2.append((arsig2 * arsig2.conjugate()).mean())

        tseries = np.vstack([arsig1, arsig2])

        f, c = tsa.get_spectra(tseries, method=method)

        # \sum_{\omega} psd d\omega:
        est_pwr1.append(np.sum(c[0, 0]) * (f[1] - f[0]))
        est_pwr2.append(np.sum(c[1, 1]) * (f[1] - f[0]))

        # Get it right within the order of magnitude:
        npt.assert_array_almost_equal(est_pwr1, avg_pwr1, decimal=-1)
        npt.assert_array_almost_equal(est_pwr2, avg_pwr2, decimal=-1)


def test_get_spectra_unknown_method():
    """
    Test that providing an unknown method to get_spectra rasies a ValueError

    """
    tseries = np.array([[1, 2, 3], [4, 5, 6]])
    npt.assert_raises(ValueError,
                    tsa.get_spectra, tseries, method=dict(this_method='foo'))


def test_periodogram():
    """Test some of the inputs to periodogram """

    arsig, _, _ = utils.ar_generator(N=1024)
    Sk = fftpack.fft(arsig)

    f1, c1 = tsa.periodogram(arsig)
    f2, c2 = tsa.periodogram(arsig, Sk=Sk)

    npt.assert_equal(c1, c2)

    # Check that providing a complex signal does the right thing
    # (i.e. two-sided spectrum):
    N = 1024
    r, _, _ = utils.ar_generator(N=N)
    c, _, _ = utils.ar_generator(N=N)
    arsig = r + c * scipy.sqrt(-1)

    f, c = tsa.periodogram(arsig)
    npt.assert_equal(f.shape[0], N)  # Should be N, not the one-sided N/2 + 1


def test_periodogram_csd():
    """Test corner cases of  periodogram_csd"""

    arsig1, _, _ = utils.ar_generator(N=1024)
    arsig2, _, _ = utils.ar_generator(N=1024)

    tseries = np.vstack([arsig1, arsig2])

    Sk = fftpack.fft(tseries)

    f1, c1 = tsa.periodogram_csd(tseries)
    f2, c2 = tsa.periodogram_csd(tseries, Sk=Sk)
    npt.assert_equal(c1, c2)

    # Check that providing a complex signal does the right thing
    # (i.e. two-sided spectrum):
    N = 1024
    r, _, _ = utils.ar_generator(N=N)
    c, _, _ = utils.ar_generator(N=N)
    arsig1 = r + c * scipy.sqrt(-1)

    r, _, _ = utils.ar_generator(N=N)
    c, _, _ = utils.ar_generator(N=N)
    arsig2 = r + c * scipy.sqrt(-1)

    tseries = np.vstack([arsig1, arsig2])

    f, c = tsa.periodogram_csd(tseries)
    npt.assert_equal(f.shape[0], N)  # Should be N, not the one-sided N/2 + 1


def test_dpss_windows():
    """ Test a funky corner case of DPSS_windows """

    N = 1024
    NW = 0  # Setting NW to 0 triggers the weird corner case in which some of
            # the symmetric tapers have a negative average
    Kmax = 7

    # But that's corrected by the algorithm:
    d, w = tsa.dpss_windows(1024, 0, 7)
    for this_d in d[0::2]:
        npt.assert_equal(this_d.sum(axis=-1) < 0, False)

def test_dpss_properties():
    """ Test conventions of Slepian eigenvectors """

    N = 2000
    NW = 200
    d, lam = tsa.dpss_windows(N, NW, 2*NW-2)
    # 2NW-2 lamdas should be all > 0.9
    nt.assert_true(
        (lam > 0.9).all(), 'Eigenvectors show poor spectral concentration'
        )
    # test orthonomality
    err = np.linalg.norm(d.dot(d.T) - np.eye(2*NW-2), ord='fro')
    nt.assert_true(err**2 < 1e-16, 'Eigenvectors not numerically orthonormal')
    # test positivity of even functions
    nt.assert_true(
        (d[::2].sum(axis=1) > 0).all(),
        'Even Slepian sequences should have positive DC'
        )
    # test positive initial slope of odd functions
    # (this tests the sign of a linear slope)
    pk = np.argmax(np.abs(d[1::2, :N/2]), axis=1)
    t = True
    for p, f in zip(pk, d[1::2]):
        t = t and np.sum( np.arange(1,p+1) * f[:p] ) >= 0
    nt.assert_true(t, 'Odd Slepians should begin positive-going')

def test_get_spectra_bi():
    """

    Test the bi-variate get_spectra function

    """

    methods = (None,
           {"this_method": 'welch', "NFFT": 256, "Fs": 2 * np.pi},
           {"this_method": 'welch', "NFFT": 1024, "Fs": 2 * np.pi})

    for method in methods:
        arsig1, _, _ = utils.ar_generator(N=2 ** 16)
        arsig2, _, _ = utils.ar_generator(N=2 ** 16)

        avg_pwr1 = (arsig1 ** 2).mean()
        avg_pwr2 = (arsig2 ** 2).mean()
        avg_xpwr = (arsig1 * arsig2.conjugate()).mean()

        tseries = np.vstack([arsig1, arsig2])

        f, fxx, fyy, fxy = tsa.get_spectra_bi(arsig1, arsig2, method=method)

        # \sum_{\omega} PSD(\omega) d\omega:
        est_pwr1 = np.sum(fxx * (f[1] - f[0]))
        est_pwr2 = np.sum(fyy * (f[1] - f[0]))
        est_xpwr = np.sum(fxy * (f[1] - f[0])).real

        # Test that we have the right order of magnitude:
        npt.assert_array_almost_equal(est_pwr1, avg_pwr1, decimal=-1)
        npt.assert_array_almost_equal(est_pwr2, avg_pwr2, decimal=-1)
        npt.assert_array_almost_equal(np.mean(est_xpwr),
                                      np.mean(avg_xpwr),
                                      decimal=-1)


def test_mtm_lin_combo():
    "Test the functionality of cross and autospectrum MTM combinations"
    spec1 = np.random.randn(5, 100) + 1j * np.random.randn(5, 100)
    spec2 = np.random.randn(5, 100) + 1j * np.random.randn(5, 100)
    # test on both broadcasted weights and per-point weights
    for wshape in ((2, 5, 1), (2, 5, 100)):
        weights = np.random.randn(*wshape)
        sides = 'onesided'
        mtm_cross = tsa.mtm_cross_spectrum(
            spec1, spec2, (weights[0], weights[1]), sides=sides
            )
        nt.assert_true(mtm_cross.dtype in np.sctypes['complex'],
               'Wrong dtype for crossspectrum')
        nt.assert_true(len(mtm_cross) == 51,
               'Wrong length for halfband spectrum')
        sides = 'twosided'
        mtm_cross = tsa.mtm_cross_spectrum(
            spec1, spec2, (weights[0], weights[1]), sides=sides
            )
        nt.assert_true(len(mtm_cross) == 100,
               'Wrong length for fullband spectrum')
        sides = 'onesided'
        mtm_auto = tsa.mtm_cross_spectrum(
            spec1, spec1, weights[0], sides=sides
            )
        nt.assert_true(mtm_auto.dtype in np.sctypes['float'],
               'Wrong dtype for autospectrum')
        nt.assert_true(len(mtm_auto) == 51,
               'Wrong length for halfband spectrum')
        sides = 'twosided'
        mtm_auto = tsa.mtm_cross_spectrum(
            spec1, spec2, weights[0], sides=sides
            )
        nt.assert_true(len(mtm_auto) == 100,
               'Wrong length for fullband spectrum')


def test_mtm_cross_spectrum():
    """

    Test the multi-taper cross-spectral estimation. Based on the example in
    doc/examples/multi_taper_coh.py

    """
    NW = 4
    K = 2 * NW - 1

    N = 2 ** 10
    n_reps = 10
    n_freqs = N

    tapers, eigs = tsa.dpss_windows(N, NW, 2 * NW - 1)

    est_psd = []
    for k in range(n_reps):
        data, nz, alpha = utils.ar_generator(N=N)
        fgrid, hz = tsa.freq_response(1.0, a=np.r_[1, -alpha], n_freqs=n_freqs)
        # 'one-sided', so multiply by 2:
        psd = 2 * (hz * hz.conj()).real

        tdata = tapers * data

        tspectra = fftpack.fft(tdata)

        L = N / 2 + 1
        sides = 'onesided'
        w, _ = utils.adaptive_weights(tspectra, eigs, sides=sides)

        sxx = tsa.mtm_cross_spectrum(tspectra, tspectra, w, sides=sides)
        est_psd.append(sxx)

    fxx = np.mean(est_psd, 0)

    psd_ratio = np.mean(fxx / psd)

    # This is a rather lenient test, making sure that the average ratio is 1 to
    # within an order of magnitude. That is, that they are equal on average:
    npt.assert_array_almost_equal(psd_ratio, 1, decimal=1)

    # Test raising of error in case the inputs don't make sense:
    npt.assert_raises(ValueError,
                      tsa.mtm_cross_spectrum,
                      tspectra, np.r_[tspectra, tspectra],
                      (w, w))


@dec.slow
def test_multi_taper_psd_csd():
    """

    Test the multi taper psd and csd estimation functions.
    Based on the example in
    doc/examples/multi_taper_spectral_estimation.py

    """

    N = 2 ** 10
    n_reps = 10

    psd = []
    est_psd = []
    est_csd = []
    for jk in [True, False]:
        for k in range(n_reps):
            for adaptive in [True, False]:
                ar_seq, nz, alpha = utils.ar_generator(N=N, drop_transients=10)
                ar_seq -= ar_seq.mean()
                fgrid, hz = tsa.freq_response(1.0, a=np.r_[1, -alpha],
                                              n_freqs=N)
                psd.append(2 * (hz * hz.conj()).real)
                f, psd_mt, nu = tsa.multi_taper_psd(ar_seq, adaptive=adaptive,
                                                    jackknife=jk)
                est_psd.append(psd_mt)
                f, csd_mt = tsa.multi_taper_csd(np.vstack([ar_seq, ar_seq]),
                                               adaptive=adaptive)
                # Symmetrical in this case, so take one element out:
                est_csd.append(csd_mt[0][1])

        fxx = np.mean(psd, axis=0)
        fxx_est1 = np.mean(est_psd, axis=0)
        fxx_est2 = np.mean(est_csd, axis=0)

        # Tests the psd:
        psd_ratio1 = np.mean(fxx_est1 / fxx)
        npt.assert_array_almost_equal(psd_ratio1, 1, decimal=-1)
        # Tests the csd:
        psd_ratio2 = np.mean(fxx_est2 / fxx)
        npt.assert_array_almost_equal(psd_ratio2, 1, decimal=-1)


def test_gh57():
    """
    https://github.com/nipy/nitime/issues/57
    """
    data = np.random.randn(10, 1000)
    for jk in [True, False]:
        for adaptive in [True, False]:
            f, psd, sigma = tsa.multi_taper_psd(data, adaptive=adaptive,
                                                jackknife=jk)


def test_hermitian_periodogram_csd():
    """
    Make sure CSD matrices returned by various methods have
    Hermitian symmetry.
    """

    sig = np.random.randn(4,256)

    _, csd1 = tsa.periodogram_csd(sig)

    for i in range(4):
        for j in range(i+1):
            xc1 = csd1[i,j]
            xc2 = csd1[j,i]
            npt.assert_equal(
                xc1, xc2.conj(), err_msg='Periodogram CSD not Hermitian'
                )

    _, psd = tsa.periodogram(sig)
    for i in range(4):
        npt.assert_almost_equal(
            psd[i], csd1[i,i].real,
            err_msg='Periodgram CSD diagonal inconsistent with real PSD'
            )

def test_hermitian_multitaper_csd():
    """
    Make sure CSD matrices returned by various methods have
    Hermitian symmetry.
    """

    sig = np.random.randn(4,256)

    _, csd1 = tsa.multi_taper_csd(sig, adaptive=False)

    for i in range(4):
        for j in range(i+1):
            xc1 = csd1[i,j]
            xc2 = csd1[j,i]
            npt.assert_equal(
                xc1, xc2.conj(), err_msg='MTM CSD not Hermitian'
                )

    _, psd, _ = tsa.multi_taper_psd(sig, adaptive=False)
    for i in range(4):
        npt.assert_almost_equal(
            psd[i], csd1[i,i].real,
            err_msg='MTM CSD diagonal inconsistent with real PSD'
            )

def test_periodogram_spectral_normalization():
    """
    Check that the spectral estimators are normalized in the
    correct Watts/Hz fashion
    """

    x = np.random.randn(1024)
    f1, Xp1 = tsa.periodogram(x)
    f2, Xp2 = tsa.periodogram(x, Fs=100)
    f3, Xp3 = tsa.periodogram(x, N=2**12)

    p1 = np.sum(Xp1) * 2 * np.pi / 2**10
    p2 = np.sum(Xp2) * 100 / 2**10
    p3 = np.sum(Xp3) * 2 * np.pi / 2**12
    nt.assert_true( np.abs(p1 - p2) < 1e-14,
                    'Inconsistent frequency normalization in periodogram (1)' )
    nt.assert_true( np.abs(p3 - p2) < 1e-8,
                    'Inconsistent frequency normalization in periodogram (2)' )

    td_var = np.var(x)
    # assure that the estimators are at least in the same
    # order of magnitude as the time-domain variance
    nt.assert_true( np.abs(np.log10(p1/td_var)) < 1,
                    'Incorrect frequency normalization in periodogram' )

    # check the freq vector while we're here
    nt.assert_true( f2.max() == 50, 'Periodogram returns wrong frequency bins' )

def test_multitaper_spectral_normalization():
    """
    Check that the spectral estimators are normalized in the
    correct Watts/Hz fashion
    """

    x = np.random.randn(1024)
    f1, Xp1, _ = tsa.multi_taper_psd(x)
    f2, Xp2, _ = tsa.multi_taper_psd(x, Fs=100)
    f3, Xp3, _ = tsa.multi_taper_psd(x, NFFT=2**12)

    p1 = np.sum(Xp1) * 2 * np.pi / 2**10
    p2 = np.sum(Xp2) * 100 / 2**10
    p3 = np.sum(Xp3) * 2 * np.pi / 2**12
    nt.assert_true( np.abs(p1 - p2) < 1e-14,
                    'Inconsistent frequency normalization in MTM PSD (1)' )
    nt.assert_true( np.abs(p3 - p2) < 1e-8,
                    'Inconsistent frequency normalization in MTM PSD (2)' )

    td_var = np.var(x)
    # assure that the estimators are at least in the same
    # order of magnitude as the time-domain variance
    nt.assert_true( np.abs(np.log10(p1/td_var)) < 1,
                    'Incorrect frequency normalization in MTM PSD' )

    # check the freq vector while we're here
    nt.assert_true( f2.max() == 50, 'MTM PSD returns wrong frequency bins' )

########NEW FILE########
__FILENAME__ = wavelet
"""

Wavelets

"""

import numpy as np
from nitime.lazy import scipy_fftpack as fftpack


def wfmorlet_fft(f0, sd, sampling_rate, ns=5, nt=None):
    """
    returns a complex morlet wavelet in the frequency domain

    Parameters
    ----------
        f0 : center frequency
        sd : standard deviation of center frequency
        sampling_rate : samplingrate
        ns : window length in number of standard deviations
        nt : window length in number of sample points
    """
    if nt == None:
        st = 1. / (2. * np.pi * sd)
        nt = 2 * int(ns * st * sampling_rate) + 1
    f = fftpack.fftfreq(nt, 1. / sampling_rate)
    wf = 2 * np.exp(-(f - f0) ** 2 / (2 * sd ** 2)) * np.sqrt(sampling_rate /
                                                    (np.sqrt(np.pi) * sd))
    wf[f < 0] = 0
    wf[f == 0] /= 2
    return wf


def wmorlet(f0, sd, sampling_rate, ns=5, normed='area'):
    """
    returns a complex morlet wavelet in the time domain

    Parameters
    ----------
        f0 : center frequency
        sd : standard deviation of frequency
        sampling_rate : samplingrate
        ns : window length in number of standard deviations
    """
    st = 1. / (2. * np.pi * sd)
    w_sz = float(int(ns * st * sampling_rate))  # half time window size
    t = np.arange(-w_sz, w_sz + 1, dtype=float) / sampling_rate
    if normed == 'area':
        w = np.exp(-t ** 2 / (2. * st ** 2)) * np.exp(
            2j * np.pi * f0 * t) / np.sqrt(np.sqrt(np.pi) * st * sampling_rate)
    elif normed == 'max':
        w = np.exp(-t ** 2 / (2. * st ** 2)) * np.exp(
            2j * np.pi * f0 * t) * 2 * sd * np.sqrt(2 * np.pi) / sampling_rate
    else:
        assert 0, 'unknown norm %s' % normed
    return w


def wlogmorlet_fft(f0, sd, sampling_rate, ns=5, nt=None):
    """
    returns a complex log morlet wavelet in the frequency domain

    Parameters
    ----------
        f0 : center frequency
        sd : standard deviation
        sampling_rate : samplingrate
        ns : window length in number of standard deviations
        nt : window length in number of sample points
    """
    if nt == None:
        st = 1. / (2. * np.pi * sd)
        nt = 2 * int(ns * st * sampling_rate) + 1
    f = fftpack.fftfreq(nt, 1. / sampling_rate)

    sfl = np.log(1 + 1. * sd / f0)
    wf = (2 * np.exp(-(np.log(f) - np.log(f0)) ** 2 / (2 * sfl ** 2)) *
          np.sqrt(sampling_rate / (np.sqrt(np.pi) * sd)))
    wf[f < 0] = 0
    wf[f == 0] /= 2
    return wf


def wlogmorlet(f0, sd, sampling_rate, ns=5, normed='area'):
    """
    returns a complex log morlet wavelet in the time domain

    Parameters
    ----------
        f0 : center frequency
        sd : standard deviation of frequency
        sampling_rate : samplingrate
        ns : window length in number of standard deviations
    """
    st = 1. / (2. * np.pi * sd)
    w_sz = int(ns * st * sampling_rate)  # half time window size
    wf = wlogmorlet_fft(f0, sd, sampling_rate=sampling_rate, nt=2 * w_sz + 1)
    w = fftpack.fftshift(fftpack.ifft(wf))
    if normed == 'area':
        w /= w.real.sum()
    elif normed == 'max':
        w /= w.real.max()
    elif normed == 'energy':
        w /= np.sqrt((w ** 2).sum())
    else:
        assert 0, 'unknown norm %s' % normed
    return w

########NEW FILE########
__FILENAME__ = base

from inspect import getargspec

from nitime import descriptors as desc


class BaseAnalyzer(desc.ResetMixin):
    """
    Analyzer that implements the default data flow.

    All analyzers inherit from this class at least have to
    * implement a __init__ function to set parameters
    * define the 'output' property

    """

    @desc.setattr_on_read
    def parameterlist(self):
        plist = getargspec(self.__init__).args
        plist.remove('self')
        plist.remove('input')
        return plist

    @property
    def parameters(self):
        return dict([(p,
                    getattr(self, p, 'MISSING')) for p in self.parameterlist])

    def __init__(self, input=None):
        self.input = input

    def set_input(self, input):
        """Set the input of the analyzer, if you want to reuse the analyzer
        with a different input than the original """

        self.reset()
        self.input = input

    def __repr__(self):
        params = ', '.join(['%s=%r' % (p, getattr(self, p, 'MISSING'))
                            for p in self.parameterlist])

        return '%s(%s)' % (self.__class__.__name__, params)

########NEW FILE########
__FILENAME__ = coherence
import warnings

import numpy as np
from nitime.lazy import scipy_stats_distributions as dist
from nitime.lazy import scipy_fftpack as fftpack

from nitime import descriptors as desc
from nitime import utils as tsu
from nitime import algorithms as tsa

# To suppport older versions of numpy that don't have tril_indices:
from nitime.index_utils import tril_indices, triu_indices

from .base import BaseAnalyzer


class CoherenceAnalyzer(BaseAnalyzer):
    """Analyzer object for coherence/coherency analysis """

    def __init__(self, input=None, method=None, unwrap_phases=False):
        """

        Parameters
        ----------

        input: TimeSeries object
           Containing the data to analyze.

        method: dict, optional,
            This is the method used for spectral analysis of the signal for the
            coherence caclulation. See :func:`algorithms.get_spectra`
            documentation for details.

        unwrap_phases: bool, optional
           Whether to unwrap the phases. This should be True if you assume that
           the time-delay is the same for all the frequency bands. See
           _[Sun2005] for details. Default : False

        Examples
        --------
        >>> import nitime.timeseries as ts
        >>> np.set_printoptions(precision=4)  # for doctesting
        >>> t1 = ts.TimeSeries(data = np.arange(0,1024,1).reshape(2,512),
        ...                                 sampling_rate=np.pi)
        >>> c1 = CoherenceAnalyzer(t1)
        >>> c1.method['Fs'] # doctest: +ELLIPSIS
        3.1415926535... Hz
        >>> c1.method['this_method']
        'welch'
        >>> c1.coherence[0,1]
        array([ 0.9024,  0.9027,  0.9652,  0.9433,  0.9297,  0.9213,  0.9161,
                0.9126,  0.9102,  0.9085,  0.9072,  0.9063,  0.9055,  0.905 ,
                0.9045,  0.9041,  0.9038,  0.9036,  0.9034,  0.9032,  0.9031,
                0.9029,  0.9028,  0.9027,  0.9027,  0.9026,  0.9026,  0.9025,
                0.9025,  0.9025,  0.9025,  0.9026,  1.    ])
        >>> c1.phase[0,1]
        array([ 0.    , -0.035 , -0.4839, -0.4073, -0.3373, -0.2828, -0.241 ,
               -0.2085, -0.1826, -0.1615, -0.144 , -0.1292, -0.1164, -0.1054,
               -0.0956, -0.0869, -0.0791, -0.072 , -0.0656, -0.0596, -0.0541,
               -0.0489, -0.0441, -0.0396, -0.0353, -0.0314, -0.0277, -0.0244,
               -0.0216, -0.0197, -0.0198, -0.028 ,  0.    ])

        """
        BaseAnalyzer.__init__(self, input)

        # Set the variables for spectral estimation (can also be entered by
        # user):
        if method is None:
            self.method = {'this_method': 'welch',
                           'Fs': self.input.sampling_rate}
        else:
            self.method = method

        # If an input is provided, get the sampling rate from there, if you
        # want to over-ride that, input a method with a 'Fs' field specified:
        self.method['Fs'] = self.method.get('Fs', self.input.sampling_rate)

        self._unwrap_phases = unwrap_phases

        # The following only applies to the welch method:
        if (self.method.get('this_method') == 'welch' or
            self.method.get('this_method') is None):

            # If the input is shorter than NFFT, all the coherences will be
            # 1 per definition. Throw a warning about that:
            self.method['NFFT'] = self.method.get('NFFT', tsa.default_nfft)
            self.method['n_overlap'] = self.method.get('n_overlap',
                                                       tsa.default_n_overlap)
            if (self.input.shape[-1] <
                            (self.method['NFFT'] + self.method['n_overlap'])):
                e_s = "In nitime.analysis, the provided input time-series is"
                e_s += " shorter than the requested NFFT + n_overlap. All "
                e_s += "coherence values will be set to 1."
                warnings.warn(e_s, RuntimeWarning)

    @desc.setattr_on_read
    def coherency(self):
        """The standard output for this kind of analyzer is the coherency """
        data = self.input.data
        tseries_length = data.shape[0]
        spectrum_length = self.spectrum.shape[-1]

        coherency = np.zeros((tseries_length,
                              tseries_length,
                              spectrum_length), dtype=complex)

        for i in range(tseries_length):
            for j in range(i, tseries_length):
                coherency[i][j] = tsa.coherency_spec(self.spectrum[i][j],
                                                     self.spectrum[i][i],
                                                     self.spectrum[j][j])

        idx = tril_indices(tseries_length, -1)
        coherency[idx[0], idx[1], ...] = coherency[idx[1], idx[0], ...].conj()

        return coherency

    @desc.setattr_on_read
    def spectrum(self):
        """
        The spectra of each of the channels and cross-spectra between
        different channles  in the input TimeSeries object
        """
        f, spectrum = tsa.get_spectra(self.input.data, method=self.method)
        return spectrum

    @desc.setattr_on_read
    def frequencies(self):
        """
        The central frequencies in the bands
        """

        #XXX Use NFFT in the method in order to calculate these, without having
        #to calculate the spectrum:
        f, spectrum = tsa.get_spectra(self.input.data, method=self.method)
        return f

    @desc.setattr_on_read
    def coherence(self):
        """
        The coherence between the different channels in the input TimeSeries
        object
        """

        #XXX Calculate this from the standard output, instead of recalculating
        #the coherence:

        tseries_length = self.input.data.shape[0]
        spectrum_length = self.spectrum.shape[-1]
        coherence = np.zeros((tseries_length,
                              tseries_length,
                              spectrum_length))

        for i in range(tseries_length):
            for j in range(i, tseries_length):
                coherence[i][j] = tsa.coherence_spec(self.spectrum[i][j],
                                                     self.spectrum[i][i],
                                                     self.spectrum[j][j])

        idx = tril_indices(tseries_length, -1)
        coherence[idx[0], idx[1], ...] = coherence[idx[1], idx[0], ...].conj()

        return coherence

    @desc.setattr_on_read
    def phase(self):
        """ The frequency-dependent phase relationship between all the pairwise
        combinations of time-series in the data"""

        #XXX calcluate this from the standard output, instead of recalculating:

        tseries_length = self.input.data.shape[0]
        spectrum_length = self.spectrum.shape[-1]

        phase = np.zeros((tseries_length,
                            tseries_length,
                            spectrum_length))

        for i in range(tseries_length):
            for j in range(i, tseries_length):
                phase[i][j] = np.angle(
                    self.spectrum[i][j])

                phase[j][i] = np.angle(
                    self.spectrum[i][j].conjugate())
        return phase

    @desc.setattr_on_read
    def delay(self):
        """ The delay in seconds between the two time series """
        p_shape = self.phase.shape[:-1]
        delay = np.zeros(self.phase.shape)
        for i in range(p_shape[0]):
            for j in range(p_shape[1]):
                this_phase = self.phase[i, j]
                #If requested, unwrap the phases:
                if self._unwrap_phases:
                    this_phase = tsu.unwrap_phases(this_phase)

                delay[i, j] = this_phase / (2 * np.pi * self.frequencies)

        return delay

    @desc.setattr_on_read
    def coherence_partial(self):
        """The partial coherence between data[i] and data[j], given data[k], as
        a function of frequency band"""

        tseries_length = self.input.data.shape[0]
        spectrum_length = self.spectrum.shape[-1]

        p_coherence = np.zeros((tseries_length,
                                tseries_length,
                                tseries_length,
                                spectrum_length))

        for i in range(tseries_length):
            for j in range(tseries_length):
                for k in range(tseries_length):
                    if j == k or i == k:
                        pass
                    else:
                        p_coherence[i][j][k] = tsa.coherence_partial_spec(
                            self.spectrum[i][j],
                            self.spectrum[i][i],
                            self.spectrum[j][j],
                            self.spectrum[i][k],
                            self.spectrum[j][k],
                            self.spectrum[k][k])

        idx = tril_indices(tseries_length, -1)
        p_coherence[idx[0], idx[1], ...] =\
                            p_coherence[idx[1], idx[0], ...].conj()

        return p_coherence


class MTCoherenceAnalyzer(BaseAnalyzer):
    """ Analyzer for multi-taper coherence analysis, including jack-knife
    estimate of confidence interval """
    def __init__(self, input=None, bandwidth=None, alpha=0.05, adaptive=True):

        """
        Initializer function for the MTCoherenceAnalyzer

        Parameters
        ----------

        input: TimeSeries object

        bandwidth: float,
           The bandwidth of the windowing function will determine the number
           tapers to use. This parameters represents trade-off between
           frequency resolution (lower main lobe bandwidth for the taper) and
           variance reduction (higher bandwidth and number of averaged
           estimates). Per default will be set to 4 times the fundamental
           frequency, such that NW=4

        alpha: float, default =0.05
            This is the alpha used to construct a confidence interval around
            the multi-taper csd estimate, based on a jack-knife estimate of the
            variance [Thompson2007]_.

        adaptive: bool, default to True
            Whether to set the weights for the tapered spectra according to the
            adaptive algorithm (Thompson, 2007).

        Notes
        -----

        Thompson, DJ (2007) Jackknifing multitaper spectrum estimates. IEEE
        Signal Processing Magazing. 24: 20-30

        """

        BaseAnalyzer.__init__(self, input)

        if input is None:
            self.NW = 4
            self.bandwidth = None
        else:
            N = input.shape[-1]
            Fs = self.input.sampling_rate
            if bandwidth is not None:
                self.NW = bandwidth / (2 * Fs) * N
            else:
                self.NW = 4
                self.bandwidth = self.NW * (2 * Fs) / N

        self.alpha = alpha
        self._L = self.input.data.shape[-1] / 2 + 1
        self._adaptive = adaptive

    @desc.setattr_on_read
    def tapers(self):
        return tsa.dpss_windows(self.input.shape[-1], self.NW,
                                2 * self.NW - 1)[0]

    @desc.setattr_on_read
    def eigs(self):
        return tsa.dpss_windows(self.input.shape[-1], self.NW,
                                      2 * self.NW - 1)[1]

    @desc.setattr_on_read
    def df(self):
        # The degrees of freedom:
        return 2 * self.NW - 1

    @desc.setattr_on_read
    def spectra(self):
        tdata = self.tapers[None, :, :] * self.input.data[:, None, :]
        tspectra = fftpack.fft(tdata)
        return tspectra

    @desc.setattr_on_read
    def weights(self):
        channel_n = self.input.data.shape[0]
        w = np.empty((channel_n, self.df, self._L))

        if self._adaptive:
            for i in range(channel_n):
                # this is always a one-sided spectrum?
                w[i] = tsu.adaptive_weights(self.spectra[i],
                                            self.eigs,
                                            sides='onesided')[0]

        # Set the weights to be the square root of the eigen-values:
        else:
            wshape = [1] * len(self.spectra.shape)
            wshape[0] = channel_n
            wshape[-2] = int(self.df)
            pre_w = np.sqrt(self.eigs) + np.zeros((wshape[0],
                                                    self.eigs.shape[0]))

            w = pre_w.reshape(*wshape)

        return w

    @desc.setattr_on_read
    def coherence(self):
        nrows = self.input.data.shape[0]
        psd_mat = np.zeros((2, nrows, nrows, self._L), 'd')
        coh_mat = np.zeros((nrows, nrows, self._L), 'd')

        for i in range(self.input.data.shape[0]):
            for j in range(i):
                sxy = tsa.mtm_cross_spectrum(self.spectra[i], self.spectra[j],
                                           (self.weights[i], self.weights[j]),
                                           sides='onesided')
                sxx = tsa.mtm_cross_spectrum(self.spectra[i], self.spectra[i],
                                             self.weights[i],
                                             sides='onesided')
                syy = tsa.mtm_cross_spectrum(self.spectra[j], self.spectra[j],
                                             self.weights[i],
                                             sides='onesided')
                psd_mat[0, i, j] = sxx
                psd_mat[1, i, j] = syy
                coh_mat[i, j] = np.abs(sxy) ** 2
                coh_mat[i, j] /= (sxx * syy)

        idx = triu_indices(self.input.data.shape[0], 1)
        coh_mat[idx[0], idx[1], ...] = coh_mat[idx[1], idx[0], ...].conj()

        return coh_mat

    @desc.setattr_on_read
    def confidence_interval(self):
        """The size of the 1-alpha confidence interval"""
        coh_var = np.zeros((self.input.data.shape[0],
                            self.input.data.shape[0],
                            self._L), 'd')
        for i in range(self.input.data.shape[0]):
            for j in range(i):
                if i != j:
                    coh_var[i, j] = tsu.jackknifed_coh_variance(
                        self.spectra[i],
                        self.spectra[j],
                        self.eigs,
                        adaptive=self._adaptive
                        )

        idx = triu_indices(self.input.data.shape[0], 1)
        coh_var[idx[0], idx[1], ...] = coh_var[idx[1], idx[0], ...].conj()

        coh_mat_xform = tsu.normalize_coherence(self.coherence,
                                                2 * self.df - 2)

        lb = coh_mat_xform + dist.t.ppf(self.alpha / 2,
                                        self.df - 1) * np.sqrt(coh_var)
        ub = coh_mat_xform + dist.t.ppf(1 - self.alpha / 2,
                                        self.df - 1) * np.sqrt(coh_var)

        # convert this measure with the normalizing function
        tsu.normal_coherence_to_unit(lb, 2 * self.df - 2, lb)
        tsu.normal_coherence_to_unit(ub, 2 * self.df - 2, ub)

        return ub - lb

    @desc.setattr_on_read
    def frequencies(self):
        return np.linspace(0, self.input.sampling_rate / 2, self._L)


class SparseCoherenceAnalyzer(BaseAnalyzer):
    """
    This analyzer is intended for analysis of large sets of data, in which
    possibly only a subset of combinations of time-series needs to be compared.
    The constructor for this class receives as input not only a time-series
    object, but also a list of tuples with index combinations (i,j) for the
    combinations. Importantly, this class implements only the mlab csd function
    and cannot use other methods of spectral estimation
    """

    def __init__(self, time_series=None, ij=(0, 0), method=None, lb=0, ub=None,
                 prefer_speed_over_memory=True, scale_by_freq=True):
        """The constructor for the SparseCoherenceAnalyzer

        Parameters
        ----------

        time_series: a time-series object

        ij: a list of tuples, each containing a pair of indices.

           The resulting cache will contain the fft of time-series in the rows
           indexed by the unique elements of the union of i and j

        lb,ub: float,optional, default: lb=0, ub=None (max frequency)

            define a frequency band of interest

        prefer_speed_over_memory: Boolean, optional, default=True

            Does exactly what the name implies. If you have enough memory

        method: optional, dict

        The method for spectral estimation (see :func:`algorithms.get_spectra`)

        """

        BaseAnalyzer.__init__(self, time_series)
        #Initialize variables from the time series
        self.ij = ij

        #Set the variables for spectral estimation (can also be entered by
        #user):
        if method is None:
            self.method = {'this_method': 'welch'}

        else:
            self.method = method

        if self.method['this_method'] != 'welch':
            e_s = "For SparseCoherenceAnalyzer, "
            e_s += "spectral estimation method must be welch"
            raise ValueError(e_s)

        self.method['Fs'] = self.method.get('Fs', self.input.sampling_rate)

        #Additional parameters for the coherency estimation:
        self.lb = lb
        self.ub = ub
        self.prefer_speed_over_memory = prefer_speed_over_memory
        self.scale_by_freq = scale_by_freq

    @desc.setattr_on_read
    def coherency(self):
        """ The default behavior is to calculate the cache, extract it and then
        output the coherency"""
        coherency = tsa.cache_to_coherency(self.cache, self.ij)

        return coherency

    @desc.setattr_on_read
    def coherence(self):
        """ The coherence values for the output"""
        coherence = np.abs(self.coherency ** 2)

        return coherence

    @desc.setattr_on_read
    def cache(self):
        """Caches the fft windows required by the other methods of the
        SparseCoherenceAnalyzer. Calculate only once and reuse
        """
        data = self.input.data
        f, cache = tsa.cache_fft(data,
                                self.ij,
                                lb=self.lb,
                                ub=self.ub,
                                method=self.method,
                        prefer_speed_over_memory=self.prefer_speed_over_memory,
                                scale_by_freq=self.scale_by_freq)

        return cache

    @desc.setattr_on_read
    def spectrum(self):
        """get the spectrum for the collection of time-series in this analyzer
        """
        spectrum = tsa.cache_to_psd(self.cache, self.ij)

        return spectrum

    @desc.setattr_on_read
    def phases(self):
        """The frequency-band dependent phases of the spectra of each of the
           time -series i,j in the analyzer"""

        phase = tsa.cache_to_phase(self.cache, self.ij)

        return phase

    @desc.setattr_on_read
    def relative_phases(self):
        """The frequency-band dependent relative phase between the two
        time-series """
        return np.angle(self.coherency)

    @desc.setattr_on_read
    def delay(self):
        """ The delay in seconds between the two time series """
        return self.relative_phases / (2 * np.pi * self.frequencies)

    @desc.setattr_on_read
    def frequencies(self):
        """Get the central frequencies for the frequency bands, given the
           method of estimating the spectrum """

        self.method['Fs'] = self.method.get('Fs', self.input.sampling_rate)
        NFFT = self.method.get('NFFT', 64)
        Fs = self.method.get('Fs')
        freqs = tsu.get_freqs(Fs, NFFT)
        lb_idx, ub_idx = tsu.get_bounds(freqs, self.lb, self.ub)

        return freqs[lb_idx:ub_idx]


class SeedCoherenceAnalyzer(object):
    """
    This analyzer takes two time-series. The first is designated as a
    time-series of seeds. The other is designated as a time-series of targets.
    The analyzer performs a coherence analysis between each of the channels in
    the seed time-series and *all* of the channels in the target time-series.

    Note
    ----

    This is a convenience class, which provides a convenient-to-use interface
    to the SparseCoherenceAnalyzer

    """

    def __init__(self, seed_time_series=None, target_time_series=None,
                 method=None, lb=0, ub=None, prefer_speed_over_memory=True,
                 scale_by_freq=True):

        """

        The constructor for the SeedCoherenceAnalyzer

        Parameters
        ----------

        seed_time_series: a time-series object

        target_time_series: a time-series object

        lb,ub: float,optional, default: lb=0, ub=None (max frequency)

            define a frequency band of interest

        prefer_speed_over_memory: Boolean, optional, default=True

            Makes things go a bit faster, if you have enough memory


        """

        self.seed = seed_time_series
        self.target = target_time_series

        # Check that the seed and the target have the same sampling rate:
        if self.seed.sampling_rate != self.target.sampling_rate:
            e_s = "The sampling rate for the seed time-series and the target"
            e_s += " time-series need to be identical."
            raise ValueError(e_s)

        #Set the variables for spectral estimation (can also be entered by
        #user):
        if method is None:
            self.method = {'this_method': 'welch'}

        else:
            self.method = method

        if ('this_method' in self.method.keys() and
            self.method['this_method'] != 'welch'):
            e_s = "For SparseCoherenceAnalyzer, "
            e_s += "spectral estimation method must be welch"
            raise ValueError(e_s)

        #Additional parameters for the coherency estimation:
        self.lb = lb
        self.ub = ub
        self.prefer_speed_over_memory = prefer_speed_over_memory
        self.scale_by_freq = scale_by_freq

    @desc.setattr_on_read
    def coherence(self):
        """
        The coherence between each of the channels of the seed time series and
        all the channels of the target time-series.

        """
        return np.abs(self.coherency) ** 2

    @desc.setattr_on_read
    def frequencies(self):
        """Get the central frequencies for the frequency bands, given the
           method of estimating the spectrum """

        # Get the sampling rate from the seed time-series:
        self.method['Fs'] = self.method.get('Fs', self.seed.sampling_rate)
        NFFT = self.method.get('NFFT', 64)
        Fs = self.method.get('Fs')
        freqs = tsu.get_freqs(Fs, NFFT)
        lb_idx, ub_idx = tsu.get_bounds(freqs, self.lb, self.ub)

        return freqs[lb_idx:ub_idx]

    @desc.setattr_on_read
    def target_cache(self):
        data = self.target.data

        #Make a cache with all the fft windows for each of the channels in the
        #target.

        #This is the kind of input that cache_fft expects:
        ij = list(zip(np.arange(data.shape[0]), np.arange(data.shape[0])))

        f, cache = tsa.cache_fft(data, ij, lb=self.lb, ub=self.ub,
                                 method=self.method,
                        prefer_speed_over_memory=self.prefer_speed_over_memory,
                        scale_by_freq=self.scale_by_freq)

        return cache

    @desc.setattr_on_read
    def coherency(self):

        #Pre-allocate the final result:
        if len(self.seed.shape) > 1:
            Cxy = np.empty((self.seed.data.shape[0],
                            self.target.data.shape[0],
                            self.frequencies.shape[0]), dtype=np.complex)
        else:
            Cxy = np.empty((self.target.data.shape[0],
                            self.frequencies.shape[0]), dtype=np.complex)

        #Get the fft window cache for the target time-series:
        cache = self.target_cache

        #A list of indices for the target:
        target_chan_idx = np.arange(self.target.data.shape[0])

        #This is a list of indices into the cached fft window libraries,
        #setting the index of the seed to be -1, so that it is easily
        #distinguished from the target indices:
        ij = list(zip(np.ones_like(target_chan_idx) * -1, target_chan_idx))

        #If there is more than one channel in the seed time-series:
        if len(self.seed.shape) > 1:
            for seed_idx, this_seed in enumerate(self.seed.data):
                #Here ij is 0, because it is just one channel and we stack the
                #channel onto itself in order for the input to the function to
                #make sense:
                f, seed_cache = tsa.cache_fft(
                    np.vstack([this_seed, this_seed]),
                    [(0, 0)],
                    lb=self.lb,
                    ub=self.ub,
                    method=self.method,
                    prefer_speed_over_memory=self.prefer_speed_over_memory,
                    scale_by_freq=self.scale_by_freq)

                #Insert the seed_cache into the target_cache:
                cache['FFT_slices'][-1] = seed_cache['FFT_slices'][0]

                #If this is true, the cache contains both FFT_slices and
                #FFT_conj_slices:
                if self.prefer_speed_over_memory:
                    cache['FFT_conj_slices'][-1] = \
                                            seed_cache['FFT_conj_slices'][0]

                #This performs the caclulation for this seed:
                Cxy[seed_idx] = tsa.cache_to_coherency(cache, ij)

        #In the case where there is only one channel in the seed time-series:
        else:
            f, seed_cache = tsa.cache_fft(
                np.vstack([self.seed.data,
                           self.seed.data]),
                [(0, 0)],
                lb=self.lb,
                ub=self.ub,
                method=self.method,
                prefer_speed_over_memory=self.prefer_speed_over_memory,
                scale_by_freq=self.scale_by_freq)

            cache['FFT_slices'][-1] = seed_cache['FFT_slices'][0]

            if self.prefer_speed_over_memory:
                cache['FFT_conj_slices'][-1] = \
                                            seed_cache['FFT_conj_slices'][0]

            Cxy = tsa.cache_to_coherency(cache, ij)

        return Cxy.squeeze()

    @desc.setattr_on_read
    def relative_phases(self):
        """The frequency-band dependent relative phase between the two
        time-series """
        return np.angle(self.coherency)

    @desc.setattr_on_read
    def delay(self):
        """ The delay in seconds between the two time series """
        return self.relative_phases / (2 * np.pi * self.frequencies)

########NEW FILE########
__FILENAME__ = correlation
import numpy as np

from nitime import descriptors as desc
from nitime import timeseries as ts
from nitime import algorithms as tsa

# To suppport older versions of numpy that don't have tril_indices:
from nitime.index_utils import tril_indices

from .base import BaseAnalyzer


class CorrelationAnalyzer(BaseAnalyzer):
    """Analyzer object for correlation analysis. Has the same API as the
    CoherenceAnalyzer"""

    def __init__(self, input=None):
        """
        Parameters
        ----------

        input: TimeSeries object
           Containing the data to analyze.

        Examples
        --------
        >>> np.set_printoptions(precision=4)  # for doctesting
        >>> t1 = ts.TimeSeries(data = np.sin(np.arange(0,
        ...                    10*np.pi,10*np.pi/100)).reshape(2,50),
        ...                                      sampling_rate=np.pi)
        >>> c1 = CorrelationAnalyzer(t1)
        >>> c1 = CorrelationAnalyzer(t1)
        >>> c1.corrcoef
        array([[ 1., -1.],
               [-1.,  1.]])
        >>> c1.xcorr.sampling_rate  # doctest: +ELLIPSIS
        3.141592653... Hz
        >>> c1.xcorr.t0  # doctest: +ELLIPSIS
        -15.91549430915... s

        """

        BaseAnalyzer.__init__(self, input)

    @desc.setattr_on_read
    def corrcoef(self):
        """The correlation coefficient between every pairwise combination of
        time-series contained in the object"""
        return np.corrcoef(self.input.data)

    @desc.setattr_on_read
    def xcorr(self):
        """The cross-correlation between every pairwise combination time-series
        in the object. Uses np.correlation('full').

        Returns
        -------

        TimeSeries: the time-dependent cross-correlation, with zero-lag
        at time=0

        """
        tseries_length = self.input.data.shape[0]
        t_points = self.input.data.shape[-1]
        xcorr = np.zeros((tseries_length,
                          tseries_length,
                          t_points * 2 - 1))
        data = self.input.data
        for i in range(tseries_length):
            data_i = data[i]
            for j in range(i, tseries_length):
                xcorr[i, j] = np.correlate(data_i,
                                          data[j],
                                          mode='full')

        idx = tril_indices(tseries_length, -1)
        xcorr[idx[0], idx[1], ...] = xcorr[idx[1], idx[0], ...]

        return ts.TimeSeries(xcorr,
                             sampling_interval=self.input.sampling_interval,
                             t0=-self.input.sampling_interval * t_points)

    @desc.setattr_on_read
    def xcorr_norm(self):
        """The cross-correlation between every pairwise combination time-series
        in the object, where the zero lag correlation is normalized to be equal
        to the correlation coefficient between the time-series

        Returns
        -------

        TimeSeries: A TimeSeries object
            the time-dependent cross-correlation, with zero-lag at time=0

        """

        tseries_length = self.input.data.shape[0]
        t_points = self.input.data.shape[-1]
        xcorr = np.zeros((tseries_length,
                          tseries_length,
                          t_points * 2 - 1))
        data = self.input.data
        for i in range(tseries_length):
            data_i = data[i]
            for j in range(i, tseries_length):
                xcorr[i, j] = np.correlate(data_i,
                                          data[j],
                                          mode='full')
                xcorr[i, j] /= (xcorr[i, j, t_points])
                xcorr[i, j] *= self.corrcoef[i, j]

        idx = tril_indices(tseries_length, -1)
        xcorr[idx[0], idx[1], ...] = xcorr[idx[1], idx[0], ...]

        return ts.TimeSeries(xcorr,
                             sampling_interval=self.input.sampling_interval,
                             t0=-self.input.sampling_interval * t_points)


class SeedCorrelationAnalyzer(object):
    """
    This analyzer takes two time-series. The first is designated as a
    time-series of seeds. The other is designated as a time-series of targets.
    The analyzer performs a correlation analysis between each of the channels
    in the seed time-series and *all* of the channels in the target
    time-series.

    """
    def __init__(self, seed_time_series=None, target_time_series=None):
        """
        Parameters
        ----------

        seed_time_series: a time-series object

        target_time_series: a time-series object

        """
        self.seed = seed_time_series
        self.target = target_time_series

    @desc.setattr_on_read
    def corrcoef(self):

        #If there is more than one channel in the seed time-series:
        if len(self.seed.shape) > 1:

            # Preallocate results
            Cxy = np.empty((self.seed.data.shape[0],
                            self.target.data.shape[0]), dtype=np.float)

            for seed_idx, this_seed in enumerate(self.seed.data):

                Cxy[seed_idx] = tsa.seed_corrcoef(this_seed, self.target.data)

        #In the case where there is only one channel in the seed time-series:
        else:
            Cxy = tsa.seed_corrcoef(self.seed.data, self.target.data)

        return Cxy.squeeze()

########NEW FILE########
__FILENAME__ = event_related
import numpy as np
from nitime.lazy import scipy_stats as stats

from nitime import descriptors as desc
from nitime import utils as tsu
from nitime import algorithms as tsa
from nitime import timeseries as ts


class EventRelatedAnalyzer(desc.ResetMixin):
    """Analyzer object for reverse-correlation/event-related analysis.

    Note: right now, this class assumes the input time series is only
    two-dimensional.  If your input data is something like
    (nchannels,nsubjects, ...) with more dimensions, things are likely to break
    in hard to understand ways.
    """

    def __init__(self, time_series, events, len_et, zscore=False,
                 correct_baseline=False, offset=0):
        """
        Parameters
        ----------
        time_series: a time-series object
           A time-series with data on which the event-related analysis proceeds

        events_time_series: a TimeSeries object or an Events object

        The events which occured in tandem with the time-series in the
        EventRelatedAnalyzer. This object's data has to have the same
        dimensions as the data in the EventRelatedAnalyzer object. In each
        sample in the time-series, there is an integer, which denotes the kind
        of event which occured at that time. In time-bins in which
        no event occured, a 0 should be entered. The data in this time series
        object needs to have the same dimensionality as the data in the data
        time-series

        len_et: int

        The expected length of the event-triggered quantity (in the same
        time-units as the events are represented (presumably number of TRs, for
        fMRI data). For example, the size of the block dedicated in the
        fir_matrix to each type of event

        zscore: a flag to return the result in zscore (where relevant)

        correct_baseline: a flag to correct the baseline according to the first
        point in the event-triggered average (where possible)

        offset: the offset of the beginning of the event-related time-series,
        relative to the event occurence
        """
        #XXX Change so that the offset and length of the eta can be given in
        #units of time

        #Make sure that the offset and the len_et values can be used, by
        #padding with zeros before and after:

        if  isinstance(events, ts.TimeSeries):
            #Set a flag to indicate the input is a time-series object:
            self._is_ts = True
            s = time_series.data.shape
            e_data = np.copy(events.data)

            #If the input is a one-dimensional (instead of an n-channel
            #dimensional) time-series, we will need to broadcast to make the
            #data assume the same number of dimensions as the time-series
            #input:
            if len(events.shape) == 1 and len(s) > 1:
                e_data = e_data + np.zeros((s[0], 1))

            zeros_before = np.zeros((s[:-1] + (abs(offset),)))
            zeros_after = np.zeros((s[:-1] + (abs(len_et),)))
            time_series_data = np.hstack([zeros_before,
                                          time_series.data,
                                          zeros_after])
            events_data = np.hstack([zeros_before,
                                     e_data,
                                     zeros_after])

            #If the events and the time_series have more than 1-d, the analysis
            #can traverse their first dimension
            if time_series.data.ndim - 1 > 0:
                self._len_h = time_series.data.shape[0]
                self.events = events_data
                self.data = time_series_data
            #Otherwise, in order to extract the array from the first dimension,
            #we wrap it in a list

            else:
                self._len_h = 1
                self.events = [events_data]
                self.data = [time_series_data]

        elif isinstance(events, ts.Events):
            self._is_ts = False
            s = time_series.data.shape
            zeros_before = np.zeros((s[:-1] + (abs(offset),)))
            zeros_after = np.zeros((s[:-1] + (abs(len_et),)))

            #If the time_series has more than 1-d, the analysis can traverse
            #the first dimension
            if time_series.data.ndim - 1 > 0:
                self._len_h = time_series.shape[0]
                self.data = time_series
                self.events = events

            #Otherwise, in order to extract the array from the first dimension,
            #we wrap it in a list
            else:
                self._len_h = 1
                self.data = [time_series]
                #No need to do that for the Events object:
                self.events = events
        else:
            err = ("Input 'events' to EventRelatedAnalyzer must be of type "
                   "Events or of type TimeSeries, %r given" % events)
            raise ValueError(err)

        self.sampling_rate = time_series.sampling_rate
        self.sampling_interval = time_series.sampling_interval
        self.len_et = int(len_et)
        self._zscore = zscore
        self._correct_baseline = correct_baseline
        self.offset = offset
        self.time_unit = time_series.time_unit

    @desc.setattr_on_read
    def FIR(self):
        """Calculate the FIR event-related estimated of the HRFs for different
        kinds of events

        Returns
        -------
        A time-series object, shape[:-2] are dimensions corresponding to the to
        shape[:-2] of the EventRelatedAnalyzer data, shape[-2] corresponds to
        the different kinds of events used (ordered according to the sorted
        order of the unique components in the events time-series). shape[-1]
        corresponds to time, and has length = len_et

        XXX code needs to be changed to use flattening (see 'eta' below)
        """

        #Make a list to put the outputs in:
        h = [0] * self._len_h

        for i in range(self._len_h):
            #XXX Check that the offset makes sense (there can't be an event
            #happening within one offset duration of the beginning of the
            #time-series:

            #Get the design matrix (roll by the offset, in order to get the
            #right thing):

            roll_events = np.roll(self.events[i], self.offset)
            design = tsu.fir_design_matrix(roll_events, self.len_et)
            #Compute the fir estimate, in linear form:
            this_h = tsa.fir(self.data[i], design)
            #Reshape the linear fir estimate into a event_types*hrf_len array
            u = np.unique(self.events[i])
            event_types = u[np.unique(self.events[i]) != 0]
            h[i] = np.reshape(this_h, (event_types.shape[0], self.len_et))

        h = np.array(h).squeeze()

        return ts.TimeSeries(data=h,
                             sampling_rate=self.sampling_rate,
                             t0=self.offset * self.sampling_interval,
                             time_unit=self.time_unit)

    @desc.setattr_on_read
    def FIR_estimate(self):
        """Calculate back the LTI estimate of the time-series, from FIR"""
        raise NotImplementedError

    @desc.setattr_on_read
    def xcorr_eta(self):
        """Compute the normalized cross-correlation estimate of the HRFs for
        different kinds of events

        Returns
        -------

        A time-series object, shape[:-2] are dimensions corresponding to the to
        shape[:-2] of the EventRelatedAnalyzer data, shape[-2] corresponds to
        the different kinds of events used (ordered according to the sorted
        order of the unique components in the events time-series). shape[-1]
        corresponds to time, and has length = len_et (xcorr looks both back
        and forward for half of this length)

        """
        #Make a list to put the outputs in:
        h = [0] * self._len_h

        for i in range(self._len_h):
            data = self.data[i]
            u = np.unique(self.events[i])
            event_types = u[np.unique(self.events[i]) != 0]
            h[i] = np.empty((event_types.shape[0],
                             self.len_et / 2),
                            dtype=complex)
            for e_idx in range(event_types.shape[0]):
                this_e = (self.events[i] == event_types[e_idx]) * 1.0
                if self._zscore:
                    this_h = tsa.freq_domain_xcorr_zscored(data,
                                                this_e,
                                                -self.offset + 1,
                                                self.len_et - self.offset - 2)
                else:
                    this_h = tsa.freq_domain_xcorr(data,
                                                this_e,
                                                -self.offset + 1,
                                                self.len_et - self.offset - 2)
                h[i][e_idx] = this_h

        h = np.array(h).squeeze()

        ## t0 for the object returned here needs to be the central time, not
        ## the first time point, because the functions 'look' back and forth
        ## for len_et bins

        return ts.TimeSeries(data=h,
                             sampling_rate=self.sampling_rate,
                             t0=-1 * self.len_et * self.sampling_interval,
                             time_unit=self.time_unit)

    @desc.setattr_on_read
    def et_data(self):
        """The event-triggered data (all occurences).

        This gets the time-series corresponding to the inidividual event
        occurences. Returns a list of lists of time-series. The first dimension
        is the different channels in the original time-series data and the
        second dimension is each type of event in the event time series

        The time-series itself has the first diemnsion of the data being the
        specific occurence, with time 0 locked to the that occurence
        of the event and the last dimension is time.e

        This complicated structure is so that it can deal with situations where
        each channel has different events and different events have different #
        of occurences
        """
        #Make a list for the output
        h = [0] * self._len_h

        for i in range(self._len_h):
            data = self.data[i]
            u = np.unique(self.events[i])
            event_types = u[np.unique(self.events[i]) != 0]
            #Make a list in here as well:
            this_list = [0] * event_types.shape[0]
            for e_idx in range(event_types.shape[0]):
                idx = np.where(self.events[i] == event_types[e_idx])

                idx_w_len = np.array([idx[0] + count + self.offset for count
                                      in range(self.len_et)])
                event_trig = data[idx_w_len].T
                this_list[e_idx] = ts.TimeSeries(data=event_trig,
                                 sampling_interval=self.sampling_interval,
                                 t0=self.offset * self.sampling_interval,
                                 time_unit=self.time_unit)

            h[i] = this_list

        return h

    @desc.setattr_on_read
    def eta(self):
        """The event-triggered average activity.
        """
        #Make a list for the output
        h = [0] * self._len_h

        if self._is_ts:
            # Loop over channels
            for i in range(self._len_h):
                data = self.data[i]
                u = np.unique(self.events[i])
                event_types = u[np.unique(self.events[i]) != 0]
                h[i] = np.empty((event_types.shape[0], self.len_et),
                                dtype=complex)

                # This offset is used to pull the event indices below, but we
                # have to broadcast it so the shape of the resulting idx+offset
                # operation below gives us the (nevents, len_et) array we want,
                # per channel.
                offset = np.arange(self.offset,
                                   self.offset + self.len_et)[:, np.newaxis]
                # Loop over event types
                for e_idx in range(event_types.shape[0]):
                    idx = np.where(self.events[i] == event_types[e_idx])[0]
                    event_trig = data[idx + offset]
                    #Correct baseline by removing the first point in the series
                    #for each channel:
                    if self._correct_baseline:
                        event_trig -= event_trig[0]

                    h[i][e_idx] = np.mean(event_trig, -1)

        #In case the input events are an Events:
        else:
            #Get the indices necessary for extraction of the eta:
            add_offset = np.arange(self.offset,
                                   self.offset + self.len_et)[:, np.newaxis]

            idx = (self.events.time / self.sampling_interval).astype(int)

            #Make a list for the output
            h = [0] * self._len_h

            # Loop over channels
            for i in range(self._len_h):
                #If this is a list with one element:
                if self._len_h == 1:
                    event_trig = self.data[0][idx + add_offset]
                #Otherwise, you need to index straight into the underlying data
                #array:
                else:
                    event_trig = self.data.data[i][idx + add_offset]

                h[i] = np.mean(event_trig, -1)

        h = np.array(h).squeeze()
        return ts.TimeSeries(data=h,
                             sampling_interval=self.sampling_interval,
                             t0=self.offset * self.sampling_interval,
                             time_unit=self.time_unit)

    @desc.setattr_on_read
    def ets(self):
        """The event-triggered standard error of the mean """

        #Make a list for the output
        h = [0] * self._len_h

        if self._is_ts:
            # Loop over channels
            for i in range(self._len_h):
                data = self.data[i]
                u = np.unique(self.events[i])
                event_types = u[np.unique(self.events[i]) != 0]
                h[i] = np.empty((event_types.shape[0], self.len_et),
                                dtype=complex)

                # This offset is used to pull the event indices below, but we
                # have to broadcast it so the shape of the resulting idx+offset
                # operation below gives us the (nevents, len_et) array we want,
                # per channel.
                offset = np.arange(self.offset,
                                   self.offset + self.len_et)[:, np.newaxis]
                # Loop over event types
                for e_idx in range(event_types.shape[0]):
                    idx = np.where(self.events[i] == event_types[e_idx])[0]
                    event_trig = data[idx + offset]
                    #Correct baseline by removing the first point in the series
                    #for each channel:
                    if self._correct_baseline:
                        event_trig -= event_trig[0]

                    h[i][e_idx] = stats.sem(event_trig, -1)

        #In case the input events are an Events:
        else:
            #Get the indices necessary for extraction of the eta:
            add_offset = np.arange(self.offset,
                                   self.offset + self.len_et)[:, np.newaxis]

            idx = (self.events.time / self.sampling_interval).astype(int)

            #Make a list for the output
            h = [0] * self._len_h

            # Loop over channels
            for i in range(self._len_h):
                #If this is a list with one element:
                if self._len_h == 1:
                    event_trig = self.data[0][idx + add_offset]
                #Otherwise, you need to index straight into the underlying data
                #array:
                else:
                    event_trig = self.data.data[i][idx + add_offset]

                h[i] = stats.sem(event_trig, -1)

        h = np.array(h).squeeze()
        return ts.TimeSeries(data=h,
                             sampling_interval=self.sampling_interval,
                             t0=self.offset * self.sampling_interval,
                             time_unit=self.time_unit)

########NEW FILE########
__FILENAME__ = granger
"""

Analyzers for the calculation of Granger 'causality'

"""

import numpy as np
import nitime.algorithms as alg
import nitime.utils as utils
from nitime import descriptors as desc

from .base import BaseAnalyzer

# To suppport older versions of numpy that don't have tril_indices:
from nitime.index_utils import tril_indices_from

def fit_model(x1, x2, order=None, max_order=10,
              criterion=utils.bayesian_information_criterion):
    """
    Fit the auto-regressive model used in calculation of Granger 'causality'.

    Parameters
    ----------

    x1,x2: float arrays (n)
        x1,x2 bivariate combination.
    order: int (optional)
        If known, the order of the autoregressive process
    max_order: int (optional)
        If the order is not known, this will be the maximal order to fit.
    criterion: callable
       A function which defines an information criterion, used to determine the
        order of the model.

    """
    c_old = np.inf
    n_process = 2
    Ntotal = n_process * x1.shape[-1]

    # If model order was provided as an input:
    if order is not None:
        lag = order + 1
        Rxx = utils.autocov_vector(np.vstack([x1, x2]), nlags=lag)
        coef, ecov = alg.lwr_recursion(np.array(Rxx).transpose(2, 0, 1))

    # If the model order is not known and provided as input:
    else:
        for lag in range(1, max_order):
            Rxx_new = utils.autocov_vector(np.vstack([x1, x2]), nlags=lag)
            coef_new, ecov_new = alg.lwr_recursion(
                                        np.array(Rxx_new).transpose(2, 0, 1))
            order_new = coef_new.shape[0]
            c_new = criterion(ecov_new, n_process, order_new, Ntotal)
            if c_new > c_old:
                # Keep the values you got in the last round and break out:
                break

            else:
                # Replace the output values with the new calculated values and
                # move on to the next order:
                c_old = c_new
                order = order_new
                Rxx = Rxx_new
                coef = coef_new
                ecov = ecov_new
        else:
            e_s = ("Model estimation order did not converge at max_order = %s"
                                                                  % max_order)
            raise ValueError(e_s)

    return order, Rxx, coef, ecov


class GrangerAnalyzer(BaseAnalyzer):
    """Analyzer for computing all-to-all Granger 'causality' """
    def __init__(self, input=None, ij=None, order=None, max_order=10,
                 criterion=utils.bayesian_information_criterion, n_freqs=1024):
        """
        Initializer for the GrangerAnalyzer.

        Parameters
        ----------

        input: nitime TimeSeries object
        ij: List of tuples of the form: [(0, 1), (0, 2)], etc.
            These are the indices of pairs of time-series for which the
            analysis will be done. Defaults to all vs. all.
        order: int (optional)
             The order of the process. If this is not known, it will be
             estimated from the data, using the information criterion
        max_order: if the order is estimated, this is the maximal order to
             estimate for.
        n_freqs: int (optional)
            The size of the sampling grid in the frequency domain.
            Defaults to 1024
        criterion:
            XXX
        """
        self.data = input.data
        self.sampling_rate = input.sampling_rate
        self._n_process = input.shape[0]
        self._n_freqs = n_freqs
        self._order = order
        self._criterion = criterion
        self._max_order = max_order
        if ij is None:
            # The following gets the full list of combinations of
            # non-same i's and j's:
            x, y = np.meshgrid(np.arange(self._n_process),
                               np.arange(self._n_process))
            self.ij = list(zip(x[tril_indices_from(x, -1)],
                          y[tril_indices_from(y, -1)]))
        else:
            self.ij = ij

    @desc.setattr_on_read
    def _model(self):
        model = dict(order={}, autocov={}, model_coef={}, error_cov={})
        for i, j in self.ij:
            model[i, j] = dict()
            order_t, Rxx_t, coef_t, ecov_t = fit_model(self.data[i],
                                                   self.data[j],
                                                   order=self._order,
                                                   max_order=self._max_order,
                                                   criterion=self._criterion)
            model['order'][i, j] = order_t
            model['autocov'][i, j] = Rxx_t
            model['model_coef'][i, j] = coef_t
            model['error_cov'][i, j] = ecov_t

        return model

    @desc.setattr_on_read
    def order(self):
        if self._order is None:
            return self._model['order']
        else:
            order = {}
            for i, j in self.ij:
                order[i, j] = self._order
            return order

    @desc.setattr_on_read
    def autocov(self):
        return self._model['autocov']

    @desc.setattr_on_read
    def model_coef(self):
        return self._model['model_coef']

    @desc.setattr_on_read
    def error_cov(self):
        return self._model['error_cov']

    @desc.setattr_on_read
    def _granger_causality(self):
        """
        This returns a dict with the values computed by
        :func:`granger_causality_xy`, rather than arrays, so that we can delay
        the allocation of arrays as much as possible.

        """
        gc = dict(frequencies={}, gc_xy={}, gc_yx={}, gc_sim={},
                  spectral_density={})
        for i, j in self.ij:
            w, f_x2y, f_y2x, f_xy, Sw = \
               alg.granger_causality_xy(self.model_coef[i, j],
                                        self.error_cov[i, j],
                                        n_freqs=self._n_freqs)

            # All other measures are dependent on i, j:
            gc['gc_xy'][i, j] = f_x2y
            gc['gc_yx'][i, j] = f_y2x
            gc['gc_sim'][i, j] = f_xy
            gc['spectral_density'][i, j] = Sw

        return gc

    @desc.setattr_on_read
    def frequencies(self):
        return utils.get_freqs(self.sampling_rate, self._n_freqs)

    def _dict2arr(self, key):
        """
        A helper function that will generate an array with all nan's and insert
        the measure defined by 'key' into the array and return it to the
        calling function. This allows us to get matrices of the measures of
        interest, instead of a dict.
        """
        # Prepare the matrix for the output:
        arr = np.empty((self._n_process,
                        self._n_process,
                        self.frequencies.shape[0]))

        arr.fill(np.nan)

        # 'Translate' from dict form into matrix form:
        for i, j in self.ij:
            arr[j, i, :] = self._granger_causality[key][i, j]
        return arr

    @desc.setattr_on_read
    def causality_xy(self):
        return self._dict2arr('gc_xy')

    @desc.setattr_on_read
    def causality_yx(self):
        return self._dict2arr('gc_yx')

    @desc.setattr_on_read
    def simultaneous_causality(self):
        return self._dict2arr('gc_sim')

    @desc.setattr_on_read
    def spectral_matrix(self):
        return self._granger_causality['spectral_density']

########NEW FILE########
__FILENAME__ = normalization
from nitime import descriptors as desc
from nitime import utils as tsu
from nitime import timeseries as ts

from .base import BaseAnalyzer


class NormalizationAnalyzer(BaseAnalyzer):
    """ A class for performing normalization operations on time-series and
    producing the renormalized versions of the time-series"""

    def __init__(self, input=None):
        """Constructor function for the Normalization analyzer class.

        Parameters
        ----------

        input: TimeSeries object

        """
        BaseAnalyzer.__init__(self, input)

    @desc.setattr_on_read
    def percent_change(self):
        return ts.TimeSeries(tsu.percent_change(self.input.data),
                             sampling_rate=self.input.sampling_rate,
                             time_unit=self.input.time_unit)

    @desc.setattr_on_read
    def z_score(self):
        return ts.TimeSeries(tsu.zscore(self.input.data),
                             sampling_rate=self.input.sampling_rate,
                             time_unit=self.input.time_unit)

########NEW FILE########
__FILENAME__ = snr
import numpy as np
from nitime.lazy import scipy_stats as stats

from nitime import descriptors as desc
from nitime import algorithms as tsa
from nitime import timeseries as ts

from nitime.index_utils import tril_indices_from


from .base import BaseAnalyzer


def signal_noise(response):
    """
    Signal and noise as defined in Borst and Theunissen 1999, Figure 2

    Parameters
    ----------

    response: nitime TimeSeries object
       The data here are individual responses of a single unit to the same
       stimulus, with repetitions being the first dimension and time as the
       last dimension
    """

    signal = np.mean(response.data, 0)  # The estimate of the signal is the
                                       # average response

    noise = response.data - signal  # Noise is the individual
                               # repetition's deviation from the
                               # estimate of the signal

    # Return TimeSeries objects with the sampling rate of the input:
    return  (ts.TimeSeries(signal, sampling_rate=response.sampling_rate),
             ts.TimeSeries(noise, sampling_rate=response.sampling_rate))


class SNRAnalyzer(BaseAnalyzer):
    """
    Calculate SNR for a response to repetitions of the same stimulus, according
    to (Borst, 1999) (Figure 2) and (Hsu, 2004).

    Hsu A, Borst A and Theunissen, FE (2004) Quantifying variability in neural
    responses ans its application for the validation of model
    predictions. Network: Comput Neural Syst 15:91-109

    Borst A and Theunissen FE (1999) Information theory and neural coding. Nat
    Neurosci 2:947-957
    """
    def __init__(self, input=None, bandwidth=None, adaptive=False,
                 low_bias=False):
        """
        Initializer for the multi_taper_SNR object

        Parameters
        ----------
        input: TimeSeries object

        bandwidth: float,
           The bandwidth of the windowing function will determine the number
           tapers to use. This parameters represents trade-off between
           frequency resolution (lower main lobe bandwidth for the taper) and
           variance reduction (higher bandwidth and number of averaged
           estimates). Per default will be set to 4 times the fundamental
           frequency, such that NW=4

        adaptive: bool, default to False
            Whether to set the weights for the tapered spectra according to the
            adaptive algorithm (Thompson, 2007).

        low_bias : bool, default to False
            Rather than use 2NW tapers, only use the tapers that have better
            than 90% spectral concentration within the bandwidth (still using a
            maximum of 2NW tapers)

        Notes
        -----

        Thompson, DJ (2007) Jackknifing multitaper spectrum estimates. IEEE
        Signal Processing Magazing. 24: 20-30

        """
        self.input = input
        self.signal, self.noise = signal_noise(input)
        self.bandwidth = bandwidth
        self.adaptive = adaptive
        self.low_bias = low_bias

    @desc.setattr_on_read
    def mt_frequencies(self):
        return np.linspace(0, self.input.sampling_rate / 2,
                           self.input.data.shape[-1] / 2 + 1)

    @desc.setattr_on_read
    def mt_signal_psd(self):
        _, p, _ = tsa.multi_taper_psd(self.signal.data,
                                    Fs=self.input.sampling_rate,
                                    BW=self.bandwidth,
                                    adaptive=self.adaptive,
                                    low_bias=self.low_bias)
        return p

    @desc.setattr_on_read
    def mt_noise_psd(self):
        p = np.empty((self.noise.data.shape[0],
                     self.noise.data.shape[-1] / 2 + 1))

        for i in range(p.shape[0]):
            _, p[i], _ = tsa.multi_taper_psd(self.noise.data[i],
                                    Fs=self.input.sampling_rate,
                                    BW=self.bandwidth,
                                    adaptive=self.adaptive,
                                    low_bias=self.low_bias)
        return np.mean(p, 0)

    @desc.setattr_on_read
    def mt_coherence(self):
        """ """
        return self.mt_signal_psd / (self.mt_signal_psd + self.mt_noise_psd)

    @desc.setattr_on_read
    def mt_information(self):
        df = self.mt_frequencies[1] - self.mt_frequencies[0]
        return -1 * np.log2(1 - self.mt_coherence) * df
        #These two formulations should be equivalent
        #return np.log2(1+self.mt_snr)

    @desc.setattr_on_read
    def mt_snr(self):
        return self.mt_signal_psd / self.mt_noise_psd

    @desc.setattr_on_read
    def correlation(self):
        """
        The correlation between all combinations of trials

        Returns
        -------
        (r,e) : tuple
           r is the mean correlation and e is the mean error of the correlation
           (with df = n_trials - 1)
        """

        c = np.corrcoef(self.input.data)
        c = c[tril_indices_from(c, -1)]

        return np.mean(c), stats.sem(c)

########NEW FILE########
__FILENAME__ = spectral

import numpy as np
from nitime.lazy import scipy
from nitime.lazy import scipy_signal as signal
from nitime.lazy import scipy_fftpack as fftpack

from nitime import descriptors as desc
from nitime import utils as tsu
from nitime import algorithms as tsa
from nitime import timeseries as ts

from .base import BaseAnalyzer


class SpectralAnalyzer(BaseAnalyzer):
    """ Analyzer object for spectral analysis"""
    def __init__(self, input=None, method=None, BW=None, adaptive=False,
                 low_bias=False):
        """
        The initialization of the

        Parameters
        ----------
        input: time-series objects

        method: dict (optional),
           The method spec used in calculating 'psd' see
           :func:`algorithms.get_spectra` for details.

        BW: float (optional),
            In 'spectrum_multi_taper' The bandwidth of the windowing function
            will determine the number tapers to use. This parameters represents
            trade-off between frequency resolution (lower main lobe BW for the
            taper) and variance reduction (higher BW and number of averaged
            estimates).

        adaptive : {True/False}
           In 'spectrum_multi_taper', use an adaptive weighting routine to
           combine the PSD estimates of different tapers.

        low_bias: {True/False}
           In spectrum_multi_taper, use bias correction


        Examples
        --------
        >>> np.set_printoptions(precision=4)  # for doctesting
        >>> t1 = ts.TimeSeries(data = np.arange(0,1024,1).reshape(2,512),
        ... sampling_rate=np.pi)
        >>> s1 = SpectralAnalyzer(t1)
        >>> s1.method['this_method']
        'welch'
        >>> s1.method['Fs'] # doctest: +ELLIPSIS
        3.1415926535... Hz
        >>> f,s = s1.psd
        >>> f
        array([ 0.    ,  0.0491,  0.0982,  0.1473,  0.1963,  0.2454,  0.2945,
                0.3436,  0.3927,  0.4418,  0.4909,  0.54  ,  0.589 ,  0.6381,
                0.6872,  0.7363,  0.7854,  0.8345,  0.8836,  0.9327,  0.9817,
                1.0308,  1.0799,  1.129 ,  1.1781,  1.2272,  1.2763,  1.3254,
                1.3744,  1.4235,  1.4726,  1.5217,  1.5708])
        >>> s[0,0]   # doctest: +ELLIPSIS
        1128276.92538360...
        """
        BaseAnalyzer.__init__(self, input)

        self.method = method

        if self.method is None:
            self.method = {'this_method': 'welch',
                           'Fs': self.input.sampling_rate}

        self.BW = BW
        self.adaptive = adaptive
        self.low_bias = low_bias

    @desc.setattr_on_read
    def psd(self):
        """
        The standard output for this analyzer is a tuple f,s, where: f is the
        frequency bands associated with the discrete spectral components
        and s is the PSD calculated using :func:`mlab.psd`.

        """
        NFFT = self.method.get('NFFT', 64)
        Fs = self.input.sampling_rate
        detrend = self.method.get('detrend', tsa.mlab.detrend_none)
        window = self.method.get('window', tsa.mlab.window_hanning)
        n_overlap = self.method.get('n_overlap', int(np.ceil(NFFT / 2.0)))

        if np.iscomplexobj(self.input.data):
            psd_len = NFFT
            dt = complex
        else:
            psd_len = NFFT / 2.0 + 1
            dt = float
        
        #If multi-channel data:
        if len(self.input.data.shape) > 1:
            psd_shape = (self.input.shape[:-1] + (psd_len,))
            flat_data = np.reshape(self.input.data, (-1,
                                                     self.input.data.shape[-1]))
            flat_psd = np.empty((flat_data.shape[0], psd_len), dtype=dt)
            for i in range(flat_data.shape[0]):
                #'f' are the center frequencies of the frequency bands
                #represented in the psd. These are identical in each iteration
                #of the loop, so they get reassigned into the same variable in
                #each iteration:
                temp, f = tsa.mlab.psd(flat_data[i],
                            NFFT=NFFT,
                            Fs=Fs,
                            detrend=detrend,
                            window=window,
                            noverlap=n_overlap)
                flat_psd[i] = temp.squeeze()
            psd = np.reshape(flat_psd, psd_shape).squeeze()

        else:
            psd, f = tsa.mlab.psd(self.input.data,
                            NFFT=NFFT,
                            Fs=Fs,
                            detrend=detrend,
                            window=window,
                            noverlap=n_overlap)

        return f, psd

    @desc.setattr_on_read
    def cpsd(self):
        """
        This outputs both the PSD and the CSD calculated using
        :func:`algorithms.get_spectra`.

        Returns
        -------

        (f,s): tuple
           f: Frequency bands over which the psd/csd are calculated and
           s: the n by n by len(f) matrix of PSD (on the main diagonal) and CSD
           (off diagonal)
        """
        self.welch_method = self.method
        self.welch_method['this_method'] = 'welch'
        self.welch_method['Fs'] = self.input.sampling_rate
        f, spectrum_welch = tsa.get_spectra(self.input.data,
                                           method=self.welch_method)

        return f, spectrum_welch

    @desc.setattr_on_read
    def periodogram(self):
        """

        This is the spectrum estimated as the FFT of the time-series

        Returns
        -------
        (f,spectrum): f is an array with the frequencies and spectrum is the
        complex-valued FFT.
        """
        return tsa.periodogram(self.input.data,
                               Fs=self.input.sampling_rate)

    @desc.setattr_on_read
    def spectrum_fourier(self):
        """

        This is the spectrum estimated as the FFT of the time-series

        Returns
        -------
        (f,spectrum): f is an array with the frequencies and spectrum is the
        complex-valued FFT.

        """

        data = self.input.data
        sampling_rate = self.input.sampling_rate

        fft = fftpack.fft
        if np.any(np.iscomplex(data)):
            # Get negative frequencies, as well as positive:
            f = np.linspace(-sampling_rate/2., sampling_rate/2., data.shape[-1])
            spectrum_fourier = np.fft.fftshift(fft(data))
        else:
            f = tsu.get_freqs(sampling_rate, data.shape[-1])
            spectrum_fourier = fft(data)[..., :f.shape[0]]
            
        return f, spectrum_fourier

    @desc.setattr_on_read
    def spectrum_multi_taper(self):
        """

        The spectrum and cross-spectra, computed using
        :func:`multi_taper_csd'

        """
        if np.iscomplexobj(self.input.data):
            psd_len = self.input.shape[-1] 
            dt = complex
        else:
            psd_len = self.input.shape[-1] / 2 + 1
            dt = float

        #Initialize the output
        spectrum_multi_taper = np.empty((self.input.shape[:-1] + (psd_len,)),
                                        dtype=dt)

        #If multi-channel data:
        if len(self.input.data.shape) > 1:
            for i in range(self.input.data.shape[0]):
                # 'f' are the center frequencies of the frequency bands
                # represented in the MT psd. These are identical in each
                # iteration of the loop, so they get reassigned into the same
                # variable in each iteration:
                f, spectrum_multi_taper[i], _ = tsa.multi_taper_psd(
                    self.input.data[i],
                    Fs=self.input.sampling_rate,
                    BW=self.BW,
                    adaptive=self.adaptive,
                    low_bias=self.low_bias)
        else:
            f, spectrum_multi_taper, _ = tsa.multi_taper_psd(self.input.data,
                                                  Fs=self.input.sampling_rate,
                                                  BW=self.BW,
                                                  adaptive=self.adaptive,
                                                  low_bias=self.low_bias)

        return f, spectrum_multi_taper


class FilterAnalyzer(desc.ResetMixin):
    """ A class for performing filtering operations on time-series and
    producing the filtered versions of the time-series

    Parameters
    ----------

    time_series: A nitime TimeSeries object.

    lb,ub: float (optional)
       Lower and upper band of a pass-band into which the data will be
       filtered. Default: 0, Nyquist

    boxcar_iterations: int (optional)
       For box-car filtering, how many times to iterate over the data while
       convolving with a box-car function. Default: 2

    gpass: float (optional)
       For iir filtering, the pass-band maximal ripple loss (default: 1)

    gstop: float (optional)
       For iir filtering, the stop-band minimal attenuation (default: 60).

    filt_order: int (optional)
        For iir/fir filtering, the order of the filter. Note for fir filtering,
        this needs to be an even number. Default: 64

    iir_ftype: str (optional)
        The type of filter to be used in iir filtering (see
        scipy.signal.iirdesign for details). Default 'ellip'

    fir_win: str
        The window to be used in fir filtering (see scipy.signal.firwin for
        details). Default: 'hamming'

    Note
    ----
    All filtering methods used here keep the original DC component of the
    signal.

    """
    def __init__(self, time_series, lb=0, ub=None, boxcar_iterations=2,
                 filt_order=64, gpass=1, gstop=60, iir_ftype='ellip',
                 fir_win='hamming'):

        #Initialize all the local variables you will need for all the different
        #filtering methods:
        self.data = time_series.data
        self.sampling_rate = time_series.sampling_rate
        self.ub = ub
        self.lb = lb
        self.time_unit = time_series.time_unit
        self._boxcar_iterations = boxcar_iterations
        self._gstop = gstop
        self._gpass = gpass
        self._filt_order = filt_order
        self._ftype = iir_ftype
        self._win = fir_win

    def filtfilt(self, b, a, in_ts=None):

        """
        Zero-phase delay filtering (either iir or fir).

        Parameters
        ----------

        a,b: filter coefficients

        in_ts: time-series object.
           This allows to replace the input. Instead of analyzing this
           analyzers input data, analyze some other time-series object

        Note
        ----

        This is a wrapper around scipy.signal.filtfilt

        """
        # Switch in the new in_ts:
        if in_ts is not None:
            data = in_ts.data
            Fs = in_ts.sampling_rate
        else:
            data = self.data
            Fs = self.sampling_rate

        #filtfilt only operates channel-by-channel, so we need to loop over the
        #channels, if the data is multi-channel data:
        if len(data.shape) > 1:
            out_data = np.empty(data.shape, dtype=data.dtype)
            for i in range(data.shape[0]):
                out_data[i] = signal.filtfilt(b, a, data[i])
                #Make sure to preserve the DC:
                dc = np.mean(data[i])
                out_data[i] -= np.mean(out_data[i])
                out_data[i] += dc
        else:
            out_data = signal.filtfilt(b, a, data)
            #Make sure to preserve the DC:
            dc = np.mean(data)
            out_data -= np.mean(out_data)
            out_data += dc

        return ts.TimeSeries(out_data,
                             sampling_rate=Fs,
                             time_unit=self.time_unit)

    @desc.setattr_on_read
    def fir(self):
        """
        Filter the time-series using an FIR digital filter. Filtering is done
        back and forth (using scipy.signal.filtfilt) to achieve zero phase
        delay
        """
        #Passband and stop-band are expressed as fraction of the Nyquist
        #frequency:
        if self.ub is not None:
            ub_frac = self.ub / (self.sampling_rate / 2.)
        else:
            ub_frac = 1.0

        lb_frac = self.lb / (self.sampling_rate / 2.)

        if lb_frac < 0 or ub_frac > 1:
            e_s = "The lower-bound or upper bound used to filter"
            e_s += " are beyond the range 0-Nyquist. You asked for"
            e_s += " a filter between"
            e_s += "%s and %s percent of" % (lb_frac * 100, ub_frac * 100)
            e_s += "the Nyquist frequency"
            raise ValueError(e_s)

        n_taps = self._filt_order + 1

        #This means the filter order you chose was too large (needs to be
        #shorter than a 1/3 of your time-series )
        if n_taps > self.data.shape[-1] * 3:
            e_s = "The filter order chosen is too large for this time-series"
            raise ValueError(e_s)

        # a is always 1:
        a = [1]

        sig = ts.TimeSeries(data=self.data, sampling_rate=self.sampling_rate)

        #Lowpass:
        if ub_frac < 1:
            b = signal.firwin(n_taps, ub_frac, window=self._win)
            sig = self.filtfilt(b, a, sig)

        #High-pass
        if lb_frac > 0:
            #Includes a spectral inversion:
            b = -1 * signal.firwin(n_taps, lb_frac, window=self._win)
            b[n_taps / 2] = b[n_taps / 2] + 1
            sig = self.filtfilt(b, a, sig)

        return sig

    @desc.setattr_on_read
    def iir(self):
        """
        Filter the time-series using an IIR filter. Filtering is done back and
        forth (using scipy.signal.filtfilt) to achieve zero phase delay

        """

        #Passband and stop-band are expressed as fraction of the Nyquist
        #frequency:
        if self.ub is not None:
            ub_frac = self.ub / (self.sampling_rate / 2.)
        else:
            ub_frac = 1.0

        lb_frac = self.lb / (self.sampling_rate / 2.)

        # For the band-pass:
        if lb_frac > 0 and ub_frac < 1:

            wp = [lb_frac, ub_frac]

            ws = [np.max([lb_frac - 0.1, 0]),
                  np.min([ub_frac + 0.1, 1.0])]

        # For the lowpass:
        elif lb_frac == 0:
            wp = ub_frac
            ws = np.min([ub_frac + 0.1, 0.9])

        # For the highpass:
        elif ub_frac == 1:
            wp = lb_frac
            ws = np.max([lb_frac - 0.1, 0.1])

        b, a = signal.iirdesign(wp, ws, self._gpass, self._gstop,
                                ftype=self._ftype)

        return self.filtfilt(b, a)

    @desc.setattr_on_read
    def filtered_fourier(self):
        """

        Filter the time-series by passing it to the Fourier domain and null
        out the frequency bands outside of the range [lb,ub]

        """

        freqs = tsu.get_freqs(self.sampling_rate, self.data.shape[-1])

        if self.ub is None:
            self.ub = freqs[-1]

        power = fftpack.fft(self.data)
        idx_0 = np.hstack([np.where(freqs < self.lb)[0],
                           np.where(freqs > self.ub)[0]])

        #Make sure that you keep the DC component:
        keep_dc = np.copy(power[..., 0])
        power[..., idx_0] = 0
        power[..., -1 * idx_0] = 0  # Take care of the negative frequencies
        power[..., 0] = keep_dc  # And put the DC back in when you're done:

        data_out = fftpack.ifft(power)

        data_out = np.real(data_out)  # In order to make sure that you are not
                                      # left with float-precision residual
                                      # complex parts

        return ts.TimeSeries(data=data_out,
                             sampling_rate=self.sampling_rate,
                             time_unit=self.time_unit)

    @desc.setattr_on_read
    def filtered_boxcar(self):
        """
        Filter the time-series by a boxcar filter.

        The low pass filter is implemented by convolving with a boxcar function
        of the right length and amplitude and the high-pass filter is
        implemented by subtracting a low-pass version (as above) from the
        signal
        """

        if self.ub is not None:
            ub = self.ub / self.sampling_rate
        else:
            ub = 1.0

        lb = self.lb / self.sampling_rate

        data_out = tsa.boxcar_filter(np.copy(self.data),
                                     lb=lb, ub=ub,
                                     n_iterations=self._boxcar_iterations)

        return ts.TimeSeries(data=data_out,
                                 sampling_rate=self.sampling_rate,
                                 time_unit=self.time_unit)


class HilbertAnalyzer(BaseAnalyzer):

    """Analyzer class for extracting the Hilbert transform """

    def __init__(self, input=None):
        """Constructor function for the Hilbert analyzer class.

        Parameters
        ----------

        input: TimeSeries

        """
        BaseAnalyzer.__init__(self, input)

    @desc.setattr_on_read
    def analytic(self):
        """The natural output for this analyzer is the analytic signal """
        data = self.input.data
        sampling_rate = self.input.sampling_rate
        #If you have scipy with the fixed scipy.signal.hilbert (r6205 and
        #later)
        if scipy.__version__ >= '0.9':
            hilbert = signal.hilbert
        else:
            hilbert = tsu.hilbert_from_new_scipy

        return ts.TimeSeries(data=hilbert(data),
                             sampling_rate=sampling_rate)

    @desc.setattr_on_read
    def amplitude(self):
        return ts.TimeSeries(data=np.abs(self.analytic.data),
                             sampling_rate=self.analytic.sampling_rate)

    @desc.setattr_on_read
    def phase(self):
        return ts.TimeSeries(data=np.angle(self.analytic.data),
                             sampling_rate=self.analytic.sampling_rate)

    @desc.setattr_on_read
    def real(self):
        return ts.TimeSeries(data=self.analytic.data.real,
                             sampling_rate=self.analytic.sampling_rate)

    @desc.setattr_on_read
    def imag(self):
        return ts.TimeSeries(data=self.analytic.data.imag,
                             sampling_rate=self.analytic.sampling_rate)


class MorletWaveletAnalyzer(BaseAnalyzer):

    """Analyzer class for extracting the (complex) Morlet wavelet transform """

    def __init__(self, input=None, freqs=None, sd_rel=.2, sd=None, f_min=None,
                 f_max=None, nfreqs=None, log_spacing=False, log_morlet=False):
        """Constructor function for the Wavelet analyzer class.

        Parameters
        ----------

        freqs: list or float
          List of center frequencies for the wavelet transform, or a scalar
          for a single band-passed signal.

        sd: list or float
          List of filter bandwidths, given as standard-deviation of center
          frequencies. Alternatively sd_rel can be specified.

        sd_rel: float
          Filter bandwidth, given as a fraction of the center frequencies.

        f_min: float
          Minimal frequency.

        f_max: float
          Maximal frequency.

        nfreqs: int
          Number of frequencies.

        log_spacing: bool
          If true, frequencies will be evenly spaced on a log-scale.
          Default: False

        log_morlet: bool
          If True, a log-Morlet wavelet is used, if False, a regular Morlet
          wavelet is used. Default: False
        """
        BaseAnalyzer.__init__(self, input)
        self.freqs = freqs
        self.sd_rel = sd_rel
        self.sd = sd
        self.f_min = f_min
        self.f_max = f_max
        self.nfreqs = nfreqs
        self.log_spacing = log_spacing
        self.log_morlet = log_morlet

        if log_morlet:
            self.wavelet = tsa.wlogmorlet
        else:
            self.wavelet = tsa.wmorlet

        if freqs is not None:
            self.freqs = np.array(freqs)
        elif f_min is not None and f_max is not None and nfreqs is not None:
            if log_spacing:
                self.freqs = np.logspace(np.log10(f_min), np.log10(f_max),
                                         num=nfreqs, endpoint=True)
            else:
                self.freqs = np.linspace(f_min, f_max, num=nfreqs,
                                         endpoint=True)
        else:
            raise NotImplementedError

        if sd is None:
            self.sd = self.freqs * self.sd_rel

    @desc.setattr_on_read
    def analytic(self):
        """The natural output for this analyzer is the analytic signal"""
        data = self.input.data
        sampling_rate = self.input.sampling_rate

        a_signal =\
    ts.TimeSeries(data=np.zeros(self.freqs.shape + data.shape,
                                dtype='D'), sampling_rate=sampling_rate)
        if self.freqs.ndim == 0:
            w = self.wavelet(self.freqs, self.sd,
                             sampling_rate=sampling_rate, ns=5,
                             normed='area')

            # nd = (w.shape[0] - 1) / 2
            a_signal.data[...] = (np.convolve(data, np.real(w), mode='same') +
                            1j * np.convolve(data, np.imag(w), mode='same'))
        else:
            for i, (f, sd) in enumerate(zip(self.freqs, self.sd)):
                w = self.wavelet(f, sd, sampling_rate=sampling_rate,
                                 ns=5, normed='area')

                # nd = (w.shape[0] - 1) / 2
                a_signal.data[i, ...] = (
                    np.convolve(data, np.real(w), mode='same') +
                    1j * np.convolve(data, np.imag(w), mode='same'))

        return a_signal

    @desc.setattr_on_read
    def amplitude(self):
        return ts.TimeSeries(data=np.abs(self.analytic.data),
                             sampling_rate=self.analytic.sampling_rate)

    @desc.setattr_on_read
    def phase(self):
        return ts.TimeSeries(data=np.angle(self.analytic.data),
                             sampling_rate=self.analytic.sampling_rate)

    @desc.setattr_on_read
    def real(self):
        return ts.TimeSeries(data=self.analytic.data.real,
                             sampling_rate=self.analytic.sampling_rate)

    @desc.setattr_on_read
    def imag(self):
        return ts.TimeSeries(data=self.analytic.data.imag,
                             sampling_rate=self.analytic.sampling_rate)

########NEW FILE########
__FILENAME__ = test_base

from nitime.analysis.base import BaseAnalyzer
import numpy.testing as npt


def test_base():
    """Testing BaseAnalyzer"""

    empty_dict = {}
    input1 = '123'
    A = BaseAnalyzer(input=input1)

    npt.assert_equal(A.input, input1)
    npt.assert_equal(A.parameters, empty_dict)

    input2 = '456'
    A.set_input(input2)

    npt.assert_equal(A.input, input2)

    npt.assert_equal(A.__repr__(), 'BaseAnalyzer()')

########NEW FILE########
__FILENAME__ = test_coherence
import warnings

import numpy as np
import numpy.testing as npt
import matplotlib
import matplotlib.mlab as mlab

import nitime.timeseries as ts
import nitime.analysis as nta

import platform

# Some tests might require python version 2.5 or above: 
if float(platform.python_version()[:3]) < 2.5:
    old_python = True
else:
    old_python = False

# Matplotlib older than 0.99 will have some issues with the normalization of t

if float(matplotlib.__version__[:3]) < 0.99:
    w_s = "You have a relatively old version of Matplotlib. " 
    w_s += " Estimation of the PSD DC component might not be as expected"
    w_s += " Consider updating Matplotlib: http://matplotlib.sourceforge.net/"
    warnings.warn(w_s, Warning)
    old_mpl = True
else:
    old_mpl = False

def test_CoherenceAnalyzer():
    methods = (None,
           {"this_method": 'welch', "NFFT": 256},
           {"this_method": 'multi_taper_csd'},
           {"this_method": 'periodogram_csd', "NFFT": 256})

    Fs = np.pi
    t = np.arange(1024)
    x = np.sin(10 * t) + np.random.rand(t.shape[-1])
    y = np.sin(10 * t) + np.random.rand(t.shape[-1])
    # Third time-series used for calculation of partial coherence:
    z = np.sin(10 * t)
    T = ts.TimeSeries(np.vstack([x, y, z]), sampling_rate=np.pi)
    n_series = T.shape[0]
    for unwrap in [True, False]:
        for method in methods:
            C = nta.CoherenceAnalyzer(T, method, unwrap_phases=unwrap)
            if method is None:
                # This is the default behavior (grab the NFFT from the number
                # of frequencies):
                npt.assert_equal(C.coherence.shape, (n_series, n_series,
                                                     C.frequencies.shape[0]))

            elif (method['this_method'] == 'welch' or
                  method['this_method'] == 'periodogram_csd'):
                npt.assert_equal(C.coherence.shape, (n_series, n_series,
                                                     method['NFFT'] // 2 + 1))
            else:
                npt.assert_equal(C.coherence.shape, (n_series, n_series,
                                                     len(t) // 2 + 1))

            # Coherence symmetry:
            npt.assert_equal(C.coherence[0, 1], C.coherence[1, 0])

            # Phase/delay asymmetry:
            npt.assert_equal(C.phase[0, 1], -1 * C.phase[1, 0])

            # The very first one is a nan, test from second and onwards:
            npt.assert_almost_equal(C.delay[0, 1][1:], -1 * C.delay[1, 0][1:])

            if method is not None and method['this_method'] == 'welch':
                S = nta.SpectralAnalyzer(T, method)
                npt.assert_almost_equal(S.cpsd[0], C.frequencies)
                npt.assert_almost_equal(S.cpsd[1], C.spectrum)
            # Test that partial coherence runs through and has the right number
            # of dimensions:
            npt.assert_equal(len(C.coherence_partial.shape), 4)


@npt.dec.skipif(old_mpl)
def test_SparseCoherenceAnalyzer():
    Fs = np.pi
    t = np.arange(256)
    x = np.sin(10 * t) + np.random.rand(t.shape[-1])
    y = np.sin(10 * t) + np.random.rand(t.shape[-1])
    T = ts.TimeSeries(np.vstack([x, y]), sampling_rate=Fs)
    C1 = nta.SparseCoherenceAnalyzer(T, ij=((0, 1), (1, 0)))
    C2 = nta.CoherenceAnalyzer(T)

    # Coherence symmetry:
    npt.assert_almost_equal(np.abs(C1.coherence[0, 1]),
                            np.abs(C1.coherence[1, 0]))
    npt.assert_almost_equal(np.abs(C1.coherency[0, 1]),
                            np.abs(C1.coherency[1, 0]))

    # Make sure you get the same answers as you would from the standard
    # CoherenceAnalyzer:

    npt.assert_almost_equal(C2.coherence[0, 1], C1.coherence[0, 1])
    # This is the PSD (for the first time-series in the object):
    npt.assert_almost_equal(C2.spectrum[0, 0], C1.spectrum[0])
    # And the second (for good measure):
    npt.assert_almost_equal(C2.spectrum[1, 1], C1.spectrum[1])

    # The relative phases should be equal
    npt.assert_almost_equal(C2.phase[0, 1], C1.relative_phases[0, 1])
    # But not the absolute phases (which have the same shape):
    npt.assert_equal(C1.phases[0].shape, C1.relative_phases[0, 1].shape)

    # The delay is equal:
    npt.assert_almost_equal(C2.delay[0, 1], C1.delay[0, 1])
    # Make sure that you would get an error if you provided a method other than
    # 'welch':
    npt.assert_raises(ValueError, nta.SparseCoherenceAnalyzer, T,
                                                method=dict(this_method='foo'))


def test_MTCoherenceAnalyzer():
    """Test the functionality of the multi-taper spectral coherence """

    Fs = np.pi
    t = np.arange(256)
    x = np.sin(10 * t) + np.random.rand(t.shape[-1])
    y = np.sin(10 * t) + np.random.rand(t.shape[-1])
    T = ts.TimeSeries(np.vstack([x, y]), sampling_rate=Fs)
    n_series = T.shape[0]
    NFFT = t.shape[0] // 2 + 1
    for adaptive in [True, False]:
        C = nta.MTCoherenceAnalyzer(T, adaptive=adaptive)
        npt.assert_equal(C.frequencies.shape[0], NFFT)
        npt.assert_equal(C.coherence.shape, (n_series, n_series, NFFT))
        npt.assert_equal(C.confidence_interval.shape, (n_series, n_series,
                                                       NFFT))


@npt.dec.skipif(old_python)
def test_warn_short_tseries():
    """

    A warning is provided when the time-series is shorter than
    the NFFT + n_overlap.

    The implementation of this test is based on this:
    http://docs.python.org/library/warnings.html#testing-warnings

    """

    with warnings.catch_warnings(record=True) as w:
        # Cause all warnings to always be triggered.
        warnings.simplefilter("always")
        # Trigger a warning.
        # The following should throw a warning, because 70 is smaller than the
        # default NFFT=64 + n_overlap=32:
        nta.CoherenceAnalyzer(ts.TimeSeries(np.random.rand(2, 70),
                                            sampling_rate=1))
        # Verify some things
        npt.assert_equal(len(w), 1)


def test_SeedCoherenceAnalyzer():
    """ Test the SeedCoherenceAnalyzer """
    methods = (None,
           {"this_method": 'welch', "NFFT": 256},
           {"this_method": 'multi_taper_csd'},
           {"this_method": 'periodogram_csd', "NFFT": 256})

    Fs = np.pi
    t = np.arange(256)
    seed1 = np.sin(10 * t) + np.random.rand(t.shape[-1])
    seed2 = np.sin(10 * t) + np.random.rand(t.shape[-1])
    target = np.sin(10 * t) + np.random.rand(t.shape[-1])
    T = ts.TimeSeries(np.vstack([seed1, target]), sampling_rate=Fs)
    T_seed1 = ts.TimeSeries(seed1, sampling_rate=Fs)
    T_seed2 = ts.TimeSeries(np.vstack([seed1, seed2]), sampling_rate=Fs)
    T_target = ts.TimeSeries(np.vstack([seed1, target]), sampling_rate=Fs)
    for this_method in methods:
        if this_method is None or this_method['this_method'] == 'welch':
            C1 = nta.CoherenceAnalyzer(T, method=this_method)
            C2 = nta.SeedCoherenceAnalyzer(T_seed1, T_target,
                                           method=this_method)
            C3 = nta.SeedCoherenceAnalyzer(T_seed2, T_target,
                                           method=this_method)

            npt.assert_almost_equal(C1.coherence[0, 1], C2.coherence[1])
            npt.assert_almost_equal(C2.coherence[1], C3.coherence[0, 1])
            npt.assert_almost_equal(C1.phase[0, 1], C2.relative_phases[1])
            npt.assert_almost_equal(C1.delay[0, 1], C2.delay[1])

        else:
            npt.assert_raises(ValueError, nta.SeedCoherenceAnalyzer, T_seed1,
                              T_target, this_method)


def test_SeedCoherenceAnalyzer_same_Fs():
    """

    Providing two time-series with different sampling rates throws an error

    """

    Fs1 = np.pi
    Fs2 = 2 * np.pi
    t = np.arange(256)

    T1 = ts.TimeSeries(np.random.rand(t.shape[-1]),
                       sampling_rate=Fs1)

    T2 = ts.TimeSeries(np.random.rand(t.shape[-1]),
                       sampling_rate=Fs2)

    npt.assert_raises(ValueError, nta.SeedCoherenceAnalyzer, T1, T2)

########NEW FILE########
__FILENAME__ = test_correlation
import numpy as np
import numpy.testing as npt

import nitime.timeseries as ts
import nitime.analysis as nta


def test_SeedCorrelationAnalyzer():

    targ = ts.TimeSeries(np.random.rand(10, 10), sampling_interval=1)

    # Test single source case
    seed = ts.TimeSeries(np.random.rand(10), sampling_interval=1)
    corr = nta.SeedCorrelationAnalyzer(seed, targ)
    our_coef_array = corr.corrcoef
    np_coef_array = np.array([np.corrcoef(seed.data, a)[0, 1] for a in targ.data])

    npt.assert_array_almost_equal(our_coef_array, np_coef_array)

    # Test multiple sources
    seed = ts.TimeSeries(np.random.rand(2, 10), sampling_interval=1)
    corr = nta.SeedCorrelationAnalyzer(seed, targ)
    our_coef_array = corr.corrcoef
    for source in [0, 1]:
        np_coef_array = np.array(
            [np.corrcoef(seed.data[source], a)[0, 1] for a in targ.data])
        npt.assert_array_almost_equal(our_coef_array[source], np_coef_array)

########NEW FILE########
__FILENAME__ = test_granger
"""
Testing the analysis.granger submodule

"""


import numpy as np
import numpy.testing as npt

import nitime.analysis.granger as gc
import nitime.utils as utils
import nitime.timeseries as ts


def test_model_fit():
    """
    Testing of model fitting procedure of the nitime.analysis.granger module.
    """
    # Start by generating some MAR processes (according to Ding and Bressler),
    a1 = np.array([[0.9, 0],
                   [0.16, 0.8]])

    a2 = np.array([[-0.5, 0],
                   [-0.2, -0.5]])

    am = np.array([-a1, -a2])

    x_var = 1
    y_var = 0.7
    xy_cov = 0.4
    cov = np.array([[x_var, xy_cov],
                    [xy_cov, y_var]])

    #Number of realizations of the process
    N = 500
    #Length of each realization:
    L = 1024

    order = am.shape[0]
    n_lags = order + 1

    n_process = am.shape[-1]

    z = np.empty((N, n_process, L))
    nz = np.empty((N, n_process, L))

    for i in range(N):
        z[i], nz[i] = utils.generate_mar(am, cov, L)

    # First we test that the model fitting procedure recovers the coefficients,
    # on average:
    Rxx = np.empty((N, n_process, n_process, n_lags))
    coef = np.empty((N, n_process, n_process, order))
    ecov = np.empty((N, n_process, n_process))

    for i in range(N):
        this_order, this_Rxx, this_coef, this_ecov = gc.fit_model(z[i][0],
                                                                  z[i][1],
                                                                  order=2)
        Rxx[i] = this_Rxx
        coef[i] = this_coef
        ecov[i] = this_ecov

    npt.assert_almost_equal(cov, np.mean(ecov, axis=0), decimal=1)
    npt.assert_almost_equal(am, np.mean(coef, axis=0), decimal=1)

    # Next we test that the automatic model order estimation procedure works:
    est_order = []
    for i in range(N):
        this_order, this_Rxx, this_coef, this_ecov = gc.fit_model(z[i][0],
                                                                  z[i][1])
        est_order.append(this_order)

    npt.assert_almost_equal(order, np.mean(est_order), decimal=1)


def test_GrangerAnalyzer():
    """
    Testing the GrangerAnalyzer class, which simplifies calculations of related
    quantities
    """

    # Start by generating some MAR processes (according to Ding and Bressler),
    a1 = np.array([[0.9, 0],
                   [0.16, 0.8]])

    a2 = np.array([[-0.5, 0],
                   [-0.2, -0.5]])

    am = np.array([-a1, -a2])

    x_var = 1
    y_var = 0.7
    xy_cov = 0.4
    cov = np.array([[x_var, xy_cov],
                    [xy_cov, y_var]])

    L = 1024
    z, nz = utils.generate_mar(am, cov, L)

    # Move on to testing the Analyzer object itself:
    ts1 = ts.TimeSeries(data=z, sampling_rate=np.pi)
    g1 = gc.GrangerAnalyzer(ts1)

    # Check that things have the right shapes:
    npt.assert_equal(g1.frequencies.shape[-1], g1._n_freqs // 2 + 1)
    npt.assert_equal(g1.causality_xy[0, 1].shape, g1.causality_yx[0, 1].shape)

    # Test inputting ij:
    g2 = gc.GrangerAnalyzer(ts1, ij=[(0, 1), (1, 0)])

    # x => y for one is like y => x for the other:
    npt.assert_almost_equal(g1.causality_yx[1, 0], g2.causality_xy[0, 1])

########NEW FILE########
__FILENAME__ = test_snr
import numpy as np
import numpy.testing as npt
import matplotlib.mlab as mlab

import nitime.timeseries as ts
import nitime.analysis as nta


def test_SNRAnalyzer():
    Fs = np.pi
    t = np.arange(1024)
    x = np.sin(10 * t) + np.random.rand(t.shape[-1])
    y = np.sin(10 * t) + np.random.rand(t.shape[-1])

    T = ts.TimeSeries(np.vstack([x, y]), sampling_rate=Fs)

    MT = nta.MTCoherenceAnalyzer(T)
    SNR = nta.SNRAnalyzer(T)
    MT_signal = nta.SpectralAnalyzer(ts.TimeSeries(np.mean(T.data, 0),
                                                   sampling_rate=Fs))

    npt.assert_equal(SNR.mt_frequencies, MT.frequencies)
    npt.assert_equal(SNR.signal, np.mean(T.data, 0))
    f, c = MT_signal.spectrum_multi_taper
    npt.assert_almost_equal(SNR.mt_signal_psd, c)

########NEW FILE########
__FILENAME__ = descriptors
"""Descriptor support for NIPY.

Utilities to support special Python descriptors [1,2], in particular the use of
a useful pattern for properties we call 'one time properties'.  These are
object attributes which are declared as properties, but become regular
attributes once they've been read the first time.  They can thus be evaluated
later in the object's life cycle, but once evaluated they become normal, static
attributes with no function call overhead on access or any other constraints.

A special ResetMixin class is provided to add a .reset() method to users who
may want to have their objects capable of resetting these computed properties
to their 'untriggered' state.

References
----------
[1] How-To Guide for Descriptors, Raymond
Hettinger. http://users.rcn.com/python/download/Descriptor.htm

[2] Python data model, http://docs.python.org/reference/datamodel.html
"""

#-----------------------------------------------------------------------------
# Classes and Functions
#-----------------------------------------------------------------------------


class ResetMixin(object):
    """A Mixin class to add a .reset() method to users of OneTimeProperty.

    By default, auto attributes once computed, become static.  If they happen
    to depend on other parts of an object and those parts change, their values
    may now be invalid.

    This class offers a .reset() method that users can call *explicitly* when
    they know the state of their objects may have changed and they want to
    ensure that *all* their special attributes should be invalidated.  Once
    reset() is called, all their auto attributes are reset to their
    OneTimeProperty descriptors, and their accessor functions will be triggered
    again.

    Warning
    -------

    If a class has a set of attributes that are OneTimeProperty, but that can
    be initialized from any one of them, do NOT use this mixin!  For instance,
    UniformTimeSeries can be initialized with only sampling_rate and t0,
    sampling_interval and time are auto-computed.  But if you were to reset() a
    UniformTimeSeries, it would lose all 4, and there would be then no way to
    break the circular dependency chains.

    If this becomes a problem in practice (for our analyzer objects it isn't,
    as they don't have the above pattern), we can extend reset() to check for a
    _no_reset set of names in the instance which are meant to be kept
    protected.  But for now this is NOT done, so caveat emptor.

    Example
    -------

    >>> class A(ResetMixin):
    ...     def __init__(self,x=1.0):
    ...         self.x = x
    ...
    ...     @auto_attr
    ...     def y(self):
    ...         print('*** y computation executed ***')
    ...         return self.x / 2.0
    ...

    >>> a = A(10)

    About to access y twice, the second time no computation is done:
    >>> a.y
    *** y computation executed ***
    5.0
    >>> a.y
    5.0

    Changing x
    >>> a.x = 20

    a.y doesn't change to 10, since it is a static attribute:
    >>> a.y
    5.0

    We now reset a, and this will then force all auto attributes to recompute
    the next time we access them:
    >>> a.reset()

    About to access y twice again after reset():
    >>> a.y
    *** y computation executed ***
    10.0
    >>> a.y
    10.0
    """

    def reset(self):
        """Reset all OneTimeProperty attributes that may have fired already."""
        instdict = self.__dict__
        classdict = self.__class__.__dict__
        # To reset them, we simply remove them from the instance dict.  At that
        # point, it's as if they had never been computed.  On the next access,
        # the accessor function from the parent class will be called, simply
        # because that's how the python descriptor protocol works.
        for mname, mval in classdict.items():
            if mname in instdict and isinstance(mval, OneTimeProperty):
                delattr(self, mname)


class OneTimeProperty(object):
    """A descriptor to make special properties that become normal attributes.

    This is meant to be used mostly by the auto_attr decorator in this module.
    """
    def __init__(self, func):
        """Create a OneTimeProperty instance.

        Parameters
        ----------
          func : method

          The method that will be called the first time to compute a value.
          Afterwards, the method's name will be a standard attribute holding
          the value of this computation.
        """
        self.getter = func
        self.name = func.__name__

    def __get__(self, obj, type=None):
        """This will be called on attribute access on the class or instance."""

        if obj is None:
            # Being called on the class, return the original function. This
            # way, introspection works on the class.
            # return func
            return self.getter

        # Errors in the following line are errors in setting a
        # OneTimeProperty
        val = self.getter(obj)

        setattr(obj, self.name, val)
        return val


def auto_attr(func):
    """Decorator to create OneTimeProperty attributes.

    Parameters
    ----------
      func : method
        The method that will be called the first time to compute a value.
        Afterwards, the method's name will be a standard attribute holding the
        value of this computation.

    Examples
    --------
    >>> class MagicProp(object):
    ...     @auto_attr
    ...     def a(self):
    ...         return 99
    ...
    >>> x = MagicProp()
    >>> 'a' in x.__dict__
    False
    >>> x.a
    99
    >>> 'a' in x.__dict__
    True
    """
    return OneTimeProperty(func)


#-----------------------------------------------------------------------------
# Deprecated API
#-----------------------------------------------------------------------------

# For backwards compatibility
setattr_on_read = auto_attr

########NEW FILE########
__FILENAME__ = hrf
from __future__ import print_function
import numpy as np
from scipy.misc import factorial


def gamma_hrf(duration, A=1., tau=1.08, n=3, delta=2.05, Fs=1.0):
    r"""A gamma function hrf model, with two parameters, based on
    [Boynton1996]_


    Parameters
    ----------

    duration: float
        the length of the HRF (in the inverse units of the sampling rate)

    A: float
        a scaling factor, sets the max of the function, defaults to 1

    tau: float
        The time constant of the gamma function, defaults to 1.08

    n: int
        The phase delay of the gamma function, defaults to 3

    delta: float
        A pure delay, allowing for an additional delay from the onset of the
        time-series to the beginning of the gamma hrf, defaults to 2.05

    Fs: float
        The sampling rate, defaults to 1.0


    Returns
    -------

    h: the gamma function hrf, as a function of time

    Notes
    -----
    This is based on equation 3 in Boynton (1996):

    .. math::

        h(t) =
        \frac{(\frac{t-\delta}{\tau})^{(n-1)}
        e^{-(\frac{t-\delta}{\tau})}}{\tau(n-1)!}


    Geoffrey M. Boynton, Stephen A. Engel, Gary H. Glover and David J. Heeger
    (1996). Linear Systems Analysis of Functional Magnetic Resonance Imaging in
    Human V1. J Neurosci 16: 4207-4221

    """
    # XXX Maybe change to take out the time (Fs, duration, etc) from this and
    # instead implement this in units of sampling interval (pushing the time
    # aspect to the higher level)?
    if type(n) is not int:
        print(('gamma_hrf received unusual input, converting n from %s to %i'
               % (str(n), int(n))))

        n = int(n)

    #Prevent negative delta values:
    if delta < 0:
        raise ValueError('in gamma_hrf, delta cannot be smaller than 0')

    #Prevent cases in which the delta is larger than the entire hrf:
    if delta > duration:
        e_s = 'in gamma_hrf, delta cannot be larger than the duration'
        raise ValueError(e_s)

    t_max = duration - delta

    t = np.hstack([np.zeros((delta * Fs)), np.linspace(0, t_max, t_max * Fs)])

    t_tau = t / tau

    h = (t_tau ** (n - 1) * np.exp(-1 * (t_tau)) /
         (tau * factorial(n - 1)))

    return A * h / max(h)


def polonsky_hrf(A, B, tau1, f1, tau2, f2, t_max, Fs=1.0):
    r""" HRF based on Polonsky (2000):

    .. math::

       H(t) = exp(\frac{-t}{\tau_1}) sin(2\cdot\pi f_1 \cdot t) -a\cdot
       exp(-\frac{t}{\tau_2})*sin(2\pi f_2 t)

    Alex Polonsky, Randolph Blake, Jochen Braun and David J. Heeger
    (2000). Neuronal activity in human primary visual cortex correlates with
    perception during binocular rivalry. Nature Neuroscience 3: 1153-1159

    """
    sampling_interval = 1 / float(Fs)

    t = np.arange(0, t_max, sampling_interval)

    h = (np.exp(-t / tau1) * np.sin(2 * np.pi * f1 * t) -
            (B * np.exp(-t / tau2) * np.sin(2 * np.pi * f2 * t)))

    return A * h / max(h)

########NEW FILE########
__FILENAME__ = io
""" Input and output for fmri data files"""
from __future__ import print_function

try:
    from nibabel import load
except ImportError:
    e_s = "nibabel required for fmri I/O. See http://nipy.org/nibabel"
    raise ImportError(e_s)

import nitime.timeseries as ts
import nitime.analysis as tsa
import numpy as np


def time_series_from_file(nifti_files, coords=None, TR=None, normalize=None,
                          average=False, filter=None, verbose=False):
    """ Make a time series from a Analyze file, provided coordinates into the
            file

    Parameters
    ----------

    nifti_files: a string or a list/tuple of strings.
        The full path(s) to the file(s) from which the time-series is (are)
        extracted

    coords: ndarray or list/tuple of ndarray, optional.
        x,y,z (inplane,inplane,slice) coordinates of the ROI(s) from which the
        time-series is (are) derived. If coordinates are provided, the
        resulting time-series object will have 2 dimentsions. The first is the
        coordinate dimension, in order of the provided coordinates and the
        second is time. If set to None, all the coords in the volume will be
        used and the coordinate system will be preserved - the output will be 4
        dimensional, with time as the last dimension.

    TR: float or TimeArray, optional
        The TR of the fmri measurement. The units are seconds, if provided as a float
        argument. Otherwise, in the units of the TimeArray object
        provided. Default: 1 second.

    normalize: bool, optional
        Whether to normalize the activity in each voxel, defaults to
        None, in which case the original fMRI signal is used. Other options
        are: 'percent': the activity in each voxel is converted to percent
        change, relative to this scan. 'zscore': the activity is converted to a
        zscore relative to the mean and std in this voxel in this scan.

    average: bool, optional whether to average the time-series across the
        voxels in the ROI (assumed to be the first dimension). In which
        case, TS.data will be 1-d

    filter: dict, optional
       If provided with a dict of the form:

       {'lb':float or 0, 'ub':float or None, 'method':'fourier','boxcar' 'fir'
       or 'iir' }

       each voxel's data will be filtered into the frequency range [lb,ub] with
       nitime.analysis.FilterAnalyzer, using the method chosen here (defaults
       to 'fir')

    verbose: Whether to report on ROI and file being read.

    Returns
    -------

    time-series object

    Note
    ----

    Normalization occurs before averaging on a voxel-by-voxel basis, followed
    by the averaging.

    """

    # The default behavior is to assume that the TR is one second:
    if TR is None:
        TR = 1.0

    if normalize is not None:
        if normalize not in ('percent', 'zscore'):
            e_s = "Normalization of fMRI time-series can only be done"
            e_s += " using 'percent' or 'zscore' as input"
            raise ValueError(e_s)
    #If just one string was provided:
    if isinstance(nifti_files, str):
        if verbose:
            print("Reading %s" % nifti_files)
        im = load(nifti_files)
        data = im.get_data()
        # If coordinates are provided as input, read data only from these coordinates:
        if coords is not None:
            #If the input is the coords of several ROIs
            if isinstance(coords, tuple) or isinstance(coords, list):
                n_roi = len(coords)
                tseries = [[]] * n_roi
                for i in range(n_roi):
                    tseries[i] = _tseries_from_nifti_helper(
                        np.array(coords[i]).astype(int),
                        data,
                        TR,
                        filter,
                        normalize,
                        average)
            else:
                tseries = _tseries_from_nifti_helper(coords.astype(int), data, TR,
                                                     filter, normalize, average)

        # The default behavior reads in all the coordinates in the volume:
        else:
            tseries = _tseries_from_nifti_helper(coords, data, TR,
                                                     filter, normalize, average)
    #Otherwise loop over the files and concatenate:
    elif isinstance(nifti_files, tuple) or isinstance(nifti_files, list):
        tseries_list = []
        for f in nifti_files:
            if verbose:
                print("Reading %s" % f)
            im = load(f)
            data = im.get_data()
            if coords is not None:
                #If the input is the coords of several ROIs
                if isinstance(coords, tuple) or isinstance(coords, list):
                    n_roi = len(coords)
                    tseries_list.append([[]] * n_roi)
                    for i in range(n_roi):
                        tseries_list[-1][i] = _tseries_from_nifti_helper(
                            np.array(coords[i]).astype(int),
                            data,
                            TR,
                            filter,
                            normalize,
                            average)

                else:
                    tseries_list.append(_tseries_from_nifti_helper(
                        np.array(coords).astype(int),
                        data,
                        TR,
                        filter,
                        normalize,
                        average))

            # The default behavior reads in all the coordinates in the volume:
            else:
                tseries_list.append(_tseries_from_nifti_helper(coords, data, TR,
                                                               filter, normalize, average))


        #Concatenate the time-series from the different scans:
        if isinstance(coords, tuple) or isinstance(coords, list):
            tseries = [[]] * n_roi
            #Do this per ROI
            for i in range(n_roi):
                tseries[i] = ts.concatenate_time_series(
                    [tseries_list[k][i] for k in range(len(tseries_list))])

        else:
            tseries = ts.concatenate_time_series(tseries_list)

    return tseries


def _tseries_from_nifti_helper(coords, data, TR, filter, normalize, average):
    """

    Helper function for the function time_series_from_nifti, which does the
    core operations of pulling out data from a data array given coords and then
    normalizing and averaging if needed

    """
    if coords is not None:
        out_data = np.asarray(data[coords[0], coords[1], coords[2]])
    else:
        out_data = data

    tseries = ts.TimeSeries(out_data, sampling_interval=TR)

    if filter is not None:
        if filter['method'] not in ('boxcar', 'fourier', 'fir', 'iir'):
            e_s = "Filter method %s is not recognized" % filter['method']
            raise ValueError(e_s)
        else:
            #Construct the key-word arguments to FilterAnalyzer:
            kwargs = dict(lb=filter.get('lb', 0),
                          ub=filter.get('ub', None),
                          boxcar_iterations=filter.get('boxcar_iterations', 2),
                          filt_order=filter.get('filt_order', 64),
                          gpass=filter.get('gpass', 1),
                          gstop=filter.get('gstop', 60),
                          iir_ftype=filter.get('iir_ftype', 'ellip'),
                          fir_win=filter.get('fir_win', 'hamming'))

            F = tsa.FilterAnalyzer(tseries, **kwargs)

        if filter['method'] == 'boxcar':
            tseries = F.filtered_boxcar
        elif filter['method'] == 'fourier':
            tseries = F.filtered_fourier
        elif filter['method'] == 'fir':
            tseries = F.fir
        elif filter['method'] == 'iir':
            tseries = F.iir

    if normalize == 'percent':
        tseries = tsa.NormalizationAnalyzer(tseries).percent_change
    elif normalize == 'zscore':
        tseries = tsa.NormalizationAnalyzer(tseries).z_score

    if average:
        if coords is None:
            tseries.data = np.mean(np.reshape(tseries.data,
                                              (np.array(tseries.shape[:-1]).prod(),
                                               tseries.shape[-1])),0)
        else:
            tseries.data = np.mean(tseries.data, 0)

    return tseries


def nifti_from_time_series(volume, coords, time_series, nifti_path):
    """Makes a Nifti file out of a time_series object

    Parameters
    ----------

    volume: list (3-d, or 4-d)
        The total size of the nifti image to be created

    coords: 3*n_coords array
        The coords into which the time_series will be inserted. These need to
        be given in the order in which the time_series is organized

    time_series: a time-series object
       The time-series to be inserted into the file

    nifti_path: the full path to the file name which will be created
    """
    # XXX Implement!
    raise NotImplementedError

########NEW FILE########
__FILENAME__ = test_io
"""

Test the io submodule of the fMRI module of nitime

"""
import os

import numpy as np
import numpy.testing as npt

import nitime
import nitime.timeseries as ts

#Skip the tests if you can't import nibabel:
try:
    import nitime.fmri.io as io
    no_nibabel = False
    no_nibabel_msg=''
except ImportError as e:
    no_nibabel = True
    no_nibabel_msg=e.args[0]

data_path = os.path.join(nitime.__path__[0],'data')

@npt.dec.skipif(no_nibabel,no_nibabel_msg)
def test_time_series_from_file():

    """Testing reading of data from nifti files, using nibabel"""

    TR = 1.35
    ts_ff = io.time_series_from_file

    #File names:
    fmri_file1 = os.path.join(data_path,'fmri1.nii.gz')
    fmri_file2 = os.path.join(data_path,'fmri2.nii.gz')

    #Spatial coordinates into the volumes:
    coords1 = np.array([[5,5,5,5],[5,5,5,5],[1,2,3,4]])
    coords2 = np.array([[6,6,6,6],[6,6,6,6],[3,4,5,6]])

    #No averaging, no normalization:
    t1 = ts_ff([fmri_file1,fmri_file2],[coords1,coords2],TR)

    npt.assert_equal(t1[0].shape,(4,80))  # 4 coordinates, 80 time-points

    t2 = ts_ff([fmri_file1,fmri_file2],[coords1,coords2],TR,average=True)

    npt.assert_equal(t2[0].shape,(80,))  # collapse coordinates,80 time-points

    t3 = ts_ff(fmri_file1,coords1,TR,normalize='zscore')

    #The mean of each channel should be almost equal to 0:
    npt.assert_almost_equal(t3.data[0].mean(),0)
    #And the standard deviation should be almost equal to 1:
    npt.assert_almost_equal(t3.data[0].std(),1)

    t4 = ts_ff(fmri_file1,coords1,TR,normalize='percent')

    #In this case, the average is almost equal to 0, but no constraint on the
    #std:
    npt.assert_almost_equal(t4.data[0].mean(),0)

    #Make sure that we didn't mess up the sampling interval:
    npt.assert_equal(t4.sampling_interval,nitime.TimeArray(1.35))

    # Test the default behavior:
    data = io.load(fmri_file1).get_data()
    t5 = ts_ff(fmri_file1)
    npt.assert_equal(t5.shape, data.shape)
    npt.assert_equal(t5.sampling_interval, ts.TimeArray(1, time_unit='s'))

    # Test initializing TR with a TimeArray:
    t6= ts_ff(fmri_file1, TR=ts.TimeArray(1350, time_unit='ms'))
    npt.assert_equal(t4.sampling_interval, t6.sampling_interval)

    # Check the concatenation dimensions:
    t7 = ts_ff([fmri_file1, fmri_file2])
    npt.assert_equal([t7.shape[:3], t7.shape[-1]], [data.shape[:3], data.shape[-1]*2])

    t8 = ts_ff([fmri_file1, fmri_file2], average=True)
    npt.assert_equal(t8.shape[0], data.shape[-1]*2)

    t9 = ts_ff([fmri_file1, fmri_file2], average=True, normalize='zscore')
    npt.assert_almost_equal(t9.data.mean(), 0)

########NEW FILE########
__FILENAME__ = index_utils
""" Utilities for indexing into 2-d arrays, brought in from numpy 1.4, to
support use of older versions of numpy
"""

__all__ = ['tri', 'triu', 'tril', 'mask_indices', 'tril_indices',
           'tril_indices_from', 'triu_indices', 'triu_indices_from',
           ]

from numpy.core.numeric import asanyarray, subtract, arange, \
     greater_equal, multiply, ones, asarray, where

# Need to import numpy for the doctests! 
import numpy as np  

def tri(N, M=None, k=0, dtype=float):
    """
    Construct an array filled with ones at and below the given diagonal.

    Parameters
    ----------
    N : int
        Number of rows in the array.
    M : int, optional
        Number of columns in the array.
        By default, `M` is taken equal to `N`.
    k : int, optional
        The sub-diagonal below which the array is filled.
        `k` = 0 is the main diagonal, while `k` < 0 is below it,
        and `k` > 0 is above.  The default is 0.
    dtype : dtype, optional
        Data type of the returned array.  The default is float.

    Returns
    -------
    T : (N,M) ndarray
        Array with a lower triangle filled with ones, in other words
        ``T[i,j] == 1`` for ``i <= j + k``.

    Examples
    --------
    >>> np.tri(3, 5, 2, dtype=int)
    array([[1, 1, 1, 0, 0],
           [1, 1, 1, 1, 0],
           [1, 1, 1, 1, 1]])

    >>> np.tri(3, 5, -1)
    array([[ 0.,  0.,  0.,  0.,  0.],
           [ 1.,  0.,  0.,  0.,  0.],
           [ 1.,  1.,  0.,  0.,  0.]])

    """
    if M is None: M = N
    m = greater_equal(subtract.outer(arange(N), arange(M)),-k)
    return m.astype(dtype)

def tril(m, k=0):
    """
    Lower triangle of an array.

    Return a copy of an array with elements above the `k`-th diagonal zeroed.

    Parameters
    ----------
    m : array_like, shape (M, N)
        Input array.
    k : int
        Diagonal above which to zero elements.
        `k = 0` is the main diagonal, `k < 0` is below it and `k > 0` is above.

    Returns
    -------
    L : ndarray, shape (M, N)
        Lower triangle of `m`, of same shape and data-type as `m`.

    See Also
    --------
    triu

    Examples
    --------
    >>> np.tril([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)
    array([[ 0,  0,  0],
           [ 4,  0,  0],
           [ 7,  8,  0],
           [10, 11, 12]])

    """
    m = asanyarray(m)
    out = multiply(tri(m.shape[0], m.shape[1], k=k, dtype=int),m)
    return out

def triu(m, k=0):
    """
    Upper triangle of an array.

    Construct a copy of a matrix with elements below the k-th diagonal zeroed.

    Please refer to the documentation for `tril`.

    See Also
    --------
    tril

    Examples
    --------
    >>> np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)
    array([[ 1,  2,  3],
           [ 4,  5,  6],
           [ 0,  8,  9],
           [ 0,  0, 12]])

    """
    m = asanyarray(m)
    out = multiply((1-tri(m.shape[0], m.shape[1], k-1, int)),m)
    return out

# borrowed from John Hunter and matplotlib
def vander(x, N=None):
    """
    Generate a Van der Monde matrix.

    The columns of the output matrix are decreasing powers of the input
    vector.  Specifically, the i-th output column is the input vector to
    the power of ``N - i - 1``. Such a matrix with a geometric progression
    in each row is named Van Der Monde, or Vandermonde matrix, from
    Alexandre-Theophile Vandermonde.

    Parameters
    ----------
    x : array_like
        1-D input array.
    N : int, optional
        Order of (number of columns in) the output. If `N` is not specified,
        a square array is returned (``N = len(x)``).

    Returns
    -------
    out : ndarray
        Van der Monde matrix of order `N`.  The first column is ``x^(N-1)``,
        the second ``x^(N-2)`` and so forth.

    References
    ----------
    .. [1] Wikipedia, "Vandermonde matrix",
           http://en.wikipedia.org/wiki/Vandermonde_matrix

    Examples
    --------
    >>> x = np.array([1, 2, 3, 5])
    >>> N = 3
    >>> np.vander(x, N)
    array([[ 1,  1,  1],
           [ 4,  2,  1],
           [ 9,  3,  1],
           [25,  5,  1]])

    >>> np.column_stack([x**(N-1-i) for i in range(N)])
    array([[ 1,  1,  1],
           [ 4,  2,  1],
           [ 9,  3,  1],
           [25,  5,  1]])

    >>> x = np.array([1, 2, 3, 5])
    >>> np.vander(x)
    array([[  1,   1,   1,   1],
           [  8,   4,   2,   1],
           [ 27,   9,   3,   1],
           [125,  25,   5,   1]])

    """
    x = asarray(x)
    if N is None: N=len(x)
    X = ones( (len(x),N), x.dtype)
    for i in range(N-1):
        X[:,i] = x**(N-i-1)
    return X


def histogram2d(x,y, bins=10, range=None, normed=False, weights=None):
    """
    Compute the bi-dimensional histogram of two data samples.

    Parameters
    ----------
    x : array_like, shape(N,)
        A sequence of values to be histogrammed along the first dimension.
    y : array_like, shape(M,)
        A sequence of values to be histogrammed along the second dimension.
    bins : int or [int, int] or array_like or [array, array], optional
        The bin specification:

          * If int, the number of bins for the two dimensions (nx=ny=bins).
          * If [int, int], the number of bins in each dimension (nx, ny = bins).
          * If array_like, the bin edges for the two dimensions (x_edges=y_edges=bins).
          * If [array, array], the bin edges in each dimension (x_edges, y_edges = bins).

    range : array_like, shape(2,2), optional
        The leftmost and rightmost edges of the bins along each dimension
        (if not specified explicitly in the `bins` parameters):
        ``[[xmin, xmax], [ymin, ymax]]``. All values outside of this range
        will be considered outliers and not tallied in the histogram.
    normed : bool, optional
        If False, returns the number of samples in each bin. If True, returns
        the bin density, i.e. the bin count divided by the bin area.
    weights : array_like, shape(N,), optional
        An array of values ``w_i`` weighing each sample ``(x_i, y_i)``. Weights
        are normalized to 1 if `normed` is True. If `normed` is False, the
        values of the returned histogram are equal to the sum of the weights
        belonging to the samples falling into each bin.

    Returns
    -------
    H : ndarray, shape(nx, ny)
        The bi-dimensional histogram of samples `x` and `y`. Values in `x`
        are histogrammed along the first dimension and values in `y` are
        histogrammed along the second dimension.
    xedges : ndarray, shape(nx,)
        The bin edges along the first dimension.
    yedges : ndarray, shape(ny,)
        The bin edges along the second dimension.

    See Also
    --------
    histogram: 1D histogram
    histogramdd: Multidimensional histogram

    Notes
    -----
    When `normed` is True, then the returned histogram is the sample density,
    defined such that:

    .. math::
      \\sum_{i=0}^{nx-1} \\sum_{j=0}^{ny-1} H_{i,j} \\Delta x_i \\Delta y_j = 1

    where `H` is the histogram array and :math:`\\Delta x_i \\Delta y_i`
    the area of bin `{i,j}`.

    Please note that the histogram does not follow the Cartesian convention
    where `x` values are on the abcissa and `y` values on the ordinate axis.
    Rather, `x` is histogrammed along the first dimension of the array
    (vertical), and `y` along the second dimension of the array (horizontal).
    This ensures compatibility with `histogramdd`.

    Examples
    --------
    >>> x, y = np.random.randn(2, 100)
    >>> H, xedges, yedges = np.histogram2d(x, y, bins=(5, 8))
    >>> H.shape, xedges.shape, yedges.shape
    ((5, 8), (6,), (9,))

    """
    from numpy import histogramdd

    try:
        N = len(bins)
    except TypeError:
        N = 1

    if N != 1 and N != 2:
        xedges = yedges = asarray(bins, float)
        bins = [xedges, yedges]
    hist, edges = histogramdd([x,y], bins, range, normed, weights)
    return hist, edges[0], edges[1]


def mask_indices(n,mask_func,k=0):
    """
    Return the indices to access (n, n) arrays, given a masking function.

    Assume `mask_func` is a function that, for a square array a of size
    ``(n, n)`` with a possible offset argument `k`, when called as
    ``mask_func(a, k)`` returns a new array with zeros in certain locations
    (functions like `triu` or `tril` do precisely this). Then this function
    returns the indices where the non-zero values would be located.

    Parameters
    ----------
    n : int
        The returned indices will be valid to access arrays of shape (n, n).
    mask_func : callable
        A function whose call signature is similar to that of `triu`, `tril`.
        That is, ``mask_func(x, k)`` returns a boolean array, shaped like `x`.
        `k` is an optional argument to the function.
    k : scalar
        An optional argument which is passed through to `mask_func`. Functions
        like `triu`, `tril` take a second argument that is interpreted as an
        offset.

    Returns
    -------
    indices : tuple of arrays.
        The `n` arrays of indices corresponding to the locations where
        ``mask_func(np.ones((n, n)), k)`` is True.

    See Also
    --------
    triu, tril, triu_indices, tril_indices

    Notes
    -----
    .. versionadded:: 1.4.0

    Examples
    --------
    These are the indices that would allow you to access the upper triangular
    part of any 3x3 array:

    >>> iu = np.mask_indices(3, np.triu)

    For example, if `a` is a 3x3 array:

    >>> a = np.arange(9).reshape(3, 3)
    >>> a
    array([[0, 1, 2],
           [3, 4, 5],
           [6, 7, 8]])
    >>> a[iu]
    array([0, 1, 2, 4, 5, 8])

    An offset can be passed also to the masking function.  This gets us the
    indices starting on the first diagonal right of the main one:

    >>> iu1 = np.mask_indices(3, np.triu, 1)

    with which we now extract only three elements:

    >>> a[iu1]
    array([1, 2, 5])

    """
    m = ones((n,n),int)
    a = mask_func(m,k)
    return where(a != 0)


def tril_indices(n,k=0):
    """
    Return the indices for the lower-triangle of an (n, n) array.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.
    k : int, optional
      Diagonal offset (see `tril` for details).

    Returns
    -------
    inds : tuple of arrays
        The indices for the triangle. The returned tuple contains two arrays,
        each with the indices along one dimension of the array.

    See also
    --------
    triu_indices : similar function, for upper-triangular.
    mask_indices : generic function accepting an arbitrary mask function.
    tril, triu

    Notes
    -----
    .. versionadded:: 1.4.0

    Examples
    --------
    Compute two different sets of indices to access 4x4 arrays, one for the
    lower triangular part starting at the main diagonal, and one starting two
    diagonals further right:

    >>> il1 = np.tril_indices(4)
    >>> il2 = np.tril_indices(4, 2)

    Here is how they can be used with a sample array:

    >>> a = np.arange(16).reshape(4, 4)
    >>> a
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11],
           [12, 13, 14, 15]])

    Both for indexing:

    >>> a[il1]
    array([ 0,  4,  5,  8,  9, 10, 12, 13, 14, 15])

    And for assigning values:

    >>> a[il1] = -1
    >>> a
    array([[-1,  1,  2,  3],
           [-1, -1,  6,  7],
           [-1, -1, -1, 11],
           [-1, -1, -1, -1]])

    These cover almost the whole array (two diagonals right of the main one):

    >>> a[il2] = -10
    >>> a
    array([[-10, -10, -10,   3],
           [-10, -10, -10, -10],
           [-10, -10, -10, -10],
           [-10, -10, -10, -10]])

    """
    return mask_indices(n,tril,k)


def tril_indices_from(arr,k=0):
    """
    Return the indices for the lower-triangle of an (n, n) array.

    See `tril_indices` for full details.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.
    k : int, optional
      Diagonal offset (see `tril` for details).

    See Also
    --------
    tril_indices, tril

    Notes
    -----
    .. versionadded:: 1.4.0

    """
    if not arr.ndim==2 and arr.shape[0] == arr.shape[1]:
        raise ValueError("input array must be 2-d and square")
    return tril_indices(arr.shape[0],k)


def triu_indices(n,k=0):
    """
    Return the indices for the upper-triangle of an (n, n) array.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.
    k : int, optional
      Diagonal offset (see `triu` for details).

    Returns
    -------
    inds : tuple of arrays
        The indices for the triangle. The returned tuple contains two arrays,
        each with the indices along one dimension of the array.

    See also
    --------
    tril_indices : similar function, for lower-triangular.
    mask_indices : generic function accepting an arbitrary mask function.
    triu, tril

    Notes
    -----
    .. versionadded:: 1.4.0

    Examples
    --------
    Compute two different sets of indices to access 4x4 arrays, one for the
    upper triangular part starting at the main diagonal, and one starting two
    diagonals further right:

    >>> iu1 = np.triu_indices(4)
    >>> iu2 = np.triu_indices(4, 2)

    Here is how they can be used with a sample array:

    >>> a = np.arange(16).reshape(4, 4)
    >>> a
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11],
           [12, 13, 14, 15]])

    Both for indexing:

    >>> a[iu1]
    array([ 0,  1,  2,  3,  5,  6,  7, 10, 11, 15])

    And for assigning values:

    >>> a[iu1] = -1
    >>> a
    array([[-1, -1, -1, -1],
           [ 4, -1, -1, -1],
           [ 8,  9, -1, -1],
           [12, 13, 14, -1]])

    These cover only a small part of the whole array (two diagonals right
    of the main one):

    >>> a[iu2] = -10
    >>> a
    array([[ -1,  -1, -10, -10],
           [  4,  -1,  -1, -10],
           [  8,   9,  -1,  -1],
           [ 12,  13,  14,  -1]])

    """
    return mask_indices(n,triu,k)


def triu_indices_from(arr,k=0):
    """
    Return the indices for the lower-triangle of an (n, n) array.

    See `triu_indices` for full details.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.
    k : int, optional
      Diagonal offset (see `triu` for details).

    See Also
    --------
    triu_indices, triu

    Notes
    -----
    .. versionadded:: 1.4.0

    """
    if not arr.ndim==2 and arr.shape[0] == arr.shape[1]:
        raise ValueError("input array must be 2-d and square")
    return triu_indices(arr.shape[0],k)

########NEW FILE########
__FILENAME__ = lazy
"""
Commonly used nitime lazy imports are defined here, so they can be reused
throughout nitime. For an explanation of why we use lazily-loaded modules, and
how you can leverage this machinery in your code, see
:mod:`nitime.lazyimports`.

Lazily-loaded package have almost the same name as the
corresponding package's import string, except for periods are replaced with
underscores. For example, the way to lazily import ``matplotlib.mlab`` is via

    >>> from nitime.lazy import matplotlib_mlab as mlab

At this time, all lazy-loaded packages are defined manually. I (pi) made
several attempts to automate this process, such that any arbitrary package
``foo.bar.baz`` could be imported via ``from nitime.lazy import foo_bar_baz as
baz`` but had limited success.

Currently defined lazy imported packages are (remember to replace the ``.``
with ``_``) ::

    matplotlib.mlab
    scipy
    scipy.fftpack
    scipy.interpolate
    scipy.linalg
    scipy.signal
    scipy.signal.signaltools
    scipy.stats
    scipy.stats.distributions


If you want to lazily load another package in nitime, please add it to this
file, and then ``from nitime.lazy import your_new_package``.

If there's a package that you would like to lazily load in your own code that
is not listed here, use the :class:`LazyImport` class, which is in
:mod:`nitime.lazyimports`.
"""
from .lazyimports import LazyImport

# matplotlib
matplotlib_mlab = LazyImport('matplotlib.mlab')

# scipy
scipy = LazyImport('scipy')
scipy_fftpack = LazyImport('scipy.fftpack')
scipy_interpolate = LazyImport('scipy.interpolate')
scipy_linalg = LazyImport('scipy.linalg')
scipy_signal = LazyImport('scipy.signal')
scipy_signal_signaltools = LazyImport('scipy.signal.signaltools')
scipy_stats = LazyImport('scipy.stats')
scipy_stats_distributions = LazyImport('scipy.stats.distributions')

def enabled():
    "Returns ``True`` if LazyImports are globally enabled"
    import nitime.lazyimports as l
    return not l.disable_lazy_imports

########NEW FILE########
__FILENAME__ = lazyimports
""" This module provides lazy import functionality to improve the import
performance of nitime. For example, some parts of nitime leverage and import
matplotlib, which is quite a big package, yet most of the nitime code does not
depend on matplotlib. By lazily-loading a module, we defer the overhead of
importing it until the first time it is actually used, thereby speeding up
nitime imports.

A generic :class:`LazyImport` class is implemented which takes the module name
as a parameter, and acts as a proxy for that module, importing it only when
the module is used, but effectively acting as the module in every other way
(including inside IPython with respect to introspection and tab completion)
with the *exception* of reload() - reloading a :class:`LazyImport` raises an
:class:`ImportError`.

Commonly used nitime lazy imports are also defined in :mod:`nitime.lazy`, so
they can be reused throughout nitime.
"""
import sys
import types

# This flag only has affect on this module's import, and if it is set to True,
# LazyImports are performed immediately. Note: this flag is currently here
# only for debugging purposes and must be set directly in the source code,
# since nitime.lazy imports this module, and nitime.lazy is used throughout
# nitime, importing nitime will import this module.
disable_lazy_imports = False

if 'sphinx' in sys.modules:
    disable_lazy_imports = True

class LazyImport(types.ModuleType):
    """
    This class takes the module name as a parameter, and acts as a proxy for
    that module, importing it only when the module is used, but effectively
    acting as the module in every other way (including inside IPython with
    respect to introspection and tab completion) with the *exception* of
    reload()- reloading a :class:`LazyImport` raises an :class:`ImportError`.

    >>> mlab = LazyImport('matplotlib.mlab')

    No import happens on the above line, until we do something like call an
    ``mlab`` method or try to do tab completion or introspection on ``mlab``
    in IPython.

    >>> mlab
    <module 'matplotlib.mlab' will be lazily loaded>

    Now the :class:`LazyImport` will do an actual import, and call the dist
    function of the imported module.

    >>> mlab.dist(1969,2011)
    42.0
    """
    def __getattribute__(self,x):
        # This method will be called only once, since we'll change
        # self.__class__ to LoadedLazyImport, and __getattribute__ will point
        # to module.__getattribute__
        name = object.__getattribute__(self,'__name__')
        __import__(name)
        # if name above is 'package.foo.bar', package is returned, the docs
        # recommend that in order to get back the full thing, that we import
        # and then lookup the full name is sys.modules, see:
        # http://docs.python.org/library/functions.html#__import__
        module = sys.modules[name]
        # Now that we've done the import, cutout the middleman and make self
        # act as the imported module
        class LoadedLazyImport(types.ModuleType):
            __getattribute__ = module.__getattribute__
            __repr__ = module.__repr__
        object.__setattr__(self,'__class__', LoadedLazyImport)
        # The next line will make "reload(l)" a silent no-op
        #sys.modules[name] = self
        return module.__getattribute__(x)
    def __repr__(self):
        return "<module '%s' will be lazily loaded>" %\
                object.__getattribute__(self,'__name__')

if disable_lazy_imports:
    lazy_doc = """:class:`LazyImports` have been globally disabled.
    Please modify ``disable_lazy_imports`` boolean variable in
    :mod:`nitime.lazyimports` in order to leverage lazy loading of modules.
    """
    if 'sphinx' in sys.modules:
        lazy_doc = """
                   WARNING: To get Sphinx documentation to build we disable
                   LazyImports, which makes Sphinx incorrectly report this
                   class as having a base class of object. In reality,
                   :class:`LazyImport`'s base class is
                   :class:`types.ModuleType`.
                   """
        lazy_doc += LazyImport.__doc__
    class LazyImport(object):
        __doc__ = lazy_doc
        def __init__(self, x):
            __import__(x)
            self.module = sys.modules[x]
        def __getattr__(self, x):
            return self.module.__getattribute__(x)

########NEW FILE########
__FILENAME__ = six
"""Utilities for writing code that runs on Python 2 and 3"""

# Copyright (c) 2010-2013 Benjamin Peterson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import operator
import sys
import types

__author__ = "Benjamin Peterson <benjamin@python.org>"
__version__ = "1.4.1"


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform.startswith("java"):
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):
            def __len__(self):
                return 1 << 31
        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
        del X


def _add_doc(func, doc):
    """Add documentation to a function."""
    func.__doc__ = doc


def _import_module(name):
    """Import module, returning the module after the last dot."""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):

    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)
        # This is a bit ugly, but it avoids running this again.
        delattr(tp, self.name)
        return result


class MovedModule(_LazyDescr):

    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)


class MovedAttribute(_LazyDescr):

    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)



class _MovedItems(types.ModuleType):
    """Lazy loading of moved objects"""


_moved_attributes = [
    MovedAttribute("cStringIO", "cStringIO", "io", "StringIO"),
    MovedAttribute("filter", "itertools", "builtins", "ifilter", "filter"),
    MovedAttribute("filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"),
    MovedAttribute("input", "__builtin__", "builtins", "raw_input", "input"),
    MovedAttribute("map", "itertools", "builtins", "imap", "map"),
    MovedAttribute("range", "__builtin__", "builtins", "xrange", "range"),
    MovedAttribute("reload_module", "__builtin__", "imp", "reload"),
    MovedAttribute("reduce", "__builtin__", "functools"),
    MovedAttribute("StringIO", "StringIO", "io"),
    MovedAttribute("UserString", "UserString", "collections"),
    MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),
    MovedAttribute("zip", "itertools", "builtins", "izip", "zip"),
    MovedAttribute("zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"),

    MovedModule("builtins", "__builtin__"),
    MovedModule("configparser", "ConfigParser"),
    MovedModule("copyreg", "copy_reg"),
    MovedModule("dbm_gnu", "gdbm", "dbm.gnu"),
    MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
    MovedModule("http_cookies", "Cookie", "http.cookies"),
    MovedModule("html_entities", "htmlentitydefs", "html.entities"),
    MovedModule("html_parser", "HTMLParser", "html.parser"),
    MovedModule("http_client", "httplib", "http.client"),
    MovedModule("email_mime_multipart", "email.MIMEMultipart", "email.mime.multipart"),
    MovedModule("email_mime_text", "email.MIMEText", "email.mime.text"),
    MovedModule("email_mime_base", "email.MIMEBase", "email.mime.base"),
    MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
    MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
    MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
    MovedModule("cPickle", "cPickle", "pickle"),
    MovedModule("queue", "Queue"),
    MovedModule("reprlib", "repr"),
    MovedModule("socketserver", "SocketServer"),
    MovedModule("_thread", "thread", "_thread"),
    MovedModule("tkinter", "Tkinter"),
    MovedModule("tkinter_dialog", "Dialog", "tkinter.dialog"),
    MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
    MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
    MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
    MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
    MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
    MovedModule("tkinter_colorchooser", "tkColorChooser",
                "tkinter.colorchooser"),
    MovedModule("tkinter_commondialog", "tkCommonDialog",
                "tkinter.commondialog"),
    MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_font", "tkFont", "tkinter.font"),
    MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog",
                "tkinter.simpledialog"),
    MovedModule("urllib_parse", __name__ + ".moves.urllib_parse", "urllib.parse"),
    MovedModule("urllib_error", __name__ + ".moves.urllib_error", "urllib.error"),
    MovedModule("urllib", __name__ + ".moves.urllib", __name__ + ".moves.urllib"),
    MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
    MovedModule("xmlrpc_client", "xmlrpclib", "xmlrpc.client"),
    MovedModule("winreg", "_winreg"),
]
for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
del attr

moves = sys.modules[__name__ + ".moves"] = _MovedItems(__name__ + ".moves")



class Module_six_moves_urllib_parse(types.ModuleType):
    """Lazy loading of moved objects in six.moves.urllib_parse"""


_urllib_parse_moved_attributes = [
    MovedAttribute("ParseResult", "urlparse", "urllib.parse"),
    MovedAttribute("parse_qs", "urlparse", "urllib.parse"),
    MovedAttribute("parse_qsl", "urlparse", "urllib.parse"),
    MovedAttribute("urldefrag", "urlparse", "urllib.parse"),
    MovedAttribute("urljoin", "urlparse", "urllib.parse"),
    MovedAttribute("urlparse", "urlparse", "urllib.parse"),
    MovedAttribute("urlsplit", "urlparse", "urllib.parse"),
    MovedAttribute("urlunparse", "urlparse", "urllib.parse"),
    MovedAttribute("urlunsplit", "urlparse", "urllib.parse"),
    MovedAttribute("quote", "urllib", "urllib.parse"),
    MovedAttribute("quote_plus", "urllib", "urllib.parse"),
    MovedAttribute("unquote", "urllib", "urllib.parse"),
    MovedAttribute("unquote_plus", "urllib", "urllib.parse"),
    MovedAttribute("urlencode", "urllib", "urllib.parse"),
]
for attr in _urllib_parse_moved_attributes:
    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr

sys.modules[__name__ + ".moves.urllib_parse"] = Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse")
sys.modules[__name__ + ".moves.urllib.parse"] = Module_six_moves_urllib_parse(__name__ + ".moves.urllib.parse")


class Module_six_moves_urllib_error(types.ModuleType):
    """Lazy loading of moved objects in six.moves.urllib_error"""


_urllib_error_moved_attributes = [
    MovedAttribute("URLError", "urllib2", "urllib.error"),
    MovedAttribute("HTTPError", "urllib2", "urllib.error"),
    MovedAttribute("ContentTooShortError", "urllib", "urllib.error"),
]
for attr in _urllib_error_moved_attributes:
    setattr(Module_six_moves_urllib_error, attr.name, attr)
del attr

sys.modules[__name__ + ".moves.urllib_error"] = Module_six_moves_urllib_error(__name__ + ".moves.urllib_error")
sys.modules[__name__ + ".moves.urllib.error"] = Module_six_moves_urllib_error(__name__ + ".moves.urllib.error")


class Module_six_moves_urllib_request(types.ModuleType):
    """Lazy loading of moved objects in six.moves.urllib_request"""


_urllib_request_moved_attributes = [
    MovedAttribute("urlopen", "urllib2", "urllib.request"),
    MovedAttribute("install_opener", "urllib2", "urllib.request"),
    MovedAttribute("build_opener", "urllib2", "urllib.request"),
    MovedAttribute("pathname2url", "urllib", "urllib.request"),
    MovedAttribute("url2pathname", "urllib", "urllib.request"),
    MovedAttribute("getproxies", "urllib", "urllib.request"),
    MovedAttribute("Request", "urllib2", "urllib.request"),
    MovedAttribute("OpenerDirector", "urllib2", "urllib.request"),
    MovedAttribute("HTTPDefaultErrorHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPRedirectHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPCookieProcessor", "urllib2", "urllib.request"),
    MovedAttribute("ProxyHandler", "urllib2", "urllib.request"),
    MovedAttribute("BaseHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
    MovedAttribute("AbstractBasicAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPBasicAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("ProxyBasicAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("AbstractDigestAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPDigestAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("ProxyDigestAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPSHandler", "urllib2", "urllib.request"),
    MovedAttribute("FileHandler", "urllib2", "urllib.request"),
    MovedAttribute("FTPHandler", "urllib2", "urllib.request"),
    MovedAttribute("CacheFTPHandler", "urllib2", "urllib.request"),
    MovedAttribute("UnknownHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPErrorProcessor", "urllib2", "urllib.request"),
    MovedAttribute("urlretrieve", "urllib", "urllib.request"),
    MovedAttribute("urlcleanup", "urllib", "urllib.request"),
    MovedAttribute("URLopener", "urllib", "urllib.request"),
    MovedAttribute("FancyURLopener", "urllib", "urllib.request"),
]
for attr in _urllib_request_moved_attributes:
    setattr(Module_six_moves_urllib_request, attr.name, attr)
del attr

sys.modules[__name__ + ".moves.urllib_request"] = Module_six_moves_urllib_request(__name__ + ".moves.urllib_request")
sys.modules[__name__ + ".moves.urllib.request"] = Module_six_moves_urllib_request(__name__ + ".moves.urllib.request")


class Module_six_moves_urllib_response(types.ModuleType):
    """Lazy loading of moved objects in six.moves.urllib_response"""


_urllib_response_moved_attributes = [
    MovedAttribute("addbase", "urllib", "urllib.response"),
    MovedAttribute("addclosehook", "urllib", "urllib.response"),
    MovedAttribute("addinfo", "urllib", "urllib.response"),
    MovedAttribute("addinfourl", "urllib", "urllib.response"),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr

sys.modules[__name__ + ".moves.urllib_response"] = Module_six_moves_urllib_response(__name__ + ".moves.urllib_response")
sys.modules[__name__ + ".moves.urllib.response"] = Module_six_moves_urllib_response(__name__ + ".moves.urllib.response")


class Module_six_moves_urllib_robotparser(types.ModuleType):
    """Lazy loading of moved objects in six.moves.urllib_robotparser"""


_urllib_robotparser_moved_attributes = [
    MovedAttribute("RobotFileParser", "robotparser", "urllib.robotparser"),
]
for attr in _urllib_robotparser_moved_attributes:
    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
del attr

sys.modules[__name__ + ".moves.urllib_robotparser"] = Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib_robotparser")
sys.modules[__name__ + ".moves.urllib.robotparser"] = Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib.robotparser")


class Module_six_moves_urllib(types.ModuleType):
    """Create a six.moves.urllib namespace that resembles the Python 3 namespace"""
    parse = sys.modules[__name__ + ".moves.urllib_parse"]
    error = sys.modules[__name__ + ".moves.urllib_error"]
    request = sys.modules[__name__ + ".moves.urllib_request"]
    response = sys.modules[__name__ + ".moves.urllib_response"]
    robotparser = sys.modules[__name__ + ".moves.urllib_robotparser"]


sys.modules[__name__ + ".moves.urllib"] = Module_six_moves_urllib(__name__ + ".moves.urllib")


def add_move(move):
    """Add an item to six.moves."""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """Remove item from six.moves."""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError("no such move, %r" % (name,))


if PY3:
    _meth_func = "__func__"
    _meth_self = "__self__"

    _func_closure = "__closure__"
    _func_code = "__code__"
    _func_defaults = "__defaults__"
    _func_globals = "__globals__"

    _iterkeys = "keys"
    _itervalues = "values"
    _iteritems = "items"
    _iterlists = "lists"
else:
    _meth_func = "im_func"
    _meth_self = "im_self"

    _func_closure = "func_closure"
    _func_code = "func_code"
    _func_defaults = "func_defaults"
    _func_globals = "func_globals"

    _iterkeys = "iterkeys"
    _itervalues = "itervalues"
    _iteritems = "iteritems"
    _iterlists = "iterlists"


try:
    advance_iterator = next
except NameError:
    def advance_iterator(it):
        return it.next()
next = advance_iterator


try:
    callable = callable
except NameError:
    def callable(obj):
        return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)


if PY3:
    def get_unbound_function(unbound):
        return unbound

    create_bound_method = types.MethodType

    Iterator = object
else:
    def get_unbound_function(unbound):
        return unbound.im_func

    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)

    class Iterator(object):

        def next(self):
            return type(self).__next__(self)

    callable = callable
_add_doc(get_unbound_function,
         """Get the function out of a possibly unbound function""")


get_method_function = operator.attrgetter(_meth_func)
get_method_self = operator.attrgetter(_meth_self)
get_function_closure = operator.attrgetter(_func_closure)
get_function_code = operator.attrgetter(_func_code)
get_function_defaults = operator.attrgetter(_func_defaults)
get_function_globals = operator.attrgetter(_func_globals)


def iterkeys(d, **kw):
    """Return an iterator over the keys of a dictionary."""
    return iter(getattr(d, _iterkeys)(**kw))

def itervalues(d, **kw):
    """Return an iterator over the values of a dictionary."""
    return iter(getattr(d, _itervalues)(**kw))

def iteritems(d, **kw):
    """Return an iterator over the (key, value) pairs of a dictionary."""
    return iter(getattr(d, _iteritems)(**kw))

def iterlists(d, **kw):
    """Return an iterator over the (key, [values]) pairs of a dictionary."""
    return iter(getattr(d, _iterlists)(**kw))


if PY3:
    def b(s):
        return s.encode("latin-1")
    def u(s):
        return s
    unichr = chr
    if sys.version_info[1] <= 1:
        def int2byte(i):
            return bytes((i,))
    else:
        # This is about 2x faster than the implementation above on 3.2+
        int2byte = operator.methodcaller("to_bytes", 1, "big")
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io
    StringIO = io.StringIO
    BytesIO = io.BytesIO
else:
    def b(s):
        return s
    def u(s):
        return unicode(s, "unicode_escape")
    unichr = unichr
    int2byte = chr
    def byte2int(bs):
        return ord(bs[0])
    def indexbytes(buf, i):
        return ord(buf[i])
    def iterbytes(buf):
        return (ord(byte) for byte in buf)
    import StringIO
    StringIO = BytesIO = StringIO.StringIO
_add_doc(b, """Byte literal""")
_add_doc(u, """Text literal""")


if PY3:
    exec_ = getattr(moves.builtins, "exec")


    def reraise(tp, value, tb=None):
        if value.__traceback__ is not tb:
            raise value.with_traceback(tb)
        raise value

else:
    def exec_(_code_, _globs_=None, _locs_=None):
        """Execute code in a namespace."""
        if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec("""exec _code_ in _globs_, _locs_""")


    exec_("""def reraise(tp, value, tb=None):
    raise tp, value, tb
""")


print_ = getattr(moves.builtins, "print", None)
if print_ is None:
    def print_(*args, **kwargs):
        """The new-style print function for Python 2.4 and 2.5."""
        fp = kwargs.pop("file", sys.stdout)
        if fp is None:
            return
        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (isinstance(fp, file) and
                isinstance(data, unicode) and
                fp.encoding is not None):
                errors = getattr(fp, "errors", None)
                if errors is None:
                    errors = "strict"
                data = data.encode(fp.encoding, errors)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop("sep", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError("sep must be None or a string")
        end = kwargs.pop("end", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError("end must be None or a string")
        if kwargs:
            raise TypeError("invalid keyword arguments to print()")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode("\n")
            space = unicode(" ")
        else:
            newline = "\n"
            space = " "
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)

_add_doc(reraise, """Reraise an exception.""")


def with_metaclass(meta, *bases):
    """Create a base class with a metaclass."""
    return meta("NewBase", bases, {})

def add_metaclass(metaclass):
    """Class decorator for creating a class with a metaclass."""
    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        for slots_var in orig_vars.get('__slots__', ()):
            orig_vars.pop(slots_var)
        return metaclass(cls.__name__, cls.__bases__, orig_vars)
    return wrapper

########NEW FILE########
__FILENAME__ = testlib
"""Utilities to facilitate the writing of tests for nitime.
"""
#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------


#-----------------------------------------------------------------------------
# Functions and classes
#-----------------------------------------------------------------------------

def import_nose():
    """
    Import nose only when needed.
    """
    fine_nose = True
    minimum_nose_version = (0,10,0)
    try:
        import nose
        from nose.tools import raises
    except ImportError:
        fine_nose = False
    else:
        if nose.__versioninfo__ < minimum_nose_version:
            fine_nose = False

    if not fine_nose:
        msg = 'Need nose >= %d.%d.%d for tests - see ' \
              'http://somethingaboutorange.com/mrl/projects/nose' % \
              minimum_nose_version

        raise ImportError(msg)

    return nose

def fpw_opt_str():
    """
    Return first-package-wins option string for this version of nose

    Versions of nose prior to 1.1.0 needed ``=True`` for ``first-package-wins``,
    versions after won't accept it.

    changeset: 816:c344a4552d76
    http://code.google.com/p/python-nose/issues/detail?id=293

    Returns
    -------
    fpw_str : str
    Either '--first-package-wins' or '--first-package-wins=True' depending
    on the nose version we are running.
    """
    # protect nose import to provide comprehensible error if missing
    nose = import_nose()
    config = nose.config.Config()
    fpw_str = '--first-package-wins'
    opt_parser = config.getParser('')
    opt_def = opt_parser.get_option('--first-package-wins')
    if opt_def is None:
        raise RuntimeError('Nose does not accept "first-package-wins"'
                           ' - is this an old nose version?')
    if opt_def.takes_value(): # the =True variant
        fpw_str += '=True'
    return fpw_str


def test(nose_arg='nitime', doctests=True, first_package_wins=True, extra_argv=None):
    """

    Run the nitime test suite using nose.

    Parameters
    ----------

    nose_arg: string, optional
        What the first nose argument should be. Defaults to 'nitime', which
        will run all of the tests that can be found for the package, but this
        argument allows you to test a subset of the test suite, such as
        'nitime.tests.test_timeseries' or even a specific test using
        'nitime.tests.test_timeseries:test_TimeArray_comparison'.

    doctests: bool, optional
       Whether to run the doctests. Defaults to True

    first_package_wins: bool, optional
       Don't evict packages from sys.module, if detecting another package with
       the same name in some other location(nosetests default behavior is to do
       that).

    extra_argv: string, list or tuple, optional
       Additional argument (string) or arguments (list or tuple of strings) to
       be passed to nose when running the tests.

    """
    from numpy.testing import noseclasses
    # We construct our own argv manually, so we must set argv[0] ourselves
    argv = ['nosetests',
            # Name the package to actually test, in this case nitime
            nose_arg,

            # extra info in tracebacks
            '--detailed-errors',

            # We add --exe because of setuptools' imbecility (it blindly does
            # chmod +x on ALL files).  Nose does the right thing and it tries
            # to avoid executables, setuptools unfortunately forces our hand
            # here.  This has been discussed on the distutils list and the
            # setuptools devs refuse to fix this problem!
            '--exe',
            ]

    # If someone wants to add some other argv
    if extra_argv is not None:
        if isinstance(extra_argv, list) or isinstance(extra_argv, list):
            for this in extra_argv: argv.append(this)
        else:
            argv.append(extra_argv)

    if first_package_wins:
        argv.append(fpw_opt_str())
            
    if doctests:
        argv.append('--with-doctest')
    plugins = [noseclasses.KnownFailure()]
    # Now nose can run
    return noseclasses.NumpyTestProgram(argv=argv, exit=False,
            addplugins=plugins).result

# Tell nose that the test() function itself isn't a test, otherwise we get a
# recursive loop inside nose.
test.__test__ = False

########NEW FILE########
__FILENAME__ = test_algorithms
import os

import numpy as np
import numpy.testing as npt
import numpy.testing.decorators as dec

from scipy.signal import signaltools
from scipy import fftpack

import nitime
from nitime import algorithms as tsa
from nitime import utils as ut

#Define globally
test_dir_path = os.path.join(nitime.__path__[0], 'tests')


def test_scipy_resample():
    """ Tests scipy signal's resample function
    """
    # create a freq list with max freq < 16 Hz
    freq_list = np.random.randint(0, high=15, size=5)
    # make a test signal with sampling freq = 64 Hz
    a = [np.sin(2 * np.pi * f * np.linspace(0, 1, 64, endpoint=False))
         for f in freq_list]
    tst = np.array(a).sum(axis=0)
    # interpolate to 128 Hz sampling
    t_up = signaltools.resample(tst, 128)
    np.testing.assert_array_almost_equal(t_up[::2], tst)
    # downsample to 32 Hz
    t_dn = signaltools.resample(tst, 32)
    np.testing.assert_array_almost_equal(t_dn, tst[::2])

    # downsample to 48 Hz, and compute the sampling analytically for comparison
    dn_samp_ana = np.array([np.sin(2 * np.pi * f * np.linspace(0, 1, 48, endpoint=False))
                            for f in freq_list]).sum(axis=0)
    t_dn2 = signaltools.resample(tst, 48)
    npt.assert_array_almost_equal(t_dn2, dn_samp_ana)


def test_dpss_windows():
    "Are the eigenvalues representing spectral concentration near unity"
    # these values from Percival and Walden 1993
    _, l = tsa.dpss_windows(31, 6, 4)
    unos = np.ones(4)
    npt.assert_array_almost_equal(l, unos)
    _, l = tsa.dpss_windows(31, 7, 4)
    npt.assert_array_almost_equal(l, unos)
    _, l = tsa.dpss_windows(31, 8, 4)
    npt.assert_array_almost_equal(l, unos)
    _, l = tsa.dpss_windows(31, 8, 4.2)
    npt.assert_array_almost_equal(l, unos)


def test_dpss_matlab():
    """Do the dpss windows resemble the equivalent matlab result

    The variable b is read in from a text file generated by issuing:

    dpss(100,2)

    in matlab

    """
    a, _ = tsa.dpss_windows(100, 2, 4)
    b = np.loadtxt(os.path.join(test_dir_path, 'dpss_matlab.txt'))
    npt.assert_almost_equal(a, b.T)


def test_periodogram():
    arsig, _, _ = ut.ar_generator(N=512)
    avg_pwr = (arsig * arsig.conjugate()).mean()
    f, psd = tsa.periodogram(arsig, N=2048)
    df = 2. * np.pi / 2048
    avg_pwr_est = np.trapz(psd, dx=df)
    npt.assert_almost_equal(avg_pwr, avg_pwr_est, decimal=1)


def permutation_system(N):
    p = np.zeros((N, N))
    targets = list(range(N))
    for i in range(N):
        popper = np.random.randint(0, high=len(targets))
        j = targets.pop(popper)
        p[i, j] = 1
    return p


def test_boxcar_filter():
    a = np.random.rand(100)
    b = tsa.boxcar_filter(a)
    npt.assert_equal(a, b)

    #Should also work for odd number of elements:
    a = np.random.rand(99)
    b = tsa.boxcar_filter(a)
    npt.assert_equal(a, b)

    b = tsa.boxcar_filter(a, ub=0.25)
    npt.assert_equal(a.shape, b.shape)

    b = tsa.boxcar_filter(a, lb=0.25)
    npt.assert_equal(a.shape, b.shape)


def test_get_spectra():
    """Testing get_spectra"""
    t = np.linspace(0, 16 * np.pi, 2 ** 10)
    x = (np.sin(t) + np.sin(2 * t) + np.sin(3 * t) +
         0.1 * np.random.rand(t.shape[-1]))

    #First test for 1-d data:
    NFFT = 64
    N = x.shape[-1]
    f_welch = tsa.get_spectra(x, method={'this_method': 'welch', 'NFFT': NFFT})
    f_periodogram = tsa.get_spectra(x, method={'this_method': 'periodogram_csd'})
    f_multi_taper = tsa.get_spectra(x, method={'this_method': 'multi_taper_csd'})

    npt.assert_equal(f_welch[0].shape, (NFFT / 2 + 1,))
    npt.assert_equal(f_periodogram[0].shape, (N / 2 + 1,))
    npt.assert_equal(f_multi_taper[0].shape, (N / 2 + 1,))

    #Test for multi-channel data
    x = np.reshape(x, (2, x.shape[-1] / 2))
    N = x.shape[-1]

    #Make sure you get back the expected shape for different spectra:
    NFFT = 64
    f_welch = tsa.get_spectra(x, method={'this_method': 'welch', 'NFFT': NFFT})
    f_periodogram = tsa.get_spectra(x, method={'this_method': 'periodogram_csd'})
    f_multi_taper = tsa.get_spectra(x, method={'this_method': 'multi_taper_csd'})

    npt.assert_equal(f_welch[0].shape[0], NFFT / 2 + 1)
    npt.assert_equal(f_periodogram[0].shape[0], N / 2 + 1)
    npt.assert_equal(f_multi_taper[0].shape[0], N / 2 + 1)


def test_psd_matlab():

    """ Test the results of mlab csd/psd against saved results from Matlab"""

    from matplotlib import mlab

    test_dir_path = os.path.join(nitime.__path__[0], 'tests')

    ts = np.loadtxt(os.path.join(test_dir_path, 'tseries12.txt'))

    #Complex signal!
    ts0 = ts[1] + ts[0] * np.complex(0, 1)

    NFFT = 256
    Fs = 1.0
    noverlap = NFFT / 2

    fxx, f = mlab.psd(ts0, NFFT=NFFT, Fs=Fs, noverlap=noverlap,
                      scale_by_freq=True)

    fxx_mlab = fftpack.fftshift(fxx).squeeze()

    fxx_matlab = np.loadtxt(os.path.join(test_dir_path, 'fxx_matlab.txt'))

    npt.assert_almost_equal(fxx_mlab, fxx_matlab, decimal=5)

@dec.slow
def test_long_dpss_win():
    """ Test that very long dpss windows can be generated (using interpolation)"""

    # This one is generated using interpolation:
    a1,e = tsa.dpss_windows(166800, 4, 8, interp_from=4096)

    # This one is calculated:
    a2,e = tsa.dpss_windows(166800, 4, 8)

    # They should be very similar:
    npt.assert_almost_equal(a1, a2, decimal=5)

    # They should both be very similar to the same one calculated in matlab
    # (using 'a = dpss(166800, 4, 8)').
    test_dir_path = os.path.join(nitime.__path__[0], 'tests')
    matlab_long_dpss = np.load(os.path.join(test_dir_path, 'long_dpss_matlab.npy'))
    # We only have the first window to compare against:
    # Both for the interpolated case:
    npt.assert_almost_equal(a1[0], matlab_long_dpss, decimal=5)
    # As well as the calculated case:
    npt.assert_almost_equal(a1[0], matlab_long_dpss, decimal=5)

########NEW FILE########
__FILENAME__ = test_analysis
import numpy as np
import numpy.testing as npt
import nitime.timeseries as ts
import nitime.analysis as nta


def test_SpectralAnalyzer():

    Fs = np.pi
    t = np.arange(1024)
    x = np.sin(10 * t) + np.random.rand(t.shape[-1])
    y = np.sin(10 * t) + np.random.rand(t.shape[-1])

    T = ts.TimeSeries(np.vstack([x, y]), sampling_rate=Fs)

    C = nta.SpectralAnalyzer(T)

    f, c = C.psd

    npt.assert_equal(f.shape, (33,))  # This is the setting for this analyzer
                                      # (window-length of 64)
    npt.assert_equal(c.shape, (2, 33))

    f, c = C.cpsd
    npt.assert_equal(f.shape, (33,))  # This is the setting for this analyzer
                                      # (window-length of 64)
    npt.assert_equal(c.shape, (2, 2, 33))

    f, c = C.cpsd
    npt.assert_equal(f.shape, (33,))  # This is the setting for this analyzer
                                      # (window-length of 64)
    npt.assert_equal(c.shape, (2, 2, 33))

    f, c = C.spectrum_fourier

    npt.assert_equal(f.shape, (t.shape[0] / 2 + 1,))
    npt.assert_equal(c.shape, (2, t.shape[0] / 2 + 1))

    f, c = C.spectrum_multi_taper

    npt.assert_equal(f.shape, (t.shape[0] / 2 + 1,))
    npt.assert_equal(c.shape, (2, t.shape[0] / 2 + 1))

    f, c = C.periodogram

    npt.assert_equal(f.shape, (t.shape[0] / 2 + 1,))
    npt.assert_equal(c.shape, (2, t.shape[0] / 2 + 1))

    # Test for data with only one channel
    T = ts.TimeSeries(x, sampling_rate=Fs)
    C = nta.SpectralAnalyzer(T)
    f, c = C.psd
    npt.assert_equal(f.shape, (33,))  # Same length for the frequencies
    npt.assert_equal(c.shape, (33,))  # 1-d spectrum for the single channels

    f, c = C.spectrum_multi_taper
    npt.assert_equal(f.shape, (t.shape[0] / 2 + 1,))  # Same length for the frequencies
    npt.assert_equal(c.shape, (t.shape[0] / 2 + 1,))  # 1-d spectrum for the single channels


def test_CorrelationAnalyzer():

    Fs = np.pi
    t = np.arange(1024)
    x = np.sin(10 * t) + np.random.rand(t.shape[-1])
    y = np.sin(10 * t) + np.random.rand(t.shape[-1])

    T = ts.TimeSeries(np.vstack([x, y]), sampling_rate=Fs)

    C = nta.CorrelationAnalyzer(T)

    # Test the symmetry: correlation(x,y)==correlation(y,x)
    npt.assert_equal(C.corrcoef[0, 1], C.corrcoef[1, 0])
    # Test the self-sameness: correlation(x,x)==1
    npt.assert_equal(C.corrcoef[0, 0], 1)
    npt.assert_equal(C.corrcoef[1, 1], 1)

    # Test the cross-correlation:
    # First the symmetry:
    npt.assert_array_almost_equal(C.xcorr.data[0, 1], C.xcorr.data[1, 0])

    # Test the normalized cross-correlation
    # The cross-correlation should be equal to the correlation at time-lag 0
    npt.assert_equal(C.xcorr_norm.data[0, 1, C.xcorr_norm.time == 0],
                            C.corrcoef[0, 1])

    # And the auto-correlation should be equal to 1 at 0 time-lag:
    npt.assert_equal(C.xcorr_norm.data[0, 0, C.xcorr_norm.time == 0], 1)

    # Does it depend on having an even number of time-points?
    # make another time-series with an odd number of items:
    t = np.arange(1023)
    x = np.sin(10 * t) + np.random.rand(t.shape[-1])
    y = np.sin(10 * t) + np.random.rand(t.shape[-1])

    T = ts.TimeSeries(np.vstack([x, y]), sampling_rate=Fs)

    C = nta.CorrelationAnalyzer(T)

    npt.assert_equal(C.xcorr_norm.data[0, 1, C.xcorr_norm.time == 0],
                            C.corrcoef[0, 1])


def test_EventRelatedAnalyzer():
    cycles = 10
    l = 1024
    unit = 2 * np.pi / l
    t = np.arange(0, 2 * np.pi + unit, unit)
    signal = np.sin(cycles * t)
    events = np.zeros(t.shape)
    # Zero crossings:
    idx = np.where(np.abs(signal) < 0.03)[0]
    # An event occurs at the beginning of every cycle:
    events[idx[:-2:2]] = 1
    # and another kind of event at the end of each cycle:
    events[idx[1:-1:2]] = 2

    T_signal = ts.TimeSeries(signal, sampling_rate=1)
    T_events = ts.TimeSeries(events, sampling_rate=1)
    for correct_baseline in [True,False]:
        ETA = nta.EventRelatedAnalyzer(T_signal, T_events, l / (cycles * 2),
                                       correct_baseline=correct_baseline).eta
        # This should hold
        npt.assert_almost_equal(ETA.data[0], signal[:ETA.data.shape[-1]], 3)
        npt.assert_almost_equal(ETA.data[1], -1 * signal[:ETA.data.shape[-1]], 3)


    # Same should be true for the FIR analysis:
    FIR = nta.EventRelatedAnalyzer(T_signal, T_events, l / (cycles * 2)).FIR
    npt.assert_almost_equal(FIR.data[0], signal[:FIR.data.shape[-1]], 3)
    npt.assert_almost_equal(FIR.data[1], -1 * signal[:FIR.data.shape[-1]], 3)

    # Same should be true for
    XCORR = nta.EventRelatedAnalyzer(T_signal, T_events, l / (cycles * 2)).xcorr_eta
    npt.assert_almost_equal(XCORR.data[0], signal[:XCORR.data.shape[-1]], 3)
    npt.assert_almost_equal(XCORR.data[1], -1 * signal[:XCORR.data.shape[-1]], 3)

    # More dimensions:
    T_signal = ts.TimeSeries(np.vstack([signal, signal]), sampling_rate=1)
    T_events = ts.TimeSeries(np.vstack([events, events]), sampling_rate=1)
    ETA = nta.EventRelatedAnalyzer(T_signal, T_events, l / (cycles * 2)).eta

    # The events input and the time-series input have different dimensions:
    T_events = ts.TimeSeries(events, sampling_rate=1)
    ETA = nta.EventRelatedAnalyzer(T_signal, T_events, l / (cycles * 2)).eta
    npt.assert_almost_equal(ETA.data[0][0], signal[:ETA.data.shape[-1]], 3)
    npt.assert_almost_equal(ETA.data[1][1], -1 * signal[:ETA.data.shape[-1]], 3)

    # Input is an Events object, instead of a time-series:
    ts1 = ts.TimeSeries(np.arange(100), sampling_rate=1)
    ev = ts.Events([10, 20, 30])
    et = nta.EventRelatedAnalyzer(ts1, ev, 5)

    # The five points comprising the average of the three sequences:
    npt.assert_equal(et.eta.data, [20., 21., 22., 23., 24.])

    ts2 = ts.TimeSeries(np.arange(200).reshape(2, 100), sampling_rate=1)
    ev = ts.Events([10, 20, 30])
    et = nta.EventRelatedAnalyzer(ts2, ev, 5)

    npt.assert_equal(et.eta.data, [[20., 21., 22., 23., 24.],
                                  [120., 121., 122., 123., 124.]])


    # The event-triggered SEM should be approximately zero:
    for correct_baseline in [True,False]:
        EA = nta.EventRelatedAnalyzer(T_signal, T_events, l / (cycles * 2),
                                      correct_baseline=correct_baseline)

        npt.assert_almost_equal(EA.ets.data[0],
                                np.zeros_like(EA.ets.data[0]),
                                decimal=2)
    # Test the et_data method:
    npt.assert_almost_equal(EA.et_data[0][0].data[0],
                            signal[:ETA.data.shape[-1]])

    # Test that providing the analyzer with an array, instead of an Events or a
    # TimeSeries object throws an error:
    npt.assert_raises(ValueError, nta.EventRelatedAnalyzer, ts2, events, 10)

    # This is not yet implemented, so this should simply throw an error, for
    # now:
    npt.assert_raises(NotImplementedError,
                      nta.EventRelatedAnalyzer.FIR_estimate, EA)

def test_HilbertAnalyzer():
    """Testing the HilbertAnalyzer (analytic signal)"""
    pi = np.pi
    Fs = np.pi
    t = np.arange(0, 2 * pi, pi / 256)

    a0 = np.sin(t)
    a1 = np.cos(t)
    a2 = np.sin(2 * t)
    a3 = np.cos(2 * t)

    T = ts.TimeSeries(data=np.vstack([a0, a1, a2, a3]),
                             sampling_rate=Fs)

    H = nta.HilbertAnalyzer(T)

    h_abs = H.amplitude.data
    h_angle = H.phase.data
    h_real = H.real.data
    #The real part should be equal to the original signals:
    npt.assert_almost_equal(h_real, T.data)
    #The absolute value should be one everywhere, for this input:
    npt.assert_almost_equal(h_abs, np.ones(T.data.shape))
    #For the 'slow' sine - the phase should go from -pi/2 to pi/2 in the first
    #256 bins:
    npt.assert_almost_equal(h_angle[0, :256], np.arange(-pi / 2, pi / 2, pi / 256))
    #For the 'slow' cosine - the phase should go from 0 to pi in the same
    #interval:
    npt.assert_almost_equal(h_angle[1, :256], np.arange(0, pi, pi / 256))
    #The 'fast' sine should make this phase transition in half the time:
    npt.assert_almost_equal(h_angle[2, :128], np.arange(-pi / 2, pi / 2, pi / 128))
    #Ditto for the 'fast' cosine:
    npt.assert_almost_equal(h_angle[3, :128], np.arange(0, pi, pi / 128))


def test_FilterAnalyzer():
    """Testing the FilterAnalyzer """
    t = np.arange(np.pi / 100, 10 * np.pi, np.pi / 100)
    fast = np.sin(50 * t) + 10
    slow = np.sin(10 * t) - 20

    fast_mean = np.mean(fast)
    slow_mean = np.mean(slow)

    fast_ts = ts.TimeSeries(data=fast, sampling_rate=np.pi)
    slow_ts = ts.TimeSeries(data=slow, sampling_rate=np.pi)

    #Make sure that the DC is preserved
    f_slow = nta.FilterAnalyzer(slow_ts, ub=0.6)
    f_fast = nta.FilterAnalyzer(fast_ts, lb=0.6)

    npt.assert_almost_equal(f_slow.filtered_fourier.data.mean(),
                            slow_mean,
                            decimal=2)

    npt.assert_almost_equal(f_slow.filtered_boxcar.data.mean(),
                            slow_mean,
                            decimal=2)

    npt.assert_almost_equal(f_slow.fir.data.mean(),
                            slow_mean)

    npt.assert_almost_equal(f_slow.iir.data.mean(),
                            slow_mean)

    npt.assert_almost_equal(f_fast.filtered_fourier.data.mean(),
                            10)

    npt.assert_almost_equal(f_fast.filtered_boxcar.data.mean(),
                            10,
                            decimal=2)

    npt.assert_almost_equal(f_fast.fir.data.mean(),
                            10)

    npt.assert_almost_equal(f_fast.iir.data.mean(),
                            10)

    #Check that things work with a two-channel time-series:
    T2 = ts.TimeSeries(np.vstack([fast, slow]), sampling_rate=np.pi)
    f_both = nta.FilterAnalyzer(T2, ub=1.0, lb=0.1)
    #These are rather basic tests:
    npt.assert_equal(f_both.fir.shape, T2.shape)
    npt.assert_equal(f_both.iir.shape, T2.shape)
    npt.assert_equal(f_both.filtered_boxcar.shape, T2.shape)
    npt.assert_equal(f_both.filtered_fourier.shape, T2.shape)


def test_NormalizationAnalyzer():
    """Testing the NormalizationAnalyzer """

    t1 = ts.TimeSeries(data=[[99, 100, 101], [99, 100, 101]], sampling_interval=1.)
    t2 = ts.TimeSeries(data=[[-1, 0, 1], [-1, 0, 1]], sampling_interval=1.)

    N1 = nta.NormalizationAnalyzer(t1)
    npt.assert_almost_equal(N1.percent_change[0], t2[0])

    t3 = ts.TimeSeries(data=[[100, 102], [1, 3]], sampling_interval=1.)
    t4 = ts.TimeSeries(data=[[-1, 1], [-1, 1]], sampling_interval=1.)

    N2 = nta.NormalizationAnalyzer(t3)
    npt.assert_almost_equal(N2.z_score[0], t4[0])


def test_MorletWaveletAnalyzer():
    """Testing the MorletWaveletAnalyzer """
    time_series = ts.TimeSeries(data=np.random.rand(100), sampling_rate=100)

    W = nta.MorletWaveletAnalyzer(time_series, freqs=20)
    WL = nta.MorletWaveletAnalyzer(time_series, freqs=20, log_morlet=True)
    H = nta.HilbertAnalyzer(W.real)
    HL = nta.HilbertAnalyzer(WL.real)

    npt.assert_almost_equal(np.sin(H.phase.data[10:-10]),
                            np.sin(W.phase.data[10:-10]),
                            decimal=0)
    npt.assert_almost_equal(np.sin(HL.phase.data[10:-10]),
                            np.sin(WL.phase.data[10:-10]),
                            decimal=0)

########NEW FILE########
__FILENAME__ = test_descriptors
"""Tests for the descriptors module.
"""
#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

import nose.tools as nt

from nitime import descriptors as desc

#-----------------------------------------------------------------------------
# Support classes and functions
#-----------------------------------------------------------------------------


class A(desc.ResetMixin):
    @desc.auto_attr
    def y(self):
        return self.x / 2.0

    @desc.auto_attr
    def z(self):
        return self.x / 3.0

    def __init__(self, x=1.0):
        self.x = x

#-----------------------------------------------------------------------------
# Test functions
#-----------------------------------------------------------------------------


def test():
    a = A(10)
    nt.assert_false('y' in a.__dict__)
    nt.assert_equals(a.y, 5)
    nt.assert_true('y' in a.__dict__)
    a.x = 20
    nt.assert_equals(a.y, 5)
    # Call reset and no error should be raised even though z was never accessed
    a.reset()
    nt.assert_equals(a.y, 10)

########NEW FILE########
__FILENAME__ = test_lazy
import sys
import os
import nitime.lazyimports as l
import numpy.testing as npt
import numpy.testing.decorators as dec

# The next test requires nitime.lazyimports.disable_lazy_imports to have been
# set to false, otherwise the lazy import machinery is disabled and all imports
# happen at l.LazyImport calls which become equivalent to regular import
# statements
@dec.skipif(l.disable_lazy_imports)
def test_lazy():
    mlab = l.LazyImport('matplotlib.mlab')
    # repr for mlab should be <module 'matplotlib.mlab' will be lazily loaded>
    assert 'lazily loaded' in repr(mlab)
    # accessing mlab's attribute will cause an import of mlab
    npt.assert_equal(mlab.dist(1969,2011), 42.0)
    # now mlab should be of class LoadedLazyImport an repr(mlab) should be
    # <module 'matplotlib.mlab' from # '.../matplotlib/mlab.pyc>
    assert 'lazily loaded' not in repr(mlab)

# A known limitation of our lazy loading implementation is that, when it it is
# enabled, reloading the module raises an ImportError, and it also does not
# actually perform a reload, as demonstrated by this test.
@dec.skipif(l.disable_lazy_imports)
def test_lazy_noreload():
    "Reloading of lazy modules causes ImportError"
    mod = l.LazyImport('sys')
    # accessing module dictionary will trigger an import
    len(mod.__dict__)
    if sys.version_info.major == 2:
        npt.assert_raises(ImportError, reload, mod)
    elif sys.version_info.major == 3:
        import imp
        if sys.version_info.minor == 2:
            npt.assert_raises(ImportError, imp.reload, mod)
        elif sys.version_info.minor == 3:
            npt.assert_raises(TypeError, imp.reload, mod)

########NEW FILE########
__FILENAME__ = test_timeseries
import numpy as np
import numpy.testing as npt
import nitime.timeseries as ts
import nose.tools as nt



def test_get_time_unit():

    number = 4
    npt.assert_equal(ts.get_time_unit(number), None)

    list_of_numbers = [4, 5, 6]
    npt.assert_equal(ts.get_time_unit(list_of_numbers), None)

    for tu in ['ps', 's', 'D']:
        time_point = ts.TimeArray([4], time_unit=tu)
        npt.assert_equal(ts.get_time_unit(time_point), tu)

        list_of_time = [ts.TimeArray(4, time_unit=tu), ts.TimeArray(5, time_unit=tu)]
        npt.assert_equal(ts.get_time_unit(list_of_time), tu)

        # Go crazy, we don't mind:
        list_of_lists = [[ts.TimeArray(4, time_unit=tu),
                         ts.TimeArray(5, time_unit=tu)],
                        [ts.TimeArray(4, time_unit=tu),
                         ts.TimeArray(5, time_unit=tu)]]

        npt.assert_equal(ts.get_time_unit(list_of_lists), tu)
        time_arr = ts.TimeArray([4, 5], time_unit=tu)
        npt.assert_equal(ts.get_time_unit(time_arr), tu)



def test_TimeArray():

    time1 = ts.TimeArray(list(range(100)), time_unit='ms')
    time2 = time1 + time1
    npt.assert_equal(time2.time_unit, 'ms')
    time1 = ts.TimeArray(10 ** 6)
    npt.assert_equal(time1.__repr__(), '1000000.0 s')
    #TimeArray can't be more than 1-d:
    nt.assert_raises(ValueError, ts.TimeArray, np.zeros((2, 2)))

    dt = ts.TimeArray(0.001, time_unit='s')
    tt = ts.TimeArray([dt])
    npt.assert_equal(dt, tt)

    t1 = ts.TimeArray([0, 1, 2, 3])
    t2 = ts.TimeArray([ts.TimeArray(0),
                       ts.TimeArray(1),
                       ts.TimeArray(2),
                       ts.TimeArray(3)])
    npt.assert_equal(t1, t2)


def test_TimeArray_math():
    "Addition and subtraction should convert to TimeArray units"
    time1 = ts.TimeArray(list(range(10)), time_unit='ms')
    time2 = ts.TimeArray(list(range(1,11)), time_unit='ms')
    # units should be converted to whatever units the array has
    time3 = time1 + 1
    npt.assert_equal(time2,time3)
    time4 = time2 - 1
    npt.assert_equal(time1,time4)
    # floats should also work
    time3 = time1 + 1.0
    npt.assert_equal(time2,time3)
    time4 = time2 - 1.0
    npt.assert_equal(time1,time4)

    # test the r* versions
    time3 = 1 + time1
    npt.assert_equal(time2,time3)
    time4 = 1 - time2
    npt.assert_equal(-time1,time4)
    # floats should also work
    time3 = 1.0 + time1
    npt.assert_equal(time2,time3)
    time4 = 1.0 - time2
    npt.assert_equal(-time1,time4)

    timeunits = ts.TimeArray(list(range(10)), time_unit='s')
    timeunits.convert_unit('ms')
    # now, math with non-TimeArrays should be based on the new time_unit

    # here the range() list gets converted to a TimeArray with the same units
    # as timeunits (which is now 'ms')
    tnew = timeunits + list(range(10))
    npt.assert_equal(tnew, timeunits+time1) # recall that time1 was 0-10ms

    

def test_TimeArray_comparison():
    "Comparison with unitless quantities should convert to TimeArray units"
    time = ts.TimeArray(list(range(10)), time_unit='ms')
    npt.assert_equal(time < 5 , [True]*5+[False]*5)
    npt.assert_equal(time > 5 , [False]*6+[True]*4)
    npt.assert_equal(time <= 5, [True]*6+[False]*4)
    npt.assert_equal(time >= 5, [False]*5+[True]*5)
    npt.assert_equal(time == 5, [False]*5+[True] + [False]*4)
    time.convert_unit('s')
    # now all of time is < 1 in the new time_unit
    npt.assert_equal(time < 5 , [True]*10)
    npt.assert_equal(time > 5 , [False]*10)
    npt.assert_equal(time <= 5, [True]*10)
    npt.assert_equal(time >= 5, [False]*10)
    npt.assert_equal(time == 5, [False]*10)

def test_TimeArray_init_int64():
    """Make sure that we can initialize TimeArray with an array of ints"""
    time = ts.TimeArray(np.int64(1))
    npt.assert_equal(time.__repr__(), '1.0 s')

    pass



def test_TimeArray_init_list():
    """Initializing with a list that contains TimeArray should work.
    """
    for t in [0.001, ts.TimeArray(0.001, time_unit='s')]:
        tl = [t]
        ta = ts.TimeArray(t, time_unit='s')
        tla = ts.TimeArray(tl, time_unit='s')
        nt.assert_equal(ta, tla)


def test_TimeArray_repr():
    """
>>> a = ts.TimeArray([1.1,2,3])
>>> a
TimeArray([ 1.1,  2. ,  3. ], time_unit='s')
>>> t = ts.TimeArray(a,time_unit='ms')
>>> t
TimeArray([ 1100.,  2000.,  3000.], time_unit='ms')
>>> t[0]
1100.0 ms
    """



def test_TimeArray_copyflag():
    """Testing the setting of the copy-flag, where that makes sense"""

    #These two should both generate a TimeArray, with one picosecond.
    #This one holds time_unit='s'
    t1 = ts.TimeArray(np.array([1], dtype=np.int64), copy=False)
    #This one holds time_unit='ps':
    t2 = ts.TimeArray(1, time_unit='ps')
    t3 = ts.TimeArray(t2, copy=False)
    npt.assert_equal(t1, t2)
    npt.assert_equal(t2.ctypes.data, t3.ctypes.data)



def test_TimeArray_new():
    for unit in ['ns', 'ms', 's', None]:
        for flag, assertion in [(True, nt.assert_not_equal),
                (False, nt.assert_equal)]:
            #list -doesn't make sense to set copy=True
            time2 = ts.TimeArray(list(range(5)), time_unit=unit, copy=True)
            #numpy array (float) - doesn't make sense to set copy=True
            time2f = ts.TimeArray(np.arange(5.), time_unit=unit, copy=True)
            #TimeArray
            time3 = ts.TimeArray(time2, time_unit=unit, copy=flag)
            #integer
            time4 = ts.TimeArray(5, time_unit=unit, copy=True)
            #float
            time5 = ts.TimeArray(5.0, time_unit=unit, copy=True)

            npt.assert_equal(time2, time2f)
            npt.assert_equal(time2, time3)
            time3[0] += 100
            assertion(time2[0], time3[0])
            npt.assert_equal(time2[1:], time3[1:])
            npt.assert_equal(time4, time5)



def test_TimeArray_bool():
    time1 = ts.TimeArray([1, 2, 3], time_unit='s')
    time2 = ts.TimeArray([1000, 2000, 3000], time_unit='ms')
    bool_arr = np.ones(time1.shape, dtype=bool)
    npt.assert_equal(time1, time2)
    npt.assert_equal(bool_arr, time1 == time2)
    nt.assert_not_equal(type(time1 == time2), ts.TimeArray)


def test_TimeArray_convert_unit():
    """
    >>> a = ts.TimeArray([1,2,3,4])
    >>> a.convert_unit('ms')
    >>> a
    TimeArray([ 1000.,  2000.,  3000.,  4000.], time_unit='ms')
    >>> a.time_unit
    'ms'
    >>> b = ts.TimeArray([1,2,3,4],'s')
    >>> a==b
    array([ True,  True,  True,  True], dtype=bool)
    """



def test_TimeArray_div():

    #divide singelton by singleton:
    a = 2.0
    b = 6.0
    time1 = ts.TimeArray(a, time_unit='s')
    time2 = ts.TimeArray(b, time_unit='s')
    div1 = a / b
    #This should eliminate the units and return a float, not a TimeArray:
    div2 = time1 / time2
    npt.assert_equal(div1, div2)

    #Divide a TimeArray by a singelton:
    a = np.array([1, 2, 3])
    b = 6.0
    time1 = ts.TimeArray(a, time_unit='s')
    time2 = ts.TimeArray(b, time_unit='s')
    div1 = a / b
    #This should eliminate the units and return a float array, not a TimeArray:
    div2 = time1 / time2
    npt.assert_equal(div1, div2)

    #Divide a TimeArray by another TimeArray:
    a = np.array([1, 2, 3])
    b = np.array([2, 2, 2]).astype(float)  # TimeArray division is float division!
    time1 = ts.TimeArray(a, time_unit='s')
    time2 = ts.TimeArray(b, time_unit='s')
    div1 = a / b
    #This should eliminate the units and return a float array, not a TimeArray:
    div2 = time1 / time2
    npt.assert_equal(div1, div2)



def test_TimeArray_index_at():
    time1 = ts.TimeArray(list(range(10)), time_unit='ms')
    for i in range(5):
        # The return value is always an array, so we keep it for multiple tests
        i_arr = np.array(i)
        # Check 'closest' indexing mode first
        idx = time1.index_at(i)
        npt.assert_equal(idx, i_arr)

        # If we index with seconds/1000, results shouldn't vary
        idx_secs = time1.index_at(ts.TimeArray(i / 1000., time_unit='s'))
        npt.assert_equal(idx_secs, i_arr)

        # If we now change the tolerance
        # In this case, it should still return
        idx = time1.index_at(i + 0.1, tol=0.1)
        npt.assert_equal(idx, i_arr)
        # But with a smaller tolerance, we should get no indices
        idx = time1.index_at(i + 0.1, tol=0.05)
        npt.assert_equal(idx, np.array([]))

        # Now, check before/after modes
        idx = time1.index_at(i + 0.1, mode='before')
        npt.assert_equal(idx, i_arr)

        idx = time1.index_at(i + 0.1, mode='after')
        npt.assert_equal(idx, i_arr + 1)



def test_TimeArray_at():
    time1 = ts.TimeArray(list(range(10)), time_unit='ms')
    for i in range(10):
        this = time1.at(i)
        i_ms = ts.TimeArray(i / 1000.)
        npt.assert_equal(this, ts.TimeArray(i, time_unit='ms'))
        this_secs = time1.at(i_ms)
        npt.assert_equal(this_secs, ts.TimeArray(i, time_unit='ms'))
        seconds_array = ts.TimeArray(time1, time_unit='s')
        this_secs = seconds_array.at(i / 1000.)
        npt.assert_equal(this_secs, ts.TimeArray(i, time_unit='ms'))
        all = time1.at(i_ms, tol=10)
        npt.assert_equal(all, time1)
        if i > 0 and i < 9:
            this_secs = time1.at(i_ms, tol=1)
            npt.assert_equal(this_secs,
                                   ts.TimeArray([i - 1, i, i + 1], time_unit='ms'))



def test_TimeArray_at2():
    time1 = ts.TimeArray(list(range(10)), time_unit='ms')
    for i in [1]:
        i_ms = ts.TimeArray(i / 1000.)
        this_secs = time1.at(i_ms, tol=1)
        npt.assert_equal(this_secs,
                               ts.TimeArray([i - 1, i, i + 1], time_unit='ms'))



def test_UniformTime_index_at():
    time1 = ts.UniformTime(t0=1000, length=10, sampling_rate=1000, time_unit='ms')
    mask = [False] * 10
    for i in range(10):
        idx = time1.index_at(ts.TimeArray(1000 + i, time_unit='ms'))
        npt.assert_equal(idx, np.array(i))
        mask[i] = True
        mask_idx = time1.index_at(ts.TimeArray(1000 + i, time_unit='ms'),
                                  boolean=True)
        npt.assert_equal(mask_idx, mask)
        if i > 0 and i < 9:
            mask[i - 1] = True
            mask[i + 1] = True

            mask_idx = time1.index_at(
                ts.TimeArray([999 + i, 1000 + i, 1001 + i],
                time_unit='ms'), boolean=True)

            npt.assert_equal(mask_idx, mask)
            mask[i - 1] = False
            mask[i + 1] = False
        mask[i] = False

#XXX Need to write these tests:

#Test the unit conversion:
#
#def test_TimeArray_unit_conversion():

#Test the overloaded __getitem__ and __setitem:
#
def test_TimeArray_getset():
    t1 = ts.TimeSeries(data = np.random.rand(2, 3, 4), sampling_rate=1)
    npt.assert_equal(t1[0],t1.data[...,0])
    



def test_UniformTime():
    tuc = ts.time_unit_conversion
    for unit, duration in zip(['ns', 'ms', 's', None],
                             [2 * 10 ** 9, 2 * 10 ** 6, 100, 20]):

        t1 = ts.UniformTime(duration=duration, sampling_rate=1,
                            time_unit=unit)
        t2 = ts.UniformTime(duration=duration, sampling_rate=20,
                            time_unit=unit)

        #The following two tests verify that first-last are equal to the
        #duration, but it is unclear whether that is really the behavior we
        #want, because the t_i held by a TimeSeries is the left
        #(smaller) side of the time-duration defined by the bin

        #The difference between the first and last item is the duration:
        #npt.assert_equal(t1[-1]-t1[0],
        #                       ts.TimeArray(duration,time_unit=unit))
        #Duration doesn't depend on the sampling rate:
        #npt.assert_equal(t1[-1]-t2[0],
        #                       ts.TimeArray(duration,time_unit=unit))

        a = ts.UniformTime(duration=10, sampling_rate=1)
        b = ts.UniformTime(a, time_unit=unit)
        npt.assert_equal(a.sampling_interval, b.sampling_interval)
        npt.assert_equal(a.sampling_rate, b.sampling_rate)

        b = ts.UniformTime(a, duration=2 * duration, time_unit=unit)
        npt.assert_equal(a.sampling_interval, b.sampling_interval)
        npt.assert_equal(a.sampling_rate, b.sampling_rate)

        b = ts.UniformTime(a, length=100, time_unit=unit)
        npt.assert_equal(a.sampling_interval, b.sampling_interval)
        npt.assert_equal(a.sampling_rate, b.sampling_rate)

        b = ts.UniformTime(a, length=100, time_unit=unit)
        npt.assert_equal(a.sampling_interval, b.sampling_interval)
        npt.assert_equal(a.sampling_rate, b.sampling_rate)

        b = ts.UniformTime(a, length=100, duration=duration, time_unit=unit)
        c = ts.UniformTime(length=100, duration=duration, time_unit=unit)
        npt.assert_equal(c, b)

        b = ts.UniformTime(sampling_interval=1, duration=10, time_unit=unit)
        c = ts.UniformTime(sampling_rate=tuc['s'] / tuc[unit],
                           length=10, time_unit=unit)

        npt.assert_equal(c, b)

        #This should raise a value error, because the duration is shorter than
        #the sampling_interval:
        npt.assert_raises(ValueError,
                                ts.UniformTime,
                                dict(sampling_interval=10, duration=1))

    #Time objects can be initialized with other time objects setting the
    #duration, sampling_interval and sampling_rate:

    a = ts.UniformTime(length=1, sampling_rate=1)
    npt.assert_raises(ValueError, ts.UniformTime, dict(data=a,
        sampling_rate=10, sampling_interval=.1))
    b = ts.UniformTime(duration=2 * a.sampling_interval,
                       sampling_rate=2 * a.sampling_rate)

    npt.assert_equal(ts.Frequency(b.sampling_rate),
                     ts.Frequency(2 * a.sampling_rate))
    npt.assert_equal(b.sampling_interval,
                           ts.TimeArray(0.5 * a.sampling_rate))

    b = ts.UniformTime(duration=10,
                       sampling_interval=a.sampling_interval)

    npt.assert_equal(b.sampling_rate, a.sampling_rate)

    b = ts.UniformTime(duration=10,
                       sampling_rate=a.sampling_rate)

    npt.assert_equal(b.sampling_interval, a.sampling_interval)

    # make sure the t0 ando other attribute is copied
    a = ts.UniformTime(length=1, sampling_rate=1)
    b = a.copy()
    npt.assert_equal(b.duration, a.duration)
    npt.assert_equal(b.sampling_rate, a.sampling_rate)
    npt.assert_equal(b.sampling_interval, a.sampling_interval)
    npt.assert_equal(b.t0, a.t0)


def test_UniformTime_repr():
    """
    >>> time1 = ts.UniformTime(sampling_rate=1000,time_unit='ms',length=3)
    >>> time1.sampling_rate
    1000.0 Hz
    >>> time1
    UniformTime([ 0.,  1.,  2.], time_unit='ms')

    >>> time2= ts.UniformTime(sampling_rate=1000,time_unit='s',length=3)
    >>> time2.sampling_rate
    1000.0 Hz
    >>> time2
    UniformTime([ 0.   ,  0.001,  0.002], time_unit='s')

    >>> a = ts.UniformTime(length=5,sampling_rate=1,time_unit='ms')

    >>> b = ts.UniformTime(a)

    >>> b
    UniformTime([    0.,  1000.,  2000.,  3000.,  4000.], time_unit='ms')

    >>> a
    UniformTime([    0.,  1000.,  2000.,  3000.,  4000.], time_unit='ms')

    >>> b = ts.UniformTime(a,time_unit='s')

    >>> b
    UniformTime([ 0.,  1.,  2.,  3.,  4.], time_unit='s')

    >>> a = ts.UniformTime(length=1,sampling_rate=2)

    >>> b = ts.UniformTime(length=10,sampling_interval=a.sampling_interval)

    >>> b.sampling_rate
    2.0 Hz
    """



def test_Frequency():
    """Test frequency representation object"""
    tuc = ts.time_unit_conversion
    for unit in ['ns', 'ms', 's', None]:
        f = ts.Frequency(1, time_unit=unit)
        npt.assert_equal(f.to_period(), tuc[unit])

        f = ts.Frequency(1000, time_unit=unit)
        npt.assert_equal(f.to_period(), tuc[unit] / 1000)

        f = ts.Frequency(0.001, time_unit=unit)
        npt.assert_equal(f.to_period(), tuc[unit] * 1000)



def test_TimeSeries():
    """Testing the initialization of the uniform time series object """

    #Test initialization with duration:
    tseries1 = ts.TimeSeries([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], duration=10)
    tseries2 = ts.TimeSeries([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], sampling_interval=1)
    npt.assert_equal(tseries1.time, tseries2.time)

    #downsampling:
    t1 = ts.UniformTime(length=8, sampling_rate=2)
    #duration is the same, but we're downsampling to 1Hz
    tseries1 = ts.TimeSeries(data=[1, 2, 3, 4], time=t1, sampling_rate=1)
    #If you didn't explicitely provide the rate you want to downsample to, that
    #is an error:
    npt.assert_raises(ValueError, ts.TimeSeries, dict(data=[1, 2, 3, 4],
                                                           time=t1))

    tseries2 = ts.TimeSeries(data=[1, 2, 3, 4], sampling_rate=1)
    tseries3 = ts.TimeSeries(data=[1, 2, 3, 4], sampling_rate=1000,
                                    time_unit='ms')
    #you can specify the sampling_rate or the sampling_interval, to the same
    #effect, where specificying the sampling_interval is in the units of that
    #time-series:
    tseries4 = ts.TimeSeries(data=[1, 2, 3, 4], sampling_interval=1,
                                        time_unit='ms')
    npt.assert_equal(tseries4.time, tseries3.time)

    #The units you use shouldn't matter - time is time:
    tseries6 = ts.TimeSeries(data=[1, 2, 3, 4],
                                    sampling_interval=0.001,
                                    time_unit='s')
    npt.assert_equal(tseries6.time, tseries3.time)

    #And this too - perverse, but should be possible:
    tseries5 = ts.TimeSeries(data=[1, 2, 3, 4],
                                    sampling_interval=ts.TimeArray(0.001,
                                                         time_unit='s'),
                                    time_unit='ms')

    npt.assert_equal(tseries5.time, tseries3.time)

    #initializing with a UniformTime object:
    t = ts.UniformTime(length=3, sampling_rate=3)

    data = [1, 2, 3]

    tseries7 = ts.TimeSeries(data=data, time=t)

    npt.assert_equal(tseries7.data, data)

    data = [1, 2, 3, 4]
    #If the data is not the right length, that should throw an error:
    npt.assert_raises(ValueError,
                          ts.TimeSeries, dict(data=data, time=t))

    # test basic arithmetics wiht TimeSeries
    tseries1 = ts.TimeSeries([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], sampling_rate=1)
    tseries2 = tseries1 + 1
    npt.assert_equal(tseries1.data + 1, tseries2.data)
    npt.assert_equal(tseries1.time, tseries2.time)
    tseries2 -= 1
    npt.assert_equal(tseries1.data, tseries2.data)
    npt.assert_equal(tseries1.time, tseries2.time)
    tseries2 = tseries1 * 2
    npt.assert_equal(tseries1.data * 2, tseries2.data)
    npt.assert_equal(tseries1.time, tseries2.time)
    tseries2 /= 2
    npt.assert_equal(tseries1.data, tseries2.data)
    npt.assert_equal(tseries1.time, tseries2.time)


def test_TimeSeries_repr():
    """
    >>> t=ts.UniformTime(length=3,sampling_rate=3)
    >>> tseries1 = ts.TimeSeries(data=[3,5,8],time=t)
    >>> t.sampling_rate
    3.0 Hz
    >>> tseries1.sampling_rate
    3.0 Hz
    >>> tseries1 = ts.TimeSeries(data=[3,5,8],sampling_rate=3)
    >>> tseries1.time
    UniformTime([ 0.    ,  0.3333,  0.6667], time_unit='s')
    >>> tseries1.sampling_rate
    3.0 Hz
    >>> tseries1.sampling_interval
    0.333333333333 s
    >>> a = ts.UniformTime(length=1,sampling_rate=2)

    >>> b = ts.TimeSeries(data=[1,2,3],sampling_interval=a.sampling_interval)

    >>> b.sampling_rate
    2.0 Hz


    >>> a = ts.UniformTime(length=1,sampling_rate=1)

    >>> b = ts.TimeSeries(data=[1,2,3],sampling_interval=a.sampling_interval)

    >>> b.sampling_rate
    1.0 Hz
    """



def test_Epochs():
    tms = ts.TimeArray(data=list(range(100)), time_unit='ms')
    tmin = ts.TimeArray(data=list(range(100)), time_unit='m')
    tsec = ts.TimeArray(data=list(range(100)), time_unit='s')

    utms = ts.UniformTime(length=100, sampling_interval=1, time_unit='ms')
    utmin = ts.UniformTime(length=100, sampling_interval=1, time_unit='m')
    utsec = ts.UniformTime(length=100, sampling_interval=1, time_unit='s')

    tsms = ts.TimeSeries(data=list(range(100)), sampling_interval=1, time_unit='ms')
    tsmin = ts.TimeSeries(data=list(range(100)), sampling_interval=1, time_unit='m')
    tssec = ts.TimeSeries(data=list(range(100)), sampling_interval=1, time_unit='s')

    # one millisecond epoch
    e1ms = ts.Epochs(0, 1, time_unit='ms')
    e09ms = ts.Epochs(0.1, 1, time_unit='ms')
    msg = "Seems like a problem with copy=False in TimeArray constructor."
    npt.assert_equal(e1ms.duration, ts.TimeArray(1, time_unit='ms'), msg)

    # one day
    e1d = ts.Epochs(0, 1, time_unit='D')
    npt.assert_equal(e1d.duration, ts.TimeArray(1, time_unit='D'), msg)

    e1ms_ar = ts.Epochs([0, 0], [1, 1], time_unit='ms')

    for t in [tms, tmin, tsec, utms, utmin, utsec]:

        # the sample time arrays are all at least 1ms long, so this should
        # return a timearray that has exactly one time point in it
        npt.assert_equal(len(t.during(e1ms)), 1)

        # this time epoch should not contain any point
        npt.assert_equal(len(t.during(e09ms)), 0)

        # make sure, slicing doesn't change the class
        npt.assert_equal(type(t), type(t.during(e1ms)))

    for t in [tsms, tsmin, tssec]:
        # the sample time series are all at least 1ms long, so this should
        # return a timeseries that has exactly one time point in it
        npt.assert_equal(len(t.during(e1ms)), 1)

        # make sure, slicing doesn't change the class
        npt.assert_equal(type(t), type(t.during(e1ms)))

        # same thing but now there's an array of epochs
        e2 = ts.Epochs([0, 10], [10, 20], time_unit=t.time_unit)

        # make sure, slicing doesn't change the class for array of epochs
        npt.assert_equal(type(t), type(t.during(e2)))

        # Indexing with an array of epochs (all of which are the same length)
        npt.assert_equal(t[e2].data.shape, (2, 10))
        npt.assert_equal(len(t.during(e2)), 10)
        npt.assert_equal(t[e2].data.ndim, 2)
        # check the data at some timepoints (a dimension was added)
        npt.assert_equal(t[e2][0], (0, 10))
        npt.assert_equal(t[e2][1], (1, 11))
        # check the data for each epoch
        npt.assert_equal(t[e2].data[0], list(range(10)))
        npt.assert_equal(t[e2].data[1], list(range(10, 20)))
        npt.assert_equal(t[e2].duration, e2[0].duration)

        # slice with Epochs of different length (not supported for timeseries,
        # raise error, though future jagged array implementation could go here)
        ejag = ts.Epochs([0, 10], [10, 40], time_unit=t.time_unit)
        # next line is the same as t[ejag]
        npt.assert_raises(ValueError, t.__getitem__, ejag)

        # if an epoch lies entirely between samples in the timeseries,
        # return an empty array
        eshort = ts.Epochs(2.5, 2.7, time_unit=t.time_unit)
        npt.assert_equal(len(t[eshort].data), 0)

        e1ms_outofrange = ts.Epochs(200, 300, time_unit=t.time_unit)
        # assert that with the epoch moved outside of the time range of our
        # data, slicing with the epoch now yields an empty array
        npt.assert_raises(ValueError, t.during, dict(e=e1ms_outofrange))

        # the sample timeseries are all shorter than a day, so this should
        # raise an error (instead of padding, or returning a shorter than
        # expected array.
        npt.assert_raises(ValueError, t.during, dict(e=e1d))

def test_basic_slicing():
    t = ts.TimeArray(list(range(4)))

    for x in range(3):
        ep  = ts.Epochs(.5,x+.5)
        npt.assert_equal(len(t[ep]), x)

    # epoch starts before timeseries
    npt.assert_equal(len(t[ts.Epochs(-1,3)]), len(t)-1)
    # epoch ends after timeseries
    npt.assert_equal(len(t[ts.Epochs(.5,5)]), len(t)-1)
    # epoch starts before and ends after timeseries
    npt.assert_equal(len(t[ts.Epochs(-1,100)]), len(t))
    ep  = ts.Epochs(20,100)
    npt.assert_equal(len(t[ep]), 0)


def test_Events():

    # time has to be one-dimensional
    nt.assert_raises(ValueError, ts.Events, np.zeros((2, 2)))

    t = ts.TimeArray([1, 2, 3], time_unit='ms')
    x = [1, 2, 3]
    y = [2, 4, 6]
    z = [10., 20., 30.]
    i0 = [0, 0, 1]
    i1 = [0, 1, 2]
    for unit in [None, 's', 'ns', 'D']:
        # events with data
        ev1 = ts.Events(t, time_unit=unit, i=x, j=y, k=z)

        # events with indices
        ev2 = ts.Events(t, time_unit=unit, indices=[i0, i1])

        # events with indices and labels
        ev3 = ts.Events(t, time_unit=unit, labels=['trial', 'other'],
                        indices=[i0, i1])

        # Note that the length of indices and labels has to be identical:
        nt.assert_raises(ValueError, ts.Events, t, time_unit=unit,
                                                       labels=['trial',
                                                               'other'],
                                                       indices=[i0])# Only
                                                                    # one of
                                                                    # the
                                                                    # indices!

        # make sure the time is retained
        npt.assert_equal(ev1.time, t)
        npt.assert_equal(ev2.time, t)

        # make sure time unit is correct
        if unit is not None:
            npt.assert_equal(ev1.time_unit, unit)
            npt.assert_equal(ev2.time_unit, unit)
        else:
            npt.assert_equal(ev1.time_unit, t.time_unit)
            npt.assert_equal(ev2.time_unit, t.time_unit)

        # make sure we can extract data
        npt.assert_equal(ev1.data['i'], x)
        npt.assert_equal(ev1.data['j'], y)
        npt.assert_equal(ev1.data['k'], z)

        # make sure we can get the indices by label
        npt.assert_equal(ev3.index.trial, i0)
        npt.assert_equal(ev3.index.other, i1)

        # make sure we can get the indices by position
        npt.assert_equal(ev2.index.i0, i0)
        npt.assert_equal(ev2.index.i1, i1)

        #make sure slicing works
        #one_event = ts.Events(t[[0]],time_unit=unit,i=[x[0]],j=[y[0]],k=[z[0]])
        #regular indexing
        npt.assert_equal(ev1[0].data['i'], x[0])
        npt.assert_equal(ev1[0:2].data['i'], x[0:2])

        # indexing w/ time
        npt.assert_equal(ev1[0.].data['i'], x[0])

        # indexing w/ epoch
        ep = ts.Epochs(start=0, stop=1.5, time_unit='ms')
        npt.assert_equal(ev1[ep].data['i'], x[0])

        # fancy indexing (w/ boolean mask)
        npt.assert_equal(ev1[ev3.index.trial == 0].data['j'], y[0:2])

        # len() function is implemented and working
        assert len(t) == len(ev1) == len(ev2) == len(ev3)

def test_Events_scalar():
    t = ts.TimeArray(1, time_unit='ms')
    i, j = 4, 5
    ev = ts.Events(t, i=i, j=j)
    # The semantics of scalar indexing into events are such that the returned
    # value is always a new Events object (the mental model is that of python
    # strings, where slicing OR scalar indexing still return the same thing, a
    # string again -- there are no 'string scalars', and there are no 'Event
    # scalars' either).
    npt.assert_equal(ev.data['i'][0], i)
    npt.assert_equal(ev.data['j'][0], j)



def test_index_at_20101206():
    """Test for incorrect handling of negative t0 for time.index_at

    https://github.com/nipy/nitime/issues#issue/35

    bug reported by Jonathan Taylor on 2010-12-06
    """
    A = np.random.standard_normal(40)
    #negative t0
    TS_A = ts.TimeSeries(A, t0=-20, sampling_interval=2)
    npt.assert_equal(TS_A.time.index_at(TS_A.time), np.arange(40))
    #positive t0
    TS_A = ts.TimeSeries(A, t0=15, sampling_interval=2)
    npt.assert_equal(TS_A.time.index_at(TS_A.time), np.arange(40))
    #no t0
    TS_A = ts.TimeSeries(A, sampling_interval=2)
    npt.assert_equal(TS_A.time.index_at(TS_A.time), np.arange(40))

def test_masked_array_timeseries():
    # make sure masked arrays passed in stay as masked arrays
    masked = np.ma.masked_invalid([0,np.nan,2])
    t = ts.TimeSeries(masked, sampling_interval=1)
    npt.assert_equal(t.data.mask, [False, True, False])

    # make sure regular arrays passed don't become masked
    notmasked = np.array([0,np.nan,2])
    t2 = ts.TimeSeries(notmasked, sampling_interval=1)
    npt.assert_raises(AttributeError, t2.data.__getattribute__,'mask')

def test_masked_array_events():
    # make sure masked arrays passed in stay as masked arrays
    masked = np.ma.masked_invalid([0,np.nan,2])
    e = ts.Events([1,2,3], d=masked)
    npt.assert_equal(e.data['d'].mask, [False, True, False])

    # make sure regular arrays passed don't become masked
    notmasked = np.array([0,np.nan,2])
    e2 = ts.Events([1,2,3], d=notmasked)
    npt.assert_raises(AttributeError, e2.data['d'].__getattribute__,'mask')

def test_event_subclass_slicing():
    "Subclassing Events should preserve the subclass after slicing"
    class Events_with_X(ts.Events):
        "A class which shows as attributes all of the event data"
        def __getattr__(self,k):
            return self.data[k]
        pass
    time = np.linspace(0,10,11)
    x,y = np.sin(time),np.cos(time)
    e = Events_with_X(time, **dict(x=x,y=y))
    npt.assert_equal(e.x, e.data['x'])
    npt.assert_equal(e.y, e.data['y'])
    slice_of_e = e[:4]
    slice_of_e.x # should not raise attribute error
    slice_of_e.y # should not raise attribute error
    npt.assert_equal(slice_of_e.x, x[:4])
    npt.assert_equal(slice_of_e.y, y[:4])
    assert(slice_of_e.__class__ == Events_with_X)

def test_epochs_subclass_slicing():
    "Subclassing Epochs should preserve the subclass after slicing"
    class Epochs_with_X(ts.Epochs):
        "An epoch class with extra 'stuff'"
        def total_duration(self):
            """Duration array for the epoch"""
            # XXX: bug in duration after slicing - attr_onread should be reset
            # after slicing
            #return self.duration.sum()
            return (self.stop - self.start).sum()

    time_0 = list(range(10))
    e = Epochs_with_X(time_0, duration=.2)
    npt.assert_equal(e.total_duration(), ts.TimeArray(2.0))

    slice_of_e = e[:5]
    npt.assert_equal(slice_of_e.total_duration(), ts.TimeArray(1.0))
    assert(slice_of_e.__class__ == Epochs_with_X)

def test_Epochs_duration_after_slicing():
    "some attributes which get set on read should be reset after slicing"
    e = ts.Epochs(list(range(10)),duration=.1)
    npt.assert_equal(len(e.duration), len(e))
    slice_of_e = e[:3]
    npt.assert_equal(len(slice_of_e.duration), len(slice_of_e))

def test_UniformTime_preserves_uniformity():
    "Uniformity: allow ops which keep it, and deny those which break it"
    utime = ts.UniformTime(t0=0, length=10, sampling_rate=1)

    def assign_to_one_element_of(t): t[0]=42
    nt.assert_raises(ValueError, assign_to_one_element_of,utime)

    # same as utime, but starting 10s later
    utime10 = ts.UniformTime(t0=10, length=10, sampling_rate=1)
    utime += 10 # constants treated as having same units as utime
    npt.assert_equal(utime,utime10)

    # same as utime, but with a lower sampling rate
    utime_2 = ts.UniformTime(t0=10, length=10, sampling_interval=2)
    utime += np.arange(10) # make utime match utime_2
    npt.assert_equal(utime,utime_2)
    npt.assert_equal(utime.sampling_interval,utime_2.sampling_interval)

    utime = ts.UniformTime(t0=5, length=10, sampling_rate=1)
    utime *= 2 # alternative way to make utime match utime_2
    npt.assert_equal(utime.sampling_interval,utime_2.sampling_interval)
    npt.assert_equal(utime.sampling_rate,utime_2.sampling_rate)

    nonuniform = np.concatenate((list(range(2)),list(range(3)), list(range(5))))
    def iadd_nonuniform(t): t+=nonuniform
    nt.assert_raises(ValueError, iadd_nonuniform, utime)

def test_index_int64():
    "indexing with int64 should still return a valid TimeArray"
    a = list(range(10))
    b = ts.TimeArray(a)
    assert b[0] == b[np.int64(0)]
    assert repr(b[0]) == repr(b[np.int64(0)])
    assert b[0] == b[np.int32(0)]
    assert repr(b[0]) == repr(b[np.int32(0)])

def test_timearray_math_functions():
    "Calling TimeArray.min() .max(), mean() should return TimeArrays"
    a = np.arange(2,11)
    for f in ['min','max','mean', 'ptp', 'sum']:
        for tu in ['s', 'ms', 'ps', 'D']:
            b = ts.TimeArray(a, time_unit=tu)
            assert getattr(b, f)().__class__ == ts.TimeArray
            assert getattr(b, f)().time_unit== b.time_unit
            # comparison with unitless should convert to the TimeArray's units
            assert getattr(b, f)() == getattr(a,f)()

def test_timearray_var_prod():
    """
    Variance and product change the TimeArray units, so they are not
    implemented and raise an error
    """
    a = ts.TimeArray(list(range(10)))
    npt.assert_raises(NotImplementedError, a.var)
    npt.assert_raises(NotImplementedError, a.prod)

########NEW FILE########
__FILENAME__ = test_utils
import numpy as np
import numpy.testing as npt
import nose.tools as nt

from nitime import utils
import nitime.algorithms as alg


def test_zscore():

    x = np.array([[1, 1, 3, 3],
                  [4, 4, 6, 6]])

    z = utils.zscore(x)
    npt.assert_equal(x.shape, z.shape)

    #Default axis is -1
    npt.assert_equal(utils.zscore(x), np.array([[-1., -1., 1., 1.],
                                                      [-1., -1., 1., 1.]]))

    #Test other axis:
    npt.assert_equal(utils.zscore(x, 0), np.array([[-1., -1., -1., -1.],
                                                        [1., 1., 1., 1.]]))


def test_percent_change():
    x = np.array([[99, 100, 101], [4, 5, 6]])
    p = utils.percent_change(x)

    npt.assert_equal(x.shape, p.shape)
    npt.assert_almost_equal(p[0, 2], 1.0)

    ts = np.arange(4 * 5).reshape(4, 5)
    ax = 0
    npt.assert_almost_equal(utils.percent_change(ts, ax), np.array(
        [[-100., -88.23529412, -78.94736842, -71.42857143, -65.2173913],
        [-33.33333333, -29.41176471, -26.31578947, -23.80952381, -21.73913043],
        [33.33333333,   29.41176471,   26.31578947,   23.80952381, 21.73913043],
        [100., 88.23529412, 78.94736842, 71.42857143, 65.2173913]]))

    ax = 1
    npt.assert_almost_equal(utils.percent_change(ts, ax), np.array(
        [[-100., -50., 0., 50., 100.],
         [-28.57142857, -14.28571429, 0., 14.28571429, 28.57142857],
          [-16.66666667, -8.33333333, 0., 8.33333333, 16.66666667],
          [-11.76470588, -5.88235294, 0., 5.88235294, 11.76470588]]))

def test_tridi_inverse_iteration():
    import scipy.linalg as la
    from scipy.sparse import spdiags
    # set up a spectral concentration eigenvalue problem for testing
    N = 2000
    NW = 4
    K = 8
    W = float(NW) / N
    nidx = np.arange(N, dtype='d')
    ab = np.zeros((2, N), 'd')
    # store this separately for tridisolve later
    sup_diag = np.zeros((N,), 'd')
    sup_diag[:-1] = nidx[1:] * (N - nidx[1:]) / 2.
    ab[0, 1:] = sup_diag[:-1]
    ab[1] = ((N - 1 - 2 * nidx) / 2.) ** 2 * np.cos(2 * np.pi * W)
    # only calculate the highest Kmax-1 eigenvalues
    w = la.eigvals_banded(ab, select='i', select_range=(N - K, N - 1))
    w = w[::-1]
    E = np.zeros((K, N), 'd')
    t = np.linspace(0, np.pi, N)
    # make sparse tridiagonal matrix for eigenvector check
    sp_data = np.zeros((3,N), 'd')
    sp_data[0, :-1] = sup_diag[:-1]
    sp_data[1] = ab[1]
    sp_data[2, 1:] = sup_diag[:-1]
    A = spdiags(sp_data, [-1, 0, 1], N, N)
    E = np.zeros((K,N), 'd')
    for j in range(K):
        e = utils.tridi_inverse_iteration(
            ab[1], sup_diag, w[j], x0=np.sin((j+1)*t)
            )
        b = A*e
        nt.assert_true(
               np.linalg.norm(np.abs(b) - np.abs(w[j]*e)) < 1e-8,
               'Inverse iteration eigenvector solution is inconsistent with '\
               'given eigenvalue'
               )
        E[j] = e

    # also test orthonormality of the eigenvectors
    ident = np.dot(E, E.T)
    npt.assert_almost_equal(ident, np.eye(K))

def test_debias():
    x = np.arange(64).reshape(4, 4, 4)
    x0 = utils.remove_bias(x, axis=1)
    npt.assert_equal((x0.mean(axis=1) == 0).all(), True)


def ref_crosscov(x, y, all_lags=True):
    "Computes sxy[k] = E{x[n]*y[n+k]}"
    x = utils.remove_bias(x, 0)
    y = utils.remove_bias(y, 0)
    lx, ly = len(x), len(y)
    pad_len = lx + ly - 1
    sxy = np.correlate(x, y, mode='full') / lx
    if all_lags:
        return sxy
    c_idx = pad_len / 2
    return sxy[c_idx:]


def test_crosscov():
    N = 128
    ar_seq1, _, _ = utils.ar_generator(N=N)
    ar_seq2, _, _ = utils.ar_generator(N=N)

    for all_lags in (True, False):
        sxy = utils.crosscov(ar_seq1, ar_seq2, all_lags=all_lags)
        sxy_ref = ref_crosscov(ar_seq1, ar_seq2, all_lags=all_lags)
        err = sxy_ref - sxy
        mse = np.dot(err, err) / N
        nt.assert_true(mse < 1e-12, \
               'Large mean square error w.r.t. reference cross covariance')


def test_autocorr():
    N = 128
    ar_seq, _, _ = utils.ar_generator(N=N)
    rxx = utils.autocorr(ar_seq)
    nt.assert_true(rxx[0] == rxx.max(), \
          'Zero lag autocorrelation is not maximum autocorrelation')
    rxx = utils.autocorr(ar_seq, all_lags=True)
    nt.assert_true(rxx[127] == rxx.max(), \
          'Zero lag autocorrelation is not maximum autocorrelation')

def test_information_criteria():
    """

    Test the implementation of information criteria:

    """
    a1 = np.array([[0.9, 0],
                   [0.16, 0.8]])

    a2 = np.array([[-0.5, 0],
                  [-0.2, -0.5]])

    am = np.array([-a1, -a2])

    x_var = 1
    y_var = 0.7
    xy_cov = 0.4
    cov = np.array([[x_var, xy_cov],
                    [xy_cov, y_var]])

    #Number of realizations of the process
    N = 500
    #Length of each realization:
    L = 1024

    order = am.shape[0]
    n_process = am.shape[-1]

    z = np.empty((N, n_process, L))
    nz = np.empty((N, n_process, L))

    for i in range(N):
        z[i], nz[i] = utils.generate_mar(am, cov, L)

    AIC = []
    BIC = []
    AICc = []

    # The total number data points available for estimation:
    Ntotal = L * n_process

    for n_lags in range(1, 10):

        Rxx = np.empty((N, n_process, n_process, n_lags))

        for i in range(N):
            Rxx[i] = utils.autocov_vector(z[i], nlags=n_lags)

        Rxx = Rxx.mean(axis=0)
        Rxx = Rxx.transpose(2, 0, 1)

        a, ecov = alg.lwr_recursion(Rxx)

        IC = utils.akaike_information_criterion(ecov, n_process, n_lags, Ntotal)
        AIC.append(IC)

        IC = utils.akaike_information_criterion(ecov, n_process, n_lags, Ntotal, corrected=True)
        AICc.append(IC)

        IC = utils.bayesian_information_criterion(ecov, n_process, n_lags, Ntotal)
        BIC.append(IC)

    # The model has order 2, so this should minimize on 2:

    # We do not test this for AIC/AICc, because these sometimes do not minimize
    # (see Ding and Bressler)
    # nt.assert_equal(np.argmin(AIC), 2)
    # nt.assert_equal(np.argmin(AICc), 2)
    nt.assert_equal(np.argmin(BIC), 2)


def test_multi_intersect():
    """
    Testing the multi-intersect utility function
    """

    arr1 = np.array(np.arange(1000).reshape(2,500))
    arr2 = np.array([[1,0.1,0.2],[0.3,0.4, 0.5]])
    arr3 = np.array(1)
    npt.assert_equal(1, utils.multi_intersect([arr1, arr2, arr3]))


def test_zero_pad():
    """
    Test the zero_pad function
    """
    # Freely assume that time is the last dimension:
    ts1 = np.empty((64, 64, 35, 32))
    NFFT = 64 
    zp1 = utils.zero_pad(ts1, NFFT)
    npt.assert_equal(zp1.shape[-1], NFFT)

    # Try this with something with only 1 dimension:
    ts2 = np.empty(64)
    zp2 = utils.zero_pad(ts2, NFFT)
    npt.assert_equal(zp2.shape[-1], NFFT)
    

def test_detect_lines():
    """
    Tests detect_lines utility in the reliable low-SNR scenario.
    """

    N = 1000
    fft_pow = int( np.ceil(np.log2(N) + 2) )
    NW = 4
    lines = np.sort(np.random.randint(100, 2**(fft_pow-4), size=(3,)))
    while np.any( np.diff(lines) < 2*NW ):
        lines = np.sort(np.random.randint(2**(fft_pow-4), size=(3,)))
    lines = lines.astype('d')
    #lines += np.random.randn(3) # displace from grid locations
    lines /= 2.0**(fft_pow-2) # ensure they are well separated

    phs = np.random.rand(3) * 2 * np.pi
    # amps approximately such that RMS power = 1 +/- N(0,1)
    amps = np.sqrt(2)/2 + np.abs( np.random.randn(3) )

    nz_sig = 0.05
    tx = np.arange(N)

    harmonics = amps[:,None]*np.cos( 2*np.pi*tx*lines[:,None] + phs[:,None] )
    harmonic = np.sum(harmonics, axis=0)
    nz = np.random.randn(N) * nz_sig
    sig = harmonic + nz

    f, b = utils.detect_lines(sig, (NW, 2*NW), low_bias=True, NFFT=2**fft_pow)
    h_est = 2*(b[:,None]*np.exp(2j*np.pi*tx*f[:,None])).real

    nt.assert_true(
        len(f) == 3, 'The wrong number of harmonic components were detected'
        )

    err = harmonic - np.sum(h_est, axis=0)
    phs_est = np.angle(b)
    phs_est[phs_est < 0] += 2*np.pi

    phs_err = np.linalg.norm(phs_est - phs)**2
    amp_err = np.linalg.norm(amps - 2*np.abs(b))**2 / np.linalg.norm(amps)**2
    freq_err = np.linalg.norm(lines - f)**2

    # FFT bin detections should be exact
    npt.assert_equal(lines, f)
    # amplitude estimation should be pretty good
    nt.assert_true(amp_err < 1e-4, 'Harmonic amplitude was poorly estimated')
    # phase estimation should be decent
    nt.assert_true(phs_err < 1e-3, 'Harmonic phase was poorly estimated')
    # the error relative to the noise power should be below 1
    rel_mse = np.mean(err**2)/nz_sig**2
    nt.assert_true(
        rel_mse < 1,
        'The error in detected harmonic components is too high relative to '\
        'the noise level: %1.2e'%rel_mse
        )

def test_detect_lines_2dmode():
    """
    Test multi-sequence operation
    """

    N = 1000

    sig = np.cos( 2*np.pi*np.arange(N) * 20./N ) + np.random.randn(N) * .01

    sig2d = np.row_stack( (sig, sig, sig) )

    lines = utils.detect_lines(sig2d, (4, 8), low_bias=True, NFFT=2**12)

    nt.assert_true(len(lines)==3, 'Detect lines failed multi-sequence mode')

    consistent1 = (lines[0][0] == lines[1][0]).all() and \
      (lines[1][0] == lines[2][0]).all()
    consistent2 = (lines[0][1] == lines[1][1]).all() and \
      (lines[1][1] == lines[2][1]).all()

    nt.assert_true(consistent1 and consistent2, 'Inconsistent results')


########NEW FILE########
__FILENAME__ = test_viz
"""

Smoke testing of the viz module.

"""

import numpy as np
import numpy.testing as npt

from nitime.timeseries import TimeSeries
from nitime.analysis import CorrelationAnalyzer
from nitime.viz import drawmatrix_channels, drawgraph_channels, plot_xcorr, nx

try:

    nx.__class__ # will raise an error if nx could not be imported in viz
    no_networkx = False
    no_networkx_msg = ''
except ImportError as e:
    no_networkx = True
    no_networkx_msg = e.args[0]

roi_names = ['a','b','c','d','e','f','g','h','i','j']
data = np.random.rand(10,1024)

T = TimeSeries(data, sampling_interval=np.pi)
T.metadata['roi'] = roi_names

#Initialize the correlation analyzer
C = CorrelationAnalyzer(T)

def test_drawmatrix_channels():
    fig01 = drawmatrix_channels(C.corrcoef, roi_names, size=[10., 10.], color_anchor=0)

def test_plot_xcorr():
    xc = C.xcorr_norm

    fig02 = plot_xcorr(xc,
                       ((0, 1),
                        (2, 3)),
                       line_labels=['a', 'b'])


@npt.dec.skipif(no_networkx,no_networkx_msg)
def test_drawgraph_channels():
    fig04 = drawgraph_channels(C.corrcoef, roi_names)

########NEW FILE########
__FILENAME__ = timeseries
"""Base classes for generic time series analysis.

The classes implemented here are meant to provide fairly basic objects for
managing time series data.  They should serve mainly as data containers, with
only minimal algorithmic functionality.

In the timeseries subpackage, there is a separate library of algorithms, and
the classes defined here mostly delegate any computational facilities they may
have to that library.

Over time, it is OK to add increasingly functionally rich classes, but only
after their design is well proven in real-world use.

"""
#-----------------------------------------------------------------------------
# Public interface
#-----------------------------------------------------------------------------
__all__ = ['time_unit_conversion',
           'TimeSeriesInterface',
           'TimeSeries',
           'TimeInterface',
           'UniformTime',
           'TimeArray',
           'Epochs',
           'Events'
           ]
#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

import numpy as np

# Our own
from nitime import descriptors as desc
import nitime.six as six

#-----------------------------------------------------------------------------
# Module globals
#-----------------------------------------------------------------------------

# These are the valid names for time units, taken from the Numpy date/time
# types specification document.  They conform to SI nomenclature where
# applicable.

# Most uses of this are membership checks, so we make a set for fast
# validation.  But we create them first as a list so we can print an ordered
# and easy to read error message.

time_unit_conversion = {
                        'ps': 1,  # picosecond
                        'ns': 10 ** 3,  # nanosecond
                        'us': 10 ** 6,  # microsecond
                        'ms': 10 ** 9,  # millisecond
                        's': 10 ** 12,   # second
                         None: 10 ** 12,  # The default is seconds (when
                                        # constructor doesn't get any
                                        # input, it defaults to None)
                        'm': 60 * 10 ** 12,   # minute
                        'h': 3600 * 10 ** 12,   # hour
                        'D': 24 * 3600 * 10 ** 12,   # day
                        'W': 7 * 24 * 3600 * 10 ** 12,  # week
                                                        # (not an SI unit)
                        }

# The basic resolution:
base_unit = 'ps'


#-----------------------------------------------------------------------------
# Class declarations
#-----------------------------------------------------------------------------

# Time:
class TimeInterface(object):
    """ The minimal object interface for time representations

    This should be thought of as an abstract base class. """

    time_unit = None


def get_time_unit(obj):
    """
    Extract the time unit of the object. If it is an iterable, get the time
    unit of the first element.
    """

    # If this is a Time object, no problem:
    if isinstance(obj, TimeInterface):
        return obj.time_unit

    # Otherwise, if it is iterable, we recurse on it:
    try:
        it = iter(obj)
    except TypeError:
        return None
    else:
        return get_time_unit(six.advance_iterator(it))


class TimeArray(np.ndarray, TimeInterface):
    """Base-class for time representations, implementing the TimeInterface"""
    def __new__(cls, data, time_unit=None, copy=True):
        """
        Parameters
        ----------
        data : 1-d array or `TimeArray` class instance
            Time points

        time_unit : str, optional
            The time-unit to use. This should be one of the keys of the
            `time_unit_conversion` dict from the :mod:`timeseries` module,
            which are SI units of time. Default: 's'

        copy : bool, optional
            Whether to create this instance by  copy of a

        Note
        ----
        If the 'copy' input is set to False, input must be either a `TimeArray`
        class instance, or an int64 array in the base unit of the module
        (which, unless you change it, is picoseconds)


        """

        # Check that the time units provided are sensible:
        if time_unit not in time_unit_conversion:
            raise ValueError('Invalid time unit %s, must be one of %s' %
                             (time_unit, time_unit_conversion.keys()))

        # Get the conversion factor from the input:
        conv_fac = time_unit_conversion[time_unit]

        # Call get_time_unit to pull the time_unit out from inside:
        data_time_unit = get_time_unit(data)
        # If it has a time unit, you should not convert the values to
        # base_unit, because they are already in that:
        if data_time_unit is not None:
            conv_fac = 1

        # We check whether the data has a time-unit somewhere inside (for
        # example, if it is a list of TimeArray objects):
        if time_unit is None:
            time_unit = data_time_unit

        # We can only honor the copy flag in a very narrow set of cases
        # if data is already a TimeArray or if data is an ndarray with
        # dtype=int64
        if copy == False:
            if not getattr(data, 'dtype', None) == np.int64:
                e_s = 'When copy flag is set to False, must provide a'
                e_s += 'TimeArray in object, or int64 times, in %s' % base_unit
                raise ValueError(e_s)

            time = np.array(data, copy=False)
        else:
            if isinstance(data, TimeInterface):
                time = data.copy()
            else:
                data_arr = np.asarray(data)
                if issubclass(data_arr.dtype.type, np.integer):
                    # If this is an array of integers, cast to 64 bit integer
                    # and convert to the base_unit.
                    #XXX This will fail when even 64 bit is not large enough to
                    # avoid wrap-around (When you try to make more than 10**6
                    # seconds). XXX this should be mentioned in the docstring
                    time = data_arr.astype(np.int64) * conv_fac
                else:
                    # Otherwise: first convert, round and then cast to 64
                    time = (data_arr * conv_fac).round().astype(np.int64)

        # Make sure you have an array on your hands (for example, if you input
        # an integer, you might have reverted to an integer when multiplying
        # with the conversion factor:
        time = np.asarray(time).view(cls)

        # Make sure time is one-dimensional or 0-d
        if time.ndim > 1:
            raise ValueError('TimeArray can only be one-dimensional or 0-d')

        if time_unit is None:
            time_unit = 's'

        time.time_unit = time_unit
        time._conversion_factor = time_unit_conversion[time_unit]
        return time

    def __array_wrap__(self, out_arr, context=None):
        # When doing comparisons between TimeArrays, make sure that you return
        # a boolean array, not a time array:
        if out_arr.dtype == bool:
            return np.asarray(out_arr)
        else:
            return np.ndarray.__array_wrap__(self, out_arr, context)

    def __array_finalize__(self, obj):
        """XXX """
        # Make sure that the TimeArray has the time units set (and not equal to
        # None):
        if not hasattr(self, 'time_unit') or self.time_unit is None:
            if hasattr(obj, 'time_unit'):  # looks like view cast
                self.time_unit = obj.time_unit
            else:
                self.time_unit = 's'

        # Make sure that the conversion factor is set properly:
        if not hasattr(self, '_conversion_factor'):
            if hasattr(obj, '_conversion_factor'):
                self._conversion_factor = obj._conversion_factor
            else:
                self._conversion_factor = time_unit_conversion[self.time_unit]

    def __repr__(self):
        """Pass it through the conversion factor"""

        # If the input is a single int/float (with no shape) return a 'scalar'
        # time-point:
        if self.shape == ():
            return "%r %s" % (int(self) / float(self._conversion_factor),
                           self.time_unit)
        # Otherwise, return the TimeArray representation:
        else:
            return np.ndarray.__repr__(self / float(self._conversion_factor)
             )[:-1] + ", time_unit='%s')" % self.time_unit

    def __str__(self):
        """Return a nice string representation of this TimeArray"""
        return self.__repr__()

    def __getitem__(self, key):
        # return scalar TimeArray in case key is integer
        if isinstance(key, (int, np.int64, np.int32)):
            return self[[key]].reshape(())
        elif isinstance(key, float):
            return self.at(key)
        elif isinstance(key, Epochs):
            return self.during(key)
        else:
            return np.ndarray.__getitem__(self, key)

    def __setitem__(self, key, val):
        # look at the units - convert the values to what they need to be (in
        # the base_unit) and then delegate to the ndarray.__setitem__
        if not hasattr(val, '_conversion_factor'):
            val *= self._conversion_factor
        return np.ndarray.__setitem__(self, key, val)

    def _convert_if_needed(self,val):
        if not hasattr(val, '_conversion_factor'):
            val = np.asarray(val)
            if getattr(val, 'dtype', None) == np.int32:
                # we'll overflow if val's dtype is np.int32
                val = np.array(val, dtype=np.int64)
            val *= self._conversion_factor
        return val

    def __add__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__add__(self,val)

    def __sub__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__sub__(self,val)

    def __radd__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__radd__(self,val)

    def __rsub__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__rsub__(self,val)

    def __lt__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__lt__(self,val)

    def __gt__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__gt__(self,val)

    def __le__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__le__(self,val)

    def __ge__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__ge__(self,val)

    def __eq__(self,val):
        val = self._convert_if_needed(val)
        return np.ndarray.__eq__(self,val)
    
    def min(self, *args,**kwargs):
        ret = TimeArray(np.ndarray.min(self, *args,**kwargs),
            time_unit=base_unit)
        ret.convert_unit(self.time_unit)
        return ret

    def max(self, *args,**kwargs):
        ret = TimeArray(np.ndarray.max(self, *args,**kwargs),
            time_unit=base_unit)
        ret.convert_unit(self.time_unit)
        return ret

    def mean(self, *args,**kwargs):
        ret = TimeArray(np.ndarray.mean(self, *args,**kwargs),
            time_unit=base_unit)
        ret.convert_unit(self.time_unit)
        return ret

    def ptp(self, *args,**kwargs):
        ret = TimeArray(np.ndarray.ptp(self, *args,**kwargs),
            time_unit=base_unit)
        ret.convert_unit(self.time_unit)
        return ret

    def sum(self, *args,**kwargs):
        ret = TimeArray(np.ndarray.sum(self, *args,**kwargs),
            time_unit=base_unit)
        ret.convert_unit(self.time_unit)
        return ret
    
    def prod(self, *args, **kwargs):
        e_s = "Product computation changes TimeArray units"
        raise NotImplementedError(e_s)
        
    
    def var(self, *args, **kwargs):
        e_s = "Variance computation changes TimeArray units"
        raise NotImplementedError(e_s)

        
    def std(self, *args, **kwargs):
        """Returns the standard deviation of this TimeArray (with time units)

        for detailed information, see numpy.std()
        """
        ret = TimeArray(np.ndarray.std(self, *args,**kwargs),
            time_unit=base_unit)
        ret.convert_unit(self.time_unit)
        return ret


    def index_at(self, t, tol=None, mode='closest'):
        """ Returns the integer indices that corresponds to the time t

        The returned indices depend on both `tol` and `mode`.  The `tol`
        parameter specifies how close the given time must be to those present
        in the array to give a match, when `mode` is `closest`.  The default
        tolerance is 1 `base_unit` (by default, picoseconds).  If you specify
        the tolerance as 0, then only *exact* matches are allowed, be careful
        in this case of possible problems due to floating point roundoff error
        in your time specification.

        When mode is `before` or `after`, the tolerance is completely ignored.
        In this case, either the largest time equal or *before* the given `t`
        or the earliest time equal or *after* the given `t` is returned.

        Parameters
        ----------
        t : time-like
          Anything that is valid input for a TimeArray constructor.
        tol : time-like, optional
          Tolerance, specified in the time units of this TimeArray.
        mode : string
          One of ['closest', 'before', 'after'].

        Returns
        -------
        idx : The array with all the indices where the condition is met.
          """
        if not np.iterable(t):
            t = [t]
        t_e = TimeArray(t, time_unit=self.time_unit)
        if mode == 'closest':
            return self._index_closest(t_e, tol)
        elif mode == 'before':
            return self._index_before(t_e)
        elif mode == 'after':
            return self._index_after(t_e)
        else:
            raise ValueError('Invalid mode specification')

    def _index_closest(self, t, tol=None):
        d = np.abs(self - t)
        if tol is None:
            # If no tolerance is specified, use one clock tick of the
            # base_unit:
            tol = clock_tick

        # tolerance is converted into a time-array, so that it does the
        # right thing:
        ttol = TimeArray(tol, time_unit=self.time_unit)
        return np.where(d <= ttol)[0]

    def _index_before(self, t):
        # Use the standard Decorate-Sort-Undecorate (Schwartzian transform)
        # pattern to find the right index.
        cond = np.where(self <= t)[0]
        if len(cond) == 0:
            return cond
        idx_max = self[cond].argmax()
        return cond[idx_max]

    def _index_after(self, t):
        cond = np.where(t <= self)[0]
        if len(cond) == 0:
            return cond

        idx_min = self[cond].argmin()
        return cond[idx_min]

    def slice_during(self, e):
        """ Returns the slice that corresponds to Epoch e"""

        if not isinstance(e, Epochs):
            raise ValueError('e has to be of Epochs type')

        if e.data.ndim > 0:
            raise NotImplementedError('e has to be a scalar Epoch')

        if self.ndim != 1:
            e_s = 'slicing only implemented for 1-d TimeArrays'
            return NotImplementedError(e_s)

        # These two should be called with modes, such that they catch the right
        # slice
        start = self.index_at(e.start, mode='after')
        stop = self.index_at(e.stop, mode='before')

        # If *either* the start or stop index object comes back as the empty
        # array, then it means the condition is not satisfied, we return the
        # slice that does [:0],  i.e., always slices to nothing.
        if start.shape == (0,) or stop.shape == (0,):
            return slice(0)

        # Now,  we know the start/stop are not empty arrays, but they can be
        # either scalars or arrays.
        i_start = start if np.isscalar(start) else start.max()
        i_stop = stop if np.isscalar(stop) else stop.min()

        if e.start > self[i_start]:  # make sure self[i_start] is in epoch e
            i_start += 1
        if e.stop > self[i_stop]:  # make sure to include self[i_stop]
            i_stop += 1

        return slice(i_start, i_stop)

    def at(self, t, tol=None):
        """ Returns the values of the TimeArray object at time t"""
        return self[self.index_at(t, tol=tol)]

    def during(self, e):
        """ Returns the values of the TimeArray object during Epoch e"""

        if not isinstance(e, Epochs):
            raise ValueError('e has to be of Epochs type')

        if e.data.ndim > 0:
            ## TODO: Implement slicing with 1-d Epochs array,
            ## resulting in (ragged/jagged) 2-d TimeArray
            raise NotImplementedError('e has to be a scalar Epoch')

        return self[self.slice_during(e)]

##     def min(self,axis=None,out=None):
##         """Returns the minimal time"""
##         # this is a quick fix to return a time and will
##         # be obsolete once we use proper time dtypes
##         if axis is not None:
##             raise NotImplementedError, 'axis argument not implemented'
##         if out is not None:
##             raise NotImplementedError, 'out argument not implemented'
##         if self.ndim:
##             return self[self.argmin()]
##         else:
##             return self

    def max(self, axis=None, out=None):
        """Returns the maximal time"""
        # this is a quick fix to return a time and will
        # be obsolete once we use proper time dtypes
        if axis is not None:
            raise NotImplementedError('axis argument not implemented')
        if out is not None:
            raise NotImplementedError('out argument not implemented')
        if self.ndim:
            return self[self.argmax()]
        else:
            return self

    def convert_unit(self, time_unit):
        """Convert from one time unit to another in place"""

        self.time_unit = time_unit
        self._conversion_factor = time_unit_conversion[time_unit]

    def __div__(self, d):
        """Division by another time object eliminates units """
        if isinstance(d, TimeInterface):
            return np.divide(np.array(self), np.array(d).astype(float))
        else:
            return np.divide(self, d)

    __truediv__ = __div__ # called by python3

# Globally define a single tick of the base unit:
clock_tick = TimeArray(1, time_unit=base_unit)


class UniformTime(np.ndarray, TimeInterface):
    """ A representation of time sampled uniformly
    """

    def __new__(cls, data=None, length=None, duration=None, sampling_rate=None,
                sampling_interval=None, t0=0, time_unit=None):
        """

        Parameters
        ----------
        length : int
            The number of items in the time-array

        duration : float,
            the duration to be represented (given in the time-unit) of the
            array. If this item is an TimeArray, the units of the UniformTime
            array resulting will 'inherit' the units of the
            duration. Otherwise, the unit of the UniformTime will be set by
            that kwarg

        sampling_rate : float
            The sampling rate (in Hz)

        sampling_interval : float
            The inverse of the sampling_interval

        t0 : float, int or singleton `TimeArray`
            The value of the first time-point in the array (unless given as a
            `TimeArray`, should be in the time-unit)

        time_unit : str, optional
            The time unit to be used in the representation of time

        """

        # Sanity checks. There are different valid combinations of inputs
        tspec = tuple(x is not None for x in
                      [sampling_interval, sampling_rate, length, duration])

        # Used in converting tspecs to human readable form
        tspec_arg_names = ['sampling_interval',
                           'sampling_rate',
                           'length',
                           'duration']

        # The valid configurations
        valid_tspecs = [
            # interval, length:
            (True, False, True, False),
            # interval, duration:
            (True, False, False, True),
            # rate, length:
            (False, True, True, False),
            # rate, duration:
            (False, True, False, True),
            # length, duration:
            (False, False, True, True)
            ]

        if isinstance(data, UniformTime):
            # Assuming data was given, some other tspecs become valid:
            tspecs_w_data = dict(
                    nothing=(False, False, False, False),
                    sampling_interval=(True, False, False, False),
                    sampling_rate=(False, True, False, False),
                    length=(False, False, True, False),
                    duration=(False, False, False, True))
            # preserve the order of the keys
            valid_tspecs.append(tspecs_w_data['nothing'])
            for name in tspec_arg_names:
                valid_tspecs.append(tspecs_w_data[name])

        if (tspec not in valid_tspecs):
            # l = ['sampling_interval', 'sampling_rate', 'length', 'duration']
            # args = [arg for t,arg in zip(tspec,l) if t]
            raise ValueError("Invalid time specification.\n" +
                "You provided: %s \n"
                "%s \nsee docstring for more info."
                % (str_tspec(tspec, tspec_arg_names),
                  str_valid_tspecs(valid_tspecs,
                                   tspec_arg_names)))

        if isinstance(data, UniformTime):
            # Get attributes from the UniformTime object and transfer those
            # over:
            if tspec == tspecs_w_data['nothing']:
                sampling_rate = data.sampling_rate
                duration = data.duration
            elif tspec == tspecs_w_data['sampling_interval']:
                duration == data.duration
            elif tspec == tspecs_w_data['sampling_rate']:
                if isinstance(sampling_rate, Frequency):
                    sampling_interval = sampling_rate.to_period()
                else:
                    sampling_interval = 1.0 / sampling_rate
                duration = data.duration
            elif tspec == tspecs_w_data['length']:
                duration = length * data.sampling_interval
                sampling_rate = data.sampling_rate
            elif tspec == tspecs_w_data['duration']:
                sampling_rate = data.sampling_rate
            if time_unit is None:
                # If the user didn't ask to change the time-unit, use the
                # time-unit from the object you got:
                time_unit = data.time_unit

        # Check that the time units provided are sensible:
        if time_unit not in time_unit_conversion:
            raise ValueError('Invalid time unit %s, must be one of %s' %
                         (time_unit, time_unit_conversion.keys()))

        # Make sure you have a time unit:
        if time_unit is None:
            #If you gave us a duration with time_unit attached
            if isinstance(duration, TimeInterface):
                time_unit = duration.time_unit
            #Otherwise, you might have given us a sampling_interval with a
            #time_unit attached:
            elif isinstance(sampling_interval, TimeInterface):
                time_unit = sampling_interval.time_unit
            else:
                time_unit = 's'

        # Calculate the sampling_interval or sampling_rate:
        if sampling_interval is None:
            if isinstance(sampling_rate, Frequency):
                c_f = time_unit_conversion[time_unit]
                sampling_interval = sampling_rate.to_period() / float(c_f)
            elif sampling_rate is None:
                sampling_interval = float(duration) / length
                sampling_rate = Frequency(1.0 / sampling_interval,
                                          time_unit=time_unit)
            else:
                c_f = time_unit_conversion[time_unit]
                sampling_rate = Frequency(sampling_rate, time_unit='s')
                sampling_interval = sampling_rate.to_period() / float(c_f)
        else:
            if isinstance(sampling_interval, TimeInterface):
                c_f = time_unit_conversion[sampling_interval.time_unit]
                sampling_rate = Frequency(1.0 / (float(sampling_interval) /
                                                                       c_f),
                                     time_unit=sampling_interval.time_unit)
            else:
                sampling_rate = Frequency(1.0 / sampling_interval,
                                          time_unit=time_unit)

        # Calculate the duration, if that is not defined:
        if duration is None:
            duration = length * sampling_interval

        # 'cast' the time inputs as TimeArray
        duration = TimeArray(duration, time_unit=time_unit)
        #XXX If data is given - the t0 should be taken from there:
        t0 = TimeArray(t0, time_unit=time_unit)
        sampling_interval = TimeArray(sampling_interval, time_unit=time_unit)

        # in order for time[-1]-time[0]==duration to be true (which it should)
        # add the sampling_interval to the stop value:
        # time = np.arange(np.int64(t0),
        #                  np.int64(t0+duration+sampling_interval),
        #                  np.int64(sampling_interval),dtype=np.int64)

        # But it's unclear whether that's really the behavior we want?
        time = np.arange(np.int64(t0), np.int64(t0 + duration),
                         np.int64(sampling_interval), dtype=np.int64)

        time = np.asarray(time).view(cls)
        time.time_unit = time_unit
        time._conversion_factor = time_unit_conversion[time_unit]
        time.duration = duration
        time.sampling_rate = Frequency(sampling_rate)
        time.sampling_interval = sampling_interval
        time.t0 = t0

        return time

    def __array_wrap__(self, out_arr, context=None):
        # When doing comparisons between UniformTime, make sure that you return
        # a boolean array, not a time array:
        if out_arr.dtype == bool:
            return np.asarray(out_arr)
        else:
            return np.ndarray.__array_wrap__(self, out_arr, context)

    def __array_finalize__(self, obj):
        """XXX """
        # Make sure that the UniformTime has the time units set (and not equal
        # to None):
        if not hasattr(self, 'time_unit') or self.time_unit is None:
            if hasattr(obj, 'time_unit'):  # looks like view cast
                self.time_unit = obj.time_unit
            else:
                self.time_unit = 's'

        # Make sure that the conversion factor is set properly:
        if not hasattr(self, '_conversion_factor'):
            if hasattr(obj, '_conversion_factor'):
                self._conversion_factor = obj._conversion_factor
            else:
                self._conversion_factor = time_unit_conversion[self.time_unit]

        # Make sure that t0 attribute is set properly:
        for attr in ['t0', 'sampling_rate', 'sampling_interval', 'duration']:
            if not hasattr(self, attr) and hasattr(obj, attr):
                setattr(self, attr, getattr(obj, attr))

    def __repr__(self):
        """Pass it through the conversion factor"""

        #If the input is a single int/float (with no shape) return a 'scalar'
        #time-point:
        if self.shape == ():
            return "%r %s" % (int(self) / float(self._conversion_factor),
                            self.time_unit)

        #Otherwise, return the UniformTime representation:
        else:
            return np.ndarray.__repr__(self / float(self._conversion_factor)
             )[:-1] + ", time_unit='%s')" % self.time_unit

    def __getitem__(self, key):
        # return scalar TimeArray in case key is integer
        if isinstance(key, (int, np.int64, np.int32)):
            return self[[key]].reshape(()).view(TimeArray)
        elif isinstance(key, float) or isinstance(key, TimeInterface):
            return self.at(key)
        elif isinstance(key, Epochs):
            return self.during(key)
        else:
            return np.ndarray.__getitem__(self, key)

    def __setitem__(self, key, val):
        raise ValueError("""Setting of individual indices would break uniformity:
            You can either use += on the full array, OR
            create a new TimeArray from this UniformTime""")

    def _convert_and_check_uniformity(self, val):
        # look at the units - convert the values to what they need to be (in
        # the base_unit) and then delegate to the ndarray.__iadd__
        if not hasattr(val, '_conversion_factor'):
            val = np.asarray(val)
            if getattr(val, 'dtype', None) == np.int32:
                # we'll overflow if val's dtype is np.int32
                val = np.array(val, dtype=np.int64)
            val *= self._conversion_factor
        if hasattr(val, 'ndim') and val.ndim == 1:
            # we have to check that adding this will preserve uniformity
            dv = np.diff(val)
            uniformity_breaks, = np.where(dv!=dv[0])
            if len(uniformity_breaks) != 0:
                raise ValueError(
                    """All elements in the operand array must have a constant
                    interval between them in order to preserve uniformity.
                    Uniformity is broken at these indices: %s
                    """ %str(uniformity_breaks))
            self.sampling_interval += dv[0]
            self.sampling_rate = Frequency(1.0 / (float(self.sampling_interval) /
                                        time_unit_conversion[self.time_unit]),
                                        time_unit=self.time_unit)
        return val

    def __iadd__(self, val):
        val = self._convert_and_check_uniformity(val)
        return np.ndarray.__iadd__(self, val)

    def __isub__(self, val):
        val = self._convert_and_check_uniformity(val)
        return np.ndarray.__isub__(self, val)

    def __imul__(self, val):
        np.ndarray.__imul__(self, val)
        self.sampling_interval *= val
        self.sampling_rate = Frequency(self.sampling_rate / val)
        return self

    def __idiv__(self, val):
        np.ndarray.__idiv__(self, val)
        self.sampling_interval /= val
        self.sampling_rate = Frequency(self.sampling_rate * val)
        return self

    __itruediv__ =  __idiv__ # for py3k

    def index_at(self, t, boolean=False):
        """Find the index that corresponds to the time bin containing t

           Returns boolean mask if boolean=True and integer indices otherwise.
        """

        # cast t into time
        ta = TimeArray(t, time_unit=self.time_unit)

        # check that index is within range
        if ta.min() < self.t0 or ta.max() >= self.t0 + self.duration:
            raise ValueError('index out of range')
        idx = (ta - self.t0) // self.sampling_interval
        if boolean:
            bool_idx = np.zeros(len(self), dtype=bool)
            bool_idx[idx] = True
            return bool_idx
        elif ta.ndim == 0:
            return idx[()]
        else:
            return idx.view(np.ndarray)

    def slice_during(self, e):
        """ Returns the slice that corresponds to Epoch e"""

        if not isinstance(e, Epochs):
            raise ValueError('e has to be of Epochs type')

        if e.data.ndim > 0:
            raise NotImplementedError('e has to be a scalar Epoch')

        if self.ndim != 1:
            e_s = 'slicing only implemented for 1-d TimeArrays'
            return NotImplementedError(e_s)
        i_start = self.index_at(e.start)
        i_stop = self.index_at(e.stop)
        if e.start > self[i_start]:  # make sure self[i_start] is in epoch e
            i_start += 1
        if e.stop > self[i_stop]:  # make sure to include self[i_stop]
            i_stop += 1

        return slice(i_start, i_stop)

    def at(self, t):
        """ Returns the values of the UniformTime object at time t"""
        return TimeArray(self[self.index_at(t)], time_unit=self.time_unit)

    def during(self, e):
        """ Returns the values of the UniformTime object during Epoch e"""

        if not isinstance(e, Epochs):
            raise ValueError('e has to be of Epochs type')

        if e.data.ndim > 0:
            raise NotImplementedError('e has to be a scalar Epoch')

        return self[self.slice_during(e)]

    def min(self, axis=None, out=None):
        """Returns the minimal time"""
        # this is a quick fix to return a time and will
        # be obsolete once we use proper time dtypes
        if axis is not None:
            raise NotImplementedError('axis argument not implemented')
        if out is not None:
            raise NotImplementedError('out argument not implemented')
        if self.ndim:
            return self[self.argmin()]
        else:
            return self

    def max(self, axis=None, out=None):
        """Returns the maximal time"""
        # this is a quick fix to return a time and will
        # be obsolete once we use proper time dtypes
        if axis is not None:
            raise NotImplementedError('axis argument not implemented')
        if out is not None:
            raise NotImplementedError('out argument not implemented')
        if self.ndim:
            return self[self.argmax()]
        else:
            return self

    def __div__(self, d):
        """Division by another time object eliminates units """
        if isinstance(d, TimeInterface):
            return np.divide(np.array(self), np.array(d).astype(float))
        else:
            return np.divide(self, d)

    __truediv__ =  __div__ # for py3k

##Frequency:

class Frequency(float):
    """A class for representation of the frequency (in Hz) """

    def __new__(cls, f, time_unit='s'):
        """Initialize a frequency object """

        tuc = time_unit_conversion
        scale_factor = (float(tuc['s']) / tuc[time_unit])
        #If the input is a Frequency object, it is already in Hz:
        if isinstance(f, Frequency) == False:
            #But otherwise convert to Hz:
            f = f * scale_factor

        freq = super(Frequency, cls).__new__(cls, f)
        freq._time_unit = time_unit

        return freq

    def __repr__(self):

        return str(self) + ' Hz'

    def to_period(self, time_unit=base_unit):
        """Convert the value of a frequency to the corresponding period
        (defaulting to a representation in the base_unit)

        """
        tuc = time_unit_conversion
        scale_factor = (float(tuc['s']) / tuc[time_unit])

        return np.int64((1 / self) * scale_factor)


##Time-series:
class TimeSeriesInterface(TimeInterface):
    """The minimally agreed upon interface for all time series.

    This should be thought of as an abstract base class.
    """
    time = None
    data = None
    metadata = None


class TimeSeriesBase(object):
    """Base class for time series, implementing the TimeSeriesInterface."""

    def __init__(self, data, time_unit, metadata=None):
        """Common constructor shared by all TimeSeries classes."""
        # Check that sensible time units were given
        if time_unit not in time_unit_conversion:
            raise ValueError('Invalid time unit %s, must be one of %s' %
                             (time_unit, time_unit_conversion.keys()))

        #: the data is an arbitrary numpy array
        self.data = np.asanyarray(data)
        self.time_unit = time_unit

        # Every instance carries an empty metadata dict, which we promise never
        # to touch.  This reserves this name as a user area for extra
        # information without the danger of name clashes in the future.
        if metadata is None:
            self.metadata = {}
        else:
            self.metadata = metadata

    def __len__(self):
        """Return the length of the time series."""
        return self.data.shape[-1]

    def _validate_dimensionality(self):
        """Check that the data and time have the proper dimensions.
        """

        if self.time.ndim != 1:
            raise ValueError("time array must be one-dimensional")
        npoints = self.data.shape[-1]
        if npoints != len(self.time):
            raise ValueError("mismatch of time and data dimensions")

    def __getitem__(self, key):
        """use fancy time-indexing (at() method)."""
        if isinstance(key, TimeInterface):
            return self.at(key)
        elif isinstance(key, Epochs):
            return self.during(key)
        elif self.data.ndim == 1:
            return self.data[key]  # time is the last dimension
        else:
            return self.data[..., key]  # time is the last dimension

    def __repr__(self):
        rep = self.__class__.__name__ + ":"
        return rep + self.time.__repr__() + self.data.T.__repr__()

    # add some methods that implement arithmetic on the timeseries data
    def __add__(self, other):
        out = self.copy()
        out.data = out.data.__add__(other)
        return out

    def __sub__(self, other):
        out = self.copy()
        out.data = out.data.__sub__(other)
        return out

    def __mul__(self, other):
        out = self.copy()
        out.data = out.data.__mul__(other)
        return out

    def __div__(self, other):
        out = self.copy()
        out.data = out.data.__div__(other)
        return out
    
    __truediv__ =  __div__ # for py3k

    def __iadd__(self, other):
        self.data.__iadd__(other)
        return self

    def __isub__(self, other):
        self.data.__isub__(other)
        return self

    def __imul__(self, other):
        self.data.__imul__(other)
        return self

    def __idiv__(self, other):
        self.data.__itruediv__(other)
        return self

    __itruediv__ =  __idiv__ # for py3k

class TimeSeries(TimeSeriesBase):
    """Represent data collected at uniform intervals.
    """

    @desc.setattr_on_read
    def time(self):
        """Construct time array for the time-series object. This holds a
    UniformTime object, with properties derived from the TimeSeries
    object"""
        return UniformTime(length=self.__len__(), t0=self.t0,
                           sampling_interval=self.sampling_interval,
                           time_unit=self.time_unit)

    #XXX This should call the constructor in an appropriate way, when provided
    #with a UniformTime object and data, so that you don't need to deal with
    #the constructor itself:
    @staticmethod
    def from_time_and_data(time, data):
        return TimeSeries.__init__(data, time=time)

    def copy(self):
        return TimeSeries(data=self.data.copy(),
                          time=self.time.copy(),
                          time_unit=self.time_unit,
                          metadata=self.metadata.copy())

    def __init__(self, data, t0=None, sampling_interval=None,
                 sampling_rate=None, duration=None, time=None, time_unit='s',
                 metadata=None):
        """Create a new TimeSeries.

        This class assumes that data is uniformly sampled, but you can specify
        the sampling in one of three (mutually exclusive) ways:

        - sampling_interval [, t0]: data sampled starting at t0, equal
          intervals of sampling_interval.

        - sampling_rate [, t0]: data sampled starting at t0, equal intervals of
          width 1/sampling_rate.

        - time: a UniformTime object, in which case the TimeSeries can
          'inherit' the properties of this object.

        Parameters
        ----------
        data : array_like
          Data array, interpreted as having its last dimension being time.
        sampling_interval : float
          Interval between successive time points.
        sampling_rate : float
          Inverse of the interval between successive time points.
        t0 : float
          If you provide a sampling rate, you can optionally also provide a
          starting time.
        time 
          Instead of sampling rate, you can explicitly provide an object of
          class UniformTime. Note that you can still also provide a different
          sampling_rate/sampling_interval/duration to take the place of the
          one in this object, but only as long as the changes are consistent
          with the length of the data.

        time_unit :  string
          The unit of time.

        Examples
        --------

        The minimal specification of data and sampling interval:

        >>> ts = TimeSeries([1,2,3],sampling_interval=0.25)
        >>> ts.time
        UniformTime([ 0.  ,  0.25,  0.5 ], time_unit='s')
        >>> ts.t0
        0.0 s
        >>> ts.sampling_rate
        4.0 Hz

        Or data and sampling rate:
        >>> ts = TimeSeries([1,2,3],sampling_rate=2)
        >>> ts.time
        UniformTime([ 0. ,  0.5,  1. ], time_unit='s')
        >>> ts.t0
        0.0 s
        >>> ts.sampling_interval
        0.5 s

        A time series where we specify the start time and sampling interval:
        >>> ts = TimeSeries([1,2,3],t0=4.25,sampling_interval=0.5)
        >>> ts.data
        array([1, 2, 3])
        >>> ts.time
        UniformTime([ 4.25,  4.75,  5.25], time_unit='s')
        >>> ts.t0
        4.25 s
        >>> ts.sampling_interval
        0.5 s
        >>> ts.sampling_rate
        2.0 Hz

        >>> ts = TimeSeries([1,2,3],t0=4.25,sampling_rate=2.0)
        >>> ts.data
        array([1, 2, 3])
        >>> ts.time
        UniformTime([ 4.25,  4.75,  5.25], time_unit='s')
        >>> ts.t0
        4.25 s
        >>> ts.sampling_interval
        0.5 s
        >>> ts.sampling_rate
        2.0 Hz

        """

        #If a UniformTime object was provided as input:
        if isinstance(time, UniformTime):
            c_fac = time._conversion_factor
            #If the user did not provide an alternative t0, get that from the
            #input:
            if t0 is None:
                t0 = time.t0
            #If the user did not provide an alternative sampling interval/rate:
            if sampling_interval is None and sampling_rate is None:
                sampling_interval = time.sampling_interval
                sampling_rate = time.sampling_rate
            #The duration can be read either from the length of the data, or
            #from the duration specified by the time-series:
            if duration is None:
                duration = time.duration
                length = time.shape[-1]
                #If changing the duration requires a change to the
                #sampling_rate, make sure that this was explicitely required by
                #the user - if the user did not explicitely set the
                #sampling_rate, or it is inconsistent, throw an error:
                data_len = np.array(data).shape[-1]

                if (length != data_len and
                    sampling_rate != float(data_len * c_fac) / time.duration):
                    e_s = "Length of the data (%s) " % str(len(data))
                    e_s += "specified sampling_rate (%s) " % str(sampling_rate)
                    e_s += "do not match."
                    raise ValueError(e_s)
            #If user does not provide a
            if time_unit is None:
                time_unit = time.time_unit

        else:
            ##If the input was not a UniformTime, we need to check that there
            ##is enough information in the input to generate the UniformTime
            ##array.

            #There are different valid combinations of inputs
            tspec = tuple(x is not None for x in
                      [sampling_interval, sampling_rate, duration])

            tspec_arg_names = ["sampling_interval",
                               "sampling_rate",
                               "duration"]

            #The valid configurations
            valid_tspecs = [
                      #interval, length:
                      (True, False, False),
                      #interval, duration:
                      (True, False, True),
                      #rate, length:
                      (False, True, False),
                      #rate, duration:
                      (False, True, True),
                      #length, duration:
                      (False, False, True)
                      ]

            if tspec not in valid_tspecs:
                raise ValueError("Invalid time specification. \n"
                      "You provided: %s\n %s see docstring for more info." % (
                            str_tspec(tspec, tspec_arg_names),
                            str_valid_tspecs(valid_tspecs, tspec_arg_names)))

        # Make sure to grab the time unit from the inputs, if it is provided:
        if time_unit is None:
            # If you gave us a duration with time_unit attached
            if isinstance(duration, TimeInterface):
                time_unit = duration.time_unit
            # Otherwise, you might have given us a sampling_interval with a
            # time_unit attached:
            elif isinstance(sampling_interval, TimeInterface):
                time_unit = sampling_interval.time_unit

        # Calculate the sampling_interval or sampling_rate from each other and
        # assign t0, if it is not already assigned:
        if sampling_interval is None:
            if isinstance(sampling_rate, Frequency):
                c_f = time_unit_conversion[time_unit]
                sampling_interval = sampling_rate.to_period() / float(c_f)
            elif sampling_rate is None:
                data_len = np.asarray(data).shape[-1]
                sampling_interval = float(duration) / data_len
                sampling_rate = Frequency(1.0 / sampling_interval,
                                             time_unit=time_unit)
            else:
                c_f = time_unit_conversion[time_unit]
                sampling_rate = Frequency(sampling_rate, time_unit='s')
                sampling_interval = sampling_rate.to_period() / float(c_f)
        else:
            if sampling_rate is None:  # Only if you didn't already 'inherit'
                                       # this property from another time object
                                       # above:
                if isinstance(sampling_interval, TimeInterface):
                    c_f = time_unit_conversion[sampling_interval.time_unit]
                    sampling_rate = Frequency(1.0 / (float(sampling_interval) /
                                                                         c_f),
                                       time_unit=sampling_interval.time_unit)
                else:
                    sampling_rate = Frequency(1.0 / sampling_interval,
                                              time_unit=time_unit)

        #Calculate the duration, if that is not defined:
        if duration is None:
            duration = np.asarray(data).shape[-1] * sampling_interval

        if t0 is None:
            t0 = 0

        # Make sure to grab the time unit from the inputs, if it is provided:
        if time_unit is None:
            #If you gave us a duration with time_unit attached
            if isinstance(duration, TimeInterface):
                time_unit = duration.time_unit
            #Otherwise, you might have given us a sampling_interval with a
            #time_unit attached:
            elif isinstance(sampling_interval, TimeInterface):
                time_unit = sampling_interval.time_unit

        #Otherwise, you can still call the common constructor to get the real
        #object initialized, with time_unit set to None and that will generate
        #the object with time_unit set to 's':
        TimeSeriesBase.__init__(self, data, time_unit, metadata=metadata)

        self.time_unit = time_unit
        self.sampling_interval = TimeArray(sampling_interval,
                                           time_unit=self.time_unit)
        self.t0 = TimeArray(t0, time_unit=self.time_unit)
        self.sampling_rate = sampling_rate
        self.duration = TimeArray(duration, time_unit=self.time_unit)

    def at(self, t, tol=None):
        """ Returns the values of the TimeArray object at time t"""
        return self.data[..., self.time.index_at(t)]

    def during(self, e):
        """ Returns the TimeSeries slice corresponding to epoch e """

        if not isinstance(e, Epochs):
            raise ValueError('e has to be of Epochs type')

        if e.data.ndim == 0:
            return TimeSeries(data=self.data[..., self.time.slice_during(e)],
                              time_unit=self.time_unit, t0=e.offset,
                              sampling_rate=self.sampling_rate)
        else:
            # TODO: make this a more efficient implementation, naive first pass
            if (e.duration != e.duration[0]).any():
                raise ValueError("All epochs must have the same duration")

            data = np.array([self.data[..., self.time.slice_during(ep)]
                             for ep in e])

            return TimeSeries(data=data,
                              time_unit=self.time_unit, t0=e.offset,
                              sampling_rate=self.sampling_rate)

    @property
    def shape(self):
        return self.data.shape


_epochtype = np.dtype({'names': ['start', 'stop'], 'formats': [np.int64] * 2})


class Epochs(desc.ResetMixin):
    """Represents a time interval"""
    def __init__(self, t0=None, stop=None, offset=None, start=None,
                 duration=None, time_unit=None, static=None, **kwargs):
        """
        Parameters
        ----------
        t0 : 1-d array or `TimeArray`
           A time relative to which the epochs started. Per default `t0` and
          `start` are the same, but setting the `offset` parameter can adjust
           that, so that the start-times are at a fixed time, relative to t0.

        stop : 1-d array or `TimeArray`
              The times of ends of epochs

        offset : float, int or singleton `TimeArray`
            A constant offset applied to t0 to set the starts of Epochs

        start : 1-d array or `TimeArray`
              The times of beginnings of epochs

        duration : 1-d array or `TimeArray`
           The durations of intervals.

        time_unit : str, optional
              The time unit of the object and all time-related things in it.
              Default: 's'

        static : dict, optional
            For fast initialization of an `Epochs` object from another `Epochs`
            object, this dict should contain all necessary items to have an
            `Epoch` defined.

        """
        # Short-circuit path for a fast initialization. This relies on `static`
        # to be a dict that contains everything that defines an Epochs class
        # XXX: add this sort of fast __init__ to all other classes
        if static is not None:
            self.__dict__.update(static)
            # we have to reset the duration OneTimeProperty, since it refers
            # to computations performed on the former object
            self.reset()
            return

        if t0 is None and start is None:
            raise ValueError('Either start or t0 need to be specified')
        # Normal, error checking and type converting initialization logic

        if stop is None and duration is None:
            raise ValueError('Either stop or duration have to be specified')

        if stop is not None and duration is not None:
            ### TODO: check if stop and duration are consistent
            e_s = 'Only either stop or duration have to be specified'
            raise ValueError(e_s)

        if offset is None:
            offset = 0

        t_offset = TimeArray(offset, time_unit=time_unit)

        if t_offset.ndim > 0:
            raise ValueError('Only scalar offset allowed')

        if t0 is None:
            t_0 = 0
        else:
            t_0 = TimeArray(t0, time_unit=time_unit)

        if start is None:
            t_start = t_0 - t_offset
        else:
            t_start = TimeArray(start, time_unit=time_unit)

        # inherit time_unit of t_start
        self.time_unit = t_start.time_unit

        if stop is None:
            t_duration = TimeArray(duration, time_unit=time_unit)
            t_stop = t_start + t_duration
        else:
            t_stop = TimeArray(stop, time_unit=time_unit)

        if t_start.shape != t_stop.shape:
            raise ValueError('start and stop have to have same shape')

        if t_start.ndim == 0:
            # return a 'scalar' epoch
            self.data = np.empty(1, dtype=_epochtype).reshape(())
        elif t_start.ndim == 1:
            # return a 1-d epoch array
            self.data = np.empty(t_start.shape[0], dtype=_epochtype)
        else:
            e_s = 'Only 0-dim and 1-dim start and stop times allowed'
            raise ValueError(e_s)

        self.data['start'] = t_start
        self.data['stop'] = t_stop

        self.offset = t_offset

    # TODO: define setters for start, stop, offset attributes
    @property
    def start(self):
        return TimeArray(self.data['start'],
                         time_unit=self.time_unit,
                         copy=False)

    @property
    def stop(self):
        return TimeArray(self.data['stop'],
                         time_unit=self.time_unit,
                         copy=False)

    @desc.setattr_on_read
    def duration(self):
        """Duration array for the epoch"""
        return self.stop - self.start

    def __getitem__(self, key):
        # create the static dict needed for fast version of __init__
        static = self.__dict__.copy()
        static['data'] = self.data[key]
        # self.__class__ here is Epochs or a subclass of Epochs
        # and `start` is a required argument
        return self.__class__(start=None, static=static)

    def __repr__(self):
        if self.data.ndim == 0:
            z = (self.start, self.stop)
        else:
            z = list(zip(self.start, self.stop))
        rep = self.__class__.__name__ + "(" + z.__repr__()
        return rep + ", as (start,stop) tuples)"

    def __len__(self):
        return len(self.data)


def str_tspec(tspec, arg_names):
    """ Turn a single tspec into human readable form"""
    # an all "False" will convert to an empty string unless we do the following
    # where we create an all False tuple of the appropriate length
    if tspec == tuple([False] * len(arg_names)):
        return "(nothing)"
    return ", ".join([arg for t, arg in zip(tspec, arg_names) if t])


def str_valid_tspecs(valid_tspecs, arg_names):
    """Given a set of valid_tspecs, return a string that turns them into
    human-readable form"""
    vargs = []
    for tsp in valid_tspecs:
        vargs.append(str_tspec(tsp, arg_names))
    return "\n Valid time specifications are:\n\t%s" % ("\n\t".join(vargs))


def concatenate_time_series(time_series_seq):
    """Concatenates a sequence of time-series objects in time.

    The input can be any iterable of time-series objects; metadata, sampling
    rates and other attributes are kept from the last one in the sequence.

    This one requires that all the time-series in the list have the same
    sampling rate and that all the data have the same number of items in all
    dimensions, except the time dimension"""

    # Extract the data pointer for each and build a common data block
    data = []
    metadata = {}
    for ts in time_series_seq:
        data.append(ts.data)
        metadata.update(ts.metadata)

    # Sampling interval is read from the last one
    tseries = TimeSeries(np.concatenate(data,-1),
                                sampling_interval=ts.sampling_interval,
                                metadata=metadata)
    return tseries


class Events(TimeInterface):
    """Represents timestamps and associated data """

    def __init__(self, time, labels=None, indices=None,
                 time_unit=None, **data):
        """
        Parameters
        ----------
        time : array or TimeArray
            The times at which events occured

        labels : array, optional

        indices : int array, optional


        Notes
        -----


        """
        # The time data must be at least a 1-d array, NOT a time scalar
        if not np.iterable(time):
            time = [time]

        # First initilaize the TimeArray from the time-stamps
        self.time = TimeArray(time, time_unit=time_unit)
        self.time_unit = self.time.time_unit

        # Make sure time is one-dimensional
        if self.time.ndim != 1:
            e_s = 'The TimeArray provided can only be one-dimensional'
            raise ValueError(e_s)
        # Ensure that the dict of data values has a known, uniform structure:
        # all values must be arrays, with at least one dimension.
        new_data = {}
        for k, v in six.iteritems(data):
            if np.iterable(v):
                v = np.asanyarray(v)
            else:
                # For scalars, we do NOT want to create 0-d arrays, which are
                # rather tricky to work with.  So if the input value is not an
                # iterable object, we turn it into a one-element 1-d array.
                v = np.array([v])
            new_data[k] = v

        # Make sure all data has same length
        ntimepts = len(self.time)
        for check_v in new_data.values():
            if len(check_v) != ntimepts:
                e_s = 'All data in the Events must be of the same'
                e_s += 'length as the associated time'
                raise ValueError(e_s)

        # Make sure indices have same length and are integers
        if labels is not None:
            if len(labels) != len(indices):
                e_s = 'Labels and indices must have the same length'
                raise ValueError(e_s)
            dt = [(l, np.int64) for l in labels]
        else:
            dt = np.int64
            dt = [('i%d' % i, np.int64)
                  for i in range(len(indices or ()))] or np.int64

        self.index = np.array(list(zip(*(indices or ()))),
                                       dtype=dt).view(np.recarray)

        #Should data be a recarray?
##         dt = [(st,np.array(data[st]).dtype) for st in data] or None
##         self.data = np.array(zip(*data.values()),
##         dtype=dt).view(np.recarray)

        #Or a dict?
        self.data = new_data

    def __repr__(self):
        rep = self.__class__.__name__ + ":\n\t"
        rep += repr(self.time) + "\n\t"
        rep += repr(self.data)
        return rep

    def __getitem__(self, key):
        # return scalar TimeArray in case key is integer
        newdata = dict()
        newtime = self.time[key].reshape(-1)
        sl = key
        if isinstance(key, float):
            sl = self.time.index_at(key)
        elif isinstance(key, Epochs):
            sl = self.time.slice_during(key)
        for k, v in self.data.items():
            newdata[k] = v[sl]

        # XXX: I don't really understand how labels and index are supposed to
        # be used, so I'm not implementing them when slicing events - pi
        # 2010-12-04

        # self.__class__ here is Events or a subclass of Events
        return self.__class__(newtime, **newdata)

    def __len__(self):
        return len(self.time)

########NEW FILE########
__FILENAME__ = utils
"""Miscellaneous utilities for time series analysis.

XXX write top level doc-string

"""
from __future__ import print_function
import warnings
import numpy as np
from nitime.lazy import scipy_linalg as linalg
from nitime.lazy import scipy_signal as sig
from nitime.lazy import scipy_fftpack as fftpack
from nitime.lazy import scipy_signal_signaltools as signaltools


#-----------------------------------------------------------------------------
# Spectral estimation testing utilities
#-----------------------------------------------------------------------------
def square_window_spectrum(N, Fs):
    r"""
    Calculate the analytical spectrum of a square window

    Parameters
    ----------
    N : int
       the size of the window

    Fs : float
       The sampling rate

    Returns
    -------
    float array - the frequency bands, given N and FS
    complex array: the power in the spectrum of the square window in the
    frequency bands

    Notes
    -----
    This is equation 21c in Harris (1978):

    .. math::

      W(\theta) = exp(-j \frac{N-1}{2} \theta) \frac{sin \frac{N\theta}{2}} {sin\frac{\theta}{2}}

    F.J. Harris (1978). On the use of windows for harmonic analysis with the
    discrete Fourier transform. Proceedings of the IEEE, 66:51-83
    """
    f = get_freqs(Fs, N - 1)
    j = 0 + 1j
    a = -j * (N - 1) * f / 2
    b = np.sin(N * f / 2.0)
    c = np.sin(f / 2.0)
    make = np.exp(a) * b / c

    return f,  make[1:] / make[1]


def hanning_window_spectrum(N, Fs):
    r"""
    Calculate the analytical spectrum of a Hanning window

    Parameters
    ----------
    N : int
       The size of the window

    Fs : float
       The sampling rate

    Returns
    -------
    float array - the frequency bands, given N and FS
    complex array: the power in the spectrum of the square window in the
    frequency bands

    Notes
    -----
    This is equation 28b in Harris (1978):

    .. math::

      W(\theta) = 0.5 D(\theta) + 0.25 (D(\theta - \frac{2\pi}{N}) +
                D(\theta + \frac{2\pi}{N}) ),

    where:

    .. math::

      D(\theta) = exp(j\frac{\theta}{2})
                  \frac{sin\frac{N\theta}{2}}{sin\frac{\theta}{2}}

    F.J. Harris (1978). On the use of windows for harmonic analysis with the
    discrete Fourier transform. Proceedings of the IEEE, 66:51-83
    """
    #A helper function
    D = lambda theta, n: (
        np.exp((0 + 1j) * theta / 2) * ((np.sin(n * theta / 2)) / (theta / 2)))

    f = get_freqs(Fs, N)

    make = 0.5 * D(f, N) + 0.25 * (D((f - (2 * np.pi / N)), N) +
                                   D((f + (2 * np.pi / N)), N))
    return f, make[1:] / make[1]


def ar_generator(N=512, sigma=1., coefs=None, drop_transients=0, v=None):
    """
    This generates a signal u(n) = a1*u(n-1) + a2*u(n-2) + ... + v(n)
    where v(n) is a stationary stochastic process with zero mean
    and variance = sigma. XXX: confusing variance notation

    Parameters
    ----------

    N : int
      sequence length
    sigma : float
      power of the white noise driving process
    coefs : sequence
      AR coefficients for k = 1, 2, ..., P
    drop_transients : int
      number of initial IIR filter transient terms to drop
    v : ndarray
      custom noise process

    Parameters
    ----------

    N : float
       The number of points in the AR process generated. Default: 512
    sigma : float
       The variance of the noise in the AR process. Default: 1
    coefs : list or array of floats
       The AR model coefficients. Default: [2.7607, -3.8106, 2.6535, -0.9238],
       which is a sequence shown to be well-estimated by an order 8 AR system.
    drop_transients : float
       How many samples to drop from the beginning of the sequence (the
       transient phases of the process), so that the process can be considered
       stationary.
    v : float array
       Optionally, input a specific sequence of noise samples (this over-rides
       the sigma parameter). Default: None

    Returns
    -------

    u : ndarray
       the AR sequence
    v : ndarray
       the unit-variance innovations sequence
    coefs : ndarray
       feedback coefficients from k=1,len(coefs)

    The form of the feedback coefficients is a little different than
    the normal linear constant-coefficient difference equation. Therefore
    the transfer function implemented in this method is

    H(z) = sigma**0.5 / ( 1 - sum_k coefs(k)z**(-k) )    1 <= k <= P

    Examples
    --------

    >>> import nitime.algorithms as alg
    >>> ar_seq, nz, alpha = ar_generator()
    >>> fgrid, hz = alg.freq_response(1.0, a=np.r_[1, -alpha])
    >>> sdf_ar = (hz * hz.conj()).real

    """
    if coefs is None:
        # this sequence is shown to be estimated well by an order 8 AR system
        coefs = np.array([2.7607, -3.8106, 2.6535, -0.9238])
    else:
        coefs = np.asarray(coefs)

    # The number of terms we generate must include the dropped transients, and
    # then at the end we cut those out of the returned array.
    N += drop_transients

    # Typically uses just pass sigma in, but optionally they can provide their
    # own noise vector, case in which we use it
    if v is None:
        v = np.random.normal(size=N)
        v -= v[drop_transients:].mean()

    b = [sigma ** 0.5]
    a = np.r_[1, -coefs]
    u = sig.lfilter(b, a, v)

    # Only return the data after the drop_transients terms
    return u[drop_transients:], v[drop_transients:], coefs


def circularize(x, bottom=0, top=2 * np.pi, deg=False):
    """Maps the input into the continuous interval (bottom, top) where
    bottom defaults to 0 and top defaults to 2*pi

    Parameters
    ----------

    x : ndarray - the input array

    bottom : float, optional (defaults to 0).
        If you want to set the bottom of the interval into which you
        modulu to something else than 0.

    top : float, optional (defaults to 2*pi).
        If you want to set the top of the interval into which you
        modulu to something else than 2*pi

    Returns
    -------
    The input array, mapped into the interval (bottom,top)

    """
    x = np.asarray([x])

    if  (np.all(x[np.isfinite(x)] >= bottom) and
         np.all(x[np.isfinite(x)] <= top)):
        return np.squeeze(x)
    else:
        x[np.where(x < 0)] += top
        x[np.where(x > top)] -= top

    return np.squeeze(circularize(x, bottom=bottom, top=top))


def dB(x, power=True):
    """Convert the values in x to decibels.
    If the values in x are in 'power'-like units, then set the power
    flag accordingly

    1) dB(x) = 10log10(x)                     (if power==True)
    2) dB(x) = 10log10(|x|^2) = 20log10(|x|)  (if power==False)
    """
    if not power:
        return 20 * np.log10(np.abs(x))
    return 10 * np.log10(np.abs(x))


#-----------------------------------------------------------------------------
# Stats utils
#-----------------------------------------------------------------------------

def normalize_coherence(x, dof, copy=True):
    """
    The generally accepted choice to transform coherence measures into
    a more normal distribution

    Parameters
    ----------
    x : ndarray, real
       square-root of magnitude-square coherence measures
    dof : int
       number of degrees of freedom in the multitaper model
    copy : bool
        Copy or return inplace modified x.

    Returns
    -------
    y : ndarray, real
        The transformed array.
    """
    if copy:
        x = x.copy()
    np.arctanh(x, x)
    x *= np.sqrt(dof)
    return x


def normal_coherence_to_unit(y, dof, out=None):
    """
    The inverse transform of the above normalization
    """
    if out is None:
        x = y / np.sqrt(dof)
    else:
        y /= np.sqrt(dof)
        x = y
    np.tanh(x, x)
    return x


def expected_jk_variance(K):
    """Compute the expected value of the jackknife variance estimate
    over K windows below. This expected value formula is based on the
    asymptotic expansion of the trigamma function derived in
    [Thompson_1994]

    Paramters
    ---------

    K : int
      Number of tapers used in the multitaper method

    Returns
    -------

    evar : float
      Expected value of the jackknife variance estimator

    """

    kf = float(K)
    return ((1 / kf) * (kf - 1) / (kf - 0.5) *
            ((kf - 1) / (kf - 2)) ** 2 * (kf - 3) / (kf - 2))


def jackknifed_sdf_variance(yk, eigvals, sides='onesided', adaptive=True):
    r"""
    Returns the variance of the log-sdf estimated through jack-knifing
    a group of independent sdf estimates.

    Parameters
    ----------

    yk : ndarray (K, L)
       The K DFTs of the tapered sequences
    eigvals : ndarray (K,)
       The eigenvalues corresponding to the K DPSS tapers
    sides : str, optional
       Compute the jackknife pseudovalues over as one-sided or
       two-sided spectra
    adpative : bool, optional
       Compute the adaptive weighting for each jackknife pseudovalue

    Returns
    -------

    var : The estimate for log-sdf variance

    Notes
    -----

    The jackknifed mean estimate is distributed about the true mean as
    a Student's t-distribution with (K-1) degrees of freedom, and
    standard error equal to sqrt(var). However, Thompson and Chave [1]
    point out that this variance better describes the sample mean.


    [1] Thomson D J, Chave A D (1991) Advances in Spectrum Analysis and Array
    Processing (Prentice-Hall, Englewood Cliffs, NJ), 1, pp 58-113.
    """
    K = yk.shape[0]

    from nitime.algorithms import mtm_cross_spectrum

    # the samples {S_k} are defined, with or without weights, as
    # S_k = | x_k |**2
    # | x_k |**2 = | y_k * d_k |**2          (with adaptive weights)
    # | x_k |**2 = | y_k * sqrt(eig_k) |**2  (without adaptive weights)

    all_orders = set(range(K))
    jk_sdf = []
    # get the leave-one-out estimates -- ideally, weights are recomputed
    # for each leave-one-out. This is now the case.
    for i in range(K):
        items = list(all_orders.difference([i]))
        spectra_i = np.take(yk, items, axis=0)
        eigs_i = np.take(eigvals, items)
        if adaptive:
            # compute the weights
            weights, _ = adaptive_weights(spectra_i, eigs_i, sides=sides)
        else:
            weights = eigs_i[:, None]
        # this is the leave-one-out estimate of the sdf
        jk_sdf.append(
            mtm_cross_spectrum(
                spectra_i, spectra_i, weights, sides=sides
                )
            )
    # log-transform the leave-one-out estimates and the mean of estimates
    jk_sdf = np.log(jk_sdf)
    # jk_avg should be the mean of the log(jk_sdf(i))
    jk_avg = jk_sdf.mean(axis=0)

    K = float(K)

    jk_var = (jk_sdf - jk_avg)
    np.power(jk_var, 2, jk_var)
    jk_var = jk_var.sum(axis=0)

    # Thompson's recommended factor, eq 18
    # Jackknifing Multitaper Spectrum Estimates
    # IEEE SIGNAL PROCESSING MAGAZINE [20] JULY 2007
    f = (K - 1) ** 2 / K / (K - 0.5)
    jk_var *= f
    return jk_var


def jackknifed_coh_variance(tx, ty, eigvals, adaptive=True):
    """
    Returns the variance of the coherency between x and y, estimated
    through jack-knifing the tapered samples in {tx, ty}.

    Parameters
    ----------

    tx : ndarray, (K, L)
       The K complex spectra of tapered timeseries x
    ty : ndarray, (K, L)
       The K complex spectra of tapered timeseries y
    eigvals : ndarray (K,)
       The eigenvalues associated with the K DPSS tapers

    Returns
    -------

    jk_var : ndarray
       The variance computed in the transformed domain (see
       normalize_coherence)
    """

    K = tx.shape[0]

    # calculate leave-one-out estimates of MSC (magnitude squared coherence)
    jk_coh = []
    # coherence is symmetric (right??)
    sides = 'onesided'
    all_orders = set(range(K))

    import nitime.algorithms as alg

    # get the leave-one-out estimates
    for i in range(K):
        items = list(all_orders.difference([i]))
        tx_i = np.take(tx, items, axis=0)
        ty_i = np.take(ty, items, axis=0)
        eigs_i = np.take(eigvals, items)
        if adaptive:
            wx, _ = adaptive_weights(tx_i, eigs_i, sides=sides)
            wy, _ = adaptive_weights(ty_i, eigs_i, sides=sides)
        else:
            wx = wy = eigs_i[:, None]
        # The CSD
        sxy_i = alg.mtm_cross_spectrum(tx_i, ty_i, (wx, wy), sides=sides)
        # The PSDs
        sxx_i = alg.mtm_cross_spectrum(tx_i, tx_i, wx, sides=sides)
        syy_i = alg.mtm_cross_spectrum(ty_i, ty_i, wy, sides=sides)
        # these are the | c_i | samples
        msc = np.abs(sxy_i)
        msc /= np.sqrt(sxx_i * syy_i)
        jk_coh.append(msc)

    jk_coh = np.array(jk_coh)
    # now normalize the coherence estimates and take the mean
    normalize_coherence(jk_coh, 2 * K - 2, copy=False)  # inplace
    jk_avg = np.mean(jk_coh, axis=0)

    jk_var = (jk_coh - jk_avg)
    np.power(jk_var, 2, jk_var)
    jk_var = jk_var.sum(axis=0)

    # Do/Don't use the alternative scaling here??
    f = float(K - 1) / K

    jk_var *= f

    return jk_var


#-----------------------------------------------------------------------------
# Multitaper utils
#-----------------------------------------------------------------------------
def adaptive_weights(yk, eigvals, sides='onesided', max_iter=150):
    r"""
    Perform an iterative procedure to find the optimal weights for K
    direct spectral estimators of DPSS tapered signals.

    Parameters
    ----------

    yk : ndarray (K, N)
       The K DFTs of the tapered sequences
    eigvals : ndarray, length-K
       The eigenvalues of the DPSS tapers
    sides : str
       Whether to compute weights on a one-sided or two-sided spectrum
    max_iter : int
       Maximum number of iterations for weight computation

    Returns
    -------

    weights, nu

       The weights (array like sdfs), and the
       "equivalent degrees of freedom" (array length-L)

    Notes
    -----

    The weights to use for making the multitaper estimate, such that
    :math:`S_{mt} = \sum_{k} |w_k|^2S_k^{mt} / \sum_{k} |w_k|^2`

    If there are less than 3 tapers, then the adaptive weights are not
    found. The square root of the eigenvalues are returned as weights,
    and the degrees of freedom are 2*K

    """
    from nitime.algorithms import mtm_cross_spectrum
    K = len(eigvals)
    if len(eigvals) < 3:
        print("""
        Warning--not adaptively combining the spectral estimators
        due to a low number of tapers.
        """)
        # we'll hope this is a correct length for L
        N = yk.shape[-1]
        L = N / 2 + 1 if sides == 'onesided' else N
        return (np.multiply.outer(np.sqrt(eigvals), np.ones(L)), 2 * K)
    rt_eig = np.sqrt(eigvals)

    # combine the SDFs in the traditional way in order to estimate
    # the variance of the timeseries
    N = yk.shape[1]
    sdf = mtm_cross_spectrum(yk, yk, eigvals[:, None], sides=sides)
    L = sdf.shape[-1]
    var_est = np.sum(sdf, axis=-1) / N
    bband_sup = (1-eigvals)*var_est

    # The process is to iteratively switch solving for the following
    # two expressions:
    # (1) Adaptive Multitaper SDF:
    # S^{mt}(f) = [ sum |d_k(f)|^2 S_k(f) ]/ sum |d_k(f)|^2
    #
    # (2) Weights
    # d_k(f) = [sqrt(lam_k) S^{mt}(f)] / [lam_k S^{mt}(f) + E{B_k(f)}]
    #
    # Where lam_k are the eigenvalues corresponding to the DPSS tapers,
    # and the expected value of the broadband bias function
    # E{B_k(f)} is replaced by its full-band integration
    # (1/2pi) int_{-pi}^{pi} E{B_k(f)} = sig^2(1-lam_k)

    # start with an estimate from incomplete data--the first 2 tapers
    sdf_iter = mtm_cross_spectrum(yk[:2], yk[:2], eigvals[:2, None],
                                  sides=sides)
    err = np.zeros((K, L))
    # for numerical considerations, don't bother doing adaptive
    # weighting after 150 dB down
    min_pwr = sdf_iter.max() * 10 ** (-150/20.)
    default_weights = np.where(sdf_iter < min_pwr)[0]
    adaptiv_weights = np.where(sdf_iter >= min_pwr)[0]

    w_def = rt_eig[:,None] * sdf_iter[default_weights]
    w_def /= eigvals[:, None] * sdf_iter[default_weights] + bband_sup[:,None]

    d_sdfs = np.abs(yk[:,adaptiv_weights])**2
    if L < N:
        d_sdfs *= 2
    sdf_iter = sdf_iter[adaptiv_weights]
    yk = yk[:,adaptiv_weights]
    for n in range(max_iter):
        d_k = rt_eig[:,None] * sdf_iter[None, :]
        d_k /= eigvals[:, None]*sdf_iter[None, :] + bband_sup[:,None]
        # Test for convergence -- this is overly conservative, since
        # iteration only stops when all frequencies have converged.
        # A better approach is to iterate separately for each freq, but
        # that is a nonvectorized algorithm.
        #sdf_iter = mtm_cross_spectrum(yk, yk, d_k, sides=sides)
        sdf_iter = np.sum( d_k**2 * d_sdfs, axis=0 )
        sdf_iter /= np.sum( d_k**2, axis=0 )
        # Compute the cost function from eq 5.4 in Thomson 1982
        cfn = eigvals[:,None] * (sdf_iter[None,:] - d_sdfs)
        cfn /= (eigvals[:,None] * sdf_iter[None,:] + bband_sup[:,None])**2
        cfn = np.sum(cfn, axis=0)
        # there seem to be some pathological freqs sometimes ..
        # this should be a good heuristic
        if np.percentile(cfn**2, 95) < 1e-12:
            break
    else:  # If you have reached maximum number of iterations
        # Issue a warning and return non-converged weights:
        e_s = 'Breaking due to iterative meltdown in '
        e_s += 'nitime.utils.adaptive_weights.'
        warnings.warn(e_s, RuntimeWarning)
    weights = np.zeros( (K,L) )
    weights[:,adaptiv_weights] = d_k
    weights[:,default_weights] = w_def
    nu = 2 * (weights ** 2).sum(axis=-2)
    return weights, nu

def detect_lines(s, tapers, p=None, **taper_kws):
    """
    Detect the presence of line spectra in s using the F-test
    described in "Spectrum estimation and harmonic analysis" (Thompson 81).
    Strategies for detecting harmonics in low SNR include increasing the
    number of FFT points (NFFT keyword arg) and/or increasing the stability
    of the spectral estimate by using more tapers (higher NW parameter).

    s : ndarray
        The sequence(s) to test. If s.ndim > 1, then test sequences in
        the last axis in parallel

    tapers : ndarray or container
        Either the precomputed DPSS tapers, or the pair of parameters
        (NW, K) needed to compute K tapers of length n_pts.

    p : float
        The confidence threshold: under the null hypothesis of
        a locally white spectrum, there is a threshold such that
        there is a (1-p)% chance of a line amplitude being larger
        than that threshold. Only detect lines with amplitude greater
        than this threshold. The default is 1/N, to control for false
        positives.

    taper_kws
        Options for the tapered_spectra method, if no DPSS are provided.

    Returns
    -------

    (freq, beta) : sequence
        The frequencies (normalized in [0, .5]) and coefficients of the
        complex exponentials detected in the spectrum. A pair is returned
        for each sequence tested.

        One can reconstruct the line components as such:

        sn = 2*(beta[:,None]*np.exp(i*2*np.pi*np.arange(N)*freq[:,None])).real
        sn = sn.sum(axis=0)

    """
    from nitime.algorithms import tapered_spectra, dpss_windows
    import scipy.stats.distributions as dists
    import scipy.ndimage as ndimage
    N = s.shape[-1]
    # Some boiler-plate --
    # 1) set up tapers
    # 2) perform FFT on all windowed series
    if not isinstance(tapers, np.ndarray):
        # then tapers is (NW, K)
        args = (N,) + tuple(tapers)
        dpss, eigvals = dpss_windows(*args)
        if taper_kws.pop('low_bias', False):
            keepers = (eigvals > 0.9)
            dpss = dpss[keepers]
        tapers = dpss
    # spectra is (n_arr, K, nfft)
    spectra = tapered_spectra(s, tapers, **taper_kws)
    nfft = spectra.shape[-1]
    spectra = spectra[...,:nfft/2 + 1]

    # Set up some data for the following calculations --
    #   get the DC component of the taper spectra
    K = tapers.shape[0]
    U0 = tapers.sum(axis=1)
    U_sq = np.sum(U0**2)
    #  first order linear regression for mu to explain spectra
    mu = np.sum( U0[:,None] * spectra, axis=-2 ) / U_sq

    # numerator of F-stat -- strength of regression
    numr = 0.5 * np.abs(mu)**2 * U_sq
    numr[...,0] = 1; # don't care about DC
    # denominator -- strength of residual
    spectra = np.rollaxis(spectra, -2, 0)
    U0.shape = (K,) + (1,) * (spectra.ndim-1)
    denomr = spectra - U0*mu
    denomr = np.sum(np.abs(denomr)**2, axis=0) / (2*K-2)
    denomr[...,0] = 1;
    f_stat = numr / denomr

    # look for lines in each F-spectrum
    if not p:
        # the number of simultaneous tests are nfft/2, so this puts
        # the expected value for false detection somewhere less than 1
        p = 1.0/nfft
    #thresh = dists.f.isf(p, 2, 2*K-2)
    thresh = dists.f.isf(p, 2, K-1)
    f_stat = np.atleast_2d(f_stat)
    mu = np.atleast_2d(mu)
    lines = ()
    for fs, m in zip(f_stat, mu):
        detected = np.where(fs > thresh)[0]
        # do a quick pass through the detected lines to reject multiple
        # hits within the 2NW resolution of the MT analysis -- approximate
        # 2NW by K
        ddiff = np.diff(detected)
        flagged_groups, last_group = ndimage.label( (ddiff < K).astype('i') )
        for g in range(1,last_group+1):
            idx = np.where(flagged_groups==g)[0]
            idx = np.r_[idx, idx[-1]+1]
            # keep the super-threshold point with largest amplitude
            mx = np.argmax(np.abs(m[ detected[idx] ]))
            i_sv = detected[idx[mx]]
            detected[idx] = -1
            detected[idx[mx]] = i_sv
        detected = detected[detected>0]
        if len(detected):
            lines = lines + ( (detected/float(nfft), m[detected]), )
        else:
            lines = lines + ( (), )
    if len(lines) == 1:
        lines = lines[0]
    return lines


#-----------------------------------------------------------------------------
# Eigensystem utils
#-----------------------------------------------------------------------------

# If we can get it, we want the cythonized version
try:
    from _utils import tridisolve

# If that doesn't work, we define it here:
except ImportError:
    def tridisolve(d, e, b, overwrite_b=True):
        """
        Symmetric tridiagonal system solver,
        from Golub and Van Loan, Matrix Computations pg 157

        Parameters
        ----------

        d : ndarray
          main diagonal stored in d[:]
        e : ndarray
          superdiagonal stored in e[:-1]
        b : ndarray
          RHS vector

        Returns
        -------

        x : ndarray
          Solution to Ax = b (if overwrite_b is False). Otherwise solution is
          stored in previous RHS vector b

        """
        N = len(b)
        # work vectors
        dw = d.copy()
        ew = e.copy()
        if overwrite_b:
            x = b
        else:
            x = b.copy()
        for k in range(1, N):
            # e^(k-1) = e(k-1) / d(k-1)
            # d(k) = d(k) - e^(k-1)e(k-1) / d(k-1)
            t = ew[k - 1]
            ew[k - 1] = t / dw[k - 1]
            dw[k] = dw[k] - t * ew[k - 1]
        for k in range(1, N):
            x[k] = x[k] - ew[k - 1] * x[k - 1]
        x[N - 1] = x[N - 1] / dw[N - 1]
        for k in range(N - 2, -1, -1):
            x[k] = x[k] / dw[k] - ew[k] * x[k + 1]

        if not overwrite_b:
            return x


def tridi_inverse_iteration(d, e, w, x0=None, rtol=1e-8):
    """Perform an inverse iteration to find the eigenvector corresponding
    to the given eigenvalue in a symmetric tridiagonal system.

    Parameters
    ----------

    d : ndarray
      main diagonal of the tridiagonal system
    e : ndarray
      offdiagonal stored in e[:-1]
    w : float
      eigenvalue of the eigenvector
    x0 : ndarray
      initial point to start the iteration
    rtol : float
      tolerance for the norm of the difference of iterates

    Returns
    -------

    e : ndarray
      The converged eigenvector

    """
    eig_diag = d - w
    if x0 is None:
        x0 = np.random.randn(len(d))
    x_prev = np.zeros_like(x0)
    norm_x = np.linalg.norm(x0)
    # the eigenvector is unique up to sign change, so iterate
    # until || |x^(n)| - |x^(n-1)| ||^2 < rtol
    x0 /= norm_x
    while np.linalg.norm(np.abs(x0) - np.abs(x_prev)) > rtol:
        x_prev = x0.copy()
        tridisolve(eig_diag, e, x0)
        norm_x = np.linalg.norm(x0)
        x0 /= norm_x
    return x0

#-----------------------------------------------------------------------------
# Correlation/Covariance utils
#-----------------------------------------------------------------------------


def remove_bias(x, axis):
    "Subtracts an estimate of the mean from signal x at axis"
    padded_slice = [slice(d) for d in x.shape]
    padded_slice[axis] = np.newaxis
    mn = np.mean(x, axis=axis)
    return x - mn[tuple(padded_slice)]


def crosscov(x, y, axis=-1, all_lags=False, debias=True, normalize=True):
    """Returns the crosscovariance sequence between two ndarrays.
    This is performed by calling fftconvolve on x, y[::-1]

    Parameters
    ----------

    x : ndarray
    y : ndarray
    axis : time axis
    all_lags : {True/False}
       whether to return all nonzero lags, or to clip the length of s_xy
       to be the length of x and y. If False, then the zero lag covariance
       is at index 0. Otherwise, it is found at (len(x) + len(y) - 1)/2
    debias : {True/False}
       Always removes an estimate of the mean along the axis, unless
       told not to (eg X and Y are known zero-mean)

    Returns
    -------

    cxy : ndarray
       The crosscovariance function

    Notes
    -----

    cross covariance of processes x and y is defined as

    .. math::

    C_{xy}[k]=E\{(X(n+k)-E\{X\})(Y(n)-E\{Y\})^{*}\}

    where X and Y are discrete, stationary (or ergodic) random processes

    Also note that this routine is the workhorse for all auto/cross/cov/corr
    functions.

    """
    if x.shape[axis] != y.shape[axis]:
        raise ValueError(
            'crosscov() only works on same-length sequences for now'
            )
    if debias:
        x = remove_bias(x, axis)
        y = remove_bias(y, axis)
    slicing = [slice(d) for d in x.shape]
    slicing[axis] = slice(None, None, -1)
    cxy = fftconvolve(x, y[tuple(slicing)].conj(), axis=axis, mode='full')
    N = x.shape[axis]
    if normalize:
        cxy /= N
    if all_lags:
        return cxy
    slicing[axis] = slice(N - 1, 2 * N - 1)
    return cxy[tuple(slicing)]


def crosscorr(x, y, **kwargs):
    """
    Returns the crosscorrelation sequence between two ndarrays.
    This is performed by calling fftconvolve on x, y[::-1]

    Parameters
    ----------

    x : ndarray
    y : ndarray
    axis : time axis
    all_lags : {True/False}
       whether to return all nonzero lags, or to clip the length of r_xy
       to be the length of x and y. If False, then the zero lag correlation
       is at index 0. Otherwise, it is found at (len(x) + len(y) - 1)/2

    Returns
    -------

    rxy : ndarray
       The crosscorrelation function

    Notes
    -----

    cross correlation is defined as

    .. math::

    R_{xy}[k]=E\{X[n+k]Y^{*}[n]\}

    where X and Y are discrete, stationary (ergodic) random processes
    """
    # just make the same computation as the crosscovariance,
    # but without subtracting the mean
    kwargs['debias'] = False
    rxy = crosscov(x, y, **kwargs)
    return rxy


def autocov(x, **kwargs):
    """Returns the autocovariance of signal s at all lags.

    Parameters
    ----------

    x : ndarray
    axis : time axis
    all_lags : {True/False}
       whether to return all nonzero lags, or to clip the length of r_xy
       to be the length of x and y. If False, then the zero lag correlation
       is at index 0. Otherwise, it is found at (len(x) + len(y) - 1)/2

    Returns
    -------

    cxx : ndarray
       The autocovariance function

    Notes
    -----

    Adheres to the definition

    .. math::

    C_{xx}[k]=E\{(X[n+k]-E\{X\})(X[n]-E\{X\})^{*}\}

    where X is a discrete, stationary (ergodic) random process
    """
    # only remove the mean once, if needed
    debias = kwargs.pop('debias', True)
    axis = kwargs.get('axis', -1)
    if debias:
        x = remove_bias(x, axis)
    kwargs['debias'] = False
    return crosscov(x, x, **kwargs)


def autocorr(x, **kwargs):
    """Returns the autocorrelation of signal s at all lags.

    Parameters
    ----------

    x : ndarray
    axis : time axis
    all_lags : {True/False}
       whether to return all nonzero lags, or to clip the length of r_xy
       to be the length of x and y. If False, then the zero lag correlation
       is at index 0. Otherwise, it is found at (len(x) + len(y) - 1)/2

    Notes
    -----

    Adheres to the definition

    .. math::

    R_{xx}[k]=E\{X[n+k]X^{*}[n]\}

    where X is a discrete, stationary (ergodic) random process

    """
    # do same computation as autocovariance,
    # but without subtracting the mean
    kwargs['debias'] = False
    return autocov(x, **kwargs)


def fftconvolve(in1, in2, mode="full", axis=None):
    """ Convolve two N-dimensional arrays using FFT. See convolve.

    This is a fix of scipy.signal.fftconvolve, adding an axis argument and
    importing locally the stuff only needed for this function

    """
    s1 = np.array(in1.shape)
    s2 = np.array(in2.shape)
    complex_result = (np.issubdtype(in1.dtype, np.complex) or
                      np.issubdtype(in2.dtype, np.complex))

    if axis is None:
        size = s1 + s2 - 1
        fslice = tuple([slice(0, int(sz)) for sz in size])
    else:
        equal_shapes = s1 == s2
        # allow equal_shapes[axis] to be False
        equal_shapes[axis] = True
        assert equal_shapes.all(), 'Shape mismatch on non-convolving axes'
        size = s1[axis] + s2[axis] - 1
        fslice = [slice(l) for l in s1]
        fslice[axis] = slice(0, int(size))
        fslice = tuple(fslice)

    # Always use 2**n-sized FFT
    fsize = 2 ** int(np.ceil(np.log2(size)))
    if axis is None:
        IN1 = fftpack.fftn(in1, fsize)
        IN1 *= fftpack.fftn(in2, fsize)
        ret = fftpack.ifftn(IN1)[fslice].copy()
    else:
        IN1 = fftpack.fft(in1, fsize, axis=axis)
        IN1 *= fftpack.fft(in2, fsize, axis=axis)
        ret = fftpack.ifft(IN1, axis=axis)[fslice].copy()
    del IN1
    if not complex_result:
        ret = ret.real
    if mode == "full":
        return ret
    elif mode == "same":
        if np.product(s1, axis=0) > np.product(s2, axis=0):
            osize = s1
        else:
            osize = s2
        return signaltools._centered(ret, osize)
    elif mode == "valid":
        return signaltools._centered(ret, abs(s2 - s1) + 1)


#-----------------------------------------------------------------------------
# 'get' utils
#-----------------------------------------------------------------------------
def get_freqs(Fs, n):
    """Returns the center frequencies of the frequency decomposotion of a time
    series of length n, sampled at Fs Hz"""

    return np.linspace(0, float(Fs) / 2, float(n) / 2 + 1)


def circle_to_hz(omega, Fsamp):
    """For a frequency grid spaced on the unit circle of an imaginary plane,
    return the corresponding freqency grid in Hz.
    """
    return Fsamp * omega / (2 * np.pi)


def get_bounds(f, lb=0, ub=None):
    """ Find the indices of the lower and upper bounds within an array f

    Parameters
    ----------
    f, array

    lb,ub, float

    Returns
    -------

    lb_idx, ub_idx: the indices into 'f' which correspond to values bounded
    between ub and lb in that array
    """
    lb_idx = np.searchsorted(f, lb, 'left')
    if ub == None:
        ub_idx = len(f)
    else:
        ub_idx = np.searchsorted(f, ub, 'right')

    return lb_idx, ub_idx


def unwrap_phases(a):
    """
    Changes consecutive jumps larger than pi to their 2*pi complement.
    """
    pi = np.pi

    diffs = np.diff(a)
    mod_diffs = np.mod(diffs + pi, 2 * pi) - pi
    neg_pi_idx = np.where(mod_diffs == -1 * np.pi)
    pos_idx = np.where(diffs > 0)
    this_idx = np.intersect1d(neg_pi_idx[0], pos_idx[0])
    mod_diffs[this_idx] = pi
    correction = mod_diffs - diffs
    correction[np.where(np.abs(diffs) < pi)] = 0
    a[1:] += np.cumsum(correction)

    return a


def multi_intersect(input):
    """ A function for finding the intersection of several different arrays

    Parameters
    ----------
    input is a tuple of arrays, with all the different arrays

    Returns
    -------
    array - the intersection of the inputs

    Notes
    -----
    Simply runs intersect1d iteratively on the inputs
    """
    arr  = input[0].ravel()
    for this in input[1:]:
        arr = np.intersect1d(arr, this.ravel())

    return arr

def zero_pad(time_series, NFFT):
    """
    Pad a time-series with zeros on either side, depending on its length

    Parameters
    ----------
    time_series : n-d array
       Time-series data with time as the last dimension

    NFFT : int
       The length to pad the data up to.

    """

    n_dims = len(time_series.shape)
    n_time_points = time_series.shape[-1]

    if n_dims>1:
        n_channels = time_series.shape[:-1]
        shape_out = n_channels + (NFFT,)
    else:
        shape_out = NFFT
    # zero pad if time_series is too short
    if n_time_points < NFFT:
        tmp = time_series
        time_series = np.zeros(shape_out, time_series.dtype)
        time_series[..., :n_time_points] = tmp
        del tmp

    return time_series


#-----------------------------------------------------------------------------
# Numpy utilities - Note: these have been sent into numpy itself, so eventually
# we'll be able to get rid of them here.
#-----------------------------------------------------------------------------
def fill_diagonal(a, val):
    """Fill the main diagonal of the given array of any dimensionality.

    For an array with ndim > 2, the diagonal is the list of locations with
    indices a[i,i,...,i], all identical.

    This function modifies the input array in-place, it does not return a
    value.

    This functionality can be obtained via diag_indices(), but internally this
    version uses a much faster implementation that never constructs the indices
    and uses simple slicing.

    Parameters
    ----------
    a : array, at least 2-dimensional.
      Array whose diagonal is to be filled, it gets modified in-place.

    val : scalar
      Value to be written on the diagonal, its type must be compatible with
      that of the array a.

    Examples
    --------
    >>> a = np.zeros((3,3),int)
    >>> fill_diagonal(a,5)
    >>> a
    array([[5, 0, 0],
           [0, 5, 0],
           [0, 0, 5]])

    The same function can operate on a 4-d array:
    >>> a = np.zeros((3,3,3,3),int)
    >>> fill_diagonal(a,4)

    We only show a few blocks for clarity:
    >>> a[0,0]
    array([[4, 0, 0],
           [0, 0, 0],
           [0, 0, 0]])
    >>> a[1,1]
    array([[0, 0, 0],
           [0, 4, 0],
           [0, 0, 0]])
    >>> a[2,2]
    array([[0, 0, 0],
           [0, 0, 0],
           [0, 0, 4]])

    See also
    --------
    - diag_indices: indices to access diagonals given shape information.
    - diag_indices_from: indices to access diagonals given an array.
    """
    if a.ndim < 2:
        raise ValueError("array must be at least 2-d")
    if a.ndim == 2:
        # Explicit, fast formula for the common case.  For 2-d arrays, we
        # accept rectangular ones.
        step = a.shape[1] + 1
    else:
        # For more than d=2, the strided formula is only valid for arrays with
        # all dimensions equal, so we check first.
        if not np.alltrue(np.diff(a.shape) == 0):
            raise ValueError("All dimensions of input must be of equal length")
        step = np.cumprod((1,) + a.shape[:-1]).sum()

    # Write the value out into the diagonal.
    a.flat[::step] = val


def diag_indices(n, ndim=2):
    """Return the indices to access the main diagonal of an array.

    This returns a tuple of indices that can be used to access the main
    diagonal of an array with ndim (>=2) dimensions and shape (n,n,...,n).  For
    ndim=2 this is the usual diagonal, for ndim>2 this is the set of indices
    to access A[i,i,...,i] for i=[0..n-1].

    Parameters
    ----------
    n : int
      The size, along each dimension, of the arrays for which the returned
      indices can be used.

    ndim : int, optional
      The number of dimensions

    Examples
    --------
    Create a set of indices to access the diagonal of a (4,4) array:
    >>> di = diag_indices(4)

    >>> a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])
    >>> a
    array([[ 1,  2,  3,  4],
           [ 5,  6,  7,  8],
           [ 9, 10, 11, 12],
           [13, 14, 15, 16]])
    >>> a[di] = 100
    >>> a
    array([[100,   2,   3,   4],
           [  5, 100,   7,   8],
           [  9,  10, 100,  12],
           [ 13,  14,  15, 100]])

    Now, we create indices to manipulate a 3-d array:
    >>> d3 = diag_indices(2,3)

    And use it to set the diagonal of a zeros array to 1:
    >>> a = np.zeros((2,2,2),int)
    >>> a[d3] = 1
    >>> a
    array([[[1, 0],
            [0, 0]],
    <BLANKLINE>
           [[0, 0],
            [0, 1]]])

    See also
    --------
    - diag_indices_from: create the indices based on the shape of an existing
    array.
    """
    idx = np.arange(n)
    return (idx,) * ndim


def diag_indices_from(arr):
    """Return the indices to access the main diagonal of an n-dimensional
    array.

    See diag_indices() for full details.

    Parameters
    ----------
    arr : array, at least 2-d
    """
    if not arr.ndim >= 2:
        raise ValueError("input array must be at least 2-d")
    # For more than d=2, the strided formula is only valid for arrays with
    # all dimensions equal, so we check first.
    if not np.alltrue(np.diff(arr.shape) == 0):
        raise ValueError("All dimensions of input must be of equal length")

    return diag_indices(arr.shape[0], arr.ndim)


def mask_indices(n, mask_func, k=0):
    """Return the indices to access (n,n) arrays, given a masking function.

    Assume mask_func() is a function that, for a square array a of size (n,n)
    with a possible offset argument k, when called as mask_func(a,k) returns a
    new array with zeros in certain locations (functions like triu() or tril()
    do precisely this).  Then this function returns the indices where the
    non-zero values would be located.

    Parameters
    ----------
    n : int
      The returned indices will be valid to access arrays of shape (n,n).

    mask_func : callable
      A function whose api is similar to that of numpy.tri{u,l}.  That is,
      mask_func(x,k) returns a boolean array, shaped like x.  k is an optional
      argument to the function.

    k : scalar
      An optional argument which is passed through to mask_func().  Functions
      like tri{u,l} take a second argument that is interpreted as an offset.

    Returns
    -------
    indices : an n-tuple of index arrays.
      The indices corresponding to the locations where mask_func(ones((n,n)),k)
      is True.

    Examples
    --------
    These are the indices that would allow you to access the upper triangular
    part of any 3x3 array:
    >>> iu = mask_indices(3,np.triu)

    For example, if `a` is a 3x3 array:
    >>> a = np.arange(9).reshape(3,3)
    >>> a
    array([[0, 1, 2],
           [3, 4, 5],
           [6, 7, 8]])

    Then:
    >>> a[iu]
    array([0, 1, 2, 4, 5, 8])

    An offset can be passed also to the masking function.  This gets us the
    indices starting on the first diagonal right of the main one:
    >>> iu1 = mask_indices(3,np.triu,1)

    with which we now extract only three elements:
    >>> a[iu1]
    array([1, 2, 5])
    """
    m = np.ones((n, n), int)
    a = mask_func(m, k)
    return np.where(a != 0)


def tril_indices(n, k=0):
    """Return the indices for the lower-triangle of an (n,n) array.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.

    k : int, optional
      Diagonal offset (see tril() for details).

    Examples
    --------
    Commpute two different sets of indices to access 4x4 arrays, one for the
    lower triangular part starting at the main diagonal, and one starting two
    diagonals further right:

    >>> il1 = tril_indices(4)
    >>> il2 = tril_indices(4,2)

    Here is how they can be used with a sample array:
    >>> a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])
    >>> a
    array([[ 1,  2,  3,  4],
           [ 5,  6,  7,  8],
           [ 9, 10, 11, 12],
           [13, 14, 15, 16]])

    Both for indexing:
    >>> a[il1]
    array([ 1,  5,  6,  9, 10, 11, 13, 14, 15, 16])

    And for assigning values:
    >>> a[il1] = -1
    >>> a
    array([[-1,  2,  3,  4],
           [-1, -1,  7,  8],
           [-1, -1, -1, 12],
           [-1, -1, -1, -1]])

    These cover almost the whole array (two diagonals right of the main one):
    >>> a[il2] = -10
    >>> a
    array([[-10, -10, -10,   4],
           [-10, -10, -10, -10],
           [-10, -10, -10, -10],
           [-10, -10, -10, -10]])

    See also
    --------
    - triu_indices : similar function, for upper-triangular.
    - mask_indices : generic function accepting an arbitrary mask function.
    """
    return mask_indices(n, np.tril, k)


def tril_indices_from(arr, k=0):
    """Return the indices for the lower-triangle of an (n,n) array.

    See tril_indices() for full details.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.

    k : int, optional
      Diagonal offset (see tril() for details).

    """
    if not arr.ndim == 2 and arr.shape[0] == arr.shape[1]:
        raise ValueError("input array must be 2-d and square")
    return tril_indices(arr.shape[0], k)


def triu_indices(n, k=0):
    """Return the indices for the upper-triangle of an (n,n) array.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.

    k : int, optional
      Diagonal offset (see triu() for details).

    Examples
    --------
    Commpute two different sets of indices to access 4x4 arrays, one for the
    upper triangular part starting at the main diagonal, and one starting two
    diagonals further right:

    >>> iu1 = triu_indices(4)
    >>> iu2 = triu_indices(4,2)

    Here is how they can be used with a sample array:
    >>> a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])
    >>> a
    array([[ 1,  2,  3,  4],
           [ 5,  6,  7,  8],
           [ 9, 10, 11, 12],
           [13, 14, 15, 16]])

    Both for indexing:
    >>> a[iu1]
    array([ 1,  2,  3,  4,  6,  7,  8, 11, 12, 16])

    And for assigning values:
    >>> a[iu1] = -1
    >>> a
    array([[-1, -1, -1, -1],
           [ 5, -1, -1, -1],
           [ 9, 10, -1, -1],
           [13, 14, 15, -1]])

    These cover almost the whole array (two diagonals right of the main one):
    >>> a[iu2] = -10
    >>> a
    array([[ -1,  -1, -10, -10],
           [  5,  -1,  -1, -10],
           [  9,  10,  -1,  -1],
           [ 13,  14,  15,  -1]])

    See also
    --------
    - tril_indices : similar function, for lower-triangular.
    - mask_indices : generic function accepting an arbitrary mask function.
    """
    return mask_indices(n, np.triu, k)


def triu_indices_from(arr, k=0):
    """Return the indices for the lower-triangle of an (n,n) array.

    See triu_indices() for full details.

    Parameters
    ----------
    n : int
      Sets the size of the arrays for which the returned indices will be valid.

    k : int, optional
      Diagonal offset (see triu() for details).

    """
    if not arr.ndim == 2 and arr.shape[0] == arr.shape[1]:
        raise ValueError("input array must be 2-d and square")
    return triu_indices(arr.shape[0], k)


def structured_rand_arr(size, sample_func=np.random.random,
                        ltfac=None, utfac=None, fill_diag=None):
    """Make a structured random 2-d array of shape (size,size).

    If no optional arguments are given, a symmetric array is returned.

    Parameters
    ----------
    size : int
      Determines the shape of the output array: (size,size).

    sample_func : function, optional.
      Must be a function which when called with a 2-tuple of ints, returns a
      2-d array of that shape.  By default, np.random.random is used, but any
      other sampling function can be used as long as matches this API.

    utfac : float, optional
      Multiplicative factor for the upper triangular part of the matrix.

    ltfac : float, optional
      Multiplicative factor for the lower triangular part of the matrix.

    fill_diag : float, optional
      If given, use this value to fill in the diagonal.  Otherwise the diagonal
      will contain random elements.

    Examples
    --------
    >>> np.random.seed(0)  # for doctesting
    >>> np.set_printoptions(precision=4)  # for doctesting
    >>> structured_rand_arr(4)
    array([[ 0.5488,  0.7152,  0.6028,  0.5449],
           [ 0.7152,  0.6459,  0.4376,  0.8918],
           [ 0.6028,  0.4376,  0.7917,  0.5289],
           [ 0.5449,  0.8918,  0.5289,  0.0871]])
    >>> structured_rand_arr(4,ltfac=-10,utfac=10,fill_diag=0.5)
    array([[ 0.5   ,  8.3262,  7.7816,  8.7001],
           [-8.3262,  0.5   ,  4.6148,  7.8053],
           [-7.7816, -4.6148,  0.5   ,  9.4467],
           [-8.7001, -7.8053, -9.4467,  0.5   ]])
    """
    # Make a random array from the given sampling function
    rmat = sample_func((size, size))
    # And the empty one we'll then fill in to return
    out = np.empty_like(rmat)
    # Extract indices for upper-triangle, lower-triangle and diagonal
    uidx = triu_indices(size, 1)
    lidx = tril_indices(size, -1)
    didx = diag_indices(size)
    # Extract each part from the original and copy it to the output, possibly
    # applying multiplicative factors.  We check the factors instead of
    # defaulting to 1.0 to avoid unnecessary floating point multiplications
    # which could be noticeable for very large sizes.
    if utfac:
        out[uidx] = utfac * rmat[uidx]
    else:
        out[uidx] = rmat[uidx]
    if ltfac:
        out[lidx] = ltfac * rmat.T[lidx]
    else:
        out[lidx] = rmat.T[lidx]
    # If fill_diag was provided, use it; otherwise take the values in the
    # diagonal from the original random array.
    if fill_diag is not None:
        out[didx] = fill_diag
    else:
        out[didx] = rmat[didx]

    return out


def symm_rand_arr(size, sample_func=np.random.random, fill_diag=None):
    """Make a symmetric random 2-d array of shape (size,size).

    Parameters
    ----------
    n : int
      Size of the output array.

    sample_func : function, optional.
      Must be a function which when called with a 2-tuple of ints, returns a
      2-d array of that shape.  By default, np.random.random is used, but any
      other sampling function can be used as long as matches this API.

    fill_diag : float, optional
      If given, use this value to fill in the diagonal.  Useful for

    Examples
    --------
    >>> np.random.seed(0)  # for doctesting
    >>> np.set_printoptions(precision=4)  # for doctesting
    >>> symm_rand_arr(4)
    array([[ 0.5488,  0.7152,  0.6028,  0.5449],
           [ 0.7152,  0.6459,  0.4376,  0.8918],
           [ 0.6028,  0.4376,  0.7917,  0.5289],
           [ 0.5449,  0.8918,  0.5289,  0.0871]])
    >>> symm_rand_arr(4,fill_diag=4)
    array([[ 4.    ,  0.8326,  0.7782,  0.87  ],
           [ 0.8326,  4.    ,  0.4615,  0.7805],
           [ 0.7782,  0.4615,  4.    ,  0.9447],
           [ 0.87  ,  0.7805,  0.9447,  4.    ]])
      """
    return structured_rand_arr(size, sample_func, fill_diag=fill_diag)


def antisymm_rand_arr(size, sample_func=np.random.random):
    """Make an anti-symmetric random 2-d array of shape (size,size).

    Parameters
    ----------

    n : int
      Size of the output array.

    sample_func : function, optional.
      Must be a function which when called with a 2-tuple of ints, returns a
      2-d array of that shape.  By default, np.random.random is used, but any
      other sampling function can be used as long as matches this API.

    Examples
    --------
    >>> np.random.seed(0)  # for doctesting
    >>> np.set_printoptions(precision=4)  # for doctesting
    >>> antisymm_rand_arr(4)
    array([[ 0.    ,  0.7152,  0.6028,  0.5449],
           [-0.7152,  0.    ,  0.4376,  0.8918],
           [-0.6028, -0.4376,  0.    ,  0.5289],
           [-0.5449, -0.8918, -0.5289,  0.    ]])
      """
    return structured_rand_arr(size, sample_func, ltfac=-1.0, fill_diag=0)


# --------brainx utils------------------------------------------------------
# These utils were copied over from brainx - needed for viz


def threshold_arr(cmat, threshold=0.0, threshold2=None):
    """Threshold values from the input array.

    Parameters
    ----------
    cmat : array

    threshold : float, optional.
      First threshold.

    threshold2 : float, optional.
      Second threshold.

    Returns
    -------
    indices, values: a tuple with ndim+1

    Examples
    --------
    >>> np.set_printoptions(precision=4)  # For doctesting
    >>> a = np.linspace(0,0.2,5)
    >>> a
    array([ 0.  ,  0.05,  0.1 ,  0.15,  0.2 ])
    >>> threshold_arr(a,0.1)
    (array([3, 4]), array([ 0.15,  0.2 ]))

    With two thresholds:
    >>> threshold_arr(a,0.1,0.2)
    (array([0, 1]), array([ 0.  ,  0.05]))
    """
    # Select thresholds
    if threshold2 is None:
        th_low = -np.inf
        th_hi = threshold
    else:
        th_low = threshold
        th_hi = threshold2

    # Mask out the values we are actually going to use
    idx = np.where((cmat < th_low) | (cmat > th_hi))
    vals = cmat[idx]

    return idx + (vals,)


def thresholded_arr(arr, threshold=0.0, threshold2=None, fill_val=np.nan):
    """Threshold values from the input matrix and return a new matrix.

    Parameters
    ----------
    arr : array

    threshold : float
      First threshold.

    threshold2 : float, optional.
      Second threshold.

    Returns
    -------
    An array shaped like the input, with the values outside the threshold
    replaced with fill_val.

    Examples
    --------
    """
    a2 = np.empty_like(arr)
    a2.fill(fill_val)
    mth = threshold_arr(arr, threshold, threshold2)
    idx, vals = mth[:-1], mth[-1]
    a2[idx] = vals

    return a2


def rescale_arr(arr, amin, amax):
    """Rescale an array to a new range.

    Return a new array whose range of values is (amin,amax).

    Parameters
    ----------
    arr : array-like

    amin : float
      new minimum value

    amax : float
      new maximum value

    Examples
    --------
    >>> a = np.arange(5)

    >>> rescale_arr(a,3,6)
    array([ 3.  ,  3.75,  4.5 ,  5.25,  6.  ])
    """

    # old bounds
    m = arr.min()
    M = arr.max()
    # scale/offset
    s = float(amax - amin) / (M - m)
    d = amin - s * m

    # Apply clip before returning to cut off possible overflows outside the
    # intended range due to roundoff error, so that we can absolutely guarantee
    # that on output, there are no values > amax or < amin.
    return np.clip(s * arr + d, amin, amax)


def minmax_norm(arr, mode='direct', folding_edges=None):
    """Minmax_norm an array to [0,1] range.

    By default, this simply rescales the input array to [0,1].  But it has a
    special 'folding' mode that allows for the normalization of an array with
    negative and positive values by mapping the negative values to their
    flipped sign

    Parameters
    ----------
    arr : 1d array

    mode : string, one of ['direct','folding']

    folding_edges : (float,float)
      Only needed for folding mode, ignored in 'direct' mode.

    Examples
    --------
    >>> np.set_printoptions(precision=4)  # for doctesting
    >>> a = np.linspace(0.3,0.8,4)
    >>> minmax_norm(a)
    array([ 0.    ,  0.3333,  0.6667,  1.    ])
    >>> b = np.concatenate([np.linspace(-0.7,-0.3,3),
    ...                             np.linspace(0.3,0.8,3)])
    >>> b
    array([-0.7 , -0.5 , -0.3 ,  0.3 ,  0.55,  0.8 ])
    >>> minmax_norm(b,'folding',[-0.3,0.3])
    array([ 0.8,  0.4,  0. ,  0. ,  0.5,  1. ])
    """
    if mode == 'direct':
        return rescale_arr(arr, 0, 1)
    else:
        fa, fb = folding_edges
        amin, amax = arr.min(), arr.max()
        ra, rb = float(fa - amin), float(amax - fb)  # in case inputs are ints
        if ra < 0 or rb < 0:
            raise ValueError("folding edges must be within array range")
        greater = arr >= fb
        upper_idx = greater.nonzero()
        lower_idx = (~greater).nonzero()
        # Two folding scenarios, we map the thresholds to zero but the upper
        # ranges must retain comparability.
        if ra > rb:
            lower = 1.0 - rescale_arr(arr[lower_idx], 0, 1.0)
            upper = rescale_arr(arr[upper_idx], 0, float(rb) / ra)
        else:
            upper = rescale_arr(arr[upper_idx], 0, 1)
            # The lower range is trickier: we need to rescale it and then flip
            # it, so the edge goes to 0.
            resc_a = float(ra) / rb
            lower = rescale_arr(arr[lower_idx], 0, resc_a)
            lower = resc_a - lower
        # Now, make output array
        out = np.empty_like(arr)
        out[lower_idx] = lower
        out[upper_idx] = upper
        return out


#---------- intersect coords ----------------------------------------------
def intersect_coords(coords1, coords2):
    """For two sets of coordinates, find the coordinates that are common to
    both, where the dimensionality is the coords1.shape[0]"""
    # Find the longer one
    if coords1.shape[-1] > coords2.shape[-1]:
        coords_long = coords1
        coords_short = coords2
    else:
        coords_long = coords2
        coords_short = coords1

    ans = np.array([[], [], []], dtype='int')  # Initialize as a 3 row variable
    # Loop over the longer of the coordinate sets
    for i in range(coords_long.shape[-1]):
        # For each coordinate:
        this_coords = coords_long[:, i]
        # Find the matches in the other set of coordinates:
        x = np.where(coords_short[0, :] == this_coords[0])[0]
        y = np.where(coords_short[1, :] == this_coords[1])[0]
        z = np.where(coords_short[2, :] == this_coords[2])[0]

        # Use intersect1d, such that there can be more than one match (and the
        # size of idx will reflect how many such matches exist):
        idx = np.intersect1d(np.intersect1d(x, y), z)
        # Append the places where there are matches in all three dimensions:
        if len(idx):
            ans = np.hstack([ans, coords_short[:, idx]])

    return ans


#---------- Time Series Stats ----------------------------------------
def zscore(time_series, axis=-1):
    """Returns the z-score of each point of the time series
    along a given axis of the array time_series.

    Parameters
    ----------
    time_series : ndarray
        an array of time series
    axis : int, optional
        the axis of time_series along which to compute means and stdevs

    Returns
    _______
    zt : ndarray
        the renormalized time series array
    """
    time_series = np.asarray(time_series)
    et = time_series.mean(axis=axis)
    st = time_series.std(axis=axis)
    sl = [slice(None)] * len(time_series.shape)
    sl[axis] = np.newaxis
    zt = time_series - et[sl]
    zt /= st[sl]
    return zt


def percent_change(ts, ax=-1):
    """Returns the % signal change of each point of the times series
    along a given axis of the array time_series

    Parameters
    ----------

    ts : ndarray
        an array of time series

    ax : int, optional (default to -1)
        the axis of time_series along which to compute means and stdevs

    Returns
    -------

    ndarray
        the renormalized time series array (in units of %)

    Examples
    --------

    >>> ts = np.arange(4*5).reshape(4,5)
    >>> ax = 0
    >>> percent_change(ts,ax)
    array([[-100.    ,  -88.2353,  -78.9474,  -71.4286,  -65.2174],
           [ -33.3333,  -29.4118,  -26.3158,  -23.8095,  -21.7391],
           [  33.3333,   29.4118,   26.3158,   23.8095,   21.7391],
           [ 100.    ,   88.2353,   78.9474,   71.4286,   65.2174]])
    >>> ax = 1
    >>> percent_change(ts,ax)
    array([[-100.    ,  -50.    ,    0.    ,   50.    ,  100.    ],
           [ -28.5714,  -14.2857,    0.    ,   14.2857,   28.5714],
           [ -16.6667,   -8.3333,    0.    ,    8.3333,   16.6667],
           [ -11.7647,   -5.8824,    0.    ,    5.8824,   11.7647]])
"""
    ts = np.asarray(ts)

    return (ts / np.expand_dims(np.mean(ts, ax), ax) - 1) * 100


#----------Event-related analysis utils ----------------------------------
def fir_design_matrix(events, len_hrf):
    """Create a FIR event matrix from a time-series of events.

    Parameters
    ----------

    events : 1-d int array
       Integers denoting different kinds of events, occuring at the time
       corresponding to the bin represented by each slot in the array. In
       time-bins in which no event occured, a 0 should be entered. If negative
       event values are entered, they will be used as "negative" events, as in
       events that should be contrasted with the postitive events (typically -1
       and 1 can be used for a simple contrast of two conditions)

    len_hrf : int
       The expected length of the HRF (in the same time-units as the events are
       represented (presumably TR). The size of the block dedicated in the
       fir_matrix to each type of event

    Returns
    -------

    fir_matrix : matrix

       The design matrix for FIR estimation
    """
    event_types = np.unique(events)[np.unique(events) != 0]
    fir_matrix = np.zeros((events.shape[0], len_hrf * event_types.shape[0]))

    for t in event_types:
        idx_h_a = np.where(event_types == t)[0] * len_hrf
        idx_h_b = idx_h_a + len_hrf
        idx_v = np.where(events == t)[0]
        for idx_v_a in idx_v:
            idx_v_b = idx_v_a + len_hrf
            fir_matrix[idx_v_a:idx_v_b, idx_h_a:idx_h_b] += (np.eye(len_hrf) *
                                                            np.sign(t))

    return fir_matrix


#We carry around a copy of the hilbert transform analytic signal from newer
#versions of scipy, in case someone is using an older version of scipy with a
#borked hilbert:
def hilbert_from_new_scipy(x, N=None, axis=-1):
    """This is a verbatim copy of scipy.signal.hilbert from scipy version
    0.8dev, which we carry around in order to use in case the version of scipy
    installed is old enough to have a broken implementation of hilbert """

    x = np.asarray(x)
    if N is None:
        N = x.shape[axis]
    if N <= 0:
        raise ValueError("N must be positive.")
    if np.iscomplexobj(x):
        print("Warning: imaginary part of x ignored.")
        x = np.real(x)
    Xf = fftpack.fft(x, N, axis=axis)
    h = np.zeros(N)
    if N % 2 == 0:
        h[0] = h[N / 2] = 1
        h[1:N / 2] = 2
    else:
        h[0] = 1
        h[1:(N + 1) / 2] = 2

    if len(x.shape) > 1:
        ind = [np.newaxis] * x.ndim
        ind[axis] = slice(None)
        h = h[ind]
    x = fftpack.ifft(Xf * h, axis=axis)
    return x


#---------- MAR utilities ----------------------------------------

# These utilities are used in the computation of multivariate autoregressive
# models (used in computing Granger causality):

def crosscov_vector(x, y, nlags=None):
    """
    This method computes the following function

    .. math::

        R_{xy}(k) = E{ x(t)y^{*}(t-k) } = E{ x(t+k)y^{*}(t) }
        k \in {0, 1, ..., nlags-1}

    (* := conjugate transpose)

    Note: This is related to the other commonly used definition
    for vector crosscovariance

    .. math::

        R_{xy}^{(2)}(k) = E{ x(t-k)y^{*}(t) } = R_{xy}^(-k) = R_{yx}^{*}(k)

    Parameters
    ----------

    x, y : ndarray (nc, N)

    nlags : int, optional
       compute lags for k in {0, ..., nlags-1}

    Returns
    -------

    rxy : ndarray (nc, nc, nlags)

    """
    N = x.shape[1]
    if nlags is None:
        nlags = N
    nc = x.shape[0]

    rxy = np.empty((nc, nc, nlags))

    # rxy(k) = E{ x(t)y*(t-k) } ( * = conj transpose )
    # Take the expectation over an outer-product
    # between x(t) and conj{y(t-k)} for each t

    for k in range(nlags):
        # rxy(k) = E{ x(t)y*(t-k) }
        prod = x[:, None, k:] * y[None, :, :N - k].conj()
##         # rxy(k) = E{ x(t)y*(t+k) }
##         prod = x[:,None,:N-k] * y[None,:,k:].conj()
        # Do a sample mean of N-k pts? or sum and divide by N?
        rxy[..., k] = prod.mean(axis=-1)
    return rxy


def autocov_vector(x, nlags=None):
    """
    This method computes the following function

    .. math::

    R_{xx}(k) = E{ x(t)x^{*}(t-k) } = E{ x(t+k)x^{*}(t) }
    k \in {0, 1, ..., nlags-1}

    (* := conjugate transpose)

    Note: this is related to
    the other commonly used definition for vector autocovariance

    .. math::

    R_{xx}^{(2)}(k) = E{ x(t-k)x^{*}(t) } = R_{xx}(-k) = R_{xx}^{*}(k)

    Parameters
    ----------

    x : ndarray (nc, N)

    nlags : int, optional
       compute lags for k in {0, ..., nlags-1}

    Returns
    -------

    rxx : ndarray (nc, nc, nlags)

    """
    return crosscov_vector(x, x, nlags=nlags)


def generate_mar(a, cov, N):
    """
    Generates a multivariate autoregressive dataset given the formula:

    X(t) + sum_{i=1}^{P} a(i)X(t-i) = E(t)

    Where E(t) is a vector of samples from possibly covarying noise processes.

    Parameters
    ----------

    a : ndarray (n_order, n_c, n_c)
       An order n_order set of coefficient matrices, each shaped (n_c, n_c) for
       n_channel data
    cov : ndarray (n_c, n_c)
       The innovations process covariance
    N : int
       how many samples to generate

    Returns
    -------

    mar, nz

    mar and noise process shaped (n_c, N)
    """
    n_c = cov.shape[0]
    n_order = a.shape[0]

    nz = np.random.multivariate_normal(
        np.zeros(n_c), cov, size=(N,)
        )

    # nz is a (N x n_seq) array

    mar = nz.copy()  # np.zeros((N, n_seq), 'd')

    # this looks like a redundant loop that can be rolled into a matrix-matrix
    # multiplication at each coef matrix a(i)

    # this rearranges the equation to read:
    # X(i) = E(i) - sum_{j=1}^{P} a(j)X(i-j)
    # where X(n) n < 0 is taken to be 0
    # In terms of the code: X is mar and E is nz, P is n_order
    for i in range(N):
        for j in range(min(i, n_order)):  # j logically in set {1, 2, ..., P}
            mar[i, :] -= np.dot(a[j], mar[i - j - 1, :])

    return mar.transpose(), nz.transpose()


#----------goodness of fit utilities ----------------------------------------

def akaike_information_criterion(ecov, p, m, Ntotal, corrected=False):

    """

    A measure of the goodness of fit of an auto-regressive model based on the
    model order and the error covariance.

    Parameters
    ----------

    ecov : float array
        The error covariance of the system
    p
        the number of channels
    m : int
        the model order
    Ntotal
        the number of total time-points (across channels)
    corrected : boolean (optional)
        Whether to correct for small sample size

    Returns
    -------

    AIC : float
        The value of the AIC


    Notes
    -----
    This is an implementation of equation (50) in Ding et al. (2006):

    M Ding and Y Chen and S Bressler (2006) Granger Causality: Basic Theory and
    Application to Neuroscience. http://arxiv.org/abs/q-bio/0608035v1


    Correction for small sample size is taken from:
    http://en.wikipedia.org/wiki/Akaike_information_criterion.

    """

    AIC = (2 * (np.log(linalg.det(ecov))) +
           ((2 * (p ** 2) * m) / (Ntotal)))

    if corrected is None:
        return AIC
    else:
        return AIC + (2 * m * (m + 1)) / (Ntotal - m - 1)


def bayesian_information_criterion(ecov, p, m, Ntotal):
    """The Bayesian Information Criterion, also known as the Schwarz criterion
     is a measure of goodness of fit of a statistical model, based on the
     number of model parameters and the likelihood of the model

    Parameters
    ----------
    ecov : float array
        The error covariance of the system

    p : int
        the system size (how many variables).

    m : int
        the model order.

    corrected : boolean (optional)
        Whether to correct for small sample size


    Returns
    -------

    BIC : float
        The value of the BIC
    a
        the resulting autocovariance vector

    Notes
    -----
    This is an implementation of equation (51) in Ding et al. (2006):

    .. math ::

    BIC(m) = 2 log(|\Sigma|) + \frac{2p^2 m log(N_{total})}{N_{total}},

    where $\Sigma$ is the noise covariance matrix. In auto-regressive model
    estimation, this matrix will contain in $\Sigma_{i,j}$ the residual
    variance in estimating time-series $i$ from $j$, $p$ is the dimensionality
    of the data, $m$ is the number of parameters in the model and $N_{total}$
    is the number of time-points.

    M Ding and Y Chen and S Bressler (2006) Granger Causality: Basic Theory and
    Application to Neuroscience. http://arxiv.org/abs/q-bio/0608035v1


    See http://en.wikipedia.org/wiki/Schwarz_criterion

    """

    BIC = (2 * (np.log(linalg.det(ecov))) +
            ((2 * (p ** 2) * m * np.log(Ntotal)) / (Ntotal)))

    return BIC

########NEW FILE########
__FILENAME__ = version
"""nitime version/release information"""
from nitime.six.moves import map

# Format expected by setup.py and doc/source/conf.py: string of form "X.Y.Z"
_version_major = 0
_version_minor = 5
_version_micro = ''  # use '' for first of series, number for 1 and above
_version_extra = 'dev'
#_version_extra = ''  # Uncomment this for full releases

# Construct full version string from these.
_ver = [_version_major, _version_minor]
if _version_micro:
    _ver.append(_version_micro)
if _version_extra:
    _ver.append(_version_extra)

__version__ = '.'.join(map(str, _ver))

CLASSIFIERS = ["Development Status :: 3 - Alpha",
               "Environment :: Console",
               "Intended Audience :: Science/Research",
               "License :: OSI Approved :: BSD License",
               "Operating System :: OS Independent",
               "Programming Language :: Python",
               "Topic :: Scientific/Engineering"]

description = "Nitime: timeseries analysis for neuroscience data"

# Note: this long_description is actually a copy/paste from the top-level
# README.txt, so that it shows up nicely on PyPI.  So please remember to edit
# it only in one place and sync it correctly.
long_description = """
===================================================
 Nitime: timeseries analysis for neuroscience data
===================================================

Nitime is library of tools and algorithms for the analysis of time-series data
from neuroscience experiments. It contains a implementation of numerical
algorithms for time-series analysis both in the time and spectral domains, a
set of container objects to represent time-series, and auxiliary objects that
expose a high level interface to the numerical machinery and make common
analysis tasks easy to express with compact and semantically clear code.

Website and mailing list
========================

Current information can always be found at the nitime `website`_. Questions and
comments can be directed to the mailing `list`_. 

.. _website: http://nipy.org/nitime
.. _list: http://mail.scipy.org/mailman/listinfo/nipy-devel

Code
====

You can find our sources and single-click downloads:

* `Main repository`_ on Github.
* Documentation_ for all releases and current development tree.
* Download as a tar/zip file the `current trunk`_.
* Downloads of all `available releases`_.

.. _main repository: http://github.com/nipy/nitime
.. _Documentation: http://nipy.org/nitime
.. _current trunk: http://github.com/nipy/nitime/archives/master
.. _available releases: http://github.com/nipy/nitime/downloads


License information
===================

Nitime is licensed under the terms of the new BSD license. See the file
"LICENSE" for information on the history of this software, terms & conditions
for usage, and a DISCLAIMER OF ALL WARRANTIES.

All trademarks referenced herein are property of their respective holders.

Copyright (c) 2006-2012, NIPY Developers
All rights reserved.
"""

NAME = "nitime"
MAINTAINER = "Nipy Developers"
MAINTAINER_EMAIL = "nipy-devel@neuroimaging.scipy.org"
DESCRIPTION = description
LONG_DESCRIPTION = long_description
URL = "http://nipy.org/nitime"
DOWNLOAD_URL = "http://github.com/nipy/nitime/downloads"
LICENSE = "Simplified BSD"
AUTHOR = "Nitime developers"
AUTHOR_EMAIL = "nipy-devel@neuroimaging.scipy.org"
PLATFORMS = "OS Independent"
MAJOR = _version_major
MINOR = _version_minor
MICRO = _version_micro
VERSION = __version__
PACKAGES = ['nitime',
            'nitime.tests',
            'nitime.fmri',
            'nitime.fmri.tests',
            'nitime.algorithms',
            'nitime.algorithms.tests',
            'nitime.analysis',
            'nitime.analysis.tests',
            ]
PACKAGE_DATA = {"nitime": ["LICENSE", "tests/*.txt", "tests/*.npy",
                                  "data/*.nii.gz","data/*.txt", "data/*.csv"]}
REQUIRES = ["numpy", "matplotlib", "scipy"]

########NEW FILE########
__FILENAME__ = viz
"""Tools for visualization of time-series data.

Depends on matplotlib. Some functions depend also on networkx

"""
from __future__ import print_function

# If you are running nosetests right now, you might want to use 'agg' as a backend:
import sys
from nitime.six.moves import map
from nitime.six.moves import zip
if "nose" in sys.modules:
    import matplotlib
    matplotlib.use('agg')
     
# Then do all the rest of it:
import numpy as np
from scipy import fftpack
from matplotlib import mpl
from matplotlib import pyplot as plt
import matplotlib.ticker as ticker
import matplotlib.colors as colors
from mpl_toolkits.axes_grid import make_axes_locatable

from nitime import timeseries as ts
import nitime.utils as tsu
from nitime.utils import threshold_arr, minmax_norm, rescale_arr
import nitime.analysis as nta

# Matplotlib 1.3 has a bug in it, so if that's what you have, we'll replace it
# for you with a fixed version of that module:
import matplotlib
if matplotlib.__version__[:3] == '1.3':
    import nitime._mpl_units as mpl_units
    import matplotlib.axis as ax
    ax.munits = mpl_units
    
from nitime.utils import triu_indices

#Some visualization functions require networkx. Import that if possible:
try:
    import networkx as nx
    #If not, throw an error and get on with business:
except ImportError:
    e_s = "Networkx is not available. Some visualization tools might not work"
    e_s += "\n To download networkx: http://networkx.lanl.gov/"
    print(e_s)
    class NetworkxNotInstalled(object):
        def __getattribute__(self,x):
            raise ImportError(e_s)
    nx = NetworkxNotInstalled()


def plot_tseries(time_series, fig=None, axis=0,
                 xticks=None, xunits=None, yticks=None, yunits=None,
                 xlabel=None, ylabel=None, yerror=None, error_alpha=0.1,
                 time_unit=None, **kwargs):

    """plot a timeseries object

    Arguments
    ---------

    time_series: a nitime time-series object

    fig: a figure handle, opens a new figure if None

    subplot: an axis number (if there are several in the figure to be opened),
        defaults to 0.

    xticks: optional, list, specificies what values to put xticks on. Defaults
    to the matlplotlib-generated.

    yticks: optional, list, specificies what values to put xticks on. Defaults
    to the matlplotlib-generated.

    xlabel: optional, list, specificies what labels to put on xticks

    ylabel: optional, list, specificies what labels to put on yticks

    yerror: optional, UniformTimeSeries with the same sampling_rate and number
    of samples and channels as time_series, the error will be displayed as a
    shading above and below the plotted time-series

    """

    if fig is None:
        fig = plt.figure()

    if not fig.get_axes():
        ax = fig.add_subplot(1, 1, 1)
    else:
        ax = fig.get_axes()[axis]

    #Make sure that time displays on the x axis with the units you want:
    #If you want to change the time-unit on the visualization from that used to
    #represent the time-series:
    if time_unit is not None:
        tu = time_unit
        conv_fac = ts.time_unit_conversion[time_unit]
    #Otherwise, get the information from your input:
    else:
        tu = time_series.time_unit
        conv_fac = time_series.time._conversion_factor

    this_time = time_series.time / float(conv_fac)
    ax.plot(this_time, time_series.data.T, **kwargs)

    if xlabel is None:
        ax.set_xlabel('Time (%s)' % tu)
    else:
        ax.set_xlabel(xlabel)

    if ylabel is not None:
        ax.set_ylabel(ylabel)

    if yerror is not None:
        if len(yerror.data.shape) == 1:
            this_e = yerror.data[np.newaxis, :]
        else:
            this_e = yerror.data
        delta = this_e
        e_u = time_series.data + delta
        e_d = time_series.data - delta
        for i in range(e_u.shape[0]):
            ax.fill_between(this_time, e_d[i], e_u[i], alpha=error_alpha)

    return fig


def matshow_tseries(time_series, fig=None, axis=0, xtick_n=5, time_unit=None,
                    xlabel=None, ylabel=None):

    """Creates an image of the time-series, ordered according to the first
    dimension of the time-series object

    Parameters
    ----------

    time_series: a nitime time-series object

    fig: a figure handle, opens a new figure if None

    axis: an axis number (if there are several in the figure to be opened),
        defaults to 0.

    xtick_n: int, optional, sets the number of ticks to be placed on the x axis
    """

    if fig is None:
        fig = plt.figure()

    if not fig.get_axes():
        ax = fig.add_subplot(1, 1, 1)
    else:
        ax = fig.get_axes()[axis]

    #Make sure that time displays on the x axis with the units you want:
    #If you want to change the time-unit on the visualization from that used to
    #represent the time-series:
    if time_unit is not None:
        tu = time_unit
        conv_fac = ts.time_unit_conversion[time_unit]
    #Otherwise, get the information from your input:
    else:
        tu = time_series.time_unit
        conv_fac = time_series.time._conversion_factor

    this_time = time_series.time / float(conv_fac)
    ax.matshow(time_series.data)

    ax.set_xticks(list(range(len(this_time)))[::len(this_time) / xtick_n])
    ax.set_xticklabels(this_time[::len(this_time) / xtick_n])

    if xlabel is None:
        ax.set_xlabel('Time (%s)' % tu)
    else:
        ax.set_xlabel(xlabel)

    if ylabel is not None:
        ax.set_ylabel(ylabel)

    return fig


## Helper functions for matshow_roi and for drawgraph_roi, in order to get the
## right cmap for the colorbar:

##Currently not used at all - should they be removed?
def rgb_to_dict(value, cmap):
    return dict(zip(('red', 'green', 'blue', 'alpha'), cmap(value)))


def subcolormap(xmin, xmax, cmap):
    '''Returns the part of cmap between xmin, xmax, scaled to 0,1.'''
    assert xmin < xmax
    assert xmax <= 1
    cd = cmap._segmentdata.copy()
    colornames = ('red', 'green', 'blue')
    rgbmin, rgbmax = rgb_to_dict(xmin, cmap), rgb_to_dict(xmax, cmap)
    for k in cd:
        tmp = [x for x in cd[k] if x[0] >= xmin and x[0] <= xmax]
        if tmp == [] or tmp[0][0] > xmin:
            tmp = [(xmin, rgbmin[k], rgbmin[k])] + tmp
        if tmp == [] or tmp[-1][0] < xmax:
            tmp = tmp + [(xmax, rgbmax[k], rgbmax[k])]
        #now scale all this to (0,1)
        square = list(zip(*tmp))
        xbreaks = [(x - xmin) / (xmax - xmin) for x in square[0]]
        square[0] = xbreaks
        tmp = list(zip(*square))
        cd[k] = tmp
    return colors.LinearSegmentedColormap('local', cd, N=256)


def drawmatrix_channels(in_m, channel_names=None, fig=None, x_tick_rot=0,
                        size=None, cmap=plt.cm.RdBu_r, colorbar=True,
                        color_anchor=None, title=None):
    r"""Creates a lower-triangle of the matrix of an nxn set of values. This is
    the typical format to show a symmetrical bivariate quantity (such as
    correlation or coherence between two different ROIs).

    Parameters
    ----------

    in_m: nxn array with values of relationships between two sets of rois or
    channels

    channel_names (optional): list of strings with the labels to be applied to
    the channels in the input. Defaults to '0','1','2', etc.

    fig (optional): a matplotlib figure

    cmap (optional): a matplotlib colormap to be used for displaying the values
    of the connections on the graph

    title (optional): string to title the figure (can be like '$\alpha$')

    color_anchor (optional): determine the mapping from values to colormap
        if None, min and max of colormap correspond to min and max of in_m
        if 0, min and max of colormap correspond to max of abs(in_m)
        if (a,b), min and max of colormap correspond to (a,b)

    Returns
    -------

    fig: a figure object

    """
    N = in_m.shape[0]
    ind = np.arange(N)  # the evenly spaced plot indices

    def channel_formatter(x, pos=None):
        thisind = np.clip(int(x), 0, N - 1)
        return channel_names[thisind]

    if fig is None:
        fig = plt.figure()

    if size is not None:

        fig.set_figwidth(size[0])
        fig.set_figheight(size[1])

    w = fig.get_figwidth()
    h = fig.get_figheight()

    ax_im = fig.add_subplot(1, 1, 1)

    #If you want to draw the colorbar:
    if colorbar:
        divider = make_axes_locatable(ax_im)
        ax_cb = divider.new_vertical(size="10%", pad=0.1, pack_start=True)
        fig.add_axes(ax_cb)

    #Make a copy of the input, so that you don't make changes to the original
    #data provided
    m = in_m.copy()

    #Null the upper triangle, so that you don't get the redundant and the
    #diagonal values:
    idx_null = triu_indices(m.shape[0])
    m[idx_null] = np.nan

    #Extract the minimum and maximum values for scaling of the
    #colormap/colorbar:
    max_val = np.nanmax(m)
    min_val = np.nanmin(m)

    if color_anchor is None:
        color_min = min_val
        color_max = max_val
    elif color_anchor == 0:
        bound = max(abs(max_val), abs(min_val))
        color_min = -bound
        color_max = bound
    else:
        color_min = color_anchor[0]
        color_max = color_anchor[1]

    #The call to imshow produces the matrix plot:
    im = ax_im.imshow(m, origin='upper', interpolation='nearest',
       vmin=color_min, vmax=color_max, cmap=cmap)

    #Formatting:
    ax = ax_im
    ax.grid(True)
    #Label each of the cells with the row and the column:
    if channel_names is not None:
        for i in range(0, m.shape[0]):
            if i < (m.shape[0] - 1):
                ax.text(i - 0.3, i, channel_names[i], rotation=x_tick_rot)
            if i > 0:
                ax.text(-1, i + 0.3, channel_names[i],
                        horizontalalignment='right')

        ax.set_axis_off()
        ax.set_xticks(np.arange(N))
        ax.xaxis.set_major_formatter(ticker.FuncFormatter(channel_formatter))
        fig.autofmt_xdate(rotation=x_tick_rot)
        ax.set_yticks(np.arange(N))
        ax.set_yticklabels(channel_names)
        ax.set_ybound([-0.5, N - 0.5])
        ax.set_xbound([-0.5, N - 1.5])

    #Make the tick-marks invisible:
    for line in ax.xaxis.get_ticklines():
        line.set_markeredgewidth(0)

    for line in ax.yaxis.get_ticklines():
        line.set_markeredgewidth(0)

    ax.set_axis_off()

    if title is not None:
        ax.set_title(title)

    #The following produces the colorbar and sets the ticks
    if colorbar:
        #Set the ticks - if 0 is in the interval of values, set that, as well
        #as the maximal and minimal values:
        if min_val < 0:
            ticks = [color_min, min_val, 0, max_val, color_max]
        #Otherwise - only set the minimal and maximal value:
        else:
            ticks = [color_min, min_val, max_val, color_max]

        #This makes the colorbar:
        cb = fig.colorbar(im, cax=ax_cb, orientation='horizontal',
                          cmap=cmap,
                          norm=im.norm,
                          boundaries=np.linspace(color_min, color_max, 256),
                          ticks=ticks,
                          format='%.2f')

    # Set the current figure active axis to be the top-one, which is the one
    # most likely to be operated on by users later on
    fig.sca(ax)

    return fig


def drawgraph_channels(in_m, channel_names=None, cmap=plt.cm.RdBu_r,
                       node_shapes=None, node_colors=None,
                       title=None, layout=None, threshold=None):

    """Draw a graph based on the matrix specified in in_m. Wrapper to
    draw_graph.

    Parameters
    ----------

    in_m: nxn array with values of relationships between two sets of channels
    or channels

    channel_names (optional): list of strings with the labels to be applied to
    the channels in the input. Defaults to '0','1','2', etc.

    cmap (optional): a matplotlib colormap to be used for displaying the values
    of the connections on the graph

    node_shapes: defaults to circle

    node_colors: defaults to white,

    title:

    layout, defaults to nx.circular_layout
    Returns
    -------
    fig: a figure object

    Notes
    -----

    The layout of the graph is done using functions from networkx
    (http://networkx.lanl.gov), which is a dependency of this function
    """
    nnodes = in_m.shape[0]
    if channel_names is None:
        node_labels = None  # [None]*nnodes
    else:
        node_labels = list(channel_names)

    if node_shapes is None:
        node_shapes = ['o'] * nnodes

    if node_colors is None:
        node_colors = ['w'] * nnodes

    #Make a copy, avoiding making changes to the original data:
    m = in_m.copy()

    #Set the diagonal values to the minimal value of the matrix, so that the
    #vrange doesn't always get stretched to 1:
    m[np.arange(nnodes), np.arange(nnodes)] = min(np.nanmin(m), -np.nanmax(m))
    range_setter = max(abs(np.nanmin(m)), abs(np.nanmax(m)))
    vrange = [-range_setter, range_setter]

    #m[np.where(np.isnan(m))] = 0
    if threshold is None:
        #If there happens to be an off-diagnoal edge in the adjacency matrix
        #which is just as small as the minimum, we don't want to drop that one:
        eps = 10 ** -10
        G = mkgraph(m, threshold=vrange[0] - eps, threshold2=None)
    else:
        G = mkgraph(m, threshold=threshold[0], threshold2=threshold[1])
    fig = draw_graph(G,
                     node_colors=node_colors,
                     node_shapes=node_shapes,
                     node_scale=2,
                     labels=node_labels,
                     edge_cmap=cmap,
                     colorbar=True,
                     vrange=vrange,
                     title=title,
                     stretch_factor=1,
                     edge_alpha=False,
                     layout=layout
                     )
    return fig


def plot_xcorr(xc, ij, fig=None, line_labels=None, xticks=None, yticks=None,
               xlabel=None, ylabel=None):
    """ Visualize the cross-correlation function"""

    if fig is None:
        fig = plt.figure()

    if not fig.get_axes():
        ax = fig.add_subplot(1, 1, 1)
    else:
        ax = fig.get_axes()[0]

    if line_labels is not None:
        #Reverse the order, so that pop() works:
        line_labels.reverse()
        this_labels = line_labels

    #Use the ij input as the labels:
    else:
        this_labels = [str(this) for this in ij].reverse()

    #Make sure that time displays on the x axis with the units you want:
    conv_fac = xc.time._conversion_factor
    this_time = xc.time / float(conv_fac)

    for (i, j) in ij:
        if this_labels is not None:
            #Use pop() to get the first one and remove it:
            ax.plot(this_time, xc.data[i, j].squeeze(),
                    label=this_labels.pop())
        else:
            ax.plot(this_time, xc.data[i, j].squeeze())

    ax.set_xlabel('Time(sec)')
    ax.set_ylabel('Correlation(normalized)')

    if xlabel is None:
        #Make sure that time displays on the x axis with the units you want:
        conv_fac = xc.time._conversion_factor
        time_label = xc.time / float(conv_fac)
        ax.set_xlabel('Time (%s)' % xc.time_unit)
    else:
        time_label = xlabel
        ax.set_xlabel(xlabel)

    if line_labels is not None:
        plt.legend()

    if ylabel is None:
        ax.set_ylabel('Correlation')
    else:
        ax.set_ylabel(ylabel)

    return fig


#-----------------------------------------------------------------------------
# Functions from brainx:
#-----------------------------------------------------------------------------
def draw_matrix(mat, th1=None, th2=None, clim=None, cmap=None):
    """Draw a matrix, optionally thresholding it.
    """
    if th1 is not None:
        m2 = tsu.thresholded_arr(mat, th1, th2)
    else:
        m2 = mat
    ax = plt.matshow(m2, cmap=cmap)
    if clim is not None:
        ax.set_clim(*clim)
    plt.colorbar()
    return ax


def draw_arrows(G, pos, edgelist=None, ax=None, edge_color='k', alpha=1.0,
                width=1):
    """Draw arrows on a set of edges"""

    if ax is None:
        ax = plt.gca()

    if edgelist is None:
        edgelist = G.edges()

    if not edgelist or len(edgelist) == 0:  # no edges!
        return

    # set edge positions
    edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist])

    arrow_colors = (colors.colorConverter.to_rgba('k', alpha), )
    a_pos = []

    # Radius of the nodes in world coordinates
    radius = 0.5
    head_length = 0.31
    overhang = 0.1

    #ipvars('edge_pos')  # dbg

    for src, dst in edge_pos:
        dd = dst - src
        nd = np.linalg.norm(dd)
        if nd == 0:  # source and target at same position
            continue

        s = 1.0 - radius / nd
        dd *= s
        x1, y1 = src
        dx, dy = dd
        ax.arrow(x1, y1,
                 dx, dy,
                 lw=width, width=width,
                 head_length=head_length,
                 fc=edge_color, ec='none',
                 alpha=alpha, overhang=overhang)


def draw_graph(G,
               labels=None,
               node_colors=None,
               node_shapes=None,
               node_scale=1.0,
               edge_style='solid',
               edge_cmap=None,
               colorbar=False,
               vrange=None,
               layout=None,
               title=None,
               font_family='sans-serif',
               font_size=9,
               stretch_factor=1.0,
               edge_alpha=True,
               fig_size=None):
    """Draw a weighted graph with options to visualize link weights.

    The resulting diagram uses the rank of each node as its size, and the
    weight of each link (after discarding thresholded values, see below) as the
    link opacity.

    It maps edge weight to color as well as line opacity and thickness,
    allowing the color part to be hardcoded over a value range (to permit valid
    cross-figure comparisons for different graphs, so the same color
    corresponds to the same link weight even if each graph has a different
    range of weights).  The nodes sizes are proportional to their degree,
    computed as the sum of the weights of all their links.  The layout defaults
    to circular, but any nx layout function can be passed in, as well as a
    statically precomputed layout.

    Parameters
    ----------
    G : weighted graph
      The values must be of the form (v1,v2), with all v2 in [0,1].  v1 are
      used for colors, v2 for thickness/opacity.

    labels : list or dict, optional.
      An indexable object that maps nodes to strings.  If not given, the
      string form of each node is used as a label.  If False, no labels are
      drawn.

    node_colors : list or dict, optional.
      An indexable object that maps nodes to valid matplotlib color specs.  See
      matplotlib's plot() function for details.

    node_shapes : list or dict, optional.
      An indexable object that maps nodes to valid matplotlib shape specs.  See
      matplotlib's scatter() function for details.  If not given, circles are
      used.

    node_scale : float, optional
      A scale factor to globally stretch or shrink all nodes symbols by.

    edge_style : string, optional
      Line style for the edges, defaults to 'solid'.

    edge_cmap : matplotlib colormap, optional.
      A callable that returns valid color specs, like matplotlib colormaps.
      If not given, edges are colored black.

    colorbar : bool
      If true, automatically add a colorbar showing the mapping of graph weight
      values to colors.

    vrange : pair of floats
      If given, this indicates the total range of values that the weights can
      in principle occupy, and is used to set the lower/upper range of the
      colormap.  This allows you to set the range of multiple different figures
      to the same values, even if each individual graph has range variations,
      so that visual color comparisons across figures are valid.

    layout : function or layout dict, optional
      A NetworkX-like layout function or the result of a precomputed layout for
      the given graph.  NetworkX produces layouts as dicts keyed by nodes and
      with (x,y) pairs of coordinates as values, any function that produces
      this kind of output is acceptable.  Defaults to nx.circular_layout.

    title : string, optional.
      If given, title to put on the main plot.

    font_family : string, optional.
      Font family used for the node labels and title.

    font_size : int, optional.
      Font size used for the node labels and title.

    stretch_factor : float, optional
      A global scaling factor to make the graph larger (or smaller if <1).
      This can be used to separate the nodes if they start overlapping.

    edge_alpha: bool, optional
      Whether to weight the transparency of each edge by a factor equivalent to
      its relative weight

    fig_size: list of height by width, the size of the figure (in
    inches). Defaults to [6,6]

    Returns
    -------
    fig
      The matplotlib figure object with the plot.
    """
    if fig_size is None:
        figsize = [6, 6]

    scaler = figsize[0] / 6.
    # For the size of the node symbols
    node_size_base = 1000 * scaler
    node_min_size = 200 * scaler
    default_node_shape = 'o'
    # Default colors if none given
    default_node_color = 'r'
    default_edge_color = 'k'
    # Max edge width
    max_width = 13 * scaler
    min_width = 2 * scaler
    font_family = 'sans-serif'

    # We'll use the nodes a lot, let's make a numpy array of them
    nodes = np.array(sorted(G.nodes()))
    nnod = len(nodes)

    # Build a 'weighted degree' array obtained by adding the (absolute value)
    # of the weights for all edges pointing to each node:
    amat = nx.adj_matrix(G).A  # get a normal array out of it
    degarr = abs(amat).sum(0)  # weights are sums across rows

    # Map the degree to the 0-1 range so we can use it for sizing the nodes.
    try:
        odegree = rescale_arr(degarr, 0, 1)
        # Make an array of node sizes based on node degree
        node_sizes = odegree * node_size_base + node_min_size
    except ZeroDivisionError:
        # All nodes same size
        node_sizes = np.empty(nnod, float)
        node_sizes.fill(0.5 * node_size_base + node_min_size)

    # Adjust node size list.  We square the scale factor because in mpl, node
    # sizes represent area, not linear size, but it's more intuitive for the
    # user to think of linear factors (the overall figure scale factor is also
    # linear).
    node_sizes *= node_scale ** 2

    # Set default node properties
    if node_colors is None:
        node_colors = [default_node_color] * nnod

    if node_shapes is None:
        node_shapes = [default_node_shape] * nnod

    # Set default edge colormap
    if edge_cmap is None:
        # Make an object with the colormap API, that maps all input values to
        # the default color (with proper alhpa)
        edge_cmap = (lambda val, alpha:
                     colors.colorConverter.to_rgba(default_edge_color, alpha))

    # if vrange is None, we set the color range from the values, else the user
    # can specify it

    # e[2] is edge value: edges_iter returns (i,j,data)
    gvals = np.array([e[2]['weight'] for e in G.edges(data=True)])
    gvmin, gvmax = gvals.min(), gvals.max()

    gvrange = gvmax - gvmin
    if vrange is None:
        vrange = gvmin, gvmax
    # Now, construct the normalization for the colormap
    cnorm = mpl.colors.Normalize(vmin=vrange[0], vmax=vrange[1])

    # Create the actual plot where the graph will be displayed
    figsize = np.array(figsize, float)
    figsize *= stretch_factor

    fig = plt.figure(figsize=figsize)
    ax_graph = fig.add_subplot(1, 1, 1)
    fig.sca(ax_graph)

    if layout is None:
        layout = nx.circular_layout
    # Compute positions for all nodes - nx has several algorithms
    if callable(layout):
        pos = layout(G)
    else:
        # The user can also provide a precomputed layout
        pos = layout

    # Draw nodes
    for nod in nodes:
        nx.draw_networkx_nodes(G,
                               pos,
                               nodelist=[nod],
                               node_color=node_colors[nod],
                               node_shape=node_shapes[nod],
                               node_size=node_sizes[nod])
    # Draw edges
    if not isinstance(G, nx.DiGraph):
        # Undirected graph, simple lines for edges
        # We need the size of the value range to properly scale colors
        vsize = vrange[1] - vrange[0]
        gvals_normalized = G.metadata['vals_norm']
        for (u, v, y) in G.edges(data=True):
            # The graph value is the weight, and the normalized values are in
            # [0,1], used for thickness/transparency
            alpha = gvals_normalized[u, v]
            # Scale the color choice to the specified vrange, so that
            ecol = (y['weight'] - vrange[0]) / vsize
            #print 'u,v:',u,v,'y:',y,'ecol:',ecol  # dbg

            if edge_alpha:
                fade = alpha
            else:
                fade = 1.0

            edge_color = [tuple(edge_cmap(ecol, fade))]
            #dbg:
            #print u,v,y
            draw_networkx_edges(G,
                                pos,
                                edgelist=[(u, v)],
                                width=min_width + alpha * max_width,
                                edge_color=edge_color,
                                style=edge_style)
    else:
        # Directed graph, use arrows.
        # XXX - this is currently broken.
        raise NotImplementedError("arrow drawing currently broken")

        ## for (u,v,x) in G.edges(data=True):
        ##     y,w = x
        ##     draw_arrows(G,pos,edgelist=[(u,v)],
        ##                 edge_color=[w],
        ##                 alpha=w,
        ##                 edge_cmap=edge_cmap,
        ##                 width=w*max_width)

    # Draw labels.  If not given, we use the string form of the nodes.  If
    # labels is False, no labels are drawn.
    if labels is None:
        labels = map(str, nodes)

    if labels:
        lab_idx = list(range(len(labels)))
        labels_dict = dict(zip(lab_idx, labels))
        nx.draw_networkx_labels(G,
                                pos,
                                labels_dict,
                                font_size=font_size,
                                font_family=font_family)

    if title:
        plt.title(title, fontsize=font_size)

    # Turn off x and y axes labels in pylab
    plt.xticks([])
    plt.yticks([])

    # Add a colorbar if requested
    if colorbar:
        divider = make_axes_locatable(ax_graph)
        ax_cb = divider.new_vertical(size="20%", pad=0.2, pack_start=True)
        fig.add_axes(ax_cb)
        cb = mpl.colorbar.ColorbarBase(ax_cb,
                                    cmap=edge_cmap,
                                    norm=cnorm,
                                    #boundaries = np.linspace(min((gvmin,0)),
                                    #                         max((gvmax,0)),
                                    #                         256),
                                    orientation='horizontal',
                                    format='%.2f')

    # Always return the MPL figure object so the user can further manipulate it
    return fig


def lab2node(labels, labels_dict):
    return [labels_dict[ll] for ll in labels]


### Patched version for networx draw_networkx_edges, sent to Aric.
def draw_networkx_edges(G, pos,
                        edgelist=None,
                        width=1.0,
                        edge_color='k',
                        style='solid',
                        alpha=None,
                        edge_cmap=None,
                        edge_vmin=None,
                        edge_vmax=None,
                        ax=None,
                        arrows=True,
                        **kwds):
    """Draw the edges of the graph G

    This draws only the edges of the graph G.

    pos is a dictionary keyed by vertex with a two-tuple
    of x-y positions as the value.
    See networkx.layout for functions that compute node positions.

    edgelist is an optional list of the edges in G to be drawn.
    If provided, only the edges in edgelist will be drawn.

    edgecolor can be a list of matplotlib color letters such as 'k' or
    'b' that lists the color of each edge; the list must be ordered in
    the same way as the edge list. Alternatively, this list can contain
    numbers and those number are mapped to a color scale using the color
    map edge_cmap.  Finally, it can also be a list of (r,g,b) or (r,g,b,a)
    tuples, in which case these will be used directly to color the edges.  If
    the latter mode is used, you should not provide a value for alpha, as it
    would be applied globally to all lines.

    For directed graphs, 'arrows' (actually just thicker stubs) are drawn
    at the head end.  Arrows can be turned off with keyword arrows=False.

    See draw_networkx for the list of other optional parameters.

    """
    try:
        import matplotlib.pylab as pylab
        import matplotlib.cbook as cb
        from matplotlib.colors import colorConverter, Colormap
        from matplotlib.collections import LineCollection
    except ImportError:
        raise ImportError("Matplotlib required for draw()")
    except RuntimeError:
        pass  # unable to open display

    if ax is None:
        ax = pylab.gca()

    if edgelist is None:
        edgelist = G.edges()

    if not edgelist or len(edgelist) == 0:  # no edges!
        return None

    # set edge positions
    edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist])

    if not cb.iterable(width):
        lw = (width,)
    else:
        lw = width

    if not cb.is_string_like(edge_color) \
           and cb.iterable(edge_color) \
           and len(edge_color) == len(edge_pos):
        if np.alltrue([cb.is_string_like(c)
                         for c in edge_color]):
            # (should check ALL elements)
            # list of color letters such as ['k','r','k',...]
            edge_colors = tuple([colorConverter.to_rgba(c, alpha)
                                 for c in edge_color])
        elif np.alltrue([not cb.is_string_like(c)
                           for c in edge_color]):
            # If color specs are given as (rgb) or (rgba) tuples, we're OK
            if np.alltrue([cb.iterable(c) and len(c) in (3, 4)
                             for c in edge_color]):
                edge_colors = tuple(edge_color)
                alpha = None
            else:
                # numbers (which are going to be mapped with a colormap)
                edge_colors = None
        else:
            e_s = 'edge_color must consist of either color names or numbers'
            raise ValueError(e_s)
    else:
        if len(edge_color) == 1:
            edge_colors = (colorConverter.to_rgba(edge_color, alpha),)
        else:
            e_s = 'edge_color must be a single color or list of exactly'
            e_s += 'm colors where m is the number or edges'
            raise ValueError(e_s)
    edge_collection = LineCollection(edge_pos,
                                     colors=edge_colors,
                                     linewidths=lw,
                                     antialiaseds=(1,),
                                     linestyle=style,
                                     transOffset=ax.transData,
                                     )

    # Note: there was a bug in mpl regarding the handling of alpha values for
    # each line in a LineCollection.  It was fixed in matplotlib in r7184 and
    # r7189 (June 6 2009).  We should then not set the alpha value globally,
    # since the user can instead provide per-edge alphas now.  Only set it
    # globally if provided as a scalar.
    if cb.is_numlike(alpha):
        edge_collection.set_alpha(alpha)

    # need 0.87.7 or greater for edge colormaps
    if edge_colors is None:
        if edge_cmap is not None:
            assert(isinstance(edge_cmap, Colormap))
        edge_collection.set_array(np.asarray(edge_color))
        edge_collection.set_cmap(edge_cmap)
        if edge_vmin is not None or edge_vmax is not None:
            edge_collection.set_clim(edge_vmin, edge_vmax)
        else:
            edge_collection.autoscale()
        pylab.sci(edge_collection)

#    else:
#        sys.stderr.write(\
#            """matplotlib version >= 0.87.7 required for colormapped edges.
#        (version %s detected)."""%matplotlib.__version__)
#        raise UserWarning(\
#            """matplotlib version >= 0.87.7 required for colormapped edges.
#        (version %s detected)."""%matplotlib.__version__)

    arrow_collection = None

    if G.is_directed() and arrows:

        # a directed graph hack
        # draw thick line segments at head end of edge
        # waiting for someone else to implement arrows that will work
        arrow_colors = (colorConverter.to_rgba('k', alpha),)
        a_pos = []
        p = 1.0 - 0.25  # make head segment 25 percent of edge length
        for src, dst in edge_pos:
            x1, y1 = src
            x2, y2 = dst
            dx = x2 - x1  # x offset
            dy = y2 - y1  # y offset
            d = np.sqrt(float(dx ** 2 + dy ** 2))  # length of edge
            if d == 0:  # source and target at same position
                continue
            if dx == 0:  # vertical edge
                xa = x2
                ya = dy * p + y1
            if dy == 0:  # horizontal edge
                ya = y2
                xa = dx * p + x1
            else:
                theta = np.arctan2(dy, dx)
                xa = p * d * np.cos(theta) + x1
                ya = p * d * np.sin(theta) + y1

            a_pos.append(((xa, ya), (x2, y2)))

        arrow_collection = LineCollection(a_pos,
                                colors=arrow_colors,
                                linewidths=[4 * ww for ww in lw],
                                antialiaseds=(1,),
                                transOffset=ax.transData,
                                )

    # update view
    minx = np.amin(np.ravel(edge_pos[:, :, 0]))
    maxx = np.amax(np.ravel(edge_pos[:, :, 0]))
    miny = np.amin(np.ravel(edge_pos[:, :, 1]))
    maxy = np.amax(np.ravel(edge_pos[:, :, 1]))

    w = maxx - minx
    h = maxy - miny
    padx, pady = 0.05 * w, 0.05 * h
    corners = (minx - padx, miny - pady), (maxx + padx, maxy + pady)
    ax.update_datalim(corners)
    ax.autoscale_view()

    edge_collection.set_zorder(1)  # edges go behind nodes
    ax.add_collection(edge_collection)
    if arrow_collection:
        arrow_collection.set_zorder(1)  # edges go behind nodes
        ax.add_collection(arrow_collection)

    return ax


def mkgraph(cmat, threshold=0.0, threshold2=None):
    """Make a weighted graph object out of an adjacency matrix.

    The values in the original matrix cmat can be thresholded out.  If only one
    threshold is given, all values below that are omitted when creating edges.
    If two thresholds are given, then values in the th2-th1 range are
    ommitted.  This allows for the easy creation of weighted graphs with
    positive and negative values where a range of weights around 0 is omitted.

    Parameters
    ----------
    cmat : 2-d square array
      Adjacency matrix.
    threshold : float
      First threshold.
    threshold2 : float
      Second threshold.

    Returns
    -------
    G : a NetworkX weighted graph object, to which a dictionary called
    G.metadata is appended.  This dict contains the original adjacency matrix
    cmat, the two thresholds, and the weights
    """

    # Input sanity check
    nrow, ncol = cmat.shape
    if nrow != ncol:
        raise ValueError("Adjacency matrix must be square")

    row_idx, col_idx, vals = threshold_arr(cmat, threshold, threshold2)
    # Also make the full thresholded array available in the metadata
    cmat_th = np.empty_like(cmat)
    if threshold2 is None:
        cmat_th.fill(threshold)
    else:
        cmat_th.fill(-np.inf)
    cmat_th[row_idx, col_idx] = vals

    # Next, make a normalized copy of the values.  For the 2-threshold case, we
    # use 'folding' normalization
    if threshold2 is None:
        vals_norm = minmax_norm(vals)
    else:
        vals_norm = minmax_norm(vals, 'folding', [threshold, threshold2])

    # Now make the actual graph
    G = nx.Graph(weighted=True)
    G.add_nodes_from(list(range(nrow)))
    # To keep the weights of the graph to simple values, we store the
    # normalize ones in a separate dict that we'll stuff into the graph
    # metadata.

    normed_values = {}
    for i, j, val, nval in zip(row_idx, col_idx, vals, vals_norm):
        if i == j:
            # no self-loops
            continue
        G.add_edge(i, j, weight=val)
        normed_values[i, j] = nval

    # Write a metadata dict into the graph and save the threshold info there
    G.metadata = dict(threshold1=threshold,
                      threshold2=threshold2,
                      cmat_raw=cmat,
                      cmat_th=cmat_th,
                      vals_norm=normed_values,
                      )
    return G


def plot_snr(tseries, lb=0, ub=None, fig=None):
    """
    Show the coherence, snr and information of an SNRAnalyzer

    Parameters
    ----------
    tseries: nitime TimeSeries object
       Multi-trial data in response to one stimulus/protocol with the dims:
       (n_channels,n_repetitions,time)

    lb,ub: float
       Lower and upper bounds on the frequency range over which to
       calculate (default to [0,Nyquist]).

    Returns
    -------

    A tuple containing:

    fig: a matplotlib figure object
        This figure displays:
        1. Coherence
        2. SNR
        3. Information
    """

    if fig is None:
        fig = plt.figure()

    ax_spectra = fig.add_subplot(1, 2, 1)
    ax_snr_info = fig.add_subplot(1, 2, 2)

    A = []
    info = []
    s_n_r = []
    coh = []
    noise_spectra = []
    signal_spectra = []
    #If you only have one channel, make sure that everything still works by
    #adding an axis
    if len(tseries.data.shape) < 3:
        this = tseries.data[np.newaxis, :, :]
    else:
        this = tseries.data

    for i in range(this.shape[0]):
        A.append(nta.SNRAnalyzer(ts.TimeSeries(this[i],
                                    sampling_rate=tseries.sampling_rate)))
        info.append(A[-1].mt_information)
        s_n_r.append(A[-1].mt_snr)
        coh.append(A[-1].mt_coherence)
        noise_spectra.append(A[-1].mt_noise_psd)
        signal_spectra.append(A[-1].mt_signal_psd)

    freqs = A[-1].mt_frequencies

    lb_idx, ub_idx = tsu.get_bounds(freqs, lb, ub)
    freqs = freqs[lb_idx:ub_idx]

    coh_mean = np.mean(coh, 0)
    snr_mean = np.mean(s_n_r, 0)
    info_mean = np.mean(info, 0)
    n_spec_mean = np.mean(noise_spectra, 0)
    s_spec_mean = np.mean(signal_spectra, 0)

    ax_spectra.plot(freqs, np.log(s_spec_mean[lb_idx:ub_idx]), label='Signal')
    ax_spectra.plot(freqs, np.log(n_spec_mean[lb_idx:ub_idx]), label='Noise')
    ax_spectra.set_xlabel('Frequency (Hz)')
    ax_spectra.set_ylabel('Spectral power (dB)')

    ax_snr_info.plot(freqs, snr_mean[lb_idx:ub_idx], label='SNR')
    ax_snr_info.plot(np.nan, np.nan, 'r', label='Info')
    ax_snr_info.set_ylabel('SNR')
    ax_snr_info.set_xlabel('Frequency (Hz)')
    ax_info = ax_snr_info.twinx()
    ax_info.plot(freqs, np.cumsum(info_mean[lb_idx:ub_idx]), 'r')
    ax_info.set_ylabel('Cumulative information rate (bits/sec)')
    return fig


def plot_snr_diff(tseries1, tseries2, lb=0, ub=None, fig=None,
                  ts_names=['1', '2'],
                  bandwidth=None, adaptive=False, low_bias=True):

    """
    Show distributions of differences between two time-series in the
    amount of snr (freq band by freq band) and information. For example,
    for comparing two stimulus conditions

    Parameters
    ----------
    tseries1, tseries2 : nitime TimeSeries objects
       These are the time-series to compare, with each of them having the
       dims: (n_channels, n_reps, time), where n_channels1 = n_channels2

    lb,ub: float
       Lower and upper bounds on the frequency range over which to
       calculate the information rate (default to [0,Nyquist]).

    fig: matplotlib figure object
       If you want to do this on already existing figure. Otherwise, a new
       figure object will be generated.

    ts_names: list of str
       Labels for the two inputs, to be used in plotting (defaults to
       ['1','2'])

    bandwidth, adaptive, low_bias: See :func:`nta.SNRAnalyzer` for details


    Returns
    -------

    A tuple containing:

    fig: a matplotlib figure object
        This figure displays:
        1. The histogram of the information differences between the two
        time-series
        2. The frequency-dependent SNR for the two time-series

    info1, info2: float arrays
        The frequency-dependent information rates (in bits/sec)

    s_n_r1, s_n_r2: float arrays
         The frequncy-dependent signal-to-noise ratios

    """
    if fig is None:
        fig = plt.figure()
    ax_scatter = fig.add_subplot(1, 2, 1)
    ax_snr = fig.add_subplot(1, 2, 2)

    SNR1 = []
    s_n_r1 = []
    info1 = []
    SNR2 = []
    info2 = []
    s_n_r2 = []

    #If you only have one channel, make sure that everything still works by
    #adding an axis
    if len(tseries1.data.shape) < 3:
        this1 = tseries1.data[np.newaxis, :, :]
        this2 = tseries2.data[np.newaxis, :, :]
    else:
        this1 = tseries1.data
        this2 = tseries2.data

    for i in range(this1.shape[0]):
        SNR1.append(nta.SNRAnalyzer(ts.TimeSeries(this1[i],
                                    sampling_rate=tseries1.sampling_rate),
                                bandwidth=bandwidth,
                                adaptive=adaptive,
                                low_bias=low_bias))
        info1.append(SNR1[-1].mt_information)
        s_n_r1.append(SNR1[-1].mt_snr)

        SNR2.append(nta.SNRAnalyzer(ts.TimeSeries(this2[i],
                                    sampling_rate=tseries2.sampling_rate),
                                bandwidth=bandwidth,
                                adaptive=adaptive,
                                low_bias=low_bias))

        info2.append(SNR2[-1].mt_information)
        s_n_r2.append(SNR2[-1].mt_snr)

    freqs = SNR1[-1].mt_frequencies

    lb_idx, ub_idx = tsu.get_bounds(freqs, lb, ub)
    freqs = freqs[lb_idx:ub_idx]

    info1 = np.array(info1)
    info_sum1 = np.sum(info1[:, lb_idx:ub_idx], -1)
    info2 = np.array(info2)
    info_sum2 = np.sum(info2[:, lb_idx:ub_idx], -1)

    ax_scatter.scatter(info_sum1, info_sum2)
    ax_scatter.errorbar(np.mean(info_sum1), np.mean(info_sum2),
                 yerr=np.std(info_sum2),
                 xerr=np.std(info_sum1))

    plot_min = min(min(info_sum1), min(info_sum2))
    plot_max = max(max(info_sum1), max(info_sum2))
    ax_scatter.plot([plot_min, plot_max], [plot_min, plot_max], 'k--')
    ax_scatter.set_xlabel('Information %s (bits/sec)' % ts_names[0])
    ax_scatter.set_ylabel('Information %s (bits/sec)' % ts_names[1])

    snr_mean1 = np.mean(s_n_r1, 0)
    snr_mean2 = np.mean(s_n_r2, 0)

    ax_snr.plot(freqs, snr_mean1[lb_idx:ub_idx], label=ts_names[0])
    ax_snr.plot(freqs, snr_mean2[lb_idx:ub_idx], label=ts_names[1])
    ax_snr.legend()
    ax_snr.set_xlabel('Frequency (Hz)')
    ax_snr.set_ylabel('SNR')

    return fig, info1, info2, s_n_r1, s_n_r2


def plot_corr_diff(tseries1, tseries2, fig=None,
                  ts_names=['1', '2']):
    """
    Show the differences in *Fischer-transformed* snr correlations for two
    time-series

    Parameters
    ----------
    tseries1, tseries2 : nitime TimeSeries objects
       These are the time-series to compare, with each of them having the
       dims: (n_channels, n_reps, time), where n_channels1 = n_channels2

    lb,ub: float
       Lower and upper bounds on the frequency range over which to
       calculate the information rate (default to [0,Nyquist]).

    fig: matplotlib figure object
       If you want to do this on already existing figure. Otherwise, a new
       figure object will be generated.

    ts_names: list of str
       Labels for the two inputs, to be used in plotting (defaults to
       ['1','2'])

    bandwidth, adaptive, low_bias: See :func:`SNRAnalyzer` for details


    Returns
    -------

    fig: a matplotlib figure object
    """

    if fig is None:
        fig = plt.figure()

    ax = fig.add_subplot(1, 1, 1)

    SNR1 = []
    SNR2 = []
    corr1 = []
    corr2 = []
    corr_e1 = []
    corr_e2 = []

    for i in range(tseries1.shape[0]):
        SNR1.append(nta.SNRAnalyzer(ts.TimeSeries(tseries1.data[i],
                                    sampling_rate=tseries1.sampling_rate)))

        corr1.append(np.arctanh(np.abs(SNR1[-1].correlation[0])))
        corr_e1.append(SNR1[-1].correlation[1])

        SNR2.append(nta.SNRAnalyzer(ts.TimeSeries(tseries2.data[i],
                                    sampling_rate=tseries2.sampling_rate)))

        corr2.append(np.arctanh(np.abs(SNR2[-1].correlation[0])))
        corr_e2.append(SNR1[-1].correlation[1])

    ax.scatter(np.array(corr1), np.array(corr2))
    ax.errorbar(np.mean(corr1), np.mean(corr2),
                 yerr=np.std(corr2),
                 xerr=np.std(corr1))
    plot_min = min(min(corr1), min(corr2))
    plot_max = max(max(corr1), max(corr2))
    ax.plot([plot_min, plot_max], [plot_min, plot_max], 'k--')
    ax.set_xlabel('Correlation (Fischer Z) %s' % ts_names[0])
    ax.set_ylabel('Correlation (Fischer Z) %s' % ts_names[1])

    return fig, corr1, corr2


def winspect(win, f, name=None):
    """
    Inspect a window by showing it and its spectrum

    Utility file used in building the documentation
    """
    npts = len(win)
    ax1, ax2 = f.add_subplot(1, 2, 1), f.add_subplot(1, 2, 2)
    ax1.plot(win)
    ax1.set_xlabel('Time')
    ax1.set_ylabel('Window amplitude')
    ax1.set_ylim(-0.1, 1.1)
    ax1.set_xlim(0, npts)
    wf = fftpack.fft(win)
    ax1.set_xticks(np.arange(npts / 8., npts, npts / 8.))
    toplot = np.abs(fftpack.fftshift(wf).real)
    toplot /= np.max(toplot)
    toplot = np.log(toplot)
    ax2.plot(toplot, label=name)
    ax2.set_xlim(0, npts)
    ax2.set_xticks(np.arange(npts / 8., npts, npts / 8.))
    ax2.set_xticklabels(np.arange((-1 / 2. + 1 / 8.), 1 / 2., 1 / 8.))
    ax2.set_xlabel('Relative frequency')
    ax2.set_ylabel('Relative attenuation (log scale)')
    ax2.grid()
    ax2.legend(loc=4)
    f.set_size_inches([10, 6])


def plot_spectral_estimate(f, sdf, sdf_ests, limits=None, elabels=()):
    """
    Plot an estimate of a spectral transform against the ground truth.

    Utility file used in building the documentation
    """
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    ax_limits = (sdf.min() - 2*np.abs(sdf.min()),
                 sdf.max() + 1.25*np.abs(sdf.max()))
    ax.plot(f, sdf, 'c', label='True S(f)')

    if not elabels:
        elabels = ('',) * len(sdf_ests)
    colors = 'bgkmy'
    for e, l, c in zip(sdf_ests, elabels, colors):
        ax.plot(f, e, color=c, linewidth=2, label=l)

    if limits is not None:
        ax.fill_between(f, limits[0], y2=limits[1], color=(1, 0, 0, .3),
                        alpha=0.5)

    ax.set_ylim(ax_limits)
    ax.legend()
    return fig

########NEW FILE########
__FILENAME__ = _mpl_units
"""

This is a fixed copy of a module from Matplotlib v1.3 (https://github.com/matplotlib/matplotlib/pull/2591).

It was taken verbatim from Matplotlib's github repository and is, as is all of
MPL v1.3.1, copyright (c) 2012-2013 Matplotlib Development Team; All Rights
Reserved. 

1. This LICENSE AGREEMENT is between the Matplotlib Development Team
("MDT"), and the Individual or Organization ("Licensee") accessing and
otherwise using matplotlib software in source or binary form and its
associated documentation.

2. Subject to the terms and conditions of this License Agreement, MDT
hereby grants Licensee a nonexclusive, royalty-free, world-wide license
to reproduce, analyze, test, perform and/or display publicly, prepare
derivative works, distribute, and otherwise use matplotlib 1.3.1
alone or in any derivative version, provided, however, that MDT's
License Agreement and MDT's notice of copyright, i.e., "Copyright (c)
2012-2013 Matplotlib Development Team; All Rights Reserved" are retained in
matplotlib 1.3.1 alone or in any derivative version prepared by
Licensee.

3. In the event Licensee prepares a derivative work that is based on or
incorporates matplotlib 1.3.1 or any part thereof, and wants to
make the derivative work available to others as provided herein, then
Licensee hereby agrees to include in any such work a brief summary of
the changes made to matplotlib 1.3.1.

4. MDT is making matplotlib 1.3.1 available to Licensee on an "AS
IS" basis.  MDT MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR
IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, MDT MAKES NO AND
DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS
FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB 1.3.1
WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.

5. MDT SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB
1.3.1 FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR
LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING
MATPLOTLIB 1.3.1, OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF
THE POSSIBILITY THEREOF.

6. This License Agreement will automatically terminate upon a material
breach of its terms and conditions.

7. Nothing in this License Agreement shall be deemed to create any
relationship of agency, partnership, or joint venture between MDT and
Licensee.  This License Agreement does not grant permission to use MDT
trademarks or trade name in a trademark sense to endorse or promote
products or services of Licensee, or any third party.

8. By copying, installing or otherwise using matplotlib 1.3.1,
Licensee agrees to be bound by the terms and conditions of this License
Agreement.


It is distributed under the following license:
The classes here provide support for using custom classes with
matplotlib, eg those that do not expose the array interface but know
how to converter themselves to arrays.  It also supoprts classes with
units and units conversion.  Use cases include converters for custom
objects, eg a list of datetime objects, as well as for objects that
are unit aware.  We don't assume any particular units implementation,
rather a units implementation must provide a ConversionInterface, and
the register with the Registry converter dictionary.  For example,
here is a complete implementation which supports plotting with native
datetime objects::


    import matplotlib.units as units
    import matplotlib.dates as dates
    import matplotlib.ticker as ticker
    import datetime

    class DateConverter(units.ConversionInterface):

        @staticmethod
        def convert(value, unit, axis):
            'convert value to a scalar or array'
            return dates.date2num(value)

        @staticmethod
        def axisinfo(unit, axis):
            'return major and minor tick locators and formatters'
            if unit!='date': return None
            majloc = dates.AutoDateLocator()
            majfmt = dates.AutoDateFormatter(majloc)
            return AxisInfo(majloc=majloc,
                            majfmt=majfmt,
                            label='date')

        @staticmethod
        def default_units(x, axis):
            'return the default unit for x or None'
            return 'date'

    # finally we register our object type with a converter
    units.registry[datetime.date] = DateConverter()

"""
from __future__ import print_function
from matplotlib.cbook import iterable, is_numlike
import numpy as np


class AxisInfo:
    """information to support default axis labeling and tick labeling, and
       default limits"""
    def __init__(self, majloc=None, minloc=None,
                 majfmt=None, minfmt=None, label=None,
                 default_limits=None):
        """
        majloc and minloc: TickLocators for the major and minor ticks
        majfmt and minfmt: TickFormatters for the major and minor ticks
        label: the default axis label
        default_limits: the default min, max of the axis if no data is present
        If any of the above are None, the axis will simply use the default
        """
        self.majloc = majloc
        self.minloc = minloc
        self.majfmt = majfmt
        self.minfmt = minfmt
        self.label = label
        self.default_limits = default_limits


class ConversionInterface:
    """
    The minimal interface for a converter to take custom instances (or
    sequences) and convert them to values mpl can use
    """
    @staticmethod
    def axisinfo(unit, axis):
        'return an units.AxisInfo instance for axis with the specified units'
        return None

    @staticmethod
    def default_units(x, axis):
        'return the default unit for x or None for the given axis'
        return None

    @staticmethod
    def convert(obj, unit, axis):
        """
        convert obj using unit for the specified axis.  If obj is a sequence,
        return the converted sequence.  The ouput must be a sequence of scalars
        that can be used by the numpy array layer
        """
        return obj

    @staticmethod
    def is_numlike(x):
        """
        The matplotlib datalim, autoscaling, locators etc work with
        scalars which are the units converted to floats given the
        current unit.  The converter may be passed these floats, or
        arrays of them, even when units are set.  Derived conversion
        interfaces may opt to pass plain-ol unitless numbers through
        the conversion interface and this is a helper function for
        them.
        """
        if iterable(x):
            for thisx in x:
                return is_numlike(thisx)
        else:
            return is_numlike(x)


class Registry(dict):
    """
    register types with conversion interface
    """
    def __init__(self):
        dict.__init__(self)
        self._cached = {}

    def get_converter(self, x):
        'get the converter interface instance for x, or None'

        if not len(self):
            return None  # nothing registered
        #DISABLED idx = id(x)
        #DISABLED cached = self._cached.get(idx)
        #DISABLED if cached is not None: return cached

        converter = None
        classx = getattr(x, '__class__', None)

        if classx is not None:
            converter = self.get(classx)

        if isinstance(x, np.ndarray) and x.size:
            xravel = x.ravel()
            try:
                # pass the first value of x that is not masked back to
                # get_converter
                if not np.all(xravel.mask):
                    # some elements are not masked
                    converter = self.get_converter(
                        xravel[np.argmin(xravel.mask)])
                    return converter
            except AttributeError:
                # not a masked_array
                # Make sure we don't recurse forever -- it's possible for
                # ndarray subclasses to continue to return subclasses and
                # not ever return a non-subclass for a single element.
                next_item = xravel[0]
                if (not isinstance(next_item, np.ndarray) or
                    next_item.shape != x.shape):
                    converter = self.get_converter(next_item)
                return converter
            
        if converter is None and iterable(x):
            for thisx in x:
                # Make sure that recursing might actually lead to a solution,
                # if we are just going to re-examine another item of the same
                # kind, then do not look at it.
                if classx and classx != getattr(thisx, '__class__', None):
                    converter = self.get_converter(thisx)
                    return converter

        #DISABLED self._cached[idx] = converter
        return converter


registry = Registry()

########NEW FILE########
__FILENAME__ = apigen
"""Attempt to generate templates for module reference with Sphinx

XXX - we exclude extension modules

To include extension modules, first identify them as valid in the
``_uri2path`` method, then handle them in the ``_parse_module`` script.

We get functions and classes by parsing the text of .py files.
Alternatively we could import the modules for discovery, and we'd have
to do that for extension modules.  This would involve changing the
``_parse_module`` method to work via import and introspection, and
might involve changing ``discover_modules`` (which determines which
files are modules, and therefore which module URIs will be passed to
``_parse_module``).

NOTE: this is a modified version of a script originally shipped with the
PyMVPA project, which we've adapted for NIPY use.  PyMVPA is an MIT-licensed
project."""

# Stdlib imports
import os
import re

# Functions and classes
class ApiDocWriter(object):
    ''' Class for automatic detection and parsing of API docs
    to Sphinx-parsable reST format'''

    # only separating first two levels
    rst_section_levels = ['*', '=', '-', '~', '^']

    def __init__(self,
                 package_name,
                 rst_extension='.rst',
                 package_skip_patterns=None,
                 module_skip_patterns=None,
                 ):
        ''' Initialize package for parsing

        Parameters
        ----------
        package_name : string
            Name of the top-level package.  *package_name* must be the
            name of an importable package
        rst_extension : string, optional
            Extension for reST files, default '.rst'
        package_skip_patterns : None or sequence of {strings, regexps}
            Sequence of strings giving URIs of packages to be excluded
            Operates on the package path, starting at (including) the
            first dot in the package path, after *package_name* - so,
            if *package_name* is ``sphinx``, then ``sphinx.util`` will
            result in ``.util`` being passed for earching by these
            regexps.  If is None, gives default. Default is:
            ['\.tests$']
        module_skip_patterns : None or sequence
            Sequence of strings giving URIs of modules to be excluded
            Operates on the module name including preceding URI path,
            back to the first dot after *package_name*.  For example
            ``sphinx.util.console`` results in the string to search of
            ``.util.console``
            If is None, gives default. Default is:
            ['\.setup$', '\._']
        '''
        if package_skip_patterns is None:
            package_skip_patterns = ['\\.tests$']
        if module_skip_patterns is None:
            module_skip_patterns = ['\\.setup$', '\\._']
        self.package_name = package_name
        self.rst_extension = rst_extension
        self.package_skip_patterns = package_skip_patterns
        self.module_skip_patterns = module_skip_patterns

    def get_package_name(self):
        return self._package_name

    def set_package_name(self, package_name):
        ''' Set package_name

        >>> docwriter = ApiDocWriter('sphinx')
        >>> import sphinx
        >>> docwriter.root_path == sphinx.__path__[0]
        True
        >>> docwriter.package_name = 'docutils'
        >>> import docutils
        >>> docwriter.root_path == docutils.__path__[0]
        True
        '''
        # It's also possible to imagine caching the module parsing here
        self._package_name = package_name
        self.root_module = __import__(package_name)
        self.root_path = self.root_module.__path__[0]
        self.written_modules = None

    package_name = property(get_package_name, set_package_name, None,
                            'get/set package_name')

    def _get_object_name(self, line):
        ''' Get second token in line
        >>> docwriter = ApiDocWriter('sphinx')
        >>> docwriter._get_object_name("  def func():  ")
        'func'
        >>> docwriter._get_object_name("  class Klass(object):  ")
        'Klass'
        >>> docwriter._get_object_name("  class Klass:  ")
        'Klass'
        '''
        name = line.split()[1].split('(')[0].strip()
        # in case we have classes which are not derived from object
        # ie. old style classes
        return name.rstrip(':')

    def _uri2path(self, uri):
        ''' Convert uri to absolute filepath

        Parameters
        ----------
        uri : string
            URI of python module to return path for

        Returns
        -------
        path : None or string
            Returns None if there is no valid path for this URI
            Otherwise returns absolute file system path for URI

        Examples
        --------
        >>> docwriter = ApiDocWriter('sphinx')
        >>> import sphinx
        >>> modpath = sphinx.__path__[0]
        >>> res = docwriter._uri2path('sphinx.builder')
        >>> res == os.path.join(modpath, 'builder.py')
        True
        >>> res = docwriter._uri2path('sphinx')
        >>> res == os.path.join(modpath, '__init__.py')
        True
        >>> docwriter._uri2path('sphinx.does_not_exist')

        '''
        if uri == self.package_name:
            return os.path.join(self.root_path, '__init__.py')
        path = uri.replace('.', os.path.sep)
        path = path.replace(self.package_name + os.path.sep, '')
        path = os.path.join(self.root_path, path)
        # XXX maybe check for extensions as well?
        if os.path.exists(path + '.py'): # file
            path += '.py'
        elif os.path.exists(os.path.join(path, '__init__.py')):
            path = os.path.join(path, '__init__.py')
        else:
            return None
        return path

    def _path2uri(self, dirpath):
        ''' Convert directory path to uri '''
        relpath = dirpath.replace(self.root_path, self.package_name)
        if relpath.startswith(os.path.sep):
            relpath = relpath[1:]
        return relpath.replace(os.path.sep, '.')

    def _parse_module(self, uri):
        ''' Parse module defined in *uri* '''
        filename = self._uri2path(uri)
        if filename is None:
            # nothing that we could handle here.
            return ([],[])
        f = open(filename, 'rt')
        functions, classes = self._parse_lines(f)
        f.close()
        return functions, classes
    
    def _parse_lines(self, linesource):
        ''' Parse lines of text for functions and classes '''
        functions = []
        classes = []
        for line in linesource:
            if line.startswith('def ') and line.count('('):
                # exclude private stuff
                name = self._get_object_name(line)
                if not name.startswith('_'):
                    functions.append(name)
            elif line.startswith('class '):
                # exclude private stuff
                name = self._get_object_name(line)
                if not name.startswith('_'):
                    classes.append(name)
            else:
                pass
        functions.sort()
        classes.sort()
        return functions, classes

    def generate_api_doc(self, uri):
        '''Make autodoc documentation template string for a module

        Parameters
        ----------
        uri : string
            python location of module - e.g 'sphinx.builder'

        Returns
        -------
        S : string
            Contents of API doc
        '''
        # get the names of all classes and functions
        functions, classes = self._parse_module(uri)
        if not len(functions) and not len(classes):
            print 'WARNING: Empty -',uri  # dbg
            return ''

        # Make a shorter version of the uri that omits the package name for
        # titles 
        uri_short = re.sub(r'^%s\.' % self.package_name,'',uri)
        
        ad = '.. AUTO-GENERATED FILE -- DO NOT EDIT!\n\n'

        chap_title = uri_short
        ad += (chap_title+'\n'+ self.rst_section_levels[1] * len(chap_title)
               + '\n\n')

        # Set the chapter title to read 'module' for all modules except for the
        # main packages
        if '.' in uri:
            title = 'Module: :mod:`' + uri_short + '`'
        else:
            title = ':mod:`' + uri_short + '`'
        ad += title + '\n' + self.rst_section_levels[2] * len(title)

        if len(classes):
            ad += '\nInheritance diagram for ``%s``:\n\n' % uri
            ad += '.. inheritance-diagram:: %s \n' % uri
            ad += '   :parts: 3\n'

        ad += '\n.. automodule:: ' + uri + '\n'
        ad += '\n.. currentmodule:: ' + uri + '\n'
        multi_class = len(classes) > 1
        multi_fx = len(functions) > 1
        if multi_class:
            ad += '\n' + 'Classes' + '\n' + \
                  self.rst_section_levels[2] * 7 + '\n'
        elif len(classes) and multi_fx:
            ad += '\n' + 'Class' + '\n' + \
                  self.rst_section_levels[2] * 5 + '\n'
        for c in classes:
            ad += '\n:class:`' + c + '`\n' \
                  + self.rst_section_levels[multi_class + 2 ] * \
                  (len(c)+9) + '\n\n'
            ad += '\n.. autoclass:: ' + c + '\n'
            # must NOT exclude from index to keep cross-refs working
            ad += '  :members:\n' \
                  '  :undoc-members:\n' \
                  '  :show-inheritance:\n' \
                  '\n' \
                  '  .. automethod:: __init__\n'
        if multi_fx:
            ad += '\n' + 'Functions' + '\n' + \
                  self.rst_section_levels[2] * 9 + '\n\n'
        elif len(functions) and multi_class:
            ad += '\n' + 'Function' + '\n' + \
                  self.rst_section_levels[2] * 8 + '\n\n'
        for f in functions:
            # must NOT exclude from index to keep cross-refs working
            ad += '\n.. autofunction:: ' + uri + '.' + f + '\n\n'
        return ad

    def _survives_exclude(self, matchstr, match_type):
        ''' Returns True if *matchstr* does not match patterns

        ``self.package_name`` removed from front of string if present

        Examples
        --------
        >>> dw = ApiDocWriter('sphinx')
        >>> dw._survives_exclude('sphinx.okpkg', 'package')
        True
        >>> dw.package_skip_patterns.append('^\\.badpkg$')
        >>> dw._survives_exclude('sphinx.badpkg', 'package')
        False
        >>> dw._survives_exclude('sphinx.badpkg', 'module')
        True
        >>> dw._survives_exclude('sphinx.badmod', 'module')
        True
        >>> dw.module_skip_patterns.append('^\\.badmod$')
        >>> dw._survives_exclude('sphinx.badmod', 'module')
        False
        '''
        if match_type == 'module':
            patterns = self.module_skip_patterns
        elif match_type == 'package':
            patterns = self.package_skip_patterns
        else:
            raise ValueError('Cannot interpret match type "%s"' 
                             % match_type)
        # Match to URI without package name
        L = len(self.package_name)
        if matchstr[:L] == self.package_name:
            matchstr = matchstr[L:]
        for pat in patterns:
            try:
                pat.search
            except AttributeError:
                pat = re.compile(pat)
            if pat.search(matchstr):
                return False
        return True

    def discover_modules(self):
        ''' Return module sequence discovered from ``self.package_name`` 


        Parameters
        ----------
        None

        Returns
        -------
        mods : sequence
            Sequence of module names within ``self.package_name``

        Examples
        --------
        >>> dw = ApiDocWriter('sphinx')
        >>> mods = dw.discover_modules()
        >>> 'sphinx.util' in mods
        True
        >>> dw.package_skip_patterns.append('\.util$')
        >>> 'sphinx.util' in dw.discover_modules()
        False
        >>> 
        '''
        modules = [self.package_name]
        # raw directory parsing
        for dirpath, dirnames, filenames in os.walk(self.root_path):
            # Check directory names for packages
            root_uri = self._path2uri(os.path.join(self.root_path,
                                                   dirpath))
            for dirname in dirnames[:]: # copy list - we modify inplace
                package_uri = '.'.join((root_uri, dirname))
                if (self._uri2path(package_uri) and
                    self._survives_exclude(package_uri, 'package')):
                    modules.append(package_uri)
                else:
                    dirnames.remove(dirname)
            # Check filenames for modules
            for filename in filenames:
                module_name = filename[:-3]
                module_uri = '.'.join((root_uri, module_name))
                if (self._uri2path(module_uri) and
                    self._survives_exclude(module_uri, 'module')):
                    modules.append(module_uri)
        return sorted(modules)
    
    def write_modules_api(self, modules,outdir):
        # write the list
        written_modules = []
        for m in modules:
            api_str = self.generate_api_doc(m)
            if not api_str:
                continue
            # write out to file
            outfile = os.path.join(outdir,
                                   m + self.rst_extension)
            fileobj = open(outfile, 'wt')
            fileobj.write(api_str)
            fileobj.close()
            written_modules.append(m)
        self.written_modules = written_modules

    def write_api_docs(self, outdir):
        """Generate API reST files.

        Parameters
        ----------
        outdir : string
            Directory name in which to store files
            We create automatic filenames for each module
            
        Returns
        -------
        None

        Notes
        -----
        Sets self.written_modules to list of written modules
        """
        if not os.path.exists(outdir):
            os.mkdir(outdir)
        # compose list of modules
        modules = self.discover_modules()
        self.write_modules_api(modules,outdir)
        
    def write_index(self, outdir, froot='gen', relative_to=None):
        """Make a reST API index file from written files

        Parameters
        ----------
        path : string
            Filename to write index to
        outdir : string
            Directory to which to write generated index file
        froot : string, optional
            root (filename without extension) of filename to write to
            Defaults to 'gen'.  We add ``self.rst_extension``.
        relative_to : string
            path to which written filenames are relative.  This
            component of the written file path will be removed from
            outdir, in the generated index.  Default is None, meaning,
            leave path as it is.
        """
        if self.written_modules is None:
            raise ValueError('No modules written')
        # Get full filename path
        path = os.path.join(outdir, froot+self.rst_extension)
        # Path written into index is relative to rootpath
        if relative_to is not None:
            relpath = outdir.replace(relative_to + os.path.sep, '')
        else:
            relpath = outdir
        idx = open(path,'wt')
        w = idx.write
        w('.. AUTO-GENERATED FILE -- DO NOT EDIT!\n\n')
        w('.. toctree::\n\n')
        for f in self.written_modules:
            w('   %s\n' % os.path.join(relpath,f))
        idx.close()

########NEW FILE########
__FILENAME__ = build_modref_templates
#!/usr/bin/env python
"""Script to auto-generate our API docs.
"""
# stdlib imports
import os

# local imports
from apigen import ApiDocWriter

#*****************************************************************************
if __name__ == '__main__':
    package = 'nitime'
    outdir = os.path.join('api','generated')
    docwriter = ApiDocWriter(package)
    docwriter.package_skip_patterns += [r'\.fixes$',
                                        ]
    docwriter.write_api_docs(outdir)
    docwriter.write_index(outdir, 'gen', relative_to='api')
    print '%d files written' % len(docwriter.written_modules)

########NEW FILE########
__FILENAME__ = github_stats
#!/usr/bin/env python
"""Simple tools to query github.com and gather stats about issues.
"""
#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

from __future__ import print_function

import json
import re
import sys

from datetime import datetime, timedelta
from urllib import urlopen

#-----------------------------------------------------------------------------
# Globals
#-----------------------------------------------------------------------------

ISO8601 = "%Y-%m-%dT%H:%M:%SZ"
PER_PAGE = 100

element_pat = re.compile(r'<(.+?)>')
rel_pat = re.compile(r'rel=[\'"](\w+)[\'"]')

#-----------------------------------------------------------------------------
# Functions
#-----------------------------------------------------------------------------

def parse_link_header(headers):
    link_s = headers.get('link', '')
    urls = element_pat.findall(link_s)
    rels = rel_pat.findall(link_s)
    d = {}
    for rel,url in zip(rels, urls):
        d[rel] = url
    return d

def get_paged_request(url):
    """get a full list, handling APIv3's paging"""
    results = []
    while url:
        print("fetching %s" % url, file=sys.stderr)
        f = urlopen(url)
        results.extend(json.load(f))
        links = parse_link_header(f.headers)
        url = links.get('next')
    return results

def get_issues(project="nipy/nitime", state="closed", pulls=False):
    """Get a list of the issues from the Github API."""
    which = 'pulls' if pulls else 'issues'
    url = "https://api.github.com/repos/%s/%s?state=%s&per_page=%i" % (project, which, state, PER_PAGE)
    return get_paged_request(url)


def _parse_datetime(s):
    """Parse dates in the format returned by the Github API."""
    if s:
        return datetime.strptime(s, ISO8601)
    else:
        return datetime.fromtimestamp(0)


def issues2dict(issues):
    """Convert a list of issues to a dict, keyed by issue number."""
    idict = {}
    for i in issues:
        idict[i['number']] = i
    return idict


def is_pull_request(issue):
    """Return True if the given issue is a pull request."""
    return 'pull_request_url' in issue


def issues_closed_since(period=timedelta(days=365), project="nipy/nitime", pulls=False):
    """Get all issues closed since a particular point in time. period
can either be a datetime object, or a timedelta object. In the
latter case, it is used as a time before the present."""

    which = 'pulls' if pulls else 'issues'

    if isinstance(period, timedelta):
        period = datetime.now() - period
    url = "https://api.github.com/repos/%s/%s?state=closed&sort=updated&since=%s&per_page=%i" % (project, which, period.strftime(ISO8601), PER_PAGE)
    allclosed = get_paged_request(url)
    # allclosed = get_issues(project=project, state='closed', pulls=pulls, since=period)
    filtered = [i for i in allclosed if _parse_datetime(i['closed_at']) > period]
    return filtered


def sorted_by_field(issues, field='closed_at', reverse=False):
    """Return a list of issues sorted by closing date date."""
    return sorted(issues, key = lambda i:i[field], reverse=reverse)


def report(issues, show_urls=False):
    """Summary report about a list of issues, printing number and title.
    """
    # titles may have unicode in them, so we must encode everything below
    if show_urls:
        for i in issues:
            role = 'ghpull' if 'merged' in i else 'ghissue'
            print('* :%s:`%d`: %s' % (role, i['number'],
                                        i['title'].encode('utf-8')))
    else:
        for i in issues:
            print('* %d: %s' % (i['number'], i['title'].encode('utf-8')))

#-----------------------------------------------------------------------------
# Main script
#-----------------------------------------------------------------------------

if __name__ == "__main__":
    # Whether to add reST urls for all issues in printout.
    show_urls = True

    # By default, search one month back
    if len(sys.argv) > 1:
        days = int(sys.argv[1])
    else:
        days = 30

    # turn off to play interactively without redownloading, use %run -i
    if 1:
        issues = issues_closed_since(timedelta(days=days), pulls=False)
        pulls = issues_closed_since(timedelta(days=days), pulls=True)

    # For regular reports, it's nice to show them in reverse chronological order
    issues = sorted_by_field(issues, reverse=True)
    pulls = sorted_by_field(pulls, reverse=True)

    n_issues, n_pulls = map(len, (issues, pulls))
    n_total = n_issues + n_pulls

    # Print summary report we can directly include into release notes.
    print("GitHub stats for the last  %d days." % days)
    print("We closed a total of %d issues, %d pull requests and %d regular \n"
          "issues; this is the full list (generated with the script \n"
          "`tools/github_stats.py`):" % (n_total, n_pulls, n_issues))
    print()
    print('Pull Requests (%d):\n' % n_pulls)
    report(pulls, show_urls)
    print()
    print('Issues (%d):\n' % n_issues)
    report(issues, show_urls)

########NEW FILE########
__FILENAME__ = gitwash_dumper
#!/usr/bin/env python
''' Checkout gitwash repo into directory and do search replace on name '''

import os
from os.path import join as pjoin
import shutil
import sys
import re
import glob
import fnmatch
import tempfile
from subprocess import call


verbose = False


def clone_repo(url, branch):
    cwd = os.getcwd()
    tmpdir = tempfile.mkdtemp()
    try:
        cmd = 'git clone %s %s' % (url, tmpdir)
        call(cmd, shell=True)
        os.chdir(tmpdir)
        cmd = 'git checkout %s' % branch
        call(cmd, shell=True)
    except:
        shutil.rmtree(tmpdir)
        raise
    finally:
        os.chdir(cwd)
    return tmpdir


def cp_files(in_path, globs, out_path):
    try:
        os.makedirs(out_path)
    except OSError:
        pass
    out_fnames = []
    for in_glob in globs:
        in_glob_path = pjoin(in_path, in_glob)
        for in_fname in glob.glob(in_glob_path):
            out_fname = in_fname.replace(in_path, out_path)
            pth, _ = os.path.split(out_fname)
            if not os.path.isdir(pth):
                os.makedirs(pth)
            shutil.copyfile(in_fname, out_fname)
            out_fnames.append(out_fname)
    return out_fnames


def filename_search_replace(sr_pairs, filename, backup=False):
    ''' Search and replace for expressions in files

    '''
    in_txt = open(filename, 'rt').read(-1)
    out_txt = in_txt[:]
    for in_exp, out_exp in sr_pairs:
        in_exp = re.compile(in_exp)
        out_txt = in_exp.sub(out_exp, out_txt)
    if in_txt == out_txt:
        return False
    open(filename, 'wt').write(out_txt)
    if backup:
        open(filename + '.bak', 'wt').write(in_txt)
    return True

        
def copy_replace(replace_pairs,
                 out_path,
                 repo_url,
                 repo_branch = 'master',
                 cp_globs=('*',),
                 rep_globs=('*',),
                 renames = ()):
    repo_path = clone_repo(repo_url, repo_branch)
    try:
        out_fnames = cp_files(repo_path, cp_globs, out_path)
    finally:
        shutil.rmtree(repo_path)
    renames = [(re.compile(in_exp), out_exp) for in_exp, out_exp in renames]
    fnames = []
    for rep_glob in rep_globs:
        fnames += fnmatch.filter(out_fnames, rep_glob)
    if verbose:
        print '\n'.join(fnames)
    for fname in fnames:
        filename_search_replace(replace_pairs, fname, False)
        for in_exp, out_exp in renames:
            new_fname, n = in_exp.subn(out_exp, fname)
            if n:
                os.rename(fname, new_fname)
                break

            
USAGE = ''' <output_directory> <project_name>

If not set with options, the repository name is the same as the <project
name>

If not set with options, the main github user is the same as the
repository name.'''


GITWASH_CENTRAL = 'git://github.com/matthew-brett/gitwash.git'
GITWASH_BRANCH = 'master'


if __name__ == '__main__':
    from optparse import OptionParser
    parser = OptionParser()
    parser.set_usage(parser.get_usage().strip() + USAGE)
    parser.add_option("--repo-name", dest="repo_name",
                      help="repository name - e.g. nitime",
                      metavar="REPO_NAME")
    parser.add_option("--github-user", dest="main_gh_user",
                      help="github username for main repo - e.g fperez",
                      metavar="MAIN_GH_USER")
    parser.add_option("--gitwash-url", dest="gitwash_url",
                      help="URL to gitwash repository - default %s"
                      % GITWASH_CENTRAL, 
                      default=GITWASH_CENTRAL,
                      metavar="GITWASH_URL")
    parser.add_option("--gitwash-branch", dest="gitwash_branch",
                      help="branch in gitwash repository - default %s"
                      % GITWASH_BRANCH,
                      default=GITWASH_BRANCH,
                      metavar="GITWASH_BRANCH")
    parser.add_option("--source-suffix", dest="source_suffix",
                      help="suffix of ReST source files - default '.rst'",
                      default='.rst',
                      metavar="SOURCE_SUFFIX")
    (options, args) = parser.parse_args()
    if len(args) < 2:
        parser.print_help()
        sys.exit()
    out_path, project_name = args
    if options.repo_name is None:
        options.repo_name = project_name
    if options.main_gh_user is None:
        options.main_gh_user = options.repo_name
    copy_replace((('PROJECTNAME', project_name),
                  ('REPONAME', options.repo_name),
                  ('MAIN_GH_USER', options.main_gh_user)),
                 out_path,
                 options.gitwash_url,
                 options.gitwash_branch,
                 cp_globs=(pjoin('gitwash', '*'),),
                 rep_globs=('*.rst',),
                 renames=(('\.rst$', options.source_suffix),))

########NEW FILE########
__FILENAME__ = make_examples
#!/usr/bin/env python
"""Run the py->rst conversion and run all examples.

This also creates the index.rst file appropriately, makes figures, etc.
"""
#-----------------------------------------------------------------------------
# Library imports
#-----------------------------------------------------------------------------

# Stdlib imports
import os
import sys

from glob import glob

# Third-party imports

# We must configure the mpl backend before making any further mpl imports
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from matplotlib._pylab_helpers import Gcf

# Local tools
from toollib import *

#-----------------------------------------------------------------------------
# Globals
#-----------------------------------------------------------------------------

examples_header = """

.. _examples:

========
Examples
========

.. include:: note_about_examples.txt

.. toctree::
   :maxdepth: 2
   
   
"""
#-----------------------------------------------------------------------------
# Function defintions
#-----------------------------------------------------------------------------

# These global variables let show() be called by the scripts in the usual
# manner, but when generating examples, we override it to write the figures to
# files with a known name (derived from the script name) plus a counter
figure_basename = None

# We must change the show command to save instead 
def show():
    allfm = Gcf.get_all_fig_managers()
    for fcount, fm in enumerate(allfm):
        fm.canvas.figure.savefig('%s_%02i.png' %
                                 (figure_basename, fcount+1))

_mpl_show = plt.show
plt.show = show

#-----------------------------------------------------------------------------
# Main script
#-----------------------------------------------------------------------------

# Work in examples directory
cd('examples')
if not os.getcwd().endswith('doc/examples'):
    raise OSError('This must be run from doc/examples directory')

# Run the conversion from .py to rst file
sh('../../tools/ex2rst --project Nitime --outdir . .')

# Make the index.rst file
index = open('index.rst', 'w')
index.write(examples_header)
for name in [os.path.splitext(f)[0] for f in glob('*.rst')]:
    #Don't add the index in there to avoid sphinx errors and don't add the
    #note_about examples again (because it was added at the top):
    if name not in(['index','note_about_examples']):
        index.write('   %s\n' % name)
index.close()
# Execute each python script in the directory.
if '--no-exec' in sys.argv:
    pass
else:
    if not os.path.isdir('fig'):
        os.mkdir('fig')

    for script in glob('*.py'):
        figure_basename = pjoin('fig', os.path.splitext(script)[0])
        execfile(script)
        plt.close('all')
    

########NEW FILE########
__FILENAME__ = sneeze
#!/usr/bin/env python
"""Script to run nose with coverage reporting without boilerplate params.

Usage:
  sneeze test_coordinate_system.py

Coverage will be reported on the module extracted from the test file
name by removing the 'test_' prefix and '.py' suffix.  In the above
example, we'd get the coverage on the coordinate_system module.  The
test file is searched for an import statement containing the module
name.

The nose command would look like this:

nosetests -sv --with-coverage --cover-package=nipy.core.reference.coordinate_system test_coordinate_system.py

"""

import re
import os
import sys
import nose

test_file = sys.argv[1]
module = os.path.splitext(test_file)[0] # remove '.py' extension
module = module.split('test_')[1] # remove 'test_' prefix
regexp = "[\w\.]+%s"%module
compexp = re.compile(regexp)

cover_pkg = None
fp = open(test_file, 'r')
for line in fp:
    if line.startswith('from') or line.startswith('import'):
        pkg = re.search(regexp, line)
        if pkg:
            cover_pkg = pkg.group()
            break
fp.close()

if cover_pkg:
    cover_arg = '--cover-package=%s' % cover_pkg
    sys.argv += ['-sv', '--with-coverage', cover_arg]
    # Print out command for user feedback and debugging
    cmd = 'nosetests -sv --with-coverage %s %s' % (cover_arg, test_file)
    print cmd
    print
    nose.run()
else:
    raise ValueError('Unable to find module %s imported in test file %s'
                     % (module, test_file))

########NEW FILE########
__FILENAME__ = toollib
"""Various utilities common to IPython release and maintenance tools.
"""
# Library imports
import os
import sys

from subprocess import Popen, PIPE, CalledProcessError, check_call

from distutils.dir_util import remove_tree

# Useful shorthands
pjoin = os.path.join
cd = os.chdir

# Utility functions

#-----------------------------------------------------------------------------
# Functions
#-----------------------------------------------------------------------------
def sh(cmd):
    """Execute command in a subshell, return status code."""
    return check_call(cmd, shell=True)


def compile_tree():
    """Compile all Python files below current directory."""
    vstr = '.'.join(map(str,sys.version_info[:2]))
    stat = os.system('python %s/lib/python%s/compileall.py .' %
                     (sys.prefix,vstr))
    if stat:
        msg = '*** ERROR: Some Python files in tree do NOT compile! ***\n'
        msg += 'See messages above for the actual file that produced it.\n'
        raise SystemExit(msg)

########NEW FILE########
